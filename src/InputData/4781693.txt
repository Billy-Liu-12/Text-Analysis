Clustering Massive Categorical Data with Class Association Rules

Abstract  Clustering algorithms partition data sets into  groups of objects such that the pairwise similarity  between objects within the same cluster is higher than  those assigned to different clusters. Defining a  similarity measure becomes challenging in the  presence of categorical data and affects the quality  and meaningfulness of the clusters formed.

Furthermore, the curse of dimensionality diminishes  the robustness of such measures. This paper introduces  SCAR (Supervised Clustering with Association Rules)  a nontraditional algorithm for clustering massive high  dimensional categorical data. SCAR is robust to the  curse of dimensionality, it relies on association rules  as an intuitive way to evaluate the similarity between  objects and group them.

1. Introduction  Traditional clustering algorithms rely heavily on a similarity or dissimilarity measure to evaluate the proximity between pairs of data points. The Euclidean distance and other Lp norms that are usually considered as good proximity measures when clustering data with all numerical attributes, become unsuitable when some categorical attributes come into play or when all attributes are categorical. Furthermore, such distance metrics become less favorable in high dimensions due to the curse of dimensionality. While the literature is rich with algorithms for clustering continuous data, clustering categorical data remains a challenging problem. We introduce SCAR, a new algorithm for clustering categorical data. An innovation we apply is to transform the unsupervised clustering problem into a supervised learning problem and learn the candidate clusters with class association rules. Furthermore, metarules can be used to possibly merge the clusters formed. The rest of this paper is organized in four sections. In the following section, we account for literature dealing with the same problem. Section 3, the  core of the paper, is where we layout SCAR. In section 4, we compare the performance of SCAR with competing algorithms on a benchmark data set. A conclusion follows.

2. Related Work?  The most common way of measuring the proximity between categorical data is to use simple matching distance (which is a count of the number of matching attributes divided by the total number of attributes).

The Jaccard Coefficient [9] has also been used as a similarity measure between transactions in transactional data. These two non-metric proximity measures, although they reflect mutual proximity between all pairs of data points, do not give a global measure of the topology of the dataset. Relying on such non-metric proximity measures in the presence of categorical attributes limits the choice of the clustering algorithms that can be used. The minimum Spanning Tree (MST) hierarchical clustering and the hierarchical clustering with group averages [9] are widely used in such situations. The MST algorithm is known to be very sensitive to outliers while the group average algorithm has a tendency to split large clusters.

With the objective of enabling distance based clustering methods in data sets with mixed attributes, Tuv and Runger [7] suggested a procedure for mapping categorical variables to numeric scores. The scoring approach explores mutual relationships between variables in the data set and attempts to preserve the mutual information between all the variables. It uses a supervised contrasting independence clustering (SCI) method relying on CART [5] as a supervised learning tool to discover contrasts between the original data and artificially generated data. Our work uses a similar approach but exploits class association rules to form the clusters directly without creating any scoring scheme.

Several approaches that deal with the problem of clustering data with categorical attributes were suggested in the literature. Guha et al. [10] introduced ROCK, an agglomerative hierarchical clustering      algorithm, which heuristically optimizes a criterion function defined in terms of the number of links between tuples. The number of links between two tuples refers to the number of common neighbors that they have in the data set. This algorithm requires the user to select some parameters which could affect the resulting clusters.

Han et al. [11] addressed the problem of clustering transactional data. Their solution consisted of finding frequent itemsets from the transactions and constructing and then partitioning a weighted hypergraph whose vertices represent the frequent items. A scoring metric is used to assign the transactions to one of the k hypergraphs.

Gibson et al. [14] introduced STIRR, an algorithm based on nonlinear dynamical systems to cluster categorical data. Zhan et al. [16] argued that STIRR is not guaranteed to converge and suggested a modification that ensures convergence.

Peter and Zaki introduced CLICK [15] which maps the categorical clustering problem to the problem of enumerating k-partite cliques in a k-partite graph [17].

This algorithm uses a compressed representation of the data set which is assumed to fit into main memory.

The algorithm COOLCAT [12] uses an entropy based similarity measure to cluster categorical data.

Cristofor et al. [13], suggested a solution for clustering categorical data using an entropy based measure to quantify dissimilarity between clusters. Genetic Algorithms are used to form the clusters.

3. Supervised Clustering with Association  Rules  Our proposed framework relies on a technique for transforming a density estimation problem into one of supervised function approximation [8]. Suppose f(x) is an unknown density for estimation and f0(x) is a specified reference density function. For example, f0(x) might be the uniform density over the range of the variables. Assume that the data set x1; x2?xn is an i.i.d random sample drawn from f(x) and xn+1; xn+2?x2n a random sample of the same size n drawn from f0(x).

The combined sample can be treated as a random sample from the mixture density (f(x) + f0(x))/2. If we assign Y = 1 to each sample point drawn from f(x) and Y = 0 to those drawn from f0(x), then p(x)=E(Y=x)=f(x)/(f(x)+f0(x)) can be estimated by supervised learning using the combined sample (x1, y1),  (x2, y2), ? (x2n, y2n). The estimate f p(x) is then used to estimate the unknown density f(x):  ?  This result is independent of the nature of the X-space (continuous, categorical or mixed). Furthermore, the  reference density is usually dictated by the properties of the data that need to be learned and the choice of supervised learning algorithms to learn f(x) is driven by the learning objective.

For the rest of this paper, consider a data set D containing n samples. Every sample in D is described in terms of p attributes represented by the following  set . Also assume that each random  variable Xj, , can assume values from a  discrete finite set  where pj  denotes the cardinality of Sj .Thus each xi in D,  , can be written as  where xij can take any value  in Sj , and  Our objective is to cluster the n samples from D into an appropriate number of clusters. We transform this unsupervised learning problem into a supervised one as described above and use class association rules [6] along with their corresponding metarules [2] to find and then merge the clusters. We call the resulting algorithm SCAR (Supervised Clustering with Association Rules). Before we introduce the mechanics of SCAR, we explain in Sections 3.1, 3.2 and 3.3 important details about different components of the algorithm.

3.1. Clustering as Supervised Learning  For the clustering problem at hand, we are looking for high density regions of observations which exhibit strong associations between variables that could not have resulted due to randomness alone. Thus the  reference distribution for could be constructed as a joint density of independent X variables:  The original data can be used to generate a sample  from  by randomly permuting (independently)  the values of each variable Xi, . After generating the artificial data, we form a training sample (x1, y1), (x2, y2),? (x2n, y2n)?in the same way it is done in the density estimation problem above.

The training sample is then used to find conjunctive rules (associations among the variables) which indicate relatively high density regions in the original data when contrasted against the random data. In a categorical feature space, tree based classifiers such as CART [5] would be most appropriate to find these associations but their greedy nature could miss some important regions. We learn association rules [1] for an     exhaustive search for associations among the attributes from X that predict strongly, relative to the random data, the Class Y = 1.

3.2. Class Association Rules?  Association rules mining emerged as a technique for finding interesting rules from transactional databases [1]. An association rule is an expression of the form A  C, where A and C are subsets of the set of items. Here A is referred to as the set of antecedents and C as the set of consequents. The subsets A and C are disjoint. The importance of a rule is evaluated by its support and confidence. The support of a rule is the fraction of all transactions where the set of antecedents A and the set of consequents C apply simultaneously.

The support of a rule is a measure of its importance in terms of the number of transactions. The higher the support, the more important the rule is. The confidence of a rule is the fraction of transactions containing the set of antecedents A which also contain the set of consequents C. A minimum support and confidence thresholds are usually pre-specified before mining for association rules.

Association rules mining is also used in classification; the integration of classification and association rules mining is known as associative classification. CBA [6] is an association rules based classifier where the association rules with consequent restricted to the classification class attribute are mined, pruned, organized and then used for classification. In this work, the goal is to find from the pooled data set (x1, y1), (x2, y2),? (x2n, y2n), class association rules with the consequent restricted to the Class Y = 1. This could also be expressed as finding subsets of the integers  and corresponding values , ,  , such that , support of  the rule, is larger than the pre-specified support  threshold, minsup, and ,  confidence of the rule, is larger than the pre-specified confidence threshold, minconf.

The resulting rules meeting the support and confidence thresholds will have the following form:  Let represent the set of the m  resulting rules. These rules are the key to finding the  potential clusters.??  3.3. Finding metarules to merge the clusters  Metarules [2] are one-way associations between a set of rules with the same consequent. One-way  association rules refer to rules with a single antecedent and a single consequent. The resulting metarules provide a summary of the containment and overlap between the rules and hence can be used to organize the discovered rules. To find the metarules from the set R, each rule in R is mapped to the data rows which support its set of antecedents. This results in a new set  of transactions  such that  every element qj from Q is a subset of rules from R.

Metarules between the rules of R are then learned from the transactions of Q. Since the rules from R are used to find the potential clusters, and the discovered metarules express associations between the rules, they also reveal the similarities between the clusters and could hence be used to merge similar clusters when the desired number of clusters is a priori specified.

3.4. The SCAR algorithm?  We cluster the set of observations in D by transforming this unsupervised leaning problem into a supervised one as described in Section 3.1 and then finding association rules with the consequent restricted to the class Y = 1 as a supervised approach to finding predictive associations of the class Y = 1. These rules are used to assign each sample from D to the corresponding cluster. Having defined initial clusters, metarules can be used to further merge them into larger clusters.

The SCAR algorithm proceeds along four steps and an optional fifth step as described below:  1. For each attribute Xi in X , , randomly  permute  to obtain the set  . Combine the original data set D=  and the random sample .

Assign Y = 1 to each sample point from  and Y = 0 to samples from . Let Dp refer to the resulting data set.

2. Find from Dp association rules with the consequent Y = 1. Any association rules mining algorithm such as Apriori [1] could be used for this task. If Dp contains any missing observations, then they should be ignored during the association rules discovery process. When specifying minsup, this threshold controls the minimum cluster cardinality desired. When learning class association rules from the pooled data set, we suggest using minsup = 5% which is equivalent to 10% of the actual data set D. This threshold could be reduced if the goal is to detect smaller clusters. On the other hand, minconf reflects the desired accuracy in the class association rules, but a high confidence threshold may fail to cover the original data set D. At a fixed     minsup, we suggest setting minconf at a value greater than 50% such that the discovered rules cover most of the original data set D. A low confidence threshold should not be a concern here because a rule with a confidence greater than 50% still indicates that the rule is not fitting the artificial structureless data which  represents 50% of the combined data. Let  refer to the discovered rules.

3. Sort the discovered rules in decreasing order by their support then by their confidence and finally by the number of their antecedents. Let  refer to the sorted set of rules.

The indexes between parentheses indicate that the rules are in sorted order.

4. Find the clusters as follows: Set Dr = D, (Dr is used here to find the rows supporting the rules.)  Set , (C refers to the set of clusters.)  Set , (Rc represents the set of rules associated with the clusters of C.)  For each rule r(i) in Rs, , in sequence, do the following:  Find from Dr the nonempty sets Ci of rows which support the rule r(i)  If , then and  and .

repeat until i = m or until .

Assume that we find l < m, clusters .

and let represent the  corresponding set of rules from Rs used to generate each cluster of C. i.e. the cluster Ci was formed using  the rule rCi , .

5. This last step is optional, and it is where identify relationships between the clusters from C finding associations between the rules from Rc and thereafter possibly merge them. We achieve this by finding metarules between the rules of Rc.

4. Experimental Results  We use the Iris data set [4] to illustrate the suggested clustering algorithm. It is a data set with 150 random samples of flowers from the iris species Setosa, Versicolor, and Virginica. From each species there are 50 observations for sepal length, sepal width, petal length, and petal width in cm. Since this data set is well suited for distance based clustering, we compare the results of K-means with our method.

Since SCAR applies only to a categorical feature space, we discretized the iris data set using the RFDisc [3] algorithm before finding the clusters. The two sets of clusters are compared in Table 1. The values in the diagonal cells of this table show that the clusters found using SCAR match closely those found with K-means.

Table 1: Cross tabulation between the two sets of clusters  SCAR- cluster1  SCAR- cluster2  SCAR- cluster3  Total  K-means cluster 1  36 3 0 39  K-means cluster 2  10 51 0 61  K-means cluster 3  0 0 50 50  Total 46 54 50 150  The distribution of the three iris species within the three clusters formed using SCAR and those formed with K-means is illustrated in Table 2. It is clear that SCAR resulted in purer clusters in terms of iris species.?  Table 2: Distribution of the Iris species within the clusters formed using SCAR and K-means  Cluster resulting from SCAR Cluster1 Cluster2 Cluster3 Total  Setosa 0 0 50 50  Versicolor 1 49 0 50  Virginica 45 5 0 50  Total 46 54 50 150  Cluster resulting from K-means  Setosa 0 0 50 50  Versicolor 3 47 0 50  Virginica 36 14 0 50  Total 39 61 50 150  We have also compared the performance of the clustering algorithm introduced herein with the results of two other clustering algorithms when used with the Congressional Voting data and the Mushroom data [4].

For both data sets, we compared the clusters found using SCAR with the clusters generated respectively with a traditional centroid-based hierarchical clustering algorithm [9] and a robust clustering algorithm for categorical data, ROCK [10]. The experiments show that SCAR outperforms the hierarchical centroid-based algorithm and delivers a comparable performance to ROCK. The experimental setup and detailed results of these experiments are available in [3]. ?     5. Summary and conclusions  This paper introduces SCAR, a novel approach to clustering categorical data. It adopts a supervised approach to clustering. Experimental results indicate SCAR?s performance. SCAR is however, free of the shortcomings of competing algorithms: By transforming the clustering problem into supervised learning, SCAR does not need to use any proximity measure to group objects. Furthermore, class association rules deal naturally with categorical data and also enable SCAR to scale well. Finally, metarules give SCAR the ability to learn the number of clusters to be formed without any prior knowledge of the structure of the data.

6. References  [1] R., Agrawal, T., Imielinski and A. Swami, ?Mining Association Rules between sets of items in large relational Management of Data, 1993, pp.207-216.

[2] A. Berrado and G.C. Runger, ?Using Metarules to Organize and Prune the Discovered Association Rules?, Data Mining and Knowledge Discovery, 2007, 14(3), pp 409-431.

[3] A. Berrado, Supervised Rule Discovery for Rare Events in Mixed Process Data. Ph.D. Thesis, ASU, 2005.

[4] C.L. Blake and C.J. Merz, UCI Repository of Machine Learning Databases, http://www.ics.uci.edu/ mlearn/MLRepository.html, UC Irvine, 1998  [5] L. Breiman, J.H. Friedman, R.A Olshen and C.J. Stone, Classification and Regression Trees, Belmont, CA: Wadsworth, 1984  [6] B. Liu, W. Hsu, and Ma, Y, ?Integrating Classification and Association Rule Mining." KDD, pp.80-86, 1998.

[7] E. Tuv and G.C. Runger, ?Scoring Levels of Categorical Variables with Heterogeneous Data." IEEE Intelligent Systems, 2004.

[8] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning: Data Mining, Inference and Prediction, Springer-Verlag, 2001.

[9] K.J Anil and C.D Richard. Algorithms for Clustering Data. Prentice Hall, Englewood Cliffs, New Jersey, 1988.

[10] S. Guha, R. Rastogi and K. Shim, ?ROCK: A Robust Clustering Algorithm for Categorical Attributes.? Information Systems, 25(5), pp.345-366, 2000  [11] E-H. Han, G. Karypis, V. Kumar and B. Mobasher, ?Clustering based on association rule hypergraphs."  SIGMOD Workshop on Research Issues on Data Mining and Knowledge Discovery, 1997.

[12] D. Barbara, Y. Li and J. Couto, ?COOLCAT: An Entropy-Based Algorithm For Categorical Clustering.? In ACM Press, pp. 582-589, 2002.

[13] D. Cristofor, D. A. Simovici, ?An Information- Theoretical Approach to Clustering Categorical Databases using Genetic Algorithms.?, 2nd SIAM ICDM, Workshop on clustering high dimensional data, 2002.

[14] D. Gibson, J. Kleinberg, and P. Raghavan., ?Clustering categorical data: An approach based on dynamical systems." In 24th Int'l Conference on Very Large Databases, August  [15] M. Peters, M. J. Zaki, ?CLICKS: Clustering Categorical Data using K-partite Maximal Cliques.", IEEE International Conference on Data Engineering, Tokyo, Japan, April 2005  [16] Y. Zhang, A. Fu, C. Cai, and P. Heng, ?Clustering categorical data." In Proceedings of the ICDE, page 305,  [17] K. Thulasiraman and M. N. S. Swamy, Graphs: Theory and Algorithms. Wiley, 1992.

