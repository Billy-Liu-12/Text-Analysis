Memory Hierarchy Aware Parallel Priority Based Data Structures

Abstract?With the proliferation of multicore and manycore architectures, memory hierarchy plays an important role in realizing expected performance of memory intensive scientific applications. A vast majority of scientific applications require a priority based data structure to discriminate among available data elements; for instance, a priority based data structure is imperative for extracting the earliest events in a discrete event simulation, identifying urgent tasks in a parallel scheduler, or exploring most promising sub-problems in a state-space search.

Traditional priority based data structures are tree-based that makes them cache unfriendly due to the exponentially increasing distance between parent and child nodes. Fine grained large- scale applications further cause excessive contention among com- peting processors due to frequent updates. In this dissertation we propose a priority based data structure that adapts to the memory hierarchy of the underlying computer system. The top priority items are aggregated and kept into a working subset of data items that is readily accessible for processing.



I. INTRODUCTION  With the proliferation of multicore and manycore architec- tures, memory hierarchy plays an important role in realizing expected performance of memory intensive scientific applica- tions. The availability of multiple levels of cache hierarchy presents a challenge for the programmers and researchers to redesign their applications and algorithms to utilize these machines efficiently. Parallel computing community excels at utilizing multicore architectures for regular, memory inten- sive applications such as stencil kernels. Researchers have proposed auto tuners and auto optimizers that can take a traditional stencil kernel and convert it into a stencil kernel optimized for underlying multicore architecture. Such ap- plications and frameworks were relatively easier to explore due to the apriori knowledge of data access patterns. In this dissertation we are exploring priority based data structures needed by applications where data access patterns can not be predicted at compile time. There is a wide variety of such applications including discrete event simulation, state space search, and parallel scheduler etc.



II. RELATED WORK Priority based data structures are not a new area of research.

Many variations of priority queues were proposed during 90s? to cater to the needs of almost all scientific applications that require priority based data structures. These data structures can be categorized by the following two approaches reported in literature to parallelize them. The first one is to speed up the individual queue operations known from the sequential setting using a small number of processors [1], [2]. The other approach is to support insertion and deletion of k items where k is a constant [3], [4]. [5] gave an efficient implementation of Parallel Heap on bus-based shared memory machines such as sequent balance and SGI power and later on SGI Origin systems, which are NUMA, shared machines.

With the advent of multicore and manycore architectures in CPU design it has become essential to revisit these data structures. Research has also reported effective techniques to make use of multicore computers and memory hierarchy for memory intensive scientific applications such as sparse matrix vector multiplication [6], Matrix Reduction [7], and divide and conquer algorithms [8] among others. Nevertheless, these problems are regular in nature thereby exploiting data level blocking and/or thread level blocking are sufficient to exploit memory hierarchy.



III. RESEARCH GOALS In modern CPU designs each processor has multiple cores  on the same chip. Additionally, each core has its small fast private cache as well as there is a larger cache shared among all of the cores on a chip. The latest chips such as Intel Nehalem and AMD Phenom have 3 levels of cache hierarchy.

As correctly predicted by Yale Patt [9] in 2001, modern CPUs were supposed to have three or more levels of caches which means we can safely expect chip manufactures to add one or more levels of caches in the future architectures. However, unlike some manycore architectures such as GPUs, multicores do not provide researchers with explicit software control over data allocation to these different levels of available cache  2011 IEEE International Parallel & Distributed Processing Symposium  DOI 10.1109/IPDPS.2011.372   2011 IEEE International Parallel & Distributed Processing Symposium  DOI 10.1109/IPDPS.2011.372   2011 IEEE International Parallel & Distributed Processing Symposium  DOI 10.1109/IPDPS.2011.372   2011 IEEE International Parallel & Distributed Processing Symposium  DOI 10.1109/IPDPS.2011.372   2011 IEEE International Parallel & Distributed Processing Symposium  DOI 10.1109/IPDPS.2011.372   2011 IEEE International Parallel & Distributed Processing Symposium  DOI 10.1109/IPDPS.2011.372   2011 IEEE International Parallel & Distributed Processing Symposium  DOI 10.1109/IPDPS.2011.372   2011 IEEE International Parallel & Distributed Processing Symposium  DOI 10.1109/IPDPS.2011.372   2011 IEEE International Parallel & Distributed Processing Symposium  DOI 10.1109/IPDPS.2011.372     memories. An application that performs ideally on a multicore simulator may not show even half as good performance on a real multicore chip due to factors that cannot be controlled by the application. For instance, in order to control the allocation of data to a specific level of cache hierarchy, the only way is to exploit the memory replacement policy. On a computer system that uses LRU replacement policy the thread affinity to a processor can be controlled in such a manner that CPU stalls due to memory latency can be reduced significantly.

Nevertheless, it is challenging because the cache coherency protocols [10] vary among different manufacturers.

Therefore, the goals of our research are 1) To study existing priority based data structures to as-  sess their capabilities to leverage memory hierarchy on multicore processors,  2) To create a framework to control thread and memory affinity,  3) To employ prefetching to help reduce memory latency, and  4) To implement memory hierarchy aware, parallel, priority-based data structures

IV. CURRENT PROGRESS  We have successfully engineered Parallel Priority Queue [5] (PPQ) data structure for multicore architecture to understand the behavior of traditional priority based data structures on newer chips with multiple levels of memory hierarchy. For this study we chose PPQ over other similar priority queues because its wide-node structure makes it easier to exploit the spatial locality in a cache memory. Based on the amount of concurrency afforded by a parallel application, it can yield hundreds to thousands of top-priority items at a time. As shown in 1 for extremely fine-grained applications, it is 2-3 times faster than a sequential heap. For larger grain, it scales quickly to linear speedups.

Fig 1 represents the absolute speedup for PPQ. The size of heap was 226 and the node size was 213. We had a total of 256 cycles of insertion and deletion processes. For large grain sizes, it can be seen that we achieve an absolute speedup of 12.65 using 16 processors. Interestingly, for p = 7 and p = 8 processors we have super linear speed up. This is due to the better cache locality afforded by PPQ over serial heap.



V. REMAINING OBJECTIVES AND CHALLENGES  Although the memory throughput depends on the applica- tion employing our data structure, we haven?t seen satisfying results with PPQ. PPQ demonstrates poor performance in terms of memory throughput when compared with the results of the memory benchmarking tools on the underlying system.

The reason for this is the inherent difficulty of load balancing among available processors. Since PPQ is a tree-based data  Fig. 1. Overall performance of PPQ for varying granularity, heapsize = 226, k = 213  stricture, the cores handling the levels away from root have considerably more load than those handling the levels closer to the root. Moreover, the number of levels do not increase linearly with increasing heap size. PPQ currently does not support allocation of a level to multiple processor therefore it is difficult to employ a large number of processors. We are going to explore and extend other priority based data structures on multicores to analyze their behavior on these newer chips. Learning from this experience we will know the fundamental properties of priority based data structures that adapts well to multicore. For instance, the wide-node data structures appear to work well on multicores compared to single node priority queues. Such properties will guide the design of a new parallel priority based data structure. Our implementation code for PPQ is available online 1.

