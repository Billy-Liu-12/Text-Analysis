

Causal Possibility Model Structures  Lawrence J. Mazlack Computer Science  University of Cincinnati Cincinnati, Ohio 45221-0030  mazlack@uc.edu  Abslracl:  Causality occupies a position of cen- trality in human reasoning. It plays an essent ia l role in commonsense human decision-making. De- termining causes has been a tantalizing g o a l throughout human history. Proper sacrifices to t h e gods were thought to bring rewards; failure to make the proper observations to led to disaster. Today, data mining holds the promise uf extracting unsus- pected information from very large databases. The most common methods build association rules. I n many ways, the interest in association rules is t h a t they offer the promise (or illusion) of causal, or a t least, predictive relationships. However, a s so - ciation rules only calculate a joint occurrence fre- quency; they do not express a causal re la t ionship .

If causal relationships could be discouvered, i t would be very useful. This paper explores the p o s - sible representation of causality drawn from la rge data sets.

I .  INTRODUCTION  Causality occupies a position of centrality in common- sense human reasoning. It plays an essential role in human decision-making by providing a basis for choosing an action that may lead to a desired result. Considerable effort has been spent examining causation. Philosophers, mathematicians, computer scientists, cognitive psychologists, psychologists, and others have explored questions of causation beginning at least three thousand years ago.

Determining causality entails recognizing causal relation- ships; e.g., a causes to happen. Whether causality can be recognized at all has been a speculation for thousands of years. At the same time, in our daily lives, we rely on the commonsense observation that causality exists. The computa- tional concern is bow to recognize and represent common sense causal relationships.

Data mining is an advanced tool for managing large masses of data. Data mining is secondmy analysis. It is a x c o n m analysis because the data were not collected to answer qnes- tions now posed. Secondary analysis of previously collected data eliminates the possibility of experimentally varying the data to identify causal relationships.

There are several different data mining products. The most common are conditional rules or association rules. Condi- tional rules are most often drawn from induced trees while association mles are most often learned from tabular data. Of  these, the most common data mining product is association rules; for example:  Conditional rule: IF Age < 20  THEN Income < $10,000 with {belief = 0.8)  -Association rule: Customers who buy beer and sausage also tend to buy mustard  with {confidence = 0.8) {support = 0.15)  At first glance, these structures seem to imply a causal or cause-effect relationship. For example: A customer's purchase of both sausage and beer the customer to also buy mustard. In fact, when typically developed, association rules do not describe causality. Also, the strength of dependency may be very different from a respective association value. All that can be said with certainty is that associations describe the strength of CO-occurrences. Sometimes, the relationship might be causal; for example, if someone eats salty peanuts and then drinks beer, there is probably a causal relationship.

On the other hand, if a dog barks in Kabul and a cat meows in Beijing, there is probably not a causal relationship.

Causal relationships exist in the commonsense world. If someone fails to stop at a red light and there is an automobile accident, it can be said that the failure to stop was the accident's cause. Another way to think of causal relationships is counterfactually. For example, if a driver dies in an accident, it might be said that had the accident not occu1Ted; they would still be alive.

of false causal recognition. For example, a coach may win a game when wearing a particular pair of socks, then always wear the same socks to games. More interesting, is the occa- sional false causality between music and motion. For exam- ple, Lillian Schwartz developed a series of computer generated images, sequenced them, and attached a sound track (usually Mozart). While there were some connections between one image and the next, the music was not scored to the images; however, on viewing them, the music appeared to be con- nected. All of the connections were observer supplied.

A common approach to recognizing causal relationships is by manipulating variables through experimentation, how to  Complicating causal recognition are the many cases   mailto:mazlack@uc.edu   accomplish causal discouvery in purely observational data is not solved. [Observational data is the most likely to be avail- able for data mining analysis.)  Algorithms for discouvery in observational data oft . en use correlation and probability independence to find possible causal relationships. If two variables are statistically inde- pendent, it can be asserted that they are not causally related.

The reverse is not necessarily true.

Real world events are often affected by a large number of potential factors. For example, with plant growth, many fac- tors such as temperature, chemicals in the soil, types of crea- tures present, etc., can all affect plant growth. What is un- known is what causal factors will or will not be present in the data; and, how many of the underlying causal relation- ships can be discouvered among observational data.

Some define causeeffect relationships as: When a occurs, /3- occurs. This is inconsistent with our commonsense understanding of causality. A simple environment example: When a hammer hits a bottle, the bottle usually breaks. A complex environment example: When a plant receives water, it usually grows.

An important part of data mining is understanding whether there is a relationship between data items. Sometimes, data items may occur in pairs but may not have a deterministic relationship; for example, a grocery store shopper may buy both bread and milk at the same time. Most of the time, the milk purchase is not caused by the bread purchase ; nor is the bread purchase caused by the milk purchase .

Altematively, if someone buys strawberries, this may caus- ally affect the purchase of whipped cream. Some people who buy strawbemes want whipped cream with them; of these, the desire for the whipped cream varies. So, we have a conditional primary effect [whipped cream purchase) modified by a secon- dary effect [desire). How to represent all of this is open.

A largely unexplored aspect of mined rules is how to deter- mine when one event causes another. Given that a and /3 am variables and there appears to be a statistical covariability between a and p, is this covariability a causal relation? More generally, when is any pair wise relationship causal? Differ- entiation between covariabilty and causality is difficult.

Some problems with discouvering causality include: -Adequately defining a causal relation,  Representing possible causal relations, *Computing causal strengths, *Missing attributes that have a causal effect, .Distinguishing between association and causal values, *Inferring causes and effects from the representation.

Beyond data mining, causality is a fundamentally interest- ing area for machine based systems to investigate. It is an area where interest waxes and wanes; in part because of defmi- tional and complexity difficulties. Activities in both philoso- phy and psychology [Glymour, 20011 overlap and illuminate computationally focused work.

2. CAUSALITY  Centuries ago, in their quest to unravel the future, mystics aspired to decipher the cries of birds, the pattems of the stars and the garbled utterances of oracles. Kings and generals would offer precious rewards for the information soothsayers furnished. Today, though predictive methods are different from those of the ancient world, the knowledge that dependency recognition attempts to provide are highly valued. From weather reports to stock market prediction, and from medical prognoses to social forecasting, superior insights about the shape of things to come are prized [Halpem, 20001.

Democritus, the Greek philosopher, once said ?Everything existing in the universe is the fruit of chance and necessity.? This seems self-evident. Both randomness and causation are in the world. Democritus used a poppy example. Whether the poppy seed lands on fertile soil or on a barren rock is chance.

If it takes root, however, it will grow into a poppy, not a geranium or a Siberian Husky [Lederman, 19931.

Beyond computational complexity and holistic knowledge issues, there appear to be inherent limits on whether causality can be determined. Among them are: *Quantum Physics: In particular, Heisenberg?s uncertainty principle  *Knowledge of the world might never be complete because we, as observers, are integral parts of what we observe Godel?s Theorem: which showed in any logical formulation of arithmetic that there would always be statements whose validity was indeterminate. This strongly suggests that there will always be inherently unpredictable aspects of the future.

.The Turing Halting Problem: Turning (as well as Church) showed that any problem solvable by a step-by-step-pme- dure could be solved using a Turing machine. However, there are many routines where you cannot ascertain if the program will take a finite, or an infinite number of steps.

Thus, there is a curtain between what can and cannot be known mathematically.

*Chaos Theory: Chaotic systems appear to be deterministic; but are computationally irreducible. If nature is chaotic at its core, it might be fully deterministic, yet wholly unpre- dictable [Halpem, 2000, 1391.

*Space Time: The malleability of Einstein?s space time which has the effect that what is ?now? and ?later? is local to a particular observer; another observer may have contra- dictory views.

.Arithmetic Indeterminism: Arithmetic itself has random aspects that introduce uncertainty as to whether equations may be solvable. Chatin 11987, 19901 discovered that Dio- phantine equations may or may not have solutions, depend- ing on the parameters chosen to form them. Whether a pa- rameter leads to a solvable equation appears to be random.

[Diophantine equations represent well-defined problems, emblematic of simple arithmetic procedures.)     Given determinism?s potential uncertainty and imprecision, we might throw up out hands in despair. It may well be that a precise and complete knowledge of causal events is uncertain.

On the other hand, we have a common sense belief that causal effects exist in the real world. If we can develop models tolerant of imprecision, it would be useful. Perhaps, the tools found in soft computing may be useful.

2.1 Nature Of Causal Relationshipa  The term causulily is used here in the everyday, informal sense. There are several strict definitions that are not wholly compatible with each other. The formal definition used in this paper is that if one thing occurs because of another thing, we say that there is a dependent or causal relationship.

W@ Figure I .  Dingrom indicating that a is c m s d y  dcgendent on p.

Some questions about causal relationships that would be desirable to answer are: .To what degree does a cause ,S?? Is the value for ,? sensitive  to a small change in the value of a?

-Does the relationship always hold in time and in every situa- tion? If it does not hold, can the particular situation when it does hold be discouvered?

*How should we describe the relationship between items that are causally related: probability, possibility? Can we say that there is a causal strength between two items; causal strength representing the degree of causal influence that items have over each other?

-1s it possible that there might be mutual dependencies; i.e., a -,?as well as ,? +a?  Is it possible that they do so with different strengths? They can be described as shown in Figure 2. where Sjj  represents the strength of the causal rela- tionship from i to j . For example, a could be short men and ,?could be tall women. If SqP meant the romantic attrac- tion that was caused in short men by the sight of tall women, it might be that > S,, . On the other hand, some would argue that causality should be completely asymmetric and if it appears that items have mutual influ- ences it is because there is another cause that causes both. A problem with this idea is that it can force to eventual regres- sion to a first cause; whether this is true or not, it is not useful for common sense representation.

It would seem that if there are causal relationships in mar-  ket basket data, there would often be imbalanced dependencies.

For example, if a customer first buys strawberries, there may be a reasonably good chance that she will then buy whipped  cream. Conversely, if she first buys whipped cream, the subsequent purchase of strawberries may be less likely.

It is potentially interesting to discouver the absence of a causal relationship; for example, discouvering the lack of a causal relationship in drug treatment?s of disease. If some potential cause can be eliminated, then attention can become more focused on other potentials.

Prediction is not the same as causality. Recognizing whether a causal relationship existed in the past is not the same as predicting that in the future one thing will occur be- cause of another thing. For example, knowing that a was a causal (or deterministic) factor fo rp  is different than saying whenever there is a, p will deterministically occur (or even probabilistically occur to a degree 1). There may be other necessary factors.

Causal necessity is not the same thing as causal suffi- ciency; for example, in order for event 6 to occur, events a,,?,p, need to occur. We can say that a, by itself, is neces- sary, but not sufficient.

Part of the difficulty of recognizing causality comes from  identifying relevant data. Some data might be redundant, some irrelevant, some are more important than others. Data can have a high dimensionality with only a relatively few utilitar- ian dimensions; i.e., data may have a higher dimensionality than necessary to fully describe a situation. In a large collec- tion of data, complexity may be unknown Dimensionality reduction is an important issue in learning from data.

A causal discovery method cannot transcend the prejudices of analysts. Often, the choice of what data points to include and which to leave out, which type of curve to fit (linear, exponential, periodic, etc.), what time increments to use (years, decades, centuries, etc.) and other model aspects depend on the instincts and preferences of researchers.

It may be possible to determine whether a collection of data is random or deterministic using attractor sets from Chaos theory [Packard, 19801. A low dimensional attractor set would indicate regular, periodic behavior and would indicate determi- nate behavior. On the other hand, high dimensional results would indicate random behavior.

2.2 Types Of Causality There are at least three ways that things may be said to be  related.

?Coincidental: Two things happen to describe the same object  *Functional: There is a generative relationship.

.Casual: One thing causes another thing to happen. There are  ?Chaining: In this case, there is a temporal chain of events, A,,A, ,..., A,, which terminates on A,. To what degree, ifany, does A i  ( i= l .  ..., n-I) cause A.? A spe- cial case of this is a backup mechanism or a preempted alternative. Suppose there is a chain of casual d e p d -  and have no determinative relationship between them.

at least four types of causality:     ence, A, causing A,; suppose that if A, does not occur, A, still occurs, now caused by the altemative cause 5, (which only occurs if A, does not).

Conjunction (Confluence): In this case, there is a Con- fluence of events, A ,,..., A,, and a resultant event, B.

To what degree, if any, did or does A i  cause B? A special case of this is r e d w h u  causation. Say that either A, or A, can cause B ;  and, both A, and A, occur simultaneously. What can be said to have caused B?

.Network: A network of events.

*Preventive: One thing prevents another; e.g., She pre-  Recognizing and defining causality is difficult. Causal claims have both a direct and a subjunctive complexity [Spines, 20001 - they are associated with claims about what did happen, or what did not happen, or has not happened yet, or what would have happened if some other circumstance had been otherwise. The following show some of the difficulties: *Example I :  Simultaneous Plant Death: My rose bushes and my neighbor?s rose bushes both die. Did the death of one cause the other to die? (Probably not, although the deaths are associated.)  ?Example 2: Drought: There has been a drought. My rose bushes and my neighbor?s rose bushes both die. Did the drought cause both rose bushes to die? (Most likely.)  .Example 3: Trajfic: My friend calls me up on the telephone and asks me to drive over and visit her. While driving over, I ignore a stop sign and drive through an intersection. Another driver hits me. I die. Who caused my death? -- Me? -- The other driver? -- My friend? -- The traffic engineer who designed the intersection? -- Fate?

.Example 4: Umbrella: A store owner doubles her dver - tising for umbrellas. Her sales increase by 20% What caused the increase? -- Advertising? -- Weather? -- Fashion? - Chance?

*Example 5: Poison: (Chance increase without causation) Fred and Ted both want Jack dead. Fred poisons Jack?s soup; and, Ted poisons his coffee. Each act increases Jack?s chance of dying. Jack eats the soup but (feeling rather unwell) leaves the coffee, and dies later. Ted?s act raised the chance of Jack?s death but was not a cause of it.

Exactly what makes a causal relationship is open to vary- ing definition. However, causal asymmetries often play a part [Hausman, 19981. Some claimed asymmetries are: .Time order: Effects do not come before effects (at least as locally observed)  *Probabilistic independence: Causes of an event are probabi- listically independent of another, while effects of a cause are probabilistically dependent on one another.

Counterfactual dependency: Effects counterfactually depend on their causes, while causes do not counterfactually depend  venfed the catastrophe.

on their effects and effects of a common cause do not coun- terfactually depend on each other.

Over determination: Effects over determine their causes, while causes rarely over determine their effects.

*Fixiryr Causes are : ?fixed? no later than their effects Connection dependency: If one were to break the connection between cause and effect. only the effect might be affected.

2.3 Classical Dependence  Statistical dependence is interesting in this context because it is often confused with causality. Such reasoning is not correct. Two events E,;E, may be statistical dependent be- cause both have a common cause Eo. But this does not mean that E,  is the cause of E,.

For example, lack of rain (Eo) may cause my rose bush to die (E,) as well as that of my neighbor (E,). This does not mean that the dying of my rose has caused the dying of my neighbor?s rose, or conversely. However, the two events E,, E2 are statistically dependent.

A causality relationship between two events E, and E2 will always give rise to a degree of statistical dependence between them. The converse is not true. A statistical dependence be- tween two events may; but need not, indicate a causal rela- tionship between them. There is a positive correlation if  However, all this tells us that it is an interesting relationship.

It does not tell us if there is a causal relationship.

2.4 Probabilistic Causation  prob(q,b,) > prob(q) prob(b,)  Probabilistic Causation designates a group of philosophical theories that aim to characterize the relationship between cause andeffect using the tools of probability theory. A pri- mary motivation is the desire for a theory of causation that does not presuppose physical determinism.

The success of quantum mechanics, and to a lesser extent, other theories using probability, brought some to question determinism. Some philosophers became interested in devel- oping causation theories that do not presuppose determinism.

One notable feature has been a commitment to indeter- minism, or rather, a commitment to the view that an adequate analysis of causation must apply equally to deterministic and indeterministic worlds. Mellor [I9951 argues that indeterministic causation is consistent with the connotations of causation. Hausman [1998], on the other hand, defends the view that in indeterministic settings there is, strictly speak- ing, no indeterministic causation, but rather deterministic causation of probabilities.

Following Suppes [I9701 and Lewis [1986], the approach has been to replace the thought that causes are sufficient for, or determine, their effects with the thought that a cause need only raise the probability of its effect. This shift of attention raises the issue of what kind of probability analysis, if any, is up to the job of underpinning indeterministic causation.

3. REPRESENTATION  Causal representation is an underdeveloped question. The representation used constrains and supports the methods that can be used. Several representations have been proposed.

3.1 Graphs  Some authors have suggested that sometimes it is possible to recognize causal relations through the use of directed graphs (digraphs) [Pearl, 19911 [Spirtes, 19931. In a digraph, the vertices correspond to the variables and each directed edge corresponds to a causal influence. Diagraphs are not cyclic; the same node in the graph cannot be visited twice. An exam- ple is shown in Figure 3. Pearl [2000] and Spirtes [2001] use a form of digraphs called DAGS for representing causal relationships.

Figure 3 (a)  An example digraph (DAG) (b) &ample insranrioting (a).

drinking alcohol  education  J.

Salary  Figure 4. Cyclic cnusol relorionships  Sometimes, cycles exist. For example, a person?s family medical history influences both whether they are depressive and whether they will have some diseases. Drinking alcohol combined with the genetic predisposition to certain disease in- fluences whether the person has a particular disease, which then influence depression, which in tum may influence the person?s drinking habits. Figure 4 shows the cyclic digraph for this example.

Developing directed acyclic graphs from data is computa- tionally expensive. The amount of work increases geometri- cally with the number of attributes. For constraint based methods, the reader is directed to Pearl [2000], Spirtes [ZOOO], Silverstein [1998], and Cooper [1997]. For Bayesian discou- very, the reader is directed to Heckerman [I9951 and Geiger 119951.

Quantitatively describing the relationships between the nodes can be complex. One possible general model is an extension of what can be found in Rehder [2002]; shown in Figure 5. The state value is 1/0 as an event either happens or does not.

E m Fizure 5. Rehder?s quanrirative relarimship between digraph nodes; c =  PfD), m = rhe probabiliry r h r  when D iF presenl, the causal mechanism brings obour E, b = the probobiliry rhal some other (unspecified) causal mechanism brings abour E.

b  Figure 6. Figure 5 applied ion ?bird? cmmple.

3.2 Probabilify And Decision Trees  Various forms of root initiated, tree-like graphs have been suggested [Shafer, 19961. Among them are:  Probability trees: Have a probability for every event in every situation, and hence a probability distribution and expected value for every variable in every situation.

?Decision trees: Trees in which branching probabilities are supplied for some, while others are unknown. An example of these is Martingale trees.

A tree is a digraph starting from one vertice, the root. In this case, the vertices represent situations. Each edge repre- sents a particular variable with a corresponding probability.

Time ordering of the variables is represented via the levels in the tree. The higher a variable is in the tree, the earlier it is in time. This can become ambiguous for networked repre- sentations; i.e., when a node can have more than two parents and thus two competing paths (and their imbedded time se- quences). By evaluating the expectation and probability changes among the situations (or nodes) in the tree, one can decide whether the two variables are causally related.

3.3 First Order Logic  Hobbs [2000] uses first-order logic to represent causal rela- tionships. One difficulty with this approach is that the ~epre- sentation does not allow for any gray areas. For example, if an event occurred when the wind was blowing east, how could a wind blowing east-northeast be accounted for? The causality inferred may be incorrect due to the representation?s rigidity.

Nor can first order logic deal with dependencies that arc only sometimes true. For example, sometimes when the wind blows hard, a tree falls. This kind of sometimes event description can possibly be statistically described. Altema- tively, a qualitative fuzzy measure might be applied.

Another problem is recognizing differing effect strengths.

For example, if some events in the causal complex are more     strongly tied to the effect? Also, it is not clear how a rela- tionship such as the following would be represented: a causes j 3  some of the time; j3  causes a some of the time; other times there is no causal relationship.

4. EPILOGUE  Causality occupies a central position in human common- sense reasoning. In particular, it plays an essential role in common sense human decision-making by providing a basis for choosing an action that is likely to lead to a desired result.

Whether recognizing causality can be done at all has been a speculation for thousands of years. At the same time, in our daily lives, we make the conimonsense observation that causality exists. Carrying this commonsense observation further, the concem is 'how to computationally recognize a causal relationship.

Data mining holds the promise of extracting unsuspected information from very large databases. Methods have been developed to build association rules. Association rules indi- cate the strength of association of two or more data attributes.

In many ways, the interest in association rules is that they offer the promise (or illusion) of causal, or at least, predictive relationships. However, association rules only calculate a joint Occurrence frequency; not causality. There are several open questions across all types of association rules. A funda- mental question is determining whether or not recognizing an association can,lead to recognizing a causal relationship.

A particular interest is in determining when causality can be said to be stronger or weaker. Either in the case where the causal strength may be different in two independent relation- ships; or, where in the case where two items each have a causal relationship on the other.

Causality is a central concept in many branches~of science and philosophy. In a,way, the term "causality" is like "truth" -- a word with many meanings and facets. Some of the defini- tions are extremely precise. Some of them involve a style of reasoning best be supported by fuzzy logic.

Defining and representing causal and potentially causal rela- tionships is necessary to using machine-based methods. A graph consisting of a collection of directed edges will most likely not offer a sufficiently rich representation. Representa- tions that embrace some aspects of imprecision are necessary.

A deep question is when anything can be said to cause any- thing else. And if it does, what is the nature of the causality?

There is a strong motivation to attempt causality discouvery in association rules. The research concern is how to best ap proach the recognition of causality or non-causality in asso- ciation rules. Or, if there is to recognize causality as long as association rules are the result of secondary analysis?

