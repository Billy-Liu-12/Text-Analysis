Mining Associations by Pattern Structure in Large Relational Tables

Abstract  Associarion rule mining aims ar discovering parrerns whose supporr is beyond a given rhreshold. Mining par- rerns composed of irems described by an arbitrary subser of arrribures in LI forge relarional table represents a new chal- lenge and has various pracrical applicarions. including rhe evenr management sysrems rhar morivared rhis work. The anribure combinarions that define rhe irems in a parrempm- vide rhe srrucrural information of rhe parrern. Currenr as- sociarion algorirhms do nor make full use ofrhe srrucrural informarion of rhe parrerns: rhe informarion is either lost &er ir is encoded wirh arrribure values, or is consrroined by a given hierarchy or raxonomy. Pattern srrucrures con- vey imporrant knowledge abour rhe parrerns. In rhis papes we present a novel archirecrure that organizes rhe mining space based on parfem srrucrures. By exploiting rhe inrer- relarionships among parrern structures, execurion rimes for mining can be reduced significanrly. This advanrage is demonsrrared by our experiments using both synrheric and mal-fife darosers.

1 Introduction  The problem of mining frequent itemsets in a set of transactions wherein each transaction is a set of items was first introduced by Agrawal et al [ I ] .  There is a press- ing need for algorithms to support relational data min- ing 141, as a majority of datasets in the real world are stored in the relational form, or generated on the fly by other query tools (e.g. OLAP tools) in the relational form.

An item can be described by a set of attributes [ 5 ,  101.

For instance, a system event can be described by attributes such as EVENTTYPE, CATEGORY, SOURCE, APPLICA- TION, HOST, and SEVERITY. The dataset shown in Fig- ure 1 contains five events that occurred rogerher (occurred during the same time interval according to their TIDs).

One association rule or pattern supported by the dataset is ?TCPConnectionClose (EVENT) from Vesuvio  0-7695-1754-4/02 517.00 Q 2002 IEEE 482  (HOST) occurs together with Security (CATEGORY) problems during Authorization (APPLICATION)?. In- teresting rules or patterns often relate to items each de- scribed by a subser of these attributes. We call such items structured-items.

Rules or patterns composed of structured-items are often more interesting and informative because their varied com- position enables them to represent concepts at all possible granularity levels. The structures of the items in a pattern combine to form the pattern structure. Pattern structure conveys important knowledge about the patterns. Our work in mining such patterns in event data brings forth some in- teresting issues and challenges.

EXPONENTIAL SEARCH SPACE. Multi-attribute items incur a huge search space. The item in RI is defined by two attributes: EVENTTYPE, and HOST. Indeed. any subset of attributes can be used to define a structured-item. This in- troduces a combinatorial challenge: in a dataset with N at- tributes, items can be defined in as many as Z N  - 1 different ways, in other words, they have as many as ZN - 1 different structures. Furthermore, they can be combined differently to form different patterns. This introduces another comhi- natorial challenge: patterns containing k items can have as many as (2Ny-2) different pattern structures (Lemma 1).

RELATIONAL SENSITIVE. The patterns embedded in the event data, unlike supermarket purchasing patterns that are more or less stable over the time, are constantly evolving as old problems in the system being solved, and new types of problems being generated. The mining algorithms should focus on newly generated data instead of the entire history archive. Furthermore, we often need to mine data streams generated on the fly by other query tools, such as OLAP, which requires our mining algorithms to be relational sensi- tive 141, as it is often inefficient and unnecessay to first con- vert the data to a mining format, then discover all frequent itemsets among them, and finally filter out the answers to the queries. The dynamic nature of the data prevents us from running a mining algorithm once and for all, and sav- ing the results for future analysis.

mailto:psyu}@us.ibm.com   (Rec) TID EVENTTYPE CATEGORY SOURCE APPLICATION HOST SEVERITY ( I )  1001 TCPConnectionClose Network 10 System Vesuvio Low  (3) 1001 AuditFailure Security Software Authorization Magna High (4) 1001 CoreDump Memory Exception Kernel 2.4 Stromboli High ( 5 )  1001 IRQConflict Device PCI Bus System Vulcano High  (2) 1001 CiscoDCDLinkUp Network DHCP Routing Etna Low  Figure 1. Event Database (the 5 records form one  transaction, according to their TIDs)  USER PREFERENCES. Traditional association rule algo- rithms return all frequent itemsets. However, the user may only he interested in some specific pattern structures ac- cording to his domain knowledge and the current focus of study for a given dataset. For instance, a user wants to mine all patterns where EVENTTYPE and SOURCE are involved in describing the patterns. At the same time. he wants to ig- nore patterns which contain elements described by both the SOURCE and CATEGORY attributes. Note that such user preferences can not be enforced by preprocessing rhe dura ro exclude certain arrribures or values, as these attributes and values may combine in different ways to form patterns that are interesting to the user. Thus, we need a mecha- nism that is conducive to incorporate user preferences into the mining process. Although user preferences and mining constraints are beyond the scope of this paper, the frame- work proposed in this work makes such incorporation pos- sible [ I  I].

2 Related Works  Agrawal and Srikant 13, 81 led the pioneering work of mining sequential patterns. Each item in a pattern is repre- sented by a set of literals. and patterns on level k (patterns with a total number of k literals) aTe generated by joining patterns on level k - 1 .  To map our problem into that of mining sequential patterns, we convert each structured-item to a set of literals by encoding its attribute information (for instance, for each value a of attribute A, we create an item &). However, the pattern structure is lost through such en- coding: patterns on the same level take part in candidate generation and pruning as a whole, regardless of their pat- tern struclure, Le., what combination of attributes are used in describing each item in the patterns. As a result, user preferences can not be incorporated in the mining process.

The algorithm mines (unnecessarily) all frequent patterns before the answers conformed to user?s mining targets can he filtered out during the final step. It will apparently slow down the mining process when the dataset is large and high- dimensional, and it also creates difficulty in understanding the mining results since patterns are all mixed together in- stead of partitioned by their pattern structures.

Srikant et al [7] and Han et al [SI consider multi-level association rules based on item taxonomy and hierarchy.

These approaches are further extended [6.9] to handle more general constraints. In Table I ,  we compare them with HlFl. With a pre-specified hierarchy or taxonomy, the min- ing space is severely restricted. The ability to discover pat- terns ( R I , .  . . , R5, e.g.) totally depends on whether their pattern structures happen to fit into the given taxonomy or hierarchy. In contrast, our framework enables us to mine patterns without a pre-specified item hierarchy. HlFl offers the maximum flexibility in defining patterns: patterns con- taining k items can have (zNy-z) different pattern struc- tures. The search space of hierarchy and taxonomy is much smaller and their discoveries are limited by the taxonomy or the fixed hierarchy they use.

3 Definitions and Notations  Let 2) be a set of transactions, where each transaction contains a set of records, and each record is defined by a set of attributes A. Our task is to find frequent k-itemsets, how- ever, the fundamental concept of irem is defined differently.

Definition 1. Ler 7 = { t l ,  ..., t r )  be a subser of A. A structured-item I is a ser of attribute-value pairs { t l  = V I ,  ..., t k  = vk}. where vi is LI value in rhe domin  of at- rribure t ; .  We call 7 rhe item structure of I.

Based on the event data in Figure I ,  an example of a structured-item is {CATEGORY=Security, SEVER- lTY=High), and its item Structure is {CATEGORY. S E V E R - ITY}. We also use (security, high) todenote the structured- item when no confusion arises.

Definition 2. A record R is an instance ofa structured-irem r : Itl = U], ..., tk  = v*) .  ifmi = v i ,v  t i  E {t l ,  ..., t k } .

Obviously, if R is an instance of structured-item I, then V I ?  C I, R is an instance of It.

Definition 3. A pattern is a set ofsrrucrured-irems. A k- itemset pattern is a parrern conraining k srrucrured-items.

Let il and iz be two structured-items. Pattern {il , iz} is different from pattern (21 Ui2). the former being a 2-itemset pattern, the lattera I-itemset pattern     #of  different t+ of different item structures patternstructures  taxonomy limitedbytaxonomy limitedby taxonomy hierarchy N N  k 2?fk--2 HlFl 2N - 1  Table 1. A comparison of the three approaches. N is the number of attributes of the dataset. ??? means the discovery of corresponding rules depends on the particular taxonomy or hierarchy in use.

discovers RI Rz RB R4 Rg ? ? ? ? no ? ? no ? no  yes yes yes yes yes  Definition 4. A transaction S supports pattern X, if each structured-item I E X maps to a unique record R E S such that R is an instance o f I .  Pattern X has suppor t s  in datasetD ifs% of the transactions in ?D support X.

We continue our example in Figure 1. Record (2) is an instance of structured-item i l :  {SOURCE=DHCP}, and an instance of iz: {HosT=Etna], which makes i t  an in- stance of is  = i, U iz: {SOURCE=DHCP, HosT=Etna] as well. Also, we can see that record (3) is an instance of ip: { HosT=Magna}. Thus, we derive some of the 2-itemset patterns supported by transaction 1001 (the first 5 records): {i1, i4] .  {iz,i4},and {is,ia}.

However, transaction 1001 does not support 2-itemset pattern {ilriz}.  This is because il  and a z  are supported by one record and one record only in transaction IOOI. Ac- cording to the definition, in order to support { i l , i a } ,  the transaction needs to have at least two records that are in- stances of il and ia respectively.

Definition 5. We define pattern structure as a multi-set, c = {7;, ..., Ti}, where each 7; is a non-empty set of attributes. A pattern of structure c has the form p = { I , ,  ..., I k ) ,  where the item structure of I ,  is 7;.

For presentation simplicity, we use parenthesis to sep- arate each 7; in a pattern structure. For instance, as- suming A, B and C are 3 of the attributes in a dataset, we use (.A)(AB)(C) to represent pattern structure c = {?E, 72,731. where 7; = {A}, 72 = {.A, B},  73 = {C}.

This notation is used in Figure 2, for example.

4 The HIFI Framework  If there is only one attribute, then all pattern structures are of the form (A)(A) ... (A), and the problem degenerates to traditional association rule mining. However, as shown by Lemma 1. the mining space grows exponentially when the number of attributes increases.

Proposition 1. Given a dataset with N different attributes.

there m e  a total of ( z N ~ k - z )  diferentpattern structures for k-itemset patterns.

Proof An equivalent problem is: how many different out- comes are there after a throw of k dice, each having m sides? The answer is (??+[-?). In our case, we have k items, each can be defined in m = 2? - 1 ways. Thus, there are a total of (zNy-z)  different pattern structures for k-itemset patterns. 0  To handle such a huge mining space, we need to explore the relationships among different pattern structures. For in- stance, pattern structure (AB) is a specification of structure (A) and structure ( E ) .  The HlFl framework explore these links by defining the successorlpredecessor relationships.

Definition 6. Given LI pattern structure c = {TI, ...,T,}, and an attribute t E A, c?s immediate successors are in one of the following forms:  1. (7; ,..., Tm,Tm+l}, where Tm+l = { t } 2. { E  )..., 7 ;U{ t }  ,... ) 7 ; ? } , t < 7 ;  Figure 2 depicts a graph of pattern structures tightly coupled by the predecessorlsuccessor relationships for two attributes A and B. Note that a pattern structure c = {T,, ..., Tm} on level L has no more than L predecessors, where level L is defined as L = Czl \TI. the total num- ber of attributes that appear in the relevant attributes of c.

The benefits of structuring the search space in the level- wise, tightly coupled form are the following:  The framework reveals all the relationships among pat- tern structures. These relationships are essential for candidate generation and pruning.

s Instead of joining the patterns on level K to derive can- didate patterns on level K f 1 and then using the pat- terns on level K again for pruning, we can localize the candidate generation and pruning procedure to each pattern structure. For instance. in order to find frequent patterns of structure ( A ) ( A ) ( B )  and ( A ) ( E ) ( B ) ,  only 4 nodes (their predecessors) in Figure 2 need to be explored. The improvement in performance is most significant in mining high dimensional data since the search space grows exponentially as the number of at- tributes increases.

Figure 2. Mining space of the HlFl framework: a graph of tightly coupled pattern structures  The framework requires no data encoding. Each struc- ture represents a mining target and more importantly, the generated patterns are structured in the same form.

Following the links among the pattern structures, users can choose to have a more general or a more specific view of the patterns. It helps users to overcome the difficulty in interpreting and analyzing the results.

4.1 Downward Closure Properties  The extended Downward Closure Property, which holds between each pair of parent and child pattern Structures, en- ables us to eliminate candidate patterns.

Proposition 2. (Downward Closure Properry) Let p = {II, ..., I h }  beaparternofsrrucrurec andrhesupporrofp is less rhan min-sup, then  1. rhe support ofparrern p. = {b, ..., I,, Ia], where I, is an mbirraty srrucrured-item, is less rhan min-sup;  2. rhe supporr of pattern pa = { I I ,  ..., I k - 1 ,  la], where   The organization of the search space in Figure 2 is justi- fied by Lemma 2. The level-wise Apriori property has been broken down to a much finer granularity, i.e., to the pattern structure level. If enables us to localize candidate genera- tion and pruning for specific pattern structures.

4.2 Join Properties  I, 3 In. is less rhan minuup.

Proof Similar to the Apriori property 121.

In this section. we explore the most efficient way of pat- tern generation. First, we show that the patterns of a pattern structure can he generated by joining the patterns of any nvo of its predecessor structures. However, although any pair of predecessors can be used, the computational cost can be very different. We hence identify a specific type of join, referred to as simple join, which can be implemented effi- ciently if the patterns are sorted in certain order. Finally,  we present the conditions that lead to simple joins without pre-sorting.

Unique Representation of Items and Pattern Structures  To facilitate further discussions, we assume that there exists an order (e.g., alphabetical order) among '_he attributes in A.

Given a set of attributes 7 2 A. we use 7 to denoLe the of; dered sequence of the same attrib_utes. Assuming 5 and 72 are two such sequences, wesay 7; j 72 if 71 holds lexico- graphical precedence over 72. Thus, we can uniguely rep- resent a structure c by its ordered version ? = (71, ..., A), where 5 j, ..., j E .  Figure 2 shows each pattern struc- ture using the ordered representation, assuming alphabetical order among the attributes, i.e., .4 3 B.

Now,-given a pattern structure Z on level L. Z = (3, ___, f i ) ,  the patterns of c can be represented by a table with L columns: ( t l l ,  .._, tZ1, .._, tkl, ...), where (tzl, ...) is the (ordered) attributes of ?. We use ;ti, 10 denote the column defined by the j-th attribute of E ,  and we use CI. fo denore a parenr srrucrure of c resulred from raking our the k-rh column from Z. For instance, the patterns of structure c = (A)(BC)  are represented by a table of three columns as shown in Figure 3(a), while the patterns of its three parent structures are represented by tables of two columns.

The Join Operation  We show how patterns of parent structures can be joined to produce patterns of child structures. To generate the candi- date patterns of structure (A) (BC) ,  we can join the patterns of its parents ca and c1 as follows:  Example 1. Join CQ = ( A ) ( B )  and c1 = (BC)  Io derive c = (4)(BC)  SELECTc3.111, c3 .1n,  ~1 .112 FROM e%. ci WHEREe3.h = ci.111  However, not all join operations can he expressed as suc- cinctly. For example, to generate all possible patterns of     Figure 3. Representing patterns using relational schema. Notation: c.ti, is the column defined by the j-th attribute of 7;, and ck is a parent structure of c resulted from taking out the k-th column of C.

(A)(AC) by joiningcg = (.4)(A) andcl = (AC), we will have to use the following SQL:  Example 2. Join c3 = (A)(A) and c1 = (AC) ro derive c = (.4)(ac)  SELECT CASE WHEN c ~ . l i l  = e z . 1 ~ 1 THEN ~ 3 . 1 2 ,  ELSE ~ 3 . 1 1 ,  END CASE, C l . 1 1 , .

The reason of the complexity is because of the follow- ing: if p = (ao ) (a l )  is a pattern of (A)(A),  then patterns derived fromp for (A)(AC) can have two alternative forms: (ao)(al, *) and (al)(ao,*). Thus, in thejoin condition we need to compare attribute 4 of cl to both attributes of CI.

We show later that such join operations can be avoided.

The Completeness of the Join Operation  We show that the above join operations are complete. mean- ing that the result of the join contains all the patterns that have a support greater than min-sup.

Proposition 3. (Join Properry I )  Given a pattern srructure c on level L. L 2 2, the candidate parrerns derived by joining the parrerns ofany ofirs nvop'edecessorsrrucrures cunroin all rhe frequenrpatterns of e.

Proof: Let p be a pattern of structure c, and let ci and cj be any two parents of c. Removing the i-th column and then the j-th column of p, we get two subpatterns, pi and pj.

Since p is a pattern of c, according to the anti-monotonicity property, pi and pj must he patterns of c; and cj respec- tively. Thus, pattern p can be derived by joining pi and p j on their common columns. 0  Join Property I also implies that joining the patterns of a structure's immediate predecessors generates fewer candi- dates than joining its ancestors. This is because the patterns of its predecessors are a subset of thejoin results of the pre- decessors' predecessors.

According to Join Property I ,  for any pattern structure on level L ,  L 2 2, we can generate its candidate patterns by joining the patterns of any of its 2 predecessors. Other  predecessors can be used to further prune the candidate pat- terns according to the anti-monotonicity property.

Joins in the Simple Form  Our goal is to generate candidates efficiently through a join operation such as Example 1, without indexing or sorting.

We denote join operations in Example 1 as joins in the sim- ple form, while join operations in Example 2 with a disjun- tive WHERE condition in the non-simple form. Joins in the simple form can he implemented efficiently if the patterns are stored in a certain order. For instance, Example 1 can be implemented efficiently if patterns in cg and c1 are ordered by attribute B.

It is harder to implement the join operation in Example 2 effciently because rwo indices on table cg are required one on column t u ,  the other on t n ;  otherwise we have to do a linear scan on table CQ. Since patterns are generated on the fly, maintaining extra index is expensive.

Given a pattern structure c and any of its two parents ci and cj ,  it is easy to check if patterns of c can be derived by joining ci with cj  in the simple form. Assume c = {X, ..., G, ..., 7,) and parent ci = 171, ..., z, ..., 7,], where T k  = 7; U {4} .  We call e, a simple parent of e, if7; # z,VZ E c.

Proposition 4. Parterns of strucrure c con be derived by joining c, and c j  in the simple form, $both ci and c j  are simple p a r d s  o f c   Proposition 5. (Join Property U) Given aparrern srrucrure c on level L, there exisr at least two predecessor ~ t r u c r u ~ s , c; and cj. rhar can be joined in rhe simpleform to generare parrerns for c  Proof L e t Z =  (3 ,..., <), 1711 = m, and 1x1 = n. We show that cm and CL-"+I can be joined in the simple form.

c, is the p e n t  structure of c after removal of the last at- tribute of %. Assuming the removal of the last attribute of 71 results in a new sequznce of a t t r i pes ,  T ,  we have ei- ther 7' = 0, or 7' < 5 5 ,  ..., 5 5, which means there can be no 7; E c such that 7' = 7;. Similar reasoning applies to CL-,,+I. which is the parent structure of c after  Proof: Omitted for lack of space.

removing of the first attribute of x .  Since m + n 5 L, we know m # L - n + 1, according to Lemma 4, cm and  0 C L - ~ + I  can be joined in the simple form.

Efficient Candidate Generation: Merge-Join without Pre-sorting  Let?s assume patterns of the parent structures are ordered by their attribute values (i.e., patterns of (AB) (CD)  are or- dered by their values of .4. then B, C, and D). Then, candi- date patterns (of a child structure) derived by merge-joining the ordered patterns of its parent structures maintain the or- der. Thus, the new patterns can be used to merge-join with other patterns to derive patterns on the next level, which still maintain the order. We can repeat this process to gen- erate patterns on all levels through merge-joining without re-sorting the data.

For instance, say we want to derive the candidate patterns for structure (AB) (CD)(EF)  on level 6. These can be generated by joining the patterns of C6 = (AB) (CD)(E) and c j  = (AB) (CD)(F) .  We can merge-join C6 and c j because they sharethesame prefix: (AB)(CD) of length4.

Furthermore, the results of the join are still ordered by the attributes, which makes them ready to generate patterns on the next level without sorting. The question is, can patterns of every structure be derived by merge-joining two of its parents using their existing order?

Apparently, the parents that can be merge-joined to pro- duce the patterns of c must have the same first ( L  - 2) at- tributes. Thus, the only two parents that can qualify are CL and cL--l.  the two structures resulted by the removal of the last and the next-to-last column of c respectively. However, sometimes CL and CL-,  do not have the same first L - 2 at- tributes. Take B = (AB)(AB) on level L = 4 for example.

Parent structure Z4 does not exist in the form of (dB)(.4).

but rather (.4)(4B) since (A) + (4B). Thus, patterns of B4 are not ordered by the same first L - 2 = 2 attributes as 23 = (4B)(H) .  On the other hand, even if C L  and CL-I do share the first L - 2 attributes, they are not merge-joinable, if they can not even be joined in the simple form. An exam- ple of such a case is (A)(AB) .

Proposition 6. (Join Properry Ill) The candidare parrerns of a srrucfure c on level L 2 3 can be derived by merge- joining the parrerns of CL and C L - I ,  if the following are sarisjed: i )  C L  and  CL-^ can be joined in the simple form; andii)  CL and  CL-^ share rhe samefirsf L - 2 afrribures.

Proof i) guarantees only a single ordering of the patterns is required, and ii) guarantees they can be merge-joined. 0  For instance, candidate patterns of 5 out of the 6 pattern structures on level 3 in Figure 2 can be derived by merge- joining the patterns of their parents without re-sorting.

Overall. around 80% of the structures can be derived by merge-join.

5 Algorithm  The main procedure for mining the HlFl framework is outlined in Algorithm I .  We start by generating frequent itemsets on the first level, where each pattern structure has one attribute. The resulted frequent patterns are paired to generate candidate patterns on the second level. Then, on line 6 we scan the datnset to count the occurrences of each pattern on the current level, and on 7, we eliminate infre- quent patterns. Next, we generate all possible pattern struc- tures for the next level (Algorithm 2) and populate each pat- tern structure by candidate patterns by the method described in the previous section (Algorithm 3). We repeat the process until no more patterns can be generated.

Algorithm 1 HIFl(Set0fAttributes: .4, Dataset: D, Min- Support: minAup)  I: generate frequent patterns of structures on the 1st level; 2: L t 2; 3: StructureL t structures on the 2nd level; 4: CandL t join patterns on the 1 st level to generate can-  5: while CandL # B do 6 countSupport(D, CandL); 7:  8 :  L t L + l ; 9: StructureL c StructureGen(StructureL-1, A);  to.  CandidateCen(StructureL); 11: end while 12:  return (t.patternslt E StructureL};  didate patterns on the 2nd level;  eliminate candidates whose support are lower than min-sup,  Algorithm 2 StructureGen(Set0fStructures: parents) I: s t 0; 2. for each p E parents do 3: 4:  5 :  end for 6 end for 7: returns;  for each child structure c ofp do S c S U { e }  if  all of e?s parent structures exist and have non-empty pattern set;  Given all candidate patterns on a certain level, the countsupport procedurescans the dataset once to count the occurrences of each pattern. The counting itself is not a trivial problem, especially when ?exc1usive?concepts are to be supported. Efficient access to all valid items are essential for this purpose. In HIFI. we build an item free forthis task.

Algorithm 3 CandidateGen(Set0fStructures: Structures) I: for each c E Structures  do 2: 3: 4: c.patterns t mergeloin(cL, C L - I ) ; 5: else 6: sort the patterns in c, and cj; 7: c.patterns t mergeJoin(c,, cj); 8: sort the patterns in c.patterns: 9: end if  1 0  I I : I 2  Let c z  and c j  be parents of c with fewest patterns.

if joining CL and  CL-^ is less costly then  for each parent ck of e, and c~ does not take part in the join operation do  for each pattern p E c.patterns do remove p from c.patterns if the sub-pattern of p with regard to cx does not exists in ck .patterns;  13: end for IJ: endfor 15: end for  The details of countsupport and the item tree structure canbefoundin [ I l l .

Based on the structures on the previous level, a naive way of generating all the current structures is shown in Al- gorithm 2. A child structure is to be generated only if all of its parents exist and have non-empty patterns. Algorithm 2 is not optimal since each structure can have multiple succes- sors, and they are generated and tested multiple times. The implementation of HIFI u s ~ s  an efficient algorithm. Based on a parent structure$= (TI,  ..., T k ) ,  we create a subset of its child structures using the following meth_ods: iLadding a single attribute item  TI+^ to p?such that Tx 5 TI+ , ;  ii) adding a new attrib)te to an existing item to create a new item T? such that Tk 5 T?. We prove that each structure on the new level is generated once and only once [ I  I].

The core of HIFI is the candidate generation procedure shown in Algorithm 3. The rationale and the correctness are discussed in the previous section.

6 Experimental Results  Experiments on both synthetic datasets and real life datasets were camed out on a Pentium 111 machine run- ning Linux OS 2.2.1 with a 166 MHz CPU, 256M memory.

We implemented two other approaches: Hierarchy and Tax- onomy. Hierarchy is based on the ML.T* algorithms [ 5 ] .

Given a dataset with N attributes, Hierarchy enumerates all N !  possible hierarchies of N levels, and for each hierarchy, we use ML.T* to find frequent itemsets. Furthermore, we also find ?cross-level? patterns by combining frequent item- sets on different hierarchy levels [5 ] .  Taxonomy is based on EstMerge [IO]. A record with N attributes are encoded into  2N - 1 items, such that a transaction with k records contains as many as k (2N - 1) items. Taxonomy then performs an Apriori search in the extended transactions.

The synthetic datagenerator is parameterized by D num- ber of records, A number of attributes. I average records per transaction, N average number of distinct values for each attribute. and P number of maximum patterns. Both I and N are Poisson distribution parameters. The number of items in a maximum pattern is decided by the Poisson distribution with X = 112, and the number of attributes in an item is de- cided by the Poisson distribution with X = A / 2 .  We set N = 30 and P = 100. We use D?.A?.I? as a name tem- plate, for instance, D100K.A5.18 is the dataset generated by setting D = 100K. A = 5 and I = 8.

Figure 4 shows the scalability of the HIFI Algorithm.

Figure4(a) showsthe execution time increases linearly with the size of the dataset. Figure 4(b), however, shows that the performance is heavily dependent on the number of at- tributes. since more attributes brings a combinatorial growth of the number of structures. Shown in Figure 4(c), the exe- cution time also increases significantly as we increase I ,  the average number of records in a transaction. This is because when I becomes larger. patterns will contain more items and require more passes of data scans to discover. In  Fig- ure 4(d,e,f). we compared the performance of HIFI against the other two approaches, namely Hierarchy and Taxonomy.

Despite the fact that these two approaches only discover a subset of the patterns discovered by HIFI, HIFI is much more efficient because a large amount of candidate item- sets are pruned by taking advantage of the anti-monotonic relationships in  the tightly coupled mining space.

We also applied the HlFl algorithm on a real life dataset, NETVIEW, which is generated by a production network at a financial service company. Events in the dataset are grouped into transactions by their timestamps based on a 30-second interval, and on average each transaction contains about 10 events. Each event has multiple at- tributes, which include Event, Host, Severity, and Category. Overall, there are 241 event types, 2526 hosts, 6 severity levels, and 17 event categories. The patterns discovered by HIFI offers valuable insights into the under- standing of the operational environment.

7 Conclusion  We present the search space of frequent patterns in a novel architecture. where pattern structures are tightly cou- pled in  the anti-monotonic relationships. Using such rela- tionships, and an efficient candidate generation algorithm based on merge-join, our approach is able to prune away a large amount of candidate patterns, thus greatly improves the mining performance.

Unlike the level-wise mining algorithms used in Apriori     7c4  sm HlFl - 1  (a) D*.AS.IS, minsupco.M%  (d) Scalabilily on DM1K.A4.15 (e) Scalability on DZOOK.A6.IS  Figure 4. Execution Time  and its extensions [3, 81, our algorithm localizes the candi- date generation and pruning procedure to each pattern Struc- ture. Given a set of query structures, we are able to find their frequent itemsets by exploring a much smaller search space than the other approaches. Furthermore, our algorithm is re- lational and attribute sensitive in that we do not encode the attribute information of a relational table into items. The organization of the search space is also conducive to the in- terpretation and analysis of the resulting patterns.

