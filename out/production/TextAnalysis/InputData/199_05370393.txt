A Novel Association Rule Decision Algorithm based on Knowledge Space

Abstract?The data mining process is important in nature. So we must reduce response time which is imperative.  The task is difficult because of the calculation and storage intensive nature of association rule decision algorithms. We re-devised algorithms to make better the performance of association rule decision algorithms. For that  we layout an effective systems. In this paper, we present knowledge space layout methods for re- devising algorithm. Knowledge space algorithm layouts try to reduce the duplication computation by iterations and across operations of an association rule decision algorithm.

Knowledge space algorithm layouts try to improve operation performance by way of maximizing data location and re-use on the other side. And we propose the new layout for the support systems that make a great diversity of association rule decision algorithms to broaden knowledge space by minimal run cost.

Keywords-Association Rule Decision; Knowledge Space;Association rule decision; Knowledge  Discovery

I. INTRODUCTION The data mining process is important in nature. So we  must reduce response time, which is imperative. In such an environment, reducing response-time is imperative, because a lengthy delay between responses to two consecutive user queries can disrupt the flow of human perception and the formation of insight. The calculation and storage intensive nature of association rule decision algorithms makes this task especially difficult. To address the aforementioned challenge, the past few years have seen researchers make significant progress in reducing the computational complexity of association rule decision algorithms. However, the process continues to be time consuming. We believe that in order to derive high performance on next generation computing infrastructures, one must consider both re-devised algorithms and lay outing effective middleware support.

We propose to make use of two new algorithm layout philosophies, namely knowledge space algorithm layouts.

Given the iterative and interactive nature of the Data mining process, one expects there to be significant repeated computation through successive operations of an association rule decision algorithm. A knowledge space algorithm layout philosophy attempts to expose this repeated computation, buffer it in a knowledge buffer, and re-use it during successive operations of an association rule decision algorithm. As we all know that programs bringing out weak data location tends to keep a processor stalled, waiting on the  completion of data access for a large fraction of the time.

This is also known as the storage wall problem and results in weak CPU utilization. Knowledge space algorithm layouts attempt to alleviate this problem by improving data location.

Furthermore, such layouts also allow one to effectively take advantage of architectural innovations such as chip multiprocessing. From the viewpoint of lay outing middleware support, one must investigate the types of services that are desired by a range of association rule decision algorithms and how they can be layout to maximally take advantage of the available infrastructures.

In this paper we introduce some discovery methods that reduce the above-mentioned views. Specifically, we make the following contributions. First, we present the layout of systems support for knowledge-buffering and storage placement that can be utilized by a variety of association rule decision algorithm. Second, we present knowledge space algorithmic improvements for data clustering algorithms.

Third, we present knowledge space layouts in the graph mining and tree mining domains. Fourth, we present an evaluation of our layouts.



II. SYSTEMS SUPPORT Knowledge Buffering: A knowledge space-mining  framework consists of a client and a server. The server preserves a database, while the client manages a search queue, a search operation engine, and a knowledge buffer.

The search operation engine accepts a search from the search queue, and executes the search using the contents of the (local) knowledge buffer and the (remote) database.

Furthermore, using the information gathered through an operation, the search operation engine updates the contents of the knowledge buffer to improve performance when answering future queries. We propose the evolution of a data-mining technique centric knowledge buffer, containing recently constructed and/or used knowledge objects (Figure 1). Our goal is to build an infrastructure that allows a variety of association rule decision algorithms to gain the benefits of knowledge buffering with minimal implementation efforts.

This knowledge buffer will be deployed on a cluster of cru- nodes and leverage the combined aggregate main storage and disk space available on such a system. Association rule decision algorithms will interface with such a knowledge buffer at the abstraction of a user-defined Knowledge Object.

This Knowledge object must have Metadata and Knowledge fields within it, as is dictated by the interface. The metadata  2009 Second International Symposium on Computational Intelligence and Design  DOI 10.1109/ISCID.2009.91   2009 Second International Symposium on Computational Intelligence and Design  DOI 10.1109/ISCID.2009.91     for a buffer knowledge object preserves the following information: user/process identifier, technique adopted (association rule mining), technique-specific parameters (e.g.

minimum-support value), and data-specific information. This metadata can be used to determine if a buffer knowledge object can be reused for a search. While Metadata field encodes information about the information stored in the knowledge object, the Knowledge field encodes the actual knowledge. Association rule decision applications often rely on specialized pointer-based data structures, for storing the results of queries to facilitate interactive exploration. To deal with this issue, the user must implement linearism and de- linearism methods for the Metadata and Knowledge fields.

The linearism method allows the system to convert the Metadata and Knowledge fields into a binary block of storage, allowing for transmission over a network and efficient storage. The de-linearism method allows the system to convert these fields into an algorithm interpretable form.

The association rule decision algorithms will interact with the knowledge-buffer using get and put methods. The put method accepts a Knowledge Object from the client and inserts it into the knowledge buffer. The get method retrieves a knowledge object from the buffer using a user defined Search object. This Search object associates a re-use score with each knowledge object and the knowledge buffer returns the most re-usable Knowledge object. The knowledge buffer will be responsible for managing both local disk space and storage space allotted for the buffer dispersed across multiple cru-nodes. With a distributed buffer, one important issue is the effective management of distributed space and bandwidth. In a heterogeneous environment, storage and bandwidth can vary across the machines in the system. Moreover, the storage space on a machine can be shared between the application and the buffer. Thus, the buffer space on a machine may shrink, requiring that some of the buffer objects be evicted. We propose to investigate adaptive methods for placement and eviction of buffer objects in the system. One approach is to assign a priority to each buffer object. This priority can be based on the static and dynamic attributes used by buffer replacement strategy (e.g., how recently the object has been accessed, the popularity of the object relative to others, the size of the object, and the cost of computing the object). A high priority object will be buffer on a machine with high bandwidth. Since a buffer object is made up of fixed size buffer blocks, we will also develop methods to distribute the blocks across the system to take advantage of distributed bandwidth and buffer space. When a buffer object needs to be evicted from a machine, it can be staged to the local persistent buffer on the machine or it can be transferred to other machines in the environment. When selecting the machine to transfer the object to, the buffer management strategy should take into account the priority of the object as well as the cost of the transfer operation; for instance, a single transfer operation can create a cascade of transfers if the selected machine does not have space to accommodate  the object. We will examine methods to address these issues.

Storage Placement: We have also developed middleware that handles storage placement for complex data structures to improve reference location for association rule decision algorithms. The middleware supports different placement strategy and does not pollute the buffer with boundary tag information. We explain (Figure 2) these strategies in the context of the hash-tree structure. We make use of a localized placement strategy that groups related data structures together using local information present in a single subroutine. In this strategy, we use a reservation mechanism to insure that a  cru-node list with its related items collection in the leaves of the final hash-tree are together whenever possible, and that the hash-tree cru-node and items collection list header are together. This is done because an access to a list cru-node is always followed by an access to its items collection, and an access to a leaf hash tree cru-node is followed by an access to its items collection list header. We also make use of a global placement strategy that utilizes the knowledge of the entire hash tree traversal order to place the tree in storage. In our case, the hash tree is remapped in depth-first order, which closely approximates the hash tree traversal.

Figure 1. Knowledge Buffer  Figure 2. Storage placement

III. KNOWLEDGE SPACE ALGORITHM LAYOUTS Over the past decade, processor speeds have increased  much faster than DRAM speeds, leading to the creation of a gap between processor performance and storage system performance. Programs that exhibit weak data location leave the processor stalled for a large fraction of the operation time due to long storage latencies. In such situations, maximizing data location is imperative. Furthermore, it is commonly accepted that Chip Multiprocessors (CMPs) will become the de facto standard for commodity PCs. These processors contain multiple processing cores on the same die. The cores not only share all off-chip resources, but many on-chip resources, such as buffer, making storage bandwidth a bottleneck. Therefore, it is paramount that algorithm layout research effective methods to incorporate task parallelism and efficient storage system usage to insure that application performance are commensurate with such architectural advancements [3, 4, 6]. Graph Mining: Finding frequent patterns in graph databases such as chemical and biological data, XML documents, web link data, financial transaction data, social network data, and other areas has posed an important challenge to the association rule decision community. A major challenge in substructure mining (also termed graph mining) is that the search space is exponential with respect to the data set, forcing runtimes to be quite long.

Parallelizing the workload, and executing it on a CMP can mitigate these long runtimes. To achieve high scalability in a parallel graph-mining algorithm, one must overcome several fundamental obstacles. The two most difficult of these are load imbalance and efficient utilization of the storage hierarchy. The basis for effective load balancing is a task partitioning mechanism possessing sufficient granularity so as to allow each processing element to continue to perform useful work until the mining process is complete. In items collection mining, for large databases frequent-1 items generally suffice. However, for graph mining this is not the case. In graph mining, a single frequent-1 item may contain 50% or more of the total operation cost. This is because task length is largely dependent on the associatively in the dataset.

CMP systems typically have far less buffer per processor than other parallel architectures, including message-passing clusters or shared storage multiprocessors. Sharing of the L2 buffer is of particular concern. If multiple threads have independent working sets, then the effective size of the buffer is significantly diminished. This is predicated by the real estate constraints of the chip, since for a fixed chip size each additional core will use silicon previously dedicated to buffer. In graph mining, data accesses often lack temporal location because the projected data set is typically quite large.

These accesses also lack good spatial location because most of the data structures are pointer-based. We performed a simple working set study to evaluate the benefits of improved dataset graph accesses. We compared two methods for parallel graph mining. First, we forced threads to always mine their own offspring tasks. Second, we allowed threads to mine other threads? tasks; that is, task stealing. The buffer  miss rates for the first strategy were, on average; three times lower than the second strategy. The reason is that when a task is stolen, typically most of the needed data set graphs are not in buffer. As such, algorithm layout should explore and optimize the tradeoffs between improved temporal location and proper load balancing. Our solution to preserve excellent buffer utilization while still affording load balance is called Adaptive Task Partitioning, and is shown in Figure 3. Every mining task produces some number of offspring nominees from which to extend the currently mined graph.

Each nominee is then upon. With adaptive partitioning [2], each processor makes a decision at runtime for each frequent offspring extension. An offspring is an edge, which can be added to the currently mined graph to create a new frequent sub-graph. The offspring task may be mined by the creating processor, or enquired. The decision whether to mine the task is based on the current load of the system. We do not require a specific mechanism to make this decision, because in part it is based on the queuing mechanism used. In our experiments, we evaluate the size of the local thread?s queue, using a threshold of 10% of the total number of frequent-1 edges. Another option could be to set a threshold for the smallest current size of any queue. At each step in the depth- first mining process, each subtask can be enquired into the system for any other processor to mine. Thus the granularity of a task can be as small as a single mining call, which is rather fine-grained. Adaptive partitioning is depicted in Figure 3 (left). In this example, task (A-B) had two offspring, namely (A-B-C) and (A-BE). Tasks, which were enquired, are shaded gray. A circle has been drawn around a task which was allowed to grow dynamically. Task (A-B-E) was enquired, while task (A-BC) was mined by the parent process. After mining (A-B-C), only one offspring was created, which was kept by the parent. The subsequent two offspring were both mined by the parent, because the queues had sufficient work so as to allow it. The performance difference between full partitioning and adaptive partitioning is primarily due to the weak buffer performance exhibited in full partitioning. We perform a working set study to compare the temporal location of the two methods in the context of graph mining. We use Buffer grind on a single processor machine (Pentium 4 machine). Because Buffer grind currently does not profile multiple threads, we simulate 32 threads by allowing a single thread to remove from any point within 32 locations from the head of the queue with equal probability. As seen from the results in Figure 4, adaptive partitioning reduces the miss rates by 50%. This is due to the improved temporal location. As such, the database objects, which embed the offspring task?s pattern, are more likely to be in buffer. The end result is improved scalability.

Figure 3. Adaptive vs. level wise partitioning.

Figure 4. Working sets for adaptive partitioning and full partitioning.



IV. CONCLUSIONS In this paper, we presented knowledge space layout  methods to improve the performance of association rule decision algorithms that are both calculation and storage intensive. Knowledge space layouts target the computation that is repeated between iterations and operations of an association rule decision algorithm. Knowledge space layouts improve performance by targeting data location. First, the layout of systems support that can ease the evolution of knowledge space solutions and handle storage placement, for a variety of association rule decision algorithms, is presented.

Second, we showed how data clustering algorithms can be made knowledge space. Finally, we demonstrated that knowledge space layouts in the graph mining and tree  mining domains can provide significant performance improvements.

