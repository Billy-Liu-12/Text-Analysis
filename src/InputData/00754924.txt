

Abstract Constraint-based rule miners find all rules in a given data- set meeting user-specified constraints such as minimum support and confidence. We describe a new algorithm that directly exploits all user-specified constraints including minimum support, minimum confidence, and a new con- straint that ensures every mined rule offers a predictive advantage over any of its simplifications. Our algorithm maintains efficiency even at low supports on data that is dense (e.g. relational data). Previous approaches such as Apriori and its variants exploit only the minimum support constraint, and as a result are ineffective on dense data due to a combinatorial explosion of ?frequent itemsets?.

1.     Introduction Mining rules from data is a problem that has attracted  considerable interest because a rule provides a concise statement of potentially useful information that is easily understood by end users. In the database literature, the focus has been on developing association rule [1] algorithms that identify all conjunctive rules meeting user-specified con- straints such as minimum support (a statement of general- ity) and minimum confidence (a statement of predictive ability). These algorithms were initially developed to tackle data-sets primarily from the domain of market-basket analy- sis, though there has been recent interest in applying these algorithms to other domains including telecommunications data analysis [14], census data analysis [6], and classifica- tion and predictive modeling tasks in general. Unlike data from market-basket analysis, these data-sets tend to be dense in that they have any or all of the following proper- ties:1  ? many frequently occurring items (e.g. sex=male); ? strong correlations between several items; ? many items in each record.

These data-sets can cause an exponential blow-up in the resource consumption of standard association rule mining algorithms including Apriori [2] and its many variants. The combinatorial explosion is a result of the fact that these algorithms effectively mine all rules that satisfy only the minimum support constraint, the number of which is exor- bitant. Though other rule constraints are specifiable, they  are typically enforced solely during a post-processing filter step. Our approach to mining on dense data-sets is to instead directly enforce all user-specified rule constraints during mining. For example, most association rule miners allow users to set a minimum on the predictive ability of any mined rule specified as either a minimum confidence [1] or an alternative measure such as lift [5,8] (also known as interest [6]) or conviction [6]. Our algorithm can exploit such minimums on predictive ability during mining for vastly improved efficiency.

Even given strong minimums on support and predictive ability, the rules satisfying these constraints in a dense data- set are often too numerous to be mined efficiently or com- prehended by the end user. To remedy this problem, our algorithm exploits another constraint that eliminates rules that are uninteresting because they contain conditions that do not (strongly) contribute to the predictive ability of the rule. To illustrate this useful concept, first consider the rule below: Bread & Butter  Milk (Confidence = 80%)  This rule has a confidence of 80%, which says that 80% of the people who purchase bread and butter also purchase the item in the consequent of the rule, which is milk.

Because of its high confidence, one might be inclined to believe that this rule is an interesting finding if the goal is to, say, understand the population of likely milk buyers in order to make better stocking and discounting decisions.

However, if 85% of the population under examination pur- chased milk, this rule is actually quite uninteresting for this purpose since it characterizes a population that is even less likely to buy milk than the average shopper. This point has motivated additional measures for identifying interesting rules including lift and conviction. Both lift and conviction represent the predictive advantage a rule offers over simply guessing based on the frequency of the consequent. But both measures still exhibit another closely related problem illustrated by the next rule.

Eggs & Cereal  Milk (Confidence = 95%)  Because the confidence of this rule (95%) is significantly higher than the frequency with which milk is purchased (85%), this rule will have lift and conviction values that could imply to the end-user that it is interesting for under- standing likely milk buyers. But suppose the purchase of cereal alone implies that milk is purchased with 99% confi- dence. We then have that the above rule actually represents  1 Market-basket data is sometimes dense, particularly when it incorpo- rates information culled from convenience card applications for mining rules that intermix personal attributes with items purchased.

?  ?  Constraint-Based Rule Mining in Large, Dense Databases  Roberto J. Bayardo Jr.

IBM Almaden Research Center  bayardo@alum.mit.edu  Rakesh Agrawal IBM Almaden Research Center  rakesh_agrawal@ieee.org  Dimitrios Gunopulos IBM Almaden Research Center  dg@cs.ucr.edu    a significant decrease in predictive ability over a more con- cise rule which is more broadly applicable (because there are more people who buy cereal than people who buy both cereal and eggs).

To address these problems, our algorithm allows the user to specify a minimum improvement constraint. The idea is to mine only those rules whose confidence is at least minimp greater than the confidence of any of its proper sub-rules, where a proper sub-rule is a simplification of the rule formed by removing one or more conditions from its ante- cedent. Any positive setting of minimp would prevent the undesirable rules from the examples above from being gen- erated by our algorithm. More generally, the minimum improvement constraint remedies the rule explosion prob- lem resulting from the fact that in dense data-sets, the confi- dence of many rules can often be marginally improved upon in an overwhelming number of ways by adding additional conditions. For example, given the rule stating that cereal implies milk with 99% confidence, there may be hundreds of rules of the form below with a confidence between 99% and 99.1%.

Cereal &  &  &  &   Milk  By specifying a small positive value for minimp, one can trade away such marginal benefits in predictive ability for a far more concise set of rules, with the added property that every returned rule consists entirely of items that are strong contributors to its predictive ability. We feel this is a worth- while trade-off in most situations where the mined rules are used for end-user understanding.

For rules to be comparable in the above-described con- text, they must have equivalent consequents. For this rea- son, our work is done in the setting where the consequent of the rules is fixed and specified in advance. This setting is quite natural in many applications where the goal is to dis- cover properties of a specific class of interest. This task is sometimes referred to as partial-classification [3]. Some example domains where it is applicable include failure anal- ysis, fraud detection, and targeted marketing among many others.

1.1  Paper overview  Section 2 summarizes related work. Section 3 formally defines and motivates the problem of mining rules from dense data subject to minimum support, confidence, and/or improvement constraints. The next three sections define our algorithm in a top-down manner. Section 4 begins with an overview of the general search strategy, and presents pseudo-code for the top level of our algorithm. Section 5 provides details and pseudo-code for the pruning functions invoked by the algorithm body. Section 6 details the item- reordering heuristic, and Section 7 describes the rule post- processor. The algorithm is empirically evaluated in Section 8. Section 9 concludes with a summary of the contributions.

2.     Related work Previous work on mining rules from data is extensive.

We will not review the numerous proposals for greedy or heuristic rule mining (e.g. decision tree induction) and focus  instead on algorithms that provide completeness guarantees.

We refer the reader interested in heuristic approaches for mining large data-sets to the scalable algorithms proposed in [7] and [12].

There are numerous papers presenting improvements to the manner in which the Apriori algorithm [2] enumerates all frequent itemsets (e.g. [6]), though none address the problem of combinatorial explosion in the number of fre- quent itemsets resulting from applying these techniques to dense data. Other works (e.g. [4]) show how to identify all maximal frequent itemsets in data-sets where the frequent itemsets are long and numerous. Unfortunately, all associa- tion rules cannot be efficiently extracted from maximal fre- quent itemsets alone, as this would require performing the intractable task of enumerating and computing the support of all their subsets.

Srikant et al. [14] and Ng et al. [10] have investigated incorporating item constraints on the set of frequent item- sets for faster association rule mining. These constraints, which restrict the items or combinations of items that are allowed to participate in mined rules, are orthogonal to those exploited by our approach. We believe both classes of constraints should be part of any rule-mining tool or appli- cation.

There is some work on ranking association rules using interest measures [6,8,9], though this work gives no indica- tion of how these measures could be exploited to make min- ing on dense data-sets feasible. Smythe and Goodman [13] describe a constraint-based rule miner that exploits an infor- mation theoretic constraint which heavily penalizes long rules in order to control model and search complexity. We incorporate constraints whose effects are more easily under- stood by the end user, and allow efficient mining of long rules should they satisfy these constraints.

There are several proposals for constraint-based rule mining with a machine-learning instead of data-mining focus that do not address the issue of efficiently dealing with large data-sets. Webb [15] provides a good survey of this class of algorithms, and presents the OPUS framework which extends the set-enumeration search framework of Rymon [11] with additional generic pruning methods. Webb instantiates his framework to produce an algorithm for obtaining a single rule that is optimal with respect to the Laplace preference function. We borrow from this work the idea of exploiting an optimistic pruning function in the con- text of lattice-space search. However, instead of using a sin- gle pruning function for optimization, we use several for constraint enforcement. Also, because the itemset frequency information required for exploiting pruning functions is expensive to obtain from a large data-set, we frame our pruning functions so that they can accommodate restricted availability of such information.

3.     Definitions and problem statement A transaction is a set of one or more items obtained from  a finite item domain, and a data-set is a collection of trans- actions. A set of items will be referred to more succinctly as an itemset. The support of an itemset , denoted , is  I1 I2 ? In ?  I sup I( )    the number of transactions in the data-set to contain . An association rule, or just rule for short, consists of an itemset called the antecedent, and an itemset disjoint from the ante- cedent called the consequent. A rule is denoted as where  is the antecedent and  the consequent. The sup- port of an association rule is the support of the itemset formed by taking the union of the antecedent and conse- quent ( ). The confidence of an association rule is the probability with which the items in the antecedent  appear together with items in the consequent  in the given data- set. More specifically:  Other measures of predictive ability include lift [5,8], which is also known as interest [6], and conviction [6]. The conviction and lift of a rule can each be expressed as a func- tion of the rule?s confidence and the frequency of the conse- quent; further, both functions are monotone in confidence:  Though we frame the remainder of this work in terms of confidence, it can be easily recast in terms of any measure with this monotonicity property.

The association rule mining problem [1] is to produce all association rules present in a data-set that meet specified minimums on support and confidence. In this paper, we restrict the problem in two ways in order to render it solv- able given dense data.

3.1  The consequent constraint  We require mined rules to have a given consequent specified by the user. This restriction is an item constraint which can be exploited by other proposals [10, 14], but only to reduce the set of frequent itemsets considered. A frequent itemset is a set of items whose support exceeds the mini- mum support threshold. Frequent itemsets are too numerous in dense data even given this item constraint. Our algorithm instead leverages the consequent constraint through pruning functions for enforcing confidence, support, and improve- ment (defined next) constraints during the mining phase.

3.2  The minimum improvement constraint  While our algorithm runs efficiently on many dense data- sets without further restriction, the end-result can easily be many thousands of rules, with no indication of which ones are ?good?. On some highly dense data-sets, the number of rules returned explodes as support is decreased, resulting in unacceptable algorithm performance and a rule-set the end- user has no possibility of digesting. We address this prob- lem by introducing an additional constraint.

Let the improvement of a rule be defined as the minimum difference between its confidence and the confidence of any proper sub-rule with the same consequent. More formally, given a rule :  If the improvement of a rule is positive, then removing any non-empty combination of items from its antecedent will drop its confidence by at least its improvement. Thus, every item and every combination of items present in the antecedent of a large-improvement rule is an important con- tributor to its predictive ability. A rule with negative improvement is typically undesirable because the rule can be simplified to yield a proper sub-rule that is more predic- tive, and applies to an equal or larger population due to the antecedent containment relationship. An improvement greater than 0 is thus a desirable constraint in almost any application of association rule mining. A larger minimum on improvement is also often justified because most rules in dense data-sets are not useful due to conditions or combina- tions of conditions that add only a marginal increase in con- fidence. Our algorithm allows the user to specify an arbitrary positive minimum on improvement.

3.3  Problem statement  We develop an algorithm for mining all association rules with consequent  meeting user-specified minimums on support, confidence, and improvement. The algorithm parameter specifying the minimum confidence bound is known as minconf, and the minimum support bound min- sup. We call the parameter specifying a minimum bound on improvement minimp. A rule is said to be confident if its confidence is at least minconf, and frequent if its support is at least minsup. A rule is said to have a large improvement if its improvement is at least minimp.

4.     Set-enumeration search in large data-sets From now on, we will represent a rule using only its  antecedent itemset since the consequent is assumed to be fixed to itemset . Let  denote the set of all items present in the database except for those in the consequent. The rule- mining problem is then one of searching through the power set of  for rules which satisfy the minimum support, con- fidence, and improvement constraints. Rymon?s set-enu- meration tree framework [11] provides a scheme for representing a subset search problem as a tree search prob- lem, allowing pruning rules to be defined in a straightfor- ward manner in order to reduce the space of subsets (rules) considered. The idea is to first impose an ordering on the set of items, and then enumerate sets of items according to the ordering as illustrated in Figure 1.

FIGURE 1. A completely expanded set-enumeration tree over , with items ordered lexically.

I  A C? A C  A C? A  C  conf A C?( ) sup A C?( ) sup A( )  ---------------------------=  lift A C?( ) sup ?( ) sup C( ) ----------------- conf A C?( )?=  conviction A C?( ) sup ?( ) sup C( )? sup ?( ) 1 conf A C?( )?[ ] --------------------------------------------------------------=  C  A C? imp A C?( ) min A? ? A? conf A C?( ) conf A? C?( )?,( )=  C  C U  U  U 1 2 3 4, , ,{ }= {}  1 2  1,2  1,2,3  1,2,3,4  1,3  1,3,4  1,4 2,3  2,3,4  2,4   3,4   1,2,4    4.1  Terminology We draw upon the machinery developed in previous  work where we framed the problem of mining maximal fre- quent itemsets from databases as a set-enumeration tree search problem [4]. Each node in the tree is represented by two itemsets called a group. The first itemset, called the head, is simply the itemset (rule) enumerated at the given node. The second itemset, called the tail, is actually an ordered set, and consists of those items which can be poten- tially appended to the head to form any viable rule enumer- ated by a sub-node. For example, at the root of the tree, the head itemset is empty and the tail itemset consists of all items in .

The head and tail of a group  will be denoted as and  respectively. The order in which tail items appear in  is significant since it reflects how its children are to be expanded (Figure 3). Each child  of a group  is formed by taking an item  and appending it to to form . Then,  is made to contain all items in  that follow  in the ordering. Given this child expan- sion policy, without any pruning of nodes or tail items, the set-enumeration tree enumerates each and every subset of  exactly once.

We say a rule  is derivable from a group  if ,  and . By definition, any rule that can be enu- merated by a descendent of  in the set-enumeration tree is also derivable from .

Define the candidate set of a group  to be the set con- sisting of the following itemsets: ?  and ; ?  and  for all ; ?  and .

A group is said to be processed once the algorithm has com- puted the support of every itemset in its candidate set.

4.2  Top-level algorithm description  It is now possible to provide a top-level description of the algorithm, which we call Dense-Miner. The body (Fig- ure 2) implements a breadth-first search of the set enumera- tion tree with Generate-Initial-Groups seeding the search.

The groups representing an entire level of the tree are pro-  cessed together in one pass over the data-set. Though any systematic traversal of the set-enumeration tree could be used, Dense-Miner uses a breadth-first traversal to limit the number of database passes to at most the length of the long- est frequent itemset. The use of hash-trees and other imple- mentation details for efficient group processing is described in [4].

Generate-Initial-Groups could simply produce the root node which consists of an empty head and  for its tail.

However, our implementation seeds the search at the second level of the tree after an optimized phase that rapidly com- putes the support of all 1 and 2 item rules and their anteced- ents using array data-structures instead of hash trees.

Generate-Next-Level (Figure 3) generates the groups that comprise the next level of the set-enumeration tree.

Note that the tail items of a group are reordered before its children are expanded. This reordering step is a crucial opti- mization designed to maximize pruning efficiency. We delay discussing the details of item reordering until after the pruning strategies are described, because the particular pruning operations greatly influence the reordering strategy.

After child expansion, any rule represented by the head of a group is placed into  by Extract-Rules if it is frequent and confident. The support information required to check if the head of a group  represents a frequent or confident rule is provided by the parent of  in the set-enumeration tree because  and  are members of its candidate set. As a result, this step can be performed before  is pro- cessed.

The remaining algorithmic details, which include node pruning (the PRUNE-GROUPS function), item-reordering, and post-processing (the POST-PROCESS function), are the subjects of the next three sections.

5.     Pruning This section describes how Dense-Miner prunes both  processed and unprocessed groups. In Figure 2, note that groups are pruned following tree expansion as well as immediately after they are processed. Because groups are unprocessed following tree expansion, in order to determine if they are prunable, Dense-Miner uses support information gathered during previous database passes.

U g h g( )  t g( ) t g( )  gc g i t g( )? h g( )  h gc( ) t gc( ) t g( ) i  U r g h g( ) r?  r h g( ) t g( )?? g  g g  h g( ) h g( ) C? h g( ) i{ }? h g( ) i{ } C? ? i t g( )? h g( ) t g( )? h g( ) t g( ) C? ?  FIGURE 2. Dense-Miner at its top level. The input parameters minconf, minsup, minimp, and  are assumed global.

C  DENSE-MINER(Set of Transactions ) ;; Returns all frequent, confident, large ;; improvement rules present in Set of Rules Set of Groups GENERATE-INITIAL-GROUPS( , ) while  is non-empty do  scan  to process all groups in PRUNE-GROUPS( , ) ;; Section 5  GENERATE-NEXT-LEVEL( ) EXTRACT-RULES( )  PRUNE-GROUPS( , ) ;; Section 5 return POST-PROCESS( , ) ;; Section 7  T  T R ??  G ? T R G  T G G R  G ? G R R ?? G  G R R T  U  FIGURE 3. Procedure for expanding the next level of the set-enumeration tree.

GENERATE-NEXT-LEVEL(Set of groups ) ;; Returns a set of groups representing the next level ;; of the set-enumeration tree Set of Groups for each group  in  do  reorder the items in   ;; Section 6 for each item  in  do  let  be a new group with  and  return  G  Gc ?? g G  t g( ) i t g( )  gc h gc( ) h g( ) i{ }?=  t gc( ) j j follows i in the ordering{ }= Gc Gc gc{ }??  Gc  R  g g  h g( ) h g( ) C? g    5.1  Applying the pruning strategies Dense-Miner applies multiple strategies to prune nodes  from the search tree. These strategies determine when a group  can be pruned because no derivable rule can satisfy one or more of the input constraints. When a group  can- not be pruned, the pruning function checks to see if it can instead prune some items  from . Pruning tail items reduces the number of children generated from a node, and thereby reduces the search space. An added benefit of prun- ing tail items is that it can increase the effectiveness of the strategies used for group pruning. The observation below, which follows immediately from the definitions, suggests how any method for pruning groups can also be used to prune tail items.

OBSERVATION 5.1: Given a group  and an item , consider the group  such that  and  . If no rules derivable from  satisfy some given constraints, then except for rule , no rules  derivable from  such that  satisfy the given constraints.

The implication of this fact is that given a group  and tail item  with the stated condition, we can avoid enumer- ating many rules which do not satisfy the constraints by simply removing  from  after extracting rule  if necessary. The implementation of Prune- Groups, described in Figure 4, exploits this fact.

The group pruning strategies are applied by the helper function Is-Prunable which is described next. Because fewer tail items can improve the ability of Is-Prunable to determine whether a group can be pruned, whenever a tail item is found to be prunable from a group, the group and all tail items are checked once more.

5.2  Pruning strategies The function Is-Prunable computes the following values  for the given group : ? an upper-bound  on the confidence of any rule  derivable from , ? an upper-bound  on the improvement of any rule  derivable from  that is frequent, ? an upper-bound  on the support of any rule deriv-  able from .

Note that a group  can be pruned without affecting the  completeness of the search if one of the above bounds falls below its minimum allowed value as specified by minconf, minimp, and minsup respectively. The difficulty in imple- menting pruning is not simply how to compute these three bounds, but more specifically, how to compute them given that acquiring support information from a large data-set is time consuming. We show how to compute these bounds using only the support information provided by the candi- date set of the group, and/or the candidate set of its parent.

In establishing these bounding techniques in the remain- ing sub-sections, we assume the existence of a special ?derived? item  that is contained only by those transac- tions in the data-set that do not contain the consequent item- set . Similarly, for a given item , we sometimes assume the existence of an item  contained only by those trans- actions that do not contain . These derived items need not actually be present in the data-set -- they are used only to simplify the presentation. For an itemset , we have that . We also exploit the fact that , which holds whether or not  and/or  contain derived items.

5.3  Bounding confidence THEOREM 5.2: The following expression provides an upper-  bound on the confidence of any rule derivable from a given group :    where  and  are non-negative integers such that and .

Proof: Recall that the confidence of a rule  is equal to . This fraction can be rewritten as fol-  lows:  where  and  .

Because this expression is monotone in  and anti- monotone in , we can replace  with a greater or equal value and  with a lesser or equal value without decreasing the value of the expression. Consider replac- ing  with  and  with . The claim then follows if we establish that for any rule  derivable from , (1)  , and (2) . For (1), note that . It fol- lows that , and hence .

For (2), note that . Because  g g  i t g( )  g i t g( )? g' h g'( ) h g( ) i{ }?=  t g'( ) t g( ) i{ }?= g' h g( ) i{ }?  r g i r?  g i  i t g( ) h g( ) i{ }?  FIGURE 4. Top level of the pruning function.

PRUNE-GROUPS(Set of groups , Set of rules )  ;; Prunes groups and tail items from groups within ;;  and  are passed by reference for each group  in  do  do  if IS-PRUNABLE( ) then remove  from else for each  do  let  be a group with and  if IS-PRUNABLE( ) then remove  from  put  in  if it is a frequent and confident rule  while  G R G  G R g G  try_again false? g  g G i t g( )?  g? h g?( ) h g( ) i{ }?=  t g?( ) t g( ) i{ }?= g?  i t g( ) h g( ) i{ }? R  try_again true? try_again true=  g uconf g( )  g uimp g( )  g usup g( )  g g  c?  C i i? i  I c?{ }? sup I c?{ }?( ) sup I( ) sup I C?( )?=  I1 I2? sup I1( ) sup I2( )?? I1 I2  g  x x y+ -----------  x y y sup h g( ) t g( ) c?{ }? ?( )? x sup h g( ) C?( )?  r sup r C?( ) sup r( )?  x' x' y'+ ------------- x' sup r C?( )=  y' sup r( ) sup r C?( )?=  x' y' x'  y'  x' x y' y r g  x x'? y y'? h g( ) r? sup r C?( ) sup h g( ) C?( )? x x'?  r h g( ) t g( )??    , we have:  .

Theorem 5.2 is immediately applicable for computing for a processed group  since the following item-  sets needed to compute tight values for  and  are all within its candidate set: , , , and  . There are  rules derivable from a given group , and the support of these four itemsets can be used to potentially eliminate them all from consideration.

Note that if  were frequent, then an algo- rithm such as Apriori would enumerate every derivable rule.

We have framed Theorem 5.2 in a manner in which it can be exploited even when the exact support information used above is not available. This is useful when we wish to prune a group before it is processed by using only previously gath- ered support information. For example, given an unproc- essed group , we cannot compute  to use for the value of , but we can compute a lower-bound on the value. Given the parent node  of , because  is a superset of  , such a lower-bound is given by the observation below.

OBSERVATION 5.3: Given a group  and its parent  in the set-enumeration tree,  .

Conveniently, the support information required to apply this fact is immediately available from the candidate set of .

In the following observation, we apply the support lower-bounding theorem from [4] to obtain another lower- bound on , again using only sup- port information provided by the candidate set of .

OBSERVATION 5.4: Given a group  and its parent  in the set-enumeration tree,  .

When attempting to prune an unprocessed group, Dense- Miner computes both lower-bounds and uses the greater of the two for  in theorem 5.2.

5.4  Bounding improvement  We propose two complementary methods to bound the improvement of any (frequent) rule derivable from a given group . The first technique uses primarily the value of  described above, and the second directly estab- lishes an upper-bound on improvement from its definition.

Dense-Miner computes  by retaining the smaller of the two bounds provided by these techniques.

Bounding improvement using the confidence bound  The theorem below shows how to obtain an upper-bound on improvement by reusing the value of  along with another value  no greater than the confidence of the sub-rule of  with the greatest confidence.

THEOREM 5.5: The value of  where is an upper-bound on the  improvement of any rule derivable from .

Proof: Let  denote the sub-rule of  with the great-  est confidence. Because  is a proper sub-rule of any rule  derivable from , we know that  is an upper-bound on .

Because  and , we have:  .

Dense-Miner uses the previously described method for  computing  when applying this result. Computing a tight value for  requires knowing the sub-rule  of with the greatest confidence. Because  is not known, Dense-Miner instead sets  to the value of the following easily computed function:  if  has a parent , otherwise.

The fact that  follows from its definition. Its computation requires only the value of  where  is the parent of , and the supports of  and  in order to compute . The value can be computed whether or not the group has been processed because this information can be obtained from the parent group.

Bounding improvement directly  A complementary method for bounding the improvement of any frequent rule derivable from  is provided by the next theorem. This technique exploits strong dependencies between head items.

THEOREM 5.6: The following expression provides an upper- bound on the improvement of any frequent rule derivable from a given group :  where ,  and  are non-negative integers such that ,  , and  Proof sketch: For any frequent rule  derivable from , note that  can be written as:  where the first term represents  (as in Theorem 5.2) and the subtractive term represents the confidence of the proper sub-rule of  with the greatest confidence. To prove the claim, we show how to transform this expres- sion into the expression from the theorem statement, arguing that the value of the expression never decreases as a result of each transformation.

To begin, let the subtractive term of the expression denote the confidence of , a proper sub-rule of  such that  where  denotes the item  from  that minimizes .

r c?{ }? h g( ) t g( ) c?{ }? ?? y sup h g( ) t g( ) c?{ }? ?( )?  sup r c?{ }?( )? sup r( ) sup r C?( )? y'= =  uconf g( ) g x y  h g( ) h g( ) C? h g( ) t g( )? h g( ) t g( ) C? ? 2 t g( ) 1?  g  h g( ) t g( ) C? ?  g sup h g( ) t g( ) c?{ }? ?( ) y  gp g h gp( ) t gp( )? h g( ) t g( )?  g gp  sup h gp( ) t gp( ) c?{ }? ?( ) sup h g( ) t g( ) c?{ }? ?( )?  gp  sup h g( ) t g( ) c?{ }? ?( ) gp  g gp  sup h g( ) c?{ }?( ) sup h gp( ) i? c?,{ }?( ) i t g( )? ??  sup h g( ) t g( ) c?{ }? ?( )?  y  g uconf g( )  uimp g( )  uconf g( ) z  h g( )  uconf g( ) z? z max r? h g( )? conf r( ),( )?  g rs h g( )  rs rd g  conf rd( ) conf rs( )? imp rd( ) uconf g( ) conf rd( )? z conf rs( )?  imp rd( ) conf rd( ) conf rs( )?? conf rd( ) z?? uconf g( ) z??  uconf g( ) z rs h g( )  rs z  fz g( ) max fz gp( ) conf h g( )( ),( )= g gp fz g( ) conf h g( )( )=  fz g( ) max r? h g( )? conf r( ),( )?  fz gp( ) gp g h g( ) h g( ) C? conf h g( )( )  g  g  x x y+ ----------- x  x y ?+ + ---------------------?  x y ? y sup h g( ) t g( ) c?{ }? ?( )? ? min i? h g( )? sup h g( ) i{ }?( ) c? i?,{ }?( ),( )? x min max y2 y?+ minsup,( ) sup h g( ) C?( ),( )=  r g imp r( )  x? x? y?+ ------------- x? ??+  x? y? ?+ ? ??+ + -----------------------------------?  conf r( )  r  rs r rs r im{ }?= im i  h g( ) sup h g( ) i{ }?( ) c? i?,{ }?( )    Since we can only decrease the value of the subtractive term by such a transformation, we have not decreased the value of the expression.

Now, given  and , it is easy to show that , , and . Because the expression is anti-mono-  tone in  and  and monotone in , we can replace with ,  with , and  with  without decreasing its value.

We are now left with an expression identical to the expression in the theorem, except for  occurring in place of . Taking the derivative of this expression with respect to  and solving for 0 reveals it is maximized when . Note that for any rule derivable from ,  must fall between  and  . Given this restriction on , the equation is maximized at  .

We can therefore replace  with  without decreasing its value. The resulting expression, identical to that in the theorem statement, is thus an upper-bound on .

To apply this result to prune a processed group , Dense-Miner sets  to  since the required supports are known. Computing a tight value for (  where  is the item in  that minimizes this support value) is not possible given the support values available in the candidate set of and its ancestors. Dense-Miner therefore sets  to an upper- bound on  as computed by the following function:  when has a parent  and where  denotes the single item within the itemset ,  otherwise.

This computation requires only the value of  which was previously computed by the parent, and the supports of candidate set members , , , and  in order to compute .

In applying theorem 5.6 to prune an unprocessed group  , Dense-Miner computes  as above. For , it lacks the necessary support information to compute  , so instead it computes a lower- bound on the value as described in section 5.3.

5.5  Bounding support  The value of  is comparatively easy to compute because support is anti-monotone with respect to rule con- tainment. For , Dense-Miner simply uses the value of . Other anti-monotone constraints, e.g.

those discussed in [10], can be exploited with similar ease.

6.     Item ordering The motivation behind reordering tail items in the Gener-  ate-Next-Level function is to, in effect, force unpromising rules into the same portion of the search tree. The reason this strategy is critical is that in order for a group to be prun- able, every sub-node of the group must represent a rule that fails to satisfy one of the constraints. An arbitrary ordering policy will result in a roughly even distribution of rules that  satisfy the constraints throughout the search tree, yielding little pruning opportunities.

We experimented with several different ordering policies intended to tighten the bounds provided by the pruning functions. The strategy we found to work best exploits the fact that the computations for  and  both require a value , and the larger the value allowed for , the tighter the resulting bound. The idea then is to reorder tail items so that many sub-nodes will have a large value for . This is achieved by positioning tail items which contribute to a large value of  last in the ordering, since tail items which appear deeper in the ordering will appear in more sub-nodes than those tail items appearing earlier. We have found that the tail items which contribute most to this value tend to be those with small values for  . This can be seen from Observation 5.4 which yields a larger lower-bound on  when the value of summed over every tail item is  small. The policy used by Dense-Miner is therefore to arrange tail items in decreasing order of  .

7.     Post-processing The fact that Dense-Miner finds all frequent, confident,  large-improvement rules and places them into  follows from the completeness of a set-enumeration tree search and the correctness of our pruning rules, as established by the theorems from Section 5. Dense-Miner must still post-pro- cess  because it could contain some rules that do not have a large improvement.

Removing rules without a large improvement is non-triv- ial because improvement is defined in terms of all the proper sub-rules of a rule, and all such rules are not neces- sarily generated by the algorithm. A naive post-processor for removing rules without a large improvement might, for every mined rule, explicitly compute its improvement by generating and testing every proper sub-rule. Because Dense-Miner is capable of mining many long rules, such an approach would be too inefficient.

Instead, the post-processor first identifies some rules that do not have a large improvement by simply comparing them to the other rules in the mined rule set . It compares each rule  to every rule  such that  and .

If ever it is found that , then rule  is removed because its improvement is not large.

This step alone requires no database access, and removes almost all rules that do not have a large improvement.

To remove any remaining rules, the post-processor per- forms a set-enumeration tree search for rules that could potentially prove some rule in  does not have a large improvement. The main difference between this procedure and the mining phase is in the pruning strategies applied.

For this search problem, a group  is prunable when none of its derivable rules can prove that some rule in  lacks a large improvement. This is determined by either of the fol- lowing conditions:  r rs ?? 0? y? y? ?? ??  ?? y? ?? ?? 0 ?? ? y? y  x? x  x? x? y2 y?+= g x? sup h g( ) C?( )  minsup x?  x? min max y2 y?+ minsup,( ) sup h g( ) C?( ),( ) x= = x? x  imp r( )  g y sup h g( ) t g( ) c?{ }? ?( )  ? sup h g( ) im?( ) im? c?,{ }?( ) im  h g( ) g  ? sup h g( ) im?( ) im? c?,{ }?( )  f? g( ) min f? gp( ) sup h gp( ) i? c?,{ }?( ),( )= g gp i  h g( ) h gp( )? f? g( ) ?=  f? gp( )  h g( ) h g( ) C? h gp( ) h gp( ) C? sup h gp( ) i? c?,{ }?( )  g ? y  sup h g( ) t g( ) c?{ }? ?( )  usup g( )  usup g( ) sup h g( ) C?( )  uconf g( ) uimp g( ) y sup h g( ) t g( ) c?{ }? ?( )?  y  sup h g( ) t g( ) c?{ }? ?( )  sup h g( ) t g( ) c?{ }? ?( )  sup h g( ) i? c?,{ }?( )  sup h g( ) t g( ) c?{ }? ?( ) sup h g( ) i? c?,{ }?( )  sup h g( ) i? c?,{ }?( )  R  R  R r1 R? r2 r2 R? r2 r1?  conf r1( ) conf r2( )? minimp< r1  R  g R    ? There exists no rule  for which ; ?  for all rules  such that  .

After groups are processed, any rule  is removed if  there exists some group  such that  and . Because the search  explores the set of all rules that could potentially prove some rule in  does not have a large improvement, all rules without a large improvement are identified and removed.

Our post-processor includes some useful yet simple extensions of the above for ranking and facilitating the understanding of rules mined by Dense-Miner as well as other algorithms. The improvement of a rule is useful as an interestingness and ranking measure to be presented to the user along with confidence and support. It is also often use- ful to present the proper sub-rule responsible for a rule?s improvement value. Therefore, given an arbitrary set of rules, our post-processor determines the exact improvement of every rule, and associates with every rule its proper sub- rule with the greatest confidence (whether or not this sub- rule is in the original rule set). In rule-sets that are not guar- anteed to have high-improvement rules (such as those extracted from a decision tree), the sub-rules can be used to potentially simplify, improve the generality of, and improve the predictive ability of the originals.

8.     Evaluation This section provides an evaluation of Dense-Miner  using two real-world data-sets which were found to be par- ticularly dense in [4].1 The first data-set is compiled from PUMS census data obtained from  . It consists of 49,046 transactions with 74 items per transac- tion, with each transaction representing the answers to a census questionnaire. These answers include the age, tax- filing status, marital status, income, sex, veteran status, and location of residence of the respondent. Similar data-sets are used in targeted marketing campaigns for identifying a population likely to respond to a particular promotion. Con- tinuous attributes were discretized as described in [6], though no frequently occurring items were discarded. The second data-set is the connect-4 data-set from the Irvine machine learning database repository ( ). It con- sists of 67,557 transactions and 43 items per transaction.

This data-set is interesting because of its size, density, and a minority consequent item (?tie games?) that is accurately predicted only by rules with low support. All experiments presented here use the ?unmarried partner? item as the con- sequent with the pums data-set, and the ?tie games? item with the connect-4 data-set; we have found that using other consequents consistently yields qualitatively similar results.

Execution times are reported in seconds on an IBM Intel- liStation M Pro running Windows NT with a 400 MHZ Intel Pentium II Processor and 128MB of SDRAM. Execution  time includes runtime for both the mining and post-process- ing phases.

The minsup setting used in the experiments is specified as a value we call minimum coverage, where  . In the context of consequent constrained association rule mining, minimum coverage is more intuitive than minimum support, since it specifies the smallest fraction of the population of interest that must be characterized by each mined rule.

8.1  Effects of minimum improvement  The first experiment (Figure 5) shows the effect of differ- ent minimp settings as minsup is varied. Minconf in these experiments is left unspecified, which disables pruning with the minimum confidence constraint. The graphs of the fig- ure plot execution time and the number of rules returned for several algorithms at various settings of minimum support.

Dense-miner is run with minimp settings of .0002, .002, and .02 (dense_0002, dense_002, and dense_02 respectively).

We compare its performance to that of the Apriori algorithm optimized to exploit the consequent constraint (apriori_c).

This algorithm materializes only those frequent itemsets that contain the consequent itemset.

The first row of graphs from the figure reveals that apriori_c is too slow on all but the greatest settings of min- sup for both data-sets. In contrast, very modest settings of minimp allow Dense-Miner to efficiently mine rules at far lower supports, even without exploiting the minconf con- straint. A natural question is whether mining at low sup- ports is necessary. For these data-sets, the answer is yes simply because rules with confidence significantly higher than the consequent frequency do not arise unless minimum coverage is below 20%. This can be seen from Figure 7, which plots the confidence of the best rule meeting the min- imum support constraint for any given setting.2 This prop- erty is typical of data-sets from domains such as targeted marketing, where response rates tend to be low without focusing on a small but specific subset of the population.

The graphs in the second row of Figure 5 plot the num- ber of rules satisfying the input constraints. Note that runt- ime correlates strongly with the number of rules returned for each algorithm. For apriori_c, the number of rules returned is the same as the number of frequent itemsets con- taining the consequent because there is no minconf con- straint specified. Modest settings of minimp dramatically reduce the number of rules returned because most rules in these data-sets offer only insignificant (if any) predictive advantages over their proper sub-rules. This effect is partic- ularly pronounced on the pums data-set, where a minimp setting of .0002 is too weak a constraint to keep the number of such rules from exploding as support is lowered. The increase in runtime and rule-set size as support is lowered is far more subdued given the larger (though still small) min- imp settings.

1 Both data-sets are available in the form used in these experiments from http://www.almaden.ibm.com/cs/quest.

r R? h g( ) r? conf r( ) uconf g( )? minimp? r R? h g( ) r?  r R? g h g( ) r?  conf r( ) conf h g( )( )? minimp<  R  http://augustus.csscr.washington.edu/census/comp_013.html  http://www.ics.uci.edu/~mlearn/MLRepository.html  2 The data for this figure was generated by a version of Dense-Miner that prunes any group that cannot lead to a rule on the depicted support/con- fidence border. This constraint can be enforced during mining using the confidence and support bounding techniques from section 5.

minimum coverage minsup sup C( )?=    FIGURE 5. Execution time and rules returned versus minimum coverage for the various algorithms.

FIGURE 6. Execution time of dense_0002 as minconf is varied for both data-sets. Minimum coverage is fixed at 5% on pums and 1% on connect-4.

FIGURE 7. Maximum confidence rule mined from each data-set for a given level of minimum coverage.

0 10 20 30 40 50 60 70 80 90  Ex ec  ut io  n ti  me (  se c)  Minimum Coverage (%)  connect-4  apriori_c dense_0002 dense_002 dense_02        1e+06  0 10 20 30 40 50 60 70 80 90  Nu mb  er o  f Ru  le s  Minimum Coverage (%)  connect-4  apriori_c dense_0002 dense_002 dense_02        0 10 20 30 40 50 60 70 80 90  Ex ec  ut io  n Ti  me (  se c)  Minimum Coverage (%)  pums  apriori_c dense_0002 dense_002 dense_02        1e+06  1e+07  0 10 20 30 40 50 60 70 80 90  Nu mb  er o  f Ru  le s  Minimum Coverage (%)  pums  apriori_c dense_0002 dense_002 dense_02          20 25 30 35 40 45 50 55 60 65  Ex ec  ut io  n ti  me (  se c)  minconf (%)  pums connect-4        1e+06  20 25 30 35 40 45 50 55 60 65  Nu mb  er o  f Ru  le s  minconf (%)  pums connect-4             0 10 20 30 40 50 60 70 80 90 100  Hi gh  es t  Ru le  C on  fi de  nc e  (% )  Minimum Coverage (%)  pums connect-4    8.2  Effects of minimum confidence The next experiment (Figure 6) shows the effect of vary-  ing minconf while fixing minimp and minsup to very low values. With connect-4, we used a minimum coverage of 1%, and with pums, a minimum coverage of 5%. Minimp was set to .0002 with both data-sets. As can be extrapolated from the previous figures, the number of rules meeting these weak minimp and minsup constraints would be enormous.

As a result, with these constraints alone, Dense-Miner exceeds the available memory of our machine.

The efficiency of Dense-Miner when minimum confi- dence is specified shows that it is effectively exploiting the confidence constraint to prune the set of rules explored. We were unable to use lower settings of minconf than those plotted because of the large number of rules. As minconf is increased beyond the point at which fewer than 100,000 rules are returned, the run-time of Dense-Miner rapidly falls to around 500 seconds on both data-sets.

8.3  Summary of experimental findings  These experiments demonstrate that Dense-Miner, in contrast to approaches based on finding frequent itemsets, achieves good performance on highly dense data even when the input constraints are set conservatively. Minsup can be set low (which is necessary to find high confidence rules), as can minimp and minconf (if it is set at all). This charac- teristic of our algorithm is important for the end-user who may not know how to set these parameters properly. Low default values can be automatically specified by the system so that all potentially useful rules are produced. Refine- ments of the default settings can then be made by the user to tailor this result. In general, the execution time required by Dense-Miner correlates strongly with the number of rules that satisfy all of the specified constraints.

9.     Conclusions We have shown how Dense-Miner exploits rule con-  straints to efficiently mine consequent-constrained rules from large and dense data-sets, even at low supports. Unlike previous approaches, Dense-Miner exploits constraints such as minimum confidence (or alternatively, minimum lift or conviction) and a new constraint called minimum improve- ment during the mining phase. The minimum improvement constraint prunes any rule that does not offer a significant predictive advantage over its proper sub-rules. This increases efficiency of the algorithm, but more importantly, it presents the user with a concise set of predictive rules that are easy to comprehend because every condition of each rule strongly contributes to its predictive ability.

The primary contribution of Dense-Miner with respect to its implementation is its search-space pruning strategy which consists of the three critical components: (1) func- tions that allow the algorithm to flexibly compute bounds on confidence, improvement, and support of any rule deriv- able from a given node in the search tree; (2) approaches for reusing support information gathered during previous data- base passes within these functions to allow pruning of nodes before they are processed; and (3) an item-ordering heuris-  tic that ensures there are plenty of pruning opportunities. In principle, these ideas can be retargeted to exploit other con- straints in place of or in addition to those already described.

We lastly described a rule post-processor that Dense- Miner uses to fully enforce the minimum improvement con- straint. This post-processor is useful on its own for deter- mining the improvement value of every rule in an arbitrary set of rules, as well as associating with each rule its proper sub-rule with the highest confidence. Improvement can then be used to rank the rules, and the sub-rules used to poten- tially simplify, generalize, and improve the predictive abil- ity of the original rule set.

