Efficient Construction of Compact Shedding Filters for Data Stream Processing

Abstract- High-volume source streams, coupled with fluctu- ating rates, necessitate adaptive load shedding in data stream processing. When ignored, a continual query (CQ) server may randomly drop items, when its capacity is inadequate to handle the arriving data, and degrade the quality of the query results.

To alleviate this problem, filters can be used at the source nodes.

However, regular source filteriug in itself is not sufficient to prevent random dropping, because the amount of data passing through the filters can still surpass the server's capacity. In this case, intelligent load shedding can be applied by the source filters to minimize the degradatiou in result quality. In this paper, we introduce a novel type of load-shedding source filters, called Non- uniformly Regulated (NR) sifters. An NR sifter judiciously applies varying amounts of load shedding to different regions of the data space within the sifter. We formulate the problem of constructing NR sifters as an optimization one. NR sifters are compact and quickly configurable, allowing frequent adaptations, and provide fast lookup for deciding if a data item should be dropped. We structure NR sifters as a set of (sifter region, drop threshold) pairs to achieve compactness, develop query consolidation tech- niques to enable quick constructiou, and introduce flexible space partitioning mechanisms to realize fast lookup.



I. INTRODUCTION  On-line data sources are increasingly taking the form of data streams, i.e. time ordered series of events or readings.

Some of the examples include network statistics, financial tickers, and environmental sensor readings. Continual Query (CQ) systems [1], [2], [3] that can evaluate standing queries over data streams are used to integrate these data sources into today's information processing tasks, and has led to the de- velopment of more comprehensive Data Stream Management Systems (DSMSs) [4], [5], [6], [7], [8], [9].

In this paper, we study adaptive source filtering and load shedding in stream processing, a fundamental problem in stream-based CQ systems. With high-volume source streams and soaring data rates at peak times, stream processing systems demand adaptive load shedding [10]. Otherwise, when data arival rates exceed the capcity of the CQ server, the CQ server may randomly drop data items, which could signifi- cantly degrade the quality of the query results  To mitigate this problem, source filtering is a commonly applied tecbnique to reduce the amount of data fed into a CQ server For each CQ a predicate that defines a superset of the portions of the source stream needed by the CQ can be extracted [ff], [f12] Such predicates can be maintained by a query index to quickly filter out the irrelevant elements [13], [14]. However, there is a challenge: Source nodes are usually  us.ibm. com, 3psyuus. ibm. con  computationally less powerful sensor nodes. Hence, shipping the complete query index to the source nodes is usually not possible or is performance-wise undesirable. As a result, a compact summary representation of the query index is often needed to perform effective source filtering. In the rest of the paper, we use the term "query" to refer to the predicates in the query index that define useful portions of the source streams, and "filter" to refer to the summary structure shipped to the source nodes for filtering out the irrelevant or less useful data items. In general, a CQ can be more complex than the query predicate associated with it in the query index, which essentially defines a superset of the CQ's answer in the data space.

Regular source filtering in itself is not sufficient to prevent the CQ server from randomly dropping data items. This is because the amount of data matching the predicates defined by the source filters can still surpass the capacity of the CQ server, in terms of computation or communication. For instance, large number of sources coupled with CQs that have low selective query predicates can easily overwhelm the server or stress the communication links. Hence, we need an intelligent source filter to perform adaptive load shedding on the data items matching its predicates.

We introduce a novel type of source filters called non-  uniformly regulated sifters, or NR sifters, for adaptive load shedding in stream processing. An NR sifter regulates varying degrees of filtering for different regions of the data space defined by the sifter, thus the adjective "non-uniformly reg- ulated". Similar to a basic filter, an NR sifter drops items that do not match the predicates of any queries. More importantly, unlike a basic filter, an NR sifter can also drop items that do match the predicates, which may impact the quality of the query results, in order to shed load beyond what can be achieved with a basic filter. Without sifters, any additional load can only be shed by the server via random dropping.

To see the advantage and importance of an NR-sifter, consider a sub-region of the data space that has many queries defined but only few data coming in. Obviously, this sub- region is not a good choice for load shedding. Dropping items from this region will only bring a small reduction in load, but it is at the cost of a significant quality reduction in the query results. Conversely, a sub-region that contains few queries but lots of data coming in is a good choice for load shedding.

Moreover, different queries in different sub-regions may have different quality-of-service (QoS) specifications, attesting to     the need for applying varying amounts of load shedding for different regions of the data space. The NR sifters introduced in the paper provide a way to perform differentiated load shedding along the data space, by taking query density, data density, and QoS specifications into consideration. To the best of our knowledge, we are the first to study this type of source filters for adaptive load shedding in data stream processing.

noooooc  ier D orm NRifi Fig. 1: Concept of NR sifters. Variable-sized holes represent different  amounts of load shedding applied to different regions.

Figure 1 illustrates an NR sifter and compares it with a  regular filter and a uniform sifter, for the special case of two dimensional data space. Note that in an NR sifter, different amount of load shedding can be applied to different parts of the data space, represented by variable-sized holes in the figure.

The sizes of the holes and the shapes of the regions in an NR sifter filter are adaptively adjusted.

There are major challenges in building and employing NR sifters in data stream processing. (1) How do we construct NR sifters such that they provide good overall QoS in the query results, while shedding enough load to prevent random dropping? (2) How do we make the filter construction process lightweight, so that fast and frequent adaptations are feasible?

(3) How do we make the filtering process, which involves a lookup operation to decide if a data item should be dropped, fast so that it does not limit the source stream rates?

We successfully address these challenges. Specifically, we  make the following contributions- L We introduce a novel type of source filters, named non-  uniformly regulated sifters, and formulate the problem of optimal QoS-aware filter construction for them.

. In order to reduce the cost of filter construction, we decompose it into two basic problems. partitioning and configuration. The former is used to define the structure of a filter, whereas the latter is used to define its load shedding characteristics. We develop efficient and optimal filter configuration techniques using the concepts of query consolidation and sifter regions.

. We explore several data space partitioning algorithms and propose new ones for the purpose of filter construction.

We compare their performances under different work- loads, and report partitioning algorithms that best benefit particular types of workloads.

The rest of this paper is organized as follows. Section II introduces the fundamentals and the notation used Section III provides the basic problem definition. Section IV gives the details of practical filter construction for QoS aware load shedding. Section V introduces partitioning algorithms to be employed in filter construction. Section VI presents ex- perimental results. Section VII discusses related work and Section VIII concludes the paper.



II. FUNDAMENTALS AND NOTATION A. Data Model We represent the streaming items as n-dimensional records.

An item is denoted by X and is consisted of n values, denoted by (xl, ,,). X j I..n3, is a value from the domain Dj, which represents the jth dimension (attribute). We define xj e Dj = [d, d ], where d' and d- are the lower and upper bounds of the jth dimension, respectively. We denote the domain of the Ti-dimensional data item X by D, and define X C D = D1 x ..x D We consider multiple source nodes generating data streams with the same schema. We discus extensions of the data model to categorical attributes in our technical report [15].

B. Query Model We use Q to denote the set of queries (predicates) and  qi C Q, i C [l..|Q] to denote the ith query. A query can specify any number of constraints in the form of (attribute) (op) (constant), where (op) C } combined into a query expression using logical operators {A, V,-} For instance, for i = 2, a query can be in the form of -(xi < 4 A (xl > 3 V X2 < 5)). Such queries can be converted into a set of n-dimensional range queries and can be processed by the system as independent range queries with proper modifications [15]. As a result, from this point on we will assume that a query qi is associated with a range, which is an n-dimensional rectangle, denoted by R(qi) C P.

The query model can be extended to cover a broader class of  queries, as long as each query can be represented as a subspace of the n-dimensional data space corresponding to the domain D. However, the practicality of the approach presented in this paper depends on the computational simplicity of calculating the intersection of the subspace representing the query with an n-dimensional rectangle. We elaborates further on this issue in our technical report [15] and introduce other types of queries that can be supported within our framework.

Each query also has an associated weight, denoted by wi for query qi. This weight represents the relative importance of the query with respect to other queries in the system. Thus, the weights sum up to 1, i.e., EqieQ wi = 1.

C. QoS Model We enable each query to specify its QoS as a function of  the result completeness. Completeness for query qi is defined as the fraction of the query result that was reported, and is denoted by C(qj). We have C(qj) [0, 1. In an ideal scenario where there is no processing overload, we have Vq C Q C(qj = 1 As a result, the QoS of a query qi denoted by V(q), is equal to I in this case. In general, we have V(q) [0 11 When some data items are dropped due to load shedding, completeness values of the queries decrease.

We denote the decrease in the QoS per unit decrease in the completeness by ai, for query qi. In other words, the QoS is a linear function of completeness Thus we have V(qi) = ci C(qj) + @i. Since we have V(qi) = 1 when C(qi) = 1, we also have Xi = 1-ai. This strict structure     Fig. 2: Operational model  imposed on the QoS functions is a design choice. It is one of the factors that enable us to construct the filters quickly, enabling faster adaptation (see Section IV-C).

D. Filter VModel We define a filter as a function F: D [0,1]. Given a data  item X DX, F(X) gives us the drop threshold for that item.

The drop threshold defines the probability that item X will be dropped at the source, when filter F is applied. We define the process of determining whether a given data item X is to be dropped or passed under filter F, as a lookup operation. We list a number of desirable properties of an effective filter:  . Conmpactness. The filter has to be compact, since it has to be transferred to the source nodes.

* Fast Lookup. The filter has to provide fast lookup, so as not to limit the rate of source streams.

* Lightweight Construction. The computation of the filter has to be a lightweight operation, in order to provide low-overhead and thus frequent adaptation.

E. Operational Model Assume that a filter is already generated and installed on  the source nodes. An initial and trivial filter Fo can be used for this purpose, where VX C D, Fo(X) = 0. In other words, Fo passes every data item it receives, without dropping any. During the time between the installation of the last filter and the decision to replace it, the CQ server performs the following two major functionalities for the purpose of adaptation: performnance monitoring and sample collection.

First, the CQ server monitors the performance of the system with respect to how well it is performing in consuming all the incoming data items (possibly filtered at the source nodes).

Based on this, it maintains a throttler parameter denoted by z. The value o' the throttler parameter iS in the range 0 1] and defines the fraction of the aggregate source stream volume that can be processed without dropping any data items. Here aggregate source stream volume refers to the total volume of the source streams before any filtering is applied.

When the throttler value drifts beyond a system specified threshold, relative to the throttler value that was used during the construction of the last installed filter, it is decided that a new filter should be constructed. Techniques to adaptively set the throttler have been studied in different contexts, such as in  stream join adaptation [16]. Such techniques can be similarly adopted here (see [15] for details).

Second, the CQ server collects a small number of sample data items. We denote this set by U. These sample items are marked on the source nodes and are not processed by the filter, so as not to bias the perceived data distribution at the CQ server. A sampling fraction, denoted by a, is used to define the size of the set U. This sample set is later used by the CQ server to construct a filter for maximizing the QoS of the system. For instance, one of the major uses of the sample set during filter construction is to approximate the normalized frequencies of the data items, denoted by 1(X) for a data item X C D. These frequencies are particularly useful in calculating the completeness of queries under a given filter F. Concretely, for a query qi C Q, we have:  (1)CfXCR(q) (1-F(X)) f(X) dX C(Vi) fX(q fj,,,, dX.

Equation I computes the ratio of the query qi's result size under filter F to the query result size under no shedding assumption and with trivial filter Fo.

Once the performance monitoring process decides that a  significant change in the throttler has taken place, and enough number of samples are selected by the sample collection process, a new filter is constructed. Note that the focus of adaptation is on rate-bursts, since the throttler fraction depends on the rate changes. However, once a new filter is constructed, the possible changes in data item distribution are also taken into account while constructing the new filter. The assumption here is that, the rate changes happen in a much smaller time scale, comupared to the changes in data item distribution.

Figure 2 gives a higher level overview of the operational model.



III. BASIC PROBLEM DEFINITION  In this section, we define the basic problem of optimal filter construction. Given the throttler z, the item frequencies f, and the set of queries Q, our aim is to find the best filter F*, that is F*- BestFilter(z, f, Q). We define "the best" as the filter that provides the highest overall QoS, i.e., F* argmaXF V*(F f, Q). Here, V* denotes the overall QoS. For brevity, we use V (Q) and omit the other parameters. V*(Q) is defined as:  (2)V*Q) =vQ V i iW  Note that V (qi) is dependent on F and f, by way of completeness function C(cjk)  Furthermore the filter F* has to satisfy the proces sing constraint. That is, it should reduce the aggregate volume of the data items received at the CQ server to z times of its original value. The processing constraint is aimed at avoiding random dropping of data items, by matching the rate at which the data items arrive at the CQ server to the data item processing rate of the CQ server. We state the processing     constraint as follows:  1- z =I F(X) f(X) dX (3) XCD  The termI -z on the lefthand side of the processing constraint is the fraction of the data items we need to drop. The righthand side is the fraction of data items dropped by the filter F. The integration in Equation 3 can be converted to a sum for discrete domains.

Note that this formulation does not say anything about the compactness, fast lookup, or lightweight construction proper- ties of the filter. These properties depend on the structure of the filter. So far we have not put any restrictions on how a filter can be represented, and described it as a black box. Next section deals with practical filter construction and presents several design choices targeted toward achieving these properties.



IV. PRACTICAL FILTER CONSTRUCTION  In this section, we define a filter as a set of sifter regions.

Under this definition, we show how the optimal filter config- uration can be determined using the concept of query con- solidation. Sifter regions enable us to create compact filters, whereas query consolidation makes it possible to optimally and quickly configure these filters.

A. Sifter Regions We structure a filter as a non-overlapping and complete  partitioning of the data domain D. As a result, a filter consists of a set of sifter regions, denoted by S = {SS}i i C [1 1. 1 is the number of sifter regions and defines the granularity of thefilter.WehavejU3sSi =DandV{s5jc}s Sji =0 We associate with each sifter region, a single drop threshold, denoted by d(Si) for region Si C S. Thus, we have:  V;X C Si, F(X) = d(S7,) (4) Equation 4 simply states that the filter F will drop a data item X with probability d(Si), where Si is the sifter region that contains the data item X.

Note that the number of regions and drop thresholds, i.e., parameter 1, impacts the compactness of a filter as well as its performance. The larger the 1, the more fine grained is the filter, and thus the better the QoS. Yet, increasing I too much will defy the compactness requirement, and will increase the construction and lookup times. However, the good news is that our experimental results presented in Section VI show that as small as 16 regions can make a significant difference compared to using a uniformr drop threshold for the entire data space, under a wide variety of workloads.

Sifter regions also enable low-overhead evaluation of drop decisions at the source nodes, once the drop threshold for a given data item is looked up from the filter. A source node maintains 1 counter pairs, counting the number of items dropped and produced for each sifter region. If sending a newly produced item causes the ratio of these two counters to go below the drop threshold of the region, then the item is simply dropped. No random number generators are needed.

1) Construction vs. Configuration.: Given the definition of sifter regions and drop thresholds, we make a subtle but important distinction between filter construction and filter configuration. Constructing a filter involves (i) defining the sifter regions, which is performed by a partitioning algorithm, and (ii) setting the drop thresholds, which is performed by a filter configuration algorithm. We separate the sifter region partitioning from the drop threshold configuration. Here, we deal with the optimal filter configuration problem, given that the sifter regions are already defined. The separation of the two sub-problems means that the optimality of the filter configuration does not necessarily imply optimality of the filter itself. The partitioning algorithms are later discussed in Section V and are heuristic in nature.

2) Query Comnpleteness with Sifter Regions. We now ex- tend the query completeness definition to use sifter regions.

For this purpose we introduce a number of new notations.

f (Si) denotes the aggregate frequency of the items within region Si and is formally defined as f (Si) = fxs.i f(X) dX.

The frequencies for regions are calculated from the observed samples by the partitioning algorithms discussed in the next section.

Given a query qi C Q, we use S(qi) to denote the set of sifter regions that the region R(qi) of the query intersects with.

Formally, we have S(qi) = {S : Sj n R(qi) + 0 A Sj C S}.

With this definition at hand, we define the contribution of a sifter region Sj to the completeness of a query qi, denoted by C(qj, Sj) as follows:  C(qj, Sj) = (1- d(Sj)). ft(Sj - / :R ) /IS ES Sqj) f(Sk) Sk nR(qi) Sk  (5) The denominator of Equation 5 is an approximation of the frequency of items that match the query qi. The approximation is due to the assumption that the data item frequency is uniform within a sifter region. Since only the sifter region frequencies are known, the volume of the query region R(qi)'s intersection with the sifter regions in S(qi) are used to scale the frequencies. I. notation is used to denote region volumes.

Similarly, the numerator is an approximation of the frequency of data items that match the query qi and are from the sifter region Sj. The 1 -d(Sj) term on the left is the fraction of the matching data items that are passed through the filter and completes the definition.

The completeness of a query qi C Q can then be easily  defined as the sum of individual contributions from the set of sifter regions that intersect the query region. More formally, we have C(qj) = cSj5Scqj. C(qj, Sj).

B. Query Consolidation  Recall that the overall QoS metric from Equation 2 contains one term for each query. This translates into increased time spent for finding the optimal filter configuration. Given that the total number of queries is expected to be much larger than the number of sifter regions, i.e., QI 1, reducing the number of terms in V*(Q) from IQI to 1 will provide a much simpler     objective function. We achieve this via query consolidation.

Before delving into the details, we first define a useful notation that will be used for explaining query consolidation. Let Q(Si) denote the set of queries that intersect the sifter region Si C S.

Formally, we have Q(Si) = {qj: St n R(qj) : 0 A qj C Q9.

Note that the notations Q(Si) and S(qi) are duals of each other.

Query consolidation involves two logical steps: (i) decom- posing the QoS of each query into a number of additive components, one per intersected sifter region, and (ii) for each sifter region, consolidating the QoS components of different queries that intersect the region into a single QoS value.

Using the query completeness formulations from Sec- tion IV-A.2, we decompose the QoS value V(qi) as:  V(qi) = 3i + ai E C(qi Sj) (6) sj CS(qi)  Plugging Equation 6 into the overall QoS definition V9(Q) from Equation 2, we get:  V* (Q) =Ewi -Oii+ E E ai * Wi - C qi, Sj) (7) qi Q qi CQ Sj S(qi)  In Equation 7 we can perform a rotation of the two summations, making note of the property that Vqi C Q? VSj C S(qi) S_VSj C S Vqi C Q(Sj). The aim is to group the terms in the equation by regions first, instead of by queries. This yields:  V8(Q) = w, 3Wi* Si+ E a,i Wi * C(qi, Sj) (8) qi Q Sj Sqi Q(Sj)  We represent the EqiCQ(Sj) ai * wVi * C(qi, Sj) term in Equa- tion 8 by A(Sj) and refer to it as region QoS. Similarly, we use B to denote the EZq Q wi 3i term. It is important to note that by a single scan over the set of queries, all A's (region QoSs) and B can be computed. The computational complexity of region QoS calculation is given by (IQ 1) in the general case. Calculation of region QoSs is a one time operation during filter configuration. After it is performed, the overall QoS value can be expressed as a simple linear function of the drop thresholds. As a result, repeated calculation of V*(Q) with different drop thresholds during filter configuration is now a cheap operation, with complexity 0(1). Finally, using the new notation, we get:  V(Q) = B + E (1 -d(Sj)) A(Sj) (9) s GS  Equation 9 completes the discussion of query consolidation.

It replaces Equation 2 as the olbjective function.

C Problem Reformulation We now re-formulate the optimal filter configuration  problem using the concepts of sifter regions and associated drop thresholds. For brevity we further define B' = B + E A(SA) and state the problem as follows.

maximize V (Q) = B'- E A(S,) d(Si) (10) Si, s  subject to 1-z = E d(Si) f(Si) sies  {Si, Sj} c S , Id(Si) -d(Sj ) Q(Si)'0AQ(Sj)40  A  It is easily noticed that this is a linear program (LP), where drop thresholds are the problem variables. The processing constraint (the first constraint in Equation 10) is adapted from Equation 3 to use region frequencies and region drop thresholds. More importantly, we introduce a number of new constraints calledfairness constraints. The fairness constraints state that the drop thresholds of any two regions that are non-empty with respect to queries, can not differ by more than a threshold A C [0,1]. We refer to A as the fairness threshold. The aim of the fairness threshold is to ensure that the completeness of queries do not drift beyond a threshold, so that we do not end up increasing the overall QoS at the expense of diminishing the completeness and thus the QoS of some queries.

Without the fairness constraints, the problem is a fractional knapsack one, and as a result the drop threshold d(Si) of the sifter region Si with the lowest region QoS value A(Si) will be increased as miuch as possible to mieet the processing constraint. When the drop threshold reaches to 1, the sifter region with the next smallest region QoS will be increased, and so on. In the generic problem, the fairness threshold can be set to 1 to represent this special case. However, in most applications it is desirable to set a fairness threshold to allow more balanced degradation in QoS of the queries, as the throttler z decreases (more load is shed). In the other extreme case of A = 0, the scheme reduces to I = 1. The latter is equivalent to random dropping, with the exception that the dropping of data items will be performed at the source nodes using the filter, not at the CQ server end.



V. PARTITIONING ALGORITHMS  In this section, we describe a number of partitioning schemes used to determine sifter regions, and discuss their pros and cons. The first three of these schemes are:  . Uniform Grid partitioning divides the data space into I equi-sized sifter regions, V,jG[..j], RB(S,)| = BR(Sj)|.

* Data Equi-depth partitioning divides the data space into I sifter regions, such that the data frequency is same for all regions, V,j[ f],(Si) = f (Sj).

* Query Equi-depth partitioning divides the data space into I sifter regions, such that the query density is same for all regions (the center points of query regions are used as data to construct this partitioning).

Uniform grid partitioning is easy to build, requiring only 0(l) time It requires 0(n) time for lookup since for a given data item the index of the grid cell containing it on a given dimension can be found in constant time. 0(no .U) time     is needed to use the sample set U to compute the region frequencies that are needed for filter configuration. The major advantage of uniform grid partitioning is its small build time complexity. However, when the data distribution is skewed, most of the grid cells will be wasted (zero frequency regions), reducing the effectivness of the filter.

Data equi-depth partitioning uses the sample set U to build the sifter regions. This is achieved by soring the sample data items in U along the first dimension and dividing the set of samples into u equi-sized subsets of consecutive items (I = fU) and generating one partition from each subset. The same procedure is then repeated for the resulting partitions, using a different dimension. After all n dimensions are iterated, the resulting set of partitions correspond to the sifter regions. This process takes 0 (n U lgU l) time. The lookup requires O(n lg 1) time, since one binary search is needed to locate the index of the matching region, for each dimension. Note that data equi-depth partitioning does not suffer from data skewness.

On the down side, it fails to take into account query skewness.

Building a query equi-depth partitioning follows the same procedure as data equi-depth partitioning. However, since query distribution is used for building a query equi-depth partitioning, an additional post step is needed to calculate the data item frequencies. This post step takes O (n ig I1 U ) time, since one lookup is needed for each data item in U.

All three approaches lead to filters that require 0C(l) space, with the restriction that I = ur, where ua N+. Since we aim at small number of regions, this is a limiting factor. In what follows, we present a flexible partitioning algorithm, that not only can approximate the three basic approaches described so far with arbitrary 1, but also can be used to create more elaborate schemes.

A. Flexible Partitioning The idea behind flexible partitioning is to start with the  complete data space and progressively increase the granularity of the paritiuoning via (i) picking the existing partition that provides the highest partitioning benefit, and (ii) generating two new partitions from it by dividing along the best division point and dimension. The process is similar in spirit to the well known kd-tree partitioning. Different metrics used to determine the partitioning benefit, as well as division point and dimension lead to alternative partitioning schemes. For instance, the first two partitioning schemes described so far can be approximated, for arbitrary 1, as:  . Approximate Uniforn Grid: Partitioning benefit is the volume of the partition. Division point is the mid-point on the dimension of the longest edge.

* Approximate Data Eqai-depth. Partitioning benefit is number of sample data items in the partition Division point is the point that equally distributes the sample data items on the dimension of highest variance.

B. Partitioning with QDPart We now introduce a flexible partitioning algorithm named  QDPart. The main idea behind QDPart is to make use of both  data and query distributions within a partition to determine the partitioning benefit. For this purpose, QDPart uses additional information about the sample data items in U. Concretely, for each data item in U, we associate a hit value that gives the number of queries that this point satisfies. Such information is already available from the query evaluation component, since all sampled data items are processed by the CQ server. We consider a partition as a good choice for further partitioning along a given dimension if (i) it contains sample data items that are scattered along the given dimension, (ii) it contains sample data itemlns with highly varying hit values.

To compute the partitioning benefit of a given partition S'  with QDPart, we consider all n dimensions and pick the one that provides the highest benefit. Given dimension i E .[L.n], and denoting the sample data items in S' as U', we calculate the partitioning benefit as follows. We first calculate a diversity metric for dimension i. and partition S', defined as the product of two terms' (i) SSE of the hit values of data items in U' and (ii) SSE of the positions of data items in U' on dimension i, where SSE represents sum of squared errors. We denote this value by H(S', i), and refer to it as roof diversity. It represents the diversity within the partition S' along dimension i, with respect to query hit values and positions of data items. We then sort the set of sample data items in U' along dimension i. After the sort, there are U -1 possible partitionings of S'. The jth partitioning will have items indexed [l1.j in the first sub-partition denoted by S' and the items indexed [j'+3 1.. U'l] in the second sub-partition denoted by K,_ Among these alternatives, we pick the one that has the smallest sum of diversity metrics from two partitions, i.e. smallest H(S_, i) + H(S.+, i) value. We refer to this value as floor diversity. The difference between the roof and floor diversity values gives the benefit of partitioning S' along dimension i. More formally:  benefjit(S') = maxi .nj (H(SI, i)- T,inj1K..IU'I] H(Sj- ,i) + H(Sj+ i))) (1 l)  The QDPart algorithm can be adapted to use different diversity metrics by replacing the H function. Note that we made use of SSE in defining the diversity function H. Since SSE can be incrementally calculated, only two scans are needed after sorting the sample items along one of the dimensions to calculate the roof and floor diversities. As a result, the benefit value for partition S' can be computed in 0(n7 U' 1g U' ) time. This yields a worst case complexity of 0(n lI U la& U for the complete QDPart algorithm.

C Illustrative Examples  Figure 3 gives examples of flexible partitioning, for I 10 and n 2 In order from left to right we have uniform grid and data equi-depth approximations, and QDPart partitioning.

For the first two the sample data items are shown as fixed size circles. For the QDPart partitioning, the size of the circles are ad usted based on the number of hits each sample data point has Observe that, the uniform grid approximation is not perfect, but has the property that the biggest sifter region has     Fig. 3. Uniform Grid and Data Equi-depth approximations, and QDPart partitioning; 1 10, n =2  at most twice the space volume of the smallest one. On the other hand data equi-depth approximation has the property that the most populous sifter region has at most twice the data item frequency of the least populous one. QDPart partitioning, on the other hand, tries to group together sample data items that are close both spatially and in terms of their hit counts (represented by circle size in the figure).

symbol z  IQ Aci  L n  definitoon default throttler 0.5  #of sitter regions 16 of samples 1000  # of queries 1000 fairness threshold 0.5 QoS params urand [1,2] global skew 4 local skew 4  number of dimensions 2 query side lengths urand [0.005 0.01] # of hot spots 5  data distribution SKEW query distribution UNIF partitioning alg: QEDP  TABLE I: Experimental defaults

VI. EXPERIMENTAL EVALUATION  In his section we report a number of experimental results and study the impact of various parameters on the effectiveness of shedding filters. Before presenting our findings, we first describe the details of our experimental setup.

A. Experiniental Setup To evaluate NR sifters and to showcase their resiliency to  skewness in both query and data distributions, we designed a flexible workload generator. It uses the concept of hot-spot regions anid models skewness at two levels: global anid local.

We create a number of hot-spot regions, such that the data and queries are located within these hot-spots. We use a global skewness parameter, denoted by G, to vary the size of these host spot regions. Assuming that the data space is normalized to 0,11]n, hot-spots are assigned rando'm side lengths for each dimension, from the range G-1 00.5, 1]. These regions are centered at random positions within the data space. By changin GC one can model sparse cases where most of the data space is not used, as well as dense cases where all of the data space is utilized Within each hot spot region data and queries follow one of two distributions, namely a uniform distribution (UNIF in short) or a skewed distribution based on multi-dimensional normals (SKEW in short). In the latter case, the standard deviation for each dimension is assigned  randomly using the range (L G) -1 [0.5,1], where L is the local skewness parameter. The means of the multi-normals are randomly located within the hot-spot regions. Data and query distributions can each be one of {UNIF, SKEW}, resulting in four different combinations.

We compare four partitioning algorithms, namely uniform  grid (UGRID), data equi-depth (DEDP), query equi-depth (QEDP), and QDPart algorithm using flexible partitioning (FLEXP). We also compare load shedding using NR sifters against random dropping. The default values for various parameters are given in Table I. We present a large set of experiments which study the sensitivity of NR sifters to changes in these parameters. The overall QoS, as formulated in Equation 2, is the main metric for most of our experiments.

However, we also report results on construction time and filter lookup-time. All experiments were performed on a 3Ghz Intel P4 processor, using Java with Sun JDK 1.5.

B. Experimental Results We divide our experimental study into three sets of ex-  periments. The first set of experiments study the interplay of partitioning algorithms and skewness in data and query distributions. The second set of experiments study the impact of number of sifter regions on all performance metrics. The third set of experiments are sensitivity studies and examine the affect of sample size, fairness threshold, QoS parameters, and query size on overall QoS.

1) Partitioning and Skew: The graphs in Figure 4 plot overall QoS with different partitioning methods, under varying levels of local and global skewness. All four combinations of data and query distributions are shown in a total of eight graphs, the first four on global skew and the last four on local skew. Before we discuss the results in detail, we point out that it is more meaningful to examine the relative performance of the alternative approaches from these figures, instead of the general trend of overall QoS as a function of skewness, especially the local one. This is because, as the local data skewness approaches extreme values, most queries get empty results even without load shedding, artificially boosting the overall QoS.

First, we look at the case where query distributions within hot-spot regions are skewed and data distributions are unriform.

We observe that the best overall QoS achieved by the NR sifters is 60% to 50% and 35% to 65% higher compared to random dropping, along the global skewness and local skew- ness range, respectively. For this scenario the best partitioning algorithm turns out to be QDPart. The closest alternative is data equi-depth partitioning. The sub-optimal performance of data equi-depth partitioning can be attributed to its agnostic nature with respect to skewness in query distribution We also observe that query equi-depth partitioning performs the worst among sifter-based approaches and is only slightly better than random dropping. This observation holds for all of the cases we studied This shows the importance of capturing the skewness in data distribution, which is completely ignored by the query equi-depth partitioning. The performance of uniform     1 Data: Unif, Query: Skew 1 Data: Skew, Query: Unif 1 Dedp Dedp  cCo Rand Qedp 0 9 Rand Qedp o 9 C Ugrid Flex C 5 U rid Fl Im 0.81 0 0.8 0. 0.8  <07 < 0.7 < 0.7  4 6 8 10 global skew parameter, G  Data: Unif, Query: Skew DedpRand Qedp  ---Ugrid Flexp  0.5 2 4 6 8 10  global skew parameter, G  Data: Skew, Query: Unif _~  Co0 a  a)  4 6 8 10 local skew parameter, L  0.5  Data: Skew, Query: Skew  I~~~~~~~~~~~~~~~~~~~~~~  rRand Qedp  2 4 6 8 10 global skew parameter, G  Data: Skew, Query: Skew  0.9 0 09 0 0.7  08 0,8 '0.65  0.7 Rand < 0.7 Rand < 0.6 Ugrid Ugrid Dedp C Dedp Qledp Flexp  0.5i 0.5 0.59 2 4 6 8 10 2 4 6 8 110 2  local skew parameter, L local skew parameter, L  Fig. 4: Overall QoS with different partitioning methods, under varying local and global skew. All four distributions are shown  combina  0.75 Data: Unif, Query: Unif u) Rand? 0.7 UgridC5 * Dedp  e Qedp M 0.65 Flexp  < 0.6  0-.55 1 U.5 4.- 6-8 1   0.75 -  60 z30.1 3 c z=0.3 z=0.9 o anz-05 Ugrid  Eow 40 E 2 Qedp e'-'20 e   20 40 60 80 100 20 40 60 80 100 number of sifter regions, I #of sifter regions, I (JU 1 -= 1000/16)  Fig. 5: Impact of I on overall Fig. 6: Impact of I on con- QoS struction time  onx 5  4'0 aUgrid QedpCfl3 3 Dedp Flexp  3Q -  'o 00  # 20 40 60 80 100 number of sifter regions, I  Fig. 7: Impact of I on lookup time  co0 0.8  0.6  <:0.4 Skew/Skew Unif/Unif-X;,0.2 e/ Skew/Unif  + Unif/Skew  0.1 0.3 0.5 0.7 0.9 throttle fraction, z  Fig. 8: Impact of throttler on overall QoS  grid partitioning significantly degrades as the global skew increases. This is because most of the sifter regions are wasted covering the empty portions of the data space. This observation also extends to other cases shown in Figure 4.

Second, we study the case where data distributions within hot-spot regions are skewed and query distributions are uni- form. We observe that the best overall QoS achieved by the NR sifters is 35% to 20% and 40% to 2% higher compared to random dropping, along the global skew and local skew range, respectively. For this scenario the best partitioning algorithm is data equi-depth and QDPart is the closest alternative. The same observations holds for the case where both data and query distributions are skewed within hot-spot regions, except that the improvements over random dropping is higher These observations imply that none of the partitioning schemes we included in our study consistently perform the best under all workloads. We leave it as a future work to come up with a partitioning algorithm that might achieve this.

Last, we look at the case where both data and query distributions within hot-spot regions are uniform. This case is special: the query and data are proportionally distributed.

For any sub-region of the space, data and query densities are the same. We also experimented with a scenario where both data and queries are distributed normally within each hot-  spot region and the means of the normals overlap, which also represents a proportionally distributed scenario (these results are not shown in Figure 4). In both cases, the overall QoS was little impacted by the partitioning algorithm used, irrespective of the local and global skew. Furthermore, the NR sifters provided only a minor advantage of around 6% compared to random dropping. This improvement is due to the QoS-aware nature of sifters. If we were to use a QoS distribution that is skewed with respect to the locations of query regions within the data space, the improvement would have been larger.

2) Number of Sifter Regions: Figure 5 plots the improve- ment in overall QoS relative to random dropping as a function of number of sifter regions used, for different throttler values.

We make two main observations from the figure. First, the relative improvement in the overall QoS with increasing I is more pronounced with smaller throttler values. In fact, increasing I from its default value of 16 to 100 brings little improvement for z > 0.7 However when there is heavy load shedding it makes a profound difference. As an example, for z-01 increasing I to 100 boosts the relative improvement to 50c from 25o achieved with 1 16. Second, even though fine grained sifters become more useful when there is heavy load shedding, further increasing I brings diminishing returns.

This is observed by the flattening of the lines around I = 100   Ub*;, 0.6-  0.5   0 0.9  0.8a 07MO 0.8  0.5  4 6 8 110 global skew parameter, G  Data: Unif, Query: Unif  -Rand Ugrid Dedp  Flex  4 6 8 10 local skew parameter, L  ations of data and query    -z=0.1 z=0.5 z=0.3 z=0.7  2 0.03 0.04 0.05 iery side length  pact of query vol- umes  in Figure 5. This implies that compact filters are sufficiently effective in most scenarios.

Figure 6 plots the time it takes to construct a filter on the server side as a function of the number of sifter regions, using different partitioning algorithms. We observe that for 1 = 16, the construction cost, which involves performing partitioning, query consolidation, and running the linear solver, takes only about about 150 msecs, for all approaches. For I < 50, all alternatives are still close and under < 300 msecs.

However, as 1 increases further we see a surprising trend.

One would expect that filters employing variable partitioning based QDPart approaches (VARP in the figure) will be costlier to construct. Instead we see that data equi-depth partitioning takes the most amount of time, with a construction time of 2.7 secs for 1 = 100. This shows that it is the linear solver that dominates the construction time. Clearly, it takes more time to configure the data equi-depth filter. Note that for the SKEW/UNIF case (the default from Table I), data equi-depth partitioning performs the best. As a result, the additional time spent on configuration is put to good use and yields a better overall QoS value.

Figure 7 studies the impact of number of sifter regions on the lookup time, another important performance metric.

It plots the number of times the lookup operation can be performed within a second. This includes the decision to drop or forward a data item, which we consider as part of the lookup process. Figure 7 shows that the number of times the lookup can be performed by equi-depth partitioning based filters only decreases by 27% going from I = 16 to I = 100, whereas this number is 40% for QDPart based flexible partitioning.

Yet, even for I = 100, the latter can imake around 5 million lookups per second.

3) Other Sensitivity Studies. Figure 8 plots the overall QoS as a function of the throttler, for various data and query distributions. We see that, except for the UNIF/UNIF case, the improvement in overall QoS per unit increase in the throttler value is more profound for the heavy load shedding scenarios.

This shows that NR sifters are very effective when a significant portion of the load has to be shed In agreement with our  insight from Section VI-B 1, the UNIF/UNIF case, which is an example of the broader case of proportionally distributed queries and data, does not benefit from the use of sifters and follows a linear trend. This suggests emulation of random dropping behavior.

Figure 9 plots the improvement in the overall QoS relative  to random dropping, as a function of the fairness threshold A and for different throttler values. In general, the higher the fairness threshold, the more flexibility we have in setting the drop thresholds, thus higher overall QoS at the expense of fairness among queries. We observe that this trend is more pronounced for the high load shedding scenarios. For instance, when we have z = 0.3 the relative improvement raises from 5% to 40% while increasing A from 0.1 to 0.9. On the other hand, it stays almost flat for the case of z = 0.9. Note that the case of z = 0.1 is defected, since all drop thresholds except one reach to 1 after certain stage and further increase in the fairness threshold does not bring any improvement.

Figure 10 plots the improvement in the overall QoS relative to random dropping, as a function of the query side lengths and for different throttler values. When the range queries have increasing volumes, more data items match the query regions. Thus, the same fraction of the dropped stream have increasingly negative impact on the overall QoS. Figure 10 shows that the NR sifters provide slightly increased relative improvement over the random dropping as the query volumes get larger (only around 15% for the excessive load shedding scenario of z = 0. 1). However, the figure does not plot the unrealistically extreme cases. For instance, if all queries have the size of the hot spot regions, then sifters would obviously not provide any improvement over random dropping.



VII. RELATED WORK  Load shedding in stream processing has been an important topic since the inception of DSMSs [17]. Previous works in this area have so far focused on either load shedding on operator graphs [10] or specific stream operators, such as aggregations [18], [19] and joins [20], [21]. When shedding load in operator graphs, moving random drop filters as close to the input streams as possible was established as an effec- tive technique. However, load-shedding source filters that go beyond random dropping (such as NR sifters) have not been considered in the past. NR sifters, their fast construction and optimal configuration, are all new problems addressed by this paper.

Another related research is publish/subscribe, exemplified by Gryphon [22], SemCast [23], and Siena [24]. Pushing query (subscription) predicates close to the data sources (publishers) is a commonly applied paradigm in publish/subscribe systems for reducing the processing load, as well as the overall bandwidth consumption. However, the filtering performed in these systems do not impact the completeness of the query results. In other words, filters are employed to drop non- relevant data items that do not impact the query results Yet such filters cannot drop data items of lesser but non-zero value.

Query indexing is a relevant technique used in continual query systems to quickly filter out non-relevant items [13], [14]. Our NR sifters can be considered as summary query indexes, with a significant distinction: a data item is filtered out probabilistically by an NR sifter Their summary nature and use of non-uniformly regulated drop probabilities lend itself to compact and effective load shedding source filters,   50 Z=0.1  -,40 z_05 E + z=0.7 > 30 z=0.9  E 20.

0 . 10 /A0~1  .1 0.2 0.4 0.6 0.8 .9 fairness threshold, A  Fig. 9: Impact of fairness threshold   ,- e25< E 'o0 > X 20E 1 0-c 115'- *  o0 10  0.01 0.0:  qu Fig. 10: ImI    which can optimize query result quality under varying levels of load shedding dictated by dynamic stream rates, without incuring high adaptation overhead.



VIII. CONCLUSIONS  We presented and formulated the problem of adaptive source filtering and load shedding in the context of data stream processing. We introduced a novel type of source filter called non-uniformly regulated sifter, which can judiciously drop parts of the data streams at the source nodes by minimally impacting the QoS of the queries installed at the C() server.

This effectively reduces the load at the CQ server by reducing the amount of data fed into the query processor. We developed concepts and techniques to quickly construct compact, effec- tive (in terms of maximizing QoS), and efficient (in terms of lookup tiime) NR sifters. Fast construction of compact filters also enables frequent and low-overhead adaption to system dynamics, such as aggregate source stream rate and source data distribution. We presented large number of experimental results studying the behavior of NR sifters and presented a comparative study of alternative sifter construction schemes.

We showed that compact sifters are significantly superior to other alternatives, such as random dropping, especially for heavy load shedding scenarios.

