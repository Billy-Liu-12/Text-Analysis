A Distributed In-situ Analysis Method for Large-scale Scientific Data

Abstract?Recently, a massive amount of data is generated in a wide range of scientific applications such as NASA?s satellite, the large hadron collider, and large synoptic survey telescope.

Most of scientific data follows the array model, and there are various kinds of standard array formats such as HDF, NetCDF, MDSplus, and ROOT. SciDB is the most well-known DBMS that stores the array-based scientific data and processes queries on it.

SciDB is a distributed DBMS, and so, is scalable in terms of query performance. However, it has a severe drawback that takes a huge amount of time for loading a massive amount of scientific data into DBMS. That is, it is not scalable in terms of data loading. To overcome that problem, we propose a distributed in-situ analysis method that allows processing queries on raw scientific data in a distributed manner without explicit data loading. In detail, we propose the in-situ scan operator that scans necessary data of the array format and passes it to upper operators of the pipeline of a query plan. It also performs repartitioning during in-situ scanning, which is required for correct query results. Through experiments using real datasets, we have shown that the SciDB system using our method significantly outperforms the original SciDB system by orders of magnitude in terms of the performance of the first query.



I. INTRODUCTION  Recently, a massive amount of data is generated in a wide range of scientific applications such as NASA?s satellite, the large hadron collider, and large synoptic survey telescope.

The precision of measurement of such scientific observation instruments gets higher and higher, and as a results, the amount of data generated by the instruments increases rapidly. For example, the large hadron collider generates data of about 30 petabytes annually and stores data of the dozens of gigabytes per second into storage. Large synoptic survery telescope that observes the sky to find stars, planets and other objects, produces data of more than 30 terabytes every night.

Most scientific data follows the array model. DNA sequence data follows 1-D array data model, mass spectrometry data in proteomics follows 2-D array data model, and satellite image data follows 3-D array data model (2-D for spatial information and 1-D for time information). In order to represent array data, there have been proposed a number of formats including Hierarchical Data Format (HDF) [1], Network Common Data Form (NetCDF) [2], MDSplus [3], and ROOT [4]. Especially, HDF and NetCDF are widely used for storing satellite image  data as a standard format. The array data stored in these formats is usually accessed via a high-level interface that is optimized so as to utilize the maximum bandwidth of disk I/O.

Although the libraries for those formats stores and retrieves data as consecutive as possible, they do not support a high- level query interface like SQL. Thus, users usually need to use a DBMS system for storing and access the array data efficiently and conveniently by using a high-level SQL-like query interface.

In order to store array data and process high-level queries on it, SciDB [5, 13, 14] has been proposed and widely used in a lot of applications. SciDB is a distributed array data management system. In order to support efficient query processing, it loads raw array data into its storage via a series of complex loading steps. Loading the data in SciDB takes a huge amount of time due to a lot of read and write operations and repartitioning (i.e., shuffling) across network. Logically, loading data in SciDB consists of preprocessing, data distribution, local data loading, and re-dimensioning. The preprocessing step converts raw data in a array format to text data for loading. The distribution step splits the text data and distributes each piece to each SciDB instance. The local data loading step transforms N - dimensional array data into 1-dimensional array data, and then, loads it into the storage. The re-dimensioning step performs repartitioning so that each SciDB instance gathers its own partial N -dimensional array data. Since the original dimension of raw data is N , SciDB transforms 1-dimensional array data to N -dimensional array data for efficient query processing via the re-dimensioning step. In general, this re-dimensioning step is very time consuming since it requires copying data via network and a lot of read/write for preparing repartitioning.

Once data is loaded into SciDB, it can process queries efficiently via distributed query processing. However, data loading itself becomes a major bottleneck of SciDB. For instance, it takes about 12.6 hours to load the RRS data of NASA?s satellite data of 580 GB using a cluster of ten machines. It means loading 10 TB data requires about 9 days.

Many scientific observation instruments such as large synoptic survery telescope generate data of several tens of terabytes every day as mentioned above. It is hard to utilize SciDB for scientific data analysis in many real applications due to its     large amount of data loading time.

To overcome that problem, we propose a distributed in-situ  analysis method that allows processing queries on raw scien- tific data in a distributed manner without explicit data loading.

For scientific applications, the speed of data generation might be faster than the speed of data loading using SciDB. The motivation of in-situ analysis is processing a query on raw data without data loading overhead, while sacrificing the query performance a little bit. For typical business applications using DBMS, the high query performance is extremely important because a large number of queries should be processed within a reasonable time. However, the scientific applications usually require processing much less number of queries for the same data. Instead, they usually require prompt analysis of newly generated data. In detail, we propose an in-situ scan operator that scans necessary data of the array format and passes it to upper operators of the pipeline of a query plan. The in-situ scan operator performs the following two steps: local mapping and global mapping. The local mapping step transforms raw data to SciDB chunks in memory so that SciDB processes queries seamlessly without modification of query plan or query processor. The global mapping step performs repartitioning of the chunks among SciDB instances for getting correct query results. We also propose a simple distributor of raw data called HDF merger, which splits raw data without converting and distributes each piece to each machine so that SciDB can start in-situ analysis on local data on the OS file system. By getting rid of overhead of data loading almost completely, our in-situ method could improve the performance of the first query by orders of magnitude. For NASA?s real satellite datasets, the in-situ SciDB system outperforms the original SciDB system by 60 times in terms of the performance of the first query.

Our contributions are summarized as follows: ? We propose a novel distributed in-situ analysis method  that can significantly reduce the first query execution time by getting rid of data loading overhead.

? We propose a distributed in-situ scan operator that scans necessary data of the array format and passes it to upper operators of the pipeline transparently.

? We have shown that the proposed method significantly improves the query performance of the first few queries of the SciDB system.

The rest of this paper is organized follows. In Section II, we explain preliminaries for SciDB, which is a popular distributed array DBMS, and HDF data format, which is used in a wide range of the scientific fields. In Section III, we propose distributed in-situ analysis system that includes HDF merger and distributed in-situ scan operator. Section IV presents the results of experimental evaluation, and Section V discusses related works. Finally, Section VI summarizes and concludes this paper.



II. PRELIMINARIES A. SciDB  Typical business DBMS is developed to analyze efficiently business data which is represented as relation model. However,  the systems are unsuitable to analyze scientific data that fol- lows array model. Thus, various array DBMSs are developed to handle efficiently the array data. SciDB is distributed array DBMS that is widely used in the field of scientific analysis.

SciDB has been developed to satisfy all the requirements for DBMS which scientists hope to analyze scientific data such as array data model, massive scale of data processing, no overwrite storage system, processing error data and so on [5].

Figure 1 shows the architecture of SciDB that has shared- nothing architecture that is similar to Hadoop [6]. One or more daemons, called SciDB instance, are executed on each machine and a daemon has a role that performs master or slave. Master is responsible for managing metadata using PostgreSQL [7] and generating query plan. In contrast with Hadoop that master does not participate in processing data and store data, the master of SciDB processes and stores data with slaves.

When storing array data, SciDB splits the data into chunks, which are the sub-array having the same shape. The chunk is processing unit in SciDB and is distributed into instances by array practitioner having particular data replacement policy such as hash-based, range-based, and so on.

??????  ????????  ??????  ????????  ??????  ????????  ??????  ????????  ?????  ?  ? ?????  ???? ??	?  ?????????????   ??????  Fig. 1. Architecture of SciDB.

In order to analyze array data, SciDB should load the data in SciDB storage, called data loading. Data loading in SciDB takes a huge amount of time due to a lot of read and write operations and repartitioning (i.e., shuffling) across network.

We explain the data loading process of SciDB using figure 2 to verify that data loading takes lots of time. Although the data loading process consists of data distribution, local data loading, and re-dimensioning steps in figure 2, preprocessing step should be needed to convert scientific data to text data, like CSV. For executing the preprocessing step, scientists should write custom program. Writing custom application is inconvenient as well as unfamiliar for scientists. Each step is described as follow:  ? Data Distribution: splits a CSV file into CSV files having N lines and then distributes them into each SciDB instance. Two of disk I/O operations including one in master and one in slaves and one of network I/O opera- tions occur in distribution step.

? Local data Loading: transforms the distributed CSV files to Data Loading Format (DLF) of SciDB and then loads them in SciDB storage as 1-D array having line number of CSV file as a dimension. The same number of I/O operations occurs in loading step.

? Re-dimensioning: shuffles the loaded 1-D array data in order to map the data on schema of original raw data.

Two of I/O operations including one of disk I/O operation and one of network I/O operation occur.

????  ?????????????  ????  ? ???????????  ? ??????????? ????????  ??? ?	???  ???	????????  ???	????????????  ????  ????????  ????????  ?????????  ?????? ?  ?????????  ?????? ?  ?????????  ?????? ?  ????????? ????????? ?????????  ???? ???? ????  ??????????  ???  ?????????  ?????????  ???  ???????? ???????? ????????  ??????????  ???  ??????????  ???  ????????? ?????????  ?????????  ???  ?????????  ???  ???????  ??????  ? ????  ??????  Fig. 2. SciDB Data Loading.

The nine I/O operations are occurred including six of disk I/O operations and three of network I/O operations. I/O operation affects performance of data loading since the cost of I/O operation is expensive. In order to reduce I/O operations occurred during data loading, we propose in-situ analysis method processing query over raw scientific data without data loading.

B. HDF data format Typically, various scientific data formats have been de-  veloped to manage scientific data such as HDF, NetCDF, MDSpuls, ROOT, and so on. Especially, HDF and NetCDF are used to manage satellite data as popular data format.

HDF stores data that represents array, image, and table as well as NetCDF data. When storing data in HDF, HDF creates container that store diverse data such array, image and table. The one or more containers are stored in HDF file and container is represented as node of rooted direct graph model. For managing containers, HDF has mechanism that is similar to directory structure used in Unix system allowing cycle. Also, HDF file is accessed through high-level interface that is optimized to utilize maximum bandwidth of disk I/O. Although HDF libraries also stores and retrieves data as consecutive as possible, they do not support a high- level query interface like SQL. Thus, uesrs usually need to use a DBMS system for storing and access the array data efficiently and conveniently by using a high-lavel SQL-like query interface. For taking these advantages, users should need the data loading.

In order to load HDF files in SciDB, the HDF files should be converted to text data since SciDB does not load HDF  file directly. thus, custom application is needed to convert HDF file to text file. Writing the custom application is not only inconvenient but also unfamiliar for scientists. Thus, our distributed in-situ analysis method allows scientists to save their time because of removing these uncomfortable task and time-consuming task.



III. IN-SITU ANALYSIS ON DISTRIBUTION ENVIRONMENT  In this section, we present our distributed in-situ analysis method that is composed of method distributing raw files and in-situ scan operator. First, we describe distributed in- situ analysis to understand difference between our method and query processing after data loading.

A. Distributed in-situ analysis  If scientists want to analyze query for raw scientific data, the scientists should prepare custom application for preprocessing step and then load the data that is result of the preprocessing step in SciDB. Only after finishing the complex tasks, sci- entists are able to analyze wanted query for scientific data.

However, if using our distributed in-situ analysis, the wanted query is executed over raw scientific data without the complex tasks. We explain the difference between query processing after data loading and our distributed in-situ analysis in figure 3 with simple example.

???  ?????  ?  ???  ?????  ???  ?  ????????  ????  ???  ?????  ?  ???? ????  ???  ?????????  ??? ??	? ??  ??	?????  ???  ???????  ????? ??????  ????????????????????????????????????????????   ?  ??? ??? ???  ???  ????  ???  ?  ??	?????  ???? ?????  ????? ??????  ???? ??????????????????????????  ?????? ?  ???? ???  ????? ??????????? ??  ???????????? ??  ??????????????  ????????????  ??? ??? ???  Fig. 3. Comparison with existing query processing.

Figure 3(a) shows query processing after data loading in SciDB. Blue and green ellipses mean each external operation and internal operation of SciDB. Sample query means to get specific region of 3D array data stored in HDF. Result of sample query shows sea surface temperature of region that is 10?10?100 array. In order to analyze sample query, all of the data related for the query should be loaded into SciDB. (1) of figure 3(a) represents preprocessing step converting HDF to CSV. Data loading process of SciDB involves external operations for distributing and converting CSV and internal operations for loading CSV and then changing dimensions.

After finishing (1) and (2) of figure 3(a), the query is able to be processed in SciDB. Original SciDB should process the complex data loading process in order to analyze raw scientific data. However, our distributed in-situ analysis does not need the complex tasks to process query. Figure 3(b) shows the process of our distributed in-situ analysis. For adapting in-situ analysis on SciDB, we conduct HDF merger of (1) and dis- tributed in-situ scan operator of (2) in figure 3(b). HDF merger helps distributed in-situ analysis improving performance for sequence queries. Distributed in-situ scan operator is respon- sible for transforming raw scientific data into chunks, which is data representation of SciDB. We explain two methods of our distributed in-situ analysis in detail.

B. HDF Merger  Typically, raw scientific data is stored in single machine.

For analyzing the data on SciDB, the scientific data should be distributed to SciDB instances. In the case of satellite data, the data is generated as one file per specific period. If the satellite data is analyzed for successive queries, the multiple scientific files should be accessed every queries in above example of figure 4. Accessing multiple files affect decreasing performance since opening and closing file is commonly a significant overhead if the number of file increases. Thus, the merging scientific files is needed to reduce the number of file access. In order to solve the issue, we implement HDF merger that is responsible for merging multiple HDF files and distributing the files to SciDB instances. In the below example of figure 4, in-situ analysis with HDF merger ensures an file access per a query. If we analyze query for four HDF files twice, In-situ analysis without HDF merger occurs total eight file access. However, when using HDF merger, total two file access is occurred. As the files is increased, the gap of performance between both methods gets greater and greater.

??? ??? ??? ???  ??????????????????????????????  ????  ??????  ??? ?????  ????????  ??? ?????  ????????  ???  ???  ???  ???  ?????  ????????  ???  ???  ???  ???  ?????  ????????  ?????????? ????? ?????  ?????????????????????????????????  Fig. 4. Need of HDF merger.

The most DBMS reads data stored in storage through scan operator. Performing in-situ analysis on distributed environ- ment need distributed in-situ scan operator that transforms raw file to data representation of execution engine and shuffles data in the cluster. Especially, SciDB manages array data as chunk  and distributes the chunks to SciDB instances depending on chunk placement policy. We so implement two components of distributed in-situ scan operator having the philosophy of SciDB, called local mapping and global maping. Figure 5 represents the components of distributed in-situ scan operator.

Intuitively, the local mapping maps raw data on chunks so as to analyze query seamlessly in SciDB without modification except scan operator and reads only raw data required in query so as to reduce read disk I/O for improving performance. The global mapping peforms repartitioning so that each SciDB in- stance gathers its own chunks according to chunk replacement policy.

??????????????  ??????  ????????  ?????????????  ?????  ???????  ?????  ??????????  ?????  ??? ???  ???  ?????  ???????  ??????  ???????????  ?????????  ?????  Fig. 5. Components of distributed in-situ scan operator.

The local mapping consists of three step for mapping raw data on chunks, called filter, read, and build. The filter step gets filter information of submitted query for reading only data required in query. The filter information means the range of dimensions for the required data. Commonly, query for array data requires the part of entire array data. These query is called selective query. Figure 6 presents example of the selective query. Sample query means to retrieve 32 array in original 44 array. For analyzing query, the local mapping dose not access all data of array A since only green region of figure 6 is needed. In the filter step, the local mapping finds the filter information in filtering operator of the query plan such as subarray, slice, and between, which scans a certain region of array.

? ? ? ?  ?  ?  ?  ?  ? ?  ?????? ???????????????????????? ???????????  ???????  ? ?  ?  ?  ?  ????????????  ??????  Fig. 6. Example of selective query.

In the read step, the local mapping reads only the needed raw data according to the filter information obtained in the filter step. Reading only the needed data is implemented using HDF libraries that is optimized to utilize maximum bandwidth of disk I/O. Also, the read data is transformed to chunk representation in the build step. Algorithm 1 presents pseudo code for build step. Line 1 6 initialize the variables for chunk information. The line 1 is to build the list of chunks when query processes. the build step builds chunks in the list one by one through moving position on HDF data. The line 3 means current position on HDF data. Line 7 17 show that HDF data is built by moving position of HDF data on the chunk. If loop of line 7 completes once, one chunk is built. The loop of line 9 is to move position of HDF data on a chunk.

Algorithm 1 Build  Input: H /* HDF data */  Output: A /* Array */  1:  2:  3:  4:  5:  6:  7:  8:  9:  10:  11:  12:  13:  14:  15:  16:  17:  18:  C ? chunk list for query;  R ? rank of output array;  pos[R]	? coordinator of the first element for H;  chunkID ? calculate chunk ID using pos;  h ? the first element of H;  chunk ? 	?;  for each c ? C do  c.put(pos, h);  for each r ? R do  pos[r]++; /*increase index of rth dimension*/  newChunkID? calculate new chunk ID;  if(chunkID != newChunkID)  if(r != last dimension)  pos[r] ? first index current chunk;  else  c.put(pos, h);  A.put(chunk);  return A;  There is the case that location for output chunks and location for chunk replacement policy do not correspond. Thus, the out- put chunks of the local mapping should be relocated to process query seamlessly in upper operator since upper operator know that chunks are located according to chunk replacement policy.

For relocating chunks, we implement global mapping using Message Passing Interface (MPI). Figure 7 shows example of global mapping process. The chunk replacement policy in figure 7 is that blue chunks, red chunks and gray chunks are in order of instance 1, instance 2, and instance 3. In the figure 7, the output chunks in local mapping do not correspond to the chunk replacement policy and are divided to other instances. And data of a chunk is spread over instances. The left instance 1 sends chunks to other instances, except blue  chunks. the instance receives blue chunks of other instances and then merges the chunks if the chunk has same position.

The global mapping operates synchronously using barrier of MPI. If instance enters in barrier, the instance waits until all instances enter in barrier. And then, all the instances send message requiring needed chunks before sending the chunks to other instances. The instance receiving the request sends the chunks to instance sending the request. The chunks gathering in an instance is merged to a chunk if the chunks have the same chunk ID. And then, the global mapping finishes if all the instance reach the barrier again. The merged chunks are passed to upper operator.

??????????  ??????????  ??????????  ??????????  ??????????  ??????????  ??????????????????  Fig. 7. Example of Global Mapping.



IV. EXPERIMENTAL EVALUATION  In this section, we first compare the distributed in-situ analysis method with SciDB in terms of the query processing time of the first query. We measure the elapsed time of query processing for both a full scan query and selective queries.

We also compare the performance of the original data loading method of SciDB and the data loading method using our in- situ technique.

A. Environment  For experiments, we conduct all the experiments on the same cluster of ten machines. Each machine is equipped with Intel i7-5930K six-core CPU and 32 GB memory. All the ma- chines are connected via 10 Gbps Ethernet. For experiments, we use eleven real datasets of MODIS Aqua L3M which are satellite image data distributed by NASA. The data is stored in the HDF format. The size of dataset for full scan query is about 40 GB, and the sizes of datasets for selective queries are about 4.2 GB.

In terms of software versions and configurations, we use SciDB 14.13 for implementing our distributed in-situ scan operator. We assign two instance having 15 GB memory for each machine.

B. Results for full scan query  In this section, we evaluate the performance of query processing for the first query of both the original SciDB and the version of SciDB using our distributed in-situ analysis.

Figure 8 shows the elapsed time. We use the query ?SELECT count(*) FROM Array? for comparison. For SciDB, we mea- sure the times of each loading step : preprocessing time, split & distribution time, DLF transform time, 1-dimensional data loading time, and redimensioning time. For our method, we measure HDF merger & distribution time and in-situ scanning time, where there are no loading steps.

In the figure, our distributed in-situ analysis method sig- nificantly outperforms the original SciDB by more than 60 times since SciDB spends most of time for data loading.

In terms of query execution time, SciDB takes only 0.01 minutes, while our method takes 0.7 minutes. However, in terms of data loading (or distribution) time, SciDB takes about 60 minutes, while our method takes only 0.28 minutes. Thus, we can say that our in-situ analysis method improves the query performance of the first query on raw data without data loading overhead, while sacrificing the query performance a little bit.

Not only the first query, but also the first few tens of queries can be improved by this in-situ analysis method. About the first 60 queries would get benefit from using the in-situ analysis method since the performance gap between both is 60 times, and the query execution time of SciDB is negligible.

????????  ??  ?  ??  ??  ??????????  ????  ?  ??  ??  ??  ??  ??  ?  ??  ???  ??????????????????????????  ? ? ? ? ? ? ? ? ? ?  ?????????? ???????????  ?????????? ?? ?????	???????  ??????????????????? ?????????????  ??????????? ?? ???????  Fig. 8. Comparison of query execution times for the first query.

C. Results for selective query  In this section, we evaluate the query performance of selective queries rather than full scan query. We evaluate the real queries used in Korea Meteorological Administration to find red tide phenomenon of the coast of Korea in the earth.

The names of queries are FRTD, SS, and TSF, which access two data sets, three data sets, and four data sets, respectively.

The size of each data set is about 4.2 GB. All the queries are not full scan queries, but selective queries. The selectivity of those queries is about 0.7%. Figure 9 shows the performance of selective queries of the original SciDB and our method.

In the figure, our method still outperforms SciDB by orders of magnitude. After data loading, SciDB processes queries within 1?2 seconds. For the FRTD query, the performance gap is about 77 times, which is larger than the performance gap for full scan query, i.e., 60 times. This is because the time of in-situ scanning of our method decreases due to selective scanning of raw data. For the SS and TSF queries, the performance gaps are about 71 and 65 times, which are still larger than 60 times.

???  ??????  ??????  ??? ???? ????  ?  ???  ???  ???  ???  ????  ????  ????  ????  ???? ?? ???  ? ? ? ? ? ? ? ? ? ?  ?????? ????  ????????	?	????  Fig. 9. Queries related to red tide phenomenon.

D. Performance of in-situ data loading  SciDB has a feature that can store the data of query result in memory into storage. By exploiting that feature and our in-situ analysis method, we can store the resulting data of full scan query into storage. Then, the stored data is equal to the data loaded by using the original SciDB. We call this approach as in-situ data loading. In this section, we compare the data loading performance of the original SciDB and our in-situ data loading.

Figure 10 shows the data loading times of both. We use the real HDF data of 300 GB, which has the size of 2.3 TB in the CSV format. Our in-situ data loading significantly outperforms the original data loading mechanism of SciDB.

The in-situ data loading stores the query result in memory after execution of full scan query into storage, which takes 8.9 minutes. As a result, it improves the data loading performance by 65 times, compared with the original SciDB. The main reason of this large performance gap is that SciDB performs a mixture of read, write, and communication operations during data loading, while our in-situ data loading performs the series of read, communication, and write in order without mixing them.



V. RELATED WORK  There have been a number of efforts to execute query on raw files without data loading, called in-situ analysis. There are     ??  ????  ??  ???  ???  ????  ???  ????  ????  ?  ????  ????  ????  ????  ????  ????  ????  ???????????   ?????	???? ??????	?? ?????	?  ???????????????  ? ? ? ? ? ? ? ? ? ?  ???????? ??	??? ? ?? ???	?? ???  ????????? ?? ????????		??? ????????	???? ????  ???????	????????? ?????????? ???????	???  Fig. 10. Data loading using distributed in-situ analysis.

external tables [8, 9] as the basic example of in-situ analysis on relational DBMS. External tables is the way to use CSV format file to process query without data loading. Although schema of raw file is defined in database catalog, the actual data does not be loaded into DBMS.

NoDB [10] improves performance of in-situ analysis through optimization of components that read and transforms data. Selective tokenizing reduces the tokenizing costs by tokenizing data until the required attributes for query. NoDB supports positional map indexing that stores offset for at- tributes of each row in order to reduce parsing and tokenizing costs for sequence queries.

SCANRAW [11] improves performance of in-situ analysis using multi-threads that is suitable for the multi-core processor environment. In order to solve limitation that in-situ analysis is worse than query processing after data loading in the case of continuous query processing for the same raw file, SCANRAW applies speculative loading that loads a slight data before or after in-situ analysis.

Scientific Data Service/Query (SDS/Q) [12] is the in-situ analysis system for HDF on supercomputing environment.

Since supercomputing environment has the large capacity of main memory and parallel file system, SDS/Q uses in-memory engine that is faster than execution time of disk based engine.

Bitmap indexing used in SDS/Q is more suitable for analyzing multi-dimensional array than positional map indexing used in NoDB.



VI. CONCLUSIONS  In this work, we propose distributed in-situ analysis sys- tem that process query over raw scientific data without data loading. Our system also supports optimization that reads only the required data for selective query. We support HDF merger in order to guarantee accessing a file once for a query. Our distributed in-situ scan operator ensures to process a query seamlessly at upper operator through two components of Local Map and Global Map. Local Map maps raw data on chunks, which is SciDB data representation. Global Map shuffles the chunks depending on chunk replacement policy. Our system  outperforms the state-of-the-are distributed array DBMS by more than about 60 times.

