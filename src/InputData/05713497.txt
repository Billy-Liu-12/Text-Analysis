Hiding Sensitive Association Rules Using Central Tendency

Abstract? Privacy Preserving in Data Mining (PPDM) is a process by which certain sensitive information is hidden during data mining without precise access to original dataset.

Majority of the techniques proposed in the literature for hiding sensitive information are based on using Support and Confidence measures in the association rules, which suffer from limitations. In this paper we propose a novel architecture which acquired other standard statistical measures instead of conventional framework of Support and Confidence to generate association rules. Specifically a weighing mechanism based on central tendency is introduced. The proposed architecture is tested with UCI datasets to hide the sensitive association rules as experimental evaluation. A performance comparison is made between the new technique and the existing one. The new architecture generates no ghost rules with complete avoidance of failure in hiding sensitive association rules. We demonstrate that Support and Confidence are not the only measures in hiding sensitive association rules. This research is aimed to contribute to data mining areas where privacy preservation is a concern.

Keywords- Data mining, Sensitive Association Rules, Privacy preservation, Central Tendency

I.  INTRODUCTION  Association rules is one of the most popular data mining techniques [1] in use, but it is often followed with a privacy preservation concern. Personal information could be technically inferred in the process of data mining, though data mining intends to generalize across populations. The problem of privacy-preserving in data mining has many applications in domains like homeland security, customer transaction analysis, and medical database mining.

Therefore, an effective way to release the database with sensitive rules hidden is important. This leads to the research topic of sensitive rule hiding. Numerous techniques have been studied in the literature to cope with the privacy preservation problems [2, 3, 4, 5, 6, 7]. A common objective of all these techniques is to hide restricted association rules.

However, hiding these restricted association rules usually suffers from side effects. These side effects include generation of unwanted association rules, failure in hiding all of the association rules and hiding of non-sensitive genuine rules. A broader solution to these problems is to achieve the prime objective of hiding restricted association rules with ideally no side effects.

In this paper, we proposed an architecture which hides the restricted association rules with complete removal of the known side effects which are generation of unwanted, non genuine association rules while yielding no hiding failure.

Although the architecture proposed in this research is generalized to be applicable on any domain, we validated the new architecture over medical datasets which are usually characterized to be complex and have high dimensions.

The structure of the remaining paper is as follow: Section 2 presents a literature review of the existing techniques.

Section 3 describes a problem statement. Section 4 presents our proposed architecture. Section 5 elaborates results followed by a discussion. The last sections consist of comparison, conclusion and future work.



II. LITERATURE REVIEW  Hiding sensitive association rules plays a pivoted role in PPDM. Prime objective of the PPDM is to discover the non- obvious information. To achieve this objective, we reviewed literature on PPDM in order to identify the strengths and limitations of the existing techniques. Association Rule (AR) is an implication of the form described by equation 1  },,{ ?=?????= CAICIACAAR ?        (1) where I denotes set of Itemset in dataset D. A denotes Rule Antecedent or Tail. C is Rule Consequent or Head.

Antecedent is always placed on the Left Hand Side (L.H.S) of AR while Consequent is always preceded by A and written on the Right Hand Side (R.H.S) of AR. Solutions for the problem of privacy preservation have been classified into three broad classes. Border-Based Approaches, Exact Approaches and Heuristic Approaches. Only one of these classes is of the concern which in turn again classified into broad categories of data partition, data modification, and data restriction. The other two classes namely ?Border- Based Approaches? and ?Exact Approaches? are out of the problem domain.

Clifton [8] proposed the idea of limiting access to the database, fuzz the data, eliminate unnecessary grouping, augmenting data and audit. The authors in [9] presented five algorithms namely 1.a, 1.b, 2.a, 2.b, 2.c. All of these algorithms fall in the category of distortion based technique.

Algorithms 1.a, 1.b, and 2.a were aimed towards hiding association rules. Algorithms 2.b, 2.c were related to hiding large itemsets. Metrics used in all of these five algorithms were efficiency and side effects. None of these five  - 478 -    algorithms was best for all metrics. There were tradeoffs between efficiency and side effects. All of these algorithms were based on support and confidence framework. It can be particularly mentioned that these algorithms were primitive and first of their kind in hiding association rules techniques.

Side effects of these algorithms were also high.

The authors in [10] introduced a blocking technique heuristic in support and confidence framework. Privacy breach risk was minimized by keeping a high threshold while a small threshold yields high side effects. They argued that the major drawback of a blocking algorithm is the fact that the dataset, apart from the blocked values (also known as unknowns), is not distorted. Thus, an adversary can disclose the hidden rules by identifying those generated itemsets which contain question marks and lead to rules with a maximum confidence that lies above the minimum confidence threshold. If the number of all of the association rules is small then the probability of identifying the sensitive rules among them also increases. To avoid this issue, the authors proposed a blocking algorithm that purposely creates rules that were nonexistent in the original dataset (also known as ghost rules) while their corresponding itemsets contain unknowns. Thus, the identification of the sensitive rules becomes harder since the adversary is unable to tell which of the rules with maximum confidence above the minimum threshold the sensitive are and which ones are the ghost ones. However, the introduction of ghost rules leads to a decrement in the data quality of the sanitized outcome. To balance the trade-off between privacy and data loss the proposed algorithm incorporates a safety margin which corresponds to extend the sanitization being performed on the dataset. The higher the safety margin the better the protection of the sensitive rules and eventually the data quality of the resulting dataset becomes worse.

A FP-tree based method is presented in [11] for inverse frequent set mining which is based on reconstruction technique. The method defined two thresholds Minimum Support Threshold (MST) and Minimum Confidence Threshold (MCT). It makes use of a transactional database D. Rules R are generated from D with MST and MCT. In R, Sensitive rules Rh exists such that Rh ? R. These Rh represent the sensitive association rules disclosing private information to public and need to be secured. In such situation, the original database D is changed to the released database D?.

From D?, R?Rh rules are mined under the same MST and MCT. Therefore, the sensitive information through rules is prevented from exposure. A similar algorithm called SRH where sensitive rules with single antecedent and consequent were clustered is proposed in [12]. Each rule is modified in such a way that reduces its confidence. When all the sensitive association rules are hidden, clusters are converted into a modified database. This technique shows a high level of side effects both in terms of ghost rules and loss of non sensitive rules.

Two modification schemes can be found in [13, 14]. Both incorporate unknowns and aimed at hiding predictive association rules which are the rules containing the sensitive items on their LHS. Both algorithms rely on the distortion of a portion of the database transactions to lower the confidence  of the association rules. Compared to the work of [7], the algorithms presented in [13, 14] required a reduced number of database scans and exhibit an efficient pruning strategy.

However, by construction of this technique, they are assigned the task of hiding all the rules containing the sensitive items on their LHS, while the algorithm by [7] is able to hide any specific rule. The first strategy, called ISL, decreases the confidence of a rule by increasing the support of the itemset in its LHS. The second approach, called DSR, reduces the confidence of the rule by decreasing the support of the itemset in its RHS. Item ordering effect plays a pivoted role in both of algorithms. Different sanitized databases are produced under the implementation of this effect of ordering the sensitive items for hiding sensitive association rules.

Moreover, the DSR algorithm seems to be more effective when the sensitive items have high support.

In [15] a new technique is extended from the original ones characterized by ISL and DSR [13, 14]. The authors in [15] modified both equations by introducing a new counter variable as shown in equation 2 and 3.

Mconfidence(x?y)= yxruleofcounterhidingX  YUX  ?+||  ||  (2)  Msupport(x?y)= yxruleofcounterhidingN  YUX  ?+||  ||  (3) where Mconfidence and Msupport denotes Minimum Confidence and Minimum Support respectively. The hiding counter is increased until the confidence or support gets below the minimum threshold values of support and confidence. In this way it was claimed that any association rule can be preserved. However, this technique did not mention about the released database. Among the objective outlined by [9], only the first of the three objectives was achieved. By increasing the denominator value in calculation of support and confidence will not alter the original database.

Consequently, the technique did not release a database from which no sensitive rules can be generated at specified threshed values. It also did not mention the side effects including ghost rules and hiding of valid non-sensitive association rules.

A technique based on fuzzification of support and confidence framework is proposed in [16]. Variable numbers of fuzzy membership functions were applied on quantitative data. Association rules were formed out of non boolean dataset. This fact makes this technique more realistic to real world application scenarios. This technique works on decreasing only support. In this technique, two strategies were employed to be used to decrease the confidence of an association rule A?B. First strategy increases the count of support (A) without affecting the count of support (AUB).

Second strategy incorporates count of support(A) as unchanged while decreasing the count of support(AUB). The authors in [17] proposed a set of evaluation parameters including the completeness and consistency evaluation.

Unlike other techniques, their approach takes into account two more important aspects: relevance of data and structure of database. They provide a formal description that can be used to magnify the aggregate information of interest for a target database. Framework of support and confidence has  - 479 -    been widely used in majority of the formulation of the association rules. However there may suffer from problems of Too Many Rules, Good Value for Threshold, Asymmetric Property of Confidence and Misleading Association Rules.

An overview of a number of symmetric objective interestingness measures, five of which are Lift (or Interest), Chi-Square, Correlation Coefficient, Log linear analysis and Empirical Bayes correction is provided in [12].

The authors in [18] proposed three measures for capturing relatedness between item pairs. Some of these measures offer properties that can be used to distinguish significant rules from non-significant rules.

There are other measures described in [19] which are better in result as compared to traditional association rules: interesting measures as support and confidence. In the technique proposed by [19], firstly, signal and text features from images were extracted. Secondly, calculate their frequencies. Thirdly, apply well-known dimensionality reduction techniques such as stemming, stop word elimination, and Zipf?s law to prune non-interesting features.

At the last stage, the remaining features were used to generate association rules. This technique shows that Correlation Coefficient, Laplace, Kappa and J-Measure result in better clustering compared to traditional support and confidence. In original formulation of association rules, only interesting measures defined were support and confidence.

Hence, all of the techniques investigated so far are based on the common framework of support and confidence.

However, there are situations where rules are valuable but low confidence out-weights them. Likewise, the authors in [20] criticized the boolean association rules giving an example in which a user may stay at a web page for significant time, but s/he can visit other insignificant pages many times with negligible time [20]. This may lead to the visit count of insignificant web pages more than that of the significant web page. In the literature numerous interesting measures have been defined. We experimented using various statistically-inspired interestingness measures as functions to assign weights to itemsets of association rules. We described those measures which we have used in the proposed architecture mentioned in next section. Computational details of these measures can be found in [21, 22, 23, 24, 25].

In order to compare them it is required to define all of them on the same units. For this purpose all of them have been defined in terms of probability [21]. Probability of an event can be mapped to database by equation 4.

P[E]=Freq[E] / |D|                                                        (4) Ref.[1] introduced the concept of support defined as  Supp(E)=P(E). In literature support is also known as Coverage. It is defined in equation 5.

Coverage(A?C)=Supp(A)=P(C)                                  (5) Support is primary pivoted measure in defining frequent  Itemset. Frequent Itemset are subsequently used to generate association rules. Ref.[1] also introduced the concept of confidence. Confidence is defined in equation 6.

Conf(A?C)=Supp(A?C)/Supp(A)=P(A and C)/P(A) = P(C | A).                                                                                (6)  Ref.[22] replaced the Confidence with All-Confidence which exhibit downward-closed closure property. It is defined by equation 7.

All-Confidence(E) = Supp(E) / Max(Supp(e element of E))=P(E)/Max(P(element of E))                                          (7)  Ref.[23] developed the concept of Conviction defined in equation 8.

Conviction(A?C)=(1-Supp(C))/(1- Conf(A?C))=P(A)P(notC)/P(A and not C)                        (8)  Leverage was introduced by [24]. It was based on the concept of difference between two measures. Co-occurrence of Antecedent and Consequent is measured followed by calculation of their individual appearance as depicted by equation 9.

Leverage(A?C)=P(A and C)-(P(A)P(C))                    (9) Ref.[23] introduced concept of Interest which got  popularity under the name of Lift. It is defined below: Lift(A?C)=Lift(C?A)=Conf(A?C)/Supp(C)  = Conf(C?A)/Supp(A)=P(A and C)/(P(A)P(C))              (10) A characteristic of lift is that it does not suffer from the rare item problem and also does not exhibit down-ward closed closure property.



III. PROBLEM STATEMENT  In this section, first we describe some metrics related to hiding association rule mining problem. Figure 1 shows these metrics diagrammatically.

Figure 1.  PPDM Data Sharing-based Sanitization Problem.

A. Hiding Failure Hiding Failure (HF) quantifies the percentage of the  sensitive patterns that remain exposed in the sanitized dataset. Formally, it can be computed as in equation 11.

||  |'|  SAR  SAR HF =  (11) where |SAR?| denotes total number of sensitive association rules discovered in the released (sanitized) dataset D. |SAR?| correspond to sensitive association rules appearing in the original dataset D. We can formulate first research question as ?Ideally, the hiding failure is required to be null?.

- 480 -    B. Misses Cost This measure quantifies the percentage of the non-  restrictive association rules which are hidden as a side-effect of the sanitization process. It can be calculated by the equation 12.

||  |'|||  NSAR  NSARNSAR MC  ? =  (12) where |NSAR| is the number of all non-sensitive association rules in the original dataset D and |NSAR?| is the number of those non-sensitive association rules revealed in released (sanitized) dataset D?. It is noticeable that there exists a compromise between the misses cost and the hiding failure, since the more sensitive association rules one needs to hide, the more legitimate association rules are expected to miss.

The second research question can be described as ?Achieving minimum number of Misses Cost (MC) during PPDM?.

C. Ghost Rules Ghost Rules (GR) is also known as artifactual rules that  is a quantifier of the percentage of the discovered association rules that are non genuine and artifact. This measure is computed as shown in equation 13.

|'|  |'||'|  SAR  SARSARSAR GR  ?? =  (13) where |SAR| is the number of association rules discovered in the original database D and |SAR?| is the number of association rules discovered in released (sanitized) dataset D?. Third research question can be defined as: ?Ghost/Artifactual Rules should be ideally zero?  D. Recovery Factor Recovery Factor (RF) describes the possibility by which  an adversary can recover a sensitive rule based on the non- sensitive rules. If all the sub-sets of a sensitive rule are recoverable from the sanitized dataset, then the recovery of the rule is also compromised. In this case, it is assigned an RF measure value of 1, otherwise RF value is 0. Fourth research question can be formulated as: ?Recovery Factor of each of the non-sensitive association rules after process of PPDM must be zero?. In the proposed architecture, we have not incorporated this research question. On the basis of above four research questions we can devise our problem statements as below.

? Firstly ?What is the problem in association rule mining  in reference to security when conventional framework of support and confidence is used??  ? Secondly ?Is there any other measure which yields better results in privacy preservation??  ? Thirdly ?How to minimize the side effects incurred during the hiding of association rules??

IV. PROPOSED ARCHITECTURE  From our literature review, all the techniques so far are based on conventional framework comprising of support and confidence. They suffer from side effects of miss cost or ghost rules or both whereas; some failed in hiding all the  sensitive association rules. They are the stimulus of an innovative architecture by which other statistical measures are exploited for hiding sensitive association rules. Hence, we devised a weighing mechanism based on central tendency. In the proposed architecture, we considered the five well-established measures to devise the association rules. The measures used in the architecture are discussed in mathematical notation in previous section. Second part of the proposed heuristic is comprised of different weights. These weights set, comprises of four kinds of statistical measures of central tendency. These include Sum, Mode, Mean and Median. Weights of each transaction in the dataset are generated by the measurement of how much a transaction defines a set of SAR. It counts how many times a single item appears in those SAR which are sub-sets of dataset transactions. These four statistical values comprise a set of weights out of this set we take mean, median, mode and sum.

Whereas mean, median, mode and sum each represents the weight assigned to a transaction. Weight of each transaction represents the degree of its correlation to the set of SAR.

Equation 9 represents the total count of single item in SAR while this single must be a sub-set of a Transaction. Counters are represented by x, y and z whose values are set by number of transaction, number of SAR, number of Items. n is the maximum number of transaction in dataset up to which x will be counted on. The upper limits of y and z are determined by the total number of SAR and the total number of items in a SAR. Once the set of counts is determined, the weights are calculated according to equation 10 to 13.

Equation 10 and 11 both calculate the Sum and Mean of the counts which are calculated in the equation 9. Equation 12 and 13 determine the values of the Median and Mode over the set of counts.

? ===  ??=  |||,|,  1,1,1  , ISARn  zyx  TIDxSARyIzq              (9)  ? =  =  N  i  qiSUW  _ (10)  ? =  =  N  i  qi N  MEW  _                                                       (11)  { })0,(_ NiqiMedianMDW ?>=                          (12)  { })0,(_ NiqiModeMOW ?>=                             (13)  A. Flow of the Proposed Architecture The flow of the proposed architecture as shown in figure  2 starts from data preparation of comma separated text data.

The values of this dataset are converted into a boolean dataset from CSV file format. Single value frequent items are generated from this database. These single valued frequent items are filtered out based on the provided frequency of count. Next step is to apply the sub-set algorithm on this set which generates all of the frequent items within provided threshold of frequency of count. Another component known as duplicate removal continuously remove the duplicate frequent items. The generation of frequent item is done through recursive programs of three functions. We know  - 481 -    from the Apriori algorithm [1] that frequent items can determine association rules. The next step generates all of the association rules through recursive functions based on sub- set problem. At this stage we have all of the association rules whose support is below certain threshold of frequency of count. The program now marks the association rule(s) as sensitive association rules. The next steps reduce any measure of these SAR so that they are eliminated out of the set of association rules. The measures refer to the set of Confidence, Leverage, Lift, Conviction and All-Confidence.

Based on the set of SAR, we determine the weights of each transaction in the database. Pruning technique component plays a pivoted role as this component is responsible to modify the actual transactions of the database.

Figure 2.  Flowchart of the Proposed Model for PPDM.

It takes SAR one by one. First it checks whether it is a sub-set of the first transaction with highest weight in the sorted database. The database is sorted on any of the four weights. The selected SAR is always composed of two or more items. At this stage the component provides user with two options. In the first option the item with highest frequency from selected SAR is modified. Its value of 1 is flipped to zero. In the second option, the item with lowest frequency from chosen SAR is selected and then transaction is modified. In this way only a single transaction is modified and the weight of the transaction needs to be recalculated.

This component continues the process of pruning until any of the two conditions is met. The first condition is that the  measure value of every SAR falls below the minimum threshold measure value of all of the set of association rules.

The second condition is that the frequency of count of the sensitive association rule gets below the provided value on the basis of which all of the association rules were generated.

At the end we have a modified database which does not generate any sensitive association rule on the same provided frequency of count and certain minimum threshold of measure. The developed framework consists of two prime heuristic which are shown in Figures 3 and 4.

Figure 3.  Weight Generation Algorithm.

B. Heuristic 1: Weights Generation The term weight refers to the degree by which a  transaction in a dataset supports all of the sensitive association rules. If all of the items involved in sensitive association rules are found in a specific transaction, then the transaction would be assigned a maximum value. Hence a modification in this transaction will mostly change the minimum measure values of the association rules. This component first calculates the number of occurrences of each item found in all of the sensitive association rules. After this, there are four ways to find the weight of each transaction:  1 Calculate Mean-Weights All of the occurrences are added and then an average value is calculated.

2 Calculate Median-Weights Median value among all of the occurrences are found and assigned as weight  3 Calculate Mode-Weights Mode value for all of the occurrences of item in all of the sensitive association rules is calculated.

4 Calculate Sum-Weights All of the occurrences are sum up to a single value.

After calculating the weights of each transaction, we have a dataset which we can sort for any of the weights in ascending or descending order.

- 482 -     Figure 4.  Pruning Algorithm.

C. Heuristic 2: Pruning Technique This technique first sorts all of the dataset items for any  chosen weight in ascending or descending order. A sensitive association rules is chosen and checked for its certain threshold value against any selected measure. If this value is above the threshold value then it indicates this association rule is not hidden. It analyzes each item involved in this association rules. Two options are on either selecting the Item with highest frequency or lowest frequency.

Experiments have shown that they yield different results.

Sometimes, highest frequency item from SAR is a better choice and vice versa. After selection of a single item out of this association rule, we modify that item?s value in transaction dataset. Now this will definitely lower down the certain threshold value of respective association rules. This will also alter the weight Table of that modified transition and its weight will be recalculated. This process is continued until all of the sensitive association rules have their certain threshold values below the specified values. At the end we can obtain the released database.



V. RESULTS AND DISCUSSION  We validated the proposed novel architecture for hiding sensitive association rules using the datasets from UCI Machine Learning Repository dataset (http://mlearn.ics.uci.edu/databases). These dataset are often used for benchmarking data mining algorithms. For our experiments we chose those items which were convertible to boolean values. The datasets used are shown in Table 1.

TABLE I.  DATASETS USED IN OUR EXPERIMENTS  Dataset  Total Instances  Missing Instances  Total Attributes  Ordinal Attributes  Zoo Dataset 101 None 18 15 Lymphography  148 None 19 9 Thyroid0387 9172 None 29 20 Hypothyroid 4822 1658 26 18   Figure 5 and Table 2 shows respectively the results taken  on the Lymphography dataset. In these results Leverage gives better results followed by Lift. This fact indicates that other than confidence, other measures also exist which can give better results.

TABLE II.  LYMPHOGRAPHY DATASET  FOC DB Transactions  DB Items  Freq Items  Assoc Rules Ghost Rules  10 149 10 115 1332 0  We in the proposed architecture have selected association  rules with all of these characteristics including single valued, multi valued frequent items and association rules.

Figure 5.  Lymphography Results.

Figure 6 depicts the comparison results to that of others techniques. In comparison to the technique proposed by [26] the proposed technique suffers less from miss cost. As far as other side effects of ghost rules and failure in hiding sensitive association rules are concerned, we observe that the technique in our proposed architecture produces zero ghost rules and can hide all of the sensitive association rules with zero hidden failure. It gives better result to all of those techniques with side effects of ghost rules and failure in hiding sensitive association rules. The works in [2,3,4,13,26] have their techniques implemented on single valued frequent items and single valued association rules; however the proposed technique is applicable to both single valued as well as multi valued frequent items and association rules. As shown in Figure 5, the number of new rules is minimized in a specific measure which is Leverage. This case outperforms the other peer measures as well as in comparison to the other techniques discussed in the literature. Nonetheless one notable result is that in all of the works performed, the technique in the proposed architecture has zero ghost rules whereas other techniques in literature have ghost rules.

Figure 6.  Comparison results of side effects.

- 483 -

VI. CONCLUSION AND FUTURE WORK  PPDM is important in data mining. In this research paper, we introduced a novel method for concealing sensitive association rules and producing minimum side effects. The innovative elements of our proposed model are summarized: (1) For concealing sensitive data, conventional measures of support and confidence are normally used by various researchers; but we claimed that instead of this framework, there exist other measures as well. Our proposed architecture hence adopted these measures such as All-Confidence, Conviction, Leverage and Lift. (2) Along with these measures we devised a statistical weighing mechanism which is based on central tendency Mean, Median, Mode and Sum. We observed that computationally there was no difference found among these four chosen statistical weights.

It correlates that for each transaction, higher the sum of the relationship of association rules, the greater would be the other three weights and vice versa. This implies that all of these are equivalent to each other with respect to the side effects of the technique. (3) This architecture generates no artifactual or ghost rules. (4) Failure in hiding sensitive association rules is also completely eliminated. They are prime achievement in the proposed architecture. (5) Side effect of Lost Rules do exists. The inherent reason for failure in elimination of Misses Cost is characterized by marking the association rules as sensitive or non-sensitive. If sensitive rules are multi valued involving high value of measures then in order to reduce their measure value to prevent them from being discovered, Misses Cost factor would be high.

As a future work we intend to investigate other measures than those listed in [21]. Secondly, we shall introduce a new supplementary technique to this architecture. It calculates the probability of each item involved in the SAR. Based upon these frequencies the database transaction would be modified in a way to minimize the side effects of lost results. The software should dynamically select the sort-order, ascending order or descending, on each transaction basis. Also we shall pay attention to the calculation of Recovery Factor - the ability by which one can infer about the modified dataset transaction. We first calculate the recovery factors through all known available techniques and then minimize them.

