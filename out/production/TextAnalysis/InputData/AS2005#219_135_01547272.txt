An Algorithm of Association Rules Extracting Based on Granular Computing and Its Application

Abstract-In this paper, on the basis of the Apriori algorithm, a granular computing-based algorithm for association rules extracting is presented. The comparison of the running time between the presented algorithm and the Apriori algorithm is discussed , and its running procedure is illustrated by a real world example . By analyzing, it is shown that the granular computing-based algorithm for association rules extracting reduces efficiently the number of candidate elements, and avoids scanning repeatedly the information table.

Index Term--association rules, data mining, granular computing

I. INTRODUCTION D ATA mining technology has become a research hotspot  nowadays. It has a very widespread application prospect, such as monitoring the quotations in the stock market timely, comparing the price of the goods in each electrical-business website, tracking the trends of friends and competitors, collecting various information about the inside or the outside of a company. Data mining has 5 models: classification, trend analysis, clustering, association rules and sequence patterns.

Association rules are simple and practical rules. The model of the association rules is a kind of descriptive models, and the algorithm of discovering association rules is a kind of unsupervised learning methods.

There are many association rules mining algorithms developed nowadays. The Apriori algorithm presented by R.Agrawal is the most famous .It has been applied in many fields. But it has some disadvantages, such as scanning repeatedly the transaction database each time when seeking for frequent item sets.

In this paper, we discuss association rules mining from a perspective of information granules and granular computing,.

On the basis of the Apriori algorithm, we describe a new algorithm based on granular computing. Compared with Apriori, our algorithm can efficiently reduce the number of  This study is supported by the Natural Science Foundation of China (NSFC-#60173054) and the Natural Science Foundation of Jiangx province (JXPNSF- 031 1 101) in China.

T.R.Oiu and X.Q.Chen are with the School of Computer and Information Technology, Beijing Jiaotong University ,Beijing 100044,China and with the department of Computer, Nanchang University, Nanchang,Jiangxi 330029,China (e-mail:qiutaorong@sina.com).

Q.Liu is with the department of Computer, Nanchang University, Nanchang,Jiangxi 330029,China  H.K.Huang is with the School of Computer and Information Technology, Beijing Jiaotong University ,Beijing 100044,China.

candidate elements and solve the bottleneck problem caused by repeatedly scanning the information table.



II. INFORMATION GRANULES AND GRANULAR COMPUTING Since human's ability of solving problems is limited, we  usually divide enormous complex information into several simple blocks by its characteristics. Each block can be viewed as an information granule. That is, an information granule is a set of entities with similar characteristics, or similar functions or indistinguishability[1],[2],[5],[7] .

L.A.zadeh firstly presented the concept of information granularity and granulation after long-term research[5].

Information granularity g is described in [5] by a proposition: g=x is G is X, in which x is a variable taking values in a universe of discourse U, G is a fuzzy subset of U which is characterized by its membership RG , and qualifier denotes a fuzzy probability.

T.Y.Lin described information granules by view of domain[11]: Let U be a crisp set . Let B c VXU be a binary relation. For each object p e V, we associate a binary subset BpcU, where Bp={ul p B u} that consists of all elements u that are related to p by B. The map B: V-Bp or the collection { Bp } is referred to as a binary neighborhood system.

Obviously, whether Bp is clear or fuzzy depends on the characteristic of B.

In a nutshell, granular computing is geared toward representing and processing information granules[l].



III. AsSOCIATION RULES The association rule, presented by R.Agrawal firstly, is an  very important subject in the data mining . It studies how to search for the frequent mode, association, relativity or cause and effect construction embedded in the association data, transaction data or other information carriers, and decides which will happen together through analyzing data or the relation between records.

Let I={ii,i2,.. .,im} be an item set, where ik(k=1,2,...,m) is an item, it can be a sort of goods in the shopping basket, and can also be a customer in a company, and etc. Assuming D is a transaction database, each transaction T is a subset of item set (i.e. T cI )and is identified by symbol TID(Transaction Identifier). If item set X CT, then we say that transaction T includes item set X.

Association rule is a logical implicating formula as  0-7803-9017-2/05/$20.00 02005 IEEE 225    XR Y, among which XC T, Y CT, and XnY= 0. If there are s% transactions in the transaction database include X U Y, then we say that the support of association rules is s%, its probability p(X U Y) can be written as: support (X * Y)=l{TI X U Y cT T ED }I/IDI =P(X U Y); If there are c% transactions including X also include Y, then we say that the confidence of association rules X > Y is c%, and the conditional probability P(YIX) can be written as: confidence(X=>Y)= I{TI XUY CT, TED }I/I{TI X5T, T E D} I=P(YIX).

We call the rule that both satisfies minimum  support(min_sup) and minimum confidence as strong rule.

The set of item are called item set. The item set which contains K items is called K-item set. The occurring frequency of item set is the number of transactions in which the item set is contained, and it is also known as support count or count. If the occurring frequency of item set is larger than or equal to the product of min sup and the number of transactions in D, then the item set satisfies min-sup and it is called as frequent item set. The set of frequent K-item set containing k items is generally denoted by Lk.

The task of mining association rules is to try to find all the  association rules that satisfy min-sup and min_confidence in a transaction database D.



IV. GRANULAR COMPUTING--BASED ALGORITHM FOR ASSOCIATION RULES EXTRACTING  A. Classic Frequent Item Set Method (Apriori Algorithm) In 1993,Agrawal presented an important method to mine  association rules. The design for association rules extracting algorithm can be divided into two sub-problems:  (1) Finding all the items whose support are larger than or equal to min_sup to generate a frequent item set.

(2) Using the frequent item set generated in step 1 to obtain expecting rules.

Step 2 is easy ,but it is difficult to do for step 1 . Given a frequent item set Y=III2...Ik, k.2, Ij EI, 1<=j<=k, our aim is to generate all the rules that include the items of set {I, 12, ....I Ik}only. The right side of each such rules has only one item ( i.e. [Y-Ij]=>Ij, Vl.i<k). Here we adopt the definition of strong rules. Once these rules are generated, only such rules that has support and confidence not less than those given by the user would be reserved.

We use iteration to generate all the frequent item sets. The  main idea of the Apriori algorithm is as follows: (1) LI = { Frequent 1-itemsets}; (2) for (k=2; Lk-l,; k++) do begin (3) Ck=apriori-gen(Lk-l); //new candidate  item set (4) for all transactions teD do begin (5) Ct=subset(Ck,t); I/candidate item  set that included in transaction T (6) for all candidates cE C, do (7) c.count++; (8) end  (9) Lk={ce CkIc.count.minsup} (10) end (1 1) Answer=ukLk;  The procedure to generate new candidate item set is described as follows:  apriori-gen(Lk-1) {//merging  Insert into Ck Select p.item1, p.item2 .., p.itemk-, q.itemk I From Lk-l p, Lk-l q Where p.iteml= q.

iteml,p.item2=q.item2......., p.itemk-2= q.itemk2, p.itemk_l<> q.itemk_I;  //prunning For element c cE Ck do  For all (k-l)-subsets s of c do if (s 0 Lkl) then  delete c from Ck, } In the Apriori algorithm, frequent 1-itemset L, is generated  first, then frequent 2-itemset L2. The algorithm won't stop until there exists a value r that makes Lr null. In the Kth iteration, the algorithm generates the set Ck of candidate K-item set first, in which each element is generated by processing a connection between two elements in the frequent item set Lk-l. The set Ck is the candidate item set used to generate a frequent item set Lk, so the frequent item set Lk must be a subset of Ck. The Lkl p and the Lkl q in the apriori-gen procedure are referred to element p and element q in the frequent item set Lk-l.

B. Granular Computing--Based Frequent Item Set Algorithm  First, the information table(i.e. transaction database) is described by information granules, and information granules is represented by binary numeral string(i.e. bit representation).

So, each element in Ck or Lk is an information granule.

Second, on the basis of the Apriori algorithm ,we propose the granules computing based algorithm for association rules extracting. The main steps of the algorithm are described as follows:  (1) Scanning the information table, and generating the frequent item set LI (the support of elements in LI satisfying min_sup )  (2) Generating candidate item set Ck (k>=2) by the operation "AND" between the bit representation in Lk-l .

(3) In Ck ,the number of chars 1 of a element is the support of the element, and the elements that their support satisfy the min_sup are put into the frequent item set Lk. .

(4) If Lk is null, then the algorithm stops; else continue (2).

(5) Returning all frequent item set Lk obtained.

There are two attributes in the information granule. One is  the block number, and another is the number of chars 1 in the bit representation. We use symbol "no" and" count" to stand for the two attributes respectively. The block number is the identifier . In Lk the information granules that have same attribute name or attribute combination are assigned same block number.

We still use iteration to generate all the frequent item sets.

The main idea of the algorithm is as follows:  (1) G1 = {gI,g2....... g.1gj.count>=minsup,l<=i<=m}; (2) for (k=2; Gk l.4; k++) do begin (3) c= {p and ql p E Gk l,q E Gkl ,p.no<>q.no}; (4) Gk= {cl c.count>=minsup }; (5) end (6) Answer=-UkGk;  In this algorithm, frequent 1-itemset G1 is generated first, then frequent 2-itemset G2. The algorithm won't stop until there exists a value r that makes L, null. Here in the Kth iteration, each element in Gk is generated by granular computing between the two information granules with different block numbers in GklI  C. Algorithm Analysis The Apriori algorithm has two main shortcomings: one is  that it may generate many redundant or meaningless candidate elements. Another is that it repeatedly scans the information table .

The Apriori algorithm scans the information table k times repeatedly. Suppose that the number of elements in Lk is nk, then the total number of comparisons in the step (3)  r-I  is C2 at most, where r is the value when Lr is null. From k=l  step (4) to step (8) in Ck, whether each element can be put into Lk, it must be checked ,and the checking causes scanning information table. Suppose that the number of elements in Ck is Mk, then from step (4) to step (8) the number of compares  r  with the records of the information table is U* E m k times k=2  at most .When JUI or mk is much bigger, the Agrawal algorithm may result in much load of I/O. In order to reduce the size of Ck. , Agrawal introduced a pruning technology.

Because of this, when generating all frequent item set, the performance of the algorithm can be improved effectively. In addition, suppose that the number of elements in Ck is Mk, then  r  the total number of the comparisons in step (9) is Mik.

k=2  Compared with the Apriori algorithm, the granular computing-based algorithm for association rules extracting is improved mainly in two aspects: One is that redundant or meaningless candidate elements can be reduced effectively.

Another is that frequent item set Gk can be generated without scanning information table.

First, in Gk ,elements ( i.e. information granules ) are organized by blocks . So when generating elements in Gk+l, within the blocks candidate elements that are redundant or meaningless are not generated. Secondly, by the operation "and" between elements with different block numbers in Gk ,elements in Gk+I are generated without scanning information table. Thus it is obvious that the granular computing-based algorithm for association rules extracting can conquer the bottleneck phenomena caused in the Apriori  algorithm.

Moreover, Generally the number of blocks in G1 is fewer  than the number of the item set I. So the number of loops can be reduced greatly in the granular computing-based algorithm for association rules extracting.

v. GRANULAR COMPUTING-BASED ALGORITHM APPLIED TO MINING WEB USER FAVORITE PATHS MODEL  A. Web Log The web log usually adopted the ECLM log model to record  a data[lO]. Data structure is shown in figure 1. Time/Data field represents the time when web server receives the user's request. URL field represents the URL location of web page that users visited. Referer field represents the URL of cited page. If a user input a the URL directly or uses bookmark to visit web pages, then the referer field is null. Agent field is the type ofOS and browser.

IF Address Anth ID Time/Data lethod/1JDIIProtocol Status Size Referer Agent  Fig. 1 Web Log Data Structure  B. An Example We simply take three attributes in web log data structure to  illustrate the thought of the granular computing-based algorithm for association rules extracting. In addition, the values of attributes are represented by using letters and digits. Table 1 is the web log table that has been pre-processed.

In table 1, U={ul,u2,u3,u4,u5,u6,u7,u8,u9,ul0} is a universe. Suppose that the min sup is s%=15%.

First, we generate the frequent item set G1 . The method is that each equivalence class is described by n bits , where n is the cardinal of U. In a bit representation, when the ith bit is 1, then the individual ui belongs to this information granule, if it is 0, then the individual ui doesn't belong to this information granule. The number of chars 1 in the bit representation is known as the size of the information granule, i.e. the size of the support. In this example, we use 10 bits to represent information granules. If the number of chars 1 in bit  TABLE 1 INFORMATION TABLE  U User address Cited page Visited page  ul 2 A VpC  u2 1 C VpA u3 2 B VpB u4 3 D VpB u5 2 C VpB  u6 3 C VpC u7 3 A VpC  u8 2 D VpB  u9 1 A VpC ulO 1 B VpA     representation satisfies the min sup, then this information granule is put into the frequent item set G1. Scanning the information table one time, we can generate the frequent item set G1 .In this example, the result is G1={[1],[2],[3],[A],[B],[C],[D],[VpA],[VpB],[VpC]} .As table 2 shows, the elements in G1 belong to three different blocks.

TABLE 2 FREQUENT ITEM SET G,  information granules formed according to partitioning user's address in G1 (block number =1)  Name of Information The bit Support information granule representation granule of information  granule [1] [u2,u9,ulO] 0100000011 3  [2] [ul,u3,u5,u8] 1010100100 4  [3] [u4,u6,u7] 0001011000 3  information granules formed according to partitioning cited page in G1 (block number =2)  Name of Information The bit Support information granule representation granule of information  granule [A] [ul,u7,u9] 1000001010 3  [B] [u3,uI0] 0010000001 2  [C] [u2,u5,u6] 0100110000 3  [D] [u4,u8] 0001000100 2  information granule formed according to partitioning visited page in G1 (block number=3)  Name of Information The bit Support information granule representation granule of information  granule [VpA] [u2,ulO] 0100000001 2  [VpB] [u3,u4,u5,u8] 0011100100 4  [VpC] [ul,u6,u7,u9] 1000011010 4  After generating G1, by operation "and" between two elements with different block numbers in G1 ,if the number of chars 1 in the computing result satisfies the condition (i.e.

larger than or equal to the min sup),then the result is put into the frequent item set G2.The processing continues until none of pair of elements with different block numbers is processed. At last G2 is generated through GI.The result is shown in table 3. Then G3 is generated according to G2. In this example, because G3 is null, G2 is returned at last. In addition, the maximal value ofk is 3 according to the number ofblocks.

Now, we can extract association rules from G2 and we can  also get this information: user2 has a habit of visiting page VpB from cited page D. This extracted association rule is the user favorite browsing paths that we hope to obtain.

When generating G1, the information table is scanned one  time. In order to generate G2, the total comparing number is 3*4+3*3+4*3=33 at most without scanning the information table.

If the Apriori algorithm is adopted, when generating Lk, the  TABLE 3 FREQUENT ITEM SET G2  Attribute Information The bit Support combination granules representation  of information granule  (block number =1 in G2)  [l]and[VpA] [u2,u10] 0100000001 2  [2]and[VpB] [u3,u5,u7] 0010101000 3  (block number =2 in G2)  [VpB]and[D] [u4,u8] 0001000100 2  [VpC]and[A] [ul,u7,u9] 1000001010 3  information table is scanned 3 times in this example. In step (3) it needs to compare C2 =45 times to generate C2 , and   C2 =6 times to generate C3. In step (9) it needs comparing 45 times to generate L2 from C2, and 6 times to generate L3 from C3. From step (4) to step (8) the number of compares with the records of the information table is 10*51=510 times at most.



VI. SUMMARY There are mainly two shortcomings in the classical Apriori  algorithm. In recent years, many researchers present many methods to improve the classical Apriori algorithm .This paper presents a granular computing-based algorithm for association rules extracting. Compared with the classical Apriori algorithm , the algorithm can efficiently reduce the number of candidate elements and avoid the bottleneck phenomena caused in the Apriori algorithm.

