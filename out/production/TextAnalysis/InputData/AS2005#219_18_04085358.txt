<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Paper  Identification Number: 1568965532

Abstract? A decision tree is an effective means of data classification from which rules can be both expressive and precise.

However decision tree is only applicable in the applications that the data is expressed with attribute-value pairs.  Since genetic data are not attribute-pairs, the only method that we know of to make decision-tree for them is based on a greedy graph-based data mining algorithm known as DT-GBI.  Due to its greedy nature, some of the important rules may be missed.  Even though recently some attempts to make the algorithm complete has been presented, the computational complexity of algorithm increased so much that is not appropriate for practical purposes.  In this paper we present an approach to make decision tree for DNA based data, which basically uses regular association-rule algorithms.  Thus it has computational complexity which is much more tractable. Our contribution in this paper is to convert DNA based problems to regular data mining methods and by this conversion we present a method that can be applied to all classes of classification based on association rules.

Index Terms?DNA, decision support system, Sequence estimation

I. INTRODUCTION Analysis of DNA has been the center of attention of several researchers.  A lot of different form of tools and methods has been deployed to gain more understanding about DNA sequences.  Even though decision trees [1, 2] and induction rules [3, 4] have been deployed for classification purposes, we should consider each node as a categorical value and decision nodes in the tree only consist of one nucleotide.  In the last  This work was supported in part by the Japanese Munbokagokusho Ph.D. fellowship Ashkan Sami. is with Graduate School of Engineering, Tohoku University; 6-6-11-805, Aoba, Aramaki, Aoba-ku, Sendai 980-8579; JAPAN and Department of Computer Science and Engineering, Shiraz University; Shiraz, Iran (corresponding author, phone:+81-90-9747-5938; fax: +81-22-795-4847, e-mail: ashkan.sami@most.tohoku.ac.jp).

Makoto Takahashi is with Graduate School of Engineering, Tohoku University; Sendai; JAPAN. email: makoto.takahashi@most.tohoku.ac.jp decade, there has been extensive growth of known genetic information.  Different methods have been employed to reach a better understanding of DNA sequences.  As stated in regular deployment of regular decision tree we cannot have structures for decision making nodes.

To find interrelationships between DNA sequences, DNA sequences have been treated as graph structures.  Since graph structure is represented by proper relations, knowledge discovery from graph structured data poses a general problem for mining from structured data. Some examples amenable to graph mining are finding typical web browsing patterns, identifying typical substructures of chemical compounds and discovering diagnostic rules from patient history records.

Majority of the methods widely used is designed for data that do not have structure and are represented by attribute-value pairs.

Decision trees are very expressive method of presenting relationship and at the same time classification rules.  Graph structures, by all means, are no exception.  However due to NP-completeness nature of graph isomorphism, the only  approach to make a decision tree based on patterns is based on B-GBI [5] called DT-GBI [6] (Based on best of our knowledge).

Even though, they have reached a good results as far as the performance is concerned they only applied their algorithm to DNA promoter data in UC-Irvine?s Machine Learning Repository [7] which only consists of 106 sequences of    Repository [7] which only consists of 106 sequences of 57-nucleotide sequences.

Due to greedy nature of B-GBI, it is quite possible to miss some of the more important patterns.  Recently there has been an attempt to make GBI more complete by introduction of CI-BGI [8].  Even though the authors have presented the results based on same promoter data and found all the patterns.  They not only confess some restrictions for patterns that CI-BGI finds, but also the exponential increase in computational time.

The rest of this paper is as follows, in section 2 we present a review of the graph-based data mining methods and classifier based on GBI.  Section 3 provides the definition and the algorithm.  Experimental results are given in section 4 and section 5 concludes the paper.

Decision Tree Construction for Genetic Applications Based on Association Rules Ashkan Sami, Student Member IEEE and Makoto Takahashi Graduate School of Engineering Tohoku University Sendai, Japan ashkan.sami@most.tohoku.ac.jp Paper Identification Number: 1568965532

II. REVIEW OF RELATED GRAPH-BASED DATA MINING Graph-Based Induction (GBI) is a technique which was devised for the purpose of discovering typical patterns in a general graph data by recursively chunking two adjoining nodes. There can be more than one link between any two nodes.

GBI is very efficient because of its greedy search.

GBI does not lose any information of graph structure after chunking, and it can use various evaluation functions in so far as they are based on frequency. It is not, however, suitable for graph structured data where many nodes share the same label because of its greedy recursive chunking without backtracking, but it is still effective in extracting patterns from such graph structured data where each node has a distinct label or where some typical structures exist even if some nodes share the same labels.

B-GBI is improved version of GBI in three aspects: 1) incorporating a beam search, 2) using a different evaluation function to extract patterns that are more discriminatory than those simply occurring frequently, and 3) adopting canonical labeling to enumerate identical patterns accurately.

GBI employs the idea of extracting typical patterns by stepwise pair expansion as shown in Fig. 1. ?Typicality? is characterized by the pattern?s frequency or the value of some evaluation function which is calculated by the pattern?s frequency. In Fig. 1 the shaded pattern consisting of nodes 1, 2, and 3 is thought typical because it occurs three times in the graph. GBI first finds the 1?3 pairs based on its frequency, chunks them into a new node 10, then in the next iteration finds the 2?10 pairs, chunks them into a new node 11. The resulting node represents the shaded pattern.

It is possible to extract typical patterns of various sizes by repeating the stepwise pair expansion (pairwise chunking).

Note that the search is greedy. No backtracking is made. This means that in enumerating pairs any pattern which has once been chunked into one node is never restored to the original pattern.  Because of this, all the ?typical patterns? that exist in the input graph are not necessarily extracted.

Since the representation of decision tree is easy to understand, it is often used as the representation of classifier for data which are expressed as attribute-value pairs. On the other hand, graph-structure data are usually expressed as nodes and links, and there is no obvious component which corresponds to attributes and their values. Thus, it is difficult to construct a decision tree for graph structured data in a straight forward manner. To cope with this issue B-GBI regards the existence of a subgraph in a graph as an attribute so that graph-structured data can be represented with attribute-value pairs according to the existence of particular subgraphs.

the existence of particular subgraphs.

However, it is difficult to extract subgraphs which are effective for classification task beforehand. If pairs are extended in a step-wise fashion by GBI and discriminative ones are selected and extended, discriminative patterns (subgraphs) can  be constructed simultaneously during the construction of a decision tree.

When constructing a decision tree, all the pairs in data are enumerated and one pair is selected. The data (graphs) are divided into two groups, namely, the one with the pair and the other without the pair. The selected pair is then chunked in the former graphs. and these graphs are rewritten by replacing all the occurrence of the selected pair with a new node. This process is recursively applied at each node of a decision tree and a decision tree is constructed while attributes (pairs) for classification task is created on the fly.



III. APRIORI DECISION TREE We present a new method that is complete and can find all the rules containing all the association rules.  The algorithm we propose has several steps.

1)  Mapping of the nucleotides to values that regular Basket analysis can be performed on them.

2)  We use algorithm similar to CMAR [11] to find rules.

3)  We use evaluation measures like information gain [1], Gain Ration [2] GINI index [4] to find outmost prominent rule for separation.

4) The data set is divided into two sets by the prominent rule into two sets.  Thus the procedure repeats itself by going back to stage 2, until there is no more data.

5) Pre-pruning is performed to make sure there is no overfitting.

A. Mapping the Nucleotides We concatenate type of each nucleotide with its position with respect to a specific place in DNA sequence to a new categorical value.  As an example an Adenine which is located at position -30 with respect to transcriptionalstartsite is mapped to ?A-30?.

By this mapping regular Basket Analysis can be performed.

An immediate concern about usage of this method would be that in regular DNA sequences no specific ordering exist and most DNA sequences are not aligned with respect to a specific position.  Although this might be a challenge to applying the method to a DNA sequence that has not been aligned, several tools can do DNA alignment. Avid, BlastZ, Chaos, ClustalW, DiAlign, Lagan, Needle, and WABA are the most famous alignment tools available.

Figure 1. The basic idea of GBI method Paper Identification Number: 1568965532  Thus by applying the alignment tools that are presented in the previous paragraph, the sequences are ready to apply the algorithm as stated.  Since in this paper we used aligned DNA sequences as the input, we only consider the alignment (which is a challenge by itself) as a pre-processing to the proposed  method.

B. Review of CMAR Suppose a data object obj = (a1, ?, an) follows the schema (A1, ?, An) ,with A1, ?, An attributes. Attributes here are categorical. Thus, we assume that all the possible values are mapped to a set of consecutive positive integers.  By doing so, all the attributes are treated uniformly.

Let class labels be C = {c1, ?, cm}. A training data set is a set of data objects such that, for each object obj, there exists a class label cobj ?  C associated with it. A classifier ( is a function from (A1, ?, An) to C Given a data object obj, C(obj)returns a class label.

CMAR consists of two phases: rule generation and classification.  In the first phase, rule generation, CMAR computes the complete set of rules in the form of R : P ? c, where P is a pattern in the training data set, and c  is a class label    where P is a pattern in the training data set, and c  is a class label such that sup(R) and conf(R) pass the given support and confidence thresholds, respectively. Furthermore, CMAR prunes some rules and only selects a subset of high quality rules for classification.  The second phase which is classification for a given data object obj, CMAR extracts a subset of rules matching the object and predicts the class label of the object by analyzing this subset of rules.

To make mining highly scalable and efficient, CMAR adopts a variant of FP-growth method [12] which is especially good in the situations where there exist large data sets, low support threshold, and/or long patterns.  CMAR scans the training data set T once to find the set of attribute values happening more than predefined support.  In other words, the frequent item sets are identified.  Then, it sorts attribute values in support descending order and scans the training data set again to construct an FP-tree.  The class label is attached to the last node in the path.  Based on F-list, the set of class-association rules can be divided into different groups.  CMAR finds these subsets one by one.  To find the subset of rules having a specific item, CMAR traverses nodes having the attribute value and look upward to collect the projected database. We can mine the projected database recursively by constructing FP-trees and projected databases. After search for rules having the specified item, all nodes of the item are merged into their parent nodes, respectively. That is, the class label information registered in the specified node is registered in its parent node.  In other words, CMAR finds the frequent items and generate rules in one step.

CMAR was chosen because conventional association rules must be mined in a two step fashion.  First, all the frequent patterns (i.e., patterns passing support threshold) are found.

Then, all the association rules satisfying the confidence threshold are generated based on the mined frequent patterns.

The difference of CMAR from other associative classification methods is that for every pattern, CMAR maintains the distribution of various class labels among data objects matching the pattern. This is done without any overhead in the procedure of counting (conditional) databases.  Thus, once a frequent pattern (i.e., pattern passing support threshold) is found, rules about the pattern can be generated immediately. Therefore, CMAR has no separated rule generation step.

On the other hand, CMAR uses class label distribution to prune.  For any frequent pattern P , let c be the most dominant class in the set of data objects matching P.  If the number of objects having class label c and matching P is less than the support threshold, there is no need to search any superpattern (superset) P? of P of , since any rule in the form of P ? c cannot satisfy the support threshold either.

After this stage CR-tree is built for the set of rules that were  obtained.  CR-tree is a compact structure. It explores potential sharing among rules and thus can save a lot of space on storing rules. Experimental results show that, in many cases about 50-60% of space can be saved using CR-tree.  CR-tree itself is an index for rules. By this method rule retrieval becomes efficient. That facilitates the pruning of rules and using rules for classification dramatically.  For rule set shown in Fig 2 the CR-tree is presented in Fig. 3.

Since the number of rules that are generated might be huge, CMAR deploys a pruning method.  Basically the pruning method is base on the fact that rule provides a better accuracy, more coverage and at the same time provide less attributes.

C. Decision Tree Construction We can use several ranking methods for the rules like information gain [1], Gain Ration [2] and GINI index [4] to find outmost prominent rule for separation.  Thus in this stage we can evaluate all the rules based on one of the above mentioned methods.  The idea behind this stage is to find the rule that has the most important contribution in dividing the data set.  In contrast to CART and C4.5 decision tree construction methods, this stage of the process is not completely greedy.  In other    this stage of the process is not completely greedy.  In other words the algorithm does go back after the decision is made.

However, only for combination of the rules to produce more expressible results.

Based on the node that has the most prominent effect on dividing the dataset, we divide the data set based on the data that matches the rule and the other set that does not match the rule.

Since we were using pre-pruning, if we cannot find a node that can split the data with higher values than the specified value, we should only grow the tree on the other side.

Figure 2. An illustrative set of rules  Paper Identification Number: 1568965532   Figure 3:  An illustrative CR-tree of example in Fig. 2  It is a well-known fact that post-pruning will produce a more accurate result.  However since in this paper we only wanted to propose the feasibility of the method, we felt it is sufficient to use pre-pruning.

The modified CMAR algorithm that we used does not evaluate the strength of the rules based on statistical measures like weighted Chi-square test that is performed by CMAR.  In contrast, all the rules are generated without any pre-assumption.

The class labels are kept due to the fact that ranking methods require detail knowledge of class distribution that will occur due to division by that rule.

As an illustration, to calculate information gain, we must know exactly by using antecedents of the rule as a dividing factor each dataset (The sequences that posses all the itemsets and the one that does not contain the whole itemset), what the class distributions are.  In contrast confidence as defined by association rule literature does not have any significant impact on the importance or candidacy of the rule for becoming a node in the decision tree.

A performance adjustment was performed to increase the speed by deleting the itemsets that belonged to the antecedent of the rule in the dataset that obeyed the rule.  In other words, the set of data that has all the itemsets in the rule creates too many rules that one or more items of those rules are from the antecedents of the node prior to it.  Definitely these rules are not much interesting as far as ranking factors are concerned since division was based on all the sequences that have those items in them.  Moreover, since they exist in all the sequences, they do not posses much of ranking capabilities at later stages of the algorithm.  However if we do not delete them a lot of rules created will contain them.  Empirical results showed us that this performance adjustment reduced the size of the rules by 1/6 of the size without the performance adjustment. Thus it speeds up our calculations by six folds in the positive instances of the rules.

Another well known fact is that recursive partitioning of data until each subset in the partition contains data of a single class results in overfitting to the training data and thus degrades the predictive accuracy of decision trees. To improve the predictive accuracy, a pre-pruning is implemented by stop growing an overfitted tree.



IV. EXPERIMENTAL RESULTS We applied the proposed algorithm to splice-junction DNA sequence of UCI Machine Repository [7].  Splice junctions are points on a DNA sequence at which ?superfluous? DNA is removed during the process of protein creation in higher organisms.  The problem posed in this dataset is to recognize, given a sequence of DNA, the boundaries between exons (the parts of the DNA sequence retained after splicing) and introns (the parts of the DNA sequence that are spliced out). This problem consists of two subtasks: recognizing exon/intron boundaries (referred to as EI sites), and recognizing intron/exon boundaries (IE sites). (In the biological community, IE borders are referred to a ?acceptors? while EI borders are referred to as    are referred to a ?acceptors? while EI borders are referred to as ?donors?.) Since the DNA sequences in addition to A, C, G and T, had other types of nucleotides.  In the first stage we omitted all the sequences that had other nucleotides.  Even though only 15 sequences had this property we omitted them so the algorithm runs smoothly.

Secondly, since all the nucleotides were aligned already, we considered the starting nucleotide as the first base and applied the mapping to the algorithm based on that position.

The modified CMAR algorithm that we used did not use statistical tests for evaluating ?interestingness? of the rules.  In addition, we did not prone any rule after rule generation.  We obtained all the rules with their respective class frequencies.

That is, for each rule the number of sequences that would belong to each class if the rule becomes the dividing factor was found.  The calculated class distributions of the negative instances of the rule can be easily calculated by knowing the distribution of classes of the dataset.  This information was used to calculate information gain.  In other words, nonetheless we evaluate confidence, the class distribution of rule coverage was only used for entropy calculations.

The experimental results that are presented here are obtained based on Information Gain.  After construction of the rules Information Gain was used to evaluate the node for splitting.

By using the modified CMAR, we found the base rules.  As stated before, we map the nucleotides and for each class we assign a unique integer.  The constructed decision tree considering pre-pruning is illustrated in Fig. 4 of this paper.

The merging process of nodes only happens if the following node of positive instances has another rule that points to the class of the previous rule and the new node does not produce above a threshold value.  The only instance that rules were combined only happened at the root of the decision tree.  We noticed that rule ?Base 31 = G Base 32 = T =&gt; EI? produced the highest information gain.  However after division the dataset complying with the rule had ?Base 35 =&gt; EI? which did not produce a very high information gain.  Thus the algorithm combined the rules and only grow on the other side.



V. SUMMARY In this paper, first we reviewed the only paper we know about constructing a decision tree for DNA sequence data.  Then based on a modified CMAR, we presented rule extractions that can be deployed.  By measuring the information gain we can rank the rules.  The rule that has most significant increase in Paper Identification Number: 1568965532  information gain is chosen as a node and the dataset is divided based on the antecedents of the rule.  The process repeats itself until the information gain by division is less than desired value of pre-pruning.  Since based on the simple mapping that we present the computational complexity of the finding sub-structures decrease very rapidly the process is feasible and accuracy, as can be seen, is very high.  Since decision tree is very expressive and at the same time very accurate for classification, we believe this method will be of high usage for bioinformatics scientists and researchers.  Moreover the method can be applied to other problems of data mining and the overall method is a general method.

