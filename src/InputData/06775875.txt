Using Advanced ML For Improving Surveillance Accuracy

Abstract?An increase in research over the past 60 years in the field of machine learning widened its areas of application from merely making computers learn to play board games to analysis of big data. Many algorithms have been developed that are now commonly used in various fields ranging from natural language processing to computational finance and has been brought to use commercially as well. Recently, there has been an increase in research on machine learning application in the area of automated video surveillance systems. Most of these algorithms assume that both the training data and test data belong to same feature space with same distribution which might not always be true. This constraint gave rise to the concept of transfer learning which uses the knowledge from the preoccupied knowledge from other related task. This paper aims at improving the efficiency of a transfer learning based machine learning technique for object classification, MKTL framework. It can be brought to use for multiclass object classification in automated video surveillance systems.

Keywords?Support Vector Machines, transfer learning, supervised learning.

?  1 INTRODUCTION  With the increase in crime levels over the last10 years at many places, public fear has grown for safety. Security is one of the foremost necessities of a human being. The installation of CCTV cameras as a control measure has proved to be effective to some extent. A huge amount of data is produced in any video surveillance system. In the traditional surveillance systems, the recordings are monitored manually. But, this is a very strenuous task inviting a high risk of human errs.

In recent years, there has been an increase in research in the area of automation of surveillance systems. Machine learning approaches have also come into picture in addition to various image and pixel matching algorithms. E.g. agglomerative clus- tering, reciprocal nearest neighbor pair, K-nearest neighbor, K means clustering and pixel and block based similarity check.

Machine vision focuses on how a machine can be made more intelligent. Using the present ma- chine learning techniques for object detection and classification, the surveillance videos can be ana- lyzed for anomalies with minimal human assistance  and only the events that are of interest can be brought to notice up to a certain limit in efficiency.

These techniques aims at developing system that can auto detect any suspicious condition based on some prior knowledge set. Based on the availabil- ity of prior knowledge, the machine learning task can be supervised learning, unsupervised learning, semi-supervised learning or transductive learning.

For surveillance systems, supervised learning can be brought to use as set of labeled inputs in the training set can be made available. An automated video surveillance system would typically consist of three major steps: background removal, object detection, classification. Background removal in a video sequence can be done by extracting steady state component using techniques like frame differ- ence, average filtering, using kalman filters, hidden Markov model. A lot of literature in image pro- cessing provides methods for object detection. In this paper, we focus on the object classification part of the system. Here, we try to analyze an object classification method that can be brought to use in a video surveillance system for computerizing the monitoring of the videos and alerting whenever a      Fig. 1. A system for video surveillance  Fig. 2. Object detection in surveillance video  potential threat is detected. The method that we are considering to be used for classification of scene as a potential threat is the Multi Kernel Transfer Learn- ing as proposed in paper [1] with an improvement by using polynomial radial basis function kernels instead of the traditional radial basis function kernel as suggested in [6] into the framework.

2 OBJECT CLASSIFICATION USING MA- CHINE LEARNING Object classification is the task of detecting and classifying any given image of an object into one of the sets of known classes. Transfer learning  based object classification algorithm has three major aspects:  ? Feature representation ? Classification using a machine learning ap-  proach ? Transfer learning  2.1 Feature representation Images consist of pixels which are basically num- bers that represent a color. A feature is a numerical representation of an image. A good representative feature should have the following properties: Re- peatability, distinctiveness/imformativeness, locality, quantity, accuracy, efficiency.

There are a lot of different features used for representing images. Most common of them are color histogram, SIFT [11], PCA-SIFT [12], SURF [8], PHOG [15], LESH [16], GLOH [17]. For our  Feature Merits Demerits Color Histogram Easy to evaluate Considers color, ignores  shape and texture of ob- ject  SIFT Exhibits scale, rotation invariance and invari- ance to affine transfor- mation  Slow and not good at illumination changes  SURF Very fast as compared to SIFT  Not stable to rotation and illumination changes  PCA-SIFT Size of the feature is small  Not good at blurred im- ages  PHOG Good representation of local shapes, easy to evaluate  Ignores flat area and cannot deal with noisy edge region  TABLE 1 Different Feature descriptors  current problem of object classification SURF and PHOG features are used as input to the classification algorithm as in [1].

2.2 Classification Algorithms A machine learning algorithm generally has a train- ing set and a test set of data. The data in case of images consist of the feature vectors. Using the training data set, models are derived that are used to classify a test data. Different available super- vised learning algorithms include linear classifiers (SVMs), quadratic classifiers, k means clustering, boosting, decision tree, neural networks, Bayesian networks. We here briefly describe Nave Bayes classifier, SVMs and Multi-Kernel learning.

Naive Bayes Classifier The Naive Bayes assumption [14] is that all features are conditionally independent given the class label.

Given a set of variables, X = x1, x2, x..., xk, the posterior probability for the event Ei that belongs to the set of outcomes E = e1, e2, e3, ..., ek can be obtained using Bayes? rule:  p(Ei|x1, x2, ..., xk) ? p(x1, x2, ..., xk|Ei)p(Ei) (1) where p(Ei|x1, x2, ..., xk) is the probability that X belongs to Ei.

Due to the assumption of statistical independence, the probability p(X|Ei) can be written as:  p(X|Ei) ? k?  j=1  p(xj, Ei) (2)  and hence the posterior probability as:  p(Ei|X) ? p(Ei) k?  j=1  p(xj, Ei) (3)  Using Bayes? rule, a new test data X is labeled to the class that achieves the highest posterior probability. Naive Bayes classfier works well when normal distribution is involved. But it is not capable of solving more complex classification problems.

Another classifier that is complex and slow but capable of solving complex classification problem is the support vector machines.

Support Vector Machine Classifier SVM [9] is one of the most commonly used binary linear classifier. A linear classifier assumes that the data is linearly separable. In SVMs, the input data is seen as two sets of vectors in an n-dimensional space. These sets are then separated by a n ? 1 dimensional hyperplane in the space. Many a times more than one such hyperplane exist. So, the one which maximizes the margin between the two sets is chosen. Given a real vector ?x as input to the classifier, the output score is given by  y = f (?w ? ?x) = f (?  j  wjxj  ) (4)  where ?w is a real vector of weights that is learned using training samples and f is a function that maps the input ?x to a class based on the value of weighted sum of the features.

Binary classification using SVM [9]: Let there be two classes: Class 1:X = X1, X2, ...Xn Class ?1: Y = Y1, Y2, ...Ym Data points with known labels constitute the training set A ?Classifier? is trained on training set and then tested on unknown(test) set Z = Z1, Z2, ...Zk The decision function is given by  F (x) =< w, x > +b (5)  where w and b are obtained using training set and F (x) is calculated for test set. Based on the value of F (x), samples are classified using sign(F (x)).

The optimization problem consists of a loss and a regularization problem:  min w  CL(?w; y) +R(w) (6)  i.e.

min w,b  1/2||w||22+C ? i  max(0, 1?yi(< w, xi > +b)) (7)  Fig. 3. Mapping into a higher dimension space for linear SVM classifier  If the input data is not linearly separable then it can be mapped to a higher dimensional space so that it becomes linearly separable as in figure 3.

The function that maps the input data to a higher dimensional space is often referred to as kernel function.

k : X ?X ? R (8) Common kernel functions include radial basis func- tion (rbf), polynomial kernel function(poly), sigmoid     function.

RBF:  k(x, y) = exp  (? x? y ?22 2?2  ) (9)  Polynomial kernel:  k(x, y) = (1 + xTy)n (10)  Multiclass classification using SVM: The binary classifier SVM can be used for multiclass object classification by using one-vs-all approach.

In this approach, N different binary classifiers are trained to distinguish one class from the rest. The choice of kernel function k(x, y) is a key parameter in SVM. In [6], it has been proposed that instead of using a single kernel, a combination (prbf kernel) of kernels (rbf and poly) increases the performance of the SVM classifier. The prbf kernel as given in [6] is given by  prbf =  ( 1 + exp  (??(x1i ? x2i) (p ?N)  ))n (11)  where N is the dimension of the input vector and p is a kernel parameter.

Multi-Kernel Learning SVM employs a kernel function k(xi, xj) that intu- itively computes similarity between samples xi and xj . For different kinds of data sets available, one kernel is better than the other based on the input dataset. MKL focuses on how the kernel can be learnt as a linear combination of the base kernels.

Linear MKL:  K(xi, xj) = ? k  dkKk(xi, xj) (12)  The MKL algorithm solves a joint optimization problem while also learning the weights used for combining the different base kernels. This algorithm was first proposed in [2]. The original MKL used l1 norm regularization which induces sparsity in the kernel domain. Recent approaches have been using lp norm regularization. The lp norm optimization is given by:  min ?w   ? ?w ?22,p +C  N? i=1  l(?w, xi, yi) (13)  where ?w = [w1, w2, ..., wK ] and K is the number of base kernels that are combined. The optimization problem is solved using OBSCURE algorithm as given in [10].

2.3 Transfer learning The traditional machine learning[13] approaches as- sumes that the training set and test samples belong to same feature space and distribution which makes the task difficult in case only few training samples available. Transfer learning [7] is the ability of a system to use the knowledge that it acquired on learning one task in learning a new task.

Fig. 4. Traditional machine learning vs transfer learning  Transfer learning [7] can be seen as having four categorize based on what is being transferred: in- stance transfer, feature representations transfer, pa- rameter transfer, relational transfer.

The instance transfer approaches assumes that the source domain and the target domain data use the same features and labels and certain parts of the data in the source domain can be reused for learning in the target domain by reweighing. TrAdaBoost     algorithm [18] which is an extension of AdaBoost algorithm is used for reweighing.

The feature representation transfer approach aims at finding good representation of the target domain.

The knowledge that is used to transfer is embedded in the feature representation that has been learnt.

The parameter transfer approach assumes that the already known model of the classes and the new model to be learnt have some common parameters, so these parameters are transferred for modeling the new class.

The relational transfer approach assumes that the data is not independent and identically distributed and more than one relation can be used to represent them. So, it transfers the relation that is similar to both previously known class models and the new class data.

3 MULTI KERNEL TRANSFER LEARN- ING  The MKTL framework helps in learning a classifier for a new class given only a few samples by making use of the already learnt classifiers for some different set of classes. In [1], it is assumed that the feature representation of the source and the target domain could be different and the modeling method could also be different. Here, we briefly describe the MKTL as proposed in [1].

The MKTL model is meant for learning the clas- sifiers for F ? new classes given that the models for F classes apart from these F ? classes are known. The F classes are trained using some classifier which is a function f : X ? Z to predict the class to which a new test data belongs, where X is the input feature space and Z = 1, 2, ...,m given m classes.

The function f is given by  f(x) = argmaxz?Z sp(x, z)  where x is the input feature vector and sp(x, z) is the score function that denotes how confidently the input x can be assigned to class z.

If xi, yi is the training set for the F ? classes where xi?s are the feature vectors and yi?s the correspond- ing labels, then the prior knowledge is obtained as the score function as predicted by the models of the F known classes i.e. sp(xi, z) , z = 1, 2, ..., F . So, for the new class the score function is given by:  s(x, y) = w? ? ??(x, y) (14)  Fig. 5. Score of a new class sample using prior knowledge models  s(x, y) = w?(0).?(0)(x, y)+ z=F? z=1  w(y,z)??(y,z)(sp(x, z), y) (15)  where w(?) is a hyperplane and ?(?)(?, ?) : X?Y ? H maps the input data into a higher dimensional space.

w(0) in the above equation represents the model parameters corresponding to ?(0)(x, y) which is the feature mapping function for input features x. Also, w(y,z) represents the weightage assigned to the z-th prior model while predicting the input x as belong- ing to new class y where y = 1, 2, 3, ..., F ?. The optimization problem [1] can be posed as finding the vector w? that minimizes the following:  min ?w  ?(w?) + C N? i=1  l(w?,xi, yi) (16)  where ?(w? is the regularizer to avoid overfitting and can be given by  ?(w?) =  ? w? ?22,p (17)  where w? = [w(0),w(1,1), ...,w(y,z), ...,w(F ?,F )] l is the loss function which is a convex and non- negative function and for the multiclass classifica-     tion the following can be used as the loss function  l(w?,x, y) = max y? ?=y  (1? w? ? (??(x, y)? ??(x, y?)), 0) (18)  where ??(x, y) = [?(0), ?(1,1)(sp(x, 1), y), ...,  ?(y,z)(sp(x, z), y), ..., ? (F ?,F )(sp(x, F ), y)]  (19)  The optimization problem is solved using the OBSCURE[10] algorithm which is a fast subgra- dient descent algorithm.

The Animals with attributes dataset[4] has been used in [1] and a comparison is done between three models: one when there is no transfer of knowledge which corresponds to a simple machine learning method, the second one when only the prior knowledge is used for training the new class and the third the Multikernel Learning with prior knowledge. It has also been emphasized that unlike other approaches the method is not limited to a single type of feature representation and classifier for both source and target domain.

It has been observed that the Multikernel Transfer learning performs better than the other two ap- proaches. The prior knowledge model is obtained using rbf kernel.

Fig. 6. Prior Knowledge model  4 IMPLEMENTATION In this work, we have analyzed the performance of the MKTL framework [1]. We focused on obtaining a better prior knowledge matrix. We also compared the result when a different classifier is used to obtain  the prior knowledge model instead of SVM as used in [1]. We reproduced the results as in [1] with Animals with attributes dataset [4] choosing the same 40 training classes and 10 test classes as in [1] in order to see the exact comparison.

Training classes: antelope, bat, beaver, blue whale, bobcat, buffalo, chihuahua, collie, cow, dalmatian, deer, dolphin, elephant, fox, german shepherd, gi- raffe, gorilla, grizzly bear, hamster, horse, killer whale, lion, mole, moose, mouse, otter, ox, polar bear, rabbit, rhinoceros, sheep, siamese cat, skunk, spider monkey, squirrel, tiger, walrus, weasel, wolf and zebra.

Test classes: chimpanzee, giant panda, hipppopota- mus, humpback whale, leopard, persian cat, pig, raccoon, rat and seal.

We explored the different types of kernels that can be used to map the data into a higher dimensional space. The original implementation uses the rbf kernel which is a widely used kernel for SVM classifiers. The corresponding results of using poly, rbf and prbf kernel are as shown in the simulation results section.

Step 1: We used the precomputed features from the Animals with Attributes dataset [4]. Prior knowl- edge model is obtained as given in figure 6. SURF and color histogram features of the training class images are used as in [1]. The color histogram [4] for each image is obtained by taking a 3-level spatial histogram of the image (1x1, 2x2, 4x4) and then concatenating the color histograms for each of the 21 cells making it 128x21 dimensional vector.

The SURF feature [8] vectors are obtained for each image and then using k-means clustering are converted into 2000 bag of words histogram. The prior knowledge model consist of the score function sp(x, z) for each image in the test set.

Step 2: The comparison is done between the traditional machine learning approach, the prior- feature approach where only the prior knowledge is used, and the MKTL framework as in [1]. The models are tested based on the number of images available in the new class for training.

Step 3: We also attempted to use parametric approach of knowledge transfer by using Naive Bayes classifier for obtaining the prior knowledge.

But as the Naive Bayes classifier is constrained by the fact that the training and test data should belong to same feature space we used PHOG features for both training and test set.

Fig. 7. Output of the No Transfer, Prior-feature and MKTL model using rbf kernel  Fig. 8. Output of the No Transfer, Prior-feature and MKTL model using poly kernel  5 SIMULATION RESULTS  The implementation for the prior knowledge model is as given in figure 6. The figures 7, 8 and 9 shows the output of the MKTL model along with no transfer and prior-feature model using rbf, polynomial and prbf kernel, repectively. It can be seen that the model obtained using prbf kernel outperforms the model that uses either of the rbf and polynomial kernel by having a recognition accuracy of 60% as compared to 45% when 40 samples are available for training and an accuracy of 40% for as low as 5 samples in the training set.

Figures 10, 11 and 12 illustrate the result when Naive Bayes classifier is used for training with PHOG features and rbf, polynomial and prbf kernel respectively.By analyzing these figures it is intuitive that on an average the model performs better when prbf kernels are used.

Fig. 9. Output of the No Transfer, Prior-feature and MKTL model using prbf kernel  Fig. 10. Output of the No Transfer, Prior-feature and MKTL model using rbf kernel and Naive- Bayes classifier  Fig. 11. Output of the No Transfer, Prior-feature and MKTL model using poly kernel and Naive- Bayes classifier     Fig. 12. Output of the No Transfer, Prior-feature and MKTL model using prbf kernel and Naive- Bayes classifier  6 APPLICATIONS With the increase in recognition accuracy of the MKTL framework it can be brought to use for object classification in an automated video surveil- lance system. Due to the unconstrained prior model and transfer learning approach requiring very few labeled samples for training, this approach will be a step forward from the traditional machine learning approach when employed in a surveillance system.

7 FUTURE WORK In future a more robust probabilistic model for the prior knowledge representation will lead to further improvement in the accuracy of the MKTL frame- work. The MKTL framework can be tested with more datasets to generalize it for different types of data used for testing and training.

8 CONCLUSION We compared the performance of the MKTL model using SVM and Naive Bayes classifier which showed a wide gap between the two models per- formance with the former outperforming the later.

The reason is the Bayes assumption of the features being independent. We also attempted at modifying the Multi Kernel Transfer Learning framework by using prbf kernel instead of rbf kernel which turned out to be better representation of image features for obtaining the prior knowledge. The proposed change has improved the recognition accuracy of the system by about 15%. This has been possible by making use  of the fact that the better the kernel function can map an input data into a higher dimensional space, the better is the efficiency of the SVM classifier.

