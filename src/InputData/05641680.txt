Behavior understanding at railway station

Abstract- The protection of critical transportation assets and  infrastructure is an important topic in these days. In this paper,  we develop a new rule based approach to smart video  surveillance system for detecting situations where people may be  in peril, as well as suspicious action or interactions at or near  critical transportation assets. We analyze here three general  types of human involved behaviors and interactions: (i) single  pedestrian or no interaction, (ii) multiple pedestrian interactions,  and (iii) pedestrian-facility/location interactions. The behavior  analysis is accomplished through the development of geometric  and motion visual features for each pedestrian. It is very simple  and highly effective. The performance evaluation is carried out  by using the video sequences taken in the real life environments  of rail stations.

Keywords-behavior unserstanding, pedestrian, railroad  station, framework,.



I. INTRODUCTION  Security is one of the most serious issues due to increasing  fears of terrorism and crimes. Railroad station is critical site  where the role of the transportation hub is played. From the  view point, security of the railroad station is especially  significant, and this topic attracts great attention.

It is difficult to prevent the abnormal events beforehand.

However, it is possible to detect them at the early stage after  they have occurred. So, our research aims to detect the  abnormal event in the railroad station.

To detect the abnormal events, the system understanding  human behavior is required. Behavior understanding is to  analyze and recognize human motion patterns, and to produce  high-level description of actions and interactions. Recently,  human action recognition has been a widely studied topic [ 1]  [9]. In this aspect, spatio-temporal concepts based on Motion  Energy-Image and Motion-History-Image templates [2] and  models of dynamics, such as Hidden Markov Models [3],  conditional random fields [4] were widely used. Mostly, these  approaches rely on modeling the details of the action dynamics    and need lots of training data to build effective models.

Our study aims at the construction of the human behavior  understanding framework. The proposed framework is an easy  composition in which multiple features are combined. In this  paper, we introduce a new rule-based approach to human  behavior to categorize detected human activity into various  classes. First, we present the general types of human behavior  978-142446588-0/101$25.00 2010 IEEE  Kaichi Fujimura, Shunsuke Kamijo  The University of Tokyo  Tokyo, Japan  fujimura@kmj.iis.u-tokyo.ac.jp  and the system overview. Then, the experimental results carried  out over the real image sequences are reported. Finally,  concluding remarks are described.



II. AN OVERVIEW OF THE FRAMEWORK FOR  HUMAN BEHA VIOR UNDERSTANDING  A. Concept  Events that should be detected in railroad station are  classified into three general types of human behavior as  follows.

1) Single pedestrian or no Interaction: illegal gating  (crawling, over fencing), loitering, etc.

2) Multiple objects (pedestrians) interactions: object  left behind, illegal gating (following), exchanging  objects, fighting, pick-pocketing, etc.

As for the condition of camera setting, view angle of  cameras is low, because the height of camera is restricted in the  railroad station. This causes the occlusion problem due to the  crowded condition. The system must be robust for occlusion of  objects.

B. An overview of behavior understandingframework  The proposed pedestrian behavior understanding model is  an algorithmic multi-layered framework composed of three  features namely: pose feature, positioning feature (trajectory  and location), and multiple objects interaction feature, as  shown in Fig.l. Also, the objects are classified into two  Multiple objects interaction  Single object  Obja  t .

Ped Standing x Bit Y.l (Floor) c l-f--  Ped Standing .t ." y., (Floor) b I-  Ped CraWTing x aJ, Y .. J (Fence)  Obj b  .

Obj Xbh Yb1 (Floor)  Obj XbhYbl (Floor) a r-  Obj Xbh Yb1 (Floor)  Objc  . .

Ped I Standing I -'""ltYc1 (Floor) I a  Ped I Standing l .t", y" (Floor) I  Ped I Lying I XcJ.Yc? (Floor) I  Fig. I. Framework layer of behavior understanding   Object  At! : Attribution of the object  II Time sequence data of the object II  Data [T] L Pose [T] [ X [T], Y [T], W [T], H [T]  Location  STI [a] : State transition information  STI [a] State transition information  t Locationjnfo. -r Num  L Att  State_info. Ib1  Pose  Duration  State  Fig. 2. Features of the behavior understanding framework  attributes according to whether the object is a pedestrian or not  ("ped" or "obj").

Since we have usable technologies such as background  modeling and object tracking, called ST-MRF model [ 10]. This  model can identify moving blobs in the scene and provide  information regarding the position of the target over time.

Moreover, this model can overcome the occlusion problem  occurred by the crowded condition in the railroad station. Thus,  we will tackle pedestrian's behavior understanding problems  focusing on three features in the frame work mentioned above.

In the next section, descriptions of the three features are  represented.



III. FEATURES OF THE FRAMEWORK  This section describes individual features required for the    construction of framework mentioned in previous section. The  architecture of the features are illustrated in figure 2. First, time  sequence data (Data[t] in the figure 2) including pose,  trajectory (X, V), size (H, W) and location information are  calculated from the ROI (region of interest) information  obtained from tracking results of the ST-MRF model.

Secondly, both attribute (Att) and state transition information  (STI) are created from the Data[t]. The STI includes three  features (pose, location, multiple objects interaction), and the  Att and the three features construct our framework. Finally,  behavior of the object is decided by the framework. The  construction of the framework is described along this flow as  follows.

A. Tracking by ST-MRF model  The principal idea of the S-T MRF model is that  segmentation of the object region in the spatio-temporal image  is equivalent to tracking the object against occlusions (see  Figure. I). Usually, the spatial MRF segments an image pixel  by pixel. However, since the usual video cameras do not have  such high frame rates, objects typically move ten or twenty  pixels among consecutive image frames. Therefore,  neighboring pixels within a cubic clique will never correlate in  (a) Image  1(wl) 12(11) I  3(91) L  4(11) 5(11) 6(11)  \ J  7(11) 8(11) 9(11) 10(11)  11(11) 12(11) 13(11) 14(11)  wi ... Wall fl ... Floor gt  Gate  (b) Location map  Fig. 3. Tracking result and location map in the gate scene.

terms of intensities or labeling. Consequently, we defined our  Spatio-Temporal Markov Random Field model (the S-T MRF  model) so as to divide an image into blocks as a group of  pixels, and to optimize the labeling of such blocks by referring  to texture and labeling correlations among them, in  combination with their motion vectors. Combined with a  stochastic relaxation method, our S-T MRF optimizes object  boundaries precisely, even when serious occlusions occur. The  description of the ST-MRF model is represented in [ 10].

The S-T MRF model solved the occlusion problem in the    single area. Even if the occluded pedestrians were successfully  segmented by the S-T MRF model, lower part of the occluded  pedestrian does not appear in the image. In such the case,  bottom of the circumscriptive rectangle of the pedestrian in the  right image does not indicate the grounding position of him.

Therefore, appearance offsets due to occlusion must be  considered.

In our algorithm, reference value of pedestrian's height  without occlusion has been learned statistically according to  top positions of their circumscriptive rectangles. If the height of  an object is lower than the reference value, it is considered that  the object is occluded by the other object or obstacles. In the   occlusion case, an additional optimization process should be  performed as a follow.

(The position correction with occlusion)  Step. }: Acquire position (xtop, ytop) of the occluded object  as the top of circumscriptive rectangle. Here the  origin of the image is defined at upper left comer.

Step.2: Obtain the height 'h' of the pedestrians from  statistics data by referring to their top position  (xtop, ytop).

Step.3: Update the grounding position of the pedestrian as  (xtop, ytop + h).

[End of the algorithm)  Figure 3 shows an example of the tracking result of the  object by this algorithm. It is understood that ROI of the  pedestrian with occlusion is extracted by our algorithm, and the  trajectory of the objects (pedestrians) are accurately  calculated.

B. Time sequence data  Both trajectory (X, Y) and size (H, W) of an object is  calculated by the ROI information obtained tracking results of  the ST-MRF model. The position of an object (X, Y) is defined  as the center of the ROI's feet, and the trajectory of the object  is tracked as shown in figure 3 (a).

The pose and location information are also obtained from  the size of the object and the trajectory, respectively. The pose  information is extracted by an appearance feature. The  appearance feature adopted in this paper is aspect ratio of the  pedestrian's ROI mentioned in formula ( 1).

aspect ratio = W / H . . .  ( 1)    For example, as shown in figure 4, the aspect ratio of standing  (walking) pedestrian is obviously different from that of the  crawling pedestrian. Therefore, the pose feature falls into three  categories by threshold based decision as follows (2). The  thresholds (THs and THd are decided empirically.

{o if aspect ratio < TH s (standing)  pose = I if TH s :s; aspect ratio :s; TH L (crawling) . . .  (2)  2 if aspect ratio> TH L (lying)  The location information is defined by assembling the  position of the object and semantic information of the position  on the image together. The semantic information is catch from  location map as shown in figure 3(b). Pedestrians tracked as  represented in figure 3(a) are mapped into the figure 3 (b) by  referring the trajectory of the objects. The semantic location of  the position is defines as a normal floor. So, the pedestrian is  considered to be nested on the normal floor.

In addition, the attribute of the object is also judged from  the Data[t] in this timing. If the ROI of an object is motionless  frame no. 188 frame no. 204 frame 110. 235  Aspect ratio  1 .. . 1 ::::L::;::?;: ! i o:c .. . . .... . . . ;.-y<, . .... l''''' ...... . ; ............ ;.\ ... c==,  ......... ; 015  - _  ........ j  "'  Image fmme no,  101 711  Image frame no.

Fig. 4. Aspect ratio of region of interests  between To frames in the past, the object is considered as not  pedestrian. The example is given in section IV.

e. STI (State transition information)  The equations are an exception to the prescribed  specifications of this template. You will need to determine  whether or not your equation should be typed using either the  Times New Roman or the Symbol font (please no other font).

To create multi leveled equations, it may be necessary to treat  the equation as a graphic and insert it into the text after your  paper is styled.

To handle the behavior understanding process easily, STI is  recorded from the time sequence data (Data[tD by the state  transition form of each feature. The behavior understanding  process is done by the unit of STI.

STI is a state transition buffer witch has a steps information.

STI[n] has n th information including both location and state  buffer witch has b steps. The STI buffer is accumulated when  the location number of the object is updated. Similarly, state  buffer in STI is accumulated when pose of the object is  renewed, and the posed continuance time is also recorded in the  buffer. The behavior of pedestrian is basically decided by  referring the location and state buffer information in the STI (to  be described in section 4).

D. Multiple objects interaction  It is thought that there are some interactions if the object is  mutually adjacent. This information is defmed as an objects  interaction map, in our algorithms. If each object is adjacent in  a constant distance, the value in the object interaction map is  assumed to be }, otherwise the value is assumed to be O. The  information is maintained as a multiple objects interaction map  as far back as m frame in the past (see figure 5). Human  behavior understanding on multiple pedestrians becomes  possible by considering the multiple objects interaction map  with STI together (to be described in section 4).



IV. HUMAN BEHA VIOR UNDERSTANDING BY THE  PROPOSED FRAMEWORK  Basically, most of the behavior can be judged from the  features by framework mentioned in the previous section.

Details of the major behavior understanding in this paper are  shown according to the types as follows.

t=n  t=n  t=n-m I  =n-2 I  -1    3 -.

.0    1 2  -1    -1   I obJ1  obJ1  3 ... ... ... 96   -1  -1  -1  -1  -1  Fig. 5. Object interaction map  A. Single pedestrian or no interaction  96 -  -  -  -  -  -  -=r -  -  -  -  Whether the pedestrian is crawling or lying down can be  judged by referring to pose feature in the framework. However,  a behavior of the pedestrian can not be completely decided to  be normal or abnormal by this feature. For instance, even when  a pose of the pedestrian is "crawling", a behavior of the  pedestrian may be judged as "normal" if the pedestrian is  located on "bench". On the other hand, the behavior may be  judged as 'abnormal" if the pedestrian is located on "floor".

The behavior understanding method of the single pedestrian  judged by combining the pose feature with the location feature  is described as follows.

1) Illegal gating  Figure 6(b) is an example of understanding "illegal gating"  by our framework. When a certain pedestrian is posture of  "crawling" or "lying down", if the pedestrian is passing over  the position where the location is a gate of tickets of the station,  it is obviously thought as illegal gating. Moreover, the case  where the pedestrian similarly passes over the position where  the location is a fence of the station is probably illegal gating    by over fencing. In the example, it is determined that an  object A is passing gate of the station, because the attribute is  "Pedestrian" and the location is transited from "floor" to "gate".

In addition, the behavior of the object A is judged to abnormal  (illegal gating), since the pose is "crawling" when the object A  is passing the gate.

2) Loitering  The object is considered as loitering if the object's  attribution is "pedestrian", and the object returned to the  location witch num of location is the same, and if the duration  until returning is more than TL as shown in fig. 6(c). The  behavior of the object B seems to be normal wrongly if it is  only considered that he keeps moving the position where the  location is floor. This case is an example for successful  understanding of abnormal behavior of the single pedestrian by  time sequential transition of the location.

3) Human crawling or lying down  The object can be understood as "crawling" or "lying  down" by the pose feature, regardless of state transition of  location feature. However, if the location is a bench, the  behavior of the object is assumed to be normal even when the  pose is "crawling". In this way, the normal behavior can be  understood correctly by the combination of pose feature with  1(wl)  6(le)  10(11)  Obj A  Attribute  Ped  Ped  F(II) 3(ft) r(II)  ?

?

OBJA  7(ob) ":"""" 8(le 9(g!)  11(11)  12(11)  OBJ B  wlWall flFloor feFence  ob Obstacle gtGate  (a) location map  13(ft)    Pose Location Interaction  Standing 12: Floor -  Standing 9:Gate -  Ped Crawring 9:Gate -  Ped Standing 4: Floor -  [ Ill,g,' gat;'.

(b) Behavior understanding (Illegal gating)  Obj B  Attribute Pose Location Interaction  '\  Ped Standing 12: Floor -  Ped Standing II: Floor -  Ped Standing 10: Floor -  Ped Standing II: Floor -  ] Duration  > TL  Ped [ Loitering Standing 12: Floor  -  ( c) Behavior understanding (Loitering)  Fig. 6. Example of behavior understanding  for single pedestrian or no interaction  location feature.

B. Multiple objects (pedestrians) interactions  This category based on the single pedestrian behavior  understanding. Examples mentioned below are complicate  cases. They can be resolved by human behavior understanding  algorithms in this level.

/) Abandoned object  Figure 7(b) represents the behavior understanding for  abandoned object. This is one of the examples using multiple  object interaction. The object is considered to be abandoned, if  the attribute of the object is not pedestrian, and when it is left  during To. When it turns out that a pedestrian was adjacent to  the object by the object interaction map, the pedestrian is  considered as an owner of the object.

2) Suspicious pedestrian's interaction   If there is contact with other pedestrians at the time within  T D former second when a certain pedestrian is lying down on  the floor and it crowds, the contact pedestrian could be  suspicious. In figure 7(c), object F is considered as a suspicious  pedestrian, because he contacts to object E within T D frames    from when the object B was lying down on the floor.

A. A. Conditions

V. EXPERIMENT  In our experiments, the video sequences imitated by the  extra's acting in various scenes such as station gate of tickets,  or the station premise, or the station home as shown in Table 1.

They were combined to produce objects tracked frames from  which the features are extracted. Then, the proposed framework  algorithms developed in this work are applied for behavior  understanding process. In the framework, location information  on each scene is defined beforehand. The value of THs and  THL deciding pose feature is set to 0.6 and 1.5 respectively.

The duration values deciding interaction among objects such as  To or T D are uniformly configured to 50 frames. These  processes were handled in off-line. The rate is 10 frames per  second. The number of samples is twelve (five categories) as  represented in table.

B. Results  Results of the experiments are shown in figure 8.The good  performance of classification rate is proved the effectiveness of  the proposed algorithm. It was confirmed that detection of the  behaviors that had been targeted in this research had succeeded.

Additionally, there are no false detections that the behavior of  pedestrians other than the target is understood as abnormal.



VI. CONCLUSION  In this paper, the framework of the human behavior  understanding corresponding to various scenes in the station is  constructed. We have presented a simple rule based approach  for understanding human behaviors in video sequences,  addressing the main goal of surveillance systems for protecting  public transportation assets. The promising results indicate that  there is much to be said in favor of simple methods even when  the problem is complex. So, in our future works, the simple  approach would be integrated with other methods such as  silhouette analysis or detailed trajectory analysis (velocity of  objects etc.), possibly using a successive refinement strategy.

