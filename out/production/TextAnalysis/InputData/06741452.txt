A Slim and Wide Multi-Touch Tabletop Interface and  its Applications

Abstract ? A tabletop interface based on a large, slim and wide LED display is proposed. The tabletop interface uses two cameras with two reflection mirrors. Its thickness is less than 8 inches and its size is 47 inches. In order to recognize multi- touch inputs and dragging operations, the proposed system adopts a hybrid recognition approach, which combines FTIR (Frustrated Total Internal Reflection) and LLP (Laser Light Plane). The proposed tabletop interface has many applications in   restaurants, digital fashion and games. The performance of the proposed system was evaluated. It showed real-time recognition of multi-touch inputs1.

Index Terms ? Tabletop Interface, Multi-touch, Slim and Wide Display, LED, Multiple Cameras

I. INTRODUCTION Multi-touch interfaces are widely used in various digital  devices. Most users like to touch and drag icons, menus and objects presented on the multi-touch screens of digital devices.

A tabletop interface is one of the popular ways to implement multi-touch interfaces. A tabletop interface, which looks similar to a table, allows multi-touch interactions of multiple users. A tabletop interface is expected to be utilized in various application domains such as education, collaboration, entertainment and so on in the future [1].

A tabletop interface consists of a display screen and utilizes an infrared (IR) camera to recognize user inputs. A beam projector has been used to implement a large touch screen interface. The IR camera is located at a sufficient focal distance so that the interface can perceive its entire touch area and the beam projector can display objects on the large display screen [1]. Therefore, the tabletop had to be thick. However, a thick tabletop interface may not always be acceptable if its user wants to sit down and stretch his/her legs under the tabletop. Furthermore, relatively low resolution images may be produced depending on the image quality of the beam projector.

Therefore, various methods have been suggested to resolve the space constraints [2-5]. Most of these methods provided   + Corresponding Author 1 YoungSeok Ahn is with the Peratech, Korea (romansys@konkuk.ac.kr) Jun Lee is with the Institute for Media Innovation, Nanyang Technological  University, 637553 Singapore (leejun@ntu.edu.sg).

HyungSeok Kim is with the Department of Internet and Multimedia  Engineering, Konkuk University, Korea (hyuskim@konkuk.ac.kr) Jee-In Kim is with the Department of Internet and Multimedia Engineering,  Konkuk University, Korea (correspondence to jnkm@konkuk.ac.kr)  tabletop interfaces with relatively small displays. Their screen sizes (mostly under 21 inches), however, were not large enough to be used as normal tables. In order to solve this issue, a special LED-based tabletop interface was developed [6].

Since an LED display provides an infrared light per every pixel, an LED-based tabletop interface can display high quality digital content and precisely sense IR light simultaneously. Furthermore, it will be 4 inches thinner and will also provide a 40 inch wide screen [6]. However, it will be very expensive and complex to implement because of the special LED screen with many sensors.

The proposed large, slim and wide multi-touch tabletop interface system offers high resolution images and improved multi-touch performance with a hybrid method of user input recognition. The proposed system also presents software architecture for enlarging the screen size of the tabletop interface to support the seamless recognition of multiple touches and dragging operations captured by the connected multiple cameras of the tabletop interface.

The proposed tabletop interface utilizes a 47 - inch high- definition (HD) LED display as its output display. To reduce the depth of the tabletop interface system, a design including an optical reflection model including dual IR cameras and reflection mirrors is used. The proposed system also provides a hybrid recognition approach that combines the FTIR (Frustrated Total Internal Reflection) method [7] and the LLP (Laser Light Plane) method [8] to enhance the user input recognition performance. The main advantage of the proposed system is that it provides a slim multi-touch tabletop interface based on a large HD screen at a reasonable cost.



II. RELATED WORKS A tabletop interface system is equipped with a display on its  top where multiple users can interact with digital content presented on the top display. Various types of tabletop interfaces have been developed and installed in various areas such as shopping malls, exhibition halls, restaurants and so on.

Currently, most tabletop interfaces adopt IR cameras and beam projectors to implement wide touch screens. Sufficient distances and spaces are required to guarantee the optical paths of these devices. At least a 31- inch depth is required between the beam projector and the 30-inch screen [1]. Such a problem of guaranteeing distances has been regarded as a major limitation in making more flexible designs of tabletop interfaces.

In order to develop a slim tabletop interface, it is necessary to reduce the optical paths of projectors and cameras. This still remains as a big challenge. ThinSight [2], MightyTrace [3] and FLATIR [4] have an electronic board, a photo sensor board for IR emission and reception placed behind a conventional LCD screen. They have a thin display but their screen sizes are less than 21 inches. They are also complex and expensive to implement. FiberBoard [5] introduced a new recognition method using fiber optics. It has a thin structure and its screen size is 19 inches or less. The problem is that its fiber optic array is complex and expensive to build. It also has a small screen of 19 inches or less.

To resolve the distance guarantee problem, Microsoft proposed a special LED-based table top interface. Since the special LED display provides infrared light per pixel, the tabletop interface can display digital information and sense IR lights by pixels simultaneously [6]. Ahn et al proposed another slim tabletop interface using multiple cameras and multiple LCD displays [9, 10]. It also used dual mirrors to reduce its thickness. The proposed system was extended by adopting a tabletop interface using a 47 - inch screen and a recognition method combining FTIR and LLP, which enhanced recognition performance.



III. SYSTEM A. Hardware for Multi-Touch Screen  Fig. 1 shows the hardware structure of the proposed system.

Its recognition method combines the FTIR method and the LLP method to increase sensitivity of its touch sensing components.

Fig. 1 Hardware structure.

The proposed system contains two IR lights. For the LLP recognition, four IR laser emitting diodes are installed on its four corners on the top. IR LED light strips, which are 0.4 - inch thick and attached on the sides of the 47 - inch HD LED display, are utilized as the light source for FTIR recognition.

To recognize IR lights, it is modified by removing one of the sheets of the backlight unit of the LCD display module.

In the proposed system, the modified 47-inch HD LCD panel is used as the output display component. Its LCD backlight units are reconstructed so that it will be able to pass IR lights generated from the hybrid optical setup. In Fig. 2, the detailed structure of the modified backlight unit is illustrated.

In order to recognize user?s touch inputs, the reflection sheet is removed of the backlight unit of the LCD module as shown in Fig. 2.(7).

The hardware setup process is presented in Fig. 3. In Fig.

3.(A), the result of removing the reflection sheet of the backlight unit and the guide panel from the LCD display module in Fig. 2 is presented as the first step of the modification. Then, the acryl plate in Fig. 2.(8) is replaced so that user inputs on the touch screen can be recognized by the IR cameras. Fig. 3.(B) shows the result of the second step.

After the modification of the LCD display module, an IR recognition system is attached for the FTIR recognition. The LED lights (Fig. 2.(2)) are attached for the FTIR-based recognition. Fig. 3.(C) shows the results of building the touch screen in the third step. The acryl plate is surrounded by the LED lights. Finally, four LED laser diodes (Fig. 2.(1)) are combined with the touch screen. Fig. 3.(D) shows the attached LED laser diodes.

Fig. 2 The structure of the 47-inch LCD display module (1) 4 LED laser emitting diodes for the LLP recognition (2) LED light strips for the FTIR recognition (3) Acryl pate is used as the touch screen (4) LCD panel (5) Optical sheets (6) LED lights (7) Reflection sheet (8) Acryl plate.

Fig. 3 Hardware setup process (A) STEP 1: LED lights (Fig. 2.(6)) and the the reflection sheet (Fig. 2.(7)) is removed. (B) STEP 2: Combine the backlight unit with the replaced acryl plate, the optical sheets (Fig. 2.(5)) and the LCD panel (Fig. 2.(4))  (C) STEP 3: Attach  LED lights (Fig.  2.(2)) for FTIR recognition. (D) STEP 4: Attach LED laser diodes (Fig. 2.(1)) for LLP recognition.

The laser lights shine just above the surface of the acrylic plate while the lights from the LED strips travel inside the acrylic panel. When a user touches the surface of the proposed tabletop interface, the IR lights from the IR sources are scattered, as illustrated in Fig. 4, and reflected by the two total reflection mirrors. The IR cameras can detect the reflected IR lights.

One of well-known problems of the FTIR method is that its success ratio of recognition of dragging operations is low when users move their fingers on the screen. In the LLP       method, irregular noises can occur. To solve these problems of low recognition ratio of dragging and irregular noises, the proposed recognition method combined the FTIR method and the LLP method, configured the hardware settings, and adjusted their threshold values.

Fig. 4 When a user touches the screen, IR lights are scattered and recognized by the system. Laser lights are also utilized. FTIR and LLP are combined in the proposed hybrid method of recognition.

Fig. 5 Determination of the projected area according the height of the tabletop interface and the angle of the mirror in the proposed system.

The recognition component utilizes two IR cameras which are designed by considering optical characteristics of reflection as shown in Fig. 5. With this configuration, the component can recognize all touches and dragging operations on the 47-inch table screen area.

In order to reduce the height of the box of the tabletop interface (?h? in Fig. 5), two cameras and two inclined mirrors were used, as shown in Fig. 1. The recognizable area can be determined from the height of the tabletop interface and the angle of the mirror. Thus, the proposed tabletop reduces the height when the recognizable area is more than half of the original screen, as shown in Fig. 5. If the height (?h?) and the angle of the mirror (??m? in Fig. 5) are defined, the normal distance (?d?)  from the camera to the plane of the mirror can be obtained. And the small triangle in Fig. 5 is surrounded by three line segments, (h, d, and x in Fig. 5) and the angle (?? ?) is computed by equation (1).

? = 90- ?m (1)   To calculate values of lines d and x, trigonometric functions  were used, as below in equation (2).

d = h*sin ? x = h*cos ?  (2)  Second, a virtual triangle can be considered to cover the height (?h?), the minimum width of the recognizable area (?r?) and a sum of two line segments (?x? and ?k?) along the plane of the mirror. Since another camera needs to recognize another area, the mirror should be located under the touchscreen of the tabletop interface so that the sum of two lines x and k is longer than the length of the mirror.

Third, two similar triangles are presented in Fig. 5: the small one consists of h, d and x and the large one of r, k and d.

Since these triangles are similar, due to their angles being the same, the value of (?r?) can be obtained, which represents the width of the recognition area of the camera, using proportional expressions listed in equation (3).

x:d = h:r x*r = d*h r = d*h/x  (3)   Finally, the value of r can be obtained by combining the  equation (2) and the equation (3) as presented in the equation (4).

r=h*tan(90- ?m ) (4)   The recognition area may increase depending on the camera?s angle of view and the mirror reflectivity. Fig. 6 shows the inside structure of the proposed tabletop interface.

In Fig. 6.(A), two cameras are attached and two mirrors are deployed at around 30?, and the minimum recognition area is about 60% of the area of the 47 - inch touch screen, as shown in Fig. 6.(B). The height of the proposed interface is 7.5 inches, as shown in Fig. 6. (C)    Fig. 6 The inside of the hardware structure (A) IR camera, (B) Two mirrors (C) the height of the tabletop is measured as 7.5 inches.

B.  User Inputs Recognition Software   Fig. 7 IR images recognized by two cameras (A) the left side camera (B) the right side camera.

Fig. 8 The software architecture.

Fig. 9 GUI of MTView.

Fig. 7 shows IR images of two cameras of the tabletop interface. Since the proposed tabletop interface used a convex lens and inclined mirrors, perspective distortions as shown in both Fig. 7.(A) and Fig. 7.(B) occurred. The distortion problem made it difficult to compute the correct positions of user inputs on the screen. In order to solve the perspective distortion problem of images and seamlessly recognize correct touch positions and traces of dragging operations on the screen, it is essential to design and develop a software tool for processing image data from the multiple IR cameras of the tabletop interface and computing correct positions and traces of user inputs. The software architecture is shown in Fig. 8.

The proposed software consists of a multi-touch recognition library for clients called MTView (Multi Touch View) and a concurrent management server.

MTView is a unit of a multi-touch processing library. It contains a recognition algorithm of multi-touch input images from a camera, a gesture recognition algorithm and TCP/IP communication modules, as shown in Fig. 8. When the proposed tabletop interface receives multi-touch images, it uses various image processing filters to optimize the multi- touch outputs. The image processing filters are dependent on external conditions, such as what of the camera, external lights and the sizes of screens. MTView thus provides GUI to control the thresholds of the proposed system as shown in Fig.

9. The threshold values can be adjusted during the calibration stage of the tabletop interface.

When users touch the proposed tabletop interface, each client recognizes multi-touch information such as positions and gestures by processing the touch image from each IR  camera. Then, the concurrent server aggregates the touch information from each client to create a single unified set of touch input data. The concurrent server then sends out the unified touch information to applications.

Fig. 10 The overlapped area of the IR cameras during recognition of user inputs on the proposed tabletop interface.

The proposed system offers a large area for user interactions such as multiple touches and dragging operations. Therefore, a single camera may not cover the whole recognition area. If the proposed system uses two cameras for the recognition of the user inputs on the shared regions of the displays, there could be an overlapped region on the display ,where interactions will be recognized by the two cameras at the same time. Since the (?r?) value in Fig. 5 of each camera is larger than half of the recognition area of the tabletop, there is an overlapped area on the tabletop as shown in Fig. 10. Since the interface is designed so that each camera can cover more than  half  the screen area, a touch on the overlapped area will be recognized by the two cameras at the same time. Two different position coordinates will be generated by the two different cameras.

Therefore, a way to determine a single position coordinate  for the single touch should be invented.

If  || p1-p2 || < t  then, (t is threshold value, 0.001) d1 = ||p1-c1|| , d2 = ||p2-c2||  If d1 < d2 then select d1 Else if d2 < d1 then select d2  (5)   In order to solve this problem, the concurrent server uses an  algorithm that unifies the duplicate coordinate values, as shown in Fig. 11. First, the system determines whether the input position is in the overlapped area. If the input touch (?p? in Fig 10) is in the ovelapped area according to eqution (5), the system will have two position values (?p1? and ?p2? in Fig.

10) from  ?Camera 1? and  ?Camera 2?, respectively. Since the proposed system must select one of them, it calculates the distances between the two positions and the two camearas (?d1? and ?d2?) using equation (6).

d1 = ||p1-c1|| ,  d2 = ||p2-c2|| (6)       The distances (?d1? and ?d2?) are compared, and the proposed system selects the shortest one, because a shorter distance could probably be more accurate. After the calculation, the proposed system stores the current input position value for the next process. The proposed system also interpolates the recognized positions when the selected area is changed.

Fig. 11 Algorithm of unifying duplicate coordinate values in the overlapped area.



IV. APPLICATION A. Restaurant   Fig. 12 e-Menu for a restaurant.

In a restaurant, tables are used so that customers can chat with friends, read menus, order dishes, put dishes and eat on them. Since the tabletop interface is a ?table?, it can offer such important and basic functions of a table where multiple users sit around, talk and experience digital content of menus, games and so on. Fig. 12 illustrates an ?e-Menu? application for a restaurant. The users can order dishes from the e-Menu on the tabletop interface as shown in Fig. 12. The e-Menu  content can provide extra information about the selected dishes such as description, calories, prices, and so on. While the users wait for their ordered dishes, they can enjoy other digital content such as travel information, games, and so on.

B. Game An interactive game can be a good application of the  tabletop interface. When people get together beside a table, they usually play games. Since various multi-touch gestures can be used with the tabletop interface, the proposed system can adopt many games with touch gestures, such as puzzle matching games and real-time strategy games, as shown in Fig.

13. Such a game of natural gesture-based interactions can be played by many people who like simple and easy games. They can move their fingers, hands, and arms while they play the games. The actions required by the games are helpful for old people who need easy, simple and mild exercises and do not like complicated mouse actions.

Fig. 13 A puzzle matching game can be played on the tabletop interface

V.  EXPERIMENTS The performance of the proposed system was evaluated  with the following quantitative measures: real-time interaction, recognition speed and recognition accuracy.

A. Real-Time Interaction The interaction performance of the proposed system was  measured. Its average performance was measured to be around 60 fps (frames per second) with two IR cameras, which provided 640x480 pixels of resolution with a 2.8 GHz Quad Core CPU. The performance changed to 58 fps when 15 fingers touched on the screen and 55 fps when 20 fingers touched. The results of the analysis showed that, as the number of finger touches increased, the performance of the system slightly decreased. However, the system still demonstrated feasible performance for real-time interaction.

B. Recognition Speed In order to evaluate the performance of the proposed  recognition methods, i.e. the hybrid method of FTIR and LLP, a puzzle matching game as shown in Fig. 13 was used. In order to solve a puzzle, a user must touch two puzzle pieces and drag the two pieces together to the center of the display to       match them. One piece shows a word (such as a pipe, a fan, and so on) and the other piece shows the image corresponding to the word. If the user can find a match, the user would get a score. The game completes when the user matches all combinations of words and images on the display.

Twelve subjects experienced with multi-touch operations were selected. The subjects were divided into three small groups, and the recognition tests were carried out in random to reduce learning effects. The subjects touched (to select puzzle pieces) and dragged the puzzle pieces (to match the selected word to the selected image). When they finished the puzzle game, their playing times were measured for three different recognition approaches. Fig. 14 shows their average completion times. According to Fig. 14, the proposed hybrid recognition method can reduce the average playing time by 20%.

Fig. 14 Average time required to complete the puzzle matching game for different recognition methods: FTIR, LLP and the hybrid method.

Friedman tests were applied on the evaluation results of Fig.

14 for a statistical analysis. The results of the Friedman tests are described in Table 1. Because the obtained value F(Friedman Test Statistic) is greater than the Chi-Square, the difference among the recognition methods is statistically significant.

TABLE I RESULTS OF FRIEDMAN TEST  N Chi-Square df P-Value F  12 5.99 2 0.05 13.5   The twelve subjects were interviewed after the experiments, where they used ?touch? operations and ?drag? operations.

They mentioned that playing with the FTIR method was the most difficult because the drag operations frequently failed during the experiments. They also found that the touch operations were more difficult   than the drag operations with the LLP method. Most subjects were satisfied with the performance of the proposed hybrid method.



VI. CONCLUSIONS A slim and large HD LED tabletop interface using multiple  IR cameras and mirrors was proposed. Users can sit down and stretch their legs below this less-than-8-inches tabletop just like a real table. A hybrid recognition method of the FTIR  method and the LLP method was also proposed. The proposed tabletop adopting the hybrid recognition method was demonstrated and evaluated in several applications, such as e- Catalogue for digital fashion, e-Menu for a restaurant and an interactive puzzle matching game.

The experiments for the evaluation of the proposed hybrid recognition method were performed using a puzzle matching game. The results demonstrated the feasibility and the effectiveness of the proposed hybrid recognition method. The results also showed the expandability of the method because of its real-time interaction and recognition accuracy.

In future works, the authors plan to modularize the software and hardware of the tabletop interface so that they may be reconfigurable. Though modularization, a set of tabletop interface modules may be combined as desired to extend and reconfigure the tabletop interface as a huge table or a huge wall display.

