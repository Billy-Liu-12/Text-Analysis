

Abstract--- Weighted association rule mining reflects semantic significance of item by considering its weight.

Classification extracts set of rules and constructs a classifier to predict the new data instance. This paper proposes compact weighted associative classification method, which integrates weighted association rule mining and classification for constructing an efficient weighted associative classifier. Compact weighted associative classification algorithm randomly chooses one non class attribute from dataset and all the weighted class association rules are generated based on that attribute.

The weight of the item is considered as one of the parameter in generating the weighted class association rules. In this proposed work, weight of item is computed by considering quality of the transaction using link based model. Experimental results show that the proposed system generates less number of high quality rules.

Keywords: Association Rule Mining, Classification, Associative Classification.



I. INTRODUCTION  Data mining principally deals with extracting knowledge from dataset. In a world where data is all around us, the need of the hour is to extract knowledge or interesting information, which is hidden in the available data. Association rule mining is concerned with extracting a set of highly correlated features shared among a large number of records in a given database.

For example on mining the database of a store, association rule mining can bring out relationship between the items in the store based on the customers buying patterns. Although classical association rule mining algorithm reflects the statistical relationship between items, it does not reflect the semantic significance of the items [6]. To meet the user objective and business value, various weighted association rule mining method [11] [15] [7] [12] are proposed based on the weightage to items. Here the weight is mostly based on utility of an item such as profit.

Classification is also one of the most important tasks in data mining. Classification builds a model known as classifier and can be used to predict classes to new records. For example, a risk classification model can be built from a dataset of previous credit card customers and applied to classify the risk levels of new customers.

Weighted association rule mining applies unsupervised learning where no class attribute is involved in finding the association rule. On the other hand, classification uses supervised learning where class attribute is involved in the construction of the classifier. Both, weighted association rule mining and classification are significant and efficient data mining techniques. So integration of these two data mining techniques may provide efficient associative classifier[13]. In [13] we have proposed weighted associative classification based on CBA algorithm and weight for each item is generated randomly.

In this paper we proposed a new feature which includes 1. Weight for each item is calculated using HITS model. 2. The new compact weighted associative classification (CWAC) is proposed. The proposed CWAC algorithm is completely varies from CBA. In CBA, Apriori association rule mining algorithm is directly applied to find the class association rules.

Whereas Compact weighted class association rule generation algorithm randomly chooses one non class attribute from dataset and all the items are generated only based on that attribute. Thus this algorithm reduces number of itemset generation. Finally the proposed algorithm calculates the weighted support and weighted confidence for each item and determines whether the item is frequent or not.

MIT, Anna University, Chennai. June 3-5, 2011      II RELATED WORK  A Association Rule Mining  Association Rule Mining (ARM) [1], has become one of the important data mining tasks. ARM is an unsupervised data mining technique, which works on variable length data, and it produces clear and understandable rules. The basic task of association rule mining is to determine the correlation between items belonging to a transactional database. In general, every association rule must satisfy two user specified constraints, one is support and the other is confidence.

The support of a rule X  Y (X and Y are items) is defined as the fraction of transactions that contain X and Y, while the confidence is defined as the ratio support(X and Y)/support(X). So, the target is to find all association rules that satisfy user specified minimum support and confidence values.

B.  Associative Classification   Associative classification was first introduced in  [10] which focus on integrating two known data mining tasks, association rule discovery and classification. The integration done is focused on a special subset of association rules whose right hand side is restricted to the class attribute; for example, consider a rule R: X Y, Y must be a class label. Associative classification generally involves two stages. The first stage, it adopts the association rule generation methods like Apriori candidate generation [2], or FP growth [5] algorithms to generate class association rules. For example CBA [10] method employs Apriori candidate generation and other associative methods such as CPAR [17], CMAR [9] and Lazy associative classification [4,3] methods adopts FP growth algorithm for rule generation. The above step generates huge number of rules. So in the next stage, generated rules are ranked and the rules that satisfy certain threshold conditions are used to construct the classifier. After rule ranking, only the high-ranking rules are chosen to build a classifier and the rest are pruned.

The associative classification method uses support and confidence measures to evaluate the rule quality.

Support and confidence are given as  Support (X Y) =           Occurrence (X U Y) ____  Total number of transaction ----- ( 1) Confidence (X  Y) = Occurrence(X U Y) Occurrence (X)          ----- ( 2)   Any Item has a support larger than the user minimum support and confidence is called frequent  itemset. The support and confidence measures used in association rule mining and associative classification treats each transaction items equally but different transactions have different weights in real-life data sets [12].

C. Weighted Association Rule Mining   Classical ARM framework assumes that all items  have the same significance or importance i.e. their weight within a transaction or record is the same (weight=1 per item) which is not always the case. In the supermarket context, some items like jewellery, designer clothes, etc., are of much significant in terms of revenue or profit by the store. Hence weight can be used as a parameter to generate association rule mining called as weighted association rule mining [7, 12]. The generation of weighted association rules is based on user specified minimum weighted support and minimum weighted confidence thresholds. The use of weighted support and weighted confidence leads to useful mechanisms to prioritize the rule according to their importance, instead of their support and confidence alone.

D. Weighted Associative Classification   In [13] we have proposed weighted associative classification, which integrates weighted association rule mining and classification to construct the efficient weighted associative classifier. Weighted associative classifier extracts special subset of association rules called weighted class association rules (WCARs).

Weighted association rule mining uses weight as one parameter but here weights for each item item are assigned randomly. But it is very difficult to assign weights to each item.

D.  HITS Model  Kleinberg [8] used HITS algorithm in bipartite graph and weights are derived from the internal structure of the database. This proposed method uses HITS model to derive the weight for each item. Then these weights are used to compute the Weighted Class Association Rules.

III PROPOSED SYSTEM  A. Problem Definition   Let database D is a set of instances where each  instance is represented by < a1, a2 ?am , C>, where a1, a2 ?am, are attributes and C are class value each has weights. A common rule is defined as x  c, where x is a set of non class attributes and c is class label. The quality measurement factor of a rule is weighted  IEEE-ICRTIT 2011      support and weighted confidence where weighted support is (Occurrence (x & y) c / |D|)* (weight (x) + weight (y) + weight (c)), |D| denotes the total number of instances in database and weighted confidence is (Occurrence (x & y) c / Occurrence (x c)) * (weight (x) + weight (y) + weight (c)). Rule items that satisfy minimum weighted support and weighted confidence are called frequent rule items, while the rest are called infrequent rule items. Here the task is to generate the Weighted Class Association Rules (WCARs) that satisfies both minimum weighted support and minimum weighted confidence constraints. Then these WCARs are used to construct a classifier based on Confidence Support size-of-the rule Antecedent.

B. Weighted Associative Classification Rule Mining   Classification and weighted association rule mining are two different techniques involved in rule mining.

Integration of classification and weighted association rule mining consist of two steps. In the first step the weighted associative classification rules are generated using Compact Weighted Class Association Rule Generation algorithm and the second step deals with the classifier to order the rules generated in step one.

C. Compact Weighted Associative Classification Algorithm  Compact weighted class association rule generation algorithm is shown in Figure 3.1.

INPUT: DATA SET OUTPUT: ITEM SET  Step 1: Divide the dataset into training and testing  dataset.

Step 2 : Choose an attribute randomly other than class  attribute from the training dataset.

Step 3: Generate one item based on the selected  attribute.

Step 4 : Calculate weighted support.

Step 5: If weighted support of item is greater than  minimum weighted support then generate two itemset and so forth  Step 6: Calculate weighted confidence for all itemset.

CLASSIFIER ALGORITHM  Step 1: Rank the rules based on Weighted Confidence,  Weighted Support and Size of the rule antecedent.

Step 2:  Classify the test dataset using these ruleset and obtain the classifier accuracy.

Fig 3.1 Compact Weighted Associative Classification Algorithm  Weighted support and weighted confidence are given as  WSup (X U Y C)=(Occurrence (X U Y  C / |D| ) * (Weight ( X) +Weight(Y)+ Weight(C )) ---- (3)  W Conf (X U Y C) = WSup(XUY C)       ---- (4)  WSup (X C)   Where X and Y are items in the dataset, C is the set of class label. In frequent rules are pruned to get frequent class association rules. The steps are shown in the Figure 3.2.

Fig 3.2 Steps in Compact Weighted Associative Classification   The minimum weighted support and minimum weighted confidence are user defined threshold values.

The itemset that has weighted support and weighted confidence above the threshold value are called as frequent itemset and others are called as infrequent itemset which are pruned during rule generation process. This is followed by the classifier construction.

To built the classifier all the generated rules are ranked based on Weighted Confidence, Weighted Support and size-of-the rule antecedent. Then best rules are chosen and used in the classifier

IV. SIMULATED EXAMPLE   Following example is used to explain how the HITS algorithm is constructed. In the Table 4.1 sample transaction are given. This sample dataset values are transformed into bipartite graph as shown in Figure 4.1.

Calculate the item weight using HITS Model  Generate class association rules using CWAC algorithm based on weighted support and  weighted confidence  Rule ranking and pruning  Construct the Classifier  Predict the new dataset  Compact Weighted Associative Classification      TABLE 4.1 SAMPLE DATASET   TID Transaction 1 A B C D E 2 C F G 3 A B 4 A 5 C F G H 6 A F G H  A   [1]    B   [2] .   C   [3]  .  D   [4]  .  E   [5] .   F   [6] .   G   H  [HUB]   [AUTHORITY]   Fig 4.1 Bipartite Graph representation of sample dataset  The following equations are used to perform each iteration:  auth(item) =  hub (transaction)                   --- (5)  hub (Transaction) =  auth (item)             --- (6)   TABLE 4.2 ITEM WEIGHT   Item weight Item weight  A .4414 E .1746 B .2531 F .3278 C .5025 G .4662 D .1746 H .3203   The Table 4.2 shows the authority weight for each  item of sample dataset of Table 4.1.

Let us consider Table 5.1. In the first step if  CWAC algorithm randomly chooses outlook attribute then   attribute values with combination of rules {Outlook, Temp  Play}, {Outlook, Humidity Play}, {Outlook, Windy  Play} will be generated.

But the rules with {Temp, Humidity  play}, {Temp, Windy  play}, and {Humidity, Windy  play}  attribute combination rules will not be generated. This shows number of rules generated will be less compared with the CBA algorithm.



V. EXPERIMENTAL RESULTS   Consider a dataset given in the Table 5.1 consists of 14 transactions and 2 class labels. The weight of each item is calculated using HITS algorithm and weights are computed as Sunny = 0.22, Overcast=0.19, Rainy =0.24, Hot=0.17, Mild=0.29, Cool=0.18, High=0.31, Normal=0.34, False=0.26, True=0.39, yes= 0.45, No = 0.20. In the first step ,Compact weighted associative classification rule mining algorithm will find all the weighted class association rules based on weighted support and weighted confidence threshold.

TABLE 5.1  SAMPLE DATASET    The rules whose weighted support and weighted confidence, higher than user defined minimum threshold is known as weighted class association rules (WCAR?s). Then WCARs are given to classifier where it is sorted based on weighted Confidence, Weighted Support and Rule length in descending to construct the accurate Compact Weighted Associative Classifier.

TABLE 5.2  COMPARISON OF  CLASS ASSOCIATION RULES   Associative Classification  Compact Weighted Associative  Classification Sunny yes=2 (sup)=14 Sunny no=3 (sup)=21  Sunny -> Yes = 2 (Wsup) = 9.38 Sunny -> No = 3 (Wsup) = 8.82  Outlook Temp Humidity windy play Sunny Hot High False No Sunny Hot High True No  Overcast Hot High False Yes Rainy Mild High False Yes Rainy Cool Normal False Yes Rainy Cool Normal True No  Overcast Cool Normal True yes Sunny Mild High False No Sunny Cool Normal False Yes Rainy Mild Normal False Yes Sunny Mild Normal True Yes  Overcast Mild High True Yes Overcast Hot Normal False Yes  Rainy Mild High True No  IEEE-ICRTIT 2011      Overcast yes= 4 (sup)=28 Rainy yes=3 (sup)=21 Rainy no=2 (sup)=14 Hot yes=2 (sup)=14 Hot no=2 (sup)=14 Mild yes=4 (sup)=28 Mild no=2 (sup)=14 Cool yes=3 (sup)=21 High yes=3 (sup)=21 High no=4 (sup)=28 Normal yes=6 (sup)=42 TRUE yes=3 (sup)=21 TRUE no=3 (sup)=21 FALSE yes=6 (sup)=42 FALSE no=2 (sup)=14  Sunny normal yes=2 (sup)=14 Sunny hot no=2 (sup)=14 Sunny high no=3 (sup)=21 Sunny FALSE no=2 (sup)=14 Overcast  hot yes= 2 (sup)=14 Overcast high yes= 2 (sup)=14 Overcast normal yes= 2 (sup)=14 Overcast TRUE yes= 2 (sup)=14 Overcast FALSE yes= 2 (sup)=14 Rainy mild yes=2 (sup)=14 Rainy normal yes=2 (sup)=14 Rainy FALSE yes= 3 (sup)=21 Rainy TRUE no= 2 (sup)=14 Hot FALSE yes=2 (sup)=14 Hot high no=2 (sup)=14 Mild high yes=2 (sup)=14 Mild normal yes= 2 (sup)=14 Mild TRUE yes=2 (sup)=14 Mild FALSE yes=2 (sup)=14 Mild high no=2 (sup)=14  overcast -> Yes = 4 (Wsup) = 18.56 rainy -> Yes = 3 (Wsup) = 14.49 rainy -> No = 1 (Wsup) = 3.08 hot -> Yes = 2 (Wsup) = 8.68 hot -> No = 2 (Wsup) = 5.18 mild -> Yes = 4 (Wsup) = 21.46 mild -> No = 1 (Wsup) = 3.43 cool -> Yes = 3 (Wsup) = 13.33 cool -> No = 1 (Wsup) = 2.66 high -> Yes = 3 (Wsup) = 15.96 high -> No = 3 (Wsup) = 10.71 false -> Yes = 3 (Wsup) = 14.91 false -> No = 2 (Wsup) = 6.44 true -> Yes = 6 (Wsup) = 36.12 true -> No = 2 (Wsup) = 8.26 normal -> Yes = 6 (Wsup) =33.97 normal -> No = 1 (Wsup) = 4.2  Sunny -> hot -> No = 2 (Wsup) =8.26 Sunny -> mild -> No = 1 (Wsup) =4.97 Sunny -> mild -> Yes = 1 (Wsup) = 6.58 Sunny -> cool -> Yes = 1 (Wsup) = 5.95 overcast -> hot -> Yes = 2 (Wsup) = 11.3 overcast -> mild -> Yes = 1 (Wsup) =6.51 overcast -> cool -> Yes = 1 (Wsup) = 5.74 rainy -> mild -> Yes = 2 (Wsup) = 13.7 rainy -> cool -> No = 1 (Wsup) =4.34 rainy -> cool -> Yes = 1 (Wsup) = 6.09 Sunny -> mild -> high -> No = 1 (Wsup) = 7.14 Sunny -> mild ->normal -> Yes = 1 (Wsup) = 9.1 rainy -> cool -> normal -> No = (Wsup) = 6.72 rainy -> cool -> normal -> Yes = (Wsup) = 8.47 rainy -> cool -> normal -> false -> No = 1 (Wsup) = 8.54 rainy -> cool -> normal -> true - > Yes = 1  Cool normal yes=3 (sup)=21 Cool FALSE yes= 2 (sup)=14 High FALSE yes=2 (sup)=14 High TRUE no=2 (sup)=14 High FALSE no=2 (sup)=14 Normal TRUE yes=2 (sup)=14 Normal FALSE yes= 4 (sup)=28  (Wsup) =11.2     The proposed system is compared with the  classical associative classification algorithm (CBA method [10]). The table 5.2 shows the number of rules generated. The result shows proposed method generates minimal non redundant weighted class association rules on the same time these rules are accurate because of the usage of weights for each item in dataset.

A. Accuracy Computation   Accuracy measures the ability of the classifier to  correctly classify unlabeled data. It is the ratio of the number of correctly classified objects to the total number of objects in the test dataset.

TABLE 5.3 DATASET DESCRIPTION   TABLE 5.4  ACCURACY COMPARISON   Dataset CBA   CWAC  Pima 73.18 74.2   The performance of the proposed algorithm is evaluated by comparing it with the CBA Algorithm.

Conclusion   The aim of integrating classification and weighted  association rule mining is to address some important requirements arising from modern data mining processes. The development of weighted associative classification using compact weighted associative classification (CWAC) algorithm, greatly reduce the number of rules in the classifier. This paper shows how  Dataset Transaction Classes Items  Pima 768 2 15  Compact Weighted Associative Classification      to generate Compact Weighted Class Association Rules, which may greatly improve the classification accuracy. This plays a vital role in market basket analysis, medical diagnosis and in many other applications. Experimental results show that the proposed Compact Weighted Associative Classification (CWAC) method outperformed the CBA method. This work can be further applied for more number of benchmark datasets.

