Elephant, Do not Forget Everything! Efficient Processing of Growing Datasets

Abstract?MapReduce has become quite popular to analyse very large datasets. Nevertheless, users typically have to run their MapReduce jobs over the whole dataset every time the dataset is appended by new records. Some researchers have proposed to reuse the intermediate data produced by previous MapReduce jobs. However, existing works still have to read the whole dataset in order to identify which parts of the dataset changed. Furthermore, storing intermediate results is not suitable in some cases, because it can lead to a very high storage overhead.

In this paper, we propose Itchy, a MapReduce-based system that employes a set of different techniques to efficiently deal with growing datasets. Itchy uses an optimizer to automatically choose the right technique to process a MapReduce job. The beauty of Itchy is that it does not have to read the whole dataset again to deal with new records. In more detail, Itchy keeps track of the provenance of intermediate results in order to selectively recompute intermediate results as required. But, if intermediate results are small or the computational cost of map functions is high, Itchy can automatically start storing intermediate results rather than the provenance information. Additionally, Itchy also supports the option of directly merging outputs from several jobs in cases where MapReduce jobs allow for such kind of processing.

We evaluate Itchy using two different benchmarks and compare it with Hadoop and Incoop. The results show the superiority of Itchy over both baseline systems for processing incremental jobs. In terms of job runtime, Itchy is more than one order of magnitude faster than Hadoop (up to ?41 times faster) and Incoop (up to ?11 times faster).



I. Introduction  The MapReduce paradigm and especially the open source implementation Hadoop have become the de facto standard for large scale data processing [7], [19], [21], [20], [8], [9].

This success is mainly based on the ease-of-use of MapRe- duce which enables non-expert users to process terabyte-sized datasets. In practice, these large datasets frequently change over time, usually by having new data appended. One example for such changing dataset is Google?s inverted search index: webpages are added, updated, and deleted all the time. Still, the ratio of changed data is typically only a small fraction compared to the total dataset size [15]. Using the standard Hadoop MapReduce framework, one has to process all the web pages again in order to update a single page in the inverted search index. Indeed, this is not suitable for large datasets as it requires a lot of computing resources and also time until the new search index becomes available.

TABLE I Example Data for running Example  (a) Input to the first-job (SALES) id category price  r1 100 b 4 r2 189 b 6 r3 132 c 2 r4 73 f 9 r5 150 f 9  (b) Appended records (SALES?) id category price  r6 208 g 3 r7 205 c 6  A. Motivation  Let us see through the eyes of a typical Hadoop user (say Alice). Alice is a data analyst at a company and executes queries over several terabytes of data every day. A typical job (say JobAvg) for her could be similar to the following aggregation query: SELECT category, AVG(price) FROM SALES GROUP BY category. Alice runs this query over the data shown in Table I (a). For simplicity, we only consider very few records, but in reality such datasets can have many gigabytes to terabytes of data.

Alice executes JobAvg for the first time on SALES (having several terabytes) and receives the correct output: {(b,5),(c,2),(f,9)}. We denote the first run of a given MapRe- duce job as first-job from here on. After a few days, Alice appends the records in SALES? Table I (b) to the SALES Table I(a). Alice wants to retrieve the correct output: {(b, 5),(c, 4),( f , 9),(g, 3)} by running an incremental-job on the grown dataset. The main problem for appending data in MapReduce is that typically one has to consider again some records of the SALES table when processing the records in the SALES? table. In Alice?s example this would be the record (132, c, 2) in the SALES table. This record belongs to the same category c as the newly appended record (205, c, 6) and hence, Alice has to consider it for computing the correct average for category c. However, using Hadoop MapReduce, Alice has to read the entire SALES table again to recompute the correct average for category c.

B. Idea  To make Alice?s life easier, we propose Itchy (Incremental TeCHniques for Yellow elephants): a MapReduce framework to efficiently process incremental-jobs. The main observation behind Itchy is that different MapReduce jobs require dif-   DOI 10.1109/CLOUD.2013.67     ferent approaches for processing incremental-jobs. Therefore, in contrast to previous works, Itchy provides an optimiser to automatically choose the best technique for performing each incoming MapReduce job.

For MapReduce jobs producing large-size intermediate re- sults or having map functions that are not CPU intensive, Itchy stores a mapping from intermediate keys to input records in the form of Query Metadata Checkpoints (QMCs) [17].

However, in contrast to RAFT [17] that stores QMCs at a map task level (for failover purposes), Itchy stores QMCs at a intermediate key level. Hence, Itchy can recompute a given individual record independently of the map task that originally processed the individual record. Still, the QMCs might be larger than intermediate results themselves. Thus, for MapReduce jobs producing very small-size intermediate results or having CPU intensive map functions, Itchy stores intermediate results. This is different from the memoization approaches presented in [5], [16] in that we keep track of intermediate results at the intermediate-key level rather than at the task level. This allows Itchy to use these intermediate results even though the input to the task has changed.

Itchy can also decide to not store any additional information regarding intermediate results in cases that MapReduce jobs allow for merging final outputs. For example, consider a MapReduce job computing the total revenue of selling a given product. In this case, Itchy can merge the final outputs of the first-job and an incremental-job. As a result, Itchy does not have to process again any record in the map phase or intermediate results in the reduce phase.

C. Research Challenges  The idea behind Itchy triggers many interesting challenges when running a first-job and an incremental-job: (1.) First-Job. Which different kinds of QMCs can be used for improving MapReduce jobs over growing datasets? What is the tradeoff between storing QMCs and intermediate results?

How can we efficiently store QMCs or intermediate results during the first-job without any changes to users? jobs?

(2.) Incremental-Job. How can we efficiently utilize QMCs or intermediate results to recompute only some records of the SALES table? How can we do so with only minimal changes to users? How can we efficiently merge the outputs of the first-job and the incremental-job?

D. Contributions  We present Itchy, a framework to efficiently deal with grow- ing datasets. The main goal of Itchy is to execute incremental- jobs by processing only relevant parts from the input of the first-job (e.g. the SALES table) together with the input of incremental-jobs (e.g. the SALES? table). In summary, we make the following main contributions: (1.) We propose three different techniques to efficiently pro- cess incremental-jobs, namely Itchy QMC (which stores provenance information), Itchy MO (which stores the Map Output, hence intermediate data), and Itchy Merge (which combines the output of MapReduce jobs).

(2.) We show that deciding between Itchy QMC and Itchy MO is basically a tradeoff between storage overhead and runtime overhead. Thus, we present a decision model that allows Itchy to automatically balance the usage of QMCs and intermediate data to improve query performance. Thereby, Itchy can decide the best option for each incoming MapReduce job considering both job runtime and storage overhead.

(3.) We present a framework that implements the Itchy ideas in an invisible way to users including many non-trivial perfor- mance optimizations to process incremental-jobs efficiently.

(4.) We present an extensive experimental evaluation of Itchy against Hadoop MapReduce framework and Incoop.

Our results demonstrate that Itchy significantly outperforms both Hadoop MapReduce and Incoop when dealing with incremental-jobs. We also show that Itchy incurs only neg- ligible overhead when processing the first-job.



II. RelatedWork  MapReduce was originally proposed by Google for build- ing their inverted search index [7]. However, Google faced problems with the incremental nature of their web index and hence moved on to Percolator [15]. Percolator operates on top of BigTable [6] and has departed from the idea of MapReduce.

Most other works, such as Incoop [5] and DryadInc [16], propose the idea of memoization. Here, the idea is to cache outputs of map and reduce tasks. These cached intermediate results can be returned when the same task is later run over the same input. However, these two approaches has to read the entire dataset every time the dataset changes in order to identify the parts of the dataset that changed. DryadINC ad- ditionally proposes the idea of Mergable Computation similar to the idea of Itchy Merge. However, in contrast to DryadINC, Itchy Merge works on output stored in standard HDFS.

Another approach is Restore [10], which considers work- flows of several MapReduce jobs as produced by high- level languages, e.g. Pig [11]. Restore persists the outputs of individual jobs in such a workflow and can reuse these outputs for later jobs in cases where the physical plans match. However, a previous plan (and therefore its stored output) is only considered a match for a new plan if both input datasets are unchanged. Hence, it is not applicable to Alice?s problem of growing datasets. Ramp [13] considers the related problem of selective refresh. Here, the focus is on a particular output record. In other words, given some changed input data, how can we update this single output tuple efficiently? Itchy focuses on a different problem: given a set of appended records, how can we update the entire result set?. Ikeda et al. [12] also explore the concepts of provenance in MapReduce similarly to Itchy and measure the overhead for capturing provenance. However, they do not apply the concept to the idea of incremental computation. CBP [14] implements incremental bulk processing on top of MapReduce by considering stateful grouping operators. Nevertheless, users has to decide and specify which state should be persisted and used.

One can view the result of a MapReduce job as a ma- terialized view on the input data. Therefore, we also con- sider the literature on relational incremental materialized view maintenance [4], [3]. Here, the problem is slightly different: usually the materialized views are specified by declarative SQL queries. Hence, the semantics of queries are known and used for view maintenance. This is not the case for MapReduce, where queries are usually specified by black box map and reduce functions. Still, some of the ideas of relational incremental materialized view maintenance could be applied when working with high-level query languages on top of MapReduce such as HiveQL [20] or PigLatin [11].



III. HadoopMapReduce Recap  Since Itchy is based on Hadoop MapReduce, we discuss the Hadoop MapReduce workflow in more detail.

Let us look in detail at what happens when Alice executes her MapReduce job JobAVG. Before starting her MapReduce job, Alice uploads the SALES table (i.e. Table I (a)) into HDFS. Once her dataset is uploaded to HDFS, Alice can execute JobAVG using Hadoop MapReduce. In turn, Hadoop MapReduce executes JobAVG in three main phases: the map phase, the shuffle phase, and the reduce phase.

During the Map Phase Hadoop MapReduce first partitions the input dataset into smaller horizontal partitions, called input splits. Typically, an input split correspond to an HDFS block.

Then, the Hadoop MapReduce scheduler is responsible of al- locating a map task for each input split to available computing nodes. Once a map task is allocated, the map task uses a Recor- dReader to parse its input split into key-value pairs. For Alice?s MapReduce job, the RecordReader produces key-value pairs in the form (SALES.id;(SALES.category,SALES.price)) for each line of input data. The map task then executes a map-call for each of key-value pair independently. The output of a map-call might be zero or more intermediate key-value pairs. For Alice?s MapReduce job, the map output is in the form (SALES.category;SALES.price). During the Shuffle Phase Hadoop MapReduce partitions the intermediate key- value pairs by intermediate key and assigns each partition to a different reduce task. This means that the intermediate results produced by all map tasks that have the same intermediate key will end up in the same reduce task. Then, during the Reduce Phase the reduce task executes a reduce-call for each group of intermediate key-value pairs. The output of a reduce-call might be zero or more final key-value pairs. Finally, Hadoop MapReduce stores the output of reduce tasks in HDFS.



IV. Itchy  In this section, we present the ideas for efficient incremen- tal processing in Itchy (Incremental TeCHniques for Yellow elephants). In the following, we first discuss the two standard techniques Itchy uses for incremental-jobs: Itchy QMC and Itchy MO. Then, we present how Itchy supports merging different outputs for incremental-jobs. Since Itchy can use all these three techniques for processing incremental-jobs, we thus discuss the tradeoffs among them. Based on these observations,   map() emit(category, price)  MOinc  g,3 c,6  reduce() emit(category, avg(price) )  Differential Output   g,3 c,4  QMCik  b ? r1,r2 c ? r3 f ? r4,r5   map() emit(category, price)  MOadd  c,2   100     b      4 189     b      6 132     c      2 73     f       9 150     f       9  SALES id  category   price  r1 r2 r3 r4 r5  218     g       3 205     c       6  Sale' id category  price  r6 r7  Fig. 1. Incremental-Job using QMCik  we present a model that allows Itchy to automatically decide which of the three techniques to use for a given incoming MapReduce job.

A. Itchy QMC  Alice?s problem is that using Hadoop she has to reprocess the entire terabyte-sized SALES table each time she appends a few records. To address this issue, Itchy exploits the fact that relevant records in the SALES table can be identified by the intermediate key they produce. Indeed, records in the SALES table are only relevant to an incremental-job if they produce an intermediate key that is also produced by some appended record. Therefore, Itchy first processes the appended records using the provided map function. This allows Itchy to obtain the set of intermediate keys IKinc produced by the appended records. Then, Itchy uses a mapping QMCik from each intermediate key ik to offsets in the input to the first-job (i.e., QMCik: ik ? {offsets}) in order to identify those records that are relevant to the newly appended records. Let us first see how Itchy collects the QMCik mapping when running the first-job. First, Itchy processes the SALES table using the user- defined map function as in standard Hadoop MapReduce. At this point, Itchy stores a QMCik mapping for each intermediate key ik. which for JobAvg looks as follows {(b ? r1, r2), (c ? r3), ( f ? r4, r5)}. Then, as in Hadoop MapReduce, Itchy processes the intermediate results using the user-defined reduce function.

Now, let us detail how Itchy processes the appended records in the SALES? table (incremental-job). Figure 1 shows how Itchy processes an incoming incremental-job. Notice that, Itchy uses the QMCiks for identifying the relevant records in SALES. Thus, as a first step, Itchy executes the user- defined map function over the additional records in SALES? and computes the set of relevant intermediate keys IKinc 1 .

In our example, the map output when processing only the records in Sales? MOinc is {(g, 3), (c, 6)} and IKinc is {g, c}.

Then, Itchy retrieves the associated offsets from the QMCik for each intermediate key ikx in IKinc 2 . Notice that, the intermediate key g ? IKinc is not part of QMCik, because g is not produced by any record in SALES. In contrast, the record (132, c, 2) in the SALES table produces the intermediate key c, which is in IKinc. Therefore, Itchy retrieves only the relevant offset {r3} from QMCik. This means that Itchy additionally has to process the record at position r3 in the SALES table.

Itchy processes this relevant record (132, c, 2) using again the user-defined map function and produces one map output from the original SALES table, MOadd = {(c, 2)} 3 .

Notice that, Itchy post-filters the MOadd output so as to consider only relevant intermediate keys in the reduce phase as the user-defined map function might produce several inter- mediate keys for a single record. At this point, we already have all the intermediate results required for the correct result. Thus, Itchy just has to execute the user-defined reduce function over MOinc ? MOadd = {(g, 3), (c, 6), (c, 2)} 4 . As a result, the reduce phase yields the correct differential output: {(g, 3), (c, 4)}.

B. Itchy MO  As discussed in the previous section, Itchy QMC recom- putes some records from the SALES table when processing incremental-jobs. This is not suitable for jobs having ex- pensive map functions. An alternative solution for these cases is to store the intermediate values directly instead of the offsets. The set of all mappings from intermediate keys to intermediate values is the map output (MO) itself, i.e. ik ? {intermediate values}. In our running example, the map output contains the following values for the SALES table: MObase = {(b ? {4, 6}), (c ? {2}), ( f ? {9, 9})}. Then, when processing the appended records in SALES?, Itchy retrieves the required intermediate values from MObase.

Like Itchy QMC, Itchy MO first computes the output of the map phase for the records in SALES?, i.e. MOinc = {(g, 3), (c, 6)} (line 2). Next, Itchy retrieves the relevant in- termediate values from MObase, previously computed by the first-job. Recall that the intermediate key g was not produced by any record in SALES. Thus, Itchy MO only retrieves the intermediate value (c, 2) from MObase and add it to MOadd. After this operation, MOadd is equal to {(c, 2)}. Then, Itchy can execute the user-defined reduce function over the MOinc? MOadd = {(g, 3), (c, 6), (c, 2)}. As a result, Itchy again yields the correct differential output: {(g, 3), (c, 4)}.

It is worth noticing that Itchy stores the intermediate results based on the intermediate keys. In contrast, other works such as Incoop [5] perform map task memoization, which means that they store the map task output and retrieve it based on the input split (i.e. input to the map task). Memoization requires all tasks from incremental-jobs to have exactly the same input as previous tasks from the first-job to be able to reuse previously computed results. Itchy can selectively reuse parts of the previously computed results having the required intermediate keys.

C. Itchy Merge  Let us now focus on how Itchy deals with jobs where users specify a Merge function. Note that JobAvg is not suitable for merging as we cannot compute the av- erage of independently computed averages without addi- tional information. Therefore we instead consider the follow- ing query JobSum.: SELECT category, SUM(price) FROM   map() emit(category, price)  MOinc  g,3 c,6  reduce() emit(category, avg(price) )  Temporary Output   g,3 c,6   218     g      3 205     c       6  Sale' id category  price  merge(initial, inc) return initial + inc  QMCout  b ? o1 c ? o2 f ? o3  Base Output  b,10 c,2 f,18 Merged  Output  g,3 c,8    r6 r7  o1 o2 o3    Fig. 2. Incremental-Job using QMCout  SALES GROUP BY category. Here, Itchy can merge the out- put from processing only the appended records with the output of the first-job using a user-defined merge function. For JobSum, this merge function simply adds, for each intermedi- ate key, the results in the differential output with the results in the base output.

One problem here is to identify the parts in the base output to merge with while processing the appended records.

Therefore, Itchy stores some extra information (QMCouts) to identify the relevant parts in the base output. QMCout is a mapping from intermediate keys to offsets in the base output (i.e. ik ? {offsets}). When running JobSum for the first time (i.e. the first-job) Itchy first computes the base output as normal Hadoop and persists the QMCout information when writing the base output to HDFS.

Figure 2 shows Itchy?s workflow when processing the appended records in SALES? (incremental-job). First, Itchy again uses the user-defined map 1 and reduce 2 functions to obtain the temporary output. Next, for each intermediate key in MOinc, Itchy retrieves the offset of the output record that was produced by the same intermediate key 3 . For example, consider the intermediate key c in the incremental- job: As the intermediate key c also occurred during the first- job, Itchy retrieves the entry (c ? o2) from QMCout. Next, Itchy directly retrieves the record ((c, 2)) at the relevant offsets (o2) in the base output 4 . Itchy then uses the user-defined merge function to merge the retrieved records from the base output and the temporary output 5 . In our example, the merge function receives the record (c, 2) from the base output and the records (g, 3) and (c, 6) from the temporary output as input. The merge function then computes the sum outputs the merged output records (g, 3) and (c, 8) in a differential file.

D. Decision Model  At this point, the reader might have one natural question in mind: which of these three techniques (Itchy QMC, Itchy MO, or Itchy Merge) should Itchy use for a given job? As Itchy Merge requires a user-defined merge function, let us first look at the main differences between Itchy QMC and Itchy MO.

On the one hand, Itchy QMC implies that we have to recompute values, while Itchy MO allows us to simply retrieve those values. On the other hand, the intermediate results stored by Itchy MO can become quite large, while Itchy QMC only requires storing the intermediate key and an offset of 12 bytes size. Thus, Itchy has to trade storage overhead with recompu- tation time. One advantage of Itchy QMC is that the storage     Algorithm 1: Itchy?s Decision Model if isMergeFunctionSpecified() then1  Itchy Merge2 else if avgValSize <=sizeof(QMC ENTRY) then3  Itchy MO4 else5  intermediateResultSize = avgValSize * avgSelectivity * #input records;6 if (avgValSize ) / avgRuntime < threshold then7  Itchy MO8 else9  Itchy QMC10  size overhead remains constant despite the number of attributes in the reduce phase (i.e the size of intermediate results). Itchy MO stores the actual intermediate data and hence the storage size overhead scales with the number of included attributes.

The second factor impacting Itchy?s tradeoff is the time for recomputing relevant records, which mainly depends on the average map runtime. The longer it takes the map function to process one record, the higher is the runtime overhead for recomputing relevant records in Itchy QMC. In contrast, Itchy MO avoids such recomputation costs by directly retrieving the respective values.

In contrast to both Itchy QMC and Itchy MO, Itchy Merge requires neither to recompute relevant records nor to store intermediate results. Thus, Itchy Merge seems to be the best option for processing incremental-jobs in cases where the user specifies a merge function. However, it is not always possible to provide a merge function, e.g. for a join query. This is the reason why Itchy uses all three techniques and includes a decision model to choose among them.

Algorithm 1 outlines the decision model used by Itchy.

Overall, Itchy decides to use Itchy Merge whenever users specify a merge function. Otherwise, Itchy decides between Itchy QMC and Itchy MO based on the measurements average value size, average map selectivity and average map runtime.

Itchy measures these values when processing the first wave of map tasks.

E. Applicability to other MapReduce Jobs  So far, we discussed the applicability of Itchy to individual single relational queries. Naturally, the question that arises is how do Itchy?s ideas extend to more more complex MapReduce jobs having multiple relations as input? In this respect, Itchy can process MapReduce jobs having multiple relations without any alterations. Itchy MO and Itchy Merge work with the intermediate results or the final output respectively and hence are not affected by having multiple input relations. Also, since Itchy QMC stores a file identifier along with each offset, it can handle multiple input relations.

Another question that the reader might have is how does Itchy support multistage MapReduce jobs, i.e., consisting of several individual MapReduce jobs? Here, Itchy faces the problem that usually in-between-job results are deleted after a job is finished. Hence, Itchy QMC can only access the input data for the first job and Itchy Merge can access only the output of the last job. In contrast, Itchy MO persists the intermediate results for each job. Therefore, Itchy MO uses  the differential output of a stage as the incremental input to the next stage until it outputs the final differential output.



V. Implementation  A. Itchy QMC  Processing the first-job. While processing the first-job, Itchy has to store the QMCik mappings to HBase for each interme- diate key. Therefore, Itchy QMC concatenates the offsets at the end of the intermediate values during the map phase and utilises the shuffle phase to piggy back the offsets on top of the data sent from the map tasks to the reduce tasks. When deserializing the intermediate values before calling the reduce function, Itchy extracts the offsets from the intermediate val- ues. In this way Itchy has all offsets for a given intermediate key and can store them in a single HBase storage operation.

Processing the incremental-job. When processing incremental-jobs, Itchy uses the QMCik mappings to identify the records in the base input as discussed in Section IV. To do so, Itchy has to retrieve the relevant offsets from the QMCik mappings and then process the appended records together with the relevant records in the base input.

Itchy processes the appended records and parts of the base input in a single MapReduce job. This is challenging as Itchy only knows which parts of the original input it has to process after processing all appended records. In the standard Hadoop MapReduce framework all map tasks are scheduled independently and there is no control for such scheduling conditions. To deal with this requirement, Itchy introduces the concept of map units. The idea is to assign each map task to a map unit which are then executed in strictly sequential order. During the map phase Itchy only schedules those tasks that belong to the current map unit.

Only after all tasks of a map unit have finished, Itchy moves to the next map unit. In particular, Itchy assigns all map tasks processing the appended records to map unit 1 and all map tasks processing relevant records from the base input to map unit 2. Notice that, the concept of map units is independent of the scheduling policy (such as the LATE [22] scheduler) used by Hadoop as map units are implemented on a per job basis (i.e. in the JobInProgress class).

Then, when a map task processing the appended records (in map unit 1) finishes, Itchy looks up the offsets in HBase for the intermediate keys produced by this map task. One problem here is that several map tasks can produce the same intermediate key ikx. Thus, looking up the offsets for all intermediate keys in every map task can lead to a large number of unnecessary HBase lookups. To reduce the number of unnecessary HBase calls, Itchy caches the set of already seen intermediate keys between several map tasks running in the same JVM. Itchy then sorts the retrieved offsets and partitions them into HDFS files according to the blocks of the base input.

When Itchy starts the second map unit, it should schedule only map tasks for those blocks containing at least one relevant offset; the other map tasks point to blocks containing only irrelevant records. However, Hadoop MapReduce (and     therefore Itchy as well) creates map tasks when initialising a MapReduce job and Itchy does not know the relevant records till it finishes executing all map tasks in map unit 1. Therefore, Itchy will unschedule all map tasks that are not required meaning directly marking a map task as finished without executing it. For scheduled map tasks in map unit 2, Itchy reads, merges, and sorts all files containing relevant offsets.

With this sorted set of offsets, the RecordReader of a map task can pass only relevant records to the map function. Notice that, as a map-call might emit several intermediate keys for one record, the map function can emit intermediate keys that are not produced by the appended records. These intermediate keys are not required by reduce tasks to produce the correct results. In fact, these intermediate keys can actually lead to incorrect results for the such intermediate keys. Therefore, Itchy applies post-filtering of the not required intermediate keys at the end of each map task in map unit 2.

B. Itchy MO  Processing the first-job. In case of storing map outputs, Itchy has to persist the intermediate results while processing the first-job. The intermediate results are grouped and partitioned during the shuffle phase by default. Thus, the only difference from the standard Hadoop workflow is that Itchy persists intermediate results to HBase before starting each reduce task.

Processing the incremental-job. When processing the incremental-job, Itchy uses only the appended records as input.

Itchy retrieves the required intermediate values just before each reduce-call. Notice that, each reduce-call is responsible for a single intermediate key. Therefore, Itchy requires only one HBase query to retrieve the intermediate values per inter- mediate key. Itchy appends the retrieved intermediate values to the intermediate values produced by the appended records.

The reduce function processes all these intermediate values as in the standard Hadoop MapReduce workflow.

C. Itchy Merge  In contrast to Itchy QMC and Itchy MO, Itchy Merge requires users to specify a merge function.

Processing the first-job. Itchy Merge processes the first-job almost as in the standard MapReduce framework. Except at the point where reduce tasks emit the output: Itchy additionally persists the QMCout mappings to HBase. Recall, that QMCout is a mapping from intermediate key to offset in the output.

Processing the incremental-job. When processing the incremental-job, Itchy again behaves as the standard Hadoop MapReduce workflow up to the point where reducers emit the final output. Here, Itchy retrieves the offsets in the base output using QMCout for each intermediate key. Then, Itchy reads the corresponding parts from the base output at those offsets. Next, Itchy merges the retrieved parts of the base output with the just produced temporary output. Finally, Itchy writes the merged differential output to HDFS.



VI. Experiments  A. Experimental Setup  Hardware. We use a local 10 node cluster where each node has one 2.66GHz Quad Core Xeon processor, and 4x4GB of main memory. The advantage of running our experiments on this local cluster, rather than on the cloud, is that the amount of runtime variance is limited [18].

Datasets. We use two different datasets: Lineitem data as specified in the TPC-H benchmark [2] and Wikipedia dumb files as described by [1]. We generate 300GB of base input and 1GB of incremental input for both datasets.

Systems. We use Hadoop [7] and Incoop [5] as baselines.

We evaluate Itchy in all its three modes to operate, namely: Itchy QMC, Itchy MO, and Itchy Merge (where applicable).

MapReduce Jobs. We use two different jobs for lineitem.

The first job (Job1) is similar to Alice?s query, while the second job (Job2) is based on the first query of TPC-H [2].

Job1 (in SQL) SELECT l_partykey, AVG(l_extendedprice) FROM lineitem GROUP BY l_partid  Job2 (in SQL) SELECT l_returnflag, l_linestatus, sum(l_quantity), sum(l_extendedprice), sum(l_extendedprice*(1-l_discount)), sum(l_extendedprice*(1-l_discount)*(1+l_tax)), avg(l_quantity), avg(l_extendedprice), avg(l_discount), count(*) FROM lineitem WHERE l_shipdate <= date ?1998-12-01? GROUP BY l_returnflag, l_linestatus  For the Wikipedia dataset, we use the WordCount job, which is frequently used for benchmarking MapReduce systems [7], [5]. For all experiments, we run the jobs three times and report the average.

B. Upload Time  The first thing users have to do to run their MapReduce jobs is to upload their datasets to HDFS. In this respect, Itchy works in the same way as Hadoop. We observed that both Hadoop and Itchy are ? 1.6 times faster than Incoop when uploading 300GB of Lineitem data. Incoop?s overhead is due to the content-wise splitting of input data (which minimizes the number of changed input splits in case a small part of the dataset changes). Therefore Incoop identifies certain markers in the input data using Rabin fingerprints and then places the split boundaries at those markers. In contrast, Hadoop and Itchy generates fixed length input splits.

C. First-job performance  In this section, we measure Itchy?s overhead when running the first-job. Figure 3(a) illustrates the execution times for Job1 and Job2 when processing 300GB of Lineitem input data. As expected, we observe that both Itchy and Incoop incur some overhead over the execution time of Hadoop as they have to store additional information. In particular, we observe that Itchy QMC has an overhead of ?29% for Job1 and overhead          Job 1 Job 2   30462907 3098  2420 26472255  Av er  ag e  Jo b  Ru nt  im e  [se c]  Hadoop Incoop Itchy QMC Itchy MO Jobs  (a) 300GB of Lineitem data.

Av er  ag e  Jo b  Ru nt  im e  [se c]  Hadoop Incoop Itchy QMC Itchy MO Itchy Merge  Systems  (b) 300GB of WordCount data.

Job 1 Job 2  154123 140146    Av er  ag e  Jo b  Ru nt  im e  [se c]  Hadoop Incoop Itchy QMC Itchy MO Jobs  (c) 1GB of Lineitem data.

Av er  ag e  Jo b  Ru nt  im e  [se c]  Hadoop Incoop Itchy QMC Itchy MO Itchy Merge  Systems  (d) 1GB of WordCount data.

Fig. 3. Job Runtimes: (a)-(b) execution times for running the first-job; (c)-(d) execution times for running the incremental-job.

2 3 5 10     Av er  ag e  Jo b  Ru nt  im e  [se c]  Hadoop Itchy QMC Itchy MO Number of attributes in intermediate results  (a) First-job.

2 3 5 10  186161152123 151147151146   Av er  ag e  Jo b  Ru nt  im e  [se c]  Hadoop Itchy QMC Itchy MO Number of attributes in intermediate results  (b) Incremental-job.

2 3 5 10  12404115031098410483 10789108731082410803  g y  St or  ag e  Ov er  he ad  [M B]  Itchy QMC Itchy MO Number of attributes in intermediate results  (c) Storage overhead w.r.t. Hadoop.

Fig. 4. Tradeoff between QMC and MO while varying the size of intermediate values for Lineitem.

of ?15% for Job2. We observe that Itchy QMC incurs a higher overhead than Incoop for Job1, but it has a lower overhead for Job2. Also Itchy MO has a lower overhead (?10%) than Itchy QMC for Job1 and a higher overhead (?28%) for Job2.

This is because the intermediate data size for Job1 is smaller than the QMC information, while the intermediate data size for Job2 is bigger. Next, we observe that Itchy MO has roughly the same performance as Incoop for Job1, but Itchy MO performs slightly slower than Incoop for Job2. The reason is that Itchy MO stores intermediate data at the intermediate key level while Incoop does it at the task level.

Figure 3(b) shows the execution times for WordCount when processing 300GB of Wikipedia base input data. On the one hand, we observe that Itchy QMC incurs an overhead of ? 23% over Hadoop, but Itchy QMC is slightly faster than Incoop. On the other hand, we observe that Itchy MO is ?1.6 slower than Hadoop and ?1.2 slower than Incoop.

This is because the WordCount job produces a large number of intermediate keysItchy Merge performs 1.22 faster than Incoop, 1.13 faster than Itchy QMC, and 1.5 faster than Itchy MO. Still, Itchy Merge is slightly slower than Hadoop, because it has to store the mapping of intermediate keys to offsets in the output (QMCout) to HBase.

D. Incremental-job performance  We now evaluate the performance of Itchy when performing incremental-jobs. For this, we append the Lineitem and Wikipedia datasets with 1GB new data each. Notice that, for Itchy and Hadoop we only have to upload the 1GB of new records, while for Incoop we have to upload a 301GB dataset including all input from the first-job. This is because, Incoop can currently handle only a single file as input to a job while Itchy and Hadoop can use several files as input to a job.

Figure 3(c) shows the results for Lineitem. We observe that Hadoop performs quite similar as when running the first-job.

This is because Hadoop has to reprocess the entire 300GB base input in addition to the 1GB of appended records. In contrast, Itchy does not have to recompute the entire 300GB  base input. As a result, we observe that Itchy is up to ?20 times faster than Hadoop and ?10 times faster than Incoop.

Also, we observe that the performance gap between Itchy and both Hadoop and Incoop is higher for Job2, as Job2 produces larger intermediate data. This is also why Itchy QMC also slightly outperforms Itchy MO for Job2.

Figure 3(d) illustrates the results for WordCount when processing 1GB of new Wikipedia data. In these results, we plot the results for Itchy Merge as WordCount is in the Merge class. Again, Hadoop reprocess the entire 300GB input to the first-job in addition to the 1GB appended records.

As a result, Hadoop has a similar performance as when performing the first-job. For Itchy, we observe that Itchy QMC and Itchy MO are ?4.6 times faster than Hadoop and ?1.3 times faster than Incoop. However, recall, that Incoop requires users to upload the entire 301GB of data while Itchy only requires users to upload the 1GB of new records. Here, the reader can observe that the relative performance difference between Incoop and Itchy QMC and Itchy MO is not as large as in the Lineitem benchmark. This is because the WordCount incremental-job requires many intermediate values to be recomputed as frequent word often occur in a large number of documents. In contrast, Itchy Merge exploits the fact that WordCount performs a sum of frequencies per word to simply add the incremental output with the base output.

This results in an improvement factor of ?41 over Hadoop and of ?11 over Incoop.

E. Varying intermediate value size  Next, we study the tradeoff between Itchy QMC and Itchy MO. Recall that Itchy QMC requires a higher cost for recomputing some records from the first-job, while Itchy MO might require a lot of space for storing the intermediate values.

For this, we alter Job1 to include additional attributes in the intermediate values in order to vary the intermediate value size.

Figure 4(a) shows the results for the first-job when varying the intermediate value size. We observe that the job runtime for both Hadoop and Itchy QMC does not increase by much when     increasing the intermediate value size. In contrast, Itchy MO incurs a 25% increase in runtime from projecting 2 attributes to projecting 10 attributes. This is because Itchy MO cost for storing the MO mappings strongly depends on the of intermediate values, while Itchy QMC cost for storing the QMC mappings is independent of the intermediate value size.

Figure 4(b) shows the results for the incremental-job when varying the intermediate value size. Here, we observe that the job runtime for all three systems does not vary much. This is because the number of intermediate values is far less than for the first-job.

Next, we consider the other part of the tradeoff, i.e. the storage overhead. Recall that both systems have to store additional information: while Itchy QMC stores QMC map- pings, Itchy MO stores the intermediate results. Figure 4(c) illustrates the storage overhead incurred by both systems when varying again the size of intermediate values. As expected, we observe that the overhead incurred by Itchy QMC does not vary with respect to intermediate value size. This is because Itchy QMC stores offsets and hence the QMC storage does only depend on the number of intermediate keys and not on the size of intermediate values. In contrast, Itchy MO stores the intermediate values directly and hence we can see the storage size for MO increase when including additional attributes in the intermediate values. Notice that, for 2 attributes the storage size for Itchy QMC is actually larger than for Itchy MO. This is due the fact that Itchy QMC has to store the filename identifier and offset which is larger than the size of the two attributes Itchy MO has to store as intermediate value. However, as soon as Job1 start to project more attributes, the overhead incurred by Itchy MO starts getting larger than Itchy QMC.

F. Effectiveness of Itchy?s decision model  Let us now consider how well the Itchy?s decision model deals with the different benchmarks. For Job1 we observe that the average intermediate value size is as large as the size for storing a QMC mapping. Therefore, the decision model decides to use Itchy MO as for the same storage requirements it expects a better performance for the incremental-job. As we can see from Figure 3(c) this assumption holds true. For Job2, we see that the average intermediate value size is much larger as it contains more and larger attributes. In such case, the decision model considers the ratio avgValS izeavgRuntime . Again, we observe in Figure 3(c) that this decision holds true. Since this ratio is above the given threshold, Itchy decides to use the Itchy QMC technique. For the WordCount job Itchy decides to use Itchy Merge as the user has defined a merge UDF. From Figure 3(d), we observe that this decision allows Itchy Merge to significantly outperform not only Hadoop and Incoop, but also Itchy QMC and Itchy MO.



VII. Conclusion  Many current applications produce large amounts of data in a daily basis that has to be analysed as new data arrive.

In this paper, we presented Itchy (Incremental TeCHniques for Yellow elephants) for supporting MapReduce jobs over  growing datasets. As we identified that different MapReduce jobs require different approaches to deal with incremental datasets, Itchy contains a suite of three different techniques: Itchy QMC, Itchy MO, and Itchy Merge. We presented a cost-based decision model that allows Itchy to automatically chooses the right technique between those three technique for each MapReduce Job. We evaluated Itchy using two different benchmarks and compared it with Hadoop (the most popular MapReduce framework) and Incoop (a state-of-the-art system to deal with incremental datasets). The results show the superiority of Itchy for processing incremental jobs: Itchy runs up to ?20 times faster than Hadoop and up to ?10 times faster than Incoop. In cases where users can specify a merge function, Itchy can improve the performance up to a factor of ?41 compared to Hadoop up to a factor of ?11 compared to Incoop. A series of additional experiments also showed the high effectiveness of the Itchy decision model.

Acknowledments: We would like to thank the authors of Incoop for providing us access to their prototype.

