Detecting Anomaly Teletraffic Using Stochastic Self-similarity Based on Hadoop

Abstract?In recent years, the quantity of teletraffic is rapidly growing because of the explosive increase of Internet users and its applications. The needs of collection, storage, management, analysis, and measurement of the subsequent teletraffic have been emerged as one of very important issues.

So far many studies for detecting anomaly teletraffic have been done. However, measurement and analysis studies for big data in cloud computing environments are not actively being made based on Hadoop. Thus, this paper presents for detecting anomaly teletraffic using stochastic self-similarity based on Hadoop. All simulations are conducted under control of our proposed platform, called ATM tool, for anomaly teletraffic intrusion detection system on Hadoop. Our numerical results show that the values of the estimated Hurst parameter obtained from the anomaly teletraffic are much higher when compared to ordinary local area network traffic.

Keywords-Anomaly teletraffic detection; stochastic self- similarity; Hurst parameter; Hadoop; big data; cloud com- puting

I. INTRODUCTION  The number of users of the Internet and mobile devices such as smart phones and tablets has increased greatly in recent years. They could lead to the explosive growth of the amount of teletraffic. Resulting in exponentially growing amount of data, ?big data era? that these data become an economic asset has arrived there. In order to successfully and effectively use of big data in information and communi- cation, education, health care and financial services, we need to consider three major components: resources, technology for data processing and analysis, and experts who gain an insight into the meaning of the data. We focus on the second element of the data processing and analysis technology to detect anomaly teletraffic using stochastic similarity on our proposed Hadoop-based platform.

Hadoop is basically a large scaled data processing system operated by a distributed computing platform in which distribution is a core frame (concept) [1]. Hence, Hadoop efficiently enriches data storage spaces and computation capability based on parallel processing and it can enable implementation of the MapReduce programming model for cloud computing. The core fraction of Hadoop is a dis- tributed file system.

Meanwhile, the explosive growth of teletraffic for user services threatens the current networks and we face menaces from various kinds of intrusive incidents through the Inter- net. A variety of network attacks on network resources such as DDoS (Distributed Denial of Service) and PDoS (Perma- nent DoS) have continuously made serious damage [2], [3], [4]. Therefore, active and advanced technologies for early detecting of anomaly teletraffic on Hadoop-based platforms should be understood. In this paper, we have developed an automated Anomaly T eletraffic detection Measurement analysis tool, called ATM tool. Some problems and so- lutions for those systems have been also suggested. All simulations were executed under control of the ATM tool to investigate how input anomaly teletraffic with DDoS attacks can be different from ordinary local area network (LAN) traffic.

The rest of this paper is organized as follows: Section II presents a brief overview of Hadoop and anomaly tele- traffic intrusion detection systems. Section III introduces self-similar processes and estimation techniques for detect- ing anomaly teletraffic using stochastic self-similarity on Hadoop. A survey of some problems and technical solutions for anomaly intrusion detection systems on Hadoop has been investigated and a new platform has been proposed in Section IV, and some simulation results are illustrated in Section V. Finally, Section VI summarizes conclusions.



II. RELATED WORKS AND TECHNOLOGIES  Related works and technologies for detecting anomaly teletraffic using stochastic self-similarity are Hadoop and anomaly teletraffic intrusion detection system as follows.

A. Hadoop Appreciation  Hadoop is an open-source software framework that sup- ports data-intensive distributed applications. It enables ap- plications to work with thousands of computationally in- dependent computers and with petabytes of data [5], [6] . Hadoop was developed for the distributed web search and derived from MapReduce, a distributed system, and Google File System, a distributed file system. Essential components of Hadoop are the distributed storage (HDFS,   DOI 10.1109/NBiS.2013.43     Figure 1. Hadoop architecture.

Hadoop Distributed File System) and the distributed pro- cessing (MapReduce). Hadoop increases the storage space and the processing power by uniting many computers into one.

A small Hadoop cluster will include a single master and multiple worker nodes (slaves) as in Figure 1. The master node consists of a Job Tracker, TaskTracker, NameNode and DataNode. A slave or worker node acts as both a DataNode and TaskTracker. In a large cluster, HDFS is managed through a dedicated NameNode server to host the file system index and a secondary NameNode that can generate snapshots of the NameNode?s memory structures, thus preventing file system corruption and reducing loss of data.

1) Hadoop Distributed File System: HDFS is a dis- tributed, scalable, and portable file system written in Java for the Hadoop framework. Each node in a Hadoop instance typically has a single DataNode. A cluster of DataNodes form the HDFS cluster that has a NameNode in a role of metadata server. A NameNode performs various utilities of the file system namespace such as open, close, rename and etc. for files and directories.

A file is divided into more than a block that is stored in a DataNode and HDFS determines mapping between DataNodes and blocks. DataNodes run operations such as read and write that file system clients require. Since HDFS is written in Java, any Java-operating computers run Na- meNode or DataNode software, which saves the huge cost at the hardware architect phase and the management phase.

HDFS is a common file system in Hadoop but is not the only file system. For example, Elastic Computer Cloud (EC2) of Amazone uses S3, its own file system instead of HDFS. Brisk of DataStax uses Cassandra File System that includes data query and analytic functions for integration of real-time data storage and analysis.

2) MapReduce: MapReduce is a software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner [2].

Above the file systems come the MapReduce engine, which consists of one JobTracker, to which client applica- tions submit MapReduce jobs. The JobTracker pushes work out to available TaskTracker nodes in the cluster, striving to keep the work as close to the data as possible. With a rack-aware file system, the JobTracker knows which node contains the data, and which other machines are nearby. If the work cannot be hosted on the actual node where the data resides, priority is given to nodes in the same rack.

This reduces network traffic on the main backbone network.

If a TaskTracker fails or times out, that part of the job is rescheduled. The TaskTracker on each node spawns off a separate Java Virtual Machine process to prevent the TaskTracker itself from failing if the running job crashes the JVM. A heartbeat is sent from the TaskTracker to the JobTracker every few minutes to check its status. The Job Tracker and TaskTracker status and information is exposed by Jetty and can be viewed from a web browser.

A limitation of Hadoop is that it cannot be directly mounted to an existing operating system. Getting data into and out of the HDFS file system, an action that often needs to be performed before and after executing a job can be inconvenient. Another limitation is that Hadoop is not efficient for sub-second data reports and frequent data changes. Hadoop is less effective than RDBMS where frequent data insertion, deletion and update occur and the complete multi-step transaction is used for data processing.

An advantage of Hadoop is scalability, which Yahoo and Facebook show very well. Another advantage is cost- effectiveness. A lot of data cause an intensive amount of RDBMS data storage by keeping raw data in isolation.

Hadoop considers the significance of raw data and processed data equally.

B. Anomaly Teletraffic Intrusion Detection System  An Intrusion Detection System (IDS) is a security tool, and the goal is to strengthen the security of information and communication systems by monitoring network or system activities, identifying intrusions, and making reports [7], [8], [9]. In terms of analysis technologies, IDSes can be classified into two types: Signature-based Teletraffic In- trusion Detection System (ST-IDS) 1 and Anomaly-based Teletraffic Intrusion Detection System (AT-IDS). ST-IDSes apply patterns of known attacks to find attacks. For this approach, it is necessary to build a signature database expressing known attacks in advance. However, ST-IDSes  1ST-IDS is also called as Misuse-based Teletraffic Intrusion Detection System.

Figure 2. Techniques for anomaly teletraffic intrusion detection.

are not effective against new or unknown attacks in spite of their efficiency. On the other hand, AT-IDSes decide if an attack happens by checking if the deviation between a given event and the normal behavior is greater than a pre- defined threshold. It is worthwhile to note that this approach can detect previously unseen attacks. In considering the characteristics of the Hadoop framework, we focus on the AT-IDS approach for its security in this paper.

Anomaly detection technologies in Figure 2 can be categorized into three types: statistical-based, knowledge- based, and machine learning-based approaches [7]. First, the statistical-based approach obtains the system or network traffic activity, and then generates a profile corresponding to its stochastic behavior. Through this profile, observed events are checked by the deviation between their behavior and the normal one. This approach involves univariate, multivariate, time series, and self-similar models. Second, in the knowledge-based approach, audited events are clas- sified based on the pre-defined rules such as knowledge.

This approach includes finite state machines, description languages, and expert systems. Finally, the last approach, including Markov models, genetic algorithms, neural net- works, Bayesian networks, fussy logic, and clustering and outlier detection, applies the machine learning method to de- tect attacks. Figure 2 and Table I show the three approaches and their related technologies. For details, also see [7].



III. SELF-SIMILAR STOCHASTIC PROCESSES  Self-similar (or fractal) long-range dependent stochastic processes occur in many natural and man-made systems.

In particular, since such processes were discovered in the Internet and other multimedia telecommunication networks more than a decade ago ([10], [11]) they have become the subject of numerous research investigations of their nature and consequences [4].

A. Definitions of Self-similar Stochastic Processes  Theoretically, one can distinguish two types of stochas- tic self-similarity. A continuous-time stochastic process Yt={Yt1 , Yt2 , . . .} is strictly self-similar with Hurst param- eter, H, 0.5 < H < 1, if Yct and cHYt (the rescaled process with time scale ct) have identical finite-dimensional distribution for any stretching factor c, c > 0, [12], [13], [14]. This means that, for any sequence of time points t1, t2 . . . tn,  {Yct1 , Yct2 , . . . , Yctn} d= {cHYt1 , cHYt2 , . . . , cHYtn}, (1) where d= denotes equivalence in distribution. Thus, a strictly self-similar process (or ?self-similar in a narrow sense?) is self-similar in the sense of probability distribution.

Weak self-similarity of stochastic processes (or ?self- similarity in a broad sense?) becomes relevant when one restricts analysis of stochastic processes to their first two moments only , i.e. to their means, variances and co- variances. Because of that, it is also known as second-order self-similarity. For example, let a sequence {X1, X2, . . .} be a time-stationary stochastic process, defined at discrete time instances i = 1, 2, 3 . . .. Let E[Xi] = EX , V ar[Xi] = V arX , and ?k = E[(Xi ? EX)(Xi+k ? EX)]/V arX denote its mean, variance and autocorrelation coefficient of lag k, respectively.

Having grouped this sequence of random variables into batches of size m, m ? 1, we can define the aggregated pro- cess of X(m) = {X(m)1 , X(m)2 , ? ? ?}, at a given aggregation level m, where X(m)i =  m (Xim?m+1 + ? ? ?+Xim), i ? 1.

If {X1, X2, . . .} is a weakly self-similar time-stationary process, with 0.5 < H < 1, then  V ar[X (m) i ] = m  2H?2V arX, (2)  and ? (m) k = ?k, k ? 0, (3)  where ?(m)k is the autocorrelation coefficient of lag k of the aggregated process X(m). Equation (3) means that the original process and its aggregated version have an identical correlation structure. Further, it can be proved that in any weakly self-similar process with 0.5 < H < 1,  ?k ? H(2H ? 1)k(2H?2), (4) as k ? ?, and  k=+??  k=?? ?k = ?, (5)  see for example [13]. Thus the autocorrelation function decays very slowly (hyperbolically) with k. Because of these properties, these processes are also known as long-range dependent, or strongly correlated.

The Hurst parameter H , known as the self-similarity pa- rameter, is the simplest numerical characteristic of stochastic     Table I ADVANTAGES AND DISADVANTAGES OF THE AT-IDS TECHNIQUES  Approaches Techniques Advantages and Disadvantages - Univariate models - Prior knowledge about normal activity not required - Multivariate models - Accurate notification of malicious activities  Statistical-based - Time series models - Susceptible to be trained by attackers - Self-similar models - Difficult setting for parameters and metrics  - Unrealistic quasi-stationary process assumption - Finite state machines - Robustness  Knowledge-based - Description languages - Flexibility and scalability - Expert systems - Difficult and time-consuming availability  for high-quality knowledge/data - Markov models - Flexibility and adaptability - Genetic algorithms - Capture of interdependencies  Machine learning-based - Neural networks - High dependency on the assumption about - Bayesian networks the behavior accepted for the system - Fussy logic - High resource consuming - Clustering and outlier detection  self-similarity and long-range dependence. Because of that, typical quantitative studies of self-similarity and long-range dependencies, either on data traces recorded in existing networks or on data samples collected during computer simulations of networks, begin with estimation of H [12], [15], [16].

B. Estimation Methods  The degree of self-similarity for anomaly teletraffic was assessed by applying the currently available techniques [17].

These include:  ? R/S-statistic plot: used to estimate the Hurst parameter H from empirical data. An estimate of H is given by the asymptotic slope ??2 of the R/S-statistic plot, i.e., H? = ??2 [12].

? Abry-Veitch DWB: Veitch and Abry [15] proposed the so-called wavelet-based joint estimator, which estimates both H and the power parameter, an independent quantitative parameter with dimension of variance; see [15] and [18] for details. The resulting wavelet-based estimator, further called the Abry-Veitch Daubechies Wavelet-Based (or Abry-Veitch DWB) estimator, is asymptotically unbiased. It is also possible to employ more refined data analysis to obtain confidence intervals for the Hurst parameter H .



IV. SOME PROBLEMS AND PROPOSED PLATFORM FOR AT-IDS  There are some problems and technical solutions for each attribute of big data in the following. Using those technical solutions, we have considered to propose a new platform for AT-IDS on Hadoop [2], [19].

1) Storage volume: It is hard to store a massive volume of data (e.g., Exa Byte, Zetta Byte) in current systems.

Those big data are stored in HDFS and HBase, as a distributed, scalable, and portable file system.

Figure 3. A new proposed platform for AT-IDS on Hadoop.

2) Velocity: We need at least the processing power of supercomputers to get the final results from big data.

A parallel/distributed process framework MapReduce is one of the best frameworks for easily writing applications which process massive amounts of data.

3) Variety: We also consider how to store structured and unstructured data, and various types of data sources such as texts, images, and videos. Non-relational DBMS such as NoSQL saves such types of data.

4) IDS: When considering the characteristics of the Hadoop framework, anomaly-based intrusion detec- tion systems are suitable because they can detect new or unknown attacks.

5) Cost: We normally pay a high cost to implement a framework, but we can implement one with low cost if an open-source framework Hadoop is used.

Figure 3 shows architecture of an automated Anomaly Teletraffic detection Measurement analysis tool, called ATM.

The ATM system consists of four main components to     detect anomaly teletraffic in an LAN environment: collector, storage, analyzer, and graphic user interface (GUI) [3]. The ATM tool was implemented in Java and has been tested at Korean Bible University under the 64-bit Windows 7.

1) Collector module collects web page data, SNS data, and system log data from SNS open API (e.g., Face- book, Twitter), distributed file collection tools, data collection robots, and data aggregators.

2) Storage module is used to store and manage big data in file storages, data storages and structured data storages through filters and real-time analysis.

3) Analyzer module analyzes, clusters, and classifies par- allel and distributed data. This module also plays an important role in contents analysis, descriptive anal- ysis, predictive analysis, natural language processing, text mining, etc. Especially, AT-IDS is implemented to detect anomaly teletraffic using the MapReduce framework.

4) GUI module provides automated statistical analysis of results such as starting and monitoring the system, obtaining information about real-time status of the system, and real-time statistics for AT-IDS.



V. NUMERICAL RESULTS All simulations were executed under control of the ATM  tool to investigate how input anomaly teletraffic with DDoS attacks can be different from ordinary LAN traffic. The ATM tool was used to generate, collect and analyze ten- hour long data including ordinary LAN and anomaly traffic.

The estimates of the Hurst parameter obtained from the R/S-statistic and the Abry-Veitch DWB were used to analyze LAN and anomaly traffic.

? The plot of R/S-statistic in Fig. 4 (b) indicates the asymptotic slope approximates to slope 1, but the slope of the R/S-statistic in Fig. 4 (a) obtained from the LAN traffic is lower than Fig. 4 (b). The estimated Hurst parameters of Fig. 4 (b) and (a), obtained from the R/S- statistic plot, were 0.9014 and 0.8984, respectively.

? The Abry-Veitch DWB estimator provides both an estimated Hurst parameter and a confidence interval.

The results for LAN and anomaly traffic obtained from Abry-Veitch DWB estimator were 0.919 [0.889, 0.949] and 1.248 [1.218, 1.278] 2, respectively; also see Fig.

5. Our numerical results showed that the value of the estimated Hurst parameter for anomaly traffic is 35.8% higher than LAN.

Therefore, our numerical results for anomaly traffic showed that the values of the estimated Hurst parameter,  2First note that ?k = 1/2k2Hg(k?1), where g(x) = (1 + x)2H ? 2 + (1? x)2H . For H > 1, g(k?1) diverges to infinity. This contradicts the fact that ?k must be between -1 and 1 [12]. For anomaly traffic with DDoS attacks, the Abry-Veitch DWB estimator over all other scales shows an unexpected slope of the estimated Hurst parameter of H = 1.248 because of the discontinuity signature and the missing scales phenomenon. It is not even consistent with stationarity. For detailed discussion, also see [20].

obtained from the different estimation techniques, were much higher when compared to LAN traffic.



VI. CONCLUSIONS  The massive volume of data called big data for user services threatens the current networks and we have faced menaces from various kinds of intrusive incidents through the Internet. A variety of network attacks on network re- sources have continuously created serious damage. There- fore, active and advanced technologies for early detecting of anomaly teletraffic on a Hadoop-based platform are required. In this paper, we proposed an automated anomaly teletraffic measurement analysis tool, called ATM , and some problems and solutions for this system have been also investigated. Our numerical results showed that the values of the estimated Hurst parameter obtained from the anomaly teletraffic are much higher when compared to ordinary LAN traffic.

