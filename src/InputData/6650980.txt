Distributing Storage in Cloud Environments

Abstract?Cloud computing has a major impact on today?s IT strategies. Outsourcing applications from IT departments to the cloud relieves users from building big infrastructures as well as from building the corresponding expertise, and allows them to focus on their main competences and businesses. One of the main hurdles of cloud computing is that not only the application, but also the data has to be moved to the cloud.

Networking speed severely limits the amount of data that can travel between the cloud and the user, between different sites of the same cloud provider, or indeed between different cloud providers.

It is therefore important to keep applications near the data itself. This paper investigates in which way load balancing of the computational resources as well as the data locality can be maintained at the same time. We apply recent results from balls-into-bins theory to test their applicability to cloud storage environments. We show that it is possible to both balance the load nearly perfectly and to keep the data close to its origin.

The results are based on theoretical analyses and simulation of the underlying physical infrastructure of the Internet.

Keywords-load balancing; cloud computing; networks; balls- into-bins;

I. INTRODUCTION  Cloud computing offers new opportunities to outsource applications as resources can now be added or removed on- demand. Setting up a set of new servers in an Infrastructure- as-a-Service (IaaS) environment is a matter of minutes. IT departments therefore do not have to design their infrastruc- ture for peak demands, but can offload peak loads to a cloud provider.

In order to optimize the distributed infrastructure one has to dynamically balance the load between on-premise and off- premise machines which becomes especially difficult when data is involved. Moving huge quantities of data between data centers over standard network connections can take a very long time due to the limited bandwidth in wide area networks (WAN). Additionally, the data transfers over WANs can be very costly. The customers should therefore try to keep the data as local as possible in order to maximize the network bandwidth and to minimize the latency between the customer network connection and the cloud provider.

In this paper we present a load balancing approach for cloud computing. We assume that a network of n cloud  centers is given as a graph where the cloud centers are represented by the nodes and the communication links by the edges. The customers generate new storage items or data-intensive computing jobs where each item or job is initially assigned to one of the clouds. The goal is then to distribute the jobs as evenly as possible among all cloud data centers in the network. For this we suggest a load balancing algorithm that balances the load by distributing the jobs in the neighborhoods of the data centers in which they originated. Since customers usually create more than one job, we also consider the scenario in which ?runs? of items / jobs are generated on the same data center.

We model the load balancing problem as a balls-into-bins game where the goal is to distribute a set of balls evenly over a set of bins. In standard balls-into-bins games the balls are allocated to the bins independently and uniformly at random.

In this paper we consider a generalization of the standard model, the so-called ?-balancedness model from [1], [2].

Here it is assumed that every ball i comes with a set Bi of bins and that it allocates itself to a randomly chosen bin of lowest load within Bi. In [1], [2] it is shown that a good load distribution can be achieved if all bins appear in roughly the same number of bin sets Bi (the probability for a bin to be in a set should be at most a factor of ? higher than the average probability) and if the clusters are sufficiently large. Note that this model allows for correlations between the clusters Bi; for example, the same bins can always appear together in the same sets.

In our ?-balancedness balls-into-bins model the balls represent the jobs and the bins represent the cloud centers.

Every ball i originates at some bin bi and the associated bin set Bi is simply the neighborhood of bi ? where we use different definitions for the neighborhood of a bin / node.

All neighborhoods that we use define Bi as some subset of the nodes within a certain distance of bi where the ball is generated. It is easy to see that a local load balancing approach cannot work if all jobs are generated on only one or very few nodes of the network. Instead, we assume that the jobs are generated on randomly selected nodes. Then the sets Bi can be regarded as a function of the network only. The network defines n neighborhood sets, one for every  2013 IEEE 27th International Symposium on Parallel & Distributed Processing Workshops and PhD Forum  DOI 10.1109/IPDPSW.2013.148     node. And the balls randomly chose one of these n sets as their possible destinations.

In this paper we consider different network topologies to model cloud locations and their interconnections. This includes synthetic graph structures like regular graphs, pow- erlaw graphs and small world graphs, but also the real graph structure of the Internet backbones. We investigate different ways of choosing the neighborhood sets Bi and study which of the networks are suitable for defining ?-balanced sets Bi.

We show that it is possible to both balance the load nearly perfectly and to keep the data close to its origin. The results are based on theoretical analyses and simulations.



II. RELATED WORK  Data placement and data movement are two of the most difficult problems to tackle in cloud environments. Data distribution includes several constraints, e.g., the costs for moving data across wide area networks, the capacity limits of the data centers, and the perceived user latencies, data availability, and energy consumption.

Data placement in big data centers is mostly based on directly attached storage, where data is stored on the same computers, which also perform services like mail or web search [3], [4], [5]. Each data item is typically replicated to a number of computers [6]. Data availability requirements have a strong impact on the abilities to save energy, as it is not possible to power down all computers storing a replica of some data. Thereska et al. have introduced data distribution schemes which are able to spread the replicas in a way that still allows for powering down a certain percentage of the servers, achieving energy savings of up to 40% [7].

The impact of data placement on load balancing, traffic between data centers and user latency was evaluated by Agrawal et al. The proposed Volley scheme improves these quantities by a factor of nearly two [8].

Hallett et al. consider a special scenario in which medical images are processed in a cloud [9]. Since the images are very large, it is impossible to always store them locally, and costly to move them from one node to another. Therefore they suggest a data-aware placement algorithm that moves the algorithm to the data rather than the data to the algorithm.

Both our analysis and our simulations will be based on balls-into-bins games. There is a vast number of papers dealing with balls-into-bins games in their many different flavors. We shall restrict our attention to previous work that is relevant to the results presented in this paper. That is, we concentrate on protocols that achieve a fixed maximum bin load, usually one or some other, small constant, in settings where the balls? choices of bins are not (necessarily) independent or uniform.

The model used in this paper may be viewed as a gen- eralization of the multiple-choice model ? in that a cluster represents a ball?s choices ? but this is not the focus of this paper. In the case of the multiple-choice model every ball  is allowed to choose d bins independently and uniformly at random. Usually, the ball is allocated to a least loaded bin among its choices, and typically one is interested in deriving tight bounds on the maximum load of any bin, given the numbers of balls and bins. For a discussion of this type of model the reader is referred to [10], [11] and references therein. Note that the results of [12] yield constant load per bin for d = lnn, and the always-go-left protocol of [11] achieves constant load for d = ln lnn.

Most recently several papers have examined the case when bins are not chosen independently and uniformly at random (i.u.r.) [13], [14], [15]. The motivation for these models comes from the properties of P2P networks like Chord, where each bin is randomly assigned a position on a ring, and a bin is then responsible for covering the arc between itself and its nearest neighbor in a specified direction. This model leads to variable arc lengths; see e.g. [16] for details.

Most relevant to our paper, Godfrey introduced the notion of ?-balancedness [2]. The main contribution of his paper is a proof showing that if the number of balls can be bounded by m = O(n/(? log ?)) then w.h.p. his algorithm succeeds in finding an allocation with maximum load equal to one.

This bound on m is not explicitly stated in his paper; the ? ? log ?-term is hidden in his parameters k and ?, but can be easily verified. He also shows an upper bound (of greater than one) on the maximum load in the case m = n by trivially running the original algorithm a constant number of times. The corresponding proofs have been considerable simplified and generalized in [1]. Furthermore, the authors have introduced the concept of runs, where one decision might trigger the movement of many balls to a single location. This concept is closely related to cloud computing, where the decision of a company to send data to one cloud storage resource often also triggers the subsequent movement of many other data items.

The customers in our model select their set of bins based on the bandwidth and latency to the cloud providers. Corre- lations leading to ?-balancedness in the distribution of data in cloud environments are therefore based on the underlying network topology. The most important network topology in this context is the Internet. The Faloutsos siblings have shown that the number of connections per node follows the power law [17]. Faloutsos et al. have investigated three different snapshots of the Internet from 1997 and 1998. They have shown that there is a very high correlation with a power law distribution for all three cases, even if the number of nodes has increased by 45% over this period.

Besides this power law distribution, the Internet also follows the small world property, saying that the number of hops between any two nodes is very small, even in very large environments. Govidnan et al. have shown this property for the internal structure of autonomous systems [18], while Jin et al. have shown that also the connection of autonomous system follows the small world behavior [19].



III. MODEL AND ALGORITHMS  In this section we introduce notation used in the remainder of this paper.

A. Basic Definitions  We model the distributed cloud as a graph G = (V,E), where each node corresponds to a router. We assume that at most one data center can be directly attached to every routing node and that each data center is attached to exactly one router. n is the number of data centers, the number of routing nodes is ? with ? ? n. Therefore, it is possible to model data centers attached to multiple routers, and routers being attached to multiple data centers, by introducing new virtual routers. In many cases we assume that n = ?.

In the theoretical part of this paper we will use the balls- into-bins game as an abstraction. We assume that the routers are modeled by bins, i.e., the nodes of G represent bins. The bins are denoted b1, . . . , bn. The load produced by customers is modeled by balls, which are numbered 1, . . . ,m.

For a fixed bin b, all nodes that are connected by an edge to b and which have a data center directly attached are called direct neighbors. Every bin b has a fixed set of nodes in its general neighborhood, where the neighborhood is either the set of direct neighbors of b in G, or the set of all nodes with a fixed distance to b that have data centers (see Definitions 3.1 and 3.2). We assume that every node b ? V defines a set of bins Bb, corresponding to the nodes in the neighborhood of b. We shall refer to this set of bins as a cluster. BG is the set of all clusters defined by the nodes (bins) of G. B? is defined as the average size of the clusters, i.e., B? = ?v?V Bv/n.

B. Allocations Algorithms  For every ball i we assume that it originates at a fixed node b, called b(i) in the following. The ball can be allocated to any bin in the cluster Bb(i). We consider three different models determining the way in which balls originate.

Random model: In this we assume that for all 1 ? i ? m, ball i originates at every bin with the same probability (see Algorithm 1). Hence, b(i) is randomly chosen among all bins.

Model with runs: In the model with runs (see Al- gorithm 2) we assume that every ball i has the (random) choice between bi?1 or choosing a random bin by itself. This models a scenario where individual customers can generate a high load, e.g., by moving large amounts of data into the cloud and therefore where the load originates at the same router at the same time. For Algorithm 2, let p be a constant strictly between 0 and 1 (it is trivial to extend the algorithm to individual pi but we have chosen to fix one single p for ease of presentation). We assume that there is a randomly preselected set of bins B0.

Algorithm 1 for the random and the biased model 1: for ball i ? {1, . . . ,m} do 2: Ball i originates at a bin bi chosen i.u.r. defining a  bin cluster Bbi 3: I.u.r. choose a bin b ? Bbi from the set of least loaded  bins in Bbi and allocate Ball i to b 4: end for  Algorithm 2 for the model with runs 1: for ball i ? {1, . . . ,m} do 2: With probability p, choose Bi?1 for Ball i. With the  remaining probability 1 ? p, assume i originates at a i.u.r. chosen bin bi defining Bbi . Either way, let B denote the chosen cluster.

3: I.u.r. choose a bin b ? Bbi from the set of least loaded bins in Bbi and allocate Ball i to b  4: end for  Biased model: Here balls do not originate at uniformly at random chosen bins, but with a more general probability distribution. For 1 ? i ? m let Di be a random distribution that assigns ball i to a bin. Here we assume that for 1 ? i ? n the expected number of balls originating at bin bi are close to m/n. Then a distribution is called ?-balanced. For a formal definition of ?-balanced D see Definition 3.5. Let D = {D1, . . . Dm} be the set of all such distributions.

C. Neighborhoods  We consider different types of neighborhoods in this paper. Note that the direct neighborhood of a node is the 1-hop-neighborhood in the definition below.

Definition 3.1 (k-hop-neighborhood): Let d(u, v) be the distance between two vertices u and v. The k-neighborhood of a vertex v is defined as  Nk(v) = {u | u ? V, u has a data center, d(u, v) ? k}.

The k-vertices-neighborhood N|k|(v) of a vertex v is  defined to be a set of size exactly k consisting of a subset of v?s closest neighbors.

Definition 3.2 (k-vertices-neighborhood): Let us assume that |Nd?1(v)| =: s < k and |Nd(v)| ? k. Then the k- vertices-neighborhood is the union of Nd?1(v) and a set of k ? s vertices that are i.u.r. chosen from Nd(v) \Nd?1(v).

The next neighborhood uses more randomness and ran- domly chooses between all nodes in a certain distance d.

Definition 3.3 (random k-vertices-neighborhood): Assume |Nd?1(v)| < k and |Nd(v)| ? k. Then the random k-vertices-neighborhood is a set of k vertices that are i.u.r.

chosen from Nd(v).

D. ? ? balancedness In the following we will define the ?-balancedness of a  graph G = (V,E), which describes the balancedness of the number of times bins appear in the clusters defined by G.

Our definition of one-sided ?-balanced graphs is based on the definition from [1].

Definition 3.4 (One-sided ?-balanced BG): For ? ? 1, a set of clusters BG defined by G is ?-balanced if for all bins j and i.u.r. chosen bin B ? BG we have  P(bin bj ? B) ? E (   |B| | bj ? B ) ? ?  n .

Note that in the above definition E (  |B| | bj ? B  ) is the  expected size of a randomly chosen set B ? BG that contains bj .

In the following we give a formal definition of ?-balanced distribution D which estimates the balancedness of the probabilities with which users originate at the nodes.

Definition 3.5 (?-balanced D): A distribution Di is ?- balanced if for all 1 ? j ? n,  P(ball i originates at bin j) ? ?/n.

The set of distributions D is ?-balanced if for all 1 ? i ? m, Di is ?-balanced.

The set of distributions D is averaged ?-balanced if m? i=1  P(ball i originates at bin j) ? m ? ?/n.

The next definition considers both the balancedness of the graph and D.

Definition 3.6 (Averaged ?-balanced BG with D): For ? ? 1, a set of clusters BG and a set of distributions D = D1, . . . ,Dm are averaged ?-balanced if for all bins bj with 1 ? j ? n  m? i=1  P(bin bj ? Bb(i)) ? Ei (   |B| j ? B ) ? m ? ?  n .

Here Ei[1/|B| | bj ? B] is the expected size of a set B ? BG that is randomly chosen by Distribution Di and that contains bj . With a slight abuse of notation we will refer to a graph but also to the set of clusters defined by that graph as being ?-balanced.

E. Graph classes  Real-world graphs like the graph structure of the Internet, which is in the context of cloud storage of special impor- tance, are hard to tackle fully analytically. The same is true for some more complex graph topologies. Therefore we use network simulation to gain more insights on how our load balancing algorithms work for real world networks and real- istic network models. We use four different types of network topologies for the simulations. For all random graphs, we use the implementations provided by the NetworkX graph library developed by the Los Alamos National Laboratory [20].

Regular graphs: We generate random regular graphs with degree 8 using the algorithm by Steger and Wormald [21].

Powerlaw graphs: Here we use the algorithm by Holme and Kim [22]. In that model a new node vt with degree k is added in every time step t. The endpoints of the edges of vt are chosen randomly among all nodes v0, . . . , vt?1, with a probability that is proportional to the degree of the nodes.

In a second step all k outgoing edges (vt, vi1), . . . , (vt, vik) are considered again. For 1 ? j ? k, an edge from vt to a random neighbor of vij is added with probability 0.9. The later edges create dense node clusters. We use k = 8.

Small world graphs: We generate random small world graphs using the classical Watts-Strogatz algorithm [23]. The model constructs a graph with n nodes, average degree k and nk/2 edges in the following way:  The model starts with a ring consisting of n nodes called v1, . . . vn. Every node vi is connected to k other nodes.

There is an edge between vi and vj if |i?j| ? ?(mod n) for ? ? 1 . . . , k/2. Then a redirection phase is started. For every node vi, every edge (vi, vj) with i < j is replaced with a probability of ? with edge (ni, n?), where ? is chosen with uniform probability from all possible values that avoid loops and link duplication. We use only connected Watts-Strogatz graphs for our simulations. We generated graphs for k = 8 and ? = 0.9.

Autonomous System: We use a snapshot of the Au- tonomous System (AS) graph of the Internet from July 22nd, 2006. The data has been constructed from the Border Gateway Protocol (BGP) network tables published by the University of Oregon Route Views Project1. We assume for simplicity that every AS contains a cloud data center. The graph contains 22,963 autonomous systems represented as nodes and 48,436 peerings represented as edges. The mean degree of a node is 4.2 and the median degree is 2. A histogram of the degree distribution is shown in Table I.

In that figure we excluded the 9 highest degree nodes, their degree ranged from 697 to 2390. The diameter of the AS graph is 11. The minimal eccentricity, which is defined as the maximal distance of a node v to all other nodes in the graph, is 6. The mean eccentricity is 7.44 and the median eccentricity is 7.



IV. THEORETICAL RESULTS  In this section we present theoretical results for random and regular graphs showing that many graphs lead to ?- balanced neighborhoods. We assume that every node of the graph has a data center attached to it.

First we give a very simple observation. Recall that b(i) is the bin where customer i originates. b(i) can be chosen uniformly at random among all bins or using a distribution  1The snapshot has been obtained from http://www- personal.umich.edu/?mejn/netdata     Table I NODE DEGREES FOR AS GRAPH, WHERE n IS THE NUMBER OF NODES  OF DEGREE d.

d n d n d n 1 7840 12-23 453 144-167 6 2 9700 24-47 205 168-215 9 3 2219 48-71 50 216-238 2 4 907 72-95 36 239-263 2 5 471 96-119 14 287-310 5  6-12 1004 120-143 8 311-696 14  Di (see Section III). Here we assume that for all 1 ? i ? n and fixed k, N(b(i)) is the neighborhood of bin b(i), assuming any of the definitions in Section III.

Observation 4.1: Assume the balls originate in the ran- dom model. Then for all 1 ? j ? n and 1 ? i ? m  P(bin bj ? Bb(i)) = |N(b(i))| n  .

If the balls originate in the biased model with ?-balanced D, then for all 1 ? i ? n and 1 ? j ? m  P(bin bj ? Bb(i)) = ? ? |N(b(i))| n  .

If the balls originate in the biased model with averaged ?- balanced D, then for all 1 ? i ? n  m? k=1  P(bin bj ? Bb(i)) = m ? ? ? |N(b(i))| n  .

A. Regular Graphs  First we show a simple observation showing that regular graphs are ?-balanced for balls allocated in the random model and also for balls allocated in the biased model where balls can originate in some bins with a higher probability, as long as these distributions are balanced (see Definition 3.5).

Observation 4.2: Assume ? > ?? is a suitably chosen constant. Assume the balls are allocated in the random model, the biased model with ??-balanced D, or in the biased model with averaged ??-balanced D. Then every regular graph is ?-balanced for 1-neighborhoods.

Proof: The result follows directly from Observation 4.1.

The next result shows that vertex-transitive graphs are ?- balanced, again for balls allocated with distributions defined in Definition 3.5. The class of vertex-transitive graphs in- cludes symmetric graphs, hypercubes, and tori.

Lemma 4.3: Assume ? > ?? is a suitably chosen con- stant. Assume the balls are allocated in the random model, the biased model with ??-balanced D, or in the biased model with averaged ??-balanced D. Then every vertex-transitive graph G is ?-balanced for all k-neighborhoods.

Proof: Due to the symmetry of G all neighborhoods have the same size. The result follows now from Observation 4.1.

Note that a similar result does not hold for edge-transitive graphs, these graphs are not necessarily ?-balanced for all k-neighborhoods. As an example, the star is edge-transitive but not ?-balanced for the 1-neighborhood.

Unfortunately regular graphs are not necessarily ?- balanced for k-neighborhoods with k > 1. In the following we define a graph class of d-regular graphs (for odd d) that is not ?-balanced for k = 2 (see Theorem 4.6). Here we assume our random model, but the same result holds for the biased model.

Definition 4.4: G = {G(d)|d ? {3, 5, 7, ...}} is a class of d-regular graphs. G(d) has one center vertex vc that is connected with d identical subgraphs G1(d), . . . , Gd(d) with d + 2 nodes each. These subgraphs are not connected with each other at all. Each subgraph Gi(d) has  ? one top vertex vti , connected with the center v c,  ? d?1 middle vertices vmi,j , 1 ? j ? d?1, each of them incident to vti and to d? 3 other middle vertices, and  ? 2 bottom vertices vbi,j , 1 ? j ? 2, connected with each other and with all middle vertices.

As an example, Figure 1 shows G(3).

Observation 4.5: For each graph G(d) ? G we have: ? n = d ? (d+ 2) + 1 = (d+ 1)2.

? The vertices have 2-neighborhoods of different sizes:  |N2(vc)| = 1 + d+ d ? (d? 1) = d2 + 1 |N2(vti)| = 1+1+(d?1)+(d?1)+2 = 2?(d+1) |N2(vmi,j)| = 2 + (d? 1) + 1 + 1 = d+ 3 |N2(vbi,j)| = 1 + (d? 1) + 2 = d+ 2  Theorem 4.6: For any given ? there exist regular graphs such that the set of 2-neighborhoods is not ?-balanced.

Proof: We use the graph family from Definition 4.4.

Assume ? is fixed. Then we need to show that there exists a node u such that for a randomly chosen cluster B  P(bin bj ? B) ? E (   |B| | bj ? B ) ? ?  n  is not fulfilled.

If we choose the center vertex vc, we get with Observation  4.5:  Figure 1. Regular graph G(3)     E  (  |B| | bj ? B )  =  (  d2 + 1 +  d  2 ? (d+ 1) + d ? (d? 1)  d+ 3  )  d2 + 1  = 2 ? (d+ 1) ? (d+ 3) + d ? (d2 + 1) ? (d+ 3) (d2 + 1) ? 2 ? (d+ 1) ? (d+ 3) ? (d2 + 1) +  + d ? (d? 1) ? (d2 + 1) ? 2 ? (d+ 1)  (d2 + 1) ? 2 ? (d+ 1) ? (d+ 3) ? (d2 + 1) = ?(d?1)  This gives us  P(bin bj ? B) ? E (   |B| | bj ? B )  = d2 + 1  n ??(d?1) = ?  ( d  n  ) ,  which is greater than ?n if we set the degree to d > c ? ? (where c is a large enough constant).

Note, however, that n and d grow with ?. It is easy to see that ? is bounded if we fix n or d (not only G). The next theorem shows a similar result for vertex-neighborhoods (see Definition 3.2)  Theorem 4.7: For any given ?, there exists a graph G(d) such that the set of (d + 3)-vertices neighborhoods is not ?-balanced.

Proof: Again, we use G from Definition 4.4. Let ?(u) denote the minimum number of neighborhoods that contain vertex u. We will show ?(u)n >  ??s n ? ?(u)s > ? for some  u ? V .

Since k = d+3 is fixed, we can use Observation 4.5 (see  below) and get:  ?(vc)  s =  (d+ 1)2 ? (d+ 1)2 d3 + 7 ? d2 + 5 ? d+ 1  = ...

> d? 3 So, if we set the degree to d ? ? + 3, we have indeed  that ? < ?(v c)  s .

B. Random Graphs  Let G ? GE(n, p) be a random graph generated by the Erdo?s-Renyi model, i.e., G has an edge between a pair of nodes with probability p, independently of all other edges.

Let G ? G(n, r) be a random r-regular graphs, i.e., a Graph i.u.r. selected from all r-regular graphs.

Lemma 4.8: Let G ? GE(n, p) be a random graph and let BG be the set of clusters defined by the k-neighborhoods of the nodes. Let ? < 1/2 be an arbitrary constant and assume pk ? n?.

? There exists a suitable chosen constant ? such that BG  is one-sided ?-balanced.

? Assume that for 1 ? j ? m, Di assigns a probability of at most ?/n to every node. There exists a suitable chosen constant ? = ?(?) such that D and BG are averaged ?-balanced.

Proof: This follows from the fact that the k-neighbor- hood of every node of G can be regarded as a tree of depth k with a constant number of additional edges. The rest of the proof follows from Observation 4.1.

Lemma 4.9: Let G ? GE(n, r) be a random r-regular graph and let BG be the set of clusters defined by the k- neighborhoods of the nodes. Let ? < 1/3 be an arbitrary constant and assume rk ? n?.

? There exists a suitable chosen constant ? such that BG  is one-sided ?-balanced.

? Assume that for 1 ? j ? m, Di assigns a probability  of at most ?/n to every node. There exists a suitable chosen constant ? = ?(?) such that D and BG are averaged ?-balanced.

Proof: Again, this follows from the fact that the k- neighborhood of every node of G can be regarded as a tree of depth k with a constant number of additional edges.

C. Small World Graphs  If the degrees of a graph are power law distributed, the graph will not be ?-balanced for k-neighborhoods and k- vertex-neighborhoods for reasonable k. This follows directly from Observation 4.1.

Generally it will be a problem that most small world graphs have so-called hubs, which have much higher degrees than normal vertices (non-hubs).



V. SIMULATIONS  In this section we present simulations for all four graph classes and the three neighborhoods defined in Section III.

We use random graphs, power-law graphs, and small world graphs with 20,000 nodes. The AS graph has 22,964 nodes.

We generate 1,000,000 jobs and we assume that every node has a data center attached. Hence, the average load is 43.55 for AS graphs and 50 for all other graphs.

For each configuration (triplet of graph, neighborhood- type, and k-value) we perform 100 simulations. Note that for AS graphs we do not consider k-hop neighborhoods for values of k > 1, since the 2-neighborhood already covers a big constant fraction of the whole graph.

In our simulations the number of customers (or balls) is larger than the number of data center (bins). We use Algorithm 1 and Algorithm 2 as underlying protocols for the simulations and always allocate a ball i into the bin in Bbi with minimum load. If there are several bins with minimum load the ball is allocated into a randomly chosen bin among them.

A. Random Job Assignment  In this section, we focus on load distributions generated by Algorithm 1. We depict the minimum and the maximum load for random graphs (Table II), power-law graphs (Table III), Smallworld graphs (Table IV), and AS graphs (Table V). For random graphs, power-law graphs, and small world graphs the maximum load is at most 51, even for small neighborhood sizes. The load distribution on AS graphs is much worse, the minimum load is usually zero and the maxi- mum load is typically much higher than the average load and it only declines for large and randomized neighborhoods.

Table II REGULAR GRAPHS: LOAD DISTRIBUTION  Neighborhood Min. Load Max. Load 1-hop 48.45 51.00  16-vertices 49.00 51.00 32-vertices 49.00 51.00  random 16-vertices 48.86 51.00 random 32-vertices 49.00 51.00  Table III POWER-LAW GRAPHS: LOAD DISTRIBUTION  Neighborhood Min. Load Max. Load 1-hop 48.17 51.01 2-hop 48.97 51.00 3-hop 48.97 51.00  16-vertices 43.54 51.00 32-vertices 48.99 51.00  random 16-vertices 3.08 51.00 random 32-vertices 25.11 51.00  Table IV SMALLWORLD GRAPHS: LOAD DISTRIBUTION  Neighborhood Min. Load Max. Load 1-hop 47.98 51.39 2-hop 48.19 51.09 3-hop 48.17 51.15  16-vertices 48.96 51.00 32-vertices 49.00 51.00  random 16-vertices 48.68 51.00 random 32-vertices 48.99 51.00  Table V AS GRAPHS: LOAD DISTRIBUTION  Neighbourhood Min. Load Max. Load 1-hop 0 22003.57  16-vertices 0 59.22 32-vertices 11.34 45.49  random 16-vertices 0 54.51 random 32-vertices 0 49.01  Next, we have a closer look at the load distribution of AS graphs. Our goal is to see if nodes which appear in many neighborhoods will have a higher load than nodes which are only in few neighborhoods. We calculate the correlation  Figure 2. AS graphs: Avg. maximal load for Zipf distribution  k = 16 k = 32 k = 64 k = 128 k = 256       M a x im  a l L o a d (i n jo b s )  ? = 0.1  ? = 0.2  ? = 0.3  ? = 0.4  ? = 0.5  (a) k vertices neighborhood  k = 16 k = 32 k = 64 k = 128 k = 256       M a x im  a l L o a d (i n jo b s )  ? = 0.1  ? = 0.2  ? = 0.3  ? = 0.4  ? = 0.5  (b) Random k vertices neighborhood  between the number of sets a node is in and its load. The results are shown in Table VI. The table shows that the correlation is very high for the simple 1-hop neighborhood.

B. Biased Job Assignment  In this section we study the load distribution under the assumption that the balls do not originate at a random node of the graph. We assume that the origins are chosen using a Zipf-like distribution [24][25] with a parameter ? ? (0, 1), where ? = 1 is the standard Zipf distribution. This is a heavy-tail distribution that is often used to model the access distribution for web pages [26] [25]. The relative probability of a request for the ith most popular page is proportional to 1/i?.

We only present the results for AS graphs (see Figure 2) and regular graphs (see Figure 3) in this paper. Figure 2 shows that for both neighborhoods and ? ? 0.3 the average maximum load (average taken over 100 simulations) is still 50 for AS graphs for all values of k. For k ? 64, the average maximum load is still around 50 for ? = 0.5. Figure 3 exemplarily shows that the behavior for the other graph classes is similar with different values of ?.

C. Model with Runs  In this section we consider the load distribution generated by Algorithm 2. It is assumed that a sequence of jobs can originate at the same node of the network. We did simulations for all four graph classes again with p ? {0.1, 0.2, 0.5, 0.9}. Our simulations show that the maximum  Table VI CORRELATION BETWEEN DEGREE LOAD FOR AS GRAPHS  Neighborhood Corr. Neighborhood Corr.

1-hop 0.93  16-vertices 0.07 random 16-vertices 0.16 32-vertices 0.10 random 32-vertices 0.21 64-vertices 0.13 random 64-vertices 0.24 128-vertices 0.18 random 128-vertices 0.21 256-vertices 0.25 random 256-vertices 0.17     Figure 3. Regular graphs: Avg. maximal load for Zipf distribution  k = 16 k = 32 k = 64 k = 128 k = 256       M a x im  a l L o a d (i n jo b s )  ? = 0.1  ? = 0.2  ? = 0.3  ? = 0.4  ? = 0.5  (a) k vertices neighborhood  k = 16 k = 32 k = 64 k = 128 k = 256       M a x im  a l L o a d (i n jo b s )  ? = 0.1  ? = 0.2  ? = 0.3  ? = 0.4  ? = 0.5  (b) Random k vertices neighborhood  Figure 4. AS graphs: avg. maximal load for Algorithm 2  k = 16 k = 32 k = 64 k = 128 k = 256        M a x im  a l L o a d (i n jo b s )  p = 0.9  p = 0.8  p = 0.5  p = 0.2  p = 0.1  (a) k vertices neighborhood  k = 16 k = 32 k = 64 k = 128 k = 256        M a x im  a l L o a d (i n jo b s )  p = 0.9  p = 0.8  p = 0.5  p = 0.2  p = 0.1  (b) Random k vertices neighborhood  load in this model is very similar to the maximum load in the model without runs, even for p = 0.9.

We only depict the results for AS graphs (see Figure 4) and regular graphs (see Figure 5) in this extended abstract.

The results for AS graphs show that for p ? 0.5 the maximum load is still close to 43. Our simulations for the regular graphs in Figure 5 and the other graph classes (figures omitted due to space considerations) show that they can even tolerate larger values of p without seeing any noteworthy increase in the maximum load, compared to the results of Algorithm 1.

D. Correlation with ?-balancedness  In Section III, we defined the one-sided ?-balancedness of a graph as a (random graph) property (Definition 3.4).

In the case of the k-vertices neighborhood and the random k-vertices neighborhood the randomness comes from the choice of neighbors (among all neighbors) that will be used for the load balancing. Our theoretical analysis shows that the load distribution is very smooth for ?-balanced graphs. In this section we study the correlation between the maximum load and the value of ?. Again, we are only able  Figure 5. Regular Graphs: avg. maximal load for Algorithm 2  k = 16 k = 32 k = 64 k = 128 k = 256        M a x im  a l L o a d (i n jo b s )  p = 0.9  p = 0.8  p = 0.5  p = 0.2  p = 0.1  (a) k vertices neighborhood  k = 16 k = 32 k = 64 k = 128 k = 256        M a x im  a l L o a d (i n jo b s )  p = 0.9  p = 0.8  p = 0.5  p = 0.2  p = 0.1  (b) Random k vertices neighborhood  to depict our results for AS graphs and regular graphs, due to space limitations.

Table VII shows the minimum, maximum, and median value of ? for the AS graph, again averaged over 100 simu- lations. The k-vertices neighborhood has for all choices of k smaller ?-values than the random k-vertices neighborhood.

A comparison with Table V shows that the maximum load decreases with the maximum value of ?.

Table VIII shows the minimum, maximum, and median value of ? averaged over 100 simulations for randomly generated regular graphs. The k-vertices neighborhood has again for all choices of k smaller ?-values than the random k-vertices neighborhood. Nevertheless, the maximum ?- values for the randomly generated regular graphs are that low that they have nearly no impact on the maximum load (please compare with Table II), which can also be observed for powerlaw graphs and smallworld graphs.



VI. OPTIMIZATIONS  In the last section we observed that, for regular graphs, power-law graphs, and for small world graphs, all neigh- borhood types achieve a good load balance, even for small values of k. Unfortunately, in the real-world case of AS graphs these neighborhoods overload some data centers (see Table V). Especially in the case of the simple 1-hop neighborhood the maximum load is very large. The reason for the overload is that some nodes are in the neighborhood of very many data centers (they are in many sets B) so that they are the target of many load balancing actions (see Figure VI). Another drawback of our approach in the last section is that the nodes have to request the load of all nodes in the corresponding neighborhood. This can contain 16, 32, or even many more vertices in the case of the simple hop neighborhood.

In this section we first deal with the second problem and consider an approach that reduces the number of load requests per ball or customer. Then we consider two approaches (called thinning and reducing), both building     Table VII ?-VALUES FOR AS GRAPHS  Neighborhood type Min ? Median ? Max ? 1-hop 0.00 0.04 1128.77  16-vertices 0.00 0.74 156.97 32-vertices 0.01 0.78 120.42 64-vertices 0.02 0.73 96.81 128-vertices 0.00 0.71 70.90  random 16-vertices 0.00 0.84 53.49 random 32-vertices 0.00 0.83 44.96 random 64-vertices 0.00 0.81 39.26  random 128-vertices 0.00 0.78 26.06  Table VIII ?-VALUES FOR REGULAR GRAPHS  Neighborhood type Min ? Median ? Max ? 1-hop 1.00 1.00 1.00  16-vertices 0.50 1.00 1.75 32-vertices 0.56 1.00 1.46 64-vertices 0.94 1.00 1.04 128-vertices 0.78 1.00 1.24  random 16-vertices 0.24 1.00 1.93 random 32-vertices 0.51 1.00 1.49 random 64-vertices 0.79 1.00 1.17  random 128-vertices 0.71 1.00 1.31  Figure 6. AS graphs: average maximal load for different d  k = 16 k = 32 k = 64 k = 128 k = 256         M a x im  a l L o a d (i n jo b s )  d = 2  d = 3  d = 4  d = 8  d = 16  (a) k vertices neighborhood  k = 16 k = 32 k = 64 k = 128 k = 256         M a x im  a l L o a d (i n jo b s )  d = 2  d = 3  d = 4  d = 8  d = 16  (b) Random k vertices neighborhood  overlay networks, which reduce the number of sets Bv in which node v occurs, which is similar to reducing the degree of the underlying graph.

A. Reducing the number of requests  Here our goal is to reduce the number of load balancing actions per ball. We consider a variant of Algorithm 1. Ball i randomly chooses d nodes in the set Bbi and is allocated to the bin with minimum load. In the case of ties (several bins with minimum load) one of them is chosen at random.

The results for d ? {2, 3, 4, 8, 16} are shown in Figure 6.

Results for d = 1 are omitted, since the maximum load is very large, between 500 and 10,000.

Algorithm 3 Thinning protocol for node v with parameter t  1: N = {u ? V : v ? Bu} 2: N ? = ? 3: while |N | ? t and not N ? = N do 4: Choose a node u from N ?N ? i.u.r.

5: if |Bu| ? 1 then 6: Remove v from Bu 7: end if 8: N ? = N ? ? {u} 9: end while  Figure 7. AS graph: thinning parameter t and k = 64  d = 1 d = 2 d = 3 d = 4 d = 8 d = 16 d = 32   -20  -40  -60  -80  -100  L o a d C h a n g e  b y T h in n in g  t = 16  t = 32  (a) Relative change for k vertices neighborhood.

d = 1 d = 2 d = 3 d = 4 d = 8 d = 16 d = 32   -20  -40  -60  -80  -100  L o a d C h a n g e  b y T h in n in g  t = 16  t = 32  (b) Relative change for random k-vertices neighborhood.

B. Thinning  In the previous section we saw that small values of d result in a very poor load distribution. Here our goal is to try to reduce the degree of the nodes so that small values of d are sufficient for having a small maximum load.

The thinning protocol considers all nodes with a high degree occurring in many sets Bu in a random order. To reduce the degree of these nodes we consider Algorithm 3 for one of these nodes v. We try to take v out the sets Bu with v ? Bu, as long as the sets are not empty or until the degree of v is smaller than a threshold value t. The sets Bu are considered in a random order, too.

In Figure 7 and Figure 8 we show our results for k = 64 and k = 128, both for the k vertices and the random k-vertices neighborhood. The figures depict the load im- provement (decrease) for the maximum load (y-axis) for d ? {1, 2, 3, 4, 8, 16, 32} compared to the maximum load with the same values for k and d without thinning for two values of t. The simulations show that, for small values of d, the maximum load can be decreased considerably by thinning, whereas the effect is negative for larger values of d. The explanation is that for small d the algorithm is able to pre-select the good neighborhoods by neglecting overloaded nodes, while for larger d the number of choices for the algorithm is decreased.

Figure 8. AS graph: thinning parameter t and k = 128  d = 1 d = 2 d = 3 d = 4 d = 8 d = 16 d = 32    -20  -40  -60  -80  -100  L o a d C h a n g e  b y T h in n in g  t = 16  t = 32  t = 64  (a) Relative change for k vertices neighborhood.

d = 1 d = 2 d = 3 d = 4 d = 8 d = 16 d = 32   -20  -40  -60  -80  -100  L o a d C h a n g e  b y T h in n in g  t = 16  t = 32  t = 64  (b) Relative change for random k-vertices neighborhood.

Algorithm 4 Reducing protocol with parameter r 1: R = Set of the r nodes with the highest edge count 2: while R 	= ? do 3: Select node v ? R i.u.r.

4: for all u with v ? Bu do 5: Bu := Bu \ {v} 6: end for 7: R := R \ {v} 8: end while 9: for all Bv with Bv = ? do  10: k := min{k | Nk(v) ?R 	= ?} 11: Choose w ? Nk(v) \R i.u.r.

12: Bv = {w} 13: end for  C. Reducing protocol  The reducing protocol (Algorithm 4) tries to reduce the degree of the r ? n nodes with the highest degree by connecting their neighbors to non-neighboring nodes with a smaller degree. Let u be a node connected to one of the r high-degree nodes. Then u will be connected to the closest node v that is not one of the r high-degree nodes.

In Figures 9?10 we show our results for k = 16 and k = 32, both for the k vertices and the random k-vertices neighborhood. Again, the figures depict the improvement depicted as percentaged decrease in the maximum load for d ? {1, 2, 3, 4, 8, 16, 32}, compared to the maximum load for same values for k and d without reducing.

For k = 16 and k = 32 and the k-vertices neighborhood we see a large decrease of the maximum load for very small values of d. Reducing did not improve the maximum load for larger values of d. The improvement is larger for k = 16 than for k = 32. We do not get an improvement for the random k-vertices neighborhood except for k = 16 and d ? {1, 3}.



VII. CONCLUSION Cloud computing is turning form being compute centric  only to being additionally data driven. Nevertheless, the  Figure 9. AS graph: reducing parameter r and k = 16  d = 1 d = 2 d = 3 d = 4 d = 8     -19  -40  -60  L o a d C h a n g e  b y R e d u c in g  r = 5  r = 10  r = 15  (a) k-vertices neighborhood.

d = 1 d = 2 d = 3 d = 4 d = 8      -9  -20  -30  L o a d C h a n g e  b y R e d u c in g  r = 5  r = 10  r = 15  (b) random k-vertices neighborhood.

Figure 10. AS graph: reducing parameter r and k = 32  d = 1 d = 2 d = 3 d = 4 d = 8 d = 16  -10 -20 -30 -40  L o a d C h a n g e  b y R e d u c in g  r = 5  r = 10  r = 15  (a) k-vertices neighborhood.

d = 1 d = 2 d = 3 d = 4 d = 8 d = 16 -4  L o a d C h a n g e  b y R e d u c in g  r = 5  r = 10  r = 15  (b) random k-vertices neighborhood.

requirement of moving huge amounts of data between cus- tomers and data centers as well as intra data centers makes it much more difficult to balance the load, as customers will prefer to keep their data close to their own facilities.

In this paper we have shown that standard load balancing strategies work well for many classes of networks, but that they are also failing for the most important one, the Internet (modeled by AS graphs). Therefore we have introduced strategies which remove parts of the pressure from the most loaded nodes by building implicit overlay networks. The practical applicability of these strategies is that they help to understand and solve the facility location problem for cloud providers.

