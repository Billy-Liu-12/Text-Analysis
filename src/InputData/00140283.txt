

Abstract Analog CMOS circuits which model synaptic habitua-  tion, sensitization and classical conditioning found in the marine mollusc Aplysio are presented. These circuits are expected to be useful in ANNs with higher-order synapses and learning rules that perform temporal association of mul- tiple inputs. Our investigation illustrates that biological synaptic learning in invertebrates involves more elaborate mechanisms than those found in most current ANN models.



I. Introduction In previous studies we have described implementations  of artificial neural networks ( A N N s )  using low-accuracy analog CMOS transconductance circuits [4,.5I. These ICs implement conventional ANN architectures, with Hebbian [7] and contrastive Hebbian [9] learning circuitry at each synapse. We briefly outline our work with contrastive Heb- bian ANNs in section II. For the balance of this paper, we take a slightly different approach to the implementation of ANNs: rather than designing circuits to implement an ANN architecture, we propose circuits which are suggested by the behavior of a biologicol neural network.

Our circuits model threa learning paradigms present in biological synapses: habituation, sensitization and classical conditioning. Specifically, we are modeling the behavior of the marine mollusc Aplysia, which has been studied exten- sively -- see for example, [1,2]. It must be emphasized that, although our circuits are more biologically plausible than most current A N N s ,  they still present a highly simplified picture of biological neural networks. More complex aspects of biological synapse operation, such as those involving extinction and spontaneous recovery [Z], are omit- ted from the circuits described below. In doing so we achieve a balance between accurate modeling of neural biol- ogy, and mainstream ANNs.

This work draws upon two of our previous investiga- tions, [3], in which CMOS circuits implementing habitua- tion, sensitization and classical conditioning with EEPROM weight storage are described, and [4]. which deals with ana- log CMOS Hebbian learning circuits using capacitive weight storage. Other approaches to ANN implementation with learning circuitry at each synapse have been r e h d recently, including [6]. Below we begin by describing our work with contrastive Hebbian ANNs. Then, in section IJI, biological synaptic learning is reviewed. Section W presents a simplified mathematical model of this learning bahavior, which is followed by a description of the proposed analog CMOS synaptic circuit, as well as simulation results, in section V.

11. Contrastive Hebbian ANNs In this section we describe an analog CMOS implemen-  tation of a fully connected ANN with contrastive Hebbian learning [9] circuitry at each synapse.

Networks with contrastive Hebbian learning are intended for supervised learning applications, where a set of training pat- terns is used for weight learning. A key feature of contras- tive Hebbian learning is that it can be used to train weights in networks with hidden neurom, that is, neurons whose activations are neither network inputs nor outputs.

The architecture of the ANN under consideration is a fully-connectad Hopfield-type arrangement of neurons and synapses, in which there is a synaptic connection between each pair of neurons. Thus, a network of N neurons has N ( N - 1 )  synapses, and synaptic circuitry occupies the majority of siliwn chip area.

Manhattan contrastive Hebbian learning (CHI,) is a two-phase weight learning process, governed by:  vi = f ccwi, vj )  AW,I = o .sgn(V,'V; - VFV;)  (1)  (2)  I  where V: and are the activations of the i" and j" neu- rons in the clamped [9] phase, V,- and V,- are the activa- tions of the i" and j" neurons in the unclamped phase, and o is a small positive constant which determines the weight leaming rate.

I \  Other synapsm  Figure 1 Svsteni block diagram. N - 1 synoptic circuits ore connected to earh neuron circuit.

Fig. 1 is a block diagram of the ANN system: the cir- cuit is an analog CMOS approximation to ( 1 )  and (2).  Both neuron activations and synaptic weights may take on analog values in the range [ -V,V] .  Multipliers P 3  and P are tran- sconductance multipliers, whose inputs are voltages and out- put is a c-t. P and P are linear current to voltage convertas, functioning as a resistor to (VGW + Vss)/2. The comparator P7 outputs either Vdd or Vss,  depending on whether its input is above or below its threshold reference,     and Pa and Pp are conventional digital gates. Lastly, P I and P 2  are Manhattan CHL circuits, containing weight' update circuitry and weight storage capcitors. Synaptic weights are represented by the differential voltage  The circuit of Fig. 1 approximates equations (1) and (2). representing ideal network behavior, as follows.' The synaptic weight W,, is represented by the differential vol- tage, Vu - VuN.  Then, the product W,,V, in (1) is imple- mented by P 3  as Ism = b(VcL - VW)V,,  and the summa- tion operation is pedormed as current summation at the input to P, .  P ,  - P s  realize a variable gain neuron, whose transfer function may be adjusted in a variety of ways, using the NGain control signals. -  Since we implement Manhattan leaning, only the sign of neuron activations are required for learning. P7 - P S generate LV, , a binary version of each neuron activation for learning.

v,, - v,.

Figure 3 Simplified schematic diagram of the portion of Aplysia's ~ ~ N O U F  system responsible for gill withdrawal reflex.

this section, biological synaptic function is described at the behavioral level; for a description of the electro-chemical processes involved, see [I]. Fig. 3 shows a simplified schematic diagram of the portion of Aplysia's lo5 neuron narvaus svstem reswnsible for its urotective bill withdrawal -, . ._. I reflex The neural network connecting the gill (Aplvsm'~ respuatory organ), siphon (a small spout for expelling sea water), mantle and tail allows the mollusc to withdraw its gill when a stlmulus is applied to the tail. siphon or mantle In nature, this reflex has obvious evolutionary advantages, as it allows Aplysia to protect its sensitive glll at the first sign of attack Aplysio'3 response to tail. mantle and siphon stimuh may be altered dramatically by what it has learned about these atlmuh in the past In Fig 3. the three sensory neurons S N , ,  S N 2  and S N 3  receive stimuli from the mantle.

siphon and tail. respectively SN I and 5N 2 synapse directly with motor neuron M N  (S and S2), so siphon and mantle stlrnull can tngger gill withdrawal In addition, SN, synapses with the facilitating mterneuron F N ,  which in turn synapses with synapres S i  and S 2  (through I . ,  and F2) These synnpses on synapses. I .  I and f , ,  play a cntical role  Figure 2 Photomicrograph of 19 neuron, 342 synapse net- work (4.16 x 2.76mm2, 1.2pm CMOS).

Network operation proceeds as follows. First, the net- work inputs are set for a particular training pattern, while output and hidden neurons remain free (ie. unclamped).

The network is allowed to settle to its unclamped phase minimum energy state, LV,- and LV; (the binary learning activations generated by the i* and j h  neurons) wtle ,  and control signals LUPulSe' and LUPu/seN are pulsed briefly.

The binary product LV,ZV; determines whether a small quantity of charge is added to, or removed from, the storage capacitor connected to the unclamped learning circuit.

Next, network outputs are set to training pattern values, the network settles in the clamped phase, q d  learning control signals LCPuZse' and LCPulseN are pulsed briefly. As before, the binary product LV,'LV; detamines whether charge Is added to, or removed from the clamped storage capacitor. This two phase procedure is rspested for each training pattern. Typically, many passes through the entire set of training data is required to leam network weights.

Fig. 2 is a photomicrograph of the multi-project die contain- ing a 19 neuron, 342 synapse test network.

111. Basic Biological Synaptic Learning Kandel et a1 [l], in  their study of the marine mollusc  Aplysia, have shown that chemical changes in individual synapses are responsible for three important typas of learn- ing: habituation, sensitization and classical conditioning. In  in learning, as described below. For the purposes of neural modeling, the pairs S i  and F1, and S 2  and F 2  may be regarded as single ternary synapses (synapses between three neurons), in which the synaptic weight is determined by the interaction of two external signals. This is the approach that we take in the following section. Note that the schematic of Fig. 3 is highly simplifiad; in particular, each neuron illus- trated represents approximately 6 to 24 neurons operating in parallel.

Habituation may be defined as '*a decrease in the strength of a behavioral response that occurs when an ini- tially novel eliciting stimulus is repeatedly presented" [l].

Kandel et a1 have shown that repeated mild tactile stimuli to the mantle cause the gill withdrawal reflex to habituate, as the animal k n s  that the stimuli pose no danger. This learning behavior has been traced to chemical changes in the synaptic connection between mantle sensory neurons and gill motor m n s  (SJ: the effective weight of the synapse SI is reduced.

The sensitization mechanism is somewhat more com- plex. Sensitizdwn is defined as, "the enhancement of an animal's reflex response as a result of the presentation of a strong or noxious stimulus" [l]. In the case of Aplysia, noxious stimuli to the tail result in enhanced subsequent gill withdrawal in response to mild mantle stimuli. Again the locus of learning is the synapse SI, in this case through presynaptic facilitation from synapse F i .  The mechanism is as follows: tail sensory neuron firing (SA',) causes the facili- tating interneurons (FA') to fire, which in turn cause F1 to     alter the chemistry of SI ,  such that the synaptic weight of S I is increased. Not surprisingly, sensitization can reverse the effects of habituation.

?In classical conditioning, an initially weak or ineffective conditioned stimulus (CS) becomes highly effective in producing a behavioral response after it has been paired in time with a strong unconditioned stimulus (lis)?? [l]. Classical conditioning is a form of associative learning, by which an organism learns a predictive relation- ship between two stimuli. Aplysia can be classically condi- tioned by applying a mild tactile stimulus to the mantle (CS) ,  followed-approximately second later by a strong  stimulus to the tail (US). Again Fi  plays an important role.

In classical conditiomng, recent activity in SI due to the CS results in activitydependent enhancement of presynaptic ~~icilitation. Thus classical conditioning uses the same mechanism as sensitization (presynaptic facilitation), the effect of which is enhanced by the arrival of the CS  second before. In classical conditioning, the time between the CS and the US is critical: if the (i.5 arrives before the CS , no classical conditioning occurs. This makes good sur- vival sense, since Aplvsin is concerned about learning when a mild stimulus (CS) predicts a potentially threatening one ( 1 3 ) .  In Kandel?s experiments with Aplysia. CScm (siphon stimulus) was used to differentiate between the effects of sensitization and classical conditioning.

As the above discussion shows, classical conditioning is a form of associative leaining, and is therefore related to Hebbian learning [7] and its variants. However, classical conditioning is noli commuralive, because the (IS must pre- cede the U S  by a critical interval.

Kandel?s investigation of synaptic learning shows that Aplpia uses learning rules which are much more elaborate than those used by most ANNs. Aplysia employs non- commutative timing-critical associative learning, as well as two forms of non-associative learning, habituation and sen- sitization. In the following section, an abstraction of these aspects of learning in Aplvsia is presented. Note that the above description of biological synaptic function omits many interesting aspects of learning in Aplysia: see [2] for a more detailed discussion.



IV. Model of Synaptic Learning As a starting point for the development of this learning  model, we use a standard ANN model with Hebbian learn- ing described in [4]. In this model, both synaptic weights and neuron activations can take on values in the range [-V,V]. Network operation is governed by the following equations:  (3) v, = f ( p , l  VI ) I  = AV, V, - BW,l dt  (4)  where V, and VI are the current activations of the i? and j?? neurons, W,l is the weighting factor determining the effect that the j? neuron has on the irh neuron?s activation, and A and B are small constants. f (.) is a sigmoidal saturating non-linear function.

The current activation of the i* neuron is computed from a weighted summation of the current activations of all neurons which synapse on neuron i (3). Network learning, or adaptation, is governed by (4), a common form of the  Hebbian learning rule [7].

The neural network model presented above differs from  neuron biology in two important ways. Neuron activations are represented as analog values in the model, rather than as neuron firing rates. Secondly, biological neurons only have positive (unipolar) activations and synaptic weights, whereas the above model allows both positive and negative (bipolar) activations and weights.

System behavior in the model of biological synaptic function proposed in the present paper is governed by the equations:  v, = f ( E w , l k  v, ) (5 )  dw,, = C d(VI )Vk - D I VI I Wdlk + E I vk I w,lk (6) The ternary nature of these synapses is evident in ( 5 )  and (6): W,,, is the weighting factor determining the effect that correlation between the activations of the j? and k* neu- rons will have on the activation of the id? neuron. Hebbian learning, as described by (4). may be regarded as a special case of (6). in which i = j .  Using the notation of the previ- ous section. the unconditioned stimulus 11s = VU, = Vk, the conditioned stimulus CS = Vcs = VI, and (6) may be rewritten (for motor neuron MN) as:  i t  dt  - -  - Cd(Vcs)Vus - D I Vcs I W + E  I V U ~  I W (7) where the synaptic weight W is a shorthand notation for We,* = W M N . ~ ~ , ~ ~ ,  and indices CS and US represent neurons SNI and FN, respectively, rather than their activations. The term C d(Vcs)Vus implements classical conditioning, where d(Vcs) is a delayed, time-averaged version of VCS, representing the presynaptic facilitation function of F I in Fig. 3. Note that (7) differs from Hebbian learning. as it involves a correlation of two inputs, rather than an input and an output activation. Vcs must precede Vu, by a critical interval for classical conditiomng to cause a change in synaptic weight W ,  as in Aplvsia, and the size and direction of the weight change are determined by the signs and mag- nitudes of V, and VUS, as in the system described by (4).

Thus the system can learn prediclive relationships between Vcs and VUS . Note that in addition to creating a delay, the function d(V,) is gated by Vu, # 0, such that classical conditioning will not occur when Vus precedes or coincides with VCS (absence of reverse conditioning).

The second term in (7) represents habituatton. Habitua- tion occurs when V, is non-zero, and always causes W to decay towards zero. This extension of habituation to the bipolar case makes intuitive sense, since it causes non- associative weight decay when V, is active, as in habitua- tion in Aplysia. The form of the expression is also appropriate, in light of Mead?s observation, ?A great deal of inhibitory feedback in biological systems depends on activity in sensory input channels, but does not depend on the sign of the input? [SI.

Finally, the third term in (7) E Vus I W is the sensiti- zation term. Vu, activity causes an increase in the magni- tude of the weight, resulting in increased sensitivity to sub- sequent VCS signals.

In general, C > E > D ,  so associative learning dom- inates, and sensitization and habituation result in smaller non-associative weight changes. The role of habituation as a form of inhibitory (negative) feedback is apparent from    m  (7). Sensitization causes weight values to increase: how- ever, the natural saturating behavior of any practical realiza- tion of (7) will limit the effects of sensitization.



V. Synaptic Learning Circuit The analog CMOS circuits which we use to implement  our synaptic learning circuit are compact, low-accuracy amplifiers, multipliers, absolute value circuits, etc [5].  The decision to use analog, rather than digital computation results in large silicon chip area savings, at the expense of computational accuracy and repeatability. However, the biological machinery of Aplysia is even less precise, so any neural network architecture that requires high accuracy com- ponents is not biologically plausible, and therefore not of interest in the present work.

The components which we are using are transconduc tmcc circuits, where the input signals are voltages and the output is a current. Many of these circuits are variants of the circuits used in [SI, but unlike in Mead's work, the transistors in our circuits are operating above threshold (VGS > v r ) .

Vdd CSBias T B m  TCntl  Flgure 4 Synaptic circuit.

Fig. 4 is a bipolar synaptic learning circuit, built from simple analog components. This circuit approximates (7), lncorporating classical conditioning, habituation, and sensiti- zation. The synaptic weight is stored as the voltage W on capacitor CI, and weight changes are governed by:  where IC=, IH and Is are, respectively, the classical condi- tioning, habituation and sensitization weight change com- ponents. P I  - P g ,  M ,  and R ,  implement classical condition- ing. Inhibition of presynaptic facilitation (prevention of reverse conditioning) is achieved as follows: when I VU, I > V m m ,  P I  turns on MI, which turns off P Z  by  setting its bias current to zero. As a result, when 1 VU, I > Vmm, V ,  has no effect on VF,  and therefore  no classical conditioning can occur. Conversely, when I VUS 1 < V m m ,  P z  is active, and classical conditioning  can occur.

Absolute value multipliers P5 and P6 approximate habi-  tuation and sensitization. Note that W is connected to the negative input of P g ,  since habituation drives W towards zero. Below, two examples are presented which illustrate the learning behavior of this synaptic circuit. For the pur- poses of illustration, higher than typical learning rates are used, so that weight changes are evident over the relatively  short interval of these simulations.

A. Clsssical Conditioning  I ' I ' t ' $ 1  , '  -0 2  -04 1 VUSI ,--  -0 6  -08  ' ' I ' I ' ' ' ' ' ' ' ' 0 5 10 15 20 25 30 35 40  Tune us)  Figure 5 Classical conditioning and semitilation. V , precedes Vus by  critical interval, so classical condifioning occurs.

Fig. 5 shows the synaptlc circuit's response to a typical classical conditioning event. At I = 3p.s. a 2p.s conditioned stimulus (Vcs) pulse initiates a presynaptic facilitation pulse (VF).  At t e7p.s, the unconditioned stimulus (V,) is presented, resulting in an increase in the weight W ,  as the correlation relation between V,  and Vu, is learned. The critical period for classical conditioning, that is, the time after V ,  occurs in which V ,  must be presented for condi- tioning to occur, is approximately 2 to 8p.s. The shape of VF can ensily be changed by choosing different component values for the delay circuit, P3.

At t I 3Ws, Vu, is briefly pulsed negative. The presynaptic facilitation signal VF has decayed to almost zero, and thus no classical conditioning occurs. However, VUS does cause a slight increase in W ,  through the sensiti- zation mechanism. Notice that despite Vus being negative at t I 3@U, W increases, since sensitization is independent of the sign of Vus. The slow decay of W from 13 to 3Op is due to a small component mismatch in P I ,  P5 and P6.

B. Reverse Conditioning  -0.61 , , , , - -0 8  0 5 10 15 20 25 30 35 40 Time @.s)  Figure 6 Reverse condifioning. Vus occurs before VCS, so no classical Conditioning occurs.

In the example of Fig. 6, Vus occurs before VCs. and     thus no classical conditioning can occur. For 3p.1. < t < 5p.s. the presence of Vu, is preventing VCS from generating a V.G pulse, through the gating mechanism imple- mented with P i  in Fig. 4. At t = 5bs, Vu, is set to zero and V ,  begins to increase. If a second Vu, pulse were presented, for instance at t = lops, then classical condition- ing would occur.

The small weight changes from t = 2ps to 7 b ~  are due to sensitization and habituation. During the interval 2ps < I < 3w, sensitization is occurring, during the inter- val 3ps < f < 5 p .  sensitization and habituation occur (the rate of weight increase is reduced), and during 511s < I < 7p.s, only habituation occurs (the weight decays towards zero). Habituation also occurs during 3p.y < I < 5p.s in Fig. 5, but the effect on W was smaller, because the rate of habituation is determined by the product DW I Vcs I , Thus, the rate of habituation is always a frac- tion of the synaptic weight, rather than a fixed value.



VI. Implications for ANNS The development of our biology-motivated learning  model raises a number of interesting questions, including, whcther the biological learning details presented in this paper are important in ANN models. We began with a con- ventional Hebbian model, generalizing and extending it to incorporate the functions of synaptic learning found 111 Aplysia. Thus Hebbian learning (4) is a special case of (7).

in which the delay of d(.) is zero, i = j ,  and reverse wndi- tioning is allowed.

The effect of non-zero delay in d(.), together with inhi- bition of reverse conditioning, is that learning becomes non-commutative. This ?symmetry breaking? would result in non-symmetric weights (W,Ik * W,kI) in the case of a fully-connected, Hopfield-type network. This type of non- commutative learning could be incorporated into networks using contrastive Hebbian learning [9], again resulting in non-symmetric weights. Further work is planned to investi- gate this possibility.

The non-associative learning found in biological sys- tems, habituation and sensitization, may also have a role to play in ANNs. Their importance, and how to go about incorporating them into an artificial system is less clear than with non-commutative learning, because non-associative learning is not used in many ANNs. However, they are obviously important in biological neural networks, as shown by the study of a variety of animals, and therefore must be considered for inclusion in ANNs.

An important issue is whether the biological synaptic learning presented in this work depends upon a certain degree of ?hard-wiring?. For example, Aplysia has been ?wired? so that it can learn a predictive relationship between mild mantle stimuli and noxious tail stimuli. If the neural network of Fig. 3 were missing the facilitating inter- neuron FN,  then neither sensitization nor classical condi- tioning could occur. There are two interpretations of this ?pre-wiring?: it is evidence that the biology of Aplysia IS not of interest to ANN researchers, because it represents a special case in which predetermined network architecture plays a major role; or, it is an indication that tailoring a net- work architecture for a particular purpose is an essential part of neural networks, whether natural or artificial. Our ten- dency is towards the latter view, although the issue is far from settled.



VII. Conclusion This work has demonstrated that biological details,  which are omitted from most current ANN models, may be efficiently implemented with analog CMOS circuits. Three types of lesmmg, habituation, sensitization and classical conditioning, are incorporated in the synaptic learning cir- cuit which we have developed. Habituation and sensitiza- tion are types of non-associative learning, and are not often included in ANN models. Classical conditioning is a form of associative learning, related to Hebbian learning. It differs from Hebbian learning in temporally correlating two inputs at ternary synapses (synapses between three neurons), rather than the input and output of binary synapses. Classi- cal conditioning is also more complex, as it is non- commutative, resulting in a type of ?symmetry breaking? in the neural network system. Our work shows that biological synaptic learning is substantively different from current ANN learning, both in terms of learning paradigms employed, and in terms of the extensive use of ?hard- wiring? of connections in biological neural networks.

Further research is required to determine whether these aspects of neural biology are a critical part of information processing in biological and artificial neural networks.

Acknowledgements Discussions with Roland Schneider (University of Man-  itoba) and Geoffrey Hinton (University of Toronto) were very valuable in this work. The financial support of NSERC and MICRONET are also gratefully acknowledged.

