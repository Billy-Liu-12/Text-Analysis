Sorted Compressed Tree: An Improve Method of

AbstractSeveral algorithms have been proposed for association  rule mining, such as Apriori and FP Growth. In these algorithms,  a minimum support should be decided for mining large itemsets.

However, it is usually the case that several minimum supports  should be used for repeated mining to find the satisfied collection  of association rules. To cope with this problem, several  algorithms were proposed to allow the minimum support to be  adjusted without rebuilding the whole data structure for frequent  pattern mining. The Compressed and Arranged Transaction  Sequences tree (CATS tree) algorithm is one of them.

Nevertheless, CATS Tree builds its tree structure dynamically, so  that the mining process is complex and tedious. In this paper, we  present an improved algorithm called the Sorted Compressed  tree (SC tree). By pre-sorting the datasets, the tree structure can  be built statically. Moreover, association rules can be mined in a  bottom-up style instead of bi-directional in CATS tree and  recursive in FP Growth. Hence, the cost of association rule  mining is reduced. From preliminary experimental results, SC  tree is not only more efficient but is also space saving.

Keywords-association rule mining; without suppoort constrain;  CSTS tree; SC Tree

I. INTRODUCTION  Recently, the size of digital data grows in an exponential  rate. Finding out the knowledge behind the huge amounts of  data becomes an important issue. Such knowledge is derived  by statistical data and the relationship among products [1].

However, the relationships among products are not easily to  find intuitively. That is why data mining becomes an  important research topic. Among data mining methodologies,  Association Rule Mining is the most widely applied    technique.

For mining the large itemset, Apriori [2] algorithm and  Frequent Patterns growth (FP-growth) [3] algorithm are the  most well-known method. Both of them must configure a  minimum support value before mining large itemsets.

However, an appropriated minimum support value is hard to  be chosen without enough knowledge about the application  domain. Generally speaking, administrator needs to adjust the  value several times to obtain the satisfied result.

William Cheung and Osmar R. Zaiane proposed a novel  data structure called Compressed and Arranged Transaction  Sequences Tree (CATS tree) in 2003 [4]. It allowed users to  adjust the minimum support value while mining patterns and it  also improved the efficiency of the association rule mining.

Once CATS tree is built, it can be mined repeatedly with  different support thresholds without reconstructing the tree.

However, the processes of tree construction and frequent  patterns in CATS tree are quite complex and trivial. To cope  with the problem, the Sorted Compressed Tree (SC Tree) is  proposed.

Efficiency of tree construction and association rule mining  in CATS tree are improved by SC tree. The main idea is to  simplify the process of tree construction and the method of rule  mining. By pre-sorting the dataset, the data arrangement of SC  tree is consistent, so that dynamic adjustment of the tree  structure can be avoided. Moreover, association rules can be  mined in a bottom-up style instead of bi-directional in CATS  tree and recursive in FP Growth. Hence, the cost of association  rule mining can be reduced. From preliminary experimental  results, SC tree is not only more efficient but is also space  saving.



II. RELATIVE WORKS  For association rule mining, the most well-know algorithm  is Apriori algorithm. Apriori algorithm is easy to be  implemented and many algorithms are Apriori based nowadays  [5-7]. However, the main drawback of Apriori algorithm is the  low performance of mining efficiency. In this reason,  researches focused on eliminating the huge candidates during  mining process [3-4, 8-10]. The most typical algorithm is  Frequent Patterns Growth (FP Growth) algorithm proposed by  Han and et al. in 2000 [3]. Nevertheless, when the minimum  support value is changed, the pre-constructed data structure for    frequent pattern mining is useless, and a new construction is  needed.

William Cheung and Osmar R. Zaiane tried to solve this  problem and proposed a data structure called Compressed and  Arranged Transaction Sequences Tree (CATS tree) [4]. In this  algorithm, large itemsets for different minimum supports can  be found with the same data structure. That is, no  reconstruction is needed when the minimum support is changed.

Later, Grouping Compressed tree (GC tree) was proposed by  Liou in 2006 [11] to further improve the performance of CATS  tree.

A. Frequent Patterns Growth Algorithm  In the Frequent Patterns Growth (FP Growth) algorithm, an  extended prefix-tree, called frequent pattern tree (FP-tree),  was employed for storing compressed information about  frequent patterns [3, 8-10]. Bottleneck of Candidate  Generation in Apriori algorithm can be avoided in FP-growth  algorithm. FP-growth algorithm is also the representative  pattern growth based algorithm.

FP-growth algorithm improved the efficiency of mining  with three techniques: (1) Avoid scanning the database  frequently by compressing database into a highly condensed,  much smaller data structure. (2) Avoid generating candidate  itemsets by adopting pattern fragment growth method in FP-  tree-based mining. (3) Reduce the search space by  decomposing the mining task into a set of smaller tasks for  mining confined patterns in conditional databases.

FP-growth method is efficient and scalable for mining both  long and short frequent patterns and it is about an order of  magnitude faster than the Apriori algorithm. Although FP-  growth outperforms other algorithm, the difficulty of minimum  support setting still exists in this method.

B. Compressed and Arranged Transaction Sequences Tree  Algorithm  In FP-growth tree algorithm, when the minimum support  value is changed, the itemsets which is satisfied the constraint  are different. To coping with this problem, William Cheung  and Osmar R. Zaiane proposed a novel data structure called  Compressed and Arranged Transaction Sequences Tree (CATS  tree) to store the data [4]. Different from FP-growth tree, CATS  tree preserves all elements without reducing.

When minimum support threshold is changed, the tree  structure is unnecessary to be reconstructed. The only thing  needs to do is to re-execute the mining process in CATS tree  algorithm. This property shortens the system respond time  while adjusting the minimum support threshold. An example of  constructing a CATS tree is shown in Fig. 1.

Initially, the CATS Tree is empty. TID 1 (F, A, C, D, G, I,  M, P) is added by ordering. When TID 2 (A, B, C, F, L, M, O)  is added, common items, F, A, C, M are found. Items F, A, C  are extracted from TID 2 and then merged with the existent  node. Item M cannot be merged directly, so node M in CATS  Tree is swapped in front of node D and then merged. The  remaining items (B, L, O) are added beneath node M as a  branch.

The concept of prefix tree is employed for constructing tree  structure in both CATS algorithm and FP-growth, but CATS  algorithm constructs without sorting. Therefore, CATS always  needs to adjust the tree structure. The drawbacks of CATS tree  algorithm are: (1) Constructing process is complex and trivial.

(2) Arrangement of patterns in CATS tree is inconsistent. (3)  Mining operation is complicated.

Figure 1. Illustration of CATS tree construction  C. Grouping Compress Tree Algorithm  Grouping Compress Tree algorithm (GC Tree) is proposed  by Shin-Yi Liou in 2006 [11]. Firstly, they pre-process the  transaction data and then compress these data into Grouping  Compress Tree (GC Tree) data structure. Finally, they use an  efficient algorithm to find out the large item set by simplifying  the complexity of data construction and mining process.

In pre-processing stage, GC tree algorithm counts the  support value of each item and then creates an index table.

Itemset are sorted in descendent order according to their  support value and a unique index value is given to each item.

Then items in transaction database are transferred to its index  number. Because transaction data is pre-sorted, the  arrangement of the data in GC tree is consistent. It is  unnecessary to adjust the nodes while constructing, so the  constructing process is simplified.

After pre-processing, GC tree algorithm creates a left skew  tree to compress transaction data. The same subset in the tree  structure will be grouped (compressed) together. For mining,  techniques of upward counting, bottom-up mining, and    projection are employed. Although GC tree algorithm solves  the problem of complicated construction, the construction of  left skew tree still wastes the memory space.



III. SORTED COMPRESS TREE ALGORITHM  In this paper, we propose a new algorithm called Sorted  Compressed Tree (SC Tree) algorithm to cope with the  problems suffered in CATS and GC tree algorithm. We focus  on two aspects. One is to simplify the process of tree  construction, and another is to simplify the rule mining method.

SC tree algorithm mines the frequent itemset in three stages:  (1) Data pre-processing, (2) SC Tree constructing and (3) Rule  mining in SC tree. These stages will be explained in the  followings.

Identify applicable sponsor/s here. (sponsors)   A. Data Preprocessing  Sorting brings benefits for mining as it in the binary  searching tree. In our previous work proposed in 2007 [6], we  actually improved the Apriori algorithm with sorting technique.

Arrangement of sorted transactions is consistent, so it is  unnecessary to adjust the tree structure as in CATS tree.

Efficiency of tree constructing and rule mining can be  improved. Searching space can be reduced because useless  nodes can be skipped by comparing their indexes.

For a better performance and consistent arrangement, items  in SC tree will be sorted by their occurred frequency in  descendent. The preprocessing is shown in Fig. 2.

(1) Load the original transaction database.

(2) Scan the database and count the occurred frequency of all  items. Create a frequency table to record the occurred  frequency of each item.

(3) Sort the items in frequency table by the counts in  decreasing order and build an index table. This new  frequency table will be treated as Header table in the  mining process.

(4) Translate the items into index numbers. For instance, item  C will be translated to index 1 and item A be  translated to index 3.

(5) Sort the items in each transaction by its index value.

Finally, the new transaction database is obtained.

B. SC Tree Constructing  After Data Preprocessing stage, we obtain a sorted    transaction database and then start to construct the SC tree. The  constructing process is similar to FP-growth tree. An example  of constructing SC tree for TID 1 and TID 2 is shown in Fig. 3.

(1) A root is created.

(2) The first transaction leads to the construction of the first  branch in SC tree.

(3) Load each item in the next transaction. If the item exists  in SC tree, merge it and accumulate its count. Otherwise,  create a new branch. In TID 2, items 1, 2 and 3 already  exist in SC tree, so their counts will be accumulated. Then,  a new branch beneath the last common item 3 is created  for the remaining part.

(4) Repeat constructing process until all transactions are  compressed into the tree structure. The SC tree with the  five transactions is shown in Fig. 4.

By the way, a Header table will be constructed. Each item  in Header table will be linked to the corresponding node where  it was first occurred in SC tree. The nodes with same index  value (item name) will be linked to the first occurred nodes as a  linked list. The linked list will be used to mining frequent  itemset in different branches.

C. Mining in SC Tree  Nodes (items) in SC Tree are sorted, so the mining  direction in SC tree can be single direction. In order to satisfy  the constraint of minimum support (min_sup), the mining  direction in SC tree is bottom up. The mining processes in SC  tree are:  Figure 2. Database pre-processing in SC tree algorithm  Figure 3. Insert TID 1 and TID 2 in SC tree  Figure 4. Example of SC Tree   1) Filter out the non-potential items  According to the given minimum support value, SC tree  will be scanned by mining algorithm and items whose count is  larger than or equal to the minimum support constraint  (min_sup) will be find out. Those items whose count is less  than the min_sup will not be considered and their linkages will  also be ignored. Because these linkages are ignored, the items  will be filtered out in mining process. For instant, a Header  table with min_sup?3 is shown in Fig. 5.

2)  Construct conditional mining tree  In order to mine the frequent itemset in SC tree, each item    whose counts is large than or equal to min_sup will be  considered. The conditional condensed SC tree of item X will  be constructed. Item X in SC tree will be found by the linkage  recorded in Header table. Then the path along item X to the  root will be constructed. Because all the items with same index  (item name) in SC tree are linked in a linked list, the paths of  item X in different branch will be constructed either. By the  way, a Header table for mining process will be created.

An example of mining frequent itemset including item 6  with min_sup?3 is shown in Fig. 6. There are two paths, (6, 5,  3, 2, 1) and (6, 4, 1) from node 6 to the root. After constructing  these two paths, counts of items along the path will be recorded  in a Mining_Header table.

3)  Generate the frequent itemset  In mining table, a node whose count is less than the  min_sup will be deleted. Then, a conditional SC tree of  eliminated itemset will be created. Finally, the preserved paths  are the frequent itemsets.

As shown in Fig. 6, nodes whose count is less than three  will be deleted. Only item 6 and item 1 are preserved but  others are deleted. Beneath item 1, there are two preserved  items (6:2) and (6:1). After reconstructing, a new SC tree (1:3,  6:3) is created. Finally, the frequent itemset including item 6  with min_sup?3 is (1, 6).

Stage (2) and (3) will be executed for several times until all  the items in Header table whose count is large than or the same  as min_sup are considered.

Figure 5. A Header table with min_sup?3  Figure 6. . Frequent itemset mining with min_sup?3 and including item 6

IV. EXPERIMENT AND EVALUATION  In this section, we will verify the performance of SC tree,  CATS tree, and GC tree algorithm. We will focus on executing  performance and memory requirement in different minimum  support and transaction size. The equipment we used is a PC  with Pentium IV 2.8GHz processor with 2GB memory. Testing  data was generated by data generator from IBMs Almaden  project [12]. The meaning of parameters is shown as follows:  N : Number of data items  D : Number of transactions in the database  T : Average size of transactions  I : Average size of the maximal potentially frequent itemset  L : Number of maximal potentially frequent itemset    A. Efficiency Evaluation  For evaluating the performance of three algorithms, the  experiments focus on two aspects:  1) Efficiency of Tree Construction  There are 100K transactions and average size (length) of  transactions is 10 and average size of the maximal potentially  frequent itemset is 4. The result is shown as Fig. 7.

These algorithms are minimum support independent, so the  results are not influenced by different minimum supports. The  constructing time of SC tree is shorter than CATS tree and GC  tree. Then we evaluate the influences of different transaction  sizes with a fixed minimum support 0.4%. As shown in Fig. 8,  SC tree still outperforms the other two algorithms.

0.20% 0.30% 0.40% 0.50% 0.60%  Minimum Support  Ex  ec  uti  ng  T  im  e (  se  c)  CATS Tree GC Tree SC Tree  Figure 7. Tree construction time with different minimum support in  T10I4D100K database         100K 250K 500K 750K 1000K    Transaction Size  E  xe  cu  tin  g  T  im  e  (s  ec  )  CATS Tree GC Tree SC Tree  Figure 8.  Tree construction time with fixed minimum support 0.4% in  T10I4D100K~1000K database  2) Efficiency of Association Rule Mining  In this part, we focus on the efficiency of mining process.

Three algorithms are verified with different minimum supports  under 500K transactions and different transaction sizes under  fixed minimum support 0.2%. The results are shown in Fig. 9  and Fig. 10.

As shown in Fig. 9 and Fig. 10, the average mining time of  SC tree is always much less than the CATS tree and GC tree  algorithm. By concluding the experiments in this part, we can  say that SC tree algorithm outperforms CATS tree and GC tree  algorithm, especially in low minimum support cases.

0.20% 0.30% 0.40% 0.50% 0.60%  Minimum Support  E  xe  cu  tin    g  T  im  e  (s  ec  )  CATS Tree GC Tree SC Tree  Figure 9. Mining efficiency with different minimum support in T10I4D500K  database       100K 250K 500K 750K 1000K  Transaction Size  Ex  ec  ut  in  g  Ti  m  e  (s  ec  )  CATS Tree GC Tree SC Tree  Figure 10. Mining efficiency with different transaction size and fixed  minimum support 0.2%  B. Memory Requirements Evaluation  For evaluating the memory requirements of three  algorithms, the experiments focus on two aspects:  1) Memory Requirements for Tree Construction  We verify the memory requirements of tree construction  among three algorithms with different minimum supports in  T10I4D100K database. The result is shown in Fig. 11.

As shown in Fig. 11, the results are not influenced by  different minimum supports because theses algorithms are  minimum support value independent. Nevertheless, for  constructing T10I4D100K data set, CATS tree spent 15,654KB,    GC tree spent 19,881KB, and SC tree only spent 13,030KB.

SC tree algorithm saves about 17% and 34% than CATS tree  and GC tree.

Then, we evaluate the influences of different transaction  sizes with a fixed minimum support 0.4%. As shown in Fig. 12,  we can find that the average constructing space for SC tree  saves about 16% and 34% than CATS tree and GC tree.

2) Memory Requirements for Association Rule Mining  In this part, we focus on memory requirements of mining  process. Three algorithms will be verified with different  minimum support under 500K transactions and different  transaction sizes under fixed minimum support 0.2%. The  results are shown in Fig. 13 and Fig. 14.

Obviously, the average mining space for SC tree is less than  both CATS tree and GC tree.

0.20% 0.30% 0.40% 0.50% 0.60%  Minimum Support  M  em  or  y  Sp  ac  e  (K  B)  CATS Tree GC Tree SC Tree  Figure 11. Tree construction space requirement with different minimum  support in T10I4D100K database       100K 250K 500K 750K 1000K  Transaction Size    M  em  or  y  Sp  ac  e  (K  B  )  CATS Tree GC Tree SC Tree  Figure 12. Tree construction space requirement with fixed minimum support  0.4% in T10I4D100K~1000K database          0.20% 0.30% 0.40% 0.50% 0.60%  Minimum Support  M  em  or  y  Sp  ac  e  (K  B  )  CATS Tree GC Tree SC Tree  Figure 13. Memory requirements under different minimum support in  T10I4D500K database            100K 250K 500K 750K 1000K  Transaction Size  M  em  or  y  Sp  ac  e  (K  B)  CATS Tree GC Tree SC Tree  Figure 14. Memory requirements with fixed minimum support 0.2%  T10I4D100K~1000K database

V. CONCLUSION AND FURTURE WORKS  In this paper, a new tree structure called SC tree and a  mining algorithm for association rule are proposed. Advantages  of several algorithms are combined in this algorithm, and  experimental results also show that the proposed algorithm  outperforms CATS tree and GC tree mining algorithms.

For the SC tree algorithm, the mining model (tree structure)  does not need to be re-constructed while the minimum support  is changed. This property enables users to adjust the minimum  support dynamically to find out satisfied large itemsets.

Because the database is sorted before mining, the SC tree can  be mined only from the bottom-up direction, which is quite  different from the bi-directional mining in CATS tree and thus  the mining time for SC tree is shorter than CATS tree.

Moreover, the arrangement of sorted transactions is consistent,  so it is unnecessary for SC tree to adjust the tree structure as in  CATS tree. That further improves the efficiency of tree  construction in SC tree.

In addition, the memory requirements for constructing and  mining the tree structure of SC tree are also less than both the  CATS tree algorithm and the GC tree algorithm. Experimental  results show that SC tree algorithm saves 16%~ 34% of  memory space.

Although SC tree algorithm is more efficient than the other  approaches, there are still some aspects can be considered for  further improvement. One is the process of pre-sorting. When    the data set is updated, the whole database has to be re-sorted.

If only partial tree structure rather than the whole tree structure  is to be adjusted, execution time can be reduced substantially.

Another aspect is the scalability. When applying the SC  tree algorithm in a large database, employing the technique of  parallel and distributed computing can not only lightens the  loading of processers and the requirement memory space but  also suits for the architecture of disturbed computing  environment in enterprises.

REFFERENCE  [1] Peng, Y., Kou, G., Shi, Y., and Chen, Z., A Descriptive  Framework for the Field of Data Mining and Knowledge  Discovery , International Journal of Information Technology  and Decision Making, Vol. 7, Issue: 4, Page 639 - 682, 2008  [2] R. Agrawal & R. Srikant, Fast Algorithms for Mining  Association Rules, Proceedings of the 20th VLDB Conference  Santiago, pp. 487-499, September 1994.

[3] J. Han, J. Pei, Y. Yin and R. Mao, et al., Mining frequent  patterns without candidate generation: A frequent-pattern tree  approach, Data Mining and Knowledge Discovery, vol. 8, no. 1,  2004, pp. 53-87.

[4] W. Cheung and O.R. Zaiane, Incremental mining of frequent  patterns without candidate generation or support constraint,  Citeseer, 2003, pp. 111-116.

[5] G. J. Hwang, W. F. Tsai and J. C. R. Tseng, A Minimal Perfect  Hashing Approach for Mining Association Rules from Very  Large Databases, Proc. The 6th IASTED International  Conference on Internet and Multimedia Systems and  Applications, 2002, pp. 80-85.

[6] C.-K. Chiou, and J. C. R. Tseng, A Scalable Association Rules  Mining Algorithm Based on Sorting, Indexing and Triming,   Cybernetics, 2007, pp. 2257-2262.

[7] W. Pei-ji, S. Lin, B. Jin-niu and Z. Yu-lin, Mining Association  Rules Based on Apriori Algorithm and Application, Proc.

International Forum on Computer Science-Technology and  Applications, 2009, pp. 141-143.

[8] J. Pei, J. Han, B. Mortazavi-Asl, J. Wang, H. Pinto, Q. Chen, U.

Dayal and M. C. Hsu, Mining sequential patterns by pattern-   Knowledge and Data Engineering, 2004, pp. 1424-1440.

[9] G. Grahne and J. Zhu, Fast algorithms for frequent itemset     data Engineering, vol. 17, no. 10, 2005, pp. 1347-1362.

[10] L. Qihua, Z. Defu and W. Bo, A New Algorithm for Frequent  Itemsets Mining Based on Apriori and FP-Tree, Proc. WRI  Global Congress on Intelligent Systems, 2009, pp. 360-364.

[11] S.Y. Liu, An Efficiency Incremental Mining with Grouping  Compress Tree, Unpublished masters thesis, National Central  University Taoyuan Country, Taiwan, 2004.

[12] IBM Almaden Research Center,  http://www.almaden.ibm.com/cs/disciplines/iis/.

[13] S. K. Tanbeer, C. F. Ahmed and J. Byeong-Soo., Parallel and  Distributed Frequent Pattern Mining in Large Databases, Proc.

Computing and Communications, 2009, pp. 407-414.

