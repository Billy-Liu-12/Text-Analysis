Hybrid Cloud Infrastructure to Handle Large Scale  Data for Bangladesh People Search (BDPS)

Abstract? The objective of our paper is to propose and experiment a hybrid Cloud computing framework which is able to handle huge data. Previously we proposed this architecture with adequate feasibility analysis. Here we discuss a prototype system using national ID database structure of Bangladesh which is prepared. We implemented and experimented each of the modules of the system prototype. The infrastructure of our proposed system divided into two parts: locally hosted cloud and remote cloud. Remote cloud is experimented with Elastic MapReduce (EMR) of Amazon Web Service (AWS) and ?Eucalyptus? is justified for local cloud. This structure of cloud computing is handled by apache Hadoop map/reduce framework where database is implemented and experimented by HiveQL.

An experimental analysis is presented in this paper. User information visibility, authorization, authentication and integrity issues are also discussed in our literature review.

Keywords- Cloud Computing; AWS; Hadoop; Eucalyptus; HiveQL; Election Commission of Bangladesh

I.  INTRODUCTION Cloud computing is an emerging concept for Computer  network arena. Data distribution, authentication and authorization [1] are major challenges to implement an application based on public database. We proposed an Efficient and Reliable Hybrid Cloud Architecture for big Database [2].

In this work we considered national ID database, prepared by election commission (EC) of Bangladesh [3]. We implemented different modules of the system and experimented with a prototype in this paper. We also discuss some experimental findings to justify our previous proposal [3]. The structure of our proposed architecture divided into two parts. One infrastructure is locally implemented using open source ?Eucalyptus? [4], [5] and the other part of the infrastructure will be implemented on Amazon Web Service (AWS) [6] cloud. In a country like Bangladesh power failure and as well as internet connection failure are common problem. Data traffic congestion, SQL server time out issue and as a result server down is very frequent for any kind of national level searching issue like Higher Secondary school Certificate, TIN registration issue etc.

National level information access through database is an international challenge. This happened because of the risk of single server failure. To defend these problems we proposed the Hybrid cloud Structure for BDPS which will be handled by  apache Hadoop [7], [8], [9]. Apache Hadoop is an open-source software framework that supports data-intensive distributed applications. It supports the running of applications on large clusters of commodity hardware. The Hadoop framework transparently provides both reliability and data motion to applications. Hadoop uses map-reduce [7]. It divides application into various work segments and those segments can be executed or re-executed on any chosen cluster node. In addition, it provides a distributed file system that stores data on the compute nodes, providing very high aggregate bandwidth across the cluster. Both map-reduce and the distributed file systems are designed so that node failures are automatically handled by the framework. It enables applications to work with thousands of computation-independent computers and petabytes of data.

To address the [10] authentication and authorization issue and make EC?s Database in more effective, efficient and useful, we consider following ideas: 1. Everyone will have a password to access their information and take a printout in a specific format to use in official purpose. 2. Everyone can only check others information by entering info. 3. Academic, Job information, Criminal record can be entered and verified by same database. Figure 1 is showing the hybrid infrastructure for handling big data.

Figure 1.  Hybrid Structure of Cloud for Handling Big Data  This paper is formatted in following way:- section II discusses about literature review, section III describes the motivation, we proposed architecture in section IV, the experiment of Hadoop on MapReduce is discussed in section V, section VI describes about our implemented prototype and section VII discusses about  evaluation.



II. LITERATURE REVIEW Buyyaa et al. [1] have proposed architecture for market-  oriented allocation of resources within Clouds.  They have also presented a vision for the creation of global Cloud exchange for trading services. Various Cloud efforts to reveal its emerging potential for the creation of third-party services is also discussed. Lastly they have presented meta-negotiation infrastructure for global Cloud exchanges The architecture and the service model of our system are based on this model.

Makhija et al. [10] explained different existing papers, techniques and their merits and demerits. They discussed their methods of data security and privacy etc. In all those papers some haven?t described proper data security mechanisms, some were lack in supporting dynamic data operations, some were lack in ensuring data integrity, while some were lacking by high resource and computation cost. Hence their paper gives overall clue of all existing techniques for cloud data security and methods proposed for ensuring data authentication using Third Party Auditor (TPA). Mohta et al. [11] who have given algorithm which ensures data integrity and dynamic data operations. Wang et al. [12] are the first who have considered the public adaptability in their defined provable data possession (PDP) method which ensures possession of data files on untrusted storages. The author Cong Wang et al. [13] used the public key based homomorphic authenticator and to achieve a privacy-preserving public auditing system for cloud data storage security. The author Juels et al. [14] proposed a scheme ?Proof of retrievability? for large files using ?sentinels?. The security issue of our system is developed from the above discussion. Trancoso and Angeli [15] presented a brief description of GridArchSim which is going to be used for both research and education.

Ahmad et al. [16] presented a comprehensive analysis of cloud computing. They find the cloud concepts and demonstrate the cloud landscape vendors, growth of cloud computing, user concern about cloud security and worldwide web security revenue 2009 to 2015. Their business model is considered to develop our business model.



III. MOTIVATION Cloud computing [17] is a universal term for anything that  demands delivering hosted services over the Internet where Resource allocation can be adjusted. Day by day, the amount of data stored at companies like Google, Yahoo, Facebook, Amazon or Twitter has become incredibly huge. So, web applications and databases in cloud are undergoing major architectural changes to take advantage of the scalability provided by the cloud.

Cloud computing platforms possess characteristics of both clusters and Grids, with its own special attributes and capabilities such strong support for virtualization, dynamically compo sable services with Web Service interfaces, and strong support for creating 3rd party, value added services by building on Cloud compute, storage, and application services. Thus, Clouds are promising to provide services to users without reference to the infrastructure on which these are hosted.

Overall, Cloud Computing can lead to reduced costs and access to resources that perhaps any company would have difficulty in  funding or finding difficult to justify. Considering those features we proposed architecture and justified the feasibilities in our previous work [2]. We considered AWS and Windows Azure for Remote cloud implementation. Amazon Web Services (AWS) [6] is a collection of remote computing services that together make up a cloud computing platform, offered over the Internet by Amazon.com. AWS is located in 8 geographical 'Regions'. Availability Zones are isolated from each other to prevent outages from spreading between Zones.

Windows Azure [18] is a cloud computing platform and infrastructure, created by Microsoft, for building, deploying and managing applications and services through a global network of Microsoft-managed datacenters.

Open Source has advantages like leverage the work of a growing community of developers, works across multiple hardware infrastructures, possible to deploy at service providers and on-premise, customized to fit individual needs or to add additional services etc. So we considered Eucalyptus and Openstack for our model.

Eucalyptus [5] is open source software for building Amazon Web Services (AWS)-compatible private and hybrid clouds.   It allows an organization to build self-service, elastic clouds inside its datacenter using existing IT infrastructure.

Openstack [19] is a collection of open source components to deliver public and private IaaS clouds whose components are Nova, Swift, Glance, Keystone, and Quantum. IaaS Cloud Services allows users to manage VMs, Virtual networks, storage resources etc.

From the above discussion we found that AWS and Windows Azure are the options to implement Remote cloud.

Unfortunately the service of Windows Azure is not available in Bangladesh and the services are not only expensive but also limited comparing to AWS for our application. AWS made a number of resources available to the researchers some of them we used for our experiment which includes EC2, S3 etc.

Different types of instances are verified for our application.

For local cloud, a comparative study by [20] Sonali Yadav is considered where the characteristics and performance is observed for Eucalyptus, Openstack and Opennebula. From that study we found that Eucalyptus would be better option for our proposed architecture because Eucalyptus provides an EC2 -compatible cloud Computing Platform and S3-compatible Cloud Storage thus its services are available through EC2/S3 compatible APIs. Eucalyptus can leverage (drag) a heterogeneous collection of virtualization technologies within a single cloud, to incorporate resources that have already been virtualized without modifying their configuration.



IV. ARCHITECTURE OF HYBRID STRUCTURED CLOUD We considered national ID database for searching  Bangladeshi People in different purpose. An interactive web based application prototype by using hybrid structure of cloud computing has implemented in our research which is based on Hadoop with Elastic MapReduce. The EMR used four elastic (EC2) nodes that are installed on Amazon Web Service (AWS). All the core nodes including the master node is implemented on large type computing device. To address the      authentication we also enabled public key and private key ?Keypair? tool of EC2.

A. Structural Description Our proposed architecture [2] of cloud computing is  depicted in figure 1. This structure is divided into two parts.

One infrastructure will be locally implemented by using ?Eucalyptus? and the other part of the infrastructure will be implemented in well-known Amazon Web Service (AWS) cloud. On top of this infrastructure Hadoop framework would be used to implement the system. In our structure the solid lined servers are representing the ?always on? server. In local elastic cloud part those servers will be used for query handling requested by the users and in External AWS cloud those server will be used for backup and mirroring. In local elastic cloud part dashed line servers will be used as elastic computer which will be automatically ?UP? as needs' basis. The number of server will depend on the number of query request. In External AWS cloud the dashed server will be used is case of overflow request and in case of local cloud infrastructure failure. Any kind of Linux server can be used for this implementation. We used Debian Linux in our prototype cloud and Hadoop implementation. We used HiveQL [21], [22] for Database management system in our remote cloud implementation and experiment but we used MySQL for our BDPS prototype.

Figure 2.  Architecture of BDPS System  Figure 2 is showing the architecture of our proposed system. It shows that the Database is stored in the hybrid cloud which is made by Local and Remote cloud. Hadoop framework contains both structure and the Database. Data input authority and User can access this database by a user interface which is connected to Hadoop with secured communication protocol.

The selection criteria for local and remote cloud are also shown in our architecture.

B. Technology of implementation An open-source software framework Apache Hadoop [8]  supports data distributed applications. In this framework large and segmented hardware is used to run an application using  map-reduce [7] computational paradigm. In addition, Hadoop provides a distributed file system that stores data on the compute nodes, providing very high aggregate bandwidth.

We also considered Elastic Compute Cloud (EC2) and Simple Storage Service (S3) of Amazon web service (AWS) for our system. Amazon EC2 is a central part of Amazon.com's cloud computing platform. EC2 allowed us to rent virtual computers on which we could run our own application prototype. EC2 allows scalable deployment of applications by providing a Web service. We can boot an Amazon Machine Image to create a virtual machine, which Amazon calls an "instance", containing any software desired like hadoop, Hive, pig etc. We can create, launch, and terminate server instances as needed, paying by the hour for active servers, hence the term "elastic". Amazon S3 is an online file storage web service offered by Amazon Web Services. Amazon S3 provides storage through web services interfaces (REST, SOAP, and BitTorrent). We used S3 to back up our old data. We successfully experiment the data exchange between EC2 and S3 of AWS.

1) Technology of Hadoop Apache Hadoop is an open-source software framework for  storing and large scale processing of data-sets on clusters of commodity hardware. Hadoop is an Apache top-level project being built and used by a global community of contributors and users. Pig and Hive are Higher-level languages over Hadoop that generate MapReduce programs. MapReduce is a technique that distributes the processing of very large multi-structured data files across a large cluster of machines. High performance is achieved by breaking the processing into small units of work that can be run in parallel across the hundreds, potentially thousands, of nodes in the cluster. Simple data-parallel programming model designed for scalability and fault- tolerance. The challenges of cheap nodes fail, commodity network, programming distributed systems are solved by building fault-tolerance into system, push computation to the data and data-parallel programming model: users write ?map? & ?reduce? functions, system distributes work and handles faults. MapReduce programming model hides the complexity of work distribution and fault tolerance. The Principal design philosophies are make it scalable and cheap by lowering hardware, programming and admin costs. MapReduce is not suitable for all problems, but when it works, it saves quite a bit of time. The main features of Hadoop Distributed File System (HDFS) are single namespace for entire cluster and replicates data 3x for fault-tolerance.

a) Elastic MapReduce (EMR): The EMR is a web service that makes it easy to quickly and  cost-effectively process vast amounts of data. Amazon EMR uses Hadoop, an open source framework, to distribute data and processing across a resizable cluster of Amazon EC2 instances.

We implemented our hadoop on EMR where we experiment the MapReduce function through a number of SQLs by Apache Hive command.

b) Apache Hive Apache Hive is a data warehouse infrastructure built on top  of Hadoop for providing data summarization, query, and      analysis. While initially developed by Facebook, Apache Hive is now used and developed by other companies such as Netflix.

Amazon maintains a software fork of Apache Hive that is included in Amazon Elastic MapReduce on Amazon Web Services.

2) HortonWorks The Hortonworks Data Platform [12] is a framework that  allows for the distributed processing of large data sets across clusters of computers. It includes Apache Hadoop and is used for storing, processing, and analyzing large volumes of data.

The platform is designed to deal with data from many sources and formats. The platform includes various Apache Hadoop projects including the Hadoop Distributed File System, MapReduce, Pig, Hive, HBase and Zookeeper and additional components. In our current experiment we implemented the remote cloud through EMR but in case of the implementation of our hybrid cloud we would need a framework like Hortonworks Data Platform. So, we also implemented HDP [23] using four nodded cluster where one was head node using four other EC2 instances. In following part we only will discuss the result based on EMR.



V. EXPERIMENT HADOOP WITH MAPREDUCE This section will discuss the configuration and performance  of Hadoop which is implemented on Amazon Elastic MapReduce service [24].

A. Lunch Amazon MapReduce After registering with AWS we lunched an Elastic  MapReduce cluster BDPS which includes one master node and 3 core nodes that support pig and hive. In our implementation, the public domain name of our master node is: ?ec2-54-254- 211-125.ap-southeast-1.compute.amazonaws.com? and authentication SSH private key file was bdps.ppk.

1) Software and Hardware Description  TABLE I.  SOFTWARE DESCRIPTION  Software Description  Hadoop distribution  This determines which distribution of Hadoop to run on the cluster. We can choose to run the Amazon distribution of Hadoop or one of several MapR distributions. We have chosen Amazon.

AMI version  This determines the version of Hadoop and other applications such as Hive or Pig to run on the cluster. We selected AMI version 2.4.2  which means Hadoop 1.0.3.

Application s to be installed  A default Hive and Pig version is selected we can also choose other application from the Additional applications list. We did not choose additional applications.

TABLE II.  HARDWARE DESCRIPTION  Hardware Description  Network Optionally, choose a VPC subnet identifier from the list to launch the cluster in an Amazon VPC. For more information, see Select an Amazon VPC Subnet for the Cluster (Optional) in the Amazon EMR Developer Guide. We choose Launch into EC2-Classic.

Master The master node assigns Hadoop tasks to core and task nodes,  and monitors their status. There is always one master node in each cluster. This specifies the EC2 instance types to use as master nodes. Valid types are m1.small, m1.large, m1.xlarge, c1.medium, c1.xlarge, m2.xlarge, m2.2xlarge, and m2.4xlarge, cc1.4xlarge, cg1.4xlarge. We have chosen m1.large where count was by default 1 which means the number of master node is 1.

Core A core node is an EC2 instance that runs Hadoop map and reduce tasks and stores data using the Hadoop Distributed File System (HDFS). Core nodes are managed by the master node.

This specifies the EC2 instance types to use as core nodes. We have chosen m1.large where the number of core node is 3. This specifies whether to run core nodes on Spot Instances.

Task Task nodes only process Hadoop tasks and don't store data.

You can add and remove them from a cluster to manage the EC2 instance capacity your cluster uses, increasing capacity to handle peak loads and decreasing it later. Task nodes only run a TaskTracker Hadoop daemon. We did not choose any task node in our experiment. This specifies whether to run task nodes on Spot Instances.

2) EC2 Keypair Amazon EC2 uses public?key cryptography to encrypt and  decrypt login information. Public?key cryptography uses a public key to encrypt a piece of data, such as a password, then the recipient uses the private key to decrypt the data. The public and private keys are known as a key pair. To log in to any instance, it is mandatory to create a key pair, specifying the name of the key pair when launch the instance, and also need to provide the private key when connect to the instance.

Linux/UNIX instances have no password, and can use a key pair to log in using SSH. With Windows instances, can use a key pair to obtain the administrator password and then log in using Remote Desktop Connection.

B. Command Line Interface (CLI) by Putty: PuTTY is a free implementation of Telnet and SSH for  Windows and Unix platforms, along with an xterm terminal emulator. Putty allows Windows users to connect to remote systems over the Internet via Telnet and SSH. While both Telnet and SSH allow you to connect to remote systems, SSH, supported in Putty, provides for a "Secure Shell", encrypting information before it is transferred. SSH is an Internet standard, supported by many computers that also support Telnet.

C. Experiment  TABLE III.  EXPERIMENT RESULTS  Type of Instruction Total Time Process Time  Create 0.1205 N/A Show Table 0.0705 N/A Select * 0.157 N/A Drop Table 1.1805 N/A Load big data from File (3,49,900 words) 0.998 N/A SQL in Blank Database considering MapReduce 37.1595 0 SQL for small table (3X3 string) 44.235 2.901 SQL in big database for exist data considering MapReduce  44.785 4.915  SQL in big database for non-exist data considering MapReduce  43.8335 4.825  We experimented following type of instruction using PuTTY. Since executed the instruction from remote machine to      a virtual infra-structure the time of execution varies time to time. So, we counted average time that system had taken.

Table III is showing the time for different types of instructions.

In our table times are showing in Seconds. N/A means it does not involved in MapReduce operation. By this experiment, we found that due to MapReduce operation process time does not vary significantly for big or small database. Process time found almost 3 seconds for very small database and for big database it took less than 5 seconds. A dictionary of 3,49,000 different word is used to experiment big database.



VI. SYSTEM PROTOTYPE IMPLEMENTATION This National ID database of Bangladesh contains 31  information of voter which includes picture and figure print.

The total number of voter is almost 95 million on November, 2013. National ID number of Bangladesh contains 13 digits.

The structure is "DDRTTUUSSSSSS". The 1st two digits "DD" for District code of the ID holder, next one digit "R" for R.M.O code, the next two digits "TT" for Thana code, next two "UU" are for Union code and the last "SSSSSS" six digits are a sequential number for each citizen of the country. Figure 3 is showing a sample of national ID card and Figure 4 is showing the partial part of application form of election commission.

Figure 3.  Sample of National ID card of Bangladesh    Figure 4.  Application form of Election Commission  A. Deployment model for BDPS BDPS will make the database available to the general  people by using the public cloud. Here Election Commission or a National Data Center will be the service provider. These services are free for General people to access their information.

Election Commission or a National Data Center can introduce a fee for new registration and update process. Since a number of agencies will use this database, a huge amount of revenue is also possible. BDPS will also offer a pay-per-use model for the corporate user who will use this database frequently for information verification.

B. Service Model for BDPS Since PaaS helped to run the application on the web and  also provide application development toolkits, we choose PaaS  as a service model for BDPS. The user of BDPS does not manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage, but has control over the deployed applications.

One important issue in our application was the visibility.

Since it is general software and everyone have access to this database, visibility of other?s information became an important issue. In our application everyone can only verify other?s information by entering known information.

C. GUIS Our GUIs are written in PHP. First GUI in Figure 5 is the  interface of entering data. We have shown a partial record which we entered in the database. By the second GUI checked with some valid and invalid information and third GUI shows the response of information verification. By this concept we can ensure the visibility of private information from others.

Figure 5.  GUIs for information verification  D. DATABASE In our prototype, we used five tables for five kinds of data.

Our current election commission [25] stores 31 information for each voter. We kept all those data in information table.

Academic Information, Job Record, Bank Account information and Criminal Record stored in other four tables. We implemented our prototype database on MySQL. Tables IV is showing the name of fields for corresponding table. Bold-italic field means primary key.

TABLE IV.  FIELDS OF DATABASE  Table Name  Fields  Information DID, PIN_ID, Voter_ID, Name, English_name, Father_name, Mother_name, Spouse_name, Gender, Merital_status, Picture,      Qualification, Special, Date_of_birth, Birth_district, Present_address, Permanent_address, Voter_area,,Occupation,  Specification_sign, B_group, TIN, License, Passport, IRIS_DNA, Phone, Nationality, F_print, Death_date  criminal_re cord  Record_ID, National_ID, Record_no, Case_no, Type, Place, Police_station, Date, Status, Details  bank_acc_l oan  Bank_ID, National_ID, Account_name, Bank_name, Branch_name, Account_no, Card_no, Account_type, Date,  Remarks Education Education_ID, National_ID, Degree_name, Year,  Registration_no, Roll_no, Result, Marks, Remarks job_record Job_ID, National_ID, Job_title, Institute, Address,  Designation, Joining_date, Departure _date, Remarks

VII. CONCLUSION The remote cloud of our proposed hybrid structure is  justified with the experiment of Hadoop by EMR of AWS and the Local cloud is justified with the implementation of HDP.

This is a reliable and fault-tolerant system because of the characteristic of Hadoop. The main features of Hadoop Distributed File System (HDFS) are single namespace for entire cluster and replicates data 3x for fault-tolerance. We observed that the volume of data does not affect the process time significantly. We experimented a dictionary database with 3,49,000 different word and performed several HiveQL queries for that. We also experimented with some small database and found that the process time does not vary too much. So, if we use big database by increasing the number of elastic node, our system will perform efficiently. BDPS is used here as an example prototype with some unique features.

To justify our proposal we have implemented and experimented many tools of cloud computing. At the beginning we had simulated a cloud on [18] Cloudsim then we deployed Openstack and experiment some applications for performance study. After that we registered and experimented a number of applications in AWS. We lunched different type of instances like Windows, CentOS, Ubuntu etc. in different types of machine configurations. Using AWS service we deployed Hadoop on four node computing environment. Hadoop was experimented on this infer-structure. And lastly we implemented our prototype on windows apache server and verified the data entry, visibility and authentication.

Large scale data handling is a major challenge in Bangladesh. Election commission (EC) of Bangladesh has a national database with 31 types of information for 95 million of voters. Huge cost is involved to maintain this database which is supposed to use only at the time of election. Considering this point we proposed an application which would be useful to general people, government and non-government organizations along with EC. It will not only ensure the validity of data but also ensure the transparency. Our proposed structure ensured the data and database controllable and expandable according to the system?s requirement.

