Survey of Research on Big Data Storage

Abstract?With the development of cloud computing and mobile Internet, the issues related to big data have been concerned by both academe and industry. Based on the analysis of the existed work, the research progress of how to use distributed file system to meet the challenge in storing big data is expatiated, including four key techniques: storage of small files, load balancing, copy consistency, and de- duplication. We also indicate some key issues need to concern in the future work.

Keywords-big data; storage;distributed file system

I.  INTRODUCTION According to IDC (Internet Data Center), the global big  data will increase by 50 times in next decade. How to store these fast-growing, vast amounts of data? How to analysis and process these big data? A series of related problems become a common challenge faced by all enterprises, also become today's research focus. The reason why big data takes so much attention cab be summarized as follows: the network terminal changed from a single desktop to multi- terminal such as desktop, tablet PCs, book-reader PCs, mobile phones, television and so on, which greatly expands the content and scope of the network services, and improves the user's reliance on the Internet; the rapid growth of network users and the average time a user spends on the network, which makes a significant increase in the user network behavior data; the network services have changed from a single form of words to the multimedia form such as pictures, voice, video, leading to significant increase in the amount of data. All above lead to the increasing amount of data. Not just the Internet, the phenomenon of large data already exists in the fields of physics, biology, environmental ecology, automatic control and other scientific area. What?s more, big data also exist in military, communications, finance and other industries for some time. It is not a new problem, it is widespread. There is no in-depth study on it because of restricted by condition in the past. But now, with the hardware costs come down and more and more develop in science and technology, people can focus into the big data which implied the great value of.



II. THE CHARACTERISTICS OF BIG DATA The main sources of big data are data information within  the organization, sensory information in the Internet of things and interactive information in the Internet world. It is difficult to form a unified concept of big data. IDC defines it this way: Big data technologies describe a new generation of technologies and architectures, designed to economically extract value from very large volumes of a wide variety of data, by enabling high-velocity capture, discovery, and analysis [1]. This definition includes three main characteristics of big data .Two other characteristics seem relevant: value and complexity. Summarize as ?4V+C?.

? Volume: The data volume is huge, especially the huge amounts of data generated by a variety of devices, the size of the data is very large, much larger than the flow of information on the Internet, PB level will be the norm. This implies the need for large storage space and computing power.

? Velocity: The data related to perception, transmission, decision making, control open loop have very high requirements on real-time data processing. The late result is of little value. It is essentially different with traditional data processing.

? Variety: Big data may be blogs, videos, pictures, location information, etc. Even with the same kind of data, encoding, data format or other aspects may be different.

? Value: Big Data having a high commercial value. It is the reason why more and more people are concerned about the big data. But the density of the value is low. Take the video for example, during uninterrupted monitoring process, useful data may only few seconds  ? Complex: Big Data is not only complex and diverse in data types and representation. There is often a complex dynamic relationship between each data.

The change of one data may have an impact to a lot of data.



III. BIG DATA STORAGE CHALLENGES With the expansion of the application scale, the amount  of data will show the trends of explosive growth. The conventional data storage system reaches a bottleneck,  2013 12th International Symposium on Distributed Computing and Applications to Business, Engineering & Science  DOI 10.1109/DCABES.2013.21         TABLE I.  NEW CHALLENGES ON STORAGE  Characteristics Social networks Challenges  Large amount of data Large amount of data and growing rapidly Real-time Intelligence  Security Reliability Integrity  Low consumption High concurrency High efficiency  Many kinds of data Unstructured  Semi-structured Structured  High potential value  Social relationship mining  Personalized recommendation  Large fluctuations 7-8 times  unable to complete the operational task timely. Storage systems face the challenge of complex big data applications.

Take social networks for example, as shown in the TABLE I.

? Because the fluctuation of load is high and uncertain, the storage system for big data needs to dynamically match the load characteristics.

? Because of the need of high real-time in big data applications, the delay and the length of IO path in data processing should be reduced.

? High accumulation of the amount of data, high concurrent user access and high data growth require the storage system large capacity, high aggregate bandwidth, high IOPS.

Currently, there are many storage technologies to deal with big data, such as distributed file system, new type of database based on MapReduce and so on. On one hand, the great value in big data promotes development on these technologies. On the other hand, these technologies provide technical support for big data storage and make big data becoming a research hotspot in recent years. Existing technology just can meet the demand of big data storage barely. So many improvements are needed and have high research value.



IV. DISTRIBUTED FILE SYSTEM File system is the foundation of upper layer application  [2]. With the continuous development of internet applications, data is growing rapidly. So the large-scale data storage became the arduous task of the companies and research focus. Because of the limit on the expansion of storage capacity, traditional storage system is difficult to meet the big data storage.

So have to use the distributed file system to transfer system load to multiple nodes. By grouping hard disks on multiple nodes into a global storage system, distributed file system provides polymeric storage capacity and I/O bandwidth, and easy to extension according to the system scale. Generally speaking, the mainstream distributed file systems, such as Lustre, GFS, HDFS, separately store metadata and application data because of the different characteristics of store and access. Divide and rule can improve the performance of the entire file system significantly. Despite these advantages, distributed file system still has many shortcomings when it faces explosive growth of data, complex variety of storage needs. Then these  shortcomings are paid more and more attention, and become the research focus today.

A. Small Files Problem The distributed file systems, such as GFS and HDFS, are  designed for larger files. But most of the data on the Internet represent by the high frequency of small files, and more storage access are for small files in the application.

TABLE II shows the problems of traditional distributed file system in the small file management aspect.

? Small file access frequency is higher, need to access the disk for many times, so the performance of the I/O is low;  ? Small files will form a large amount of metadata, which can affect the management and retrieval performance of metadata server, and cause overall performance degradation.

? Because the file is relatively small, easy to form file fragmentation resulting in a waste of disk space;  ? To create a link for each file can easily lead to network delays.

There are some common optimization methods [3-10] as the TABLE II shown. Of course, this is not comprehensive.

In [11], the files belong to same directory will written into the same block as much as possible. This will help increase the task speed distributed by MapReduce in the future.

B. Load Balancing In order to make the multiple nodes can be a very good to  complete the task together , a variety of load balancing algorithm is proposed to eliminate or avoid unbalanced load, data traffic congestion and long reaction time.

The load balancing [10-12] can be done by two ways.

One is to prevent. One is to prevent. The I/O request is equally distributed on each storage node by the right I/O scheduling policy, so each node can be a very good job, and the situation that a part of the nodes are overloaded, but another part of the nodes are light load will not be happened.

Another way is to adjust after happened. When load imbalance phenomenon has emerged, it can be eliminated effectively by migration or copy. In general, load balancing algorithm through many years research has been relatively mature, usually divided into two types: static load balancing algorithm and dynamic load balancing algorithm.

Static load balancing algorithm has nothing to do with the current state of the system. It assigns the task by experience or pre-established strategy. This algorithm is easy, and spends little. But it does not consider the dynamic changes of the system status information, so it has blindness.

It is mainly suitable for smaller and homogeneous service systems. Classical static load balancing algorithm includes polling algorithm, ratio algorithm, priority algorithm.

Dynamic load balancing algorithm assigns the I/O task or adjust the load between nodes according to the current load of the system. Classical load balancing algorithm includes minimum connection algorithm, weighted least connection algorithm, destination address hash algorithm, source address hash algorithm.

TABLE II.  CLASSIFICATION AND ANALYSIS OF ALGORITHMS  method illustration advantage disadvantage  Metadata Management  Metadata compression reduces metadata size Improve space utilization  the metadata read performance was hurt due to extra steps of lookup file .

Performance Optimization  Utilizing prefetching and caching technologies to improve access efficiency e.g. Hot Files Caching, Metadata Caching  The improvement of the Cache hit ratio Just for particular application  Small file merging  A set of correlated files is combined into a single large file to reduce the file count.

An indexing mechanism has been built to access the individual files from the corresponding combined file.

Reduce the metadata Two indexes, affect the speed  sequence files Form by a series of binary, where key is the name of the file, the value of the file content.

Free access for small files, nor restrict how much users and files  Platform dependent  Way to store Small files stored separately in separate areas Reduce disk fragmentation Complexity of the movement  Dynamic load balancing algorithm can be divided into centralized and distributed strategy. TABLE III shows the advantages and disadvantages of them.

Load balancing algorithm in the distributed file system can also be divided into sender starting method and recipient starting method. Sender starting method starts load distribution activities by overload point, by which part of the load of the overloaded nodes is sent to light load nodes, is suitable for the system as a whole in light load condition, because of more light load nodes in system, light load node is easy to found, so frequent migration won?t happen.

Recipient starting method starts load distribution activities by light load nodes which apply for part of the load of the overloaded nodes, is relatively effective when the system as a whole is in a state of overload, the reasons are similar to the above.

TABLE III.  COMPARISON BETWEEN CENTRALIZED AND DISTRIBUTED STRATEGY  Principle  Centralized strategy  Distributed strategy The function of the dynamic load balancing is concentrated in a special load management server. The server is responsible for the load information collection and maintenance of the whole system.

There is no specific load management server, so each node need to collect, record and manage the load information of the surrounding nodes.

Advantages  Simple and less communication cost; The best node for  migration or copy can be found; Suitable for large system model  High reliability, easy to extend; There is no single point failure, paralysis of any host will not affect the normal work of the whole system.

Disadvantages  Low reliability, hard to extend; Once the load management server fails, the whole load balancing system will be paralyzed; The larger the size of the system, the more complex the management.

When the system is very large, the communication cost of load information collection will geometrically growth; Because each node only master a small amount of load information, and the load regulation is local, the result may be not satisfactory, and even have a chance to cause the load to move back.

Each type of load balancing strategy above has advantages and disadvantage, so some mix strategy emerged.

In [12], keep three queues (light load, optimal load, overload) in master server, use priority algorithm among these queues, and weighting polling algorithm in the queues.

Because the centralized load balancing has the advantages of less communication overhead that is particularly important for the large data storage, so a lot of research enhances its reliability by changing the system structure. A more common way is to decompose complex load balancing management tasks by layering or grading and accomplished by multiple servers. In [13], a hierarchical dynamic load balancing strategy was proposed. An intermediate layer was added between the metadata server and the client, which was designed to collect load status information especially to reduce the load information acquisition cost, and by adding a backup server of load management to solve the problem of low reliability. In [14], a load balancing strategies of two-stage meta server cluster file system was proposed, using the idea that the high level manage the low level, the task allocation and task processing functions of the meta server were decentralized, which improves the parallel processing ability and extensibility of the system, and put forward a model of the heat reflecting accessing frequency of the storage node to determine the ideal location of the file to be saved.

The process of load balancing scheduling is complex, especially in dynamic equilibrium. In addition to the scheduling policy, there are many other factors, such as how to collect information, what information should be collected, when should migrate task, where the task should be migrated.

More attention is paid to the related research of load evaluation index which is used to judge when to migrate.

HDFS?s equilibrium strategies use the single evaluation index of disk usage, which has considerable limitations. In [15], the evaluation function is determined by multiple attributes and the server's load condition is determined by double threshold value.

However, the method above still can't solve the limitations of threshold. Usually thresholds are calculated and set in advance, if the thresholds are set too high, when the average system load is lighter, the phenomenon that part of the nodes are idle while the other part are busy may appear; If the threshold is set too low, when the average         system load is heavier, not only the phenomenon of frequent migration will appear, but also that a migration triggers a new migration will happen. That will cause the system instability. Dynamic threshold, which determines the threshold by the current system load situation, has the potential to become one of the ways to solve this problem and has very high research value.

C. Replica Consistency In distributed file system, replication technology is  widely used to improve the reliability and performance of system. Ensure the safety and reliability of the data and fault tolerance of the system by storing multiple copies, with that just using another copy can normally access the data when a copy is destroyed. In addition, when the system is larger, the use of replication technology on different servers to store a copy of the same file helps to achieve the load balancing of the system, thereby improving overall performance.

Nowadays the master - slave replica model is adopted by the most study of the Replica consistency, and it is the most original way for the salve replica to receive the update passively from the master replica. In [16] two consistency protocols based the tree are proposed: aggressive copy coherence and lazy copy coherence. With the root node regarded as the master copy, in the aggressive copy coherence, once the data of the root node update, the update will broadcast to each copy nodes, and the copy nodes will be updated; in the lazy copy coherence, the copy nodes are accessed by comparing timestamp to check their own data whether the latest version or not, if not, they will update.

Therefore, though the aggressive copy coherence can guarantee update from the copy in a timely manner, in the big data environment, the system will be given a great deal of load pressure; while though the lazy copy coherence can reduce the network load, but will have access latency, and when the master copy is damaged it may not be restored from a copy of which has not been updated.

However, in the master - slave replica model, as the master copy is the only update source, the efficiency will by reduced and it will be the bottleneck of the system performance improvement. In [17] a non-centralized copy update consistency model based on the replica identification of timestamp was proposed, which effectively solves the bottleneck problem of the original model by substituting the master copy nodes to the slave copy nodes to trigger the update process. In addition, it also referred to a strategy of creating copy dynamically when a request exceeds the threshold, but no extra copies of deletion policy was referred, which makes a number of file copy used to be hot in the system resulting in a large number of redundant. In [18] the dynamic replication strategy was referred, dynamically changing the number of copies, including the creation and deletion.

In the actual application, load fluctuation of distributed file system of is very big, so if consistency detection can be done when the load is light, not only the data will be more reliable, but also the system resources will be fully used.

D. De-duplication Extensive redundancy exists in the distributed file  system, such as multi-user storing the same file, different versions of the unified file, similar file header of the same type file and so on. De-duplication is a very popular storage technology, effectively optimizing storage capacity. It deletes duplicate data in data set, leaving only one, thus eliminating redundant data.

The mainstream method to determine whether the data block is repeated is by the way of computing their hash value.

If the hash value of the data block matches with a value from the hash index table, it indicates that data block is repeated, and substitute it with a pointer to the data storage. With the increase of the index, the memory performance will rapidly decline. So some researches focus on the approach by which more redundant data can be reduced, and some investigate how to do data de-duplication at high speed.

Another key technology is the division of the data. In file units to detect the speed will be very fast, but even if many of the same data exist in many different files, the duplicate data in the file can?t be deleted, so the file are needed to be divided, and are tested in block units. In [19], five representative chunking algorithms of data de-duplication are introduced and their performance on real data set is compared.

By de-duplication the rapid growth of data is effectively controlled, effective storage space is increased, storage efficiency is improved, and so on. However the reliability of data will be affected, and multiple files rely on the same data block, that is to say, multiple files are damaged, if the data block is damaged. Therefore, we can consider conducting the de-duplication firstly, and then backup appropriately for the data set with low access rate, by which storage space is saved, and the reliability of the data is ensured.



V. CONCLUSION Despite the big data is popular recently, the study of it is  not just started. There have been many countermeasures and related technologies to meet the challenge of big data. This paper summarizes and analyzes four technologies of distributed file system on big data storage, and gives some outlook for further study.

