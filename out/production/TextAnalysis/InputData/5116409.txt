Mining Maximal Hyperclique pattern: A Hyperclique Pattern Growth

Abstract?Mining of confident patterns from the datasets with skewed support distributions is a very important problem in the pattern discovery field. A hyperclique pattern is presented as a new type of association pattern for mining such datasets, in which items are highly affiliated with each other. The maximal hyperclique pattern is a more compact representation of a group of hyperclique patterns. In this paper, we present a fast algorithm of mining maximal hyperclique pattern called hyperclique pattern growth (HCP-growth) based on frequent pattern tree (FP-tree). The algorithm adopts recursive mining method without any candidate generation and exploits many efficient pruning strategies. The experimental results demonstrate that our algorithm is more effective than the standard maximal hyperclique pattern mining algorithm, particularly for the large-scale datasets.

Keywords-hyperclique pattern; FP-tree; data mining; pruning strategy

I.  INTRODUCTION Mining of the confident patterns from the datasets with  skewed support distributions is a very important problem in the pattern discovery field because the appropriate minimum support threshold couldn?t be selected by the current frequent pattern mining algorithms [1,4,5]. If the threshold is too low, these approaches may lose efficiency and the frequent patterns usually contain some items which are weakly related to each other. On the other hand, if the threshold is too high, many strong affinity patterns involving items with low support levels are missed. Such a distribution has been observed in many kinds of datasets, including retail data, Web click-streams, census data and telecommunication data.

To mine these datasets, a hyperclique pattern [1,4] was proposed as a new type of association patterns in which items are highly affiliated with each other. The presence of an item strongly implies the presence of every other item that belongs to the same hyperclique pattern. A maximal hyperclique pattern is defined as a hyperclique pattern for which none of its superset is a hyperclique pattern. Maximal hyperclique patterns are more significant and desirable in many application domains, such as pattern preserving clustering [12] and text categorization [9].

Hybrid MHP algorithm [2] is an efficient approach of mining maximal hyperclique patterns for most common datasets. But for the large-scale datasets, the storage space  produced by hybrid MHP algorithm is huge, so it's difficult to run on a common pc.

As a result, the objective of this paper is to design an efficient algorithm for mining maximal hyperclique patterns for large-scale datasets, which is called hyperclique pattern growth (HCP-growth). The algorithm is based on frequent pattern tree (FP-tree) which is an effective approach to represent the data. The algorithm exploits efficient many pruning strategies and adopts recursive mining method without any candidate generation. The experimental results demonstrate that HCP-growth is more effective than hybrid MHP mining algorithm.?

II. RELATED WORKS Since Agrawal et al. [6] proposed the classical Apriori  algorithm to discover frequent patterns, some other researchers have proposed the concepts of maximal and closed frequent patterns [8,11]. Xiong et al. introduced the concept of hyperclique patterns for mining the strong affinity association rules [1,4].

Some store structures representing the datasets was also proposed in order to facilitate the traverse. Han et al.

constructed a very compact structure, FP-tree [3], which is applied by many DFS mining approaches such as FP-growth [3] and H-Mine [10]. Burdick et al. presented a vertical bitmap representation for a dataset by which can quicken the itemset generation and support counting [8]. Based on this structure, Huang et al. proposed hybrid MHP algorithm [2] for mining maximal hyperclique patterns.



III. BASIC CONCEPTS To facilitate our discussion, we first present some basic  concepts.

A. Frequent pattern tree Definition 1 A frequent pattern tree (FP-tree) is a tree structure defined as below.

1)  A FP-tree consists of one root labeled as "null", a set of item prefix subtrees as the children of the root, and a frequent-item header table.

2)  Each node in the item prefix subtree consists of three fields: item-name, count, and node-link, where item-name registers which item this node represents, count registers  2008 International Seminar on Business and Information Management  DOI 10.1109/ISBIM.2008.60   2008 International Seminar on Business and Information Management  DOI 10.1109/ISBIM.2008.60   2008 International Seminar on Business and Information Management  DOI 10.1109/ISBIM.2008.60   2008 International Seminar on Business and Information Management  DOI 10.1109/ISBIM.2008.60     the number of transactions represented by the portion of the path reaching this node, and node-link links to the next node in the FP-tree carrying the same item-name, or null if there is none.

3)  Each entry in the frequent-item header table consists of two fields, item-name and head of node-link which points to the first node in the FP-tree carrying the item-name.

FP-tree is an efficient data compression structure. The construction algorithm of a FP-tree is described in [3].

Example 1 Let the transaction database, DB, be the first two columns of Table I and the minimum support count be 3.

According to steps of the construction, the items are sorted in support descending order as show in Table II and the ordered frequent items of each transaction are shown in the last column of Table I. Then a FP-tree can be designed as shown in Fig. 1.

TABLE I.  LIST OF ITEMS IN DATABASE  TID List of Item ID Ordered Frequent Items 1 b c d e f h c f b d h e 2 a b d f a f b d 3 a c d f h a c f d h 4 a c f g h a c f h 5 a b c d e a c b d e 6 a b e a b e 7 a c a c 8 a c f h a c f h  TABLE II.  THE SUPPORT COUNT OF EACH ITEM IN DATABASE  item a c f b d h e g support 7 6 5 4 4 4 3 1   Figure 1.  The FP-tree of Example 1.

B. Hyperclique pattern Definition 2 The h-confidence of an itemset P={a1, a2,?,am}, denoted by hconf(P), is a measure that reflects the overall affinity among items within the itemset. This measure is defined as min{conf(a1?a2,?,am),conf (a2?a1,a3,?,am),?,conf((am?a1,?,am-1)), where conf is the conventional definition of association rule confidence [6].

According to the definition of conf and h-confidence,  hconf(P) = 1...

( ) { ( )}kk m  sup P max sup a =  = 1...

{ ( )}kk m  supcount(P) max supcount a =  (1)  where sup is the support of an itemset and supcount is the support count .

Definition 3 An itemset P is a hyperclique pattern (HCP) if sup(P) ?minsup and hconf(P)?minhconf, where minsup is a user-specified minimal support threshold and minhconf is a user-specified minimal h-confidence threshold.

Definition 4 For a hyperclique pattern, if none of its super sets is a HCP, it?s a maximal hyperclique pattern (MHCP).



IV. THE MINING ALGORITHM We will develop an efficient method for mining the  complete set of MHCPs based on the FP-tree structure.

Theorem 1 Given an HCP P that contains ai and aj, and sup(ai)?sup(aj), supcount(aj) has the following upper bound:  upper(supcount(aj))= ( )isupcount a  minhconf ? ? ? ? ? ?  (2)  Proof According to the definition of HCP and formula (1),  1...

( ) { ( )}kk m  supcount P max supcount a =  ?minhconf.  So  supcount(aj)?  1...

{ ( )}kk mmax supcount a= ?  ( )supcount P minhconf  ? ( )isupcount a minhconf  .

Therefore,  upper(supcount(aj))= ( )isupcount a  minhconf ? ? ? ? ? ?  .

Theorem 1 can be used to prune the search space. For example, set minhconf to 0.6 and the support count of an item ai to 3, the maximal support count of the item which can construct a HCP with ai is 5. All of the items whose support count is greater than 5 should be ignored for they can?t construct HCPs with ai. The strategy prunes the item whose support count is greater than the maximal support count and is defined as maximal support pruning strategy.

Example 2 Given minhconf=0.6 and based on Example 1, the mining process can collect all the MHCPs that a node ai participates by starting from ai's head in the header table of FP-tree shown in Fig. 1 and following ai's node-links. The process starts from the bottom of the header table.

Figure 2.  The prefix paths of e  For node e, its support count is 3. The maximal support of the items which can construct the HCP with e should not be greater than 5 according to the maximal support pruning strategy. Hence a, c are both pruned. As shown in Fig. 2, the     HCPs containing e may be in the only three paths: (f, b, d, h, e), (b, d, e) and (b, e). The count of e's each node is 1 and indicates each pattern represented by the path appears once in the database. Thus to study which items appear together with e, only e?s prefix paths count. Then these three prefix paths of e, {(f:1, b:1, d:1, h:1),(b:1, d:1),(b:1)}, form e?s sub pattern base, which is called e?s conditional pattern base.

To avoid confusion of the item ai?s support count in the whole dataset, the support count of ai in conditional pattern base is defined as relative support count, denoted by ai.rsupcount. The items and their relative support counts in e?s conditional pattern base are f:1, b:3, d:2, h:1, which mean the support counts of the patterns are fe:1, be:3, de:2, he:1.

So f and h are pruned by the conventional minimal support pruning strategy.

On the other hand, the support count of d is 4 and the relative support count of d is 2, so d should be pruned for hconf(de) is 0.5 and less than the h-confidence threshold.

This strategy is defined as item self pruning strategy whose correctness will be proved by the flowing theorem.

Theorem 2 Given an item aj which lies in the conditional pattern base of ai and an HCP P which is derived by the analysis to ai and includes ai and aj, we have aj.rsupcount ? supcount(aj) * minhconf.

Proof Since P is derived by the analysis to ai and includes ai and aj, aj.rsupcount ? supcount(P). According to formula (1), P.supcount ?  1...k m max =  {supcount(aj)} * minhconf ? supcount(aj)  * minhconf. Thus, aj.rsupcount ? supcount(aj) * minhconf.

For the remaining items, we can construct a new tree like  FP-tree which is called e?s conditional hyperclique pattern tree (CHCP-tree). Hence, e?s CHCP-tree is {(b:3)}|e. So only one HCP, (be:3), is derived and hconf(be) = 0.75. The search for HCPs associated with e terminates.

For node h, it derives the conditional pattern base as {(c:1, f:1, b:1, d:1),(c:1, f:1, d:1),(c:2, f:2)}. Notice that although e appears together with h as well, there is no need to include e here in the analysis since any HCPs involving e has been analyzed in the previous examination of e. Similar to the above analysis, b is pruned by the minimal support pruning strategy and d is pruned by the item self pruning strategy. So we derive h?s CHCP-tree, {(c:4, f:4)}|h. Then we perform the above mining procedure recursively.

The header table of h?s CHCP-tree includes 2 items, c and f. Item f derives the HCP (fh:4), and then derives fh?s conditional pattern base {(c:4)} and the CHCP-tree {(c:4)}|fh. Since fh?s CHCP-tree is not null, so fh must have a superset and isn?t a MHCP. Mining recursively, we can get the HCP fch and its CHCP-tree is null. So fch is a MHCP locally and we define it as the local maximal hyperclique pattern (LMHCP). If there is no it?s superset in all of the MHCPs previously produced, fch must be a MHCP exactly.

If we continue to analyze item c, it can derive the non- MHCP, fc, which is a subset of fch. In fact, this analysis is not necessary. If the new produced LMHCP has involved all the remaining unanalyzed items of the CHCP-tree?s header table, all of the hyperclique patterns that these items derives must be the subset of the LMHCP. So it?s no necessary to analysis these remaining items. This strategy is defined as  the remaining item pruning strategy which significantly improves the performance of the mining procedure.

Similarly, by analysis to node d, b, f, c, a, we can derive their corresponding patterns. All of the conditional pattern base, CHCP-tree, LMHCPs and MHCPs are summarized in Table III. The mining result is {be:3, cfh:4, bd:3, fd:3, ac:5}.

TABLE III.  MINING RESULT  Item Conditional pattern base CHCP-tree LMHCPs MHCPs e {(f:1, b:1, d:1, h:1),(b:1, d:1),(b:1)} {(b:3)}|e be:3 be:3  h {(c:1, f:1, b:1, d:1),(c:1, f:1, d:1),  (c:2, f:2)} {(c:4, f:4)}|h cfh:4 cfh:4  d {(c:1, f:1, b:1),(f:1, b:1), (c:1, f:1),  (c:1, b:1)} {(f:3, b:2),(b:1)}|d Bd:3, fd:3 bd:3, fd:3  b {(c:1, f:1),(f:1),(c:1)} ? b:4 ? f {(c:1),(a:1),(a:3, c:3)} {(c:4)}|f cf:4 ? c {(a:5)} {(a:5)}|c ac:5 ac:5 a ? ? a:7 ?  Summarizing the process of Example 2, we present HCP- growth algorithm to mine maximal hyperclique patterns.

HCP-growth Algorithm Input: FP-tree Tree, minsup, and minhconf.

Output: the complete set of MHCPs.

Method: call HCP-growth(FP-tree, null) Procedure HCP-growth(Tree, ?) { (1) for each item ?i in the header of Tree from bottom up do{ (2) generate pattern ?=?i?? with support=?i.rsupcount; (3) create ??s condition pattern base and CHCP-tree Tree? (4) if Tree??? then { (5) call HCP-growth(Tree?,?) to derive new patterns; (6) if the new produced local hyperclique pattern  involves all of the remaining items in the tree then return;}  (7) else if there is no ??s superset in the existing MHCPs then output ? as a MHCP;}  } The correctness and completeness of HCP-growth can be  justified by the above theorems and analysis. Now let?s examine the efficiency of the algorithm.

HCP-growth mining process scans the FP-tree of DB once and generates a small pattern-base ?i for each frequent item i, but each ?i is much smaller than the original FP-tree.

The process is then recursively performed on the small pattern-base ?i by construction a CHCP-tree for ?i which should be usually much smaller and never bigger than ?i.

Thus, each subsequent mining process works on a set of usually much smaller pattern bases and CHCP-trees.

Moreover, the remaining item pruning strategy can efficiently shorten the process of recursive mining. Thus the algorithm is efficient.



V. EXPERIMENTAL EVALUATION In this section, we present a performance comparison of  HCP-growth with the recently proposed efficient maximal hyperclique pattern mining method, hybrid MHP algorithm.

All the experiments are performed on a 2.8 GHz Pentium IV PC machine with 1GB of memory running Windows XP.

We report experimental results on three real world datasets.

First, Alarm1 and Alarm2 dataset (http://www.pris.net.cn/ alarm/alarm.htm) is the alarm transaction datasets from telecommunication networks, in which each item represents a type of alarm. Second, Retail dataset (http://fimi.cs.helsinki. fi/data/) is a retail market basket data from an anonymous Belgian retail store. The characteristics of these datasets are summarized in Table IV.

TABLE IV.   REAL DATASET CHARACTERISTICS  Data set #Item #Record Avg. Length Min. Length Max. Length Alarm1 96801 104823 28.6 1 1062 Alarm2 8261 22255 29.5 1 320 Retail 16470 88162 10.3 1 76  For Alarm1 dataset, hybrid HMP is unable to generate patterns as it runs out of memory because the storage space of all bitmaps produced by hybrid MHP algorithm is 104823*96801*2bit, about 2.4GByte, while HCP-growth only needs less than 300MBytes. Fig. 3(a) shows the run time of HCP-growth mining for Alarm1 dataset.

0.5 0.6 0.7 0.8 0.9 minhconf  Ex ec  ut io  n Ti  m e  (s ec  )  HCP-growth(minsup=0.0001) HCP-growth(minsup=0.0005) HCP-growth(minsup=0.001) HCP-growth(minsup=0.005)   (a) Execution time of HCP-growth on Alarm1 dataset      0.5 0.6 0.7 0.8 0.9 minhconf  Ex ec  ut io  n Ti  m e  (s ec  )  HCP-growth(minsup=0.0001) Hybrid MHP(minsup=0.0001) HCP-growth(minsup=0.0005) Hybrid MHP(minsup=0.0005)   (b) mining alarm2 dataset       0.5 0.6 0.7 0.8 0.9 minhconf  Ex ec  ut io  n Ti  m e  (s ec  )  HCP-growth(minsup=0.0001) Hybrid(minsup=0.0001) HCP-growth(minsup=0.0005) Hybrid(minsup=0.0005)   (c) mining retail dataset  Figure 3.  Execution time comparison  For Alarm2 and Retail datasets, the running time of HCP-growth and hybrid MHP are shown in Fig. 3(b) and (c).

Obviously, HCP-growth has good performance whatever the support threshold or h-confidence threshold is. Especially for Retail dataset, hybrid MHP needs more than 100 seconds while HCP-growth only uses less than 10 seconds.



VI. CONCLUSIONS In this paper, we present a fast algorithm of mining  maximal hyperclique pattern called hyperclique pattern growth (HCP-growth). The algorithm adopts recursive mining method without any candidate generation and exploits efficient pruning strategies, including maximal support pruning strategy, item self pruning strategy and remaining item pruning strategy. The experimental results demonstrate that our algorithm is more effective than the standard maximal hyperclique pattern mining algorithm.

