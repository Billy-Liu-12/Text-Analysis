Current Challenges and Approaches for Resource Demand Estimation in the Cloud

Abstract?The increasing popularity of Cloud computing, especially for high performance computing (HPC) applications offers a huge potential for optimizing the consumption of compute resources. Since hybrid Cloud platforms in particular offer the best balance between data security, performance, business agility and mobile support, they are used more and more frequently. In this work, we highlight the most important challenges that arise for resource demand estimation systems, especially in public and hybrid Cloud environments. We present existing approaches, separated in load- balancing - or single resource type systems - and Cloud or virtual machine (VM) type selection - or multiple resource type systems. The approaches are analyzed in different aspects including their potential to overcome the presented challenges and their applicability in different Cloud environments. Our research reveals that not all of the issues have been resolved yet but the means to achieve that are available. We conclude our work with useful suggestions that can help to overcome the remaining challenges.

Index Terms?cloud computing, resource demand estimation, survey  ?  1 INTRODUCTION  CLOUD computing nowadays has a hugeimpact on many IT solutions [1] for mostly big, but also small and medium sized enter- prises [2], [3]. Many popular Cloud definitions, e.g. from Vaquero et al. [4] and the NIST def- inition of Cloud computing [5] highlight the most important benefits of Cloud computing.

Those benefits are the flexibility and scalability, the cost benefits of the pay-as-you-go principle and the possibility to access it from almost everywhere worldwide with different devices.

On the other hand, there still exist many chal- lenges in Cloud computing that seem to be not or only partially solved [6], [7], [8]. The biggest challenges found in the literature are data security and privacy but also vendor lock- in and uncertainty about the actual cost and time benefit play quite a key role. We highlight the most important expectations and concerns that enterprises raise, based on those benefits  and challenges, whilst considering moving the whole or parts of their IT infrastructure into the Cloud. This work further elaborates on the question if they are justified or not. Finally, we particularly emphasize the importance of resource demand estimation in this particular area.

1.1 Enterprises and Cloud Computing  Due to the above characteristics of Cloud com- puting, many enterprises aim to reduce their capital expenditure (CAPEX) and the overall cost, and to increase their flexibility. A survey by Narasimhan et al. revealed that compa- nies which already use the Cloud are more interested in gaining flexibility and improved mobile access for their services than cost re- duction [9]. Gupta et al. also discovered that the number one reason for especially small and medium sized enterprises (SMEs) to use the Cloud is surprisingly not cost reduction but   DOI 10.1109/CLOUDCOM-ASIA.2013.52    DOI 10.1109/CLOUDCOM-ASIA.2013.52     business agility [3]. For the most part, cost reduction is still an important factor which leads to the conclusion that the objective for most companies is to provide business agility and mobile access while keeping the cost as low as possible. One of the biggest concerns about Cloud computing since the beginning is data security and privacy [10], [11]. That is still the case for many Cloud adopters that are already using the Cloud but other factors are almost as important ? factors like relia- bility, customizability, user adoption and ease of integration. In fact, 28 percent of the cloud adaptors, which already are using the cloud, state that the security concerns are one of the biggest misconceptions about Cloud comput- ing. More than half of the respondents believe that Cloud solutions perform better in terms of security compared to on-premise applications which is also the case for many other important properties. Those are shown in Figure 1.

9%  10%  8%  11%  14%  17%  14%  19%  23%  12%  67%  64%  63%  62%  61%  57%  55%  53%  53%  52%  Total cost of ownership  Time to value  Availability  Ease of deployment  Ease of integration  Customizability  User adoption  Reliability  Security  Vendor lock-in  Percent of respondents  Cloud solutions somewhat/significantly worse Cloud solutions somewhat/significantly better  Fig. 1. Cloud adopters? perception of cloud ap- plications versus on-premises applications. Per- centages do not add up to 100 because ?about the same? responses are excluded [9].

Since business agility and cost are still im- portant concerns, it is useful to be able to predict the amount of necessary resources in advance so that a sufficient amount of them can be set up in advance. That is not only useful because it usually takes some time for a virtual compute instance to be up and running from the time it has been started. Being able to predict in which point of time a specific  application needs more resources can be used to reserve the necessary amount of compute instances in advance, which is usually cheaper than requesting them on the fly [12].

1.2 Computational Expensive Tasks  The execution of scientific applications or ana- lytical processes in the Cloud is also rapidly gaining popularity. In many previous works it has been shown that outsourcing of com- putational expensive tasks in the Cloud can be beneficial. In [13] Deelman et al. simulated the cost performance of different execution and resource provisioning plans for a real-life as- tronomy application and found out that cost can be significantly reduced with no signif- icant impact on application performance by provisioning the right amount of storage and compute resources. We discovered that for par- allel applications the correct setup of instances can even reduce the runtime and cost of an application due to the fact that virtual com- pute instances are usually provided as packed resources, consisting of CPU, hard disk and memory [14]. The application we used in our study consumes mostly compute power. Thus, it is more efficient to select smaller compute instances with less memory but a better pro- cessing unit. On the other hand, it would be fatal to choose an instance with less memory than the application actually requires since that would result in swapping. He et al. measured already in 2010 that virtualization technology adds only a little performance overhead whilst executing HPC applications in the Cloud [15].

However, they also pointed out that due to the slow networking performance of virtual com- pute nodes in public clouds, the application of a private cloud is more beneficial. Nonetheless, they promoted that public Cloud platforms can also be utilized by scientists despite those de- ficiencies. Although private Clouds seem to be the best solution concerning runtime, especially network performance, and security they are not applicable in a scenario where data or services have to be provided publicly. Also many public Cloud providers offer different types of virtual compute instances which offer different bene- fits for one application. Therefore, it is helpful     to predict in advance, which platform offers the instances with the best performance for a specific task or if it is even useful to utilize multiple platforms at once.

The remainder of this paper is organized as follows. In Section 2 we highlight differ- ent challenges in resource demand estimation.

After general difficulties, we focus on spe- cific challenges for public Clouds and hybrid Clouds in particular. Next, in Section 3 we present different existing solutions for resource demand estimation which represent the current state in this area of research. We focus on their applicability in solving the challenges we men- tioned earlier and especially in hybrid Cloud scenarios. Finally, we conclude this work in Section 4 and present possible solutions to the problems which are still remaining.

2 CHALLENGES  Many researchers concerned themselves with resource demand estimation and the associ- ated challenges in this research area. Already in 1998, Dilleya et al. worked out different factors affecting the performance of a single web server [16]. Those are for example the web server pool size, the underlying network topol- ogy and server system configuration properties like the HTTP object cache size. An obvious difficulty is to consider many different types of hardware or arbitrary applications with differ- ent properties. Especially for distributed com- puting, resource demand estimation is most effective, if only one type of resource package has to be considered. In the optimal case, a certain number of those resources have to be allocated to compute jobs that all have the same properties. Unfortunately, that is gener- ally not the case. That the distribution of jobs in Cloud computing is important has already been demonstrated by researchers. Stantchev confirmed that replication configurations in Cloud computing can have a positive effect on non-functional properties using benchmark- ing [17]. He measured an increase of transac- tions per second and a decrease of response time with his experimental setup. Chieu et al.

discovered that an effective use of the dynamic  scaling property in Cloud computing is possi- ble for Web applications [18].

Another important challenge besides net- work and Cloud related issues is the capa- bility of an estimation system to detect nec- essary resource adaptions in real-time if not even in advance, since it usually takes up to one minute for a compute instance in the Cloud to be available after requesting it. To be cost efficient, it is also important to not reserve more resources than actually required.

Too many unused resources can be a major problem in a data center as well, especially in private Clouds where those resources cannot be utilized by other applications. Thus, a trade- off has to be made between meeting quality of service (QoS) requirements and high resource utilization [19]. Stefano et al. discovered many factors affecting the design of load balancing al- gorithms in distributed systems [20]. The most important ones include the current workload of a host and information about the structure of a process. Thus, it is required to monitor the application state for applications with a variable load for efficient load-balancing. Based on the amount of information that is avail- able from monitoring, three different categories for load-balancing algorithms exist. Black-box approaches, where no information about the actual workload is available and only the ex- ecution time can be measured, grey-box ap- proaches, that allow at least access to operating system specific values like memory consump- tion or CPU utilization, and white-box ap- proaches, which allow the complete monitoring of the whole system including application spe- cific variables. For the most part, it is only pos- sible to use a grey-box if not only a black-box approach. If only the latter is possible, another level of complexity is added to the resource demand estimation system. A load-balancing approach called Sandpiper that relies on a grey- box and a black-box approach to monitor and detect hotspots in virtual environments and to eliminate them has been presented by Wood et al. [21] They confirm that a grey-box approach can indeed improve the responsiveness of their system.

2.1 Public Clouds  To reduce the effort of setting up and config- uring a data center, to increase business agility, reduce the CAPEX and allow better access for mobile devices, public Clouds offer a conve- nient solution. However, new challenges arise with the application of public Clouds like fre- quently changing APIs. This alone is nowadays not problematic, but since vendor lock-in is still a major concern about public Clouds, it almost became mandatory to provide support for multiple platforms. Unfortunately, every platform has its own characteristics and, due to the lack of standardization, its own interfaces and APIs for accessing them [22]. Managing that is not an easy task, so a good resource demand estimation application should make use of a good Cloud integration strategy or uti- lize existing middleware for that task as well.

Another reason, why the utilization of different public Clouds is preferable is the huge per- formance differences between them [23]. That means the performance of an application and thus its resource allocation strongly depends on the Cloud provider. Further, performance differences in a single Cloud exist as well. The performance of a compute instance can depend on points in time, the physical locations of the machines and the choice of the virtual system type [24]. Dejun et al. also concluded that dif- ferent instances of the same type can have a different performance whilst the performance of a single instance is relatively stable [25].

Further problems for resource demand esti- mation are the throughput instability and delay variations in virtual networks discovered by Wang and Ng [26]. They observed that cer- tain types of virtual compute instances only receive a 40% to 50% share of the proces- sor which can cause very unstable TCP/UDP throughput among those instances. They fur- ther observed abnormally large packet delay variations among all instance types and con- cluded that the unstable network performance can indeed dramatically influence the results of network performance techniques. This problem however, seems to be only related to public Clouds. He et al. pointed out that due to the slow networking performance of virtual com-  pute nodes in public clouds, the application of a private cloud is more beneficial [15].

2.2 Hybrid Clouds Although a public Cloud has the benefit of reduced CAPEX and better deployment speed, private Clouds are even more popular amongst enterprises according to a survey by IDG in June 20131. The survey revealed that compa- nies tend to optimize existing infrastructure with the implementation of a private Cloud which results in a lower total cost of ownership (TCO). Despite that fact, 59 percent of the respondents have a portion of their IT environ- ment in the public Cloud. Therefore, they are in fact working in a hybrid Cloud environment.

Considering the above statement about slow virtual networks in public Clouds, that is a good decision, but it leads to another level of complexity for resource demand estimation algorithms. After determining if a certain task can be executed in the public Cloud or not, based on security policies, it must be decided if it is faster or maybe cheaper to execute this task in the public or in the private Cloud or if both should be utilized for complex applications.

Factors like the resource capacity of a private Cloud, which is usually much less than of a public Cloud, play an important role and, as mentioned before, the fact that some parts of an application may need to be publicly available.

3 EXISTING APPROACHES Numerous approaches to predict or estimate resource demand in the Cloud exist already.

Here, we classify them as either single or multi- ple resource type systems. The former perform best if only one type of resource package or virtual machine (VM) is available and predict the amount of VMs allocated to one specific ap- plication or a number of different applications.

Usually, load-balancing systems belong to this category. Multiple resource type systems aim to select the best VM type for one application out of different sized VMs. This selection process can also involve multiple Clouds. Conditions  1. http://www.eweek.com/cloud/ enterprises-prefer-private-clouds-survey/ - 2013-09-18     for the selection process are usually based on a service level agreement (SLA), QoS require- ments or simply minimizing runtime or cost.

An optimal solution should be able to select the most appropriate Cloud provider, including private Clouds, and the necessary number of compute instances of all available VM types for arbitrary applications.

3.1 Single Resource Type Solutions  Although most load-balancing systems can- not be used to estimate the resources for an application that is not already running, they become vital for managing resources for run- ning applications with a variable load. Some very good approaches do already exist, e.g. the Sandpiper system by Wood et al. [21] which we mentioned above. An approach for dynam- ically estimating CPU demands of applications using CPU measurements from previous exe- cutions has been presented by Pacifici et al.

in 2008 [27]. They formulated a multivariate linear regression problem and used a linear model to solve it. They further addressed prac- tical issues like insignificant flows, collinear flows, space and temporal variations and back- ground noise. Problems with their approach have been the long response times of the sys- tem, capturing background noises and missing scaling and standardization techniques. Gong et al. introduced PRESS, a PRedictive Elastic ReSource Scaling scheme for Cloud systems, which makes a prediction of future demand based on patterns extracted from previous ex- ecutions [28]. It uses the average value of the samples in each prediction window or alterna- tively, a state-based prediction approach using a discrete-time Markov chain. PRESS showed high accuracy resulting in low over provision- ing and almost none under provisioning. How- ever, it is not applicable in a hybrid Cloud environment and not able to consider different types of virtual machines at the moment. Isci et al. implemented a completely non-utility based approach for CPU demand estimation which performed well in comparison to other utility based solutions [29]. They considered an increase in the service time of a virtual machine as indicator for performance degrada-  tion. Thus, their technique is independent from the structure of an application as well as the underlying operating system which makes it more flexible. The downside is that this black- box approach cannot really predict a resource demand in advance, but on the other hand, it is quick enough to respond to demand changes almost immediately. Kousiouris et al. presented a two-level generic black-box approach for behavioral-based management across different Cloud layers [30]. They identified patterns in high-level information and translated them to low-level resource attributes. This approach showed notably good prediction accuracy and seems to be even generic enough to work with different Cloud environments including private Clouds. It is further fast enough to predict changes in real time. Unfortunately, it may not be applicable in a hybrid Cloud environment, which would be preferred to optimize the run- time and cost across multiple Clouds.

A method to measure the elasticity in Cloud computing has been developed by Islam et al. [31]. Their approach is QoS related and also of particular interest for resource demand estimation in order to make assumptions on how quick a certain Cloud platform can scale up or down. To measure the elasticity they use a penalty model that penalizes imperfections in elasticity for a given workload in monetary units. We found, that this model can also be used for an optimization function to support resource demand estimation approaches that rely on data mining techniques. The developed algorithms for calculating the over and under provisioning penalties are given in Algorithm 1 and Algorithm 2 respectively. It is important to mention that the automation of this approach is also possible if the QoS measurements of the customer are known. The downside of this method is that it requires extensive benchmark- ing.

In [32] Islam et al. developed a method to predict resource provisioning using machine learning algorithms. They verified their ap- proach with experimental results, using neu- ral networks and linear regression as learning algorithms. The prediction accuracy is quite notable which allows achieving on-demand re- source allocation in the Cloud even with sev-     Algorithm 1 Calculation of the over provision- ing penalty Po according to Islam et al. [31] - ts and te represent the start and end time, ci the cost for a resource i and Ri(t), Mi(t) and Di(t) the available, chargeable and actual supply at time t respectively.

Po(ts, te) = ? i  Po,i(ts, te)  Po,i(ts, te) =  ? te ts  ci ? di(t)dt  di(t) = ?  ???? ???  Mi(t)?Di(t) if Ri(t) > Di(t), Mi(t)?Ri(t) if Mi(t) > Ri(t)  and Di(t) >= Ri(t), 0 otherwise  Algorithm 2 Calculation of the under provi- sioning penalty Pu according to Islam et al. [31] - Q is a set of QoS properties and pq(t) the amount of unsatisfactory behavior at time t.

This amount is mapped to the financial impact in fq. The limit of acceptable unsatisfactory behavior at time t is donated by poptq (t).

Pu(ts, te) = ? q?Q  Pu,q(ts, te)  Pu,q(ts, te) =  ? te ts  (fq(pq(t))? fq(poptq (t)))dt  eral minutes delay in the hardware resource al- location. However, the experiments were only performed using the TPC-W benchmark [33] yet and for just a single VM type. Accordingly, it is not certain that this method works for arbitrary applications or in hybrid Cloud en- vironments.

Many of the previously examined ap- proaches have in common, that they were only designed and tested for scalable web appli- cations, which are a suitable target to apply dynamic scaling or load-balancing algorithms to. But for HPC or even arbitrary applications, those approaches are either not applicable or have not been thoroughly tested yet.

3.2 Multiple Resource Type Solutions  Most resource demand estimation solutions that allow the selection of a specific resource type focus on selecting one out of multiple Cloud providers for a certain application. This approach is popular because of the huge per- formance differences between Cloud providers which we explained already. However, it is almost as important to select the correct types and number of VMs that should be used. Ama- zon for example offers 17 different compute in- stance types with different specifications each2.

Li et al. performed several benchmarks to compare public Cloud providers [23] and de- rived from that a performance estimation to select the best provider for a certain job. Al- though their approach, which is called Cloud- Cmp, may not include the accurate predic- tion of runtime or cost for the selected Cloud provider, it delivers a pretty good estimation, which provider suits best for a specific ap- plication. An alternate approach has been de- veloped by Kaisler et al. [12]. Their decision framework is designed to assist managers to decide which Cloud alternative is the right for their application case based on specific require- ments like business objectives, QoS attributes and architectural decisions. They also pointed out, that different pricing strategies offered to the user by a single Cloud provider should be taken into account, i.e. many providers offer pre-paid instances which are cheaper than usual. They state in their conclusion, that espe- cially SMEs or Cloud beginners should start off with a small private Cloud and then eventually move critical parts of their IT to a public Cloud.

However, although this approach may help selecting a Cloud provider, it does not offer any real resource demand estimation.

The most promising approach, to our knowl- edge, has been presented by Li et al. With their approach called CloudProphet they are able to predict application performance in the Cloud for arbitrary applications [34]. In their work they use a trace and replay method which means the application is executed locally and the same workload is emulated in the Cloud.

2. http://aws.amazon.com/ec2/instance-types/ - 2013-09-24     The performance of the agent that emulated the workload is used to predict the performance after migration. The accuracy of this method is notably high and in combination with Cloud- Cmp, this approach offers very good support for selecting the right Cloud provider for an application without executing it in the Cloud.

However, their approach has been tested on a single compute instance type only so it does not consider different instance types yet. An obvious difficulty for this goal is the high effort and expenses for executing an agent on all different instance types. This effort needs to be reduced. Although that goal could be achieved with little effort using regression methods, it would reduce the prediction accuracy by a notable amount which is not desirable. Hybrid Clouds were also not considered in the litera- ture yet, but as this approach works with multi- ple public Cloud providers already, this seems to be only a minor problem. Another problem is that it is not certain if this approach works for scientific or high performance applications as well, since it has not been tested with those kind of applications either. Supposedly, this is due to the usually huge workload of those applications so the trace and replay method would be very slow and expensive in such a case.

4 CONCLUSION In this work, we presented current challenges for resource demand estimation in Cloud com- puting and analyzed existing approaches for solving those challenges. We discovered that many of these problems can already be solved by most of the solutions but some impor- tant research challenges remain. For most of the approaches it is still unclear whether the presented approaches are applicable in a hy- brid Cloud environment. This problem could be solved by utilizing Cloud integration tech- niques or middleware to support the estima- tion algorithm. Another benefit of this ap- proach is the resulting simplification of the development process. Further, the increasing number of HPC applications in the Cloud contains a huge potential for saving resources and thus energy and cost if resource demand  estimation algorithms can be properly applied.

Current solutions cannot yet meet that de- mand since usually extensive benchmarking is required to be able to accurately predict the resource demand for future jobs. For HPC applications, this effort is not feasible. To over- come this issue, a method to predict the de- mand of a large scale experiment based on previously executed benchmarks of small scale experiments needs to be developed. We suggest the utilization of state of the art regression techniques. However, this method may only be applicable to grey-box, if not only white- box approaches, and the prediction accuracy will possibly still be reduced by a notable amount. More research in that direction should help to discover exactly how the prediction accuracy is influenced and to derive possible solutions for that. Since it is rather difficult to develop a solution for arbitrary applications, we propose that it will be useful to utilize classification techniques like decision tree or naive Bayes to classify applications after a sin- gle or a few small scale benchmarks first. After that the most appropriate regression technique to estimate the large scale resource consump- tion can be applied based on the application class, e.g. IO-intensive, compute intensive or memory intensive. Finally, taking different VM types of each Cloud provider into account is still not supported by most of the approaches which is due to frequent changes and lack of standardization in this area. Data mining techniques like regression could be applied as well to partially solve this problem but, as discussed already, the prediction accuracy may suffer from this additional complexity.

To fully overcome this last challenge, more standardized resource descriptions and reliable provisioning of resources in Cloud computing is necessary.

