Maintenance of Discovered Association Rules in Large Databases:  An Incremental Updating Technique *

Abstract A n  incremental updating technique is developed  for maintenance of the association rules discovered b y  database mining. There have been many stud- ies on eficient discovery of association rules in large databases. However, it is nontrivial t o  maintain such discovered rules in large databases because a database may allow frequent or occasional updates and such up- dates may not only invalidate some existing strong association rules but also turn some weak rules into strong ones. In this study, an incremental updating technique is proposed for  eficient maintenance of dis- covered association rules when new transaction data are added  to a transaction database.

1 Introduction Database mining has recently attracted tremendous  amount of attention in the database research because of its wide applicability in many areas, including deci- sion support, market strategy and financial forecast.

According to many studies in knowledge discovery in databases [lo, 41, mining knowledge from databases has the following characteristics.

1. The size of the database is significantly large, it could scale up to gigabytes, terabytes, or even larger, in some applications.

2. The rules discovered is valid only in statistical terms. Users are looking for rules that hold for a significant amount of data, but not necessarily  *The research of the first three authors were supported in part by RGC (the Hong Kong Research Grants Council) grant 338 f O65/OO26. The research of the second author was also sup- ported in part by NSERC (the Natural Sciences and Engineer- ing Research Council of Canada) research grant OGP0037230 and an NCE (the Networks of Centres of Excellence of Canada) research grant, IRIS-HMI5.

for all the data. Therefore, the number of rules returned from a mining activity could be large.

3. The rules discovered from a database only reflect the current state of the database. To make the rules discovered stable and reliable, a large vol- ume of data should be collected over a substantial period of time.

These observations indicate that the promise of database mining lies in the techniques to handle a large amount of data, to manage a substantial number of rules, and to maintain the rules over a significantly long period of time. Therefore, the following two prob- lems are essential in order to make database mining a feasible technology.

1. Design efficient algorithms for mining different types of rules or patterns.

2. Design efficient algorithms to update, maintain and manage the rules discovered.

The first problem has been studied substantially with many interesting and efficient database mining algorithms reported (e.g., see [l, 2, 3, 5 ,  6, 8, 9, 111). Such database-oriented knowledge mining al- gorithms can be classified into two categories: con- cept generalization-based discovery and discovery at the primitive concept levels. The former relies on the generalization of concepts (attribute values) stored in databases and then summarization of the data reg- ularities at a high concept level. One such example is the DBLearn system [3, 51. The latter relies on the discovery of strong regularities (rules) from the database without concept generalization. Association rule [l, 2, 91 is an important type of rules discovered by this approach.

1063-6382/96 $5.00 0 1996 IEEE    However, very little work has been done on the second problem. A method for handling incremen- tal database updates for the rules discovered by the generalization-based approach was briefly discussed in [5]. However, previous work has not been seen on incremental updating of association rules. Since database updates may introduce new association rules and invalidate some existing ones, it is important to study efficient algorithms for incremental update of as- sociation rules in large databases, which is the theme of this paper.

In the pioneer work [l], it is shown that the prob- lem of mining association rules can be decomposed into two subproblems, The first problem is to find out all large itemsets which are contained by a significant number of transactions with respect to a threshold minimum suppori. The second problem is to  generate all the association rules from the large itemsets found, with respect to another threshold, the minimum con- fidence. Since it is easy to generate association rules if the large itemsets are available, major efforts in the research community have been focused on finding ef- ficient algorithms to compute the large itemsets in re- cent studies.

Among all the algorithms proposed, the Apriori (and its modifications) [l] and the DHP (Direct Hash- ing and Pruning) [9] algorithms are the two most suc- cessful. They both run a number of iterations and compute the large itemsets of the same size in each iteration, starting from the size-one itemsets. In each iteration, they first construct a set of candidate item- sets and then scan the database to count the number of transactions that contain each candidate set. The key for optimization lies on the techniques used to create the candidate sets. The smaller the number of candidate sets is, the faster the algorithm would be.

The goal in this work is to solve the efficient update problem of association rules after a nontrivial number of new records have been added to a database. As- suming that the two thresholds, minimumsupport and confidence, do not change, there are several important characteristics in the update problem.

1. The update problem can be reduced to finding the new set of large itemsets. After that, the new association rules can be computed from the new large itemsets.

2. Generally speaking, an old large itemset has the potential to become small in the updated database.

3. Similarly, an old small itemset could become large in the new database.

4. In order to find the new large itemsets, all the records in the updated database, including those from the original database, have to be checked against every candidate set.

One possible approach to the update problem is to re-run the association rule mining algorithm on the whole updated database. This approach, though sim- ple, has some obvious disadvantages. All the computa- tion done initially at finding out the old large itemsets are wasted and all large itemsets have to be computed again from scratch.

In this paper, an efficient algorithm FUP (stands for Fast Update) is presented for computing the large itemsets in the updated database. We will show that the information from the old large itemsets can be reused. Moreover, at finding the new large itemsets, the pool of candidate sets can be pruned substan- tially. Some optimization technique for reducing the database size during the update process will also be discussed.

Extensive experiments have been conducted to study the performance of FUP and compare it against the cases in which either Apriori or DHP is applied to the updated database to find the new large itemsets.

FUP is found to be 2 to 16 times faster than re-running Apriori or DHP. More importantly, the number of can- didate sets is found to be about 2-5 % of that in DHP.

This shows that FUP is very effective in reducing the number of candidate sets. Also, the overhead of run- ning FUP on an updated database is measured, and found to be only about 5-20% (which is very efficient).

The remaining of the paper is organized as follows.

A detailed problem description is given in Section 2.

The algorithm FUP is described in Section 3. Per- formance study is discussed in Section 4. Section 5 discusses the variations of the techniques, and Section 6 concludes our study.

2 Problem Description 2.1 Mining of association rules  Let I = {i l , iz , .  . . , i m }  be a set of literals, called items. Let DB be a database of transactions, where each transaction T is a set of items such that T C_ I .

Given an atemset X E I ,  a transaction T contains X if and only if X E T.  An associatzon rule is an im- plication of the form X j Y, where X C I ,  Y C I and X n Y = 8. The association rule X j Y holds in DB with confidence c if c% of the transactions in DB that contain X also contain Y. The association rule X 3 Y has support s in DB i fs% of the transactions in DB contain X U Y. Given a minimum confidence threshold minconf and a minimum support threshold     minsup, the problem of mining association rules is to find out all the association rules whose confidence and support are larger than the respective thresholds. We also call an association rule a strong rule to distin- guish it from the weak ones, i.e., those that do not meet the thresholds [6]. For an itemset X ,  its support is definited similarly as the percentage of transactions in DB which contain X .  Given a minimum support threshold minsup, an itemset X is large if its support is no less than minsup. The problem of mining asso- ciation rules is reduced to the problem of finding all large itemsets for a pre-determined minimum support  2.2 Update of association rules Let L be the set of large itemsets in the database  D B ,  s be the minimumsupport, and D be the number of transactions in DB.  Assume that for each X E L, its support count, Xsuppor t ,  which is the number of transactions in DB containing X ,  is available.

After some update activities, an increment db of new transactions is added to the original database D B ,  and d is the number of transactions in db. With respect to the same minimum support s, an item- set X is large in the updated database DB U db if the support of X in DB U db is no less than s, i.e., X.support 2 s x ( D  + d ) .

Thus the essence of the problem of updating asso- ciation rules is to find the set L? of large itemsets in DB U db. Note that a large itemset in L may not be a large itemset in L?, on the other hand, an itemset X not in L,  may become a large itemset in L?.

3 Fast Update Algorithm FUP Basically, the framework of FUP is similar to that of  Apriori and DHP. It contains a number of iterations.

The iteration starts at the size-one itemsets, and at each iteration, all the large itemsets of the same size are found. Moreover, the candidate sets at each iter- ation are generated based on the large itemsets found at the previous iteration. The features of FUP which distinguish it from Apriori and DHP are listed as fol- lows.

111 *  At each iteration, the supports of the size-k large itemsets in L are updated against the increment db to filter out the losers, i.e., those that are no longer large in the updated database. Only the increment db has to be scanned to do the filtering.

While scanning the increment, a set of candidate sets, ck, is extracted from the transactions in db, together with their supports in db counted. (Note that the size of db is in general much smaller than  that of the original database DB.)  The supports of these sets in ck are then updated against the DB to find the ?new? large itemsets.

More importantly, many sets in c k  can be pruned away by a simple check on their supports in db before the update against D B  starts. (This check will be discussed in the following.)  The size of the updated database is reduced at each iteration by pruning away some items from some transactions in the updated database.

These features combinkd together form the core in the design of FUP and make FUP a much faster algorithm in comparison with the rerunning of Apriori and DHP on the updated database. Our experimental results show a factor of 2 to 16 improvement in performance in the comparison.

The following notations are used in the remaining of the paper. Lk is the set of all size-k large itemsets (called large k-itemsets) in DB, and L i  is the set of all large k-itemsets in D B  U db. c k  is the set of size4 candidate sets in the k-th iteration of FWP. Moreover,

X.supportD, X.supportd and x.supportr/D represent the support counts of an itemset X in D B ,  db and D B  U db, respectively. The following is a detailed de- scription of the algorithm FUP. The first iteration of FUP is discussed followed by the discussion of the re- maining iterations.

3.1 First iteration: Removing size-one losers, generating size-one candidate sets, and finding size-one winners  The following properties are useful in the derivation of the large 1-itemsets for the updated database.

Lemma 1 A n  I-itemset X in the original large 1- itemsets L1 is a loser in the updated database DBUdb (i.e., not an the large I-aternset L i )  i f  and only af X.SuppOrtUD < s x ( D  + d ) .

Proof. Based on the definitions of minimum suppod and large 1-itemset. 0  Lemma 2 A n  I-itemset X not an the original large 1-itemsets L1 can become a winner an the updated database D B  U db (i.e., being included in the large 1- itemset L i )  only if X.Supportd 2 s x d .

Proof. Since X is not in the original large 1-itemsets L1, X.SuPPOTtD < S X D. If X.supportd < s x d , then X.supportuD = X.SUppOPtD + X.supportd < s x ( D  + d ) .  That is, X cannot become a large item in the updated database. Thus we have the lemma.

n losers candidate set C1 scan DB  Figure 1: Processes in the first iteration of FUP.

Based on these properties, the finding of large 1- itemset L', in the updated database D B U d b  is outlined as follows. (The steps in the outline are described graphically in Figure 1 for easy understanding.)  1. Scan the increment db ,  for all itemsets X E L1, update its support count X.supportuD. Once the scan is completed, all the losers in L1 are found by checking the condition XsupportvD < s x ( D  + d )  on all X E L1 (according to Lemma 1.) By removing the losers, the itemsets in L1 which remain large after the update are identified.

2. In the same scan, a set C1 is created to store, for each T E db,  all size-one itemset X T which is not in L1. This becomes the set of candidate sets and their support in db  can also be found in the scan. More importantly, according to Lemma 2, if X E C1 and X.supportd < s x d ,  X can never be large in D B U d b .  Because of this, all the sets in C1,  whose support counts are less than s x d, are pruned off. This gives us a very small candidate set for finding the new size-one large itemsets.

3 .  A scan is then conducted on D B  to update the support count X.supportuD for each X E C1. By checking their support count, new large itemsets from C1 are found. By combining with those iden- tified in L1, the set of all size-one large itemsets, L',, is generated.

Example 1 A database DB is updated with an in- crement db such that D = 1000, d = 100 and s = 3%.

I1,12,I3, and I4 are four items. I1 and I2 are the large itemsets in L1 with I1.supportD = 3 2 ,  and Iz.supportD = 3 1 .

Assume that Il.supportd = 4 and I2.supportd = 1.

After a scan on db, we have I1.SUppOrtUD = 36 > llOOx3%and I2.supportuD = 32 < 1 1 0 0 ~ 3 % .  Hence  Iz is a loser, and only I1 is included in L', (i.e., remains to be large in the updated database.)  Assume that Is and I4 are two itemsets which are not in L1 but occur in the increment db.  Both I3 and 14 are potential candidate sets. In the scan of db, it is found that I3.supportd = 6 and I4.supportd = 2.

Since I4.supportd < s x d = 3% x 100, it is removed from the candidate set C1 (i.e., it is unnecessary to check I4 against the updated database.) Only I3 is included in C1. Suppose that I3.supporit~ = 28 is obtained in the scan of D B .  Thus, I3.supportu~ = 34  > 1100 x 3%, and 13 is included in L i .  0  In comparison with the first iteration of Apriori and DHP, FUP first filters out the losers and obtains the first set of winners from the original large 1-itemsets by examining only the incremental database db.  It also filters out from the remaining candidate set in db those items whose occurrence frequencies are too small to be considered as potential winners. Both func- tions are performed in a single scan of the incremental database db.  It then scans the original dattabase DB once to check the remaining potential winners. In con- trast, Apriori and DHP must take all the data items as size-one candidate sets and check them against the whole updated database. A much smaller candidate set gives FUP a competitive edge in performance when compared with Apriori and DHP.

3.2 Second iteration and beyond: Remov-  ing other losers, pruning candidate sets, and finding remaining winners  The following properties are useful in the derivation of the large k-itemsets (where k > 1) for the updated database.

Lemma 3 I f { X l ,  . . . ,  X k - l }  isaloser a i t h e ( k - 1 ) - t h  i tera t ion  ( i . e . ,  the i t e m s e t  is i n  Lk-1  but not  i n L i - l ) ,  a large k- i t emse t  an L k  (for any  k) containing     t h e  i t e m s e t  cannot  be a w i n n e r  in t h e  k - t h  i tera t ion (i.e., being included in t h e  large k - i t e m s e i  LL).

Proof. This is based on the property that all t h e  sub- se t s  of a large i t e m s e t  must also be large, proved in [2].

Lemma 4 A k - i t e m s e t  { X I , .  . . , xk} in t h e  original large k - i t e m s e t s  Lk is a loser ( i e . ,  no t  in t h e  large k - i t e m s e t  Lk) in t h e  updated database D B  U db  i f  and only  zf {XI , .  . . , Xk}.supportUD < s x ( D  + d ) .

Proof. Based on the definitions of minimum support and large k - i t e m s e t .

Lemma 5 A k - i t e m s e t  {XI , .  . . , xk} n o t  in t h e  orig- ina l  large k - i t e m s e t s  Lk can become a winner (z.e., be- ing included in t h e  large k - i t e m s e t  L i )  in t h e  upda ted database D B  U db only  if { X I , .  . . ,Xk}.supportd 2 s x d .

Proof. Based on the similar reasoning as for Lemma 2.

Based on the above properties, the finding of large 2-itemset L', in the updated database D B  U db is out- lined as follows.

1. Similar to the first iteration, losers in L2 will be filtered out in a scan on db. The filtering is done in two steps. Firstly, according to Lemma 3 ,  some losers in Lz can be filtered out without checking them against db. The set of losers L1 - L/, have been identified in the first iteration. Therefore, any set X E L2, which has a subset Y such that Y E LI - L i ,  cannot be large and are filtered out from L2 without checking against db. Secondly, a scan is done on db and the support count of the remaining sets in L2 are updated and the large itemsets from L2 are identified.

2. Similar to the first iteration, the second part at this iteration is to find the new size-two large itemsets. The key is to generate a small set of candidate sets. The set of candidate sets, C2, is generated, before the above scan on db starts, by applying the apriori-gen function on L', [a]. The sets in L2 are excluded when creating Cz because they have already been handled. The support count of the itemsets in C2 are accumulated in the same scan of db. The itemsets in C2 can now be pruned by checking their support count. For all X E C2, if X.supportd < s x d ,  X is removed from C2. Based on Lemmas 5, all the removed sets cannot be large in DB U db.

3. The last step is to scan DB to update the sup- port count for all the itemsets in C2. At the end  of the scan, all the sets X E C2, whose support count X.supportUD 2 s x ( D  + d ) ,  are identi- fied as the new large itemsets. The set L;, which contains all the large itemsets identified from L2 and C2 above, are the set of all the size-two large it emsets.

Example 2 A database LIB is updated with an in- crement d b  such that D = 1000, d = 100 and s = 3%.

I 1 , 4 ,  13, and I4 are four items and the size-1 and size- 2 large itemsets in D B  are L1 = {II, 1 2 , 1 3 }  and L2 = { I1 12, I 2  13}, respectively. Also I1 I 2  .supportr, = 50 and I213.support~ = 31. Suppose FUP has completed the first iteration and found the "new" size-1 itemsets Li = {II, Iz, 14}. This example illustrates how FUP will find out L', in the second iteration.

Note that I3 E L1 - Li ,  therefore, the set l 2 I 3  E L2 is a loser and is filtered out. For the remaining set I1112 E L2, FUP scans db to update its support count. Assume that 1112.supportdb = 3. Since 1112.supportu~ = (3+50) > 3 % ~  1100, therefore, I 1 I 2  is large in D B U d b and is stored in Li.

Secondly, FUP will try to find out the "new" large itemsets from db. Note that apriori-gen applied on Li generates the candidate set C2 = {I~12,1114,12~4}.

Since Ill, E L2 has already been handled, it is re- moved from Cz. For the remaining sets 11 I 4  and 4 4 in C2, FUP scans db to  update their support counts.

Suppose IlI4.supportd = 5 and 1214.supportd = 2.

Since 1214.supportd = 2 < 3% x 100, it cannot be a large itemsets in D B  U db. Therefore, I214  is removed from the candidate set C2.

For the remaining set I l l 4  E C2, FUP scans D B  to update its support count. Suppose I1 I4.support~ = 30. Since I 1 I 4 . s u p p o r t ~ ~  = 30+5 > 3% x 1100, it is a large itemset in the updated database. Therefore I1 I4 is added into L i .  At the end of the second iteration,   FUP first filters out losers from L2.

Lk = (1112,1114)  is returned.

The same algorithm is applied to the later iterations until no large itemsets is found. At the k-th iteration of FUP, the whole updated database is scanned once.

However, for the large ( k  - 1)-th itemsets in the orig- inal database, they only have to be checked against the small increment db. For the new large itemsets, their candidate sets are extracted from the increment and are pruned according to their support count in the increment. This pool for candidate sets is much smaller than those found by using either Apriori or DHP on the updated database. This shows that FUP is a much faster algorithm than the previous rule min- ing algorithms on database updates.

3.3 The FUP Algorithm  is presented as follows.

Based on the above discussion, the FUP algorithm  Algorithm 1 FUP: A fast update algorithm for maintenance of association rules on database up dates.

Input: (1) DB: the original database (with its size, i.e., the total number of transactions, equal to D);  (2) Lk: the set of all large k-itemsets in L)B, where k = 1 , .  . . , r ;  (3) db: an increment database (with its size equal to d ) ;  and (4) s: the minimum support threshold.

Output: L': The set of all large itemsets in D B  U db.

Method:  The 1st iteration: /* find L i ,  the set of all large 1- itemsets in DB U db */  w = L1; c = 0;  L: = 0 ;  P = 0; /* W: winners, C: candidate sets, L i :  initialized, P: for optimization */ for-all T E db do /* scan db  */  forall 1-itemset X C T do { if X E W then X.supportd++; else { i f X $ C then { C = C U { X } ;  X.supportd = 0; } /*init the support count and add X into C */ X .supportd ++ ; }  1; forall X E W do /*put winners into Li  */ if X.SUPPOT~UD >_ s x ( D  + d ) then L: = Li U { X } ;  if X . S U p p O T t d  < S X d forall X E C do /*prune candidate sets in C */  then { C =  C- { X } ;  P = PU{X}; } /* P will be used for optimization. */  forall T E D B  do /* scan DB */  if X E C then X.supportD++; if X E P then removes X from T ; /* Transaction T is reduced */  forall 1-itemset X 2 T do {  1; forall X E C do /*put winners into L',*/ if X . s u p p o r t u ~  2 s x ( D  + d ) then L i  = L i  U { X } ;  return L i .  /* end of the 1st iteration */  The k-th iteration: /* for IC = 2 or larger, repeat this program fragment to find L',, the set of all large  k-itemsets in the updated database, until either L', returned is empty or db  = 0 */  W = Lk;  L', = 0 ; /* W: winners; L i  initialized */  /* the size-k candidate sets */  /* prune off losers in W */ forall (k-1)-itemset Y E Lk-1 - L',-l do  C = apriori-gen(li-l) -Lk;  forall k-itemset X E W do  if Y X then { W = W - {X}; break; } forall T E db do { /* scan db  */  forall X E Subset(W, T )  do X . ~ u p p ~ ~ t d + + ; /* Subset(W,T) returns all the sets in W contained in T [2] */  for-all X E Subset(C, T) do X.supportd++; /* find support of all X E C */  Reduce-db(T); /*Some items in transactions in db can be removed, discussed in next section*/  for211 X E W do  /*put the winners from W into L i  "/ if X . s u p p o r t v ~  _> s x (D + d ) then L; = L', U { X } ;  forall X E C do /* prune candidate sets in C */  forall T E D B  do { /* scan D B  */ if X.supportd < s x d then C = C - { X } ;  forall X E Subset(C,T) do X.supportr>++; Reduce-Db (T) ; } /* Some items in transactions in D B  can be removed, discussed in next section */  for-all X E C do /* put the winners from C into L', "/ if X.supportuD 2 s x ( D  + d ) then L', = L; U { X } ;  return L',. /* The end of the k-th iteration */  Rationale: The algorithm follows the lemmas and dis- cussions in Sections 3.1-3.2. Moreover, the state- ments for the reduction of the size of the database are reasoned at the next subsection. Hence the al- gorithm correctly finds all the association rules in  0 the updated database and terminates.

3.4 Reduction of the size of the updated database  FUP applies the techniques used in DHP to reduce the size of the updated database. At the first itera- tion, all the candidate sets which do not have enough support in the increment db  are stored in the set P .

Later, during the scan of the original database, all items in P can be removed from all the transactions,     because they will not appear in any large itemset in the later iteration.

At any k-th iteration, some items in db or D B , which is not needed for finding large itemsets in the next iteration, can be identified and hence removed.

At any k-th iteration, during the scan in the incre- ment db, while FUP is counting the support for sets in the candidate sets C and W ,  for each transaction TI the Reduce-db function is called. It counts, for each I E TI the number of sets in C and W which contain I .  This number gives an upper bound on the num- ber of large k-itemsets that contain I .  If this number is smaller than k, then I cannot belong to any large (k+l)-itemset, and hence can be removed from all the transactions. Using this number, Reduce-db can prune off some items from db.

After the set C has been pruned against db,  it can be seen that any items in D B  which does not belong to any set in L k  or C will not belong to any large IC + l-itemset. Therefore, in the scanning of D B  to compute the supports of sets in C, all items that do not belong to any set in L k  or C can be removed. In the FUP algorithm, the function Reduce-DB performs this reduction.

In FUP, we have also integrated the direct hashing technique in [9], which further reduces the number of the candidate sets used in iteration two.

4 Performance Study In order to assess the performance of FUP, experi-  ments are conducted to compare its performance with that of Apriori and DMP. The experiments were per- formed on an AIX system on an RS/6000 workstation with model 410. As will be presented in the follow- ing, the result shows that FUP is much faster than the most successful mining algorithm with respect to updating association rules. FUP performs 2 to 6 times faster than DHP for a moderate size database of 100,000 transactions. When the database is scaled up to 1,000,000 transactions, the speed-up is 2 to 16 times. As explained before, the key of the speed-up lies on the much smaller amount of candidate sets. In some cases, the number of candidate sets generated were counted, and it was found that the amount gen- erated in FUP is reduced to the range of 1.5 - 5% of that in DHP. This is a very significant reduction.

As mentioned above, we also tested FUP with some very large databases. It was found that FUP actually performs much better in larger databases.

4.1 Generation of synthetic data  The databases used in our experiments are syn- thetic data generated using the same technique intro- duced in [l] and modified in [9]. The parameters used  I L I  Number of transactions in database D B Number of transactions in the increment d Mean size of the transactions Mean size of the maximal potentially large itemsets Number of potentially large itemsets Number of items  Table 1: Parameter Table.

TlO.l4.DlOO.dl 3 8.00 1 1 a E 6.00 E 4.00 0 z 2*oo 3 0.00  6.00% 4.00% 2.00% 1.00% 0.75% W  Minimum support  DHPFUP ~ p r i 0 1 - i ~ ~ ~  Figure 2: Performance Ratio.

are similar to those in [9] except that the size of the increment is an additional parameter. Table 1 is a list of the parameters used in our synthetic database.

In the following we use the notation Tx.Iy.Dm.dn, modified from the one used in [9], to denote a database in which D = m thousands, d = n thousands, IT1 = x, and 111 = y. In our experiments, we set ILI = 2000, N = 1000, and the secondary parameters S, = 5 , P, = 50, and M j  = 2000. S, is the clustering size used in the generation of potential large itemsets. P, is the pool size to store potential large itemsets from which transactions will receive their items. M j  is the multiplying factor associated with the pool. Readers not familiar with these parameters please refer to [l,  The way we create our increment is a straight for- ward extension of the technique used to synthesize the database. In order to do comparison on a database of size D with an increment of size d. A database of size ( D  + d )  is first generated and then the first D transactions are stored in the database D B  and the remaining d transactions is stored in the increment db. Since all the transactions are generated from the same statistical pattern, it models very well real life  91.

11 0.14.01 0O.dl 5 0.06  a 0.05  f 0.04 Z 8 0.03 C  0 0 5 0.02 g 8 0.01 K 0.00  6.00% 4.00% 2.00% 1.00% 0.75%  Minimum Support 0 FUP/DHP FUP/APRIORI  i3 E! 3.: Q 2.5 e 2  1.5  15K 25K 75K 125K 175K 250K 350K In creme n t Size  Figure 3: Reduction on Candidate Sets.

Figure 4: Speed Up Ratio vs Increment Size.

updates.

4.2  We have compared the performance of FUP against that of DHP and Apriori. The first comparison was done on an updated database T10.14.DlOO.dl. The performance ratios between them are shown in Fig- ure 2. In our implementation of the DHP, a hash table of size 100 is used, and hashing is only used in the generation of the size-2 candidate sets. This is the same policy used in [9]. For small support, FUP is 3 to 6 times faster than DHP, and 3 to 7 times faster than Apriori. For larger support, it is less costly to re-run the mining algorithm on the updated database since the number of large itemsets is relatively smaller. In- terestingly, FUP is still 2 to 3 times faster in this case.

4.3 Reduction on the number of candi-  As explained before, FUP substantially reduces the number of candidate sets generated. The effect is par- ticularly significant at  the first iteration. In Figure 3, the chart shows the ratio of the number of candidate sets generated by FUP when comparing with the two mining algorithms. The amount of reduction ranges from 98% to 95% when FUP is compared to DHP. It is even greater when it is compared with Apriori.

4.4 Performance of FUP with large incre-  ment In general, the larger the increment is, the longer it  would take to do the update. Also, the gain in speed- up would slow down. Two sets of experiments have been performed to support this analysis. A database T10.14.DlOO.dmwith updates of lK,  5K and 10K were generated, and different updates with different sup- ports were done by FUP and DHP. For the same sup-  FUP versus DHP and Apriori  date sets  port, the speed-up ratio decreases when update size increases. For example, when the support is 2%, the ratio decreases from 5.8 to 3.7.

We also want to find out whether the decreas- ing of the performance ratio as the size increases in the update would eventually bring the performance of FUP down to that of DHP. In the same setting of T10.14.DlOO.dm, we increase the increment size m from 10K gradually to 350K for comparison. The per- formance ratio is plotted in Figure 4. A gradually level off only appears when the increment size is about 3.5 times the size of the original database. The fact that FUP still exhibits performance gain when the incre- ment is much larger than the original database shows that it is very efficient.

4.5 Small overhead of FUP  We also have done some experiments for the pur- pose of analyzing the overhead incurred by the FUP.

In general, if the time to compute the set L? from an updated database DB U db is added to the time to compute the original set L of large itemsets from the database DB by a mining algorithm, the sum would be larger than that if the same mining algorithm was applied directly on DBUdb to compute L?. The differ- ence of these two time values is a measurement of the overhead of the update. If the overhead is small, then it indicates that the update was done very efficiently.

We have designed some experiments to analyze the overhead of FUP by measuring this difference. It was found that the bigger the increment is, the smaller this overhead becomes. In our experiment, what was dis- covered is that, when the increment is !much smaller than the original database, the 0verhea.d percentage     ranges around 10 - 15%. Once the increment is larger than the original size, the overhead decreases very rapidly from 10 to 5%. This is a very encouraging result because it shows that FUP not only can benefit update with small increment, it actually works very well in the case of large increment.

4.6 Performance in scaled-up databases  Our last experiment is done in a scaled-up database.

The database is T10.I4.D1000.d10 which contains 1 million transactions. The performance ratio between DHP and FUP in this scaled-up database, ranges from 3 to 16. The result shows that the gain from FUP will in fact increase if the database becomes larger. This shows that FUP is very adaptive to size increase and can be applied to very large databases.

5 Discussion and Conclusions We studied an efficient, fast, incremental updating  technique for maintenance of the association rules dis- covered by database mining. The developed method strives to determine the promising itemsets and hope- less itemsets in the incremental portion and reduce the size of the candidate set to be searched against the original large database. The method is implemented and its performance is studied and compared with the best algorithms for mining association rules studied so far. The study shows that the proposed incremen- tal updating technique has superior performance on database updates in comparison with direct mining from an updated database.

The incremental updating technique is applicable to the databases which allow frequent or occasional updates when new transaction data are added to a transaction database. We have also investigated the cases of deletion and modification of a transaction database.

Recently, there have been some interesting stud- ies at finding multiple-level or generalized association rules in large transaction databases [6, 111. The exten- sion of our incremental updating technique for mainte- nance of multiple-level or generalized association rules in transaction databases is an interesting topic for fu- ture research.

