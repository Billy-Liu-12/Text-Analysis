Fast Change Point Detection for Electricity Market Analysis

Abstract?Electricity is a vital part of our daily life; therefore it is important to avoid irregularities such as the California Electricity Crisis of 2000 and 2001. In this work, we seek to predict anomalies using advanced machine learning algorithms, more specifically a Change Point Detection (CPD) algorithm on the electricity prices during the California Electricity Crisis. Such algorithms are effective, but computationally expensive when applied on a large amount of data. To address this challenge, we accelerate the Gaussian Process (GP) for 1-dimensional time series data. Since GP is at the core of many statistical learning techniques, this improvement could benefit many algorithms. In the specific Change Point Detection algorithm used in this study, we reduce the overall computational complexity from O(n5) to O(n2), where the amountized cost of solving a GP projet is O(1). Our efficient algorithm makes it possible to compute the Change Points using the hourly price data during the California Electricity Crisis. By comparing the detected Change Points with known events, we show that the Change Point Detection algorithm is indeed effective in detecting signals preceding major events.



I. INTRODUCTION  The California Electricity Crisis of 2000 and 2001 is reported to have cost the state?s economy about 40 billion dollars [27]. From May 2000 to December 2001, the state experienced severe shortages in electric power caused by unusual weather, state deregulation policies, as well as illicit market manipulation by energy companies [9], [25]. Electricity prices skyrocketed by up to a factor of 800%, as depicted in Figure 1. To allow the market regulators and participants time to respond such irregularities, we aim to detect some leading indicators for such catastrophic events.

Previously, we have applied the similar idea of seeking leading indicators in the stock market [4]. The more general theme is to extract insight from massive amounts of data [11], [22]. In this spirit, we seek to develop an algorithm that is capable of detecting subtle signs of trouble from the available data about the electricity market. However, the detection algorithm used in the earlier study relies on the structure of the stock market that is not present in the electricity market [6]. In this work, we explore a class of machine learning techniques known as Change Point Detection algorithms [1], [2], [17].

Given a time series, Change Points are instances where the process producing the measurements undergoes abrupt and significant changes [2, Ch. 1]. Assuming the time series follows a certain generative model, the Change Point Detection  (CPD) algorithms aim to identify changes in the parameters of the model or changes in the model itself. Given a time series such as the electricity market, the change points de- tected could suggest changes in important factors affecting the electricity market. Correlating these change points with known events could be useful in understanding the operations of the electricity market and identifying anomalies. While CPD has been used in many applications including robotics and process control, CPD is especially relevant to financial time series, where risk resulting from parameter changes is often neglected in existing models [13], [19]. In this work, we choose to focus on one of the most effective CPDs, known as the Bayesian Online Change Point Detection (BOCPD) [19].

This method avoids the subtle pitfalls of most others and has been demonstrated to be able to detect true change points in nonstationary time series [19].

A notable challenge in using sophisticated statistical learn- ing methods like CPD is that they are computationally ex- pensive. Therefore, they are typically ill-suited for working with large amounts of data. Most existing studies on CPD use only hundreds of data points [19], whereas large time series from financial applications might have thousands or tens of thousands of data points. Our first objective in this work is to reduce the computational complexity of the BOCPD algorithm so it can deal with large data records.

In this work, we exercise the new algorithm with the California Spot Market electricity prices, known as the ISO (or CAISO) prices. They can be thought of as the difference between the actual price and the price set in the day-ahead market [5], [6], therefore, they can be negative in value. This makes it quite different from the prices of typical commodity.

However, this does not present any additional difficulty to CPD algorithms. Various published reports [15], [7], [12], [27] have provided details of Enron manipulation schemes including oversubscribing congested transmission lines and causing artificial regional differences, creating uncertainty in the spot markets. We choose to study the ISO prices because these manipulations are more likely to be reflected as the irregularities in ISO prices. The specific data collection we use is from University of California Energy Institute 1. This data set contains the electricity prices from 1998 to 2003.

1http://www.ucei.berkeley.edu/       Fig. 1. The historical CA ISO price in north California from April 1998 to July 2003.

Since there is a significant amount of documented evidence surrounding the events during this time period, any change point our program might detect could be compared against information in literature. This makes the data particularly useful for studying Change Point Detection algorithms.

In the remainder of this paper, we provide a brief overview of the Gaussian Process in Section II and the BOCPD algo- rithm in Section III. In Section IV, we present the techniques used to accelerate the Gaussian Process in the BOCPD al- gorithm. We first present the covariance matrix used in the Gaussian Process in a semi-separable form, and then describe a recursive solution procedure that produces n solutions in O(n) time. We briefly describe the implementation of the BOCPD using the new Gaussian Process and measure its performance against another version using a well-known implementation of the Gaussian Process. These performance results are presented in Section V, where we also describe how the detected change points are related to known events reported in the literature.



II. GAUSSIAN PROCESS  The Gaussian Process (GP) is a popular regression tool with many different uses [17], [21]. In this work, it is used as the core of a change point detection procedure. This section provides a brief overview about its computational complexity and its use in the change point detection procedure.

Formally, a Gaussian Process is a stochastic process xt (t ? T ), for which any finite linear combination of samples has a multivariate Gaussian distribution. GPs are nonparametric Bayesian, and can be considered as a nonparametric prior over functions [17]. At its core, GP is a stochastic process that assigns its input points to a Gaussian distribution and uses the Gaussian distribution to make predictions about new values. As a non-parametric model, GP makes no underlying assumptions about its inputs other than a specified mean function (m), which is usually set to zero (m(x) = 0), and  a covariance function (?) parameterized by a set of hyper- parameters. A popular choice is the set of Mate?rn covariance functions defined by [16]  ?(x, x?) =  ?2 ?(s+ 1)  ?(2s+ 1)  s? i=0  (s+ i)!

i!(s? i)!

(? 8?r  ?  )s?i e  ( ? ?  2?r ?  ) ,  (1)  where r = ?x?x??; ? and ? are hyper-parameters; ? = s+1/2 is a half-integer; and ?(?) is the gamma function. For ? = 3/2, ?(x, x?) takes a simpler form  ?(x, x?) = ?2 ( 1 +  ? 3r  ?  ) e  ( ? ?  3r ?  ) . (2)  Assuming the availability of some noisy observations y1, ? ? ? , yn of the dependent variable y at points x1, ? ? ? , xn, one can use GP regression to estimate the value of y at a new point xn+1. Let ?n be the standard deviation of the noise. If we define the covariance matrix as  K =  ????? ?(x1, x  ? 1) ?(x1, x  ? 2) ? ? ? ?(x1, x?n)  ?(x2, x ? 1) ?(x2, x  ? 2) ? ? ? ?(x2, x?n)  ...

...

. . .

...

?(xn, x ? 1) ?(xn, x  ? 2) ? ? ? ?(xn, x?n)  ?????+ ?2nI, (3)  then the best estimate for yn+1 is  y? = K?K?1 (y1, y2, ? ? ? , yn)T (4) with variance  var(y?) = K?? ?K?K?1KT? , where K?? = ?(xn+1, xn+1) + ?2n and  K? = (?(xn+1, x?1), ?(xn+1, x ? 2), ? ? ? , ?(xn+1, x?n)) .

Fig. 2. Gaussian Process  As K?1 is involved, the above expressions typically require O(n3) operations and O(n2) memory to compute.

Although GP is computationally expensive, its non- parametric nature and its ability to provide a confidence interval allows it to adapt better to the changes in data than a typical parametric model could, thus yielding superior predictions. Figure 2 illustrates the GP approximating the data and a 95% confidence interval.

Figure 3 compares the GP regression with the less expen- sive, parametric ARIMA model; GPs smaller errors testify to its higher accuracy. Our primary objective in this work is to reduce the computational cost of GP while retaining its effectiveness.



III. BAYESIAN ONLINE CHANGE POINT DETECTION  The Change Point Detection (CPD) [2], [3], [18] is an algorithm that detects changes in sequential data unders the assumption that the sequence data is composed of several runs. A run is best defined as the data of a specific time interval where the data fits a stochastic process without large deviations. In practice, it is not always clear how to split two consecutive runs. More generally, dividing a long sequence of data into runs in a challenging task. CPD algorithms generally work by estimating the length of the run (or run length) at every data point.

Let rt be the random variable representing the run length at time t. The goal of the CPD algorithm is to find the distribution of the random variable, p(rt). The Bayesian Online Change Point Detection (BOCPD) [19] finds the (distribution of) run length with a Bayesian update equation.

Given a sequence of data up to time t ? 1, y1:t?1, and the distribution of run length p(rt?1), the BOCPD algorithm  predicts the run length rt and data yt as follow,  p(yt, rt) = p(yt, rt|y1:t?1)p(y1:t?1) ? p(yt, rt|y1:t?1) =  ? rt?1  p(yt, rt|y1:t?1, rt?1)p(rt?1)  = ? rt?1  p(yt|y1:t?1, rt?1)p(rt|y1:t?1, rt?1)  = ? rt?1  p(yt|yt?rt?1:t?1)p(rt|y1:t?1, rt?1)  Here, p(yt|yt?rt?1:t?1) is an Underlying Predictive Models (UPM) which here, is the Gaussian Process, as used in [19].

p(rt|y1:t?1, rt?1) is a Hazard function, which is choosen as a constant function in this study.

Intuitively, the run length represents the length of a time segment with similar statistical behavior. At each time t, GP is used to compute conditional probabilities p(yt|y(t?r):(t?1)) for all the possible values of the run length rt?1 ? [1, t? 1].

Such probabilities are then used to determine the run length based on the recursive formula above.

BOCPD invokes GP on each possible combination of sub- sequence of data records. Give an input time series of n data records, there are n(n + 1)/2 subsequences. Given that GP has a computational complexity of O(n3), the overall computational complexity of BOCPD is O(n5). This cost is justified by its power in detecting subtle changes in important applications [2], [3], [18], [19]. However, in most published reports, CPD algorithms typically handle only a few hundred data points. In the next section, we describe a strategy that could significantly reduce the computational cost and make BOCPD suitable for large data sets.

Fig. 3. GP vs. ARIMA: GP generally has smaller errors.

1: procedure BOCPD(y1:n) ? Input data 2: p(r0=1)? 1; p(r0 ?= 1)? 0 3: t? 1 4: for t < n do 5: t? t+ 1 6: fH(rt=1)? c ? fH : Hazard Function 7: for all j > 1 do 8: fH(rt=j)? p(rt?1=j ? 1) 9: end for  10: rlen? 1; tot = 0 11: while t? rlen > 0 do 12: f(yt, rlen)? pGP (yt|yt?rlen:t?1) ? pGP :  Gaussian Process 13: f(rt=rlen)? f(yt, rlen) ? fH(rt=rlen) 14: tot? tot+ f(rt=ren) 15: rlen? rlen+ 1 16: end while 17: for all rlen do 18: p(rt=rlen)? f(rt=rlen)/tot ? Normalize 19: end for 20: end for 21: return (p(r1), ? ? ? , p(rt))) ? The dist. of run lengths 22: end procedure  Fig. 4. BOCPD algorithm

IV. SEMI-SEPARABLE MATRICES  In this section, we describe a technique that takes advantage of the algebraic structure in the matrix K in Equation 3 to effectivly reduce the cost of solving n Gaussian Processes in O(n) time.

When using a GP on a 1-dimensional time series, where xt for each time t is a real number the covariance matrix K in (3), which, here, is based on the Mate?rn function (2),  has a special matrix structure. To illustrate, we assume that x1 < x2 < ? ? ? < xn have been arranged in ascending order.

We can rewrite K as  K = D + triu ( PQT  ) + ( triu  ( PQT  ))T , (5)  where we have used the Matlab notation triu(?) to denote the strictly upper triangular part of a given matrix; and  D = diag ( ?2 + ?2n, ?  2 + ?2n, ? ? ? , ?2 + ?2n ) ,  P = ?  ????? e  (? 3x1 ?  ) x1e  (? 3x1 ?  )  ...

...

e  (? 3xn ?  ) xne  (? 3xn ?  )  ????? ,  Q = ?  ?????? ( 1 +  ? 3x1 ?  ) e  ( ? ?  3x1 ?  ) ? ? ? e  ( ? ?  3x1 ?  )  ...

...(  1 + ? 3xn ?  ) e  ( ? ?  3xn ?  ) ? ? ? e  ( ? ?  3xn ?  )  ?????? .

Equation (5) also holds for the more general Mate?rn function in (1), with a diagonal matrix D and n? (s+ 1) matrices P and Q.

Matrices of the form (5) are known as Semi-Separable matrices, with a large literature on their fast factorization and inversion [8], [26]. Below we describe a recursive procedure to compute uTK?1v for any vectors u = (u1, ? ? ? , un)T and v = (v1, ? ? ? , vn)T in O(n) time. For this purpose, write  D = diag (d1, ? ? ? , dn) ,  P =  ??? p T ...

pTn  ??? ,     and  Q =  ??? q T ...

qTn  ??? .

To begin, define  A?k = (qk ? ? ? qn)K?1k:n,k:n  ??? q T k ...

qTn  ??? , ?k = (uk ? ? ?un)K?1k:n,k:n  ??? vk...

vn  ??? , U?k = (qk ? ? ? qn)K?1k:n,k:n  ??? uk...

un  ??? , V?k = (qk ? ? ? qn)K?1k:n,k:n  ??? vk...

vn  ???, where we have used the Matlab notation Kk:n,k:n to denote the tailing (n?k+1)?(n?k+1) submatrix of K. The following recursion allows for the computation of all of A?k, U?k, V?k and ?k in O(n) time without explicitly inverting any Kk:n,k:n.

Let  A?n+1 = 0,  U?n+1 = 0,  V?n+1 = 0,  ?n+1 = 0.

Do for k = n, n? 1, ? ? ? , 1: d?k = dk ? pTk A?k+1pk, ?k = qk ? A?k+1pk, u?k = uk ? pTk U?k+1, v?k = vk ? pTk V?k+1, ?k = ?k+1 +  u?kv?k  d?k ,  A?k = A?k+1 + ?k?  T k  d?k ,  U?k = U?k+1 + u?k?k  d?k ,  V?k = V?k+1 + v?k?k  d?k .

Now let v = (y1 y2, ? ? ? , yn)T . To compute y? in equa- tion (4), all that is needed is to apply the above recursion with u = K?.

However, there is a rather remarkable feature about the above recursion. By their definitions, ?k is in fact the predic- tion y? based on the points (xk, yk), ? ? ? , (xn, yn) for every  1 ? k ? n. In other words, we have computed all n predictions for y? in O(n) time. The computation of the variances in equation (4) follows a similar pattern.

The dominant cost of the probabilities p(yt|y(t?r):(t?1)) is in the GP predictions of yt and their variances given y(t?r):(t?1) for all r < t. The above recursion can thus be used to compute all these predictions and variances, and therefore all the probabilities p(yt|y(t?r):(t?1)), in O(t) time, leading to amortized O(1) time for each GP and each probability, and quadratic time for BOCPD.

For the optimal use of GP and BOCPD, the hyperparameters can be selected through an optimization procedure, such as maximum likelihood. The semi-separable matrix structure of the covariance matrix K can also be exploited to perform hyperparameter training in linear time.

The squared-exponential, ?(x, x?) = ?2exp(?x?x ??22  2?2 ), is another popular covariance function. It is known to be well- approximated by the Mate?rn functions. This approximation allows us to utilize the recursion above to perform rapid GP regression and BOCPD for the squared-exponential as well.

This vast improvement in scalability allows us to run BOCPD in Matlab with more than 10,000 data points on a laptop overnight, a previously huge task that could be realistically attempted with only the fastest supercomputers.



V. EXPERIMENTAL RESULTS  We have implemented a version of BOCPD in Matlab following the description by the original authors [19]. The initial version of the code uses GPML to solve the Gaussian Processes [17]2. A faster version is also implemented using the algorithm described in the previous section to solve the Gaussian Processes. In this section, we will present some timing results to compare the two versions of BOCPD and discuss the changes points detected.

The 1-dimensional time series used in our study consists of prices at different time period. The raw time series is expected to have one value per hour, however, there are some hours with missing values. In addition, we use smaller samples in many timing tests. To accomodate these variations, we explicitly record the time values, which are the xi in the earlier discussions. The yi values are the corresponding prices.

Figure 5 shows the time used by the two versions of the BOCPD algorithm. The test runs with different sized samples of the hourly ISO prices from 2008 to 2001. Because the GPML code uses the efficient Matlab built-in functions to solve the linear systems, the BOCPD with GPML actually can handle up to 1000 data points in a reasonable amount of time. Event in this case, the new method, marked BOCPD- GPSS, is at least 10 times faster than BOCPD with GPML.

Furthermore, we see that BOCPD with GPSS can easily handle 10,000 data points even though our algorithm uses interpreted Matlab statements, while the BOCPD with GPML did not finish within 24 hours.

2Information about GPML is available at http://www.gaussianprocess.org/ gpml/.

?2  ?1       Number of Records  A ve  ra ge  R un  tim e  (s ec  on ds  )      (E xp  ec te  d)  BOCPD?GPML BOCPD?GPSS (run 1) BOCPD?GPSS (run 2)  Fig. 5. The time (seconds) used by the two versions of the BOCPD algorithm using GPML and our GPSS.

An execution of our algorithm on the market data produces Figures 6 and 7, which display several runs during 2000 and 2001, the years of the California Electricity Crisis. These runs are separated by change points, represented by green lines, and sometimes coincide with the dates in Table I, a chronology of important events relating to the Crisis.

Among the change points detected in year 2000, see Fig- ure 6, the first change point was July 1. This is the date when the price cap was reduced from $750/MWh to $500/MWh.

Prior to this date, there were signification volatility in the ISO prices; and there are also evidence of price manipulations from sources including the Enron email archive [20], [24], [23]. The price cap reduced again to $250/MWh in early August, 2000.

However, by this time, even the ISO prices during the off-peak hours are quite high. The two change points in September appear to be related to two instances where the minimum prices in each day have reached over $100/MWh. Of course, these are only anecdotal observations, further work is needed to systematically test the validity of all change points.



VI. CONCLUSION  In this paper, we present a strategy to significantly accelerate the Gaussian Process on 1-dimensional time series by taking advantage of the structure of the matrices. The technique represents the covariance matrix in semi-separable form and  then applies a recursive solution procedure. The overall effect is that we are able to reduce the computational complexity of the Bayesian Online Change Point Detection (BOCPD) from O(n5) to O(n2) on a time series of n records. Since GP is at the core of many machine learning techniques, reducing the solution time for GP could benefit many applications.

To demonstrate the efficiency of the new GP algorithm, we apply it to a change point detection procedure that makes extensive use of GP. In our timing measurements, we see that that the new GP algorithm significantly reduces overall execution time (by more than a factor of 10). We further demonstrate that BOCPD can effectively identify important events around the California Electricity Crisis from the price information alone. The changes detected include seasonal and policy changes, as well as market manipulations. Therefore, we believe the Change Points detected are useful in monitoring market activities.

Additional work is needed to further develop the GP and establish the effectiveness of BOCPD. Our solution strategy currently only apply to 1-Dimensional time series, we are working on extending this to 2- and 3-Dimensional cases. In the discussion of the Gaussian Process, we mentioned that different kernels that could be used in GP. One direction of future work would be explore ways to accelerate Gaussian Processes using a variety of different kernels. The current     Fig. 6. The ISO prices during 2000 (blue dots) with Gaussian processes from different runs separated by change points (green lines).

Fig. 7. The ISO prices during 2001 (blue dots) with Gaussian processes from different runs separated by change points (green lines).

implementation of the fast Gaussian Process is in Matlab scripts. We plan to rewrite the software in C or C++. This has the potential to speed up the software considerably.

