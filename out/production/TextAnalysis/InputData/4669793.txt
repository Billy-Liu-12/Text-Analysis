Extracting Non-Redundant Approximate Rules from Multi-Level Datasets

Abstract   Association rule mining plays an important job in knowledge and information discovery. Often the number of the discovered rules is huge and many of them are redundant, especially for multi-level datasets. Previous work has shown that the mining of non-redundant rules is a promising approach to solving this problem, with work in [14,18,19,20] focusing on single level datasets. Recent work by Shaw et. al. [15] has extended the non-redundant approaches presented in [14,18,19] to include the elimination of redundant exact basis rules  from multi- level datasets. In this paper, we propose an extension to the work in [14,15,18,19,20] to allow for the removal of hierarchically redundant approximate basis rules from multi-level datasets through the use of the dataset?s hierarchy or taxonomy. Experimentation shows our approach can effectively generate both multi-level and cross level non-redundant rule sets which are lossless.

1. Introduction   Since its introduction in [1], association rule mining is now an important and widely used data mining technique.

The aim of this technique is to extract frequent patterns, interesting co-occurrences and associations amongst sets of items in large transactional databases. Traditionally there has been two steps in obtaining association rules: first, determine the frequent patterns or itemsets using the constraint of minimal support and second generate the rules from these frequent patterns/itemsets using the constraint of minimal confidence. With this approach, the basis of an interesting or useful rule has been whether its confidence exceeds a user defined threshold. This approach is widely known as the frequent itemset approach. Much work has been done in developing more efficient algorithms or data structures to make computing these rules quicker and much effort has also been focused on improving the determination of the frequent itemsets [2,3,4,7,16].

One technique that has developed from the traditional frequent itemset approach is the usage of frequent closed itemsets, originating from the mathematical theory of  Formal Concept Analysis (FCA). It has been demonstrated to be a powerful technique for data analysis [13,21]. Its major advantage is its ability to reduce the number of rules, providing a more concise and lossless representation. Often too many association rules containing redundancies are discovered which often become overwhelming and difficult to comprehend.

Through the use of frequent closed itemsets the issue of redundancy can be dealt with by deriving non-redundant association rules [14,18,19,20,22]. However, this work has only dealt with redundancy in single level datasets.

Multi-level datasets (where the items are not all at the same concept level) contain information at different levels and to obtain it all, techniques that take all the levels into account are needed [6,8,9,11,12]. Rules derived from multi-level datasets can also have the same issues with redundancy as those from single level datasets. While existing approaches used to remove redundancy in single level datasets [14,19] can be adapted for use in multi-level datasets, they still fail to remove all of the redundancies present, namely the redundancy of hierarchy, where one rule at a given level gives the same information as another rule at a different level.

This paper looks into hierarchical redundancy for approximate basis association rules (which have a confidence of less than 1) and proposes a continuation and extension of the work in [15]. From this a more concise non-redundant approximate basis rule set can be derived. We also show that this more concise non- redundant approximate basis rule set is lossless and that all approximate rules can be derived.

The paper is organized as follows. Section 2 discusses related work. The basics behind association rule mining are given in Section 3. We present the definition of hierarchical redundancy and introduce our approach for deriving the non-redundant approximate basis rule set in Section 4. Experiments and results are presented in Section 5. Lastly, Section 6 concludes the paper and our proposed work.

DOI 10.1109/ICTAI.2008.54    DOI 10.1109/ICTAI.2008.54    DOI 10.1109/ICTAI.2008.54     2. Related Work   Much work in the field of association rule mining has focused on finding more and more efficient ways to discover all of the rules.  This has meant less work has focused on the issue of the quality of the discovered association rules. However, complete rule enumeration is often intractable in data sets with a very large number of multi-valued attributes.

One approach being taken is to determine which rules are redundant and remove them, thus reducing the number of rules a person or user has to deal with while not reducing the information content [14,19,22]. These approaches are showing a lot of promise and work in [19] shows that reductions of over 80% can be achieved in some datasets. This work has only focused on datasets where all items are at the same concept level (known as single level datasets). Thus they do not consider redundancy that can occur when there is a hierarchy among items.

A multi-level dataset is one which has an implicit taxonomy or concept tree, like shown in Figure 1. The items in the dataset exist at the lowest concept level but are part of a hierarchical structure and organization. Thus for example, ?Old Mills? is an item at the lowest level of the taxonomy but it also belongs to the higher concept categories of ?bread? and ?white bread?.

Figure 1.  Example taxonomy of a multi-level dataset.

Because of the hierarchical nature of a multi-level  dataset, a new approach to finding frequent itemsets for multi-level datasets is needed. Work has been done in adapting approaches originally made for single level datasets into techniques usable on multi-level datasets.

Work presented in [5] shows one of the earliest approaches proposed to find frequent itemsets in multi- level datasets and later was revisited in [6]. This work primarily focused on finding frequent itemsets at each of the levels in the dataset and did not focus on cross-level itemsets (those itemsets that are composed of items from two or more different levels). Referring to Figure 1 for an example, the frequent itemset {?Dairyland-2%-milk?, ?white-bread?} is a cross-level itemset as the first item is from the lowest level, while the second item is from a different concept level. In fact the cross-level ideas were an addition to the work being proposed.  Further work  proposed an approach which included finding cross-level frequent itemsets [17]. This later work also performs more pruning of the dataset to make finding the frequent itemsets more efficient.

However, the majority of work has focused on finding the frequent itemsets as efficiently as possible and the issue of redundancy in single level datasets. Some brief work presented by Han & Fu [5] discusses removing rules which are hierarchically redundant, but it relies on the user giving an expected confidence variation margin to determine redundancy. Recent work by Shaw et. al. [15] proposed an approach for discovering and removing hierarchically redundant exact basis association rules (which have a confidence of 1) derived from multi-level datasets. This work built upon redundancy removal work presented in [14,18,19,20] to expand these techniques into the area of multi-level datasets. The work in [15] started to fill the gap that is present when it comes to dealing with hierarchical redundancy in association rules derived from multi-level datasets. This work attempts to reduce that gap further.

3. Mining Frequent Patterns   From the beginning of association rule mining in [1], the first step has always been to find the frequent patterns or itemsets. The simplest way to do this has been through the use of the Apriori algorithm [2]. However, Apriori was not designed to work on extracting frequent itemsets at multiple concept levels in multi-level datasets. It is designed for use on single level datasets.  But, it has since been adapted for multi-level datasets.

One adaptation of Apriori to multi-level datasets is the ML_T2L1 algorithm [5,6]. The ML_T2L1 algorithm uses a transaction table that has the hierarchy information encoded into it. As the original work shows [5,6], ML_T2L1 does not find cross-level frequent itemsets.

We have added the ability for it to do this. At each level below 1 (so starting at level 2) when large 2-itemsets or later are derived the Apriori algorithm is not restricted to just using the large n-1-itemsets at the current level, but can generate combinations using the large itemsets from higher levels. The only restrictions on this are that the derived frequent itemset(s) can not contain an item that has an ancestor-descendant relationship with another item within the same itemset and that the minimum support threshold used is that of the current level being processed (which is actually the lowest level in the itemset).

A second, more recent adaptation of Apriori for use in multi-level datasets is a top-down progressive deepening method by Thakur, Jain & Paradasani in [17]. This approach was developed to find level-crossing association rules by extending existing multi-level mining techniques and uses reduced support and refinement of the     transaction table at every hierarchy level. This algorithm works very similarly to ML_T2L1 presented previously in that it uses a transaction table which has the hierarchy encoded into it and each level is processed individually, one at a time.

The two algorithms mentioned above have been used to generate frequent itemsets in our experiments which are explained in Section 5.

4. Generation of Non-Redundant Approximate Multi-level Association Rules   The use of frequent itemsets as the basis for association rule mining often results in the generation of a large number of rules and this is a widely recognized problem. Recent work has demonstrated that the use of closed itemsets and generators can reduce the number of rules generated [14,18,19,20,21]. This has helped to greatly reduce redundancy in the rules derived from single level datasets. However, redundancy still exists in the rules generated from multi-level datasets. This redundancy we call hierarchical redundancy. Here in this section we first introduce hierarchical redundancy in multi-level datasets and then detail our work to remove this redundancy in approximate basis rules without losing any information.

4.1. Hierarchical Redundancy   Whether a rule is interesting is usually determined through its support and confidence values. However, this does not guarantee that all of the rules that have high enough support and confidence actually convey new information. To demonstrate this, the following is an example transaction table for a multi-level dataset (Table 1).

This simple multi-level dataset has 3 levels with each item belonging to the lowest level. The item ID in the table store/holds the hierarchy information for each item.

Thus the item 1-2-1 belongs to the first category at level 1 and for level 2 it belongs to the second sub-category of the first level 1 category. Finally at level 3 it belongs to the first sub-category of the parent category at level 2.

From this transaction set we use the ML_T2L1 algorithm with the cross level add-on (as described previously) and a minimum support value of 4 for level 1 and 3 for levels 2 and 3 to discover the frequent itemsets. From this example dataset we discover 9 1-itemsets, 21 2-itemsets and 9 3-itemsets. From these frequent itemsets the closed itemsets and generators are derived (Table 2). The itemsets, closed itemsets and generators come from all three levels.

Finally from the closed itemsets and generators the association rules can be generated. In this example we use  an approach known as Reliable Approximate Basis [20], which is an extension to ReliableExactRule approach presented in [18,19] to generate basis rules. The discovered rules are from multiple levels and include cross-level rules (due to cross-level frequent itemsets).

The Reliable Approximate Basis approach can remove redundant rules, but as we will show, it does not remove hierarchy redundancy. The rules given in Table 3 are the approximate basis rules derived from the closed itemsets and generators in Table 2 when the minimum confidence threshold is set to 0.5 or 50% (Table 3).

Table 1. Simple multi-level transaction dataset.

Transaction ID Items 1 [1-1-1, 1-2-1, 2-1-1, 2-2-1] 2 [1-1-1, 2-1-1, 2-2-2, 3-2-3] 3 [1-1-2, 1-2-2, 2-2-1, 4-1-1] 4 [1-1-1, 1-2-1] 5 [1-1-1, 1-2-2, 2-1-1, 2-2-1, 4-1-3] 6 [2-1-1, 3-2-3, 5-2-4] 7 [3-2-3, 4-1-1, 5-2-4, 7-1-3]   Table 2. Frequent closed itemsets and generators derived from the frequent itemsets.

Closed Itemsets Generators [1-*-*] [1-*-*] [2-*-*] [2-*-*] [1-1-*] [1-1-*] [2-1-*] [2-1-*] [1-1-1] [1-1-1] [2-1-1] [2-1-1] [1-*-*, 2-*-*] [1-*-*, 2-*-*] [1-*-*, 2-2-*] [2-2-*] [2-*-*, 1-1-*] [2-*-*, 1-1-*] [1-1-*, 1-2-*] [1-2-*] [1-1-*, 2-2-*] [2-2-*] [1-*-*, 2-2-1] [2-2-1] [2-*-*, 1-1-1] [2-*-*, 1-1-1] [1-2-*, 1-1-1] [1-2-*, 1-1-1] [1-*-*, 2-1-*, 2-2-*] [1-*-*, 2-1-*] [2-1-*, 2-2-*] [2-*-*, 1-1-*, 1-2-*] [2-*-*, 1-2-*] [1-1-*, 1-2-*, 2-2-*] [1-2-*, 2-2-*] [1-1-*, 2-1-*, 2-2-*] [1-1-*, 2-1-*] [2-1-*, 2-2-*] [1-*-*, 2-1-1, 2-2-*] [1-*-*, 2-1-1] [2-2-*, 2-1-1] [1-1-*, 2-1-1, 2-2-*] [1-1-*, 2-1-1] [2-2-*, 2-1-1] [1-1-*, 2-2-1, 1-2-*] [2-2-1] [2-1-*, 1-1-1, 2-2-*] [2-1-*, 2-2-*] [2-1-*, 1-1-1] [2-2-*, 1-1-1] [2-2-*, 1-1-1, 2-1-1] [2-2-*, 1-1-1] [2-2-*, 2-1-1] [1-1-1, 2-1-1]   The Reliable Approximate Basis approach/algorithm lists all of the rules (in Table 3) as important and non- redundant. However, we argue that there are still redundant rules in this approximate basis set. This type of redundancy is beyond what the Reliable Approximate Basis approach/algorithm was designed to identify and eliminate. Looking at the rules in Table 3 we claim that rules 5 and 10 are redundant to rule 1, rule 7 is redundant to rule 3, rule 11 is redundant to rule 6 and rule 21 is redundant to rule 12 to name some.

Table 3. Approximate basis rules derived from closed itemsets and generators in Table 2.

No. Rule Supp Conf 1 [1-*-*] ==> [2-*-*] 0.666 0.933 2 [2-*-*] ==> [1-*-*] 0.666 0.933 3 [1-*-*] ==> [2-2-*] 0.666 0.933 4 [2-*-*] ==> [1-1-*] 0.666 0.933 5 [1-1-*] ==> [2-*-*] 0.666 0.8 6 [1-1-*] ==> [1-2-*] 0.666 0.8 7 [1-1-*] ==> [2-2-*] 0.666 0.8 8 [1-*-*] ==> [2-2-1] 0.5 0.7 9 [2-*-*] ==> [1-1-1] 0.5 0.7 10 [1-1-1] ==> [2-*-*] 0.5 0.75 11 [1-1-1] ==> [1-2-*] 0.5 0.75 12 [1-*-*] ==> [2-1-*, 2-2-*] 0.5 0.7 13 [2-1-*] ==> [1-*-*, 2-2-*] 0.5 0.75 14 [2-2-*] ==> [1-*-*, 2-1-*] 0.5 0.75 15 [2-*-*] ==> [1-1-*, 1-2-*] 0.5 0.7 16 [1-1-*] ==> [2-*-*, 1-2-*] 0.5 0.6 17 [1-2-*] ==> [2-*-*, 1-1-*] 0.5 0.75 18 [1-1-*] ==> [1-2-*, 2-2-*] 0.5 0.6 19 [1-2-*] ==> [1-1-*, 2-2-*] 0.5 0.75 20 [2-2-*] ==> [1-1-*, 1-2-*] 0.5 0.75 21 [1-1-*] ==> [2-1-*, 2-2-*] 0.5 0.6 22 [2-1-*] ==> [1-1-*, 2-2-*] 0.5 0.75 23 [2-2-*] ==> [1-1-*, 2-1-*] 0.5 0.75 24 [1-*-*] ==> [2-1-1, 2-2-*] 0.5 0.7 25 [2-1-1] ==> [1-*-*, 2-2-*] 0.5 0.75 26 [2-2-*] ==> [1-*-*, 2-1-1] 0.5 0.75 27 [1-1-*] ==> [2-1-1, 2-2-*] 0.5 0.6 28 [2-1-1] ==> [1-1-*, 2-2-*] 0.5 0.75 29 [2-2-*] ==> [1-1-*, 2-1-1] 0.5 0.75 30 [1-1-*] ==> [2-2-1, 1-2-*] 0.5 0.6 31 [1-2-*] ==> [1-1-*, 2-2-1] 0.5 0.75 32 [2-1-*] ==> [1-1-1, 2-2-*] 0.5 0.75 33 [1-1-1] ==> [2-1-*, 2-2-*] 0.5 0.75 34 [1-1-1] ==> [2-2-*, 2-1-1] 0.5 0.75 35 [2-1-1] ==> [2-2-*, 1-1-1] 0.5 0.75  For example, the item 1-1-1 (from rule 10) is a  descendant of the more general/abstract item 1-*-* (from rule 1). Thus rule 10 is in fact a more specific version of rule 1. Because we know that rule 1 says 1-*-* is enough to fire the rule with consequent C, whereas rule 10 requires 1-1-1 to fire with consequent C, any item that is a descendant of 1-*-* will cause a rule to fire with consequent C. It does not have to be 1-1-1. Thus rule 10 is more restrictive. Because 1-1-1 is part of 1-*-* having rule 10 does not actually bring any new information to the user, as the information contained in it is actually part of the information contained in rule 1. Thus rule 10 is redundant.

The exception to this would be if a rule which would normally be considered redundant in fact has a higher confidence value than the rule it is being considered redundant to. Since approximate association rules are measured by their confidence, which indicates their strength, trustworthiness, accuracy and/or reliability, it is important to ensure those rules with a high confidence are kept and made available. Thus from the list of  approximate basis rules in Table 3, rule 34 (1-1-1 ==> 2- 2-*, 2-1-1) would normally be considered redundant to rule 27 (1-1-* ==> 2-1-1, 2-2-*) as the antecedent of rule 34 is a descendant of the antecedent of rule 27. However, the confidence of rule 34 is 0.75, while rule 27 has a confidence of only 0.6. Thus we have more confidence in that rule 34 is correct than rule 27 and so rule 34 is stronger. Because of this rule 34 should be kept in the approximate basis rule set and should not be considered redundant.

Hierarchical redundancy has already been defined previously in [15] for exact basis association rules. From definition 1 in [15] and the previously described details for hierarchical redundancy in approximate basis rule sets, we propose the following definition for hierarchical redundancy in approximate basis association rules.

Definition 1 (Hierarchical Redundancy for Approximate Basis): Let R1 = X1 => Y with confidence C1 and R2 = X2 => Y with confidence C2 be two approximate association rules, with exactly the same itemset Y as the consequent. Rule R1 is redundant to rule R2 if (1) the itemset X1 is made up of items where at least one item in X1 is descendant from the items in X2 and (2) the itemset X2 is entirely made up of items where at least one item in X2 is an ancestor of the items in X1 and (3) the other non-ancestor items in X2 are all present in itemset X1 and (4) the confidence of R1 (C1) is less than or equal to the confidence of R2 (C2).

From this definition, if for an approximate association  rule X1 => Y (1) there does not exist any other approximate rule X2 => Y such that at least one item in X1 shares an ancestor-descendant relationship with X2 containing the ancestor(s) and all other items X2 are present in X1 or (2) the confidence value of rule X1 => Y is greater than the confidence of rule X2 => Y where the rule X2 => Y is a more general/abstract version of rule X1 => Y, then X1 => Y is a non-redundant rule. To test for redundancy, we take this definition and for our work add another condition for a rule to be considered valid. A rule X => Y is valid if it has no ancestor-descendant relationship between any items in itemsets X and Y. Thus for example 1-2-1 => 1-2-* is not a valid rule, but 1-2-1 => 1-1-3 is a valid rule. If this condition is not met by any rule X2 => Y2 when testing to see if X1 => Y is redundant to X2 => Y, then X1 => Y is a non-redundant rule as X2 => Y is not a valid rule.

4.2. Generating Non-Redundant Approximate Basis Rules   Because we wish to remove hierarchical redundancy on top of the non-hierarchical redundancy already being removed, our proposed approach uses closed itemsets and generators to discover the non-redundant approximate basis rules. Pasquier et. al. [14] and Xu & Li [18,19] and Xu, Li & Shaw [20] have all proposed condensed/more     concise bases to represent non-redundant exact basis (rules whose confidence is 1) and approximate basis (rules whose confidence is less than 1) rules. From these works two bases for non-redundant approximate basis rules from single level datasets can be defined here.

Definition 2 (MinMaxApprox Basis (MMA)): Let C be the set of frequent closed itemsets and G be the set of minimal generators of the frequent closed itemsets in C.

MMA = { }cgGgCcgcgr ???=> )(&&|)\(: ? where )(g?  is the closure of g.

Definition 3 (Reliable Approximate Basis (RAB)): Let C be the set of frequent closed itemsets and G be the set of minimal generators of the frequent closed itemsets in C.

RAB =  ? ? ?  ? ? ?  ?  ? ? ?  ? ? ?  ?  ???? =>>=>  ??? ???=>  ')'(,',,', ))''\('())\((  ))')'\((( ,)(&&|)\(:  cgggGgCcwhere gcgconfgcgconf  orgccg cgGgCcgcgr  ?  ?   where )(g?  is the closure of g, )'(g?  is the closure of g? and conf refers to the confidence measure of an association rule.

Using these two definitions (2 and 3) along with our definition for hierarchical redundancy in approximate basis (1) we build the basis for generating non-redundant multi-level approximate basis association rules. Thus our approaches are defined as follows (where HRR stands for Hierarchical Redundancy Removal).

Definition 4 (MinMaxApprox with HRR Basis (MMA- HRR)): Let C be the set of frequent closed itemsets and G be the set of minimal generators of the frequent closed itemsets in C.

MMA-HRR = { cgGgCcgcgr ???=> )(&&|)\(: ? , there exists no ')'(&'&'|)''\(':' cgGgCcgcgr ???=> ? or  ))''\(':'())\(:( gcgrconfgcgrconf =>>==> if there exists  }')'(&'&'|)''\(':' cgGgCcgcgr ???=> ? where g is a descendant set of g? and g? is an ancestor set of g and )''\()\( gcgc = and g? has no ancestors and/or descendants of )''\( gc  and where )(g?  is the closure of g, )'(g?  is the closure of g? and conf refers to the confidence measure of an association rule.

Definition 5 (Reliable Approximate Basis with HRR (RAB-HRR)): Let C be the set of frequent closed itemsets and G be the set of minimal generators of the frequent closed itemsets in C.

RAB-HRR =  ? ? ?  ? ? ?  ?  ???? =>>=>  ??? ???=>  ,')'(,',',', ))''\('())\((  ))')'\((( ,)(&&|)\(:  cgggGgCcwhere gcgconfgcgconf  orgccg cgGgCcgcgr  ?  ?   there exists no  ')'(,',',', ))''\('())\((  ))')'\((( ,)(&&|)\(:      cgggGgCcwhere gcgconfgcgconf  orgccg cgGgCcgcgr  ???? =>>=>  ??? ???=>  ?  ?   or ))\(:())\(:( 1111 gcgrconfgcgrconf =>>==> if there exists  ? ? ?  ? ? ?  ?  ???? =>>=>  ??? ???=>  ')'(,',',', ))''\('())\((  ))')'\((( ,)(&&|)\(:      cgggGgCcwhere gcgconfgcgconf  orgccg cgGgCcgcgr  ?  ?   where g is a descendant set of g1 and g1 is an ancestor set of g and )\()\( 11 gcgc = and g1 has no ancestors and/or descendants of )\( 11 gc  and where )(g?  is the closure of g, )( 1g?  is the closure of g1 and conf refers to the confidence measure of an association rule.

Thus the algorithms to extract non-redundant multi- level approximate basis rules are as follows.

Algorithm 1: MinMaxApprox with HRR Input: Set of frequent closed itemsets and generators Output: Set of non-redundant multi-level approximate basis rules 1. MinMaxApprox  ? 2. nonRedundant = true 3. for k = 1 to max generator length ? 1 do  4.    for all k-length generator Gg ?  5.       for all kjCc >? | j = closed itemset (c) length & )(gc ??  do  6.          if  (c.supp / g.supp >= minconf)  7.             for all k-length generator Gg ?' where gg ?'  do  8.                for all kjCc >?' | j = closed itemset (c?) length &  )'(' gc ??  do  9.                   if (c?.supp / g?.supp >= minconf) 10.                      if (g? ancestor of g) & (g descendant of g?) &  ((c?\g?) = (c\g))  & !(g? ancestor of (c?\g?)) &  !(g? descendant of (c?\g?)) &  (c?.supp / g?.supp >= c.supp / g.supp)  11.                         nonRedundant = false 12.                         break for loops at line 7 & 8 13.             if (nonRedundant) 14.                then insert {r: g => (c\g), c.supp / g.supp} into  MinMaxApprox  15.             nonRedundant = true 16. return MinMaxApprox           Algorithm 2: Reliable Approximate Basis with HRR Input: Set of frequent closed itemsets and generators Output: Set of non-redundant multi-level approximate basis rules 1. ApproxBasis  ? 2. nonRedundant = true  3. for each Cc ?  4.    for each Gg ? such that )(gc ??  5.       if (c.supp / g.supp >= minconf) 6.          if for all Cc ?'  & for all Gg ?' such that )'(' gc ??  &  gg ?' & (we have ))')'\((( gccg ???  or ))''\('())\(( gcgconfgcgconf =>>=> )  7.             for all Gg ?1  where gg ?' do  8.                if (g1 ancestor of g) & (g descendant of g1)  9.                   for all Cc ?1  do  10.                      if ( 11 gc ? ) & ((c\g) = (c1\g1)) &  !(g1 ancestor of (c1\g1)) & !(g1 descendant of (c1\g1)) & (c1.supp / g1.supp >= c.supp / g.supp)  11.                        nonRedundant = false 12.                        break loops at line 7 & 9 13.         else 14.            nonRedundant = false 15.         if (nonRedundant) 16.            then insert {r: g => (c\g), c.supp / g.supp} into ApproxBasis 17. return ApproxBasis  5. Experimental Results   Experiments were conducted to test and evaluate the effectiveness of the proposed approach to deriving hierarchically non-redundant approximate basis rule set and to confirm that it is also a lossless basis sets. This section presents and details the experiments undertaken and the results achieved.

5.1. Datasets   We used 8 datasets to test our approach to discover whether it reduced the size of the approximate basis rule set and to test that the basis set was lossless, meaning all the rules could be recovered. We used the same datasets used by Han & Fu [5,6] and Thakur, Jain & Paradasani [17] which had seven and eight transactions respectively and are named H1 and T1 respectively. We also used 4 randomly built datasets which were composed of 10, 20, 50 and 200 transactions. Thus the first 6 datasets are identical to the ones used in [15]. Finally, the last two datasets used in our experiments are based on the Book- Crossing dataset (available from http://www.informatik.uni-freiburg.de/~cziegler/BX/) [23]. From this dataset built a transactional dataset that contained 92,005 records and 14,172 inner and leaf  categories. We then cut the dataset at the second and third concept level and obtained two datasets which we used during our experiments. Thus we had a dataset (known as BC2) with 92,005 records and 270 categories with 2 concept levels and another dataset (known as BC3) with 92,005 records and 960 categories with 3 concept levels.

We were limited to smaller datasets due to efficiency problems suffered by the algorithms used to find the frequent itemsets.

The experiments aim to find associations among the items in each of the datasets. The process to discover the association rules involves three steps. Firstly, the frequent itemsets are discovered through the use of minimal support values for each hierarchy level. We have implemented two approaches; Han & Fu?s ML_T2L1 approach presented in [5,6] with the addition to the base algorithm so as to find cross-level itemsets, and Thakur, Jain & Paradasani?s algorithm (referred to as CLI) to find cross-level itemsets (along with normal itemsets) presented in [17]. Second, from the frequent itemsets, the frequent closed itemsets and generators are derived. We have implemented the CLOSE+ algorithm proposed by Pasquier et. al. in [14] to achieve this. Finally, the association rules are built. In these experiments we derive the rules using four different algorithms, which include the non-hierarchical Pasquier?s et. al. MinMaxApprox basis (referred to as MMA) [14] and Xu, Li & Shaw?s Reliable Approximate Basis (referred to as RAB) [20]. As well as a modified version of Pasquier?s et. al. work in [14] to include removing hierarchical redundancy (referred to as MMA with HRR) and a modified version of Xu, Li & Shaw?s work in [20] to include removing hierarchical redundancy (referred to as RAB with HRR).

5.2. Results   The primary objective of the experiments is to determine how well our proposed work performs at removing hierarchical redundancy in datasets even when other redundancy eliminating processes are included. The other objective is to ensure and demonstrate that our work is lossless and no information is lost. We can confirm that our approach recovers all approximate rules from multi- level datasets by comparing the modified versions of MinMaxApprox basis and Reliable Approximate Basis (which include our work to remove hierarchically redundant rules) against unmodified versions for each dataset to ensure that each recover the same set of approximate rules. We also compare the size of the approximate basis set generated by each of the four approaches to see what reduction in the basis set can be achieved. For all of the testing undertaken, the minimum confidence threshold for the association rules was set at 0.5. Tables 4 & 5 present the results obtained from the built datasets showing the number of approximate basis rules obtained and the percentage reduction achieved.

Tables 6 & 7 present the results obtained from the datasets derived from the Book-Crossing dataset.

Table 4. Results for built datasets where ML_T2L1 with cross level add-on is used to extract frequent  itemsets.

Approximate Basis Approx  Rules Data  set  MMA MMA with HRR  % RAB RAB with HRR  %  H1 36 27 25 35 26 25 68  T1 22 22 0 22 22 0 43  T2 181 161 11 166 146 12 2047  T3 700 587 16 398 347 12 1447  T4 2546 2085 18 1608 1387 13 4332  T5 6427 4844 24 3415 2970 13 7267   Table 5. Results for built datasets where CLI is used to  extract frequent itemsets.

Approximate Basis Approx  Rules Data  set  MMA MMA with HRR  % RAB RAB with HRR  %  H1 30 22 26 29 21 27 46  T1 3 3 0 3 3 0 6  T2 110 96 12 98 84 14 934  T3 412 347 15 225 185 17 496  T4 1906 1546 18 1187 1006 15 2690  T5 5393 3924 27 2717 2295 15 5720   As can be seen, the use of our approach reduces the  approximate basis rule set for nearly all cases we tested using the built datasets. In some instances the basis set was only reduced by a few rules, but in other cases there was a more significant reduction in the size of the basis set. For example, in Table 5 for dataset T5 there was a reduction of 1469 rules from 5393 to 3924, which is about 27.2% (with a reduction of 24.6% in Table 4 for dataset T5), and in Table 5, the reduction was around 26.6 to 27.5% for dataset H1 (and in Table 4 the reduction was around 25% for dataset H1) and around 12.7 to 14.3% for dataset T2 in Table 5. For other datasets the reduction is between about 11 to 18%. By using this approach we have successfully reduced the size of the approximate basis and by doing so it may help to make it more possible to effectively use the extracted association rules without overwhelming a user.

Table 6. Results for BC2 & BC3 datasets where MLT2L1 with cross level add-on is used to extract  frequent itemsets.

Approximate Basis Approx  Rules Data  set  MMA MMA with HRR  % RAB RAB with HRR  %  BC2a 214 208 2 201 197 2 220  BC2b 299 291 2 282 276 2 336  BC2c 396 385 2 373 364 2 433  BC2d 896 856 4 819 786 4 950  BC2e 2724 2601 4 2574 2463 4 2979  BC2f 2861 2733 4 2711 2595 4 3116  BC3a 35518 34052 4 35018 33587 4 35518   Table 7. Results for BC2 & BC3 datasets where CLI is  used to extract frequent itemsets.

Approximate Basis Approx  Rules Data  set  MMA MMA with HRR  % RAB RAB with HRR  %  BC2a 210 193 8 201 184 8 213  BC2b 281 267 5 265 255 3 311  BC2c 343 334 2 327 318 2 373  BC2d 879 827 6 809 763 5 935  BC2e 2676 2468 7 2534 2339 7 2914  BC2f 2708 2474 8 2566 2345 8 2946  BC3a 44150 39917 9 NA NA  44150  BC3b 35656 35482 1 NA NA  35656   For the datasets derived from the Book-Crossing  dataset [23] our approach for generating non-redundant multi-level approximate basis rules results in a smaller rule set. However, the reduction is low and the results also show that there are no expanded approximate association rules at all for the BC3a and BC3b datasets. We believe that the low reduction the approximate basis rule set in the Book-Crossing datasets is due to the sparseness of positive ratings (where a record indicates that a book in a category has been rated by the user that this record represents). For the BC2 datasets with 92,005 transactions (users) and 270 categories at the 2nd concept level there is only 427,422 actual ratings out of a possible 24,841,350 ratings. This equates to only 1.72%. Also, for the BC3 datasets with 92,005 transactions (users) and 960 categories at the 3rd concept level there is only 817,669 actual ratings out of a possible 88,324,800 ratings. This only equates to 0.926%. Thus for these two datasets most of the categories are unrated and each user has only rated a few categories at a given concept level.

For each test conducted we reviewed the expanded approximate association rules, i.e., those approximate     rules derived from the approximate basis. The tables show the number of expanded rules for each dataset. All four approaches were checked to ensure that they all derived identical expanded approximate rule sets. For all of our tests this was the case. Thus, the results show that our approach, while reducing the size of the approximate basis set does not lose any information and the expanded set of rules can be completely recovered.

6. Conclusion   Redundancy in association rules affects the quality of the information presented and this has a negative effect on its usefulness. The goal of redundancy elimination is to improve the quality and usefulness of the rules. Our work aims to remove hierarchical redundancy in multi-level and cross level rules from multi-level datasets, thus reducing the size of the rule set to improve the quality and usefulness, while remaining lossless. We have proposed an approach which removes hierarchical redundancy through use of a dataset?s hierarchy/taxonomy structure.

7. Acknowledgements   Computational resources and services used in this work were provided by the HPC and Research Support Unit, Queensland University of Technology, Brisbane, Australia.

8. References  [1] R. Agrawal, T. Imielinski & A. Swami, ?Mining  Association Rules between Sets of Items in Large Management of Data (SIGMOD?93), Washington D.C., USA, 1993, pp. 207-216.

[2] R. Agrawal & R. Srikant, ?Fast Algorithms for Mining Association Rules in Large Databases?, in 20th Santiago, Chile, 1994, pp. 487-499.

[3] F. Bodon, ?A Fast Apriori Implementation?, in IEEE ICDM Workshop on Frequent Itemset Mining Implementations (FIMI?03), Melbourne, FL, 2003.

[4] A. Das, W.-K. Ng & Y.-K. Woon, ?Rapid Association Rule Mining?, in CIKM?01 ACM, Atlanta, Georgia, USA, 2001.

[5] J. Han & Y. Fu, ?Discovery of Multiple-Level Association Rules from Large Databases?, in 21st International Conference on Very Large Databases, Zurich, Switzerland, 1995, pp. 420-431.

[6] J. Han & Y. Fu, ?Mining Multiple-Level Association Rules Data Engineering, Vol. 11, pp. 798-805, Sep/Oct., 1999.

[7] J. Han & J. Pei, ?Mining Frequent Patterns by Pattern- Growth : Methodology and Implications?, ACM SIGKDD Explorations Newsletter, Vol. 2, pp. 14-20, Dec., 2000.

[8] T.-P. Hong, K.-Y. Lin & B.-C. Chien, ?Mining Fuzzy Multiple-Level Association Rules from Quantitative Data?, Applied Intelligence, Vol. 18, pp. 79-90, Jan., 2003.

[9] M. Kaya & R. Alhajj, ?Mining multi-cross-level fuzzy weighted association rules?, in 2nd International IEEE Conference on Intelligent Systems, 2004, pp. 225-230.

[10] B. Liu, M. Hu & W. Hsu, ?Pruning and Summarizing the on Knowledge Discovery and Data Mining (KDD?99), San Diego, CA, 1999, pp. 125-134.

[11] B. Liu, M. Hu & W. Hsu, ?Multi-Level Organization and Summarization of the Discovered Rules?, in Conference on Knowledge Discovery in Data, Boston, Massachusetts, USA : ACM Press, 2000.

[12] K.-L. Ong, W.-K. Ng & E.-P. Lim, ?Mining Multi-Level Rules with Recurrent Items Using FP?Tree?, in 3rd and Signal Processing, Singapore, 2001.

[13] N. Pasquier, Y. Bastide, R. Taouil & L. Lakhal, ?Efficient mining of association rules using closed itemset lattices?, Information Systems, Vol. 24, Issue 1, pp. 25-46, 1999.

[14] N. Pasquier, R. Taouil, Y. Bastide, G. Stumme & L.

Lakhal, ?Generating a Condensed Representation for Association Rules?, Journal of Intelligent Information Systems, Vol. 24, pp. 29-60, 2005.

[15] G. Shaw, Y. Xu & S. Geva, ?Eliminating Redundant Association Rules in Multi-level Datasets?, in Proceedings (DMIN?08), 2008, Las Vegas, USA, To appear.

[16] Y. G. Sucahyo & R. P. Gopalan, ?CT-ITL : Efficient Frequent Item Set Mining using a Compressed Prefix Tree with Pattern Growth?, in 14th Australasian Database Conference, Adelaide, Australia, 2003.

[17] R. S. Thakur, R. C. Jain & K. P. Pardasani, ?Mining Level- Crossing Association Rules from Large Databases?, Journal of Computer Science, Vol. 12, pp. 76-81, 2006.

[18] Y. Xu & Y. Li, ?Mining Non-Redundant Association Rules Based on Concise Bases?, International Journal of Pattern Recognition and Artificial Intelligence, Vol. 21, pp. 659- 675, Jun., 2007.

[19] Y. Xu, & Y. Li, ?Generating Concise Association Rules?, in 16th ACM Conference on Conference on Information and Knowledge Management (CIKM?07), Lisbon, Portugal, 2007, pp. 781-790.

[20] Y. Xu, Y. Li & G. Shaw, ?Concise Representations for Approximate Association Rules?, in Proceedings of the Cybernetics (SMC?08), 2008, Singapore, To appear.

[21] M. J. Zaki, ?Generating Non-Redundant Association Rules?, in Proceedings of the KDD Conference, 2000, pp.

34-43.

[22] M. J. Zaki, ?Mining Non-Redundant Association Rules?, Data Mining and Knowledge Discovery, Vol. 9, pp. 223- 248, 2004.

[23] C-N. Ziegler, S. M. McNee, J. A. Konstan & G. Lausen, ?Improving Recommendation Lists Through Topic Diversification?, in Proceedings of the 14th International World Wide Web Conference (WWW?05), 2005, pp. 22- 32, Chiba, Japan.

