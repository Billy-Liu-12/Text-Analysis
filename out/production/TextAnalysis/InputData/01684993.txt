Two-Level Automatic Classification applied to Bulky Data Bases

Abstract  Classifying the data of a bulky base on the basis of high number of attributes is not an easy task because of the scarcity of the adequate methods present in the literature.

These methods generally resort to the reduction of the number of data using sampling techniques or the analysis in Principal components (APC). Problems are often encountered, namely the complexity of calculation, the slowness of execution and the relevance of the results. We developed for this purpose, an approach of Two-level automatic classification allowing to transform a bulky base into an exploitable group of classes for the extraction of knowledge and decision-making. The robustness, the precision and the optimality of our approach are shown through its comparison with the traditional approach of classification (classification of the original data base), and this, through the results produced following the application of two approaches to a bulky data base. These results include both the clusters and the Knowledge Map formed by association rules generated on the original base on the one hand, and the summary ofBIRCH on the other hand.

Key words Automatic Classification, KOHONEN maps, method KMEANS, BIRCH Algorithm, decision trees, association rules 1. Introduction  When assumptions are checked by the data, i.e., generally, when the data are supposed to follow laws looking like an exponential family (Gaussian, Binomial, Poisson...), the statistical techniques of modeling are optimal. However, as soon as the distributional assumptions are not checked and the assumed relations between the variables are not known, other methods must then necessarily be considered taking into account the algorithmic complexity of calculations. These methods were often developed in another disciplinary environment: Data Mining, which calls upon methods of hierarchical classification, algorithms of dynamic reallocation (KMEANS), networks of neurons (self- organizing map of KOHONEN), becomes a credible  alternative. However, when the base is of bulky size, the methods of Data Mining become insufficiently powerful to treat bulky series of data. Numerous comparative researches on methodologies have been undertaken for years. It has become more frequent to resort to the reduction of the number of data. It is obvious that methods such as sampling or Principal Components Analysis (PCA) are very appreciated by researchers.

They could, however, show certain weaknesses in term of complexity of calculation and slowness of execution.

In order to answer these specific problems, we conceived an approach of automatic classification Two- Level able to transform a bulky base into a group of exploitable classes for the extraction of knowledge and decision-making. Accordingly, the starting point of this solution (first level) is the extraction of a sample representative of the original data. This could be carried out, either with one of the traditional techniques of sampling (random, with equal or unequal probabilities...), or with the APC method (through identification of the relevant attributes). Nevertheless, a sophisticated method was presented in [ZRL96]. It allows a considerable reduction of the initial base by the generation of a compact summary in an tree diagram; BIRCH algorithm (Balanced Iterative Reducing and Clustering using Hierarchies). This is why we opted for it as the core of the first level of our classification approach, thus ignoring, the sampling procedures and/or the APC.

The following step (second level) is the extension of the methods and usual algorithms of automatic classification not supervised at outings of the first level. Indeed, it was a question, on the one hand, of adapting the approaches of Dated Mining and of choosing its optimal approach, acting not to apply them to the data, but to the summary generated by BIRCH in the form of graph. In addition, we set as secondary objective the generation of a chart of knowledge (Knowledge Map) made up of the group of association rules drawn at the same time from the original base and summary of algorithm BIRCH.

2. Some classification algorithms  0-7803-9521-2/06/$20.00 ?2006 IEEE.

a state of the art on classification algorithms can be found in [HKAOO ] and [ BBE02 ]. We point out the principal methods here.

2.1. KOHONENMap  It is about a not supervised ' training network and still competitive in way that one not only learns how to model the space of the entries by prototypes, but also to build a chart with one or two dimensions allowing to structure this space. These charts are characterized by a topological property, called "conservation of topology", making it possible to guarantee that similar data (within the meaning of a certain criterion) are projected or visualized on positions close in space to visualization.

Thus, they pursue the double goal of the topographic clustering which consists in determining under relevant groups of the data entity while storing information on the similarity of the clusters. According to this topographic process of clustering, the data input, displayable in vector form with dimensions, are brought back to classes which are self-organized according to a one-dimensional (string) or a two-dimensional structure (chart square, rectangular, hexagonal or even irregular) of nodes on which the vicinity relations are set in advance.

2.2. KMEANS  Given a corpus of data all belonging to the same Euclidean space. The average center algorithm (or KMEANS) allows partitioning these data in K classes (K fixed in advance). Each class is represented by a point in space commonly called prototype. The name of this algorithm comes owing to the fact that, during its unfolding, each prototype is the center of gravity of the data which it gathers in the corresponding class.

2.3. The decision trees  The decision trees are composed of a hierarchical structure in form of tree. This structure is built thanks to methods of training per induction starting from examples. The tree thus obtained represents a function which makes the classification of examples, while being based on the knowledge induced starting from a base of training. Because of that, they are also called graphs of induction (Induction Decision graphs). A little more formal definition of the decision trees is as follows: "a decision tree is a directed graph, without cycles, whose nodes carry a question, the arcs of answers, and the leaves of conclusions, or final classes ". Among the methods of training based on the most known decision trees, we can find ID3 algorithm [QUI75] and CART method [BF084]. These systems are rather similar; the  principal difference between these two systems lies in the choice of measurement used for the selection of the attributes during the construction of the tree. This measurement is generally founded on the Shannon information theory (entropy and profit of information - used in ID3). Method ID3 is at the origin of several other systems. One of most known is the C4.5 system, developed more recently by Quinlan [QUI93].

2.4. Association Rules  Introduced by AGRAWAL and Al. [AIS93], the method of extraction of the association rules was proposed initially to allow the analysis of the sales of supermarkets in order to extract rules of type: "If x Then y".

As HEBRAIL [HEB02] points it out, even if it were developed in a marketing goal, the method can be used in any other fields if the structure of the data allows it.

An association rule is an implication of the form x => ywherex c I, y c Iand xGny ={ }; with the set ={i= j2*. i..r }of m elements called items and the unit D = {lt, 't2 [,otn} of n elements called transactions.

A rule thus comprises a premise part (either antecedent or condition) made up of a unit of items2 and a conclusion part (either consequent or result) made up of a unit ofy items disjoint of x. The relevance of a rule is measured by its support s and its confidence c. There are two visions to express the support and confidence. From the probabilistic point of view, each subset of items associates the event according to which the transaction contains the items of this subset. The support is thus expressed by the probability of simultaneously carrying out events X and Y:  s=sup(X > Y)=P(X nY)= P(YX)P(X)  Where X is the event "the transaction contains all the items of the set x " and Y is the event "the transaction contains all the items of the set Y'. Confidence is equal to the probability of realization of the event Y knowing that event Xis carried out:  c = conf(X > Y) = P(X=() ) sup(X > Y) sup(X)  The higher the support is, the more frequent is the rule.

The more confidence is raised, the less there are counterexamples to the rule.

1 The possible classes are not known in advance, and the examples available are not labeled. C is thus to gather in same a cluster (or groups) the objects considered as similar, to constitute the classes.

2 An item is literal corresponding to a value or a set of values for an attribute selected in the data base.

0-7803-9521-2/06/$20.00 ?2006 IEEE.

classification (KOHONEN SOM and 2.5. Method BIRCH KMEANS).

BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) is a method of classification conceived by T ZHANG and Al in 1996 [ZRL96]. It makes it possible to reduce the initial set of data in a set of under-clusters (or subclasses), presented in a tree diagram (CF-tree), in order to simplify the problem of classification. BIRCH is incremental, i.e. it needs only one sweeping of the data file. It tries to minimize the cost of input/output by organizing the data treated in a structure of tree balanced with a limited size. BIRCH addresses mainly to the broad data bases by initially generating a more compact summary which keeps as much information as possible on the distribution of data and then classifying the summary generated instead of the original data base. In terms of assigned time, of use of the memory, quality of classification and stability of the algorithm, and by comparing it with other algorithms, BIRCH proves to be the best means of classification available which can be adopted at the very broad data bases.

3. Application  The objective of this article is to come out of a bulky data base with clusters having a semantic relevance (maximum homogeneity intra-cluster and heterogeneity inter-clusters maximum), this in an automatic way, proceeding by two levels and getting supported by traditional techniques of Mining Dated. Indeed, the traditional approach of classification generates a certain insufficiency in the case of a high number of data. We hope to succeed on the one hand in highlighting quite homogeneous groups in an automatic way, and on the other hand in confirming the contribution of the "Bi- level" character of classification, and finally in generating a "Knowledge Map" by extraction of association rules among the various variables of the data base. The flow chart of the following page makes it possible to trace the various stages of our step.

Two approaches were primarily implemented:  * Traditional approach of classification:  Application of KOHONEN SOM method and KMEANS to the original base.

e In our approach we consider the following two levels:  - First level: Generation of a compact summary of the initial base: the selected method is BIRCH;  - Second level: Classification of the summary generated at the time of the first level: call of methods of  We also generated decision trees of the original base and that produced by algorithm BIRCH. We carried out then the extraction of association rules of the type "If conditions THEN consequences", to reach at the end a "Knowledge Map" advantageous for decision-making.

Figure 1 displays the two approaches.

Figure 1 - Our approach vs. Classic approach  3.1. Description ofthe data  For our research, we considered a data base Spam. The Concept "Spam" is very diverse; it can relate to advertisements of marketing on pornographic products...

the data of our file include a set ofN = 4601 individuals described by p = 58 characteristics (or attributes) of which 57 are continuous and nominal. The majority of the attributes indicate if a word or a particular character is frequently met in the mail. Discrete variable "Spam" splits up the set of data in two categories:  * mails Spam or commercial; * mails not Spam.

3.2. Traditional approach ofclassification  Classif ication of the algorithm of KOHONEN to the original base  the application of the algorithm of KOHONEN on the original data made it possible to build a string length of 8.

0-7803-9521-2/06/$20.00 ?2006 IEEE.

* the clusterization proved of low quality. It did not exceed 0.753.

* The clusters obtained were characterized by: * Quite unbalanced cardinals (of 1664 to 54); * close semantics for some (cluster 6 and  cluster 7, for example).

* an error of classification of about 5,2%  (compared to target classification: 1813 Spam and 2788 not Spam) was detected following the separation of mails Spam and not Spam.

* classification required a little long execution time (350 ms).

Application of the method of the KMEANS to the original base  We applied the method ofKMEANS by fixing in advance the number of clusters at 8. To this sake, we use the module "Clustering -- K-Means "of the TANAGRA4 software.

* the quality of clusterization proved to be higher than that produced by the algorithm of KOHONEN: 0.86 0.75 previously obtained against with KOHONEN.

* the cardinals of the classes were very unbalanced (from 5 to 2732) and the execution of the algorithm much slower than that of the algorithm ofKOHONEN (642 ms against 350 ms).

* semantic relating to the different clusters were almost identical to those of the clusters produced by KOHONEN.

Generation of a decision tree starting from the original base  Considering that the methods of generation of decision trees ID3 and C4.5 treat only variables with discrete values, we rebuilt our data base by discretizing each of its variables. This operation was made by division of the set of the observations in ten equal intervals. The results so obtained are introduced like entries of application KSW 5 to produce a tree of 102 nodes, which required an execution time of about 1027ms  Extraction of rules of association starting from the original base: Knowledge Map  3 The closer this value is to the unit, the higher is quality.

4 It is an open source software with the direction that is possible to any researcher to reach the code and to add its own algorithms. IT is conceived by R. RACOTOMALALA [RAK05] for teaching and research.

5 It is about a platform of experimentation, conceived singularly for the generation of the decision trees and the rules of association.

The automatic extraction of the rules of association, with the help of the algorithm A priori, generated 122 rules whose 45 have a confidence lower than 80%, representing 3700 of the total and 47 have a confidence equal to 100%, representing 38.5%. The required Time for extraction of this Knowledge Map was 845 ms. We quote here some rules as examples.

1. If (CF_! = 10, CRL TOTAL = 9, 10, 6, 7 or 8, WF_MEETING = 10) then SPAM = 0 (c = 100.0%)  2. If (CF_! = 8, CRL_TOTAL = 5, 6 or 7, WF HP = 4, WF EDU = 5 or 9, WF 1999 = 5, WF RECEIVE = 5, WF LAB = 5, WF_85 5 ou 9, WF_WILL 9 or 10) ALORS SPAM= 0 (c 50.0?O)  3. If (CF_! 3, WF REMOVE = 5, CF_$ 4 or 7, WF_FREE 4, WF_FONT = 5, WF_MONEY = 5 or 9, WF GEOUGE = 5, WF HP = 4, WF 650 = 5 ou 9, CRL LONGES 3, 1, 2 or 4, WF INTERNE = 5, WF RECEIVE 10) then SPAM = 0 (c = 50.00/O)  4. If (CF_! 3, WF REMOVE = 5, CF $ = 4 or 7, WF_FREE 4, WF_FONT = 5, WF_MONEY = 5 or 9, WF GEORGE 5, WF HP = 4, WF 650 = 5 or 9, CRL LONGEST 3, 1, 2 or 4, WF INTERNET = 5, WF RECEIVE = 5 or 9, WF CREDIT = 5, WF YOU  10,7, 8, 5 or3, WF RE =4)then SPAM= 0 (c 87.8%)  5. If (CF_! 3, WF REMOVE = 5, CF_$ 4 or 7, WF FREE 4, WF_FONT = 5, WF_MONEY = 5 or9, WF GEORGE = 5, WF HP = 4, WF_650 = 5 or 9, CRL LONGEST = 3, 1, 2 or 4, WF INTERNET= 5, WF RECEIVE = 5 or9, WF CREDIT = 5, WF YOU 10, 7, 8, 5 ou 3) then SPAM = 0 (c 100.0%0)  It is significant to notice that several of the extracted rules are not of an interest for the user (rules 2 and 3, for example) besides, we note the existence of rules which are more specific than others without improving quality of predictions (rules 4 and 5, for example).

3.3. Our approach  To avoid the problems arising from the high number of data, an approach "Two-Level" is employed in order to improve the performances:  First level.

Application of BIRCH algorithm to the original data base  For this level, we opted for BIRCH algorithm for the generation of a compact summary of the original data base. For this purpose, we carried out an implementation of BIRCH in C++. the application of  0-7803-9521-2/06/$20.00 ?2006 IEEE.

this algorithm on the totality of the data allowed to generate a compact summary in the form of CF-tree comprising 1510 node-leaves (or subclasses), that is to say a reduction of 67,2% of the size of the initial base.

The maximum size of these subclasses was fixed a priori at 6. The outlook of the tree CF obtained is represented in the shape of a file XML (cftree.xml) which expresses very well the hierarchical structure of CF-tree that one obtains at the end of the tree construction. The produced subclasses of the construction of the tree are given in a textual file (clusters.txt). These subclasses are only the node-leaves of the CF tree defined by:  * the number of elements which they contain (the maximum is fixed a priori at 6);  * the linear sum of the data which they contain (vector of dimension 57);  * the sum with square of the components.

Second level.

Application of the algorithm of KOHONEN on the summary generated by BIRCH  Following the introduction of algorithm BIRCH into the procedure of classification, and in comparison with the results of the application ofKOHONEN on the rough basis, we notice that:  * the quality of the clusterization increased by 74,900 to 80,7%;  * the time execution was decreased 350 ms to 140 ms, that is to say a reduction of 60%;  * the allocated memory was reduced of 1088 KB to 414 KB, that is to say a profit of62%;  * an error rate of 1,10% only. Thus, the introduction of the character "Two-Level" into the procedure of classification made it possible to improve quality in terms of memory capacity and execution time allocated while producing clusters whose semantic ones seem strongly with those relating to the clusters generated by the traditional approach  Application of the algorithm of KMEANS on the summary generated by BIRCH  The quality of classification, noticed following the classification of the data by the algorithm KMEANS (0.94), the execution time necessary (230 ms against 642 ms for the application ofKMEANS to the original base) and the error rate (1.7% against 3.400 following the application of KOHONEN to the original base) still prove the contribution of our approach compared to the traditional approach.

The same procedure of discretization of the attributes is applied for the summary of BIRCH. The number of nodes obtained is much lower (48 instead of 102 nodes).

The execution is much faster (796 ms against 1027 ms).

Extraction of rules of association starting from summary BIRCH: Knowledge Map  The application of the algorithm A priori to the base formed by the subclasses generated by BIRCH, made it possible to extract 64 rules from association among which 16 have a confidence lower than 80%, representing 25.0% of the total and 27 have a confidence of 100%, or 42.2%. The execution time was about 463 ms (against 845 ms). This still proves the primacy of our approach compared to the traditional approach.

Some rules are given here as examples.

1. if (LS_TELNET = 5, LS LONGEST = 1, LS OUR = 3 or 5, LS MEETING = 5 or 10, LS RECEIVE = 4, 8 or 9, LS RE = 3, 7, 5 or 6, LS_LAB = 10 or 9, LS-GEORGE = 3) then SPAM = 1 (c = 100.0%)  2. if (LS_TELNET = 5, LS LONGEST = 2, LS YOU 1 or 10, LS TECHNOLOGY = 4, 10 or8, LS 1999 10, LS GEORGE = 10) then SPAM = 0 (c =  100.0%0)  3. if (LS_TELNET = 5, LS LONGEST = 3, LS RE = 3, 9, 10, 8, 7 or5, LS PARTS = 5, LS HPL = 4, LS_3D = 4, 9 or 8, LS BUSINESS = 9, LS-GEORGE = 9) then SPAM =0 (c = 100.0%)  Thus, and whatever the algorithm chosen at the second level (SOM of KOHONEN, KMEANS, ID3 or A priori), our Two-Level approach made it possible to improve quality of classification of the data of study. So the application of the traditional techniques of classification on a sample (in our case, the summary generated by BIRCH) extracted the base makes it possible to extend the knowledge produced for this sample to all the data of the original base, while guaranteeing results of good quality, even in the case of a base with high size. The table which follows recapitulates the characteristics of various methods applied on the one hand to the original base, and on the other hand with the summary of algorithm BIRCH.

Generation of a decision tree starting from the summary produced by BIRCH  0-7803-9521-2/06/$20.00 ?2006 IEEE.

[Qualite L Temps d 'exicutian ( ]sI T 8~~~~~~~ASEODlIGINElblE  SOM le KOHONEN |\i|t  IKIEANSI.2 642 |D3a 102 nrjeudL, 0211  Apri ci | 2 R.A. doni W on unLeccnf > s45  R,SRME;r DE L'ALGO'RIt E BIRCH  |SONIdelKOHONEN|019140  T E;NtEANS | ~0.94 T e30  TIL D I 43n eud 796I - T ioTi | g R. A. doi3t5% o31t une1 conf ~> 30% 463I  Table 1 - Characteristics of the methods applied to the original base and the summary of algorithm  BIRCH  4. Conclusion  The major challenge of this work is to face the deficiencies met by classifying bulky data bases, by the way the complexity of calculation, the slowness of execution, the relevance of the class generated... To answer these problems, we proposed an approach of classification Two-Level able to transform a high number of data into a unit of exploitable classes for a reliable decision-making extraction of knowledge and better. The first level of this approach is centered on the algorithm BIRCH which allows a considerable reduction of the original base by the generation of a compact summary in a tree diagram (CF-tree). The second level is the application of the usual methods of classification on the summary of BIRCH algorithm, thus passing not imply to the formation of representative classes ,but also to the generation of Knowledge Map composed of the list of the most relevant rules of association. In addition, and in order to show the effectiveness of our Bi-level approach, in comparison with the traditional approach of classification, we carried out an experimentation of two approaches on a basis of data Spam. Results produced following the application of algorithms SOM ofKOHONEN and KMEANS, on the original basis on the one hand, and the summary of algorithm BIRCH on the other hand, as well as Knowledge Map generated on these two bases proved well the superiority of our Two-Level approach compared to the traditional approach of classification.

5. Prospects  A Two-level approach for classification for the bulky data bases has been proposed by this article. However, with an aim of an even higher optimizing approach and of extending it over other problems of classification, certain axes of thought deserve to be considered.

Without claiming to be exhaustive, here are some:  * In case of bases storing a large number of data (million) which handles many variables (thousands), the proposed Two-level approach could lead to good results? Or perhaps would it be necessary to consider more than one level?

* Up to which level of exactitude, the results extracted from a sample of a whole of data could be extended to the whole base?

* That an algorithm arrives at good result is a good thing, still it is necessary that it does it in a reasonable time. An algorithm can thus require many resources (memories, time, disk space...) to achieve a result, while another c better conceived one would do it in a more effective way. The comparison of the effectiveness of the algorithms is made thanks to the notation "large 0" ("O "meaning" about") or notation of Landon, which describes the asymptotic behavior of the mathematical functions, and by extension allows to indicate the speed with which it (or its descriptive curve) increases or decreases.

Thus, by bringing an algorithm closer to one of the notations large 0, one is measurement to directly compare its amplitude with that of another algorithm, and thus to see which is the most adapted for work to achieve, or the framework in which it is carried out.

6. References  [AIS93]: R. AGRAWAL, T. IMIELINSKI and A.

SWAMI, "Mining associations between sets of items in large databases", ACM SIGMOD Conference on management of data, Washington DC, USA, pp. 207 - 216, 1993.

[BBE02]: P. BALEZ and M. BEAL, "Algorithmes de Data Mining", Proceedings of the Eighteenth National Conference on Artificial Intelligence, pp.114 - 127, 2002.

[BF084]: L. BREIMAN, J. FRIEDMAN, R. OLSHEN, A. STONE, "Classification and Regression Trees", Belmont, CA: Wadsworth. 1984.

[HEB02]: G. HEBRAIL. "Introduction 'a la recherche de regles d'associations et de sequences frequentes", Bases de donnees et Statistique, Dunod, Paris (France), Chap.

8, 2002.

[HKAOO]: J. HAN et M. KAMBER, "Data Mining concepts and techniques", Morgan Kaufmann Publishers, 2000.

[KOH97]: T. KOHONEN, "Exploration of very large databases by self-organizing maps", Proceedings of the  0-7803-9521-2/06/$20.00 ?2006 IEEE.

Houston, Texas, USA, pp. 1-6, 1997.

[KOH95]: T. KOHONEN, "Self-Organizing Maps", Springer Series in Information Sciences, , Springer, Berlin, Heidelberg, New York, 1995.

[QUI93]: J. QUINLAN, "C4.5: Programs for Machine Learning", Morgan Kaufmann Publishers, San Mateo, U.S.A, 1993.

[QUI75]: J. QUINLAN, " Machine Learning". IJCAI, Vol. 1, pp. 363-369, 1975.

[RAK05]: R. RAKOTOMALALA, "TANAGRA : un logiciel gratuit pour l'enseignement et la recherche", in Actes de EGC'2005, RNTI-E-3, vol. 2, pp.697-702, 2005.

[ZHA00]: C. ZHANG, S. ZHANG, "Association rules mining models and algorithms", Springer- Verlag Publishers in Lecture Notes on Computer science, Volume 2307, p. 243, 2000.

[ZRAOO]: D. ZIGHED et R. RAKOTOMALALA, "Graphes d'induction et Data Mining", Hermes, Paris, 2000.

[ZRL96]: T. ZHANG, R. RAMAKRISHNAN and M.

LIVNY, "Birch: A new data clustering algorithm and its applications", ACM SIGMOD, Vol. 25, pp. 103 - 114, 1996.

0-7803-9521-2/06/$20.00 ?2006 IEEE.

