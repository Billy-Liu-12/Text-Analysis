GUFI: a new algorithm for General Updating of Frequent Itemsets

Abstract  Incremental maintenance of association rules is an in- teresting problem that has been tackled in several research works. Even though literature abundance in this way, there is still a lack of methods that process the complete main- tenance problem, that is, incremental maintenance for any type of update (insertion or deletion of data) and for any new support threshold. In this paper, we develop a new method of general incremental maintenance. The experi- mentations showed that our algorithm is more efficient than classical maintenance approaches.

1 Introduction  Mining association rules is one of important KDD tasks thanks to its applicability in several fields [11], [9] and [5]. The literature includes several association rules min- ing (ARM) algorithms that process huge databases [1], [7], [19], [14] etc. However, databases are periodically up- dated. So maintenance of initially mined association rules becomes necessary, since these rules describe the database before its updates and do not describe it any more after.

The trivial solution to this problem is to rerun an ARM algorithm on the whole of the updated database. But this solution ignores previously mined association rules which makes it costly in comparison with another type of main- tenance which is performed in incremental manner [3]. In- deed, incremental maintenance methods take old associa- tion rules into account as well as inserted and/or deleted data to compute the new ones. This way makes incremental maintenance methods less costly than classical ones. How- ever, the drawback of incremental methods is that most of them operate for the same evaluation measurement (support threshold) of the initial mining [4], [12], [20] and [8] which reduces the parameters choice of the data analyst, i.e., the data analyst cannot choose a new support threshold for the maintenance operation. Other incremental maintenance al- gorithms function only in the case of data insertion, ignor-  ing the case of data deletion [2], [17] and [15]. On the other hand, classical approach allows to the data analyst choosing the mining parameters and function for any data updating kind since the whole of mining operation will start from the beginning.

In this paper, we try to conceive a new method that joins the advantages of classical maintenance methods (mainte- nance under any support threshold and for any data updat- ing kind) with incremental maintenance ones (computation efficiency).

The remainder of the paper is organized as follows: in Section 2 we formulate the problem of association rules incremental maintenance. In Section 3, we introduce the different kinds of itemsets that are included in the updated database, thereafter we present our solution in Section 4.

Then in Section 5, we relate the different scenarios of our algorithm, we present the algorithm pseudo-code in Section 6 and an illustrative example in Section 7. Section 8 shows experimentations results led on our algorithm. Finally, in Section 9 we conclude our work and give some perspec- tives.

2 Incremental maintenance problem  Let I={i1,i2,. . . ,in} be a set of n items. Let DB be a database of D transactions with scheme < tid, items >.

Each transaction is identified by a transaction identifier tid and is included in I (items). An association rule is X ? Y with X,Y ? I , X ?= ? and X ? Y = ?. Association rule support of X ? Y is the occurrence number of Z = X?Y in DB denoted by Z.supportDB , its confidence is the ratio Z.supportDB/X.supportDB . Given a support threshold minsup and a confidence threshold minconf , association rule mining problem is computing association rules with supports exceeding minsup% and confidences exceeding minconf %. An itemset is a set of items, it is said to be fre- quent in DB if its support exceeds minsup%. ARM prob- lem is divided into two subproblems: (1) frequent itemsets generation and (2) association rules computation from fre- quent itemsets. The whole of the association rule problem   DOI 10.1109/CSEW.2008.38    DOI 10.1109/CSEW.2008.38    DOI 10.1109/CSEW.2008.38    DOI 10.1109/CSEW.2008.38    DOI 10.1109/CSEW.2008.38    DOI 10.1109/CSEW.2008.38     is often reduced to frequent itemset mining, because once frequent itemsets are generated, association rules compu- tation becomes a straightforward problem that is less costly comparing to the first subproblem [1]. In our work we focus only on frequent itemsets mining step.

Let L be the set of frequent itemsets in DB and minsup the support threshold under which L was mined. After some updates of DB -consisting in inserting an increment db+ of size d+ and in deleting a decrement db? of size d?-, we obtain DB? = DB ? db+ \ db?. The size of DB? is de- noted by D?. The problem of frequent itemsets incremental maintenance consists in computing L?: the set of frequent itemsets in DB? under a new support threshold denoted by minsup?.

3 Kinds of itemsets  Having the set L that includes frequent itemsets in DB according to minsup, our goal is to compute L? including frequent itemsets in DB? following to minsup?. Itemsets in L? come from L or from its complement in I2 (universe of itemsets) that is L?. Itemsets of L? coming from L are called persistent itemsets. In fact, they persisted to data updating and support threshold change and they remained frequent in DB? under minsup?. Itemsets that were in L but are not in L? are called looser itemsets. However, itemsets of L? coming from L? are called winner itemsets. These itemsets have won since they was infrequent in DB but became fre- quent now thanks to data updating and/or support threshold change.

So, our algorithm objective is to extract winner and per- sistent itemsets since L? is exclusively composed of these two itemsets kinds.

4 Description of GUFI (General Update of Frequent Itemset)  GUFI is the algorithm we conceived to resolve incre- mental maintenance problem defined in Section 2. It is a level-by-level algorithm like [1], it proceeds as follows:  For each iteration (equivalent to each level), GUFI gener- ates the set of candidate itemsets of size k namely Ck from the set of frequent itemsets of size k Lk?1? via candidate generation function Apriori Gen of [1]. The first set C1 is the set of all items in DB?. Now, frequent itemsets of Lk? are certainly in Ck, which contains two kinds of itemsets:  1. Itemsets that are frequent in DB under minsup. These itemsets could persist. They are stored in a set called PPk since they are potentially persistent.

2. Itemsets that are infrequent in DB under minsup.

These itemsets are potentially winner, we store them in a set called PWk.

Thus, the set Ck is divided into two complementary sets; the set PPk = Ck ? Lk including candidate itemsets that are in Lk (potentially persistent itemsets) and the set PWk = Ck \ Lk including the remainder candidate item- sets (potentially winner itemsets). We recall that supports of itemsets in PPk and PWk in DB are known. Then, we scan the increment db+ and the decrement db?, we update supports of itemsets in PPk (so we get their supports in all DB?) and also supports of itemsets in PWk. We can already distinguish persistent itemsets (support ? minsup? ? D?) from looser ones (support < minsup? ? D?). To com- plete Lk?, we must now find the winner itemsets. That is why we compute supports of potentially winner itemsets in DB? = DB \ db?. Having already their supports in db+, we get their supports in the updated database DB? and we keep only winner ones (support ? minsup? ?D?).

The scan of the data increment and data decrement to compute itemsets supports of PPk and PWk is relatively light comparing to the scan of DB? which has a more larger size. The return to DB? is very costly but at the same time necessary to extract winner itemsets from PWk.

In this work, our contribution is the optimization of the scan of DB? by pruning the set PWk. Indeed, thanks to proposition 1, we know that some itemsets of PWk cannot win, we delete them before beginning the scan of DB?. In this proposition we compute a candidate pruning threshold denoted by cpt. Each itemset X in PWk having

X.supportdb+?X.supportdb? < cpt cannot win even if it is included in minsup % of DB (it is the maximum support that could have a potentially winner itemset in DB since it was infrequent in DB under minsup), and will be pruned from PWk before starting the original database scan.

Proposition 1 (Pruning the set PWk) Let X be a poten- tially winner itemset (X ? PWk) and cpt = minsup? ? (d+ ? d?) + D ? (minsup? ? minsupp) + 1. If

X.supportdb+ ? X.supportdb? < cpt 1 then X cannot win.

Proof Assume that X ? PWk. X wins if X.supportDB? ? minsup? ?D? ? X.supportDB + X.supportdb+ ? X.supportdb? ? minsup? ?D? ? X.supportdb+ ? X.supportdb? ? minsup? ? D? ?

X.supportDB However, the maximum support of X in DB is minsup ? D ? 1 (we recall that X is not frequent in DB under minsup, so X.supportDB < minsup ?D) ? X.supportdb+ ? X.supportdb? ? minsup? ? D? ? (minsup ?D ? 1) ? X.supportdb+ ? X.supportdb? ? minsup? ? (D + d+ ? d?)? (minsup ?D ? 1)  1Candidate Pruning Threshold.

? X.supportdb+ ? X.supportdb? ? minsup? ? (d+ ? d?) + D ? (minsup? ?minsup) + 1 ? X.supportdb+ ?X.supportdb? ? cpt  We illustrate here the utility of proposition 1 via a lit- tle example: Assume that a mining operation has been made on a database DB under an absolute support thresh- old (minsup ?D) of 5. After updating DB (inserting db+ and deleting db?) and changing support threshold percent- age minsup?, the absolute support threshold (minsup??D?) becomes 6. Now, all potentially winner itemsets must exist at least in two transactions in db+ \db? (cpt = 2), else they will not win since their maximum support in DB is 4.

Figure 1 illustrates a simplified description of our algo- rithm process:  Figure 1. Simplified process of GUFI.

5 The three scenarios of GUFI  Thanks to proposition 1, the number of potentially win- ner itemsets is reduced which makes scan of DB? less costly. However, checking proposition 1 might be useless in some cases. Indeed, if X is a potentially winner itemset, it must satisfy X.supportdb+ ?X.supportdb? ? cpt else it will be pruned. Now, X.supportdb+ ?X.supportdb? is into the interval [?d?, d+], so if cpt itself is outside the in- terval, then checking proposition 1 will be useless. That is why we distinguish three possible scenarios:  5.1 First scenario: ?d? ? cpt ? d+  In this case, proposition 1 has to be checked and the set PWk will be pruned of all itemsets that check the propo- sition. The process related to this scenario is described in figure 1.

5.2 Second scenario: cpt > d+  We already said that X ? PWk wins if and only if it satisfies X.supportdb+ ? X.supportdb? ? cpt. But in this scenario, cpt > d+, which makes X.supportdb+ ?

X.supportdb? surely less than cpt. So it is useless to check proposition 1 in this scenario.

We also know, thanks to proposition 2, that potentially winner itemsets can never win, so in this scenario there is no return to DB? because we have no potentially winner itemsets.

Proposition 2 If cpt > d+ Then all itemsets in PWk can- not win.

Proof Let be X an itemset of PWk, we know that: (1) X.supportDB ? minsup ?D ? 1 (X is infrequent in DB under minsup) (2) X.supportdb+ ? d+ (3) ?X.supportdb? ? 0 since X.supportdb? ? 0 (1),(2) and(3)? X.supportDB? ? minsupp?D+d+?1(i)  Now Cpt > d+ ? minsup??(d+?d?)+D?(minsup?? minsup) + 1 > d+  ? minsup? ? d+ ?minsup? ? d? + D ?minsup? ?D ? minsup + 1 > d+  ? minsup? ? d+ ? minsup? ? d? + D ? minsup? > D ?minsup + d+ ? 1 ? minsup?? (D + d+? d?) > D?minsup + d+? 1 (ii) (i) and (ii)? X.supportDB? < minsup?? (D + d+? d?)  According to proposition 2, the set Lk? is composed ex- clusively by the persistent itemsets that are extracted from PPk after scanning increment db+ and decrement db?.

Computation of the set L? is illustrated in figure 2.

5.3 Third scenario: cpt < ?d?  Like in the second scenario, checking proposition 1 is useless since all potentially winner itemsets satisfy the con- dition X.supportdb+ ? X.supportdb? ? cpt. In this scenario, PWk is not pruned and the scan of DB? is very costly because we must update supports of all poten- tially winner itemsets to extract winner ones in DB? under minsup?. We also know, thanks to proposition 3, that all potentially persistent itemsets of PPk are frequent in DB?.

Proposition 3 If cpt < ?d? Then all itemsets of PPk are frequent.

Proof Let X be an itemset of PPk, we know that: (1) X.supportDB ? minsup ?D     Figure 2. L? computation in the second sce- nario  (2) X.supportdb+ ? 0 (3) ?X.supportdb? ? ?d? since X.supportdb? ? d? (1), (2) and (3)? X.supportDB? ? minsup ?D ? d? (i) Now, cpt < ?d? ? minsup?? (d+?d?)+D? (minsup??minsup)+1 < ?d? ? minsup? ? d+ ?minsup? ? d? + D ?minsup? ?D? minsup + 1 < ?d? ? minsup? ? d+ ? minsup? ? d? + D ? minsup? < D ?minsup ? d? ? 1 ? minsup?? (D + d+? d?) < D?minsup? d?? 1 (ii) (i) and (ii)? X.supportDB? ? minsup?? (D + d+? d?)  According to proposition 3, all frequent itemsets in DB under minsup (? PPk) are persistent, i.e., there is no looser itemsets. The set Lk? is then composed by (1) all itemsets of PPk (or Lk since in this scenario, PPk = Lk) and (2) winner itemsets that will be identified after scanning DB?.

Computation of the set L? is illustrated in figure 3.

Figure 3. L? computation in the third scenario  6 Algorithm GUFI  We present in this section the pseudo-code of the algo- rithm GUFI. Algorithm 1 is the main procedure. According to cpt value, it calls the suitable procedure for the detected scenario.

Algorithm 1: Main procedure GUFI  Input: DB?, D, db+, d+, db?, d?, L, minsup, I, minsup? Output: L?  01 cpt = minsup? ? (d+ ? d?) + D ? (minsup? ?minsup) + 1  02 If ?d? ? cpt ? d+ Then  03 GUFI1 (DB?, L, I, minsup, db+, db?, minsup?, D, d+, d?, cpt)  04 ElseIf cpt > d+ Then  03 GUFI2 (L, minsup, db +, db?, minsup?, D, d+, d?)  06 Else  07 GUFI3 (DB?, L, I, minsup, db+, db?, minsup?, D, d+, d?)  08 End If  Algorithm 2: The procedure GUFI1  Input: DB?, D, db+, d+, db?, d?, L, minsup, I, minsup?, cpt Output: L?  01 k = 1; Lk? = ?  02 While Lk? ?= ? or k = 1 do  /*Initialization of the candidates in Ck*/  03 If (k = 1) Then  04 Ck = I  05 Else  06 Ck = Apriori Gen(Lk?1?)  07 End If  /*Computation of potentially winner and potentially persistent itemsets.*/  08 P Pk = Ck ? Lk 09 P Wk = Ck \ P Pk /*Updating supports of potentially persistent and potentially winner itemsets  in db+ and db?.*/  10 For t ? db? loop  11 C = Subset(t, P Pk ? P Wk)  12 For X ? C loop  13 X.sup db? = X.supdb? + 1  14 End Loop  15 End Loop  16 For t ? db+ loop  17 C = Subset(t, P Pk ? P Wk)  18 For X ? C loop  19 X.supdb+ = X.supdb+ + 1  20 End Loop  21 End Loop  /*Deleting looser itemsets*/     22 For X ? P Pk loop  23 If X.supDB ?X.supdb? + X.supdb+ < minsup? ? (D + d+ ? d?)  Then  24 P Pk = P Pk \ {X}  25 End If  26 End Loop  /*Pruning potentially winner itemsets that check proposition 1*/  27 For X ? P Wk loop  28 If X.supdb+ ?X.supdb? < cpt Then  29 P Wk = P Wk \ {X}  30 End If  31 End Loop  32 If (P Wk ?= ?) Then  /*Scanning DB ? to update supports of the remaining potentially winner  itemsets.*/  33 For t ? DB? loop  34 C = Subset(t, P Wk)  35 For X ? C loop  36 X.supDB? = X.supDB? + 1  37 End Loop  38 End Loop  /*Keeping winner itemsets.*/  39 For X ? P Wk loop  40 If X.supDB? + X.supdb+ < minsup? ? (D + d+ ? d?) Then  41 P Wk = P Wk \ {X}  42 End If  43 End Loop  44 End If  /*The set Lk? is composed of persistent and winner itemsets.*/  45 Lk? = P Pk ? P Wk 46 L? = L? ? Lk?  47 k = k + 1  48 End While  Algorithm 3: The procedure GUFI2  Input: D, db+, d+, db?, d?, L, minsup, minsup? Output: L?  01 k = 1; Lk? = ?  02 While Lk? ?= ? or k = 1 do  /*P Pk includes initially old itemsets that are frequent in DB. P Wk is  ignored in this scenario since there is no itemsets that can win.*/  03 If (k = 1) Then  04 P Pk = Lk  05 Else  06 Ck = Apriori Gen(Lk?1?)  07 P Pk = Ck ? Lk 08 End If  /*Updating supports of potentially persistent itemsets in db + and db?*/  09 If (P Pk ?= ?) Then  10 For t ? db? loop  11 C = Subset(t, P Pk)  12 For X ? C loop  13 X.supdb? = X.supdb? + 1  14 End Loop  15 End Loop  16 For t ? db+ loop  17 C = Subset(t, P Pk)  18 For X ? C loop  19 X.supdb+ = X.supdb+ + 1  20 End Loop  21 End Loop  /*Deleting looser itemsets.*/  22 For X ? P Pk loop  23 If X.supDB?X.supdb?+X.supdb+ ? minsup??(D+d+?d?)  Then  24 L? = L? ? {X}  25 End If  26 End Loop  27 End If  /*In this scenario, Lk? is composed only by P Pk.*/  28 L? = L? ? Lk?  29 k = k + 1  30 End While  Algorithm 4: The procedure GUFI3  Input: DB?, D, db+, d+, db?, d?, L, minsup, I, minsup? Output: L?  01 k = 1; Lk? = ?  02 While Lk? ?= ? or k = 1 do  /*Initialization of the candidates in Ck.*/  03 If (k = 1) Then  04 Ck = I  05 Else  06 Ck = Apriori Gen(Lk?1?)  07 End If  /*Computation of potentially winner and persistent itemsets. To note that in  this scenario, there is no looser.*/  08 P Pk = Lk  09 P Wk = Ck \ P Pk /*Updating persistent itemsets supports and potentially winner itemsets  supports in db+ and db?.*/  10 For t ? db? loop  11 C = Subset(t, P Pk)  12 For X ? C loop  13 X.sup db? = X.supdb? + 1  14 End Loop  15 End Loop  16 For t ? db+ loop  17 C = Subset(t, P Pk ? P Wk)  18 For X ? C loop     19 X.supdb+ = X.supdb+ + 1  20 End Loop  21 End Loop  /*Scanning DB ? to update potentially winner itemsets supports.*/ 22 For  t ? DB? loop  23 C = Subset(t, P Wk)  24 For X ? C loop  25 X.supDB? = X.supDB? + 1  26 End Loop  27 End Loop  /*Keeping winner itemsets.*/  28 For X ? P Wk loop  29 If X.supDB? + X.supdb+ < minsup? ? (D + d+ ? d?) Then  30 P Wk = P Wk \ {X}  31 End If  32 End Loop  /*The set Lk? is composed of persistent and winner itemsets.*/  33 Lk? = P Pk ? P Wk 34 L? = L? ? Lk?  35 k = k + 1  36 End While  7 An illustrative example  In this section, we try to illustrate GUFI over a database example presented in table 1. Let DB be a database with schema < tid, items >. DB includes six transactions.

To illustrate GUFI, we perform three incremental mainte-  tid items 1 ABC 2 ABC 3 ABC 4 AB 5 AC 6 AC  Table 1. Transactional database  nances such that each one corresponds to a different sce- nario. We perform three executions of GUFI, in each one we modify algorithm parameters (increment db+, decre- ment db?, minsup and minsup?).

7.1 First scenario: d? ? cpt ? d+  We assume that initial mining operation has been made under the support threshold minsup = 4/6  66.66%. The resulting frequent item- sets set is L = {A6, B4, C5, AB4, AC5} 2. We also assume that minsup? = 3/6 = 50%, that  2Integers in subscript are supports of itemsets in the database  bd? = {(1, ABC), (2, ABC)} and that bd+ = {(7, BC), (8, AC)}. We compute cpt = 0: it is the first scenario. We run manually GUFI to maintain L: C1 = I , PP1 = L1 = {A6, B4, C5} and PW1 = I \ L1 = ?.

After scanning db?, PP1 = {A4, B2, C3}. After scanning db+, PP1 = {A5, B3, C5}. At the iteration k = 1, we have not potentially winner itemsets, and that is why we update only supports of potentially persistent itemsets when scanning db+ and db?. minsup? being fixed at 50%, L1? = {A5, B3, C5}. We go to next iteration and we compute C2 = Apriori Gen(L1?) = {AC,AB,BC}.

Computation of PP2 and PW2 from C2; PP2 = C2?L2 = {AB4, AC5} and PW2 = C2 \ L2 = {BC}. After scan- ning db?, PP2 = {AB2, AC3} and PW2 = {BC?2}.

After scanning db+, PP2 = {AB2, AC4} and PW2 = {BC?1}. Here, we delete looser item- sets, which makes PP2 = {AC4} and we check if potentially winner are over or under cpt. In our case, PW2 includes only one itemset BC with BC.supportdb+ ? BC.supportdb? = ?1 <= cpt.

BC is pruned because it checks proposition 1, thus we have no need to scan DB?. So PW2 is an empty set. The set of frequent itemsets in this iteration is L2? = PP2 ? PW2 = {AC4}. Then we compute C3 = Apriori Gen(L2?) = ?, L3? is consequently empty which stops our algorithm with L? = {A5, B3, C5, AC4}.

7.2 Second scenario: cpt > d+  We assume that initial mining operation has been made under the support threshold minsup = 3/6 = 50%. The resulting frequent itemsets set is L = {A6, B4, C5, AB4, AC5, BC3, ABC3}. We also as- sume that minsup? = 5/6  83.33%, that bd? = {(1, ABC), (2, ABC)} and that bd+ = {(7, AC), (8, AC)}. We compute cpt = 3 > d+; we are in the second scenario.

In this scenario, we know that L? is composed exclu- sively by persistent itemsets (proposition 2). So we have looser itemsets and no winner ones, consequently, the re- turn to DB? is useless. We compute PP1 = L1 = {A6, B4, C5}. After scanning db? and db+, PP1 = {A6, B2, C5}, so L1? = {A6, C5}. Then, we go to the next iteration and we compute C2 = Apriori Gen(L1?) = {AC}. PP2 = C2 ? L2 = {AC}. After scanning db? and db+, PP2 = {AC5}, so L2? = {AC5}. C3 = Apriori Gen(L2?) = ?, so L3? is also empty which stops GUFI for L? = {A6, C5, AC5}.

7.3 Third scenario: cpt < ?d?  We assume that initial mining operation has been made under the support threshold minsup = 5/6  83.33%.

The resulting frequent itemsets set is L = {A6, C5, AC5}.

We also assume that minsup? = 1/6  16.66%, that bd? = {(1, ABC), (2, ABC)} and that bd+ = {(7, BC), (8, BC)}. We compute cpt = ?3 < ?d?; it is the third scenario.

In this scenario, we know there is no looser itemsets (proposition 3) and there is very probably winner item- sets. C1 = I , and so PP1 = C1 ? L1 = {A6, C5} and PW1 = C1 \ L1 = {B}. After scanning db?, we have PP1 = {A4, C3}. After scanning db+, we have PP1 = {A4, C5} and PW1 = {B2}. We are obliged to return to DB? to compute supports of PW1 itemsets.

We obtain PW1 = {B4}. After extracting winner item- sets according to minsup?, we have L1? = PP1 ? PW1 = {A4, B4, C5}. The computation of C2 is done: C2 = Apriori Gen(L1?) = {AC,AB,BC}. Then, we com- pute PP2 and PW2 from C2: PP2 = C2 ? L2 = {AC5} and PW2 = C2 \ L2 = {AB,BC}. After scanning db?, we have PP2 = {AC3}. After scanning db+, we have PP2 = {AC3} and PW2 = {AB0, BC2}. We return to DB? to update PW2 = {AB2, BC3}. We deduce that the set L2? is {AB2, BC3, AC3}.Then we go to next iteration k = 3, we compute C3 = Apriori Gen(L2?) = {ABC}.

The set PP3 = ? since L3 = ?. On the other hand PW3 = {ABC}. So we scan DB? to update this set and we obtain PW3 = {ABC1}. The result is L3? = {ABC1} and so L = {A4, B4, C5, AB2, BC3, AC3, ABC1}. The set C4 = Apriori Gen(L3?) being empty GUFI stops.

8 Experimentations  To assess GUFI performance, we have implemented Apriori [1], FP-Growth [7] and GUFI in C++. Exper- imentations was performed on Windows XP platform in a personnel computer equipped with Pentium IV and a 2 Go of central memory. We generated a synthetic database T10.I2.D80 ? 10 + 10 [4]: T being the average of items number per transaction, I the average size of potentially fre- quent itemsets, D the size of the database in thousands of transactions and?x+x the size of the decrement and of the increment in thousands transactions. We use this database in the following experimentations.

8.1 Comparison between Apriori, FP- Growth and GUFI  In this experimentation, we mined from DB the set L under the support threshold 0.06%. Then, we performed maintenance operation over L for an interval of support thresholds going from 0.04% to 0.4% by Apriori, FP- Growth and GUFI. The two first algorithms perform a sim- ple mining operation over the updated database DB? for support thresholds cited above. On the other hand, GUFI  takes old frequent itemsets into account to maintain L. Per- formance results are showed in figure 4 and illustrate GUFI advantage thanks to reduction of candidate itemsets set, spe- cially the set PWk which lightens the scan of DB?.

Figure 4. Comparison between Apriori, FP- Growth and GUFI  8.2 Effect of initial mining operation  In this experimentation, we did two initial mining opera- tions from original database DB. The first mining operation was performed under the support threshold 0.06% to obtain the set L1 3 of frequent itemsets in DB, the second mining operation was performed under the support threshold 1% to obtain the set L2 of frequent itemsets in DB. Then, we maintained the set L1 and the set L2 for the same updating operations (insertion of db+ and deletion of db? from DB) using our algorithm GUFI.

We note that maintenance operation realized by GUFI using L1 is more efficient than maintenance operation real- ized by GUFI using L2 (see figure 5). That was expected since more the support threshold is small, more the set L is big. More L is big, more PPk is big (PPk = Lk ?Ck) and more PWk is small since the two sets are complementary in Ck (PWk = Ck \ PPk). More PWk is small, more the scan of DB? is quick, which makes GUFI more efficient.

9 Conclusion  In this work, we tried to resolve exhaustively the prob- lem of incremental maintenance of frequent itemsets. The algorithm GUFI maintains a set of frequent itemsets after data updating under any new support threshold fixed by the data analyst. Three scenarios are treated depending on al- gorithm parameters. GUFI is especially efficient in the first and second scenarios. Our perspective is to define an ARM  3L1 is the name of the whole of frequent itemsets set for different sizes, to not confuse with L1 which is a subset of L and the set of frequent itemsets of size one.

Figure 5. Effect of initial mining operation  strategy to avoid as long as possible the third scenario. For example, experimentations leaded on effect of initial min- ing operation showed that more initial support threshold is small, more maintenance operation is efficient. Authors of FUP algorithm [3] has extended their work to study when FUP is more efficient in [13]. A similar study for GUFI will be so interesting.

GUFI may be also very interesting for mining associa- tion rules in inductive databases ([10] and [6]). Indeed, in this kind of databases, association rule mining is performed via user inductive query. Often, the data analyst (end-user) formulates several inductive queries in the same session.

So, it will be possible to deduct a query result from another one previously performed if we assume that queries results are stored.

We also note that generalized association rules mainte- nance ([16] and [18]) is an interesting problem especially as defined taxonomy may be updated by the user. Up- dating items taxonomy invalidates generalized association rules hence the importance of maintenance [8].

