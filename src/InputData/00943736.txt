An Efficient Clustering Algorithm for Mining  Fuzzy Quantitative Association Rules

Abstract Mining association rules on categorical data has been discussed widely of late years. It is a relatively difficult problem in discovery of association rules from numerical data, since the reasonable intervals for unknown numerical attributes or quantitative data may not be discriminated easily. In this paper, we propose an efficient hierarchical clustering algorithm based on variation of density to solve the problem of interval partition. We define two main characteristics of clustering numerical data: relative inter-connectivity and relative closeness. By giving a proper parameter, a, to determine the importance between relative closeness and relative inter-connectivity, the proposed approach can generate a reasonable interval automatically for the user. The experimental results show that the proposed clustering algorithm can behave a good performance on both of clustering results and speed.

1. Introduction Data mining is a key step on the processes of knowledge discovery. One of the most important topics of data mining is to find association rules from a transaction database efficiently [ 11. The mining of itemized association rules, also referred to be Boolean association rules, has been studied extensively of late years. A general association rule can be view as being defined on attributes of a relation, and has the expression of form X s Y ,  where X and Y are conjunctions of a set of conditions C,. The condition C, for a categorical attribute is A, = v,, where v, is the item value of the attribute A,. An association rule holds when the rule satisfies user-specified minimum support and confidence. The support of an association rule X 5 Y  is defined as the ratio of the number of tuples satisfying the condition XAY and the total number of tupes in the database. The confidence is the ratio of the number of tuples satisfying XAY and the number of tuples satisfying X .  While an attribute is numerical  type, the condition C, for a quantitative attribute is denoted as v, E [f,k, u,J, where [Ck, u,k] is an interval of numerical attribute A, and v, is a value in the domain of attribute A,. The mining process in quantitative attributes is more difficult than in categorical attributes relatively, because intervals involved in quantitative association rules may not be concise and meaningful enough for users. In order to discover association rules with quantitative attributes, an efficient method to find meaningful intervals is necessary.

Many researchers propose different approaches to solve the problem of interval partition. Srikant er al.

[ 151 use equi-depth partitioning to mine quantitative rules, which separates intervals by their relative ordering and quantities equally. However, they do not consider the relative distance between values and density of an interval. Miller and Yang [I31 apply Birch clustering [ 181 to identify intervals and proposed a distance-based association rule to improve the semantics of intervals. At the same time, ARCS [12] proposed by Lent er al. present a geometric-based algorithm, BitOP, for performing the clustering in numerical attributes; They show that clustering is a possible solution to figure out the meaningful regions and support the discovery of association rules.

Although clustering provides a useful technique to discriminate dense space, it is not feasible for all clustering algorithms. Cheng et al. [4] suppose that most of the clustering algorithms, such as DBSCAN [SI, BIRCH [18], CURE [6],  and STING [16], do not satisfy the requirement of identifjing clusters embedded in subspace of multi-attributes. Thus, they develop an entropy-based subspace clustering method called ENCLUS to handle the mining of subspace in numerical attributes.

Another possible solution is fuzzy set theory. In contrast with quantitative clustering, fuzzy linguistic- based approaches focus on the qualitative filtering.

Yager [ 181 introduces hzzy linguistic summaries to  0-7803-7078-3/0v$10.00 (C)u)ol IEEE. Page: 1306    provide summarization on different attributes. Hirota and Pedrycz [7], [7], [I41 propose a context-sensitive fuzzy clustering method based on Fuzzy C-means to construct rule-based models. However, the context- sensitive FCM method cannot deal with the data consisting of both numerical and categorical attributes.

For solving the qualitative knowledge discovery, Au and Chan [2] employ fuzzy linguistic terms to relational databases with numerical and categorical attributes, then proposed the F-APACS method [3] to discovery fuzzy association rules. Hong et al. [9] give definitions of the support and confidence for fuzzy membership grade and design a data mining approach based on fuzzy sets to find association rules with linguistic terms of human knowledge.

However, the specified fuzzy linguistic terms in fuzzy association rules can be given only when we have understood the properties of attributes. In real life, contents of attributes may be unknown and meaningful intervals are not concise and crisp enough usually. In this paper, we propose an automatic clustering method for discovering hzzy quantitative association rules.

The proposed clustering method is based on the variation of densities. For two neighbor clusters CI and C,, we merge them into a single cluster C? if the density of C? is close to the densities of original cluster C, and C2 A discriminate hnction and a hierarchical algorithm are developed to process all clusters recursively. At last, we use a determined function to get the best clusters. For one dimensional quantitative attribute, a cluster indicates an interval. The membership grade of fuzzy interval then can be obtained by the approach of fuzzy C-mean and fuzzy quantitative association rules can be generated. We also show an experimental result with about 8,000 numerical data. The clustering results is close to the visual partitions of human and behave an efficient performance in speed.

This paper is organized as follows. In Section 2, the definitions of fuzzy quantitative association rules are given. Then, we introduce the idea and have a formal description for the proposed clustering method in Section 3. The experimental results of the proposed clustering method with different parameters are shown in Section 4. Finally, Section 5 concludes a summary and gives some directions of future research.

2. Fuzzy Quantitative Association Rules We first give formal definitions for fuzzy quantitative association rules. Let R = (A, ,  A*, ... , A,} be a set of  attributes in a relational database, where IR( = m is the number of attributes in R. Let t [ R ]  stand for the tuples in R and lt[R]I = n be the number of tuples in the database R. Assume that A ,  is an attribute with quantitative domain D~,, I <  i 5 m. Let 12, = [ I l k ,  u,kl be the kth interval of the attribute A,,  where Ilk E DA,, u,k E DAr, and /,k I q. An item value of attribute A, is denoted as t[A,]. Let C,, denote the condition t[A,]e I:, , C, and Cy be the conjunctions of conditions CA,, I <  i I m. The sets ofXand Y represent the sets of attributes in the conjunctions of conditions CX and Cy, respectively. The number of tuples in the database satisfying the conditions C, and C y  are respectively denoted as IC, 1 and (Cy I. We define a quantitative association rule over a set of crisp intervals as follows:  Definition 1 [ 151 A quantitative association rule is an implication of the form C x a  Cy, where, XcR, Y c R , a n d X n  Y =  0.

A rule C, 3 Cy holds with confidence c and support s if  I C x ~ C y (  / lCxf 2 c, and lCx~CyI  / n 2 s.

In Definition 1, the conditions in association rules are based on crisp interval data. For unknown numerical values, intervals may not be clear enough in general.

Therefore, we define the concept of fuzzy intervals as follows. Assume that a set of fuzzy intervals sets denoted as (A , , ,  AI2, . . ., AIK} is given for the attribute A,. The membership function is pA#k for the kth fuzzy interval in attribute A,, such that  pA,k: DA, * [0,1].

The fuzzy interval AIk, k = 1,2, ..., K are defined as  If pA~~k(t[A,])=l, the item r[A,] belongs to the fuzzy interval AIk certainly. If ,&t(f[A,])=O, it is no doubt that the item r[A,] are not in the fuzzy interval A,k. Let I, and Iy be the conjunctions of fuzzy intervals in the attributes sets of X and Y. Based on hzzy  interval, we define fuzzy quantitative association rules over a set of fuzzy intervals as follows:  Definition 2 A hzzy quantitative association rule is an implication of form 1,- ly, where, X c R ,  Y c R ,  and X n Y = 0 .

A rule I, 3 Iy holds with confidence c and supports if  0-7803-7078-3/0U$l0~00 (C)U)ol IEEE. Page: 1307    3. The Clustering Method In the section, we present the proposed automatic clustering method formally. The symbols used in the algorithm are defined in Section 3.1. The clustering algorithm is described in Section 3.2. We also give a membership grade generating algorithm based on the clustering results and fuzzy C-mean to generate the membership functions of clusters rapidly.

3.1 Notations Since we only consider the algorithm on a single attribute here, each interval on an attribute can be viewed as a cluster. The symbols used in our proposed method are defined in the following.

n : the total number of tuples in original database.

r : the initial number of intervals in an attribute.

N, : the number of intervals.

C, : the ith interval of .an attribute.

h, : the number of data contained in the ith intervals.

1, : the length of the ith intervals.

: the centroid of ith interval.

4 :  the difference between the density of interval C, and the density of the interval which is merged by two neighbor intervals C, and C,+I.

a, : the distance of the centroids in interval C, moved after the merging of two neighbor intervals C, and C,+ I .

RI : (Relative Inter-Connectivity)  The degree of relative inter-connectivity between two neighbor intervals C, and Cf+l.

RC: (Relative Closeness) The degree of relative closeness between two neighbor intervals C, and Clcl.

( 5 )  E l :  the cost function for evaluating if C, and C,+I should be merged. a > 0.

L  E, = RC, x RI,= (6)  di :the set of all N," intervals after k merging processes.

P :  ? 7 :  3.2  the connectivity variance of d, the h: is the number of data in interval C, after k merging processes.

(7)  the closeness variance of @, the hr is the number of data in initial interval C,.

The Algorithm After the definitions of all notations, we make a preprocess in the first step for the original numerical data in the algorithm. The preprocessing is to quantize the original data using an acceptable minimum interval.

The minimum interval can be set by users. Once upon the minimum interval was determined, we can partition the domain of the attribute into r intervals initially.

Then, we compute the cost hnctions for each two neighbor intervals and merge the intervals with minimum E,. Of course, the number of intervals will be decreasing and the length for merged intervals will be increasing after each merging process. Next, we just  0-7803-7078-3/0V$10.00 (C)U)ol IEEE. Page: 1308    only re-compute the cost function of the merged intervals in the previous merging process and merge the merge the intervals with minimum E, again. Such merging processes will be repeated until only one interval remains. When the merging processing stops, we find the value k with min{ p + $). The set of d is the best partition. The detailed algorithm is shown in the following.

Input: one dimension data set t [ A ] ,  rand a.

Output: the interval set d Step 1: Quantize original data set t [ A ] ,  and compute  h,. Nck=r, k 0 ,  b" ={CO, CI, C2, . . ., C,,,'-I} Step 2: Compute E,, Os i 5 NCk - I .

Step 3: Compute and vk.

Step 4: Find C, and C,+I with min(E,lO I I <  NCk-1},  merge C, and Cl+,, Nlh"=N,k- 1 .

Step 5: Re-compute E,-l and &.re-numbering uk to be  @ * I .

Step 6: If Nlk" = 1, goto Step 7.

Otherwise, k=k+l, goto Step 3 .

Step 7: Find m i n ( e + $ > ,  output uk.

The parameters, c a n d  4, are used to determine the best interval partitions. The parameter f is used to estimate the inter-connectivity among intervals. The parameter qk is used to estimate the closeness within a interval. While the summation of the two parameters has the smallest value, we claim that there is the best interval partition at this time. Users usually need to give the number of partitions in the traditional cluster method. It is difficult for users to assign the number of clusters when the characteristics of data are unknown.

In our method, the best number of clusters can be determined automatically.

3.3 Membership Functions Generating The algorithm proposed in Section 3.1 and 3.2 also supports improvement of rapid membership fimction generating. Here, we propose a efficient algorithm for membership function generation based on fuzzy C- mean. Instead of using the initial random data to be the centroids of the clusters, we use the centroids obtained from our proposed algorithm. The procedure of membership function generating is described as follows:  Step I : Initializing the C points by y~,, the centroids of the intervals in set uk.

Step 2: Compute p,(q"), the membership grades of Go belonging to C,k, 0 -< j < r- 1, 0 5 i < N,.k- 1 .

Step 3 :  Compute Cost, if Cosr is smaller than a user- specified value E, then the algorithm halts, otherwise, goes to Step 4.

Step 4: Compute y ~ ,  and go to Step 2 .

We have successfully partitioned a numerical attribute into the best intervals and produced their membership values. By means of the discovered intervals and the fuzzy membership generating function, we can depict the fuzzy intervals as defined in Section 2. The fuzzy quantitative association rules in a database thus are able to be discovered through the given support s and confidence c. The detailed mining algorithm is similar to the traditional itemized data mining except the definitions of support and confidence. Of course, there exists another improvement to find the rules, but it is out of the issue we discussed in this paper.

4. Experimental Results We implement the proposed algorithm and make some experiments. Only one data set is shown here due to the limitation of paper length. The number of data is about 8,000. The domain of each data is an integer between 0 and 64000. We first quantize all data into r initial intervals. The different r had done by setting to be 128, 256, 512 and 1024. Here, only the results of r = 256 are presented. The histogram of original data after quantization is shown in Figure 1.  The different a values produce distinct clustering results, as shown in Figure 2 to Figure 5 .  For our visual judgement, there is the best result when a = 2.5. But, for the data set with different distribution, the value of a will not have the same value for obtaining the best result.

5. Conclusions The data mining process from numerical attributes is more difficult for analysts than itemized data in general.

0-7803-7078-3/0U$l0.00 (C)2001 IEEE Page: 1309    The main obstacle is the lack of meaningful property in data and the vagueness of intervals. The contribution of this paper is to proposed an automatic interval partition method and an efficient fuzzy membership function generator. By the proposed method, we can find the fizzy quantitative association rules effectively.

Our future work is to extended the proposed method to the clustering of multi-dimensional features. Also, we try to find a more efficient mining algorithm for fuzzy quantitative association rules .

