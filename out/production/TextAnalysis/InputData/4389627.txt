Fuzzy Rule Base Generation through Genetic Algorithms and Bayesian Classifiers - a Comparative Approach

Abstract  The definition of the Fuzzy Rule Base is one of the most important and difficult tasks when designing Fuzzy Sys- tems. This paper discusses the results of two different hy- brid methods investigated earlier, for the automatic gener- ation of fuzzy rules from numerical data. One of the meth- ods proposes the creation of Fuzzy Rule Bases using ge- netic algorithms in association with a heuristic for prese- lecting candidate rules. The other, named BayesFuzzy, in- duces a Bayes Classifier using a dataset previously gran- ulated by fuzzy partitions and then translates this Classi- fier into a Fuzzy Rule Base. A comparative analysis be- tween both approaches focusing on their main character- istics, strengths/weaknesses and easiness of use is carried out. The reliability of both methods is also compared by analyzing their results in a few knowledge domains.

1. Introduction  Systems based on Fuzzy Logic, generally called Fuzzy Systems (FS), have been successfully used for the solution of problems in many areas, including pattern classification, optimization and control of processes [9, 24].

The Fuzzy Systems of interest in this work are those known as Rule Based Fuzzy Systems (RBFS). A RBFS has two main components: a Knowledge Base (KB) and an In- ference Machine (IM). The KB comprises the Fuzzy Rule Base (FRB), i.e., a set of fuzzy rules that represents a given problem, and the Fuzzy Data Base (FDB), which contains the definitions of the fuzzy sets related to the linguistic vari- ables used in the FRB. The IM is responsible for the appli- cation of a reasoning process that uses inferences to derive the output or conclusion of the system, based on both the rules and the known facts.

Various approaches have been used for the automatic generation of the KB from numerical data, representing samples (or examples) of a problem. Clustering algorithms  [21], neural networks [20] and Genetic Algorithms (GA) [7] are among the most well-succeeded techniques. Recently there has been a considerable research effort focusing on the use of GA [11] in the design of FS. This initiative coined the term Genetic Fuzzy System (GFS), which are, basically, FS with a learning process based on GA [7].

A very promising approach is the use of GA to gen- erate FRBs using previously defined and fixed fuzzy sets [12, 15, 17]. This approach was adopted by Castro & Ca- margo who proposed a method consisting of three stages: an attribute selection process, a GA to induce the rules and, in sequence, another GA to eliminate unnecessary rules [3].

However, depending on the number of variables and sets in the defined partition, the total number of possible rules can be extremely large, making it difficult to generate and cod- ify the chromosomes and, consequently, the whole genetic learning process becomes overloaded.

As an alternative approach to deal with the dimensional- ity problem, Cintra & Camargo proposed a genetic genera- tion of FRBs from a set of candidate rules preselected by a heuristic criteria based on the Degree of Coverage (DoC), as described in [4, 5], and used in part of the experiments described in this paper.

A totally different proposal for the automatic genera- tion of FRBs from data can be found in [16], where the combination of the fuzzy granulation of datasets with a Bayesian Classifier (BC) learning process is investigated.

The method proposed, named BayesFuzzy (BF), aims at im- proving the comprehensibility of an induced BC by translat- ing it into a FRB.

Besides presenting a description of the main characteris- tics of both, the method based on GA with preselection of candidate rules as well as the BayesFuzzy method, the ob- jective of this paper is to compare the results of both meth- ods in a few knowledge domains.

The remaining of the paper is organized as follows. In Section 2 the fundamental concepts of Fuzzy Classification Systems and Bayesian Networks and Classifiers are pre- sented. Section 3 describes the genetic generation of FRBs   DOI 10.1109/ISDA.2007.30    DOI 10.1109/ISDA.2007.30     and Section 4 describes the BayesFuzzy method. Section 5 discusses the experiments and comparisons concerning both approaches. Finally, the conclusions and perspectives are presented in Section 6.

2. Fundamental Concepts  The goal of this section is to highlight the main concepts of fuzzy classification systems and Bayesian Networks and Classifiers, in order to provide the necessary technical back- ground for the following sections.

2.1. Fuzzy Classification Systems  Classification is an important task employed in many dif- ferent areas, such as pattern recognition, decision making and data mining. A classification task can be roughly de- scribed as: Given a set of objects E = {e1, e2, ..., eM}, also named patterns, which are described by n attributes, assign a class Cj from a set of classes C = {C1, C2, ..., CJ} to an object ep , ep = (ap1, ap2, ..., apn).

Fuzzy Classification Systems (FCS) are RBFS designed to perform a classification task, that requires the attribute domains to be granulated by means of fuzzy partitions. The linguistic variables in the antecedent part of the rules repre- sent attributes and the consequent part represents a class. A typical classification fuzzy rule can be expressed by:  Rk : IFX1is A1l1AND...ANDXnis AnlnTHEN Class = Cj (1)  in which Rk is the rule identifier, X1, ..., Xn are the at- tributes of the pattern considered in the problem (repre- sented by linguistic variables), A1l1 , ..., Anln are the lin- guistic values used to represent the values of these attributes and Cj is the class, fuzzy or not, the pattern belongs to.

An inference mechanism applies the FRB to the pattern to be classified, determining the class it belongs to. Most of the FCSs use the Classic Fuzzy Reasoning Method (CFRM) [12], that classifies a pattern using the rule that has the high- est compatibility degree with the pattern, as described next.

Let ep = (ap1, ap2, ..., apn) be a pattern to be classified and {R1, R2, ..., RS} the set of S rules of a classification system, each with n antecedents. Let Aili(api), i = 1, ..., n, be the membership degree of attribute value api in the i-th fuzzy set of fuzzy rule Rk as defined in (1). The CFRM applies the following steps to classify the pattern ep:  1. Calculates the compatibility degree between pattern ep and each rule Rk, for k = 1, ..., S.

Compat(Rk, ep) = t(A1l1(ap1), A2l2(ap2), ..., Anln(apn))  in which t denotes a t-norm.

2. Finds the rule Rkmax with the highest compatibility degree with the pattern:  max{Compat(Rk, ep)}, k = 1, 2, ..., S  3. Assigns class Cj to pattern ep , where Cj is the class predicted by rule Rkmax found in the previous step.

2.2. Bayesian Networks and Classifiers  A Bayesian Network (BN) [23] G has a directed acyclic graph (DAG) structure. Each node in the graph corresponds to a discrete random variable in the domain. In the BN graph, an edge Y ? X describes a parent child relation, where Y is the parent and X is the child. All parents of X constitute the parent set of X , denoted by ?(X). Each node of the BN structure is associated to a Conditional Probabil- ity Table (CPT) specifying the probability of each possible state of the node, given each possible combination of states of its parents. If a node has no parents, its CPT gives the marginal probabilities of the variable it represents.

When learning BNs from data, the BN variables repre- sent the dataset attributes. When using algorithms based on heuristic search, the initial order of the dataset attributes can be an important issue. Some algorithms depend on the vari- able ordering to determine the direction of an edge, since an attribute can only be a possible parent of those that follow it, in the attribute ordered list.

While a BN encodes a joint probability distribution over a set of random variables, a BC aims at correctly predicting the value of a designated discrete class variable, given a vec- tor of attributes (predictors). Methods for inducing BNs can be used to induce BCs as well. The BC learning algorithm used in BayesFuzzy is based on the BN learning algorithm known as K2 [10, 16].

3. The Genetic Generation of Fuzzy Rules  When Genetic Fuzzy Systems are concerned specifi- cally with the generation or optimization of RBFS, they are named Rule Based Genetic Fuzzy Systems (RBGFS). In the context of the RBGFS, the well-known methods that com- bine the genetic and fuzzy approaches for the generation of KBs can be divided into two main groups: methods that ad- just KB components (Genetic Adaptation) and methods that build KB components (Genetic Construction).

Included in the group of Genetic Adaptation are the methods that initiate the process with an existing FRB or FDB and use GA to improve the performance of the sys- tem by adjusting or adapting one or more parts of the KB [1, 2, 13, 14, 18]. The group called here Genetic Construc- tion includes the methods that use GA to effectively build     or design one or more components of the KB and com- prises the approaches that produced the largest number of researches [6, 15, 17].

The proposal described in this section, which has been previously introduced in [4, 5] can be included in the groups of Genetic Construction with an additional preprocessing step. The method uses fuzzy partitions defined before the whole learning process, and criteria based on heuris- tic knowledge for the preselection of candidate rules to be considered by GA that build the final FRB. The selection of candidate rules is performed here as a means of reducing the search space and simplifying the chromosome codification.

A similar approach can be found in [19] where GA are used to select the rules that will form the FRB from a set of candidate rules generated from numerical datasets using rule evaluation measurement from data mining (confidence and support).

In the automatic generation of FRB using GA, the search space is defined by combinations of a certain number of rules from all the possible rules, considering the variables of the problem and the defined fuzzy sets.

As the number of variables increases, the set of possible rule combinations that will form the KB increases exponen- tially, interfering in the result of the learning process, and, in some situations, even making it unfeasible.

The proposed heuristic knowledge is associated to the DoC of the rules. Although the DoC alone is not a selection parameter that tells which rules should be part of the FRB, it allows the discard of a large number of possible rules, without any quality loss for the generated FRB. The rules to be discarded are the ones with low or null DoC value. The calculation of the DoC is presented next.

Let E = {e1, e2, ..., eM} be a set of examples. The DoC of rule R with relation to E (DoCR) is defined as:  DoCR = M?  i=1  {DoC(R,ei)}  so that DoC(R,ei)is the DoC of rule R with respect to exam- ple ei, obtained through the aggregation of the membership degrees of the attribute values of ei in the corresponding fuzzy sets appearing in the antecedent part of R.

In the method described here, once the fuzzy partitions of the attribute domains are defined, the DoC value is cal- culated for all possible rules which are then ordered by the decreasing value of DoC. This step allows the use of very simple criteria to select candidate rules as well as a simple representation of rules in the chromosomes.

The method proposed by Wang & Mendel (WM) [25, 26] was used as a reference for the definition of several param- eters in the proposed method. The two criteria used to pre- select the candidate rules were:  1. Consider the rules from the ordered set until all the  rules present in the FRB generated by the WM method have been chosen;  2. Consider the rules from the ordered set including all the rules with non-null DoC.

The two criteria originated two different versions of the method concerning the preselection phase. In both cases, the set of candidate rules is then used as a reduced search space for the generation of the FRB using GA.

The preselection of candidate rules and their ordering al- low the identification of each rule by its position in the or- dered list and induce a simple binary codification. The size of each chromosome was set as the total number of prese- lected rules with a direct correspondence between the rule position in the ordered list and the gene position in the chro- mosome, so that 0 represents an inactive rule, and 1 repre- sents an active rule. Figure 1 presents a chromosome with 10 positions, represented in the binary system, with rules 1, 4, 5, 6 and 9 active and all the others inactive.

Figure 1. Binary chromosome representing a complete Rule Base  In the initial population, each chromosome was created with a percentage of active rules based on the number of rules generated by the WM method. The chromosomes were randomly generated and conflicting rules eliminated.

To avoid the occurrence of redundancies and to improve the clearness and understanding of the FRB, the fitness value was defined based on the correct values of the Cor- rect Classification Rates (CCR) as well as on the number of rules in the base using a self-adaptive algorithm presented in [4]. For each chromosome the fitness function evaluates its CCR and then divides this value by a penalty constant which is based on the relation between the number of ac- tive rules in the chromosome and a reference value found as the best (smallest) number of rules (Best NR) for the best (highest) CCR found so far. The constant values used for the penalization, shown in Table 1, were defined empirically.

The number of rules and the CCR of the FRB generated by the WM method were used as initial values for the ref- erence parameters Best NR and Best CCR respectively.

For each generation the two parameters were automatically updated.

4. The BayesFuzzy Method  The BayesFuzzy method proposed in [16] allows trans- lating the knowledge represented by a Bayesian Classifier     Table 1. Penalization rates for the fitness of the chromosomes according to their number of active rules  Number of Rules Fitness Value ? Best NR CCR ? Best NR ? 1.5 CCR/1.25 ? Best NR ? 2 CCR/1.5 ? Best NR ? 3 CCR/2 > Best NR ? 3 CCR/3  (BC) into a Fuzzy Rule Base. Considering D a domain dataset described by N attributes, the method applies a fuzzyfication process in D producing a fuzzyfied dataset D?. BayesFuzzy then induces a BC from D? and extracts from the BC the classification rules that can be used as the FRB of a fuzzy system.

The rule extraction process is guided by the MAP (max- imum a posteriori) approach. Thus, based on a BC, one rule is created for each possible value of the classifier variables and the class identification is done following the most prob- able state of the class attribute. This is a computationally expensive procedure mainly because it is very common the presence of hundreds or thousands of variables in proba- bilistic models [8]. In most cases, however, many variables may only be relevant for some types of reasoning; very rarely all of them will be relevant in the reasoning process associated to one single query. Therefore, focusing only on the relevant part of a BC is fundamental when translating it into a set of rules. In this sense, BayesFuzzy explores the Markov Blanket concept to select the attributes to be used in the antecedent part of the rules. Thus, the number and the complexity of rules are minimized along with the rule ex- traction process. The attribute selection strategy, however, does not guarantee a minimal rule set. Therefore, a pruning step may be conducted after the rule set generation.

Figure 2 presents the BC Rule Extraction procedure [16] used by BayesFuzzy to extract rules from a given BC.

Steps 1 to 5 described in Figure 2 are the initialization steps. In step 6 the rules are extracted from the BC and in- serted into the FRB (called RSR in the algorithm) using the MAP approach; this step is based on the intuition that the best explanation for a piece of evidence is the most prob- able state of the world, given the evidence. Along these lines, each rule corresponds to a specific instantiation of each variable (antecedents) and the most probable inferred class (consequent). Step 7 consists of removing from the set of rules those containing superfluous conditions; this can be seen as a naive pruning step.

An interesting issue about the BayesFuzzy is that the BC structure provides a simple and efficient mechanism (Markov Blanket) to reduce the number and the complex-  Figure 2. BC Rule Extraction procedure  ity of the rule set. Another interesting characteristic is that the BC built by BayesFuzzy can be used for predicting the value of any variable (i.e., each variable can be seen as a class variable). This allows a reduction in the time needed to build models when more than one variable can play the role of class variable.

5. Experiments and Analysis of Results  In this section the knowledge domains and obtained re- sults are presented and discussed. All experiments with GA were performed with 250 iterations, elitism rate of 5%, crossing rate of 70% and mutation rate of 5%. For the BF method, the experiments were performed using a 10-fold cross validation strategy. The experiments with GA and WM were performed with a 5-fold cross validation strategy due to time restrictions.

The 4 domains used are available at the UCI Machine Learning repository [22]. The choice of each dataset was based on their attribute type (numerical-valued ones). Only 4 randomly selected attributes for each domain were used in the process. Table 2 summarizes the domain characteristics     giving the total number of instances as well as the number of attributes.

Table 2. Domain characteristics Domain # Instances # Attributes (class included) Diabetes 724 5 MPeG 392 5  Iris 150 5 Machine 209 5  For each domain 3 distinct partitions were defined, with 3, 5 and 7 fuzzy sets for each input attribute, totalizing 12 different experiment setups. For each of the 12 setups, 4 dis- tinct approaches were evaluated: the BF method, the WM method, GA with preselection of the best rules (GA I) (se- lecting all rules in the RB generated by the WM algorithm), and GA with preselection of rules with non-null DoC (GA II).

Table 3 shows the total number of possible rules for each created partition (column Total) and the number of rules in the FRBs produced by each of the 4 approaches. The nu- merical suffix added to each domain name represents the number of fuzzy sets for each input variable.

Table 4 presents the CCRs for the FRBs generated in each experiment.

The results show that GA I produced FRB with higher CCRs for almost all experiment setups. The results with preselection of the best rules are also generated with a smaller number of iterations necessary for the GA to con- verge to an optimum result, reducing the processing time.

This reduction is possible due to the reduction of the active rules in each chromosome.

The preselection of candidate rules has proved to be a promising approach to reduce the search space and thus speed up the learning process as a whole. The possible re- finements of the method of preselection of candidate rules used here can be explored as an efficient tool to cope with the dimensionality problem.

As the rule set reduction done by BayesFuzzy is based on the identification of the most relevant attributes for the classification task, the Bayesian and fuzzy techniques col- laboration, proposed in this method, are suitable mainly in applications having irrelevant attributes. In such domains, the Markov Blanket attribute selection principle, embedded in BayesFuzzy, enables the reduction of the number of rules as well as their complexity (number of antecedents in each rule). Considering also that BayesFuzzy uses a Bayesian classification approach to perform the class prediction, do- mains in which traditional BCs perform well also favor the BayesFuzzy classification rates.

Table 3. Number of rules Domain Total GA I GA II BF WM  Diabetes 3 162.0 14.2 28.4 8.0 24.0 Diabetes 5 1,250.0 51 67.4 16.3 84.4 Diabetes 7 4,802.0 38.6 75.4 37.1 159.2 MPeG 3 243.0 13.0 16.4 4.6 20.0 MPeG 5 3,125.0 36.8 46.0 13.6 46.2 MPeG 7 16,807.0 34.4 51.2 5.2 76.6  Iris 3 243.0 7.6 13.8 7.7 15.0 Iris 5 1,875.0 15.0 41.2 21.4 44.8 Iris 7 7,203.0 54.4 61.4 42.0 67.2  Machine 3 243.0 6.2 12.8 9.5 13.8 Machine 5 3,125.0 21.6 25.4 50.5 29.0 Machine 7 16,807.0 26.0 27.8 109.04 33.0  Table 4. Correct Classification Rates Domain GA I GA II BF WM  Diabetes 3 100.0 99.77 71.1 91.1 Diabetes 5 99.97 99.39 74.3 89.0 Diabetes 7 92.9 94.9 76.5 86.9 Average 97.62 97.53 73.9 89.0 MPeG 3 87.51 86.32 83.9 79.3 MPeG 5 78.69 74.12 63.7 76.8 MPeG 7 64.8 51.81 50.9 62.3 Average 77.0 70.15 66.1 72.8  Iris 3 99.71 98.7 95.7 100.0 Iris 5 100.0 100.0 94.7 100.0 Iris 7 98.31 97.3 94.7 94.7  Average 99.34 98.67 95 98.23 Machine 3 94.2 95.1 93.2 93.7 Machine 5 94.3 92.0 93.2 95.6 Machine 7 91.4 86.9 86.4 92.8 Average 93.3 91.33 90.9 94.03  6. Conclusion  This work has presented a comparative analysis of two different approaches for the automatic generation of Fuzzy Rule Bases from datasets, namely a genetic-based approach and a Bayes-based approach.

The main idea of the genetic approach is the use of very simple knowledge about the dataset to reduce the number of possible rules and thus reduce the search space. The reduc- tion is accomplished in the initial phase by the selection of candidate rules; potential rules that cannot cooperate in the classification process are discarded before the genetic pro- cess begins. Two different criteria were used to direct the choice of the rules; experiments have showed that the crite- rion based on WM were more successful as far as the CCR values and the number of rules are concerned.

In the Bayes approach, a Bayesian Classifier is used to generate rules from a dataset that is granulated, meaning that the attribute domains are organized in categories rep- resented by fuzzy partitions. In the sequel, the classifier is translated into a Fuzzy Rule Base.

It is important to emphasize that both GA approaches are dependent on the number of variables of a problem, as well as on the initial fuzzy partition of each variable do- main. Particularly, the influence of the initial partition on the results can be observed in Table 2 and Table 3.

A domain characteristic that favors the use of BayesFuzzy is the presence of more than one class attribute.

It can be illustrated considering a domain D having M class attributes. In such a domain, BayesFuzzy will build a single BC to extract M classification Rule Sets. Thus, the com- putational effort to induce the BC is not proportional to the number of class attributes. For this reason as future work it is intended to explore data domains which allow a better understanding of this characteristic.

The paper discusses the very relevant topic of fuzzy rules generation, focusing on two hybrid approaches, since it has become clear that the automatic generation of Fuzzy KBs can substantially profit from using learning techniques based on different methodologies.

7. References  [1] P. Bonissone, P. S. Khedkar, and Y. T. Chen. Genetic algo- rithms for automated tuning of fuzzy controllers, a trans- Fuzzy Systems, FUZZ-IEEE:674?680, 1996.

[2] J. Casillas, O. Cordo?n, M. Jesus, and F. Herrera. Genetic tuning of fuzzy rule deep structures preserving interpretabil- ity and its interaction with fuzzy rule set reduction. IEEE Transactions on Fuzzy Systems, 13:13?29, 2005.

[3] P. A. D. Castro and H. A. Camargo. Focusing on inter- pretability and accuracy of genetic fuzzy systems. IEEE In- ternational Conference on Fuzzy Systems, pages 696?701, 2005.

[4] M. E. Cintra and H. A. Camargo. Fuzzy rules generation using genetic algorithms with self-adaptive selection. (sub- mitted for publication), 2007.

[5] M. E. Cintra and H. A. Camargo. Fuzzy rules generation with preselection of candidate rules. (submitted for publica- tion; in Portuguese), 2007.

[6] O. Cordo?n, F. Herrera, and P. Villar. Generating the knowl- edge base of a fuzzy-based system by the genetic learning 9(4):667?674, 2001.

[7] O. Cordo?n, F. Herrera, F. Gomide, F. Hoffmann, and L. Mag- dalena. Special issue on genetic fuzzy systems. Fuzzy Sets and Systems, 141, 2004.

[8] M. J. Druzdzel. Qualitative verbal explanations in Bayesian belief networks. Artificial Intelligence and Simulation of Be- havior Quarterly, 94:43?54, 1996.

[9] D. Dumitrescu, B. Lazzerini, and L. Jair. Fuzzy Sets and Their Application to Clustering and Training. CBC Press, 2000.

[10] C. G. and E. Herskovitz. A Bayesian method for the induc- tion of probabilistic networks from data. Machine Learning, 9:309?347, 1992.

[11] D. E. Goldberg. Genetic Algorithms in Search, Optimization and Machine Learning. Addilson-Wesley, 1989.

[12] A. Gonza?lez and R. Pe?rez. Slave: A genetic learning system Systems, 7:176?191, 1999.

[13] H. B. Gurocak. A genetic-algorithm-based method for tun- ing fuzzy logic controllers. Fuzzy Sets and Systems, 108- 1:39?47, 1999.

[14] F. Herrera, M. Lozano, and J. L. Verdegay. Tuning fuzzy logic controllers by genetic algorithms. International Jour- nal of Approximate Reasoning, 12:299?315, 1995.

[15] F. Hoffmann. Combining boosting and evolutionary algo- rithms for learning of fuzzy classification rules. Fuzzy Sets and Systems, 141:47?58, 2004.

[16] E. R. Hruschka-Jr, H. A. Camargo, M. E. Cintra, and M. C.

Nicoletti. BayesFuzzy: using a Bayesian classifier to induce a fuzzy rule base. FUZZ-IEEE, 2007. (accepted for publi- cation).

[17] H. Ishbuchi, T. Nakashima, and T. Murata. Performance evaluation of fuzzy classifier systems for multidimensional pattern classification problems. IEEE Transaction on Fuzzy Systems, Man and Cybernetics, 29(5):601?618, 1999.

[18] H. Ishibuchi, T. Murata, and I. B. Turksen. Single-objective and two-objective genetic algorithms for selecting linguis- tic rules for pattern classification problems. Fuzzy Sets and Systems, 89:134?150, 1997.

[19] H. Ishibuchi and T. Yamamoto. Fuzzy rule selection by multi-objective genetic local search algorithms and rule evaluation measures in data mining. Fuzzy Sets and Systems, 141:59?88, 2004.

[20] S. R. Jang, C. T. Sun, and E. Mizutani. Neuro-Fuzzy and Soft Computing. Prentice Hall, 1997.

[21] T. W. Liao, A. K. Celmins, and R. J. Hammell. A fuzzy C- Means variant for the generation of fuzzy term sets. Fuzzy Sets and Fuzzy Systems, 135:241?257, 1997.

[22] C. J. Merz and P. M. Murphy. UCI repository of machine learning databases, [http://www.ics.uci.edu]. Irvine, CA, UCLA, 1998.

[23] J. Pearl. Probabilistic reasoning in intelligent systems. Mor- gan Kaufmann, 1988.

[24] W. Pedrycz. Fuzzy Modelling: Paradigms and Practice.

Kluwer Academic Press, 1996.

[25] L. Wang. The WM method completed: a flexible fuzzy sys- tem approach to data mining. IEEE Trans. on Fuzzy Systems, 11:768?782, 2003.

[26] L. Wang and J. Mendel. Generating fuzzy rules by learning from examples. IEEE Trans. on SMC, 22:414?1427, 1992.

