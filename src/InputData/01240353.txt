Load Balancing on PC Clusters with the Super-Programming Model *

ABSTRACT  Recent work in high-performance computing has  shifted attention to PC cluster.. For PC-clusters, member  nodes are independent computers connected by general-  purpose networks. The latency of data communications is  long and load balancing among the nodes becomes a  critical issue. We introduce a new model for program  development on PC clusters, namely the Super- Programming Model (SPM) to address this issue. In SPM  PC clusters are modeled as a single virtual machine with  PC as their processing units. The workload is modeled as a  collection of Super-Instructions (SIs). Each SIs can  achieve a limited workload. Application programs are  coded using SIs. SIs are dynamically assigned to available  PC at run time. For limited workload, no SIs overloads any  PC.  Therefore, dynamic load balancing becomes an easier  task.  We apply SPM to mining association rules. Our  experiments show that under normal conditions the  workload is balanced very well. A performance model is  also developed to describe the scalable behavior of SPM.

1. Introduction  High-performance computing has recently shifted its  attention to PC clusters containing commercial off-the-  shelf (COTS) nodes, for cost-effective parallel computing  [1-3].  This trend makes high-performance computing  much less expensive and more accessible. These systems  are suitable for large-scale problems, such as data mining  of very large databases [1].   Each node of a PC cluster is  an independent computer running a general-purpose  operating system. A general-purpose interconnection  network, that most often is an Ethernet-based, connects  these nodes together. Data communication among the PCs  is controlled by application layer software rather than by  lower-level system software or hardware. The latency of  * This work was supported in part by the U.S. Department  of Energy under grant ER63384.

data communications on a PC cluster is usually longer than  on parallel processing systems that contain specialized  hardware for communication networks. Therefore,  programming models developed for the latter are not  suitable for programming PC clusters. For PC clusters, it is  more difficult to exploit low-level or fine-grain parallelism  existing in programs.  It is more appropriate to adopt  coarse- or medium-grain programming. This reduces the  adverse effect of long communication delays.  For PC  clusters, load balancing among member computers  becomes a critical issue for high performance. It tries to  ?appropriately? assign tasks among processing nodes so  that minimize the idle time of the processing nodes while  other nodes are busy. Only if the workload on these  member computers is well balanced, we will be able to  achieve high performance. Otherwise, some computer  nodes will be idle for significant periods of time during the  computation and the overall efficiency of the system will  diminish.

In existing programming models, the way to  decompose applications is normally function-oriented.

Applications are decomposed into function units. To reuse  code in a given application domain area, these units are  implemented as library functions. For example, BLAS  (Basic Linear Algebra Subprograms) has "building block"  routines for performing basic vector and matrix operations  [4]. They are commonly used in the development of high  quality linear algebra software. Each subprogram  completes a single operation, such as matrix-matrix  multiplication, no matter how large the matrices are. In  this fashion, the workload of each ?block? is various, it is  very difficult to balance.

For this reason, we introduce the Super-Programming  Model (SPM) for cluster computing. The workload is modeled as a collection of Super-Instructions (SIs), that  have limited atomic workload. Application programs are  modeled as Super-Programs (SPs) coded with SIs. At run time, SIs are dynamically assigned to available PCs. The  maximum execution time for each SI is well estimated and  adjusted with parameters. SPM makes the workload easier  to balance among the PC nodes.  If the degree of     parallelism in the super-program is much larger than the  number of nodes in the cluster, then nodes have little  chance to be idle. The workload will be balanced very  well.

SPM can be adopted for any parallel application. To  effectively support application portability among different  computing platforms and to also balance the workload in  parallel systems, we suggest that an effective instruction  set architecture (ISA) be developed for each application  domain.  For a particular application domain, it is not  difficult to determine a set of frequently used operations.

These operations then become part of the chosen ISA. SPs  utilize these SIs in the coding process.  Then, as long as an  efficient implementation exists for each of these SIs on  given computing platforms, code portability is guaranteed  and good load balancing becomes more feasible by  focusing on scheduling SIs.  Ideally, the chosen ISA  should be orthogonal, containing as few SIs as possible  that are also adequate to develop any program in the  corresponding application domain with the smallest  possible number of general-purpose instructions. In this  paper, we apply the SPM model to a data mining problem,  in order to prove that it can address the load balancing  problem very well.

2. Relevant Research  2.1 The Mining Association Rules  Mining association rules is a typical data mining  problem. It can be modeled as follows:  Let I = { a1, a2, a3,  ? , am } be a set of items and DB =  T1, T2, T3, ? ,Tn  be a transactions database with items in I. A pattern is a set  of items in I. The term itemset is used interchangeably with the term set of items or pattern. The transaction  represents an itemset that occurs in a database. The  number of items in a pattern is called the length of the pattern. Patterns of length k are sometimes called k-item  patterns. The support s(A) of a pattern A is defined as the  number of transactions in the database DB containing A.

An association rule is an association relationship between  a pair of patterns expressed in the form R: X s, Y, which  means X implies Y, where X and Y are exclusive patterns  ( X  Y =  ) of I. X and Y are called the pre-pattern and  post-pattern of rule R, respectively. s and  are the support and confidence of the rule R, respectively. The support s of  rule R is defined as the support of the pattern obtained by  joining X and Y (s = s(X Y)).  Finally, the confidence  of rule R is defined as s(X Y)/ s(X). Given a transactions database DB, a minimum support threshold smin and a  minimum confidence threshold min, the problem of  finding the complete set of association rules with support  and confidence no less than these support and confidence  thresholds, respectively, is called the association rules  mining problem.  A pattern A is a frequent pattern (or a  frequent set) if A?s support is not less than a predefined minimum support threshold smin. Also, given a transaction  database and a minimum support threshold smin, the  problem of finding the complete set of frequent patterns is  called the frequent patterns mining problem.

Techniques for discovering association rules have  been studied extensively [5-7, 9, 11]. Many approaches  transform the association rules mining problem to the  frequent patterns mining problem. The most popular one is  the Apriori algorithm [6]. Many relevant studies adopt an  Apriori-like approach  [10,13].  The Apriori algorithm is  based on the following property: if any k-length pattern is  not a frequent pattern in the database, then any (k+1)-  length pattern that includes this pattern can never be a  frequent pattern in the database. Using this property, any  verified short frequent patterns can help in screening  longer candidate patterns. The algorithm works as follows:  I? = {x | x  I and x is a frequent item} // Find all frequent items by scanning DB  P1= { 1-length patterns {x} | x  I?}  For (k = 2; Pk-1  ; k++) do begin Ck = apriori_gen(Pk-1)  For any pattern c Ck  {c.count = 0}  For all transactions T DB{  For any pattern c Ck { if (c  T)  c.count++} }  Pk  = {c | c Ck, c.count   smin } End  Answer =  Pk Initially, P1 gets all frequent patterns with a single  item. After that, iteratively the algorithm calls the function  ?apriori_gen? that generates a complete set Ck of k-length  candidate patterns; then, their support is counted by  scanning transactions containing these candidate patterns;  the set Pk of all k-length patterns is generated by pruning  Ck to eliminate infrequent patterns. Once Pk is empty, the  iteration is terminated. The union of variable length  frequent patterns,  Pk, forms the complete set of frequent  patterns in which association rules can be identified. Each  transaction is checked to see if it supports some candidate  patterns. To speed up this checking operation, a hash tree  of candidate patterns is used.

2.2 Parallel Algorithms and Load Balancing  Several efforts have focused on the development of  algorithms for data mining on parallel platforms [7, 8, 11-  14].  Several techniques have been developed for dynamic  load balancing using some form of load estimate [8, 12].

In previous related research, all consideration of load  balancing is based on an estimation of the computation  workload. For example, the workload of the join operation  was estimated in [8] based on the size of equivalence     classes. To count the support of candidate patterns, it was  also suggested to estimate the related workloads. In [11],  static load balancing was embedded in the data partition  algorithm.

Many studies have proved that computing the counts  of candidate patterns is the most computationally  expensive step in the algorithm. The only way to compute  these counts is to scan the entire transactions database.

Most algorithms focus on computing the support of  patterns in parallel [7, 8, 11]. These algorithms can be  classified into two basic types based on what types of data  are partitioned. They are either count distribution (CD)  algorithms or data distribution (DD) algorithms. In a CD  algorithm [2], the entire candidate set is copied into all the nodes. Transaction data are partitioned and each node is  assigned an exclusive partition. The allocation of workload  is controlled by partition of transaction. A DD algorithm partitions the set of candidate patterns for exclusive  assignment to processing nodes [2]. This partitioning is  done in a round-robin fashion. Each node is responsible for  computing the counts for its locally stored subset of the  candidate patterns for all the transactions in DB. The  allocation of workload is controlled by partition of  candidate. But both partition of candidates and partition of  transactions are well workload estimation of the count.

Thus, load balancing based on load estimation cannot  be perfect. To conclude, past approaches to load balancing  for mining association rules in databases did not  demonstrate the versatility of the dynamic load balancing  technique that we propose in this paper. Our results also in  Sections 4 and 5 support our claim.

2.3 Parallel Implementation of Data Mining  Many researchers have implemented relevant  algorithms and evaluated their performance on parallel  machines or supercomputers, such as the IBM SP2 [7],  SGI Power Challenge [8], and Cray T3D [11]. Some  researchers experimented on both parallel machines and  PC cluster [1, 13]; But they applied identical algorithms.

They did not consider adjusting the algorithm for the  chosen computing platform; for example, to reduce the  effect of long delay on PC clusters, they tried to improve  the PC-interconnection network.

3. A Super-Programming Model for Mining  Association Rules  3.1 The Super-Programming Model  In the super-programming model (SPM), the system is  modeled as a single virtual machine with PCs as  processing units. The workload is modeled as a collection  of SIs. Like instructions for processors, SIs are expected to  complete a task with limited workload that can make the  execution time quite predictable. i.e. SPM is workload-  oriented. An example of such an SI is ?compute the  supports for a set of candidate patterns, where the number  of patterns is no more than k, against a block of transaction  data?; k is a design parameter. SIs model atomic workload  units The maximum execution time for each SI is well  estimated. It is determined by design parameters.

Designers can choose the parameters so that all SIs have  similar maximum execution time. Any large task is  implemented by executing more than one SI. Application  programs are modeled as Super-Programs (SPs). They are  composed of SIs.

A runtime environment supports the execution of SPs.

At run time, each SI is dynamically assigned to a PC to  execute if and only if the PC has resource. Each SI can  only be executed on a single node. SIs are executed  parallel if they do not depend on each other. Extending the  functional unit to handle multiple SIs makes the high-level  parallelism not only to be determined by the algorithm but  also by the ISA designer. Increasing the degree of high-  level parallelism makes easier the task of balancing the  workload.

3.2 Design Issues for the Super-Instruction Set  There exist three main issues :  1) Portability of SIs. SIs are implemented by  software routines that can be executed on PC nodes. Since  SIs are dynamically assigned to PCs, they could be  executed on any PC. This requires implementing SIs that  are portable throughout PCs in the cluster. For a  heterogeneous system, it becomes a major task.

2) Completeness and orthogonality of the SI set.

The SI set creates an abstract layer. It should encapsulate  the underlying support system. An application should be  described completely by using these SIs. Thus, the SI set  should provide all basic operations to support such  abstractions. Considering the storage capacity and the  programming capability of general-purpose computers, the  number of SIs can be unlimited. Also SIs can be as robust  as needed. There is no real need to improve any resource  in order to provide a larger instruction set. Thus, the SI set  is open to expansion to match the application domain?s  requirements, as needed. In other words, the SI set is  completely application dependent. To enhance software  component reuse, it heavily depends on the application  domain. Another issue is the orthogonality of the SI set.

That is, SIs should not have any functionality overlap in  terms of the types of major tasks that they carry out. This  way, the SI set will have reasonable size without any  redundancy for better program maintenance, ease of  algorithm development, efficiency and good portability.

3) High-level name space for data references.

Member PCs are completely independent processing units.

They may have different independent logical space. SIs are  dynamically assigned to a PC. Thus, SP running on a  cluster needs a global logical space. SIs should only  reference data with names (or Ids) in this space rather than  reference their operands with local addresses on the  underlying procedures implemented SIs.. The runtime  environment will map the global Object to local space.

3.4 Data Blocks for Mining Association Rules  The operands for SIs are blocks of data. Such a data  block is called a super-data block (SDB).  SDBs are  primary entities for high-level super-programming; they  are used as build-in data in ordinary programming. High-  level super-programs build their data structures using  SDBs. SIs manipulate these SDBs. Each SDB has its own  global ID, as data in an ordinary program have their own address. The data included in an SDB can be  loaded/cached/stored at any node by runtime support  systems.  The data blocks have limited maximum size.

Thus the workload of SIs is limited. This way, assigning a  significantly large task to a single node is avoided. For  mining association rules, we have designed a set of super-  data blocks as shown in Table 1.

Table 1 Summary of Super-data blocks types  Name Content  BlockOfItems A list of distinct items covering a  continuous, exclusive partition  BlockOfTransa  ction  A set of transaction data  BlockOfJoinR  esult  A list of candidate patterns without  checking for frequent sub-patterns  BlockOfCandi  dates  A list of candidate patterns. All of their  sub-patterns are frequent patterns  BlockOfFreque  ntSet  A list of frequent patterns with the same  length  BlockOfRules A set of alreadymined association rules  3.5 ST Set for Mining Association Rules  We have designed an SI set for mining association  rules in large transaction databases. A summary is shown  in Table 2.

Table 2 Summary of a super-instruction set for mining association rules  SI name Parameters Function  LoadDataBlock Reference to the data source; an SDB of  transactions; maximum size of the SDB  Gets a block of data residing outside of the system  CountItemSupport ID of the extracted transaction block; ID of  the map model  Extracts all distinct items and counts the support of  items appearing in a block of raw transaction data  ShrinkItemBlock SDB of items; the threshold of support Prunes items in an SDB of items  GetFrequentItems  Block  A list of SDBs of items; SDB of frequent  items; generated SDB of 1-length frequent  patterns; mapping object  Creates an SDB of frequent items and an SDB of  1-frequentItemSet  ShrinkTransaction  Block  ID of the original SDB of transactions;  ID of the result SDB of transaction;  Shrinks a block of transactions  MergeTransaction  Block  A list of IDs of pruned SDBs of transactions Merges a list of pruned SDB blocks of transactions  into one SDB  GenCandidates  Block  [An frequent pattern] or [A list of frequent  patterns]; an SDB of frequent patterns; a  global mapping object; the generated SDB  Generates an SDB of candidate patterns.

FilterCandidates An SDB of candidates; an SDB of frequent  patterns; the global mapping object  Screens candidate patterns in a SDB by comparing  their sub-patterns with frequent patterns stored in  another SDB .

