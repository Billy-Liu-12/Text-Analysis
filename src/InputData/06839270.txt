Suggested Techniques for Clustering and Mining of  Data Streams

Abstract? The buzz word in research is Big Data. Big Data gets characterized by 5 V?s: Volume, Velocity, Variety, Veracity and Value of data. Volume in order of penta bytes, velocity which refers  to  click  stream  data  in  various  domains,  variety comprising  of  heterogeneous  data,  veracity  indicating  the cleanliness  of  data  and  value  emphasizing  on  the  return  on investment for companies  who invest in Big Data technologies.

This Big Data is better modeled not as persistent tables but in the form of transient data streams which need different clustering and mining techniques to be effectively processed and managed.

In  this  paper  some  suggestions  on  online  learning  through clustering and mining of stream data are presented.

Keywords?Data Streams, Big Data, Clustering, Mining

I.  INTRODUCTION Mr. Francis X Diebold[1], economist at the University of  Pennsylvania, was the first person to coin the term Big Data in the  year  1990.   The concept  of  Big  Data  was  first  used  in Macroeconomic measurement and Forecasting.  Watson et.al, were not only able to work with 500 indicators measured over 500 months, they also predicted that in future there would be thousands  of  indicators  measured  daily  or  in  higher dimensionalities and stored with much cheaper cost. Thus the Big Data movement came into existence. This can be attributed to the fact that storage cost has drastically reduced in the past decade,  Technologies  have  come  into  existence  which  can handle  process  complex  queries  and work  with  data  in  any format.  Human beings now create 2.5 quintillion bytes of data per day. This acceleration in the production of information has created a need for new technologies to analyze massive data sets.

Big Data analytics is the process of analyzing and mining Big  Data.  It  helps  in  producing  operational  and  business knowledge at an unprecedented scale and specificity.  [2] Big data  technologies  can  be  grouped  into  two  heads.  Batch processing  and  Stream  processing.  Batch  processing  works with  Petabytes  of  data  with  responsiveness  ranging  from minutes  to  hours  whereas  Stream  Processing  works  with several Gigabytes of data which should respond in seconds to minutes. Each of these processing has its own set of challenges and caters to different sectors of people. The challenges in Big Data arise during capture or data acquisition, curation, storage,  search, sharing, transfer,  analysis and visualization. All these challenges are compounded by the fact that big data deals with massive amounts of highly volatile, heterogeneous data.

With  increasing  data  storage  capacities  and  massive advancement  in  processing  technologies,  there  has  been  a heavy demand for systems, which are capable to mine massive and continuous streams of real-world data [6, 7, 8]. This stream data  usually  arise  ,  in  the  fields  of  temperature  monitoring, precision  agriculture,  urban  traffic  control,  stock  market analysis, network security, to name a few. A data stream is a transient,  continuously increasing  sequence  of  time  stamped data. Since data is a changing concept in such data streams, mining  data  streams  is  quite  cumbersome  task.  The development of corresponding stream processing systems is a topic of active research.

The remaining paper is split into different sections. Section II  talks  about  Challenges  in  Big  data  Processing,  which includes Batch Processing, Stream Processing and approaches used  in  the  same.  Section  III  discusses  various  algorithms available  for  clustering  transient  data  streams.  Section  IV focusses  on  mining  techniques  for  data  streams.  Section  V concludes  with  the  overall  suggestions  for  clustering  and mining data streams.

II. CHALLENGES IN BIG DATA PROCESSING.

The  collection  of  data  is  very  massive.  It  would  be  infeasible to store such massive data in a centralized database.

Divide and conquer  strategy with parallel  processing of  sub problems may opt out to be a practical solution in such cases.

Hadoop Map Reduce came as a major solution to Big Data problem. [3].  Hadoop is a software framework developed in JAVA for distributed data management and processing. Core Hadoop is  made up of  two components  Hadoop Distributed File  System  (HDFS)  and  Map  Reduce.  HDFS  hosts  large datasets  which  are  redundantly  stored  across  multiple machines.  It  ensures  fault  tolerance  and data availability for parallel processing of large datasets. For this purpose, HDFS divides  a  large  file  into smaller  blocks called as  data nodes which operate in parallel.

MapReduce is a programming model for processing large datasets in a parallel fashion. It has two functions: a Map and a      Reduce. The Map Reduce function [5] works in the following manner:  (a) The input file is first passed over the map stage which produces the corresponding (key, value) pairs  (b) This output is passed on to the shuffling stage which transfers the mapper?s output to the reducers based on the appropriate key  (c) Finally the reduce stage processes the received pairs and outputs the final result.

Big data being very massive poses very many challenges on computing and  paradigm shifts  on large  scale  optimization.

Although  massive  parallelism,  fault  tolerances  are  feasible, statistical  algorithms  are  yet  to  be  explored  on  such  high dimensional large scale data. Due to the vast accumulation on online data, more pronounced algorithms are required which caters to streams of data.

But  processing data in form of streams also comes with its own set of impediments. Since data streams are characterized by a continuous high rate  of  incoming data,  they flow into computer  systems  continuously  with  varying  arrival  rates, unlike traditional  data sets.  Thus, it  creates  a  few issues  in processing and mining of the data stream.

1. [9] The data elements in a stream arrive online and hence are transient and not in batches.

2. The stream processing system has no control over the order in which the elements arrive either within a data stream or across streams.

3. [9,  10]  The  data  stream  is  potentially  unbound, massive and almost unlimited in size.

Due to the reasons cited above, it is impossible to maintain all the elements of a data stream. This makes it essential for an on-line stream data processing system to fulfill the following requirements.  Firstly,  to  analyze  a  data  stream,  each  data element should be examined at most once.  Secondly, although new  data  elements  are  continuously  generated  in  a  data stream,  memory  usage  for  data  stream  analysis  should  be confined  finitely.  Thirdly,   in  order  to  utilize  the  result instantly  on  request  by  an  user,   newly  generated  data elements should be processed as fast as possible, which would produce  up-to-date result analysis  of a data stream.

4.  [9]  The  storage  constraints  of  data  stream  also  poses  a problem. This is because, once an element in a data stream has been  processed,  it  is  discarded  or  archived.  The  memory available to store vast amounts of stream data is also relatively small.

Thus stream processing systems need to cater to above issues while solving the problems in clustering and mining transient data streams. The next two sections discuss various clustering and  mining  techniques  to  efficiently  manage  online  stream data under different conditions and dimensions.



III. DATA STREAM CLUSTERING In  a  data  stream,  massive  unbounded  sequence  of  data  elements  is  continuously  generated  at  a  rapid  rate.  [7] Clustering is one of the most important  and frequently used  data analysis techniques. It refers to the task of grouping a set of  objects  in  such  a  way  that  objects  in  the  same  group, referred to as a cluster, are more similar, to each other in some way than to those in other groups (clusters). It is a key task of exploratory  data  mining,  commonly  used  for  statistical  data analysis [11, 12] Stream data clustering is a challenging area of research  that  attempts  to  extract  useful  information  from continuously arriving data. As the stream data is massive and updated in real time, traditional clustering methods cannot be used to process it. Hence, the need to discover new clustering methods  is  becoming  more  and  more  urgent.  Limitations imposed  by  available  computational  resources  typically constrain our ability to process  such data and restrict,  if not completely remove our ability to revisit previously seen data points.

Research  in  data  stream  clustering  algorithms  can  be divided  into  two  categories:  using  traditional  clustering algorithms  for  data  streams  and  new data  stream clustering algorithms  proposed  in  recent  times.  We discuss  these  two categories separately.

A. Using Traditional Clustering Algorithms  The concept of data stream clustering was first proposed in the year 2000, then researches in this field attracted broad attentions.  Traditional  clustering  algorithms  can  be categorized  based  on  their  characteristics  as  follows: density-based,  probability-based,  correlation-based  and mixed  attribute-based.  There  are  different  methods  to process  data  streams,  such  as  sliding  window  method, dynamic  grid  method,  multi-classifier  method,  and dissimilarity matrix method. All of these methods have the same  research  goal  that  they  are  committed  to  find  an effective way to deal with massive high-dimensional data streams, or try to solve the clustering problems that data information  is  uncertain,  dynamic  and  distributed  in arbitrary shape.  In  the early development  of  data  stream clustering,  the  focus  of  the  research  was  to  construct  a suitable data structure for data streams based on traditional methods.

BIRCH algorithm, Local Search algorithm, Stream algorithm and  CluStream  algorithms  are  some  classic  data  stream clustering methods proposed at the early time. [13] BIRCH is one  of  the  most  primitive  clustering  methods.  Though designed  for  traditional  data  mining,  it  is  quite  suitable  for very large  data  base so it  has  been applied for  data stream mining  as  well.  BIRCH  worked  on  two  concepts:  micro clustering and macro clustering. Using these two concepts, it could  overcome  two  main  difficulties  encountered  in agglomerative  method  of  clustering  i.e.  scalability  and  the inability to undo what was performed in the previous step. It worked on two steps: first it scanned data base and built a tree which included information about data clusters. In the second step,  BIRCH refined  the  tree  by  removing  sparse  nodes  as outliers  and  concentrated  on  original  clusters.  The  main disadvantage  of  this  method  is  was  the  limitation  in  the      capacity  of  leaf  as  BIRCH  worked  well  only  for  spherical clusters.

Guha et.  al.  in  [14]  proposed a  Local  Search  Algorithm. It used divide and conquer strategy to get the clustering results of  data  streams.  This  algorithm  could  only  describe  data streams, but it did not reflect the changes of data streams, still less forecast and analyze the tendency.

So in 2002, the new algorithm?Stream was proposed [13, 15]. It was a continuation and development of Local Search algorithm.  In  this  method,  K-Medians  algorithm  was leveraged to cluster objects base on SSQ (Sum of SQuared Distances)  criterion  for  error  measuring.  During  the  first scan, objects are grouped and medians of each group are gathered and associated to a weight, based on the number of objects in the cluster. In the next step, these medians are clustered  until  the  root.  In  most  cases,  the  clustering performance of  Stream algorithm was found to be better than BIRCH and the classic k-means algorithm. However, it  had  two main  disadvantages,  namely,  time granularity and data evolving [12, 16].

Addressing  the concerns  of  BIRCH and Stream algorithms, CluStream  algorithm,  which  worked  only  discrete  attribute values,  was  proposed  [12,  16].  This  algorithm  provided  a framework, in which the clustering process is divided into two stages:  online micro-clustering and offline macro-clustering.

In  the  micro-clustering  phase,  the  online  statistical  data collection is carried out and information about data locality is maintained in terms of micro-clusters. This process does not depend  on  any  user  input  such  as  the  time  horizon  or  the required  granularity  of  the  clustering  process.  The  micro- clusters  are stored in  a  structure  called  the pyramidal  time- frame i.e. they are stored at snapshots in time which follow a pyramidal pattern. Storing the clusters in a pyramidal pattern provides  an  effective  tradeoff  between  the  storage  and  the recall  ability  of  summary  statistics  gathered  from  different time horizons. The compact summary statistics of the micro- clusters  are used by the offline macro-cluster  component  to create  clusters  of  different  spatial  and  temporal  granularity.

Macro-clustering,  being  dependent  on  Micro-clustering,  the results  of  micro-clustering  are  important  to  the  entire  data stream clustering.

Since CluStream worked only for discrete attribute values, and did not show good results with high dimensional data, HPStream (High-dimensional Projected Stream) algorithm [17],  which  worked  for  both  discrete  and  continuous attribute values, was proposed. This method incorporated a fading cluster and projection based clustering methodology.

The  fading  cluster  structure  integrated  the  historical  and current  data  with  a  user-specified  or  user-tunable  fading factor.  The  data  projection  method  reduced  the dimensionality of the data stream to a subset of dimensions that minimized the radius of cluster groupings. Experiments were  carried  out  to  show that  by projecting  data  onto a smaller set of dimensions both synthetic and real world data sets could be more accurately processed with better quality.

B. New Data Stream Clustering Algorithms A  lot  of  research  is  being  done  to  put  forward  new  clustering  algorithms  for  stream data.  One  property  of  high dimensional  data  stream is  scarcity  which  is  aggravated  by uncertainty. The use of the uncertainty information is helpful in accounting  for  the  effects  of  the  uncertainty  on  the  sparse effects in high dimensional data. Authors in [18] have tried to use projection technique to solve the problem of clustering of uncertain data streams.

Recent researches have also explored the idea of clustering stream data  using outlier  detection.  However,  the traditional outlier  detection  methods  based  on  distance  measure  is  not very  effective  in  clustering  data  streams  as  there  are  more chances of a particular data point to be wrongly identified as an outlier  in  a  stream data.  So  authors  in  [19]  proposed  using clustering to identify the outliers. Three conditions were used for  identifying  outliers.  If  1  or  all  the  3  conditions  were satisfied then the data point in a data stream was considered to be  an  outlier.   The  first  condition  checked  if  the  object belonged  to  any  existing  cluster.  If  it  did  not,  then  it  was considered to be an outlier. The second condition checked the distance  between  the  object  and  an  existing  cluster.  If  the distance was found to be large, the object was considered as an outlier.  Finally,  the  third  condition  checked  if  an  object belonged to a sparse cluster. If it did, it was taken as an outlier.

Fig.  3  shows the  algorithm out  outlier  detection  in  data stream clustering.

Fig 3: Algorithm to detection outlier in stream data with clustering [20]  The  authors  of  the  outlier  method  however,  had  only proposed it as an algorithm. No experimental results were put forward on the same. Also, the threshold for time slice of the sliding window was yet to be explored. Hence it could be only considered as a suggestive approach for data stream clustering.

A major problem with processing the data in data streams is the uncertainty of the data for which, authors in [20] proposed P-Stream,  a  probabilistic  stream  clustering  algorithm.  They      came up with the concept of strong cluster, weak cluster and excessive  cluster  for  the  probability  tuples  in  data  streams.

They also designed  a cluster  selection strategy,  which could effectively assign each arrived tuple to an appropriate cluster.

P-Stream also designs the method to find candidate clusters, and  uses  Model  Outdated  Process  to  deal  with  outliers  to overcome  the  various  flaws  of  CluStream  algorithm  for processing outliers.

Researchers have also proposed the use of wavelet synopsis based algorithms of parallel data streams [21]. This algorithm mentions  two  concepts:  Hierarchical  Amnesic  Summary (HAS)  and  Discrete  Wavelet  Transforms  (DWT)  [22,  23].

Using these two tools, one can construct synopsis structure W- HAS,  which  is  wavelet  based  HAS in  the  hierarchical  data stream. Then dynamically calculate the approximate distance and  clustering  center,  and  apply  W-HAS  to  the  on-line clustering of parallel data streams.

Over the years, a lot of active research has been directed towards  using  evolutionary  techniques  for  data  stream clustering  and  are  still  continuing.  Data,  once  clustered  is mined for relevant information from the clusters. In the next section, we discuss some of the relevant mining techniques that can be used on large stream data.



IV. DATA STREAM MINING Data Mining is the knowledge which has to be extracted  from the large amount of data. As the concept of databases is moving towards Big Data and a large amount of stream data is getting generated at a very high speed, data stream mining is gaining importance as an upcoming area of research. In  this section,  we  discuss  some  prominent  algorithms  which  were used for data stream mining.

Incremental  learning  (IL)  takes  place  in  dynamic  data streams.  Since  there  is  a  flow  of  data,  data  streams  are subjected to concept drifts. These concept drifts occur either because  the  predictive  rules  are  not  invoked  regularly  or because  new  features  are  being  discovered  on  and  off.  To overcome  the  concept  drifting  problems,  suitable  mining algorithms are required, so that the vital information is never lost.  Such  mining  algorithms  are  invariably  incremental  in nature.  These  incremental  algorithms maintain  a  window of examples and hypothesis and are in the constant look out of either new hypothesis or existing patterns. FLORA family of algorithms takes care of dynamic adjustment of window sizes during the incremental learning process. [24, 25]  In  [24],  the author proposed an adaptive sliding window algorithm  ADWIN  for  detecting  change  and  keeping  the updated statistics from a data stream, and using it as a black- box in place of counters or accumulators in algorithms initially not designed for drifting data. The mining algorithms work on change detectors and estimators.

Some  of  the  earlier  algorithms  for  data  stream  mining referred  by the author were FLORA,  SVM (Support  Vector Machines),  OLIN  (Online  Information  Network),  CVFDT (Concept -adapting Very Fast Decision Trees),  UFFT (Ultra-  Fast  Forest  of Trees) algorithms. Based on these algorithms, the basic data stream algorithms can be classified as shown in the Table I.

Table I Types of Time Change Predictor and some examples [24]  Incremental  Learning  (IL)  takes  place  in  dynamic  data streams. For handling network traffic  and intrusion-detection data, Info-Fuzzy Network (IFN) was proposed in [25]. IFN is based  on  supervised  learning  and  a  decision-tree  learning methodology  was  used  for  inducing  accurate  classification model.

The OLIN algorithm extended the IN algorithm for mining continuous and dynamic data streams. The system takes input from the sliding window of training examples.  The learning module built the classifier model in accordance to the meta- learning  module  which  helped  in  building  the  classifier module. This classifier model worked in a supervised manner and predicted the class label of the next arriving example in the Data Stream. The online learning system contained three main parts: the Learning Module  responsible for applying the IN algorithm  to  the  current  sliding  window  of  examples;  the Classification Module  responsible for classifying the incoming examples using the current  network; and the Meta Learning Module which controls the operation of the Learning Module.

A. Dimensionality Reduction Data Stream contains data of high dimensionality. Since it  is  difficult  to  perform  clustering  on  high  dimensional  data, various data  dimensionality reduction techniques  have  to  be applied  before  the  clustering  process.  Some  of  the  data dimensionality  reduction techniques consolidated in  [26]  are SVM (Support Vector Machines) for supervised data and SVD (Singular  Value Decomposition), PCA (Principal Component Analysis),  LDA  (Linear  discriminant  Analysis),  ICA (Independent  Component  Analysis)  and  CCA  (Canonical Correlation Analysis) for unsupervised data. In this paper, the author proposed a dimension reduced weighted fuzzy c-means algorithm for  stream data.  High-definition videos in internet are examples  of such data sets.  Firstly,  dimension reduction technique was applied to convert the higher dimensional data stream in to lesser dimensions (Two dimensions) and then the      weighted fuzzy C-means algorithm was applied for better data stream clustering.

B.  Ensemble-based Classifiers Ensemble  learning  has  become  a  common tool  for  data  stream classification,  due  to  its  capability  of  handling  large volumes  stream  data.  Ensemble  learning  does  both  the operation of  clustering as  well  as classifiers.  It  first  clusters small  chunks of  data and then combines all  base models to form an ensemble classifier.  But because of concept drifting problem, it  becomes harder  to combine clusters  and classify them  under  an  ensemble  framework.  In  [27]  a  weighted average method was used for prediction which combines the clusters and classifiers. A weight vector reversely proportional to the classifier?s accuracy on the up-to-date data was assigned.

But if most of the data are recent data, giving a weight value becomes difficult. So a graph was first modeled with Vertices representing  the  cluster  centers  and  edges  represent  the similarity between the cluster centers. This graph was updated by latest stream data values, when the old values are discarded.

The base  models  are  weighted  according  to  the  consistency with up-to-date data model.

The labeling of stream data, in [28] was done selectively.

For  improving  the  prediction  accuracy,  the  variance  was minimized. The variance corresponds to the error rate and a Minimum Variance (MV) principle aims at reducing this error rate, signifying an improvement in the classifier?s efficiency.

This  MV  principle  was  combined  with  optimal  weighting module to build an active learning framework for data streams.

In  [29],  instead of using a linear  scan of all base classifiers during prediction, which has disadvantages like large volumes of data streams with concept drifting and requirement of timely responses, authors proposed a novel E-Tree indexing structure to organize the base classifiers.  In  this case,  base classifiers trained  using  decision  trees  and  decision  rules  can  be represented as spatial objects in the decision space. The spatial objects are indexed using E-trees.

Some tools have been developed for mining stream data.

The  prominent  amongst  them  is  MOA  (Massive  Online Analysis) [30]. MOA is an open source framework for stream mining. It?s similar to WEKA and written in Java Language. It has  a  collection  of  machine  learning  algorithms  for classification, clustering and outlier detection and provides a benchmark for data stream mining community.

C. Evolutionary techniques for Data Stream Mining Genetic Based Machine Learning techniques are potential  evolutionary techniques for herding large scale data analysis.

The authors in [31] elaborate on software solutions, hardware acceleration  techniques,  parallelization  methods  and  data intensive  computing.  Authors  in  [32]  paper  proposed  a framework  to  build  predictive  models  for  data  streams  for labeled and unlabeled data.  They put forward a relational  k- means  based  transfer  semi-supervised  SVM  learning framework (RK-TS3VM), which intended to leverage labeled and  unlabeled  samples  to  build  prediction  models.  Training unlabeled data poses a major problem. If some similarities are identified  among  chunk  of  data,  they  can  be  utilized  for  classification purposes. But, in stream data, this is never the case  because  of concept  drifting.  So in  mixed data streams, which has both labeled as well as unlabeled data, if the amount of concept drift and the pattern can be identified, then mining of such mixed data streams might become easier. Based on the above problems 4 types of solutions can be identified.

Type I: labeled with same distributions of yet-to-cum data  Type  II:  labeled  with  similar  distributions  of  yet-to-cum data  Type III:  unlabeled with same distributions of yet-to-cum data  Type IV: unlabeled with similar distributions of yet-to-cum data  A  combined  framework  was  proposed  for  mixed  data streams. A new algorithm, RK-TS3VM was proposed for type I II and III and a relational k-means was proposed for type IV.

Just like clustering and classification algorithms are used in mining  data  streams,  association  mining  also  plays  an important role in mining data streams for finding the frequent itemsets. In this context, Authors in [33] proposed an algorithm GUIDE (Generation of Temporal maximal utility itemsets from data Streams). This algorithm reconsiders two issues in utility mining. The two issues being,  the utilities of different items being different and the second, the frequent itemsets might not produce  the  same  utility.   The  basic  idea  of  this  algorithm GUIDE is that  it  finds the maximum itemsets from specific time  period  to  current  time.  The  important  utilities  of  the itemsets are picked up in a single scan and stored in a data structure TMUI (Temporal Maximal Utility Itemset tree). This algorithm  is  merged  with  data  stream  mining  for  chain  of hypermarkets.

In addition to TMUI, a Compact Pattern Stream tree (CPS- tree)  was proposed in [34]. CPS is an efficient  technique to discover recent frequent patterns from a high-speed data stream over a sliding window. FP-Tree requires two scans to achieve a frequency descending tree structure. But CPS requires only a single scan and maintains an I-list. The CPS tree construction takes place in two phases: insertion and restructuring. Insertion phase captures the stream content as per the current sort order of  I-list.  The  restructuring  phase  restructures  the  tree  in  a frequency-descending  order  of  item sets  in  the  data  stream.

This I-list is dynamically updated with current data. Because of its adaptive nature,  CPS was found suitable for handling the concept drifts in massive data streams.



V. CONCLUSION Data  stream  mining  is  a  very  important  and  growing  research domain. A number of algorithms have been proposed and implemented for clustering and mining stream data. As far as Stream Clustering Algorithms are concerned,  BIRCH can be  used  in  those  application  areas  where  speed  is  a  major concern.  These  may  include  applications  like  Storage Optimization,  Data  Partitioning,  Index  Construction,  Web Document Clustering etc.  [14]. All Stream based algorithms like  CluStream,  HPStream  etc.  can  be  used  in  those      applications  like  Network  Intrusion  and  Target  Marketing where reliability is a major concern.

Data stream mining can be used in tracking and mining evolving web user  trends from web logs.  Ensemble models can be used in applications where data arrives at a very high speed like Web traffic,  spam and intrusion detection. Much research  work  is  aimed  at  using  Genetic  Based  Machine Learning in bioinformatics and biomedicine domains.

All  the  algorithms  have  their  own  advantages  and limitations. Since the volume and velocity of big data keeps increasing  on  a  daily  basis,  more  advanced  techniques  for clustering and mining such humongous data is the need of the hour.

