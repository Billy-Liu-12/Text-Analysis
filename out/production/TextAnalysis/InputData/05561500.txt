by Distributed Learning Automata

Abstract - Predict the customer behavior in the shopping is  important from two aspects: one is from the perspective of goods  suppliers and the other from shop owners. Both groups want to  know that their customers interest in which goods and buy which  sequence of the goods. In the this paper we provide a way in  finding sequences of the customers shopping, which in  comparison with the previous methods, it works better and we  demonstrate that it could obtain sequences of the customers  shopping in shorter time than the previous methods. Finding of  the sequences is very essential for suppliers of goods and shop  owners and will lead to an increase in annual profit. In this  article, we provide a method of finding two-member and higher  sequences by distributed learning automata; its costs is lower  than the other methods. We examined it on online basket data of  costumer shopping and it is clear that the results are much  better.

Keywords: Customer, Predict, Learning Automata, Shopping,  Sequence Pattern

I.    INTRODUCTION  Prediction of customers behavior in shopping is one of the  important problems for suppliers of goods and shop owners.

The latter group will change manner of arrangements of goods  in their shops, manner of goods order, manner of  warehousing and marketing by these information. Suppliers of  goods and shop owners, by the usage of the information about  the fact that customers have bought which products in  sequence, could gain a lot of profits by providing and  arranging of them in the shelves.

When a customer refer to a shop, he may found some of the  necessary categories very hard, or he could not found that, or  he may forgot what is the most necessary, but if behavior and  manner of the customers shopping have been studied, it will  lead suppliers of goods and shop owners to arrange their    orders in accordance with requests of the customers and the  most important fact that they could arrange goods in a way  that the customers could follow goods in sequence. This will  cause the customers not to be confused in finding their  considered products, they will be able to find them fast and/or  in some cases they could buy some goods, which are in the  sequence of shopping and profited more the owners. So, study  and obtain sequences of goods shopping - which could be in  two-member sequences or higher- is an essential matter and  one of the important parameters in profitability of a firm or  shop owners. For example, buying a computer, then its desk  and game CDs is a sequence in database of a store. This is  relates to sequence pattern mining, which some works were  done previously on them. We will consider the previous works  in this field, as follows; and then examine the suggested  method, and finally we will examine the suggested method  and previous methods on an online database of Mushroom  and basket data of customers shopping and demonstrate that  the suggested method is working better than the previous ones  and could find a sequence of iterations of data set in shorter  time.



II. Efficient and Scaleable Methods of sequence pattern  mining and Literature Review  Concept of sequence pattern mining was considered in  transaction databases for investigating the information for the  first time [1]. Suppose that I = {i1, i2, , im} was set of all of  the items in a transaction database, such as D. item set with  k-item  that includes k items of I set, will be alternative, only  if number of available transactions in D consists of  was not  lower than |D|. In this definition  is a threshold limit that is  determined by the user, which is called minimum support and  |D| is the number of current transactions in D.

is a number between 0 and 1, which demonstrates a  percentage of D transactions that consists of ; Including  means that a transaction has all of the available items in .

Agrawal, 1994, observes an interesting property, named  Apriori, in alternative items set for the first time [2].

According to this property, a set of k-items is alternative,  only when all of its items subsets were alternative. Progress  of using alternative items of k-items will continues in  producing candidates of (k+1) items and alternation or not  alternation test of them with a complete scanning on    transaction database, up to derivation of all of alternative  items available in transaction database. This method is  constitution of Apriori [2] algorithm and its substitution [3],  which is provided by Mannila.

After introducing of Apriori, lots of studies and efforts  were done for improving its efficiency and generalization.

Park -1995 tries to use hashing technique for improving its  efficiency [5]. In the same year, Savasere uses blocking  method in improving the algorithm. In this method, the  transaction base is categorized to non-overlapping sections by  Predict the customer behavior in the shopping  ma.esmaeilpour@gmail.com va.naderifar@gmail.com  rs@ftsm.ukm.my  Mansour Esmaeilpour Vahideh Naderifar  978-1-4244-6716-7/10/$26.00  2010 IEEE 1668  distinguishing some of the big alternative item sets with two  scanning of transaction database in accordance with  investigated great item sets, and the results of separate  scanning of sections are integrated. Toivonen, 1996 uses  sampling for improving efficiency of Apriori [6]. In 1997, a  method for dynamic counting of item sets (DIC) was provided  by Brin [7]. In this method, it is tried not to consider a separate  scanning for testing alternation or non-alternation of k-items  candidates, but tries to distinguish alternation of more  candidates (with whatever length) in each scanning of the  transaction database, thus leads to decrease of number of  scanning of the transaction database in Apriori algorithm.

By overlapping in counting of number of different sizes  patterns, DIC algorithm may cause decreasing in number of  scanning of the transaction database. Increasing exploration is  another improving in Apriori, which is done in 1996, by  Cheung [8]. Algorithms, which were available before  providing this method, assumed the transaction database  without changing and used alternative item sets as well as  investigated association rules without any change.

Other efforts that are done for improving of Apriori, are  based on parallelization of investigation algorithms. These  methods are as follows: provided Method by Park in 1995 [9],  Agrawal Method in 1996 [10], Cheung Method in 1996 [11],  and Zaki Method in 1997 [12]. Similarity of these methods is  removing of Apriori Algorithm from the scope of serial  investigation and directing it towards parallel or distributed    investigation.

Han introduced a method in 2000, called FP-Growth [15],  which investigates a complete set of all of the alternative item  sets without producing of candidates. This method was based  on divide and conquers.

Frequency methods were introduced as a substitution or  improving of FP-Growth; including primary production-depth  of the alternative item sets, which is provided by Agrawal in  2001 [17]. Another method is H-Mine algorithm that is  introduced by Pei in 2002 [16]. This method uses investigation  of a superstructure in sequence pattern mining. Liu, in 2002-3,  considers two methods of making substitution trees [18], and  up-bottom and bottom-up measurement of similar trees, which  were produced in FP-Growth [19]. Finally, Grahne and Zhu in  2003, provide a usage on the basis of an array of structure of a  prefix tree, and apply it for efficient measurement in Pattern  Growth method [20].

In 2004, Yang introduces analysis of maximum alternative  patterns measurement time complexity (in the worst mode)  [21]. This analysis demonstrates that this complexity is  NP-hard. And finally Ramesh distinguishes linear distribution  of set of the alternative item sets and maximum alternative  item sets in 2003 [22].

The suggested work was done on the basis of learning  automata. Then, we will introduce learning automata and its  species, and in the nexst part, the suggested approach will be  examined.



III. LEARNING AUTOMATA  A learning automata is composed of two parts,  I- A stochastic automata with number of limited actions,  II- A stochastic environment. Each action selected by potential  environment is assessed and answer is given to a learning  automata. Learning automata uses this answer and selects its  action for the next stage. Figure 1 shows relationship between  learning automata and environment [4].

Figure 1. Environment and learning automata and relationship between  them  A.     Environment  Environment can be shown with E= { , , c} which  = { 1, , m} is set of inputs, ={ 1, , m}is set of  outputs and c= {c1, c2, , cr} is set of penalty probabilities.

When has two elements, environment is called P-Model.

1=1 as penalty and 1=0 as reward. In environment of  Q-Model, (n) is finite output set with more than two values  between [0,1] and in S-Model, (n) is a continuous random  variable in the range [0,1]. Ci is set of probability of  undesirable answer [13]. In a stationary environment, values ci  remain unchanged while in non stationary environment, these  values change during time learning automata are classified  into two groups with variable structure or fixed structure.

Then we describe variable learning automata structure.

B.    Variable Learning Automata  Variable learning automata is shown with { , , P, T}  which m,...,1 is set of automata actions,  m,...,1 is set of automata inputs, PmPP ,...,1 is  probability sequence for selection of each one of the actions  and )().().(a1 nPnBnTnP  is learning algorithm[13].

In this kind of automata, if action ai is selected in stage and  receives desirable answer, probability of Pi(n) increases and  other probabilities decrease. For undesirable answer,  probabilities of Pi(n) decrease and other probabilities increase.

Any way, changes are made in such a way that sum of Pi(n)  remains equal to one. In the bellow we can see an example of  learning algorithm in variable learning automata in P-Model  [14].

The following algorithm is an example of linear learning  algorithms in variable automata.

Desirable answer  Pi(n+1)=Pi(n)+a[1-Pi(n)]                         (1)  Pj(n+1)=(1-a)Pj(n) j , j i  Undesirable answer  Pi(n+1)=(1-b)Pi(n)                                  (2)  Pj(n+1)=(b/r-1)+ 1-b)Pj(n) j,   j i   Where a is reward parameter and b is penalty parameter  and r is number of actions. With regard to values of a and b,  when a = b, is said, LRP(Linear Reward Penalty), when b be  very low than a, is said, LR&P(Linear Reward Epsilon Penalty)  and when b equals to zero, is said, LRI(Linear Reward  Inaction).[13]  C.   Distributed Learning Automata  Distributed Learning Automata is a network of the  learning automata which cooperates with each other for  problem solution, in this network; colleague learning    automata, at any time, only one automata is active. Number of  the actions which can be done by one automata in DLA equals  to number of automata which have connected to his automata.

Selection of an action by learning automata in this network  leads to activation of learning automata connected to learning  automata isomorphic with this action [14].

On the other hand, selection of an action by learning  automata in this network is isomorphic with activation of  another learning automaton in this network.

The model which we consider for DLA network is a graph  of which each vertex is a learning automaton. Presence of  edge (LAi, LAj) in this graph means that selection of  i  j  action by LAi leads to activation of LAj. Number of actions  selected by LAk is shown as  k  r  kkk PPPP ,...,, 21 .

In this set, value kmP indicates probability of action  k  ma ,  selection of kma action by LAk leads to activation of LAm-k  indicates number of actions which can be performed by LAk  automata.

Figure 2. Distributed Learning Automata with three Learning automata

IV. THE PROPOSED METHOD  In this part, we will consider the alternative sequence  pattern mining in a data set, in which we could extract  two-member, three member, ., n-member sequences  because it is possible to use from these sequences in the  manner of arrangements of goods in stores. Here, obtaining  the biggest sequence is not considered, but obtaining each  sequence with whatever frequency is important in order that  the sequences could be used in the abovementioned categories.

In this method, the distributed learning automata, p model,  and LRI learning algorithm with variant actions and reward of  0.5 are used. A learning automata is attributed to each node.

Each learning automata has a task to do, which is in fact the  sequence of the shopping. The task starts with single-items  and finishes with n-items. It is done as follows:  1) Give value to support and Threshold =1/r, where r is    automata actions ( Threshold is Minimum acceptable  amount for each sequence that dependent to support  amount)  2) Start with considering single-items of the customers  shopping set and importing them in random graph.

3) A random graph will be made of the sequences.

4) Each sequence that is actually subset of the  customers shopping, when import in the random graph,  will rewards in accordance with relation 1, and amount of  the reward will be 0.5; and probabilities of path application  or pi will be up dated. All of the sequences imported in the  graph and pi will be up dated towards them.

5) The graph will be scanned and each edge of the graph  that its probability of application or pi was less than  Threshold = Threshold + a [1 - Threshold] will be deleted  (deleted edges do not considered as active edges in learning  automata. So, these edges won t be used in the next stages  and no up dating will be done in their probability of actions  and they will be valued by -1 in order that despite of up  dating, it will be considered with no effect or -1).

6) Arrange the remained edges according to the biggest  value of pi (these edges could be a path, are sequence of a  shopping that obtains in each stage of the algorithm, then  sequence of 1-member to n-member will obtain).

7) If there is still a set remaining of the customers  purchase set, return to stage 4.

After each stage, the obtained graph- which is created  according to input sets- will be scanned and those paths that  probability of their actions is not less than  Threshold = Threshold + a [1 - Threshold], will be sent to  output as a k-member set with the most iteration.

This algorithm will continue for finding the most iteration  of shopping list by the customers up to when there were no  shopping subsets available. In each iteration, some of the  edges of the graph- that starts at first as a graph with one-  length paths, and finally after removing of some of the nodes  and paths, it finishes with n-length paths- were removed, and  in fact the graph is pruning and leads to decrease of the costs  and finding shopping sequence than other methods. Thus, the  random graph encounters with removal of some of the paths  and its domain decreases, and high complexity in obtaining  final answer of each stage decreases. By implementation of    each stage of the algorithm, we could obtain the most iteration  in two-member, three-member, , n-member sequences. This  algorithm, together with other algorithms was examined in  database Mushroom and customers shopping basket data.

Results of evaluation of the suggested method, associated with  other methods, are presented as follows.

ALGORITHM DLA_SPM  Input: I transaction data set, Threshold, G (V,E);  Output: Subset of I;  // each subset that contain of more frequent pattern.

Begin  Construct the DLA from G;  Init Support; Threshold=1/r;  // r is number of actions.

For all subset of one members of I do  // subsets of I is started from one members.

While (p>= Threshold) do  // p is amount of probability of each actions.

RewardPath (Path in G);  End While  End for  For i =1 to support do  Threshold = Threshold + a [1- Threshold];  // initializing the threshold.

// a is amount of Reward.

End for  For all subset of I do  // subsets of I is started from two members to n members.

While (p>= Threshold) do  // p is amount of probability of each actions.

RewardPath (Path in G);  End While  Print the each path that p >= Threshold;  // in each iteration we can find the each subset of I  //that contain of more frequent pattern.

End for  End DLA_SPM;  Figure 3. Shows the algorithm for sequence pattern mining in random  graph via DLA

V. EVALUATION of PROPOSED METHOD  In the proposed method, distributed learning automata are    used. A learning automata is attributed to each nodes. In each  stage, some of the items were imported to the random graph.

By importing of each item to the graph, the learning automata  will be active and up date its probability of application as  a=0.5 in accordance with relation 1; it will continue up to  when all of the subsets of the customers shopping set were  examined.

Finally, this method was examined on online database of  Mushroom and customers shopping basket data. Mushroom  database has 22 fields and 8123 records that states properties  of mushroom in different modes and was obtain from the UCI  (machine learning database) [23]. Customers shopping basket  data has 30 fields and 88159 records that contain of 16450  items. This file contains transactions from a Belgian retail  store [24]. The present method was implemented for  investigation of alternative patterns of a set with n members.

For implementing of simulation program, programming  language of C++ Standard Template Library with 2GB RAM,  CPU core2Dua 2.00 GHz and windows XP SP/2 operating  system. Its results are presented in figures 4, 5 in comparison  with other methods. This work was done by different supports  and demonstrates better results and efficiency of the suggested  method.

Figure 4. Shows comparison the other method with the proposed method in  Mushroom data set  Figure 5. Shows comparison the other method with the proposed method in  Mushroom data set

VI.   CONCLUSION  In the present study, distributed learning automata method  was examined for sequence pattern mining in a set of  sequence data. This method could assist in a great extent to  suppliers of goods or shop owners in finding sequence of the  customers shopping. We demonstrated that by this method,  we are able to extract in two-member, three-member , ,  n-member that have had the most iteration and provide them  to the shop owners for decision making and accurate  arrangements of goods in the shelves. It was compared with  other methods and its results show higher efficiency of the  proposed method than the other ones. Finding of the  categories leads to increase of satisfaction of the customers in  shopping, increase of the stores profits and decrease of the  customers  confusion in finding their necessary products.

