Mining Association Rules from Stars

Abstract  Association rule mining is an important data mining problem. It is found to be useful for  conventional relational data. Howevec previous work has mostly targeted on min- ing a single table. In real life, a database is rypically made up ofmultiple tables andone imponant case is where some of the tables form a star schema. The tables typically corre- spond to entity sets and joining the tables in a star schema gives relationships among enriry sets which can be v e v  in- teresting information, Hence mining on the join result is an importantpmblem. Based on characteristics of the star schema we propose an efficient algorithm for  mining as- sociation rules on the join result but without actually per- forming the join operation. We show that this approach can signifrrantly out-perform the join-then-mine approach even when the latter adopts a fastest known mining algorithm.

1 Introduction  Association rules mining [AIS93, AS941 is identified as one of the important problems in data mining. Let us first define the problem for a database D containing a set of transactions, where each transaction contains a set of items.

An association rule has the form of X + Y where X and Y are sets of items. In such a rule, we require that the fre- quency of the set of items X UY is above a certain threshold called the rninsup. The frequency (also known as support) of a set of items Z is the number of occurrences of Z in D, or the number of transactions in D that contain Z. The con6dence of the rule should also be above a threshold. By confidence we mean the probability of Y given X.

The mining can be divided into two steps: first we find the sets of items that have frequencies above minsup, which we call the frequent itemsets. Second, from the sets of frequent itemsets we generate the association rules. The first step is more difficult and is shown to he,NF-hard. In our subsequent discussion we shall focus on the first step.

- wangk@cs.sfu.ca  The techniques in association rule mining has been ex- tended to work on numerical data and categorical data in  more conventional databases [SA96, RS981, some re- searchers have noted the importance of association rule mining in relation to relational databases [STAOO]. Tools for association rule mining are now found in major products such as IBMs Intelligent Miner, and SPSS's Clementine.

In real databases. typically a number of tables will he defined. In this paper, we examine the problem of mining association rule from a set of relational tables. In particu- lar we are interested in the case where the tables form a star structure [CD97] (see Figure 1). A star schema consists of a fact table ( F T )  in  the center and multipledimension tables.

We aim to mine association rules on the join result of all the tables [JSOO]. This is interesting because the join result typically tells us the relationship among different entities such as customers and products and to discover cross enti- ties association can be of great value. The star schema can be considered as the building block for a snowflake schema and hence our proposed technique can be extended to the snowflake structure in a straightforward manner.

Figure 1. Star with 3 Dimensional Tables  At first glance it may seem easy to join the tables in a star schema and then do the mining process on the joined result.

However, when multiple tables are joined, the resulting ta-  0-7695-1754-4/02 $17.00 Q 2002 IEEE 322    ble will increase in size many folds. There are two major problems: Firstly, in large applications, often the join of all related tables cannot be realistically computed because of the many-to-many relationship blow up, large dimension tables, and the distributed nature of data.

Secondly, even if the join can be computed, the multi- fold increase in bath size and dimensionality presents a bugh overhead to the already expensive frequent itemset mining step: ( I )  The number of columns will be close to the sum of the number of columns in the individual tables.

As the performance of association rule mining is very sen- sitive to the number of columns (items) the mining on the resulting table can take much longer computation time com- pared to mining on the individual tables. (2) If the join re- sult is stored on disk, the YO cost will increase significantly for mukipre scanning steps in data mining. (3) For mining frequent itemsets of small sizes, a large portion of the YO cost is wasted on reading the full records containing irrel- evant dimensions. (4) Each tuple in a dimension table will be read multiple times in one scan of the join result because of duplicates of the tuple in the join result.

We exploit the characteristics of tables in a star schema.

Instead of ?joining-then-mining?, we can perform ?mining- then-joining?, in which the ?joining? part is much less costly. Our strategy never produces the join result. In the first phase, we mine the frequent itemsets locally at each dimension table, using any existing algorithm. Only rele- vant information are scanned. In the second phase, we mine global frequent itemsets across two or more tables based on local frequent itemsets. Here, we exploit the following pruning strategy: if X U Y is a frequent itemset, where X is from table A and Y is from table B, X must be a frequent itemset and Y must be a frequent itemset. Thus, the first phase provides all local frequent itemsets we need for the second phase. The difficulty lies in the second phase.

One major challenge in the second phase is how to keep track of the many-to-many relationship in the fact table without generating the join result. We make use of the feature that a foreign key for a dimension table A can ap- pear many times in the join result, which allows us to intro- duce some structure to record the key once, together with a counter for the number of duplications. We also make use of the idea of semi-join in relational databases to facilitate the mining process. From these ideas we propose a set of al- gorithm and data structures for mining association rules on a star schema which does not involve a join step of the ta- bles involved. Experiments show that the proposed method is efficient in many scenarios.

2 Problem Definition  Consider a relational database with a star schema.

There are multiple dimension tables, which we would de-  note as A, E ,  C ,  ..., each of which contains only one pri- mary key denoted by transaction id ( r i d ) ,  some other at- tributes and no foreign keys. (Sometimes we simply refer to A, B, C ,  ..., as dimensions.) 0;. bi ,  ci denote the transaction id (rid) of dimension tables A,  B, C, respectively. We as- sume that the attributes in the dimension tables are unique.

(If initially two tables have some common attributes, re- naming can make them different.) We assume that attributes take categorical values. (Numerical values can be parti- tioned into ranges, and hence be transformed to categorical values [RS98].) The set of values for an attribute is called the domain of the attribute.

Conceptually, we can view the dimension table in terms of a binary representation, where we have one binary attribute (or we call an ?item?) corresponding to one ?attribute-value?? pair in the original dimension table. We also refer to each tuple in A or the binary representation as a transaction. For example, consider Figure 2, wl. w2, w3 are attribute names for dimension table A, and the value of attribute w1 for transaction 01 is Rz. In the conceptual binary representation in Figure 2, we have attributes for ?wl = Ro?, ?vl = R1??vl = R2?, ... (we call them zlr z2, 23, ..., respectively). For transaction 01 in table A, the value of attribute z3 is 1 (81 = R2 is TRUE), and the values of 2 1  and 2?2 both equal to 0 (FALSE). In our remaining dis- cussions, binary items (one item for each ?attribute-value? pair) in the conceptual binary representation would be used.

x i l z i 2 x i  denotes the itemset that is composed of items z i , ,  xi.. z;,, ... We assume an ordering of items which is adopted in any transaction and iremser. E.g. z1 would al- ways appear before x2 if they exist together in some uansac- tion or itemset. This ordering will facilitate our algorithm.

Figure 2. Dimension Table and its Binary Rep resentation  There is one fact table, which we denote as FT. FT has attributes of ( t id (A) ,  t i d (B) ,  t id(C),  ...), where t id (A) is the tid of table A. That is, FT stores the rids from dimen- sion tables A, B ,  C ,  . . .as foreign keys. (Later we shall dis- cuss the more general case where FT also contains some other attributes.) In an ER model, typically, each dimen- sion table corresponds to an entity set, and FT corresponds to the relationship among the entity sets. The relationships among entity sets can be of any form: many-to-many, many- to-one, one-to-many, or one-to-one.

We are interested to mine association rules from the star structure. In particular we shall examine the sub-problem of     finding all frequent itemsets in the table T resulting from a natural join of all the given tables (FT W A W B W C . . .).

The join conditions are given by FT.Tid(A) = Adid, FT.Tid(B) = B.tid, FT.Tid(C) = C.tid, ... In the fol- lowing discussions, when we mention frequent itemset we always refer to the frequency of the itemset in the table T.

We assume that a frequency threshold of rninsup is given for the frequent itemsets. A frequent item corresponds to a frequent itemset of size one.

In our mining process, the dimension tables will he kept in the form of the VTV (Vertical Tid-Vector) representa- tion [SHSOO] with counts. Specifically, suppose there are TA, TB, TC transactions in tables A, B, C respectively. For each frequent item x in table A, we store a column of TA hits, the it* bit is 1 if item x is contained in transaction i, and 0 otherwise. We also keep an array of TA entries where the irA entry corresponds to the frequency of tid i in F T .

3 The Proposed Method  First we present a simple example to show the idea of discovering frequent itemsets across dimension tables with- ant actually performing the join operation. We shall use a data type called tiddist in our algorithm. It is an ordered listof elementsoftheformtid(count), wheretidisatrans- action ID, and count is a non-negative integer. Given two t i d h t s  L 1 ,  Lz .  theunion L1 U L z  isthe listoftid(count), where t id appears in either L1 or L2,  and the count is the sum of the counts of tid in L1 and L?. The intersection of two tidlists L1, Lz is denoted by L1 n L?,  which is a listoftid(count), where t id appears in both L1 and Lz and the count is the smaller of the counts of tid in L1 and Lz .

Suppose we have 2 dimension tables A, B,  and a fact table FT. The following are some of the tiddists we shall use.

t i d ~ ( x i )  : a tid-list for xi. where x; is an attribute ( item ) of table A. In each element of tid(count) in the list, lid is the id of a transaction in A that contains xi, and count is the number of occurrences of the rid in FT. If the tid of a transaction that contains xi  does not appear in tidA(xi), the count of it is 0 in FT.

E.g. t i d ~ ( x 3 )  = {a1 (5 ) ,a3 (Z) }  means that the tids of transactions in A that contain x3 are a l  and a3; a1 appears 5 times in FT, and a3 appears 2 times.

t i d A  (X) where X is an itemset with items from A, it is similartotidA(xi) except xi isreplaced by X. lidA(r;x,) can beobtained bytidA(xi) ntidA(xj).

B+ey(a,): Given a tid a, from A, B l e y ( a , )  de- notes a lid-list of tid(count), where tid is a tid from B and count is the number of Occurrences of tid together with a, in FT.

E.g. B-key(al) = {b3(4),b5(2)} means that a1b3 oc- curs 4 times in F T ,  and a ib j  occurs 2 times.

B l id (x i ) :  Given an item xi in A, B l id (x i )  denotes a lid-list, of tid(count), where tid is a tid of E,  and count is the number of times tid appears together with any tid ~j of A such that transaction aj contains xi in A.

Blid(X): similar to B l i d ( x i )  except item xi is re- placed by an itemset X from A.

Example 3.1 Suppose we have a star schema for  a num- ber of dimension rabies related by a fact table FT. The followingfigure shows 2 of the dimension tables A and B, and the projection of FT on the two columns that contains transaction IDS for A, B, but without removing duplicate tuples.

B  Some of the iids fmm the dimension tables may not ap- pear in FT (e.g. a2, b l ) .  Suppose minsup isset 105. Wefirst mine frequent itemsets from each of A and B. For example, X I  andx3appearfogetherinal, a3, thetotalcountofal,a3 in FT is 6, hence X I 2 3  is a frequent itemset. N e n  we check $ a  frequent itemset from A can be combined with a fre- quent itemset fmm B to form a frequent itemset of greater size. yly6 is a frequent itemset fmm B with frequency = 5.

We want to see $ X I X ~  can be combined with yly6 to form a frequent iternset, the steps are outlined as follows:  1.  tida(Z1) = tal(4),a3(2)], t i d ~ ( 2 ~ )  = {01(4),(13(2)}.

tidn(zrza) = tidA(z1) n tidA(zs) = {01(4),a3(2)}, ~ ~ ~ B ( Y I Y S  ) = { b ( 2 ) ,  b5(3)} .

2. B k e d a l )  = { 4 ( 1 ) , b a ( l ) , b ~ ( 2 ) ] ,  3. B-tid(z1zs) = Bkey(a1) U BAey(aa) Bkey(os) = { b ( l ) , b s ( l ) ] .

= { b 2 ( 2 ) ,  h ( l ) , b 5 ( 3 ) } .

4. B-tid(Z123) ntidB(ylYG) = &(2) ,b5 (3 ) ) .

5. Tke combined frequency = total count in rhe list  { b z ( Z ) , b ~ ( 3 ) }  = 5.

Hence the iternset x1 x3yly6 is frequent. 0  In general, to examine the frequency for an itemset X that contains items from two dimension tables A and B, we can do the following. We examine table A to find the set of transactions TI in A that contain the A items in X.

Next we determine the transactions Tz in B that appear with such transactions in FT. Note that this is similar to the derivation of an intermediate table in a semi-join strategy, where the result of joining a first table with the key of a second table are placed, the key of the second table is a     foreign key in this intermediate table. In the mean time, the set of transactions T' in B that contain the B items in X are identified. Finally TZ and T3 are intersected, and the resulting wunt is obtained.

The use of tidlist is a compressed form of recording the wciurences of lid's in the fact table. Multiple Occurrences would be condensed as one single entry in a tidlist with the count associated.

Initial Step: In order to do the above, we need to have some initial informationabout tidA(xi) for each item xi in each dimension table A. One scan of a dimension table can give us the list of transactions for all items. In one scan of FT we can determine all the counts for all transactions in all the dimension tables. In the same scan, we can also de- termine B.key(ai) for each l id  ai in each dimension table.

3.1 Overall Steps  For simplicity, let us first assume that there are 3 dimen-  Step 1 : Preprocessing sion tables A,  B, C. The overall steps of our method are:  Read thedimension tables, convert them intoVTV (Vertical Tid-Vector) format with counts (see Section 2).

Perform local mining on each dimension table. This can be done with any known single table algorithm with a slight modification of refering to the counts of transactions in FT which has been collected in the initial step described in Sec- tion 2. The time taken for this step typcially is insignificant compared to the global mining steps.

Step 3 : Global Mining Step 3.1 : Scan the Fact Table  Step 2 : Local Mining  Scan the Fact Table IT and record the information in some data structures.

We set an ordering for A, E ,  C. First we handle tables A and B with the following 2 steps:  Step 3.2 : Mining size-two iremsets This step examines all pairs of frequent items x and y.

which are from the two different dimension tables.

Repeat the following for k = 3 , 4 , 5  ..... Candidates are gen- erated by the union of pairs of mined itemsets of size k - 1 differingonly in the last item. The technique of generation would be similar to FORC [SHSOO]. Next count the fre- quencies of the candidates and determine the frequent item- sets.

After Steps 3.2 and 3.3, the results will be all frequent itemsets formed from items in tables A and/or B. This can be seen as the frequent itemset mined from a single dimen- sion table AB. Similar steps as Steps 3.2 and 3.3 are then applied for the tables A B  and C to obtain all frequent item- sets from the star schema.

Step 3.3 : Mining the rrstfor A and B  3.2 Binding multiple Dimension Tables  We can easily generalize the overall steps above from 3 dimension tables to N dimension tables. Suppose there are totally N dimension tables and a fact table FT in the star schema. We start with two of the dimension tables, say A and B. We apply Steps 3.2 and 3.3 above to mine all frequent itemsets with items from A and/or B without joining the tables with F T .  This set of itemsets is called F A B .  We call Steps 3.2 and 3.3 a binding step of .4 and B.

After binding, we treat A and B as a single table AB and hegin the process of binding AB with another table, this is repeated until all N dimensions are hound. Some notations we shall use are: . F A  denotes the set of frequent itemsets with items from A, F A B  denotes thesetoffrequent itemsets withitems from tables A and/or B. FAB* denotes the set of frequent itemsets of the form XY, where X is either empty set or an itemset from A, and Y is either an empty set or an item- set from B with size k. E.g. F A B  = { X I ,  y ~ , z ~ y ~ ] .  E.g.

FAk denotes the set of frequent itemsets of size k from A. denotes the set of frequent itemsets in which the subset of items from A has size i and subset of items from B has size j .  E.g. suppose xi's are items from table A, and yj's are items from table B. we may have FAaB1 =  After performing "binding", we can treat the items in the combined itemsets as coming from a single dimension table. For example, after "binding" A and E ,  we vinually combine A and B into a single dimension table AB, and all items in FAB are from the new dimension table AB. We always "bind" 2 dimension tables at each step, and iterate for N - 1 times if there are totally N dimension tables. At the end all frequent itemsets will be discovered.

Figure 3 shows a possible ordering of the "bind" opera- tions on four dimension tables: A, E ,  C, D.

F A B ,  = {YIYZ, Y ? Y 3 , ~ 1 Y 1 Y Z , . l Y Z Y 3 ) .

{Xlzzyl, 23Z4Yz).

10 12 I5 16  Figure 3. An example of "binding" order  We need to do two things to combine two dimension ta- bles: (1) To assign each combination of tid from A and tid from B in FT a new tid, and (2) to set the tid in the t iddists for items in A B  to the corresponding new t i d .

Consider an example in Figure 4, for a FT relating to     A I  R  Figure 4. Concatenating tids after "binding"  4 dimensions A,  E ,  C, D, after "binding" A and E ,  the columns storing tid(A) and tid(B) would be concatenated.

Each combination of tid(A) and tid(B) would be assigned a new tid. A and E would be combined into A B .  For example, before "binding", item z1 appears in transac- tions a1,u3 and (14. t idA(z1)  = { a 1 ( 2 ) , a 3 ( 1 ) ,  a q ( l ) } .

After "binding", since a1 corresponds to new tid t l  and tz .  a3 corresponds to new tid t4, u4 corresponds to new tid 15.  Therefore t i d A ( z I )  is updated to tidAB(z1) =  Similarly, when FAB is then "bound" with F C ,  A B  is combined with C and FT would be updated again. Note that in Figure 4, the tables with attribute newl id  and the multiple fact tables are not really constructed as tables, but instead stored in a structure which is a prefix tree.

We always bind a given dimension table with the result of the previous binding because the tid of the dimension ta- ble allows us to apply the technique of a foreign key as de- scribed in the previous section. The ordering can be based on the estimated result size of natural join of the tables in- volved, which can in turn be estimated by the dimension table sizes. A heuristic is to order the tables by increasing table sizes for binding.

3 3  Prefix'keeforFT  {tl(l),tZ(l),t4(l),t5(1)1.

In Step 3.1 of the overall steps, the fact table F T  is scanned once and the information is stored into a data struc- ture which can facilitate the mining process. The data struc- ture is in the form of a prefu free. Each node in the prefix tree has a label (a rid) and also a counter. We need only scan FT once to insert each tuple into the prefix tree. Suppose we have 3 dimensions A,  E ,  C, and a tuple is us, b z ,  cz, we  enter at the root node and go down a child with label as, from a3 we go down to a child node with label b 2 ,  from bz we go to a child node labeled c2. Every time we visit a node, we increment the counter there by 1. If any child node is not found, it is created, with the counter initialized to 1.

Hence level n of the prefix tree corresponds to tid's of the nth dimension fable that would be "bound. When search- ing for a foreign tidJist, we can go down the path specified by the prefix. In this way, theforeign key and the global fre- quency in the it* iteration can be efficiently retrieved from the i + l t h  level of theprefur free. Figure 5 shows how a fact table is converted to a prefix tree.

Figure 5. Prefix Tree structure representing FT  1 5' collapse +'"" level 2 $('  level 3 4'1  Figure 6. Collapsing the prefix tree  Use of the p reh  tree - the foreign key: The prefix tree is a concise structuring of FT which can facil- itate our mining step. When we want to "bind" F A with F E ,  we have to check whether an itemset (e.g.

zl) in F A  can be combined with an itemset in F B (e.g. y l ) ,  We need to obtain the information of a for- eign key in the form of l i d l i s t  (e.g. BAid(z1) ) .  Let t i d a ( z l )  = { u 1 ( 2 ) , 4 1 ) ) .  We can find B l e y ( a 1 )  by searching the children of a ,  which are labeled bl(l), b 2 ( 1 ) .  similarly let B_key(az) = {b2(2)}. As a result, B _ t i d ( z l y l )  = B.key(hl)  U B l e y ( a 2 )  = {bl(l),bz(3)].

Collapsingthe p d i x  tree: Suppose A and B are bound, AB is the derived dimension. If B is not the last dimension to be bound, we can collapse the prefix tree by one level. A new root node is built, each node at the original second level becomes a child node of the new rootnode. The subtree under such a node is kept intact in the new tree. Figures 5     and 6 illustrate the collapse of one level in a prefix tree.

To facilitate the above, we create a horizontal pointer for  each node in the same level so that the nodes form a linked list. A unique A B f i d  is given to each of the nodes in the second level, which corresponds to the collapsed table AB.

These unique lids at all thc levels can he assigned when the prefix tree is first built.

Updating tid We need to do the following with the collapse of the prefix tree. After binding two tables A and B,  a ?derived dimension? A B  is formed. We update the tidlists stored with the frequent itemsets and items that would be used in the following iteration, so that all of them are referencing to the same (derived) dimension tahle. For example, t idA(X)  ortidB(Y) are updated t o t i d ~ ~ ( X )  or t i d ~ ~ ( Y ) .

D n rn s  3d Maintaining frequent  itemsets in FI-trees  number of dimensions number of transactions in each dimension tahle number of attributes in eachdimension table lareest size of freauent itemsets  In both local mining and global mining (Steps 2 and 3), we need to keep frequent itemsets as they are found from which we can generate candidate itemsets of greater sizes.

We keep all the discovercd frequent itemsets of the same size in a tree structure which we call an FI-tree (FI stands for Frequent Itemset). Hence itemsets of FAsB1 is mixed with itemsets of FAZBD, the first one belongs to FABL, the second belongs to FAB1.

4 Related Work  Some related work can he found in [JSOO], where the joined table T is computed hut without being materialized.

When each row of the joined table is formed, it is processed and thereby storage cost for T is avoided. In the processing of each row in the table, an array that contains the frequen- cies for all candidate itemsets is updated. As pointed out by the authors, all itemsets are counted in one scan and there is no pruning from one pass to the next as in the apriori- gen algorithm in [AS94]. Therefore there can be many can- didate itemsets and the approach is expensive in memory costs and computation costs. The empirical experiments in [JSOO] compare their approach with a base case of apply- ing the apriori algorithm on a materialized table for T.  It is shown that the proposed method needs only 0.4 to 1 times the time compared to the base case. However, there are new algorithms in recent years such as [HPYOO, SHSOO] which are shown by experiment to often run many times faster than the apriori algorithm. Therefore, the approach in [JSOO] may not be more efficient than such algorithms.

5 Experiments  We generate synthetic data sets in a similar way as [HPDWOI]. First, we generate each dimension tahle in- dividually, in which each record consists of a number of attribute values within the attribute domains, and model the existence of frequent itemsets. The parameters are listed in the following table:  I  I I lage,i numhcrof inn,aciionr with acommon itemsci d 1 domm w e  of &h alinbuie (samc for 21 attibuica) The domain SIX d of an attribute is the numher of  dif-  ferent values for the attribute, N h i s h  is the number of items derived from the attribute-value pars  for the attribute.

The value of n is set as 1000 in our expenment. After generating t m w t i o n s  for each dimension table, we gener- ate FT b a d  un parameters in the following table:  tup 1 large! frequency of thc aisooiauon N ~ C S 1.1 I numberor maximal poieniially frequenl itcmscts  I N I number of noise transactions I In constructing F T ,  there can be correlations among two or more dimension tables so that some frequent itemsets contain items from multiple dimensions. For the case of two dimensions, we want the rids associated with the same group of transactions with common frequent itemsets from one dimension table to appear at least sup times together with another group of tid sharing common frequent item- sets from another dimension table. In doing so, frequent itemsets across dimensions from these 2 groups would ap- pear with a frequency count greater than or equal to sup, after joining the two dimension tables and FT. We repeat this process for (151 times, so that IL( maximal potentially frequent itemsets would be formed (by maximal, we mean that no supersel of the itemset is frequent).

In order to generate some random noise, transactions which do not contain frequent itemset an generated. N rows in F T  are generated, in which each tid from the di- mension tables is picked randomly.

We compare our proposed method with the approach of applying FF-tree algorithm [HPYOO] on top of the joined tahle T.  We assume T is kept on disk and hence requires VO time for processing. FE-tree requires two scanning of T during the FP-tree mining. The YO time required is up to 200 seconds in our experiments. It t u n s  out that the table join time is not significant compared to the mining time.

All experiments are conducted on SUN Ultra-Enterprise Generic.106541-18 with SunOS 5.7 and 8192MB Main Memory. The average disk U 0  rate is 8MB per second.

Programs are written in C+t. We calculate the total execu- tion time of mining multiple tables as the sum of required     I*  0 5 10 15 20 25 30  Figure 7. Running time for (A$) related and (A,B,C) related datasets  CPU and WO times, and that of mining a large joined table as the CPU and WO times for joining and FP-tree mining.

For the local mining step in our approach, we use a verti- cal mining technique as in FORC [SHSOO]: frequent item- sets of increasing sizes are discovered The t i d l i s t  of the itemsets are used to generate size k + 1 frequent itemsets from size k frequent itemsets, which is by intersecting the t i d l i s t s  of pairs of size k frequent itemsets with only one differing item. In the experiments, we compare the running time of mas1 (our proposed method, implementing t i d l i s t as a linked list structure), masb (our proposed method, im- plementing t i d l i s t  as a fixed-size bitmap and an array of count), and fpt (the join-beforsmine approach with FP- tree) with different data setting in 3 dimension tables A, B, C and a fact table FT. In most cases, mosb runs slightly faster than mad, but needs a b u t  10 times more memory.

In the first dataset, we model the situation that items in A and B are strongly related, such that frequent itemsets contain items across A and B, while items in C are not involved. In such cases, transactions containing frequent itemsets from A and B can be related to Br transactions in C randomly. Br is set to 100 in all of our experiments reported here. (We have varied the value of Br and discov- ered little change in the performance.) The default values of other parameters used are :  numkrof  mnramun\ in the joined whlc number of atmbutes in each dimension whlc  I 50K I 10  When we increase the number of items, the running time of f p t  increases steeply, while that of both masl and masb would increase almost linearly. Running time of FP-tree  n u m b e r d a ~ h s s c h d r d 0 " I a t l e  (b)  Figure 8. Running  time for mixture datasets  grows exponentially with the depth of the tree, which is de- termined by the maximum number of items in a transaction.

In this case, performance of our proposed method outper- foms  FP-tree, especially when the number of items in each transaction is large. (see Figure 7(a))  When the number of transactions in the joined table increases, running time of both methods would increase greatly. masl and masb are about 10 times faster than Jpt (see Figure 7(b)). We also vary the percentage of random noise being included in the datasets, (see Figure 7(c)), both masl and musb are faster than Jpt.

In the second dataset, we model the case that -4, B, Care all strongly related, so that maximal frequent itemsets al- ways contain items from all of A, B and C .  Compared with the previous dataset, performance of our approach does not vary too much, while the running time of fpi is faster in some cases (Figures 7(b) and (c)). With the strong correla- tion, there would he less different patterns to be considered and the FP-tree will be smaller. However, we believe that in real life situation such a strong correlation will be rare.

In real life application, there are often mixtures of rela- tionships across different dimension tables in the database.

In the third group of dataset, we present data with such mix- ture. In particular, 10% of transactions contain frequent itemsets from only A, B, C, respectively, 15% contam fre- quent itemsets from AB, BC, AC respectively, 10% con- tain frequent itemsets from ABC, and 15% are random noise. We investigate how the running times of mas1 and f p t  vary against increasing number of items in each trans- action, and increasing number of transactions in  T.

In Figure 8(a), we vary the number of transactions from 20K to 60K, while kzeping the number of attributes in each dimension table to he 30 In Figure 8(h), we vary the num- ber of attributes in each dimension table from 10 to 60, and keep the number of transactions to be 30K. In this case, run- ning time vi fpt grows much fastertban our approach. This demonstrates the advantage of applying our method, which would he more significant when we have more dimension tables so that the number of items in the T will be large.

6 Conclusion  We propose a new algorithm for mining association rules on a star schema without performing the natural join.

We show by experiments that it can greatly outperform a method based on joining all tables even when the naive approach is equipped with a state-of-the-art efficient algo- rithm.

Our proposed method can he generalized to be applied to a snowflake structure, where there is a star structure with a fact table F T ,  but a dimension table can be replaced by an- other fact table FT? which is connected to a set of smaller dimension tables. We can consider mining across dimen- sion tables related by FT? first. We then consider the result as a single derived dimension, and continue to  process the star structure with F T .  This means that we always mine from the ?leaves? of the snowflake.

Our current experiments assume that the data structures we use can he kept in main memory. It will be of interest to study the case where disk memory is required for the inter- mediate steps. Since disk access is more expensive and our intermediate structure has been designed to he more com- pact than the fact table, we expect good performance to be found for the proposed approach in such cases. We have  not examined in our experiments the cases where the num- her of dimension tables is large. More study will be needed for these considerations.

In general, F T  can contain attributes other than rid from dimension tables. In this case, we group all these attributes, put them in a separate new dimension table, and apply the same techniques to mine it as other dimension tables.

Acknowledgements This research is supported by the RGC (the Hang Kong Research Grants Council) grant UGC REECUHK 4179/01E. We thank Yin Ling Cheung for her help in the experiments and performance analysis.

