Semantics Oriented Association Rules

Abstract - It is well known that relational theory carries very little semantic. To mine deeper semantics, additional modeling are necessary. In fact, some ?pure? association rules are found exist even in a randomly generated data. In this paper, we consider the relational database in which every attribute value bas some additional information, such as price, fuzzy degree, neighborhood, or security compartment and levels. Two types of additions are considered: one is structure added, the other is valued-added. Somewhat a surprise, the additional cost in semantics checking is found very well compensated by the pruning of non-semantic rules.

1. INTRODUCTION  In relation theory, attribute domains are Cantor sets; the interactions among members of real world objects are ?forgotten.? For data mining, additional modeling of attribute domains are needed to organize deeper semantics of the data.

We will term such addition to the existing data model; semantic added modeling. In this paper, we will consider two aspects; one is structure added, the other value added.

2. MOTIVATION- ASSOCIATION RULES IN RANDOM DATA.

In the experiment [4], a totally discrete data are randomly generated. Somewhat a surprise, we find some association with substantial supports of length 2 (though they did not meet our artificial high requirement of supports). This computation implies that frequency itself may not be an adequate criterion for meaningful pattems; see the Table 3 in Experiments Report, Section 7. So semantic modeling seems necessary for database mining.

3. SEMANTIC ADDED MODELING  What would be the ?correct? mathematical structure to capture the semantics of real world objects? This is a question that has many ad hoc answers; we decide to consult the history. Model theory of the first order logic uses a cantor set together with relational structures and functions to model the real world. We will follow it; attribute domains are assumed to have all such structures. Previously we have explored the simplest structure, namely, one binary relation is added to each attribute domains [5], [6], [9], [lo], [12], [ l l ] ,  [7]; totally, they induce finitely many binary relations on the universe (of eneitites). In [3], we consider one real valued function for each domain. In this paper, we combine the two,  T. Y. Lin  Department of Mathematics and Computer Science San Jose State University,  San Jose, CA 95 192 tylin@cs.sj su.edu  namely, on each domain we have one binary relation and one function.

4. STRUCTURE ADDED DATA MODEL BINARY RELATIONS  We will examine the case each attribute domain is assumed to have one binary relation. Its geometric corresponding concept is called a binary neighborhood system (BNS). In the case of equivalence relation, a BNS is a partition. The binary relation on each attribute domain in turns induced a binary relation on the universe. So on the universe, there are finitely many binary relations. We will examine the impact of such added structure on data mining.

4.1 Crisp/Fuzzy binary neighborhood systems  4.1.1. A binary relation (BR) is a subset  B c V x U .

It defines a set  called elementary (basic or binary) neighborhood at p E V  4.1.2. A binary neighborhood system (BNS) denotes either the map B: p + Bp, or the family {BP 1 p E V}. The map B has also been called a binary granulation(BG). The set V together with BNS is called a BNS-space on U or simply BNS-space if V and U is the same.

Proposition. BNS, BR and BG are equivalent to each other.

4.1.3. The induced equivalence relation: Note that BG,  B:p  E V + B p ?  2 u ,  induces a partition as follows: The collection of complete inverse images B-l(Bp) forms a partition on V, and hence an equivalence on V. We use EB to denote this equivalence relation. We may drop the subscript, if B is understood.

4.1.4. Fuzzifications: Binary relation and neighborhood system can be fkzified; in other words, instead of being a  mailto:ewlouie@almaden.ibm.com   subset of V x U, it could be a fuzzy subset (a membership h c t i o n  FB: V x U +[O. 11 )  4.2 Structure Added Data Models  A traditional relation instance can be viewed as a knowledge representation that maps each entity to a tuple of attribute values. Table 1 illustrates the notion 0f.a relation instance on the universe U={ul, u2, u3, u4, u5 }.

U I K I (S# i STATUS j CITY) UI  I + 1 (SI / TWENTY / CI)  Table 1. An Information Table; arrows and  ~3 I j I (S3 i TEN Q I j I (S4 i TWENTY i C1)  I US I j I (S5 j THIRTY C3) Table 1. An Information Table; arrows and  parentheses will be suppressed  In geographical attribute domain, one can use binary relation to capture the "near" semantics. So on CITY attribute, we assume a binary relation holds in the domain; see Table 2  CITY I CITY C1 I c1  I c 1  I c 2  I I c 2  I c 1  I  L. I J  t C 3  I c 2 I c 3  I c 3  Table 2 "near"-Binary Relation  In numerical attribute, such as STUATS, we have the order binary relation ''5.'' Next, we express both B and "5. " in BNS format:  Note that attributes can be regarded as projections. The CITY attribute is a map, denoted by CITY again.

CITY: U + Dom(CITY),  Which, in each tuple of Table 1, assigns the element in the first column to the element in the last column. The inverse map CITY-1 induces a BNS on U. So we have  Each binary relation, say B, induced an equivalence relation E. In this case the neighborhood is an equivalence class  We should have similar results for "4'; we denote it by 0 (order binary relation).

Definition. The 3-tuple  (U, Aj, Dom(Aj), j=1,2,. . ., n)  is called structure added data model, where U is the universe, AJ is the attributes.

For the example in Table 1, the structure added model is  (U, B, {Cl, C2, C3}, 0, {TEN, TWENTY, THIRTY})  4.3. The Impact ofAdded Structure to Data Mining  In mining such a data model, first concern is the cost in checking the added structure. So experiments have been conduct in [4]. Somewhat a surprise, the cost is well compensated by the saving. It does have cost in checking the continuity of association rules, however, the pruning of non- continuous rules save the time in computing the long rules.

One beauty of continuity is that the compositions of continuous rules are also continuous, so the only cost is at the length 2.

0-7803-7280-8/02/$10.00 02002 JEEE 957    Table 4 is generated with some embedded semantics. That is, some associations are embedded in the algorithm of generating the test data. From the table it is clear finding ?pure? association rules is expensive.

In Table 5, some neighborhoods (one binary relation) are generated. Based on such a structure, the cost of finding continuous association rule is greatly reduced. From the artificial data, it ?proves? that this BNS theory is promising.

The next step is to test on real world applications.

5. VALUED ADDED DATA MODEL  In this section we will consider the case the function valued will be part of the model  5.1. Valued Added Granular Data Model  Definition. The 4-tuple  (U, Aj, Xj, Dom(Aj), j=1,2,. . ., n)  is a Value Added Granular Data Model or VA-Granular Data Model, where for each AJ, a value add function is defined on each domain Dom(AJ) :  X j  : Dom(Aj) + M  where M is either a Cantor set W, the security lattice SC, real numbers, or [0, 11.

Proposition 1. If M is 2? then X j is a binary neighborhood system, which is equivalent to a binary relation.

Proposition 2. If M is SC, the security lattice then Xj is a classification and the granular data model is a MLS data model.

Proposition 3. If M is real number, then Xj is a random variable.

Random variable is not a variable varies randomly, it is merely a function whose numerical values are determined by chance; please see [3] for connecting the mathematics to intuition.

Proposition 4. If M is [0, 13, then X j could be a grade of fizziness. In this case the granular data model is a fizzy database.

5.2. The Impact of Added Structure Model to Data Mining  The difference between this and last sections is that the values of the function do participate in computing. For example, the existing of real valued function implies the existing of a neighborhood system on an attribute domain D (a topological space).. However, the imposed constraints are imposed more than on the structure of D, we use the real values. Table 5 and 6 say the computing of VA-association is quite expensive, if we use values alone. We would like to comment that by assigning the ?nearest? or ?smallest? neighborhood at each point, we have a BNS. In next, project, we will use this BNS, called nearest neighborhood system, and values.

6. PATTERNS PRESERVING STRUCTURES  We collect some generalized ?standard patterns: [5].

Let A and B be two attributes of a relation-with-additional- semantics. Let c, d be two values of A and B respectively.

Let NEIGH(c), NEIGH(d) be the respective elementary granules . It is clear that c = NAME(NEIGH(c)) and d = NAME( NEIGH(d)).

Let Card( .)be the cardinal number of a set 0.

Structure added association rules  1 A formula c + d is a continuous or semantic decision rule, if the inclusion NEIGH(c) E NEIGH(d) is continuous.

2 A formula A + B is a continuous or semantic universal decision rule, iff V c E A 3 d E B such that NEIGH(c) cNEIGH(d). This rule is equivalent to extension hct ional dependence..

3. A formula c + d is a robust continuouslsemantic decision rule, if NEIGH(c) ENEIGH(d) and Card(Pc) 2 threshold [SI.

4 A formula c + d is a soft continuouslsemantic decision rule (strong rule), if NEIGH(c) is softly included in NEIGH(d), NEIGH(c) 0 NEIGH(d) [12].

5. Weak association rule: A pair (c, d) is an association rules, if Card (NEIGH(c) n NEIGH(d)) 2 threshhold.

6 .  A pair (c, d) is said to be in a relation (or database), if it is a sub-tuple of a tuple that belongs to a relation (or database).

7. A pair (c, d) in a given relation is one-way (c +d) continuous (or semantic) if every x E Bc, there is at least one y E Bd such that (x, y) is in the given relation.

8. A pair (c, d) in a given relation is a two way continuous (or semantic) if (c +d) and (d +c) are both continuous.

0-7803-7280-8/02/$10.00 02002 IEEE 958    9. Semantic association rule: A pair (c, d) is an association rule iff the pair is an association rule and two way continuous.

10. Soft association rule: A pair (c, d) is a soft association rule, if Card (NEIGH(c) n NEIGH(d)) 2 threshhold. [5], [61  Valued added association rules 1 1. In-the-average-association rule: Two attributes Ai and Aj is associated in the average, if JE(Xi)- E(Xj)l I where E(.) is the expected value, and I I is the absolute value.

12. Fuzzy decision rule: A formula c + d is a fuzzy decision rule, if E, G Ed and X(c) I X(d), where E, and Ed are the equivalence classes of c and d. In other words, c and d are the names of the equivalence class E, and Ed  13. Security leak rule: A formula c + d is a security leak decision rule, if E, c Ed and X(c) I X(d.).

14. Value added association rule (VA-association rule) [3]  14.1. Sum-version: A granule (sub-tuple) b=(bl nb2  n. . .

n b  ) is a q-VA-association rule if Sum(b) 2 sq , where  where xjo = P?(bj).

q  sum@) = ~j ,j,*p(xJo) = ~ q j = 1  P?Cbj)*lbl/lUI,  14.2 Min-version: A granule (sub-tuple) b=(bl nb2  n. . .

n b  ) is a q-VA-association rule if Min(b) ) 2 sq , where Min(b) =Minj xjo*p(xjo) = Minq i=lh(bj)*lbl/lUI  q  14.3 Max-version: A granule (sub-tuple) b=(bl nb2  n. .

n b  ) is a q-VA-association rule if Max(b) ) 2 sq , where  Max(b) =Maxj xJo*p(xJ0) = Maxqi=l(f(b-i)*lbl).

14.4. Traditional: The Max and Min-versions are the traditional one iff the profit function is the constant=l.

Recall that we are concerning only with the supports, so association rules have no directions. Since we are using granules, a q-association rule is equivalent to a q-large granule; the former q means the length of tuple, the latter q means the length of the intersections. The frequency of an itemset is the cardinal number of a granule (= a finite intersection of elementary granules)  In general, there are no apriori criteria for value added case. However, if we require the thresholds increase with the lengths, that is,  Then there are apriori criteria: q-large implies all sub-tuples are (q-+large, where i 2 0.

Value added granular data model allows us to import many probability theory into data mining. We list a sample here and more work will be reported in the near future.

7. EXPERIMENT REPORTS  Here are the ACRONYMNs: q= length; c =candidate; a=association rules; s=support count; 6 = time need to generated next rows (in seconds)  7.1. Random Data  Table 3 is the results of finding association rules on randomly generated data:  1. The relation has 100000 rows and 16 columns; we  2. The distinct attribute is limited to 10; there are real world require the support to be 12000 items.

medical data meet this constraints.

Table 3. Randomly generated data  7.2. Structure Added Computing  Table 4 and 5 is the same as Table 3 except the distinct attribute is limited to 20.

0 . 0 0 0 s 0.661s 0 . 0 0 0 s  120 0 .000s 120 0 . 0 4 0 s  I I I 1 2 0  I 0 . 0 0 0 s  I 13 I 5 6 0  I I I 0 . 0 0 0 s  I I I  I560 I I 0.190s I I I  I I560 I 0 . 0 0 0 s  1 14 I1820 I I I 0.090s I  0-1803-1280-8l02/$10.00 02002 IEEE 959    I I 1 8 2 0  I I 0 . 7 4 1 s  1 6 I 1 0 6  I 0 . 0 1 0 s  1 0 6  0 . 0 0 0 s  43  0 . 0 0 0 s 43  0 . 0 3 0 s  I I 4 3  I 0 . 0 0 0 s 8 I 1 0  I 0 . 0 0 0 s    1 1 4 4 0  7 6 . 3 3 0 s 1 1 4 4 0  7 . 0 2 0 s  1 1 4 4 0  0 . 0 0 0 s 1 2 8 7 0  1 6 4  - 4 7 7 s  9 1 10 0 . 0 0 0 s  0 . 0 0 0 s 1 0 . 0 0 0 s   1 1 4 4 0  8 . 6 3 3 s 1 1 4 4 0  0 . 0 0 0 s  8008  1 4 9 . 7 9 5 s  I I I 0 . 0 0 0 s  1 2  4 3 6 8  3 . 9 4 6 s 4 3 6 8  0 . 0 0 0 s  1 8 2 0  1 . 7 8 3 s 1 8 2 0  1 1 . 6 1 6 s  1 4  5 6 0  0 . 5 9 1 s 5 6 0  0 . 0 0 0 s  1 2 0  0 . 0 7 0 s 9 c 0 0  S a 6 0 . 0 0 0 s  9 5  0 .010s   12 0 0 . 1 4 0 s 1 2 0  0 . 0 0 0 s  1 6  0 . 0 0 0 s 1 6  0 . 0 2 0 s  1 6  0 . 0 0 0 s  a 4 9  4 9  0 . 0 0 0 s  500 0.OlOS 0 . 0 7 0 s  0  0 0 . 0 0 0 s  0 0 . 0 0 0 s  0 1 . 4 5 2 s 0 0 . 0 1 0 s  1 3 9 9 3  2 5 . 3 9 7 s  6567 1  1 0 . 0 0 0 s 0 . 0 0 0 s  65535 65535 725.563s  5 I 2 7 8 3 2  I I 1 1 9 8 . 8 8 6 s  l o  I 2 . 8 2 4 s 1 1 5 2  0 . 6 7 1 s  1 6  1 6  0 . 0 0 0 s 2 1 2 0  0 . 0 7 0 s  0 0 . 0 2 0 s  0 2 . 6 1 4 s 4 3 3 0 6 6  3 3 8 . 6 4 7 s  1 2 0  0 . 0 4 0 s  2 0 . 0 1 0 s 3 1 1 9 4 4 5  I 1 1 5 0 . 7 1 6 s  I o  I 1 . 3 1 2 s  1 0 2 2 2 8  0 0 . 1 8 0 s 1 9 7  0 . 0 0 0 s  0 . 0 0 0 s 4 9  3 3 5  7 2 6 . 9 9 5 s  0 . 0 0 0 s 1 . 0 8 2 s  4368  2 . 0 7 3 s 4368  0 . 0 0 0 s  1 3 . 3 3 9 s I I  I 8 0 0 8  I I 4 . 3 7 6 s  I I I I I8008 I 0 . 0 0 0 s  I  I I10 I I 0.010s I  I 1 2 8 7 0  I I 8 . 7 9 2 s I I 1 2 8 7 0  I 0 . 0 0 0 s  110 l o  I I I 0 . 0 0 0 s  I  1 9  I 1 1 4 4 0  I I I 1 9 9 . 5 9 7 s  I I l o  1 0  I 0 . 0 0 0 s  I  semantic rules is inexpinsive I I  I I 8 0 0 8  I 0 . 0 0 0 s  I  7.3. Valued Added Computing  Table 6 is the results of finding association rules based on data with real valued function:  1. The relation has 500 rows and 8 columns; we require the  2. The distinct attribute is limited to 20.

weights greater than 5.9 I I I I 1 8 2 0  I 0 . 0 0 0 s  I  1 7  I 4 0 0 0  I I I 0.561s I 1 6  11 I I I 0 . 0 0 0 s 1 11 I 0.010s I I I l o  I 0 . 4 6 0 s  I  I I  Table 4: Finding ?pure? association rule is expensive  1 0  ( 0  I I I 0 . 0 0 0 s  I  I 5 4  I 0 . 0 0 0 s  I 1 3  I118 I I I 0 . 0 0 0 s  I  I I  I I 8 7  I 0 . 0 1 0 s  I 0 . 0 0 0 s 0.011s 0 . 0 6 0 s 0 . 0 0 0 s 0.010s  1 6 6  0 . 0 8 0 s  0-7803-7280-8/02/$10.00 02002 lEEE 960    8. CONCLUSIONS  The advantage of data mining by granular computing are: 1. it is fast in mining classical relations, granular computing  is faster than Apriori [13], [14] because the "database scan" are replaced by bit operations.

2. the use of granular computing is extend to Yea1 world" databases (semantically richer relations); its cost is well compensated by pruning. Such extra semantics may be able to use for analyzing unexpected, peculiar rules [ 151.

