

Abstract?Cloud services are fundamentally supported by Data Center Networks (DCNs). With the fast growth of cloud services, the scale of DCNs increases rapidly, leading to a big concern on system scalability due to multiple constraints. Based on optical switching and transmission, this paper proposes a scalable DCN architecture with distributed placement of optical switches and server racks at different nodes in a given optical network. This solves the scalability issue by relaxing power and cooling constraints, and reducing the number of (electronic) switches using high-capacity optical switches, as well as simplifying internal connections of the DCN using wavelengths in the optical network. Moreover, the distributed optical switches provide service access interfaces to demands in vicinities, and thus reduce the transmission cost of the external traffic. The major concern is the additional delay and cost for remote transmissions of the DCN internal traffic. To this end, we study the component placement problem in DCNs under a given set of external demands and internal traffic patterns. By leveraging among multiple conflicting factors such as scalability and internal overhead of the DCN as well as the transmission cost of external traffic, we propose an ILP (Integer Linear Program) to minimize the overall cost of a DCN while satisfying all service demands in the network. This addresses both scalability and cost minimization issues from a network point of view.

Keywords-Cloud services, data center networks (DCNs), optical switching, placement, scalability.



I.  INTRODUCTION Most of the existing and emerging cloud services are  provided by Data Center Networks (DCNs) [1-3]. Basically, a DCN is a factory-scale and massively parallel computing and storage resources. It consists of a huge number of servers organized in racks, which work in parallel with data exchange via a switch network such as Clos [4-6] or Fat-Tree [7-9].

With the rapid growth of cloud services, the scale of DCNs is getting large with exponentially increased number of servers [10]. The fast scale expansion of DCNs leads to a big concern on system scalability, which is constrained by many environmental factors such as site and warehouse choice, system cooling, power supply capacity, as well as energy  efficiency and carbon dioxide footprint, etc.

Other than environmental factors, scalability is constrained  by the internal architecture of DCNs as well. Existing DCNs generally adopt electronic switches and routers with multiple stages of O/E and E/O transceivers, leading to bandwidth bottleneck and high power consumption, as well as complex interconnects and bulky systems due to the limited modular switching capacity and thus a large number of switches and ports. On the other hand, it is shown that over 70% power can be saved in DCNs if electronic switches are further replace by optical ones [11]. Due to the high switching capacity of optical switches, the number of switches can be decreased as well to reduce the system size with much enlarged system bandwidth.

In recent years, several all-optical or hybrid electrical- optical switches have been demonstrated, including c-Through, Helios, DOS, Proteus, Petabit and IRIS, etc. A complete survey on this can be found in [12]. All of those optical switches target at possible applications in future high performance DCNs.

Though optical switches generally have a reconfiguration overhead for adjusting the tunable optical components and synchronizing the system, performance guaranteed switching (i.e., no packet loss and packet delay is bounded) can be achieved using the state-of-the-art traffic scheduling algorithms [13-16]. In this paper, we adopt optical switches in the proposed architecture.

As the current architecture is not viable for future high performance DCNs, it is desirable to cluster the server racks and switches into multiple sets (defined as component sets) and put them at different places, especially different nodes in an optical network (municipal, regional or national). As such, power supply, system cooling and warehouse accommodation can be handled in a distributed and less bulky manner, and wavelengths in the optical network can be used to provide DCN internal interconnects.

Challenges also exist. First, placing optical switches and racks at different network nodes introduces additional delay to the DCN internal traffic due to optical transmissions in fibers, which is about 5ms per 1000km. The delay is generally tolerable in municipal or regional area networks, but could be notable if the switches and racks are separated across a few thousands kilometers in a national area network. In the latter case, the proposed idea can be used for more delay-tolerant DCN processes, such as file replication in GFS (Google File System) [17] and MapReduce [18].

Another issue is the long-haul transmission cost of the DCN internal traffic. According to the statistics from Microsoft  Jie Xiao*, Bin Wu*, Xiaohong Jiang?, Achille Pattavina?, Hong Wen? and Lei Zhang* *School of Computer Science and Technology, Tianjin University, Tianjin, P. R. China ?School of Systems Information Science, Future University Hakodate, Hakodate, Japan  ?Dipartimento di Elettronica e Informazione, Politecnico di Milano, Italy ?NKLC, University of Electronic Science and Technology of China, Chengdu, P. R. China  E-mails: {jiexiao001, binwu.tju}@gmail.com, jiang@fun.ac.jp, pattavina@elet.polimi.it, sunlike@uestc.edu.cn, lzhang@tju.edu.cn  Scalable Data Center Network With Distributed Placement of Component Sets in Optical Networks  This work is supported by the Major State Basic Research Program of China (973 project No. 2013CB329301 and 2010CB327806), the Natural Science Fund of China (NSFC project No. 61271165), the Research Fund for the Doctoral Program of Higher Education of China (RFDP project No.

20120185110025 and 20120185110030), and the Fundamental Research Funds for the Central Universities. The work is also supported by and carried out in Tianjin Key Laboratory of Cognitive Computing and Application, School of Computer Science and Technology, Tianjin University..

[19], internal traffic in current DCNs is about four times more than the external traffic. While distributed placement increases the transmission cost of internal traffic, it may also reduce that of external traffic (for service requests and delivery) due to the distributed DCN service interfaces placed closer to clients. As a result, finding a suitable set of nodes to place proper component sets is an important issue for balancing internal and external transmission costs of the DCN. Meanwhile, it is desirable that a component set (placed at a single node) consists of those racks with heavy inter-rack traffic, while leaving light-loaded internal traffic for long-haul transmissions.

So, rack clustering is also an important issue. Those factors make system cost minimization a very complex yet interesting optimization problem.

In this paper, we propose a scalable DCN architecture with distributed placement of optical switches and racks in a given optical network. As mentioned above, this solves the current centralized power supply and system cooling issues and simplifies DCNs by using high-capacity optical switches and high-speed wavelength connections embedded in the optical network, as well as reducing power consumption of the DCN.

To support the proposed architecture, we further study the switch and rack (i.e., component) placement problem with predefined external demands and internal traffic pattern of the DCN. An ILP (Integer Linear Program) is proposed to solve the placement problem with the objective of minimizing the overall system cost while scaling the DCN. The long-haul transmission delay of the DCN internal traffic is translated into a countable cost factor, such that it can be taken into account in the overall cost minimization.

The rest of the paper is organized as follows. Section II describes the proposed architecture and the placement problem.

Section III formulates the ILP for optimally solving the problem.

Numerical results are presented in Section IV. We conclude the paper in Section V.



II. ARCHITECTURE AND PROBLEM  A. The Proposed Architecture As discussed in Section I, our key idea is to spread the DCN  component sets over a certain area of the network. Fig. 1 illustrates the proposed architecture, where the components inside a dashed ellipse form a component set to be placed at a single node. Basically, the network nodes denoted by the ellipses and the inter-node fiber connections are embedded in the optical network, and the DCN in the rectangle is just to show the functionality of the components in a logical manner.

Besides, it is possible that the content and core switches are placed at the same node as the server racks and aggregation switches, though they appear to be separate in Fig. 1. This depends on the cost minimization process as discussed later.

In Fig. 1, the ToR (Top-of-Rack) and content switches are electronic routers or OpenFlow [20] switches, whereas the aggregation and core switches are optical crossbar or packet switching fabrics. An aggregation switch is always collocated with a set of server racks at the same node and is responsible for data switching among them, and the traffic in the same rack is switched by the ToR switch. The content and core switches are always collocated. The former provides service interfaces for external requests and the latter for inter-node data switching of the DCN internal traffic. The core and aggregation switches are interconnected by optical wavelengths and fibers. Logically, they form a typical multi-stage switch network based on folded- Clos (or Fat-Tree) structure [7-9].

Traffic is divided into external traffic (for service requests and delivery) and DCN internal traffic by content switches.

Each node in the network has a specific amount of service demands, which can be served by a nearby content switch to minimize the external transmission cost. Nevertheless, routing demands to different content switches provides a mechanism to balance the loads over the corresponding core switches, by slightly increasing the transmission cost of the external traffic.

B. The Placement Problem A major task of our work is to make the DCN scalable by  distributing the component sets to different nodes. Accordingly, a scalability related cost is defined to increase with the size of the component sets. Distributed placement reduces this cost but leads to additional delay and transmission cost for the DCN internal traffic (defined as the DCN internal overhead). Such a tradeoff is further complicated by the external demands and internal traffic patterns of the DCN, which require the switches and racks to be clustered into proper component sets and be placed at suitable nodes.

More specifically, it is desirable to find a set of nodes in vicinity to each other for component set placement, such that the transmission cost and delay of the DCN internal traffic can be minimized while bulky component sets can be avoided. On the other hand, demands at individual nodes need to be served with small transmission cost for external traffic, which translates to a more distributed placement of service interfaces and component sets. For a specific DCN service, the worst-case internal traffic pattern among the racks is relatively stable. It can be estimated and planned at the network planning stage.

Based on the internal traffic pattern, it is desirable to put those racks with heavy inter-rack traffic into the same component set placed at a single node, such that data switching among them can be locally handled without incurring additional long-haul  Optical fibers and wavelengths  ?  Optical network & Internet  Rack  DCN clients  ToR Switch  Optical core switch Content switch  Fig.1. Distributed placement of optical switches and racks in DCNs.

DCN  Optical aggregation switch  Network node        transmission cost. As a result, each component set tends to be large, but this contradicts the scalability. Moreover, component placement is constrained by the specific network topology as well. Other than properly clustering the racks, we still need to find a suitable set of nodes in the network and match each of them with one or several component sets for placement. All the above conflicting factors (external and internal traffic, rack clustering and node mapping, and scalability) are coupled together, leading to a very complex yet interesting cost minimization problem.

Given a set of DCN service demands at the nodes in a network and the service-oriented internal traffic pattern among the racks of a DCN, we need to solve the following set of coupled sub-problems with the objective of minimizing the overall system cost by taking the scalability related cost and the DCN internal overhead into account: 1) determine the number of content and core switches required; 2) cluster the switches and racks into proper component sets; 3) place the component sets at suitable nodes in the network; 4) find the service request and delivery routes from the demanding nodes to the DCN service interfaces (i.e., the content switches); 5) figure out the switching and routing scheme for the DCN internal traffic; and 6) achieve load balancing among the core switches.



III. ILP FORMULATION To quantify the increasing cost for bulky component sets,  we assume that the basic cost of investment of a component set at a particular warehouse is characterized by the scalability related cost function in Fig. 2. If the number of server racks in the component set exceeds a specific limit (e.g., , ? 4 in Fig. 2), the basic cost (e.g., system cooling, power supply and warehouse rent, etc.) will step up accordingly. For simplicity, we also let the scalability related cost include the costs of ToR and aggregation switches as well as optical interconnects inside the component set, since they are generally proportional to the number of racks in the component set as well.

Based on notations in Section III.A, we formulate the optimal ILP (Integer Linear Program) in Section III.B. It minimizes the overall system cost for deploying a DCN in a given optical network under a set of predefined external demands and DCN internal traffic. By translating the long-haul transmission delay of the DCN internal traffic into a cost factor comparable to others, the ILP takes into account the DCN internal overhead and the scalability related cost as in Fig. 2.

A. Notation List Inputs:  V:  The set of all nodes in network G(V, E).

E:  The set of all bidirectional links in network G(V, E).

R:  The set of all server racks in the DCN.

Q:  The maximum switching capacity of a core switch.

:  The cost of a pair of core and content switches.

:  Basic cost saving if a pair of core and content switches  is placed at the same node as a set of server racks.

: Transmission cost of per unit traffic between nodes u  and v in the optical network. In this work, it is assumed as the distance between nodes u and v, and = .

:   Cost scaling factor for taking the long-haul transmission delay of DCN internal traffic into account. > 1 and  is the cost of the total DCN internal overhead for per unit internal traffic transmission between nodes u and v, where the cost of delay counts for ? 1 .

:  DCN service demand at a network node ? .

:  The bidirectional DCN internal traffic load between two  racks i and j, and = .

K:   The number of shadowed blocks in Fig. 2.

? :   Step size of the basic cost increase as shown in Fig. 2.

:  Boundary of the number of racks in a component set as  shown in Fig. 2.

:   A predefined constant and ? ? ? ?? .

:   A predefined constant and ? ? ? .

:   A predefined constant and ? | |.

Variables: :  Binary variable. It takes 1 if there is a pair of core and  content switches at node ?  and 0 otherwise.

: Binary variable. It takes 1 if rack ?  is placed at node ?  and 0 otherwise.

:  Binary variable. It takes 1 if a pair of core and content  switches is collocated with a set of server racks at node ?  and 0 otherwise.

: Binary variable. It takes 1 if the number of racks at node ?  exceeds ( ? ) and 0 otherwise.

: Binary variable. It is defined in , ? | <  and , ? | ? . It takes 1 if rack i is placed at node m  and j at n, and 0 otherwise.

:  Nonnegative integer variable. It is the demands (counted  in wavelengths) at node ?  that are served by the content and core switches at node ? .

: Nonnegative integer variable in , ? | < . It is the total amount of DCN internal traffic between nodes m and n (counted in wavelengths) that are switched by the core switch at node ? .

B. ILP Formulation minimize ? + ?? + ??   + +?? ,? ? ? (1) Subject to  ? = 1, ? ? ; (2) Fig. 2. Scalability related cost function.    = 0 ?  ? ?  ?  Number of racks  Basic cost      ? + ? 1,  ? , ? : < , ? , ? : ? ; (3)? 1 ? ,? , ? ? ; (4)? 1 ? , ? ? ; (5)? 1 ? ? , ? ? , ? ; (6) ? = ? ,? , ? , ? : < ; (7)  ? + ? ,? ? , ? ? ; (8) ? = , ? ? ; (9) ? , ? ? ; (10)? ? , ? ? . (11)  Objective (1) minimizes the overall system cost. The first  term is the cost of all content and core switches. The second term is the sum of the basic cost at all nodes where the DCN component sets are placed, where bulky component set is punished more according to the scalability related cost function as illustrated in Fig. 2, and  = 0 if no rack is placed at node m . The third term formulates the total transmission cost of all external traffic. The DCN internal overhead due to long-haul transmissions is counted in the fourth term, where both delay and transmission costs of all internal traffics are counted by using . Finally, a cost saving is deducted as formulated in the fifth term if a pair of content and core switches are collocated with a set of server racks due to resource sharing. Besides, no matter how the racks are clustered and placed, the total cost of all racks is a constant and thus ignored in (1).

Constraint (2) says that each rack is placed at one node. If rack i is placed at node m and rack j at node n,  will take the value of 1 to identify this case as formulated in (3). Constraints (4)-(5) check whether a pair of content and core switches are placed at node u. If any inter-node DCN internal traffic is switched at node u as formulated in (4), or any external demand is served by the DCN service interface at u as formulated in (5), then a pair of content and core switches are placed at this node.

Constraint (6) identifies the zone (i.e., the shadowed block in Fig. 2) where the number of racks at node m falls into, such that  Node (v)  0: Seattle  1: Palo Alto  2: San Diego  3: Salt Lake City  4: Boulder  5: Lincoln  6: Houston  7: Urbana  8: Ann Arbor  9: Atlanta  10: Pittsburgh  11: Ithaca  12: Princeton  13: Washington                (c)  Demand at each node and demands routing.

Demand routing 0 ? 7 1 ? 7 2 ? 7 2? 13 3 ? 7 4 ? 7 5 ? 7 6 ? 13 7 ? 7 8 ? 8 9 ? 10 10 ? 10  (d)  Link cost in distance (kilometers).

Link   Cost Link   Cost Link   Cost  (1, 2)   834 (1, 3)   1152 (2, 6)   2520 (3, 4)   684  (3, 8)     2820 (4, 5)     870  (6, 13)   2364 (7, 10)   846  (8, 11)     720 (8, 12)     942 (9, 10)   1008 (10, 11)   438  (0, 1)   1338 (0, 2)   2056 (0, 7)   3408 (4, 6)     1746  (5, 7)     864 (6, 9)     1350 (10, 12)   540  (11, 13)   468 (12, 13)   312  (7, 8)          8           6  u  (7, 10)        10         28  (m, n)  (f)  Inter-node DCN internal traffic.

(7, 12)        10         5  (7, 13)        13         8  (8, 12)         8           8  (7, 12)        12         7  (8, 10)         8          20  (b)  Internal traffic load between two server racks.

(i, j)    (i, j)     (i, j)  (0, 4)   6 (0, 5)   6 (0, 6)   6 (0, 7)   6 (0, 8)   6 (1, 2)   8  (1, 6)   4 (1, 7)   4 (1, 8)   4 (2, 3)   6 (2, 4)   4 (2, 5)   2  (3, 4)   4 (3, 5)   2 (3, 6)   0 (3, 7)   2 (3, 8)   4 (4, 5)   2  (0, 1)   10 (0, 2)   8 (0, 3)   6  (1, 3)    6 (1, 4)   4 (1, 5)   4  (2, 6)   2 (2, 7)   2 (2, 8)   4  (5, 6)   4 (5, 7)  4 (5, 8)   4 (6, 7)  6 (6, 8)  6 (7, 8) 8  (4, 6)   2 (4, 7)   2 (4, 8)   4  (i, j)  Simulation Parameters  =150000  =100000  Q=39   =1.5  1=0  2=2  3=4  4=6  ?1=200000 (e)  Simulation parameters.

?2=300000   ?3=500000   ?4=800000 (8, 12)        12         12  (10, 12)      12          20  (12, 13)      13          6  (8, 13)        13          4  (10, 13)      13          10  (a) Distributed placement in US network. 11 ? 13 12 ? 13 13 ? 13  Rack: 5 6   Rack: 2 3   Rack: 0 1   Rack: 7 8  Rack: 4  Fig. 3.  ILP based optimal solution with an overall system cost of 1566848 for distributed placement of the DCN.

K=4   =1000   =1000  =100        the corresponding basic cost can be properly calculated in the second term of (1). Constraint (7) specifies that the total amount of all inter-node DCN internal traffic equals to that switched by all core switches, where  is formulated in (3). Constraint (8) limits the sum load of external and internal traffic at a core switch to its maximum switching capacity. Constraint (9) formulates the flow conservation property of the external demands. Finally, constraints (10)-(11) check whether a pair of content and core switches is collocated with any racks at a particular node u, which provides  to count the last term in (1).



IV. NUMERICAL RESULTS Simulations are carried out based on the US network with  14 nodes and 21 links, as shown in Fig. 3a. We consider 9 server racks in the DCN, with internal traffic load between an arbitrary pair of server racks defined in Fig. 3b. External demands on the DCN service and per-unit-traffic transmission cost at each link are defined in Fig. 3c and Fig. 3d, respectively.

Fig. 3e gives simulation parameters, including those for the scalability related cost function.

The ILP based optimal solution is shown in Fig. 3a. Racks are divided into 5 component sets and are placed at nodes 7, 8, 10, 12 and 13. There is a core switch at each of these nodes.

Note that the number of required core switches is determined by the optimization based on the given DCN internal traffic pattern and external demands, instead of being predefined.

Fig. 4c lists the routes of external demands in the optimal ILP solution, where the number above each arrow indicates the traffic load on the corresponding link or switched locally at the same node (e.g., 7? 7). We can see that most of the demands are served by their closest core switches. Nevertheless, the 7 units of demands at node 2 are split into 5+2 and are served by core switches at nodes 7 and 13, respectively. This is because the core switch at node 7 runs out of its switching capacity, and thus two units of demand at node 2 must take a slightly longer route to be served by the core switch placed at node 13. In fact, this shows that the proposed architecture provides a mechanism for balancing traffic loads among the core switches via external demands routing in the optical network.

Fig. 4f shows how the inter-node internal traffics are routed and switched. Internal traffics between two component sets are generally switched by one or both of the two core switches collocated with the component sets. However, 5 units of the internal traffic between nodes 7 and 12 are served by the core switches at node 10, because the switches at nodes 7 and 12 have no spare capacity to switch this 5 units of internal traffic.



V. CONCLUSION We proposed a scalable Data Center Network (DCN)  architecture by using optical switches and interconnect with distributed placement of switches and racks across a given optical network. This solves the critical concern on scalability of DCNs, but leads to additional internal overhead due to delay and transmission costs of the DCN internal traffic. By leveraging among multiple conflicting factors and taking sufficient care of the DCN internal overhead, we formulated the cost minimization problem to reduce the overall system cost of deploying a DCN in a distributed manner. An ILP (Integer Linear Program) was proposed to place the DCN component sets under a given set of external demands and internal traffic pattern. Simulation experiments validate the correctness of the  ILP and the proposed architecture. Our work addresses both scalability and cost minimization issues from a network point of view for practical deployment of DCNs in an optical network.

