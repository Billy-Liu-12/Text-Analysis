?Fuzzy Association Rule Mining based Frequent Pattern  Extraction from Uncertain Data?

Abstract?frequent pattern mining is one of the most important research topics for many real life applications in the area of data mining. Frequent item set originates from association rule mining that uses to find association rules of items in large transactional database. Many existing algorithm to mine for the frequent itemset from static transaction database that is definite known and precise. In this paper we conducted a study on uncertain data and fuzzy association rule mining approach. The main contribution of this paper is three manifolds. First we presented a review on existing method for finding frequent patterns form uncertain data. Second a new approach is proposed for finding frequent patterns from uncertain data, and third, the experiments are carried out to evaluate the performance of proposed approach for uncertain data. In this approach we have used fuzzy concept and originate the frequent patterns. The experimental results from the survey demonstrate that this proposed approach for valuable frequent pattern set the influential uncertain data mining.

Keywords: - Uncertain Data, fuzzy set, fuzzy logic, Association Rule Mining, precise data.



I. INTRODUCATION Data mining is the analysis of large datasets to find  unsuspected relationship and to summarise the data in novel ways that are both understandable and useful to the data holder [1,2,11,12,13].Popularly referred to as knowledge discovery [12] in database (KDD) lies at the interface of database technology machine learning, high performance computing, statistics etc.[8] There are many research topics in data mining; one of the most important topics is association rule mining.

Mining frequent patterns is a sub process of association rule mining. Frequent item set mining is a common data mining task for many real life applications association rule mining helps to generate the previously unknown , potentially useful set of the item that are frequently co-occurring. It support to motivated by decision support problems faced by many mining algorithms have focused for discovering association rules [4, 8, 10, 13, 18, 22 ].

In 1994 Agrawal and Srikant [22] proposed Apriori algorithms to generate frequent itemset in a database. Since then almost studies in the field have focused and improved efficiency of mining process. Other algorithm like as FP- growth algorithm by Han at.el.[12], Depth-first backtracking, Pincer algorithm, graph based algorithm [24] etc.

These algorithms mine traditional statics database (like web data, market data etc.) that contains precise data. When mining these data where item are either present in or absent from. A transaction (Boolean data mining), or each attribute of a transaction is associated with a quantitative values, however, there are (e.g. environmental survey, medical diagnosis, stock market), in that users are uncertain about the presence or absence of some item or event [2, 12, 15].that need to be modified in order to handle uncertain data. The Figure.1 shows classification of data mining algorithms based on nature of data.

Figure 1: A Tree Structure of data mining methods can be classified on the data.

Uncertain Data: - The concept of a possibility measure was introduced by Zadeh [30]. A possibility measure on a universe is a function from (0, 1), the basic difference between precise and uncertain data are that each transaction of the later contains items and their existential probabilities [5, 10, 13, 15]. The uncertainty of such suspicion can be expressed in terms of existential probability [13, 16]. In uncertain data, each item in a transaction is associated with probability that indicate the possibility that the item exists in the transaction. Normally we can say it is over predication to come the item in the transition.

This kind of data item set is called uncertain dataset. Figure.2 shows that the transactional dataset of Precise and uncertain data.

Data Mining  Mining on Precise Dataset  Mining on Uncertain  Classification on Data  Clustering on Data  Association Rule Mining  Other Methods  Hard Clustering  Soft Clustering  UApriori  UFP-growth              Figure 2: Difference between Precise and Uncertain Data   An uncertain dataset T consists of n transactions T1, T2, T3 . . .

, Tn. A transaction Tn contains a number of items. Each item Im in Tn is associated with a non-zero probability PTn (Im), which indicates the likelihood that item Im is present in transaction Tn.

For example a shop has (i) 76% bread sold. (ii) 25% butter (iii) 64% milk and (iv) 81% egg. That denote as I1, I2, I3, I4 is respectively. It is represented as in Table 1:   I1 I2 I3 I4  T1 0.76 0.25 0.64 0.81  T2 0.24 0.15 0.68 0.48  T3 0.12 0.34 0.48 0.45  Table 1: Uncertain Data  Each item indicates the chances that those item exist, rather  than the item has definitely or doesn?t have them.

To find the frequent patterns from uncertain data is not as  simple as such are precise data[5,6,7,10]. So normal approaches those are work for precise data isn?t feasible for uncertain data. These are just example of real life situation in which data are uncertain, hence efficient algorithm for mining uncertain data as in demand [4,8].

It is generally recognized that, the various real world relationships are basically fuzzy. Therefore, the concept of fuzzy sets can help for data mining and discover novel and meaningful patterns from data. Fuzzy logic is a logical system, which is an extension of multivalued logic. It is almost synonymous with the theory of fuzzy set, a theory this relates to classes of objects with unsharp boundaries in which membership is a matter of degree.

Fuzzy association rules [1, 8, 13, 15, 25] are extension of association rules based on fuzzy sets. Fuzzy association rules extraction is performed as the following steps. Let F = {A, B, C} denotes the fuzzy itemset that consists of fuzzy sets in different attribute, where A, B, and C denote the fuzzy sets.

Support of the itemset F is defined as:  s (F)= ?F(Zi)/n Where Zi denotes the transaction of the database, ?F(Zi)  denotes the membership value calculated by the product operation (t-norm) of each item(Fuzzy Set) in F, and n denotes the total number of transactions in the database. It is obvious that the definition is equivalent to the wel1-known "Boolean  mining problem" when the item sets consist of "crisp sets." using the support value, confidence of the fuzzy association rule A->B is calculated by:  (A B)= s (A U B)/s (A) Where A and B are fuzzy item sets. The fuzzy association  rule is extracted when these values of the rule are more than predefined minimal support and pre-defined minimal confidence. One of the main problems of mining fuzzy association rules is how efficiently find the "frequent itemsets" from the database.

This research study used fuzzy based rule mining approach on uncertain data to find the fuzzy association rule that reduces the database scans and improve the efficiency and accuracy.



II. LITERATURE REVIEW There are several different approaches for frequent pattern  mining of uncertain data[5,7,10,18,21]. Recently, the concept of fuzzy sets has been generally used in association rules mining. This technique has been used in various applications, such as finding fuzzy association rules from quantitative transactions, work with biological data, genetic algorithms etc.

Unfortunately, many researches have been done in certain data mining. Such as Hong et al. [13,14] was proposed in 2003 to finding fuzzy association rules from quantitative transactions.

by Hu et al. [15,16] proposed in 2004 finding fuzzy association rules from both quantitative and categorical attributes by dividing them into various linguistic values. Chen and Weng [9] propose a new approach in 2008 to discover association rules from imprecise ordinal data. Although this work is interesting, the significant power of this model is limited because it cannot handle other types of uncertain data.

Some researchers have extended association rules mining techniques to imprecise or uncertain data. somebody have proposed different approaches and design framework.

In 2006 Wang Kay Ngai et. al has proposed, ?Efficient Clustering of Uncertain Data?[26] in this paper they studied the problem of uncertain object with the uncertainty regions defined by pdfs. They describe the min-max-dist pruning method and showed that it was fairly effective in pruning expected distance computations. They used four pruning methods those are independent to each other and can be combined to achieve an even higher pruning effectiveness.

In 2007 C. K. Chui et.al. has proposed new approach,? Mining frequent itemset from uncertain data.? [7]They introduced the U-Apriori algorithm,  that is a modified version of the Apriori algorithm, to work on such datasets. They identified the computational problem of U-Apriori and proposed a data trimming framework to address this issue. In that experiment, that it achieves high performance gain in terms of both computational cost and I/O cost.

Charu C. Aggrawal et.al was proposed in 2008, ?A Framework for Clustering Uncertain Data Streams?[6] they provide a method for clustering. They use general model of the uncertainty in which we assume that only a few statistical measures of the uncertainty are available. They show that in which we assume that only a few statistical measures of the uncertainty are available.

TID            Items  T1                  A, B, C   T2          B, D   T3      A, B, D  TID              I1          I2  T1                 76%                 85%  T2                 67%                 24%  Precise Dataset Uncertain Dataset  710 2012 World Congress on Information and Communication Technologies    Charu C. Aggrawal et. al. was proposed in 2009, ?Frequent Pattern Mining with Uncertain Data?[5], they extended several existing classical frequent itemset mining algorithms for deterministic data sets, and compared their relative performance in terms of efficiency and memory usage. Due to inclusion of probabilistic information, uncertain data has quite different trade off than the deterministic data.

Cheng-HsiungWeng et. al. was proposed in 2009 ?Mining fuzzy association rules from uncertain data?[8] they providing an excellent framework for handling uncertain data.

Unfortunately several issues that, this paper assumed that experts assigned the maximum deviation threshold.

Carson Kai-Sang Leung et. al. was proposed in 2010, ?Efficient Algorithms for the Mining of Constrained Frequent Patterns from Uncertain Data?[3] they proposed U-FPS algorithms for efficient mining of frequent patterns that satisfy the user-specified succinct constraints from databases of uncertain data.

Peiyi Laila A. Abd-Elmegid et. al was proposed in 2010, ?Vertical Mining of Frequent Patterns from Uncertain Data?[17] they extend the state-of-art vertical mining algorithm Eclat for mining frequent patterns from uncertain data generate the proposed UEclat algorithm. In this paper they studied the problem of mining frequent itemsets from existential uncertain data using the Tid set vertical data representation. They also perform comparative study between the proposed algorithm and the well known algorithms.

Toshihiko Watanabe [25] in 2010 proposed ?An Improvement of Fuzzy Association Rules Mining Algorithm Based on Redundancy of Rules?, it was a basic algorithm based on the Apriori algorithm for rule extraction utilizing redundancy of the extracted rules. They improve the efficiency of the association rules mining and to prune the redundant rules extracted. But they not succeed to handle the huge data mining problem, and further improvement for fuzzy association rules mining is given.

Tang et. al. was proposed in 2011, ?Mining Probabilistic Frequent Closed Itemsets in Uncertain Databases? [19] this paper defines probabilistic support and probabilistic frequent closed itemsets in uncertain databases for the first time. It also proposes a probabilistic frequent closed itemset mining (PFCIM) algorithm to mine probabilistic frequent closed itemsets from uncertain databases.

As pointed out in the literature review of previous research as applied fuzzy sets to discover fuzzy association rules from certain data rather than uncertain data. Since the possibility theory is appropriate for human to express their opinions, judgments and decisions with uncertainty [7], in this approach to combine these both theories and then discover fuzzy association rule from uncertain data. The uncertain theory is closely related to fuzzy set theory. In the next section explain proposed approach with an example.



III. PROPOSED MTEHOD In this paper, we have proposed the approach that mines  uncertain data and find frequent patterns that satisfying the user-specified succinct constraint. To illustrate working of proposed approach with an example is discussed in this section.

Table.2 describes the parameters that are used in this paper.

S. No. Notation Meaning  1 Ck The set of Candidate itemsets with k items  2 Lk The set of large Fuzzy itemsets with k items  3 n Total Number of item in Database  4 T Uncertain Dataset  5 m Total number of items  6 Min_supp Multiple Minimum Support  Table 2: Parameters Table  An illustrative example  Let us consider the following database transactions consisting of uncertain data. .

Transactions Contents  T1 {I1:0.9, I2:0.7, I3:0.7, I5:0.6,I6:0.4} T2 { I1:0.9, I3:0.8, I5:0.7} T3 { I2:0.9, I4:0.9, I5:0.1} T4 {I2:1.0, I3:0.3, I4:1.0, I6:0.4, I7:0.3} T5 {I1:0.8, I3:1.0: ,I5: 0.8} T6 {I2:0.4, I3:0.5, I4:1.0}  T7 {I1:0.8, I3:0.9, I7:0.2}  T8 {I2:1.0, I3:0.1, I4:1.0, I5: 0.4}  T9 {I3:0.1, I6:0.5}  T10 {I1:0.7, I2:1.0, I5:0.5, I6:0.8} Table 3: Uncertain Dataset T   In the above database of uncertain data, each transaction  contains items and their corresponding existence probability.

For the example there are five items I1, I2, I3, I5 and I6 in the first transaction T1, where existence probability of there items are 0.9, 0.7, 0.7, 0.5, and 0.4 respectively.

Proposed Algorithm:-   Input:  A set of Uncertain Data T = (T1, T2, T3?.,Tn) where Tn={(I1:fi1), (I2:fi2)??.(In:Fin)} Output: A set of Frequent Patterns.

START  1. Scan Dataset T and create 1-itemset L1.

2012 World Congress on Information and Communication Technologies 711    2. If (min_supp <= Total Support Value) Then 3. Insert item into Candidate Fuzzy set C1 4. Else 5. Remove item from 1- Frequent itemset 6. For(k=2; Lk-1 ?; k++) // find fuzzy frequent k  itemset Lk i. Ck= Candidate_gen(Lk-1, Min_supp)  ii. Lk= {Ck  Support= Count/Tn >= Min_supp) 7. For all the fuzzy frequent k-itemsets 8. Calculate the Confidence Value of the possible  association rules.

Conf(A->B)= Supp(A U B)/ Supp(A)  END   In first step, proposed algorithm scans the uncertain dataset T one time and accumulates support of each itemset with their transaction id that is shown in Table.4.

Itemset Support Count Support of  1-itemset I1 {0.9/T1, 0.9/T2, 0.8/T5, 0.8/T7,  0.7/T10} 4.1  I2 {0.7/T1,0.9/T3,1.0/T4, 0.4/T6, 1.0/T8, 1.0/T10}  5.0  I3 {0.7/T1, 0.8/T2, 0.3/T4, 1.0/T5, 0.5/T6, 0.9/T7, 0.1/T8, 0.1/T9}  4.4  I4 {0.9/T3,1.0/T4, 1.0/T6, 1.0/T8} 3.9 I5 {0.6/T1, 0.7/T2,0.1/T3, 0.8/T5,  0.4/T8, 0.5/T10} 3.1  I6 {0.4/T1,0.4/T4,0.5/T9,0.8/T10} 2.1 I7 {0.3/T3,0.2/T7} 0.5  Table 4: 1-Itemset L1after Scanning database T  Let the user specified support threshold If min_ supp= 2 then removed each row which support is less then min_supp, because they are infrequent itemset will never be frequent. The {I7} cannot be considered for Further process because their support count <=min_supp. Now filtering Table.4 and construct Table.5 for candidate 1-frequent itemset C1.

Itemset Support Value I1 4.1 I2 5.0 I3 4.4 I4 3.9 I5 3.1 I6 2.1  Table 5: Candidate 1- frequent itemset C1  Now Generate 2-item set L2 using Table.4 and Table.5.

Itemset Support Count Support of 2-itemset  (I1,I2) {0.7/T1,0.7/T10} 1.4 (I1,I3) {0.7/T1,0.8/T2,0.8/T5,0.8/T7} 3.1 (I1,I4) {--} -- (I1,I5) {0.6/T1, 0.7/T2, 0.8/T5, 0.5,T10} 2.6  (I1,I6) {0.4/T1, 0.7/T10} 1.1 (I2,I3) {0.7/T1, 0.3/T4, 0.4/T6, 0.1/T8} 1.5 (I2,I4) {0.9/T3, 1.0/T4, 0.4/T6,1.0/T8} 3.3 (I2,I5) {0.6/T1, 0.1/T3, 0.4/T8, 0.5/T10} 1.6 (I2,I6) {0.4/T1, 0.4/T4, 0.8/T10} 1.6 (I3,I4) {0.3/T4, 0.5/T6, 0.1/T8} 0.9 (I3,I5) {0.6/T1, 0.7/T2, 0.8/T5, 0.1/T8} 2.2 (I3,I6) {0.4/T1, 0.3/T4, 0.1/T9} 0.8 (I4,I5) {0.1/T3, 0.4/T8} 0.5 (I4,I6) {0.4/T4} 0.4 (I5,I6) {0.4/T1,0.5/T10} 0.9  Table6: 2-itemset L2   If min_supp= 1.0 then in Table.6 {(I1, I4), (I3, I4), (I3, I6),  (I4, I5), (I4,I6), (I5,I6)} cannot be considered for further process because their support <=min_ supp. Now construct Table.7 for candidate 2-frequent itemset C2.

Itemset Support Value  (I1,I2) 1.4 (I1,I3) 3.1 (I1,I5) 2.6 (I1,I6) 1.1 (I2,I3) 1.5 (I2,I4) 3.3 (I2,I5) 1.6 (I2,I6) 1.6 (I3,I5) 2.2  Table 7: Candidate 2- frequent itemset C2  Now generate 3- frequent itemset L3  Itemset Support Count Support of 3-Itemset  {I1,I2,I3} {0.7/T1} 0.7 {I1,I2,I5} {0.6/T1, 0.5/T10} 1.1 {I1,I2,I6} {0.4/T1, 0.7/T10} 1.1 {I1,I3,I5} {0.6/T1, 0.7/T2, 0.8/T5} 2.1 {I1,I3,I6} {0.4/T1}  0.4 {I1,I5,I6} {0.4/T1,0.5/T10} 0.9 {I2,I3,I4} {0.3/T4,0.4/T6, 0.1/T8} 0.8 {I2,I3,I5} {0.6/T1,0.1/T8} 0.7 {I2,I3,I6} {0.4/T1,0.3/T4} 0.7 {I2,I4,I5} {0.1/T3,0.4/T8} 0.5 {I2,I4,I6} {0.4/T4} 0.4 {I2,I5,I6} {0.4/T1, 0.5/T10} 0.9  Table 8: 3-Itemset L3  712 2012 World Congress on Information and Communication Technologies     min_supp= 0.50 then construct candidate 3-frequent itemset C3   Itemset Support Value {I1,I2,I3} 0.7 {I1,I2,I5} 1.1 {I1,I2,I6} 1.1 {I1,I3,I5}                     2.1 {I1,I5,I6} 0.9 {I2,I3,I4} 0.8 {I2,I3,I5} 0.7 {I2,I3,I6} 0.7 {I2,I4,I5} 0.5 {I2,I5,I6} 0.9  Table 9: Candidate 3- frequent itemset C3   Now generate 4- frequent itemset L4   Itemset Support Count Support of 4-Itemset  {I1, I2, I3, I5} {0.6/T1} 0.6 {I1,I2, I3,  I6} {0.4/T1} 0.4 {I1,I2, I5, I6} {0.4/T1, 0.5/T10} 0.9 {I2, I3, I4, I5} {0.1/T8} 0.1 {I2, I3, I4, I6} {0.3/T4} 0.3 {I2, I3, I5, I6} {0.4/T1} 0.4  Table 10: 4-Itemset L4   min_supp= 0.50 then construct candidate 4-frequent itemset C4 .

Itemset Support Value {I1, I2, I3, I5} 0.6 {I1,I2, I5, I6} 0.9  Table 11: Candidate 4- frequent itemset C4  Finally, to summarize by applying our proposed approach that capture the content of uncertain data in example, we found frequent patterns {I1, I2, I3, I5} and {I1, I2, I5, I6} in level C4.

Now calculate confidence of each possible fuzzy association rules as follows in Conf (I2=>I4) = Supp (I2 UI 4) / Supp (I2) = 3.3/ 3.9 = 0.85 Similarly the confidence calculated for frequent itemset in Table.12.

Rules Confidence Calculation Conf (I3=>I5)=> Supp (I3 U I5) / Supp (I5)=  2.2/ 3.1 = 0.71 Conf ((I2 U I4) => (I2 U I5))  Supp( I2 U I4 U I5) / Supp(I2 U I4) = 1.5/3.3 = 0.45  Conf (( I1 U I2 U I3) => I1 U I5)  Supp( I1 U I2 U I3 U I5) / Supp(I1 U I5) = 0.6/2.6 = 0.24  Table 12: Rule Generation

IV. EXPERIMENTAL EVALUATION In this section we assess the performance of our algorithm and find the results. To demonstrate the performance of proposed approach, we have used large number transaction set of uncertain data, and compared its performance with other existing approaches. Clearly our approach shows the effectiveness with minimum count ratio as compare with to U- apriori, UF-Tree & UF-growth algorithm, as it captures the whole data set in only one scan. Figure.3 shows the experimented count ratio performance of our approach with these Algorithms. The experiments were performed on an Intel core 2 Duo, 2.94 GHz system running Windows 7 professional with 2 GB of RAM.

Figure 3:  Count Ration Performance Compairsion

V. CONCLUSION Most of the research finds the patterns from transactional data that containing precise data, so there are many real life conditions, when we deal with uncertain data. In This paper, we have used concept of fuzzy approach, in uncertain dataset and used multiple minimum supports to find fuzzy association rules in a given uncertain data set. This works well with problem of uncertain data relationships, which are represented by fuzzy set concepts. The proposed approach can thus generate large frequent itemsets and then derive fuzzy association rules from uncertain dataset. The advantage of fuzzy association rule mining is that; the algorithm performs scanning our dataset only one time. By this also we have to do less effort for the calculation of various relations. This approach scale well when handling large amount of dense data. As ongoing work, we are investigating ways to further reduce the data structure problems, and generated frequent patterns will be useful for clustering.

2012 World Congress on Information and Communication Technologies 713

VI. ACKNOWLEDGMENT This work is supported by research grant from MPCST,  Bhopal M.P., India, Endt.No. 2427/CST/R&D/2011 dated 22/09/2011.

