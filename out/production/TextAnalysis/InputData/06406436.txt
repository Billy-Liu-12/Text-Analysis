Learning Cost-Sensitive Rules for Non-Forced Classification

Abstract? Building accurate classifiers is very desirable for many KDD processes. Rule-based classifiers are appealing be- cause of their simplicity and their self-explanatory nature in describing reasons for their decisions. The objective of classifiers generally has been to maximize the accuracy of predictions. When data points of different classes have different misclassification costs it becomes desirable to minimize the expected cost of the classification decisions. In this paper we present an algorithm for inducing a rule based classifier that (i) shifts the class boundaries so as to minimize the cost of misclassifications and (ii) refuses to announce a class decision for those regions of the data space that are likely to contribute significantly to the expected cost of decisions. We compare our results with other rule based classifiers such as the C4.5, CN2 and GARC for the cases of uniform and non-uniform misclassification costs of different classes.



I. INTRODUCTION  Rule based classifiers build models based on simple intuitive rules extracted from data. Over the years rule based classifiers like C4.5, CN2, and association rule based classifiers have been developed and used. The rules generated by these meth- ods are usually sets of feature-value conditions which, when met, predict a class. Their accuracy usually falls short of more complex classifiers like the Support Vector Machines or Neural Networks but they make up for it with their simplicity and ease of interpretation.

Most rule induction algorithms work by iterating through the feature space, finding sets of rules that split datasets into relatively pure classes. In the process they seek to cover promising parts of the feature-value space to form rules, and also cover every instance in the training dataset. Additionally, most classifiers always make a call on the class of a given test instance. In some domains where the cost of misclassification is high, it is better to not assign a test instance any label than to risk misclassifying it. Choosing to keep an instance ?undecided? may be cheaper than misclassifying it.

Another drawback with most of the rule-induction algo- rithms is that they restrict a data tuple in the dataset to helping define only one rule or pattern. This is because of the exclusive partitioning of the dataset into subsets in every iteration. This is done to reduce the computational cost and to find the smallest set of rules to classify most of the data.

However, by doing so a classifier can overlook significant and useful rules or patterns that may have had sufficient support if data tuples were available for sharing and were allowed to contribute towards justifying multiple rules.

When a dataset has different costs or penalties associated  with misclassifications from different classes, classifiers that blind to such non-uniform costs can perform poorly even though their overall classification accuracy is high. These is- sues can be addressed by either wrapping a cost-blind classifier inside a meta algorithm, like AdaCost or MetaCost, by post processing of classification rules, or by building cost-sensitive classifiers from the ground up[9] [10][5][21][13]. Lomax and Vadera have presented an extensive survey of cost sensitive decision tree classifiers in [18].

Our algorithm for rule-induction overcomes the above draw- backs by performing a systematic search on the feature space using a cost sensitive entropy-based measure for rule selection and also for pruning. Our approach does not impose the single coverage constraint, that is, we permit a data tuple to belong to multiple sets of tuples, each set becoming the basis of possibly generating a rule. Thereby we generate rules that embody meaningful patterns, while at the same time making sure that weak rules are not generated. When a data situation corresponding to a potentially weak rule is encountered, the rule is not generated, and thus at deployment the classification system does not make a prediction. This reduces the cost of making wrong classifications.

The rest of the paper is organized as follows: in section 2 we review some rule based classification algorithms and cost sensitive decision trees. Section 3 explains some preliminary concepts that are used in our approach followed by an explana- tion of our classification method in section 4. A comparison of the performance on some publicly available data sets is presented in section 5, followed by the conclusions of our work and contributions.



II. RELATED WORK A. Rule based classifiers  Classifiers like C4.5, CN2, and decision trees were among the first rule based classifiers. These algorithms work by iteratively splitting the dataset into smaller sets based on the best attribute such that one or more of the subsets formed have a higher percentage of instances belonging to one class than any other. Information gain, entropy or the Gini score have been used to choose the next best feature with which to split the dataset. The disadvantages of these algorithms is that they work in a greedy manner and keep splitting the dataset and ignoring parts of the dataset already covered by a rule.

They thus continue forming classification rules for only the remaining section of the dataset. This approach may lead to   DOI 10.1109/ICDMW.2012.62    DOI 10.1109/ICDMW.2012.62     generation of rules that are locally optimal and may ignore some good candidates due to the exclusive splitting of data performed early in the process. This greedy approach tends to reduce the number of rules generated since a more compact set of rules with somewhat lower accuracy is preferred over a much larger set of rules.

In contrast to the tree induction algorithms, association rule mining is mostly done on binary datasets. As a result, most as- sociation rule based classifiers need to discretize or transform the dataset into a binary format. It is recommended that each continuous feature be converted into a categorical or discrete values feature using entropy based or MDL discretization [16].

Bing Liu et al. introduced the algorithm for classification based on association rules[16][1]. This algorithm exploits par- allels between building of association rules and classification rules. They present CBA-RG, an association rule based clas- sifier which they show to perform comparably with C4.5. The CBA algorithm works by generating and ordering candidate association rules where the variables on the right side are always class labels. It employs post-hoc pruning strategies to resolve any redundant or conflicting rules that are generated.

If a rule has high confidence and support, it is accepted as a classification rule and the rows (data tuples) in the database that support the rule are removed from further participation in the algorithm. They improved upon their algorithm with CBA(2) by incorporating multiple support thresholds[17]. The reason being that in datasets where class frequencies are unbalanced, a single threshold may be high for some classes and low for others.

One of the major issues with association rule based clas- sifiers is that as the number of features increases, the set of candidate rules that need to be generated grows exponen- tially. Therefore, efficiently generating rules and pruning off unpromising areas of the search space while maintaining high efficiency have been the main concern of these algorithms.

CBA uses the apriori method of rule generation whereas other approaches like CMAR and L3 use a more efficient FP tree based rule generation method[2][14]. A greedy approach to generating candidates can also be used to reduce the number of candidate rules. This approach is seen in CPAR and GARC[26][6].

Chen et al. have presented GARC, Gain based Association Rule Classifier, based on a modification of the idea used in CBA[6]. It improves upon CBA by using information gain to reduce the number of candidate rules generated and therefore reduces the size of the final set of rules and also the processing time. GARC uses information gain to identify the attributes that best splits the database and then generates only those rules that contain this attribute. It performs comparably to CBA while generating a small fraction ( 5%) of the rules generated by CBA.

When using a rule based approach, it is possible for two or more rules with different class labels to be applicable to the same instance. Such conflicts are resolved either by defining a precedence ordering over the rules and choosing a single rule with the highest precedence or by analyzing the correlation  between high confidence rules to make a decision[22][14].

When compared to most other rule based approaches our  approach differs primarily in the following ways: ? We identify good quality biclusters within the data table.

These can be viewed as rectangular regions in the data table with all permutations of rows or columns being allowed. These biclusters can overlap each other. That is, a tuple or a feature can be included in multiple biclusters. If the quality metric of the bicluster is above some threshold a corresponding rule is generated.

? Our method does not force class labels on instances that are prone to be misclassified. That is, tuples may not match feature-value combination for any of the good quality biclusters identified during learning phase. In this case a class label is not announced.

? We form a middle ground between techniques that use single trees and multiple trees (bagging) since we do not impose the single coverage constraint but we also do not build multiple trees to cover all important concepts embedded in the data.

When very strict criteria are used for accepting a bicluster to generate a rule, we generate only those rules that have very high accuracy. Therefore, if an instance is likely to be misclassified, none of the generated rules will match the instance and it will be labeled ?undecided?.

Thabtah presents a detailed survey [22] about the issues of concern in association rule based classification and the different existing algorithms.

B. Cost Sensitive Classifiers  The concept of misclassification costs, whether uniform or non-uniform, has been addressed for a long time[25][4]. Much effort has been put into adapting the existing cost-insensitive (accuracy- maximizing) decision tree learning algorithms to make them cost sensitive. This has been done in a number of ways such as boosting, re-sampling the classes to reflect their misclassification costs, wrapper methods like MetaCost, or incorporating misclassification costs into priors calculation[10] [9][18].

Most early approaches perform post-hoc pruning on rule sets generated by cost blind classifiers[20]. Once the set of rules is generated, a misclassification cost metric is computed for each rule and rule with high expected costs are discarded.

Other approaches included cost considerations during rule construction with existing algorithms such as C4.5CS, CSGain and GINIAlteredPriors. These approaches calculate and assign instances of each class a weight based on a class?s misclassi- fication costs and frequency in the dataset. These weights are incorporated into information theoretic measures and used to guide the decision tree generation[19][4][20][13][8][15].

Algorithms like UBoost and AdaBoost take a boosting approach to cost-sensitive classification by working with com- plex weight functions to build stronger classifiers from weak classifiers[23][24]. MetaCost is a cost-sensitive algorithm that uses bagging to reduce misclassification costs[9]. MetaCost builds a classifier by learning rules on different subsets of     the original dataset, where each subset is re-sampled from the original dataset based on misclassification cost of each class. Some other approached are based on the use of genetic algorithms or stochastic methods to induce decision trees or to induce multiple decision trees and then use them as an ensemble system to classify test instances. A survey of cost- sensitive decision tree classifiers by Lomax and Vadera is presented in [18].

Our method approaches the task of cost-sensitive classifi- cation by incorporating a cost-sensitive entropy based metric into a systematic search of the feature-value space. This metric is used to evaluate the merit of each good bicluster identified in the dataset by our search algorithm. We do not pre-compute weights or prior probabilities before the search starts, rather the expected cost of each rule is computed as it is generated.

This expected cost is dependent only on the class distribution within the bicluster form which the rule is generated rather than within the entire dataset. Additionally, as mentioned before, instances can be shared among biclusters for different rules, and this helps in finding more meaningful and low-cost rules overall.



III. PRELIMINARIES  A. Generating association rules for real valued data  Traditionally, association rule mining is done on binary datasets. Therefore, in order to perform association rule mining on real valued datasets, the dataset needs to be transformed into a format that can be mined in this way. Bing Liu et al.

propose discretization of values in each column using Fayyad and Irani?s MDL method [16][11]. We have used the same MDL entropy based discretization for transforming values in each column from continuous to discrete in all our datasets.

B. Biclustering and finding interesting patterns  The task of generating rules for a dataset where class labels for instances are known can be thought of as looking for feature-value patterns that match instances of one class but not the others. When the class labels are unknown, clustering is used to find interesting patterns in the data across features and instances by grouping similar instances together, and getting insights into the data. When clustering is performed on features and instances simultaneously, it is called Biclustering, and it generates subsets of features and corresponding subsets of data instances. All the instances included in a bicluster are described by the rule formed with the features included in the bicluster. If the instances in a bicluster are predominantly from one class or the expected cost of forming a rule for the bicluster is low then this rule can be retained as a good rule for classification.

C. Accuracy and coverage  Most classifiers are built by covering all training instances and are intended to predict the class of every test instance presented. If the classes are arranged in a linearly separable manner in the data space such an approach to classification works well and delivers high accuracy for most training and  testing sets. However, in datasets where some classes or parts of them overlap in the data space, and some parts of these classes don?t overlap, building a classifier that classifies all instances of all classes with have reduced accuracy because of the overlap regions. This usually results in some low quality/ confidence rules. It is preferable in some situations to cover a limited number of training instances as long as the confidence in the predicted classes remains very high. Also, in some cases it is acceptable for a classifier to be able to predict instances of only one class[1][3].

Our approach builds on this idea of accepting partial classi- fication in order to get higher accuracy or lower expected cost on the test instances that are classified. A model that consists of only high confidence rules reveals the highly structured and separable parts of the data space while saying nothing about regions in which instances are prone to be misclassified.



IV. APPROACH  A. Defining a rule  A rule is a set of constraints on features and their values. A rule, R= {(fi, vi)} is a set of feature-value pairs, where feature fi has value vi. It can be read as ?feature fi has value vi?. A rule, R= {(fi, vi)}, is applicable to an instance in the dataset if, for each feature fi in the rule, the value of the feature fi in the instance is the same as vi. The length of a rule is the number of feature-value pairs that the rule contains.

B. Support and coverage of a rule  The support of a rule R, for a class Ci, Suppi(R), is the fraction of data instances that belong to the ith class from among all the instances that satisfy the rule R. The support of a rule Supp(R) is the largest support value that the rule enjoys from among all the classes whose instances in satisfies.

The support values of a rule for different classes, along with the misclassification costs of instances are used to form a metric for determining the quality of the rule. The support of a rule is also used as one of the pruning metrics in our approach. If the support of a rule is lower than some preset minimum threshold defined in general or for a particular class, it can be discarded and no rules that are a specialization of it need to be generated.

The coverage of a rule coverage(R) is the set containing the row IDs for of all data instances, of all classes, that satisfy the conditions of the rule.

C. Quality of a rule  We define the quality of a rule as the cost-weighted entropy of it?s support across instances of different classes that satisfy it, or as the entropy of different misclassification costs. That is,  quality(R) or entropy(R) = - ?  ci ? logm(ci), where ci = Suppi(R) ? cost(i), m= number of classes in the database, and     cost(i) is the cost of misclassifying an instance of class i as belonging to that of any other class. We call this entropy- based metric as ?quality? but a low value of this metric is desirable for a rule to be good. As with entropy values, the maximum contribution to the quality value is made by the class that should be predicted by this rule.

Our method assumes that the cost of misclassifying an instance is fixed, irrespective of which other class it gets misclassified as. When all the misclassification costs are equal (say, to 1), the above formula reduces to that of entropy based on support across different classes.

Each bicluster identified by our search algorithm is evalu- ated by this quality metric and if is considered good then a rule corresponding to the bicluster is generated. A good rule is one which has low entropy since it means that most of it?s support or cost comes from only one class, which is the class that the rule will then predict. This metric is also used to prune the search space during the candidate generation phase.

If the entropy of a rule is below a threshold, called tgood, we do not need to make the rule more specific by including more features. The reason being that rules generated by further specialization may have better quality (lower the entropy) but will not cover any extra instances. The support for any such rules, after specialization, can only go down. Also, those rules that have quality values higher than some threshold, tbad, can be discarded as unpromising rules. However, this threshold must be kept reasonably high to allow the intermediate rules to develop by further specializations for a few iterations before they are discarded.

baseSet? makeBasicRules baseSet? prune(baseSet) workingSet? baseSet while workingSet ?= ? do nextWorkingSet? ? for each r in workingSet do  for each b in baseSet do rk ? combineRules(r, b) if entropy(rk) > tbad then continue  else if supp(rk) ? minSupp(k) then continue  else if entropy(rk) ? tgood then if increasesCoverage(rk) then acceptedRules? acceptedRules ? rk updateCoverage(rk)  end if else nextWorkingSet? nextWorkingSet ? rk  end if end for  end for workingSet? nextWorkingSet  end while Algorithm 1: Rule Generation  In order to find good rules we examine features, their values and interactions among feature-value pairs in an iterative and systematic manner using the prefix based search. For each candidate set of feature-value pairs we determine the set of satisfying instances, consider the resulting bicluster, and compute the quality(R) for the rule R that corresponds to this bicluster. We accept rules that have low quality metric and continue to process the other rules by leaving them for further specializations.

First, the set of all rules of length one are generated.

makeBasicRule iterates over each feature and makes a rule for each distinct value of that feature and returns the set of all rules formed.

ruleSet? ? for each fi in F do  for each distinct vj in fi do ruleSet? ruleSet?{(fi, vj)}  end for end for return ruleSet  Algorithm 2: makeBasicRules  From this set, rules that have a support lower than a pre- specified threshold minsupp are discarded. Also, the rules from this set that have quality metric value less than tgood are removed from the set baseSet and are added to the set of rules accepted for the classifier, called acceptedRules. This new set of rules of length one represents a set of basic rules and is stored in baseSet.

For the first iteration of the algorithm, all rules from the set baseSet are copied into a temporary set called workingSet.

In each iteration, a rule, Ra = {(fi, vi)}, from workingSet is specialized by adding a condition defined by a rule from the set baseSet in the function combineRules.

For example, given a rule from workingSet R1 = {(f1, v3)}, (read as ?feature f1 has value v3?, and a rule from baseSet, Rbase1 = {(f4, v1)}, the following new rule can be formed by the set union of their conditions: Rb = {(f1, v3), (f4, v1)} (read as ?feature f1 has value v3 and feature f4 has value  v1?). The support for Suppi(Rb) will contain those rows(data instances) which were common to both R1 and Ra and belonged to class i. A temporary label is assigned to the new rule representing the class that it predicts, i.e. the class that has highest contribution to the quality metric described above.

Once a new rule, Rb, is formed, the following conditions are checked:  1) Is the entropy of the rule higher than tbad.

2) Is the support , Supp(Rb), less than the minimum  support for the class it predicts.

If either condition is true for the new rule, the algorithm  continues to the next rule without adding it to the set of final rules, acceptedRules.

If neither of those two conditions are true but the entropy of the rule is greater than tgood it is added to the set of rules to     be processed in the next iteration. If the quality metric value is less than tgood the rule is added to acceptedRules if it increases the cover of it?s predicted class by at least one addi- tional data instance. Once a rule is added to acceptedRules the cover of its predicted class is updated using the cover of the rule, whereas instances of other classes are kept unmarked since better rules that cover them still need to be found.

As the rule becomes more specialized, its support across different classes changes. The objective is to find rules that have much lower ci in one class than in any other. Such a rule will have lower entropy (an quality metric value) and is suitable for use as a classification rule.

Once a good rule is found, it is added to the set acceptedRules and is assigned the label of class C where class C is the class with maximum ci for the rule. The rows of the class C that conform to the conditions defined in the accepted rule are marked as covered but are not removed from further participation in the rule generation process.

Rules that are below a threshold of quality are added to nextWorkingSet for further examination or specialization in the next iteration.

A candidate rule can be discarded if one of the following applies to it:  ? It does not have support greater than a minimum thresh- old for any class.

? Its quality metric value is greater than tbad.

? Its quality metric value is less than tgood and support  greater than minSupp, but all instances of it?s predicted class are already covered by one or more rules in acceptedRules.

At the end of an iteration, the rules in workingSet are replaced by the rules in the nextWorkingSet, the process is iterated, and the iterations end when one of the following conditions is met:  ? Rules of length F have been generated, where F is the number of features in the dataset. That is, after F iterations.

? nextWorkingSet is empty because all the generated candidate rules were pruned off.

? The rules in acceptedRules set cover all the instances across all classes.

When training ends, acceptedRules contains a set of rules with their associated quality metric values and class labels.

When a new instance needs to be classified, it is assigned the label of the rule that applies to this instance and had the lowest quality metric value during training. Thus, the quality metric is used to resolve conflicts when two or more rule with different class labels apply to an instance. If no rule in acceptedRules applies to a new instance, its class is not announced and the instance is marked as undecided.



V. RESULTS  We ran our algorithm on ten datasets taken from UC Irvine?s machine learning data repository[12]. Eight of these datasets have two classes and the other two have three classes.

We wish to highlight two different aspects of the algorithm we present here. In the first case better prediction accuracy is achieved only by excluding the highly overlapped and confused regions of the data space. This case is also equivalent to the costs of misclassification being uniform across all the classes. Lower classification cost is achieved by ignoring the data space regions with high expected cost. In the second case we consider the case of non-uniform misclassification costs.

TABLE I EFFECT OF MAKING UNDECIDED CALL ON ACCURACY OF CLASSIFIER  Dataset KNN SVM Decision Tree  CN2 Our Algo- rithm Quality Cut- off  Accuracy %  No call %  Australian Credit Ap- proval  83 87.15 85.3 84.01 0.1 86.99 12.11  0.2 85.82 8.84 0.3 85.01 4.13 0.4 83.39 1.82 0.5 84.74 0.14 0.6 84.03 0.0 0.7 85.57 0.0 0.8 84.66 0.0 0.9 84.37 0.0  Breast Cancer  95.24 95.81 94.29 94.43 0.1 96.35 1.32  0.2 95.29 0.18 0.3 94.82 0.18 0.4 95.45 0 0.5 97.15 0 0.6 97.15 0 0.7 94.78 0 0.8 94.78 0 0.9 94.78 0  Cleveland Heart  80.55 84.73 77.8 81.98 0.1 78.53 6.3  0.2 79.64 3.91 0.3 79.13 2.6 0.4 78.01 1.63 0.5 80.24 0.43 0.6 81.9 0.32 0.7 79.75 0.1 0.8 76.63 0.0 0.9 74.56 0.0  Hepatitis 82.34 82.77 80.64 82.55 0.1 84.29 4.38 0.2 83.94 3.27 0.3 82.29 2.17 0.4 81.77 1.57 0.5 83.63 1.27 0.6 81.94 0.29 0.7 81.92 0.42 0.8 83.27 0 0.9 82.08 0  Iris 95.78 95.56 97.11 95.56 0.1 100 18.88 0.2 100 4.44 0.3 95.55 0 0.4 95.55 0 0.5 95.55 0 0.6 95.55 0 0.7 95.55 0 0.8 95.55 0 0.9 95.55 0  Table-I and Table-II here give insight into the gains achieved for the first case. The columns to the left provide accuracy results obtained by using the public domain software available from the ?Orange? toolset web site [7]. These results are     obtained using the same dataset partitions for training and testing (70% for training and 30% for testing). We run each algorithm ten times for different partitionings of training sets and report the average accuracy in this and other tables that follow. The accuracy of predictions in this case is calculated as the percentage correct predictions out of the total predictions made. That is, the undecided cases are not included for computing the accuracy. The percentage of undecided cases is included in the last column of the table. It should be noted that even if the undecided cases are counted as wrong class label, our algorithm, in many cases, achieves better accuracy than that of the other rule generation algorithms. This contribution comes from being able to include training instances in multiple biclusters.

TABLE II EFFECT OF MAKING UNDECIDED CALL ON ACCURACY OF CLASSIFIER.

(CONTINUED)  Dataset KNN SVM Decision Tree  CN2 Our Algo- rithm Quality Cut- off  Accuracy %  No call %  Labor 91.76 80 71.18 82.94 0.1 92.78 8.33 0.2 92.61 2.77 0.3 90.16 3.88 0.4 89.47 5 0.5 96.11 0 0.6 77.77 0 0.7 77.77 0 0.8 77.77 0 0.9 72.22 0  Statlog Heart  82.72 83.58 79.38 81.98 0.1 78.99 5.43  0.2 76.81 2.46 0.3 77.88 0.61 0.4 75.18 2.96 0.5 72.4 1.11 0.6 74.28 0.12 0.7 75.67 0 0.8 76.17 0 0.9 71.35 0  Pima 75.11 77.88 76.67 75.24 0.1 86.33 51.86 0.2 85.54 47.87 0.3 85.26 43.89 0.4 84.22 30.3 0.5 83.98 25.1 0.6 80.64 12.98 0.7 79.12 6.06 0.8 78.3 0.43 0.9 76.62 0  Sonar 80.95 84.13 74.29 78.57 0.1 80.53 5.03 0.2 80.91 3.81 0.3 81.14 3.31 0.4 80.38 1.53 0.5 81.09 1.15 0.6 81.12 1.15 0.7 81.24 0.31 0.8 78.15 0.15 0.9 72.87 0  Waveform 80.48 83.65 75.08 80.79 0.1 90.5 63.98 0.2 86.79 29.46 0.3 83.8 9.64 0.4 82.08 2.65 0.5 81.05 0.38 0.6 78.73 0.12 0.7 72.16 0.07 0.8 66.69 0.0 0.9 65.94 0.0  From this table we can see that we can gain higher accuracy for our predictions as long as we are willing to keep some test instances undecided. As the quality metric cutoff value for rule acceptance increases (accept rules with higher entropy), more of the impure rules are accepted. These rules were possibly  discarded at lower cutoff values. This results in the fraction of undecided instances reducing but the accuracy becoming worse. For example, in the Iris dataset at quality cutoff of 0.2, we can achieve 100% accuracy on the test samples for which predictions are made, while not giving a label to only 4.4% of the instances. Similar behaviour can be seen in the Pima dataset. However, choosing very low cutoff may even cause overfitting of the classifier to the dataset. This can be seen in datasets where the performance is worse at 0.1 entropy cutoff compared to 0.5 cutoff.

Next, we show in Table-III and Table-IV the performance characteristics for our algorithm when each class has a dif- ferent misclassification cost. For each dataset, we ran the algorithm while varying the relative misclassification cost between 5:1 and 1:5 for pairs of classes. For the cost sensitive version of the algorithm we chose the quality cutoff to be 0.8 for rule acceptance which can be thought of as 75% of the potential misclassification cost being attributed to the class being predicted by the rule.

TABLE III CLASS-WISE PERFORMANCE WITH VARYING MISCLASSIFICATION COSTS-  TWO CLASS DATASETS.

Dataset Cost ratio  Correct No call  Australian Credit Ap- proval  Class0 Class1 Class0 Class1  5:1 0.93 0.56 0 0 4:1 0.92 0.61 0 0 3:1 0.85 0.77 0 0 2:1 0.94 0.7 0 0 1:1 0.87 0.78 0 0 1:2 0.79 0.91 0 0 1:3 0.44 0.96 0 0 1:4 0.32 0.94 0 0 1:5 0.4 0.92 0 0  Sonar Rocks Mines Rocks Mines 5:1 0.83 0.68 0 0 4:1 0.91 0.54 0 0 3:1 0.89 0.62 0 0 2:1 0.89 0.58 0 0 1:1 0.8 0.77 0 0 1:2 0.82 0.8 0 0 1:3 0.61 0.89 0 0 1:4 0.53 0.93 0 0 1:5 0.59 0.89 0 0  Pima Negative Positive Negative Positive 5:1 0.99 0.19 0 0 4:1 0.99 0.17 0 0 3:1 0.98 0.16 0 0 2:1 0.99 0.18 0 0.02 1:1 0.93 0.45 0.03 0.07 1:2 0.76 0.79 0.04 0.07 1:3 0.4 0.93 0 0 1:4 0.3 0.95 0 0 1:5 0.2 0.97 0 0  Although relative cost ratios for each pair of classes in a three class case can be varied, it is easier to understand the characteristics of our algorithm and also to understand and visualize the results when the costs are varied for only one class pair at a time. For the Iris dataset in particular, it is known that the setosa data points are linearly separable from others using different attributes. For this reason the misclassification costs for setosa were kept constant while the relative costs for the other two classes were varied to see how the algorithm performs.

TABLE IV CLASS-WISE PERFORMANCE WITH VARYING MISCLASSIFICATION COSTS-  TWO CLASS DATASETS.(CONTINUED)  Dataset Cost ratio  Correct No call  Labor Bad Good Bad Good 5:1 0.83 0.88 0 0.02 4:1 0.78 0.91 0 0.03 3:1 0.85 0.89 0 0.02 2:1 0.78 0.95 0.02 0.02 1:1 0.8 0.92 0 0.01 1:2 0.93 0.9 0 0.02 1:3 0.87 0.89 0 0.04 1:4 0.9 0.81 0 0.02 1:5 0.8 0.87 0 0.02  Hepatitis Die Live Die Live 5:1 0.77 0.58 0.01 0 4:1 0.79 0.7 0.01 0 3:1 0.56 0.89 0 0 2:1 0.36 0.91 0 0 1:1 0.7 0.86 0 0 1:2 0.48 0.89 0 0 1:3 0.71 0.83 0 0 1:4 0.75 0.83 0 0 1:5 0.62 0.85 0 0  Statlog Heart  Class  Class  Class  Class  5:1 0.84 0.59 0 0 4:1 0.88 0.57 0 0 3:1 0.87 0.6 0 0 2:1 0.9 0.51 0 0 1:1 0.72 0.78 0 0 1:2 0.55 0.89 0 0 1:3 0.65 0.81 0 0 1:4 0.64 0.79 0 0 1:5 0.52 0.82 0 0  Cleveland Heart  Buff Sick Buff Sick  5:1 0.85 0.61 0 0 4:1 0.85 0.61 0 0 3:1 0.88 0.64 0 0 2:1 0.84 0.66 0 0 1:1 0.77 0.77 0 0 1:2 0.56 0.91 0 0 1:3 0.68 0.82 0 0 1:4 0.61 0.82 0 0 1:5 0.5 0.85 0 0  Breast Cancer  Malignant Benign Malignant Benign  5:1 1 0.88 0 0 4:1 0.98 0.88 0 0 3:1 0.99 0.88 0 0 2:1 0.99 0.89 0 0 1:1 0.88 0.97 0 0 1:2 0.91 0.96 0 0 1:3 0.94 0.96 0 0.01 1:4 0.6 0.98 0 0 1:5 0.22 0.99 0 0.01  The results in Table-V show that when the cost of mis- classifying an instance of a class is high, the accuracy for predicting that class is also higher. For example, in the Pima dataset, when the cost of misclassifying a negative instance is 5 times than that for the other class, the accuracy for predicting that class is 99% whereas that for the positive class is about 20%. And when the costs are reversed, the respective class accuracies are also reversed. The learned rules very much reflect the relative costs of misclassifying instances of a class. In Table-III we can observe the progression of relative misclassification costs from 5:1 to 1:5 in any dataset, and can see that the focus of the rule set also shifts according to the cost ratios.

The plot in Figure 1 shows the relative correct classification for the two classes of the Australian Credit Approval Dataset as the relative cost of misclassification changes. Similar plot for the Pima Indians Diabetes dataset shows the gradual  TABLE V  Dataset Cost ratio  Correct No call  Class  Class  Class  Class  Class  Class  Waveform 1,1,5 0.58 0.66 0.97 0.01 0.00 0.00 1,1,4 0.59 0.69 0.97 0.01 0.00 0.00 1,1,3 0.63 0.77 0.94 0.00 0.01 0.00 1,1,2 0.72 0.82 0.86 0.01 0.00 0.00 1,1,1 0.79 0.83 0.83 0.00 0.01 0.00 2,1,1 0.88 0.76 0.74 0.00 0.01 0.01 3,1,1 0.86 0.74 0.74 0.00 0.01 0.01 4,1,1 0.91 0.69 0.66 0.00 0.01 0.01 5,1,1 0.94 0.62 0.62 0.00 0.01 0.01  Correct No call  Setosa Versi Virgin Setosa Versi Virgin Iris data points  1,1,5 1 0.75 0.97 0 0 0  1,1,4 1 0.73 0.96 0 0 0 1,1,3 1 0.89 0.99 0.02 0 0 1,1,2 1 0.85 0.97 0 0 0 1,1,1 1 0.87 0.93 0 0 0 1,2,1 1 0.98 0.85 0 0 0 1,3,1 1 0.99 0.88 0 0.01 0.01 1,4,1 1 0.96 0.91 0 0 0 1,5,1 1 0.99 0.65 0 0 0  Fig. 1. Correct classification (Y axis) vs cost ratios(X axis) for Australian Credit dataset  change in the correct classifications for the two classes. The class with higher relative cost of misclassifications gets more of its instances classified correctly by the learned rule set.

Also, from the table we can see that only a negligible number of instances were left undecided. The Pima dataset has larger overlap among its classes therefore when the misclassification costs are non-uniform the rule set tends to lean in favor of the high cost class resulting in a much smaller correct classification rate for the other class.

Plots in Figure 3 and Figure 4 show the correct classification rates for the three class datasets.

A. Choosing a cutoff  For the uniform misclassification costs version of the al- gorithm where all classes have the same costs, lower quality metric cutoffs ensure that strict and very pure rules will be found for both classes. However, when the costs of classes  Fig. 2. Correct classification (Y axis) vs cost ratios(X axis) for Pima dataset     Fig. 3. Correct classification (Y axis) vs cost ratios(X axis) for Iris dataset  Fig. 4. Correct classification (Y axis) vs cost ratios(X axis) for Waveform  are non-uniform, we would be willing to accept rules that are less pure as long as the cost of misclassifying the expensive class is reduced. Keeping the quality metric threshold low may prove to be counter productive since the algorithm will accept only shorter, more general rules for the class with higher costs in order to catch all instances of that class. And it will also be generating more accurate lower quality metric rules for the class with lower misclassification cost to reach the coverage limit. This is not desirable since quality metric value defines the preference ordering among the applicable rules when choosing the label of a class.

One solution is to choose a higher quality metric cutoff for the reason mentioned earlier, while another way to address this issue would be to mark instances of all classes as covered by a rule instead of marking only those instances that belong to the class the rule predicts. A justification for doing the latter would be that it is better to misclassify other classes than to generate more accurate rules for them that may catch instances of the class that has higher misclassification costs.



VI. CONCLUSION We have presented an algorithm based on cost-weighted  entropy and efficient search of the rule space to determine rule sets that minimize the expected cost of misclassifications.

The results presented here and compared to other algorithms for rule generation demonstrate that our algorithm performs at par or better than most of the other algorithms for the case of uniform misclassification costs. For the non-uniform mis- classification costs our results are very intuitive and justifiable.

Our algorithms also have the advantage that we don?t need to perform weighted sampling of the data before generating the rules.

