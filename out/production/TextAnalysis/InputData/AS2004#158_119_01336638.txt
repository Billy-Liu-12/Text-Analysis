<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">2004 IEEE  International Symposium on Cluster Computing'and the Grid

Abstract In this paper, we propose a new algorithm, named Distributed Mar-Miner (DMM), for mining maximal frequent itemhcts from databases. A frequent itemset is maximal if nonc of its superaets is frequent. DMM requires very low communication and synchronization overhead in distributed computing systems. DMM has the local mining phase and the global mining phase. During the local mining phase, each nodc mines the local database to discover the local maximal frequent itemsets, then they form a set of maximal candidate itemsets For the top-down search in the subsequent global mining phase. A new prefix-tree data structure is developed to facilitate the storage and counting of the global candidate itemsets of different sizes. This global mining phase using the prefix-tree can work with any local mining algorithm. We implemented DMM on a cluster of workstations and evaluated its performance for various cases. DMM demonstrates better performance than other sequential and parallel algorithms, and its performance is quite scalable, even when there are large maximal frequent itemsets (i.e., long patterns) in databases.

Key words: Distributed data mining, maximal frequent itemsets, association rules, scalability.

1 Introduction Discovery of association rules is an important problem in data mining. With numerous practical applications, such as consumer market-basket data analysis, it has received tremendous amount of attention from the data mining re- search community. Let Z = { i , , z 2 , .  . . ,&amp;} be a set of items. Let V be a set of transactions, where each transac- tion T contains a set of items. An association rule is an This research was supported in pm by LexisNexis, NCR. and Wright Brothers Instirule (WBI).

0-7803-8430-X/04620.00 2004 IEEE implication of the form X + Y ,  where X c 1, Y C Z, and X n Y = 4. The association rule X +. Y holds in the database D with confidence c i f  c% of transactions in V that contain X also contain Y .  The association rule X =+ Y has supporr s i f s %  of transactions in D contain X U Y .

Mining association rules is to find all association rules that have support and confidence greater than or equal to the user-specified minimum support (called minsup) and min- imum confidence (called minconf), respectively [I] .  The first step in the discovery of association rules is to find each set of items (called itemset) that have co-occurrence rate above the minimum support. An itemset with at least the minimum support is called a frequenr iremser. The size of an itemset represents the number of items contained in the itemset, and an itemset containing k items will be called a k-itemset. The second step of forming the association rules from the frequent itemsets is straightforward as described in [I]: For every frequent itemset f ,  find all non-empty sub- sets of f .  For every such subset a, generate a rule of the form a +. (f - a )  if the ratio of support(f) to support(a) is at least minconf.

In mining association rules, the most time-consuming job is finding all frequent itemsets from a large database  with respect to a given minimum support. Many sequen- tial and parallel algorithms have been proposed to solve this problem. The most common sequential algorithms are Apriori [ I ]  and its variations. Apriori-like algorithms em- ploy a strict bottom-up, breadth-first search and enumerate every frequent itemset [3]. They require multiple passes    every frequent itemset [3]. They require multiple passes over the database. In the first pass, the occurrences of in- dividual items are counted and frequent 1-itemsets are de- termined. Then the frequent l-itemsets are used to gener- ate the potentially frequent 2-itemsets, called candidate 2- itemsets. In the second pass, we count the occurrences of the candidate 2-itemsets, so that we can determine the Fre- quent 2-itemsets. Frequent 2-itemsets are used to generate the candidate 3-itemsets, and so on. This process is repeated until there is no new candidate itemset generated.

denotes the set of frequent (k - 1)- In Apriori, if Fk 2004 IEEE International Symposium on Cluster Computing and the Grid itemsets discovered in the (k - 1)th pass over the database, then the set of candidate k-itemsets for the next pass, de- noted by c k ,  is obtained by the natural join of Fk 1 and F k  on the first k-2  items. For example, if F .  includes { 1, 2} and { 1,3}, then {I,  2,3} is a candidate 3-itemset. How- ever, a candidate k-itemset is pruned if any of its (k - 1)- subsets is not frequent. Thus, for { I ,  2, 3) to remain as a candidate 3-itemset, {Z, 3} also should be a frequent 2- itemset. This subset-infreqeency based pruning step pre- vents many candidate itemsets from being counted in each pass.

Due to the large size of the database to be mined, parallel data mining is very promising, and a few parallel algorithms were proposed for mining association rules. The most well- known one is the Count Distribution algorithm [Z], which is a parallel version of the Apriori algorithm. In Count Distri- bution, the database is partitioned and distributed over mul- tiple processing nodes initially. At each pass k ,  every node collects local counts of the same set of candidate k-itemsets.

Then, these counts are merged between processing nodes, so that each node can identify the frequent k-itemsets and generate the candidate ( k  + 1)-itemsets for the next pass, as in Apriori. To merge the local support counts of the candi- date itemsets, synchronization between nodes is required at every pass, and maintaining the same set of candidate item- sets in all the nodes is redundant.

In Apriori-like algorithms, if there is a frequent itemset with length 1 ,  then they will generate and count its Z L  sub- sets. This exponential complexity makes Apriori-like al- gorithms just suit for mining small frequent itemsets (i.e., short patterns) [3]. To address this problem, algorithms that can efficiently mine the maximal frequent itemsets (MFI) were proposed. The basic idea is that if we find a large fre- quent itemset (i.e., long pattern) early, we can avoid count- ing all its subsets because all of them are frequent. So, these algorithms always look ahead and try to find large frequent itemsets early. Once we find all the maximal frequent item- sets, all frequent itemsets can be obtained from them.

In this paper, we propose a new parallel algorithm, named Distributed Max-Miner (DMM), for mining maxi- mal frequent itemsets. It is implemented on a Linux cluster system where the database is partitioned over all the nodes involved in the mining.

DMM has two phases: local mining phase and global mining phase. In the local mining phase, each node directly applies a sequential MFI algorithm, such as the Max-Miner algorithm [3], to find all local maximal frequent itemsets in its local database. Then, all nodes exchange and merge local MFIs and their local support counts to construct an initial global candidate set. In the global mining phase, all nodes scan their local databases to count the support of global can- didates in each pass. If a global candidate k-itemset is not globally frequent, all its (k-1)-subsets are included into the global candidate set for the next global pass. This global mining phase will continue until the global candidate set is empty. We used the sequential Max-Miner for the local mining phase, but DMM can use any sequential MFI algo- rithm for the local mining phase.

DMM allows all nodes independently mine local MFIs    DMM allows all nodes independently mine local MFIs first, then uses a top-down search in the global mining phase. As a result, DMM can reduce the synchronization and communication overhead, which is critical in the dis- uibutedenvironment. DMM requires only one synchroniza- tion at the end of the local mining phase, and just a few syn- chronizations during the global mining phase in most cases.

Thus, the total number of synchronizations is much smaller than that of Count Distribution.

DMM uses a prefix tree structure to store and count the global candidates with different lengths during the global mining phase. To maintain thc prefix tree as small as possi- ble, three techniques are used in DMM to reduce the size of candidate set: ( I )  estimation of the support of some global candidates; (2) subset-infrequency based pruning; and (3) superset-frequency based pruning. Our experiments show that ( I )  and (3) can effectively reduce the size of the ini- tial global candidate set, whereas (2) and (3) can make the candidate set shrink considerably during the global mining phase.

The rest of the paper is organized as follows. Section 2 reviews more details of the Max-Miner algorithm, which is a basis of the proposed DMM algorithm. Section 3 de- scribes the DMM algorithm, and Section 4 presents the re- sults of performance analyses and comparisons of the DMM and Count Distribution algorithms in various cases. Section 5 contains some conclusions.

2 Max-Miner Algorithm Unlike Apriori-like algorithms, the Max-Miner algo- rithm [3] extracts only the maximal frequent itemsets, from wbicb all frequent itemsets can be obtained. Max-Miner always attempts to look ahead in order to identify large fre- quent itemsets early, so that all subsets of these discovered frequent itemsets can be pruned from the search space. This method is called superset-frequency based pruning. Max- Miner used the set-enumeration tree to represent the search space. How to construct the set enumeration tree is illus- trated in Figure 1 for four items: 1, 2, 3, and 4. The set- enumeration tree lists all combinations of the four items from level 0 to level 4 .

Each node in the tree is called a candidate group, and a candidate group g consists of two components which are actually two itemsets. The first itemset is called the bead of the group and denoted by h ( g ) .  The second itemset is called the tad of the group and denoted by t (g) .  t(g) is an ordered set and contains all the items not in h(g) but can 2004 IEEE International Symposium on Cluster Computing and the Grid I 23.4 Figure 1 .  Search space of Max-Miner potentially appear in any subnode dcrived from node g. For example, the node (i.e., candidate group) g1 enumerating item I ,  which is the leftmost node at level 1, has two com- ponents: h(g1) = {l} and t(g1) = {2,3,4}.

The main procedure of Max-Miner can be explained as follows. From the root of the tree at level 0, we count the support of I-itemsets. Only the I-itemsets which are frequent can be enumerated at level I .  In this example, 4 nodes are generated at level 1 if 1, 2, 3, and 4 are all frequent I-itemsets. For the node 91, which corre- sponds to item I at level 1, we need to count the sup- port of (h(gl)Ut(gl)} = {1,2,3,4}. If the support of {h(gl) U t(gl)} is greater than or equal to minsup, then we do not need to expand the tree from the node g1 any- more due to the superset-frequency based pruning. Simi- larly for the node g 2 .  which corresponds to item 2 at level 1, the support of {h(g2) U t(g2)} = {2,3,4} is counted. If {h(gz) U t (gz ) }  is frequent, then the subnodes of node g2 are not generated at level 2.

At any node g, if { h ( g )  U t ( g ) }  is not frequent, then for each item i in t (g) ,  we check if (h (g )  ui} is frequent. If    each item i in t (g) ,  we check if (h (g )  ui} is frequent. If {h(g) ui} is frequent, a corresponding subnode is gener- ated. The head of the subnode is {li(g) Ui} and its tail contains all the item j ,  such that {h (g )  U j }  is frequent and j comcs after i in t (g) .  For example, if {l, Z}, (1,3} and { 1,4} are frequent when (1,2,3,4} is not frequent, then we generate three suhnodes of node 91. They are enu- merated at level 2 as: [h(g') = {l,Z}, t (g ' )  = {3:4}], [h(g") = {L3}, t (g")  = (4}1, and [h(g"') = {1,4}, t(g"') = {}I. On the other hand, if { h ( g )  Ui} is not frequent, a subnode having { h ( g )  ui} as the head is not generated because none of its superset would be frequent.

This corresponds to the subset-infrequency based pruning of candidates. This process continues level by level until the whole tree is completed.

3 Distributed Max-Miner (DMM) Algorithm We present the DMM algorithm in this section. It is implemented on a shared-nothing multiprocessor system where each node has local memory and a local disk. All nodes communicate with each other by passing messages through a communication network. The database is evenly divided into N partitions { D o ,  D', D 2 , .  . . , DN I}, one for each of the N nodes (Po,  P', P 2 , .  . . , PN I} involved in the parallel mining, so that each node has the same num- ber of transactions allocated.

For DMM, the set of local maximal frequent itemsets discovered at node Pt is LhP, for 0 &lt; i 5 N - 1. For each pass in the global mining phase of DMM, the candidate set at each node is the set of all global potential maximal fre- quent itemsets that should be counted during the pass. The global candidate set is denoted by GCk, where k represents the kth pass in the global mining phase. The final rcsult is the set of all (global) maximal frcquent itemsets and is denoted by GM. For any k-itemset, we will call cach of its (k - 1)-subsets a largestproper subset.

3.1 Distributed Max-Miner(DMM) DMM includes two phases: local mining phase and global mining phase. In the local mining phase, each pro- cessing node Pz ,  for 0 &lt; i &lt; N - 1, applies the sequen- tial Max-Miner algorithm on its local database Dz to find the set of local maximal frequent itemsets L W .  Then, LMo, L M ' ,  . . . , L M N  are exchanged and merged be- tween nodes into the initial global candidate set GCI for the first global pass. GC' includes only maximal itemsets and is available at each node.

Since any itemset which is globally frequent must be fre- quent in at least one local database, each global frequent itemset should appear in GC1 or be a subset of some max- imal itemset in GC1. Thus, a top-down search is applied for the itemsets in GC1 in the global mining phase although these itemsets may have different length. Obviously, the maximumsizeofGCl isILMoI+ILM'I+ . . .+ I  LI\.IN ' 1 , where ILM'I represents the size of L M k .  However, since some itemsets may be frequent in multiple local databases, the size of GC1 is usually smallcr than this upper bound.

Furthermore, after the local mining phase, we already have local counting information related to these global candi- dafes. It allows us to estimate the global counts of some candidate itemsets, so that we can reduce the size of GC1.

This estimation method is described in Section 3.2.2.

With the initial global candidate set GC1, all nodes start the first pass of global mining phase. In pass k, each node scans its local database to count the occurrencies of the can- didates in GCk, then the nodes exchange and merge local counts to find the global frequent itemsets. If the global 2004 IEEE International Symposium on Cluster Computing and the Grid support of a candidate is above niinsup, it is included in the set of global frequent itemsets, named Frequentset; other- wise, it is included in the set of infrequent itemsets, named InfrequentSet. At the same time, all the largest proper sub- sets of each infrequent itemset are considered to he included    sets of each infrequent itemset are considered to he included in GCkCl for the next pass.

In DMM, we utilize InfrequentSet and Frequentset to perform the subset-infrequency based pruning and superset- frequency based pruning on GCkfl. Before each largest proper subset of an infrequent itemset is included in GCk+l as a new candidate, we check whether it can he pruned or not. If a new candidate appears in  InfrequentSet or has any subset in it, then it is already idrntified as infrequent. In this case, we should split it into its largest proper subsets, and repeat the same procedure. This subset-infrequency based pruning technique help us avoid unnecessary computations, especially for large candidates. Wc can break them down early into subsets that are close to the really frequent item- sets.

After the suhsct-infrequency based pruning step, we can check if each remaining candidate appears in Frequentset or has any supersct in it. If yes, it will not be included in GCk+l according to the concept of superset-frequency based pruning. In fact, after the first couple of passes, there may be many frequent itemsets already identified. They help us remove a large number of new candidates, espe- cially those split from short infrequent itemsets. In prac- tice, these two pruning techniques considerably reduce the size of candidate sets, pass after pass. How to maintain Fre- quentSet and InfrequentSet are described in Section 3.2.2.

The global mining phase must continue until there is no more candidate for the following pass.

3.2 Features of DMM 3.2.1 Prefix Tree for Counting Global Candidates with Different Length I Structure of the Prefix Tree: In DMM, we use the prefix tree to count the global candidates with different length. An example prefix tree is shown in Figure 2 together with some candidate itemsets to be counted.

The root node is at level 0, while the first item of each candidate itemset is placed at level 1, the second item at level 2, and so on. Each node in the prefix tree corresponds to an item and is connected to maximum two other nodes, one through a child link and the other through a brother link. All the items following a common prefix item in dif- ferent candidate itemsets are regarded as the children of the prefix item, and they are linked through brother links at the W d . 1 .  M: PI: *.U.C.E.r p2: A.". E n. *.B.F P4 *.C.D.F PI: i3.c P6 c. ".I PI: c.0.c p8: C.E.F p9: 6.P.G PI0:B.T.

P l l '  A, ". c PI?:*.C P I 3 C . E m4.e H Figure 2. Prefix tree structure items appearing in the candidates. Node A is the first child of the root node as it comes first in the lexicographical or- der among the children of the root node, and other children are linked one by one in ascending order through brother links. Similarly, nodes E and C have a common prefix A in different candidate itemsets, so they are linked together at level 2 as children of node A at level I, while only node B is directly connected to node A through a child link. If a node corresponds to the last item of an candidate itemset, then it references the candidate itemset through the candi- date pointer. In Figure 2, each of those last nodes is repre-    sented as a gray node or a black node, depending on whether it is a internal node or a leaf node in the prefix tree.

Construction of the Prefix Tree: Initially, the prefix tree has only the root node whose brother and child pointers are null. Then, we insert the candidate itemsets, one by one.

Suppose there is a candidate k-itemset 111, Iz, .  . . , Ik}.

This candidate itemset can be inserted into the prefix tree in k steps, one for each item. First, we check whether the root node has children. As described above, every node at level I should he the first item of a candidate itemset. If the root node doesn't have a child at all, then we need to create a node of I1 and link it as the first child of root node. If the root node has children, we can reach the first child of the root node through its child link, then check other children through the brother links to make sure whether 11 is already at level 1. If not, we need to create a node of I1 at the right position to keep the lexicographical order of the items at level 1. From the node of Il at level 1, we repeat the same procedure for the next item I2 in order to have it at level 2.

This procedure is repeated until the last item I ,  is located or inserted at level IC, and then it will reference the candidate itemset through its candidate pointer.

same level in their lexicographical order, but only the first child is directly connected to the prefix item through a child link. The root node corresponds to a null item and has four children in Figure 2: A, B, C and E ,  which are the first Counting the Global Candidates Using the P r e b  Tree: For each transaction, we use a recursive method to count all candidates appear in the transaction. Figure 3 shows 2004 IEEE International Symposium on Cluster Computing and the Grid how to count the candidates in a transaction containing {A ,  B, D ,  E ,  G}. We begin with the whole transaction at the root node. At level I ,  items A,  B and E are matched, and only E references a candidate, so we increase the count of the candidate itemset { E }  by one. Then, we recursively process the transaction segment {B ,  D ,  E ,  G} on node A, { D ,  E ,  G }  on node B, and {G} on node E. These three operations let us enter the next level. At level 2, A's first child B appears in {B, D ,  E ,  G}  but does not reference any candidate. So, we continue processing { D ,  E ,  G} on this B node. Back to the node B at level 1, where we pro- cess { D ,  E ,  G} on it, we can find it has no child appearing in { D ,  E,G}, so we simply stop here without processing on any of its children. For the node E at level I ,  its one child G is matched. Since this G references a candidate, we increase its count by one. On this branch, the transac- tion segment becomes cmpty after processing G, so we also stop here. Now, from thc node B at level 2, we can enter level 3. Node E,  the second child of node B, is matched and the corresponding candidate is counted. When we try to process {G} on node E ,  we cannot find a matching child since E has no child. So, after the transaction is scanned, the counts of {A,  B,  E} ,  { E } ,  and { E ,  G }  are increased by one.

...............

,-.:"!!V".GI ,i Figure 3. Counting the global candidates us- ing the prefix tree 3.2.2 Reduction of the Global Candidate Set For DMM, how to reduce the sire of the initial global can- didate set GCl and the subsequent global candidate sets as much as possible is very important for the overall pertor- mance. Three techniques were used to solve this problem: global support estimation, subset-infrequency based prun- ing, and superset-frequency based pruning.

Support Estimation of the Global Candidates: When we merge the local maximal frequent items (MFIs) from all processing nodes, we need to keep just one copy of each ~    MFI that is frequent in more than one node as a global can- didate. If a maximal itemset is frequent at all nodes, obvi- ously it is also a global maximal frequent itemset. We just need to accumulate its local support counts and put it into the FrequentSet. Such global candidates, however, are very few. Fortunately, even though most itemsets in GC1 appear as local MFls in just one or a few nodes, many of them of- ten have their supersets frequent in other nodes. In that case, the support counts of the supersets of a candidate allow us to estimate the minimum support count of the candidate in those nodes. For example, suppose that itemset { A ,  B,  G} is a local MFI with the local count of 4000 in node I, while {A,  B ,  C, E ,  G} and { A ,  B ,  G, K }  are local MFIs in node 2 with local counts of 3800 and 4200, respectively. We can then estimate that the local support count of {A, B ,  G}  in node 2 should he at least 4200. By this way, we can esti- mate the minimum support count of any itemset in a node if any of its supersets is frequent in that node. Obviously, the estimated minimum support count o f a  candidate is the lagest  support count of all its supersets in that node.

Subset-Infrequency Based Pruning and Superset- Frequency Based Pruning: During thc global mining phase, DMM maintains the following scts: GCk ( k  2 11, FrequentSet and InfrequentSer. They are changing dy- namically with the progress of the mining process. The global mining phase continues until GC, is empty, for some k 2 1, to ensure that we will not miss any global maximal frequent itemset. Eventually, FrequentSet will include all the global MFIs. Since DMM uses Frequentset and InfrequentSet to perform the superset-frequency based pruning and the subset-infrequency based pruning, maintaining these two sets without any rediindancy is important to make the pruning steps efficient. After each pass in the global mining phase, we determitle whether each global candidate is maximally frequent or not. If a global candidate is frequent, we put it into FrequentSet only if none of its supersets is already in that set. On the other hand, i f  a global candidate is infrequent. we put it into InfrequentSet only if none of its subsets is already in that set. For example, if {A, B, G, H }  is infrequent but {A ,  H }  is already in InfrequentSet, we do not insert it into InfrequentSer, because any superset of { A ,  B,  G, H }  will be also pruned by {A,H} when the subset-infrequency based pruning is applied.

If a global candidate k-itemhet is identified as infrequent, we split it into k (k  - 1)-subsets, which will be considered as new candidates. However, some of them may not be a valid candidate for the next global pass if it appears in In- frequentSet or has a subset in it. In that case, we need to split the invalid candidate into its largest proper subsets. For ex- ample, if {A,  B, C, D }  is infrequent and its subset {A, D} is in InfrequentSer, we will continue the splitting until we 2004 IEEE International Symposium on Cluster Computing and the Grid get the following new candidates: { A ,  B, C}, {B ,  C,  D } , (A,B},  ( A ,  C}, { B , D }  and {C,D}.  In practice, these two pruning techniques can make the global candidate set shrink drastically for each pass.

3.2.3 Cube-based Communication between Processors To perform the communication between processing nodes efficiently, we impose a logical binary n-cube structure on the processing nodes. Then, the nodes can exchange and merge the local count information through increasingly higher dimensional links between them [4]. In the n-cube, there are 2n nodes, and each node has n-bit binary address.

Also, each node has n neighbor nodes which are directly linked to that node through differcnt dimensional links. For example, there are 8 nodes in a 3-cube structure, and node ( 0 0 0 ) ~  is directly connected to (001)2, (010)~  and (100)~ through a I st-dimensional link, a 2nd-dimensional link, and a 3rd-dimensional link, respectively. Thus, in the n-cube, all the nodes can exchange and merge their local counts in    all the nodes can exchange and merge their local counts in T I  steps, through each of the n different dimensional links.

When n = 3, the three exchange and merge steps are: step 1: node ( 1 ) 2  exchange and mcrge, where denotes a don't-care bit.

step 2: node ( 0 )2  and node ( 1 )Z exchange and merge.

step 3: node (0 )2  and node (1 )2  exchange and merge.

O)z and node ( 3.3 Pseudo-code of DMM As we assume a homogeneous distributed computing environment where all the processing nodes are the same, we just give the pseudo-code of the DMM algorithm running on a node P'.

/*Local Mining Phase */ P" applies the sequential Max-Miner algorithm on D' and stores local MFIS into LM'; n = log, N :  I* N processing nodes are used for mining *I for ( j  = 1;j 5 n;j + +) P' exchange and merge LM' with that of a neighbor node through the jth-dimensional link, and the result is stored in LiM'; GC, = +; FrequenrSer= 6; foreach local MFI x in LM' { if the estimated global support o fx  is above the minimum support superset of x; else put x into.GC1: then put x into FrequenrSer unless it contains a I apply the suprrset-frequency based pruning on GCI; ~ /*Global Mining Phase */ InfrequenrSer= 4; /* global pass k ,  for k 2 1 */ while (GCk # 4) { Pi scans D' to count the candidates in GCk; for ( j  = I ; j  5 n ; j  + +) P' exchange and merge the local counts of GCk members with a neighbor node through the jth-dimensional link; foreach candidate x in GCx { if the support of x is above the minimum support then put x into FrequentSer unless it contains a superset of x; subset of x; else put I into InfreqrcenrSer unless it contains a } foreach candidate inserted into InfrequenrSer in the current pass { split the infrequent candidate into new candidates (i.e., its largest subsets); apply the subset-infrequency based pruning on these new candidates: those candidates pruned by the subset-infrequency based pruning are put back into InfmquenrSer; /* this process will continue until no new candidate either appears in InfrequenrSer or has any subset in i t  */ apply the superset-frequency based pruning on the new candidates; /* remove those candidates which appear in Frequentset or put the new candidates that passed the two pruning operations into GCI,+~ for the next global pass k + I; k + + ; have any superset in  it *I GM = FrequentSer: I* GM is the set of all maximal frequent itemsets */ 4 Performance Evaluation Our test platform is an &amp;node Linux cluster system    Our test platform is an &amp;node Linux cluster system where nodes are connected by a Fast Ethernet switch. Each node has a 800 Mhz Pentium processor, 512 MB memory, and a 40 GB disk drive. The processes are communicating using the MPI (Message Passing Interface) 151.

The databases used in our experiments are synthetic sales transaction databases generated as in [I]. All parameters used for generating databases are described in Table 1. For all databases, c = 0.5, m = 0.5, U = 0.1, ILI = 2000 and N I  = 1000. Table 2 lists all databases used in our perfor- mance evaluation experiments. The size of each database is about 360 MB. When running the parallel algorithm on a database, we need to partition it into local databases. To balance the size of the local databases, each transaction is randomly allocated to a node.

2004 IEEE Interna6onal Symposium on Cluster Computing and the Grid ID1 /TI ILI N I c m D In order to compare the performance of DMM and Count Distribution, we also implemented Count Distribution on the same platform.

Number of vansactiom in the database Average size of the transactions Average size of the maximal potentially frequent itemxu Numkraf maximal potentially frequent itemels Number of items Comlaliun level Mean of the comption level Variance of the camption level Table 1. Synthetic database parameters Table 2. Databases 4.1 Improvement of DMM over Count Distribu- tion We ran both DMM and Count Distribution on different synthetic databases with different minsup values. If we de- tine TCD and TDM,U as the execution times of CD and DMM, respectively, then the speedup of DMM over CD is TCD/TDMA,. In Table 3, the speedup of DMM is shown for different databases listed in the first column and for dif- ferent values of minsup listed in the first row. In these ex- periments, all 8 nodes in our cluster system were used.

Table 3. Speedup of DMM over CD (8-node case) As minsup decreases, DMM begins to show more and more improvement in our tests. As shown in Table 3, when the 111 value of the database is large, such as 8 or IO, even if minsup is as high as 0.58, DMM is faster than Count Dis- tribution with a speedup above 2.5. It is because a large 11 value results in large frequent itemsets (i.e., long patterns), which benefits DMM. If minsup is less than 0.25%, DMM outperforms Count Distribution considerably.

DMM uses the local and global mining phases to re- duce the overall synchronization and communication re- quirement, but the global mining phase still needs several passes over the database and incurs some extra computation overhead. In our cluster system, since the communication speed between nodes is high, the benefit of reduced syn- chronization and communication overhead is not enough to offset the effect of extra passes during the global mining phase. However, this feature of DMM may be attractive to some distributed systems where the communication cost is relatively high.

4.2 Synchronization Requirement of DMM and Count Distribution We compared the number of synchronizations needed between processing nodes in DMM and Count Distribu-    between processing nodes in DMM and Count Distribu- tion. In DMM, the local mining phase needs only one syn- chronization. So, the total number of synchronizations is the number of passes needed in global mining phase plus one. Table 4 shows the comparison results. Here, we de- fine SDMM and SCD as the numbers of synchronizations needed in DMM and CD, respectively. The first row of the table lists various values of minsup, and the first column lists the names of databases. The values in each entry of the table represents SD,MM :SCD.

Table 4. Comoarison of svnchronization re- quirement When minsup is high, DMM is comparable to or a lit- tle bit slower than Count Distribution. We also ran Apriori and Max-Miner for these cases, and found that Max-Miner doesn?t show much improvement over Apriori, either. That is because the high minsup limits the number of frequent itemsets and the length of those frequent itemsets. Thus, the effect of look-ahead technique used by Max-Miner is  not clearly shown, and naturally DMM has the same result.

DMM needed just two times of synchronization in the best cases. In other cases, the number of synchronizations needed for DMM was also much smaller than that of CD, mainly because DMM requires only one synchronization during the local mining phase.

2004 iEEE International Symposium on Cluster Computing and the Grid 4.3 Communication Requirement of DMM and Count Distribution In Count Distribution, all nodes have the same set of candidate itemsets in each pass. So, every node needs to exchange the same amount of count information with oth- ers. In DMM, nodes need to exchange two types of data: candidates and their counts. For the merging of local MFIs to construct the first global candidate set, each node per- forms log, N send and log, N receive operations when N processing nodes are used. Since the set of local MFIs in one node may he different from those in other nodes, the amount of data each node sends or receives varies at each communication step. In each global pass, all nodes have the same global candidate set and exchange the same count in- formation in log, N steps. To make it simple, we computed the average amount of data each node sends and receives during the whole mining.

Let?s consider the difference in the meaning of candi- dates of the two algorithms as the number of candidates determines how much data need to he exchanged between processing nodes during the mining. In Count Distribution, its candidates are the potential frequent itemsets generated, as in Apriori. In DMM, after the local mining phase, can- didates involved in the communication are just the potential maximal frequent itemsets; i.e., all local MFIs and some of their subsets that are not global MFIs. Compared with the set ofcandidates in Count Distribution, the set of candidates in DMM is very small. Thus, DMM requires much less communication than Count Distribution even though DMM needs to merge the candidates first (after the local mining phase) and then exchange the count information (during the global mining phase).

When the minsup is very low, Count Distribution tends to discover a large number of short frequent patterns, so that there are a large number of candidates in early passes.

This results in a very high communication overhead be- tween nodes. On the other hand, in  DMM, the increase in the number of short frequent patterns usually results in a small change in the number of local MFIs. Thus, even though low minsup value may affect the local mining phase of DMM, it has a relatively small impact on the communi- cation overhead during the global mining phase. Therefore, as the minsup decreases, DMM performs better than Count Distribution in terms of communication requirement.

Distribution in terms of communication requirement.

We implemented two versions of Count Distribution: one is using the n-cube communication, and the other is using the all-to-all communication. We compared the av- erage amount of data each node communicates with oth- ers when we executed DMM and Count Distribution on the T30J08D2954K database with various values of minsup, and the results are shown in Figure 4.

As shown in Figure 4, the communication overhead of - 400 a 350 B 300  g 200 $ 250 150 s d 10.3 1 075 0 5  0 4  0 3  025 0 2  015 0 1 Minimum Suppon (%) Figure 4. Comparison of communication re- quirement DMM is much lower than that of Count Distribution. Even though DMM needs to cxchange candidates at the end of the local mining phase and some candidates may consist of many items, the total amount ofdata to he transferred is still relatively small, because Count Distribution must exchange the count information for much larger candidate sets. Com- pared with Count Distribution using the all-to-all commu- nication scheme, DMM demonstrates a big improvement in communication for all cases. Here, we?d like to emphasize that the advantage of DMM in communication requirement comes from its much smaller size of candidate sets and the n-cube communication scheme.

4.4 Sensitivity Analysis of DMM In this section, we analyze the characteristics of the DMM algorithm in terms of speedup and sizeup. All tests were performed with a minsup of 0.25%.

4.4.1 Speedup We measured the speedup of DMM as the number of pro- cessing nodes was increased while the database size re- mained the same. For the databases listed in Table 2, we kept the same database size of 360 MB, but the database was partitioned into 2,  4, and 8 parts when the number of nodes were 2,4, and 8, respectively.

Figure 5 shows the execution time of DMM on the 2- node, 4-node, and 8-node systems. To demonstrate the speedup, we also ran the sequential Max-Miner for each database on a single node. As the number of nodes was doubled, the execution time of DMM was reduced by about 40% to 50%. Even though DMM may not achieve the linear speedup, it still shows a good speedup.

When DMM is executed on the T40110D2256K database using 2 nodes, its execution time is small. This is 2004 IEEE International Symposium on Cluster Computing and the Grid ,4000 3 12000 I ; iwoo E P 2 6000 w 4wo 5 8000 0.7 because, when the number of nodes is small, the datadistri- bution characteristic of each data partition is very similar to that of the whole database. So, after the local mining phase, the initial global candidate set would be similar to the set of    the initial global candidate set would be similar to the set of global MFIs. As a result, during the global mining phase, the communication and synchronization overhead is low.

- 0 2 4 6 8 1 0 Number of Nodes Figure 5. Speedup of DMM 4.4.2 Sizeup  For the sizeup test, we fixed the system to the 8-node con- figuration, and distributed each database listed in Table 2 to the 8 nodes. Then, we increased the local database sire at each node from 45 MB to 215 MB by duplicating the initial database partition allocated to the node. Thus, the data distribution characteristics remained the same as the local database size was increased. This is different from the speedup test, where the database repartitioning was per- formed when the number of nodes was increased. The per- formance of DMM is affected by the database repartitioning to some extent, although it is usually very small. During the sizeup test, the local mining result of DMM is not changed at all at each node.

The results shown in Figure 6 indicate that DMM has a very good sizeup property. Since increasing the size of local database did not affect the local mining result of DMM at each node, the total execution time increased just due to more disk U 0  and computation cost which scaled almost linearly with sizeup.

5 Conclusions In this paper, we proposed a new parallel maximal fre- quent itemset (MFI) mining algorithm, named Distributed Max-Miner (DMM), for shared-nothing multiprocessor sys- tems. DMM is a parallel version of Max-Miner, and it re- quires low synchronization and communication overhead compared to other parallel algorithms. In DMM, Max- Miner is applied on each database partition during the lo- 0 45 90 135 180 225 270 Amwnt of Data per Node (ME) Figure 6. Sizeup of DMM cal mining phase. Only one synchronization is needed at thc end of this phase to construct thc initial global candi- date set. In the global mining phase, a top-down search is performed on the candidate set, and a prefix tree is used to count the candidates with different length efficiently. Usu- ally, just a few passes are needed to find all global maximal frequent itemsets. Thus, DMM largely reduces the number of synchronizations required between processing nodes.

Compared with Count Distribution, DMM shows a great improvement when some frequent itemscts are large (i.e., long patterns). A cube-based communication scheme is employed by DMM for efficient communication between nodes; and global support estimation, subset-infrequency based pruning, and superset-frequency based pruning are used to reduce the size of global candidate set. DMM has very good speedup and sizeup properties.

