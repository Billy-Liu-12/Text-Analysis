Modeling Higher Level Processing Functions Inherent to the Human Brain

Abstract  The most significant feature of the information pro- cessing in the brain might be the autonomy based on the motivation and self reward (MSR) to form a processor se- quence intending to find out the solution to a problem we are facing. In this paper, we show some preliminary ideas to incorporate the concept of MSR in designing brain-like information processing means, based on physiological and engineering points of view. We propose a hybrid neural network model as an extension of Hebb's rule, to be hy- pothesized for the function of association areas in the cere- bral cortex. The generated neural network model is tested on the problem of segmentation of brain magnetic reso- nance images (MRI).

Introduction  formation science is to formulate brain-like information processing paradigms, which are applicable to real world problems. There are many unknown processing mechan- isms within the brain, especially those related to the limbic emotional system and higher order processing systems in the neo-cortex. We try to develop a new paradigm, based primarily on neural network models to realize a practical means of taking into account firstly correspondences to physiological findings, and secondly their feasibility in re- al-world problems.

Unlike the Von Neuman type computer, cortical pro- cessors in our brain have to be chained together to form a meaningful process sequence by themselves. This process of learning involves predictive, memorizing and self-re- warding mechanisms. Such a claim is supported by the physiological evidence in investigating the effects of mem- ory, the predictive rewarding function of dopamine neu- rons, and the effects of lesions in the human brain. From physiological point of view, the reward is given as a part  One of the major challenges to the contemporary in-  Prefrontal cort Association corte  Figure 1 Proposed neural circuit for emotion: J. Papez, 1937 (thick lines) enhanced by P. Mac- Lean (thin lines)  0-7803-5529-6/99/$10.00 01999 IEEE 109  http://titech.ac.jp mailto:pms.titech.ac.jp   of the learning process. As the task is memorized, mechan- isms of internal prediction and self-reward will perform whenever there is a coincidence between behavioral reac- tion and external stimuli. In a psychological interpretation, we may give some self-reward to those processes which made us feel comfortable. At the same time, from the bio- logical point of view, the self rewarding criteria have to be very simple to be easily realized in biochemical form.

The circuit in Fig.1 is a simplified diagram of the limbic system, which includes sensory inputs, processing mechanism for the input information, self-rewarding mechanism based on psychological evaluation, and output for controlling human actions. It shows the extended emo- tion circuit, designed by MacLean and based on the earlier studies of Papez [I]. There are extensive and direct connec- tions between the neocortical areas and the hippocampal formation and the amygdala. Some of the most compelling biological evidence of higher level functions has come from studies of the association areas of the cerebral cortex.

In this study the main interest lays with the limbic asso- ciation cortex and the related limbic system. The limbic area of the cortex is responsible for memory and emotional and motivational aspects of behavior [2]. Biological evi- dence shows that the amygdala is implicated in the process of learning (particularly tasks that require coordination of information from different sensory modalities) and social judgment as part of the emotion-analyzing mechanism [3, 41.

Neural network model  Our interpretation of the introduced biological neural system is shown in Fig.2. The basic idea is to model the control function of the limbic system and the related asso- ciation cortex with the ?seek a near-optimal solution  control?, which is responsible for choosing the correct pro- cessor or sequence of processors for the solution of a given problem. The decision of this unit is based on the output of the ?inference and induction unit?. The latter is bound to consider preliminary knowledge on the topic and perform computational rules in order to ?infer? the appropriate in- formation for the ?control? unit. Upon achieving some so- lution, the result is forwarded to the ?evaluation of found solution? unit. The self-rewarding mechanisms will provide the feedback for the ?inference? unit and therefore ?close? the learning cycle.

We base our artificial neural network model on modi- fied predictive Hebbian learning rules. In other words, we incorporate the idea that the changes in synaptic strengths represent the associations between inputs. However, the Hebb rule alone will not permit predictive relationships formed in conditional tasking [5]. It is known that sensory stimuli through learning can come to act as predictors of reward or punishment. In our model, we view the predic- tion as a computational goal of the system.

Hebbian learning rules are correlational in the sense that the changes in synaptic strengths represents the asso- ciation between inputs. There are other relationships that are important to learn about events, such as the temporal order of the inputs. Conditioning experiments have shown that through learning, sensory stimuli can come to act as predictors of reward, punishment, and other salient stimuli.

Prediction can be viewed as a computational goal of a sys- tem that must operate in an uncertain and variable environ- ment.

The neural network simulating the general model is shown in Fig.3. It consists of feature extractor, marked AIW (Answer-in-Weights neural network), three subnets, and winner-take-all layer. Each subnet is trained to recog- nize one class only.

j j j ?  (.,,,,ion seek a near-optin - c o n 4  ?7  evaluation o found soluti  inference ant induction un  I self-rewarding mechar I  Figure 2 Higher - level processing model based on self-reward     winner-take-all  -A-  MRI image tappeddelay line  Figure 3 Proposed neural network system  The unit denoted AIW decomposes the image into Ha- damard basis functions and provides the weight coefficients as input to the subnets. Details about AIW can be found in [61.

We conjecture that this type of architecture is very flexible in terms of:  a) choosing the subnet structure and activation fun- ction. The latter can be linear, Gaussian or elliptic;  b) choosing discriminant function of the subnet; c) choosing the type of subnets included in the struc-  ture.

The subnets in this case are multilayered perceptrons  (MLP) with sigmoid activation functions, trained with backpropagation rule and the discriminant function of the subnets is  k, =I  where ck, are the coefficients in the upper layer and w, is the weight vector. The idea is to present the inputs in a tappeddelay line and each input vector to produce a predic- tion for its immediate future sample. During the training phase the object is to minimize the squared prediction error of the training patterns x from one class:  2 62 2  E =  zz[ ( l / l+e-? ) -x j+ , ] m=O j = O  During the test phase a pattern is input into all subnets.

The subnet with the smallest prediction error is the as- signed class for the presented pattern. The inputs are 63 di- mensional vectors, given serially to the subnets via AIW.

Simulations and discussion  For the simulation we have chosen the problem of segmentation of brain magnetic-resonance images (MRI) into white and gray matter and cerebro-spinal fluid (CSF).

The contemporary medical imaging technology provides complementary diagnostic tools such as x-ray computed  tomography, magnetic resonance imaging or positron emission tomography. In routine works, these images are interpreted visually and qualitatively by radiologists. How- ever, the need for quantitative information, i.e. precise de- tection of structure boundaries or characterization of tissues is becoming increasingly important in the clinical environ- ment. The routine quantitative analysis of these data is in- feasible by manual methods, therefore the full potential of modem medical technology will require at least partial au- tomation of the analysis process.

There are many difficulties which prevent the full au- tomation of the MR images analysis, namely: intensity variations due to the choice of time parameters, radio fre- quency coil, imager imposed inter-patient variations, inter- slice intensity variations, and problems with assessing the accuracy of the methods due to lack of gold standard.

The multidimensional nature of M R  image data is a characteristic to be considered, as it provides information about three tissue dependent parameters: the spin-lattice re- laxation time, the spin-spin relaxation time, and the proton density. Moreover, recent developments of techniques that permit the registration of several imaging modalities, such as X-ray CT, PET, and MRI increases the dimensionality of the data thus strengthening the potential of multidimen- sional classification techniques.

Possible areas of application include the automatic or semiautomatic delineation of areas to be treated prior to ra- dio surgery, the delineation of tumors before and after sur- gical or radio surgical intervention for response assessment, or the volumetric measurement of the white and gray mat- ter and of (CSF) for the study of the degenerative diseases.

For our simulations we have adopted a T2-weighted MRI slice. The image is 256x256 pixels, 256 gray levels within 8 bits/pixel presentations. The image to be seg- mented is divided into blocks of 8x8 pixels; each block is decomposed and the respective coefficients from each pixel presentation are taken as features for this pixel. We conjec- ture that this type of input vectors provides ?normalized?, intensity-independent input information and, therefore, un- biased pixel feature vectors, which is essential when seg-     menting MRI. After this preprocessing stage, each pixel?s feature vector is fed to all three MLPs in the proposed hy- brid architecture (Fig.3). Each MLP is trained to recognize only one class, i.e. gray matter, white matter, or CSF.

There are 63 inputs to each MLP. Although the number of coefficients after AIW is 64, the largest-value coefficient is discarded from the feature vector.

The neural network was trained on randomly chosen pixels from the shown slice (FigAa) and tested on the whole image. The resulting segmentation is pictured in Fig..lb, where the black portion denotes CSF, the gray is the gray matter and the white zones refer to the white mat- ter. The overall performance for the volume averaged 90% correct segmentation. Though not excellent, this result supports our conjectures about the learning and generaliza- tion properties of our model for modeling higher-level functions in the brain.

Conclusion  In this paper we have formulated our preliminary con- cerns and experimental results in modeling the motivation and self-reward mechanisms when designing brain-like in- formation processing means. The proposed hybrid neural network represents the controlling functions, inherent to the limbic system and associative cortex, which we have chosen as a model. As future research goals, we aim to in- corporate more pronounced predictive and reward mechan- isms based on matching (or coincidence) between the sen- sory information and the system response. On a more engi- neering level, the MLP has many defects, which prevents this type of network to serve as true model of brain func- tions. Hence, we view the processing units as a hybrid bet- ween supervised and unsupervised modes of learning.

The proposed neural network model supports our gen- eral concept and theories and the demonstrated results are satisfactory in terms of generalization and learning abili- ties. The model can be expanded and new features added in accordance with the biophysiological evidence.

