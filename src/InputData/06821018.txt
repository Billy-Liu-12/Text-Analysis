Automatic Analysis of Large Data Sets: A Walk-Through on Methods from Different

Abstract?Analyzing data is one of today?s hot topics. A complete list of fields of research and buzzwords associated with automatic analysis would extend beyond this document.

The importance of this topic stems from the amount of data currently produced in research, engineering, and other fields.

The size of these data sets renders manual analysis infeasible.

Automatic analysis methods are required to cope with the data sets produced. The algorithms for filtering, categorization, and analysis have a long tradition and are manifold. This raises the question for the best algorithm. The authors of this paper give an overview about manifold automatic analysis approaches along with a classification of these approaches with regard to three different fields of research.

Keywords?comparison, monitoring, visualization, analysis, data mining, clustering, genetic algorithm, correlation, machine learning, sequence alignment, graph search, resilience

I. INTRODUCTION  In our daily work in the fields of monitoring, tracing, profiling, and analysis of grid, cloud, cluster and HPC systems we see an ever-increasing amount of data. These are, e.g., monitoring data collected on large numbers of sequential programs as well as tracing data generated by massively parallel application runs. Furthermore a lot of other disciplines like life-sciences, astrophysics, or text analysis have to deal with a rising amount of data. Consequently, one could assume that comparing data sets in an automatic or semi-automatic fashion is a common task and categorizing data is just a matter of selecting the appropriate algorithm. Yet after some research we had to realize, that there is no ready-for-use algorithm available. Subsequently, we extended our research to cover more literature, started discussions and concluded that data comparison is not as easy as we hoped for. Contrary to the large amount of related work in this field, no generic ready- to-use algorithm, like for number searching or sorting, is avail- able. An adoption to the respective problem case is necessary and the most suitable algorithm needs to be identified.

In this paper we focus on automatic methods for identifi- cation of irregular or unusual behavior. Irregular data differs from usual, often repeating and already analyzed sets of data.

This data is of special interest and needs to be selected for further manual analysis. This identification can be realized by comparison of all data sets against a reference. A reference can be, e.g., a selected single job or artificially generated from his- torical behavior. Data with high dissimilarity to the reference  is identified as irregular. This process can dramatically reduce the amount of data which needs to be analyzed manually.

Subsequently, the time and attention of the analyst is used more efficiently, allowing a deeper and more accurate evaluation of occurring problems and faults. Additionally, irregularities caused by jobs or system components in complex job execution systems, like HPC, Grid, and Cloud systems, can be found automatically and visualized transparently.

The remainder of this paper is organized as follows. In Sec. II we describe our analysis problems. We give an overview of common visualization techniques for the analysis of data sets in Sec. III. In Sec. IV we present a set of automatic analysis methods and evaluate these methods with regard to our problem cases in Sec. V. In Sec. VI we summarize our contributions and describe future work.



II. FIELDS OF INTEREST  Each author works on a different field of research that requires to find irregularities, compare data sets to identify discrepancies and variations, or to categorize data. Following we give a brief introduction to these three fields of research.

A. Job-Centric Monitoring in Distributed Computing Environ- ments  For job-centric monitoring we use a grid-based monitoring system called AMon [44] utilizing the monitoring infrastruc- ture SLAte [26], [27]. We are looking for a way to present the monitoring data in a more meaningful way than to merely show thousands of bars and lines where irregularities are easily overseen. Yet we are also looking for general methods that can be directly adapted to other environments working with large job numbers. Such environments could be large cluster, HPC, or cloud systems, in particular government, hybrid, and federated clouds.

Monitoring data is recorded using variable time inter- vals, ranging from seconds to minutes. Used are common monitoring techniques [51] without privileged access rights.

The recorded data contains information about the job like consumed CPU time, CPU load, and main memory. Also recorded is information about the executing system like free main memory or the state of the storage systems. In addition, scheduling information such as time, date, and wall-clock time of the job is collected.

DOI 10.1109/CLOUDCOM-ASIA.2013.47    DOI 10.1109/CLOUDCOM-ASIA.2013.47     B. Tracing of Massively Parallel Applications  Performance optimization is essential in the design of scalable parallel applications. With increasing complexity and size of today?s HPC-machines this task is getting increasingly important. The identification and elimination of bottlenecks and inefficiencies is mandatory to achieve scalability above tens of thousands of processes. Tracing has proven to be a powerful approach for performance optimization. Using tracing application behavior is recorded in detail by storing a series of events from an application run. Events could include entering or leaving a function, or sending MPI messages.

Each event consists of a time-stamp along with relevant data, e.g., function name, or bytes of data transmitted. The detailed performance information included in traces make them very suitable for performance analysis. For instance they can be used to detect synchronization delays, communication ineffi- ciencies, or varying performance behavior over time. However, this advantage also presents a challenge. Parallel application runs can easily produce extremely large traces, exceeding hundreds of gigabytes [50]. Manual analysis of large trace volumes remains challenging, even with help of analysis tools like Vampir [5] or Scalasca [70]. Currently, users must rely on visual inspection of event traces, which is cumbersome and error-prone to say the least.

C. Dealing with Faults in HPC Systems  Several studies [4], [12] agree that the number of compo- nents in future computing systems will increase. The studies also predict that the failure rate of single components will not improve. As a result the dependability of the whole system will decrease and the probability of faults cannot be ignored any longer. Our goal is to detect faults in large data sets such as system log files, hardware monitoring data, application traces, and profiles. Furthermore, we want to understand and predict the effects of these faults on applications performance to enable developers to implement efficient error handling methods.

Usually computing systems show correlated failures. In other words, an error often implies further errors. As a result, we not only have to detect single errors but rather find correlations between multiple faults and generate models for these error chains. Finally we combine the resilience data with application performance data to determine the effects of a fault on applications performance. For recording performance data we use established techniques like tracing and profiling.



III. VISUALIZATION TECHNIQUES  The most common method to analyze data is manual inspection by the analyst. Manual inspection is the method of choice when it is not clear how to obtain the desired information automatically, for instance, analyzing the root- cause of an error (during a job execution or a machine crash) or optimizing a process.

Today most visualization tools provide graph and statistic displays. Graph displays usually show one or multiple param- eters over time. Some versions use color-coding to present more information on the available screen space. Fig. 1 shows the GUI of the tool AMon [44] as an example of a color-coded display. Each bar represents an individual job with a measured  Fig. 1. Screen-shot of the AMon-GUI showing monitoring data of multiple jobs (one bar per job) with a color-coded representation of the consumed memory of the individual jobs  parameter, e.g., used memory, in a color-coded fashion. This method is only scalable to hundreds of jobs, not only because of the limited screen space. The more data to be visualized the more challenging it gets for the analyst to find relevant information or correlations in reasonable time.

Another strategy is to reduce the amount of data by accumulation. However, data reduction comes along with the risk of losing relevant information. Typical representations of accumulated data are profiles and statistics. Although profiles and statistics are more scalable with respect to available screen space, their ability to identify abnormal behavior is limited.



IV. ANALYSIS METHODS  Following we describe a selection of algorithms and cat- egories of algorithms which appear to be suitable for our filtering and analysis challenges. This selection is based on experience, discussions with scientists from various fields, and literature research. It covers a wide range of algorithms and provides an overview of solving strategies applicable to a large number of analysis problems from multiple research areas.

A. Clustering Algorithms  Clustering algorithms divide data into groups (clusters).

The resulting groups should be meaningful and facilitate understanding and exploration of the data. However clustering itself is a difficult action and is based on many assumptions and contexts. As a result many differing clustering algorithms have been developed. One of the most simple and known clustering algorithm is k-means [25], [43], [48]. k-means partitions the data in a number of Voronoi cells by minimizing. It assign each data point to the cluster with the nearest mean. The number of the cells (clusters) k needs to be known a priori. To guide the choice of k the tool silhouette [56], [63] is helpful. It measures and allows to compare the cluster quality for various numbers of k. An adaption of k-means is the k-medoids algorithm [34], [35]. In contrast to the k-means algorithm, which computes cluster centers from data points, k-medoids chooses data points as cluster centers. This makes the algorithm more robust to noise and outliers. Both algorithms have a tendency to produce     clusters of the same size, which might not be appropriate for all data sets. A different clustering approach is applied by the DBSCAN algorithm [14]. This algorithm builds clusters using the density of data points. Very similar, close to each other, data points build a cluster. The algorithm can produce clusters of arbitrary shape and size and is capable of identifying noise points. A complete taxonomy of clustering techniques is provided by Jain et al. [32], [33].

All described clustering algorithms are based on points in n-dimensional space which forbids a direct application using measurement series data. A transformation from series of measurements to n-dimensional points, e.g., by compression methods, is required. There are various algorithms to realize this. One example are Fourier-coefficient-based methods which tend to focus on frequency information of time series [16]. The wavelet-based approach described by numerous authors [30], [58], [62], [66] additionally includes timing information. A stepwise approximation approach is described by several au- thors [36], [65].

The combination of clustering algorithms and compression is a very generic method and can be used for all kinds of time series data. For instance Gavrilov et al. [21] use it to analyze the stock-market.

Considering the large set of available solutions, the most challenging part is selecting the most adequate algorithms for clustering and compression along with identifying good parameters. Only properly adapted and configured algorithms will produce meaningful groups out of the input data.

B. Cross-Correlation  Correlation functions, known from the field of signal analysis [67], calculate the similarity of two functions. The cross-correlation function is defined as follows:  (f1 ? f2)(?) =  ? f1(t) ? f2(t+ ?) dt (1)  ? denotes the offset between the functions f1 and f2. There are also adaptations for discrete signals available and it is possible to adapt the continuous function to non-discrete sequences1 of measurements by using interpolation.

A typical application for cross-correlation is to determine the time-based shift of two signals. An example is shown in Fig. 2. x1(t) is the transmitted signal while x2(t) is the received signal. To determine the time shift between both signals (e.g., to calculate the distance the signal traveled for usage in radar systems) the cross-correlation is calculated.

The maximum value of the cross-correlation indicates the corresponding value of ? , which is the time shift between f1 and f2.

The classical usage of cross-correlation allows to align two functions with one arbitrary offset. For more complex scenarios containing dynamic changes of the offset and extra loops with repeating sequences a transformation of one of the functions is needed. This transformation can be realized by applying a defined fixed offset. Consequently, the offset in the cross-correlation can be removed by defining ? = 0. This  1Non-discrete sequences are series of measurements where it is not guar- anteed that the measurements are taken at constant time intervals.

Fig. 2. Example of two similar, non-identical functions shifted by time ?  leads to an optimization problem. The cross-correlation has to be maximized by finding a transformation respecting the dynamic time shift caused by altering input data due to, e.g., heterogeneous hardware (CPU or memory speed) or system noise. Without this transformation the cross-correlation result would be useless as similarity metric.

One strategy to determine the dynamic time shift of a complete time series is educated guessing. Educated guessing uses characteristic points like fast rising CPU usage, starting of jobs, or writing output to a file. It matches corresponding points in both series for a coarse alignment and recalculates the cross-correlation for each pair. Depending on the cross- correlation result the respective pair of corresponding points is retained or discarded. Additional optimization strategies are genetic algorithms or machine learning.

C. Genetic Algorithms  Genetic algorithms [64] evolve a start set of solutions towards better solutions. They describe a optimization heuristic inspired by natural evolution. Goal is to find a solution, called a genome, with a high fitness [29]. A genome consists of a sequence of parameters which are called characters that represent a solution to an arbitrary problem. A high fitness classifies a particular genome providing a better solution then other genomes.

The first step in a genetic algorithm is the definition of a start set of genomes, a population. The size of the first population has to be carefully balanced. If the size is too large, a lot of computing resources are needed to process the evolution step from population to child population. Also more evolution steps are needed to produce good results. Otherwise, if the population is too small, the searched parameter range is too limited and the result quality might seriously deteriorate.

Hence, the size of the population is one of the fundamental parameters [23] and needs to be carefully chosen.

Two common strategies exist for the selection of the genomes (solutions) for the first population. One strategy is to randomly generate a first population [64]. This enables a search over the whole parameter range but tends to require large populations what poses high demands on computing power. The other strategy [11] uses an already found solution set as base for further optimization. This reduces the size of the population along with the required computing power dramatically but also reduces the searched parameter range.

To evolve from one population to the child population, containing better solutions (genomes), the concepts of selec- tion, inheritance, and mutation are applied. Selection is the     first step in the evolution process from a population to a child population. Therefore the genomes are ranked. This is done by testing how well each genome solves the optimization prob- lem. Thus, a function calculating the quality of the solution produced by a genome is required. This function also defines the target of the optimization and has enormous influence on the found solution. After the ranking the best genomes, usually the best 50%, are selected as source for the generation of a new population.

After the selection the inheritance step generates a new population using the selected genomes. There are various methods to combine genomes to new ones. A simple solution is to randomly create pairs of genomes and to combine the first half of one genome with the second half of the other genome. The number of characters taken from the first genome is also chosen randomly. The order of characters in a genome might influence the computed solution [23]. For instance the parameter A might improve a solution only if parameter B has an appropriate value. In such case both parameters must be developed together and the inheritance step should tend to take both parameters from the same genome. In the end of the inheritance the child population needs to have the same size as the original population.

The last step to complete the evolution phase is mutation.

Therefore some genomes are chosen randomly and one or more characters are altered. The number of mutations depends on the algorithm configuration. If the number is too small, the algorithm behaves like a hill climbing search [22]. A too high mutation rate results in a random search and can prevent a stabilization of the population.

The complete evolution process requires many repetitions.

The number of repetitions can be either set by the user or the evolution process is repeated until the population is stable. In a stable population most or all genomes are the same.

The result of the genetic algorithm is a dominant or the fittest genome in the last population. Why one particular genome is chosen as result cannot be told [2]. It is not guaranteed that the result is reproducible by a re-run of the algorithm. There is also no guarantee for the globally best solution.

D. Sequence Comparison  Comparing sequences is heavily done in the field of bioinformatics using sequence alignment techniques. Goal is to compare gene and protein sequences. For instance, to find genes in large genome databases spanning several species. The hope is that the knowledge of many occurrences of a particular gene in nature helps in understanding its functionality. The difficulty is that genes are likely to mutate across species and along the evolutionary process. That makes them hard to spot since searching for direct matches is not sufficient.

The alignment algorithms need to identify similar sequences.

Mutated gene sequences may consist of equal areas, having the same sequence of nucleotides in both genes. Areas can be different if they contain different nucleotides at the same sequence position. The third possibility is a missing section, called a gap, in one sequence.

The alignment of two sequences is computed using dy- namic programming that finds the optimal alignment for  ?? ??  ????  ?                  ? ? 	?  ? ? ?  ?  ? ???  Fig. 3. Alignment of sequence A and B  arbitrary sequences [3], [24], [53]. For example consider an alignment of the following two sequences:  Sequence A: m c a c m a m Sequence B: m c a c b c m b m  We use the following notation: Sequence A is of length M and sequence B of length N . The ith event in A is Ai and the jth event in B is Bj .

The dynamic programming algorithm separates the full pairwise alignment problem into independently optimizable sub-problems and then evaluates alignments with scores. It uses a recursive scoring scheme to find the optimal alignment of sub-problems. The scoring scheme is adapted to the individ- ual comparison case. A typical example for arbitrary sequences is as follows:  ?i,j = 2 Match Score ?i,j = ?1 Mismatch Score ? = ?1 Gap Score  ?i,j = 2 is chosen if position Ai is the same as Bj , otherwise, if Ai differs from Bj then ?i,j = ?1 is used. In case that either Ai or Bj are aligned to a gap the gap penalty of ? = ?1 is applied. Based on the scores, we define the following recursive scoring scheme for a dynamic programming matrix P :  Pi,j = max  ?? ? Pi?1,j?1 + ?i,j , Match/Mismatch Pi,j?1 + ?, Gap in Sequence A Pi?1,j + ?. Gap in Sequence B  We find the optimal alignment by computing the path with the highest score through the matrix P from PM,N to P0,0. We show the alignment result for our example using our algorithm in Fig. 3.

The classical dynamic programming algorithm has quadratic complexity with respect to the sequence lengths, O(M ? N), in time and memory. This renders the align- ment of large sequences impossible due to the limited size of available main memory. A modification of the algorithm proposed by Hirschberg [28] computes the optimal alignment with quadratic time complexity but with only linear memory complexity with respect to the longest sequence.

A similar approach is proposed by Myers [52]. His O(ND) algorithm is used in the Unix diff tool to compare text input.

Unlike the dynamic programming approach the algorithm does not use a scoring scheme. Instead it searches the shortest path through the edit graph of the two sequences. The shortest path identifies equal and different areas of the sequences.

Compared to the dynamic programming approach this algo- rithm is less flexible. Due to the missing scoring scheme it can only decide between two states, equal and different.

The dynamic programming algorithms can work with arbitrary states by assigning different scores to possible combinations of sequence items. The advantage of the O(ND) algorithm is its performance compared to dynamic programming algorithms.

The performance of the O(ND) algorithm is dependent on the result of the comparison. Two almost equal sequences can be compared with nearly linear time complexity. The more the sequences differ the slower the O(ND) algorithm gets. Two completely different sequences require, like dynamic programming, quadratic time complexity.

E. Pattern Matching and Intrusion Detection  Intrusion detection is a field of research which takes the challenge to detect attacks and intrusions into computing sys- tems and networks. The used methods and algorithms aim to detect patterns that indicate an attack. Often intrusions follow already seen patterns. Consequently, many implementations for intrusion detection look for these patterns. Examples for such implementations are Snort [55], STATE [31], IDES [46], IDIOT [38], and Bro [54]. A fuzzy definition of intrusion patterns is applied by Dickerson [10].

A special variant of an intrusion is a worm. A worm tries to replicate itself and infect additional systems. This behavior forms a known pattern with only little dependence on the actual worm. The detection of such behavior is described by several authors [37], [47], [60], [68]. However, the detection of a worm is bound to a defined scenario, an internal computer network connected to an external one. This complicates the adaption to other fields of research.

The search for already known patterns does not allow to find new, not yet understood behavior or attacks. To mitigate this disadvantage additional concepts have been developed to cope with new or changing intrusions. One of the early concepts is to analyze the statistic of occurring events as described in Sec. IV-F. Another method is machine learning which is presented in Sec. IV-G.

F. Statistic of Events  The basic idea of this approach is the comparison of statistics of events. A description of this strategy is given by Denning [9]. Later on several authors extended this concept [39], [59].

In this approach the first step is to define events. An event can be the direct occurrence of an action like a log-in attempt into a system or a derivative value like a high network load for some seconds. The second step is to compare the rate of these events against a defined expectation. The expectation is based on historical data and can be simple minimal or maximal rates or distributions over time. This approach also requires the adaption of the expectation to changing user behavior.

G. Machine Learning  Machine learning as described in the context of intrusion detection [7], [40]?[42] uses a self-learning system. The sys- tem is trained with historical data. The training sets teach the system how to distinct intrusions from normal user behavior.

After the training phase the skilled system is used to detect intrusions.

Machine learning can also be used in other fields of re- search. We are interested in using machine learning to distinct normal from irregular behavior.

For a non-expert in the field of machine learning it is challenging to select a good algorithm for the actual problem scenario. Also the preparation of sufficient training data is cumbersome and error-prone. After the training phase it is recommended to test whether the training was successful.

Therefore, in addition to the training data, also test data sets are required.

There are additional (self-)learning algorithms available.

They are in general similar to machine learning. Neural network based algorithms are described in [8], [47]. Agent based methods can be found in [1], [61].

H. Tree and Graph Search  Relationships and dependencies between individual events in data sets can be modeled as trees or graphs. This is used for instance to organize databases or to construct XML documents.

Due to enormous research effort in the area of tree and graph search algorithms both data structures can be queried efficiently today.

In general, tree and graph search algorithms are differenti- ated into depth-first and breadth-first methods [15]. However, processing the whole data set is impractical at large-scale.

In such case heuristics improve the time to find a satisfying solution [71]. Heuristics do not guarantee to find the optimal solution. Nevertheless, this constraint can often be accepted.

There are also algorithms which rely on adjustments by the user to guide the process of finding a solution for a given problem. These approaches are similar to machine learning described in Sec. IV-G.



V. EVALUATION  Following we provide an evaluation of the methods pre- sented in Sec. IV. Since we are interested in several fields of research we divide this section according to our fields of interest as described in Sec. II. This also allows to evaluate the methods in the actual problem context. Furthermore, we provide a summary of the evaluation in Table I.

A. Series of Measurements  Job centric monitoring requires the analysis of measure- ment series. Where a series of measurements represents the collected monitoring data of one job. A measurement point in a series consists of a time-stamp along with relevant character- istics about the job itself (e.g., consumed CPU-time) and the respective computing resource (e.g., free main memory).

Clustering algorithms can be applied to measurement series in general. Necessary for clustering is a prior compression of the time series data. Subsequently, it has to be ensured that the compression does not eliminate relevant information. However, with the goal of finding unknown anomalies it is hard to distinguish relevant from non-relevant information at this stage of the analysis. Also the clustering algorithm parameters have to be configured according to the actual scenario. Finally, self- learning clustering algorithms, e.g., based on machine learning     TABLE I. EXPECTED USEFULNESS OF ANALYZES METHODS FOR THE THREE TOPICS OF RESEARCH  MONITORING TRACING RESILIENCE  Clustering Algorithms ? ? ? Correlation ? ? ? Genetic Algorithms ? ? ? Sequence Comparison ? ? ? Pattern Matching ? ? ? Statistic of Events ? ? ? Machine Learning ? ? ? Tree and Graph Search ? ? ?  ? solves problem at least partially and is already in usage ? very promising algorithm ? probably usable with strong adaption ? does not fit well  strategies, look most promising. They are also the most com- plex variants requiring considerable effort for adaption to new problem cases.

Cross-correlation appears to be a direct match to identify irregular measurement series. The challenge is to optimize the algorithm to avoid false-positives caused by heterogeneous systems with, e.g., different CPU and network speeds. This can be realized by educated guessing or other optimization strategies like genetic algorithms.

Sequence comparison algorithms use input data in the form of a sequence of characters, omitting all timing information.

However, the exclusion of all timing information eliminates a relevant part of the information. Thus, this approach is not considered a suitable solution.

Pattern matching or searching for known problems is already implemented for job centric monitoring [13]. However, these methods are not capable of detecting new, undefined problems and can lead to a high false-positives rate.

Statistic of events is a straight-forward and easy to adapt algorithm. It can be automatically adapted to actual job behav- ior. For instance the adaptation can be applied by continuous calculation of statistic over historical job behavior. This ap- proach looks very promising.

Machine learning algorithms require a learning phase to correctly train the algorithm. The algorithm results depend highly on the quality of the learning phase. However, the compilation of a suitable, high-quality collection of training examples is very time consuming and error-prone. Thus, we give preference to other approaches first.

The data format required to apply tree and graph search algorithms as well as for direct use of genetic algorithms differs from the generated data format used in job-centric monitoring. Due to the needed adaption an implementation of these approaches requires considerable effort.

B. Performance Optimization of Parallel Applications  In the field of performance optimization of parallel appli- cations a range of methods is already successfully applied.

Casas et al. [6] use wavelet and cross-correlation algo- rithms to analyze parallel application executions. With wavelet analysis they can automatically divide the execution in ini- tialization, finalization, and computation phase. Using cross- correlation on the computation phase identifies the individual iterations. The automatic identification of the application struc- ture enables further analysis methods like detection of load- balance problems or automatic speedup analysis.

Gamblin et al. [20] use clustering for the performance anal- yses of massive parallel applications. The clustering process builds groups of similar behaving processes, representing the application behavior. For each group a representative process, representing the behavior of the individual group, is selected.

The analyst can focus on the representative processes and does not need to analyze a massive number of processes anymore.

Scalasca [70] is able to automatically find predefined patterns, like synchronization wait-time, in the trace data.

However, Scalasca does not guarantee the detection of all occurrences of the predefined patterns and is only able to detect a subset of all possible performance problems.

Weber et al. [69] use sequence alignment methods to compare event traces of parallel application runs. Due to the large number of events manual inspection and comparison of the traces is time consuming and error-prone. The described alignment methods compare the traces and automatically iden- tify differences and similarities.

Genetic algorithms, machine learning, and graph search algorithms seem not do be suitable for automatic analysis of trace data. At least they need an extensive adaption. For instance the large amount of events in traces along with the variety of possible event types and their complex dependencies render the preparation of suitable training sets for machine learning algorithms an error-prone task. It is challenging to find real application traces with isolated performance problems.

Furthermore it is questionable if the algorithms are able to learn the characteristics of certain performance problems and are able to detect trained problems in new traces. This is especially true if the traces originate from new applications.

Using statistics of events is suitable to detect a subset of performance problems. For instance, it is an appropriate method to detect outliers in the data.

C. Dealing with Faults in HPC Systems  Common research in the field of resilience at large scale is mostly based on statistical data, for example system log files.

Martino [49], Lu [45], and Fu et al. [17] analyze statistical failure data and use clustering to group failures which correlate in time and space domain. Analysis of statistical failure data can indicate possible failure correlations. However, it does not relate causes and effects directly, because statistical data can not model relationships between failures.

Gainaru et al. [19] combine signal analysis and data mining techniques to analyze log files. In this work they use signal correlation to distinguish individual event types as well as data- mining concepts to analyze correlations between events in the presence of faults. In addition, Gainaru et al. [18] use machine learning techniques to predict normal and faulty application behavior based on the analyses of message transfers.

Fault trees and reliability graphs can be used to model systems. Nevertheless, they are not suitable to analyze system availability [57]. Markov chains or generalized stochastic Petri nets are more suitable in this case.



VI. CONCLUSION  Methods for automatic or semi-automatic analysis of large data sets are still an active research field. Nevertheless, a wide range of methods is already available. These methods enable automatic identification of irregular data, and hence, provide support for tracking down new phenomenons, causalities, or faults. Consequently, the amount of information requiring manual inspection as well as the time required for the analysis might be reduced dramatically.

This work presents a wide range of algorithms relevant for automatic data analysis. Since most papers focus only on a single method or compare only very similar or suc- cessive versions of algorithms we present a very rare and useful overview. Most analysis methods highly depend on the data format and analysis context. Hence, we surveyed the algorithms for applicability in three different scenarios.

These use cases have been selected from current research areas.

Therefore, our evaluation of data analysis methods serves as a basis and starting point for scientists facing similar problems.

A detailed description of the adaption, implementation, and evaluation of the presented algorithms with example data is beyond the scope of this work. For detailed information about individual methods a comprehensive list of related work is provided. Additionally, a more detailed evaluation of selected methods is planned as future work.

