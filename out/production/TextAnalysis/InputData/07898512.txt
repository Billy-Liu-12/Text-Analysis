784 IEEE SIGNAL PROCESSING LETTERS, VOL. 24, NO. 6, JUNE 2017

Abstract?In this note, we analyze an iterative soft/hard thresh- olding algorithm with homotopy continuation for recovering a sparse signal x? from noisy data of a noise level ?. Under suit- able regularity and sparsity conditions, we design a path, along which the algorithm can find a solution x?, which admits a sharp reconstruction error ?x? ? x???? = O(?) with an iteration com- plexity O( ln ?ln ? np), where n and p are problem dimensionality and ? ? (0, 1) controls the length of the path. Numerical examples are given to illustrate its performance.

Index Terms?Continuation, convergence, iterative soft/hard thresholding (IST/IHT), solution path.



I. INTRODUCTION  S PARSE recovery has attracted much attention in machinelearning, signal processing, statistics, and inverse problems over the last decade. Often, the problem is formulated as  y = ?x? + ? (1)  where x? ? Rp is the unknown sparse signal, y ? Rn is the data with the noise ? ? Rn of level ? = ???, and the matrix ? ? Rn?p with p? n has normalized columns {?i}, i.e., ??i? = 1, i = 1, . . . , p. The desired sparsity structure can be enforced by either the ?0 or ?1 penalty, i.e.,  min x?Rp  ??x? y?2 + ??x?t , t ? {0, 1} (2)  where ? > 0 is the regularization parameter.

Among the existing algorithms for minimizing (2), itera-  tive soft/hard thresholding (IST/IHT) algorithms [1]?[4] and their accelerated extension [5], [6] are extremely popular. These  Manuscript received March 1, 2017; revised April 9, 2017; accepted April 10, 2017. Date of publication April 12, 2017; date of current version April 24, 2017.

The work of Y. Jiao was partially supported by the National Science Foundation of China (NSFC) (11501579) and the National Science Foundation of Hubei Province (2016CFB486). The work of B. Jin was supported by the Engineering and Physical Sciences Research Council under Grant EP/M025160/1. The work of X. Lu was supported by the NSFC (11471253 and 91630313). The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Marco Felipe Duarte. (Corresponding author. Xiliang Lu.)  Y. Jiao is with the School of Statistics and Mathematics and Big Data Institute, Zhongnan University of Economics and Law, Wuhan 430063, China (e-mail: yulingjiaomath@whu.edu.cn).

B. Jin is with the Department of Computer Science, University Col- lege London, London WC1E 6BT, U.K. (e-mail: bangti.jin@gmail.com; b.jin@ucl.ac.uk).



X. Lu is with the School of Mathematics and Statistics and the Hubei Key Laboratory of Computational Science, Wuhan University, Wuhan 430072, China (e-mail: xllv.math@whu.edu.cn).

Color versions of one or more of the figures in this letter are available online at http://ieeexplore.ieee.org.

algorithms are of the form  xk+1 = T?k ? ( xk + ?k?t(y ? ?xk )  ) (3)  where ?k is the stepsize, and T? is a soft- or hard-thresholding operator defined componentwise by  T?(t) = {  max(|t| ? ?, 0)sgn(t), IST ?{|t|>?2?}(t), IHT  (4)  where ?(t) is the characteristic function. Their convergence was analyzed in many works, mostly under the condition ?k < 2/???2 . This condition ensures a (asymptotically) con- tractive thresholding and thus the desired convergence [1]?[4].

Meanwhile, it was observed that the continuation along ? can greatly speed up the algorithms [6]?[10]. Nonetheless, as pointed out by Tropp and Wright [11] ?... the design of a robust, practical, and theoretically effective continuation algorithm remains an interesting open question ....? There were several works aiming at filling this gap. In the works [12], [13], a proximal-gradient method with continuation for the ?1 problem was analyzed with linear search, under the sparse restricted eigenvalue/restricted strong convexity condition. Recently, a Newton-type method with continuation was studied for ?1  and ?0 problems [14], [15]. In this work, we present a unified approach to analyze IST/IHT with continuation and a fixed stepsize ? = 1, denoted by iterative soft/hard-thresholding with continuation (ISTC/IHTC). The challenge in the analysis is the lack of monotonicity of function values due to the choice ? = 1.

The overall procedure is given in Algorithm 1. Here, ?0 is an initial guess of ?, supposedly large, ? ? (0, 1) is the decreas- ing factor for ?, and Kmax is the maximum number of inner iterations (for a fixed ?). The choice of the final ?? is given in (5) below. Distinctly, the inner iteration does not need to be solved exactly (actually one inner iteration suffices the desired accuracy of the final solution x?, cf. Theorem 2), and there is no need to perform stepsize selection.

In Theorem 2, we prove that under suitable mutual coher- ence (MC) condition on the matrix ? (cf., Assumption 2.1 and Remark 2.2), ISTC/IHTC always converges.



II. CONVERGENCE ANALYSIS  The starting point of our analysis is the next lemma.

Lemma 1: For any x, y ? R, there holds  |T?(x+ y) ? x| ? { |y| + ? IST |y| + ?2? IHT.

See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

JIAO et al.: ITERATIVE SOFT/HARD THRESHOLDING WITH HOMOTOPY CONTINUATION FOR SPARSE RECOVERY 785  Algorithm 1: Iterative Soft/Hard-Thresholding With Con- tinuation.

1: Input: ? ? Rn?p , y, ?0 , ? ? (0, 1), ??, Kmax ? N, x(?0) = 0.

2: for ? = 1, 2, ... do 3: Let ?? = ????1 , x0 = x(???1).

4: If ?? < ??, stop and output x? = x0 .

5: for k = 0, 1, ...,Kmax ? 1 do 6: xk+1 = T?? (x  k + ?t(y ? ?xk )).

7: end for 8: Set x(??) = xKm a x 9: end for  Proof: By the definition of the operator T?, cf. (4),  |T?(x+ y) ? x| ? |T?(x+ y) ? (x+ y)| + |y|  ? { |y| + ? IST |y| + ?2? IHT  which completes the proof of the lemma. ? Let the true signal x? be s-sparse with a support A?, i.e.,  s = |A?|, and I? the complement of A?. Recall also that the MC ? of the matrix ? is defined by ? = maxi ?=j |??i, ?j ?| [16].

Assumption 2.1: The MC ? of ? satisfies ?s < 1/2.

The proper choice of the regularization parameter ? is essen-  tial for successful sparse recovery. It is well known that under Assumption 2.1, the choice ? = O(?) for the ?1 penalty and ? = O(?2) for the ?0 penalty ensures ?x? x???? = O(?) [15], [17]. Thus, we consider the following a priori choice  ?? =  ? ???  ???  C1?, with C1 >  1 ? 2?s, for ISTC  C0? 2 , with C0 >  2(1 ? 2?s)2 , for IHTC.

(5)  In practice, one may consider a posteriori choice rules [18].

Now, we can state the global convergence of Algorithm 1.

Theorem 2: Let Assumption 2.1 hold, and ?? be chosen by (5). Suppose that ?0 is large, Kmax ? N, and  ? ?  ? ???  ???  [2?s/(1 ? 1/C1), 1), for ISTC [(  2?s 1 ? 1/(2C0)1/2  )2 , 1  ) , for IHTC.

Then, Algorithm 1 is well defined, and the solution x? satisfies 1) supp(x?) ? A?, 2) there holds the error estimate  ?x? ? x???? ? {  (C1 ? 1)?/(?s), for ISTC (?  2C0 ? 1 ) ?/(?s), for IHTC.

Furthermore, if mini?A? |x?i | is large enough, then supp(x?) = A?.

Proof: We only prove the assertion for ISTC, since that for IHTC is similar. The choice of C1 in (5) implies C1 > 1 and  2?s 1?1/C1 < 1, and thus, the choice of ? makes sense.

First, we consider the inner loop at lines 5?7 of Algorithm 1 and omit the index ? for notational simplicity. Let Ek = ?xk ? x???? , and ? = 1?1/C1?s . Consider one IST iteration from xk to xk+1 . The key step to the convergence proof is the following  implication: with Ak = supp(xk ) Ak ? A? and Ek ? ??  ? Ak+1 ? A? and Ek+1 ? ??? ?? ? ??. (6) Now, we show this claim. It follows from (1) and ??i? = 1 the following componentwise expression for the update:  xk+1i = T?(x k i + ?  t i(y ? ?xk ))  = T? ( x?i + ?  t i(?A??Ak \{i}(x  ? ? xk )A??Ak \{i} + ?) ) .

By the hypothesis in (6), Ak ? A?, Ek ? ??, ? ? ??, and (5), we deduce that for any i ? I?  |x?i + ?ti(?A??Ak \{i}(x? ? xk )A??Ak \{i} + ?)| ? |?ti(?A?(x? ? xk )A? | + |?ti?|  ? ?sEk + ? ? (  C1  + ?s? )  ? = ?  by the definition of ?, and the second inequality follows from [15, Lemma 2.1]. Hence, |xk+1i | ? |T?(?sEk + ?)| = 0, which implies directly Ak+1 ? A?. Meanwhile, under (6) and (5), for any i ? A?, by Lemma 1, we deduce  |xk+1i ? x?i | ? ? + |?ti(?A?\{i}(x? ? xk )A?\{i}| + |?ti?|  ? ? + ?(s? 1)Ek + ? ? ? + ?s?? + 1 C1  ?  = (  1 + C1  + ??s )  ? = 2? ? ???.

Thus, we have Ek+1 ? ???, i.e., the claim (6) holds.

Next, we prove the following assertion by mathematical in-  duction: for all ? with ?? ? ??, there holds supp x(??) ? A?, ?x(??) ? x???? ? ???? . (7)  Since ?0 is large, it satisfies (7). Now, assume (7) holds for ???1 , i.e., supp x(???1) ? A? and ?x(???1) ? x???? ? ?????1 . When Algorithm 1 runs lines 3?7 for ?? , since x0 = x(???1), then we have A0 ? A? and E0 ? ??? . From (6), we obtain that for all k ? 1, Ak ? A? and Ek ? ???? . In particular, if we choose k = Kmax , then (7) holds for ?? . When Algorithm 1 terminates for some ?? < ??, then ???1 ? ?? and x? = x(???1). From (7), we have supp x? ? A? and ?x? ? x???? ? ??? = (C1 ? 1)?/(?s). Likewise, if mini?A? |xi | > (C1 ? 1)?/(?s), property (ii) implies supp(x?) = A?.

Last, we briefly discuss IHTC. For the choice C0 in (5),  ? ? [( 2?s1?1/(2C0 )1 / 2 )2 , 1) makes sense. With ? = 1?1/(2C0 )1 / 2  ?s , a similar argument yields  Ak ? A? and Ek ? ? ?  2?  ? Ak+1 ? A? and Ek+1 ? ? ?  2??.

The rest follows like before, and thus, it is omitted. ? Remark 2.1: The proof works for any choice Kmax ? 1, in-  cluding Kmax = 1. In practice, we fix it at Kmax = 5. This together with Theorem 2 allows estimating the complexity of Algorithm 1. At each iteration, one needs to compute matrix- vector product ?x and ?ty, and for each ?, the number of iterations is bounded by Kmax . The overall cost depends on the decreasing factor ? by O( ln ?  ? ln ? np) = O(  ln ? ln ? np).

786 IEEE SIGNAL PROCESSING LETTERS, VOL. 24, NO. 6, JUNE 2017  Remark 2.2: Conditions similar to Assumption 2.1 have been widely used in the literature, for analyzing Orthogonal Matching Pursuit (OMP) [17], [19], [20] (with (2s? 1)? ? 1) and for bounding the estimation error of Lasso [21], [22] (with 7s? < 1 and 4s? ? 1). Thus, Assumption 2.1 is fairly standard.

Examples of matrices with small MC ? include that formed by equiangular tight frame and random sub-Gaussian matrices [23]. Furthermore, we note that other similar conditions, e.g., restricted eigenvalue condition and RIP conditions, were also used to derive error bounds of the type ?x? x??2 = O(?) for proximal-gradient homotopy (PGH) algorithms [12], [13] and Greedy methods, e.g., Compressive Sampling Matching Pursuit (CoSaMP) [24], NIHT [25], and CGIHT [26].



III. NUMERICAL RESULTS AND DISCUSSIONS  Now, we present numerical examples to show the convergence and the performance of Algorithm 1. First, we give implemen- tation details, e.g., data generation, parameter setting for the algorithm. Then, our method is compared with several state-of- the-art algorithms in terms of reconstruction error and recovery ability via phase transition.

A. Implementation Details  Following [6], the signals x? are chosen as s-sparse with a dy- namic range DR := max{|x?i | : x?i ?= 0}/min{|x?i | : x?i ?= 0}.

The matrix ? ? Rn?p is chosen to be either random Gaussian matrix, or random Bernoulli matrix, or the product of a partial fast Fourier transform (FFT) matrix and inverse Haar wavelet transform. Under proper conditions, such matrices satisfy Assumption 2.1. The noise ? has entries following independent and identically distributed N(0, ?2).

We fix the algorithm parameters as follows: ?0 = ??ty?? and ?0 = ??ty?2?/2 for ISTC and IHTC, respectively [14], [15], decreasing factor ? = 0.8. Since the optimal ?? depends on the noise level ?, which is often unknown in practice, we predefine a path ? = {??}N?=0 with ?? = ?0?? and N = 100.

Then, we run Algorithm 1 on the path ? and select the optimal ?? by the Bayesian information criterion [14]. All the computa- tions were performed on an eight-core desktop with 3.40 GHz and 12-GB RAM using MATLAB 2014a. The MATLAB package ISHTC for reproducing all the numerical results can be found at http://www0.cs.ucl.ac.uk/staff/b.jin/companioncode.html.

First, we illustrate Theorem 2 by examining the influence of sparsity level s, coherence?, and noise level? on IHTC recovery on three settings (n = 500, p = 1000, and DR = 100):  1) random Gaussian ?, ? = 1e-2, s = 10 : 10 : 100.

2) random Gaussian ?, s = 50, ? = 1e-4,1e-3,1e-2,1e-1,1.

3) ? is random Gaussian with correlation, where the pa-  rameter ? controls the coherence ? (see [27, Sec. 5.1] for details). In general, a larger parameter ? gives a larger ? (a typical example: ? = 0.19 for ? = 0; ? = 0.33 for ? = 0.15; ? = 0.56 for ? = 0.3; and ? = 0.74 for ? = 0.5). We choose ? = 0 : 0.05 : 1, s = 10, ? = 1e-3.

The results in Fig. 1 are computed from 100 independent realizations. It is observed that when the sparsity level s and noise level ? and incoherence ? are small, IHTC recovers the exact support with high probability, as implied by Theorem 2.

B. Comparison of ISTC With ?1 Solvers  Now, we compare ISTC with four state-of-the-art ?1 solvers: GPSR [8] (http://www.lx.it.pt/mtf/GPSR/), SpaRSA [9] (http://www.lx.it.pt/mtf/SpaRSA/), PGH method [12] (https: //www.microsoft.com/en-us/download/details.aspx?id=52421),  Fig. 1. Exact support recovery probability versus s, ?, and ? . (a) s (b) ? (c) ? .

TABLE I NUMERICAL RESULTS (CPU TIME AND ERRORS), WITH RANDOM BERNOULLI ?, OF SIZE p = 10 000, 18 000, n = ?p/4?, s = ?n/40?, WITH DR = 100  AND ? = 5E-2  p method time (s) nMV Re?2 Ab??  ISTC 1.0 58 4.21e-3 2.66e-1 PGH 1.7 419 4.14e-3 2.66e-1  10 000 SpaRSA 3.4 302 4.13e-3 2.63e-1 GPSR 3.0 256 4.25e-3 2.71e-1 FISTA 5.3 505 4.30e-3 2.65e-1 ISTC 3.3 58 4.34e-3 2.88e-1 PGH 5.6 443 4.25e-3 2.85e-1  18 000 SpaRSA 11.4 309 4.25e-3 2.84e-1 GPSR 9.5 258 4.36e-3 2.91e-1 FISTA 17.2 506 4.40e-3 2.74e-1  Fig. 2. Empirical phase transition curves for ISTC, PGH, SpaRSA, and GPSR, with ? = s/n and ? = n/p.

and FISTA [5] (implemented as https://web.iem.technion.ac.il/ images/user-files/becka/papers/wavelet_FISTA.zip).1  The numerical results [CPU time, number of matrix-vector multiplications (nMV), relative ?2 error (Re?2), and absolute ?? error (Ab??)] are computed from ten independent realizations of random Bernoulli sensing matrices with different parameter tuples (n, p, s,DR, ?), as shown in Table I. It is observed that ISTC yields reconstructions that are comparable with that by other methods but at least two to three times faster. Furthermore, it scales well with the problem size p.

Next, we compare the empirical performance of ISTC with other methods by their phase transition curves in the ?-? plane, with ? = s/n and ? = n/p. When computing the curves, we fix the dimension p = 1000, partition the range (?, ?) ? [0.1, 1]2 into a 30 ? 30 equally spaced grid, and run 100 independent simulations at each grid point. The s-sparse signal x? ? Rp , ma- trix ? ? Rn?p , and data y ? Rn are generated as [28, Fig. 13].

Fig. 2 plots the logistic regression curves identifying the 90% success rate for the algorithms. IHTC exhibits similar phase transition behavior as other methods.

1All the codes were last accessed on February 23, 2017.

JIAO et al.: ITERATIVE SOFT/HARD THRESHOLDING WITH HOMOTOPY CONTINUATION FOR SPARSE RECOVERY 787  Fig. 3. Reconstructed signals and their PSNR values.

TABLE II 1-D SIGNAL  method CPU time PSNR  IHTC 0.41 51 OMP 1.20 49 NIHT 0.96 46 CoSaMP 0.49 26 CGIHT 0.98 49  Left: 1-D signal with n = 665, p = 1024, s = 247, and ?=1e-4. Right: 2-D image with n = 34 489, p = 262 144, s = 7926, and ? = 3e-2.

C. Comparison of IHTC With Greedy Solvers  Now, we compare IHTC with four state-of-the-art greedy methods for the ?0 problem, to recover 1-D signal and benchmark MRI image. These methods include OMP [19] (https://sparselab.stanford.edu/SparseLab_files/Download_files/ SparseLab21-Core.zip), normalized IHT (NIHT) [25] (http:// www.gaga4cs.org/), CoSaMP [24] (http://mdav.ece.gatech.

edu/software/SSCoSaMP-1.0.zip), and conjugate gradient IHT (CGIHT) [26] (http://www.gaga4cs.org/).

The underlying 1-D signal and 2-D MRI image are com- pressible under a wavelet basis. Thus, the data can be chosen as the wavelet coefficients sampled by the product of a par- tial FFT matrix and inverse Haar wavelet transform. For the 1-D signal, the matrix ? is of size 665 ? 1024 and consists of applying a partial FFT and an inverse two-level Harr wavelet transform. The signal under wavelet transform has 247 nonze- ros, and ? = 1e-4. The results are shown in Fig. 3 and Ta- ble II. The reconstruction by IHTC is visually more appealing than that of the others (cf., Fig. 3). The results by AIHT and CoSaMP suffer from pronounced oscillations. This is further confirmed by the peak signal-to-noise ratio (PSNR) value de- fined by PSNR = 10 ? log V 2MSE , where V is the maximum abso- lute value of the true signal, and MSE is the mean squared error of the reconstruction. Table II also presents the CPU time of the 1-D example, which shows clearly that IHTC is the fastest one.

For the 2-D MRI image, the matrix ? amounts to a par- tial FFT and an inverse wavelet transform, and it has a size 34 489 ? 262 144. The image under eight-level Haar wavelet transformation has 7926 nonzero entries and ? = 3e-2. The nu- merical results are shown in Fig. 4 and Table III. All ?0 methods produce comparable results, but the IHTC is fastest.

Next, we compare the empirical sparse recovery perfor- mance of IHTC with these greedy methods by means of phase transition curves in the ?? plane, with ? = s/n and ? = n/p.

When computing the curves, we fix the dimension p = 1000,  Fig. 4. Reconstructed MRI images and their PSNR values.

TABLE III 2-D IMAGE  method CPU time PSNR  IHTC 6.1 28 OMP 932 28 NIHT 9.4 27 CoSaMP 14.3 26 CGIHT 7.9 27  Fig. 5. Empirical phase transition curves of IHTC, OMP, CoSaMP, NIHT, and CGIHT, with ? = s/n and ? = n/p.

partition the range (?, ?) ? [0.1, 1]2 into a 90 ? 90 uniform grid, and run 100 independent simulations at each grid point.

Like before, the s-sparse signal x? ? Rp , matrix ? ? Rn?p and data y ? Rn are generated as [28, Fig. 13]. Fig. 5 plots the logistic regression curves identifying the 90% success rate for the algorithms. IHTC exhibits comparable phase transition phe- nomenon with other greedy methods, whereas CoSaMP per- forms slightly worse than others.



IV. CONCLUSION  In this paper, we analyze an IST/IHT algorithm with homo- topy continuation for sparse recovery from noisy data. Under standard regularity condition and sparsity assumptions, sharp reconstruction errors can be obtained with an iteration complex- ity O( ln ?ln ? np). Numerical results indicated its competitiveness with state-of-the-art sparse recovery algorithms. The results can be extended to other penalties, e.g., Minimax Concave Penalty (MCP) [29] or Smoothly Clipped Absolute Deviation Penalty (SCAD) [30].

