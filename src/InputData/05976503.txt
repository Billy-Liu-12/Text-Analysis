2011 3rd Conference on Data Mining and Optimization (DMO)  28-29 June 2011, Selangor, Malaysia

Abstract? Data mining is the process of finding correlations or patterns among dozens of fields in large relational databases.

While Association Rules Mining (ARM) algorithm especially the Apriori algorithm has been an active research work in recent years. Diverse improvement varies in term of producing more frequent items and also generating further k-length. The idea is to produce better pattern and more interesting rules. In this paper, we propose new approach for ARM based on Multiple Attribute Value within the non-binary search spaces.

The proposed algorithm improves the existing frequent pattern mining by generating the most frequent values (item) within the attribute and generate candidate based on the frequent attribute value. The main idea of our work is to discover more meaningful frequent items and maximum k-length items. The experimental results show that our proposed MAV frequent pattern mining enhance the impact in generating more frequents items and maximum length  Keywords- Multiple Attribute; frequent pattern mining; Frequent Items; Apriori

I.  INTRODUCTION Knowledge discovery or also known as data mining is the  processes involve penetration into tremendous amount of data with the help from computer technology for analysing the data. Data mining is a process of discovering interesting knowledge by extracting or mining from large amount of data and the process of finding correlations or patterns among dozens of fields in large relational databases[1, 2].

Association mining is one of the data mining tasks. The main task is to identify the relationship or correlation between items in dataset.  Extensive surveys on the association mining and also frequent pattern mining have been conducted by[3, 4]. Almost a decade numbers of issues related to improve the capability of the algorithm including searching strategy, pruning techniques and data structure involved. The improvements are toward producing more meaningful rules by satisfying minimal support and also confidence constraint. There are also researches related to improvements of the algorithm to meet the domain needs.

[4-10].

The association mining process generally consists of two main steps. The first step is to find the frequent itemset and subsequent of a frequent itemset that is inferred from frequent pattern generated. The second part in association is to generate strong association rules from the frequent itemsets. Finding the association between items and set of items provides valuable information across the domain such as marketing analysis, web and network log for pattern and also for the surveillance domain such as detecting the intrusion and also relation towards determining the outbreak cases in public health. Apriori-based algorithm is divided into 2 main categories: Candidate Generation and Test Approach[11] and Pattern Growth Methods[12].

The Candidate Generation and Test approach are based on  discovering the frequent itemsets through series of iteration and generate candidate itemsets. Reduction or pruning the itemset is based on the computation of support; normally supplied by user. Based on the minimum support, the pruning process will remove the non frequent items. Number of extension in Apriori includes Hashing, portioning, sampling techniques, dynamic itemset counting and incremental, parallel and distributed mining. While Pattern Growth Methods is based on divide and conquer approach that will generate frequent pattern without candidate generation. The technique is based on tree data structure.

Based on tree projection algorithm, database recursively projects the data sequence into smaller sub databases in order to reduce accessing database. H-Mine is the first algorithm that considers the pattern-growth. Improvement and other explorations of FP Growth include using hyper- structure by building alternative tree, top-down and bottom- up tree exploration and also using array ?based implementation.

The rest of the paper is organized as follow. An overview  of Apriori Algorithm is discussed in section 2. In section 3 we present the proposed Multiple Value Itemset algorithm with example.  Section 4 focuses on algorithm performance test, section 5 will discuss the conclusion and future work.



II. RELATED WORKS The improvement of the Apriori normally based on the  structure and computational speed on the binary space.

Though wide variety of approaches in performing frequent mining have been proposed but none of these solutions have clear bound in performing multiple attribute value of the items within the non-binary space. Multiple attribute value brings the information in form of a set of attributes in row as a transaction. Each attribute carries multiple sets of values.

For example, Item A contains the value of x1 and y1 and item B contains the value of x2 and y2. Computational of frequent items in binary space cannot differentiate that value x1 and x2 because it only considers the presence or absence of the data and does not distinguish the data values.

There are three approaches in searching for the frequent  item in frequent mining specifically for the Association Rule Mining (ARM). Traversing iteratively into a dataset phase by phase is the one of the approach. While extraction of maximal frequent pattern is the techniques applied in H- Mine. The third approach through the close algorithm. The classification on the approach was reported by [7] and enhanced by [9, 10] by classifying improvement algorithm for the frequent mining into sampling-based, counting and hashing-based.

Numerous improvement of Apriori algorithm tend to  decrease itemset candidate by comparing the infrequent subset before scanning  a database[13], a filtering method that ignores the transaction records that is not appropriate for the generation of frequent items [14]. (Optimizing  the candidate generation )[15] while RAAT [16] reduces redundant pruning by decreasing  candidate?s 2-itemset and adopts tagging counting. Another improvement is to maximize the length of frequent itemsets. [17, 18] LFIMiner contains conditional pattern base pruning (CPP) and Frequent Item Pruning (FIP) and dynamic ordering to find all the maximum length. References [19] uses parallel implementation to reduce the drawback in Apriori using tree structure. Greedy approach proposed by [16] tries to reduce the memory load in concurrent processing of Apriori.

(Improving the Apriori based on the value to consider within the items in the dataset). Generally the Apriori algorithms are computed based on binary representation. The above improvement of Apriori algorithms are based on binary representation.

Though a wide variety of approaches in performing  Apriori for frequent mining has been proposed, none of these solutions has clearly explained the performed multiple attribute value of the items. Hence for example, an item 1 consists of multiple value such as [1,2,3] and item 2 has value of [1,2,3].  For each transaction the values of item 1 and item 2 can be more than one occurrence.  The proposed algorithm finds the frequent item based on multiple attribute  value of real data repository and generates more reliable association rules. Generally Apriori principles concept is based on the observation that should the item is frequent, and then there is a possibility that the subsequent would also be frequent. If the item is not frequent then the superset could not be frequent also. The algorithm is based on the above principle and it iterates over two phases, the phase of candidate generation and the phase of verification, at each level. Since Apriori is a level-wise algorithm, it generates frequent itemsets one level at-a-time, from itemsets of level- 1 to the longest frequent itemsets. At each level, new candidate itemsets is constructed using frequent itemsets obtained in the previous level. At each level, the transaction database is scanned once to determine the actual support count of every candidate. Example, If X is not frequent, and then XY cannot be frequent. Based on this principle, the pruning step will eliminate the infrequent item based on the threshold value (known as minimum support) normally predetermined by the user. Generating the possible combination (length of the item known as k-length) of items (itemsets) is done after pruning. The process is repeated until no more candidate itemsets can be generated.

The main idea of this study is to introduce ARM based on  Apriori in the non-binary search space. The frequent item is calculated based on the value of the item/attribute. This is different from common apriori as the calculations are based on the frequency within the transaction by considering all the possible value within the item. Our proposed algorithm attempts to overcome the limitation of representation in data where in ARM the data is commonly represented in the market basket or transaction for which it is in binary space form.



III. THE PROPOSED METHOD  A. The basic concept Mining frequent itemsets using MAV presents the  interesting challenges over the traditional binary search space.  In this approach, we consider the items as ?attributes?.

Each attribute may contain more than one attribute values.

Each transaction reflects the data point in the dataset or set of transactions.

Let D be a set of transaction {T1,  T2,  T3,  T4,  T5, ???..  Tn,}. Assume the attribute is an item. Let P = set of items {P1,  P2,  P3,  P4,  P5,  ???..  Pi,} denotes as items/attribute. For each Pi there exist multiple values, P = {Pi1,  Pi2,  Pi3,  Pi4,  Pi5,  ???..  Pij,}.  Let Ti  has pnj items, so we can write a transaction as ti = {Pij,  Pij,  Pij,  Pij,  Pij, ???..  Pijn,}. Following the above reason an indication to calculate frequency for each attribute value can be defined as below where MAV is Multiple Attribute Value:    ? Pij = Pkj where i ? k , ? MAVij =               (1)         The tables below illustrate the multiple attribute value (MAV) function.  Table I presents the sample of possible value for each attribute P and T as the data point in the transaction. The MAV first scans the dataset and calculate the frequency of each attributes value separately based on formula stated above (1). The setting of minimum support 20% for pruning functions are used to illustrate the elimination of candidate generations. The frequent value for each attribute is stored in the projected table as depicted in Table II. To visualize the improvement, we consider the dataset as single dimension database and not in transaction format.

TABLE I: Non-binary dataset (TransTab)  Tn P1 P2 P3 P4 P5 t1 b a b b d t2 a b b d d t3 b a b d d t4 a b b b b t5 a a b c d t6 a a a d a t7 a a b a d t8 c b a a b t9 c a b b a t10 b b b a b     TABLE II : The frequent value count T P1 P2 P3 P4 P5 a 5 6 2 3 2 b 3 4 8 3 3 c 2 - - 1 - d - - - 3 5  Based on the user supply minimum support, those transaction < than minimum support (X) will be remove from the transaction dataset. The new dataset will only contain trasaction that meets the definition below.

? Pij where MAVij ? X ELIMINATE ti        (2)  The elimination proses as illustrated in table III. Database is visited again for the tagging as in table III. The new transaction table as in table IV, where NewTransTabij ? TransTabij is produced after the tagging is completed.

TABLE III: Tagging for elimination  Tn P1 P2 P3 P4 P5 MAV t1 b a b b d t2 a b b d d t3 b a b d d t4 a b b b b t5 a a b c? d ? t6 a a a? d a? ? t7 a a b a d t8 c? b a? a b ? t9 c? a b b a? ?  t10 b b b a b    TABLE: IV: NewTransTab Tn P1 P2 P3 P4 P5 t1 b a b b d t2 a b b d d t3 b a b d d t4 a b b b b t5 a a b a d t10 b b b a b   The second step in generic Candidate Generation process  in Apriori is to produce candidate.  NewTransTable as the projection database will be visited to generate the candidate for generating more k-length.  Ppj will then, multiple by itself to generate the candidate and pruning based on user supply minimum support. The redundant candidate generated will be sorted before pruning to eliminate the possibilities of redundant attribute values. The process will continue until no more candidates can be generated.

B. The Algorithm The main idea of MAV is to find the frequent itemset from dataset that are not in binary space form and in the transaction (market basket) format. The MAV is designed to find frequent itemset based on attribute value. Fig.1.

formalizes the proposed MAV. It has two main phases.

Phase 1 consists of the mining for frequent items in each attribute value from the database using the multiple attribute value function. Phase 2 involves the computation of the possible k-length itemset by sorting and eliminating steps before pruning takes place.

The facts that many fast frequent patterns of mining are  available [18] and [7] and computation complexity have been taken into consideration. We take into account that our algorithm computational cost will be (number of P * V) * K- length) where P is attribute and V is attribute value.

Figure 1.  Multiple Attribute Value Function  Based on figure 1 for the MAV algorithm, line 3 and 4 are the important component in the algorithm to generate frequent item and also candidate generation. Line 6 to 10 is the computation generating the frequent item. The sub module is interpreting each attribute value  Pi for each attribute P and for each Pi will have value such as {P1,P2?Pi} and each Pi having different value such as {Pia, Pib, .. Pij}. Each of P will be accumulated and will be devided with numbers of P as in line 8. Line 8-10 the algorithm will compare against the user supply minimum support for elimination proses. Candidate generation start at line 11 to 21. At this stage the sub modul involved combination and also elimination process. The process involve combination of group of frequent item Lk-1 with Lk-1(Pij value), as example for L2 =  {{1,2}, {1,3}, {2,3}, {2,5}, {3,4}, {3,5}}, the candidate result as C3 = {{1,2,3}, {2,3,5}, {3,4,5}}.

During elimination process if (i-1) is a subset to NewC to c will not be generated  when it not part of Lk-1. The newC will be sort and elimate for redundancy encounted from TempCandidate.

The proposed algoritm was motivated based on the above working example as in table I to IV.



IV. RESULT AND DISCUSSSION The algorithm developed based on algorithm obtain from [20] using Dynamic Itemset Count (DIC)[6] . The performance of MAV is measured based on number of frequent items (FI) and the maximum length of frequent items (max-length) generated. The experiment is tested upon 17 benchmark datasets obtained from UCI Machine Learning Repository that are the Iris Plant (IRP) Zoo (ZOO), Australian Credit  (ACC), glass (GLS), Coil2000 (COL),  Breast Cancer (BCE), Lymphography (LYM), Cleveland (CLV), Heart Disease (HDE), Echoli (ECO). Those benchmarks dataset were obtain from[21]. Added dataset focus on medical dataset are Hayes-Roth (HYR), The Monk?s Problem (MNK), Balance Scale Weight & Distance (BSWD), Hepatitis (HPT), Horse Colic (HORSE), Dermatology (DMT) and Contraceptive Prevalance(CONTRA. Series of experiment are carried out using the same dataset with minimum support ranging from 20%, 40%, 60%, 80% and 100%. Setting in parameter is based on Quick Parameter Setting concept [22-27] . The main objective of various parameter applied to project the relation between numbers of frequent item and k-length generated based on number of item in dataset.

The MAV algorithm is run on the WindowsXP operating system, CPU with Intel(R) 1.83GHz, memory with 1GB using JAVA. While the APRIORI (AP) and FILTER APRIORI (FP) is the algorithms within the WEKA for association task mining.

In the series of the experiment, we perform frequent  mining to determine k-length and number of frequent item generated by the algorithm. Table V visualizes the performance of both algorithms in generating the k-length of the itemset and also frequent item generated.

Based on our experiment, MAV is unable to generate any frequent items and also k-length on min_supp 100% and it true to all dataset. Our derived conclusion is that due to the complexity of the attribute value, will not allowed the generation of frequent item at 100% support. The equivalent result was obtained from WEKA (Filter Apriori and Classic Apriori).

During our experiment, we recorded in dataset ZOO,  LYM, HDE and HPT our MAV was not recorded due to our computation > 30 minute. Our MAV results as in table V was recorded starting at 40% min_supp for the above dataset.

Based on Table V, the comparison between number of  item and numbers of frequent item generated with numbers of max_length produced. Table V projected dataset CONTRA, BSWD and ECO having large numbers of item but only generated lesser item compared to CLV. We projected our result as on graph in figure 2 below. We derived to the conclusion that large database or having larger number of transaction will not promise to generate more frequent item. The generation of frequent item was reliant on the structure of dataset depending on attribute within the attribute.

IR P  Z O  O  A C  C  G L  S  C O  L  B C  E  L Y  M  C L  V  H D  E  E C  O  H Y  R  M O  N K  B S  W D  H P  T  H O  R S E  D M  T  C O  N T  R A  IT E  M N  U M  B E  R S  DATASET  Numbers of Records vs Numbers of frequent item  TOTAL RECORDS  FREQUENT ITEMS   Figure 2.  Comparison between numbers of record and numbers of item  generated  TABLE V: Experimental Result  DATASET DATA FREQUENT ITEM  K- LENGTH  IRP 58 57 5 ZOO 101 514* 6* ACC 328 136 5 GLS 214 159 4 COL 160 51 5 BCE 483 224 6 LYM 148 417* 7* CLV 302 765 6 HDE 161 218* 5* ECO 336 91 5 HYR 102 20 2  MONK 432 35 2 BSWD 625 18 1  HPT 155 775* 7* HORSE 300 260 5  DMT 366 464 7 CONTRA 1272 192 5  *BASED ON MIN_SUP 40%   Based on figure 3, we analyze at the min_supp 20%,  we identified dataset GLS, COL, BCE, CLV, HORSE, DMT and CONTRA our proposed algorithm able to produce more frequent item compare to AP and FP. While for dataset ECO, HYR and MONK we manage to produce equal result.

Our MAV are unable to produce more frequent item on other 7 dataset including ZOO, LYM, HDE and HPT. Overall performance MAV able to produce more frequent item at 20% minimum support compared to comparison algorithm.

Our achievement in generating frequent item is at 40% minimum support as in figure 4.

20% 40% 60% 80% 100%  it em  co un  t  Minimum support  Frequent item generated  More  equal  less   Figure 3.  Comparison between MAV, AP and FP in generating the  frequent item         20% 40% 60% 80% 100%  IT EM  S  Frequent Item generated  MAV  APRIORI  FILTER_AP    Figure 4.  Frequent Item generated  Based on figure 3 and 4, we can conclude based on overall performance of MAV compared to AP and FP our achievement is recorded at 40% minimum support and equivalent results on 20%, 60%, 80% and 100%.

20% 40% 60% 80% 100%  K -l  e n  g th  Minimum Support  K-Length generated  More frequent Item  Equal frequent Item  Less frequent item   Figure 5.  Comparison between MAV, AP and FP in generating the  maximum k_length                   20% 40% 60% 80% 100%  K- LE  N G  TH  K-LENGTH generated  MAV  APRIORI  FILTER_AP   Figure 6.  Comparison between MAV, AP and FP in generating the  maximum k_length  The other explanation that could be derived from our experiment is that MAV algorithm able to produce an equal length with comparison algorithm for all 17 dataset. Our performance are consistently derived the comparative result with AP and FP except on minimum support 20%. This can be explain on dataset ZOO,LYM,HDE and HPT we are not generating any values due to our computation time > 30 minute as in figure 5 and 6.



V. CONCLUSION Frequent itemset mining is one of the most important  areas of data mining specifically in association. Existing implementation of Apriori based algorithms focus on binary dataset. In this paper, we revise [19] implementation for non- binary dataset. We consider the item values in generating the frequency based on attribute. Each attribute in the dataset may consist of multiple values. The computation is based on Apriori concept.  We also introduce the multiple attribute value function in generating frequent items.  Our experiments show that the improved Apriori offers a significant improvement in terms of identifying frequent items for more meaningful result over existing solutions.

Generally MAV algorithm for the frequent mining process is able to stand a line with comparison algorithm at certain minimum support such as at 80% with 0.04 t-tests.

While in producing maximum k-length we are able to get 0.02 and 0.05 significance changes. Table below show the t- test results.

Table VI. Statistical Test  algorithm Result Statistic  Min_supp Mean T 2-t  Item 40% .35714 1.794 .096  80% .57143 2.280 .055 ?  K-Length 40% .50000 2.463 .029 ?  80% .57143 2.280 .040 ?    We will further enhance the algorithm to reduce the complexity cost as we believe our computation is higher since we evaluate each value independently. We will view the optimization perspective in the algorithm for improving the complexity costs.

