Cognitive Modeling of Symbolic-like Relationships  with the Adaptive Neural Network Associator (ANNA)

Abstract-In their influential articles [1]-[2] (Science, 283, 1999), Marcus, Vijayan, Bandi Rao and Vishton (pp. 77-80) and Pinker (pp. 40-41) argue that a prominent model of associative learning, the simple recurrent network, SRN [3], would fail to simulate rule-learning by seven-month-old infants.

Furthermore, the authors argue for the consideration of rule- based, symbolic explanations. Subsequently, several authors proposed variations of the simple recurrent network that were better able to model the infant data, but Marcus argued that these models would themselves implement (hidden) rule- mechanisms. Moreover, he was able to show in a further experimental test that one of these models predicted exactly the opposite of what was found in an infant learning experiment.

My paper proposes ANNA, a new recurrent neural network architecture that is fully based on associative mechanisms, i.e.

ANNA does not implement rules. ANNA succeeds to simulate the infant results. This includes Marcus? recent experimental tests. I will therefore argue that the results of Marcus dd not necessarily prove that infants make use of rules (though they might apply rules). Moreover, ANNA shows that rule/symbolic- like relationships can at least sometimes arise out of associations.

1. INTRODUCTION  It has often been asked whether the assumption of rule- based, symbolic mechanisms is necessary to explain the formation of abstract representations in human learning. It has also been asked whether associative learning processes, a type of learning processes that is not based on symbolic rules, are able to explain the formation of abstract representations alone. In associative leaming models, connections between units representing items, concepts, words, etc. are formed through simple associations.

Connection strengths (=weights) between units are changed based on a general learning rule as well as repeated exposure to the material that the model is supposed to learn. This general learning rule is often an error correcting algorithm such as the delta rule. In Psychology, the delta rule has become very popular because it was able to simulate many  I  Wolfson College and Experimental Psychology Barton Road Cambridge CB3 9BB, UK rs272liijcam.ac.uk  0-7803-7898-9/03/$17.00 02003 IEEE 2146  behavioral phenomena. Associative learning models thus have a tradition in psychology and are analogous to many connectionist modelslneural networks, because the same learning principles apply to many neural networks.

Psychologists therefore often consider neural networks as examples of associative learning. The simple recurrent network, also known as Elman-network [3], has become a particularly prominent model in psychology and turned out to be one of the most cited papers in Cognitive Science, Psychology and Psycholinguistics, [4], p. 25. This model was able to simulate many human learning processes, which my colleagues and myself were also able to confirm, e.g. [5]-[8].

The question arises whether associative learning models such as the SRN or other neural networks would be fully able to explain human learning behavior and other phenomena in Cognitive Science. This point of view is considered a possibility by many authors, e.g. [9]-[IZ], in particular when taking a developmental perspective, i.e. when studying infant learning behavior. In this case it was often suggested that infant learning may be guided by statistical processes alone, which can be emulated with associative models/neural networks.

Steven Pinker, Gary Marcus and his colleagues do not agree with the notion that infant learning can be explained by associative principles alone [I]-[2], [4]. Although none of them denies the existence of associative mechanisms in human learning, they argue that rule-based processes are also required in order to explain particular experimental data on infant learning. An overview of their experimental task and their results will be provided later when I attempt to model them with ANNA. Steven Pinker [2] further argues more generally ahout the human mind and brain, e.g. that  ?the theory of symbol processing seems better suited to explaining the brain?s ability to handle complex ideas and the aspects of language that communicate them,? p. 41.

Subsequently, many scientists have questioned whether the infant data provide enough evidence in favor of assuming rule-based, symbolic processes, e.g. [13]-[16]. After  http://rs272liijcam.ac.uk   summarizing the results of Marcus and his colleagues [I], I will give an overview of the skeptical arguments provided by these other authors [13]-[16]. Subsequently, 1. will refer to a novel experiment by Marcus [4] that was able to defend his position. He showed that another particular model lacking algebraic-like rules could not explain his data. Following this, I will provide a detailed overview of my novel model, which 1 named ANNA. 1 will show that this associative model, which does not incorporate any algebraic-type rule, is able to simulate all the results considered here.

11. SUMMARY OF THE EXPERIMENTS BY MARCUS, VIJAYAN, BANDI RAO AND VISHTON (1999)  These experiments have shown that 7-month-old infants show longer attention to sequences of phonemes with unfamiliar structure than to sequences of phonemes with familiar structure. The authors suggest that these infants were able to learn sequences of phonemes at an early stage in development, because they were bored by sequences that had the same grammar as the sequences they had been trained on, and attended longer to sequences with a novel grammar.

Infants were assigned to two conditions, which either consisted of an ABA or an ABB condition. In both conditions, they received a two minutes long speech sample containing repetitions of three phoneme sequences that followed one of the grammars. In the ABA grammar, these were 16 different sequences of phonemes such as ?li ti li?, ?ni gi ni? etc. In the ABB grammar, these were 16 different sequences of phonemes such as ?li ti ti?, ?ni gi gi?. After the training phase was over, infants were given a test phase, where they received 12 novel sequences of phonemes that they had not experienced during the training phase, e.g. ?WO fe WO? or ?we fe fe?. One half of the test trials corresponded to the grammar the infants had earlier been familiarized with, whilst the other half of the test trials were constructed with the other grammar, i.e. the one that the infants had not been trained on. The authors found that infants attended significantly longer to the sequences of phonemes they had not been trained on. This suggests that they have recognized the novelty of the grammar in the test phase and that they have habituated to the grammar from the training phase. The authors interpret their findings in the following way: They argue that infants have leamed an algebraic rule of the grammar rather than having relied on the statistical regularities of the training set. They carried out additional experiments to strengthen their assumption that this was indeed the case, i.e. that infants have relied on an abstract rule rather than on leaming sequences of particular phonetic features and rather than on habituating to a specific single property that makes these grammars different [l]. In both additional experiments they argued against these possibilities and in favor of the abstract rule mechanism. They further argue that a model like the simple recurrent network (SRN)  would not be able to simulate. the infant results. This is because the SRN in its classical form [3] cannot transfer to novel sequences of phonemes. The SRN can leam the sequences of phonemes it has been trained on, but when novel phonemes are applied, it cannot transfer the underlying rule of the trained sequences to the novel sequences. Because the SRN, which is based on associative learning mechanism and not on algebraic rules, as well as basic statistical systems based on transitional probabilities cannot simulate the infant results, the authors argue ?that [7-month-old] infants can represent, extract, and generalize abstract algebraic rules? [I], p. 77. With algebraic rules, they mean algebra-type rules that represent relations between variables (the variables would be placeholders for the phonemes in their experiment).

Subsequently, this interpretation was challenged by a number of authors who either proposed modifications of the SRN or altemative models that were successful in simulating the infant data. A brief summary of these papers and the replies by Marcus will be provided now, and more detailed accounts can be found elsewhere [13]-[19].

111. ALTERNATIVE INTERPRETATIONS  The interpretation of Marcus and colleagues was criticized by Altmann and Dienes [13], because even before Marcus and his colleagues published their paper [I], Altmann, Dienes and Gao had successfully designed a neural network that elaborated on an SRN [17]. This model subsequently turned out to be successful in terms of simulating the infant data. It makes use of batch weight update and weight freezing mechanisms of a core set of weights, as well as mapping mechanisms between the trained set of stimuli and the novel set of stimuli. This way, the model is required to learn the mapping to and from the encoding formed by the SRN. As a response to these comments, Marcus acknowledged that be had misunderstood their model [18]. He was able to defend his interpretation, however, by arguing that this elaboration of the SRN was a purpose-built model incorporating an extemal mechanism to tell the model what to do (e.g. when to freeze particular weights). These mechanisms may go well beyond associative learning. Even more striking is the following argument [4], pp. 64-65. Marcus trained their model on the following problem: An artificial phoneme sequence of the type ABA was supposed to transfer to a sequence with the same ABA grammar, but different phonemes. In the test phase, the model was also tested on sequences where the phonemes were reversed, e.g. if WO fe WO occurred in training, it was now tested on the grammatical fe WO fe and the ungrammatical fe WO WO. The model performed better on the ungrammatical phoneme sequences than on the grammatical ones. Marcus and his colleague Bandi Rao also tested infants on this task and found that children showed exactly the opposite result. This may suggest that infants learn to represent the underlying rule. In     contrast, the model had performed better on the ungrammatical sequences. Marcus argues that the reason for this is that the third phoneme in the ungrammatical sequences had been the same as the third phoneme in the grammatical ones during the training phase, e.g. WO fe WO vs. f e  WO WO (whilst fe WO fe has no exact overlap with WO fe WO apart from the abstract rule).

Seidenberg and Elman [I41 also challenged the interpretation that Marcus? results would prove the learning of abstract algebraic rules by 7-month-old infants. They successfully modified the SRN so that it was able to simulate the infant data. As a response to their contribution, Marcus [19] argued that Seidenherg and Elman had extended the SRN by adding an external teacher, which would do nothing else than implement an algebraic rule into their model. After looking at Seidenberg and Elman?s model in detail, I was not able to find any evidence for Marcus? arguments, as no rule of the type Marcus? argues was actually built into the model.

Nevertheless, I would argue that their model may go beyond the principles of pure associative learning. This is because they added a pre-training phase to their network, and the model was given the chance to learn about similarity and dissimilarity of stimuli. The weights from the pre-training phase were frozen and these weights later contributed to the performance of the model (i.e. during the test phase).

According to Marcus [4] p. 66 and [19], the implementation of rules also holds true for the SRN variation designed by Negishi [IS], who made use of statistical algorithms to capture the continuous vowel structure of the phonemes from the infant experiment. Based on the information given in the journal Science, I cannot assess whether Negishi?s modification of the SRN is based on rule-hased principles or not. The reason for this is because the implementation of his model is not publicly available in the link provided by Science Magazine (not even with an online license to Science). Another neural network that successfilly simulated the infant data was provided by Shultz [16]. He proposed cascade-correlation neural networks that add hidden units as training progresses. These types of networks also apply curvature and slope information from the error surface when making weight changes. In addition, Shultz applied analog encoding of the stimuli, e.g. all the phonemes that could represent the As in ABA and ABB were coded by unique numbers, e.g. the phoneme ga was coded as number 1, the phoneme li as number 3, the phoneme ni as number 5 and the phoneme fa as number 7. Similarly, the phonemes that could represent the Bs in ABA and ABB were also coded by unique numbers, e.g. ti was coded as number 2, na was coded as number 4, gi was coded as number 6 and la was coded as number 8. The novel phonemes in the test phase are interpolated between these numbers, e.g. the phoneme WO would for instance receive an average value of 2.5. Albeit capable in terms of simulating the infant data, Marcus [4] criticizes that ?crucial to the success of the model is the coding scheme? and ?Shultz uses each node as a variable that  represents a particular position in the sequence? p. 65, i.e.

Marcus comes to the conclusion that this model and the previously mentioned successful models implement operations over variables, which he argues to be the same as an algebraic rule. After having summarized alternative interpretations, I would like to provide my own model, ANNA, which is clearly distinct from all of the previous approaches as it is only based on what psychologists consider associative processes and as it does not implement operations over pre-specified variables.

IV THE ADAPTIVE NEURAL NETWORK ASSOCIATOR  Like many models in the previous section, my approach was also based on the SRN and associative processes, so I start by considering theoretical issues ahout associative learning first.

ANNA uses the same learning algorithm (backpropagation) and the same coding typically chosen in an SRN, which is considered an associative model by Steven Pinker [2], ?Marcus has also demonstrated that a kind of associative network frequently toutet as a ruleless model of language learning, J. Elman?s Simple Recurrent Network, does not discriminate the patterns in the way these infants do? p. 41.

Apart from these basic associative mechanisms, ANNA uses other associative mechanisms where activation is spread to those units where error values become minimal (these are no pre-specified units or variables, and this process could apply to any units in the network. This process will he clarified later). Error computation is arguably one of the most important mechanisms in modern associative learning theories and has for instance been proposed in the Rescorla- Wagner Learning Rule, e.g. [ZO]. This goes beyond classical theories of associationism, which are summarized by Steven Pinker [2],  ?In this passage from his 1748 Enquiry Concerning Human Undersfanding, David Hume summarizes the theory of associationism. The mind connects things that are experienced together or look alike, and generalizes to new objects according to their resemblance to known ones. Replace Hume?s ?ideas? or ?sensible qualities? with ?stimuli? and ?responses,? and you get the behaviorism of Ivan Pavlov, John Watson, and B.F. Skinner. Replace the ideas with ?neurons? and the associations with ?connections,? and you get the neural network models of D.O. Hebb and the school of cognitive science called connectionism? p. 40.

This statement about associationism does not account for the variety of associative learning theories that exist nowadays. Moreover, it does not refer to the fact that many modern associative learning theories compute errors, e.g. the Rescorla-Wagner Leaming Rule, the McLaren, Kaye and Mackintosh Theory, the Adaptively Parametrised Error     Correcting System (APECS) etc. ANNA stands in line with error computing approaches and is therefore not the same as the classical 'school of associationism from the eighteenth century.

I will now start by summarizing ANNA'S basic mechanisms in a simplified form. ANNA makes use of the fact that the 'basic SRN [3] can learn the statistical regularities of the phoneme sequences. Therefore, ANNA has an SRN sub-component. As in the SRN, both trained and novel phonemes are coded as vectors. In ANNA, there are connections from the vectors representing the trained items to the vectors representing the novel items. Here is one example: Take the sequence li ti li, where li is coded ( I ,  0, 0, 0) and ti is coded (0, 1, 0, 0). An SRN would easily be able to learn that li predicts ti which in turn predicts 1;. Thus, the predicted activity of the second step in the sequence would be similar to (0. I ,  0.9,O. 1 , O .  1) and the prediction of the third step would be something like (0.9, 0.1, 0.1, 0.1). One would then present the novel inputs WO (0, 0, 1,O) andfe (0, 0, 0, 1) to ANNA and let them establish connections between the vector components in the trained phonemes and the vector components in the novel phonemes where the delta (= error) values would become minimal. The first phoneme li can only be associated with the phoneme WO because both of them occur at the first position in the sequence. In this case, the delta value between vector component 1 in li ( I ,  0, 0, 0) and vector component 3 in WO (0, 0, 1, 0) would be minimal and these components could therefore be associated (m vs. 1 - 0=1 for the rest of the comparisons). Similarly, vector components 2 , 3  and 4 in li and vector components 1 ,2  and 4 in WO would be equated as well because of their minimal delta values (0-O=O; multiple equations do not make a difference because these components all have minimal activities). The same associations can be formed between vectors ti (0, I ,  0, 0) andfe (0, 0, 0, l), that both occur at the second step of the sequence. Given that li ti predicts li as the third phoneme in the sequence and because there is an association between li and WO and between ti and fe. the sequence WO fe could predict a WO as its third element, because it would produce the corresponding output for the final phoneme of the trained sequence li ti li at its respective position in the novel 'sequence WO f e  W O .  Instead of outputting something like li: (0.9, 0.1, 0.1, O.l), the novel sequence could output a WO: (0.1, 0.1, 0.9, 0.1) due to the previous associations for which the delta values have become minimal and due to the spreading activation from vector component 1 to vector component 3.

The same is possible if one reverses the training sequences in testing, e.g. if training on wofe WO and testing onfe wofe vs. f e  W O  WO. In this case, vector component 1 could stand for WO (0.9, 0.1, 0.1, 0.1) and vector component 2 for fi (0.1, 0.9,0.1,0.1).

Such an operation could still he considered associative, as it is not an operation over pre-specified variables, hut rather a general error computation and a spread of activation to  wherever the error values become minimal. The example chosen here was to enable better understanding. However, the critical reader may recall that there was not only one trained phoneme sequence and one novel phoneme sequence in the Marcus et al. study. Rather, there were 16 phoneme sequences in the training set, and there were 12 novel sequences (6 grammatical ones and 6 ungrammatical ones).

However, the principle remains the same: If one out of the 16 phoneme sequences from the training set (you can pick one at random) is associated with a novel sequence and activation is spread to wherever the error value becomes minimal, the result will be the same. To explain in detail how this process is implemented into ANNA, it might he helpful to have a look at the basic architecture of ANNA (Figure 1).

Output vector Output vector components to components to predict next predict next phoneme of phoneme of  sequences) sequences  t I weights 2  weights I  Context units Phonemes at time t- I Phonemes  t  Fig. I :  T h e  adaptivc neural nchvork associafor (ANNA)  As mentioned before, ANNA has an SRN sub-component.

These are the input units (trained phonemes), the context units, the hidden units and the left side of the output units, i.e. the output vector components of the trained sequences.

The SRN learns sequential information by predicting the next phoneme in the sequence. The SRN has an extra layer of context units and copy-back connections from the hidden units lo the context units. As a consequence, the context units store exact copies of the previous hidden units' activities.

The context units provide the SRN with a temporal memory.

Apart from the copy-hack connections, all other connections in the SRN are adjustable. This is done with the Backpropagation learning algorithm [21]-[24]. The extra- components of ANNA can be found on both the input and output level. The curved lines underneath the input level indicate where ANNA establishes connections between the vector components in the trained phonemes and the vector components in the novel phonemes. As mentioned before, this happens where the delta (= error) values become minimal. The curved lines above the output level are exactly identical to the curved lines underneath the input level. In order to make this figure more comprehensible, I have drawn 2 types of curved lines: The one underneath the input level and the one above the output level. The curved lines above the output level spread activation from the vector components of the trained phonemes to the vector components of the novel phonemes for which the delta (= error) values have become minimal at the input level, i.e. if the vector representing A ( I ,  0, 0, 0) was associated with C (0, 0, 1,O) at the input level, a prediction of output A through vector component 1, e.g. (0.88, 0.1, 0.12, 0.1 1) will spread to component 3, e.g. (0.12, 0.1, 0.88, 0.1 I)  at the output level.

In turn, the activity of component 3 will spread to component 1 at the output level. Instead of exchanging the activities via spreading activation, ANNA could also perform an average computation for the not-activated units and just spread activation in one direction from component 1 to component 3. The result would be the same: The remaining units would stay inactive. The following simulation study will show that ANNA can deal with what is considered an algebraic rule by Marcus and his colleagues. As can be inferred from ANNA?s architecture, the learning takes place in the SRN and the spreading activation and error computation between trained and novel items takes place in the testing session. Therefore, the parameters of the SRN are only of interest for the learning session. The model was trained for 10,000 training trials, i.e. it received 10,000 presentations of either the ABA grammar or the ABB grammar. The learning rate was 0.1, there was no momentum term and the number of hidden units was IO.  In order to measure the consistency of the simulations, each of them was repeated 25 times (after implementing them with different random seeds) and it was assessed how many times and to what extent ANNA would transfer to novel items with the same grammar as the one that was experienced during the training phase. All in all, there were 50 simulation studies (25 times training ANNA on ABA and testing transfer to novel items with grammar ABA and 25 times training ANNA on ABB and testing transfer to novel items with grammar ABB). The most widely accessible form of Backpropagation [24] was chosen and based on past considerations, i.e. [I71 and [24], bias values for each hidden and output unit were changed in a similar way as the weights.

Target values were 0.9 and 0.1 instead of 1 and 0. A justification for these target values can also be found in [24].

Iv SIMULATION RESULTS  In each of the simulations, ANNA perfectly predicted the correct transfer to novel sequences in both the ABA grammar and the ABB grammar. As a consequence, there were 25 successful transfers and 0 unsuccessful transfers in the ABA condition and 25 successful transfers and 0 unsuccesshl transfers in the ABB condition. When performing statistical chi? tests, this yields highly significant results: chi?(df=l)=25, p<.OOI and chiz(df=1)=23.04, p<.OOI when applying the Yates correction for continuity due to the fact that there is only one degree of freedom (some statisticians suggest to apply the Yaks correction for continuity, because otherwise they fear the chi? test might lead to progressive decisions. After the highly significant finding and after carrying out the Yates correction for continuity, this scenario can safely be excluded).

After having seen that ANNA transfers in each of the simulation studies, it will be of interest to what extent ANNA shows transfer. This was very much dependent on the SRN, because the SRN was the network where the learning of trained items took place. The interesting finding was that the SRN perfectly learned to match the target values, i.e. with vector component values of 0.9 if the target value was 0.9 and with vector component values of 0.1 if the target was 0. I .  This happened for each of the output units in each of the 25 simulations on the ABA grammar, and it also happened for each of the output units in each of the 25 simulations on the AB9 grammar. Because there is no decay term after ANNA performs the error computation between trained and novel items (at least in ANNA?s first version which is presented here), ANNA shows perfect and symbolic-like transfer to the novel items, i.e. the vector components in the novel sequences also have activities of 0.9 and 0.1. Since ANNA does this perfect transfer for each of the simulations, the averages at the respective locations are also 0.9 and 0.1 and their standard errors are 0, not just if li f i  [i maps on W O fe WO,  but also if W O  fe WO is reversed to fe WO fe. Such a performance is typically only known for symbolic, rule-based models. An overview of these results is given in Table I .

TABLE 1 RESULTS ON ANNA?S TRANSFER TO NOVEL PHONEMES WITH THE SAME  UNDERLYlNFGR*MMARASTHETRAMED PHONEMES  If one aims for worse performance in order to simulate human data, one could simply include a decay term into ANNA that decreases activation when it is spread between vector components. One might criticize that ANNA cannot     transfer after the first step in the sequence, because if a novel item WO is presented for the first time, ANNA cannot predict the novel itemfe. Both phonemes have never been presented to the model before, so there cannot be any learning at the first step. But the same would happen in humans. Humans who hear a novel phoneme for the first time cannot predict the next novel phoneme if the grammar is based on three phonemes (as in ABA and ABB). Where it gets interesting is when wofe have been presented and ANNA has to predict what is to follow next. Like the infants, ANNA can then perform in a quasi-symbolic way to predict WO (completing wofe WO) in the ABA condition andfe (completing wofefe) in the ABB condition.

Iv CONCLUSIONS  ANNA shows that the results by Marcus and his colleagues can in principle be solved by a neural network that is purely based on error computation. Therefore, I would argue that Marcus? results do not prove that infants apply algebraic rules. Marcus may in turn argue that my model implicitly makes use of rules. However, ANNA only performs rulelsymbolic-like, and the rule is discovered through general error computation rather than operations over placeholders or pre-specified variableslvector components. General error computation is considered associative (and very popular in psychology for more than 30 years, e.g. [ZO]). In the light of ANNA?S results, I think i t  is premature lo argue that symbol processing is generally better suited, as done in [2] p. 41. I do not deny the existence of rule-based processes in humans, but 1 argue that the present results on phoneme learning cannot prove them in infants. This is because it is possible to construct models like ANNA that are not based on algebraic rules or operations over variables, but nevertheless succeed to simulate the data. In adults, on the other hand, there might be more evidence for rule-based processes that associative models cannot account for [8]. These rule-based processes seem to co-exist along with associative processes, as various results have revealed [ 5 ] ,  [8], [25],  [26] .  At least my arguments for a hybrid rule-baseUassociative system to explain human learning stands in line with Marcus? and Pinker?s general assumption that both rules and associations are necessary.

REF ER EN c E s  [ I ]  G.F. Marcus, S. Vijayan, S. Bandi Rao, P.M. Vishton, ?Rule Learning by Scven-Month-Old Infana,?Scienee, vol. 283,77-80, 1999.

[2] S. Pinker, ?Out of thc Minds of Babes,? Science, vol. 283,40-41, 1999.

[3] J.L. Elman, ?Finding stmcturc in timc,? Cognirive Science, vol. 14, pp.

179-211, 1990.

[4] G.F. Marcus, The Algebraic Mind lnlcgrofing ,Counnecrionism ond Cogmirive Science, Cambridge, MA: MIT-Prcss, 2001  [5 ]  R. Spicgcl and I.P.L. McLaren, ?Rccurrent Neural Networks and Symbol Grounding,? Proceedings of the lnlernalionoi Join1 INNS/IEEE Conference on NeurolNetwonCS, pp. 320-325,2001.

[6]  R. Spiegel, M. Suret, M.E. Le Pelley and I.P.L. McLaren, ?Analping State Dynamics in a Recurrent Neural Network.? Proceedings of [he IEEE World Congress on Compulolionol Intelligence. Conjerence: International Join1 lNNS/IEEE Conference on Neural Networkr, pp. 834-839,2002.

[7] R. Spiegcl, M.E. Le Pelley, M. Suret, I.P.L. M c h e n ,  ?Combining Fvrry Rules and a Neural Network in an Adaptive System,? Proceedings of lhe IEEE World Congrrrs on Compulolionoi lmeiligence. Conference: IEEE Internotional Conference on Furry Syslem, pp. 340-345.2002.

[8] R. Spiegcl, Humon ond Machine Learning of Spolio-Temporal Sequences: An Experimental and Compurarional Invesligolion, PhD Thesis, Univcrsity of Cambridge (UK). 2002.

[9] M.H. Christiansen and N. Chater, ?Toward a Connectionist Model of Recursion in Human Linguistic Performance,? Cognilhe Science, vol. 23, pp. 157-205, 1999.

[IO] J.L. Elman, E.A. Bates, M.H. Johnson, A. Karmiloff-Smith, D. Parisi, K. Plunkett, Relhinking Innolencrs: A Conneclionisl Perspeclive on Dwelopmenl, Cambridge, MA: MIT-Press, 1996.

I l l ]  K. Plunkelt, J.L. Elman, Exercises in Rethinking Innateness. A Handbookfor Connectionlsl Simuialiom, Cambridge, M A  MIT-Press, 1997.

[I21 P. McLcad, K. Plunkett, E.T. Rolls, lnrroduclion IO Conneclionisl Modeiiing ofcognitive Prococesses, Oxford Oxford University Press, 1998.

[I31 G.T.M. Almann and 2. Dienes, ?Rule Lcaming by Seven-Month-Old Infants and Neural Networks.? Science, vol. 284, p. 875, 1999 iih technical comments and online version of Science Magazine] [I41 M. Seidenberg and J.L. Elman. ?DO infants leam grammar with algebra or statistics?? Science, vol. 284, p. 433, 1999 [in leners and online version of Scievcc Magazine and htl~:llcrl.ucsd.cdul-elmanlPaocrsiMVRVsii~.~html] 1151 M. Negishi, ?Do infants leam grammar with algebra or statistics?? Science, vol. 284, p. 433, 1999 [in letters and online version of Science Magazine] [I61 T.R. Shultz, ?Rule learning by Habihlation can be Simulated in Neural Networks,? Proceedings of l e  Twenty-Firs1 Annual Conjerence of !he Cognilive Science Society. pp. 665-670, 1999.

[I71 2. Dienes, G.T. Altmann and S.J. Gao, ?Mapping across Domains Without Feedback A Neural Network Model of Transfer of Implicit Knowledge,? CogniliveScience, vol. 23, pp. 53-82, 1999.

[I81 G.F. Marcus, ?Rulc Lcaming by Sewn-Month-Old Infants and Newal Networks,? Science, vol. 284, p. 875, 1999 [in technical comments and online version of Science Magazine] 1191 G.F. Marcus, ?Do infants leam grammar with algebra or slatislid? Science, vol. 284, p. 433, I999 [in letters and online vcrsion of Science Magazine] 1201 RA.  Rcacorla and A.R. Wagner, ?A theory of Pavlovian conditioning: Variations in thc effectiveness of reinforcement and non-reinforcement,? in Ciarsical condirioning If: Currenl research and theory, A.H. Black and W.F.

Prokasy, E&. Ncw York Appleton-Cenhlry-CraIs, 1972, pp. 64-99.

[21] P. Werbos, Beyond regression: new molsforpredicrion and analysis in behaviorsciences. PhD Thcsis, Harvard University, Cambridge, MA., 1974.

[22] Y. Le Cun, ?Une Proctdure dApprentisrage pour Reseau i Seuil Asymduique,? Cognilivo 8s: A la Fronlicre de I?lnleiiigence Ami$cieile des Sciences de io Conaissonce des Neurosciences, pp. 599-604, 1985.

1231 D.B. Parker, Learning logic, Technical Repan TR-47. Centcr for Computational Research in Economics and Managemcot Science, MIT, Cambridge, MA., 1985.

1241 D.E. Rumelhan, G.E. Hinton and R.J. Williams, ?Learning lntemal Rcpresentatians by Error Propagation,? in Poraliei dimibured proeesring, vol. I., D.E. Rumelhan and J.L. McClelland, E&. Cambridgc, MA; MIT- Press, 1986, pp. 318-362.

I251 R. Spiegel and I.P.L. McLarcn, ? H u m  Scquence Leaming: Can Associations Explain Everything?? Proceedings of the Twenty-Third Annual Conference ofrhe Cognilive Science Society, pp. 976-981,200l 1261 R. Spiegel and I.P.L. McLaren, ?Abstract and associatively-based representations in human sequence learning,? Phil. Trans. Roy. Soc. London.

vol. B, in press, to appear in 29 July 2003 issue.

