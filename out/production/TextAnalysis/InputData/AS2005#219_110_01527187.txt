STBAR: A MORE EFFICIENT ALGORITHM FOR ASSOCIATION RULE  MINING

Abstract: The discovery of association rules is an important  data-mining task for which many algorithms have been proposed. However, the efficiency of these algorithms needs to be improved to handle large datasets. In this paper, we present an algorithm named STBAR for mining association rules, which is improved from TBAR. STBAR employs dynamic pruning, which can effectively cut down the infrequent concatenations. Experiments with UCI dataset show that STBAR is more efficient in speed about 10%~30% than TBAR, which outperforms Apriori, a famous and widely used algorithm.

Keywords: data mining; association rule; dynamic pruning  1. Introduction  Since R.Agrawal [1] put forward Apriori algorithm, Association rule mining becomes one of the most popular research in data mining, and explorers in this field improve it from various aspects in order to enhance its efficiency, reduce the time cost in scanning database.

Let T be a dataset, which is represented as follows: T={t1,t2,?, tn },where ti is a record (1?i?n). Meanwhile, let I be the set of data items, I={i1, i2, ?, in} , where ij (1? j?n) is a data item of T. Each ti is a subset of I and every record has its own identifier. And suppose X,Y I and X? Y=?. If a rule X?Y is true for T, we reach a conclusion that X?Y is an association rule. Let sup be the support of the rule X?Y in the dataset T, and conf be the confidence of this rule. They are defined as follows respectively:  ?  sup: Support (X?Y)=|{t | t contains X and Y}| / |T| * 100% conf: Confidence (X?Y)=|{t | t contains X and Y}| / |{t | t contains X}| * 100%=Support (X?Y) / Support (X)*100%  Let Minconf and Minsup be the thresholds of confidence and support specified by the user respectively.

Association rule mining is to find all the rules whose  confidences and supports are greater than or equal to Minconf and Minsup in the dataset.

The association rule mining is usually broken down into two sub-problems: (1)  Finding all the frequent itemsets whose supports are  no less than Minsup; (2) Generating the association rules derived from the  frequent itemsets and inspecting their confidences. If the confidence is not less than Minconf, the corresponding rule is an association rule that we attempt to find.

Of the two steps, the efficiency of association rule  mining mainly depends on the first step. After the largest frequent itemsets have been produced, the relevant association rules can be gained immediately.

The core of the mining algorithm is to identify the frequent itemsets whose supports are not less than Minsup, and this step is more time-consuming. If there are m items, 2m itemsets may exist. In fact, however, only a small part of itemsets is frequent. M.J.Zaki [2] regards it as NP-hard. The scalability of an algorithm is very crucial in association rule mining. With the dataset expanding, the efficiency of Apriori algorithm drops apparently. It is not difficult to find that low efficiency is the deficiency of Apriori algorithm.

Many researchers have tried to improve the efficiency from different angles, and decrease the time of scanning datasets and the candidate number. For example, N.Pasquoer [3] put forward an algorithm that uses closed itemset lattices to mine association rule. Eui-Hong Han [4] brought forward an algorithm based upon the parallel machines. [4,5] describe two parallel algorithms for discovery of association rules. [6] presents an efficient method of clustering the transaction records to the k-th cluster table. Besides these improvements, WeiWang [7] etc.

al extends the traditional association rule problem by allowing a weight to be associated with each item in a transaction to reflect the interest of each item within the transaction. Lifang Gu [8] present an algorithm which can      find association rules based on a set of new interestingness criteria. [9] introduces Association Rules Network, which is a hyper graphical model to represent a special class of association rules.

TBAR algorithm based on tree, presented by F.Berzal [10], improves the Apriori algorithm on reducing the storage requirements. Each node of the tree is a pair <a: v>, which 'a' is an attribute, 'v' is the value of 'a'. If the value of 'a' in tuple t is 'v', then t contains pair <a: v>. TBAR can only treat with the relational database, can't handle the transaction dataset, and lack pruning in the processing additionally. Based on TBAR, we bring forward STBAR (Super-TBAR), which is an improved algorithm in mining the association rules, and it has the ability of dynamic pruning.

The rest of the paper is organized as follows. In Section 2.1, we will introduce the concept of dynamic pruning and the implementation of STBAR algorithm given in Section 2.2. In section 3, we give experiments about these two algorithms using UCI dataset. And finally we present our conclusions in section 4.

2. STBAR: TBAR with the ability of dynamic pruning  2.1. Dynamic pruning  We take a transaction database D proposed by J.Han [11] for example in Table 1. Set the threshold of confidence to be Minconf=50%, we can obtain several rules in D with Apriori algorithm.

Table 1  Database D TID List of item_Ids  100 I1, I2, I5  200 I2, I4  300 I2, I3  400 I1, I2, I4  500 I1, I3  600 I2, I3  700 I1, I3  800 I1, I2, I3, I5  900 I1, I2, I3  STBAR employs the tree-based storage, which is analogous to TBAR. Each node in the tree is not a 2-tuple < a, v >, but a 3-tuple < a, v, t>, in which 'a' is the attribute, 'v' is the number of tuples which satisfy the condition, 't' is a flag whose value is 1 or 0. This flag will decide whether an  item can concatenate the items found in the paths from the root of the tree to the current item or not.

Figure 1 is a tree created by STBAR from the database D. Compared with Apriori, TBAR can save plenty of storage, whereas STBAR can save more candidate itemset space than TBAR. In the itemset tree, itemsets are arranged in sequence. The first level corresponds to the frequent itemsets L1, and the second level corresponds to the frequent itemsets L2, and so on. If searching the K-itemset, we must traverse this tree from the root to level Lk.

In Figure 1, the middle num in each 3-tuple is the support from the root to Lk. If the last num in a 3-tuple is '1', it means that all the supports from the root to Lk are no less than Minsup. '0' is the oppositeness, meanwhile it also indicates the concatenation from the root to the current item is infrequent. The line with arrowhead derived from each item stands for the generation of corresponding itemset.

Three levels marked with L1, L2 and L3 in Figure 1 are corresponding to the frequent itemsets of Apriori.

Dynamic pruning and the generation of corresponding itemset are processed at the same time by STBAR. On level L1, each item's support satisfies the condition, so the value of 't' in each 3-tuple is '1'. From the beginning of item I4 on level L1, all the items, which appear at the right of I4 are duplicated to the corresponding position in the next level.

For example, we copy I4, I5 on level L1 to the child position of I3 on level L2. After generating temporary L2, we judge the support of all items in L2 by scanning the database. For example, the support of I3, I4 is 1 (Note: The comma between I3 and I4 means "and"), so on level L2, the deleting mark in 3-tuple < I4, 1, 0> is set. By analogy, the others are respectively given on level L2.

STBAR can delete all the infrequent concatenations.

From right to left on level L2, we can find that I4 and I5 are not frequent, so the concatenations of I3, I4 and I3, I5, both are not frequent. Based on the monotone: any subset of a frequent itemset is frequent, otherwise, if one subset is infrequent, this itemset must be infrequent. We draw a conclusion that the concatenation of I3, I4, I5 must be infrequent. On level L2, from right to left, we can conclude that it is impossible to draw any lines from the third unit.

Likewise, from left to right, in the first unit on level L2, we cannot draw any lines from I3 and its right-hand items, but we can draw a line from I2. By scanning the database, we can obtain that the support of the concatenation I1, I2, I3 is 2, so being the support of I1, I2, I5. On level L3, it is determinant on level L2 that the concatenation of I3 and I5 is infrequent, so any line cannot be drawn from I3. Finally, we obtain two largest frequent itemsets, {I1, I2, I3} and {I1,I2,I5}.

This result is the same as that of Apriori and TBAR [10].

2.2.   Mining Algorithm  STBAR takes full advantages of dynamic pruning, which can cut down the infrequent concatenations during the tree constructing. It can be described as follows: Step 1: Generate all the frequent 1-itemsets, and store them  in the 3-tuples.

Step2: According to the order In-1 ? I1, generate itemset L2,  validate whether it can constitute a frequent itemsets or not, and set the corresponding flag in the 3-tuple.

Step 3: For each sub-itemset in L2, recursively generate Ln according to Step 2.

Step 4: Seek for the tree's depth.

Step 5: Find out the longest concatenations in the tree.

Step 6: Produce all the association rules. This step is just as  TBAR doing.

For example, the depth of the itemset tree is 3 in  Figure 1, and the two frequent concatenations are I1, I2, I3 and I1, I2, I5. We can easily obtain all the association rules.

3. Experimental analysis  As C++ STL is powerful, STBAR and TBAR are both implemented with C++ STL, compiled with Microsoft Visual C++ 6.0. STBAR and TBAR have been applied to several classical dataset, which can be obtained from the UCI Machine Learning Database Repository at http://www.ics.uci.edu/~mlearn/MLRepository.html.

?  Zoo Dataset: Be from the UCI Warehouse, and also  can be obtained in the handbook of Forsyth's PC/BEAGLE.

?   Sponge Dataset. This dataset, also from the UCI Repository, includes descriptions of 76 instances with 45 attributes.

?   Chess Dataset: This dataset, taken from the UCI Repository, has 37 attributes, and the first 36 attributes describe the chessboard, and the last judge who is the winner.

Platform: Standard PC, AMD Athlon XP 2200+  1.81GHz, 512M DDR266 SDRAM, with Windows XP Professional. The result of the experiments is listed in Table 2.

The processing time listed in Table 2 only includes the cost in finding all the frequent itemsets, without that in generating the association rules, because their difference lies in how to handle the infrequent concatenations.

As shown in Table 2, compared with TBAR, STBAR has little time-consuming about 10%~40%, whether the Dataset is small or large. Besides this advantage, STBAR employs dynamic pruning, which can cut down the unnecessary concatenations, and accordingly it can save  more memory than TBAR. When the memory is not enough, it seems that pruning is more important. At the same time, because the operating system reduces the paginations, the CPU processing time is decreased greatly.

The comparisons of STBAR and TBAR on each Dataset are shown in Figure 2, Figure 3 and Figure 4.

0.5   1.5  0.9 0.8 0.7 0.6 0.5  Support  Ti m  e co  st (  s )  STBAR TBAR   Figure 2. Zoo Dataset      0.9 0.8 0.7 0.6 0.5 Support  Ti m  e co  st (  s ) STBAR TBAR  Figure 3. Sponge Dataset   0.9 0.8  0.7     Ti m  e co  st (  s )  Support  STBAR TBAR  Figure 4. Chess Dataset      4.  Conclusions  Effectively mining association rules is important in data mining. Based on dynamic pruning, STBAR algorithm not only saves plenty of storage space, but also has the system response time shortened. It can effectively find the association rules from the dataset. Experiments with UCI dataset show that this algorithm is more efficient than TBAR, which outperforms Apriori, a famous and widely used algorithm.

Acknowledgements  This research is Supported by the Aeronautical Science Foundation of China under Grant No. 02F52033 and the Hi-Tech Research Project of Jiangsu province under Grant No. BG2004005.

Thanks to three anonymous referees for their constructive comments.

