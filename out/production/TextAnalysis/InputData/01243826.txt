Extended Multi-word Trigger Pair  Language Model Using Data Mining Technique"

Abstract - a good language model is essential to a postprocessing algorithm for recognition systems. Trigger pair model has been used to investigate long distance dependent relationship. However, previous trigger pair model has only one wordfor its trigger. It is desirable that more words can be observed in the trigger for a betfer prediction of the triggered word In this work, we view estoblishing triggerpair model as mining association rules in a large database and create a multiple words trigger pair model by using an adapted Apriori algorithm. The new trigger pair model can be used in the stage offinding best path from a word lattice as traditional trigger pair model can. Specially, it can be used tu correct mistakes remaining in the final result as well. Those mistakes would be unavoidable for other language models.

Keywords: pattern recognition, Chinese recognition, language model, data mining.

1 Introduction Language models (LM) are commonly used in post  processing algorithms of recognition systems to improve the recognition rate [2][4]. During postprocessing, a language model serves as a knowledge base to assist the recognition system to make a better decision. N-gram model, widely used in recognition systems, reflects the relationship between current word and (N-1) immediately preceding words. Due to data sparseness, the largest value of N is usually 3. This limits ow view on the history. In other words, we have a near-sighted vision on the history.

On the other hand, to make a better prediction of the cment word, we need to know more about the history. In previous works, researchers have made much effort towards this goal. In [9][3], Kneser and Bahl created topic- dependent language model, that is, for each predefmed topic, they generated a separate N-gram model. When using the language model, first the topic of current text has to be determined, since for different topic different N-gram pattem will be used. Iyer and Ostendolf used a similar approach in [8], except that they capture sentence-level topic characteristics instead of word-level characteristics.

Bellegarda used SVD technique to decompose a word by document matrix and to construct a latent semantic language model in [6] .

The trigger pair technique, suggested by Lau, R. [lo], represents another research effort along this direction. The basic idea in trigger pair mechanism is to investigate the Mutual Information (MI) between trigger pistory) and triggered word (the predicted word) which reflects the degree of dependency between them. This information can help the recognizer to pick a correct word from the candidate list in the postprocessing stage. However, in previous research works on trigger pair model, there is a common weakness - the trigger and triggered pair are both single word. This situation is due to the formidable computational workload in obtaining trigger pairs whose trigger andor triggered word are composed of multiple words. In practice, we are only interested in those trigger pairs whose trigger is composed of multiple words and whose triggered word is a single word. With this kind of trigger pair, we can predict the current word (triggered word) based on the knowledge of what words occurred previously (multiple words trigger) in history. Hereinafter, we refer to this kind of trigger pair as multiple words triggerpair. We can use the model not only in picking the best path from a word lattice, but also in correcting mistakes remaining in the best path. Those mistakes would otherwise be unavoidable for other language model  2 Introduction of Trigger Pair Model  The idea of trigger pair is actually motivated by the reading experience of human beings. When a reader is reading an illegible or mistakenly typed word, helshe can usually guess the correct word by referring to one or more words in the context highly associated with it. Typical highly associated word pairs are "bo th...and", "movie ... star", etc. This kind of dependent relationship can be used in predicting the current word based on the knowledge of previously occurred and highly associated words. Usually, a trigger pair can he denoted as 4 + B , where A,, represents the trigger, and B the triggered word.

The distance between the trigger and the triggered word  262 * 0-7803-7952-7/03/$17.00 0 2003 IEEE.

may exceed 3 words. This is very meaningful for we can use the trigger pair model as a complement to the N-gram model which has short view on the history.

2.1 Window sue for trigger pair model  In this study, we determined an appropriate window sue of 6 by establishing a family of distanced b i g "  models, justasin[l3][12].

2.2 Selection of trigger pairs  Usually, we use formula (1) or (2) to select trigger pairs.

Formula (1) only takes the extent of correlation between the trigger and the triggered word into account. In contrast, formula (2) takes both the extent of correlation and utility of the trigger pair into account. In this study, we use formula (2) as the criteria for selecting the most interesting trigger pairs into the model. Audiences can refer to [13][12][1] for a detailed discussion about the two formulas.

Where; PW : the probability of event X, P(x) = 1 - P(X) : the probability that event A does not  occur;  Where, P(X I r )  : the conditional probability of X given Y, P(X,Y) : the joint probability of event X and event Y  occur together.

We use AMI to select the most meaningful trigger pairs,  while use MI [I] to measure the actnal degree of the dependent relationship between the trigger and triggered word. We will use this value in o w  later application.

where the numerator is the joint probability of & and B the dominator is the product of probabilityP(A,) and P ( B ) .

2.3 Distance-related or Distance-unrelated  Given a trigger and a triggered word, the distance between the trigger and the triggered word may he different, measured by the number of words between them.

When establishing a trigger pair model, we can neglect the difference in distance. We simply consider those trigger pairs, which share the same trigger and the same triggered word but have different interval distances, to be the same.

This kind of trigger pair model is called Distance-unrelated  trigger pair model. Certainly, the information provided by such kind of trigger pair model is less accurate, since the information contained in trigger pairs of different interval distance is mixed together. If we create separate trigger pair model for different interval distance, then we can obtain more accurate information. We call this kind of trigger pair model as Distance-related trigger pair model. In this study, we established a Distance-related trigger pair model.

3 Multi-word Trigger Pair As previously explained, the multiple words trigger pair  will contain multiple words in the trigger. For ease of understanding, we use a Tri-word trigger pair as an example in the following explanation. It can be represented as 4 , A i  + B . 4 appears in the second last sentence; A,' appears in the last sentence; B appears in the current sentence. We can refer to 4 as the first trigger, A: as the second trigger, and B as the triggered word. Such an arrangement enables us to capture long distance dependent relationship. We only picked those Tri-word sequences which occurred more than a predefined times over a huge amount of training text. Those Tri-word sequences might not occur by chance and should he considered quite stable.

Hence, we can use those Tri-word pattems to correct mistakes in a sentence. The details on such kind of application will be presented in section 5 .

Our method for establishing the Tri-word trigger pair  model, discussed below, can easily be extended for creating more complex trigger pairs where there are more constituent words for the trigger and their locations in the context are free. For example, they may appear in previous sentences or in the same sentence as the triggered word is.

To further explain the Tri-word trigger pair, we use the  following example to illustrate what a Tri-word trigger pair looks l i e .  Assume three sentences form following scenario, tic: -c:c;c: -c: -c:c; , c;c: -c:c:c:-c: -c$: , coco , - ,c,c,-c,o-c,o 0 a  where, C represents a single Chinese character, a group of Cs standing together represents a word. Words in a sentence are separated by the sign "-"_ C; represents the I- th character of the m-th sentence. We can form a Tri-word trigger pair by randomly picking up three words respectively from the second last sentence, the middle sentence and the current sentence. For instance, CiC; , CiC$: and C:Cp can form a Tri-word trigger pair. CiC: is the first trigger (AA), C:C:C: the second trigger ( 4  ), and C:Cp the triggered word (B),  There are 32,000 words in our vocabulary.

Theoretically, the number of possible three-word sequences (a Tri-word trigger pair may be viewed as a three-word sequence) is 32,000x32,000x32,0003.2768x10". It is     computationally infeasible to investigate such huge amount of trigger pairs to select best ones. We get around the problem by using an adapted Apriori algorithm. The Apriori algorithm is used by researchers in data mining field to discover association rules from a huge amount of database records.

3.1 Apriori algorithm  The main task for data mining researcher is to mine out valuable association rules from database. These d e s  can be used to guide later activities. See [7] for a detailed explanation about the Apriori algorithm.

WI, can generate two word sequence, {W,, W,} and {WI, Wl} for C2, since the fmally obtained trigger pair is sequential. We can produce the two-word sequence by sim  (4) scan the training database to count the number Of occurrences of each word sequence in Cz. One occurrence of {W;, Wi) means W; auuears in the urevious sentence,  CI  3.2 Adapted Apriori algorithm  Our goal is to fmd a Tri-word trigger pair. The trigger consists of two words, one located in the second last sentence (AA ), the other located in the last sentence ( 4 ), the triggered word (B) is located in current sentence. Such kind of trigger pair can be viewed as an association rule looks like, A: and A i  = B .  Hence, it is possible for us to use Apriori algorithm, which possesses good pruning capability during searching, to discover Tri-word trigger pair in a huge amount of tex!. However, we have to adapt it before using it.

We can think each word inthe vocabulary as the item in the transaction records, each sentence as the transaction record in the database and the corpus as the database. One difference between the two cases is that trigger pair is sequential while itemset is not. Thus, we made some changes in the Apriori algorithm to fit the trigger pair case.

The procedure is described as follows. Note that we will use word sequence, instead of trigger pair, in the middle steps, since those three-word patterns are not trigger pairs yet in the middle steps.

(1) Initially, each word in the vocabulary is a member of CI. We scan the training data to count the number of occurrences of each word.

count The notation W , A W ,  represents the pattern Wi  . _ _  1, . ~~ meanwhile Wi appears in the current sentence.

W I )  6000  (5 )  Put word sequences with count over or equal to 20 into  followed by w k  with n sentences apart. For the study in this  LZ  WI, w2 WI, w3  wz, w4 200 w2, w5 200  (6) We generate member candidates for C3 by joining members of LZ in a different way than that used processing transaction records. First, we pick one member in 4, say, {W;, WJ. Then, we see ifthere is another word sequence in Lr whose fust word happens to be W,, say, {W,, wk}. If such a word exists, we can form a three-word sequence {Wi, Wj, Wk](the last word of {Wi, Wj] is the same as the first word of (W,, Wk}) to be a candidate for C3. At this moment, it is just a candidate for C3. Next, we check if the count for W, -% W, meets the support count requirement.

'  L -  I .... ..... I I {  W"1 I 10  (2) Suppose that the support count is 20 and totally 5 words in CI is eli ible for L,. pi  w4 2000  (3) Form CZ by making all -possible combinations of the members in L1. Note every two members in LI, say WI and  paper, n=2, W,  &W, represents Wi followed by Wk with 2 sentence apart. For example, Wi appears in the second last sentence and Wk appears in the current sentence. If (Wi, W,, wk) meets this condition, it is qualified for C3.

For this purpose, we have to generate a model containing the information about W, &Wk. We use &*to represent this model.

Following the same reasoning line, if there are {Wi, Wj, wk} and {W,, w k ,  W,} in L3, the candidate {Wi, Wj, wk, W,) for C4 can be formed since the fwst two words of {Wj, wk, W,] are the same as the last two words of {W, W,, Wk}. If TAW, meets the requirement of support count, then (W;, W,, Wk, Wl} is a true member for C4. If we want to determine if {Wl, W2, ... W,] is a member of 6, fmt we see if the {Wl, Wz ,... Wm.,} and {WZ, W3 ,... W, }are     both member of L,, then see if the count of W, 4 W, meets the support count requirement.

This framework for member generation keeps the  property of downward closure [ 5 ]  as in the original Apriori algorithm. The property of downward closure means all subsets of a frequent itemset of sue (i+l) would also be frequent. For the trigger pair case, downward closure means all subsequences of a frequent word sequence (trigger pair) of sue (i+l) would also be frequent.

I count (7) Count the number of occurrences of members in C3.

C3  (8) There are two members meet the requirement on support count and go into L3.

WI, w2, w3 w 9 w2, w4  (9) There is no overlap between the two members in L3.

Hence. the iteration terminates.

After obtaining frequent three word sequences, we use the average mutual information to pick out the most interesting ones. Take the word sequence (WI, W,, WS) as an example. Unliie the case of processing transaction records where many association rules may be derived from a single frequent itemset, there is only one possible Tri- word trigger pair rule can be derived from a three-word sequence. For example, from (Wl, W2, W5), we have:  ~ w l ~ w 2 ) = ~ w s l Here, the trigger is (Wl, W2) and the triggered word is  (WSl.

The reason we use AMI as a metric for trigger pair  selection is it is a good measure of the expected benefit provided by the trigger in predicting the triggered word in terms of information theory[l3][12]. We modified the formula (3) to (4) in order to calculate the AMI of the Tri- word trigger pair.

As described in step (6), the adapted algorithm significantly reduce the number of trigger pairs whose AMI   would be evaluated, therefore the computer workload is alleviated. Actually, the adatapted Apriori algorithm is figured out based on following fact: sequence (W,, W2 ,... Wm.l) and (W2, W3 ,... W,,, ) being frequent, and w;*w, meeting the support count requirement are necessary conditions. The necessary conditions are minimal requirement for (Wl, W2, ... Wm ) being frequent. We use these necessary conditions to reduce the search space. To carry out the search space reducing operation in step (6), two suppolting language models are needed, one is such a language model that can reflect the concurrence information of two words with one word in the previous sentence and the other word in the current sentence; the other supporting language model is such a language model that can reflect the concurrence information of two words with one in the second last sentence and the other word in the current sentence, that is, L,' . For later reference, we call the former supporting language model as Supporting Language Model A, and the latter one Supporting Language Model B.

4 Construction of Models We established the traditional trigger pair model, the Tri-  word trigger pair model and the word bigram (N-gram model, N=2) model all over a IS million characters training text. It contains a wide variety of topics, such as economy, f m c e ,  law, technology, stock, etc.

As mentioned in section 2, we constructed a distance-  related trigger pair model with a window size of 6 words.

We basically follow the t radi t io~l  method to construct the distance-related trigger pair model, except that we did not construct a trigger pair model for the case that the trigger is immediately followed by the triggered word, i.e., d=l. This kind of information is already captured by the bi-gram language model. Hence, we only constructed these trigger pair models with distance of 2,3,4,5 or 6 words.

First, we established distance4 (&2,3,4,5,6) word bigram models. Based on these models we then construct trigger pair models. We rule out those distance4 bigram patterns which occur less than 2 times, since they are not stable. In other words, they occur by chance in the training text. Additionally, We set a threshold of 0.1 for AMI. That is, only these trigger pairs whose AMI values are greater than 0.1 will be collected and their MI values will be calculated.

We follow the adapted Apriori algorithm to construct the Tri-word trigger pair model. The file size of Supporting Language Model A and Supporting Language Model B are 92862Ohytes and 859535bytes, containing 70298 and 65102 entries respectively. The size of the file containing C, is 56M or so, including 3,826,403 entries. In our study, the support count is 30. The threshold for AMI is 0.1. The file size of Tri-word trigger pair model is about SOMbytes, including around 1.2 million entries. The MI value for the Tri-word trigger pair is calculated according to formula ( 9 ,    The bi-gram model has a size of 6.5Mbytes, and smoothed by the Katz smoothing technique.

5 Application The function of the special Tri-word trigger pair is  two-fold in our research. First we combine it with traditional N-gram model to fmd M best paths from a word lattice when we apply Dynamic Programming to the word lattice (see section 6 for explanation of word lattice). In this application, the MI values of the Tri-word trigger pair are used. After we get M best candidates, we pick the top best one. Most likely some mistakes remain in it. The Tri-word trigger pair is utilized again but in a different way to correct the remaining mistakes. With the special Tri-word trigger pairs, we can correct the mistakes in three different ways.

(1) Based on the 4 in the second last sentence and 4 in the last sentence, correct mistakes in current sentence; (2) Based on Ai in the last sentence and B in current sentence, correct mistakes in the second last sentence; (3) Based on the A; in the second sentence and B in current sentence, correct mistakes in the last sentence. (2) and (3) are very special and attractive, since it allow us to correctmistakes in the history which would othenvise never be salvaged. As a matter of fact, the three ways of correctipg error are the very motivation for establishing such a Tri-word trigger pair model with the 3 words in the second last sentence, the last sentence and the current sentence respectively.

In the following description, we illustrate how to correct mistakes in the current sentence in way (I). As for correcting mistakes in way (2) or (3). the process is the same.

Actually, we focus on those positions in the top best path where there are at least two successive single character words appearing. We have found that 80 percent mis- recognized characters in the top best path are single character words.. Hence, we put our effort on salvaging these errors caused?by single character words. Assume, the recognition results of the second last sentence, the last sentence and the current sentence form following scenario.

ctc: -c:c:c: -c: -c:c:, c:c: -c:c;c: -c:c: -c:c:, c:cp - cc,oc -d - c: - G  C t ,  Ct and C: are consecutive three words. First, we focus on C: . It is the sixth character in the current sentence, and in the character lattice, we have produced 12 candidates for each characters. Now, we pick top k candidates from the sixth column (in our system, k=2 or 3).

Then, we interrogakthe Tri-word trigger pair model to see whether there are any trigger pairs which meet following requirements, (a) their triggered word is a 2-character or 3- chracter word; (b) the fust character of their triggered word  is one of the top k candidates; (c) the first trigger and second trigger of these trigger pairs satisfying (a) and (b) will be matched with one word in the second last sentence and one word in the last sentence respectively. The triggered word of these trigger pairs satisfying above 3 conditions is treated as word candidate. Next, we focus on C t  . Find out all possible word candidates for this position in the same way. This example is about a scenario containing consecutive 3 words. We stop at the character C,? since a multiple character word consists of at least two characters. If we are facing consecutive h words, we will stop the process at the fi-l)-fh character. In the end, alI possible word candidates as well as original words, like C:Cp, C:C:Ct, Ct , C: and C: , form a word lattice.

We find the best path from the word lattice by using the combined model of the N-gram model and the trigger pair model. As we can imagine, the word candidates suggested by the matched trigger pairs may contain character outside the 12 candidates produced by the recognizer since only the first character in these word candidates is required to he one of the fmt k candidates. Hence, it is possible the f~ recognition rate may exceed the top 12 recognition rate, as shown in OUT experiment. After fmishing one iteration of correction in the way of (l), (2) and (3), we can conduct another, since new words may turn up after finishing previous iteration.

The above description is the process of the error- correcting in way (I). The implementations of error- correcting in way (2) and (3) are the same.

The following example is to illustrate how the error in the current sentence can be corrected in the way (I).

Second last sentence:  Last sentence: 5? d %e a& Current testing sentence: E P q  &% iB% W% m s5i 8 Topbestpath: EP 5J E!?.% iB% 87 I m ZB% 8 Correctedresult: EP 5J &% 67% %@ Btl ZS5i 8 Used Tri-word trigger pair &&I %?E W%  RE r;il H4k #is +>G #Ea $53  Note that the charxter ??$5?? is salvaged, even though it is not included in the 12 candidates for column 8.

6 Experiments  Text  Original System System System Top 12 rate(%) Arate ph) B rate Crate rate(%)  82.56 89.64 90.16 91.37 91.42 (%) (%)  A I  I I B  I  System C outperfom System B by 1.21 percent(ahsolute1y) on text A, by 1.04 percent(ahsolute1y) on text B; System C outperfom System A by 1.73 percent(ahsolute1y) on text A, and by 1.53 percent(absolute1y) on text B.

7 Conclusion In this study, we successfully introduce the data mining  technique into the language modeling field and construct a multiple words trigger pair language model. Our experiments show that the new language model can he used not only in iinding the hest path from a word lattice but also in correcting errors remaining in the fmal result. By introducing the data minimg technique, we propose a new research direction for language modeling.

