Mining to Compress Table Constraints

Abstract?In this paper, we propose an extension of our mining-based SAT compression framework to constraint sat- isfaction problem (CSP). We consider n-ary extensional con- straints (table constraints). Our approach aims to reduce the size of the CSP by exploiting the structure of the constraints graph and its associated microstructure. More precisely, we apply itemset mining techniques to search for closed frequent itemsets on these two representations. Using Tseitin extension, we rewrite the whole CSP to another compressed CSP equiva- lent with respect to satisfiability. Our approach contrasts with the previous proposed technique by Katsirelos and Walsh, as it does not change the inner-structure of the constraints.

Experiments on some CSP instances show that our approach can achieve interesting compression rate.

Keywords-Constraint satisfaction problems, table constraints, compression, data mining.



I. INTRODUCTION  The table constraint has been considered for a long time as particularly important in constraint satisfaction problems (CSP). Indeed, one of the most used formulation of CSP consists in expressing each constraint in extension or as a relation among variables with associated finite domains.

Many research works consider table constraints as the stan- dard representation. Indeed, any constraint can be expressed using a set of allowed or forbidden tuples. This is clearly a simple and general way to represent any constraint involving discrete variables. However, the size of these kind of exten- sional constraints might be exponential in the worst case.

Even if these kind of constraints are used fore modeling several kind of problems such as configuration problems, the space complexity remains an obstacle for their wide use for modeling real-world combinatorial problems. Moreover, the increasing use of constraint programming for modeling and solving real-world combinatorial problems inevitably leads to a substantial increase in the size of the associated constraints networks.

In constraint satisfaction problems, Katsirelos and Walsh [14] proposed for the first time a compression algorithm for large arity extensional constraints. The proposed algorithm attempts to capture the structure that may exist in a table constraint. The authors proposed an alternative representa- tion of the set of tuples of a given relation by a set of compressed tuples. The proposed representation may lead to an exponential reduction in space complexity. However, the compressed tuples may be larger than the arity of the original constraint. Consequently, the obtained CSP does not  follow the standard representation of the table constraint.

The authors use decision trees to derive a set of compressed tuples. In Propositional Satisfiability (SAT), Jabbour et al.

[12], proposed the first mining based approach to compress Boolean formulae in conjunctive normal form (CNF). It combines both frequent itemset mining techniques [1] and Tseitin?s encoding [18] for a compact representation of CNF formulae. Its originality lies in the exploitation of data mining techniques to discover structural hidden knowledge and their use for compression and automatic reformulation of some well-known constraints. To the best of our knowledge, this contribution presents the first application of data mining techniques to propositional satisfiability.

In this paper, we extend our SAT mining based compres- sion approach [12] to derive a new compact representation of the table constraints. Our extension to CSP is obtained thanks to two rewriting rules: (1) constraints hypergraph and (2) microstructure based compression rules. More precisely, we first show that our previous SAT Mining compression approach can be extended to deal with constraints networks by considering the constraints hypergraph as a transaction database, where the transactions correspond to the scope of the constraints and the items to the variables of the CSP. The (closed) frequent itemsets correspond to subsets of variables shared most often by the different constraints of the CSP. Then, using auxiliary variables, we show how such constraints can be rewritten while preserving satisfiability.

Secondly, we consider each table constraint individually and we derive a new transaction database made of a sequence of tuples i.e. a set of indexed tuples. Each value of a tuple is indexed with its position in the constraint. By enumerating closed frequent itemsets on such transaction database, we are able to search for the largest rectangle in the table constraint.

Similarly, we show how such constraint can be compressed while preserving the traditional representation. Finally, we provide a microstructure based compression rule to deal with the particular case of binary constraints. Our main goal behind this research issue is to achieve significant reductions of space complexity of extensional constraints.



II. TECHNICAL BACKGROUND A. Frequent Itemset Mining Problem  Let I be a set of items. A set I ? I is called an itemset.

A transaction is a pair (tid, I) where tid is a transaction identifier and I is an itemset. A transaction database D is a   DOI 10.1109/ICTAI.2015.68    DOI 10.1109/ICTAI.2015.68    DOI 10.1109/ICTAI.2015.68     tid itemset 001 Spaghetti, Olive oil, T omato,Mozzarella 002 Spaghetti, Parmesan,Olive oil, Beef 003 Chili pepper,Anchovies 004 Eggs  Table I AN EXAMPLE OF TRANSACTION DATABASE D  finite set of transactions over I where for all two different transactions, they do not have the same transaction identifier.

We say that a transaction (tid, I) supports an itemset J if J ? I . The cover of an itemset I in a transaction database D is the set of identifiers of transactions in D supporting I: C(I,D) = {tid | (tid, J) ? D and I ? J}. The support of an itemset I in D is defined by: S(I,D) =| C(I,D) |.

Moreover, the frequency of I in D is defined by: F(I,D) = S(I,D)|D| .

Example 1: Let us consider the transac- tion database over the set of items I = {Spaghetti, Tomato, Parmesan,Beef,Olive oil, Mozzarella, Chili pepper,Anchovies, Eggs} (Table 3).

We have S({Spaghetti, Olive oil},D)= |{001, 002}|=2 and F({Spaghetti, Olive oil},D)= 24 .

Let D be a transaction database over I and ? a minimal support threshold. The frequent itemset mining problem consists in computing the following set: FIM(D, ?) = {I ? I | S(I,D) ? ?}.

The problem of computing the number of frequent item- sets is #P -hard [10]. The complexity class #P corresponds to the set of counting problems associated with a decision problems in NP . For example, counting the number of models satisfying a CNF formula is a #P problem.

Let us now define two condensed representations of the set of all frequent itemsets: maximal and closed frequent itemsets.

Definition 1 (Maximal Frequent Itemset): Let D be a transaction database, ? a minimal support threshold and I ? FIM(D, ?). I is called maximal when, for all I ? ? I , I ? /? FIM(D, ?) (I ? is not a frequent itemset).

Definition 2 (Closed Frequent Itemset): Let D be a trans- action database, ? a minimal support threshold and I ? FIM(D, ?). I is called closed when for all I ? ? I , C(I,D) ?= C(I ?,D).

One can easily see that if all the closed (resp. maximal) frequent itemsets are computed, then all the frequent itemsets can be computed without using the corresponding database. Indeed, the frequent itemsets correspond to all the subsets of the closed (resp. maximal) frequent itemsets.

Clearly, the number of maximal (resp. closed) frequent itemsets is significantly smaller than the number of frequent  itemsets. Nonetheless, this number is not always polynomial in the size of the database [20]. In particular, the problem of counting the number of maximal frequent itemsets is #P - complete (see also [20]).

Many algorithms have been proposed for enumerating closed frequent itemsets. One can cite Apriori-like algo- rithm, originally proposed in [2] for mining frequent itemsets for association rules. In our experiments, we consider one of the state-of-the-art algorithm LCM for mining closed frequent itemsets proposed by Takeaki Uno et al. in [19].

In theory, the authors prove that LCM exactly enumerates the set of closed frequent itemsets within polynomial time per closed itemset in the total input size. Let us mention that LCM algorithm obtained the best implementation award of FIMI?2004 (Frequent Itemset Mining Implementations).

B. Constraint Satisfaction Problems: Preliminary Defini- tions and Notations  A constraint network is defined as a tuple P = ?X ,D, C?.

X is a finite set of n variables {x1, x2, . . . , xn} and D is a function mapping a variable xi ? X to a domain of values D(xi) = {vi1 , vi2 . . . vidi }. We note d = max{di|1 ? i ? n} the maximum size of the domains, and V = ?x?XD(x) the set of all values. C is a finite set of m constraints {c1, c2, . . . , cm}. Each constraint ci ? C of arity k is defined as a couple ?scope(ci), Rci? where scope(ci) = {xi1 , . . . , xik} ? X is the set of variables involved in ci and Rci ? D(xi1)? . . .?D(xik) the set of allowed tuples i.e.

t ? Rci iff the tuple t satisfies the constraint ci. We define the size of the constraint network P as |P| = ?c?C |Rc| where |Rc| =  ? t?Rc |t| and |t| = |scope(c)|. A solution to  the constraint network P is an assignment of all the vari- ables satisfying all the constraints in C. A CSP (Constraint Satisfaction Problem) is the problem of deciding whether a constraint network P admits a solution or not.

We denote t[x] the value of the variable x in the tuple t and by t[X] the projection of the tuple t on the set of variables X (a sub-tuple). Let t1 = (v1, . . . , vk) and t2 = (w1, . . . , wl) be two tuples (of values or variables), we define the non-commutative operator ? by t1 ? t2 = (v1, . . . , vk, w1, . . . , wl). Let P = ?X ,D, C? be a CSP instance, c = ?scope(c), Rc? ? C a constraint and s = (x1, . . . , xk) a sequence of variables such that V ar(s) ? scope(c) where V ar(s) is the set of variables of s. We denote by Rc[s] the following set of tuples:  Rc[s] = {(t[x1], . . . , t[xk]) | t ? Rc}  C. Constraint Graph Based Compression  1) CSP instance as transaction database: We describe the transaction database that we associate to a given CSP P = ?X ,D, C?. It is obtained by considering the set of variables X as a set of items, and each constraint c ? C is associated to a transaction containing the set of variables in     its scope with tidc as a transaction identifier.

Definition 3: Let P = ?X ,D, C? be a constraints net- work. The transaction database associated to P , denoted T DP , is defined over the set of items X as follows:  T DP = {(tidc, scope(c)) | c ? C} 2) Constraints Graph Rewriting Rule (CGR): We  provide a rewriting rule for reducing the size of a constraints network. It is mainly based on introducing new variables.

Definition 4 (CGR rule): Let P = ?X ,D, C? be a con- straints network, s = (x1, . . . , xk) a sub-tuple of variables from X and {c1, c2, . . . , cn} ? C a subset of n constraints of C s.t. s ? scope(ci) for 1 ? i ? n. To rewrite P , we introduce a new variable y /? X and a set N of l new values s.t. V ? N = ? and l = |?ni=1 Rci [s]|. Let f be a bijection from  ?n i=1 Rci [s] to N . We denote by Pg the constraint  network ?X g,Dg, Cg? obtained by rewriting P with respect to s and f : ? X g = X ? {y}; ? Dg is a domain function defined as follows: Dg(x) = D(x) if x ? X , and Dg(y) = N .

? Cg = C \ {c1, . . . , cn} ? C ? , where C ? = {c0, c?1, . . . , c?n} s.t.

? c0 = ?{y} ? {x1, . . . , xk}, {(f(a1, . . . , ak), a1, . . . , ak)|(a1, . . . , ak) ??n  i=1 Rci [s]}? ? c?i = ?(scope(ci) \ s) ? {y}, {t[scope(ci) \ s] ? (f(t[s]))|t ? Rci , t[s] ??n  j=1 Rcj [s]}?  It is important to note that our CGR rewriting rule achieves a weak form of pairwise consistency [13]. A constraint network is pairwise consistent (PWC) iff it has non-empty relations and any consistent tuple of a constraint c can be consistently extended to any other constraint that intersects with c.

Definition 5 (Pairwise consistency): [3], [13] Let P = ?X ,D, C? be a constraints network. P is pairwise consistent iff ?ci, cj ? C, Rci [scope(ci) ? scope(cj)] = Rcj [scope(ci) ? scope(cj)] and ?c ? C, Rc ?= ?.

As pairwise consistency deletes tuples from a constraint relation, some values can be eliminated when they have lost all their supports. Consequently, domains can be filtered if generalized arc consistency (GAC) is applied in a second step. As a side effect, our CGR rewriting rule maintains some weak form of PWC. Indeed, in Definition 4, when a sub-tuple t[s] /? ?nj=1 Rcj [s], the tuple t is then deleted and does not belong to the new constraint c?i.

Example 2: Let P = ?X ,D, C? be a constraints network, where X = {x1, . . . , x4}, D(x1) = , . . . ,= D(x4) = {a, b} and C = {c1, c2} where c1 = ?{x1, x2, x3}, {(b, a, a), (a, a, b), (a, b, a)}? and c2 = ?{x2, x3, x4}, {(a, b, a), (b, a, a), (b, a, b)}?. Let s = (x2, x3) s.t. s ? scope(c1) and s ? scope(c2). By applying the CGR rule on P , we obtain Pg = ?X g,Dg, Cg? s.t.

? X g = X ? {y} ? ?i(1 ? i ? 4),Dg(xi) = {a, b}. We have?2  j=1 Rcj [s] = {(a, b), (b, a)}. We define f((a, b)) = c, f((b, a)) = d. Then Dg(y) = {c, d}.

? Cg = {c0, c?1, c?2} ? c0 = ?{y, x2, x3}, {(c, a, b), (d, b, a)}?; ? c?1 = ?{x1, y}, {(a, c), (a, d)}?  and c?2 = ?{x4, y}, {(a, c), (a, d), (b, d)}?  In this simple example, using one additional variable, we reduce the size of the constraint network from |P| = 18 to |Pg| = 16. The value b can be eliminated by GAC from the domain of x1.

3) Necessary and sufficient condition for size reduction: Let P = ?X ,D, C? be a constraints network, and s = (x1, . . . , xn) ? X be a sub-tuple of variables corresponding to a frequent itemset Is of Pg where the minimal support threshold is greater or equal to k. Let {c1, . . . , ck} ? C be the set of constraints such that s ? scope(ci) for 1 ? i ? k.

Suppose that the constraints network P is pairwise consis- tent, in such a case, all the relations associated to each ci for 1 ? i ? k contain the same number p of tuples. Under such worst case hypothesis, the size of P can be reduced by at least r = (n ? p ? k ? (p ? k + n ? p + p)). Let us consider again the example 2. The reduction is at least r = (2 ? 3 ? 2) ? (3 ? 2 + 2 ? 3 + 3) = 12 ? 15 = ?3.

If we consider, the tuple (b, a, b) ? Rc1 eliminated by the application of the CGR rule. This results in subtracting 5 from the second term of r. Consequently, we obtain a reduction of 2.

Regarding the value of k, one can see that the compression is interesting when r > 0 i.e. k > n+1n?1 . Indeed, if n < 2 then there is no reduction. Thus, there are three cases : if n = 2, then k ? 4, else if n = 3 then k ? 3, otherwise k ? 2. Therefore, the constraints network is always reduced when k ? 4. We obtain exactly the same condition as in our mining based compression approach of propositional CNF formula [12]. This is not surprising, as CGR rule is an extension of our Mining4SAT approach [12] to CSP.

4) Closed vs. Maximal: In [12], we used two condensed representations of the frequent itemsets: closed and maximal.

We know that the set of maximal frequent itemsets is included in that of the closed ones. Thus, a small number     of fresh variables and new clauses are introduced using the maximal frequent itemsets. However, there are cases where the use of the closed frequent itemsets is more suitable. The example given in [12] shows the benefit that can be obtained by considering closed frequent itemsets. In our Mining for CSP approach we search for closed frequent itemsets.

D. Microstructure Based Compression  In this section, we describe our compression based approach of Table constraints. First, we show how a Table constraint c can be translated to a transaction database T Dc. Secondly, we show how to compress c using itemset mining techniques.

1) Table constraint as transaction database:: Obviously, a table constraint c can be translated to a transaction database T Dc. Indeed, one can define the set of items as the union of the domains of the variables in the scope of c (I = ?x?scope(c)D(x)) and a transaction (tid, t) as the set of values involved in the tuple t ? Rc. This naive representation is difficult to exploit in our context. Let I = {a, b, c} be a frequent itemset of T Dc. As the variables in each transaction (or tuple) associated to the values in I are different, it is difficult to compress the constraint while using both classical tuples and compressed tuples [14]. To overcome this difficulty, we consider tuples as sequence, where each value is indexed by its position in the tuple.

Definition 6 (Indexed tuples): Let P = ?X ,D, C? be a constraint network, and ci ? C a table constraint such that scope(ci) = (xi1 , xi2 , . . . , xini ). Let t ? Rci a tuple of ci.

We define indexed(t) = (t[xi1 ]  1, t[xi2 ] 2, . . . , t[xini ]  ni) as an indexed tuple associated to t, i.e. each value of the tuple is indexed with its position in the tuple.

Definition 7 (Inclusion, index): Let c be a table constraint with scope(c) = {x1, . . . , xn} and t = (v1, . . . , vn) ? Rc a tuple of c. We say that s = (w1, . . . , wk) is a sub-tuple of t, denoted s ? t, if ?1 ? i1 < i2 < ? ? ? < ik ? n such that w1 = vi1 , . . . , wk = vik . We define index(t) = {1, . . . , n}, while index(w) = {i1, . . . , ik}.

We also define vars(index(t)) = scope(c) and vars(index(w)) = {xi1 , . . . , xik}.

Definition 8: Let P = ?X ,D, C? be a constraints net- work, and c ? C a table constraint where scope(c) = {x1, . . . , xn}. The transaction database associated to c, denoted T Dc, is defined over the set of items I =?  t?Rc{t[x1] 1, . . . , t[xn]  n} as follows:  T Dc = {(tidt, indexed(t))|t ? Rc}  tid itemset 001 a1, b2, b3, a4  002 a1, a2, b3, b4  003 a1, b2, a3, a4  004 b1, b2, a3, a4  005 b1, b2, b3, a4  Table II T Dc A TRANSACTION DATABASE ASSOCIATED TO c  Example 3: Let P = ?X ,D, C? be a constraints net- work, where X = {x1, x2, x3, x4}, D(x) = D(y) = D(z) = D(t) = {a, b}. Let c ? C a ta- ble constraint, such that scope(c) = {x1, x2, x3, x4} and Rc = {(a, b, b, a), (a, a, b, b), (a, b, a, a), (b, b, a, a), (b, b, b, a)}. The transaction database T Dc associated to c is defined as follows.

Let I = {b2, a4} be an itemset of T Dc. We have S(I, T Dc) = |{001, 003, 004, 005}| = 4, index(I) = {2, 4} and vars(index(I)) = {x2, x4}.

2) Microstructure Rewriting Rule (MRR): We now pro- vide a rewriting rule to reduce the size of a table constraint.

Definition 9 (MRR rule): Let P = ?X ,D, C? be a constraints network and c ? C be a table constraint with scope(c) = {x1, x2, . . . xn} and |Rc| = m. Let I = {vi11 , . . . , vikk } be an itemset of T Dc and Y = vars(index(I)) = {xi1 , . . . , xik}. In order to rewrite c using I , we introduce a new variable z /? X and a set N of l new values such that V?N = ? and l = |?t?Rc t[Y ]|. Let f be a bijection from  ? t?Rc t[Y ] to N . We denote by P  m the constraints network ?Xm,Dm, Cm? obtained by rewriting c w.r.t. I and f : ? Xm = X ? {z}; ? Dm is a domain function defined as follows: Dm(x) = D(x) if x ? X , and Dm(z) = N .

? Cm = C \ {c} ? C ? , where C ? = {c0, c?} s.t.

? c0 = ?{z} ? Y, {(f(a1, . . . , ak), a1, . . . , ak)|  (a1, . . . , ak) ? ?  t?Rc t[Y ]}? ? c? = ?(scope(c) ? Y ) ? {z}, {t[scope(c) ? Y ] ?  (f(t[Y ]))|t ? Rc}?  Example 4: Let us consider again the example 3.

Applying the MRR rewriting rule to c with respect to I = {b2, a4}, and f where f((b, a)) = c1 and f((a, b)) = c2, we obtain the following two constraints: c0 = ?{z, x2, x4}, {(c1, b, a), (c2, a, b)}? and c? = ?{x1, x3, z}, {(a, b, c1), (a, b, c2), (a, a, c1), (b, a, c1), (b, b, c1)}?  It is easy to see that in example 4, applying MRR rule leads to a constraints network of greater size. In what follows, we introduce a necessary and sufficient condition for reducing the size of a table constraint.

x y a a a b a c b b b c b d  tid itemset a a, b, c b b, c, d  Table III FROM BINARY CONSTRAINT TO TRANSACTION DATABASE  3) Necessary and sufficient condition for size reduction: Let c be a table constraint, p the number of tuples in Rc, and s = (v1, . . . , vn) be a sub-tuple of values corresponding to a frequent itemset Is of T Dc where the minimal support threshold is greater or equal to k. Let {t1, . . . , tk} be the set of tuples such that ti[vars(index(s))] = s for 1 ? i ? k.

The size of Rc can be reduced by at least r = (n?k? (p+ 1+ n+ (p? k)). Let us consider again the example 4. The reduction is at least r = (2? 4? (5 + 1 + 2 + (5? 4))) = 8?9 = ?1. In this example, we increase the size of c by one value. Indeed, |Rc| = 20 and |Rc0 |+ |Rc? | = 6 + 15 = 21.

Regarding the value of k, one can see that applying MRR rule is interesting when r > 0 i.e. k > 2?p+n+1n+1 . In the previous example, no reduction is obtained as 4 > 2?5+2+12+1 .

(4 > 4, the condition is not satisfied).



III. COMPRESSING BINARY CONSTRAINTS  In this section, we show how we extend our approach for compressing table binary constraints. We first show how to derive a transaction database from a table binary constraint.

Then, we use a compression method similar to MRR rule.

Definition 10: Let c = ?{x, y}, Rc? be a binary constraint and v ? D(x). The (x, v)-transaction in c, denoted I(x,v,c), is the set {v? ? D(y) | (v, v?) ? Rc}.

Definition 11: Let c = ?{x, y}, Rc? be a binary constraint. The transaction database T Dbc associated to the binary constraint c corresponds to {(v, I(x,v,c)) | ?t ? Rc, t[x] = v}.

The Table III represents a binary constraint c and its corresponding transaction database T Dbc.

Let I an itemset and S a set of pairwise disjoint  itemsets. We denote by E(I, S) the itemset obtained from I by removing the itemsets in S. For instance, E({a, b, c}, {{a, b}, {c, d}}) = {c}.

Definition 12 (MRRb Rule): Let P = ?X ,D, C? be a con- straints network, c = ?{x, y}, Rc? ? C a binary constraint and S a set of pairwise disjoint itemsets in the transaction database T Dbc associated to c. In order to rewrite c using S, we use a fresh variable z /? X and a set N of |S| fresh values. Let f be a bijection from S to N . We denote by Pb  the constraint network ?X b,Db, Cb? obtained by rewriting c w.r.t. S and f as follows: ? X b = X ? {z}; ? Db is a domain function defined as follows: Db(x) = D(x) if x ? X , and Db(z) = N ?D(x).

? Cb = C \ {c} ? C ? , where C ? = {c0, c?} such that: ? c0 = ?{x, z}, {(v, f(I)) | v ? D(x), I ?  S and I ? I(x,v,c)} ? {(v, v) | v ? D(x) and |E(I(x,v,c), S)| > 0}?  ? c? = ?{z, y}, {(f(I), v) | I ? S and v ? I} ? {(v, v?) | v ? D(x) and v? ? E(I(x,v,c), S)}?  Example 5: Let us consider the binary constraint c of Example ??. We take S = {{b, c}} containing a single itemset of T Dbc with S({b, c}) = 2 . Let S? = {e} be the set of fresh values such that f({a, d}) = e. Applying the MRRb rule on c using S, we obtain the two following binary constraints c0 = ?{x, z}, Rc0? and c? = ?{z, y}, Rc??:  x z a e a a b e b b  z y a a b d e b e c  Contrary to the CGR and MRR rules (non binary CSPs), MRRb can be applied directly by considering only disjoint closed frequent itemsets. The reduction for this rule can be computed as follows. Let tx be the number of x values such that |E(I(x,v,c),S)| > 0. Then, the reduction for this rule is r = n ? k ? n ? k ? tx. For the previous example, r = 2 ? 2? 2? 2? 2 = ?2, showing that the frequent item set {(b, c)} is not interesting for compression purposes.



IV. EXPERIMENTS In this section, we present an experimental evaluation of  our approach. We have tested every rule independently, in order to catch the real compression impact. All tests have been performed on a Intel Core i7 2,7 GHz (8 GB RAM).

For each rule the compression process starts by generating the associated transaction database. For the constraint graph rewriting rule, a single transaction database is generated for the whole constraints network, whereas for the other two rules based on the microstructure, a transaction database is generated for each table constraint of the instance. In the second step, we enumerate the closed frequent itemsets for each of these transaction database. For the enumeration process, we use the LCM 5.3 implementation (updated version of LCM 2 [19]).

For the first two rules (CGR and MRR), we used a greedy approach. More precisely, closed frequent itemsets are sorted in descending order with respect to their surface (|I|?S(I,D)). The best closed frequent itemset is then used to compress the CSP instance if it is interesting in terms of size reduction (i.e. the reduction computed for this itemset is strictly greater than 0). Then, we take the next best frequent itemset and that does not intersect with the previous used     one. The same process is iterated until there is no more interesting frequent itemset.

We have first applied the constraints graph rewriting rule (CGR) on the 490 n-ary benchmarks containing table constraints expressed only with a set of allowed tuples.

Table IV highlights the obtained results on representative sample of families of instances.

The first column indicates the name of the instance 1.

The next two columns represent respectively the size of the instance (in MB or KB) before (Sinit) and after (Scomp) applying the compression rules. The third column indicates the reduction rate (red = 1 ? Scomp/Sinit) in percentages.

The next two columns are CPU time (in seconds) that respectively represent the time needed to solve the instance before (ti) and after compression (tc). The seventh column (dc) indicates the compression time in seconds. The penul- timate column represents the number of frequent itemsets (#FIS) that were actually used to perform the reduction.

The last column provide some indications on the size of the frequent itemsets: the size of the smallest (respectively largest) itemset (sm/bg).

In all our experiments, we used Abscon 112-v4 CSP solver to compare the solving time needed to solve the original CSP instance and the one obtained after applying the compression process. Time out was fixed to 30 minutes.

The results show that the efficiency of the constraint graph rule heavily depends on the family of benchmarks.

For instance, CGR rule achieves a high compression rate on rand-10-20-10 family of benchmarks. As an example, for the benchmark rand-10-20-10-10000-0 ext.xml, compression adds two intermediate variables, two constraints of arity 5, while reducing the total number of tuples from 50000 to 32779 (reduction rate of 44%). Most of the time, only few frequent itemsets are used in the compression process.

For the previous mentioned benchmark, only two frequent itemsets of size 4 have been used for the reduction. This shows that great improvements in term of space can be obtained even with smaller itemsets. For the compression time, it varies from less than a second for small instances to approximately one minute for bigger instances. Considering solving time, even if no general behavior can be drawn, interesting improvements are obtained on some instances such as half-n25-d5-e56-r7-24.xml. On the other side, compression can also slow down the solving time (e.g.

rand-3-20-20-60-632-34 ext.xml).

This rule does not present the same behavior on the whole families of benchmarks. For instance, no reduction could be achieved on all crosswords series of benchmarks. Even if many frequent itemsets are found, none of them reduces the total size of the table constraints, either because the number of tuples is too small or the  1Note that name of the CSP instances were normalized.

instance name Sinit Scomp red ti tc dc lard-83-83 66,74 38,78 41,9% 16.1 8.4 167 lard-84-84 70,09 5,62 92% 11.6 4.3 169 lard-89-89 88,58 50,89 42,6% 15.4 10.8 214 lard-90-90 92,72 6,82 92,6% 14.8 4.7 181 lard-91-91 96,96 26,29 72,9% 15.2 11.2 258 lard-92-92 101,35 7,26 92,8% 26.2 8.5 213 large-80-sat ext 58,63 6,02 89,7% 9.8 3.9 106 large-80-unsat ext 46,44 4,49 90,3% 10.1 3.7 55 large-96-sat ext 122,39 9,80 92% 25.4 9.6 245 large-96-unsat ext 96,44 7,46 92,3% 21.5 9.2 206 hanoi-7 ext 0,074 0,074 0% ? ? 1 rand-2-40-* ? ? 0% ? ? ? 15  Table V MICROSTRUCTURE BASED COMPRESSION FOR BINARY CONSTRAINTS  (MRRb)  frequency of the patterns is not great enough. Note that on these instances, the time need for compression is very small.

We have also applied the microstructure based rewriting rule (MRR) on these n-ary CSP instances. The compression rate is clearly very low on these available instances. This can be explained by looking at the necessary and sufficient condition for size reduction (see Section II-D3). More precisely, for a given constraint and closed frequent itemset, MRR is efficient if the surface of the frequent itemset is strictly two times greater than the number of tuples of the table constraint. This condition is quite demanding and was not usually met for the set of benchmarks we tested.

In our second experiments, we applied the microstructure rewriting rule (MRRb) for binary CSP instances. Instead of considering each closed frequent itemset after another, we directly applied compression using the set of the best and pairwise disjoint closed frequent itemsets. We evaluated this rule on the 91 binary benchmarks taken from the 2009 CSP competition, with constraints expressed in extension and containing only table constraints where the relation are represented with the set of allowed tuples. Table V contains a representative selection of these results. The size of the instances Sinit (column 2) and Scomp (column 3) are expressed in Mo. The different CPU times ti, tc and dc are given in seconds. The last line represents all the benchmarks with names starting with rand-2-40-*  The results show that the MRRb rule achieve a high com- pression rate on many families of benchmarks. In fact, for the lard and large families, the compression rate is greater than 40 %. However, for hanoi and rand-2-40 families no compression is observed.

When the compression rule can be applied, the compression time is higher than for the previous rules. In fact, closed frequent itemsets have to be computed for every relation.

For example, 3403 relations are considered in the lard-83- 83 benchmark. Consequently, this rule can be used as an     instance name Sinit Scomp red ti tc dc #FIS sm/bg half-n25-d5-e56-r7-1.xml 31,36 MB 27,4 MB 12,6% 148.9 701.1 24 11 2/2 half-n25-d5-e56-r7-24.xml 31,31 MB 26,93 14,0% T.O. 501.2 47 10 2/3 rand-10-20-10-5-10000-0 ext.xml 1 MB 0,56 MB 44,1% T.O. T.O. 28 2 4/4 rand-10-20-10-5-10000-7 ext.xml 1 MB 53 KB 94,7% T.O. 10.4 21 2 3/4 rand-10-20-10-5-10000-13 ext.xml 1 MB 0,49 MB 51,4% T.O. T.O. 14 2 4/6 rand-10-20-10-5-10000-19 ext.xml 1 MB 0,93 MB 6,9% T.O. T.O. 2 1 3/3 rand-3-20-20-60-632-16 ext.xml 1,27 MB 1,27 MB 0,0% ? ? 1 ? ? rand-3-20-20-60-632-34 ext.xml 1,3 MB 1,27 MB 2,2% 144.8 247.9 2 3 2/2 rand-3-20-20-60-632-fcd-40 ext.xml 1,34 MB 1,29 MB 3,7% 26.8 100.8 3 2 3/4 rand-8-20-5-18-800-9 ext.xml 22,49 MB 18,69 MB 16,9% 55.2 449.4 23 5 3/3 rand-8-20-5-18-800-14 ext.xml 22,52 MB 18,24 MB 19,0% 164.7 130.8 26 6 3/3 rand-n23-d3-e16-r15-t150000-2.xml 72 MB 55,37 MB 23,1% 686 134.1 54 3 4/8 rand-n23-d3-e16-r15-t150000-3.xml 72 MB 51,81 MB 28,0% 457.4 536 7 2 5/6 crossword-m1c-ogd-vg10-13 ext.xml 2,96 MB 2,96 MB 0,0% ? ? 2 ? ? dubois-100 ext.xml 29 KB 29 KB 0,0% ? ? ? 1 ? ?  Table IV CONSTRAINT GRAPH BASED COMPRESSION (CGR)  offline compilation process. Interestingly, we also observe some improvements in term of solving time. This comes from the fact that it takes less time to load smaller instances (and smaller relations). For the benchmarks that cannot be reduced with this rule (hanoi and rand-2-40 families), the overhead takes only few seconds (less than 15 seconds).

As a summary, the compression rate depends on the frequency of patterns in terms of shared variables between constraints and common sub-tuples of values in table con- straints. This explains why our approach works well only on some families of benchmarks. For instance, on some families of instances (large-*, lard-*, rand* and half-*), our rules achieve a good compression rate. Families such as renault* and aim* contain conflicts table constraints that our rules cannot handle yet. For BDD family, all constraints share the same single huge table constraint. In this case, applying our rules create new constraints, while keeping a reference to the huge table constraint.



V. RELATED WORKS  In [14], a compression algorithm for table constraints is proposed. The authors introduced a compact representation for a set of tuples constructed using decision trees. As men- tioned in the introduction, the proposed representation does not follow the standard representation of table constraints.

Moreover, only GAC is investigated in their experiments.

With our approach, the representation is not modified, and then any CSP solver can be applied on the compressed instances. In [14], the authors don?t provide a dedicated CSP solver (for constraints expressed as c-tuples).

Several forms of compression have been proposed re- cently, including Multi-valued Decision Diagrams (MDDs) [5], deterministic finite automata [17] and Tries [7]. In Propositional Satisfiability (SAT), Jabbour et al. [12], pro- posed the first mining based approach to compress Boolean formulae in conjunctive normal form (CNF). It combines both frequent itemset mining techniques and Tseitin?s en-  coding for a compact representation of CNF formulae.

More recently, Gharbi et al [8], followed the idea of using itemset mining techniques to compress table constraints.

They particularly investigate the possibility of combining both static and dynamic reduction techniques by proposing a new compressed form of table constraints based on frequent pattern detection, and exploiting it in STR (Simple Tabular Reduction). This last contribution follows the issue proposed in [14], where the new compressed table constraint does not fit the standard representation.

Recently, we proposed the first application of data mining techniques to compress Boolean formulas in conjunctive nor- mal form [12]. Independently, Manthey et al. [16] proposed a novel preprocessing technique to automatically reduce the size of Boolean formulas. This technique, called Bounded Variable Addition (BVA), exchanges clauses for variables.

The authors limit the search to detect only some specific patterns using a pattern matching based algorithm. They focus on those patterns for which the new variable occurs positively in binary clauses only, while the occurrences of the complement are unrestricted. Following our mining based approach for compressing Boolean formulas, in [11], we proposed a preliminary version of our extension to constraint satisfaction problems. More recently, Gharbi et al. [9] proposed a filtering approach for table constraints: it combines a new compression technique using the concept of sliced table constraints and an optimized adaptation of the tabular reduction (as in STR2). Except the fact that the authors followed the idea of using data mining techniques for patterns detection, the proposed approach introduced the concept of sliced table contraints, leading to a new representation of the table constraints. Our approach aims to preserve the inner-structure by deriving a compressed CSP using the same representation of the constraint table.

Finally with the advent of big data, we observe a renewal of interest on the design efficient compression techniques.

For web graphs or social networks, several techniques have     been proposed to compress such huge data (e.g. [4], [6]).



VI. CONCLUSION AND FUTURE WORKS  In this paper, we propose a mining based compression approach, for reducing the size of constraints satisfaction problems when constraints are represented in extension. It can be seen as a preprocessing step that aims to discover hidden structural knowledge that are used to decrease the size of table constraints. Our approach uses frequent itemset mining techniques for discovering interesting substructures, and exploit them for a compact representation of table constraints. Our approach is able to compact a CSP by considering both its associated constraints graph and mi- crostructure. We also proposed an original way to mine and compress binary constraints. The experimental evaluation on several families of CSP instances show the interesting reduction rates. Our proposed rewriting rules apply to ta- bles constraints expressed by allowed tuples of values. As a future work, it would be interesting to generalize our compression based approach to constraints defined using either allowed or forbidden tuples. Finally, as our approach is based on a greedy algorithm, designing a better heuristic or exact algorithm in order to derive the most compact possible representation is a short term perspective.

