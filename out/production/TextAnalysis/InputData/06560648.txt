Low-Power Area-Efficient Large-Scale IP Lookup Engine Based on Binary-Weighted Clustered Networks

ABSTRACT We propose a novel architecture for low-power area-efficient large-scale IP lookup engines. The proposed architecture greatly increases memory efficiency by storing associations between IP addresses and their output rules instead of stor- ing these data themselves. The rules can be determined by simple hardware using a few associations read from SRAMs, eliminating a power-hungry search of input addresses in TCAMs. The proposed hardware that stores 100,000 144-bit entries is evaluated under TSMC 65nm CMOS technology.

The dynamic power dissipation and the area of the proposed hardware are 4.6% and 30.6% of a traditional TCAM, re- spectively while maintaining comparable throughput.

Categories and Subject Descriptors B.3.2 [Memory Structures]: Design Styles  General Terms Algorithm, Design, Performance  Keywords Associative memory, TCAM, neural network  1. INTRODUCTION As the use of applications that require high bandwidth  increases, Internet routers have become key hardware in the Internet backbone. Data transmission rates in the routers are large, such as in OC-768 (40Gb/s). For these require- ments, Internet routers require fast IP-lookup operations utilizing hundred thousands of entries or more. Each router forwards packets toward their final destinations based on longest prefix matching (LPM) that determines the closest location to the final destination among several candidates.

The length of a packet is up to 32 bits for IPv4 and 144 bits for IPv6. The packets contain binary strings and wildcards.

The hardware of the LPM has been designed based on the use of ternary content-addressable memory (TCAM) [1, 2,  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.

DAC ?13, May 29 - June 07 2013, Austin, TX, USA.

Copyright 2013 ACM 978-1-4503-2071-9/13/05 ...$15.00.

3, 4], trie-based schemes [5, 6], and hash-based schemes [7, 8]. TCAMs compare a search word with all entries stored in TCAM cells in parallel and realize high-speed lookup oper- ations. In contrast, the large area of the cell (16 transistors vs. 6 transistors in a static random access memory (SRAM) cell) and the brute-force searching cause large power dissipa- tion and inefficient hardware architecture for large forward- ing tables. Trie-based schemes store prefixes and locations based on a binary-tree structure that is created based on portions of stored IP addresses. The searching is performed by traversing the tree until an LPM is found. The hardware can be designed using SRAMs instead of TCAMs, which potentially lowers power dissipation. However, deep trees require multi-step lookups. Hash-based schemes use one or more hash tables to store prefixes. The benefit of hashing is scalability as table size is increased with length-independent searching speed. The hash-based schemes have a possibility of collisions that requires post-processing to decide on only one output and requires reading many hash tables for each length of stored strings.

In this paper, a low-power large-scale IP lookup engine based on clustered neural networks (CNNs) is proposed. The CNN is a neural network recently presented in [9] and stores data using only binary weighted connections between sev- eral clusters. Compared with a classical Hopfield Neural Network (HNN) [10], the CNN needs less complex functions while learning (storing) larger number of messages. The hardware implementation of the CNN has also been reported in [11]. For the IP lookup engine, the proposed architecture based on novel algorithms is designed to store three values (?0?, ?1?, ?don?t care?) by extending the original CNN that can store only binary data. Unlike TCAMs that store IP addresses themselves, the proposed hardware stores the as- sociations between IP addresses and output rules, increasing memory efficiency. The output rule can be determined by simple hardware using a few associations read from SRAMs, which greatly reduces the search power dissipation compared with that of a TCAM that requires a brute-force search. As both IP addresses and their rules can be stored as associa- tions in the proposed IP lookup engine, an additional SRAM that stores rules in the conventional TCAM-based IP lookup engine is not required.

The rest of this paper is organized as follows. Section 2 reviews an IP lookup scheme based on a traditional TCAM.

Section 3 describes the proposed IP lookup scheme based on CNNs. Section 4 presents the hardware implementation.

Section 5 evaluates the performance and compares to the related work. Section 6 concludes the paper.

10.25.130.4  Input controller search lines  P rio  rit y  en co  de r  Stored words  43.120.10.23  D ec  od er  r  match lines  43.120.10.X  A D  C  43.120.10.23  output controller  D  TCAM SRAM  Figure 1: IP lookup using TCAM and SRAM. IP addresses and output rules themselves are stored.

2. REVIEW OF IP LOOKUP SCHEME Fig. 1 shows an IP lookup scheme using a TCAM and a  SRAM [12]. TCAMs contain large number of entries from hundreds to several hundred thousands. Each entry contains binary address information and a wildcard (X) in TCAMs, while only binary information is stored in binary CAMs.

The size of the entry is several dozen to hundreds (e.g.

128, 144 bits for IPv6 [13]). An input address is broad- cast onto all entries through search lines and one or more entries are matched. The priority encoder finds the longest prefix match among these matched entries and determines a matched location that is an address of a SRAM contains rules. Since the matched location in the TCAM corresponds to an address of the SRAM, the corresponding rule is read.

For example in Fig. 1, an input address is 42.120.10.23 and matches two entries: 42.120.10.23 and 42.120.10.X. Al- though two matched locations are activated, the matched location corresponding to 42.120.10.23 is selected. Finally, a ?rule D? is read from the SRAM. TCAMs perform high- speed matching based on one clock cycle in small number of entries: however there are some drawbacks when the num- ber of entries is large, such as network routing. Since the search lines are connected to all entries, large search-line buffers are required, causing large power dissipation. The power dissipation of the search lines is the main portion of the all power dissipation. In terms of area, the number of transistors in a TCAM cell is 16, while it is 6 in a SRAM cell, causing area inefficient hardware implementation.

3. IP LOOKUP BASED ON CLUSTERED NEURAL NETWORK  3.1 IP lookup scheme without wildcards The proposed IP lookup engine is designed by extending  the CNN [9]. Fig. 2 shows an IP lookup scheme without wildcards. The IP lookup scheme contains both functions of a TCAM and a SRAM by storing associations between IP addresses and their rules. There are c input clusters and c? output clusters. Each input cluster consists of l neurons and each output cluster consists of l? neurons. In the example shown in Fig. 2, c=4 and l=16 are set in the input cluster, while c?=1 and l?=16 are set in the output cluster. The input address has 16 bits and the output rule has 4 bits.

3.1.1 Learning process In the learning process, IP addresses and their correspond-  ing rules are stored. Suppose that a k-th learning addressmk is composed of c-sub messages mk1...mkc and a k-th learn-  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  0 1 2 4 5 6 7 8 9 10 11  12 13 14 15  0 2 3 4 5 6 7 8 9 10 11  12 13 14 15  0 1 2 3 5 6 7  8 9 10 11 12 13 14 15  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  input cluster (1st)  input cluster (2nd)  input cluster (4th)  input cluster (3rd)      IP address  1.9.10.12 5.3.0.4  Rule  14.11.12.6 8  output cluster  input address  input address  input address  input address  w(5,1)(1,1) w(4,4)(1,1)  v(n4,4 )v(n5,1 )  Figure 2: IP lookup without wildcards based on CNN. The IP lookup scheme contains both func- tions of TCAM and SRAM.

ing rule m?k is composed of c ?-sub messages m?k1...m  ? kc? . The  length of the address is c ? log2l bits and that of the rule is c??log2l? bits. The address is partitioned into c-sub messages mkj(1 ? j ? c), whose size is ? = log2l bits. Each sub mes- sage is converted into a l-bit one-hot signal, which activates the corresponding neuron in the input cluster. The rule is also partitioned into c?-sub messages mk?j?(1 ? j? ? c?), whose size is ?? = log2l? bits. Each sub message is converted into a l?-bit one-hot signal, which activates the correspond- ing neuron in the output cluster. During the learning pro- cess of M messages m1 ... mM that include input addresses and rules, the corresponding patterns (activated neurons) C(m) are learned. Depending on these patterns, connec- tions w(i1,j1)(i2,j2) between i1-th neuron of j1-th input clus- ter and i2-th neuron of j2-th output cluster are stored in the following:  w(i1,j1)(i2,j2) =  ???? ???  1, if  ?? ?  ?m ? {m1...mM} and C(m)j1 = i1 and C(m)j2 = i2  0, otherwise  (1)  This process is called ?Global learning?. Suppose that M messages are uniformly distributed. An expected density d defined as memory utilization for ?Global learning? is given by the following equation:  d = 1? ( 1? 1  ll?  )M . (2)  For example, d=0.3 means that stored bits of ?1? is 30% of the whole bits in the memory.

3.1.2 Retrieving process In the retrieving process, an output neuron (rule) is re-  trieved using a c ? ?-bit input messages. The input message min is partitioned into c-sub messages minj(1 ? j ? c), which are converted into l-bit one-hot signals. In each in- put cluster, one neuron that corresponds to the l-bit one-hot signal is activated. The value of each neuron v(ni1,j1)(i1 < l, j1 ? c) in the input cluster is given by:  v(ni1,j1) =  { 1, if minj1 = i1 0, otherwise  (3)  This process is called ?Local decoding?. Then, values of neurons v(n?i2,j2) in the output cluster, where n  ? i2,j2(i2 <  l?, j2 ? c?) is the j2-th neuron of the i2-th input cluster, are        input cluster (1st)  input cluster (2nd)  input cluster (4th)  input cluster (3rd)  output cluster        Dummy neurons  w? (1,1)(9,2)  w? (0,3)(4,4)     w(9,2)(8,1) w(10,3)(14,1)  Local learning  Glocal learning  Figure 3: Learning process in IP lookup scheme with wildcards based on CNN.

Table 1: Learned messages in Figs. 3 and 4.

IP address IP address w/ dummy Rule 5.3.0.4 5.3.0.4 1  1.9.10.12 1.9.10.12 14 1.9.10.X 1.9.10.8 8 7.8.2.X 7.8.2.15 4  updated using the following equation:  v(n?i2,j2) = l?1? i1=0  c? j1=1  w(i1,j1)(i2,j2)v(ni1,j1). (4)  In each output neuron, the maximum value vMaxj2 is found and then output neurons are activated using the following equations:  vMaxj2 = max v(n ? i2,j2) (5)  v(n?i2,j2) = {  1, if v(n?i2,j2) = vmaxj2 0, otherwise  (6)  These (4)-(6) processes are called ?Global decoding?. In the example shown in Fig. 2, the input message is 5.3.0.4.

After the process (2), the value of the 1-th neuron in the output cluster is 4 and becomes the maximum value. Hence, the rule ?1? is retrieved.

3.2 IP lookup scheme with wildcards  3.2.1 Learning As an IP routing table contains wildcards (X), the IP  lookup scheme is extended to store wildcards in the table.

Fig. 3 shows a learning process in the proposed IP lookup scheme with wildcards. There are two different connections: w?(i,j)(i?,j?) and w(i1,j1)(i2,j2). w  ? (i,j)(i? ,j?) represents connec-  tions between activated neurons in the input clusters, while w(i1,j1)(i2,j2) represents connections between activated neu- rons in the input and output clusters. In the learning process for w?(i,j)(i?,j?)(i, i  ? < l : j, j? ? c) shown in Fig.3, an address learned is partitioned into c-sub messages whose size is ? = log2l bits. If the sub message is binary information, it is converted into a l-bit one-hot signal, which activates the cor- responding neuron in the input cluster. If it is a wildcard, no neuron is activated in the input cluster. The output neurons are activated to store connections using the same algorithm in the IP lookup scheme without wildcards. Depending on corresponding patterns (activated neurons) C(m), connec- tions w?(i,j)(i?,j?) between i-th neuron of j-th input cluster and i?-th neuron of j?-th input cluster are stored in the fol-     3 0      9 10  input  address  input address  input address  input address  No connection -> Deactivation     Dummy neuron is activated  Figure 4: Retrieving process in IP lookup scheme with wildcards based on CNN.

lowing:  w?(i,j)(i?,j?) =  ?????? ?????  1, if  ???? ???  ?m ? {m1...mM} and j + 1 = j?  and C(m)j = i and C(m)j? = i  ?  0, otherwise  (7)  This process is called ?Local learning?. In the example shown in Fig. 3, there are two connections for a stored address is 7.8.2.X because C(m)1 = 7, C(m)2 = 8, C(m)3 = 2 C(m)4 = X.

In the learning process for connections between the in- put and the output clusters w(i1,j1)(i2,j2)(i1 < l : i2 < l  ? : j1 ? c : j2 ? c?) shown in Fig. 3, dummy neurons are activated when learned messages include wildcards. Sup- pose that the last half of learned messages has wildcards randomly. If a sub message is a wildcard, the wildcard is replaced by dummy information that is binary one. The dummy information is determined by a function using the first half of learned messages. In the paper, the function is realized by XORing two sub messages of the first half of learned messages. The k-th learning sub-messages mkj that contain wildcards (X) are replaced as mdkj by the following equation:  mdkj =  ?? ?  mk(j? c ) ?mk(j? c  +1)( mod c  ), if  { mkj = X and j > c   mkj , otherwise (8)  In the example shown in Fig. 3, a learned message 1.9.10.X has a wildcard. The wildcard is replaced by XORing 9?1 and becomes 8. After making dummy neurons, process (1) is performed by using mdk instead of mk in order to make connections between the input and output clusters and then these connections are stored.

3.2.2 Retrieving In the retrieving process, the proposed IP lookup scheme  checks whether input messages are stored or not using stored connections. If an activated neuron corresponding to an input sub-message minj1 in the j1-th input cluster doesn?t have a connection to an activated neuron in the precedent input cluster, minj1 can be decided as?non stored messages?.

Hence, the activated neuron is de-activated and alternatively a dummy neuron is activated as the following two rules.

First, the number of connections conj1 in the j1-th input cluster is given by:  conj1 =  j1? j=1  w(min(j?1),j?1 )(minj,j). (9)    10-11  10-10  10-9  10-8  10-7  10-6  10-5  10-4  10-3  10-2  80000  100000  120000  140000  160000  180000  200000  S to  re d  ru le  -e rr  or r  at e  (P R  E R  )  The number of learned messages (M)  l=512, c=16, l?=1024, c?=1  w/o wildcards w/ wildcards and w/o dummy  w/ wildcards and w/ dummy  Figure 5: PRER vs. M . The IP lookup scheme can store 100,000 144-bit IP addresses at negligible low probability of PRER (<10  ?8).

If conj1 is less than (j1? 1), the input sub-messages are not stored. Then, the input sub-messages minj1 are replaced as msinj1 by dummy information as follows:  msinj1 =  ???? ???  min(j1? c2 )  ? min(j1? c2+1)( mod c2 ), if {  conj1 < j1 ? 1 and j1 >  c  minj1 , otherwise (10)  These (9),(10) processes are called ?Input selection?.

Then, v(ni1,j1) is obtained based on equation (3) by using msinj1 instead of minj . In the example shown in Fig. 4, the input address is 1.9.10.6. Since the activated neuron ?6? in the 4th input cluster doesn?t have a connection to the activated neuron ?10? in the 3rd input cluster, the activated neuron is de-activated. Instead, a dummy neuron ?4? in the 4th input cluster is activated.

The subsequent process is ?Global decoding? based on (4)-(6). As the first-half input clusters have only binary information, the summation of w(i1,j1)(i2,j2)v(ni1,j1) from the first-half ones must be the half of the number of input clusters, if output neurons are a candidated rule. In the IP lookup scheme with wildcards, (4) is modified by the following three equations:  enai2,j2 = l?1? i1=0  c/2? j1=1  w(i1,j1)(i2,j2)v(ni1,j1). (11)  sumi2,j2 =  l?1? i1=0  c? j1=  c +1  w(i1,j1)(i2,j2)v(ni1,j1). (12)  v(n?i2j2) = {  sumi2,j2 , if enai2,j2 = c  0, otherwise (13)  After these processes, (5) and (6) are performed.

Fig. 5 shows stored rurle-error rates (PRER) vs. the  number of learned messages (M) in the IP lookup schemes (l = 512, c = 16, l? = 1024, c? = 1). In the proposed schemes, there might be ambiguity that more than two neurons (rules) are retirieved in the stored rules because they might have the maximum value after ?Global decoding?. PRER is defined as the probability of the ambiguity. In the figure, the proposed IP lookup schemes store 144-bit (c ? log2l) ad- dresses (messages) for IPv6 [13]. PRER are evaluated by simulations. Unlike IPv4, since packet traces for the long  Memory block 1  ((c-1)l2 bits)  Input selection  Decoding module  IP address  Input IP address  mk  min  w? (i,j)(i? ,j? ) w(i1,j1)(i2,j2)  Memory block 2  (cc? ll? bits)  v(ni1,j1 ) v(ni2,j2 ) msin  Local learning Global learning  Dummy generator  mk? Rule  mdk  Mismatch 2AmbiguityMismatch 1  Figure 6: The proposed IP lookup engine.

prefix table are not available to the public and the prefix ta- ble is still small [14], we choose addresses randomly for the evaluation. The stored addresses are uniformly distributed.

Random-length wildcards appear in the last half of addresses (72 bits). If the range of addresses that have wildcards is changed, the prefix length can be changed.

PRER is strongly dependent of M . In the proposed IP lookup scheme with wildcards, if a dummy neuron is the same neuron that already stored, the both outputs (rules) might be retrieved, which slightly increases PRER compared with that of the IP lookup scheme without wildcards.

Adding dummy neurons are very effective to lower PRER that is about five orders of magnitude reduction of that with- out dummy at M=78,643. The proposed IP lookup scheme with wildcards can store 100,000 144-bit IP addresses at negligible low probability (PRER < 10  ?8).

PRER is also simulated when the learned messages are  correlated. The word length in the IP lookup scheme with wildcards is 64 bits (l = 256, c = 8, l? = 1024, c? = 1). The first 8 bits of the learned messages are selected from 64 fixed patterns of 256 (28). The rest of the learned messages are uniformly distributed. Random-length wildcards appear in the last half of addresses (32 bits). At M=10,000, PRER using the correlated patterns is 5.30?10?7 while PRER using uniformly distributed patterns is 1.69 ? 10?7.

Unlearned input messages are detected as ?mismatch? us- ing two steps. In the first step, the number of local con- nections conc/2 in (9) is checked in ?Input selection?. If it is not the same as (c/2) ? 1, an unlearned input message can be detected as ?mismatch? because all local connections related to the input message are not stored (?Mismatch 1?).

If an unlearned input message is detected as ?match? by other stored connections, the number of global connections enai2,j2 is checked at?Global decoding?. If all enai2,j2 are not the same as (c/2), an unlearned input message can be detected as ?mismatch? because all global connections be- tween the input message and all output rules are not stored (?Mismatch 2?).

4. HARDWARE IMPLEMENTATION Fig. 6 shows an overall structure of the proposed IP  lookup engine with wildcards. The learning process is ?Lo- cal learning? in (7) and ?Global learning? in (8) and (1).

The retrieving process is that: 1) each input-sub message is replaced by a dummy message if it is not stored (?Input se- lection?) in (9) and (10), 2) connections between c activated neurons in the input clusters are read from a memory block 1 (?Local decoding?) in (3), and 3) an output neuron is retrieved based on connections between neurons of the input and the output clusters in a memory block 2 in (11)-(13), (5), and (6) (?Global decoding?).

0mk(c/2+1)  mk1mk2  1, if mk(c/2+1)=X 0, otherwise  mdk(c/2+1) Sub-dummy generator  w? (i,j)(i? ,j? )  l:1 MUXmin2 l:1 MUXminc mux1 muxc-1  mux1 min  min1...min(c/2)c ? / 2c ?  min(c/2+1)  min1min2  misc ?  One-hot decoder  v(ni1,j1 ) Sub-dummy generator  mux(c/2)  (a) (b)  mux1mux(c/2-1) Mismatch 1  Figure 7: Circuit diagrams: (a) dummy generator, and (b) Input selection block.

There are ((c-1)l2)-bit SRAMs for ?Local learning? and c?ll?-bit SRAMs for ?Global learning?. In ?Local learn- ing?, a sub-input k-th learning address mkj and the subse- quent one mkj? are converted into one-hot l-bit signals at row and column decoders, respectively. Then, a w(i,,j)(i?,,j?) is stored in the memory block 1 if both are not a wildcard.

In ?Global learning?, the last half of an input address is replaced by dummy information using a dummy generator that includes c  sub-dummy generators if each sub-input ad-  dress is a wildcard shown in Fig. 7 (a). The sub-dummy generator contains l 2-input XOR gates and multiplexors.

Using the sub-input address with dummy information mdkj1 and the corresponding rule m?kj2 , a w(i1,j1)(i2,j2) is stored in the memory block 2.

In the retrieving process, an input address min is parti- tioned into c-sub messages and ((c-1)l) connections w(i,,j)(i?,,j?) are read from the memory block 1.

w(minj ,j)(min(j+1),j+1) in (9) are selected in a multiplexor  of an input-selection module shown in Fig. 7 (b). Then, the last half of the input address is replaced by dummy in- formation if these corresponding connections are not found.

The output msin that contains the first half of the input address and the generated last half of input address is sent to the memory block 2. The one-hot decoder transforms msin to v(ni1,j1). In the memory block 2, (c?l?) connections (w(i1,j1)(i2,j2)) are read by msin and are sent to a decoding module shown in Fig. 8. The decoding module contains (c?l?) global decoder, c? max-function blocks and c? ambi- guity checkers, where c? is set to 1 in Fig. 8. In the global decoder, c  2-input AND gates and c  -input AND gate gener-  ate an enable signal to a c -input adder, where these circuits  are corresponding to (11)-(13). There are a l?-input max- function block that decides an activated neuron v(ni2,j2)=1.

The ambiguity checker checks that two neurons are activated simultaneously.

5. EVALUATION We designed the proposed IP lookup engine with wild-  cards based on TSMC 65nm CMOS technology. The param- eters of the hardware are determined by simulations shown in Fig. 5 as l=512, c=16, l?=1024 and c?=1. The IP lookup engine stores 100,000 144 entries with PRER is less than 10?8. 15 SRAMs (256kb) are designed for the local learning block and 32 SRAMs (256kb) are designed for the global learning block. For the purpose of comparison, a reference TCAM is also designed. The TCAM also stores the same  v(n0,c/2+1) w(0,c/2+1)(0,1) v(n0,c) w(0,c)(0,1)  c/2-input adder  v(n? i2,1 )  Global decoder  Global decoder v(n? 1,1 )  v(n? l? -1,1 )  l?- in  pu t m  ax fu  nc tio  n bl  oc kv(n? 0,1 )  Ambiguity  l?  v(n0,1) w(0,1)(0,1) v(n0,c/2) w(0,c/2)(0,1)  Global decoder  max(v(n? i2,j2 ) )    Mismatch 2  Figure 8: Decoding module  Table 2: Performance comparisons.

Reference TCAM Proposed  Throughput [Gbps] 52.0 48.3 Dynamic power [W] 3.03 0.14 Static power [W] 0.24 0.09 Energy metric 0.584 0.028 [fJ/bit/search] Memory [Mb] 14.4 11.75  (TCAM) (SRAM) Equivalent size 38.4 11.75 (SRAM) [Mb]  Number of transistors 256M 77M  entries as the proposed IP lookup engine. A TCAM cell is designed using a NAND-type cell that is composed of 16 transistors [12]. Each entry has 144 TCAM cells and is designed based on a hierarchy design style for high-speed matching operations. The TCAM is divided into 20 sub- TCAMs that have 5,000 entries for a power reduction of search lines. A priority encoder attached to the TCAM has 100,000 inputs and 17-bit outputs.

Table 2 shows performance comparisons between the refer- ence TCAM and the proposed IP lookup engine by HSPICE simulation. For learning (storing) process, the proposed IP lookup engine takes 100,000 clock cycles to store 100,000 messages (entries) as well as the conventional TCAM. For retrieving (matching) process, the proposed engine takes two clock cycles. The worst-case delay is 1.44 ns in the latter part, where the delay of the max function block is 60 % of the whole delay. Throughput is defined as length/worst- case delay. The dynamic power dissipation of the proposed circuit is just 4.6 % of that of the reference TCAM, be- cause the proposed circuit doesn?t require search lines whose power dissipation is a large portion of that of the TCAM. In terms of area, the TCAM cell contains 16 transistors while the SRAM cell contains 6 transistors. Hence, the required memory size of the proposed circuit is 11.75 Mb that is 30.6 % of the equivalent area of the reference TCAM. The mem- ory area is 96 % of the total area in the proposed circuit. A lookup speed of the proposed IP lookup engine is 201 Gb/s that is over 40Gb/s (OC-768), while a packet is considered to be 75 bytes.

The performance of the proposed IP lookup engine de- pends on the number of clusters and neurons. Under the same learned messages, large number of neurons and small number of clusters achieve low PRER while requiring large memory sizes and computation blocks. In contrast, small number of neurons and large number of clusters reduce mem- ory sizes and computation blocks while having high PRER.

Hence, depending on applications that can afford PRER, the    Table 3: Performance comparisons with related works.

TCAM [1] DTCAM [2] IPCAM [3] eDRAM [4] Trie [6] Hash [7] Proposed  Length [bits] 512 144 32 (128)a 23 63 32 144 Number of entries 21,504 32,768 65,536 16,000,000 318,043 524,288 100,000  (1.38M)b  Throughput [Gbps] 76.8 20.6 32 4.6 12.6 6.4 48.3 Power dissipation [W] 12.26 2.0 7.33 0.6 - 5.5 0.14  Energy metric [fJ/bit/search] 5.53 2.96 0.159 0.007 - 1.64 0.028 Memory [Mb] 10.5 4.5 2 432 31.19 60 11.75  Equivalent size (SRAM) [Mb] 28 - 7.33 - - - - Equivalent size (SRAM) for 144-bit length [Mb] 7.88 4.5 32.99 1039 - - 11.75  Technology [nm] 130 130 65 40 (FPGA) 130 65  aThis method can be extended to 128 bits for IPv6.

b1 IPCAM word is approximately equivalent to 22 TCAM words.

performance can be determined by changing the number of neurons and clusters.

Table 3 shows performance comparisons with related works.

The design in [1] is a straightforward implementation using TCAMs. In [2], eDRAMs are used to reduce the size of TCAM cells for low power dissipation; however they tend to be complex process. In [3], several entries are shared using special-purpose CAM cells to reduce the number of entries required. In [4], the prefix match is realized by reading can- didates from eDRAMs and hence the energy metric is very small. However, as the memory size is O(2n) where n is the word length, an unacceptably large memory of 1039 Mb is re- quired for long words (e.g. 144 bits). A trie-based method in [6] (PC trie-4) realizes a memory-efficient IP lookup engine using a prefix-compressed trie and also uses a hash function to reduce memory accesses to off-chip memory in order to achieve high throughput. A hash-based method in [7] re- duces power dissipation using a collision-free hash function compared with the TCAM in [1].

6. CONCLUSIONS We have proposed the low-power large-scale lookup en-  gine based on binary-weighted clustered neural networks (CNN). By extending the CNN, the proposed IP lookup en- gine efficiently stores IP addresses and their outputs (rules) as connections in SRAMs, reducing the memory require- ment and eliminating power-hungry TCAMs. The retrieving (matching) process is performed in parallel, realizing high- throughput IP lookup engines. Under TSMC 65nm CMOS technology by HSPICE simulation, the proposed IP lookup engine reduces the energy dissipation and the area to 4.6 % and 30.6 % of those of a traditional TCAM, respectively.

Compared with other related works, the proposed IP lookup engine realizes one of the lowest energy metric while deal- ing with large number of long entries. In future work, we will apply the extended CNN to many applications that cur- rently use TCAMs. In addition, we will consider eliminating errors related to PRER.

7. ACKNOWLEDGMENTS The authors would like to thank Vincent Gripon and Hooman  Jarollahi for their helpful discussions.

8. REFERENCES [1] B. Gamache et. al., ?A fast ternary CAM design for IP  networking applications,? in Proc. 12th IEEE ICCCN,  Oct. 2003, pp. 434 ? 439.

[2] H. Noda et. al., ?A cost-efficient high-performance dynamic TCAM with pipelined hierarchical searching and shift redundancy architecture,? IEEE JSSC, vol. 40, no. 1, pp. 245 ? 253, Jan. 2005.

[3] S. Maurya et. al., ?A dynamic longest prefix matching content addressable memory for IP routing,? IEEE TVLSI, vol. 19, no. 6, pp. 963 ?972, June 2011.

[4] Y. Kuroda et. al., ?A 200Msps, 0.6W eDRAM-based search engine applying full-route capacity dedicated FIB application,? in Proc. CICC, 1/4, Sept. 2012.

[5] W. Eatherton et al., ?Tree bitmap: hardware/software IP lookups with incremental updates,? SIGCOMM Comput. Commun. Rev., vol. 34, no. 2, pp. 97?122, Apr. 2004. [Online]. Available: http://doi.acm.org/10.1145/997150.997160  [6] M. Bando et. al., ?Flashtrie: Beyond 100-Gb/s IP route lookup using hash-based prefix-compressed trie,? IEEE/ACM Transactions on Networking, vol. 20, no. 4, pp. 1262 ?1275, Aug. 2012.

[7] J. Hasan et. al., ?Chisel: A storage-efficient, collision-free hash-based network processing architecture,? in Proc. 33rd ISCA, 203/215, June 2006.

[8] S. Dharmapurikar et al., ?Longest prefix matching using bloom filters,? IEEE/ACM Trans. Networking, vol. 14, no. 2, pp. 397 ? 409, April 2006.

[9] V. Gripon and C. Berrou, ?Sparse neural networks with large learning diversity,? IEEE Trans. Neural Networks, vol. 22, no. 7, pp. 1087 ?1096, July 2011.

[10] J. J. Hopfield, ?Neural networks and physical systems with emergent collective computational abilities,? in Proc. the National Academy of Sciences, vol. 79, no. 8, pp.2554-2558, Apr 1982.

[11] H. Jarollahi et. al., ?Architecture and implementation of an associative memory using sparse clustered networks,? in Proc.ISCAS, 2901/2904, May 2012.

[12] K. Pagiamtzis and A. Sheikholeslami, ?Content-addressable memory (CAM) circuits and architectures: a tutorial and survey,? IEEE JSSC, vol. 41, no. 3, pp. 712?727, Mar. 2006.

[13] H. Po-Tsang and H. Wei, ?A 65 nm 0.165 fJ/bit/search 256 ? 144 TCAM macro design for IPv6 lookup tables,? IEEE JSSC, vol. 46, no. 2, pp. 507 ?519, Feb. 2011.

[14] ?Border gateway protocol (BGP),? http://bgp.potaroo.net.

