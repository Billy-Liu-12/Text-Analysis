PREDICTION CONFIDENCE FOR ASSOCIATIVE CLASSIFICATION

Abstract: Associative classification which uses association rules for  classification has achieved high accuracy in comparison with other classification approaches. However, the confidence measure which is used to select association rules for classification may not conform to the prediction accuracy of the rules. In this paper, we propose a measure for association rules called prediction confidence to measure the accuracy of the prediction of association rules. An approach for estimating the prediction confidence of a rule is also given. The use of prediction confidence instead of confidence measure helps gather better association rules for associative classification. As a result, a more accurate associative classifier can be constructed using the prediction confidence measure.

Keywords: associative classification; association rules  1. Introduction  Association rules [1] present implication relations among data. Associative classification (AC) focuses on using association rules which contain the relation between data items and class labels for classification [2]. This kind of association rules can be used to predict the data labels of unknown data samples. The degree of implication of an association rule is measured by the confidence measure. A high confidence value means a reliable implication. In other words, a rule with a high confidence value implies an accurate prediction.

However, the confidence value of a rule does not imply directly to its prediction accuracy. The reason is that association rules are mined and measured based on the training set while the prediction is carried out on data samples of a test set. And there is always a difference between the two datasets. The result is that, to classify a data sample, the association rules with the highest confidence values may not mean the best accuracy.

In this paper, we propose a measure for association rules called prediction confidence to measure the accuracy of the prediction of association rules. An approach for estimating prediction confidence of a rule is also presented.

The use of prediction confidence instead of the confidence  measure helps select better association rules for AC. As a result, a more accurate associative classifier can be constructed using the prediction confidence measure.

2.  Associative Classification  An association rule R is an implication X ? Y occurring in a transactional database, where X and Y are sets of items or itemsets [1]. The support of the rule supp(R) is the percentage of transactions containing X and Y with respect to the number of all transactions. The confidence of the rule conf(R) is the percentage of transactions that contains Y among the transactions containing X. Association rule mining aims to generate all rules of which support and confidence values are greater than or equal to the predefined support and confidence thresholds respectively.

supp(R) =  P[X ? Y]     (2.1)  conf(R) = P[X]  Y]P[X ?      (2.2)  Associative classification [2,3,4] uses association rules for classification. It generally consists of three major processes: rule generation, rule selection and classification.

Firstly, in the rule generation process, the set of association rules is mined from a training dataset based on the support and confident thresholds. Next, the rule selection process evaluates the association rules obtained from the rule generation process and selects the subset of rules, which gives the best classification accuracy, to form a classifier.

Finally, in the classification process, new data samples in a test dataset are classified using the classifier.

The set of association rules is generated in the form of iset ? cl where iset is an itemset and cl is a class. These association rules are referred to as class association rules [2]. Let RS be the set of association rules selected from the rule selection process. In the classification process, to classify a test sample d, the rule R: iset ? cl in RS with the highest confidence value that matches with d (i.e. d contains iset) is selected. The test sample d is then assigned to class cl of R [2]. The AC approach is based mainly on the implication relationships of the association rules. The rule iset ? cl means that if a given test sample contains iset,      then it will probably belong to class cl. The confidence measure, say 80%, guarantees the reliability of the classification decision. The test sample is classified into class cl as most of the transactions (80%) in the training dataset that contain iset fall into class cl.

The AC approach was first introduced in [2] with the Classification Based on Associations (CBA) algorithm. In [3], Liu et al. suggested the use of multiple class support thresholds that produce a more balanced numbers of association rules over different classes during the rule generation process. In [4], Li et al. used multiple rules instead of a single rule in the classification process. In this approach, all the rules that matched with a test sample are collected and grouped according to their class labels in order to classify the test sample. The test sample is then assigned to the class from the best group based on a statistical measure called max?2. This multiple rule classification approach is similar to single rule classification in the sense that higher confidence rules are given higher weights in classifying unknown test samples.

3.  Prediction Confidence  Let us consider the classification process in which association rules are used to classify test data samples. Let R: iset ? cl be a rule selected for the classification process.

For a given test data sample d, if d matches with R (i.e. d contains itemset iset) then it will be predicted as belonging to class cl using R. Suppose there are x test samples containing iset and y test samples containing both iset and cl in the test set. The classification process, therefore, will predict correctly y among x test samples (i.e. y/x) using rule R. Thus, the accuracy of the prediction is the confidence value of R in the test set. We call it as prediction confidence.

The prediction confidence of a rule shows the accuracy when the rule is used to predict unknown test data samples.

Definition 3.1: Prediction confidence of an association rule is the average accuracy of the prediction when the association rule is used to predict the class labels of unknown data samples.

Note that, the confidence value of a rule conventionally refers to the value of confidence counted in the training set. Given the sets of training data TR and test data TS, we denote the confidence of R as confTR(R) and the prediction confidence of R as confTS(R) (see Figure 1).

Obviously, the prediction confidence should play a major role in selecting and ranking association rules for AC. A rule should be considered more important with higher weight than rules that have lower prediction confidence values during the classification process. Currently, the confidence measure is considered as the goodness measure  of association rules in AC. Associative classification uses confidence to generate and select rules, and predict class labels for data samples.

Class association rules  Training set (TR)  Ru le g  ene rati  on  Test set (TS)  Classifcation  Confidence confTR(R)  Prediction confidence confTS(R)  Rule selection     Figure 1: Confidence and prediction confidence.

In order to specify the prediction confidence, we will examine the difference between confidence and prediction confidence. We refer the difference of confidence and prediction confidence of a rule R as confidence decrease (denoted as decr(R)) of R. The confidence decrease presents the decrease in the confidence of the same rule counted in the training set, from which the rules are mined, and the test set, to which the rules are applied for data prediction. Note that if a rule has the prediction confidence higher than the confidence, then the confidence decrease of the rule will be negative.

)()()( RconfRconfRdecr TSTR ?=    (3.1)  We consider the following scenario of AC for a dataset D which is divided into the training set TR and test set TS.

Let N and n be the numbers of data samples, S and s be the numbers of data samples containing iset, and C and c be the numbers of data samples containing both iset and cl respectively in D and TR. The other numbers of samples related to R that can be computed from N, S, C, n, s and c are shown in Table 1.

Table 1: Numbers of data samples related to rule R.

R: iset ? cl D TR TS Number of samples N n N-n - containing iset S s x = S-s - containing both iset and cl C c y = C-c - containing iset but not cl S-C s-c S-C-(s-c) - not containing iset  N-S n-s N-S-(n-s)   We call s and c as the support and confidence numbers  of rule R. They represent the absolute values of support and confidence of R on the training set. The confidence and prediction confidence values of R can be calculated as follows:  confTR(R) = c/s       (3.2)      confTS(R) = (C-c)/(S-s)     (3.3)  The conditions for support and confidence numbers (i.e. s and c), and the number of training data samples (i.e. n) are listed below:  cond1(s,c) = 1 ? s ? s ? S-1    (3.4) cond2(s,c) = 0?c ? c?C ? 0? s?c ? s?c ? S?C (3.5) cond(n) = 0?n ? n?N ? 0? n?s ? n?s ? N?S (3.6) cond(s,c) = cond1(s,c) ? cond2(s,c)   (3.7)  The condition (3.4) ensures that rule R may play some roles in the AC processes. The first expression (i.e. 1 ? s) means there is at least one data sample matching with R so that the rule may be generated during the rule generation process. The second expression (i.e. s ? S-1) shows that there is at least one test sample matching with R so that it may be used in classifying data samples. The other conditions are implied from the fact that all the numbers in Table 1 must be non-negative. The conditions for s and c are combined to cond(s,c) in (3.7). Here, the tight constraints for s, c and n are needed because we will use them to examine all possible combinations of s, c and n subsequently.

Problem 3.1: Given an association rule R and a dataset D, determine the average value of confidence decrease of R when D is arbitrarily divided into training and test sets.

To obtain the confidence decrease value of rule R, we will examine all possible divisions of N data samples of dataset D into training set TR and test set TS. Each division of D corresponds to a combination of n data samples for TR and the remaining N-n data samples for TS (0 ? n ? N). For each combination, the confidence and prediction confidence are determined. The average value of confidence and prediction confidence are respectively the average values of the confidence and prediction confidence values of all possible combinations.

Given R and D, the numbers of data samples N, S and C can be obtained. For each combination of s, c and n, the number of different combinations of the training set TR can be calculated in formula (3.8). Here, n samples in TR include c out of C samples containing both iset and cl, s-c out of S-C samples containing iset but cl, and n-s out of N-S samples that do not contain iset. Then, for each pair of s and c, the number of different combinations of the training set TR can be computed in formula (3.9) using comb(s,c,n) with all possible values of n satisfying condition (3.6)  CCC sn SNcs CScCncscomb ???? ??=),,(    (3.8) ?=  )(  ),,(),( ncond  ncscombcscomb    (3.9)  Corresponding to each combination of n data samples in the training set, there will be one combination of N-n data samples in the test set (i.e. the remaining N-n data samples). For each combination, the confidence and prediction confidence values of R can be calculated from s and c using formulas (3.2) and (3.3). Thus, the average values of confidence, prediction confidence and confidence decrease of rule R can be calculated as:  ? ? ?  =  ),(  ),(  ),(  )(),(  ),(  cscond  cscond TR  cscomb  s ccscomb  CSconf    (3.10)  ? ? ?  ??  =  ),(  ),(  ),(  )(),(  ),(  cscond  cscond TS cscomb  sS cCcscomb  CSconf   (3.11)   ?  ? ? ???  =  ),(  ),(  ),(  )(),(  ),(  cscond  cscond  cscomb  sS cC  s ccscomb  CSdecr   (3.12)  Table 2 shows the values of the confidence decrease of rule R calculated using formula (3.12) with different values of S, C and N. The support and confidence numbers S and C are set such that conf(R) = C/S = 80%. As shown in Table 2, the confidence decrease is very close to zero. It means that, in the classification process, if we arbitrarily choose a rule in RS (the set of rules of the classifier mentioned in section 2), the confidence and prediction values of the same rule will probably be the same.

Table 2: Average values of confidence decrease.

S=5, C=4 S=10, C=8 S=100, C=80 N = 100 -1.11E-16 3.33E-16 1.11E-16 N = 1000 -1.11E-16 -1.11E-16 -4.88E-15 N = 10000 2.22E-16 4.44E-16 -1.73E-14   In AC, however, rules contributing to the classification  process depend on their confidence values. We refer the contribution or effect of a rule during the classification process as classification weight of the rule. In single rule classification, classification weight represents the probability with which if the rule matches with a data sample, then it is chosen for classifying the data sample. In multiple rule classification, the classification weight represents the weight of each rule when multiple rules matching with a data sample are used together to decide the class label for the data sample. For both cases of AC using      single and multiple rules, the classification weight of a rule is proportional to its confidence value. In other words, the classification weight of a rule is a monotonic function f(confTR(R)) of its confidence value (i.e. f(x) ? f(y) for x ? y).

We call f as classification weight function. The classification weight function should also return a non-negative value that should not exceed 1 (i.e. with probability of 100% in single rule classification). Simple examples of classification weight function are f(x) = x and f(x) = x2.

Considering the classification weight, the average values of the confidence and prediction confidence of the rules used in classification are recalculated bellow:  ?  ?  ?  ??  =  ),(  ),(  )(),(  )()(),(  ),(  cscond  cscond TR  s cfcscomb  s c  s cfcscomb  CSconf   (3.13)  ?  ?  ?  ? ???  =  ),(  ),(  )(),(  )()(),(  ),(  cscond  cscond TS  s cfcscomb  sS cC  s cfcscomb  CSconf (3.14)  ?  ?  ?  ? ?  ???  =  ),(  ),(  )(),(  )()(),(  ),(  cscond  cscond  s cfcscomb  sS cC  s c  s cfcscomb  CSdecr  (3.15)  Note that confTR(S,C) and confTS(S,C) in (3.13) and (3.14) respectively are not purely the average confidence and prediction confidence values of the set of the selected rules. The classification frequencies of the rules are also included. From (3.15), the confidence decrease of rule R with different values of N, S and C with the classification weight functions f(x) = x and f(x) = x2 is given in Table 3.

Table 3: Average values of confidence decrease of classified rules.

Classification weight function  S=5, C=4  S=10, C=8  S=100, C=80  N = 100 12.15% 5.09% 0.41% N = 1000 12.15% 5.09% 0.41%f(x)=x N = 10000 12.15% 5.09% 0.41% N = 100 19.36% 9.44% 0.81% N = 1000 19.36% 9.44% 0.81%f(x)=x2 N = 10000 19.36% 9.44% 0.81%   We have the following observations from the  confidence decrease values which are shown in Table 3.

Firstly, association rules used during the classification process will probably have a positive confidence decrease.

It means that the prediction confidence value is smaller than the confidence value. Secondly, apart from confidence, prediction confidence also depends on the support numbers and classification weight function. It is clear that the confidence decrease is higher with a smaller value of support number. For example, for f(x) = x2, the confidence decrease is approximately 19.36% for C = 4, S = 5. It means that if a rule R with confidence value of 80% occurs only in 5 data samples of dataset D, then the average prediction confidence value is only 80% - 19.36% = 60.6%. With a considerable large support number, the confidence decrease is close to zero. Thirdly, we also observe that the confidence decrease value is the same for different numbers N of data samples. In other words, the confidence decrease depends on support number (i.e. S) rather than the support (i.e. S/N) of association rules. However, for the same dataset (i.e. N), support is directly proportional to the support number. That is, if two rules have the same confidence value, the one with larger support will have a larger support number, and consequently causes a smaller confidence decrease. In other words, it has a higher prediction confidence.

Once the confidence decrease is determined, the prediction confidence value in the test set can be obtained from the confidence value in the training set using (3.1).

Unfortunately, there are still problems to be solved: ? The classification weight function (i.e. f(x)) is not  specified.

? In practical AC, for a rule R we can only obtain the  support and confidence values (i.e. s and c) in the training set. The whole dataset of N data samples with the support and confidence numbers of rule R (i.e. S and C) are unknown.

? The calculation of the confidence decrease is quite expensive with huge numbers of possible combinations of data samples.

4.  Estimation of Prediction Confidence  4.1.  Estimation of Classification Weight  Function Assumptions:  ? Rules are independent in matching with data samples.

That is, given a data sample d and two rules R1 and R2, the two events: R1 matches with d and R2 matches with d are independence:  P[R1 match_with d ? R2 match_with d] = P[R1 match_with d] x P[R1 match_with d]   (4.1)  ? Rule distribution is even. For a data sample d, let P(x,y)      is the probability that there is at least a rule R matching with d, and R satisfies x ? conf(R) and conf(R) < y.

With even distribution of rules we may imply P(x1,y1) = P(x2,y2) for any x1, y1, x2, y2 such that x1 - y2 = x2 - y2.

Now, let M be the P(x,y) value of the range of 1% confidence difference such that M = P(0,1%). The value M is called the matching factor.

Let?s consider a data sample d and a rule R with confidence value x = confTR(R). From the assumption on even distribution of rules, the probability in which there is no rule with confidence value in a range of 1% confidence difference (e.g. [99%-100%]) matching with d is 1 - M.

From the assumption on rule independence, the probability with which there is no rule with confidence above x (i.e.

from 100x% to 100%) matching with d is .

This is also the probability in the case of single classification, with which if R matches with d, then R is selected for classifying d (because there is no rule with confidence greater than the confidence value of R (i.e. x) matching with d):  xM 100100)1( ??  )1(100)1()( xMxf ??=       (4.2)  4.2.  Estimation of Confidence Decrease  In the previous section, we have estimated the classification weight function so that we can estimate the confidence decrease decr(S,C) of rule R. Unfortunately, the estimation is based on the support and confidence numbers S and C counted in the dataset D. In practical AC, we can only obtain the support and confidence numbers s and c counted on the training set TR. We denote decrTR(s,c) as the confidence decrease of R with the given support and confidence numbers s and c counted from the training set DTR. The problem is that we need to estimate decrTR(s,c) from decr(S,C).

To estimate the confidence decrease we consider leave-one-out cross validation in which the test set consists of only one data sample. Firstly, we calculate decr(S,C). For leave-one-out test, the training set contains n = N - 1 data samples. Thus, from (3.6), the expression n ? s ? N ? S is equivalent to N ? 1 ? s ? N ? S or S ? 1 ? s. Now, let?s consider the condition (3.4), we have s ? S ? 1. This means that s = S ? 1. Similarly, from s ? c ? S ? C, we imply S ? 1 ? c ? S ? C or C ? 1 ? c. Thus we may imply that c = C or c  = C - 1. There are only two combinations of (s,c) which are (S-1, C-1) and (S-1,C).

C  CScomb  CCC CCC  SN  SN  CS  CS  C  C  SN  SN  CS  CS  C  C  =??=  ??=?? ?  ?  ?  ?  ?  ???  ?  ???  ?  ?  )1  )1(1)1(11 )1,1(  (4.3)  SNCSC ?????  CS  CScomb  CCC CCC  SN  SN  CS  CS  C  C  SNCSC  ?=??=  ??=? ?  ?  ??  ?  ??  )1(11 ),1(    (4.4)  The formula (3.15) can then be rewritten as (4.5). With a pair of S and C, the possible values of s and c are (S-1, C-1) and (S-1, C). In contrast, with a pair of s and c, the possible values of S and C are (s+1, c+1) and (s+1, c). Thus, with the support and confidence numbers s and c counting from the training dataset, the required decrTR(R) function can be estimated in between decr(s+1,c+1) and decr(s+1,c).

We estimate decrTR(R) as the average value of decr(s+1,c+1) and decr(s+1,c):  )],1()1,1([ 1),( csdecrcsdecrcsdecrTR ++++=  (4.6)  The prediction confidence can be obtained using the following formula:  conf    (4.7) ),(),(),( csdecrcsconfcs TRTRTS ?= or  ).,.().,.()( cRsRdecrcRsRconfRconf TRTRTS ?=  (4.8)  Here, R.s and R.c represent the support and confidence numbers of rule R respectively.

5.  Performance Evaluation  In this section, we have implemented a simple version of AC [2,3] denoted as AC-S for evaluating the effectiveness of the prediction confidence measure. The value of the prediction confidence measure is calculated from confidence decrease which is estimated using formula (4.8). AC-S consists of only two steps of AC, namely rule generation and classification. After the conventional rule generation process, the confidence values of the rules are replaced with the corresponding prediction confidence values. We use single rule classification in the classification process and there is no rule selection process.

The performance of popular AC approaches is obtained from [3,4] for comparison. The approaches  )  ()() 1(  )) 1()  ((  )(  )  ()() 1(  )  ()(  )  1(  )  (),1()  ()1,1(  ) )1(1  ()  (),1() )1( )1(   ()  ()1,1( ),(  ? ??+  ? ?  ?  ? ?  ? ?  ? ?  ??  =  ? ??+  ? ?  ?  ? ?  ? ??+  ? ?  ? ? ?  ? =  ? ??+  ? ?  ???  ?? ?  ? ?  ? ?  ??+ ?? ??  ? ? ?  ? ? ?  ??? =  S CfCS  S CfC  S Cf  S Cf  S CSC  S CfCS  S CfC  S C  S CfCS  S SC  S CfC  S C  fCScomb S C  fCScomb  SS CC  S C  S C  fCScomb SS CC  S C  S C  fCScomb CSdecr  (4.5)      include Classification Based on Associations (CBA) algorithm [2], CBA(2) [3] and CMAR [4]. CBA(2) is an improved version of CBA with the use of multiple class support thresholds. CMAR is different from CBA in that it uses multiple rules instead of a single rule in the classification process.

The thresholds for mining association rules are set the same as the other approaches. The total minimum support is set to 1% and minimum confidence is set to 50%. AC-S has achieved good accuracy with the matching factor M set in the range [0 - 0.1]. The best value for M depends on the dataset. We have implemented AC-S with M set to 0.04.

The AC-S-Opt shows the best accuracy of AC-S with the optimal value of M in each dataset. The experiment is carried out using the same set of 26 datasets used in [2,3,4] which is obtained from the UCI Machine Learning Repository [5]. The discretization process, which discretizes continuous data into intervals and maps them into items, is performed using the Entropy method from the MLC++ machine learning library [6]. Table 4 shows the performance results in error rate for different approaches.

The preliminary results have shown that the accuracy of AC-S is better than CBA but worse than CBA(2) and CMAR. For the first 20 datasets, AC-S is even better than CBA(2) and is comparable to CMAR. When obtaining the best performance with optimal matching factor for each dataset, the AC-S-Opt is similar to CMAR in the overall accuracy. The very simple but effective implementation of AC-S shows the effectiveness of the prediction estimation.

The experimental results have shown the effectiveness of the adjustment for the confidence measure for AC.

6.  Conclusion  Associative classification takes the advantage from association rule mining in extracting high quality rules that can accurately generalize a dataset. However, as association rules and classification are two different tasks, there is a difference between the confidence of an association rule and its classification prediction. In this paper, we have discussed the relation between association rules and their prediction capability. The prediction of association rules is quantified by the proposed prediction confidence measure.

An effective estimation for the prediction confidence measure which is computationally simple is also given.

Table 4. Experimental results in error rate.

