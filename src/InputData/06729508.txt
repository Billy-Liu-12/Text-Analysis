Permutation-based Sequential Pattern Hiding

Abstract?Sequence data are increasingly shared to enable mining applications, in various domains such as marketing, telecommunications, and healthcare. This, however, may expose sensitive sequential patterns, which lead to intrusive inferences about individuals or leak con?dential information about organi- zations. This paper presents the ?rst permutation-based approach to prevent this threat. Our approach hides sensitive patterns by replacing them with carefully selected permutations that avoid changes in the set of frequent nonsensitive patterns (side-effects) and in the ordering information of sequences (distortion). By doing so, it retains data utility in sequence mining and tasks based on itemset properties, as permutation preserves the support of items, unlike deletion, which is used in existing works. To realize our approach, we develop an ef?cient and effective algorithm for generating permutations with minimal side-effects and distortion.

This algorithm also avoids implausible symbol orderings that may exist in certain applications. In addition, we propose a method to hide sensitive patterns from a sequence dataset. Extensive experiments verify that our method allows signi?cantly more accurate data analysis than the state-of-the-art approach.



I. INTRODUCTION  Organizations increasingly share sequence data to enable mining applications, including, among others, market basket analysis, web and process modeling, and preference-based services. However, the mining of these data may result in the exposure of sensitive sequential patterns, which allow intrusive inferences about individuals, or are con?dential to the organization (e.g., they provide competitive edge). This threat has transpired in various domains [1], [6], and it may lead to unsolicited advertising to customers, customer pro?ling, as well as ?nancial and reputational loss to organizations.

Consider, for example, the data in Fig. 1(a), which an insurance company aims to share with marketing partners. A record in these data contains the order that a customer follows when buying different insurance products a to h (the id column is not released). These data can be mined to ?nd interesting patterns, such as the number of customers who bought sports car, travel, and home insurance in this order (i.e., the records containing the sequential pattern [f, g, b]), or the number of customers who bought travel, home, and trailer insurance (i.e., the records containing the itemset {g, b, d}). However, sharing these data allows the exposure of the sensitive sequential patterns in Fig. 1(b) through sequential pattern mining [3].

These patterns are considered exposed (actionable) by the insurance company, when they appear in at least 3 records.

To prevent the exposure of sensitive sequential patterns, the number of records in which these patterns appear (i.e., their  support) must be lower than a minimum support threshold, which is speci?ed by data owners, based on domain knowledge and/or application requirements. To achieve this, all existing methods [1], [8] hide sensitive sequential patterns by deleting symbols from them. To preserve data utility, these methods aim to minimize changes in the set of frequent nonsensitive sequential patterns (each such change is termed a side-effect) and symbol deletions. Fig. 1(c) shows the result of applying the state-of-the-art method [8] to the data in Fig. 1(a), with min- imum support threshold 3. Notice that the method prevented the exposure of the sensitive sequential patterns in Fig. 1(b) by deleting 3 symbols. However, the nonsensitive patterns n1 to n5 in Fig. 1(d) are no longer frequent (i.e., 5 side-effects have occurred), because their support is lower than 3.

Clearly, deletion is a very drastic operation, which reduces the support of symbols and often incurs signi?cant utility loss.

Consider, for example, the data in Fig. 1(c). Deleting a from record 2 in Fig. 1(a), reduces the support of 6 nonsensitive patterns containing this symbol, including n1 and n2 that led to side-effects. Furthermore, deleting d from records 6 and 8 in Fig. 1(a), prohibits 12 (out of 48) sets of insurance products, shown in Fig. 1(e), from being discovered from the data in Fig.

1(c), by applying frequent itemset mining [2] with minimum support threshold 3. In addition to frequent itemset mining, deletion may harm data utility for other tasks based on itemset properties, such as association rule mining [2] and clustering using frequent itemsets [7], as well as for tasks in which patterns with both unordered and ordered components need to be discovered (e.g., actionable partial order mining [11]).

Contributions In this paper, we propose the ?rst, permu- tation-based approach to hiding sensitive sequential patterns.

Our approach is founded on the observation that the re- placement of a sensitive sequential pattern with any of its permutations in a record can reduce its support and lead to its hiding. This is because a permutation induces a different, non-sensitive1 ordering of the symbols in the sensitive pattern.

For example, the sensitive pattern s1 = [a, c, e], corresponding to the purchase of insurance for motorbike, jet-ski and then truck, was replaced by [a, e, c] in record 2 of Fig. 1(f). The ordering [a, e, c] is not deemed sensitive by the insurance company, because it does not provide competitive advantage gained by promoting truck insurance to customers who have purchased insurance for motorbike and then jet-ski. The use of permutation can greatly bene?t data utility in many domains  1Subsequences of sensitive patterns are not considered sensitive in all related works (e.g., [1], [15]) and lifting this assumption is trivial.

DOI 10.1109/ICDM.2013.57     id symbols 1 a b c d e f 2 a b c e 3 a c e h b 4 f g c e a b 5 d f g h b 6 d h b f g 7 f h g b 8 c d f g e 9 d h f g b  (a)  Sensitive patterns s1 = [a, c, e] s2 = [d, f, g] s3 = [d, h, b]  (b)  id symbols 1 a b c d e f 2 * b c e 3 a c e h b 4 f g c e a b 5 d f g h b 6 * h b f g 7 f h g b 8 c * f g e 9 d h f g b  (c)  Side-effects n1 = [a, c] n2 = [a, e] n3 = [d, b] n4 = [d, g] n5 = [d, h]  (d)  Lost itemsets {d, g} {d, h} {b, d, g} {b, d, h} {d, f, h} {d, g, h} {d, g, f} {b, d, g, f} {b, d, g, h} {b, d, f, h} {d, g, f, h} {b, d, g, f, h}  (e)  id symbols 1 a b c d e f 2 a b e c 3 a e c h b 4 f g c e a b 5 d g f b h 6 d b h g f 7 f h g b 8 c d g f e 9 d h f g b  (f)  Candidates d? = [a, c]  d+ = [c, b] (g)  Fig. 1: (a) original dataset S and (b) sensitive patterns S . State-of-the-art method [8]: (c) output (* denotes a deleted symbol), (d) side-effects, and (e) lost frequent itemsets. PH: (f) output (swapped symbols are highlighted), and (g) candidate patterns.

where multiple orderings of symbols are possible, as in market basket analysis and preference-based services. This is because, unlike deletion, permutation preserves the support of symbols.

The number of permutations of a sensitive sequential pattern grows factorially with its number of symbols, which provides great choice of permutations. For example, up to 5034 permutations can be used to replace a sensitive pattern comprised of 7 symbols. However, to preserve data utility, the selected permutation needs to (I) avoid side-effects and (II) minimize changes in the ordering information of the record in which the permutation appears (distortion). Moreover, a sensitive pattern is typically replaced by many different permu- tations and in a subset of the records in which it appears. Both the generation of appropriate permutations and the selection of the aforementioned subset of records introduce signi?cant computational challenges. To address these challenges, our work makes the following speci?c contributions.

First, we investigate the problem of avoiding side-effects, incurred by permutation. We provide tight conditions for identifying patterns that lead to side-effects. These conditions are based on symbol order and support, and they apply to nonsensitive patterns that may become infrequent (candidates for lost) or frequent (candidates for ghost), after permuting a sensitive pattern. Consider the patterns in Fig. 1(g) and that the pattern s1 = [a, c, e] in Fig. 1(b) must be hidden using a minimum support threshold 3. d? is a candidate for lost, whereas d+ is a candidate for ghost, because the support of d? (resp., d+) may become lower than (resp., at least) 3. This occurs, for example, when s1 is permuted to [c, e, a] in the record 2 of Fig. 1(a), which becomes [c, b, e, a].

Given candidates of each type, we prove that generating a permutation that avoids side-effects is NP-complete.

Second, we develop PDPG, an algorithm aiming at generat- ing permutations that avoid side-effects and have minimum dis- tortion. PDPG employs Cayley distance [13] to measure distor- tion, which quanti?es the change of the ordering information of a record, based on the number of symbol swaps. For instance, both permutations [a, e, c] and [e, a, c] of s1, which transform record 2 in Fig. 1(a) to [a, b, e, c] and [e, b, a, c], respectively, do not cause side-effects with respect to the candidates in Fig.

1(g), but PDPG generates [a, e, c], as it has a lower (better) Caley distance. Note that [a, e, c] incurs a smaller change of the ordering information of record 2, because it is produced by a single swap of c and e in s1, whereas two swaps are required to produce [e, a, c]. Furthermore, PDPG is coupled with a  heuristic for generating permutations with minimal number of side-effects, when no permutation that avoids all side-effects can be constructed. Moreover, this algorithm does not generate permutations containing implausible symbol orderings (e.g., a pattern representing the addition of an extra driver to a car insurance policy before the purchase of the policy). Thus, data utility is preserved and the identi?cation of such orderings as artifacts of sanitization is prevented.

Third, we propose PH, a novel sequential pattern hiding al- gorithm that works as follows: (I) it selects records with a small upper-bound on the number of side-effects that can be incurred by their sanitization (i.e., the hiding of all sensitive patterns appearing in the records), and (II) it sanitizes each selected record, by replacing the sensitive patterns in the record with appropriate permutations, generated by PDPG. For example, when applied to Fig. 1(a) with minimum support threshold 3, PH produced the data in Fig. 1(f). As no side-effects were incurred and no symbols were deleted, the data produced by PH remain as useful as the original data, for sequential pattern mining and tasks based on itemset properties, as opposed to the data in Fig. 1(c).

Fourth, we experimentally demonstrate that our approach permits highly accurate sequential pattern mining, frequent itemset mining, and estimation of the support of items and frequent itemsets, which is important in several data mining tasks. Speci?cally, PH signi?cantly outperformed the state-of- the-art method [8] in terms of data utility, as it incurred at least 21% fewer side-effects, preserved at least 77% more frequent itemsets, and was many times more effective at preserving the support of items and frequent itemsets.

Paper organization Section II presents the background, Section III our permutation-based methodology, and Section IV our sanitization algorithm. Sections V, VI, and VII present related work, our experimental evaluation, and a discussion on attacks based on background knowledge, respectively. Section VIII concludes the paper.



II. BACKGROUND  In this section, we present necessary concepts to explain our approach and formulate the problem statement.

Preliminaries Let A = {a1, a2, . . . , a|A|} be an alphabet of size |A| and S = {t(1), t(2), . . . , t(|S|)} be a collection of sequences with cardinality |S|. S will also be referred to as     the original dataset and its elements as transactions. The i-th transaction in S is denoted with t(i) = [s(i)1 , s  (i) 2 , . . . , s  (i)  |t(i)| ],  where s(i)l ? A, l ? [1, |t (i)|]. A transaction t(i) contains  symbols that are not necessarily distinct, its length (i.e., number of symbols) is denoted with |t(i)|, and it has an id t(i).id = i. A sequence s = [s1, s2,. . . , s|s|] has length |s| and is a subsequence of sequence s? = [s?1, s?2,. . . , s?|s?|], denoted s ? s?, if there exist integers 1 ? i1 ? i2 . . . ? i|s| such that s1 = s  ? i1 , s2 = s?i2 ,. . . , s|s| = s  ? i|s|  . We also write that s? is a supersequence of s and that s is contained in s?. The set of common symbols of s and s? is denoted with s??s?.

The support of a sequence s in S, denoted by supS(s), is de?ned as the number of transactions in S that contain s as a subsequence. Given a support threshold minSup, a sequence s is a frequent sequential pattern, if supS(s) ? minSup, and infrequent otherwise. The problem of sequential pattern mining [3] is to ?nd the set of all frequent sequential patterns in S, given minSup. This set is denoted by FS,minSup, and we may omit S and/or minSup, when it is clear from the context.

The support has an anti-monotonic property, i.e., supS(s) ? supS(s  ?), if s ? s?.

A permutation ?(E) of a set of elements E = {0, . . . , n?1}  is a bijection (1-1 mapping) from E to itself. In this paper, E refers to positions of symbols in sequences, and not to the actual symbols of A, unless stated otherwise. Given a sequential pattern s, ?(s) refers to a permutation of positions of s. For simplicity, we may use symbols of s that coincide with their positions.

A permutation ? is represented in two-line notation as  ? =  ( 0 1 . . . n? 1  ?(0) ?(1) . . . ?(n? 1)  ) , (1)  where the mapping is {i ? ?(i)}, ?i ? E . The permutation that ?xes all elements (i.e., maps each i ? E to itself) is the identity permutation I. The product of permutations ? and ? is given by their composition ? = ??, ?(i) = ?(?(i)). So, the permutations multiply in the order in which they are written.

The inverse permutation ??1 of ? is a permutation such that ???1 = ??1? = I.

Given a permutation ?(s), we can group the elements in s and ?(s) that trade their places (swap) in Equation (1) to obtain the cycle notation of ?(s). Figure 2(a) presents the cycle notation of all permutations of s = [0, 1, 2].

?(s) cycle # swaps to notation recover s  [0, 1, 2] [(0), (1), (2)] 0 [0, 2, 1] [(0), (1, 2)] 1 [1, 0, 2] [(0, 1), (2)] 1 [1, 2, 0] [(0, 1, 2)] 2 [2, 0, 1] [(0, 2, 1)] 2 [2, 1, 0] [(0, 2), (1)] 1  (a)  ???? ??????????????????? ??????????????????????? ?????????????????????????? ?????????????????????????????  ???????????????????????????????????????????????  ?????? ?????	????????????????  (b) Fig. 2: (a) Permutations ?(s) of s = [0, 1, 2], their corresponding cycle notation, and the number of swaps needed to recover s. (b) The triangle of Stirling?s cycle numbers for n = 4.

Given x1, . . . , xn distinct elements of E , a cycle (x1, . . . , xn) is a permutation that maps x1 to x2, x2 to x3, ..., xn to x1, while ?xing any other element in E . A cycle (x1, . . . , xn) has length n and is a product of n?1 transpositions (i.e., cycles of length 2) [13], i.e.,  (x1, x2, . . . , xn) = (x1, x2)(x1, x3) . . . (x1, xn). (2)  For example, the cycle (0, 1, 2) corresponds to [1, 2, 0] and is the product of transpositions (0, 1)(0, 2). In fact, applying the transposition (0, 1) to s = [0, 1, 2], changes s to [1, 0, 2], and applying (0, 2) to [1, 0, 2], changes s to [1, 2, 0].

The Cayley distance between two permutations ? and ?, denoted with C(?, ?), is de?ned as the minimum number of transpositions required to change ? to ? by multiplication [13]  C(?, ?) = min{n|??1 . . . ?n = ?}, (3)  where ?i is a transposition (swap). Thus, C(?, I) denotes the Cayley distance between ? and I, the identity permutation of the ?rst argument. In particular, it holds that  C(?, I) = n? c(?), (4)  where c(?) is the number of cycles in ? and n is the number of symbols in ?. For example, C([0, 2, 1], I) = 3 ? 2 = 1, as [0, 2, 1] has 2 cycles.The number of permutations of n symbols that have k cycles is equal to the Stirling cycle number [17]:[n  k  ] = [n? 1 k ? 1  ] + (n? 1)  [n? 1 k  ] (5)  k-Cycle permutation generation The generation of all k- cycle permutations (i.e., permutations with k cycles) of a sequence with n symbols can be performed by the East- side/west-side algorithm [20]. This algorithm recursively con- structs k-cycle permutations by following a triangle similar to that in Figure 2(b), for the given n and k, in a bottom-up manner. Let L(n, k) be the set of k-cycle permutations of n symbols, which is split into two subsets; one with permutations in which the symbol n is alone in a cycle (east-side), and another with permutations in which n shares a cycle with other symbols (west-side). To generate the east-side subset, the algorithm takes the set L(n ? 1, k ? 1) and inserts, into each permutation, a cycle that contains only symbol n.

To generate the west-side subset, it takes L(n ? 1, k) and, for each permutation ? in this set, it constructs n ? 1 new permutations. This is performed by inserting the symbol n successively, between consecutive elements in each of the cycles in ?. For example, to construct L(3, 2), the algorithm takes L(2, 1) = {[(0, 1)]} and creates a new cycle, which contains only 2, to obtain [(0, 1), (2)] (east-side element).

Then, it takes L(2, 2) = {[(0), (1)]} and inserts the symbol 2 into each position between consecutive symbols in [(0), (1)] to obtain [(0, 2), (1)] and [(0), (1, 2)] (west-side elements).

The generation of a random k-cycle permutation, can be performed by the Random-permutation East-side/west-side algorithm [20]. This algorithm constructs L(n, k) by ?rst selecting randomly an east-side or west-side permutation. The probability of choosing the east-side element is  [ n?1 k?1  ] / [ n  k  ] . If  an east-side element is selected, then it recursively chooses a random permutation from L(n ? 1, k ? 1) and inserts a new cycle, which contains only symbol n, into it. Otherwise, it recursively selects a random permutation from L(n?1, k) and inserts n into a randomly chosen position, between consecutive elements in L(n? 1, k). This algorithm needs O(n2) time.

In the following, we de?ne the problem we aim to solve.

Problem (Sequential pattern hiding) Given the original dataset S, a support threshold minSup and a set of sensitive sequential patterns S (selected by data     owners), construct a version S? of S such that: (I) supS?(s) < minSup, for each s ? S, (II) FS? = FS ? S?, where S? = {s? ? FS? |s ? s?, s ? S}, and (III) S? is constructed with minimum distortion2.

The problem requires sanitizing the original dataset S so that (I) no sensitive sequential pattern s ? S can be mined from the released dataset S? at a support threshold minSup or higher; (II) no side-effects are introduced to S? in terms of (i) sequential patterns in FS ? S? that are infrequent in S? (lost) or (ii) infrequent sequential patterns in S that are frequent in S? (ghost), and (III) S? and S are as ?similar? as possible (the notion of distortion will be discussed later). Unfortunately, the problem is NP-hard (the proof follows from [1]). Also, note that we consider a single support threshold minSup.

Extensions for multiple support thresholds are straightforward and can be implemented following [1].

The three goals in our problem are not equally important.

That is, a sequential pattern hiding method must protect all sensitive sequential patterns, but it should prioritize the mini- mization of the number of side-effects over that of distortion [8]. This is because it is essential that the released dataset enables the discovery of nonsensitive frequent patterns.



III. HIDING METHODOLOGY  This section presents our PDPG algorithm and a heuristic for increasing its effectiveness. Subsequently, it explains how this algorithm deals with forbidden patterns (e.g., implausible symbol orderings). PDPG takes as input candidate patterns for lost and ghost, which are identi?ed as discussed below.

Identifying candidates for ghost and lost Let s be a sensitive pattern and ?(s) be the set of all its permutations. Observe that the replacement of s with any of the |s|! permutations in ?(s), except the identity permutation, in a transaction, can lead to reducing the support of s by 1. Thus, to hide the sensitive pattern s from the original dataset S, it suf?ces to perform such replacements in supS(s) ? (minSup ? 1) transactions. However, replacing s by a permutation may decrease or increase the support of nonsensitive sequential patterns in S, incurring lost or ghost patterns, respectively.

This can be prevented by identifying permutations that avoid side-effects and using them to replace s.

We now provide detailed conditions for the decrease or increase of the support of a nonsensitive pattern s?, when a sensitive pattern s in a transaction t is replaced with its permutation ?(s). The ?rst type of conditions we consider are symbol-based and explain the change in the support of s?, based on the interplay of symbols in s and s?, as follows:  S1 When s??s? ?= ? (i.e., s and s? share symbols), the support of s? may increase or decrease.

S2 When the conditions (a)-(c) below hold simultaneously, the support of s? may decrease:  a) s and s? have a common subsequence s?? b) s and s? co-occur in t, by sharing s?? c) the permutation of s causes at least one of symbol  in s?? to change its relative ordering with respect  2Following [8], the loss of a sequence s? in FS that is a supersequence of a pattern s ? S is not considered as side-effect, as s? will be inevitably hidden when we hide s.

to other symbols in s??. This requires one or more swaps between: (i) symbols in s??, or (ii) a symbol in s \ s?? and another in s??.

S3 When the conditions (a)-(c) below hold simultaneously, the support of s? may increase:  a) s and s? have a common set of symbols s? = s??s?, but s? is not a subsequence of s, and s is not a subsequence of s?  b) s co-occurs in the transaction t with a permutation ?(s?) of s?, by sharing s?  c) the permutation of s reverses ?(s?) and recovers s?. The reversal of ?(s?) requires one or more swaps between: (i) symbols in s?, or (ii) a symbol in s \ s? and another in s?.

There are two special cases for s?? in S2: (i) s?? = s?, when s? ? s , and (ii) s?? = s, when s ? s?. Similarly, the special cases for s? in S3 are: (i) s? = s?, and (ii) s? = s. Examples for conditions S2 and S3 are shown in Figs. 3(a) and 3(b).

Swapping t symbols in s s? s?? t ?(s) after ?(s)  s?? [a, b, c] [a, d, c] [a, c] [ a , b, d, c ] [c, b, a] [ c , b, d, a ] s \ s??, s?? [a, b, c] [d, b, c] [b, c] [ a , d, b, c ] [c, b, a] [ c , d, b, a ] s \ s??, s?? [b, a] [d, c, a] [a] [d, b , c, a ] [a, b] [d, a , c, b ]  (a) Swapping t symbols in s s? s? t ?(s) after ?(s)  s? [a, b, c] [c, d, a] {a, c} [ a , b, d, c ] [c, b, a] [ c , b, d, a ] s \ s?, s? [a, b, c] [b, d, c] {b, c} [ a , d, b , c] [b, a, c] [ b , d, a , c] s \ s?, s? [a, b, c] [a, d, b] {a, b} [a, b , d, c ] [a, c, b] [a, c , d, b ]  (b)  Fig. 3: Permuting s to ?(s) in a transaction t can (a) decrease or (b) increase the support of s?. Swapped symbols are highlighted.

We also show that, whether or not a nonsensitive pattern s? can lead to a side-effect, depends on its support, before permuting s, and on the number of transactions to which permutation is applied, denoted with ?supp(s|?(s)). Combin- ing this observation with conditions S2 and S3, leads to the following conditions, which are used to identify candidates for lost and ghost.

C1 A frequent, nonsensitive pattern s? may become infrequent (lost), as a result of permuting s in ?supp(s|?(s)) transac- tions in S, if it satis?es S1 and S2, and its support, before permuting s, satis?es  supS(s ?) < minSup+ ?supp(s|?(s)). (6)  C2 An infrequent, nonsensitive pattern s? may become frequent (ghost), as a result of permuting s in ?supp(s|?(s)) transactions in S, if it satis?es S1 and S3, and its support, before permuting s, satis?es  supS(s ?) ? minSup??supp(s|?(s)). (7)  Observe that C1 models a worst case, in which s? sat- is?es S1 and S2, and it co-occurs with s, in each of the ?supp(s|?(s)) transactions. In this case, the support of s? in S decreases by ?supp(s|?(s)), and, for s? to become infrequent, we need supS(s?) ? ?supp(s|?(s)) < minSup, from which we get (6). A similar logic is applied to derive C2.

Conditions C1 and C2 can signi?cantly improve the ef- ?ciency of identifying candidates for lost or ghost, as, in practice, they apply to a small fraction of the nonsensitive patterns. To employ these conditions, we ?rst use Equations (6) and (7), which establish bounds on the support of patterns, then     apply S1 to prune pairs of s and s? that do not share symbols, and last check S2 or S3. This heuristic order is effective at minimizing the number of checked patterns.

Checking conditions S1 and S2 is straightforward, because s? is a subsequence of the transaction t. On the other hand, the checking of S3 requires taking into account the transaction and the potentially many ways to recover s? (see S3(c)).

To check whether s? can be recovered by permuting s, we develop a simple algorithm, called Restricted Subsequence of a Permutation (RSP). RSP matches s? to the symbols in t in order to identify whether s? can occur in this transaction, after permuting s. RSP needs O(|s?|) time and space, and it is illustrated in the following example.

Example 1 Consider the patterns s and s?, and the transac- tion t = [a, b, d, c] in the ?rst row of Fig. 3(b). RSP replaces the symbols of s = [a, b, c] in t with placeholders #i, for i = 0, 1, 2, to obtain [#0,#1, d,#2], and then performs a pattern matching of s? = [c, d, a] in [#0,#1, d,#2]. Let ? = {a, b, c} be the set of symbols from s that must be matched in [#0,#1, d,#2] and V a vector holding intermediate matching results. Initially, every placeholder points to the same set of symbols (i.e., #i = ?, for i = 0, 1, 2), as it can be assigned to any symbol in ?. Next, RSP considers each symbol in s? in the order of appearance. First, it considers c, which is matched to V [0] = #0 and subtracted from ?. Then, RSP considers d. Thus, it skips #1 and matches V [1] = d. In this case, ? remains unchanged. Last, RSP considers a, which is matched to V [2] = #2 and subtracted from ?. This results in V = [c, d, a], #1 = b, so s? may appear in t and satis?es S3.

Generating pattern-constrained permutations with mini- mal Cayley distance Before discussing how permutations that avoid side-effects and minimize Cayley distance can be constructed, we de?ne the notion of pattern-constrained permutation. Let s be a sensitive sequential pattern, t be a transaction in S, and D?(t)(s) and D+(t)(s) be sets of candi- date patterns to lose and gain occurrence in t, respectively, as a result of replacing s with certain of its permutations.

The permutation ?(s) is pattern-constrained, if all patterns in D?(t)(s) and in D+(t)(s) do not lose and gain occurrence in t, respectively, when ?(s) replaces s. Generating a pattern- constrained permutation is NP-complete, as proven below.

Theorem 1 Given a sensitive sequential pattern s, a transac- tion t ? S, a set D?(t)(s) of candidates to lose occurrence and a set D+(t)(s) of candidates to gain occurrence in t, ?nding a pattern-constrained permutation ?(s) of s is NP-complete.

Proof: (Sketch) The problem can be reduced from the NP- complete SAT problem. The reduction involves: (I) construct- ing a Boolean formula B in CNF, whose literals correspond to different ways in which patterns in D?(t)(s) and D+(t)(s), as well as s, can appear in t, and (II) showing that B is true if and only if ?(s) solves our problem. ?  Furthermore, we de?ne the notion of partial permutation, which is central to our permutation generation algorithm.

Given a pattern s, a partial permutation of s, ?(s|m), is a permutation of a pre?x of positions of s, from 0 (start of s) to m ? 1, for m ? |s|. Given a transaction t and a pattern d?(t)(s) ? D?(t)(s) (respectively, d+(t)(s) ? D+(t)(s)), a  partial permutation ?(s|m) of s, where m ? |s|, is:  R1 satis?able w.r.t. d?(t)(s) if a) C(?(s|m)(d?(t)(s)), I) ? (0, n?m], or b) C(?(s|m)(d?(t)(s)), I) = 0 (perfect ordering)  R2 satis?able w.r.t. d+(t)(s) if a) C(?(s|m)(d+(t)(s)), I)= 0 and n?m > 0, or b) C(?(s|m)(d+(t)(s)), I)> 0 (disorder)  where ?(s|m)(d?(t)(s)) and ?(s|m)(d+(t)(s)) denotes the result of applying ?(s|m) to transaction t and then projecting on d?(t)(s) and d+(t)(s), respectively.

R1 states that the occurrence of d?(t)(s) can be preserved, by applying swaps involving the remaining symbols of s, if there are such symbols (i.e., n ? m > 0). Furthermore, according to R2, if there are remaining symbols, the occurrence of d+(t)(s) may be avoided, since a single swap suf?ces for that. Moreover, by comparing R1 and R2, it can be seen that, preventing a pattern d+(t)(s) from gaining an occurrence in t and becoming ghost, is much easier than preventing a pattern d?(t)(s) from losing an occurrence in t and becoming lost.

The following example illustrates a satis?able permutation w.r.t. a pattern d?(t)(s) and a pattern d+(t)(s).

Example 2 Consider the transaction t = [c, d, f, g, e], the sensitive sequential pattern s = [d, f, g], and the candidate patterns d?(t)(s) = [c, d, f ] and d+(t)(s) = [f, d, e] to lose and gain an occurrence in t, respectively. By referring to symbol positions, we can write t = [0, 1, 2, 3, 4], s = [1, 2, 3], d?(t)(s) = [0, 1, 2], and d+(t)(s) = [2, 1, 4]. Also, let ?(s|2) = [2, 1] be a partial permutation of s. Observe that  ? ?(s|2)(d?(t)(s)) = [0, 2, 1], C(?(s|2)(d?(t)(s)), I) ? (0, 1]. Thus, ?(s|2) is satis?able w.r.t. d?(t)(s), ac- cording to R1(a).

? ?(s|2)(d+(t)(s)) = [2, 1, 4], C(?(s|2)(d+(t)(s)), I) = 0 and n?m = 1 > 0. Thus, ?(s|2) is satis?able w.r.t.

d+(t)(s), according to R2(a).

In fact, s becomes [2, 1, 3] after applying ?(s|2), and, by applying the swap (2, 3) to s, we obtain s = [3, 1, 2] and t = [0, 3, 1, 2, 4]. This recovers the occurrence of d?(t)(s) and prevents the occurrence of d+(t)(s) in t.

Note that conditions R1 and R2 assume a single pattern d?(t)(s) and d+(t)(s), respectively. However, there can be a set of candidatesD?(t)(s) to lose occurrence in a transaction t, and a set of candidates D+(t)(s) to gain occurrence in t. When there are multiple patterns in D?(t)(s) (resp., D+(t)(s)), we apply condition R1 (resp., R2) to each of these patterns, and use the maximum of the Caley distance, over the patterns, to determine if ?(s|m) is satis?able or not.

To tackle our problem, we must generate pattern- constrained permutations that hide all sensitive patterns, from each transaction of S that needs to be sanitized, and incur minimum distortion. Clearly, the generation of such permuta- tions remains NP-complete. Furthermore, applying Random- permutation East-side/west-side (see Section II) and then     checking if the generated permutation is satis?able, is pro- hibitively expensive, due to the extremely large number of random trials required.

Thus, we develop the Pattern-constrained Distance-based Permutation Generation (PDPG) algorithm. Given a transac- tion t, containing a sensitive pattern s, PDPG aims at gen- erating a random, partial permutation ?(s|m) that is pattern- constrained and has minimum Cayley distance (i.e., maximum number of cycles k). The following steps describe PDPG.

1) Let ?(1|1) be the permutation [(0)] in cycle notation.

2) For m = 2, . . . , n and for k = m, . . . , 1, execute steps 2  to 6.

3) Apply Conditional Random Search to choose the type of  ?(s|m) in terms of cycle structure.

4) If ?(s|m) is of west-side type, then  i Generate a set of positions P leading to satis?- able extensions of ?(s|m).

ii Randomly select a position from P .

iii Extend ?(s|m) using the selected position.

5) If ?(s|m) is of east-side type, then extend ?(s|m) using the next available position for a new cycle.

6) If m = n, then return ?(s|m), which is satis?able and has the minimum Cayley distance.

PDPG uses a function Conditional Random Search to guide the random search in the space of possible solutions, while trying to generate ?(s|m) (step 3). This function ?rst computes all possible extensions of ?(s|m) (i.e., each random partial permutation ?(s|m + 1) that can be obtained from ?(s|m)) for the current m and k, and, based on these extensions, it computes the distribution of satis?able extensions from east- side and west-side permutations. This is required to compute the probability of selecting an east-side or west-side permu- tation, which is not known in advance. If ?(s|m) is of west- side type, then PDPG: (I) constructs a set P , containing all the positions that will lead to satis?able extensions of ?(s|m), (II) randomly selects a position in P , and (III) uses this position to extend ?(s|m), as in the Random-permutation East-side/west- side algorithm (step 4). If ?(s|m) is of east-side type, the algorithm extends ?(s|m), using the next available position for creating a new cycle (step 5). Last, when m = n, PDPG returns a satis?able permutation ?(s|m) with the minimum Cayley distance.

Fig. 4 shows how PDPG generates a permutation, for a transaction t = [0, 1, 2, 3, 4], sensitive pattern s = [1, 2, 3], and candidate patterns d?(t)(s) = [2, 3, 4] and d+(t)(s) = [3, 2, 4].

The algorithm follows the triangle in Fig. 4(b) and generates a randomly selected, satis?able ?(s|m) at each element of the triangle (i.e., L(m, k)). That is, PDPG starts by ?1 = ?(1|1) and extends it to ?2 = ?(2|2), which is of east-side type. The extension is performed using position 1, which is the next available position for a cycle. Then, PDPG generates ?3 = ?(2|1), which is of west-side type by extending it using position 1. Last, ?4 is constructed and returned for m = n = 3 and k = 2. Note that, replacing s with ?4, transforms t to [0, 2, 1, 3, 4]. Thus, d?(t)(s) = [2, 3, 4] (resp., d+(t)(s) = [3, 2, 4]) does not lose (resp., gain) occurrence in t.

However, it may not be possible for PDPG to satisfy all constraints, due to the NP-completeness of the problem. In this case, we employ a heuristic that aims to generate a permutation  m k ?(s|m) constraints d?(t)(s) = [2, 3, 4] d+(t)(s) = [3, 2, 4]  Satisf. C(d?(t)(s), I) Satisf. C(d+(t)(s), I) 1 1 ?1 = [(0)] true 0 true 1 2 2 ?2 = [(0), (1)] true 0 true 1 2 1 ?3 = [(0, 1)] true 0 true 1 3 2 ?4 = [(0, 1), 2] true 0 true 1  (a)  (b) Fig. 4: (a) Generating a satis?able partial permutation, and (b) the steps on the triangle.

satisfying the maximum possible number of constraints. The heuristic, called Rank Reduce PDPG (RR-PDPG), ranks the constraints, based on their chance of being satis?ed, and then progressively reduces the number of considered constraints given as input to PDPG. Our heuristic is based on the following observations:  ? The shortest patterns in D?(t)(s) are the easiest to be preserved, since they require fewer positions to have a particular ordering. Therefore, we rank the patterns in D?(t)(s) in increasing order of length.

? The longest patterns in D+(t)(s) are the hardest to be created, since they require more symbols to follow a particular ordering. Therefore, we rank the patterns in D+(t)(s) in decreasing order of length.

? The patterns in D+(t)(s) are easier to be preserved than those in D?(t)(s). Therefore, we ?rst attempt to satisfy the constraints for D+(t)(s).

The following steps describe RR-PDPG.

1) Rank the patterns in D?(t)(s), in increasing order of length, and the patterns in D+(t)(s), in decreasing order of length.

2) Let v? and v+ be vectors containing all patterns in D?(t)(s) and D+(t)(s), respectively. The vector v? is sorted increasingly w.r.t. the length of patterns in D?(t)(s), whereas the vector v+ is sorted decreasingly w.r.t. the length of patterns in D+(t).

3) Let variables n? = |v?| and n+ = |v+| store the ranges in v? and v+ respectively.

4) Recursively select the n+ ?rst elements in v+ and try to ?nd a permutation satisfying these elements using PDPG.

If this fails, set n+ = n  +  and execute step 4 again. Else,  continue to step 5.

5) Recursively select the n? ?rst elements in v? and try  to ?nd a permutation satisfying n? and the previously selected n+ elements using PDPG. If this fails, set n? = n?  and execute step 5 again. Else, return.

After ranking the patterns, RR-PDPG tries to ?nd a large subset of candidates for ghost, which are satis?ed, by recur- sively selecting the ?rst half of elements in v+ (steps 1-4).

Then, it attempts to add a subset of candidates for lost to the subset already found, by recursively selecting the ?rst half of elements in v? (step 5). Steps 4 and 5 are executed O(log(M)) times, where M = max{|D?(t)(s)|, |D+(t)(s)|},     and an unconstrained permutation with minimum Caley dis- tance is generated, in the worst-case when n+ = n? = 0.

The worst case time complexity of PDPG is O(n3M) (vs. O(n2) for the unconstrained case), where n is the length of the sensitive pattern. This is because O(n2) is needed to follow the triangle, O(n) time to construct the extensions, and M to check their satis?ability. Thus, RR-PDPG needs O(n3M log(M)), in the worst case. Since n is a small constant in practice, both PDPG and RR-PDPG are very ef?cient.

Dealing with forbidden patterns In certain application domains, such as process mining, there may be implausible symbol orderings. Such orderings, henceforth referred to as forbidden patterns, must be prevented from occurring in the sanitized data to help data utility and ensure that attackers do not use them to distinguish between original and sanitized transactions. To achieve this, forbidden patterns are given as input to PDPG and treated similarly to candidates for ghost, with the difference that they are checked using conditions S1 and S3 but not C2. Thus, PDPG generates only permutations that contradict all forbidden patterns, and, when this is not possible, it does not invoke RR-PDPG but returns control to PH, which deals with them, as will be detailed later.

Example 3 Consider the transaction [a, b, c, d, e] that records a salesperson?s actions [(a) new service promotion, (b) current service cost inquiry, (c) cheaper service promotion, (d) client accepts terms, (e) offer discount], when marketing a new service. The pattern [a, d, e] is sensitive and the pattern [d, a] is forbidden, as terms cannot be accepted before new service promotion. PDPG produces the permutation [a, e, d], which contradicts [d, a] and cannot be ruled out, as a discount is typically offered before term acceptance. This is in contrast to permutations following [d, a], which are not generated.

Forbidden patterns can be speci?ed by data owners, based on domain expertise. When data owners have limited or no domain expertise, such patterns can be discovered based on beliefs [18] or mined from the original dataset [19]. Unless otherwise stated, we will henceforth assume that all generated permutations contradict the speci?ed forbidden patterns.



IV. PERMUTATION-BASED HIDING (PH)  This section presents the Permutation-based Hiding (PH) algorithm. PH works in three phases, in which: (I) candidate patterns for lost and ghost are identi?ed, (II) transactions that will be sanitized are selected, and (III) the selected transactions are sanitized. The pseudocode of PH describes these phases.

In the ?rst phase (steps 3-7), PH iterates over each sensitive pattern and creates the sets of candidates for lost and ghost, as well as a set containing all candidates, for the sensitive pattern (steps 4-6). A set containing the candidates for all sensitive patterns, is also created (step 7). In the next phase (step 8), PH uses a function PST (to be described later) in order to select transactions that will be sanitized. During the last phase (steps 9-16), PH sanitizes each selected transaction. In steps 10-11, the algorithm creates two sets u? and u+, which contain the candidates for lost with support minSup, and the candidates for ghost with support minSup? 1, respectively.

The patterns in u? will become lost, if their support decreases after permutation, whereas those in u+ will become ghost, if  Algorithm: Permutation-based Hiding (PH) 1 DS ? ? 2 S? ? ? // identification of candidate patterns  3 foreach sensitive pattern s ? S do 4 D?(s) ? each pattern satisfying condition C1, for every t ? S 5 D+(s) ? each pattern satisfying condition C2, for every t ? S 6 D(s) ? D?(s) ? D+(s) 7 DS ? DS ? D(s) 8 S ? PST (S,S?,minSup, S,DS) // transaction selection // transaction sanitization  9 foreach transaction t ? S do 10 u? ? each pattern in D?(t)(s) with support minSup in S, for every  sensitive pattern s ? t 11 u+ ? each pattern in D+(t)(s) with support minSup ? 1 in S, for  every sensitive pattern s ? t 12 foreach sensitive pattern s ? t do 13 Hide s using the following steps ordered w.r.t. their precedence:  i Apply PDPG and replace s with the generated permutation.

ii Apply RR-PDPG and replace s with the generated permutation, if step i fails and no patterns are forbidden.

iii Apply symbol deletion, if steps i and ii fail.

14 Update t, u? , u+ , D?(t)(s), and D+(t)(s) to re?ect the hiding.

15 Update the support of patterns in D?(s) and D+(s) to re?ect the  sanitization of t.

16 Move the sanitized transaction t from S to S?.

17 return S?  Algorithm: Potential Side-effects based Transfer (PST) 1 rank(t) ? 0, for every transaction t ? S 2 S? ? each t ? S that does not support any sensitive pattern 3 foreach sensitive pattern s ? S do 4 D(s) ? retrieve D(s) from DS 5 foreach transaction t ? S do 6 rank(t) ? rank(t) + (|D(s)|+ 1) 7 Sort transactions in S in decreasing order of rank.

8 foreach transaction t ? S do 9 if sup  S? (s) < minSup ? 1, for every sensitive pattern s ? t then 10 S? ? S? ? t 11 S ? S \ t 12 return S  their support increases. In step 13, PH sanitizes the sensitive patterns in the selected transaction by applying: (i) PDPG, using the candidates in u? and u+, (ii) RR-PDPG, if step (i) fails and no patterns are forbidden, and (iii) element deletion, if steps (i) and (ii) fail. Next, PH updates the transaction and its corresponding sets of candidate patterns, to re?ect the hiding (step 14). After that, the algorithm updates the support of patterns in D?(s) and D+(s) to re?ect the sanitization of the transaction, and moves the transaction to the sanitized dataset, which is subsequently returned (steps 15-17).

PH sanitizes a sensitive pattern using symbol deletion only in the extreme case, in which there is not a suf?cient number of available permutations to apply. Thus, deletion is a rare operation, employed only for very short patterns (e.g., patterns of size two that have only one available permutation). PH aims at minimizing the distortion caused by this operation, by deleting the symbol with the largest multiplicity in the transaction, or the largest support in the dataset, if all symbols in the pattern have multiplicity one.

We now describe the Potential Side-effect based Transfer (PST) function. PST transfers two types of transactions, which will not be sanitized, to the dataset S: (I) those that do not support any sensitive pattern, and (II) those whose transfer does not create frequent sensitive patterns in S?. Also, to preserve data utility, PST favors transactions of type II that could lead to many side-effects. After transferring transactions of type I, PST iterates over sensitive patterns, and retrieves the set of candidates for lost and ghost, for a sensitive pattern (steps 2-4). The size of this set is an upper bound on the     number of side-effects, which would be caused by permuting the pattern. Thus, PST ranks all transactions in S, so that transactions that support many patterns with large bound, obtain a higher rank (steps 5-6). Then, PST sorts transactions in decreasing order of rank, and transfers each transaction t of type II, if every sensitive pattern supported by t has a support of less than minSup? 1 (steps 7-11). Last, PST returns S.

The worst case time complexity of PH is dominated by ?nding candidates for lost and ghost, among the obtained frequent patterns. Thus, PH needs O(|S| ? |FS,sup| ? |f |) time, where |S| is the size of the original dataset, |FS,sup| is the number of obtained frequent patterns, and |f | is the average length of the obtained frequent patterns.



V. RELATED WORK  Our work follows the knowledge hiding direction of priva- cy-preserving data mining, whose goal is to prevent the dis- covery of sensitive patterns through mining. Most research in this area focuses on hiding itemsets, and association or classi?cation rules [4], [15]. Unlike these works, we consider sequential patterns that are more challenging to hide than itemsets, due to their complex semantics.

Abul et al. [1] introduced the problem of sequential pattern hiding and proposed a deletion-based method that minimally affects the support of nonsensitive patterns. Their method does not focus on preventing side-effects and may fail to construct high-quality solutions [8]. Gkoulalas-Divanis et al. [8] devel- oped Side-effect Based Sequence Hiding (SBSH), an algorithm that addresses these limitations. SBSH identi?es symbols to delete, by performing search on graphs that represent match- ings between sensitive patterns and transactions. However, due to the high computational cost of graph search, SBSH searches only a small part of the graph and does not minimize the distortion needed to avoid side-effects. Besides employing permutation, PH addresses the shortcomings of SBSH to better preserve data utility. Speci?cally, it avoids the expensive graph search, while incurring minimal side-effects and distortion by: (I) favoring the sanitization of transactions that incur a small number of side-effects, through the use of PST, and (II) replacing sensitive patterns with permutations that prevent side-effects and minimize Cayley distance, through the use of PDPG and RR-PDPG.

An orthogonal direction of privacy-preserving data mining is anonymization, which aims to prevent the disclosure of individuals? private or sensitive information. Anonymization approaches for sequential data have been considered in [5], [14]. These approaches cannot be applied to solve our problem due to their different privacy and utility objectives.

Dataset |S| |A| Avg. |s(i)| minSup MSN 989818 17 5.7 3711,4949,9898,19796 BMS 59602 497 2.5 59 TRUCKS 273 100 20.1 20,40,50,60,67,80  TABLE I: Characteristics of datasets and minSup values

VI. EXPERIMENTS  We compare PH against SBSH [8], the state-of-the-art method in terms of data utility and ef?ciency. We capture data utility by considering sequence mining, frequent itemset  mining, as well the difference between the distribution of the support of items (respectively, frequent itemsets) in the original and the sanitized data. Sequential patterns were mined using Pre?xSpan [16], which was applied with differentminSup and to a varying number of sequences of different lengths. We use the notation n  l to refer to the hiding of n sensitive patterns  of length l. Three benchmark datasets, MSNBC (MSNBC) from http://?mi.cs.helsinki.?, BMS-WebView-1 (BMS) [21], and TRUCKS (TRU) from http://www.chorochronos.org, have been used, as in [1], [8]. Table I shows the characteristics of these datasets and the minSup thresholds used to mine and sanitize them (default values appear in bold). All experiments ran on an Intel Xeon at 2.4 GHz with 12GB of RAM.

Data utility We ?rst demonstrate that PH permits more accurate sequential pattern mining than SBSH. Fig. 5 shows the percentage of side-effects incurred when sets of patterns of varying length are hidden. As can be seen, the side-effects for PH are fewer by 21%, 24% and 28% on average, in BMS, TRU, and MSNBC, respectively. Furthermore, PH incurred very few or no ghosts (less than 1.5% in TRU and MSNBC, when many long patterns were hidden, and 0% in BMS). This veri?es the effectiveness of PDPG.

Next, we demonstrate that PH produces signi?cantly more useful data than SBSH for tasks beyond sequential pattern mining. In Fig. 6, it can be seen that PH preserved at least 77% more frequent itemsets than SBSH (on average). On the other hand, up to 20%, 33%, and 13% of frequent itemsets are lost when SBSH is applied to BMS, TRU, and MSNBC.

Interestingly, the percentage of frequent itemsets lost by PH is low, even when there are many sensitive patterns of length 3, which have a small number of available permutations, and it becomes negligible for longer patterns.

In addition, PH allows more accurate estimation of the support of items and frequent itemsets, which is important in various tasks. This can be seen in Fig. 7, which shows the KL- divergence scores for PH and SBSH. KL-divergence measures support estimation accuracy, based on the difference between the distribution of the support of items (or frequent itemsets) in the original and sanitized data [12]. Note that PH retained the support distribution of items and frequent items in BMS, and achieved at least 10.5 and up 42 times lower KL-divergence scores than SBSH. Similar results were obtained for frequent itemsets, mined at the same support threshold as in the previous experiments, and are shown in Fig. 8. The good performance of PH in the experiments of Figs. 6 and 7 highlights the bene?t of permutation. Observe in Fig. 9(a) that SBSH deleted a small percentage of items to sanitize BMS, but this had a negative impact on utility. On the contrary, PH deleted no items.

Then, we measured utility as a function of minSup (the same threshold was used in pattern mining and hiding). Fig.

9(b) shows the KL-divergence scores for items and frequent items, and Fig. 9(c) for frequent itemsets, when minSup varies. Note that, unlike SBSH, PH preserved the support of items and frequent items, as well as all frequent itemsets.

Similar results were observed for BMS and MSNBC (omitted).

Last, we measured the impact of forbidden patterns on data utility. Fig. 9(d) shows the side-effects incurred, when 50 sensitive patterns of length 5 were hidden, and a varying     (a) (b) (c)  Fig. 5: Side-effects (%) for (a)BMS, (b)TRU, and (c)MSNBC (ghosts are shown in yellow).

(a) (b) (c) Fig. 6: Lost frequent itemsets (%) for (a)BMS, (b)TRU, and (c)MSNBC.

(a) (b) (c) Fig. 7: KL-divergence for items and frequent items in (a)BMS, (b)TRU, and (c)MSNBC.

(a) (b) (c) Fig. 8: KL-divergence for frequent itemsets in (a)BMS, (b)TRU, and (c)MSNBC.

(a) (b) (c) (d) Fig. 9: (a) Deleted items (%) in BMS. KL-divergence for (b) items and frequent items, (c) frequent itemsets in TRU vs. minSup.

(d) Side-effects (%) for TRU when 10%, 20%, and 30% of patterns with length 2, 3, or 4 are forbidden (ghosts are shown in yellow).

percentage of their randomly selected permutations with length in [2, 4] are forbidden. PH incurred a small number of side- effects, even in demanding scenarios, in which many short patterns are forbidden (i.e., few permutations are possible).

Furthermore, as shown in Fig. 10(a), the percentage of lost frequent itemsets for PH is lower than that for SBSH.

Ef?ciency Experiments with runtime are reported in Figs.

10(b), 10(c), and 10(d). PH was slower than SBSH, due to the signi?cantly larger problem space it considers. However, PH scaled linearly with the number of sensitive patterns and took less than 10 seconds. The results for runtime as a function of minSup, are shown in Fig. 10(d). Both algorithms needed     (a) (b) (c) (d) Fig. 10: (a) Lost freq. itemsets (%) for TRU with 10% forbidden patterns. Runtime (sec) for (b) BMS, (c)TRU and (d)TRU vs. minSup.

more time when minSup was smaller, because there are more nonsensitive patterns that need to be dealt with, and SBSH was 2.3 times faster on average. Moreover, the overhead of PH for dealing with forbidden patterns was minimal (e.g., in the order of milliseconds in the experiments in Figs. 9(d) and 10(a)).



VII. PREVENTING BACKGROUND KNOWLEDGE ATTACKS  Our work follows the most common, minimum harm approach [9], [10], which dictates hiding sensitive patterns below a data owner speci?ed minSup threshold to help data utility. This approach makes the reasonable assumption that sensitive patterns are not exposed when their support is below minSup, as they model unexpected knowledge that is not discoverable from external datasets, commonly considered in microdata anonymization [12]. Nevertheless, it is worth noting that our permutation-based framework can offer protection against attackers who may (I) discover a sensitive pattern s when its support is lower, but ?close? to minSup, and there are ?few? nonsensitive patterns with the same support as s, and/or (II) exclude certain permutations of s from consideration.

In fact, due to the factorial growth of the number of permutations of a sensitive pattern s with its length, protection against both classes of attackers can be provided by generating a ?suf?cient? number of permutations by PDPG and using them to replace s. More concretely, a maximum uncertainty approach that hides s using the maximum possible number of different permutations (i.e., min(sup(s) ? 1, |s|! ? 1)) can be adopted. This approach offers stronger privacy but lower utility than the minimum harm approach, because it aims at minimizing the probability of discovering s among nonsensitive, infrequent patterns (the permutations of s are among these patterns), as well as the probability of inferring s by excluding some of its permutations. Alternatively, a more ?exible, bounded uncertainty approach can be employed to hide s by replacing it with at least c different permutations, all having support at most ?, where c ? 1 and ? < minSup are data owner speci?ed parameters. This approach takes a middle line between the maximum uncertainty and minimum harm approach, which are mapped to it by setting c to its maximum value and 1, respectively. Both approaches can be easily implemented by imposing restrictions to PDPG.



VIII. CONCLUSIONS  Employing deletion to hide sensitive sequential patterns reduces the usefulness of data in sequential pattern mining and tasks based on itemset properties. In response, we proposed a novel, permutation-based approach and the PH algorithm for sanitizing sequence data with minimal side-effects and  distortion. Our experiments veri?ed that PH permits more effective data analysis than the state-of-the-art method.



IX. ACKNOWLEDGMENTS  Robert Gwadera was supported by the EU project Ope- nIoT (ICT 287305). Grigorios Loukides was supported by a Research Fellowship from the Royal Academy of Engineering.

