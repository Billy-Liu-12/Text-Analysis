Exploiting Out-of-Vocabulary Words for  Out-of-Domain Detection in Dialog Systems

Abstract? Multi-domain dialog systems often encounter user requests for out-of-domain (OOD) service. This paper focuses on detecting these requests. The proposed OOD detection method is included in a multi-domain detection component naturally. This component consists of multiple in-domain verifiers: an in-domain verifier accepts a user utterance when it belongs to the domain and rejects the utterance otherwise. To detect OOD requests without using an actual OOD corpus, the in-domain verifiers exploit the occurrence of out-of-vocabulary words. In experiments, the proposed OOD detection method was more accurate than three baseline methods.

Keywords? binary classification, multi-domain detection, training data

I. INTRODUCTION Multi-domain dialog systems should detect the intended  domain from a user request and provide dialog service to that domain. Most research on domain detection has adopted two assumptions: (1) all user utterances belong to at least one domain (the in-domain assumption), and (2) all user utterances belong to at most one domain (the single-domain assumption).

Under these two assumptions, traditional multi-domain dialog systems detect only one domain and provide dialog service to it [1,3,8]. However, assumption (1) is unrealistic because most users do not know the exact boundaries of the domains, so utterances can be out-of-domain (OOD); and assumption (2) is unrealistic because domains may not have clear boundaries; so utterances can belong to more than one domain. For example, a multi-domain dialog that serves TV Program Guide, VOD Guide, and TV Device Control domains would detect no domain from a user utterance ?my brother is going to party tonight?, but both the TV Program Guide and VOD Guide domains from a user utterance ?Do you have animation??  No previous work has focused on eliminating the need for both of these unrealistic assumptions at the same time. Ryu et al. [10] proposed a multi-domain selection framework that first detects one or more candidate domains from a user utterance at one time, and then determines one or more final domains by considering dialog history, but this method adopts the in-domain assumption. Lane et al. [6] proposes an OOD  This work was supported by the IT R&D program of MSIP/KEIT. [10044508, Development of Non-Symbolic Approach-based Human-Like Self-Taught Learning Intelligence Technology]  detection method that does not require an actual OOD corpus, but this method adopts the single-domain assumption, and does not consider out-of-vocabulary (OOV) words during OOD detection.

In this paper, we propose an OOD detection method that uses neither the in-domain assumption nor the single-domain assumption. We applied our OOD detection method to the multi-domain detection (MDD) component, which consists of N in-domain verifiers, where N is the number of domains. The in-domain verifier of a domain should accept user utterances that belong to that domain and reject all other utterances; i.e. all in-domain verifiers should reject OOD utterances.

To obtain reliable training data for those in-domain verifiers, we exploited a manually-designed formal description of capabilities of domains [10]. We do this because an in-domain verifier cannot simply use the target domain?s corpus as positive examples and the remaining domains? corpora as negative examples directly. The remaining domains? corpora contain utterances that can belong to the target domain, and this overlap of corpora may cause the in-domain verifier to make frequent false negative errors. Manually annotating positive and negative labels can solve this problem, but the work is labor-intensive and time-consuming; furthermore, when a new domain is added, the work should be repeated on all existing corpora.

To improve OOD detection based on positive examples and negative examples obtained using the above process, we also defined a new method that exploits the existence of OOV words. Given a user utterance, we replaced each OOV word with a predefined OOV tag and each named entity with its type label. Traditional classifiers do not have OOV words in their feature space, and therefore cannot consider them. Although the occurrence of OOV words can be ignored in most classification problems, it is obvious evidence that an utterance is OOD. Furthermore, we automatically generated a virtual OOD corpus as additional negative examples, because we assumed that we did not have an actual OOD corpus [6].

The remainder of the paper is organized as follows: Section II describes the method of obtaining proper positive examples and negative examples. Section III introduces the proposed OOD detection method in detail. Section IV describes and analyzes the materials used in the experiments. Section V demonstrates the experimental design and results. Finally,     Section VI discusses our work, draws conclusions, and provides a suggestion for future work.



II. MULTI-DOMAIN LABEL ANNOTATION To assign positive or negative labels to an utterance, we  automatically assigned a multi-domain label to the utterance.

For example, when an utterance is annotated as a multi-domain that is in the intersection of domains X and Y, the utterance is positive example for both X and Y. To annotate those multi-domain labels, we referred to a hierarchical domain model (HDM) that considers already-annotated dialog acts and named entities of each utterance in a corpus. An HDM is a manually-designed formal description of capabilities of domains; it consists of base domains and virtually-expanded domains.

For example, the HDM of a Smart TV dialog system have three base domains: TV Program Guide, Video-on-Demand (VOD) Guide, and TV Device Control1 (Fig. 1). In addition, the HDM have three virtually-expanded domains: Common, TV Channel Control, and Video Content Guide. When an utterance ?Do you have animation?? includes a search_program dialog act and a genre named entity, the utterance can be handled in Video Content Guide, TV Program Guide, and VOD Guide. So the multi-domain label of the utterance is Video Content Guide, because Video Content Guide is the most general domain among the three domains. As a result, the utterance is used as a positive example in the in-domain verifiers of TV Program Guide and VOD Guide.



III. EXPLOITING OUT-OF-VOCABULARY WORD The existence of OOV words is important evidence that an  utterance is OOD. However, named entities including person names and movie titles often contain OOV words; thus OOV words can occur frequently in both in-domain and OOD utterances. Therefore, inspired by the lexico-semantic pattern  1 Some researchers argue that these three domains can be designed as just one domain; this makes sense in text-classification perspective. However, to provide real service in end-user applications, multiple detailed domains should be defined, each with its minimal service boundaries. For example, in contrast to VODs, TV programs have start time and end time properties, and can be recorded. So we designed three instinct domains for Smart TV.

(LSP) [7], we proposed OOV-LSP, which is composed of lexical entries, named entity labels, and OOV tags. We first replaced each named entity with its type label as ?@<TYPE>?, then replace each OOV word with the OOV tag as ?$OOV?.

We regarded that a word is not OOV when it occurs more than once in any LSP vocabulary dictionary. For example, the in-domain verifier of TV Program Guide first converted an utterance ?my brother is going to party tonight? into an LSP ?my brother is going to party @TIME?, then converted the LSP into an OOV-LSP ?my $OOV is going to $OOV @TIME? because the words ?brother? and ?party? are OOV in all LSP vocabulary dictionaries (Fig. 2). As a result, the in-domain verifier can reject the utterance by identifying the existence of OOV words. The remainder of the paper introduces the method of in-domain verifier training and in-domain verification exploiting OOV words.

A. In-Domain Verifier Training 1) LSP Vocabulary dictionary construction. We construct  a positive example LSP vocabulary dictionary (LVDP) from positive examples and negative example LSP vocabulary dictionary (LVDN) from negative examples (Fig. 3). Based on our definition of OOV, LSP vocabulary dictionaries have named entity labels and lexical words that occur more than once in the source example.

2) Training data converting. We obtained final training data by converting original training data to OOV-LSPs automatically (Fig. 4). We converted original positive examples and negative examples into OOV-LSPs using both LVDP and LVDN. In addition, we converted negative examples into OOV-LSPs using only LVDP. For example, we converted original negative example utterance ?change the input source to tv? into an OOV-LSP ?change the $OOV $OOV to tv? because the words ?input? and ?source? are OOV in LVDP. We used those OOV-LSPs as the virtual OOD corpus because we  Fig. 3. LSP vocabulary dictionary construction process.

LSP Vocabulary Dictionary  Construction  LSP Vocabulary Dictionary of Positive Example  LSP Vocabulary Dictionary  Construction  LSP Vocabulary Dictionary of Negative Example  @GENRE @TIME @TITLE are is on tv what when ?  @GENRE are change input newest source the to tv what ?  Original Positive Example 1. when is /family guy/TITLE/  on tv 2. what /animations/GENRE/  are on /now/TIME 3. ?  1. change the input source to tv  2. what are the newest /animation/GENRE/  3. ?  Original Negative Example  Fig. 2. The OOV-LSP Generation Process.

My brother is going to party tonight!

my brother is going to party tonight  my $OOV is going to $OOV @TIME  my brother is going to party @TIME  Normalization  LSP Converting  OOV-LSP Converting  Fig. 1. Multi-Domain Label Annotation using a Hierarchical Domain Model that Consists of Three Base Domains: TV Program Guide, VOD Guide, TV  Device Control.

Common  Video Content Guide  VOD GuideTV Program GuideTV Device Control  TV Channel Control  change_volume(amount) next_channel() previous_channel()  response_yes() response_no()  play_program(genre, time, title) search_program(channel_name,  channel_no, genre, time, title)  play_program(genre, title) search_program(genre, title)  play_program(genre, released_year, title) search_program(genre, released_year, title)  change_channel(channel- name, channel-no)  Multi-Domain Label Annotation  User Utterance Multi-Domain Label  Domain Hierarchy  ?Do you have animation?? - search_program(  GENRE = animation) video_content_guide     do not have an actual OOD corpus.

3) Feature extraction and model optimization. We trained  each in-domain verifier by using its final positive examples and final negative examples. We used the unigram features from source examples and optimized the model of a binary classifier.

B. In-Domain Verification To extract OOV-LSP feature from a user utterance, an  in-domain verifier first recognized named entities in the utterance. In named entity recognition, we used both a sequence labeling classifier and a precompiled named entity list as in [1]. Then, the in-domain verifier considered the recognized named entities of the utterance to generate an LSP from the utterance. Next, the in-domain verifier consulted LVDP and LVDN to generate an OOV-LSP from the LSP. After this stage, the in-domain verifier accepted or rejected the OOV-LSP according to the trained the binary classifier of the in-domain verifier.



IV. MATERIALS We collected in-domain corpora for a Smart TV and an  OOD corpus from experts (Table I). The in-domain corpora consisted of 2725 utterances that belonged to TV Program Guide, VOD Guide, or TV Device Control domains. OOD corpus consisted of 1503 utterances that belong to various domains excluding the three target domains. All utterances in in-domain corpora belonged to one of six multi-domain labels based on HDM (Table II). These multi-domain labels were manually annotated by experts and used for evaluation only.

To validate the prerequisite that OOD utterances contain more OOV words on average than do in-domain utterances, we used five-fold cross-validation to measure the OOV ratio of each corpus based on each LSP vocabulary dictionary (Table III). The results showed that OOD corpus contained proportionally more OOV words (0.449) than the target domain?s corpus (0.029) and the remaining domains? corpora (0.257) in all vocabulary dictionaries. We also used five-fold cross-validation with these corpora to evaluate the MDD performance.



V. RESULTS  A. Experimental Design 1) Statistical model. To implement in-domain verifiers, we  used Maximum Entropy (ME) classifier [9]. To implement named entity recognizers, we used linear chain Conditional Random Fields (CRFs) [5].

2) Evaluation metrics. We evaluated both MDD and OOD detection. To measure MDD performance, we used precision, recall, and F1 score as in [10]. To measure OOD detection performance, we used false acceptance rate (FAR), false rejection rate (FRR), and equal error rate (EER) as in [6]. We also evaluated named entity recognition performance by measuring precision, recall, and F1 score. This is because the results of named entity recognition are exploited in the LSP conversion process.

3) Baseline methods. To validate the proposed method (?Word+HDM+OOV+LSP?), we implemented three baseline methods. (1) As the first baseline (?Word?), we trained in-domain verifiers by using the target domain?s corpus as positive examples and the remaining domainss corpora as negative examples. (2) As the second baseline (?Word+HDM?), we obtained more reliable training data by consulting HDM to annotate the multi-domain labels on corpora. Afterwards, we used the new data to train in-domain verifiers. (3) As the third baseline (?Word+HDM+OOV?), we extended the second baseline to exploit OOV word features without considering named entities.

Fig. 4. Training data conversion process. LVDP and LVDN are the LSP vocabulary dictionaries of positive examples and negative examples  respectively.

Original Positive Examples 1. when is /family guy/TITLE/  on tv 2. what /animations/GENRE/  are on /now/TIME 3. ?  Final Positive Examples 1. when is @TITLE on tv 2. what @GENRE are on  @TIME 3. ?  OOV-LSP Conversion  LVDP LVDN  1. change the input source to tv  2. what are the newest /animation/GENRE/  3. ?  Original Negative Examples  OOV-LSP Conversion  1. change the input source to tv  2. what are the newest @GENRE  3. ?  Final Negative Examples  LVDP LVDN  OOV-LSP Converting  1. change the $OOV $OOV to tv  2. what are the $OOV @GENRE  3. ?  TABLE I BASIC PROPERTIES OF COLLECTED CORPORA  Domain # Utterances # Vocabulary # LSP Vocab.

TV Program Guide 1432 892 371 VOD Guide 814 728 291 TV Device Control 479 221 177 Out-of-Domain 1503 1452 1452 Average 1057 823 573  TABLE II BASIC PROPERTIES OF MULTI-DOMAIN LABEL ANNOTATED CORPORA  Domain # Utterances # Vocabulary # LSP Vocab.

TV Program Guide 770 646 285 VOD Guide 215 228 162 TV Device Control 422 188 156 Video Content Guide 942 840 289 TV Channel Control 324 229 127 Common 52 50 50 Average 454 364 178  TABLE III OOV RATIO OF CORPUS BASED ON EACH LSP VOCABULARY DICTIONARY  Domain Target Remaining OOD TV Program Guide 0.021 0.106 0.378 VOD Guide 0.028 0.245 0.413 TV Device Control 0.039 0.420 0.556 Average 0.029 0.257 0.449     B. Experimental Results (1) Results obtained using the first baseline showed that  na?ve domain detection component had low accuracy in MDD (Tables IV, V).

(2) Results obtained using the second baseline showed that obtaining reliable training data reduced false negative errors dramatically (Tables IV, V).

(3) Results obtained using the third baseline showed that the existence of OOV words is obvious evidence of OOD (Tables IV, V).

(4) Results obtained using the proposed method showed that replacing named entity with their labels improved the effectiveness of exploiting OOV words (Tables IV, V). In MDD, precision increased from 0.629 to 0.862, recall increased from 0.641 to 0.890, and F1 score increased from 0.635 to 0.876. In OOD detection, FAR decreased from 0.138 to 0.066, FRR decreased from 0.133 to 0.036, EER decreased from 0.135 to 0.043. These results were obtained when the average named entity recognition F1 score was 0.888 (Table VI): The F1 scores were 0.880 for TV Program Guide domain, 0.919 for VOD Guide domain, and 0.864 for TV Device Control domain.



VI. CONCLUSION In this paper, to eliminate the need for both the in-domain  assumption and the single-domain assumption, we proposed an OOD detection method that exploits OOV words. To our knowledge, this is the first work that circumvents both assumptions at the same time. The MDD component that includes the proposed OOD detection method obtained higher accuracy than three baseline methods.

The proposed method yielded good results in an MDD that has a small number of domains. However, HDM of large-scale multi-domain dialog systems would be too complex and difficult for humans to design; i.e. obtain reliable training data from original training data by consulting HDM is difficult in those dialog systems. Therefore, we plan to apply semi-supervised learning techniques to obtain reliable training data.

We also plan to extend our work to joint modeling of natural language understanding (NLU). Joint modeling of NLU simultaneously models all or a subset of NLU processes: domain detection, dialog act classification, and named entity recognition [2,4,11]. However, previous work on this approach has used both the in-domain assumption and the single-domain assumption; we expect that applying joint modeling to our work can improve the accuracy of overall NLU processes.

