A Novel Multivariate Discretization Method for Mining Association Rules

Abstract?Data mining aims at discovering useful patterns in large datasets. In this paper, we present a novel multivariate discretization method for finding association patterns based on clustering and genetic algorithm. This method consists of two steps. Firstly we adopt a density-based clustering technique to identify the regions that possibly hide the interesting patterns from data space. Confined to the data in these regions, we then develop a genetic algorithm to simultaneously discretize multi-attributes according to entropy criterion. The effectiveness of the proposed method is demonstrated with the experiment on a real data set.

Keywords-association rule; discretization; clustering; genetic algorithm;  data mining

I.  INTRODUCTION Association rule was first presented and used to find  relationships between attributes [1, 2, 3]. The general form of association rules is like: C1 C2, where C1 and C2 are called antecedence and consequence, respectively. Both of them are conjunctions over conditions. The form of a condition is like Aj=vj (for categorial or numeric attribute) or Aj [lj, uj] (for numeric attribute), where Aj is an attribute and, vj, lj and uj are the values in the domain of Aj.

The quality of a rule is often described by support and confidence. In this paper, we denote them with sup and conf, respectively. The support of the rule equals the support of condition C1 C2. The support of a condition is the ratio of the number of tuples satisfying it to the number of the whole tuples. The confidence of the rule is the ratio of the support of C1 C2 to the support of C1. Mining association rules is to find all the rules satisfying the minimum support and confidence thresholds.

On the research of association rules, the usage of discrete values can bring us some benefits. Firstly, discrete values are closer to the knowledge-level representation than continuous ones. For both users and experts, the association rules represented by discrete features are easier to understand, use and explain. Secondly, the discrete data are more reduced and simplified. As a result, the discovery procedure of association rules using discrete features is more efficient and effective. In addition to these advantages, there are necessities for us to use discrete values in association rules. As proposed in [1], the rules are composed of frequent itemsets that are identified by counting the items. If we assume that the items consist of only continuous values without discretization, there will be numerous items because of the infinite number of continuous values. And the search space (the number of itemsets) will explode with the exponential number of items. Even though all itemsets are counted, most of their  frequencies are too low to satisfy the minimum support threshold because most of continuous values appear only few times. That is also the reason that most existing efficient algorithms can only deal with discrete values.

In this work, we propose an algorithm (MVD-CG) based on the idea of transforming the problem of unsupervised discretization in association rules into a supervised problem. Within the support-confidence framework, we find that a rule with high confidence usually makes the corresponding data space have a high density. Thus, we firstly use a density-based clustering technique to identify the regions with high densities.

Regarding every region as a class, we then develop a genetic algorithm to simultaneously discretize multi- attributes according to entropy criterion.

The remainder of this paper is organized as follows. In section II, we give some related work on discretization for mining association rules. In section III, we propose our MVD-CG algorithm. In section IV, we test the performance of our algorithm on a real dataset by comparing it with MVD. We conclude our study in section V.



II. RELATED WORK The past few decades have seen many researches on  discretization for mining association rules. Ludl and Widmer [4] investigated the problem of discretizing numeric variables for unsupervised learning algorithms such as association rule miners. They determined the cut points of the target variable by projecting the values of other variables on it. Stephen Bay [5] argued that a major disadvantage of [4] was that they only considered how pairs of variables interact and didn't examine higher-order combinations. To handle this problem, proposed a multivariate discretization (MVD) algorithm. After he had split the range of attributes into depth-equal or width-equal basic intervals, he combined the adjacent similar intervals determined by STUCCO [6]. However, MVD also has some disadvantages. The first is that the tendency of combing the intervals with the lowest merged support is not always appropriate. A simple example of this is to consider the case where we are examining the heights of three groups, {5ft, 5ft 4in, 6ft}, and we are interested in height difference of 10 inches or more. We assume that the supports of three groups are 2, 1 and 1, respectively. Then the second group will be merged with the third into a single group. It results in an average height of 5ft 8in that is 8 inches from the first group. And it will be further emerged with the first group. In contrast, If the second group is first merged with the first group, we get an average height of 5ft 1.3in that is 10.7 inches far away  2009 Asia-Pacific Conference on Information Processing  DOI 10.1109/APCIP.2009.102   2009 Asia-Pacific Conference on Information Processing  DOI 10.1109/APCIP.2009.102     from the third group. The second disadvantage is that MVD really discretizes the attributes one at a time instead of discretizing them simultaneously. Apart from the methods above, newly proposed methods can be found in [7, 8].



III. PROPOSED MULTIVARIATE DISCRETIZATION METHOD  In this section, we will propose our MVD-CG (multivariate discretization based on density-based clustering and genetic algorithm) algorithm. We list its pseudocode as follows.

Algorithm MVD-CG Input data, , ?, minpts, popsize, genn, nb, selectn,  crossp, mutp Output interv   Step   1.  data = Normalize(data); Step   2.  clus_ordering = OPTICS(data, , minpts); Step 3. datawithclus = Cluster(clus ordering, ?,  minpts); // ?< Step  4.  interv = MVD_G(datawithclus, popsize,  genn, nb, selectn, crossp, mutp).

Because we will use some distance metric in OPTICS  [9], we need to normalize their ranges into [0 1] before performing MVD-CG in order to balance the role of every attribute in distance metric. That is what the step 1 in algorithm MVD-CG will do. The major part of MVD-CG is divided into two steps: finding high-density regions (HDRs) and determining the cut points with a genetic algorithm based on HDRs. We will examine them in detail in subsections A and B.

A. Finding High-Density Regions The first task mentioned in the end of last paragraph  will be completed by a density-based algorithm called OPTICS [9] and clustering on its results. That is why the step 2 and 3 exist in MVD-CG, where two parameters are used,  and minpts. The former is the neighborhood distance and the latter is the minimum number of points in the neighborhood. As regards the settings of their values, the former is set with the value of r of the formula 1[9], that is,    ( 1) 2 ,  DS d  d  dVolume k r  N ?  ? ? ? + =  ? (1)   where VolumeDS denotes the volume of a d-dimensional data space, DS.  It contains N data points. k equals minpts and ?  denotes the Gamma-function. OPTICS creates an augmented ordering of the data points instead of explicitly clustering data points. Based on the cluster-ordering, we can form clusters on data with another parameter 0< ?< .

It is the clusters that give us the class information referred by the discretization procedure.

B. Discretization with Genetic Algorithm Now we come to the step 4 of MVD-G. In this  subsection, we will optimize the discretization by a genetic algorithm in terms of two criterions: (1) choosing the cut points that finely separate each HDR from the others; (2) preventing the cut points from separating the data evenly. As usual, our genetic algorithm [10] involves the following aspects: individual coding, operators, fitness function and population initialization.

Individual coding: We adopt binary coding, in which every binary bit is set to be 1/0 to represent the presence/absence of a potential cut point. Every potential cut point corresponds to a point separating two adjacent basic intervals that are formed by depth-equal or width- equal split. If there are m attributes to be discretized and we split the attribute Ai(1 ? i ? m) into ni  basic intervals, a chromosome has 1  m i in=  bits.

Selection operator: We adopt tournament [10] selection operator which consists of randomly choosing k individuals from the population and let them play a tournament.

Crossover operator: We use one-point crossover [10]. Actually, we set randomly a crossover point for every attribute of an individual.

Mutation operator: Every bit is reversed with a particular probability.

Fitness function: Let D be a valid discretization scheme: {d1, d2, ?, dm}, where di ={di,1, ?, di,j, ?} ? dom(Ai) is a set of cut points for the attribute Ai. The elements in di are sorted on the ascending order. Based on D, we get a set of consecutive interval set, R: {r1, r2, ?, rm}, where ri is a set of consecutive intervals for the  attribute Ai. Formally, , , 1  , , , 1  [ ) | | [ ] | |  i j i j i i j  i j i j i  d d if j r r  d d if j r +  +  < =  = ,  where |ri| = |di | - 1 . Then we can define a Cartesian set V over R, 1  m i iV r== ? . Any element vj  of V corresponds  to a region formed by cut points in the data space. If data are clustered into c classes, we let countk(j) (k=1, ..., c) denote the number of data points belonging to the kth cluster in vj . Then countk, count(j) and count denote the number of data points belonging to the kth class, the number of data points falling into vj and the number of all data points, respectively. Again, let pk(j) (=countk(j)/count(j)), pk(= countk/count) and p(j)(= count(j)/count) be the probability of the presence of the kth class in the jth region, the probability of the presence of the kth class and the probability of the presence of the jth region within V. Having these denotations, we can evaluate a discretization scheme with the next formula based on the entropy gain ratio [7] in information theory:   ( ) / inf ,f Ent I split o= ?                   (2)  where 1 log  c k k kEnt p p== ? is the entropy of c  classes and | | ( ) ( )1 V j j jI p Ent== , where  ( ) ( ) ( ) 1 log  j c j j k k kEnt p p== ? . splitinfo is the split  information under D and computed by     | | ( ) ( ) 1inf log  V j j jsplit o p p== ? . Considering the  two criterions mentioned in the beginning of this subsection, the numerator in formula 2 implies how finely a particular discretization separates the clusters and the denominator prevents the presence of even split.

Population initialization: We initialize the population based on a candidate chromosome that is composed of bounds of all classes. An individual is produce by a logical And operation on a random binary number and the candidate chromosome. For example, we get four basic intervals, {a1, ?, a4} for attribute A and three intervals, {b1, ?, b3}, for attribute B. The data are clustered into two classes. The range of the first class on A is from a1 to a4 and that of the second class on A is from a2 to a4. Then the part of the candidate on A is 1 1 0 1. We can get the part on B, 0 1 0, in a similar way. We get the candidate chromosome, 1 1 0 1 0 1 0. If the random number is 1 0 0 1 1 1 0, we get the individual, 1 0 0 1 0 1 0(1 1 0 1 0 1 0 And 1 0 0 1 1 1 0).

Algorithm MVD-G Input: datawithclus, popsize, genn, nb, selectn, crossp,  mutp Output: interv   1. basinterv=GetBasicInterv(datawithclus, nb); 2. pop=Initialize(popsize, nb); 3. for i=1:genn 4.    for j=1:popsize 5.       c[i]=CountforCombInterv(pop[i], datawithclus); 6.       f[i]=GetFitness(pop[i], c[i]); 7.    end//for j 8.  nextpop[1..selectn]=SelectTop(pop, f, selectn); 9.    for j=selectn+1:popsize 10.     [p1, p2]=SelectPar(pop, f); 11.     [chd1, chd2]=Crossover(p1, p2, crossp); 12.     nextpop[j]=Bestof(chd1, chd2); 13.  end//for j 14.  pop=Mutation(nextpop, mutp); 15.end//for i  The pseudocode of MVD-G is list above. At line 1, the  function GetBasicInterv is used to split the ranges of the attributes to be discretized into depth-equal or width-equal intervals with nb denoting the number of basic intervals.

At line 2, the function Initialize is used to initialize the population with the parameter popsize denoting the size of it. The population will evolve for genn generations.

Before we calculate the fitness for every individual in population at line 6, we have to count countk(j) for very class in all regions at line 5. Based the current generation, we generate the next one in two ways. One is to take the top selectn individuals from the current population sorted descendingly by fitness value at line 8. The other way is to produce offsprings by applying crossover operator with probability crossp on two parent individuals that are selected by SelectPar function from the current population. Every parent individual is selected using roulette-wheel method [10]. Then we select the one with the best fitness from parents and children into the next  generation. At line 14, the mutation operator is applied with probability mutp on the next generation.



IV. EXPERIMENTS In this section, we perform MVD and MVD-CG on  IPUMS [12] dataset and compare the results from different aspects. This data set is available on http://www.ipums.umn.edu/. In the experiment, we only select three continuous attributes age, educrec and inctot and we sample 3,000 out of total 88,443 instances from IPUMS dataset. To evaluate the performance of MVD with different parameters, we run it four times on IPUMS with = 0.01, 0.03, 0.05 and 0.07, respectively.

TABLE  I. THE COMPARISION OF MVD-CG AND MVD    Agorithm # of  frequent itemsets  # of rules the average  of support  the average of  confidence EMVD-  BDC ( ?=0.07)  27 20 27.57% 98.58%  MVD ( =0.01) 31 3 5.57% 98.1%  MVD ( =0.03) 39 20 14.19% 90.55%  MVD ( =0.05) 37 19 15.35% 93.93%  MVD ( =0.07) 17 12 31.38% 93.65%   Using the discretization schemes got by MVD and  MVD-CG, we can examine the performances of two algorithms in terms of rules derived from the discretized dataset. We perform Apriori algorithm [1] to get the association rules with the values of the minimum support and confidence threshold being 5% and 80%, respectively.

We try to compare MVD-CG with MVD on the quantity and quality of rules. The relevant results are list in TABLE  I. Firstly, we examine the quantity of rules. It can be noticed from column 3 that the rules MVD-CG produced are more or as many as those MVD did, although the former led to less frequent itemsets than most of MVD experiments did. It implies that MVD produced more rules whose confidence values are below the threshold. This difference is not out of our expectations because MVD-CG discretizes variables based on the HDRs where some patterns with relatively high confidences are hidden. Secondly, let's examine the quality of rules. We list the average of support and confidence of all rules the top support and the top confidence in column 4 and 5. Obviously, MVD-CG gets much higher average confidence than MVD.



V. CONCLUSIONS In this work, we present a multivariate discretization  algorithm specially for producing association rules. The idea is transforming an unsupervised discretization task to a supervised one at first and then finding the optimized cut points. To test the performance of our algorithm, we do experiments on a real dataset. We compare the quantity and quality of rules formed in terms of the cut points produced by these two algorithms. As a result, we argue that the former outperforms the latter on finding association rules with high confidence. Possible     extensions of this work could include applying new distance metric in clustering, as well as making comparisons with other methods on more real datasets.

