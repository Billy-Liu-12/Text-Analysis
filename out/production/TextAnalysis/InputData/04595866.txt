Identifying Frequent Items in P2P Systems Mei Li

Abstract? As peer-to-peer (P2P) systems receive growing ac- ceptance, the need of identifying ?frequent items? in such systems appears in a variety of applications. In this paper, we define the problem of identifying frequent items (IFI) and propose an efficient in-network processing technique, called in-network filtering (netFilter), to address this important fundamental prob- lem. netFilter operates in two phases: 1) candidate filtering: data items are grouped into item groups to obtain aggregates for pruning of infrequent items; and 2) candidate verification: the aggregates for the remaining candidate items are obtained to filter out false frequent items. We address various issues faced in realizing netFilter, including aggregate computation, candidate set optimization, and candidate set materialization. In addition, we analyze the performance of netFilter, derive the optimal setting analytically, and discuss how to achieve the optimal setting in practice. Finally, we validate the effectiveness of netFilter through extensive simulation.



I. INTRODUCTION  Peer-to-peer (P2P) systems, such as BitTorrent [1], Gnutella [2], and Napster [3], have become popular platforms for sharing and exchanging voluminous information among thou- sands or even millions of users. Since there is no centralized administration in P2P systems, peers have to collaborate with each other to perform various tasks including routing, indexing, and searching. Moreover, various system statistics and valuable user access information are distributed amongst peers, making applications that need a global view of such information/statistics particularly difficult to operate1. Since many applications require the statistics for frequently accessed files (e.g., MP3), frequently happened events (e.g., network attacks), frequently issued queries (e.g., keywords), etc., in this paper we focus on developing efficient techniques for identifying frequent items in P2P systems2.

Here we first define the problem of identifying frequent item (IFI) in P2P systems. Assume that a P2P system has N peers and the data set of interest A has n distinct data items (e.g., recording number of downloads of n pop songs).

Peer i (1?i?N ) has a local item set Ai ? A. Each item (x) in Ai has a local value, denoted by vix, indicating the value associated with Item x at Peer i. The global value of Item x, denoted by vx, is the sum of the local values at all the peers in the system, i.e., vx =  ?N i=1 v  i x. Given a threshold value t, IFI  This research was supported in part by National Science Foundation under IIS-0328881, IIS-0534343 and CNS-0626709.

1A music marketing firm may want to find out which MP3 songs have been downloaded more than 10,000 times in the past week.

2Since those events and access patterns are usually logged in data files, we collectively refer them as data items.

identifies the frequent items whose global values are greater than t. IFI can then be formally defined as:  IFI(A, t) = {x|x ? A, vx ? t}.

Essential operations in a variety of P2P applications can  be transformed into the problem of IFI as summarized in Table I. In the table, we summarize these operations, their corresponding applications, and the local item set maintained at a peer (Peer i with 1?i?N ) and the local value associated with an item in transformation to the problem of IFI. All of these applications can be realized by identifying the frequent items with global values greater than a given threshold value.

Although the problem of IFI is prevailing in P2P systems, it is not yet addressed by previous works based on the authors? best knowledge. In this study, we investigate this problem in depth. We assume that peers form an unstructured P2P systems where no global index structure is maintained.

Effectively identifying the frequent items in such unstructured P2P systems is challenging. A naive solution is to collect the global value for each item and then output the items with global values exceeding the threshold as the results. However, the frequent items are usually a small portion of the whole set of items in the system. Therefore, collecting the global value for each item obviously is an overkill.

In this study, we propose an efficient in-network filtering technique called netFilter for identifying frequent items in P2P systems. netFilter operates in two phases: 1) candidate filtering and 2) candidate verification. To perform candidate filtering, items are grouped into disjoint item groups, and the aggregates3 for these item groups are obtained from the system. These aggregates of item groups act as filters to prune majority of unqualified item groups from further considera- tion. A small subset of items is retained as the candidates.

Candidate verification is then invoked to obtain the global values for these candidates to verify whether they truly satisfy the threshold condition. The set of frequent items identified through netFilter is precise in terms of the following two aspects. First, there is no false positives (infrequent items that are incorrectly reported as frequent items) or false negatives (frequent items that are not identified). Second, the reported global values of the frequent items are precise.

3The aggregate for an item or a set of items refers to the combined value for the item(s) from different peers in the system. For instance, the aggregate for Item x (ax) is  ? i?P v  i x (P ? {1, 2, ...,N}). When P = {1, 2, ...,N},  the aggregate for x is also the global value of x. In such cases, we use global value and aggregate interchangeable if the context is clear.

DOI 10.1109/ICDCS.2008.78    DOI 10.1109/ICDCS.2008.78     TABLE I  APPLICATIONS OF IFI  Operations Applications Local item set (at Peer i) Local value (of Item X at Peer i)  Frequent keywords identification Cache management Keywords appearing in the Number of queries (among the queries queries issued by Peer i issued by Peer i) that keyword X appears in  Frequent documents identification Search technique design Documents stored at Peer i Number of replicas for document X maintained at Peer i  Frequently co-occurring keyword Query refinement Pairs of keywords co-occurring Number of queries (among the queries pairs identification in the queries issued by Peer i issued by Peer i) that keyword pair X  co-occur in Popular peers identification Content mirroring Peers that provide satisfactory Number of queries (among the queries  Incentive mechanism results to the queries issued by issued by Peer i) that peer X provides Peer i satisfactory results to  Frequently contacted peer pair Network topology optimization Pairs of source/destination Number of packets (among the packets identification Social relationship analysis addresses that Peer i has seen passing through Peer i) that are exchanged  in the packets passing through it between source and destination pair X Large flow of traffic (to certain Denial of service attack Destination addresses that Peer Size of the flow (among the packets destination) identification detection [5] i has seen in the packets passing through Peer i) that destine  passing through it to address X Frequent byte sequences Internet worm detecting [10] Byte sequences that appear in Number of flows (among all of the identification the traffic passing through flows passing through Peer i)  Peer i containing byte sequence X  h  P3 1   c d e  P1 1   a b d  P21   d f g  2 2 2  a b c d e f g h 1 1 1 3 1 1 1 1   global values  item group aggregates  items  Fig. 1. An illustrative example of netFilter.

The example in Figure 1 illustrates the basic concept behinds netFilter. The system has three peers (P1, P2 and P3). Their local item sets are depicted in the tables besides the peers, where the left and right columns represent the identifiers of the items and their local values, respectively. The threshold value (t) is 3. We assume the eight items are assigned to four item groups as follows: Items a and b are assigned to Item-group 1, and so on. During candidate filtering, we collect an aggregate for each item group. Among the four item groups, only Item-group 2 has its aggregate above the threshold value. Thus, only the two items (Items c and d) belonging to Item-group 2 are retained as the candidates.

During candidate verification, the global values for Items c and d are obtained as 1 and 3, respectively. Thus, Item d is returned as the result.

Three important technical issues need to be addressed in netFilter: 1) aggregate computation, i.e., how to obtain the aggregate for an item or a set of items from all the peers spread in the system; 2) candidate set optimization, i.e., how to obtain a good candidate set with majority of them truly satisfying the threshold condition; 3) candidate set materialization, i.e., how to generate or materialize the candidate set for candidate verification with no peer having a global view of the complete item set in the system. We propose techniques to address these issues. Moreover, to optimize the performance of netFilter, we derive the optimal setting mathematically, and discuss how to achieve the optimal setting in practice. Finally, we evaluate the effectiveness of netFilter through extensive simulations.

The rest of this paper is organized as follows. In Section II,  we review some relevant works. The design details of netFilter and its analysis are presented in Section III and Section IV, respectively. The evaluation of netFilter is presented in Section

V. We draw the conclusion and outline future directions in Section VI.



II. RELATED WORKS  Reference [6] proposes techniques to address iceberg query, which is relevant to IFI. However, the technique is designed for centralized systems where all items reside at a single site.

[5], [10] propose techniques to identify large flow or frequent byte sequences in the network. Similarly these works assume that all data are transferred to a centralized coordinator or multicasted to a small number of sites for processing. In our work, the data are inherently distributed among peers.

Thus, these techniques are infeasible due to the excessive communication overhead.

Some works investigate collecting the approximate count for an item in the networks (e.g., [13], [15]). [7], [8] propose gossip mechanisms to obtain the aggregates in P2P systems.

Different from these works, we focus on identifying frequent items rather than obtaining the counts for all the items. [4] proposes a technique to perform top-k retrieval in P2P systems.

This work is different from our study in the following two aspects. First, top-k retrieval only needs to report k items, while in IFI, the number of items that need to be reported might be very large. Second, in [4], the authors assume that each item only appears in the local item set of one peer, i.e., the (non-zero) local value of an item at a peer is the global value of this item. On the contrary, we do not make such an assumption. Instead, we need to address for which items we need to collect the global values for and how the global value for an item is collected from the system.

The works that are most relevant to our study are [9], [12]. These works investigate obtaining an approximate set of frequent items with ? error tolerance. The item sets returned in these studies have two types of errors: 1) false positives; and 2) errors on the reported global values of items. Different from     items above threshold  candidate verification candidate set materialization  aggregate computation item groups above threshold  aggregate computation candidate set optimization  candidate filtering  local item sets  Fig. 2. The operations of netFilter.

these studies, we investigate obtaining a precise set of frequent items without the aforementioned errors. Obtaining a precise set of frequent items is necessary in certain applications. For instance, false positives are not desirable in network attack detection. As another example, in cache management, the precise global values of the frequent items (e.g., keywords or documents) are required to facilitate cache replacement. The proposed techniques in [9], [12] are not applicable to address the problem of obtaining the precise set of frequent items, which we are addressing here.



III. IN-NETWORK FILTERING  netFilter consists of two phases: candidate filtering and candidate verification. We summarize the basic operations of netFilter in Algorithm 1 where candidate filtering and candidate verification correspond to Lines 1-3 and Line 4 in Algorithm 1, respectively.

Algorithm 1 Algorithm for netFilter.

1: Partition items into item groups.

2: Obtain aggregate for each item group.

3: Prune unqualified item groups (with aggregates below the threshold), and  obtain the items in the remaining item groups as candidates.

4: Obtain the global values for the candidates, and return the items with  global values above the threshold as the results.

Three important issues need to be addressed carefully in netFilter:  ? Aggregate computation. Both candidate filtering and can- didate verification require computing the aggregates (cor- responding to the item groups during candidate filtering and candidate items during candidate verification, respec- tively). Peers have to collaborate with each other to obtain the global value for an item or a set of items.

? Candidate set optimization. The effectiveness of netFilter relies on the ?goodness? of the candidate set, i.e., how many items in the candidate set truly satisfy the threshold, and how many do not. The latter are false positives in the candidate set. How to reduce the number of false positives in the candidate set is an important issue4.

? Candidate set materialization. After candidate filtering, the candidate set should be materialized, and then dis- seminated to the system for candidate verification. Given the item groups satisfying the threshold, a peer would be able to produce the complete candidate set if it had the knowledge of the complete set of items in the whole system. Unfortunately, each peer normally only has a partial set of items in its local item set, i.e., Ai ? A.

4Note that the false positives here refer to infrequent items in the candidate set. There is no false position in the final set of frequent items (returned after candidate verification).

Figure 2 illustrates the operations of netFilter and the issues involved in different phases of netFilter. Our strategies to address these issues are detailed in the subsequent subsections.

A. Aggregate Computation  To obtain the aggregate for an item or a set of items, two possible approaches are: 1) gossip-based aggregate compu- tation (called gossip aggregation in short) [8], [15], where peers exchange their current aggregates with their neighbors till the aggregates (almost) converge to the global values, 2) hierarchy-based aggregate computation (called hierarchical aggregation in short) [13], where peers form into a hierarchical infrastructure and pass the aggregates in a bottom up fash- ion along the hierarchy. The gossip aggregation mechanisms proposed in the literature require multiple (O(logN)) rounds of communication among peers till the aggregates (almost) converge. Among these proposals, [8] is vulnerable to peer failure. The technique proposed in [15] is more fault-resilient but only obtains approximate aggregates. On the other hand, hierarchical aggregation normally requires one or two rounds of communications among peers to obtain the precise aggre- gates, even though it is more sensitive to the leave or failure of the peers at the higher levels of the hierarchy.

In this paper, we follow the basic principle of hierarchical aggregation to design our aggregate computation. The major concern with forming a hierarchy in P2P systems is the frequent update of the hierarchy upon peer join/leave/failure.

To avoid this, we only recruit peers that are more stable (e.g., being online for a longer time) to perform netFilter where other peers forward their local item sets to one of these peers participating in netFilter. The assumption of existence of stable peers is reasonable as discussed in [14]. Nevertheless, the proposed technique is applicable to a well-designed gossip aggregation, which is left as our future work. In the following discussions, if unspecified otherwise, peers refer to the peers participating in netFilter. In the following, we first discuss how to obtain the aggregates along the hierarchy and how to update the hierarchy upon peer join/leave/failure.

1) Forming Hierarchy: Peers form a hierarchy based on the principle of breadth-first-search (BFS). Basically, each peer (Peer i with 1?i?N ) becomes a node in the hierarchy at depth d(i) as follows (d(i) is the length of the shortest path in terms of logical hops from the root of the hierarchy to this peer). A designated peer is first chosen as the root node of the hierarchy (with depth as 0). This designated peer could be a randomly selected peer, the most stable peer, or a peer that is close to the center of the network. In this study, we choose a peer randomly as the root node and leave other options for future exploration. The immediate neighbors of the root node become the nodes at depth 1 in the hierarchy. They set the root node as their upstream neighbor and set themselves as the downstream     neighbors of the root node. The immediate neighbors of the peers at depth 1 that are not yet included in the hierarchy become the nodes at depth 2 in the hierarchy. This process continues till we reach the peers whose immediate neighbors are all already included in the hierarchy, i.e., these peers have no downstream neighbors. Such peers are the leaf nodes of the hierarchy. The nodes that are neither the root or leaf nodes are called the internal nodes of the hierarchy.

In netFilter, the communication cost incurred at the peers located at the higher levels of the hierarchy is almost the same as that incurred at the peers located at the lower levels of the hierarchy (as detailed in Section IV-A). Thus, this aggregate computation mechanism does not result in performance bottle- neck at the root of the hierarchy. Nevertheless, the hierarchy is still vulnerable to single point of failure. We can construct multiple hierarchies to alleviate this issue similar to [13]. For presentation brevity, we omit the details.

Figure 3 illustrates one example of the hierarchy. The peers depicted by the squares are participating in netFilter. These peers form a hierarchy with four levels. Other peers depicted by the dark circles connect to one of the peers participating in netFilter and report their local item sets to them.

Fig. 3. An illustrative example of aggregate computation.

Multiple peers might simultaneously issue requests for identifying frequent items with different threshold values. In- stead of forming a separate hierarchy and invoking individual netFilter for each request, we use the following technique. The requests from different peers are first forwarded to the root node, which then invokes netFilter with the threshold value t set to the minimum threshold value among all the requests that it receives. The returned result set is the superset of the result sets for the requests with larger threshold values. We can simply form the proper result set for each request from this superset and forwards it to the corresponding peer requesting such a frequent item set. In this way, multiple requests from different peers are sharing the same hierarchy and netFilter process.

2) Computing Aggregates: To compute the aggregate for an item or a set of items, the peers corresponding to the leaf nodes propagate the corresponding local values to their upstream neighbors. A peer representing an internal node merges its own local value for the item(s) under consideration with the values received from its downstream neighbors, and then forwards the merged result to its upstream neighbor. Eventually, the root node has the final aggregate for the item(s).

3) Updating Hierarchy: In most deployed P2P systems, peers exchange heartbeat messages with their neighbors pe- riodically to inform the aliveness among each other. Here we modify these heartbeat messages slightly by including a DEPTH counter, indicating the depth of the message sender in the hierarchy, so that they can be used to update the hierarchy upon peer join/leave/failure. To accommodate a new peer participating in netFilter, the upstream neighbor and downstream neighbors of the newly joined peer are set up similarly as described in Section III-A.1. The repair of the hierarchy upon peer leave/failure is more complex. Upon detecting that the upstream neighbor leaves or fails by the lack of heartbeat messages from this neighbor for a predefined time interval, a peer invokes the repair of the hierarchy as follows.

It first sets its depth in the hierarchy as ?. In addition, the downstream neighbors of this peer are recursively informed to set their depth as ?. When a peer with depth as ? receives a heartbeat message from a neighbor (Pi) with depth (d(Pi)) less than ?, it becomes a node at depth (d(Pi)+1) in the hierarchy by setting Pi as its upstream neighbor.

B. Candidate Filtering  To perform candidate filtering, items are first partitioned into disjoint item groups. The aggregates for item groups are obtained through aggregate computation as presented in Section III-A.2. These aggregates of the item groups act as filters to filter out unqualified items (i.e., those in the item groups with aggregates below the threshold) and retain the items in the item groups with aggregates above the threshold as the candidates. We call the items with their global values below the threshold as light items and the items with their global values above the threshold as heavy items. Similarly, we call the item groups with their aggregates below (above) the threshold as light (heavy) item groups. In the following, we first discuss how items are partitioned into item groups.

We then discuss strategies for candidate set optimization.

1) Item Partitioning: Given the fact that a peer only has a partial set of items in the system, a solution to partition items into item groups that requires a priori knowledge of the complete set of items in the system would mandate heavy coordination among peers. Here a natural solution for item partitioning is hashing. Each of the n items is mapped to one of the g item groups through a hashing function h(x) : A ? B, where A and B are the set of items and set of item groups, respectively. g is referred to as the filter size. Each peer obtains the local values for the item groups as follows. It assigns each of its local items to one of the g item groups and increases the local value of the corresponding item group accordingly.

2) Candidate Set Optimization: Since the aggregate of an item group is the summation of the global values of all the items in the item group, some items in the heavy item group might have their global values below the threshold but are retained as the candidates, i.e., these candidates are false positives. There are two types of false positives in the candidate set caused by two different reasons:     1) Homogeneous false positive The global values of all the items in a heavy item group are below the threshold but their summation exceeds the threshold (the heavy item group consists of light items only).

2) Heterogeneous false positive Some items with their global values below the threshold are grouped into the same item group with other items with global values above the threshold (the heavy item group consists of some light items and some heavy items).

Two strategies are proposed to minimize the number of false positives in the candidate set:  Strategy 1: Setting the filter size properly. Small g (filter size) incurs low communication overhead during candidate filtering but results in a large number of false positives in the candidate set. Large g reduces the number of false positives in the candidate set but incurs high communication overhead.

Therefore, we should set g to achieve a nice tradeoff between the number of false positives and the communication overhead incurred during candidate filtering.

Strategy 2: Applying multiple filters. To further reduce the number of false positives, we apply multiple (f ) filters.

Each filter is defined by a hash function, i.e., h(x)i (1 ? i ? f) : A ? Bi, which maps one of the n items to the set of item groups Bi. An item becomes a candidate only if each of the f item groups that it belongs to is heavy.

Filter 4  2 3 54 6 7 1098  1 2 3 54 6 7 1098  1 2 3 54 6 7 1098  1 2 3 54 6 7 1098 objects candidates  Filter 1  Filter 2  Filter 3   Fig. 4. An illustrative example of multiple filters.

We use Figure 4 to illustrate an example for multiple filters. We apply four filters and obtain four sets of aggregates with one corresponding to each filter. The shaded rectangles indicate heavy item groups and blank rectangles indicate light item groups. Suppose an item x is hashed by the four filters to Item-groups 1, 5, 2, and 3, respectively. Another item y is hashed to Item-groups 7, 5, 10, and 1, respectively. Since all of the four item groups that Item x belongs to are heavy, Item x becomes a candidate. On the other hand, Item y is pruned since one of the item groups that Item y belongs to (Item-group 1 according to Filter 4) is light.

Note that appropriate settings of f and g are crucial to the effectiveness of netFilter. An analysis that derives the optimal settings for them will be given in Section IV.

C. Candidate Verification  In order to perform candidate verification, we need to address candidate set materialization, i.e., generating the can- didate set so that peers can compute the global values for these candidates. If the root of the hierarchy has a complete view of all the items in the system, it can materialize the candidate set by examining the heavy item groups obtained from candidate  filtering. However, in practice, the root may not have the complete set of items in the system. One possible solution is to perform another aggregate computation to obtain the complete set of items in the system. Given the large number of items, this aggregate computation is almost as costly as the naive approach, and thus it is not feasible.

We observe that with the list of heavy item groups provided, each individual peer can determine which of the items in its local item set are candidates. Thus, each individual peer can materialize part of the candidate set and obtains a partial candi- date set. During aggregate computation, these partial candidate sets are merged implicitly. When we reach the root of the hierarchy, we obtain the complete set of candidates and their global values. Based on this observation, we propose to per- form candidate set materialization and aggregate computation for the candidates (called candidate aggregate computation) in an integrated fashion as illustrated in Algorithm 2. Eventually, the root node has the global value for each item in the complete set of candidates exactly. It then outputs those items with global values above the threshold as the results.

Algorithm 2 Algorithm for candidate set materialization and candidate aggregate computation.

1: The root propagates the identifiers of the heavy item groups downwards  along the hierarchy recursively.

2: Upon receiving the identifiers of the heavy item groups, a peer material-  izes the candidate set according to its local item set and obtains a partial candidate set.

3: The peers corresponding to the leaf nodes of the hierarchy start to propagate the pairs of ?identifier, local value? of the items in its partial candidate set to their upstream neighbors.

4: A upstream neighbor merges its partial candidate set with the partial candidate sets received from the downstream neighbors, updates the values for the candidates accordingly, and propagates the merged result upwards along the hierarchy.



IV. ANALYSIS OF NETFILTER  In the following, we first derive the cost model for netFilter.

From the cost model, we derive the optimal setting for the filter size g (the number of item groups per filter) and the number of filters f . We then discuss how to achieve the optimal settings for netFilter in practice. For comparison, we also derive the cost model for the naive approach where the host nodes forward their local item sets along the hierarchy. As we explained before, we recruit a set of stable peers to form the hierarchy. Thus, we expect that the events of join/leave/failure of the peers in the hierarchy are rare. In addition, both of netFilter and the naive approach under consideration are based on hierarchical aggregation. Therefore, we ignore the cost incurred by hierarchy formation and update.

Table II lists the symbols used in the following discussions.

Without loss of generality, we assume that t is expressed as ??v in the following discussions, where v is the summation over all of the local values of all the items in the system (i.e., v =  ?n j=1  ?N i=1 v  i j) and t is ? fraction of v. We call ?  as the threshold ratio. In addition, we assume that we have the values of v and N through simple aggregate computation.

To obtain v, each peer contributes a single value, i.e., the summation over all of the local values of the items in its     local item set, to the final aggregate. To obtain N , each peer contributes the single value of 1 to the final aggregate. The aggregate computation for v and N can be combined with other aggregate computation since they only need to propagate one single value along the hierarchy, respectively.

TABLE II  SYMBOLS  Symbols Descriptions N Number of peers in the network n Number of distinct items in the system o Number of distinct items in the local item set of a peer t Threshold value ? Threshold ratio (t = ??v) v? Average global value of items v?light Average global value of light items r Number of heavy items w Number of heavy item groups fp Number of false positives b Number of downstream neighbors per peer h Height of the hierarchy g Size of filter (number of item groups per filter) f Number of filters sa Size of the value representing an aggregate sg Size of the identifier of an item group si Size of the identifier of an item  Since the main focus of this paper is to identify frequent items at a minimum communication overhead, we use the following performance metric in the analysis:  ? Communication cost: the average number of bytes prop- agated per peer.

Communication cost includes the cost incurred by candidate filtering and candidate verification. The cost incurred by candidate verification itself includes the cost to disseminate the identifiers of the heavy item groups to the system and the cost to compute the aggregates for the candidates. We refer the cost incurred by candidate filtering, dissemination of the identifiers of the heavy item groups, and candidate aggregate computation as candidate filtering cost, candidate dissemination cost and candidate aggregation cost, respectively.

A. Cost Model for netFilter  During candidate filtering, a peer propagates the aggre- gate for each item group. Thus, the candidate filtering cost is sa?f ?g. After candidate filtering, the root propagates the identifiers of the heavy item groups to the system. Thus the candidate dissemination cost is sg?f ?w. To obtain the global value for a candidate item, each peer propagates the pair of ?identifier, value? for this item. Therefore, the candidate aggregation cost is (sa+si)?(r+fp). We can then derive the total cost incurred by netFilter, denoted by Cfilter , as  Cfilter = sa ? f ? g + sg ? f ? w + (sa + si) ? (r + fp). (1) Note that the same candidate filtering cost is incurred at  all the peers except the root node in netFilter (the candidate filter cost is 0 at the root node) since all these peers need to propagate the aggregates for all the item groups to their upstream neighbors in the hierarchy. Similarly, the same can- didate dissemination cost is incurred at all the peers except the  leaf nodes in netFilter (the candidate dissemination cost is 0 at the leaf nodes). The peers located at the higher levels of the hierarchy have higher candidate aggregation cost. However, we expect that the total number of heavy items and false positives after candidate filtering is small, and thus the total communication cost is not dominated by candidate aggregation cost. Putting all together, the communication cost incurred at the peers located at the higher levels of the hierarchy is not significantly higher than that incurred at the peers located at the lower levels of the hierarchy. Therefore, netFilter does not impose a performance bottleneck at the root of the hierarchy as mentioned in Section III-A.1.

B. Cost Model for the Naive Approach  We derive the communication cost incurred by the naive approach, denoted by Cnaive, as (the details on the derivation are included in the complete version [11]):  (sa + si) ? o ? Cnaive ? (sa + si) ? o ? (h ? 1). (2) The above result may seem surprising since intuitively we expect that the communication cost incurred by the naive approach is O(n?N). The reason that this cost is smaller than O(n?N) is that a peer only needs to propagate the pairs of ?identifier, value? for the items with nonzero values (these items are the union of the items in its own local item set and the items propagated from its downstream neighbors).

Averaging over all the peers, the number of items that a peer needs to propagate is in the order of o, resulting in the above Formula.

C. Optimal Setting for the Size of Filters (g)  Given the average global value of light items in the system (v?light), in order to avoid homogeneous false positives, no more than tv?light items should be hashed to an item group.

Therefore, g should be greater than n?v?lightt . Since t = ??v and v = n?v?, we obtain the optimal setting for g (i.e., gopt) as  gopt = c + v?light ? ? v? (3)  with c as a small positive constant.

D. Optimal Setting for the Number of Filters (f )  We argue that the optimal f setting (denoted by fopt) is the one that can make the number of heterogeneous false positives (denoted by fp2) as small as  g?sa sa+si  . At this point, netFilter incurs the minimum communication overhead, denoted by Copt. We first derive the formula for fp2 before we explain the reason.

The number of heterogeneous false positives (fp2) is the total number of light items multiplied by the probability of heterogeneous false positives (denoted by pfp2). The total number of light items is (n ? r). pfp2 equals the probability that a light item resides in the same item groups with a heavy item for all the f filters. In the following, we derive pfp2. We first consider a specific filter i (1 ? i ? f ). The probability that a heavy item resides in one specific item group among     the g item groups for filter i is 1g , and the probability that a light item does not reside in the same item group with this heavy item for filter i is 1 ? 1g . From this, we can then derive the probability that a light item does not reside in the same item group with any of the r heavy items for filter i is (1 ? 1g )r. Therefore, the probability that a light item does reside in the same item group with at least one of the r heavy items for filter i is 1 ? (1 ? 1g )r . We can then derive pfp2 (equal to the probability that a light item resides in the same item groups with a heavy item for all the f filters), i.e., pfp2 = (1 ? (1 ? 1g )r)f . Thus,  fp2 = (n ? r) ? pfp2 = (n ? r) ? (  1 ? (  1 ? 1 g  )r)f . (4)  We now explain why we can achieve the minimum com- munication cost (Copt) with the optimal f setting (fopt) as the one that makes fp2 as  g?sa sa+si  . Since the number of heavy item groups normally is much smaller than the filter size (i.e., w<<g), the candidate dissemination cost is much smaller than the candidate filtering cost and is ignored in the following derivation. With the proper setting on g as discussed in Section IV-C, we avoid homogeneous false positives, and thus fp = fp2. Therefore, Formula 1 can be simplified as follows:  Cfilter ? sa ? f ? g + (sa + si) ? (r + fp2). (5) According to the above formula, when we increase f to  fopt+1, we increase the candidate filtering cost by g?sa. How- ever, the decrease on fp2 will be at most  g?sa sa+si  (since fp2 = g?sa  sa+si when f = fopt, and fp2 >= 0 when f = fopt+1).

We can then derive the candidate aggregation cost decreases at most by (sa+si)? g?sasa+si = g?sa. Therefore, the communication cost incurred by netFilter when f = fopt+1 is not smaller than Copt. Increasing f further will incur communication cost greater than that incurred when f = fopt+1.

On the other hand, when we decrease f to fopt?1, the candidate filtering cost decreases g?sa. According to Formula 4, fp2 increases y times with y = 11?(1?1/g)r , i.e., the increase on fp2 is  g?sa sa+si  ?(y-1). In most cases y is greater than 2.

We can then derive the increase on the candidate aggregation cost is greater than (sa+si)? g?sasa+si = g?sa. Therefore, the communication cost incurred when f = fopt?1 is greater than Copt. Similarly, decreasing f further will increase the commu- nication cost further beyond that incurred when f = fopt?1.

From above discussion, we can see that the communication cost is the minimum when f = fopt. Replacing fp2 by g?sasa+si in Formula 4, we can derive  fopt = ? log 1  1?(1? 1 g  )r  (sa + si) ? (n ? r) g ? sa  ? . (6)  E. Setting netFilter Optimally In Practice  From Formulae 3 and 6, we can see that in order to obtain the optimal setting for g and f , we need to know v?light, v?, n, and r (with ?, sa, and si given). In the following, we explain how to obtain the estimation for these values in practice.

We perform random sampling to obtain v?, v?light n, and r. In the following, we explain how to obtain v? and v?light.

The estimation for n and r are obtained in similar fashion (interested readers please refer to [11] for details). We propose to randomly select a few branches in the hierarchy, e.g., the peers along the path from the root to the leaf nodes, for sampling. Each of the sampled peers randomly selects some of the local items from its local item set, for which the aggregates are collected from these sampled peers.

Assume there are x distinct items in the sampled item set, and each of such items has an aggregate from the sampled peers as v?i (1 ? i ? x). We estimate the global value of each of such items as v?i =  v?i?v? x i=1 v  ? i  (as mentioned in the beginning of this section, v is the summation over all of the local values of all the items in the system, and can be obtained by simple aggregate computation). We then derive the estimation for v?light, denoted by v?light, as  v?light =  ? 1?i?x v?i<t  v?i? 1?i?x v?i<t  , (7)  and the estimation for v?, denoted by v?, as  v? =  ? 1?i?x v?i  x . (8)

V. PERFORMANCE EVALUATION  We first tune the performance of netFilter by varying the size of filters (g) and the number of filters (f ). We then evaluate the performance of netFilter under different settings of data skewness (?) and threshold ratios (?). For comparison, we also implement the naive approach and evaluate its performance5.

In the following, we first describe the simulation setup and performance metrics. We then present the details of the sim- ulation results.

The simulation parameters and their default values (unless otherwise stated) are given in Table III. Most of these pa- rameters are self-explanatory. More details for some of the parameters are given as follows. We use zipf distribution (with data skewness parameter ?) to model the distribution of values for items. We generate 10?n instances of these items with their frequencies (global values) following zipf-distribution.

We then randomly distribute these 10?n items to the N nodes.

Therefore, the number of items on each peer is 10?nN (the default is 10?10   1000 = 1000). Without loss of generality, we use 4-bytes integers to represent the aggregate values, identifiers of item groups, and identifiers of items.

We use the total communication cost defined in Section IV as the main performance metric. To better understand the performance of netFilter, we distinguish the candidate filtering  5As mentioned in Section II, some studies [9], [12] obtain an approximate set of frequent items and the communication cost incurred is O(a  ? ) where a is  either a constant or proportional to log(n) and ? is the given error tolerance.

We do not compare with these techniques since the result set returned in these studies is approximate, which is different from our focus here. In addition, when the given error tolerance is very small, the communication cost incurred by these techniques is even higher than the cost incurred to obtain a precise set of frequent items using our technique.

TABLE III  PARAMETERS USED IN THE SIMULATIONS  Symbols Descriptions Default N Number of peers in the network 1000 n Number of distinct items in the system 105  o Number of distinct items per peer 1000 ? Threshold ratio 0.01 ? Skewness of zipf distribution 1 b Number of downstream neighbors per peer 3 sa Size of the value representing an aggregate 4 bytes sg Size of the identifier of an item group 4 bytes si Size of the identifier of an item 4 bytes  cost, candidate dissemination cost and candidate aggregation cost at some points. If unspecified otherwise, the communi- cation cost refers to the lumped sum of these three individual costs.

A. Effect of the Filter Sizes  We conduct experiments to evaluate the effect of the filter sizes on the performance of netFilter. Figure 5(a) shows the average number of candidates propagated per peer during candidate verification and the number of heavy item groups with g varying from 25 to 500 (the number of filters is set to 3). For readability, the y-axis is on log scale. From this figure, we can see that when the filter size is very small (?50), the filtering performance is poor. In fact, at this range of filter sizes, none of the items are pruned. Thus, the candidate verification performs similarly as the naive approach, and the average number of candidates propagated per peer during candidate verification is in the order of the size of local item set6. When the filter size increases, the filtering performance improves significantly, resulting in significant reduction on the number of candidates propagated per peer. When the filter size increases further, the decrease on the number of candidates propagated per peer becomes less significant.

0  50  100 150 200 250 300 350 400 450 500  size of filter (g)  number of candidates number of heavy item groups            0  50  100  150  200  250  300  350  400  450  500  co m  m un  ic at  io n  co st  ( by  te s)  size of filter (g)  total cost candidate filtering cost  candidate dissemination cost candidate aggregation cost  (a) (b) Fig. 5. Effect of filter sizes.

The major trend observed from the number of heavy item groups is that it increases initially, then decreases. When the filter size is small (? 50), the number of items in an item group is very large, which makes all item groups heavy. Thus, at this range of filter sizes, the number of heavy item groups increases with the filter size. When the filter size becomes larger (? 75), the number of items in an item group becomes less, resulting in less number of heavy item groups.

Figure 5(b) shows the communication cost incurred by netFilter under the same setting as for Figure 5(a). We also  6The reason that this number is smaller than the total number of candidates (in this case, n) is the same as explained in Section IV-B.

plot the candidate filtering cost, candidate dissemination cost and candidate aggregation cost in the figure. The candidate filtering cost increases linearly with the filter size. The can- didate aggregation cost and candidate dissemination cost are proportional to the number of candidates propagated per peer and the number of heavy item groups as displayed in Figure 5(a), respectively. This confirms our analysis in Section IV-A.

With above discussion, we can easily explain the plot for the total cost in Figure 5(b). The total cost initially increases slightly, then decreases significantly, followed by an increase again. When the filter size is very small (<100), the total cost is dominated by the candidate aggregation cost. This explains the initial increase of the total cost (since candidate aggregation cost increases when g ? 50). When the filter size becomes larger (? 100), the improved filtering performance results in significant decrease on the candidate aggregation cost. When the filter size increases further, the total cost is dominated by the candidate filtering cost, explaining the latter increase of the total cost. Among all of the tested cases, the total cost reaches the smallest when g is 100. This confirms our analysis in Section IV-C. According to Formula 3, gopt = c +  v?light ??v? (c is a small positive constant). Since ? is  0.01 and v?lightv? is around 0.8, we obtain gopt = c + 80.

B. Effect of the Number of Filters  We vary the number of filters (f ) from 1 to 10 (the filter size is set to 100) and evaluate its effect on the performance of netFilter. From Figure 6(a), we see that the number of candidates propagated per peer decreases with the number of filters, which confirms the benefits of multiple filters as discussion in Section III-B. The major trend observed on the number of heavy item groups (Figure 6(a)) is that it almost increases linearly with the number of filters, which is expected.

1  2  3  4  5  6  7  8  9  10  number of filter (f)  number of candidates number of heavy item groups         1  2  3  4  5  6  7  8  9  10  co m  m un  ic at  io n  co st  ( by  te s)  number of filter (f)  total cost candidate filtering cost  candidate dissemination cost candidate aggregation cost  (a) (b) Fig. 6. Effect of number of filters.

Figure 6(b) shows the communication cost under the same setting as for Figure 6(a). We also plot the candidate filtering cost, candidate dissemination cost and candidate aggregation cost. When the number of filters is small, the candidate aggregation cost is large. With the increase of the number of filters, the filtering performance improves, resulting in reduced candidate aggregation cost. On the other hand, the major trend observed on both candidate filtering cost and candidate dissemination cost is that they increase linearly as expected.

The total cost initially decreases, then increases. When the number of filters is small, the total cost is dominated by the candidate aggregation cost. The initial decrease is due to the improved candidate aggregation cost. With the further increase     on the number of filters, the total cost is dominated by the candidate filtering cost, which explains the latter increase.

Among all of the tested cases, the total cost is the smallest when f is 3, confirming our derivation in Section IV-D.

C. Effect of Data Skewness  We conduct experiments to evaluate the effect of data skewness on the performance of netFilter. Figure 7(a) and (b) show the results under different ? values (data skewness) with n as 105 and 106, respectively. We adopt the optimal setting for netFilter obtained from similar experiments as in the above two subsections (the filter size is set to 100, and the number of filters is set to 3 and 5 under n as 105 and 106, respectively). We include the communication cost incurred by the naive approach for comparison. For readability, the y-axis is on log scale. From these figures, we can see that the cost incurred by netFilter is much smaller than that incurred by the naive approach. For instance, with n as 106, the cost incurred by netFilter is only 2% ? 5% of that incurred by the naive approach. In addition, the cost incurred by both netFilter and the naive approach decreases with the data skewness. For netFilter, this is explained by the improved filtering performance under very skewed data. For the naive approach, as the data skewness increases, the average number of distinct items that a peer propagates along the hierarchy is reduced, resulting in reduced communication cost.

0  1  2  3  4  5c om  m un  ic at  io n  co st  ( by  te s)  data skewness (?)  netFilter naive     1e+06  0  1  2  3  4  5c om  m un  ic at  io n  co st  ( by  te s)  data skewness (?)  netFilter naive  (a) n = 105 (b) n = 106  Fig. 7. Effect of data skewness.

D. Effect of Threshold  To evaluate the effect of the threshold, we conduct experi- ments under different threshold ratios (?). Figure 8 shows the results with ? set to 0.1, 0.01 and 0.001. Again, we adopt the optimal setting for netFilter (the size of filter and number of filters are set to (10, 6) under ? = 0.1, (100, 5) under ? = 0.01, and (1000, 2) under ? = 0.001). From this figure, we can see when the threshold ratio increases, the communication cost decreases. This is as expected since larger threshold implies less number of items satisfies the threshold condition.



VI. CONCLUSION  The need of identifying frequent items (IFI) appears in a variety of P2P applications. In this study, we investigate the problem of IFI in unstructured P2P systems and made the following contributions: 1) We identify and formally define the problem of IFI, and discuss how the need of addressing IFI exists in a variety of applications in P2P systems. 2) To      1e+06  0  1  2  3  4  5  co m  m un  ic at  io n  co st  ( by  te s)  data skewness (?)  netFilter: ?=0.001 netFilter: ?=0.01  netFilter: ?=0.1 naive  Fig. 8. Effect of threshold (n = 106).

address IFI, we propose an efficient two-phase in-network processing approach, netFilter, and address various issues faced in realizing netFilter. 3) We analyze the performance of netFilter, derive the optimal setting mathematically, and discuss how to set netFilter optimally in practice. 4) We conduct extensive performance evaluation to demonstrate the effectiveness of our proposal.

In the future, we plan to investigate a fault-tolerant gossip aggregation that can obtain the precise aggregates from the network and extend the solutions proposed in this study on gossip aggregation.

