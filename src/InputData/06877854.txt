JUST COMPRESS AND RELAX: HANDLING MISSING VALUES IN BIG TENSOR ANALYSIS

ABSTRACT  In applications of tensor analysis, missing data is an important issue that is usually handled via weighted least-squares fitting, imputation, or iterative expectation-maximization. The resulting algorithms are often cumbersome, and tend to fail when the percentage of miss- ing samples is large. This paper proposes a novel and refreshingly simple approach for handling randomly missing values in big ten- sor analysis. The stepping stone is random multi-way tensor com- pression, which enables indirect tensor factorization via analysis of compressed ?replicas? of the big tensor. A Bernoulli model for the misses, and two opposite ends of the tensor modeling spectrum are considered: independent and identically distributed (i.i.d.) tensor elements, and low-rank (and in particular rank-one) tensors whose latent factors are i.i.d. In both cases, analytical results are estab- lished, showing that the tensor approximation error variance is in- versely proportional to the number of available elements. Coupled with recent developments in robust CP decomposition, these results show that it is possible to ignore missing values without losing the ability to identify the underlying model.

Index Terms? Tensor decomposition, multi-way arrays, miss- ing elements, missing values, big data, CANDECOMP / PARAFAC, tensor completion, imputation.

1. INTRODUCTION  Tensors are data structures indexed by three or more indices, (?, ?, ?, ? ? ? ) - a generalization of matrices, whose elements are in- dexed by two indices (?, ?) for (row,column), respectively. Tensors have found numerous and diverse applications across science and en- gineering [1], and recent advances in large-scale web-enabled data collection and mining have made tensor analysis even more impor- tant.

Missing data is an important issue and a growing concern in many applications of matrix and tensor analysis, especially those relying on user input (e.g., ratings in recommender systems) or web- crawling for data collection. Data can be randomly missing, mean- ing that the misses are more-or-less independent of one another, in which case we often talk of missing elements, or missing values; or systematically missing, roughly meaning that misses are structured, follow certain patterns, and entire blocks or pieces of the data can be missing. The latter case is typically more challenging. The simplest possible model of random misses is that of repeated Bernoulli trials, or simply a Bernoulli model.

Tensor decomposition with missing data has been considered in [2], and later in [3], for the most basic rank decomposition model  ?E-mail: marc0312@umn.edu.

?E-mail: nikos@umn.edu, Tel: +16126251242, Fax: +16126254583.

Supported in part by NSF IIS-1247632.

known as parallel factor analysis (PARAFAC) [4] or canonical de- composition (CANDECOMP) [5], nowadays often abbreviated as CP for CANDECOMP-PARAFAC. There are two basic ways to han- dle missing data: i) fit the model only to the available data (instead of forcing it to produce zeros or other arbitrary values at places where data is missing); or ii) try to fill-in the missing data first, then fit the model to the ?full? data. Fitting the model only to what is avail- able can be accomplished by 0-1 weighting of the cost function.

Alternating optimization and Gauss-Newton type CP algorithms for missing data were considered in [2], while a more lightweight first- order method was proposed in [3]. One important problem is that weighting usually complicates the fitting algorithm when additional constraints are imposed, often forcing one to employ small-block co- ordinate updates which are slow. Missing data interpolation, on the other hand, is often ad-hoc, without (full) regard to the underlying model, and it may introduce serious artifacts in the final solution.

Iterative model fitting and data interpolation is often performed us- ing expectation-maximization [3], but this entails a heavier computa- tional burden, and requires a fairly accurate statistical model as prior information. In the absence of any side information, optimal inter- polation of the missing data reduces to estimating the model from the available data, and using the estimated model to interpolate - in which case there is no point in iterating, since the interpolated data are a perfect match to the estimated model.

This paper proposes a novel and refreshingly simple approach for handling randomly missing values in big tensor analysis. The stepping stone is random multi-way tensor compression, a tool that was recently introduced in [6?8] to enable analyzing big tensors that do not otherwise fit in fast memory, and to parallelize their analysis.

The main idea is as follows. Every compressed sample in the re- duced tensor replica(s) is a weighted sum of elements of the big data tensor, with weights determined by the random mode compression matrices. Normalized by the number of terms, this can be viewed as a sample average that converges to its expected value, under appro- priate conditions. When data is missing, it may still be possible to accurately estimate the expectation limits, provided there is enough data available - irrespective of how much data is missing. A suitable ergodic property is all that is needed for sample averages to converge to ensemble averages.

For analysis purposes, statistical models for the full data and the miss process should be adopted. A simple Bernoulli model for the misses, and two opposite ends of the tensor modeling spectrum are considered: independent and identically distributed (i.i.d.) tensor elements, and low-rank (and in particular rank-one) tensors whose latent factors are i.i.d. In both cases, analytical results are estab- lished, showing that the tensor approximation error variance is in- versely proportional to the number of available elements. Coupled with recent results on robust CP decomposition [9, 10] ensuring that small perturbation in the data corresponds to small perturbation in the latent factors, these developments show that it is possible to ig-     nore missing values without losing the ability to identify the under- lying model.

2. TENSOR DECOMPOSITION AND COMPRESSION  Notation: A scalar is denoted by an italic letter, e.g. ?. A column vector is denoted by a bold lowercase letter, e.g. a whose ?-th entry is a(?). A matrix is denoted by a bold uppercase letter, e.g., A with (?, ?)-th entry A(?, ?). A tensor is denoted by an underlined bold uppercase letter, e.g., X, with (?, ?, ?)-th entry X(?, ?, ?). For three vectors, a (? ? 1), b (? ? 1), c (? ? 1), their outer product a ? b ? c is an ? ? ? ? ? three-way tensor with (?, ?, ?)-th element a(?)b(?)c(?). Such a tensor is simple, in the sense that all its ??? columns are constant multiples of a single column; and the same is true for its rows, and its ?fibers?; and even for its matrix ?slabs? along any one of its three ?modes?. This is a rank-one tensor, in direct analogy to a rank-one matrix, which is the outer product of two vectors. The vec(?) operator stacks the columns of its matrix argument in one tall column; ? is the Kronecker product; and ? stands for the Khatri-Rao (column-wise Kronecker) product.

Rank decomposition for tensors: The rank of tensor X is the smallest number of rank-one tensors that sum up to X  X =  ?? ?=1  a? ? b? ? c? .

This relation can be expressed element-wise as  X(?, ?, ?) =  ?? ?=1  a? (?)b? (?)c? (?).

X comprises? slabs of size ? ? ? , denoted {X?}??=1. With X := [vec(X1), ? ? ? , vec(X?)], it can be shown that  X = (B?A)C? ?? x := vec(X) = (C?B?A)1,  where, A := [a1, ? ? ? ,a? ], and similarly for B, C, 1 is a vec- tor of all 1?s, and we have used the vectorization property of the Khatri-Rao product vec(AD(d)B? ) = (B?A)d, where D(d) is a diagonal matrix with the vector d as its diagonal. The above rank decomposition model for tensors is known as PARAFAC [4], CANDECOMP [5], or CP. Among many remarkable tensor proper- ties, perhaps the most useful one is that low rank tensor decomposi- tion (i.e., CP) is often unique, even for moderately high ranks; see, e.g., [11, 12].

I L  M  I  J  J  L  K  M X N  Y _  _  U  V T  K N  WTT  Fig. 1. Illustration of tensor compression from a tensor X of size ? ? ? ?? to smaller tensor Y of size ??? ??  Tensor compression: With the goal of enabling efficient CP de- composition of big tensors, [6] proposed compressing X into a far smaller tensor Y, and showed that, when the latent components (the columns of A, B, C) are sparse, then it is possible to recover them from the latent components of Y. They proposed to multiply every slab of X with U? from the ?-mode, with V? from the ?-mode, and with W? from the?-mode. They proposed using random compres- sion matrices U (???), V (??? ), and W (??? ), with ? ? ? , ? ? ? , ? ? ? and ??? ? ???; see Fig. 1. Using random compression matrices helps to ensure identifiability in this setting.

With u? denoting the ?-th column of U, v? the ?-th column of V, and w? the ?-th column of W, the elements of the compressed tensor can be expressed as  Y(?,?, ?) = ?  (?,?,?)  u?(?)v?(?)w?(?)X(?, ?, ?).

Notice that the overall effect of compression is trilinear with respect to the elements of the compression matrices, but linear in X(?, ?, ?).

Furthermore, due to the fact that the overall compression is separa- ble across modes, if X admits a CP decomposition parametrized by (A,B,C), then Y will admit a CP decomposition parametrized by (A?, B?, C?), with A? := U?A, B? := V?B, C? := W?C. Notice that the number of components (= number of columns) is the same for both decompositions. When A has sparse columns, then it is possible to recover A from the seemingly under-determined linear system A? := U?A by exploiting sparsity. This last step fails when the columns of A are not sparse. A simple but instrumental idea in [7, 8] is that one can instead spawn a number of independently compressed replicas of X in this case, analyze them in parallel, and combine the results to over-determine the problem without recourse to sparsity.

With or without sparsity, the results in [6?8] open a completely new way to compute the CP decomposition of big tensors, that of- fers ?decoupled? parallelization, in-memory processing for the paral- lel threads, and compatibility with cloud computing and storage. The results in [8] suggest using i.i.d. zero-mean matrices for compressing the different modes of the tensor. Here we will instead advocate us- ing nonzero-mean compression matrices, and show how this simple and seemingly inconsequential modification allows us to effectively handle missing elements without any other alterations.

3. MAIN RESULTS  Define ? = {(?, ?, ?) ? 1 ? ? ? ?, 1 ? ? ? ?, 1 ? ? ? ?}, and ? = {(?, ?, ?) ? X(?, ?, ?) is available}, and let ? be the tensor calculated using the available elements of X, i.e.,  ? (?,?, ?) = ?  (?,?,?)?? u?(?)v?(?)w?(?)X(?, ?, ?), (1)  If we normalize the compressed tensor Y by the number of elements of X, and similarly normalize Y? by the expected number of avail- able elements ?[???] we get  ??(?,?, ?) =  ?? ? ?  (?,?,?)?? u?(?)v?(?)w?(?)X(?, ?, ?)  ??(?,?, ?) =  ?[???] ?  (?,?,?)?? u?(?)v?(?)w?(?)X(?, ?, ?) (2)  Note that we use the subscript ??? rather than the more common ?n? for normalization to avoid confusion with the index n. The rea- son we choose to normalize by the expected number of available     elements, ?[???], rather than the actual number, ???, is because it yields an unbiased estimator whose statistics are easier to analyze, as ??? is itself a random variable. In practice, we can use the actual number of elements, ??? , as an estimate of the expected number of elements ?[???]. In order to evaluate this approximation, we define the normalized error tensor ?? as the difference between the com- pressed tensor ?? and its estimate from available elements only ?? , i.e., ?? = ?? ? ?? . Define  ??(?, ?, ?) = ?[(?, ?, ?) ? ?] = {  1, if (?, ?, ?) ? ? 0, if (?, ?, ?) /? ? (3)  ?(??(?, ?, ?)) =  ?[???]??(?, ?, ?)?  ?? ? (4)  This allows us to rewrite ?? and ?? as  ??(?,?, ?) =  ?[???] ? ? ??(?, ?, ?)u?(?)v?(?)w?(?)X(?, ?, ?)  ??(?,?, ?) = ? ? ?(??(?, ?, ?))u?(?)v?(?)w?(?)X(?, ?, ?)  We assume a Bernoulli model for the available elements, with pa- rameter ? = ????[(?, ?, ?) ? ?]. The elements of U, V, and W are all independently drawn according to u?(?) ? ?? (?? , ?? ), v?(?) ? ?? (?? , ?? ), and w?(?) ? ?? (?? , ?? ), where ?? ,?? , and ?? are continuous marginal densities. Define ?? := ?2? + ?  ? , and likewise for ?,?,? . We have the following result  (proofs are deferred to the journal version due to space limitations).

Claim 1  ?? := ?[??(?,?, ?)] = 0 ?? := ?[??(?,?, ?)] = ?[??(?,?, ?)]  =  { ???? ???? if X is random i.i.d.

????? ???????? if X is rank-F w/ i.i.d. loadings  Claim 2 For i.i.d. X(?, ?, ?) ? ??(?? , ??), where ??(?? , ??) is an arbitrary probability distribution, we get  ?[(??(?,?, ?))2] = (1? ?) ??? ? ???? ?? ?? (5)  ?[(??(?,?, ?)) 2] =   ?? ????? ?? ?? + ??  ?? ????? ? ??  ?  + ? ?  ?? ???? ? ???  ? +  ? ???  ?? ? ??? ? ?  ??  ?  + ? ?  ?? ?? ??? ???  ? +  ? ???  ?? ? ? ??? ?  ??  ?  + ? ?? ?  ?? ? ? ??  ? ???  ? +  ? ?? ???  ?? ? ? ? (6)  where ? ? := (??1), ? ? := (??1),?? := (??1). When ?? ?= 0 ?[????2? ] ?[????2? ]  ? (1? ?) ??? ? (1 +  ?2? ?2?  )(1 + ?2? ?2?  )(1 + ?2? ?2?  )(1 + ?2? ?2?  )  else for ?? = 0 we have ?[????2? ] ?[????2? ]  = (1??) ?  .

Claim 3 For X(?, ?, ?) = ??  ?=1 a? (?)b? (?)c? (?), assuming that the elements of a? , b? and c? are all i.i.d. random variables drawn from a? (?) ? ??(??, ??), b? (?) ? ??(??, ??), and c? (?) ?  ??(??, ??), with ?? := ?2? + ?2?, ?? := ?2? + ?2? , ?? := ?2? + ?2? , and ? ? := (? ? 1), we have  ?[(??(?,?, ?))2] = (1? ?) ??? ? ???? ???  ( ? ??2??  ??  ? + ??????  ) ?[(??(?,?, ?))  2] =  ?? ????? ??? ( ? ??2??  ??  ? + ??????  ) + ??  ?? ????? ? ??  ( ? ??2??  ??  ? + ?????  ?  ) + ? ?  ?? ???? ? ???  ( ? ??2??  ??  ? + ???  ???  ) + ? ???  ?? ? ??? ? ?  ??  ( ? ??2??  ??  ? + ???  ??  ?  ) + ? ?  ?? ?? ??? ???  ( ? ??2??  ??  ? + ?  ?????  ) + ? ???  ?? ? ? ??? ?  ??  ( ? ??2??  ??  ? + ?  ????  ?  ) + ? ?? ?  ?? ? ? ??  ? ???  ( ? ??2??  ??  ? + ?  ??  ???  ) + ? ?? ???  ?? ? ? ??  ? ?  ??  2?2?? ??  ?  ?[????2? ] ?[????2? ]  ? (1? ?) ??? ? (1 +  ?2? ?2?  )(1 + ?2? ?2?  )(1 + ?2? ?2?  )( ? ?  ? + ?????? ??2??  ??  ?  )  when ??, ??, ?? are all ?= 0; otherwise ?[???? ? ]  ?[????2? ] = (1??)  ? .

For low-enough ? , applying theorem (2.3) in [10], we get that the reconstructed loadings also have the same rate of convergence, up to a scaling polynomial that is independent of (?, ?? ?).

4. SIMULATION RESULTS  All simulation results reported here are averaged over 104 Monte- Carlo trials. For every trial, we generate new X, U, V, and W drawn from an i.i.d. Gaussian distribution, and a new i.i.d.

Bernoulli ?. For U, V, and W we fix (?, ?) = (1, 0.1); for X, these parameters are depicted in the figures. We define ??? :=  10 log10  ( ?[????2? ] ?[????2? ]  ) , and simulate five different sizes of the origi-  nal tensor, including three symmetric and two asymmetric cases. In all cases, we compress to 10 ? 10 ? 10. Figs. (2), (3), and (4), show the theoretical and actual SNR of Y using normalization by the expected number of elements (as analytically derived), and also using the actual number of elements, for i.i.d. and rank-1 X. Notice that the SNR with normalization by the actual number of elements is better than the one that uses the expected number of elements for ?? = 1; but this is not necessarily true for small ?? . Figs. (4) and (5) show the SNR of the compressed tensors and the reconstructed loadings of X using PARACOMP [7, 8]. Note that the symmetric cases yield higher SNR for the reconstructed loadings for the same number of elements ?? ?.

5. CONCLUSIONS  We presented a novel and simple way to handle missing elements in factor analysis of big tensors. The main idea is to randomly com- press the tensor while ignoring the missing values - except for nor- malization. Assuming random Bernoulli misses, we proved that the     ?5  ?4  ?3  ?2  ?1   ?20  ?10        Bernoulli Density parameter (?)  S N R  Y (d B )  SNR of Y, for X IID, (?X, ?X) = (0.1, 1)     Actual SNR using expected number of available alements Actual SNR using actual number of available alements Theoretical SNR Lower bound Theoretical SNR (Exact)  I = J = K = 200  I =4 0 0, J = K = 5 0  I = J = K = 50  Fig. 2. SNR of compressed tensor Y for different sizes of i.i.d. X  ?5  ?4  ?3  ?2  ?1   ?10          Bernoulli Density parameter (?)  S N R  Y (d B )  SNR of Y, for X IID, (?X, ?X) = (1, 1)     Actual SNR using expected number of available alements Actual SNR using actual number of available alements Theoretical SNR Lower bound Theoretical SNR (Exact)  I = J = K = 200  I = J = K = 50  I = 400, J = K = 50  Fig. 3. SNR of compressed tensor for different sizes of i.i.d. X  error variance in the compressed domain is inversely proportional to the number of available elements, for two stochastic tensor mod- els: i.i.d. and low-rank with i.i.d. latent loadings. For both cases, we provided analytical formulas for the SNR in the compressed domain, and confirmed our analysis with simulations. Our simulations also confirmed that the latent loadings can be recovered this way, even with a large percentage of missing elements.

6. REFERENCES  [1] A.K. Smilde, R. Bro, and P. Geladi, Multi-way Analysis: Applications in the Chemical Sciences, Wiley, 2004.

[2] G. Tomasi and R. Bro, ?PARAFAC and missing values,? Chemometrics and Intelligent Laboratory Systems, vol. 75, no. 2, pp. 163?180, 2005.

[3] E. Acar, D.M. Dunlavy, T.G. Kolda, and M. Mrup, ?Scalable tensor factorizations for incomplete data,? Chemometrics and Intelligent Lab- oratory Systems, vol. 106, no. 1, pp. 41?56, 2011.

[4] R.A. Harshman, ?Foundations of the PARAFAC procedure: Models and conditions for an ?explanatory? multimodal factor analysis,? UCLA Working Papers in Phonetics, vol. 16, pp. 1?84, 1970.

[5] J.D. Carroll and J.J. Chang, ?Analysis of individual differences in mul- tidimensional scaling via an n-way generalization of Eckart-Young de- composition,? Psychometrika, vol. 35, no. 3, pp. 283?319, 1970.

?5  ?4  ?3  ?2  ?1   ?10          Bernoulli Density parameter (?)  S N R  Y (d B )  SNR of Y, for rank(X) = 1, (?X, ?X) = (1, 1)     Actual SNR using expected number of available alements Actual SNR using actual number of available alements Theoretical SNR Lower bound Theoretical SNR (Exact)  I = J = K = 50  I = 400 J = K = 50  I = J = K = 200  Fig. 4. SNR of compressed tensor for different sizes of rank-one X  ?5  ?4  ?3  ?2  ?1   ?5            Bernoulli Density parameter (?)  S N R  A (d B )  SNR of loadings A, for rank(X) = 1, (?X, ?X) = (1, 1)     (I,J,K) = (200,200,200) (I,J,K) = (100,100,100) (I,J,K) = (50,50,50) (I,J,K) = (200,100,50) (I,J,K) = (400,50,50)  Fig. 5. SNR of recovered loadings for different sizes of rank-one X  [6] N.D. Sidiropoulos and A. Kyrillidis, ?Multi-way compressed sensing for sparse low-rank tensors,? IEEE Signal Processing Letters, vol. 19, no. 11, pp. 757?760, 2012.

[7] N.D. Sidiropoulos, E.E. Papalexakis, and C. Faloutsos, ?A Parallel Al- gorithm for Big Tensor Decomposition Using Randomly Compressed Cubes (PARACOMP),? in Proc. IEEE ICASSP 2014, May 4-9, Flo- rence, Italy (submitted).

[8] N.D. Sidiropoulos, E.E. Papalexakis, and C. Faloutsos, ?PArallel RAn- domly COMPressed Cubes (PARACOMP): A Scalable Distributed Ar- chitecture for Big Tensor Decomposition,? IEEE Signal Processing Magazine, Jan. 2014 (submitted).

[9] A. Bhaskara, M. Charikar, and A. Vijayaraghavan, ?Uniqueness of tensor decompositions with applications to polynomial identifiability,? http://arxiv.org/abs/1304.8087, Apr. 2013.

[10] A. Bhaskara, M. Charikar, A. Moitra, and A. Vijayaragha- van, ?Smoothed Analysis of Tensor Decompositions,? http://arxiv.org/abs/1311.3651, Nov. 2013.

[11] J.B. Kruskal, ?Three-way arrays: Rank and uniqueness of trilinear de- compositions, with application to arithmetic complexity and statistics,? Linear Algebra and its Applications, vol. 18, no. 2, pp. 95?138, 1977.

[12] N.D. Sidiropoulos and R. Bro, ?On the uniqueness of multilinear de- composition of N-way arrays,? Journal of chemometrics, vol. 14, no.

3, pp. 229?239, 2000.

