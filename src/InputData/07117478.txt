Data Acquisition System for the Belle II Experiment Satoru Yamada, Ryosuke Itoh, Katsuro Nakamura, Mikihiko Nakao,

Abstract?The Belle II experiment, a new generation B-factory experiment at KEK to search for new physics beyond the standard model, is scheduled to start in 2017. The maximum trigger rate is expected to reach as high as 20 kHz with the raw event size of more than 1 MB and we set the designed trigger rate to 30 kHz for the data acquisition (DAQ) system. This requirement for the data acquisition is a big challenge. The Belle II DAQ system is a synchronous system based on a pipelined trigger flow control.

Data from all subdetectors except for the Pixel Detector (PXD) digitized by the front-end are transferred to unified readout modules (COPPER) via a high speed optical link (Belle2link). The data are then collected by the readout PCs via Ethernet and the event building is performed in multiple steps. The built events are fed into High Level Trigger (HLT) to perform a software trigger.

For processing data of the PXD, an FPGA-based readout system (ONSEN) is used. ONSEN performs a data reduction by exploiting expected hit positions of charged tracks reconstructed by the HLT. In order to test the full functionality of our DAQ design, we developed a down-scaled version of the Belle II DAQ system.

The system was used in the combined beam test of Silicon Vertex Detector (SVD) and PXD in the DESY test beam performed in January of 2014. A slow control and real time data monitoring system were also implemented in this test. All components of this complex readout chain successfully cooperated to collect the beam data.

Index Terms?Belle II, data acquisition.



I. INTRODUCTION  T HE experimental confirmation of the Standard Model(SM) in particle physics is nearly completed by the discovery of a Higgs boson by ATLAS and CMS experiments.

However, there still remains open questions that the SM does not explain. The Belle II experiment [1] aims at finding new physics signals beyond the SM by measuring heavy flavor de- cays precisely and searching for rare signals. The measurement will be performed by the Belle II detector at the high luminosity collider, SuperKEKB accelerator.

The SuperKEKB accelerator is now being built in the same  tunnel where the KEKB accelerator was built. The new accel- erator is designed based on the ?nano-beam? scheme to achieve  Manuscript received June 17, 2014; revised November 28, 2014; accepted March 18, 2015. Date of publication June 03, 2015; date of current version June 12, 2015.

S. Yamada, R. Itoh, K. Nakamura, M. Nakao and S. Y. Suzuki are with High  Energy Accelerator Research Organization (KEK), Ibaraki 305-0801, Japan (e-mail: satoru.yamada@kek.jp).

T. Konno is with Tokyo Metropolitan University, 192-0397 Tokyo, Japan.

T. Higuchi is with Kavli Institute for the Physics and Mathematics of theUni-  verse (Kavli IPMU), The University of Tokyo, Tokyo 113-8654, Japan.

Z. Liu and J. Zhao are with Institute of High Energy Physics, Chinese  Academy of Sciences, Beijing 100190, China.

the design luminosity which is 40 times higher than KEKB and to collect ab integrated luminosity in 10 years of operation.

The Belle II detector is a magnetic spectrometer of a large solid angle which surrounds the interaction point. The detector con- sists of several different subdetectors. The Pixel Detector (PXD) and Silicon Vertex Detector (SVD) are for the detection of the decay vertex of the B-mesons. Outside of the vertex detectors, the Central Drift Chamber (CDC) is located as a main tracking detector for a precise measurement of the momentum and the energy loss of charged particles. To identify the type of hadrons, the Time-of-Propagation (TOP) counter and the proximity-fo- cusing Aerogel Ring Imaging CHerenkov detector (ARICH) are located in the barrel and endcap regions, respectively. The Elec- tromagnetic Calorimeter (ECL) is located outside of the PID de- tectors and consists of about 9000 CsI(Tl) crystals for the mea- surement of photon energy and angular coordinates as well as for the electron identification. The outermost detector is the neu- tral and muon detector (KLM) located outside of the super- conducting solenoid magnet.

Due to the increase of the luminosity and introducing new  subdetectors, the trigger rate is expected to reach 20 kHz. The Belle II data acquisition (DAQ) system is required to handle 30 kHz trigger rate, which is several 10 times larger than that of the Belle experiment. The event size after the Level-1 (L1) trigger is also increased by about 30 times. The Belle II DAQ system [2] therefore needs to handle the increased large data flow.



II. BELLE II DAQ SYSTEM  The Block diagram of the Belle II DAQ system is shown in Fig. 1. The trigger and clock signals are distributed to the detector front-end electronics (FEE) boards by a unified timing system with Frontend Timing Switch (FTSW) modules [3].

After the reception of the trigger, the digitized signals by the FEE board except for the PXD are transferred to a unified readout board called as COmmon Pipelined Platform for Electronics Readout (COPPER) [4] via a high speed optical link (Belle2link) [5]. The COPPER board is equipped with a Linux-operated CPU card for the data handling. The data are then collected by a readout PC via Ethernet and sent to a High Level Trigger (HLT) input server where the event building is performed. The built events are fed into HLT worker nodes to perform a software trigger [6].

The digitized data of the PXD are collected by Data Han-  dling Hybrid (DHH) and DHH Concentrator (DHHC) modules [7] and forwarded to ATCA-based modules called as ONline SElector Node (ONSEN) [8]. Inside the ONSEN system, data  See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

Fig. 1. Schematic view of the DAQ system for the Belle II experiment.

reduction is performed by the information of the region-of-In- terest (ROI) on the PXD sensor calculated from the tracking in- formation from other detectors. The reduced data from the PXD are finally merged with the HLT output on a storage server by the full event builder and stored in a ROOT file.

In addition to the functions described above, the DAQ system  is required to have enough performance to handle large data flow produced by the detector front-end electronics. In the Belle II experiment, the average trigger rate is expected to be 20 kHz at the designed luminosity of cm s and we set 30 kHz as a designed trigger rate for the DAQ system, considering the safety margin. The event size from the FEE is estimated to be 100 kB and 1 MB for the non-PXD detectors and the PXD, respectively. Since it is difficult to store all the received data in a disk storage, online data reduction is necessary. We will reduce the event rate to 1/3 by HLT and the even size of PXD to 1/10 by the ROI scheme. As for the data storage, we require ten storage units to be capable of recording the reduced data at the speed of 3 GB/s. In the following of this section, we will explain each part of the Belle II DAQ system in detail.

A. Trigger and Timing Distribution System The L1 trigger decision of the Belle II experiment is de-  signed to select B meson decay events with more than 99% ef- ficiency, and also charm and tau events with more than 90% ef- ficiency. There are five sub-trigger systems for the CDC, ECL, TOP, ARICH and KLM. They collect hit information from their FEE boards and send their outputs to the Global Decision Logic (GDL) which makes the final trigger decision [9]. The trigger system consists of cascaded Universal Trigger Boards (UT3).

This VME based board is equipped with a Virtex-6 FPGA and I/O ports to handle Xilinx Rocket I/O, LVDS, NIM and clock signals. The total trigger latency is about s and the FEE boards keep data until the L1 trigger decision is made.

The L1 trigger signals are provided to the detector FEE  boards as well as the COPPER readout boards [10]. The signals are distributed over O(1000) nodes by FTSW modules. This module has a Virtex5 FPGA and 24 RJ45 connectors. The cas- caded FTSW modules are connected by 4 pairs of a Category-7 (CAT7) cable, through which LVDS signals driven by an FPGA on the FTSW is transmitted. The Information of the L1 trigger  is delivered with 8b10b encoding. The deserialization and serialization are also performed by the FPGA.

Whereas the minimum time interval between consecutive  triggers is set to be 190 ns, it takes 13.2 or s for the SVD FEE to send one event data downstream, which corresponds to 3 or 6 samples of the waveform, respectively. Therefore, it causes large deadtime at the trigger rate of 30 kHz if triggers are blocked until the FEE finishes handling the current event. To avoid this situation, trigger signals are handled in a pipelined manner [3], where the FTSWs distribute trigger signals without waiting for the completion of data processing by the FEE. In this scheme, trigger signals from GDL are stored in a queue on the FEE and the data of each event are processed sequentially.

Together with the trigger signals, clock signals for the FEE  and COPPER boards are also distributed. The frequency of the clock is 127 MHz, which is derived from the SuperKEKB RF clock. The jitter of the clock signal is requested to be below 20 ps for the timing performance of the detector. Through the trigger and timing distribution network, the status information of the FEE and COPPER boards is sent back to the main trigger source for the data flow control.

B. Belle2link The triggered events are sent from the FEE boards to the  downstream DAQ system. We employ a unified data transmis- sion scheme between the FEE and COPPER readout boards so that we can develop the system at lower costs and achieve easy maintenance. This scheme, called Belle2Link, utilizes Xilinx Rocket I/O for high-speed serial data transfer. The bandwidth up to 3.125 Gbps handled by Rocket I/O is enough for the data transmission of the Belle2link, because the downstream band- width is restricted by the COPPER board which has one Gigabit Ethernet port for the data output.

The Belle2link consists of firmware modules in the FPGA on  the FEE board, a transmission path between the FEE board and the COPPER board and a receiver card attached to the COPPER.

In the FEE part of the Belle2link, the firmware modules collect the data digitized by the FEE and trigger and timing information from the FTSW. The data are then stored in a FIFO and a header and a trailer are attached. The formatted data are transferred from an SFP transceiver through an optical fiber which provides electrical isolation between the FEE and the readout COPPER boards. A dedicated clock by a high precision on-board crystal is used for the data transmission. The sent data are received by a newly developed daughter card on the COPPER board, called as a High Speed Link Board (HSLB). It has a SFP transceiver, a Virtex-5 FPGA and an interface connector with the COPPER board.

C. COPPER Readout System Owing to the unified data transmission scheme from the FEE  boards, the readout boards in the DAQ system can also be uni- fied. We employ COPPER boards to collect data from the FEE boards of all subdetectors except for the PXD. The COPPER boards were used in the Belle DAQ system as a readout system and we have much experience with them. The block diagram of COPPER is shown in Fig. 2. The basic functions of the COPPER board is to receive data via the Belle2link, process and send the    YAMADA et al.: DATA ACQUISITION SYSTEM FOR THE BELLE II EXPERIMENT 1177  Fig. 2. Block diagram of COPPER used for the Belle II experiment. HSLB, TTRX and PrPMC are daughter cards on a COPPER board. The HSLB receives data from the FEE and store them in on-board FIFO. User programs on PrPMC read out the data and send them downstream via Ethernet. The TTRX communi- cates with the FTSW; it receives clock signals and sends the status of COPPER.

data to a readout PC. The COPPER board has four slots to attach FINESSE daughter cards, which are HSLB cards in the Belle II DAQ system.

The COPPER board is equipped with a Processor PCI Mez-  zanine Card (PrPMC) to read and process data from the HSLB cards through a PCI-local bus bridge. A new PrPMC card for the Belle II experiment was developed by KEK and Densan Co. Ltd [11]. It includes an Intel Atom 1.6 GHz CPU, 512 MB DRAM and a Gigabit Ethernet port for data transmission. There is an- other slot for a PMC daughter card called as Trigger Timing Re- ceiver (TTRX), through which the COPPER board receives an external clock and send its status information to the L1 trigger source.

The COPPER board is a VME-based board but a backplane  is not used for data transfer for the Belle II DAQ. Data re- ceived by a HSLB are formatted by the FPGA on the HSLB and stored in a 1024 kB on-board FIFO on the COPPER board.

There are four such FIFOs, each of which corresponds to one of four FINESSE cards.When the FIFO?s occupancy level exceeds a threshold, hardware interrupt signal is sent to the PrPMC and DMA data transfer from the FIFO to the memory on the PrPMC is started. During the data transfer, driver software collects and merge the data from the four FIFOs. Like other Belle II DAQ computer nodes, we employ Linux OS to run DAQ software on the PrPMC. The DAQ software reads data from the FIFOs, for- mats, checks and sends them to a downstream readout PC via Gigabit Ethernet.

In the Belle II experiment, the maximum event size that one  COPPER has to handle is estimated to be around 1 kB from the innermost layer of the SVD. We produced dummy data with event size of 1 kB and event rate of 30 kHz by FPGAs on HSLBs and confirmed that a COPPER board can read out and send the data downstream with CPU usage of less than 60% on the PrPMC.

D. BASF2 Framework  BASF2 is a new software framework developed for the Belle II offline analysis software. It is also used for the online DAQ software so that we can port offline reconstruction software to the HLT easily. In the BASF2 framework, event-by-event data processing is divided into independent modules, each of which  is written by C++. The series of the modules for the data pro- cessing is defined and executed in a Python script file. Data ex- change between different modules is carried out through ?Data- Store? buffer manager which enable a module to obtain objects stored by other modules from the DataStore and store a new or modified object. The DataStore handles data as a ROOT object.

Therefore it is straightforward to store the data in a ROOT file.

When an object are transferred to other DAQ nodes over a net- work, serialization and deserialization are performed by BASF2 software running on each node.

E. Flow Control System During the DAQ operation, a sudden increase of the event  rate or slowdown in data processing on a DAQ node increases a buffer occupancy level in some DAQ nodes. Once the buffer be- comes full, some data are discarded and remaining data may be corrupted and it is difficult to proceed the data processing. The Belle II DAQ system employs flow control with the trigger and timing distribution network to avoid this buffer-full problem.

In the FEE boards of the Belle II experiment, trigger signals  are handled in a pipeline fashion so that the board can accept up to 5 consecutive triggers. The trigger source is prohibited to generate above 5 consecutive triggers to avoid the buffer over- flow in the FEE boards. The dead time due to this trigger pro- hibition is expected to be 3.4% at a trigger rate of 30 kHz. In addition to that, the FEE boards send their FIFO status to the trigger source via the trigger and timing distribution network. If the FIFO status is almost full, the trigger source does not issue a new trigger.

In the downstream from the COPPER readout boards, if some  DAQnodes cannot handle the increased data flow, back pressure will be applied to COPPER on-board FIFOs. A COPPER board has a register for the FIFO status, which contains status bits for empty, almost empty, between almost empty and almost full, almost full and completely full. In this case, the COPPER sends the FIFO status via the TTRX to the trigger source and trigger will not be generated while the FIFO status is almost full.

F. Event Building In the Belle II DAQ system, data from more than 200  COPPER readout boards are transferred to 50 readout PC and the event is partially built, called event builder 0. Data size reduction is also performed in the readout PC by merging redundant information in headers and trailers. The data are then sent to HLT input servers via network switches, which concen- trate the data from all subdetectors except for the PXD, called event builder 1. Network switches has many Gigabit Ethernet ports for the readout PCs and 10-Gigabit Ethernet ports for the HLT input servers. The estimated data flow between the readout PCs and HLT input servers is 3 GB/s at a trigger rate of 30 kHz. The network switches are required to handle this data flow with little packet loss to avoid network congestion.

The data from HLT output servers are merged on a storage server with the PXD data delivered from the ONSEN over a 10-Gigabit Ethernet network, called as event builder 2, which builds a full event before storage.

To study the network capability to handle data flow of 3 GB/s  between readout PCs and HLT units, a test with a down-scaled     system was performed. In the test setup, dummy data are trans- mitted from forty Gigabit Ethernet ports on five PCs to two net- work switches (ARISTA 7048T). The data are then sent to an- other network switch (ARISTA 7120T) via two 10-Gigabit Eth- ernet lines and finally gathered by one PC with four 10-Gigabit Ethernet interfaces which are aggregated together. In this setup, the maximum throughput of MB s was achieved. In the Belle II DAQ system, we will employ successor models of the network switches: four ARISTA 7048T-A will be used for the first stage of the switching network and four ARISTA 7150 for the second stage.

G. High Level Trigger When considering the limited resources of the storage, it is  not realistic to store all the data from the FEE boards after the L1 trigger. An online data reduction scheme therefore needs to be implemented. There are two ways to reduce the data rate; the re- duction of the event size and the reduction of the event rate. The HLT performs the event rate reduction by using reconstructed events. The HLT first applies an event selection based on track multiplicities, vertex information and energy deposit from data of the CDC and ECL. Further ?physics-level? event selection is then performed with the result of the full event reconstruction.

Since we use the common software framework, BASF2, recon- struction software modules developed for the offline analysis can be ported to the HLT without modification. After applying the event selections, the event rate will be reduced to 1/3 of the L1 trigger rate, which is estimated from the experience of the Belle experiment. The data flow will be reduced to 2/3 because the reconstruction results by the HLT are attached with the data.

The processing hardware for one HLT unit amounts to 24  worker PC nodes connected with input and output servers via a 10-Gigabit Ethernet network located in the same rack. Events are distributed by the input server to the worker nodes and pro- cessed in parallel. We will prepare ten HLT units at the begin- ning of the Belle II experiment and add more as the luminosity increases.

The reconstructed results by the HLT are used not only for  the event selection but also for the calculation of ROI on the DEPFET sensors of the PXD to reduce the even size. The ROI is derived from the tracking information of the SVD and CDC and sent to the ONSEN system. The ROI and high level trigger decision should be made and delivered to the ONSEN within 5 seconds, otherwise the ONSEN system cannot keep the raw PXD data in its memory.

H. PXD DAQ System Raw data from the PXD are handled by a dedicated DAQ  system because its even size is large (1 MB/event) and online size-reduction is necessary. Sensor signals of the PXD are dig- itized by a Drain Current Digitizer (DCD) chip on the same ladder and then sent to a Data Handling Hybrid (DHH) module, which is an Advanced Mezzanine Card (AMC) attached with an ATCA career board. Another AMC card for data concentra- tion (DHHC) on the same ATCA board collects the data from five DHH cards. The DHHC card also receives a L1 trigger and sends the selected events to the ONSEN system via an ATCA backplane. The ONSEN is also an ATCA module and stores the  data until it receives the information of the high level trigger and ROI selection from the HLT. The data rate from the DHHCs to the ONSENs will be 30 GB/s in total. Applying the ROI selec- tion, the PXD event size is reduced to 1/10 and together with the rate reduction by the HLT, the total data rate from the PXD is reduced to 1/30.

There is another option for the ROI calculation, which is per-  formed by a Data Concentrator (DATCON) AMC module. This module collects SVD hit information from the SVD FEE boards via an independent data path from the Belle2link and calculates the ROI with an FPGA using Huff transformation. The resultant ROI value is not as precise as that by the HLT but it can provide the ROI to the ONSEN more quickly.



I. Storage  The data from theONSEN and theHLT output servers are dis- tributed to storage units. One storage unit consists of a storage server and RAID array installed in the same rack of a HLT unit.

The RAID array has 4 units of 12 disk drives, each of which is 3 TB in size. After getting together the data flow from the ONSEN of 1 GB/s and the HLT of 2 GB/s, total 3 GB/s data are written on the disk. The recorded raw data files are then copied to offline file servers later. One of 10 storage units has already been installed at the experimental site. Using dummy data, we achieved to write data on the storage at the speed of 700 MB/s, which is more than twice as much as the designed value of 300 MB/s.

J. Slow Control  The slow control system of the Belle II DAQ system should cover the following items: ? run control for the trigger and DAQ system ? configuration for the FEE and subdetectors including HV  control ? online monitoring and logging To communicate between different DAQ nodes for the slow  control system, Network Shared Memory2 (NSM2) was devel- oped, which is a software library to enable the synchronization of the shared memory on each node using UDP and TCP com- munications over the network. The NSM2 is the upgrade ver- sion of NSM [12] used in the Belle experiment. EPICS is also used to handle complicated voltage control for the SVD and the PXD subdetectors. A bridge for the communication between NSM-based and EPICS-based systems is also prepared.



III. FUNCTIONALITY CHECK AT THE BEAM TEST  To demonstrate basic functionalities of the Belle II DAQ system, a test beam experiment with SVD and PXD sensors was performed in January of 2014 at DESY. In this test, a COPPER-based system and an AMC-based system were used for the SVD and PXD DAQ systems, respectively. The online reduction of the PXD event size was exploited with ROI information.

Although the number of readout electronics modules used for  this test is about 1/40 for PXD and 1/100 for non-PXD detectors compared with the full Belle II DAQ system, this DAQ system for the test includes most of the key components which should    YAMADA et al.: DATA ACQUISITION SYSTEM FOR THE BELLE II EXPERIMENT 1179  Fig. 3. Schematic diagram of the DAQ system for the beam test at DESY.

be realized in the Belle II experiment: trigger and timing distri- bution by FTSW modules, readout from the FEE by COPPER boards and AMC modules, event building, online tracking and ROI calculation by HLT, online data reduction of PXD event size by the ROI information, final integration of both non-PXD and PXD data and recording data on the storage. Therefore, this test is important as a proof of concept before moving on to scaling up to the full DAQ system.

A. Detector and Trigger Setup The schematic diagram of the DAQ setup is illustrated in  Fig. 3. The detector consists of 1 PXD and 4 SVD layers. The PXD layer has pixels of m m each and the thickness is m. For the SVD sensors, the 1st layer and other three layers have active area of mm mm mm  mm, respectively. The detector assembly was located inside a solenoid magnet generating 1 T magnetic field. The sensors were aligned along the beam line and irradiated by an electron beam whose averaged momentum can be varied between 2 and 5 GeV/c. Outside of the array of the layers, a beam telescope with monolithic active pixel sensors was located, which was provided by DESY test beam facility and used for confirmation of the beam tracks. A Trigger Login Unit (TLU) module pro- vided an external trigger based on the coincidence of four scin- tillators placed on the beam line. The trigger signals as well as clock were distributed over FEE boards and readout COPPER boards via FTSW modules.

B. DAQ System for the Beam Test Signals from the PXD and SVD sensors were transmitted  to FEE boards placed outside of the detector area. In the PXD part, a DHHmodule received digitized data from the sensor and a DHHC module collected them and sent L1 triggered events to the ONSEN node, which applied the data reduction and sent the reduced data to event builder 2. In this beam test, AMC cards were installed in microATCA crates instead of using ATCA boards. As for the SVD part, two FADC boards received signals from the sensors and digitized them. Two Fast Trans- mitter Boards (FTB) then sent data to the two COPPER readout boards via Belle2link. To test another option for the ROI calcu- lation, SVD hit information was also sent to DATCON AMC cards, where the FPGA based tracking was performed. After processing data on the COPPER boards, data were sent to a readout PC, where the data from the two COPPER boards were merged. The merged data were forwarded to a downscaled HLT unit which includes three worker nodes. In the HLT, tracking  Fig. 4. Event processing rate on the readout PC during the beam test at DESY.

software was implemented to calculate the ROI on the PXD sensor. The calculated ROI information was delivered to the ONSEN nodes and the event size was reduced. The two data streams were merged on a storage server and the built event was recorded in a ROOT file.

C. Status of DAQ System During Beam Test  In this beam test, the full readout chain from the sensors to storages was realized. The DAQ system was running stably and handled the throughput of the data which was around 1 MB/s.

The event rate can be varied with the beam tuning and was around several hundred Hz as shown in Fig. 4. The run con- trol on the basis of NSM2 and online monitor worked well and the EPICS system successfully control the sensor configuration.

Online tracking by the HLT is performed successfully with SVD data and data quality monitor showed a track in real-time.

D. Test of Backpressure Flow Control  As described in the previous section, the flow control system was implemented in the DAQ system for the beam test and it monitored the status of COPPER onboard FIFOs. One concern of the flow control is that the FIFO buffers may become full from the ?almost full? state by event data coming from the FEE boards due to the triggers issued before the trigger prohibition.

To avoid this problem, we need to set ?almost full? threshold level properly.

To check this flow control scheme at the beam test, we issued  dummy triggers to generate a large data flow which the DAQ system could not handle. The test setup is shown in Fig. 5. In this test, unlike the actual Belle II DAQ system, the HSLB was used as a dummy data source and the FTSW sent a trigger signal to COPPER so that the HSLB would write dummy data to FIFO on the COPPER board. The FTSW then monitored the status of the FIFO and blocked triggers when the FIFO status was ?al- most-full?. The result of this test with various input trigger rates is shown in Fig. 6. As the trigger rate is increased over 50 kHz, the CPU usage of DAQ processes on the PrPMC almost reaches 100%, which causes back pressure on the COPPER FIFO. The     Fig. 5. Setup for the test of flow control by the FTSW. Dummy data are pro- duced in the HSLB when a trigger signal from the FTSW is received by the TTRX. The status of FIFO on COPPER is sent to the FTSW and the trigger rate is controlled depending on the FIFO status.

Fig. 6. Ratio of transmitted (solid circles) or blocked (solid triangles) triggers to total input triggers with various input trigger rates. When CPU usage (open circles) on PrPMC saturates with higher input trigger rates, the FTSW starts to block triggers to avoid FIFO overflow. The event size of the dummy data is 1 kB.

ratio of blocked triggers then starts to increase. Even in this situ- ation where backpressure was applied to the COPPER, we took data continuously at the reduced trigger rate by the flow control system and confirmed that this buffer full never occurred.

E. Real-time Monitoring with Data Quality Monitor  Data Quality Monitor (DQM) was developed and used at the beam test for monitoring reconstructed events in the HLT. The DQM is based on an event server and event viewers. The event server collects pre-scaled events after the reconstruction by the HLT and create ROOT histograms. They are then distributed over the event viewers, which are invoked by users to monitor the histogram. In the beam test, we monitored various parame- ters online with this DQM, including hit-map, track information and the momentum of the electron beam.

F. Test for ROI Data Reduction Scheme  In the tracking software running on the HLT, raw data from the SVD are unpacked and converted to hit information of each row and column of the SVD strips. The obtained hit information is used to find clusters. The track finder uses those clusters as an input and finds tracks by the cellular automaton method [13].

The track parameters of the track candidates are calculated by the track fitter.

The ROI-based data reduction was first tested by sending some fixed patterns from the HLT to the ONSEN to check that an image of the PXD sensor plane was cut out correctly. After- wards the real ROI derived from SVD tracks was delivered to the ONSEN. Data from the beam telescope placed at both sides of the sensors were also analyzed to obtain precise information about the alignment of the sensors.

In the accumulated data of the hit position on the PXD sensor,  the PXD hits concentrates around the expected position from the extrapolated SVD tracks on the PXD plane, which confirms that the ROI data reduction scheme worked correctly.



IV. CONCLUSION  In the Belle II experiment, the luminosity is 40 times larger than in the Belle experiment, which is a big challenge for the DAQ system to handle the large data flow. The features of the Belle II DAQ system are the unified readout system from the frontend electronics boards, high-speed data transfer and pro- cessing with state-of-art techniques and products, precise event reconstruction with the offline analysis software for the online event selection, the ROI-based online event-size reduction and so on. The Belle II DAQ system is now under development for the start of Belle II physics run in 2017. In the beam test held in January of 2014, we established a full readout chain and real- ized the online data reduction scheme with a down scaled DAQ system, which is an important milestone for the development of the full-scale Belle II DAQ system.

