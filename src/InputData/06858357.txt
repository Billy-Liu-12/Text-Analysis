DataCenter 2020: Near-memory Acceleration for Data-oriented Applications  Edward Doller, Ameen Akel, Jeffrey Wang, Ken Curewitz, and Sean Eilert

Abstract  In the years between now and 2020, we should expect continued exponential data growth [15][16].  A number of ongoing advances in storage: the transition to solid-state drives (SSDs), the scaling of NAND flash capacity, and advanced silicon packaging techniques will dramatically increase the capacity of storage subsystems over the same timeframe.  This will significantly reduce the ratio of storage bandwidth to storage density.  Consequently, the majority of data in 2020 will either be cold or will require near-memory acceleration to pull rich information out of the sea of big data.

We argue that, increasingly over time, value lies not merely in the size of the data, but rather in what one can do with it.

Introduction Non-volatile memories (NVMs) like NAND flash in solid-  state drives (SSDs) have changed the landscape of modern computing and storage subsystems; they enable significant performance improvements over incumbent technologies like rotating disk drives.  SSDs leverage relatively low-latency NAND flash memory to provide 10-1000X faster sector access than rotating disk.  From the systems perspective, the addition of NAND flash provides significant benefits. From the memory perspective, though, systems designed for rotating media leave memory bandwidth underutilized and add latency overhead for data access [4][8][11].

In big data problems, a system must separate the sparsely stored interesting data from one or more enormous datasets [8].  In this case, the processor shoulders the burden of filtering the incoming data down to an information-rich subset of interest.  Likewise, the storage system must transfer the (mostly unused) data across the storage link, resulting in unnecessary utilization, congestion [13], and power consumption [9][10].

To transform data into useful information, an architecture enabling near-memory acceleration through parallel filtering and preprocessing leverages high internal bandwidth, massive parallelism, and low latencies nearer to the physical location of stored data to perform routine data-oriented operations.

Many previous studies posit similar tradeoffs with disks [5][12], though we aim to prove the concept?s usefulness in SSDs.  We hypothesize that a solid-state storage system equipped with this capability will enable systems designers to dramatically improve system efficiency. Systems can offload tedious (but not computationally expensive) data manipulation tasks to the massively-parallel storage subsystem, which will provide information-rich datasets to expensive, currently underutilized, central processors [6][7].

Micron is researching this concept. We have coined the term ScaleIN to refer to storage and memory devices that are aware of the structure of their contents such that they can pre-filter and enable basic computation on the content at the lowest possible level.  We elaborate a few of the potential advantages of this approach.

Capacity & Physical Size Today?s datacenters scale out at the rate of ~petabytes per  rack due to the capacity limitations of rotating disks and the relatively high cost of SSDs.  For those with deep pockets, storage devices with capacities on the order of 1 PB/U are available today [14].  Though, following modern NAND scaling trends and advanced stacking techniques, we expect that in-package capacities will increase by more than 50X (by 16X via scaling and by 4x via stacking) by 2020.  This will enable straightforward deployment of exabyte-scale racks.

Surprisingly, the major problem with this scenario is not the cost.  At this density, system bandwidth becomes a major issue.  Even at 1 TB/s, it would take almost 2 weeks to traverse the data in one rack.  This is perhaps sufficient if the problem to be solved involves archival and retrieval of old photos of friends? kids? little league practices, family vacations, grandma?s birthday and Fido pestering a frog.  If the problem to be solved involves data manipulation, then system bandwidth must increase on the order of 1,000,000X.

We demonstrate our vision for near-memory filtering and preprocessing in today?s systems and highlight the benefits of providing solely information-rich data to high-performance processors.

Bandwidth Consider a representative 2.5? 6 Gb/s SATA SSD: Its  internal architecture contains 64 die organized as 8 die per channel on 8 channels enumerated in Table I and II:  TABLE I READ BANDWIDTH  Interface Type  Bandwidth Relative Increase  Host 0.6 GB/s 1 X Channel 333 MB/s/ch * 8 ch = 2.6 GB/s 4.3 X  Total Die 150 MB/s/die * 64 die = 9.6 GB/s 16 X   TABLE II BURST WRITE BANDWIDTH  Interface Type  Bandwidth Relative Increase  Host 0.6 GB/s 1 X Channel (Not Limited) 2 X  Total Die 20 MB/s/die * 8 die/ch * 8 ch = 1.2 GB/s 2 X Note that this analysis ignores write amplification, block  erasure, read/write collisions, non-uniform channel requests, and other factors.  It is intended to demonstrate the potential benefit of storage devices architected to minimize the impact of these factors.

With todays flash technology, our representative 2.5? SSD  contains 1 TB of flash.  With flash scaling and improved stacking technology the 2020 version of this SSD should scale to 64 TB with roughly 4X higher internal bandwidth.

The host interface bandwidth will also increase, but the overall internal/external bandwidth disparity in 2020 will grow to approx. 16X-32X?a 4X increase!  In other words, systems designers will have an increased need for near- memory acceleration in the foreseeable future.

Before jumping to the conclusion that one could push all computation to NAND and immediately reap enormous benefits, we must point out that modern NAND devices can operate with bit error rates on the order of 1%.  While there are some applications that can tolerate high error rates, for general-purpose applications, one should assume that one must operate on corrected data.  Since the correction algorithms and hardware are relatively complex, integration of these functions below the channel level requires significant changes to the memory devices themselves.  One could, however, architect a controller with many more channels and fewer devices per channel to achieve a higher bandwidth to capacity ratio.  Though, this change wouldn?t make sense in the current environment as any benefits would be squandered at the host interface but the rules differ considering near- memory acceleration.

For large systems, the bandwidth restriction is even more pronounced as many drives in a RAID or JBOD enclosure are all accessed through a single host bus adapter.  The degree of bandwidth limitation scales with the cost of the storage interface and is likely to amount to a cumulative bandwidth of less than 10 disks; though, the subsystem may have on the order of 100 disks.  Thus, we should count another 10-100X factor in bandwidth inefficiency at the rack level.

Latency There are numerous latencies involved in accessing data  from storage devices.  We can view the IO latency components as two major factors: software and storage hardware.  Software latencies refer to the time a storage request spends solely on the host processor, while storage hardware latencies refer to the time a request takes in the storage hardware (usually attached via an external PCI- Express or SATA interconnect).  When driving a slow storage technology, like rotating disk, software latencies often appear insignificant; however, when driving a much lower-latency medium, like a flash-based SSD, the software latencies become a much more significant latency factor [4].

Furthermore, system-level transfer times between the storage hardware and the underlying host processor can add noticeable latency to an IO request.

It is interesting to note that system-level features like OS- level IO caches, added to compensate for slow storage, represent a significant part of the software latency component.

These features frequently get in the way of achieving optimal performance from SSDs.  The industry is gradually refactoring systems to leverage the true value of SSDs.

IO latency factors on a Linux-based OS have been measured in [4].  The authors measure a non-optimized IO read request latency of 21.5us for a low-latency SSD.  The software latency component totaled 13.2us and the hardware transfer time required 0.93us/KB.  Compared to a 1-10ms rotating disk latency, the software and hardware transfer latencies are insignificant.  The additional overhead perceived for the software layers is far outweighed by the benefit of the bundled caching mechanisms.  Though, for a low-latency SSD with a 10-100us hardware latency, the additional 13.2us software latency adds significant overhead and can represent sizable performance degradation.

In the ScaleIN architecture, accesses to the storage subsystem from the host system occur at a much higher level of abstraction.  As such, accesses are much less frequent and  the impact of latency penalties for each access represents a much smaller overall factor.  In a conventional architecture, traversal of a data-dependent data structure?like a B-tree- based index?involves numerous dependent, serialized sector reads with each suffering the system level latency penalty.  In contrast, the same function in the ScaleIN architecture involves only a single access to the drive [e.g. tree_find(key, root, device)].  An application perceives the system-level latency penalty only once for the entire operation.

Additionally, in decoupling the lower-level IO requests from the host system, designers may more-easily optimize request latencies for each memory type.

Energy  The topic of energy consumption in complex compute and storage systems is not simple.  Results depend heavily on the specifics of the workload and hardware chosen.  Let?s consider some major categories of energy consumption and some generalities around them.

A. Compute  In the presence of an information-rich data source and a compute intensive algorithm, modern multi-core processors are very efficient at performing low-energy computation.

In many cases today, big data can be easily equated to sparse information.  In other words, large amounts of data must be processed in a trivial fashion to generate an information rich dataset that is intensively processed.  For this case, the central processor spends much of its time waiting on IO.  On receipt of the IO result, in the majority of cases the data doesn?t contain the desired information and is thus discarded.  Pushing these trivial operations to storage allows the processor to be used in a more efficient manner for computation or to go to sleep and save energy.

B. Aggregation  In many cases today, server-like systems are being constructed for the simple task of aggregating results from a number of simple storage devices.  It is arguable that many of these architectures will change to a more JBOD-like topology with the advent of advanced SSDs that can reduce data sets to a more manageable form.

C. Interface  As discussed in the previous section, transmission of data from an SSD ASIC through the host interface and HBA and into the processor core induces latency relative to the latency required to read the data into the SSD ASIC in the first place.

This movement of data to the host also represents an energy cost.  The specifics depend greatly on the system architecture.

MySQL Measurements  To demonstrate some of these concepts, we experimented with custom firmware versions of the Micron M500, a modern commercially available SSD.  The firmware was modified to perform list and tree searches with on-drive resources.  We created a host-side ScaleIN library that provides application programmers with a simple object-based API (e.g. list and tree creation, management, and traversal functions) while abstracting away the management of these structures across multiple underlying SSDs.

At the application level above the library, we created a custom MySQL storage engine.  This storage engine uses the ScaleIN library to perform all table operations necessary for management of its tables and indices.  Additionally, we  2014 Symposium on VLSI Circuits Digest of Technical Papers    modified the MySQL kernel to pass conditionals and indices to the storage engine.

We conducted experiments in two stages.  In the first stage, we limited the acceleration to list-search, effectively allowing the SSD to autonomously search through all records of a non- indexed table for those matching a specific criteria.  In the second stage, we built a system with 24 total drives and added B+-tree searching capability to the firmware.  This allowed us to experiment with searches of indexed tables.

A. System Overview  In stage one of our experiment, we employed 8 M500 SSDs attached to an Intel i5-2300 system with 8GB of DDR3 DRAM.  The motherboard in our base system allowed connection of up to six SATA drives.  In order to connect the full eight drives, we added an LSI 9212-4i4e PCI-Express 2.0 SAS controller [2].  We connected four SSDs to both the native controller and the LSI controller as a JBOD.  This left a native motherboard port for the host OS HDD.  We used Ubuntu Linux 10.04 LTS and test using MySQL 5.5.27.

In stage two, we increased the total to 24 drives with an external 2U SAS expansion enclosure connected to the same Intel i5-2300 system through an LSI 9207-8e PCI-Express 3.0 6Gb/s SAS controller.  This allowed access all 24 drives with up to 2.2GB/s bandwidth.

B. The ScaleIN Distributed Storage Engine  We focused our changes to the base system configuration on MySQL server and the Micron M500 SSD on-board firmware.  Because of the limited firmware space we optimized for the subsystem in MySQL that allowed for the greatest increase in parallelism and the greatest decrease in data movement with the least amount of complexity: the pluggable storage engine.  For this system, we limited our study to read-only transactions, though we may extend our implementation to include write-based transactions.

The ScaleIN distributed storage engine consists of two subsystems: the storage engine and the workers.  The storage engine subsystem resides on the host machine, while the workers reside in the firmware of the SSDs.  We spread database records equally across each worker in the system.

The storage engine is responsible for distributing queries among available workers.  Instead of retrieving each individual record or a collection of records from a drive for processing on the host CPU, the engine instructs workers to locally search their portions of the database and return results.

When a worker exhausts its local buffer space or completes its search, it returns the results back to the engine for aggregation and transmission back to the kernel.  Upon completion by all workers, the aggregated result is returned to the kernel from the storage engine and query results are subsequently returned to the client.  As an interesting side note, the kernel still checks records to ensure that they match  the search criteria but in this case it receives 100% hit rate of records matching the search criteria because mismatches were filtered by the workers.  The effect of this is that workers need not be perfect in their filtering of the mismatching data.

So long as they return all records that match, the search will be functional regardless of what fraction of the mismatching records they are capable of filtering.

For non-indexed searches, workers manage a list or set of lists contained within their storage.  For indexed searches, workers manage a B+-tree contained within their storage.  In both cases, workers accept commands from the storage engine directed at an on-disk structure that contains field definitions in its root sector.  Each command spawns a local search thread in the worker.  When a worker finds a record that matches the current query, it copies the fields of interest to a local buffer for that query.  Once the buffer fills, or the search completes, the worker indicates that results are ready for host-side transfer.  In the current implementation, each worker allows up to three concurrent queries.

ScaleIN Performance  This section explores the performance of the ScaleIN system running a simple MySQL benchmark.

A. MySQL Non-Indexed Query Performance  First, we explore performance searching non-indexed table field.  This search requires every record in the table to be sequentially examined, an O(n) operation.  We consider 32 cases of 10 threads accessing a non-indexed, read-only table with 10 million records: 40 or 4K Byte record sizes; stored in either the ScaleIN or MyISAM storage engine; and partitioned across 1 to 8 drives (Figure 1 (a) and (b)).

In the case of 40B records, MyISAM performs better than ScaleIN for cases where partitioning is across less than three drives (3.2X for one partition or drive).  This is expected, as the Intel core that runs MyISAM is much more capable for low-thread-count performance than the small core in the M500.  For cases where we partition across more than three drives, ScaleIN outperforms MyISAM by 1.24-2.28X.  For 4KB records, MyISAM outperforms the ScaleIN system with less than four drives (2.7X for 1 drive).  ScaleIN outperforms MyISAM with more than four (1.4X for 8 drives).

Regardless of the record size, host CPU utilization varies vastly between the two systems: MyISAM uses 100% and ScaleIN uses less than 1%.  This significantly impacts energy per operation.  More importantly, this factor leads to seamless linear performance scaling with the number of ScaleIN partitions in the 40B case.

To combat increasing database storage requirements, database designers frequently resort to techniques like sharding (which dedicates multiple database instances to small portions of a larger table).  The ScaleIN approach  Fig. 1 Performance comparison of both non-indexed (a)(b) and indexed (c) searches between the ScaleIN, MyISAM, and InnoDB storage engines.  The results in (a) and (b) are normalized to single-disk M500 results, while the results in (c) are raw measured values.

1  2  3  4  5  6  7  8  S pe  ed up  Number of M500s  (a) 40B Records  ScaleIN MyISAM InnoDB  1  2  3  4  5  6  7  8  Number of M500s  (b) 4KB Records         0  5  10  15  20  25  T ra  ns ac  tio ns  /S ec  on d  Number of M500s  (c) Indexed Records  2014 Symposium on VLSI Circuits Digest of Technical Papers    compliments sharding, as one can use multiple mysqld instances with the ScaleIN architecture.

B. MySQL Indexed Query Performance  Having demonstrated performance improvement on an O(n) non-indexed search by distributing the problem among many workers, we next analyze an O(log(n)) indexed search.

We expect less significant gains here, as fewer storage accesses are required in an indexed search.  We show query bandwidth in Figure 1 (c), in transactions per second versus number of drives.  Here, we observe a roughly 60% performance improvement for 24-drives.

In addition to a throughput improvement, we observed improved quality of service with indexed searches (Figure 2 (a), (b), and (c)).  While all engines perform better with more drives, the ScaleIN engine performs best in all cases except the trivial case of only one drive.  Note that these results are presented on a log scale.

C. Energy Comparison  ScaleIN also provides significant energy savings over conventional approaches.  We measured power consumption in each of the query cases benchmarked.  The data show that the linear search benchmark on MyISAM consumes 140W on average, while ScaleIN only consumes 81W due to reduced CPU utilization.  We measured high CPU utilization for all indexed benchmarks, which led to similar power measurements.  This implies that even complex functions pushed to the SSD do not increase power.  Figure 3 shows measured operations per unit energy.  When compared to MyISAM, we observe an energy reduction for indexed workloads of 1.57-1.72X for 7+ threads (1.13-3.96X for 2+ threaded non-indexed workloads).  This shows the energy efficiency of the highly parallel ScaleIN architecture.

Conclusion  Decision makers in the storage industry have begun to use terms like IO bottlenecks, memory bottlenecks, the memory wall, and others.  With some basic projections on scaling of storage densities and bandwidth, we showed that these problems should be expected to get worse over time, not to improve.  Near-memory acceleration changes the rules and thus allows us to think about these problems differently.

Near-memory acceleration can provide orders of magnitude increases in bandwidth to the enormous data sets of the future to compensate for the ever-increasing disparity between storage capacity and bandwidth.  This paper has demonstrated an example case where acceleration primitives provided real system value through improved scalability that may be realized through cost reductions, performance improvements, or energy reductions. With predictable advances in the storage industry, we will be able to store copious amounts of  data.  With realizable advances in near-memory acceleration, though, it will be possible to turn this big data into useful, information-rich data sets; a much more exciting proposition!

