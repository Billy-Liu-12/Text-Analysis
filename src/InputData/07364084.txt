On a New Approach to the Index Selection Problem using Mining Algorithms

Abstract?Considering the wide usage of databases and their ever growing size, it is crucial to improve the query processing performance. Selection of an appropriate set of indexes for the workload processed by the database system is an important part of physical design and performance tuning. This selection is a non-trivial tasks, especially considering possible number of native indexes in modern databases.

We introduce a new approach to the index selection problem using data mining. The method recommends the creation of indexes as well as the type of each index. This results in more precise index recommendations that allows not only to create ascending and descending indexes, but also special indexes supported by the database system. Mining of queries results in candidate indexes for which virtual indexes get created. As the approach does not require modifications of the database system, it is generically applicable. Evaluations of the scalability are given for different workloads for the document-based NoSQL database MongoDB.

Keywords-index selection; index type; frequent itemset; min- ing; performance tuning; query optimizer; MongodB; index recommendation; NoSQL database;

I. INTRODUCTION As the size of data is getting bigger, tuning performance  of databases is also getting more important to researchers, enterprises and database administrators. One of the aspects of database performance tuning is the challenge of choosing appropriate indexes [1]. Indexes are associated with the logi- cal organization of data which accordingly refer to physical address where the real data is stored. Having appropriate indexes in the system speeds up the process of accessing the data for processing. As much as having proper indexes in place improves the performance, having unused indexes not only wastes resources, but also has unnecessary overhead on the system that decreases its performance.

Therefore the Index Selection Problem (ISP) is a crucial challenge in physical design of databases that involves choosing a subset of indexes to be created on a collection in the database, in order to minimize the response time for workloads that are issued against the database [2]. Index selection is proven to be an NP-complete problem [3].

Atomizing the process of finding proper indexes in the database for its corresponding workloads improves the qual- ity of system management by minimizing the risk of missing indexes specially for new applications that an administrator has not yet being informed about.

In this paper, we propose a new approach to select best related indexes for frequent queries in a workload, which we  named Mining Index Selection Approach (MISA). Consider- ing an increasing number of native specialized indexes that nowadays are provided by each database, it is very important to take into account not only the fields to be indexes, but also the type of the corresponding indexes. The novelty of our approach is to consider recommendation of the type of index as important as selecting index fields themselves.

Index type selection is done by analyzing special operators that indicates the need for special corresponding indexes.

We utilize the query optimizer of the database itself to select best index. Each database usually has its own special query optimizer (either cost-based or rule-based) that analyzes queries, generates one or more query plans and eventually based on its criteria, determines the most efficient query execution mechanism [4]. We also take advantage of frequent itemsets mining algorithms [5] to build all combinations of fields for each frequent query.

By creating candidate indexes - determined by frequent itemsets - on a small representative subset of a real dataset, we provoke the optimizer to choose the best index for that workload from among all provided indexes. An advantage of this approach is to create virtual indexes that requires no modification in database?s code. We propose the indexes picked by the optimizer as recommended indexes. This way we are ensured that our recommended indexes, are consistent with what query optimizer considers to use.

Since none of so far developed approaches to the index selection has been applied to NoSQL databases and also be- cause of the prerequisites of the projects that we are involved in [6], [7] , we chose to apply our approach to a document- based NoSQL database, MongoDB. Despite this choice, our approach is not tied to any particular model of database.

In this paper, we also provide the results of scalability of our approach for three different in-production workloads and datasets from scientific meteorological projects that we are involved in [6], [7].

Section II is dedicated to survey related work. Our contributions to the index selection problem are listed in section III. In section IV, we first show that the number of possible indexes even for a not so large datasets are so enormous that we should limit them by only taking into account the fields that are appearing in frequent queries of a workload. Some terminologies and background knowledge are also described in the beginning of this section. This section contains two subsections that respectively explain      special scenarios for query processing regarding index se- lection and the architecture and algorithm of our Mining Index Selection Approach (MISA). Afterwards, in section V, a description of workloads and results of scalability of the approach are included. We finally conclude our work in section VI and also give an outlook to future work.



II. RELATED WORK  There has been a lot of studies and practices on the index selection problem. Douglas Comer in [3] shows that the Optimal Index Selection Problem (OISP) is NP-complete.

He proves that even a simplified version of this problem is at least as hard as OISP itself. His work is a motivation for the development and usage of efficient approximate algorithms that will result in finding good-enough or close to optimal solutions.

The early trend for making an index selection approach was to develop an external tool, mostly with help of linear programming optimization techniques [8]. In between these approaches, there were also intensive studies that benefit from data mining techniques, such as in [9], Aouiche et al.

investigate Close algorithm [10] to build all frequent item- sets. But despite huge effort that people invest in developing such approaches, their approaches were suffering from the separation of their optimization methods from the optimizer of the database itself. Having individual cost functions in the index selector and database optimizer could end up in situations that there might be some indexes proposed that the query optimizer would not consider for running queries.

Another pitfall with such approaches is their disconnection from the optimizer itself. When there is a newly updated version of the optimizer, the cost function of index selector might not be adequate anymore.

More recent approaches tend to consider involving the query optimizer of the database itself in the task of index selection. This is what all these approaches [11], [12], [13], [14], [15], despite their differences, have in common.

This approach ensures consistency between recommended indexes and the indexes that the optimizer actually is using and eliminates the worry about recommending indexes that would not be used by the optimizer.

In the work by Chaudhuri et al. [15], although they use the query optimizer to decide about which indexes to use, they call the optimizer more than once. They also modify the database server to support the simulation of the existence of hypothetical indexes (virtual indexes) in an environment called ?what-if?. In order to empower the optimizer to estimate the cost of each query with hypothetical indexes, they load the catalog tables of an SQL server and the corresponding distribution information of the indexes.

In [12], Valentin et al. first show that the number of possible indexes for a given table with n columns is very large. Therefore, in order to limit number of indexes that will be presented to the optimizer, they developed an algorithm  that combines various set of columns- of EQUALITY and RANGE predicates - that might be exploited in a virtual index. They generate the statistics for virtual indexes based on information on index cardinalities, B+-Tree levels, and the number of leaf pages and non-leaf pages deducted from corresponding table and column statistics.

In [14] and [13], a clustering and a heuristic method is used respectively, to create proper combinations of indexes to be presented to the query optimizer.

The following section lists the contributions that our ap- proach makes in providing a solution for the index selection problem on modern databases.



III. CONTRIBUTIONS Our solution to index selection problem and our developed  Mining Index Selection Approach (MISA) are distinct from the above related work in several ways. The uniqueness of our approach relies in combination of:  ? Recommendation of index type for proposed indexes.

? Using frequent itemset as a heuristic method to build a  certain order of combined indexes out of fields of each frequent query.

? Use of query optimizer to select the final recommended indexes.

? Our approach to create virtual indexes which eliminates any modification in the database server code.

? Applying the approach to a document-based NoSQL database.

In most modern databases, the means for having special- ized indexes like spatial indexes or indexes on texts are provided. Nevertheless none of the present index selection approaches has recommended the type of the index. They only recommend the combination of the fields that the index should be created on and assume that all of the indexes should simply be sorted either in ascending or descending order. Whereas in this paper, we first introduce a formula to calculate the number of possible indexes on fields and then in our approach, we introduce index type recognition for the first time. We are as concerned about recommending indexes types as we are concerned about proposing the right index combinations. Having such features in such a database management accomplice will not only reduce the overhead for the administrator of the system to choose proper indexes, but also provide a good skeleton for developing an automatic index selection system.

Since every database optimizer has its own cost function to estimate query plans, we chose to propose all important combination of possible indexes for each query as a batch to the optimizer of the database, instead of developing an individual cost function. Accordingly, we rule out the risk of recommending an index that might be ignored by the database optimizer. In addition, proposing all queries as a batch to the optimizer minimizes our calls to it to one per workload.

In order to create all of the index combinations, we utilize two well-known mining algorithms for association rules discovery: Apriori [16] as a Breadth-first search (BFS) approach and Eclat [17] as a Depth-first search (DFS) ap- proach [18]. Both of these algorithms build frequent itemsets out of a workload?s queries. By this approach, we mitigate the construction of combinations for not so frequent queries, and therefore, lessen the overhead on the optimizer.

Although we considered several approaches to create Candidate indexes for the optimizer to choose and return appropriate ones, we followed through with creating all combinations of indexes in a certain order on a representative subset of the collections (see section IV).

This approach works more effectively on database systems with good design, i.e. that similar structred data are placed in the same collection. Having such design ensures us that the random subset of documents are representative of the whole collection that we want to recommend indexes for.

By this approach, not only we put a negligible overhead on the database, but also we do not need any modification of the database at all. Hence, although we are taking advantage of the optimizer decision making, no interference and changes in the database is needed. This enhances the value of our approach, because it can be easily adopted by other databases.

The problem of choosing proper indexes for a workload of queries has been only studied on relational databases so far, but we show the result of performance measurements of our approach on a MongoDB database, a document-based NoSQL database. In order to mitigate the complexity of index selection problem, we develop an efficient, generic and straightforward method that can be adapted to other types of NoSQL as well as relational databases with low level of implementation overhead.

In the following section we explain in detail our approach and the designed architecture.



IV. MISA APPROACH  We refer to the index set that should be proposed to the query optimizer as candidate indexes and the indexes that the will be either recommended to end user or created on the real dataset at the end as recommended indexes. We also address the set of queries issued on collections of the database by users as workload.

Number of Possible Candidate Indexes By considering the fact that there might be s possible index types including traditional ascending and descending indexes and special index types, e.g. spatial, text, etc. indexes, the number of possible indexes on modern databases for a collection with n fields in their documents can be calculated from the following formula:  n?  k=1  (sk)n!

(n? k)! (1)  where k is number of fields and k ? n. There are n possible choices of indexes for the first field of the document. Co- ordinately, for the second field, there remain (n-1) choices.

Adding more fields to the combination of the indexes the total number will grow to a n(n? 1)(n? 2)...(n? k + 1) or n!/(n ? k)!. This number, for each given k, should be multiplied by sk, that adds the possibility of having several types to the equation. As a result of formula (1), even for a collection with an average of 5 fields in each document and possibility to generate only 4 indexes, the number of possible indexes exceeds 150,000.

In order to omit the number of candidate indexes, we decided not to pay attention to the dataset in each collection, but to the series of queries that are issued on each collection in the workload and make a combination only on the fields that appear in the queries. We assume that the users have primary knowledge of the datasets and do not frequently query for the fields that do not exists in the database. Since the whole operation of analyzing the workload, generating candidate indexes and letting the optimizer choose between them is costly, we do not want to process all of the queries in the workload, but only those which are issued frequently.

Thus we use a frequent itemsets algorithm.

We considered usage of several known frequent itemsets algorithms. To the clarify functionality of frequent itemsets, we should mention that we are dealing with items (in our case, fields of each query) and baskets (in our case, queries).

Each basket consist of a set of items. If I is a set of items, the support for I is defined as the number of baskets for which I is a subset. We say I is frequent, if its support is equal or more than a configured support threshold [19].

We implemented Apriori [16] and Eclat [17]. Both of these algorithms build frequent itemsets out of query work- load. So if a query is frequent enough - above the administra- tor configured threshold - we make a set of combination out of its fields. Not so frequent queries will be automatically ig- nored by this method. By using frequent itemsets algorithms, we apply a simple heuristic: although all combinations of a query fields are build, only one order of each combination is presented to the database as candidate indexes. Since we are taking advantage of mining algorithms to propose a solution for the index selection problem, we named our approach as Mining Index Selection Approach (MISA).

A. Special Queries Scenarios  We apply our approach for the index selection problem to a MongoDB 2.6 database[20], a document-based open source NoSQL [21] database. Each MongoDB instance can contain several databases and each database can hold a number of collections, which are corresponding to tables in relational databases [22]. A collection is a set of BSON [23] documents that correspond to rows in SQL terminology.

Each document can contain several key-value pair, none as a field in MongoDB terminology, that can be considered as     Workload  User Interface  MongoDB Optimizer  Profiles  Syntax Analyzer  Miner  Config Evaluator  Creator   Virtual Index Collection  Index Extension File        9 10  DB Server  R Environment  Prime-Set  Candidate-Set  Query-Set  Chosen-Set   MISA Components  MongoDB Components  Figure 1. The Architecture of Mining Index Selection Approach (MISA).

column in SQL databases. The primary key in MongoDB is automatically set on ? id? field. This field is generated by creating any document in the database automatically and is always indexed. In addition to ascending and descending indexes on single fields, MongoDB also offers compound indexes on a combination of fields. In addition, geospatial, text, and hashed indexes can be created, as well as multi-key indexes on arrays [24].

For analyzing the queries and statistical computing, we utilize the R environment [25]. In order to connect to Mon- goDB from the R environment, the rmongodb package [26] was used. Frequent itemsets are used by including arules package and make use of its Apriori and Eclat algorithms.

In pursuance of making sure that frequent itemsets is the proper algorithm to build combination of indexes, an under- standing of MongoDB optimizer behavior is also helpful.

The query optimizer of MongoDB is described in [27]. It applies the following rules to select index for a query:  1) If query includes a sort part, it tries always to sort using an index.

2) It attempts satisfying all fields in the query selector with useful index constraints.

3) If the query includes a range or a sort, then optimizer chooses an index that last key used in the index can satisfy the range or sort.

If there is no obvious index to utilize, based on the above mentioned rules, the optimizer uses a simple heuristic to choose the index: it checks the number of required document scanning for each of the proper indexes and selects the one with least number of scanned documents. If more than one  index fulfill these rules and considered to be optimal, then one of these optimal indexes will be chosen arbitrarily.

None of the above rules implies any restriction in ben- efiting from frequent itemset, contrarily they approve that building combinations of fields appearing in the query and presenting them to the optimizer to choose is a proper solution.

Yet, we like to make as few combinations as possible.

Therefore, as will be shown in subsection IV-A, as soon as we detect some specific fields or operators, we take a corresponding action to limit the combination set.

In order to narrow down number of possible itemset combinations, we restrict our suggestions as soon as we detect certain fields or operators in the query. Some of these special cases are described in the following:  id Field: As mentioned earlier in section IV, the id field is created and indexed automatically in MongoDB. In some workloads like WKL 3, which will be discussed in section V, users might issue a primary query to get all of the id values and then start querying documents by their unique identifier. But since this field is already indexed, there is no need to trigger MISA for queries that contain id field. Even if there are other fields appearing in this that query, still the query can be answered by id index. Therefore, when this field is detected in a query, we simply skip processing the query. It means that query?s fields would not be added to the Prime-Set and the query itself would not be added to Query- Set in Figure 1. (Prime-Set and Query-Set are introduced in subsection IV-B)     2dsphere Operators: MongoDB has a number of special indexes [24], including a 2dsphere index to handle geospatial information. There are specific operators to query geospatial datasets in MongoDB [28], that mostly include either $geo or $near. Therefore if these patterns would be recognized in conjunction to a field in a query, that field will be associated with 2dsphere lable and only 2dsphere index will be suggested for that field. For example, consider the following query:  db.places.find( { loc: { $geoWithin: { $geometry:  { type : "Polygon" , coordinates: [ [ [ 0, 0 ], [ 3, 6 ], [ 6, 1 ], [ 0, 0 ] ] ] }  } } } )  This query looks for all of documents that their ?loc? field contains a value (array of longitude and latitude) equivalent to geolocations that are within the specified coordinates which make a polygon. If this query is frequent enough, a {loc: 2dsphere} index would be proposed.

text Operators: Texts can also be indexed in MongoDB.

There are specific evaluation operators for querying text that all contain $text pattern. Although this seems straightforward and similar to how we handled geospatial index proposal, in practice we could not use this approach to introduce text indexes. Because running a query with a text operator on a field without having text index on it, results in an error in MongoDB. Therefore, the current version of our approach does not provide text index proposals.

sort Operator: As described by the optimizer decision making rules in the beginning of section IV, the fields that appear in sort part of a query should be considered and are better to be in the last position in an index. Therefore, for a query with both selector part and sort part, all of the fields in both parts are considered. The value of sort fields is taken directly as the value for the field?s index.

The sort fields are combined with fields that appear to the selector part of the query, but will be placed after them. If a field appear both in selector part and sort part of a query, in order to avoid duplication, only the sort field and its value are added to prime-set. As an example, consider the following query:  db.fromR.find( { "item" : 6 , "name" : "house" } ).sort( { "item" : 1 } )  This query looks for all documents, in which ?item? field contains value 6 and field with ?name? key contains ?house? string value. In order to look for the matches, the documents are requested to be sorted based on their ?item?  fields. According third rule of MongoDB optimizer, the sort key should appear last. Therefore, a combination as {name:1, item:1} is added to prime-set.

If no special type is recognized then the field would be associated with a simple ascending index recommendation, like {attribute : 1}. In the following, the designed structure of our approach is described.

B. Architecture and Algorithm of MISA  The architecture of our Mining Index Selection Approach is depicted in Figure 1. In order to utilize MISA, users should first trigger it and give the name of their target database and collection as input. This step is labeled as step 1 in Figure 1.

For the sake of tracking the queries to the database, activation of profiling service of MongoDB is required.

Then in step 2, the Syntax Analyzer queries the database for the queries of the corresponding collection from within all activities that are profiled in profiler of MongoDB. The query get executed in the server side and the results are send back to function. The set of queries that are send as respond in step 3 makes the workload for MISA. Then the Syntax Analyzer by parsing the field of each query would associate each field?s attribute with its proper index type based on what has been described in subsection IV-A. The current version of MISA is considered to propose indexes only for query operations. Therefore, although the Syntax Analyzer keeps a record of number of read operation in the workload and also write operations (like insertion, update, and deletion), it only parses the fields of read operation queries.

Each query operation is added to the Query-Set that subsequently is passed to Config Evaluator. Query-Set does not contain any duplicated query. For each query, the com- bination of all of its fields and their conjuncted index type makes a new entry in Prime-Set. Since more frequent queries should have higher priority to be recommended, none of the similar queries are eliminated from this set. The Prime-Set is passed in step 4 to the Miner.

The Miner executes the mining algorithm. Users have the possibility to choose their arbitrary mining algorithm to build the itemsets. Currently, the support threshold for frequent set algorithms are set manually by the user. There is also a user-definable error level to eliminate the output of frequent itemsets, if they are too many for the system to handle all of them in RAM. The itemsets above this error level makes the Candidate-Set that in step 5 is passed to Config Evaluator.

Since number of entries in the Candidate-Set can be very large, creating all of them on the database makes a huge overhead and demands lots of storage and processing time.

Therefore, as mentioned in section III, we chose to create indexes of the Candidate-Set on a representative subset of the corresponding collection. Thus, no modification to the database server is required. This method works best on the database systems that has a unified design in which all the     similar - not identical - structured documents are placed in one collection.

For this purpose the Config Evaluator copies a subset of the corresponding collection to the Virtual Index Collection and creates all of the indexes in Candidate-Set on this virtual collection, labeled as step 6. Then it re-run the queries in the Query-Set against this virtual collection in step 7. Consecutively, the query optimizer of the database gets automatically engaged and choose the proper index for planning to run the queries. Afterwards in step 9, the Config Evaluator controls which indexes are chosen and adds them to the Chosen-Set.

Depending on user?s configuration, the Chosen-Set in step 10, can either be recommended to the user or be passed to Creator to automatically create them on the target collection in step 11.

In the following section, some results of performance analysis of the implemented approach are depicted.



V. PERFORMANCE MEASUREMENTS  To analyze the performance, first some proper workloads are needed. Since we do have access to the data and log of the queries issued to the production database used by meteorologists [6], [7], we decided to measure performance of MISA with use of real datasets and workloads. Although there are several collections with different sets of workload to them in production system that the MISA has been tested on, but for the sake of concision only the results of three of them are discussed in this paper.

Table I presents a summary of these three workloads and their corresponding dataset in the database. The work- loads are named as WKL 1, WKL 2 and WKL 3. These workloads are the result of monitoring and logging the queries processed by the database in the same period of time. Since currently we ignore all of the queries containing write operations and do not propose any index relating to such operation, these workloads are pruned from their write operations. Therefore, the number of queries in table I for each workload shows only number of read operations.

The first workload is WKL 1. Although it is issued against a fairly large number of documents in the dataset, it contains mostly rather simple queries. That is the reason that as shown in table II number of Candidate-Set is only 1. This workload contains special queries with 2dsphere operators which MISA could successfully recognize and suggest corresponding index for them. The number of queries in WKL 2 is not that large, but this workload contains the most complicated queries among all three of these workloads. One example of complicated queries in WKL 2 is as following:  find({ "obs_mode" : 39169, "zpd_time" : {  "$lte" : [ 2005, 2, 22, 3, 59, 59 ],  "$gte" : [ 2004, 12, 31, 20, 0, 1 ]  }, "version" : "V5", "avail_tang" : 17 }).sort({ "zpd_time" : 1 })  This query searches for all of the documents that their corresponding values for ?obs mode?, ?version? and ?avail tang? are respectively 39169, ?V5? and 17. Also the ?zpd time? field of the matches should contains a value indicating the time later than 31st of December 2004 at 20:00:01 and prior to 22nd of February 2005 at 3:59:59.

In order to find the match, this query request the documents to be sorted based on their ?zpd time? values.

Such complicated query requires paying attention to the order of range and equality matches as well as a careful study of the appeared fields cardinality in the database. In addition, there is also a field in sorting part of the query.

But as shown in Figure 2 for WKL 2, it takes less than 2 seconds for MISA to build all combinations of this query?s field, build them on the virtual index collection and propose the optimizer selected combination as the required index, even for matter of 600 queries. As presented in table II, there are 15 different combinations that are presented to the Virtual Index Collection for this workload. The index proposed by MISA, in this case, is the following combined index which contains all of the fields of the query and has the sorted key in its last position: {"avail_tang" : 1, "obs_mode" : 1, "version" : 1, "zpd_time" : 1}  WKL 3 is our largest workload which contains much more queries. These queries are applied to a dataset that although contains fewer number of documents, but each document includes a number of multi-dimensional arrays [7] which makes the size of this dataset much larger in comparison to the other datasets. This workload contains lots of queries with the special field id in them. According to table II, the only Chosen index for this workload does not include id.

Eliminating fields of id from queries has surely a positive effect on the performance of MISA, especially for such a large workload.

One can conclude from the characteristics of the work- loads presented in table I and their corresponding proposed candidates shown in table II, that each of these workloads contains mostly not identical but similar queries. This is because each of these datasets are used for specific applica- tions. Nevertheless, since each of the queries is processed by MISA individually, these workloads are proper to be used as input of MISA for performance analysis.

There are several performance aspects of the MISA that     Table I SUMMARY OF WORKLOADS AND THE DATASET THAT WORKLOADS ARE APPLIED TO.

Worklods Queries in Workload (#) Documents in Dataset (#) Size Of Dataset (MB) WKL 1 1,591 6,375,825 1459.32 WKL 2 678 4,707,583 2226.80 WKL 3 30,879 2,118,921 6437.12  can be measured. In Figure 2, the results of scalability of MISA for each of our chosen workloads are exhibited. For each of the diagrams, x-axis contain different number of queries in the workload and y-axis indicate average run time of MISA to recommend indexes. In order to eliminate any caching effect on these results, before running each test, database is restarted and also the memory of the R is cleared.

The average time shown on y-axis contains only the time that MISA takes to propose proper corresponding indexes.

The time for creation of the indexes on the corresponding datasets are not considered in these results. The error bars are the results of standard deviation.

All these graphs demonstrate that the MISA scales very well with the number of queries. Even for complicated query sets like in WKL 2 and for large number of queries like in WKL 3, the average time taken by MISA is ideal (less than 2 and about 35 seconds, respectively).

Considering the fact that none of the other approaches has studied the matter of index selection with use of a NoSQL database a comparison of our results to others might not be completely fair. Because they all used a SQL language in their input workloads, they could use a unified workload set and therefore could compare their results easily with each other. Nevertheless the effect of having different workloads seems negligible, when one compare number of queries in our tested workloads and corresponding response time of MISA with other approaches. As a concrete example, one can look into the results of scalability in [29]. They tested scalability of their system for only up to 25 queries and showed that their algorithm response time is less than 10 seconds for such number of queries. Whereas we tested MISA with above 30,000 queries of WKL 3 and received recommendations in about 35 seconds. Also for 600 compli- cated queries of WKL 2, MISA requires less than 2 seconds to recommend proper indexes.

As mentioned before, our approach is highly dependent on the level of support threshold configured by the user for  Table II NUMBER OF CANDIDATE INDEXES THAT INTRODUCED TO VIRTUAL  COLLECTION AND NUMBER OF RECOMMENDED INDEXES THAT PROPOSED TO USER.

Worklods Candidate Recommended indexes (#) indexes (#)  WKL 1 1 1 WKL 2 15 1 WKL 3 3 1  frequent itemset algorihtm and frequency of a query in a workload. As a result, for a worst case scenario where all queries are different in a workload or no query is frequent enough to be considered by the frequent itemset algorithm, MISA proposes no indexes to be created. This is a rare situation for a database in production and even if such scenario happens, one might tolerate slow answer for a one- time issued query from a database.



VI. CONCLUSION AND FUTURE PLANS  In this paper, we proposed a generic method to solve the index selection problem. The novelty of our approach is to consider proposing type of indexes as important as proposing the field to be indexed. We based our approach on usage of corresponding database?s query optimizer cost function to choose in between of proposed candidate to it. We proposed the usage of frequent itemset algorithms to build combinations of candidate indexes. In order to avoid modifications of the database code, and also avoiding creating all candidate indexes on the corresponding dataset as well as speeding up the process of choosing best indexes via optimizer, we proposed applying candidate indexes to a representative subset of dataset.

Our approach is generic and can be applied to any type of databases. We had two reasons for applying our MISA approach to a MongoDB database. We utilize MongoDB in our production systems for several scientific meteorological projects. Another reason was the fact that index selection solutions has so far never been applied to any NoSQL database for scientific purposes.

In this paper, we showed that the result of scalability for this approach with use of several real production workloads on their corresponding real datasets, which each contains different level of complexity in their application queries.

Based on our results we plan to release an open implemen- tation of MISA with a convenient user interface. We want to consider the ratio of read-to-write operations of workloads in order to optimize the number of available indexes. Also, we will add support for sparse indexes.

