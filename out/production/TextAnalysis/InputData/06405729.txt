FHAR A New Text Association Rule Algorithm Based On Concept Vector and its  application

Abstract?A novel text association rule approach FHAR algorithm is presented. To overcome the defect of traditional keywords which does not take into account the semantic relation between keywords, FHAR algorithm in the paper is based on concept vector. The density of semantic field and the weight of meaning are used to determine the concept of the keywords, which not only adds the texts semantic, but also reduces vector dimensions; FHAR algorithm adopts improved HASH table for efficient large itemset generation. The stored address of itemsets is determined by a new hash function. Based on the new hash table, tree structure is constructed. When FHAR algorithm is applied to text mining, the text association rule is derived. Experiments show FHAR algorithm possesses higher efficiency and accuracy than Apriori algorithm.

Keywords-association rule; text mining; concept vector

I. INTRODUCTION Mining  association  rules  in  transaction  databases has  been  demonstrated  to be useful and technically feasible in several  application areas[1],particularly in retail sales. Let I={i1,i2,?,im} be a set of  items. Let D be a set of transactions, where each transaction T is a set of items, such that T I? .An association rule is an implication of the form X Y? , where X I? , Y I? ,and X Y? ?? .

Association rule in Chinese text mining is to find out relationship between different eigen words from the texts [2].

The texts D is considered as transaction database I={i1,i2,?,im},which is the set composed of m eigen words.

The text in D is regarded as the transaction in database. The text consists of eigen words, i1,i2,?,ik, which belong to I.

The large itemset of the eigen words is derived to find association rule satisfying the minimum support value. The association relationship between the texts is known by extracting the relationship of  the eigen words.

Apriori algorithm is ofen applied to association rule[3].

The main defect of Apriori algorithm is the problem of scanning the database repeatedly.

FHAR algorithm adopts improved HASH table for efficient large itemset generation[4]. The stored address of itemsets is determined by a new hash function. Based on the new hash table, tree structure is constructed.

The traditional text mining based on keywords does not take into account the semantic relation between keywords[5],  which causes the concept of the text vector is not accurate enough. The selection of meaning and adoption of density semantic field is to determine the concept of the keywords.

Then the text vectors are transformed to concept vectors, which not only adds the texts semantic, but also reduces vector dimension. FHAR algorithm possesses higher efficiency and accuracy than Apriori algorithm.



II. CONSTRUCT CONCEPT VECTOR Descriptive language in HowNet is concept and meaning  [6]. Concept is a description of word semantics. Descriptive DEF presents a concept. Meaning is the smallest meaningful unit of a concept.

The density of semantic field, which is sum of the frequency of the word in a text, is used to measure magnitude of the text to the word.

Definition 1: Polysemant W, the density of semantic field of the i DEF:  ( ) ( )  n  i j j  density w tf t ?  ??  (1) Where tf(tj) is the frequency of the j word tj in the i  semantic field in the text, n is the word number in the semantic field.

Definition 2: The value DEF is determined as formula (2) max( ( ))iDEF density w?                                    (2)  Definition 3: The formula of the weight of meaning is as followed.

1( ) (log( ) )i tree rootw DEF w d a cn b ?      (3) Where W(DEFi) is the weight of the i meaning in  DEF,wtree is the weight of the concept tree of the meaning, droot is the level of the concept tree.

Definition 4: The definition of meaning is as followed.

max( ( ))jjw w DEF?                                         (4)  Where The  meaning with largest weight is used as the ultimate meaning  of  word w.

Concept vector space is not based on the words. The eigen word set of the texts are acquired after Chinese word segmentation and preprocessing. Concepts are extracted from the keywords of the texts. The concept vectors are constructed. Cvc algorithm is as followed.

DOI 10.1109/MINES.2012.113    DOI 10.1109/MINES.2012.113    DOI 10.1109/MINES.2012.113    DOI 10.1109/MINES.2012.113    DOI 10.1109/MINES.2012.113    DOI 10.1109/MINES.2012.113    DOI 10.1109/MINES.2012.113    DOI 10.1109/MINES.2012.113     Cvc algorithm: Input: Eigen vector of the texts:  vt(d)={t1,w1(d);t2,w2(d);?;tn,wn(d)},threshold? Output: concept vector of  the  texts: vc(d)={c1,w1(d);c2,w2(d);?;cm,wm(d)} 1) Select keyword ti from text d, judge whether ti exits  or not in Hownet.

2) if ti is unlisted word  if the word frequency of  ti wi(d)< ? then filter  out  Else the concept of ti is ci={ti},and concept c1 and word frequency wi(d)  are joined in the concept vector vc(d)  else ti is listed word, then query Hownet. The  conc- e pt of ti is acquired with semantic field and  meaning.

If ti has one definition of DEF, then goto If ti has mantic fields, then uses formula (1)  and (2) to remove ambiguity of ti According to formula (3), compute the  weight wi(sj) of meaning sj in DEF ; according to formula(4),the largest weight of meaning ti is used for the concept ci of ti,  max( ( ))i kkc w s? ,moreover, compute the word frequency of ci  if wi(d)> ? i joins the concept vector vc(d)  else filters the meaning sj.

3) Repeat the above steps, until the keywords of the  text d are scanned. The text concept space is   1 1 2 2( ) { , ( ); , ( );...; , ( )}c m mv d c w d c w d c w d? .

1 1 2 2( ) { , ( ); , ( );...; , ( )}t m mv d t w d t w d t w d?  is converted   to  1 1 2 2( ) { , ( ); , ( );...; , ( )}c m mv d c w d c w d ct w d? with concept mapping, where m n,ci is a mutually independent concept, which is some synonym aggregate, not simple word. Wi(d) is the weights of different concepts.



III. FHAR ALGORITHM FHAR stands for tree-based association rule mining with  HASH table, which represents the sets of candidate and frequent itemset to reduce the computing resources. FHAR stores all the itemsets found in a tree data structure. This compact representation provides substantial storage savings when compared to the Apriori algorithm. HASH table can improve the efficiency of query. FHAR refers to such potentially interesting as ?relevant itemsets? instead of ?frequent itemsets? We will use the following algorithm to find all the relevant itemsets:  Set.Init (MinSupport)  Itemsets=set.Relevants(1); K=2;  While (k? columns && itemsets k) { itemsets=set.candidates(k);  If (itemsets>0) Itemsets=set.Relevants(k);  K++; }  Where set denotes the itemset tree data structure. The method Init creates and initializes the data structure.

Relevants(k) generates Lk while candidates(k) creates Ck from Lk-1.Once all the relevant itemsets have been found, the association rule derived from them can be obtained by the proper traversal of the itemset tree.

A. Stored Address  Acquistion Based on Hash Table  HASH table is used to determine the stored address[7]. The same level itemsets  are continuous addresses, which are Ck.

Other continuous positions are Ck-1,and etc.

HASH function is as followed.

H(X,Y)=(|X*2-Y)|mod m)+1                                      (5)  Where p is the number of the item in itemset.. m is the  smallest prime larger than p? ?  ? ?? ? .

If the address conflicts,  chained list is used to hash table.

For example: All terms are as table 1.

TABLE 1 THE ADDRESS OF  THE ITEM OF  ITEMSET  A B C D E F ? W X Y Z 1 2 3 4 5 6 ? 23 24 25 26  The definition of Hash function is:H(X,Y)=(|(X*2- Y)|mod 17)+1  Where X is the first item of the itemset, Y are the  terms of the later of the itemset.

Suppose L1={{A},{B},{C},{D},{E},{F}}.The address of hash table of candidate set C2 is as followed table 2  TABLE  2   HASH  STRUCTURE OF  2-ITEMSET  The address of hash table  1 2 3 4 5  Two-itemset BD BC CD DE AF The address of chained list  1 2 3 4  Two-itmset AB AC AD AE  H(B,D)=(|(2*2-4)|mod 17)+1=1 H(B,C)=(|2*2-3)|mod 17)+1=2 H(C,D)=(|2*3-4)|mod17)+1=3  H(D,E)=(|2*4-5)mod 17)+1=4 H(A,F)=(|2*1-6)mod 17)+1=5 H(A,B)=(|1*2-2)mod 17)+1=1 H(A,C)=(|1*2-3)mod 17)+1=2 H(A,D)=(|1*2-4)mod  17)+1=3 H(A,E)=(|1*2-5)mod  17)+1=4     Since H(B,D)=H(A,B),AB and BD are in the same chained list .

B. FHAR Data Structure When compared to the Apriori algorithm, the data  structure employed by ARTREE provides substantial storage savings. The number of k-itemsets contained in the itemset tree equals the number of items located at level L[k].The corresponding k-itemsets can be retrieved by concatenating the items found in the paths from the root of the tree to each item located at level L[k].The support of any k-item is stored in the tree accompanying the kth item of the itemset.

A hash table can be used within a node for optimization purpose when the number of items in a node is greater than a given threshold. A hash table can be indexed by the pair a:v.

According to formula (5), stored address of itemset is acquired. In order to generate Ck+1, you must create a child node for each item a:v at level L[k].The items with the same attribute as a:v can be discarded when working with relational databases. The support of each candidate (k+1)- itemset in the tree is counted and all the non-relevant itemsets are pruned, leaving Lk+1 at level L[k+1] of the tree.

C. Itemset Tree Initialization The Init primitive creates the itemset tree data  structure,sets ites parameters. Hash techniques are used to provide efficient access to the itemsets in the tree.

D. Candidate generation The Candidates (k) primitive is in charge of the  candidate k-itemset generation. The process consists in copying all the items to the right of a given item in the current node to a newly created child node. If a MinSupport threshold is used, the relevant itemsets will be those candidate k-itemsets whose support count after the database scan is higher than the user-specified threshold. In order to count the candidate k-itemsets support, several hashing techniques can be used.

E. Rule generation The last primitive of the itemset tree ADT,rules, returns  all the association rules derived from the itemsets contained in the tree. Two tree iterators are needed.

The steps are as follow: (1) Obtaining all the relevant k-itemsets from the  itemset tree.(k  2) First, it tries to expand the current k-itemset looking for children of the actual node. If no such children exist, it looks for alternative k-itemsets in the same node of the tree.

Finally, if there are no more alternatives, it backtracks to find any relevant m-itemsets derived from the first m-1 itesms of the current k- itemset,with a different mth item.

(2) Giving all the proper subsets of a given itemset. It looks for extensions of the current subitemset in a depth-first way. Computing the support and  confidence of the relevant itemsets, The support and confidence of the relevant itemsets larger than the given support and confidence is derived to find the association rule.



IV. APPLICATION OF FHAR ALGORITHM IN TEXT MINING The concept vectors of the texts are acquired with cvc  algorithm. The text association rules are derived when the concept vectors are applied to FHAR algorithm.



V. EXPERIMENTS  We collected 1000 documents that are obtained from www.cnki.net selected 2136 eigen words. The texts consist of economic, education, military, P.E, entertainment. The weights of the above keywords are applied to FHAR algorithm to find the text association rule.

When support=20%, confidence=70%, five text association rules are derived.

Rule 1: if   then college Rule 2: if economic then economy Rule 3: if gold then game Rule 4: if actor then film Rule 5:if missile then national defence  From the five text association rules, the class is identified.

The texts of education expound the content of English and college. With the text association rule, many texts of the same topic are found.

When support=5%, confidence=50%, 25 text association rules are derived.



VI. CONCLUSTION The paper presents a new the acquiring the keywords and  a new text association rule ARTREE algorithm. The application of ARTREE algorithm in text mining is very important. Experiment demonstrates the efficiency of the ARTREE algorithm.

