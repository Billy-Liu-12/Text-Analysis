FPGA-Based HPC Application Design for Non-Experts

Abstract?In the current era of big-data computing, most non- engineer domain experts lack the skills needed to design FPGA- based hardware accelerators to address big-data problems in their fields. This work presents bFlow, a development environ- ment that facilitates the assembly of such accelerators, specifically those targeting FPGA-based hybrid computing platforms, such as the Convey HC series. This framework attempts to address the above problem by making use of an abstracted, graphical front-end more friendly to users without computer engineering backgrounds than traditional, HDL-based design environments, as well as by accelerating bitstream compilation by means of incremental implementation techniques. bFlow?s performance, usability, and application to big-data life-science problems were tested by participants of an NSF-funded Summer Institute organized by the Virginia Bioinformatics Institute (VBI). In about one week, a group of four non-engineering participants made significant improvements to a reference Smith-Waterman implementation, adding functionality and scaling theoretical throughput by a factor of 32.



I. INTRODUCTION  Today?s world is one of ?information everywhere,? in which the size and availability of valuable data sets is rising rapidly, especially in the physical and life sciences. Consider also the genome of a single human individual?processing this data set involves the analysis of three billion DNA base pairs. This fact is extremely relevant given the expectation that in the near future next-generation sequencing machines will produce whole human genomes in a matter of hours [1]. This flood of information creates the need for exceptional computing performance in order to process such data sets within tolerable times.

At the forefront of big-data analysis efforts are domain experts, e.g. bioinformaticians who explore genome hypothe- ses by analyzing large quantities of genome data [2]. Such domain experts may be skilled programmers; however, their productivity?hypothesis discovery and verification rate?is limited by the performance capacity of available computing resources. For example, the architecture of data-center class computing platforms is designed for speed over a general mix of problems, and efficiently applying such platforms to domain-specific data structures and algorithms such as DNA/protein sequence alignment is often not straightforward [3]. Heterogeneous computing machines like the Convey Hybrid-Core (HC) servers have potential to address this gap, since they enable the creation of application-specific, FPGA- based accelerators tightly coupled with general-purpose pro-  cessing resources (i.e. CPUs). This tight coupling is achieved through the use of custom instructions and cache-coherent, shared memory space [4]. Unfortunately, development flows for such accelerators require extensive computer engineer- ing and digital design expertise, rendering the use of these platforms impractical for most experts in the life sciences community.

In this work a development flow targeting hybrid-core systems such as the Convey HC servers is proposed with the purpose of aiding non-engineering domain experts in the assembly of big-data hardware accelerators. Such a flow should help such experts address ?-omics? problems on an unprecedented computational scale [2]. Since graphical design environments often provide a simpler, more intuitive mapping to parallel hardware than text-based languages, a graphical design entry tool, namely DataIO?s Azido (see Figure 1), was selected as the flow?s front-end. Accelerators are described in Azido using a graphical, algorithmic language built on simple, canonincal primitives, and the tool facilitates development for heterogeneous platforms within a single environment [5]. In this work, an Azido System Description (SD) was prepared to target the Convey HC architecture. This description acts as a plugin to Azido, encapsulating specifications of the HC platform organization and interface details. Most of these interface details are abstracted to simpler, higher level struc- tures within Azido to ease development. Furthermore, they are complemented with software helper routines (C/C++) that wrap up the co-processor API into a framework more intuitive to the typical big-data user.

This paper is organized as follows. Section II provides background and discusses the need for usable design envi- ronments for big-data accelerator design, Section III describes the approach taken in this work to address this need, and Section IV presents both qualitative and quantitative results.

Conclusions and future work are given in Section V. This paper is an extended version of [6], and contains experimental results.



II. BACKGROUND  Design productivity for FPGA-based computing has suf- fered from the contemporary ASIC ?design productivity gap? and has unique needs and opportunities not adequately ad- dressed by existing FPGA design tools. In [7] Nelson et al. proposed a productivity model that exposes three key      Fig. 1. The Azido graphical algorithm description environment.

contributors to high design productivity: multi-level design reuse, high-level design abstractions, and a more interactive verification environment that increases the number of devel- opment turns per day. All of these are necessary to improving design productivity, e.g. the use of high-level (above HDL) languages would significantly reduce functional simulation time, thereby increasing turns per day. Also, describing designs in a hierarchical, high-level language can promote reuse by making designs more portable.

High-level synthesis is a very active area of research, and many high level design tools and approaches exist (e.g.

ImpulseC, AutoESL, SysGen, Catapult C). The primary focus of these tools is to reduce time to solution for designers who have considerable computer engineering expertise. While working from a high level of abstraction and guaranteeing functionally correct output, they require design input (e.g.

pragma statements) concerning low-level hardware details in order to produce efficient RTL output. Hence, such languages are generally inappropriate for use by those in the life- sciences community, which is comprised primarily of software programmers with no hardware design experience.

A. Convey hybrid-core architecture and design flow  The use of heterogeneous computing architectures like the Convey HC platform (CPU + FPGA) for fixed-point [8] and Nebula (CPU + GPU) for floating point algorithms [9], consti- tutes one approach to meeting big-data processing demands.

The Convey HC-1 system, which is considered in this work, consists of a commodity Intel Xeon host server extended with a custom co-processor board, as seen in Figure 2. This board contains four large FPGAs (Xilinx part XC5VLX330) called Application Engines (AE), each augmented with cache- coherent, high bandwidth memory access (eight memory controllers per AE). Each configuration of the Convey co- processor is called a ?personality,? and makes application- specific functions available to the host server processor as custom instructions [10].

To enable the creation of custom personalities, Convey provides a Personality Development Kit (PDK) flow, which  Fig. 2. Overview of the Convey hardware-software interface.

is illustrated in Figure 3. The PDK encourages the follow- ing development process, when starting with a software-only implementation [11]:  1) Analyze the application?s performance bottlenecks and heavily-used data structures, and identify core function- ality that can be moved to the FPGA-based co-processor for acceleration. Extract this functionality into a set of software kernels, which will serve to model the desired co-processor behavior.

2) Map the kernels to the subset of the Convey Instruction Set Architecture (ISA) reserved for custom personalities.

The system can be verified using the functional, system- wide simulation framework provided by Convey.

3) Describe the kernels using HDL (Verilog or VHDL) or synthesized netlists (e.g. EDIF), and verify their func- tionality using the same simulation environment. This framework contains bus-functional models of all AE interfaces, and co-simulates the host software with the co- processor HDL through the Verilog Procedural Interface.

4) Lastly, the personality is implemented to a bitstream package using the Xilinx FPGA design flow, and run in real-time on the co-processor hardware.

The personality can execute on up to all four FPGAs, pro- viding significant computational capability; however, the PDK does not address the needs and skills of big-data users, most of whom have little FPGA development experience.

B. Azido hardware design environment  Azido (Figure 1) is a graphical, object-oriented design environment based on the Implementation Independent Algo- rithm Description Language (I2ADL) [5]. The tool attempts to abstract the low-level complexities of digital hardware design to higher-level algorithmic objects more intuitive to designers without traditional hardware design skills. Also, it simplifies and accelerates design by heavily encouraging object reuse and providing an extensible library of I2ADL primitives, known as the CoreLib. Bitstream implementation is performed by Azido System Descriptions (SDs), which are script-based ?plugins? to Azido, encapsulating details of the target platform?s architecture.

Among several existing graphical design environments, Azido was selected primarily for three reasons: 1) it provides     Fig. 3. Overview of Convey?s Personality Development Kit (PDK) flow for developing custom personalities [11].

? Local Machine  ? Remote Shadow fax  Com pute  Cluster  Azido Desktop Environment  -Convey Hybrid Core Platform  Algorithm Design  Synthesis (HC) Simulation (x86)  Personality Compilation  System Simulation  Compilation (cnyCC)  Host Executable  EDIF to Verilog Conversion  Verilog Netlist  Run  Verilog Wrap Logic  Run-Time Interaction  Host C/C++ Application  Host Application Development  EDIF Netlist  Personality Release (cae_fpga.tgz)  Fig. 4. Movement of data within bFlow.

a flexible design environment and core library capable of servicing many application domains at multiple levels of abstraction, 2) the System Description-based implementation framework facilitates extention of the tool to many target platforms, and 3) beyond standard schematic-capture abilities, Azido provides some dynamic features, such as automatic data typing and graphical polymorphism. Other tools considered  were National Instrument?s LabVIEW FPGA Module [12] and MathWork?s Simulink [13]. The former was set aside early because of its constraint to specific target platforms, as well as the the FPGA module?s lack of many dynamic features provided by Azido. Simulink, on the other hand, can produce platform-independent VHDL and Verilog; however, its lack of graphical polymorphism was prohibitive.

C. bFlow contributions  The bFlow approach presented in this work attempts to provide a simplified and portable accelerator development flow supporting rapid prototyping of big-data algorithms in hardware. Design productivity is improved with a high-level graphical front-end (i.e. Azido) and an accelerated compila- tion process employing incremental implementation strategies facilitated by qFlow [14] and Xilinx Hierarchical Design Flow [15]. Because of the graphical front-end and abstract objects, the flow lends itself to designers without HDL expertise more than traditional flows.

This framework supports the growth of the third-party ?personalities ecosystem? through the distribution of open- source accelerator implementations, not unlike that observed in the Linux community [16]. This can lift the usual restriction to closed-source personalities provided by system vendors.

Finally, the bFlow approach is portable?design is performed locally on a desktop computer, accelerator implementation is performed using a remote HPC cluster (e.g. VBI?s Shadowfax compute cluster [17]), and the personalities are executed remotely on the Convey Hybrid-Core platforms. Thus, the designer need only install Azido and the Convey HC System Description on his local machine.



III. DEVELOPMENT FLOW IMPLEMENTATION  This approach is divided into three efforts: 1) configuring Azido to target the Convey HC-1 platform, 2) the acceleration of the back-end compilation process, and 3) a hands-on test of the complete flow. The complete, resulting design framework is shown in Figure 4. Initially, the accelerator is designed and synthesized within the Azido application on the user?s local machine. After synthesis, an EDIF netlist produced by Azido is transferred to VBI?s Shadowfax cluster [17], where it is combined with custom wrap logic and implemented to a bitstream using either an accelerated compilation process or Convey?s standard implementation flow. The bitstream is packaged into a personality file and placed in shared storage accessible to VBI?s Convey HC servers. At this point the host CPU software that utilizes the accelerator can be compiled with the Convey C/C++ compilers and executed on the HC platform. During execution, probes and stimuli instantiated in the design provide real-time diagnostic information to the user within the Azido tool.

Local testing of individual algorithmic blocks is accom- plished by Azido?s built-in x86 System Description, and rough system simulation can be done locally by instantiating Com- ponent Object Model (COM) objects throughout the design.

For example, COM objects that stream files in and out of the     ?????? ??????????? ??????  ???? ???  ????????????  ??? ???  ???  ??????????? ?????? ???? ????  ???????? ???  ????? ???  ????? ??????? ????????????  ?????? ??????  ??????????????? ??????? ???????  ?????? ???????  ?????? ???????  ?????? ???????  ?? ???  ?????????????  ? ? ?  ???  ?????? ?????????  Fig. 5. The bFlow personality application engine (AE) architecture.

Azido design can be used to approximate the functionality of the memory stream objects found in the HC System Description. Alternatively, cycle-accurate simulation of the entire HC system (host CPU + AEs) can be performed on the server itself using Convey?s supported simulation flow and a structural Verilog netlist of the Azido design, generated by an EDIF-to-Verilog conversion utility developed in-house.

A. HC system description and software helper routines  The assembly of a design environment usable by those with little digital design expertise was accomplished through the use of Azido with a Convey HC System Description and a collection of convenient software routines. The SD for the HC platforms can be divided into three components: 1) a communication implementer for the transfer of probe results and stimuli between Azido and the personality at run-time, 2) an abstracted view of the co-processor dispatch inter- face, and 3) streaming abstractions for the memory interface.

Compilation of the Azido design produces a Hybrid-Core personality, the architecture of which is shown in Figure 5.

Desktop Environment  Azido  COM Object  Convey Hybrid-Core Server  Co-ProcessorHost CPU  Azido/MPIP Relay MPIP  AE0 AE1  AE3AE2  SSH Tunnel  Fig. 6. Azido/Hybrid-Core communication architecture.

1) Azido communication implementer: The first compo- nent, which is invisible to the designer, handles all communi- cation between the Azido environment and the HC platform  to provide run-time diagnostic probing and stimulation of the running personality. This is achieved through the use of a COM object instantiated within the Azido HC SD, an SSH tunnel from the designer?s desktop to the HC server, a utility running on the Convey x86 host processor, and a Convey- provided telnet server, mpip, which provides access to the HC?s management ring interface. Azido transmits user input from the run-time widget window to the COM object, which in turn transfers the data through the tunnel to the relay utility on the Convey platform host processor. This process sends register read/write commands through the mpip telnet server to the management ring, both capturing and exciting signals in the design (see Figure 6).

Fig. 7. Collection of Convey dispatch interface objects in Azido (left) and example usage (stream inverter) of the streaming memory abstractions (right).

2) Convey dispatch interface abstraction: The second con- sists of the Azido abstraction of the AE dispatch interface, which are shown at the left in Figure 7. Logic external to the Azido-generated netlist decodes incoming dispatch instructions and presents a simplified interface to the Azido designer. The AE general (AEG) registers are exposed as simple read/write blocks in Azido, and the co-processor cus- tom instructions are exposed as blocks with a ?Start? output.

Management of the co-processor idle state is implemented external to the Azido design.

3) Memory streaming abstraction: Lastly, to conceal the complexities of random memory access and address arith- metic, bFlow contains a simplified, streaming abstraction to the memory controllers available to each AE. These ?stream- ers? provide two abstractions to the designer: 1) a ?source? module and 2) a ?sink? module, to be used to stream a block of data from memory into the AE and stream data out to memory, respectively. A usage example of the Azido modules for these two abstractions, both of which comply with Azido?s GDBW flow-control convention, is provided at the right in Figure 7.

4) Software helper routines: In addition to the System De- scription objects available to the designer within Azido, several software routines were developed to simplify the configuration and execution of a custom personality. This is accomplished by wrapping the Convey-provided, low-level assembly routines in higher-level routines encapsulated in a C++ class. These routines include helper functions for reading and writing the AEG registers, calling co-processor custom instructions, and copying data to/from co-processor memory. Furthermore, a subclass containing additional abstractions that simplify?at     the cost of flexibility?interaction with the Azido streaming objects (e.g. file I/O) is also provided.

B. Extension of Smith-Waterman reference implementation  To verify the usability and productivity that bFlow aims to provide, it was placed in the hands of four non-engineering participants of the NSF-funded Summer Institute organized by VBI. Prior to the start of the event, the students were asked to review [18] and gain a basic understanding of the systolic array approach to implementing the Smith-Waterman sequence alignment algorithm.

The Smith-Waterman algorithm, a local sequence alignment algorithm proposed in 1981 and well known to the bioinfor- matics community, is often used to compute the similarity between two nucleotide or protein sequences, S and T , by computing a scoring matrix of size S ? T [19]. This work considers a common case in which S, the query, is quite short?at most one or two thousand base pairs long, and T , the reference, is quite long?millions of base pairs. This al- gorithm yields the optimal local alignment, given a nucleotide substitution matrix and affine gap penalties, and consists of two steps: 1) computation of the scoring matrix and location of best alignment and 2) the traceback of the best alignment starting from the matrix cell with the highest score. Since performing the traceback operation requires only the subset of the scoring matrix containing the best alignment, this step is often postponed until the location of the best alignment is determined, which can be done by tracking the highest-scoring cell during the first step.

A reference implementation (see Figure 8) based on [18] was created in the Azido environment in about five days by by one of the authors, who has substantial HDL experience and knowledge of the Smith-Waterman algorithm. This design was given to the Summer Institute participants along with the task of adding functionality and improving performance within one week.

Smith-Waterman Azido Design  Memory Streamer  Memory Streamer  AEG Regs  Systolic Cell Array  PE0 PE1 PEn T  PE2  Params: qrySeqBaseAddr qrySeqLen  Query Sequence Collector  S0 S1 S2 Sn  Params: refSeqBaseAddr refSeqLen  Results CollectorResults:  bestMatchScore bestMatchIndex  . . .

Fig. 8. High-level architecture of the reference systolic array implementation of the Smith-Waterman scoring matrix fill operation. This was given to the NSFSI participants with the task of adding functionality and improving throughput in about a week.

C. Partial implementation flows with qFlow and Partitions  Acceleration of the bitstream compilation process is achieved by two methods, Xilinx Hierarchical Design Parti- tions flow [15] and qFlow [14]. Both are incremental, partial  implementation frameworks that reduce build times through high-level management of the Xilinx ISE implementation process. The key to both approaches is the exploitation of the high-level architecture of all Convey personalities? specifically, the inclusion of interface logic that remains nearly static throughout the development process. This logic consists of interfaces to the eight available memory con- trollers, a memory crossbar, and hardware that communicates with the dispatch and management processors. All of this consumes roughly 25% of each of the HC-1?s AEs (Xil- inx part XC5VLX330) and, when using Convey?s traditional compilation process, is re-implemented each build, costing precious minutes of development time. Given the rarely- changing nature of this logic, both of the following strategies accelerate compilation by implementing it once, constraining its placement to the edges of the FPGA near the I/O, and then preserving it?s placement and routing during consecutive builds of the personality, For these consecutive builds, the dynamic, Azido-generated logic is placed in a ?dynamic region? at the center of the device.

1) Partitions flow: The first approach taken makes use of the Xilinx Hierarchical Design Partitions flow [15]. Two partitions were selected for this flow: 1) a top-level partition containing the entire FPGA design and 2) the Azido-generated logic. After an initial compilation, the top-level is preserved, and remains mostly static as the Azido user logic evolves.

This top-level is re-implemented only when major changes to memory access patterns are needed. The application of this flow is very simple, requiring only some Makefile extensions and a few constraint files (*.ucf).

2) qFlow: This second utilizes a subset of the qFlow framework [14], a tool for accelerating back-end compilation of designs with hierarchical structure similar to that enforced by the Convey PDK. Though qFlow offers a superset of the functionality provided by Partitions, the application discussed here is very similar. When compared to the partitions-based approach discussed above, qFlow provided generally much faster compilation times (see Section IV-B); however, as qFlow is a research product, it was much more difficult to work with.



IV. RESULTS  The participants were successful in their attempt to im- prove the performance and functionality of the reference SW implementation. Modifications included logic to maintain the index of the highest scoring alignment, and a transition from a single cell array to multiple arrays, with the goal of achieving a linear speedup proportional to the number of arrays by searching multiple partitions of the reference sequence in parallel. The first modification was simple, and included the use of Azido?s counter, maximum, multiplexer, and register objects; however, the second involved significant changes to the high-level structure of the implementation?specifically, duplication of the systolic cell array and addition of logic at the front and back of the arrays to split the incoming stream and collect results from each array, respectively. Hence, by splitting the reference into multiple chunks (up to 32) and streaming     TABLE I BUILD TIMES (MEAN OF THREE RUNS) FOR CONVEY?S STANDARD FLOW,  THE PARTITIONS FLOW, AND QFLOW, WHERE THE SPEEDUP OVER THE STANDARD FLOW IS GIVEN IN PARENTHESES. NOTE: DEVICE UTILIZATION  LISTED IS THE UTILIZATION DUE TO ONLY THE USER LOGIC.

Design Cells Device Util. (%) Mean Build Time (min)  LUTs FFs Standard Partitions qFlow  sw 1x8 8 1.83 2.16 89.10 65.60 (1.36) 26.58 (3.35) sw 1x12 12 2.43 2.44 89.90 54.16 (1.66) 26.12 (3.44) sw 1x16 16 3.02 2.73 104.09 57.92 (1.80) 32.94 (3.16) sw 1x24 24 4.22 3.30 98.80 76.85 (1.29) 34.75 (2.84) sw 1x32 32 5.41 3.87 102.20 72.89 (1.40) 38.27 (2.67) sw 4x8 32 5.62 4.10 96.90 61.58 (1.57) 39.20 (2.47) sw 1x48 48 7.79 5.01 128.50 82.95 (1.55) 43.02 (2.99) sw 1x64 64 10.18 6.15 129.73 87.92 (1.48) 53.75 (2.41) sw 4x16 64 10.36 6.34 130.08 84.74 (1.54) 53.94 (2.41) sw 4x32 128 19.85 10.81 165.99 173.62 (0.96) 97.33 (1.71) sw 4x48 192 29.34 15.29 173.22 sw 4x64 256 38.83 19.76 208.89  each chunk into separate, parallel pipelines, the accelerator can consume the reference up to 32 times faster than the single-pipeline approach. Using this technique, the participants achieved a realized 4x bandwidth increase from 150 million to 600 million bases per second (bps), and a theoretical speedup of 32x to 4.8 billion bps, given enough parallel cell arrays.

However, it should be noted that using parallel cell arrays does require preprocessing of the reference sequence, and as the number of pipelines increase, the maximum query sequence length decreases due to resource limitations.

A. Usability challenges  One difficulty experienced by the students during their use of the flow was synchronous design?especially synchronizing multiple flows of data. Azido?s CoreLib contains objects that attempt to address these difficulties by using a Go-Done-Busy- Wait protocol; however, these objects lack elegance and create a visual mess within the graphical environment. Futhermore, many of these use large queues by default for flow control, consuming excessive FPGA resources.

Another complication of the design process was timing closure. Under the standard parameterization, the Convey PDK enforces a clock rate of 150 MHz, which is easily broken by moderately long chains of asynchronous operations.

However, Azido neither analyses the design nor enforces any timing restrictions at compile time; hence, whether or not a design meets timing is determined only during the ?hidden? implementation processes, and will result in the loss of the abstraction that Azido provides if a constraint is not met and the design proves dysfunctional.

B. Compilation performance  The performance of the alternative build flows is given in Table I and visualized in Figure 9. Each Smith-Waterman configuration is named with convention sw MxN, where M is the number of parallel systolic arrays and N is the length of each array. The median speedup over the standard, Convey- provided flow for the partitions-based approach 1.51, while that of qFlow was 2.76. The last two configurations tested,  ? ?  ? ? ? ?  ? ? ?  ? ?  ?  ?  ? ?  ? ?  ?  ? ? ?  ?  ? ? ? ?  ? ? ? ? ?  ?  ? Standard  ? Partitions  ? qFlow  1x8  H8L 1x12  H12L 1x16  H16L 1x24  H24L 1x32  H32L 4x8  H32L 1x48  H48L 1x64  H64L 4x16  H64L 4x32  H128L 4x48  H192L 4x64  H256L        B u  ild T  im e  Hm in  L  Fig. 9. Build times for Convey?s standard flow, the Partitions-based flow, and qFlow for twelve different variations of the Smith-Waterman Azido implementation (cell counts are given in parentheses).

4 ? 48 and 4 ? 64, could not be placed into the dynamic region?note that the same dynamic region area constraints were used for both flows. Note the jump in build time from configuration 4? 16 to 4? 32 due to the increased utilization of the dynamic region.



V. CONCLUSIONS AND FUTURE WORK This work presents bFlow, an FPGA-based big-data ac-  celerator development environment significantly more usable by non-engineers than traditional accelerator design tools.

This increase in productivity and usability is accomplished by promoting object reuse, providing an high-level, abstracted graphical design environment, and increasing turns-per-day by reducing bitstream compile times.

The framework was applied to the Convey Hybrid-Core heterogeneous computing platforms, and its usability and productivity tested by participants of the NSF-funded bioin- formatics Summer Institute at the Virginia Bioinformatics Institute. In a week, the participants successfully extended and accelerated a reference Smith-Waterman FPGA imple- mentation designed in Azido, achieving a theoretical speedup of 32x (realized speedup of 4x) and additional functionality.

However, significant deficiencies in Azido?s graphical syntax, specifically the poor synchronization abstractions, proved to be major barriers to the participants? creation of complex logic.

In addition to the front-end developments, compilation times for the Convey HC-1 server were reduced through the use of the Xilinx Partitions flow [15] and the qFlow framework [14] as alternatives to Convey?s standard implementation flow. The median compilation speedups for these flows were 1.51 and 2.76, respectively.

Given the above limitations of bFlow in its current state, consideration should be given to the support of other graphical design environments and hetergeneous platforms. Since the design environment is used for only the hardware accelerator, the front-end could be easily replaced with another that provided the same memory abstraction support. Adapting the flow to hetergenous target platforms other than the Convey HC series would require rewriting the software helper routines and re-implementing the Azido wrapper (see Figure 5).

Future research efforts include the consideration of other design front-ends, such as LabVIEW [20], and the exploration of memory abstractions other than simple streaming objects.

It is clear that, as the usability of FPGA development en- vironments is increased through heavy use of abstractions, some flexibility and/or performance is lost (e.g. restriction to streaming memory in this work). Therefore, future work should include gaining a better understanding of the influence of domain-specificity on the usability and performance of accelerator design flows.

