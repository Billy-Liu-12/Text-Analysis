Classification of Big Velocity Data via Cross-Domain Canonical Correlation Analysis

Abstract?Many classification techniques work well only under a common assumption that the training and test data are drawn from the same feature space and the same distribution. How- ever, big velocity data usually show disobedience of this as- sumption. For example, in the field of web-document classifi- cation, new document is continuously emerging every day.

Transfer learning aims at leveraging the knowledge in labeled source domains to predict the unlabeled data in a target do- main, where the distributions are different in domains. As one of the important research directions of transfer learning, one kind of approaches focus on the correspondence between pivot features and all the other specific features from different do- mains, to extract some relevant features that may reduce the difference between the domains, have attracted wide attention and study. However, the limitation caused by the vague mean- ings in different domains prevents these algorithms from fur- ther improvement. To tackle this problem, we propose a cross-domain canonical correlation analysis algorithm called CD-CCA by applying Canonical Correlation Analysis (CCA) to transfer learning. CD-CCA can learn a semantic space of mul- ti-view correspondences from different domains respectively and transfer the knowledge by dimensionality reduction in a multi-view way. Experimental results on the 144?6 classifica- tion problems in 20Newsgroups, show that CD-CCA can sig- nificantly improve the prediction accuracy.

Keywords-big velocity data classification;transfer learning; canonical correlation analysis  ? 1 INTRODUCTION As an important learning method in machine learning, clas- sification algorithm targets on adapting the knowledge learnt from a labeled source domain to an unlabeled target domain, where the documents from the source and target domains are independent and identically distributed. Many learning techniques work well only under a common assumption: the training and test data are drawn from the same feature space and the same distribution. However, big velocity data usually show disobedience of this assumption. For example, in the field of web-document classification, new document is con- tinuously emerging every day. In this case, the labeled data obtained in one time period may not follow the same distri- bution in a later time period. When the features or distribu- tion change, most classification models need to be rebuilt from scratch using newly collected training data. Of course,  we can achieve this by spending lots of human efforts and time to annotate huge amount of training data to train a new classifier. However, it is so expensive that we cannot do this all the time. This leads to the research of transfer learning.

Transfer learning aims at minimizing the differences be- tween distributions of different domains in order to minimize the cross-domain prediction error. Among proposed ap- proaches, feature-representation-transfer is an important research direction of transfer learning. The intuitive idea behind this approach is to learning a ?good? feature repre- sentation for the both domains. In this case, the knowledge used to transfer across domains is encoded into the learnt feature representation [Pan and Yang 2010]. Represented by the common knowledge, source domain and target domain become more similar. In this way, [Blitzer, et al. 2006] pro- poses a single-view transfer learning algorithm, Structure Correspondence Learning (SCL), in which a low dimen- sional semantic space is generated via a single-view corres- pondence to depict the common knowledge across source and target domain. However, SCL has some limitations by utilizing the single-view correspondence, caused by the va- gue meanings in different domains [Ji, et al. 2011]. In order to learn the semantic space of multi-view correspondences, we propose a cross-domain canonical correlation analysis algorithm called CD-CCA by applying Canonical Correla- tion Analysis (CCA) to transfer learning.

The rest of this paper is organized as follows. Section 2 discusses related work. Section 3 gives a general overview and defines some notations we will use later. Section 4 de- scribes CD-CCA algorithm in detail. Section 5 illustrates experimental results to show the effectiveness of CD-CCA.

Section 6 gives the conclusion and discussion.

? 2 RELATED WORK Transfer learning is widely present in the human activities.

The more features shared by the two different domains, the easier it is to transfer knowledge, otherwise the more diffi- cult, or even "negative transfer". For example, if a person can ride a bike, he can easily learn to drive a motorcycle; a man who is familiar with backgammon, it can easily knowledge migrated to learn Chinese chess [Shi, 2009]. However, even if shared features exist between domains, there may have "negative transfer", for example, a person who can ride a bike will find it difficult to ride a tricycle because of their different center of gravity.[Blitzer, et al. 2007] summarized the rea-      sons that how features can change behavior across domains into three parts. The first type of behavior is when predictive features from the source domain are not predictive or do not appear in the target domain. The second is when predictive features from the target domain do not appear in the source domain. There is a third type, features which are positive in one domain but negative in another, but they appear very infrequently.

Structure Correspondence Learning (SCL) makes use of the unlabeled data from the target domain to extract some relevant features that may reduce the difference between the domains [Blitzer, et al. 2006]. The first step of SCL is to define a set of pivot features on the unlabeled data from both domains. Briefly, pivot features are those representative words occurring frequently across all domains, with detailed specification in Subsection 3.3. Then, SCL seeks to learn the common knowledge across domains in the semantic space by applying the SVD procedure to a correspondence matrix. It is notable that this correspondence matrix mixes up all the other features across domains except pivot features, and measures the statistical correlations between them. Finally, standard discriminative algorithms can be applied to the augmented feature vector to build models. The augmented feature vector contains all the original features appended with the new shared features. [Ben-David, et al. 2007] showed experi- mentally that SCL could reduce the difference between do- mains; however, how to select the pivot features is difficult and domain dependent. SCL-MI [Blitzer, et al. 2007] uses Mutual Information (MI) instead of using common frequency to choose the pivot features that have high dependence on the labels in the source domain.

[Ji, et al. 2011] points out the limitations of SCL caused by the vague meanings of pivot features in different domain. For example, in DVD domain, if one product reviewer says "read-the-book", he means this DVD is "boring". So "read-the-book" has high correlation with "boring" in DVD domain. While looking at Book domain, if one product re- viewer says "read-the-book", he means the book is "well-written" or "great". In order to alleviate this problem, Ji, et al. proposes a new transfer learning method by utilizing Multi-View Principal Component Analysis (MVPCA). In MVPCA, firstly they choose  pivot features as SCL does.

Secondly, in every single domain, they learn a correspon- dence between pivot features and the other features. In this way, MVPCA separate the single-view correspondence of SCL algorithm into multi-view correspondence. Those words with vague meanings can only show its domain dependent meaning and they are not correlated with those contradictory words simultaneously in the single correspondence. Finally, MVPCA learns a low dimensional semantic space.

The intuitive idea of SCL, SCL-MI and MVPCA is to extract the cross-domain correlations and learn the low di- mensional semantic space. In this low dimensional space, the distributions of source domain and target domain become more similar. Canonical Correlation Analysis (CCA) is a natural way to solve the problem of learning the semantic space of multi-view correspondences.

? 3 PROBLEM SETTING AND BASIC CONCEPTS In this section, we first introduce some basic concepts and mathematical notations used throughout this paper, and then formulate framework.

3.1 Problem Setting In this paper, we study the problem of transfer learning for text categorization. We assume that the documents from the source and target domains share the same space of word feature, also, they share the same set of document labels.

Under these assumptions, we study how to predict the class labels of the documents accurately in the target domain with a different data distribution.

A domain is denoted by , which  is a feature space  and label ; denotes an instance in domain  which is a term vector of a document with  representation;  denotes the label of instance ;  denotes the number of instances.  de- notes the j-th coordinate value of instance and .

Source domain is denoted by ,  , .  is a positive\negative instance if ,  is an unlabeled instance if  . ,  and  are similarly defined in the target do- main. We further assumed that:  a) b) c) This paper only focuses on one source domain and  one target domain.

3.2 Canonical Correlation Analysis Canonical correlation analysis (CCA) is a standard statistical analysis methods,  proposed by Hotelling in 1936 and widely used in economics, medicine and meteorology and other fields. CCA is a data analysis and dimensionality reduction method similar to PCA. While PCA deals with only one data space, CCA is a technique for joint dimensionality reduction across two spaces that provide heterogeneous representations of the same data.

Let  and  be two set of random variables. Consider the linear combination  and . The problem of canonical correlation analysis reduce to find optimal linear transformation  and , which maximizes the correlation coefficient between  and  in accordance with that between  and . That is:  Where  and  are the within-set covariance matrix and is the between-sets covariance matrix. Since the solution  of Eq. (1) is not affected by rescaling  and  either to- gether or independently, the optimization of  is equivalent to maximizing the numerator subject to  and  . Then with Lagrange multiplier method, we can get        Which is a generalized eigenproblem of the form .

A sequence of  and  can be obained by eigenvectors descending ordered by the corresponding maximal eigenva- lues, which indicating the explained correlation.

In some literature, CCA is often described as the follow- ing:  3.3 Pivot Feature The efficacy of SCL depends on the choice of pivot features.

For the part of speech tagging problem, frequently occurring words in both domains were good choices, since they often correspond to function words such as prepositions and de- terminers, which are good indicators of parts of speech.

However, for document classification, we require that pivot features should be good predictors of the source label. Same as SCL-MI, in order to achieve the great discriminative abil- ity across domains, we utilize mutual information as the measurement of the statistical correlation between features and class labels.

To construct the pivot feature set , we pick up  fea- tures  with the highest mutual information values and group them into . Mutual information between  and  is computed on labeled source domain  according to Eq. (2):  ? 4 CROSS-DOMAIN CANONICAL CORRELATION ANALYSIS  After  is ready, Cross-domain Canonical Correlation Analysis (CD-CCA) can be constructed. CD-CCA consists of two views on source and target domains respectively. One view is the correlation between pivot features  and other features  is ; the other view is the correlation between pivot features and   and other features is .

Then we take source domain  as an example to illustrate how to measure the correlation between pivot features and other features. Let  and  be two set of random variables, which  is pivot feature space  and  is other fea- ture space of source domain . For each instance  , we get a paired sample , where  and  are obtained from the values of pivot fea- tures  and the values of other features . Without loss of generality, we assume that  and  are both centered.

By applying CCA on  and , we can get two optimal linear transformation  and , which maximizes the correlation coefficient between and . That is:  (3)  s.t.

Similarly, let  be a set of random variables, which is other feature space of target domain . For each in- stance ,  represents the values of other features  . Then, the correlation between pivot features and other features in target domain  can be constructed in the same way:  s.t.

Up to now, strong correlations between pivot features and specific features in source domain and target domain have been set up.  and  are the within-set covariance matrix of pivot features in source domain and target domain respectively. In order to take full advantage of the shared knowledge in both domains, we use  instead of  and  .  is the within-set covariance matrix of pivot features in .

Finally, the objective function is formularized as follows:  s.t.

4.1 Problem Solving The corresponding Lagrange is  Taking derivatives with respect to , we ob- tain  (6)     (7) Multiply  on both sides of Eq. (5),  Multiply  on both sides of Eq. (6), =    (9)  Multiply  on both sides of Eq. (7), (10)  Substitution Eq. (9), Eq. (10) into Eq. (8) gives   Let and , then  The solution of Eq. (11) reduces to the following genera- lized eigenproblem:  From Eq. (12), we get new basis vectors , ,  by taking  the top m eigenvectors. Then for any instance , the extracted paired sample  can be projected, and  is the desired mapping to the low dimen- sional shared feature representation. Similarly, each instance  can be projected onto . That is shown in Figure 1. Such that we have two domains in a three-dimensional space and the correlation between the projections of the instances onto these basis vectors is maximized.

Now let us move to the details of Cross-domain Canonical Correlation Analysis algorithm, as shown in Table 1.

Table 1. Cross-domain Canonical Correlation Analysis algorithm Input:  Labeled source domain ; Unlabeled source domain  Output: ,  and  ; classifier 1. Pick up  features  with the highest mutual  information values and group them into ; 2. Extract paired samples  , ;  3.  Compute matrices ,  ;  4. Take the top m eigenvectors ,  ,  from Eq. (12);.

5. Estimate a target classifier  from source labeled data  according to the specification in subsection 4.2.

is the output.

th  sh  i Bsw  i t?w  i Aw  sf  tf  ( )?? ,  ( )?? ,  ( )?? TiBsTiA w,w ( )?? Ti tTiA ?w,w   Figure 1. The projections of the instances from both domains  onto the i-th basis vector ,  and  4.2 Estimating a Target Classifier from Source Domain Given  and , our algorithm fits a linear predictor of the following form from source labeled data :  Which  are the weights for pivot features and other features of source domain respectively. Since Figure 1 is three-dimensional,  is shared between domains. The projections onto the shared space  must have the same predictions between domains, since they map to the same point. But  which are predictive directions may not be shared with the target domain. Although they will not appear in the target, we must estimate weights  for them in order to correctly calibrate the weights for the shared subspace  . The weights for directions  cannot be learned, which do not appear in Eq. (13). These directions essentially bias our source predictor with respect to the target domain.

So an optimal source linear predictor can always be written in the form of Eq. (14).

? 5 EXPERIMENTAL RESULT ANALYSIS In this section, we show experiments to validate the effec- tiveness of the proposed algorithm. To simplify the discus- sion, we only focus on the binary classification problems (the number of document clusters is two) in the experiment, while the algorithm can be naturally applied for multi-class cases.

5.1 Data Preparation 20Newsgroups is one of the benchmark data sets for text categorization. Since the data set is not originally designed for cross-domain learning, we need to do data preprocessing.

The data set is a collection of approximately 20,000 new- sgroup documents, which is partitioned evenly into 20 dif- ferent newsgroups. Each newsgroup corresponds to a dif- ferent topic, and some of the newsgroups are very closely related. Thus they can be grouped into certain top category.

They are shown in Table 2. For example, the top category sci contains four subcategories sci.srypt, sci.electronics, sci.med and sci.space, the top category talk contains four subcatego- ries talk.politics.guns, talk.politics.mideast, talk.politics.misc, talk.religion.misc.

Table 2. 20Newsgroups Category Each category contains four subcategories  comp comp. graphics, comp.sys.mac.hardware, comp.sys.ibm.pc.hardware, comp.os.ms-windows.misc  rec rec.autos, rec.motorcycles, rec.baseball, rec.hockey sci sci.crypt, sci.med, sci.electronics, sci.space talk talk.politics.guns, talk.politics.mideast,  talk.politics.misc, talk.religion.misc For the top categories comp, rec, sci and talk, any two top categories can be selected to construct 2-class classification problems, including comp vs. rec comp vs. sci comp vs.

talk rec vs. sci rec vs. talk and sci vs. talk. Then we take sci vs. talk as an example to illustrate how to construct 2-class classification problems. We randomly select one subcategory from sci and one subcategory from talk, which denote the positive and negative data respectively. The test data set is similarly constructed as the training data set, ex- cept that they are from different subcategories. Thus, the constructed classification task is suitable for transfer learning due to the fact that (1) the training and test data are from different distribution since they are from different subcate- gories; (2) they are also related to each other since the posi- tive (negative) instances in the training and test set are from the same top categories. For the data set sci vs. talk, we to- tally construct 144( ) classification tasks. The other five data sets are constructed similarly with sci vs. talk. Fi- nally, we totally get 144 6 classification tasks.

5.2 Baseline Methods and Implementation Details We compare CD-CCA with some baseline classification methods, including the supervised algorithms of Logistic Regression (LG) and Support Vector Machine (SVM), also the cross-domain methods of Structure Correspondence Learning (SCL).

The  weights are used as entry values of the word-document matrix. The threshold of document fre- quency with the value of 15 is used to decrease the number of features. The baseline methods LG is implemented by the toolkit LIBLINEAR [Fan, et al., 2008], SVM is given by the toolkit LIBSVM [Chang, et al., 2011], using the linear kernel.

After some preliminary test, we set the trade-off parameters of CD-CCA, . The number of pivot features, m, is set to 100 and the dimensionality p of latent semantic space in SCL is set to 70.

5.3 Experiment Results Next, we present detailed experiment results. We compare these classification approaches on the data set sci vs. talk, rec vs. talk, rec vs. sci, comp vs. rec, comp vs. sci and comp vs.

talk. All the results are recorded in Figure 2. The 144 prob- lems of each data set are sorted by increasing order of the performance of LG. Thus, the x-axis in Figure 2 can also indicate the degree of difficulty in knowledge transferring.

From the results, we have the following observations: (1) CD-CCA is significantly better than the supervised learning algorithms LG and SVM, which indicates that the traditional supervised learning approaches cannot perform well on the transfer learning tasks; (2) Also, CD-CCA is much better than the cross-domain methods of Structure Correspondence Learning (SCL).

Additionally, we compare these classification algorithms by the average performance of all 144 problems from each data set, and the results are listed in Table 3. L and R denote the average results when the accuracy of LG lower and higher than 65%, while Total represents the average results on all 144 problems. [Ji, et al., 2011] also preform experi- ments on 20Newsgroups data set in order to conduct the comparisons between MVPCA/VMVPCA and SCL. Rele- vantly, in Table 3, S/M/VM represents the highest perfor- mance for each data set in [Ji, et al., 2011]. These results show that CD-CCA is an effective approach for transfer learning, and has strong ability to transfer knowledge. These results show that CD-CCA is an effective approach for transfer learning, and has strong ability to transfer know- ledge.

Table 3: Average Performances (%) on 144 Problem Instances of Each Data  Set Data sets   LG SVM SCL-MI S/M/VM  sci vs.

talk  L  60.12 62.24 59.62 75.80  85.13 R  73.42 71.94 72.61 90.20  Total 68.89 68.64 68.19 88.48 rec vs.

sci  L  58.83 56.27 58.71 79.38  78.95 R  75.72 72.42 75.38 94.25  Total 67.39 64.46 67.16 86.71 rec vs.

talk  L  62.81 68.33 61.56 76.01  92.42 R  77.27 75.47 76.19 93.14  Total 75.36 74.53 74.26 93.04 comp  vs.

rec  L  61.60 60.78 61.26 89.78  92.52 R  84.92 79.17 84.92 95.74  Total 84.60 78.91 84.59 95.70 comp  vs.

sci  L  59.71 58.59 59.68 71.81  75.30 R  75.74 71.31 75.37 90.11  Total 72.73 68.93 72.43 87.34 comp  vs.

talk  L  / / / 93.85  / R  89.59 90.39 88.30 96.66  Total 89.59 90.39 88.30 96.66       ? 6 CONCLUSION Canonical correlation analysis (CCA) is a st tools, used to analyze the correlation betwee random variables. It is a natural way to le space of cross-domain correlation. By app Correlation Analysis (CCA) to transfer lea proposes a new method called Cross-do Correlation Analysis (CD-CCA).

