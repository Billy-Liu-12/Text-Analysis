A NOVEL ASSOCIATIVE CLASSIFICATION ALGORITHM: A  COMBINATION OF LAC AND CMAR WITH NEW MEASURE OF

Abstract: In recent, Association Classification not only has widely  adopted but also has performed well in data mining. The literatures have been argued that the small disjunction and using multiple class-association rules have significant effect on classification accuracy. This paper is based on CMAR (Classification based on Multiple Class-Association Rules) and Adriano Veloso proposed Lazy Associative Classifier algorithm for Small Disjunction mining. In addition, we collocate with a new weight calculation method in our algorithm to solve weight bias problem of CMAR. This paper uses UCI 26 data set for experiment on our proposed algorithm. The finally results convincingly demonstrated that our proposed algorithm is high accuracy.

Keywords: ssociative Classification; Association Rule; Data mining;  Multiple Rules; CMAR; LAC;  1. Introduction  Associative classification was proposed by Bing Liu et al [1], which called CBA algorithm. It refers to Srikant and Agrawal proposed apriori association rule [2][3]. The apriori association rule is a way to deal with transaction data of associative mining. The main target is through it to find out those transaction items relationship. Beside, it can mine out knowledge with comprehensibility in large data size situation so that it enables user to further research in potentially unknown rules. The CBA algorithm was proposed by Bing Liu et al. That is inherited two indexes of apriori-support and confidence. Using both indexes become thresholds in rule mining and further step for use of rules pruning. The purpose is in order to reduce redundant and harmful rules. The reason why it is to ensure all obtained rules are high quality and obtain good accuracy considered by efficiency and effectiveness. Subsequently, Wenmin Li et al. proposed CMAR algorithm [4]. The main difference is adding a concept of multiple class-association rules in  CMAR algorithm. In short, it is using two or more rules for predicting a class of test instance, and it replaces traditional method of using only one rule in classification phase. After calculating a weight, classifier will take the highest weight of class group, and then using it classifies a test instance.

Finally, it had been demonstrated that using multiple rules classification can obtain good accuracy.

Small Disjunction problem had been argued by Robert C. Holte et al since 1980. The small disjunction problem is meaning that a rule may important for some instances but own low support so that the rules would be removed in rule mining phase. Therefore, it caused a problem that the accuracy may be decreased [5] [6]. CMAR algorithm adopts support and confidence been association rule threshold. It also lies in support characteristic of downward-closure to reduce calculation cost, so as to cut down the training cost. But, it may cause a problem that some important rules may be losing so that some instances may not be able to classified correctly. f only we decrease minsup to find out of those low support rules, that will meet a large number of rules and noise rules to be generated. The former causes calculating burden; the latter causes accuracy decrease.

The weight bias indicates that the weight calculation method is too overweight to reflect the fact. This paper is based on CMAR algorithm of using multiple class-Association rules for classification. In used Weighted X2, it is batter than using chi-square test be rule weight because Weighted X2 overcomes bias in some situation.

However, it still has had bias in some situation.

This paper is based on CMAR algorithm, and collocates with Adriano Veloso et al.?s lazy associative classification (LAC) for small disjunction rules mining.

Beside, we propose a new weight calculation method in order to improve CMAR Weighted X2 bias problem (which will be introduced in section 3) and to obtain high classification accuracy.

The rest of this paper is arranged as follows. The          section 2 introduces CMAR algorithm and LAC algorithm.

The section 3 induces our proposed algorithm and discusses how to integrate LAC algorithm into our algorithm. The section 4 is experiment result of this paper. The section 5 is our conclusion.

2. Materials and Methods  2.1. CMAR  CMAR algorithm was proposed by Wenmin Li et al.

since 2001 [ ] [7]. The major feature is using multiple rules to classify an instance. In the past, the first thing is to sort all of the rules before the start of classification in associative classification. The sorting priority is order by confidence > support > length of rule. It is in order to evaluate what rules have own the highest priority for using.

In classification phase, the first thing is to scan rules from high priority to low, and then take the first-matched rule to classify test instance. After finding the first-matched rule, we will use the rule?s class to classify test instance.

However, CMAR algorithm is unlike single rule classification method. It adopts multiple class-association rules to classify a test instance. The algorithm major concept is to pick out those matched rules which match test instance. According to those rules, algorithm will calculate their Weighted X2 by rule class groups. Then, the class of the maximal Weighted X2 of group is selected to classify test instance.

In parts of classification phase, CMAR is a multiple class-association rules algorithm. When a test instance will be classified, CMAR algorithm will select a number of rules. Then, pick out the rule group which one of rule groups has own the highest Weighted X2.

More detail, the first thing is to collect rules, that the rules can correctly classify test instance. After that, we have to compare those rules whether all rules have same class. If totally obtained rules have same class, CMAR will use this class to classify test instance. Oppositely, if obtained rules are not same, those rules will be grouped by classes of rules.

Then, after calculated each group?s Weighted X2, the CMAR will use own the highest Weighted X2 value of group?s class to classify test instance.

?= 2   max   X XXXweighted  eT T  cPcPX ||) ||  )sup()sup()}sup(),(min{sup(max 22 ?= (1)  ))sup(|))(|sup(|(|  )sup())sup(|(|  ))sup(|)(|sup(  )sup()sup(  cTPTcPT  cTPcP e  ?? +  ?  + ?  +=  In parts of weighted calculation, CMAR algorithm adds X2-testing conception called Weighted X2 (formula 1).

In max X2, the CMAR paper has explanted that ?The max X2 is upper bound of X2-value against rule?.

The CMAR algorithm has three rule pruning methods but we have no explanation in this paper. The details can be seen in paper of CMAR.0  2.2. LAC  Lazy Associative Classifier was proposed by Adriano Veloso et al. since 2006 [5], and the subject is to solve small disjunction problem. LAC is a lazy learning algorithm. The algorithm generates rules only when the needing being classified instance is come in. The LAC algorithm can be divided into two phases: (1) The first phase is data set filter, it retains the training instances which each instance has a tuple identical to the test instance at least one. That is to say, among the training set, if a training instance has no at least one tuple identical to the test instance, it will be removed. Otherwise, it will be retained.

(2) The second phase is using after phase one filtered training set to generate rules. Hence, the filtered training set will have absolute relation with test instance. And those rules of low support may have enough support to pass the threshold.

Table 1. Data set, example source: [5]  Play Outlook Temperature Humidity Windy  yes rainy cool normal false no rainy cool normal true  yes overcast hot high false  no sunny mild high false yes rainy cool normal false  yes sunny cool normal false yes rainy cool normal false  yes sunny hot normal false  yes overcast mild high true no sunny mild high true  ?(yes) overcast hot low true                 Table 2. Filtered data set, example source: [5]  Play Outlook Temperature Humidity Windy  no - - - true  yes overcast hot - -  yes - hot - -  yes overcast - - true no - - - true  ?(yes) overcast hot low true   Taking an example to explain the algorithm steps,  table 1 is a data set. The rows are training instances except last one is a test instance. Phase one is to filter the training instances which have no at least one tuple identical to the test instance. In table 1, we can see the corresponding attribute {Outlook overcast}, {Temperature hot}, {Humidity low} and {Windy true}. Therefore, the retained training set must to own those attributes at least one. The filtered training set has been presented in table 2.

In phase two, it is using phase one results to start rules mining (shown in table 2). For minsup 40% situation, we get two rules and we can correctly classify test instance.

1. {outlook=overcast?play=yes} 2. {temperature=hot?play=yes}   Oppositely, we use general associative classification method for rules mining in table 1. The minsup is identical to 40%, the result of rule mining as follows:   1. {windy=false and humidity=normal?play=yes} 2. {windy=false and temperature=cool?play=yes}   A thing can be found from mined rules, the traditional  associative classification method can?t satisfy test instance probably because of small disjunction problem.

In order to accelerate the algorithm performance, LAC has a cache mechanism called pool of entries. Every rule and data are stored in items. A symbol {X C} represents a rule, and the item concept is <key, data>. Key={ X C }, data={Support, Confidence and Info.gain}. In the process of the LAC algorithm, first it checks whether a candidate rule is already in the cache. If an entry be found, the entry will be used. Otherwise, the candidate rule will be insert to entries pool and both support and confident after calculating and examining this candidate rule. Moreover, it is in order to ensure that the rules will not exceed memory limit. It takes a count of the use frequently to decide which of the rule being discarded. If quantity of entry will over the size, cache mechanism will discard least used entry, and  then the new entry is added to cache. Hence, we can ensure that the memory will not beyond the limit.

3. Methodology  In section 3, we will discuss our proposed algorithm and experiment design. Our algorithm is based on CMAR algorithm, we adopt using multiple associative-rules classification and similar rules pruning method. On support calculation method, we adopts support calculation method of LAC algorithm to overcome small disjunction problem for raising the accuracy. Moreover, we propose a new weight calculation method. It is different than Weighted X2 of CMAR algorithm so that we can overcome the weight bias problem.

3.1. Rules Producing  1   items=allRuleItmes(dataSet); 2   C1= candidateGen(items); 3   F1=dropLowLacSupportRules (C1, dataSet); 4   CAR1=dropLowConfdenceRules(C1); 5   insertRulesToCR_Tree(CAR1,CR_Tree); 6   For(k=2; Fk-1? ;k++)Do 7      Ck=candidateGen(Fk-1, items); 8      Fk=dropLowLacSupportRules(Ck, dataSet); 9      CARk=dropLowConfdenceRules(Ck); 10     CARk =delSpecificRules(CARk, CR_Tree); 11     insertRulesToCR_Tree(CARk, CR_Tree); 12   End 13  CARs=  pruningRulesBasedOnDatabaseCoverage(CARs);  Figure 1. Rules producing pseudo code  In our proposed algorithm, rule producing phase includes rules pruning and rules mining. Our rules pruning is to cite from rules pruning of CMAR algorithm. This paper adopts two kinds of rules pruning: (1) Only reserve general and high-confidence rules. (2) Selecting rules based on database coverage.

In parts of rules producing, because the support calculation method of LAC algorithm is different to traditional algorithm, so the variant FP-Growth of CMAR algorithm is not suitable for our algorithm. Therefore, this paper adopts apriori-like algorithm to rules mining. In classification phase, classifier of this paper adopts a concept of using multiple rules from CMAR algorithm. Beside, the classifier collocates with our proposed new weight calculation method to achieve optimum accuracy.

Rules producing pseudo code of this paper is shown in figure 1. Line 1 is to collect all attribute items from dataset for rule permutation using. There are some different than other algorithms. An example of CBA algorithm, its first          thing is to obtain frequent items. That is to filter low support attribute items. If the support of attribute item is below threshold, it will be removed from item set. However, we use LAC algorithm proposed support computing method so that we can?t filter any attributes when start. Because of we depending on the LAC algorithm mechanism, the training set size will be changed when we filter the training set. Hence, the filter step must to be omitted by using LAC algorithm. Line 2 is to permute all attribute items for producing probable combination of rules. Line 3 is support testing. This support computing is progressing by LAC algorithm proposed support computing mechanism That is that we will filter training set once before every computing support. It only reserves training instance rows that the training instance of attribute items fit with rule conditions at lease one. If a training instance has no reserve any attribute items, it would be removed from training set. Hence, this training instance will not appear in current training set.

More detail can be seen in LAC algorithm. In parts of support testing, if a candidate rule has not pass the support threshold, it will be removed from candidate rule set. This candidate rules will not continuously permute after removed. Beside, this rule will not be inserted into rule set.

Line 4 is confidence testing. If a rule has not pass, it also will not be inserted into rule set. But this rule will permute continuously. Line 5 is to insert rule into CR-Tree. About CR-Tree more detail can be seen in paper of CMAR. Next, line 6-12 is to produce k-items rule in loop way. Basically, line 7-9, 11 is similar before we described. In parts of line 11, we add a specific rule testing. We reserve general rule but specific rule. Specific rule testing is progress in CR-Tree. The CR-Tree can avoid linear scanning in whole rules. It stores rules by tree structure so that it can accelerate specific rule testing executing time. In last of line 13, we start executing rules pruning that selecting rules based on database coverage, the more details can be seen in original paper of CMAR. .

3.2. Classification  1   matchedRules = getMatchedRules(CR_Tree, target); 2   If matchedRules?s Class is same Then 3     isCorrect = classifyTarget(matchedRules.Class, target); 4     return isCorrect; 5   Else 6     For (i=0; matchedRulesi?; i++) Do 7       groupWeightedclass += getWeighted(matchedRulesi); 8     End 9     maxGroup= getMaxGroupWeighted(groupWeighted); 10    isCorrect = classifyTarget (maxClassGroup.Class, target); 11    return isCorrect; 12  End If  Figure 2. Classification pseudo code  In classification phase, this paper adopts CMAR algorithm proposed classification method of using multiple class-Association rules. It is to use one or more rules to classify a test instance. In addition, classification phase also collocates with a new weight mechanism to calculate each weight of rule groups. Thus, we can pick the best prediction out for classifying test instance.

Figure 2 is a classification phase pseudo code of this paper. Line 1 is to collect all of rules that the rule conditions are matched target test instance. Then, insert matched rules into matchedRules for storing. after storing matched rules, Line 2 is to check matched rules whether all class of  rules is identical. If all class of rules were identical, we will use this class to classify target test instance. And we store the classified result in isCorrect and return them. Otherwise, line 6-8 will be executed. Line 6-8 is to calculate the weight of rules, and to add up weight to the identical class of rule group. Line 9-11 is to pick maximal weight rule groups out and uses this class of rule group to classify target test instance. Finally, we store the classification result in isCorrect and return them.

In addition, this paper also has proposed a new weight calculating method, which is different than CMAR algorithm. The major objective is to overcome the weight bias problem. The meaning of weight bias is that the weight may be not fair in some situation. In some situation, the weight calculation may give some groups too more weight value so that it can?t reflect the fact. This paper adopts multiple class-Association rules of CMAR algorithm to be our classification method. The Weighted X2 can solve some bias when directly using X2 been our weight. However, it is still bias in some situation. We will demonstrate about this bias with an example.

Example 1: table 3 is a loan data set. This example has own two association rules.

? R1 job=no  rejected (support=18, nfidence=60%)  ? R2 ed=univ  approved (support=190m nfidence=95%)  Table 3. An example of weight bias.

R1 approved rejected total  job=yes 438 32 470  job=no 12 18 30  total 450 50 500   Intuitively, general classification method will pick high confidence rule for classification between R1 and R2.

Hence, rule R2 will be picked out because it has higher          confidence than R1. However, in example 1, R1 and R2 of X2-value are 88.6 and 14.8, respectively. But, in CMAR algorithm, R1 and R2 of Weighted X2 are 27.36 and 4.91, respectively. It can be seen from example 1 that using X2-testing and Weighted X2 can not actually reflect fact in this situation. In view of this, this paper proposes a new weight method in association rule so that the new weight can actually reflect fact and obtains better accuracy. This paper proposed a new weight calculation method is based on confidence. It is to utilize confidence itself provided quality information to be a base of weight calculation. And it also collocates with exponential function and power function for using. Therefore, it can readjust effect of confidence in rule group due to the functions decrease the effect of low quality rules. This paper proposed weight calculation method is represented in formula 2. The p1 and p2 are adjustable arguments, it affects variation of rule weight value for each matched rules.

?= )*exp( 21 pconfpWeighted  (2)  Using example 1 to demonstrate, the arguments p1 and  p2 are set to 2. The R1 and R2 are 2.05 and 6.08, respectively. R2 will be choice in our proposed method. But, R1 will be choice in CMAR algorithm. Thus, using our proposed weight calculation method can avoid bias of Weighted X2 of CMAR in example 1 situation.

In parts of section 4, we test our proposed algorithm on 26 data sets from UCI Machine Learning Repository.

The 26 data sets has often been used to experiment of classification algorithm accuracy, and so are we.

4. Experiment  4.1. Experiment Environment  This paper uses UCI 26 data sets for experiment [9]. It is the most popular on classification algorithm. Those are anneal, austral, auto, breast, cleve, crx, diabetes, german, glass, heart, hepatic, horse, hypo, iono, iris, labor, led7, lymph, pima, sick, sonar, tic-tac, vehicle, waveform, wine and zoo data sets, respectively.

The experiment were performed on Windows 7 64bits and Intel i7-860(OC 3.5G) and 4GByte RAM. The program was constructed by Microsoft Visual C++ 2008 Express and was compiled into 64 bits. The experiment?s arguments settings are according to the most common settings in associative classification algorithm. The support threshold (minsup) is set to 1%, confidence threshold (minconf) to 50%, and coverage threshold to 4, and p1 and p2 of formula 3 is set to 2.

4.2. Experiment Result  In parts of rule producing limit, it is according to CMAR algorithm to limit the number of rules on 9 data sets.

Other data sets will be completely mining. The limitation of number of rules is represented by table 4. In table 4, the condition is the meaning of number of X in rule X Y. For example, the number of conditions is 6 in data set of anneal.

It represents that permutation of attribute items will be end when the conditions of rule have more than 6 attribute items. Then, the algorithm will into next phase. Due to that, we can easily avoid that rules producing cannot be performed in finite resource when attribute items are too more.

Table 4. Limitation of the number of conditions.

Data Sets Conditions Data Sets Conditions Anneal 6 Sick 3 Auto 4 Sonar 4 Hypo 3 Vehicle 4 Iono 3 Zoo 6  Lymph 6  This paper of experiment compares to CBA ,CMAR,  LAC and L3[8] algorithms. In parts of CBA and CMAR algorithms results, we cited both data directly form paper of CMAR provided results. But, LAC and L3 algorithms data cite from their original papers. The experiment results can be seen from table 5, we win 11 data sets over other algorithms. Our classification algorithm accuracy is 87.1.

This paper proposed algorithm of accuracy is greater than CBA algorithm 2.41%, CMAR algorithm 1.88%, LAC algorithm 1.12% and L3 1.01%.

5. Conclusion  This paper is based on CMAR algorithm, and successfully integrates support calculation method of LAC algorithm into this paper. The experiment proves that using support calculation method of LAC algorithm to obtain small disjunction rule. After combining with multiple associative-rule method, we can effectively raise the accuracy. On the other hand, this paper proposed a new weight calculation method not only greater than CMAR and LAC algorithm but also greater than L3 algorithm 1.01%. In addition, this paper proposed algorithm usually has high accuracy whether the number of rules or number of candidates are high limitation or low limitation. Hence, the experiment demonstrates to us that our algorithm has features of stable and high accuracy.

In parts of follow-up, this paper has no perform all of          L3 used data sets. Therefore, follow-up pursuer might consider performing more data set so that it could test our proposed algorithm whether it still has stable and high accuracy. Beside, using FP-Growth to mining rule is not suitable for support calculation method of LAC algorithm so far. Because of support calculation method of LAC algorithm is different than other classification algorithm.

But, we can still test other algorithm or data structure to combine with FP-Growth in rule mining. It might accelerate performed time in rule mining and support calculation so that the whole rule producing time could be cut down.

