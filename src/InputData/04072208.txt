A New Improvement on Apriori Algorithm

Abstract  Efficiency has been concernedfor several years in the research ofassociation rules mining. In this paper, based on the improvement on the classical Apriori algorithm, a high-dimension oriented Apriori algorithm is proposed Unlike existed Apriori improvements, our algorithm adopts a new method to reduce the redundant generation of sub-itemsets during pruning the candidate itemsets, which can obtain higher efficiency of mining than that of the original algorithm when the dimension ofdata is high.

Theoretical proof and analysis are given for the rationality ofour algorithm. Experiments with datasets ofKDD Cup 1999 validate our work.

1. Introduction  Since the first proposal of association rules mining by R. Agrawal [1] [2], many researches have been done to make mining frequent itemsets efficient and scalable. As one of the focuses at present, the Apriori- like algorithms find frequent itemsets based upon an iterative bottom-up approach to generate candidate itemsets. But, Apriori-based algorithms suffer from several deficiencies: too many scans of the transaction database when seeking frequent itemsets, too large amount of candidate itemsets generated unnecessarily, the redundant generation of identical sub-itemsets and the repeated search for them in the database, and so on.

In this paper, the High-Dimension Oriented Apriori algorithm, the HDO Apriori for short, is proposed for miming the association rules in the high-dimensional data. Based on the classical Apriori, the new algorithm can cut down the redundant generation of identical sub-itemsets from candidate itemsets, by means of pruning the candidate itemsets with the infrequent itemsets with lower dimension. So it can obtain a higher efficiency than that of the original algorithm when the dimension of data is high.

In this paper, the related work is presented in Section 2. One of the deficiencies of the classical Apriori algorithm, to be tackled by the HDO Apriori algorithm, is introduced in Section 3. Section 4 presents the principle and the process of the new algorithm. Section 5 shows the experimental results and analysis. Section 6 offers conclusion of this work.

2. Related Work  To improve the performance of the association rules mining, different ways have been proposed. One of them is updating the algorithms to reduce scans of the database or redundant generation of itemsets. The existing approaches are either FP-growth [3] based or Apriori-like.

In the FP-growth algorithm, without the high time cost in the generation of candidate itemsets, the elevation of the efficiency is attributed to the condensation of data into the structure FP-tree, which can facilitate to both the generation of association rules and the update of the mining result by fewer scans of the new data. And there are also some improved algorithms based on it, such as P-tree algorithm [4].

However, Apriori-like algorithms also put emphasis on reduction of candidate itemsets. For example, the improved Apriori algorithm based on Granular Computing [5] makes use of information granules to symbolize the different characteristics in candidate itemsets for indications, which reflect whether the itemsets can be used to generate candidate itemsets with others or not. The essence of this algorithm lies in reducing redundant candidate itemsets during their generation. There are also other techniques applied to improving the Apriori algorithm, such as hash-based techniques, partitioning, transaction reduction, sampling, and dynamic itemset counting [6-9].

Different from other improved algorithms on Apriori, in this paper, the HDO algorithm adopts a new method to prune candidate itemsets with infrequent itemsets instead of frequent itemsets. By the property of infrequent itemsets, candidate itemsets can be  1-4244-0605-6/06/$20.00 C2006 IEEE. 840    pruned validly with the infrequent itemsets with lower dimension. And the deficiency, redundant generation of identical sub-itemsets from candidate itemsets, of the original algorithm can be tackled. So the efficiency of the association rules mining can be improved when the dimension of data is high.

3. Problem Description  Before investigating the classical Apriori algorithm to illustrate its deficiency, two variables are to be declared:  Lk denotes the set of k-dimensional frequent itemsets.

Ck denotes the set of k-dimensional candidate itemsets.

From the detailed Apriori algorithm in [2], we can find the main parts of the Apriori Algorithm are composed of three essential procedures: the function Join, the function Prune, and the validation of candidate itemsets. The function Prune is used to delete the unnecessary candidate itemsets in advance, so the time complexity can be reduced by fewer scans of the database when validating candidate itemsets.

But, the deficiency just lies in this function, because as one of its important steps, it's time-consuming to verify whether all the k-subsets of a (k+1)-itemset are frequent itemsets or not. Actually, in the original algorithm, a large amount of k-dimensional sub- itemsets, which include identical items but are contained in different (k+1)-itemsets, can be both generated and verified repeatedly. For example:  Let C4 be a 4-itemset, and set C4= { {a,b,c,d}, {a,b,c,e}, {b,c,d,e}, {a,b,d,e} }, where  a,b,c,d,e are the instances of different items.

When the function Prune is called, {a,b,c} is  generated as the subsets of {a,b,c,d} and {a,b,c,e} respectively. Moreover, in this procedure, it is also verified twice. Hence, during the process of miming, the same verification recurs unnecessarily at a high frequency, which seriously deteriorates the efficiency.

To tackle the deficiency, a new opinion about deleting multiple infrequent itemsets during just one scan of candidate itemsets is proposed in this paper. If we can obtain multiple infrequent supersets in Ck+1 of a certain infrequent itemset by a single scan of Ck, the efficiency will be greatly promoted.

4. Improved Algorithm  4.1. Principle of the Improvement  The improvement is mainly in the function Prune based on the following property of infrequent itemset:  Property 1. Let Y 5I and X5c Y. If the X is an infrequent itemset, Yis also an infrequent itemset [10].

This property enables the deletion of the infrequent itemsets from Ckj1 with the infrequent itemsets in Ck.

Some (k+1)-itemsets share the identical k-subsets, so by the property, multiple infrequent supersets in Ck+1 of a given infrequent itemset can be searched out by a single database query when the query condition is set to be the attributes of the infrequent itemset. Compared with the large amount of repeated generation of sub- itemsets and only a single result from one database query in the function Prune of the classical Apriori algorithm, the efficiency is highly promoted.

4.2. Algorithm in Detail  To implement the improvement, a new variable Lk is introduced.

Lk denotes the complement of set Lk where the universal set is Ck.

And if an itemset is verified to be an infrequent itemset, it will be inserted into Lk instead of being deleted.

Hence, to indicate whether an itemsets is a frequent itemset or not, a flag item is added to the itemset. And correspondingly, the procedures of validating Ck to generate Lk are changed: first select the itemsets included in Ck but not in Lk then compare their supports to the minimum support.

And the function Prune is modified into the following procedures: first select each of the itemsets in Lk sequentially, and then use it as the query condition to search for their supersets in Ck+1, and finally insert the searching result into Lk+1l  So the particular modifications are given as follows:  1) The procedures in the main function of Apriori algorithm is modified as follows:  Improved Apriori Algorithm: Input: DB, minsupport; Output: frequent itemsets; Method: begin  Lo:= 0 ; Frequentltemsets:= 0 C1:= { {i}| ie I}; k:= 1; Lk := {ce Ck Isupport(c)<minsupport} while (Ck#&0 ){  Lk:={CE CkI support(c)> =minsupportA c E Lk };  Lk :={C C E Ck A C Lk}; //update Lk  1-4244-0605-6/06/$20.00 C2006 IEEE. 841    Ck ,:= Apriori - gen (Lk); k:= k+ 1;  Frequentltemsets:= Frequentltemsets U Lk; } return Frequentltemsets;  end  2) The function Prune is changed as follows:  Function Prune Input: Ck+i; Lk Output: Lk+l; Method: begin  for V c Lk  {for Vse Ck+j if 3 k- subsets(s) = c insert s into Lk+l }}  end  Though the efficiency is highly improved, the validity still needs to prove. Whether Lk+1 screened  out from Ck+1 by Lk has contained all the infrequent itemsets or not is to be discussed in the following parts.

4.3. Validity of the HDO Algorithm  To prove the validity of the HDO algorithm equals to validate the completeness of screening Ck+1 with Lk' that is, before the supports of itemsets are  compared with the minimum support, Lk+1 has contained all the itemsets which can be deleted when the function Prune of the classical Apriori algorithm is performed. So for the proof of the validity, a new variable Nk is defined to denote the set of all the k- itemsets which can be deleted by the function Prune of the classical Apriori algorithm.

Hence, the definition of Nk ensures such a proposition to be true: Proposition 1. Not all the k-subsets of an itemset in Nk+1 are members of Lk.

Otherwise, if all the k-subsets of a certain itemset in Nk+1 are contained in Lk, by the procedures of the function Prune of the classical Apriori algorithm, the itemset can't be selected into Nk+1. It is contradictory to the definition of Nk+1. Thus this proposition holds. And this proposition is a necessary condition for the proof of the validity.

Then, to validate the completeness, the following propositions are to be proved ahead: Proposition 2. If c is an itemset in Ck+1, then all the k- subsets of c are members of Ck.

Proposition 3. Given the candidate (k+1)-itemsets Ck+1, if c is an itemset in Nk+1, then there exists at least one k-subset of c in Lk  Here is the proof of Proposition 2:  Since Proposition 2 is true, we can infer by Proposition 2:  If c is an itemset in Nk 1 then all the k-subsets of c are members of Ck  And by Proposition 1, the following statement can be reduced:  If c is an itemset in Nk 1, then there exists at least one k-subset of c either in Lk or beyond Ck.

Hence, if c is an itemsets in Nk 1, then there exists one k-subset of c in Lk or more.

Since Proposition 3 holds, by the procedures of the function Prune, we can assert that before validating the candidate itemsets, Lk+1 is equal to Nk+1. So the validity of the HDO algorithm is proved.

1-4244-0605-6/06/$20.00 C2006 IEEE.

Proposition: Given Vse Ck 1, if c is a k-subset of s, then V ce Ck Proof  For k1I, the proposition is evidently true.

For k=2, assume s is a 3-itemset generated  from two frequent 2-itemsets {x, a} and {x, b}, where x, a, b denote different items. Since {x, a} and {x, b} are frequent itemsets, hence {a} and {b} are frequent 1-itemsets. Then {a, b} is a third 2-subset of s, and {a, b} E C2. Thus, the proposition is true.

For k>=3, given se Ck 1, assume s is generated from two frequent k-itemsets {Xk 1, a} and {Xk 1, b}, where Xk 1 denotes a (k-1)-itemset that doesn't contain the corresponding items including a or b.

Then, because subsets of frequent itemsets are frequent itemsets, Xk is also a frequent itemset.

Let c be an arbitrary k-subset of s.

For c {Xk 1, a} or {Xk 1, b}, since ce Lk, hence  CE Ck For cX{Xk1, a} and c . {Xkl, b}, c must be  Yk-2, a, b} whereyk-2 is a (k-2)-subset ofXk 1I Since Yk-2, a} is a (k-1)-subset of {Xk1, a},  then {Yk-2, a} is a frequent itemset.

Similarly, {Yk-2, b} is also a frequent itemset.

Then, by the definition of Ck, {k-2, a, b}e Ck,  hence V ce Ck.

Thus, the proof is complete.

4.4. Estimation of the Performance  Since the validity of the improved algorithm has been proved above, in this part, the theoretical efficiency of the performance will be estimated.

Let ts be the time cost of a single scan of the database. Let t, be the time cost of generating Ckj1 from Lk. The variable mk is set to be the amount of itemsets in Ck without Nk; the variable 'kj1 is set to be the amount of itemsets in Ck 1; and the variable nk is set to be the amount of itemsets in Lk. A denotes the amount of the records in the database, and N denotes the dimension of the data. Set R =nkllk .Then, the total runtime of the classical Apriori algorithm is  N k+1I n Z (tSxmk + tC + Ik+l x 2 x ts xA  and the total runtime of the HDO Apriori algorithm is N I Z (ts x mk + tc +lk+lxtS x k) (2) n=1l  From the formulas, the difference in the runtime between the two algorithms depends on N, the dimension of the data, and R, the ratio ofLk to Ck. IfN is very large, and on the condition that the growth rate ofR is lower than that of k, the dimension of itemsets, the runtime can be highly reduced. And also compared with the classical Apriori algorithm, when other parameters are fixed, the efficiency of the HDO Apriori algorithm can be improved relatively with a large value ofR.

5. Experiments and Analysis  For different requirement, two rounds of experiments are performed to compare the performance between the HDO Apriori algorithm and the classical Apriori algorithm. Results and analysis are given in the following parts.

5.1. Data Preparation  All the experiments are performed on a 2.8GHz Intel Pentium PC with 512MB memory, running on the Windows XP Professional OS. Programs are coded in C++ on the platform ofC++ Builder 6.0.

The dataset is obtained from the KDD Cup 1999 Data [KDD99] without the records ofDOS (Denial Of Service). There is a large amount of recurrences in the records of DOS, and the high recurrences can make the amount of candidate itemsets explosively increase as the geometric series. So for cutting down the time consumptions during the experiments, the elimination of this type of records is performed ahead. Moreover, for the convenience of data processing, the character strings have been classified and replaced by digital  symbols. And some numerical attributes whose values cover a larger range but too few recurrences, such as the length of the datagram, have been clustered into less than 20 classes by the K-means algorithm.

After the preprocessing of the dataset, 59436 records are obtained with 42 attributes. And 12 of the attributes are selected for the association rules mining.

5.2. Experiments and Analysis  In the first round, the experiment is designed to display the influence ofN, the dimension of the data.

During the experiment, the minimum support is set to be 0.836% (500/59436), and the dimension is set to 5,8,12 respectively.

The result is given as follows:  Table 1. The comparison of the runtime at different dimensions between the classical Apriori algorithm and the HDO Apriori algorithm.

Classical HDO Apriori Apriori  N=5, minsupport= 0.836% 228s 251s N=8, minsupporh 0.836% 151m 145m N=12, minsupporh 0.836% 12.3h 11.5h  We can contrast the changes in efficiency from the result: when N is relatively small, the classical Apriori algorithm still prevails over the HDO Apriori algorithm in the runtime; but with the increase ofN to some content, the HDO Apriori algorithm tends to prevail; and with the further increase of N, the advantage of the HDO Apriori algorithm can be greatly enhanced.

Actually, in this process, the given minimum support and the large value ofN cause the growth rate ofR to be fairly lower than that of k, so the runtime is reduced comparatively.

In the second round, the experiment is designed to display the influence of R. Because the growth of the ratio R results only from the reduction of the minimum support when N is given, and the minimum support is the only parameter input manually, the experiment is operated to display the influence of the minimum support instead. During the experiment, the dimension k is set to be 5, and the minimum support is set to 0.084% (50/59436), 0.252% (150/59436), 0.836% (500/59436) respectively.

The result is given as follows:  1-4244-0605-6/06/$20.00 C2006 IEEE. 843    Figure 1. The comparison of runtime at different minimum supports between the original Apriori and the improved Apriori  From Figure 1, we can know that: with the reduction of the minimum support, the efficiency of the HDO Apriori algorithm can change correspondingly from a lower level to a higher level than that of the classic Apriori algorithm.

Verified by the experiments, when the dimension of data is high, the efficiency of the HDO algorithm is greatly improved. And also, if the minimum support is small enough, the comparative improvement can also be obtained on efficiency.

5.3. Discussions  For different datasets, the proper values of minimum support to show the advantages of the HDO Apriori algorithm are different, but we have not found a method to figure them out, because they partly depend on the internal characteristic of data. For example, assume some types of data can be divided into two sorts by support and there is a large gap in the values of the supports between the two sorts of data.

So a small change of the minimum support within the gap can influence just a little on the number of frequent itemsets. Hence, for the ratio R has increased just a little, the efficiency changes hardly. On the contrary, if the supports of data distribute averagely, the influence of the minimum support can be evident. Hence, the HDO algorithm will be surely applicable to the latter types of data.

In contrast, when the dimension of data is high, the advantage of the HDO Apriori algorithm is evident no matter what the internal characteristic of data is. That's why our algorithm is high-dimension oriented. But if we want to precisely predict how much the efficiency can be improved at given data, for the influence of the differences in data, further research is also required.

proposed to update the classical Apriori algorithm.

Through pruning candidate itemsets by the infrequent itemsets with lower dimension, the present algorithm can reduce the redundancy while generating sub- itemsets and verifying them in the database. Validated by the experiments, it can obtain higher efficiency when the dimension of data is high. Meanwhile, for different data, we still need further research to find methods to estimate how much improvement the HDO Apriori algorithm can implement.

Reference  [1] R. Agrawal, T. Imielinski, and A. Swami, "Mining Association Rules Between Sets of Items in Large Databases", Proceedings of the ACM SIGMOD Conference on Management of data: 207-216, May 1993.

[2] R. Agrawal, and R. Srikant, "Fast Algorithms for Mining Association Rules", In Proc. VLDB 1994, pp. 487-499.

[3] C. Liu, H. Lu, J. X. Yu, W. Wang, X. Xiao, "AFOPT: An Efficient Implementation of Pattern Growth Approach", In SDM 2003.

[4] Hao Huang, Xindong Wu and Richard Relue, "Association Analysis with One Scan of Databases", University of Vermont Computer Science Technical Report CS-02-3, 2002  [5] Taorong Qiu, Xiaoqing Chen, Qing Liu, Houkuan Huang, "An Algorithm of Association Rules Extracting Based on Granular Computing and Its Application", May, 2005.

[6] J.S. Park, M.S. Chen, and P.S. Yu, "An Effective Hash- based Algorithm for Mining Association Rules", SIGMOD'95, San Jose. CA, May 1995.

[7] A. Savasere, E. Omiecinski, and S. Navathe, "An Efficient Algorithm for Mining Association Rules in Large Databases", VLDB'95, 432-443, Zurich, Switzerland.

[8] H. Toivonen, "Sampling Large Databases for Association Rules", VLDB'96, 134-145, Bombay, India, Sept. 1996.

[9] S. Brin, R. Motwani, J. D. Ullman, and S. Tsur, "Dynamic Itemset Counting and Implication Rules for Market Basket Analysis", SIGMOD'97, Tucson, Arizona, May 1997.

[10] Pei-qi Lin, Zeng-zhi Li, Yin-lung Zhao, "Effective Algorithm of Mining Frequent Itemsets for Association Rules", Proceedmgs of the Third International Conference on Machine Leaming and Cybemetics, Shanghai, 26-29 August 2004.

6. Conclusion  In this paper, the HDO Apriori algorithm is  1-4244-0605-6/06/$20.00 C)2006 IEEE.

