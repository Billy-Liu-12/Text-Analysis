DMPML Data Mining Preparation Markup Language

Abstract  In this paper we propose the language DMPML as an alternative to the standardization of the data preparation phase in a KDD process. DMPML is based on XML and uses XSL transformations to map raw data into processed data. DMPML features, such as extensibility, robustness and platform independence, support exchanging of data preparation projects among DMPML producers in an ef- ficient way. This promotes work reusability and experience interchange among similar projects.

1. Introduction  For years, institutions have been storing enormous amounts of data in magnetic devices. However, in spite of the collective awareness that these great volumes of data possess a huge amount of information on the business of the institutions, intelligent technologies capable of extract- ing useful knowledge from these robust masses of data have not been available until the beginning of the 1980?s.

In response to this need, the KDD (Knowledge Discov- ery in Databases) process was proposed in 1989. This pro- cess has been defined as ?The nontrivial extraction of im- plicit, previously unknown, and potentially useful informa- tion from data? [6].

The KDD process includes the following phases: domain exploration, data preparation, data mining and interpreta- tion of the results. In the first phase, the problem and the solution space are explored. In the second phase, the data are selected, cleaned and transformed to serve as input data to the data mining phase. In the data mining phase, intelli-  gent methods are applied for the extraction of patterns with useful knowledge on the investigated problem. And in the last phase the extracted patterns are manipulated to generate interpretable knowledge for humans.

The development of standards to promote the reusabil- ity of the inputs and outputs of the different phases of the KDD process [13, 8] is a field of study that receives a lot of attention from researchers. Nowadays it is possible to find a significant number of emerging technologies based on XML (Extensible Markup Language) [4] specially cre- ated to provide an unified standard to the KDD process.

This paper proposes an alternative language, also based on XML, to standardize specifically the data preparation phase. It uses declaratives rules of transformation stored in XSL (Extensible Stylesheet Language) [5] files. This choice appears to be the most appropriate since the XSL technology grows in parallel to the XML technology. Be- sides, XSL transformations allow direct mapping between raw data to processed data in XML files without additional software.

To apply this approach to real projects of data prepara- tion, it is necessary to elaborate a set of XML files espe- cially tailored to allow direct use of XSL transformation rules. This is the main reason why DMPML (Data Mining Preparation Markup Language) was created. This XML- based language is described in greater details in the follow- ing sections of this document.

The rest of this paper is organized as follows: Section 2 surveys some technologies applied to the KDD process.

Section 3 makes a brief introduction to XML and its associ- ated technologies, and justifies the XML usage in this work.

Section 4 gives more details on the concepts and founda- tions of DMPML. Section 5 shows a practical example us-     ing the DMPML language. Finally, section 6 presents our conclusions and proposes future work.

2. Related work  This section surveys some proposed standards that can be applied to the KDD process. Some of them are based on the relational model and use query languages; others are based on XML.

2.1. Languages Based on the Relational Model  The Data Mining Query Language, described in [10], is a query language used in a data mining system called DB- Miner [9]. It adopts a syntax very similar to SQL and its ob- jectives are the extraction of different types of knowledge, such as association rules, discriminant rules, classification rules, and characteristic rules in a relational database and data warehouses in multiple levels of abstraction.

DMQL provides the means to specify:  ? relevant data,  ? types of knowledge to be mined,  ? previous knowledge to be used in the data mining pro- cess,  ? limits and metrics to evaluate the discovered patterns,  ? visual representation of the discovered patterns.

Two positive characteristics of DMQL are: (1) based on the results of queries, new queries can be executed inter- actively; and (2) it was developed to work with traditional databases.

The main weakness of DMQL is that it is essentially cen- tered on the extraction phase. So, to perform pre and post- processing operations, the language uses SQL or additional tools because it does not have the resources to make these operations.

MSQL [11] is a query language for association rules cre- ated as an extension to SQL. Its main characteristics are:  ? hability to nestle SQL expressions, such as ordering and grouping, being able to divide a query in many parts to facilitate the creation of the query commands.

? support to the closure property and availability of ope- rands to manipulate the results of previously executed queries.

? creation of association rules based on data in response to a query. Based on a data set, MSQL returns a set of rules that satisfy the set chosen in the query. This is performed by the use of the GETRULES operand, which creates rules and saves them in a rules base.

? manipulation of the results of performed queries:  ? search in an existing rule base (called rule min- ing). By the use of the SELECTRULES operand it is possible to search association rules in an ex- isting rule base.

? cross-over between data and association rules to make possible the identification of data subsets that satisfy or violate a given set of rules. This is performed by the use of the VIOLATE and SAT- ISFY operators.

? basic support to pre and post-processing.

MSQL could be extended to have a better support to the pre and post-processing steps. Even having support to oper- ators named CREATE ENCODING (provides discretization of continuous attributes) and SELECTRULES (selects rules in a base rule), it does not provide support to complex post and pre-processing operations, like sampling, for example.

The MINE RULE [14, 15] operator was created as an extension to SQL. It supports the extraction of association rules of relacional databases and its storage in a separate relation (supports closure). The main characteristics of this operator are:  ? allows the selection of the relevant data set in the database (like in DMQL and MSQL), so the associ- ation rules will be based only on the chosen data.

? definition of the structure of the rules to be mined and the restrictions that will be applied to them. This way, only rules with specific characteristics will be selected.

? definition of which data can be part of an association rule.

? basic support of post-processing, with search and se- lection of found association rules.

As in MSQL, data pre-processing in the MINE RULE is limited to operations that can be performed in SQL. It is not possible to obtain samples of data before extraction, and discretization might be done by the user. A positive point is the well defined semantics of its operations, like few other languages offer. To check a detailed comparison between these three data mining languages, see [2] or [3].

2.2. Languages Based on XML  One of these standards is a query language called KD- DML (KDD Markup Language) [19]. It is ?a middleware language and system designed to support the development of final applications or higher level systems which deploy a mixture of data access, data preprocessing, extraction and     deployment of data mining models? [17]. By a model, it un- derstands the output of the data mining process ? the knowl- edge. It was introduced [1] as an environment where the knowledge extraction problems (the input to the data min- ing phase) and their results (the output) were represented as XML documents. Later on, it was updated to support some operations of the preprocessing phase.

The KDDML language has been designed by consider- ing the KDD process as a query process, where the opera- tions within a query can be nested. This means that with KDDML it is possible to interactively create queries, exe- cute them, obtain results and process new queries on the re- sults obtained. The execution of the queries can be done by external programs or by supported operators implemented in an interpreter of the KDDML system. As stated in [17] ?a KDDML query is an XML-document where XML tags correspond to operations on data/models, XML attributes correspond to parameters of those operations and XML sub- elements define arguments passed to the operators?.

Another XML-based initiative to standardize the KDD process is PMML (Predictive Model Markup Language) [18]. It started as a language to represent predictive mod- els produced by data mining systems [7]. Later on, it was updated to support other data mining models, like decision trees, neural nets, polynomial regression and others [20]. It is based on XML, like KDDML, and focuses on the output of the data mining phase of the KDD process, describing the inputs to data mining models, the transformations used to prepare data for data mining, and the parameters which define the models themselves. It is being developed by the Data Mining Group (DMG), which is formed by data min- ing systems developers like IBM, Microsoft, SPSS and Or- acle.

The KDDML and PMML approaches are both relevant initiatives that contribute to the standardization and formal- ization of the KDD process. However, both have the same problem: the standardization effort usually concentrates on data mining techniques and algorithms, data mining query languages, knowledge semantics, optimization techniques, post-processing, etc. In fact, little attention is given to the data preparation phase, especially to the cleaning step.

However, we believe this is a big mistake since the data preparation phase is responsible for approximately 80% of all efforts in a KDD process applied to real problems [16].

It is in this phase that the information is consolidated and the complexity of the data mining tasks is reduced. If the data in this stage is not prepared properly then all the fol- lowing process may become a huge waste of time, energy and money.

The first relevant initiative especially created to standard- ize the overall KDD process1 using XML-based technolo- gies is the DMSL (Data Mining Specification Language)  1With the emphasis on the data preparation and transformation steps.

approach [12]. DMSL identifies five main primitives that play the major roles in the KDD process, represented by five sections in a DMSL document:  ? The data model: represents a data schema that defines the shape of the initial input data to be mined, together with other data mining specific information like data type, data form, granularity, and data scale.

? The data mining model: defines transformations of the initial input data into whatever shape is needed for data mining. This is where everything about the data preparation and transformation is stored.

? The domain knowledge: knowledge that can be used by a data mining task.

? The description of the data mining task: specifies a data mining task over a data mining model.

? The knowledge: contains the result of the data mining task.

The DMSL is based on a theoretical framework that can be used to represent the whole KDD process. The frame- work is built on three mathematical pillars:  ? relations are used to represent data and data mining matrices (and their fields);  ? graphs are used to capture the structure of matrix and field dependencies;  ? functions are used to realize executive functionality and to express all the existing internal relationships within the framework.

Although DMSL is based on a sound theoretical frame- work, to the best of our knowledge there is no software im- plementation of DMSL available yet.

We can see that many languages have been proposed to standardize different phases of the KDD process. Yet, until now, more than 15 years after the definition of the knowl- edge discovery process, none of these technologies is in wide use. The benefits of having a standard language to represent the whole process are obvious. A good aproach to achieve a standard is to analyze the proposed languages (like the ones presented before), compare its advantages and disadvantages and mix them together trying to maintain their strenghts and minimize the effetcs of their weaknesses.

3. The XML Language  3.1. XML Concepts  XML is a language which has become very popular as the means to store data in text files as well as the mean- ing of these data. With XML it is possible to represent the     structure of the document, with no regard to the way it is to be presented. The data are organized in elements, which are similar to HTML tags. However, in HTML, the tags are predefined and immutable. In XML it is possible to rede- fine and extend the definition of elements and its attributes.

These characteristics mean XML is an extensible metalan- guage, a language that is used to define other languages and is not static.

One of the most popular applications of XML documents is to use them as the means to transfer data among different applications. Also, because XML documents are text files, its contents can be read using any text editor.

3.2. XML and Data Preparation in KDD  XML files seem a good alternative to the standardiza- tion of data preparation in KDD because it offers flexibil- ity and adequate structural organization to have persistence and good documentation, in an efficient and cheap form, in all the sub phases of data preparation. Besides this, it is possible to create schemas to validate the content of the files, using DTD?s (Document Type Definition) and/or XML Schemas, and to transform the content of an XML file using declarative rules defined in XSL files. DTD?s and XML Schemas are useful to reach the desired level of stan- dardization and formalization, while the use of XSL files is an efficient alternative to eliminate the necessity of devel- oping specific code to implement data transformation.

Even with these benefits, some concerns arise. The most frequent is the size of the XML documents. XML docu- ments always use more space than the corresponding raw data because we attach to each value its semantics tipically using elements and attributes. With the capacity of mag- netic disks increasing every day and the time used to access data inside the disks decreasing, this should not be a big problem. There is also the solution of data compression that performs very good rates concerning text files.

4. DMPML  The data preparation phase is subdivided in three sub phases: data selection, cleaning and data enrichment, and data codification.

In the data selection sub phase the primary objective is to choose only the relevant attributes from the complete set of attributes available in the raw data sources. The selected subset is sent to the mining algorithm. The main motivation to this selection is to try to optimize the processing time of the mining algorithm by shortening the search base.

The cleaning sub phase includes a verification of infor- mation consistency, the correction of possible mistakes, and the insertion or deletion of null or redundant values. In this  phase duplicate and/or corrupted data are identified and re- moved. The execution of this phase corrects data, elimi- nating unnecessary queries that would be executed by the mining algorithm and would affect its processing and effi- ciency. The data enrichment consists of aggregating more information into the actual data so that it can contribute to the process of knowledge discovery. In other words, the data enrichment is any process capable of increasing the quality of the information available where the main goal is to improve the mining algorithm efficiency.

In the codification sub phase, the goal is to transform data so that they can be used/processed by the mining al- gorithm, which can be a decision tree, an artificial neural network, a clustering algorithm, etc.

A conventional neural network, for example, normally accepts as input only numerical values (or scalars) that are between 0 and 1, or between -1 and 1, depending on the activation function of the artificial neurons that are inside the RNA layer. Because of this restriction, the numerical attributes are usually mapped to normalized numerical val- ues inside the required interval. The categorical attributes are coded into binary representations using, for example, a binary representation of length M and with N bits turned on or equals to 1.

In decision trees, it is usually needed that the numerical attributes be ?categorized? using percentils or even nominal representation intervals. The number 10, for example, could correspond to the interval named ?from 0 to 20?.

The DMPML aims to cover all these data preparation phases. To reach this goal, the DMPML proposes the cre- ation of four different types of XML files. These are:  ? IDDP (Input Data for Data Preparation). This file is constituted by the original data selected and extracted from any type of source data. This source data may be a relational database, a datawarehouse, a datamart or even a text file with some kind of formatting for its fields and records. The IDDP files have the original data that will be processed and later transformed into an appropriate input to a data mining algorithm.

? DPDM (Data Processing for Data Mining). To process and transform the information contained in IDDP files, it is necessary to have a data preparation project adequate to the relevant data. Data preparation projects encapsulate directives and processed data and are stored in DPDM files. A unique DPDM file may have many data preparation projects for different IDDP files. And the IDDP files can only be processed and transformed if a data preparation project was specifi- cally built to be applied to process it.

? XSL Transformation file. The transformation or mapping of values from IDDP files is performed with     the application of a XSL file. It is expected that this XSL file contains declarative rules that permit map- ping the original values contained in the IDDP files into data ready to be used in data mining. According to the structure anticipated by DMPML, each XSL file contains the transformation rules necessary to gener- ate input data to one type of data mining. This way, there should be one specific XSL file to generate trans- formed data to neural networks, another to inducted associative rules, another to induction trees, and so on.

? IDDM (Input Data for Data Mining). IDDM files have an internal structure that is very similar to the IDDP files. Nevertheless, IDDM files store trans- formed data ready to be used as input data by any min- ing algorithm. IDDM files are generated from IDDP files that are processed and transformed using the in- formation and transformation directives contained in DPDM and XSL files.

Figure 1 shows the sequence in which the phases of data preparation are executed so that it generates an IDDM file based on an IDDP file.

In the following subsections, the contents of the IDDP, DPDM and IDDM files are described in deeper details.

4.1. IDDP Documents  The IDDP files of DMPML store the original (not pro- cessed) data, usually extracted from a relational database (RDB), a datawarehouse (DW) or even a text file. IDDP files provide the input data for the generation of data prepa- ration projects contained in DPDM files.

Every IDDP file contains a root element named <IDDP>. And, inside this root element, there are two other elements: <properties> and <variables>. The <properties> element contains information about the IDDP file and about the original data source used to create it (RDB, DW or text file).

The <variables> element contains the data as well as in- formation about the variables (or attributes) that exist based on the extracted view. Each variable is identified by the <variable> element. And each <variable> element en- capsulates a list of <value> sub elements, where each of these sub elements encapsulates the values extracted from the data source of the IDDP file. This way, if the data source is a text file with 200 columns and 1000 records then the <variables> element will have 200 <variable> elements.

And each <variable> will contain 1000 <value> sub ele- ments, one for each value of the data source.

4.2. DPDM Documents  DPDM documents store data preparation projects which, in turn, encapsulate guidelines and processed data used for  the transformation of values of IDDP files into values of IDDM files. All the data preparation projects contained in a DPDM file are generated based on values of an IDDP file.

The guidelines of the data preparation project and the prepared data are delimited by elements named <project> that are inside the root element named <DPDM>.

The <project> element contains two subelements: one called <properties> and the other called <variables>. The first one encapsulates information about the data prepara- tion project itself where as the second one contains all the guidelines and necessary data to map IDDP files into IDDM files.

Inside a DPDM file, the <variables> element is com- posed by a list of sub elements specification named <variable>. Each <variable> element is related to a vari- able contained in the associated IDDP file and contains at least three sub elements: <transformationDirectives>, <qualityInfo> and <distinctValues>. The first one con- tains a list of specifications of the methods adopted to gen- erate the transformed values, the second one contains some statistics about the data and the third one contains a list of distinct values of the related variable. This list is formed by the values of the IDDP file and by their transformed values, calculated through the use of the transformation methods specified in <transformationDirectives>. Typically, cate- gorical variables will have a restricted number of distinct values, whereas the numerical values and the date variables will often form a huge list of distinct values.

4.3. IDDM Documents  The IDDM files of DMPML store the transformed data, ready to be used as input data to some data mining specific algorithm.

IDDM files are generated based on three distinct files: one IDDP file with the original (non processed) data, one DPDM file with the related data preparation project and one XSL transformation file to the selected mining algorithm.

Every IDDM file contains a root element named <ID- DM>. Inside this root element, there are two other ele- ments: <properties> and <variables>. The former encap- sulates information about the IDDM file and the latter con- tains the transformed values of each variable contained in the IDDP associated file.

The <variables> element contains a list of <variable> sub elements. And each <variable> element encapsulates a list of sub elements <value> which encapsulate the trans- formed data, ready to be used by some specific data mining algorithm.

The number of elements <variables> and <value> of an IDDM file are always the same as those observed in the IDDP file. This happens because the variables and the stored values in the IDDM files are simply the IDDP values     Figure 1. Transformation of IDDP files (original data) in IDDM files (processed data) during the sub phases of the data preparation phase.

after the transformation is applied.

5. Example  In this section, we present a working example of our framework to make it easier the understanding of the pre- processing phase using XSLT. We will shorten the size of the files, emphasizing on the main components of the frame- work to make its understanding easier. We present: an IDDP file, containing the raw data and how its creation is done; a DPDM file, containing the pre-processing directives created based on the IDDP file; a XSL transformation file, which will obtain the codifications to be used by the neu- ral networks algorithm; and finally an IDDM file, resulting from the data transformation, which contains the codifica- tions ready to be used by a data mining algorithm.

5.1. IDDP File  Initially the raw data are stored in a database, a data warehouse, etc. The relevant data should be selected and ex- ported to the IDDP file format. Nowadays many databases export data in some XML format. A XSL transformation file can then be used to convert the data that are in the for- mat exported by the database to the IDDP format. If a XML database is used, we provide the XML Schema of the IDDP file and it will use it to check the validity of the file auto- matically.

Below is an example of an IDDP file that contains the original data from some bank clients.

<?xml version="1.0" encoding="ISO-8859-1"?> <IDDP> <variables> <variable id="000" name="AGE">  <value id="00000">30</value> <value id="00001">35XX</value> <value id="00002">40</value> <value id="00003">50</value>  <value id="00004">33</value> </variable> <variable id="001" name="MARITAL STATUS"> <value id="00000">SINGLLE</value> <value id="00001">MARRIED</value> <value id="00002">SINGLE</value> <value id="00003">WIDOW</value> <value id="00004">SINGLE</value>  </variable> <variable id="002" name="CONTRACT DATE"> <value id="00000">10/05/2003</value> <value id="00001">09/04/1999</value> <value id="00002">20/05/2002</value> <value id="00003">04/08/2000</value> <value id="00004">12/07/2001</value>  </variable> <variable id="003" name="GOOD/BAD"> <value id="00000">0</value> <value id="00001">1</value> <value id="00002">1</value> <value id="00003">0</value> <value id="00004">1</value>  </variable> </variables>  </IDDP>  In the example above, there are four variables to repre- sent a client. The first one is a numeric variable that repre- sents the age of the bank?s client. The next variable is cat- egorical and indicates the marriage status of the client. The third variable is a date and represents the date the client has opened an account in the bank. The last variable is categor- ical and represents if the client is a good or bad payer based on the usage of his/her bank credit card, where 0 (zero) is considered a bad payer and 1 (one), a good one.

5.2. DPDM File  A DPDM file is generated based on the IDDP file pre- sented in the previous subsection. To make this convertion, it is necessary a program that will ask the specialist for the outliers and missing intervals, to which value it should con- vert outliers and missing values (typically average and ex- treme values), which are the inconsistent values and how they should be treated. The program will then use the val-     ues presented by the specialist to generate the file below, performing the codifications needed by the data mining al- gorithm through an option in the program. The program actually process data to the neural network data mining al- gorithm. It is being improved in the number of codification values that it can generate and the graphical user interface to provide better usability.

If we look carefully at the aforementioned example, we can see that some values are inconsistent. For example, there is one age with value 35XX and a marital status with value SINGLLE. The specialist ought to somehow tell the program to consider SINGLLE values as SINGLE.

Below, we show an example of a generated DPDM file based on the IDDP file presented before.

<variable type="numeric" id="000" name="AGE"> <outliersAndMissingIntervals> <minMissing>  <value name="original" value="35"/> <value name="originalNormalized" value="0.2"/>  </minMissing> <minOutliers>  <value name="original" value="40"/> <value name="originalNormalized" value="0.4"/>  </minOutliers> ...

</outliersAndMissingIntervals> <transformationDirectives> <outlierMapValue>average</outlierMapValue> <missingMapValue>extreme</missingMapValue>  </transformationDirectives> <distinctValues> <distinctValue isMissing="true" isOutlier="true">  <value name="original" value="35XX"/> <value name="originalNormalized" value="0.2"/> <value name="similar" value="35"/> <value name="similarNormalized" value="0.2"/>  </distinctValue> <distinctValue isMissing="false" isOutlier="false">  <value name="original" value="40"/> <value name="originalNormalized" value="0.4"/> <value name="similar" value="40"/> <value name="similarNormalized" value="0.4"/>  </distinctValue> ...

</distinctValues> </variable>  In the <variable> element above we present the in- formation regarding the AGE variable. It contains the <outliersAndMissingIntervals> that indicates the intervals of the outliers and missing values informed by the special- ist in the DPDM program generator. The <distinctValues> element contains the original and coded values to be passed to the data mining program. It may contain a list of coded values. In addition, the same DPDM file can be used as an input to many data mining algorithms. We can see that the <value> subelements with value 35XX can be substi- tuted by the value 35, which was informed by the specialist.

The list of coded values can contain any values that will be used by data mining algorithms. In the next subsection we present the code that extracts the correct codification for the neural networks algorithm.

5.3. XSLT File  The data transformation process works as follows: each value of the IDDP file will be coded to a value that will be used by a specific data mining algorithm. The XSLT file will choose which specific value from the coded list is to be used. It will also check if the value is an outlier or missing and, if so, it will query the <transformationDirectives> element and retrieve the proper value that must substitute the original value, like av- erage or extreme. The intervals and their codification are specified in the <outliersAndMissingIntervals> element.

Below we present a template used to get the codification of outliers and missing values for artificial neural networks.

<xsl:template match="outliersAndMissingIntervals"> <xsl:param name="original"/> <xsl:param name="mapValue"/> <xsl:param name="extreme"/> <xsl:choose> <xsl:when test="$mapValue = ?average?"> <xsl:value-of select="average/value [@name = ?originalNormalized?]/@value"/>  </xsl:when> <xsl:when test="$mapValue = ?extreme?"> <xsl:if test="$extreme = ?outlier?"> <xsl:if test="$original &lt; minOutliers/ value[@name = ?original?]/@value"> <xsl:value-of select="minOutliers/value[ @name = ?originalNormalized?]/@value"/>  </xsl:if> <xsl:if test="$original &gt; maxOutliers/ value[@name = ?original?]/@value"> <xsl:value-of select="maxOutliers/value[ @name = ?originalNormalized?]/@value"/>  </xsl:if> </xsl:if> <xsl:if test="$extreme = ?missing?"> <xsl:if test="$original &lt; minMissing/ value[@name = ?original?]/@value"> <xsl:value-of select="minMissing/value[ @name = ?originalNormalized?]/@value"/>  </xsl:if> <xsl:if test="$original &gt; maxMissing/ value[@name = ?original?]/@value"> <xsl:value-of select="maxMissing/value[ @name = ?originalNormalized?]/@value"/>  </xsl:if> </xsl:if>  </xsl:when> </xsl:choose>  </xsl:template>  The next template will extract the correct codification based on the type of the variable. For each value in the IDDP file it will find a <distinctValue> element which con- tains a <value> subelement whose name attribute is equal to ?original? and its value attribute is equal to the value pre- sented in the IDDP file. If it is a categorical variable, it will obtain the binary code through the <value> element whose name attribute value is equal to ?binaryCode?.

If it is a data variable, it will obtain the normalized value of the number of days since a specific data, in this case, 01/01/1970.

If the variable is numeric, it will use the template pre- sented before to check if the value is an outlier or missing and, if so, discover the proper value to substitute it.

<xsl:template match="distinctValue"> <xsl:param name="value"/> <xsl:param name="variable"/> <xsl:if test="child::value[@name = ?original? and @value = $value]">  <xsl:choose> <xsl:when test="$variable/@type = ?categoric?">  <xsl:value-of select="child::value[ @name = ?binaryCode?]/@value"/>  </xsl:when> <xsl:when test="$variable/@type = ?date?">  <xsl:value-of select="child::value[ @name = ?dayCountNormalized?]/@value"/>  </xsl:when> <xsl:when test="$variable/@type = ?numeric?">  <xsl:if test="@isMissing = ?true?"> <xsl:variable name="extreme"> missing </xsl:variable> <xsl:apply-templates select="$variable/  outliersAndMissingIntervals"> <xsl:with-param name="mapValue" select="$ variable/transformationDirectives/  missingMapValue"/> <xsl:with-param name="extreme"  select="$extreme"/> <xsl:with-param name="original" select="  child::value[@name = ?similar?]/@value"/> </xsl:apply-templates>  </xsl:if> <xsl:if test="@isMissing != ?true?"> <xsl:if test="@isOutlier = ?true?">  <xsl:variable name="extreme"> outlier </xsl:variable> <xsl:apply-templates select="$  variable/outliersAndMissingIntervals"> <xsl:with-param name="mapValue" select="$variable/transformationDirectives/  outlierMapValue"/> <xsl:with-param name="extreme" select="$extreme"/>  <xsl:with-param name="original" select="child::value[@name = ?similar?]/  @value"/> </xsl:apply-templates>  </xsl:if> <xsl:if test="@isOutlier != ?true?">  <xsl:value-of select="child::value[ @name = ?similarNormalized?]/@value"/>  </xsl:if> </xsl:if>  </xsl:when> </xsl:choose> </xsl:if>  </xsl:template>  Here is the big advantage of the proposed framework: based on a single DPDM document it is possible to asso- ciate many different XSLT files to convert raw data into transformed data ready to be used by data mining algo- rithms. Each XSLT file needs to be defined only once and it only retrieves the information that is necessary to its spe- cific data mining algorithm. There can be one XSLT file for artificial neural networks, other for decision trees, and so on. All without any change in the DPDM document. When a XSLT is created for a specific data mining algorithm, it can be used to get the information to that algorithm in any DPDM file.

5.4. IDDM File  The document that follows is the output file from the transformation process using the XSLT file over the IDDP and DPDM files.

It is easy to see that it is very similar to the IDDP file.

All its variable contents are coded values ready to be used by an artificial neural network.

<?xml version="1.0" encoding="ISO-8859-1"?> <IDDM> <variables> <variable id="000" name="AGE"> <value id="00000">0.2</value> <value id="00001">0.5</value> <value id="00002">0.4</value> <value id="00003">0.8</value> <value id="00004">0.2</value>  </variable> <variable id="001" name="MARITAL STATUS"> <value id="00000">001</value> <value id="00001">100</value> <value id="00002">001</value> <value id="00003">010</value> <value id="00004">001</value>  </variable> <variable id="002" name="CONTRACT DATE"> <value id="00000">1</value> <value id="00001">0.877524216</value> <value id="00002">0.970858644</value> <value id="00003">0.917172878</value> <value id="00004">0.945247086</value>  </variable> <variable id="003" name="GOOD/BAD"> <value id="00000">1</value> <value id="00001">0</value> <value id="00002">0</value> <value id="00003">1</value> <value id="00004">0</value>  </variable> </variables>  </IDDM>  6. Conclusion  In this paper we proposed an unified format for the stan- dardization and documentation of the preprocessing phase in the KDD process. This format, based on the DMPML specification, does not depend on a specific data mining software system and can be applied to projects of data preparation regardless of its level of difficulty.

Besides the standardization and good documentation, other relevant benefits offered by DMPML are:  ? No need to use a relational database to store informa- tion generated by the preprocessing sub phases (selec- tion, cleaning, enrichment and codification) because data will be stored in XML files. So, we are not locked in a database that may not offer tools to perform neces- sary tasks to the preprocessing sub phases or may offer tools that do not perform the tasks satisfactorily;  ? No need to implement special software to transform raw data into data ready to be applied in a specific data mining algorithm. This eliminates the necessity     to develop proprietary code, resulting in less time of development and correction of possible bugs that may appear in the implementation of this code. The data transformation tasks will be done by XML parsers, making it possible to test many parsers to verify which one satisfy the necessities of the project like efficiency, portability, etc.;  ? It is possible to create specific transformation rules (XSL) to specific data mining algorithms without re- defining the data processing project (DPDM) related to the raw data (IDDP). The XSL transformation rules are similiar. Their only difference is on the values that they will obtain based on the data mining algorithm.

? It is not necessary to create a DPDM file to each data mining algorithm. One DPDM file can contain as many codifications as needed to match different data mining algorithms;  ? Great potential of reusing XML files with prepro- cessed data (DPDM) and XSL rules of a mining project in another one that uses data with similar attributes. It is not necessarily needed to rebuild the DPDM file if the IDDP file changes. If the new data are similar to the ones of the IDDP file, their codification and infor- mation to perform the transformation are present in the DPDM file, because it is separate from the raw data.

Only if the new raw data are very different from the original ones it is needed to rebuild the DPDM file.

At this point, the DMPML is composed of:  ? A program that receives an IDDP file as input and out- puts a DPDM file with the necessary information to the data transformation. It creates the necessary codifica- tions to the neural networks algorithm, coding numeric variables (through the normalized similar value), cat- egoric variables (by the usage of a binary code) and a date type (normalizing the quantity of days counted based on a specific date), among other codifications;  ? DTD?s and XML Schemas to the three kinds of files that compose the DMPML structure so it is possible to guarantee that these files are well formed and valid;  ? A XSL transformation file to the neural networks data mining algorithm.

As future work to be added to the DMPML language are the following topics:  ? To extend the program that generates DPDM files to create more codifications to other mining algorithms, in addition to neural networks, such as decision trees, regression, genetic algorithms, etc.;  ? To test the efficiency of this program in situations where the quantity of variables and values of raw data is very high. In this case, the usage of DOM or SAX to create the file of pre-processed data and data trans- formation would be compared to identify the most ef- ficient option;  ? To change this program and make it show a list of pos- sible values that can substitute an inconsistent value and offer the possibility to the user indicate explicitly the value to be substituted, if none of the values pre- sented by the program is correct;  ? Inclusion of new elements whose interpretation is de- fined by the user, so that he/she can add data not sup- ported by DMPML;  ? Integrate the DMPML language with other XML lan- guages developed to represent the KDD process. We could use DMPML to perform the data preparation phase, send the data to the KDDML engine where the data mining process would occur and generate a PMML document with the knowledge discovered.

? Create a plugin for a XML database to integrate the DMPML into it. By doing this, the data preparation phase would happen inside the database, without the need to create a separate environment to pre-process data.

