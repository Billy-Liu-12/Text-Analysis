Random Projections for Reduced  Dimension Radar Space-Time Adaptive Processing

Abstract-Multisensor radar data is high-dimensional and suf? fers from the curse of dimensionality. For example, in radar space time adaptive processing (STAP), training data from neighboring range cells is limited since the statistical properties vary signifi? cantly over range and azimuth. This precludes implementation of the full-dimension optimal detectors, i.e. the minimum variance distortionless response (MVDR) filter. In this paper we reduce the dimension of the problem by random sampling, i.e. by projecting the data into a random d-dimensional subspace. The Johnson-Lindenstrauss theorem provides theoretical guarantees which explicitly state that the low dimensional data after random projections is only very slightly perturbed when compared to the data from the original problem in an 12 norm sense. Statistical analysis via probabilistic bounds is provided for a measure of whiteness of the random projected STAP. Random projections offers two advantages, first, it permits implementation of classical detectors in the small sample size regime. Second, it offers significant computational savings permitting possible real time solutions. Both these advantages are however at the cost of reducing the clairvoyant output SINR for radar STAP. These trade? spaces also exist for other radar multisensor frameworks, not just limited to STAP. Hence we recommend exercising some caution and restraint when applying random sampling techniques such as random projections and compressive sensing for practical radar.

Keywords-Random projections, Radar, STAp, Compressive Sensing, Probabilistic bounds, Dimensionality Reduction, Sketching, Streaming, Big Data

I. INTRODUCTION  A powerful result by Johnson and Lindenstrauss (JL) [1] states that a higher dimensional data set can be operated upon by a Lipschitz function and the resulting lower dimensional output data set has similar pairwise distances as the origi? nal data set. Random projections employs this concept, but projects the data into a random subspace spanned by d-random vectors. It is noticeable that in a vector space the number of "nearly orthogonal vectors" exponentially increase as the di? mension increases. This is exploited by the random projections principle, and it was shown that this technique preserved the pairwise distances of the data after projection via statistical expectations, with a prescribed but tolerable variance [2]-[6].

Multisensor radar data such as those from multispectral imaging, electro-optical/infrared (EOIIR), and RF sensor data suffer from the curse of dimensionality. In particular, for radar space time adaptive processing (STAP), data becomes heterogeneous within a few cells from the cell under test  This work was sponsored by US AFOSR under project 13RYIOCOR.

All views and opinions expressed here are the authors own and does not constitute endorsement from the Department of Defense.

[7]-[12]. Therefore, sufficient trammg data is unavailable to implement the optimal filter. This problem is further compounded for multistatic radar STAP since the statistics depend on the placement geometry of the transmit and receive arrays [13], [14]. Further, the STAP filter in the original higher dimension setting is computationally expensive due to the inversion of large, almost always non-sparse matrices.

In this paper, we analyze random projections for radar STAP.

This technique for STAP is appealing from two perspectives.

Firstly, it reduces the dimension of the problem thereby leading to faster computations in inverting the covariance matrices.

Second, in the reduced dimension setting, fewer samples are needed to generate representative sample covariance matrices, thereby overcoming the curse of dimensionality. These claims we make so far look quite promising, to the extent of enticing, but of course there is a trade off, and we lose in signal to interference to noise ratio (SINR) after random projections.

Reason being, the JL theorem guarantees preservation of distances but not other practical metrics like the SINR.

We also note that the restricted isometry property (RIP) in compressive sensing (CS) is subsumed by the JL theorem [15]. Furthermore, the concept of random projections via random sampling can be considered as the component which performs "sub-sampling" in the CS architecture. Therefore, with the loss of SINR due to random sampling, we advocate restraint when applying techniques such as compressive sensing and random projections to practical problems, but we are cautiously optimistic. Nonetheless when a loss of SINR is not a concern, random projections affords a significant computational cost reduction for radar STAP, but this is seldom the case in practice.

Prior Work: Researchers from theoretical computer science were the first to use random projections but their motivation was different [2]-[6]. Specifically, they wanted to speed up the algorithms by retrieving smaller representative dataset(s) rather than retrieving the entire dataset, thereby speeding up the algorithms and also preserving memory for some big data problems. Our motivation is dimensionality reduction from random projections as a solution for the scarcity of training data problem in radar STAP [16], and of course as a straight? forward computational cost reducer. Random projections is closely related, and could be considered as a predecessor to compressive sensing (CS) and the older "sketching" or "streaming" techniques (see [17]-[20]) in the computer science literature but does not deal with signal recovery or estimation as CS, but howver uses the same concept of random sam? pling. Other reduced dimension STAP algorithms exist (see [7], [8], [11] and references therein) and the popular JDL algorithm [21]. Unlike the random projections technique, these approaches make parametric assumptions, and operate with    different dimension reducing transformations for different cells under test (c.u.t.) but for the same data set and may have implications on constant false alarm rates (CFAR).

Contributions: In radar STAP, the scarcity of training data makes it difficult to obtain a full rank sample covariance matrix, which is desired for implementing the MVDR detector.

By using random projections, we reduce the dimensionality of the training data, thereby reducing the number of samples needed to estimate the reduced-dimension covariance matrix.

We use a modified whiteness measure to determine the efficacy of random projections, and derive probabilistic upper and lower bounds on the whiteness test. The trade-off between dimen? sionality reduction and SINR performance is demonstrated via simulations. Our simulations demonstrate that the loss of SINR is approximately identical, and across the board for various choices of the sampling distributions.



II. MODEL  The STAP model is well known and is described succinctly, as follows. We consider an airborne radar looking at the ground below. The radar consists of M calibrated sensor elements and transmits a burst of N pulses. The objective is to test for the presence or absence of a target. AssumiRP narrowband operation and a receive filter vector W E <eM , the hypotheses may be formulated as  'tio :y =c+n+i 'til :y =exx+c+n+i  (1)  where y E <e M N is the space time snapshot measurement, x E <eM N is the deterministic and known spatio-temporal response of the target, and ex is a complex amplitude. The clutter, noise and interference responses are the vectors, c,n, i, respectively, assumed each to be zero mean, and mutually independent in the statistical sense [16]. Assume that the random vectors in (1) could be modeled with a covariance matrix, denoted as R E <eMNxMN = Rc + Rn + Rj, where each matrix is the respective covariance of the clutter, noise and interference. Then, the clairvoyant SINR (assumes knowledge of covariance matrix), denoted as (SINRc) is,  lexwHxl2 -1 SINRc = HR  ' WO =R x (2a) W W SINR? =SINRc(wo) = lexI2xHR-1x. (2b)  where in (2a), the (optimum) weight vector, Wo maximizes the SINR. As evident from (1) and (2a), the weight vector is essentially a whitening match filter. In (2b), the clairvoyant SINR evaluated at the optimum weight vector in (2a) is denoted as SINR?.

If the all the distributions are assumed complex Gaussian, then the weight vector in (2a) is also optimum from the likelihood ratio test sense.

A. Computational complexity and a "small" data problem  From (1), (2a), we see that for every spatio-temporal cell, the weight vector involves inverting matrices. The complexity is then O(M3 N3) making real time implementation of STAP computationally prohibitive. Furthermore, we note that in practical cases, only partial knowledge of R is available, and hence the sample covariance matrix, :R, is used as surrogate instead of R in the STAP implementation. The sample covari? ance matrix is constructed in the usual maximum likelihood  way, by averaging (outer products of ) responses from nearby cells in the vicinity of the cell under test (c.u.t.). Unfortunately, the radar scene is homogeneous only for a few cells near the C.U.t. [16]. Therefore, we do not have the enough samples to form a sufficiently representative sample covariance matrix, let alone invert it! This is a well known problem in radar STAP but is also endemic for other sensing modalities, not just radar.

Another SINR metric useful when dealing with the sample covariance matrix is the normalized SINR, denoted as  xH:R,-lx SINRn = A A ? (3)  (xHR -1 RR -Ix) IxHR -lxl  The normalized SINR is always between (0,1] and measures a loss of performance of the system when using the sample matrix instead of the true covariance matrix. When the sample matrix is not invertible, the simplest practical solution is diag? onal loading and will be explored in more detail in Section IV.

Motivation: Assume that we have L :s; M N samples to construct the sample covariance matrix. To motivate a solution to our problem, recall the 10hnson-Lindenstrauss theorem.

Theorem 1. (Johnson Lindenstrauss (JL) [1], [22 j, [23]) For some 0 < Eo < 1, and for any set Xc IRMN of L:S; MN data points in IRM N, there exists a Lipschitz mapping, fe): IR M N -+ IRd such that for all u,v E X  (I-Eo)llu-vI12:s; Ilf(u)-f(v)112:s; (I+Eo)llu-vI12  where d> i'Yln( (L? l, " is a universal constant, and g( Eo) is an 9 Eo  arbitrary function of Eo, and f (.) may be found in randomized polynomial time.

We note that the reduced dimension, d, is independent of the original dimension of the problem, i.e. M N, but is dependent only on the L samples in IR M N (or <eM N). For specific types of g(Eo), see [1], [22], [23], and references therein. The lL the? orem is unique in that it states that by projecting the data onto a lower dimension, the pairwise distances are only slightly per? turbed. The original theorem was proved from the real domain but can be readily extended to the complex domain as well.

The idea then is to reduce the dimension of the spatio? temporal measurement vector y in (1) to a lower dimension d < < M N, with the lL theorem providing guarantees that the newly projected data is representative of its original higher dimension counterpart. In other words, if the C.U.t. has no tar? get, then its lower dimension projection also is similar ( in a l  2 norm sense) and is comprised of clutter, noise and interference like components. Likewise, if the C.U.t. has a target embedded in clutter, noise and interference, its lower dimension counter? part is also representative of the same (in a l2 norm sense). The reduced dimension problem's covariance is now d x d and if d < L, the lower dimension covariance is invertible. The in? verse matrix computation re?uired for the new weight vector is also reduced from O(M3N ) to O(d3) flops with d? MN.

A numbers game: Consider 10 sensors and 32 pulses, the inverse covariance matrix requires 3203 = 32,768,000 = 3.27 million flops, and atleast 320 samples from neighboring cells to compute a full rank sample covariance matrix. Let us reduce the dimension by half (d = M N /2), then the inverse requires 1603 = 409,6000 = 409.6 thousand flops with atleast 160 samples from neighboring cells to compute a full rank sample covariance matrix. Indeed, these numbers sound promising, but of course, we will see subsequently that a trade-off emerges.

B. Random Projections  The JL theorem is general, and does not specify the type of the lower dimension Lipschitz mapping, fO. Furthermore, the fidelity constant Eo is difficult to ascertain readily, and depends on d, the chosen dimension via a function g(Eo). A solution is then to project the data into a lower d-dimensional subspace spanned by d random vectors. It was shown that, with some prescribed variance, on an average the pairwise distances between the data points after projection are preserved [2].

Consider such a lower dimension transformation denoted by matrix T E <cM Nxd, the new null hypothesis is then, Ho : Y t = TH ( Ct + nt + it) and the new alternate hypothesis is then, HI : Yt =TH (o:x+c+ n+ i). Likewise, the new optimum weight vector (Wt E <cd), new covariance matrix (Rt E <cdxd), new clairvoyant SINR at this new optimum weight vector (SINR?t) and new normalized SINR (SINR"t) are expressed as:  Wt =Rt1Xt,Rt =THRT,SINR?t = lo:I2XfRt lXt (4a) H ' -1 Xt Rt Xt SINRnt =  H A 1 A 1 H 1 (4b)  (Xt Rt RtRt Xt)lxt Rt Xtl where as usual, in the lower dimension setting, Xt = TH x. We note that as long as rank(T) = d, it may be shown readily that the resulting covariance matrix is also full rank. The number of samples L is unchanged with this transformation and when d<S:.L the resulting sample matrix Rt=THRT is invertible.

Conditional Statistics The appeal of using random projec? tions is also in that the matrix T is held constant for the entire data set including all the cells under test. Hence, as is assumed usually, invoking that the distributions in (1) are complex normal, let f(YIHo) =CN(O,R) and f(yIH1) =CN(o:x,R).

Then, under the random projections paradigm, we work with the conditional distributions in the new reduced dimension hypotheses (since T is constant for all cells under test), as f(YIHo, T) = CN(O,Rt) and f(YtIH1, T) = CN(o:xt, Rt).

Indeed we are also interested in the behavior of random projections for all other instances of T, and this analysis is not tractable in closed from but can be shown via Monte Carlo simulation as shown in Section IV.

1) Distributions: In general we want the columns of : I T=  y'd [t1,t2,???,td]  to be i.i.d, zero mean and have constant variance in order to guarantee the pairwise norm preservation in the data on the average.

Non-Sparse (Normal): Typically, ti,i=I,2, ... ,d are chosen from CN(O,I) or when T is purely real, ti,i = 1,2, ... ,d are sampled from N(O,I) , where I is the identity matrix. When the columns of T are normal (real or complex) the random projections is non-sparse. Other continuous distribution may also be used but the theoretical analysis becomes complicated.

Sparse: Other sparse versions of random projections ex? ists, and for T complex, the real and complex elements of ti, i = I, 2, ... , d are sampled independently from the discrete distribution given below  with prob.

with prob.

with prob.

Ij2p  l-ljp.

Ij2p  (5)  When T is purely real, then elements of the columns are sampled from a discrete distribution similar to (5) but with ,jf5 instead of -/Pfi.

(Achiloptas [6]) For p = I or p = 3 in (5), resulting in a sparse random projection (more zeros, leading to sampling fewer data-points).

( Lihas) [2]) For p= y'd, or p=djln(d) in (5), with p > > 3 in both cases, these distributions result in very sparse random projections [2]. We do not recommend using p = dj In(d) because several of our Monte Carlo instantiations, resulted in non full rank covariance matrices for particular values of d. The choice of p = dj ln(d) is aggressive and is motivated from the exponential tail error bounds (Chernoff, Hoefdding etc.) of several distributions.

Non-sparse Discrete (Bernoulli): Another simple but of? tentimes overlooked distribution in the random projections literature is the Bernoulli with equi-probable symbols ?l and scaled appropriately so that each element in T has zero mean and unit variance. These distributions are used in Section IV to generate elements of T for both complex and real cases.



III. STATISTICAL ANALYSIS  Consider the following which proves useful in the sequel.

Let x rv N(O,cri),y rv N(O,O"?) be two independent normal random variables. Then the probability density function of w = xy is (see also [24], [25]).

(6)  where K(-) 0 is the modified Bessel function of the second kind. This may be shown easily since the Joint density f( w,x) (w,x) from the transformation of random variables using the Jacobian, is f(w,x) (w,x) = 1;1 fx(x)fy( w jx). Then, f( w) = J  oo f ( )d Th t ? J  oo exp( _x2 /2"f _w2 /2x2"D d (w,x) w, x x. a IS, X7rV"'''2 x, -00 0 substituting x2 = t in this integral and using the fact that 00 Jexp(-?t-(3/t)dt=2Ko(2JM),?{,B} >O,?{r} >0 [26, ,pg. o 00 2 2 2 384, eq. 3.471.9], we have f(w)=Jexp(-?!crg2t"2)dt= o _1_KO (v'W2). Now since Ko(-w) E <C, for all nala2 ala2 W > 0, w E R, and a probability density function cannot assume complex values, we consider only the positive values, i.e. v9 = I w I which gives the desired result.

The following lemma also proves useful in the analysis.

Lemma 1. Let x rv N(O, O"?I)y rv N(O, O"?I), be two independent vectors in Rd. Then, the probability density function of IxTYI is 2f(z),where :  f(z) = I d Izl  ?-l Wo I-d (M) (7a) (0"10"2)d/222 f(dj2) , 2 0"10"2 M d+l ) = v 2 d+l d Izl 2 K d-l (_lz_1 (7b)  J1f(0"10"2)-2 22 f(dj2) 2 0"10"2 and Wa,b(X) is the WhittakerW function [27].

d Proof" Rewrite xTy= LZi =Z, where zi,i=I,2, ... ,d i=l    are independent, and are distributed according to (6). The characteristic function of the pdf in (6), IP(w), is its Fourier transform, since (6) is an even function, the Fourier transform is simply the Fourier cosine transform. Then this implies  j(Zi) r=F? IPi(W), where F, F-1, Fe denote the Fourier, F-l  inverse Fourier, and Fourier cosine transforms respectively. It 00 may now be shown that, IPi(W) = J j(zi)exp(-ziW)dzi =  - 00 2 J -I- Ko (?) cos(wzi)dzi =  J I  where o ?Tal a2 al a2 al a2 w2 +-:h-0"10"2 00  in deriving this, we used J Ko(azi) cos(wzi)dzi o  2?,?{a} > 0 [28, pg. 49, 1.12.40]. Now the probability w +a density function, j(z) C IT IPi(W). Using this and  i=l 00 the fact that J (fJ + iw) - d/2 (fJ - iw) - d/2 exp( iwz )dz =  - 00 _d/2 Iz l  d /2-1 ( )  27r(2fJ) r(d/2) Wo, ";  d 2fJlzi ,d > l,?{fJ} > 0 [26, pg.

366, eq. 3.384.96], we obtain (7a). To obtain (7b), we use the fact that Wo,v (z) = J2z/7r Kv(z) [27]. ? A. Measure of whiteness of random projections STAP  Practically, one measure of the STAP performance is by a whiteness test, or essentially how close RtlRt is to the identity matrix, captured by the scalar metric III- RtIRtIIF /d. However, this is not easy to analyze for random projections STAP. We choose a natural surrogate, IIRt - RtIIF /d , which naturally arises as a term in a tight upper bound on III - Rt1RtIIF /d . Our objective next is to probabilistic ally bound II Rt - Rt II F / d precisely in the theorem next but before that, let us define,  ?R:=Rt-Rt.

We note that ?R is not necessarily positive semi-definite but is indefinite.

Theorem 2 . . For T = ? [tl' t2, ' " , td J E mMNxd, where ti rv N(O, I), i = I, 2, ... , d and ti, tj are statistically independent. Let Pr{ Iiti 112 ? 17d = E1 and Pr{lt[ tj I ? 172} = E2,( i,j) = 1,2, ... d,i 01 j, E2::; 1- (1-Edd, and AmaxO denotes the maximum eigenvalue.

1) Pr{tf ?Rti ::;Amax(?R)17d is at least (I-Ed and Pr{lt[ ?Rtj I ::; Amax(I?RI)1]2} is at least (1-E2)  2) P {IIRt- RtIIF < Vd.\max(6.R)1/1 +d(d-1).\max(I6.RI)1/2} . r d _ d2 IS at most (l-Edd and at least max (0,d(1-Ed+?(1-E2)-(d2+d-2) /2)  3) Pr{ IIRt-d RtIIF > jd.\max(6.R)1/1 +??d-1).\max(I6.RI)1/2}  is at most E? and at least max ( O,dEI + d(d;-l) E2 -(d2 +d-2) /2) .

Proof We first note that IIRt-d RtIIF = IITT ?RTIIF/d.

Now TT ?RT is a matrix whose off diagonal elements are t[ ?Rtj, (i,j) = 1,2, ... ,d, i 01 j and whose diagonal elements are t[ ?Rti' i = 1,2, ... , d. Now since ?R is Hermitian, t[ ?Rti ::; Amax(?R)lltiI12 i.e. with probability 1. But, ti  is a random vector, with Pr{ II ti 112 ? 17d = E I and hence Pr{tf ?Rti::; Amax(?R)17d ? (l-EI)' Now we can readily show that It[ ?Rtjl = ITr{?Rtitnl ::; Amax(I?RI)lt[tjl, i.e. with probability 1. From similar arguments, it is readily observed that Pr{lt[ ?Rtj I ::; Amax(I?RI)172} ? (1-E2)'  To prove the remaining, consider the events partitioned as follows: ?h = nEi,i = l,2, ... ,d and ?h = nEij,(i,j) = 1,2, ... , d, i 01 j where Ei: tf ?Rti ::; Amax(?R)171 and Eij : Itt ?Rtj I ::; AmaX(I?RI)172' We note that since ti,tj,V,i,j 01 i are independent, the events Ei,i = l,2, ... ,d are also independent. Using this fact and the Boole Frechet inequalities [29], we have statement 2 of the theorem. To prove statement 3, we apply the same technique but instead on the complement events, Ei,Eij. ?  The constants, E1 is derived from the chi-square distribution, with d degrees of freedom, and E2 can be computed from the distribution corresponding to the density given in (7a) or from the density in (7b). If EI, E2 are chosen appropriately, then the results in Th. 2 provides precise bounds. For example, E1 = Ie - 06,E2 = Ie - 06,d = 100, then (1-E1)d = 0.999 and max(O,d(l-Ed + d(d;-l) (1-E2) -(d2+d-2) /2) =0.95

IV. SIMULATIONS  Before we present the radar simulations, we first validate our analysis of Lemma 1 numerically.

A. Validating Lemma 1 We first validate the theoretical results in Lemma 1 for  d = 40,200. The histograms are shown in Fig. 1 with the corresponding density function shown in red, 10,000 Monte Carlo trials were used to generate these histograms.

B. Radar Simulations  Next, we present the radar simulations. A linear array geometry is assumed, with M = 10 sensors transmitting N = 16 pulses. The data is matched filtered and the random transformation is applied for a particular cell under test. In the subsequent simulations we vary d with 0 < d::; M N. The covariance model is similar to that used in [13], [14], and is therefore known. Representative data from neighboring cells is then generated from this covariance matrix class by using the Cholesky decomposition and the multivariate standard normal distribution.

In the next simulation in Fig. 2, we use the sample covariance matrix instead of the actual covariance matrix. We compare the normalized SINR for the original full dimensional problem as in (3) and normalized SINR for the dimension reduced problem as in (4b). The number of samples used to generate the data is fixed as L = 2d. Similar number of samples are also used for the original STAP problem (full dimensional) and compared with the dimension reduced random projection technique. Complex normal distributions were used in generating the transformation matrices, and were identical for the 500 Monte Carlo trials but were different for different d's. The mean normalized SINR is show in Fig. 2. For the original problem, when L::; 160,L = 2d the resulting sample covariance matrix is rank deficient, and diagonal loading was used. When L ? 160 i.e. d ? 80 the sample matrices are full rank, and hence diagonal loading was not used. Results with a load factor equal to 50 times the minimum eigenvalue    of the true covariance matrix, and 100 times the mInllnUm eigenvalue are shown in Fig. 2. We note that in practice, the true covariance is never known, so the load factor has to be obtained from other techniques, for example optimizing some function of the SINR with the sample covariance matrix instead, indeed at the cost of increased computations.

We see that random projections performs well, and is close to the RMB predicted rule (black dashed line in Fig. 2) [10], [16], when sample covariance matrices are used. A significant computational reduction is afforded by the random projections.

We note that the normalized SINR from random projection is also slightly higher than the RMB prediction for small subspace dimension, d. This is a Monte Carlo effect, and we observed very high variance of the SINR for small dimensions of random projections. From this phenomena, it is erroneous to conclude that working in the lower dimension subspaces leads to gains in normalized SINR. On the contrary however, com? paring the clairvoyant SINR, (2a), of the full dimensional prob? lem to the clairvoyant SINR of the reduced dimension problem, (4a), and as seen in Fig. 3, we see that the overall clairvoyant SINR of the dimension reduced problem also decreases when d decreases. The choice of the distribution used for generating T is irrelevant here, as seen in Fig. 3. For generating the results in Fig. 3, we used 500 Monte-Carlo trials, and for each trial we used different random projection matrices from various real and complex families of distributions. The clairvoyant SINR for the reduced dimension problem as in (4a) is therefore also random since T varies for each trial. The mean and variance of the clairvoyant SINR of the dimension reduced problem is de? picted in Fig. 3. We observe from Fig. 3, the mean normalized SINR is nearly identical for both real distributions and complex distributions. The variance however is slightly different, but nonetheless follows similar trends among the distributions.

(a) (b)  Fig. 1: Probability density function of IxT YI, for xrvN(O,I),yrvN(O,I). For (a) d=40,(b) d=200.



V. CONCLUSIONS  We proposed dimensionality reduction in radar STAP using random projections leading to, firstly, straightforward computa? tional cost reductions, and secondly permitting implementation of the optimal MVDR detectors in sample starved regimes. It was analytically demonstrated that the advantage of random projections is at the expense of a loss of SINR. Probabilistic upper and lower bounds of the whiteness measure after random projections were analytically derived for radar STAP.

