PISAR: Progressive Image Search And Recommendation System by Auto-interpretation and

Abstract?Many image search engines nowadays still struggle with the semantic gap between low level image features and high level image concepts. Some solutions are proposed to bridge the gap by using surrounding texts of images or by adding tags on images by single user. However, they can only provide obscure or limited information about images. Another problem is that users may not know exactly what they want when they search for images. In this work, we proposed a Progressive Image Search And Recommendation system, named as PISAR, to reduce the semantic gap by incorporating the auto- interpretation and user behavior. PISAR is able to progressively improve the interpretation of images and provide a list of recommendation. The evaluation results show that with the help of auto-interpretation and user behavior, the performance of search results and recommendation results can be progressively improved.

Index Terms?Image, search, recommendation, auto- interpretation

I. INTRODUCTION  In recent years, more and more multimedia applications are proposed. Among these, the image search and recommendation plays a significant role. Many famous software organizations have provided their own image search engines. However, they still struggle with the semantic gap between low level image features and high level image concepts. The best way to describe concepts in images is to explain every image by human beings. Yet, this is not feasible in the real world. Some solutions are proposed to bridge the gap by using surrounding texts of images. Others try to include tags on images by a single user. Although these methods may interpret images to a certain extent, the perceptions described by surrounding texts and few tags are far away from whole concepts of images.

Surrounding texts can only provide obscure information about images. It is difficult to extract true ideas of images from lots of noisy terms in surrounding texts. On the other hand, adding tags on images by a single user may limit the reveal of information. It is nearly impossible to describe an image thoroughly by few people. In addition, the information added into images will not evolve at all. The ability of the image search function is thus fixed.

To remedy these, we conduct the image search problem by incorporating valuable information from user behavior. When users search for some images, we model users? knowledge by auto-interpretation and search transactions. In this work, we propose a PISAR system, which stands for Progressive Image Search And Recommendation system. PISAR creates an interface to search images in our database acquired from Flickr [3]. Each image contains some tags describing the concepts of itself. However, as in Flickr, few tags by a single user cannot interpret whole concepts of a certain image. When users browse images or select images related to query terms, PISAR will automatically add new tags in these images. If a certain tag has already been added in the image, PISAR will reenforce the importance of that tag by increasing the frequency of the tag. With user feedbacks and different levels of importance, PISAR is able to progressively improve the interpretation of images and thus refine search results.

Another problem is that users may not exactly know what they really want when they search for some images. In the scenario of the image search problem, users may be uncertain about what query terms should be applied to retrieve images with desired concepts. In addition, users are prone to provide less query terms. It is difficult for search engines to precisely catch the concepts which users really want. Moreover, when users browse some targeted images, they are usually interested in more relevant images related to targeted ones. To meet the demand of uncertainty and other interests, we develop recommendation functions in PISAR. PISAR provides a list of recommendations based on tag-tag relationships and image- image relationships. For any query term and any image, PISAR recommends the most relevant tags and images in our database. The tag-tag relationship and image-image relation- ship is calculated by co-occurrence matrix. Meanwhile, PISAR records search logs to improve recommendation results. By incorporating the association rule technique [1], PISAR is able to find hidden relationships of tags and images based on user behavior.

The evaluation results show that with the help of auto- interpretation and user behavior, the self-improving feature can     Fig. 1. Search Result Layout  Fig. 2. Single Image Layout  be well applied to the search problem and the recommendation problem. The search results and recommendation results can be progressively improved.

The rest of this paper is organized as follows. Section II introduces the system architecture and advantageous features of PISAR. The way how PISAR conducts search problems and recommendation problems are explained in Section III and Section IV. Section V presents the related works and Section VI shows evaluation results. Finally, conclusions are given in Section VII.



II. PISAR SYSTEM  PISAR can not only provide the image search function, but also return valuable recommendations for tags and images to users. Section II.A shows the system architecture of PISAR.

Two merits of PISAR, auto-interpretation and user behavior are introduced in Section II.B and Section II.C.

A. System Architecture  Figure 1 shows the search result interface of PISAR. At the top of search window, users can input one or more keywords they want as query terms. When users click on the search button, PISAR will provide initial search results based on tags of images which are related to query terms. Then, users can select those images which they think are relevant to the search concepts. When users click on the ?Re-search? button, PISAR will explore similar images in the database by these positive examples. Meanwhile, for any query term, PISAR recommends the most relevant tags according to the tag-tag relationship in the database and provides a list of recommendation tags for users? interests on the right hand side. In addition, users can browse a single image as shown in Figure 2. For any single image, PISAR shows all tags belonging to this image with their frequencies at the bottom of the image. There is an input field for users to add any new tags related to this image.

Moreover, the most relevant images according to image-image the relationship is shown on the right hand side. As time goes by, the database will be progressively updated to provide better search and recommendation results.

B. Auto-interpretation  It is noted that the most precise high-level concepts of images are obtained from human beings. However, to manually add detailed descriptions or complete keywords into each image is not viable. Unlike Flickr system, each image is tagged by only a single user and the frequency of each tag in an image is always 1. To improve the understanding of images, we design an auto-interpretation mechanism to extract more hidden concepts of images from multiple users? knowledge.

When users search images by some query terms, these key- words will be added into targeted images automatically. If a tag is not shown in an image, the tag will be added into this image. If a tag is already in an image, the frequency of this tag will be increased. The auto-interpretation mechanism proceeds in following two circumstances.

(1) When a user sends query terms to PISAR, a list of relevant images are retrieved and shown to the user. If the user clicks one of images to view this single image, the image is regarded as highly related to the query terms. Therefore, these query terms will be added into this image as new tags.

(2) In the re-search phase, the user provides a set of positive examples to retrieve better results. These selected images are considered as highly related to query terms. Therefore, these query terms are added into each of selected images as well.

In addition to the auto-interpretation mechanism, we still allow users to manually add tags into any image in the single image page. These voluntary feedbacks also help to enhance the performance of PISAR.

The merit of this auto-interpretation mechanism is that high- level concepts of images can be collected progressively by multiple users? knowledge. By this collaborative method, the frequencies of relatively relevant tags will increase and become remarkable compared to other irrelevant tags. The undesirable effect of small amount of noise that are added into the system will vanish quickly. Therefore, the performance of PISAR will be upgraded as more and more users utilize the system.

C. User Behavior  Users clicking behavior not only helps the auto- interpretation mechanism but also helps to refine relationships between tags and relationships between images. Initially, a session is started whenever a user applies new query terms in the system. The session continues until new query terms are re-applied. In such a context, all tags clicked by a user in a session are viewed as related tags and they are recorded as a tag transaction. Similarly, all images clicked by a user in a session are transformed as an image transaction. PISAR collects these transactions automatically while users utilize the system. The technique of association rule mining [1] is exploited to find hidden relationships of tags and images from these transactions. The association rule mining is to ?discover the important associations among items such that the presence of some items in a transaction will imply the presence of other items in the same transaction [6].? Therefore, by analyzing tag transactions with the association rule mining algorithm, PISAR can find a list of related tags whenever a certain tag is queried.

Fig. 3. The Process Flow of Image Search  Similarly, the related images are obtained by analyzing image transactions.



III. SEARCH PROBLEMS  There are two types of image search problems in PISAR.

When users input some query terms, the first problem is to obtain relevant images which are related to users? intentions.

This kind of problem is similar to text-based image retrieval (TBIR) problem except we use tags information. When user choose some positive examples returned in the first query, the second problem is to find relative images again. We model the second problem as a classification problem by the fusion of low-level features and high-level concepts, which is usually known as a Multi-Example Contents-Based Retrieval (ME- CBR) problem [9].

A. Search by Query Terms  When users input some query terms, PISAR leverages the term frequency (TF) concept to indicate the important score of the query terms for all images in the database. Meanwhile, we also calculate the document frequency (DF) of the query terms to diminish the influence of general terms. Based on the TF-IDF technology [5], the images are ranked for each query tag and the results are fused with equal weights among the query tags. Those images which are the most relevant to the query are returned by the system. Each result page contains less than or equal to 20 relevant images as shown in Figure 1. Meanwhile, to broaden the concept diversity, we reserve at least five random images in each result page. Figure 3 shows the process flow.

B. Search by Multiple Examples  Another image search problem occurs when users select some positive examples to do re-search process. To learn the semantic meaning of the query, we leverage not only low- level features but also high-level concepts to conduct image re-search problem.

For low-level features, we use Multi-Bag Support Vector Machines (MB-SVM) [9] as our classifier. The MB-SVM approach is illustrated in Figure 4. As our goal is to partition testing image database into two fixed classes, say positive  Fig. 4. The Diagram of MB-SVM  and negative images, SVM is a useful statistical model for our application. We choose HSV color space with regional information as the feature space. With regional information, it is more likely to retrieve sub-concepts hidden in the image. To extend the idea of using SVM for the query model, we extend the positive examples selected by users by k-nearest neighbors.

We select visually closed images from the 5-nearest neighbors of positive examples. In addition, we also generate pseudo- negative examples for SVM model. The pseudo-negative ex- amples are generated automatically from the image database randomly. In order to have more realistic classifier model, we construct three SVM classifiers based on different pseudo- negative examples and use the average of these SVM models.

Moreover, the candidate testing images are obtained from the 50-nearest neighbor results, which significantly decrease the execution time of the re-search procedure.

For high-level concepts, we utilize the tag information in images and model it similar as a ME-CBR problem.

Nevertheless, we leverage the tag information instead of the visual contents in the images. This problem is defined as Multi-Example Tag-Based Retrieval (ME-TBR), which is a generalization of traditional text-based image retrieval (TBIR).

The procedures of proposed ME-TBR are as follows. At first, we choose all the tags in the selected positive examples as high-level concepts. However, we can not regard all the tags in the images as salient. Instead, the tags which appear in at least one-half of selected images are considered as salient tags.

This is different from the conventional TBIR, which regards all the input query tags provided by users as meaningful. Then, the TBIR technique is applied to these salient tags based on TF-IDF values. Finally, the TF-IDF values of tags in images are fused to represent the final scores of extracted results. We regard the values of TF of tags as the importance of high-level concepts. This is different from the conventional TBIR, which always refers to query tags as equally important.

The final re-search result is the weighted combination of MB-SVM list and ME-TBR list.

(b) Image-tag matrix Mi_t  T T T T  I  I  I  b b b  b b b  b b  b  m  i  n1 n2  ij  1 2 j    11 12 1m  21 22 2m  :  : I n bnm  Mi_t =  ??  ?  ?  ?  : : :  (a) Tag-image matrix Mt_i  I I I I  T  T  T  a a a  a a a  a a  a  n  i  m1 m2  ij  1 2 j    11 12 1n  21 22 2n  :  : T m amn  Mt_i =  ??  ?  ?  ?  : : :  Fig. 5. Tag-image matrix Mt i and image-tag matrix Mi t

IV. RECOMMENDATION PROBLEMS The recommendation functions consist of tag recommen-  dation and image recommendation. The tag recommendation problem is defined as: ?Given a set of tags, Q, find a list of ranked tags which are most relevant to the set of tags Q.? The image recommendation problem is defined as: ?Given an image, i, find a list of ranked images Ri which are most relevant to the image i.?  The problem is that how to determine the degree of rel- evance between tags and between images. We integrate two kinds of information to develop recommendation functions in PISAR. They are the association matrix from co-occurrence relationships and association rules from user behavior. The details are presented as follows.

A. Association Matrix  Based on the co-occurrence relationship of tags on images, we construct a tag association matrix to represent the degree of relevance between tags. The primary idea is that if two tags have a high probability to appear in the same images, they have a high degree of relevance. Initially, a tag-image matrix Mt i, as shown in Figure 5(a), is conducted to record the term frequency of each tag in each image. There are m distinct tags, i.e., m rows in Mt i, and n images, i.e., n columns in Mt i, in the image database. The element dij in Mt i denotes the frequency of a tag Ti appearing in image Ij . To determine the degree of relevance between tags, we multiply Mt i with its transpose matrix MTt i to obtain an m by m tag association matrix At, as shown in Figure 6(a). The element cij in At denotes the degree of relevance between tag Ti and Tj . In this way, for tag Tk, the tags with top n values in row k are the n most relevant tags to tag Tk. In addition, regarding the case of multi-tags search, we fuse the co-occurrence relationships of all query tags to obtain the most relevant results.

On the other hand, the same procedure is followed to produce an n by n image association matrix Ai, as shown in Figure 6(b), by multiplying Mi t with its transpose matrix MTi t. For image recommendation, the primary idea is that if two images have more common tags, they have a high degree of relevance.

Furthermore, as mentioned earlier, PISAR possesses the auto-interpretation mechanism. This indicates that the tag- image matrix Mt i and the image-tag matrix Mi t will be altered continually. Thus, the tag association matrix At and the image association matrix Ai are required to be updated accordingly to maintain the most up-to-date relevant relation- ship. Note that whenever a tag is added into an image, one  (a) Tag association matrix At  T T T T  T  T  T  c c c  c c c  c c  c  m  i  m1 m2  ij  1 2 j    11 12 1m  21 22 2m  :  : T m cmm  At = Mt_i Mt_iT =  ?? ?  ?  ?  : : :  (b) Image association matrix Ai  I I I I  I  I  I  d d d  d d d  d d  d  n  i  n1 n2  ij  1 2 j    11 12 1n  21 22 2n  :  : I n dnn  ?? ?  ?  ?  : : : Ai = Mi_t Mi_tT =  Fig. 6. Tag association matrix At and image association matrix Ai  corresponding row and one corresponding column of At and Ai should be updated. For instance, as shown in Figure 5(a) and Figure 6(a), if a user adds tag Ti into image Ij , aij in Mt i will be increased by one. In this case, the values in row i and column j of At have to be updated. However, to re- calculate an element requires to do the inner product of two row vectors in Mt i or Mi t. Thus, the complexity of updating each element in At and Ai is O(n) and O(m) respectively, where n is the total number of images and m is the total number of appearing tags. It is obvious that the update will be extremely time-consuming since the dimensions of Mt i and Mi t are quite large.

To resolve this problem, we propose an efficient procedure to update matrices At and Ai. The main observation is that both Mt i and Mi t are quite sparse, i.e., there are large numbers of zeros in both matrices. Therefore, instead of recording each element in large matrices Mt i and Mi t, we only retain non-zero elements in matrices. Based on this modification, the complexity of doing the inner product of two row vectors reduces to O(l1 + l2), where l1 and l2 are the lengths of two row vectors. Moreover, we observe that if tag Ti is added into image Ij , only the degree of relevance between tags which appear in image Ij will be altered. Similarly, for image relationships, only the degree of relevance between images which contain tag Ti will be altered. In light of above two observations, the update procedure of PISAR substantially reduces the complexity and is much more efficient. In addition, on purpose of reducing the server loading, we do the update scheme in batch mode since the relationships between tags and between images will not change violently.

B. Association Rules  The association rule mining from user behavior is to find hidden relationships between tags and between images from user knowledge. Let C be the population of tags or images.

For tag transactions, C is the entire set of tags. For image transactions, C is all images in the database. Given a mini- mum support threshold min sup and a minimum confidence threshold min conf , an association rule is an implication of the form A =? B whose support ? min sup and confidence ? min conf , where A ? C, B ? C, and A ? B = ?. The support of an association rule A =? B is the percentage of transactions that contain A ? B. The confidence of an association rule A =? B is the percentage of transactions containing A that also contain B.

In PISAR, an Apriori-like algorithm [1] is applied to generate association rules. It is noted that we merely take account of association rules with one-item set in the left-hand     side. For instance, the rule < a1 > =? < b1, b2, b3 > is included but the rule < a1, a2 > =? < b1, b2, b3 > is omitted.

As for the multi-tags search, we integrate rules of these tags for recommendation.

Overall, we fuse the normalized results of the association matrix and association rules with equal weight to generate recommendation lists of tags and images. Moreover, it is worth emphasizing that the update schemes of two approaches will progressively enhance the recommendation performance of PISAR.



V. RELATED WORKS  There are many outstanding works on the image retrieval, recommendation and image annotation. For the image search, Tong [12] applied SVM for image retrieval. Jeon [7] proposed an automatic image annotation and retrieval system. For both image search and recommendation, Yang [14] proposed a con- text search and recommendation system for shared consumer photos. For image annotation, Ames [11] found that users annotate their photos because tags allow them to find their own images. Feng [2] used multiple Bernoulli models for image annotation and Li [8] presented a real-time annotation method.

For the tag-tag or image-image relationship measurements, Furnas [4] attempted to identify patterns and features of tags and developed algorithmic models connecting tags with existing indexing algorithms. Wu [13] proposed a so-called Flickr distance to measure the similarity of images. Park [10] applied the vector space distance to rank the similarity of tags.

Although there are several works on image search and rec- ommendation, image annotation and similarity measurement, our proposed PISAR system is the first platform to combine all the above features together. In addition, PISAR is the first system which is able to progressively improve the search and recommendation quality by auto-interpretation and user behavior.



VI. EVALUATION RESULTS  In this section, we evaluate search and recommendation performances of PISAR by user survey. For now, the image database contains 7000 images from 14 most popular cate- gories on Flickr. They are scenery, landscape, landmark, night, street, building, river, lake, beach, mountain, forest, sunset, sunrise, and sky. We retrieved 500 image samples and their tags for each group from Flickr. All copyrights for images are remained by the image owners. We will not use them for any commercial purpose. Then, as time goes by, the tag database will be progressively updated to provide better search and recommendation results.

A. Search Results  First, we investigate the effectiveness of fusion of MB- SVM and ME-TBR from re-search results. Let ?beach? be the testing query term. Figure 7(a) indicates the testing case which selects four positive images to do re-search. All of them contain blue skies, three of them contain water in parts of images, and two of them have palm trees. Figure 7 (c) and  (d) show re-search results based on MB-SVM and ME-TBR.

We skip the random generated images in this experiment. MB- SVM leverages low-level features from HSV color histogram with regional information. Therefore, all results are similar in the HSV color domain. However, there are only 7 out of 20 images relevant to ?beach.? Many images contain buildings and blue skies but are irrelevant to the query concept. ME- TBR leverages high-level features from tag information. There are 10 out of 20 images relevant to ?beach.? However, some images still contain irrelevant tags. Finally, Figure 7(b) shows re-search results based on fusion of MB-SVM and ME-TBR.

There are more than 16 out of 20 relevant images. We can tell that results of fusion are superior to any individual approach mentioned above.

To evaluate the search performance of PISAR, we do a user survey to manually determine relevant or not for each search result. After 34 days since the system got online, we select 10 most popular query terms in our system. They are bikini, ocean, skyscraper, calm, beauty, nature, clouds, sky, face and blue. The query terms includes tangible object, abstract concept, and emotional adjective. As shown in Figure 8, the transaction number represents the number of queries applied in our system. The first point of each line, transaction number = 0, represents the search result of Flickr. At this point, images and tags in our database are the same as they in Flickr.

This is used as our baseline. The average relevance of image search result increases as the number of transactions increases.

It is noted that the improvement of relevance reaches 16.5% for top 5 and top 10 search results and 25% for top 15 and top 20 search results.

B. Recommendation Results  For tag and image recommendations, we use the same 10 most popular query terms in the previous experiment. We provide 25 tag recommendations for each set of query terms and 20 image recommendations for each image. Figure 9 and Figure 10 show average relevance of tag recommendations and image recommendations respectively. In Figure 9, the quality of top 5 tag recommendations is originally good enough.

Therefore, the improvement is marginally. Nevertheless, the relevance of top 25 tag recommendations improves signifi- cantly. The quality of all top n recommendation improves when more users utilize PISAR. The more queries users applies, the more tags are added into our database. As such, PISAR is able to interpret image concepts more precisely. The improvement of the tag recommendation ranges from 21.3% to 57.3% for top 5 to top 23. Figure 10 also shows the quality of image recommendation improves as the number of transactions increases. The improvement of the image recommendation is 34.9% for top 5 and 44.0% for top 10.



VII. CONCLUSION  In this work, we proposed a Progressive Image Search And Recommendation system. PISAR conducts the problem of reducing the semantic gap by incorporating the auto- interpretation and user behavior. PISAR is able to progressively     (c) MB-SVM (up)       (d) ME-TBR (down)  (b) Fusion of both models  (a) Positive examples  Fig. 7. Re-search Results  0.6  0.7  0.8  0.9   0 6000 12000 18000 24000 30000  A ve  ra ge  R el  ev an  ce  Number of Transactions  Search Result  Top 5 Top 10 Top 15 Top 20  Fig. 8. Image Search   0.2  0.4  0.6  0.8   0 6000 12000 18000 24000 30000  A v er  ag e  R el  ev an  ce  Number of Transactions  Tag Relevance  Top 5  Top 10  Top 15  Top 20  Top 25  Fig. 9. Tag Recommendation  0.6  0.7  0.8  0.9   0 6000 12000 18000 24000 30000  A v er  ag e  R el  ev an  ce  Number of Transactions  Image Relevance  Top 5  Top 10  Fig. 10. Image Recommendation  improve the interpretation of images and thus refine search results. PISAR also provides a list of recommendation based on tag-tag relationship and image-image relationship. The eval- uation results show that with the help of auto-interpretation and user behavior, the performance of reducing semantic gap can be well applied to the search problem and the recommendation problem. The search results and recommendation results can be progressively improved.

