A Novel Communication Strategy between PE and NI in NoC-based MPSoC

Abstract?More and more computing cores will be integrated in the future SoC to deal with the growing application requirement. For such Multi-Processor System-on-Chip (MPSoC) with high integration and large complexity, one key issue is the on-chip communication among various cores. By the way of analyzing the communication process, this paper proves that the bottleneck of inter-core communication is the interaction between computing and communicating element.

Targeting high efficiency of such interaction, a novel interaction strategy which combines a Lookup Table (LUT) mechanism and DMA mode, is proposed.  In this strategy, a hardware message address table is created in the network interface (NI), and NI can transfer the data from the network to local memory directly by looking up this table. It can remarkably reduce the usage of CPU interruption during communication, which is significant to improve the communication performance. The experimental results show that the proposed strategy brings better performance than the traditional buffer pool strategy when the message size is big, and performs the same when the message size is small.

Keywords- MPSoC; NoC; NI; Transfer Mode; LUT

I.  INTRODUCTION It has been widely accepted that Multiprocessor System-  on-Chip (MPSoC) is the most promising way to keep on exploiting the high level of integration provided by the semiconductor technology and on matching the constraints imposed by the embedded system market in terms of performance and power consumption [1]. For such Multi- Processor System-on-Chip (MPSoC) with high integration and large complexity, one key issue is the on-chip communication among various cores. Network-on-Chip (NoC) [2, 3], which is proposed as a new interconnection architecture in MPSoC, bears better scalability, reliability, compatibility, and flexibility. Nowadays, more and more NoC-based MPSoC architectures are proposed and implemented [4]. NoC-based MPSoC is composed of PEs, Router-Network and Network Interfaces (NIs) basically. As an important component, NI is in charge of data translation between routers and PEs, hiding architecture dependant aspects. Hence, the implementation of the NI is an important part to get high communication throughput and good performance.

Some previous works have discussed the design of NI.

Praveen Bhojwani[5] proposed a Core Network Interface (CNI) architecture to bound on-chip communication latency  jitter so as to provide for predictable end-to-end latency but not guarantee least possible end-to-end latency. In order to achieve low latency packet processing, Daewook Kim[6] presented a novel NI architecture that utilizes a Gray code based packet reordering methodology. Heikki Kariniemi[7] presented a Network Interface  called Micronswitch Interface (MSI) which provides mechanisms for efficient buffer management and fault-tolerant communication besides basic functionalities. However, they only consider NI as a component of NoC and focus on designing NI structure targeting specific routing strategies or higher pack/unpack efficiency.

In fact, NI is the key component that decouples processors from communication. The interaction mechanism between NI and processors will affect the communication performance and efficiency significantly. It hasn?t been well studied till now. In this paper, through evaluating and analyzing the communication process in NoC-based MPSoC, the bottleneck of communication is identified and the importance of interaction mechanism between NI and processors is proved. Based on the analysis of different interaction mechanisms, a novel approach called LookUp Table (LUT) is proposed. This mechanism can not only guarantee high transmission rate, but also induce few interacts between CPU and NI, which is significant to improve the communication performance.



II. ANALYSE OF COMMUNICATION COST  A. NoC-based MPSoC The basis of this paper stands on a NoC-based  distributed-memory MPSoC, and this MPSoC is based on a full-system co-simulation MPSoC platform which is presented in details in [8]. Fig.1 shows a system-level overview of this system. As illustrated in this figure, the hardware subsystem is composed of a matrix of PE. Each PE, which includes a CPU, physical memory and some peripherals, is connected to the network through Network Interface (NI). NI communicates with CPU through DMA mechanism, which can free up CPU cycles. The software subsystem consists of the target parallel application, message passing library and the operating system. The parallel programming model of this system is MPI [9] which is now actively developed. The parallel application with MPI format is accomplished by several parallel tasks which can communicate with each other by calling the API of MPI library. And the operating system capsules the details of  Administrator ???  Administrator ???    different hardware and provides the same interface to MPI library.

Figure 1.  A NoC-based distributed-memory MPSoC system  From the hardware point of view, NI is in charge of data translation between routers and PEs (to pack/unpack data packets), hiding architecture dependant aspects, and from the software point of view, the NI is the manager to inject messages from SW MPI API, through the NI driver, to the interconnection network. Hence, the implementation of the NI together with the NI driver is an important piece to get high communication throughput and good performance.

B. Communication process There are two kinds of inter-task communication in the  MPSoC system above: communication between tasks in the same PE which is accomplished by shared memory mechanism, and communication between tasks on different PEs which is accomplished by the message passing mechanism and is the issue we concern. Fig. 2 shows a layered view of the process of communication between tasks in different PEs.

Figure 2.  Process of inter-task communication  As illustrated in this figure, the communication process can be divided into three stages:  Sending preparation: MPI_Send() API of MPI library should be called when task i need to send a message to task j.

After finishing the work of error checking and data pretreatment, MPI_Send() will call the driver of NI which will configure the NI and start the data transfer process.

Data transfer: the data will be pushed to the FIFO of NI from the sending buffer firstly, which is called NI-SEND process. Then it will be transferred through the network to the target node, which is called NET-TRAN process. And at  last, it will be written into the receiving buffer from the FIFO of target NI, which is called NI-RECV process.

Receiving return: when the data is ready in the receiving buffer, the target task j will be woken up if it has called MPI_Recv(). And MPI_Recv() will check the message and return finally.

C. Analysze of commnication cost The cost of each communication stage is evaluated and  analyzed using PingPong benchmarks [10] under different data length and the results are shown in Fig.3.

La te  nc y(  cy cl  e) Sending  Preparation Data Transfer Recving  Return  1 packet 4 packets 16 packets   Figure 3.  Cost of each communication stage  From the results, it can be seen that the latency of sending preparation stage, where only some configuration work is done, is short. And the latency of receiving return stage, which is used to wake up the task, is constant under different data length, because this part of work is independent of the data length. But the latency of data transfer stage increases with the increasing of data length.

And it occupies a large proportion in total latency: 45% under data of 1 packet length; 59% under data of 4 packet length; 79% under data of 16 packet length. Therefore the performance and efficiency of data transfer stage will affect the communication cost and performance of MPSoC significantly.

The process of data transfer stage is further analyzed.

Experience data shows that the latency of NET-TRAN is only 40-50 cycles which is less than 1% of total latency in Fig3, even if the network is saturated [11]. In the process of NI-SEND the work of data transfer from memory to FIFO of NI is fully completed by hardware, while in the process of NI-RECV additional interaction between NI and CPU is needed to transfer the data from FIFO of NI to receive buffer.

This kind of interaction, which demands interruption as the HW/SW interface, costs huge and will also induce network congestion. Therefore NI-RECV process is the performance bottleneck of the communication, and its efficiency mainly depends on the interaction mechanism between CPU and NI.

NI-RECV process will be well studied in the following paragraph.

Administrator ???

III. DESIGN STRATEGY OF NI-RECV  A. Strategy of transfer mode When the data has been pushed into the FIFO of NI in  the destination node, the strategy of how to transfer the data to the receiving buffer in local memory is the key to improve the communication performance, and it is determined by the interaction mechanism between NI and CPU. Commonly there are two kinds of such mechanisms:  I/O access and DMA mode. In the I/O access mode, CPU reads data from FIFO of NI and then writes data to the memory directly through searching the address according to the tag of the data. Although this mechanism can guarantee low hardware and control cost, it will also induce low transfer rate and cost huge CPU resource. On the contrary, DMA can separate computation from communication on the cost of additional hardware. When in the process of NI-RECV, DMA pops the data from FIFO of NI and write them to the memory.  DMA mode can guarantee high data transfer rate without occupying additional CPU resource. Transfer strategy based on DMA mode will be discussed as follows.

In DMA mode, although CPU is freed up from communication, NI cannot identify the destination task of the data which is necessary for finding out the address of receiving buffer. When NI receives data from the network, it should trigger an interruption to request CPU to find out the destination task and configure DMA. It will not cost too much for once. However, when lots of message arrives NI at the same time, interruption will be sent frequently and CPU should configure DMA repeatedly, which will affect the communication performance significantly.

In traditional parallel computer network domain, buffer pool mechanism is proposed to solve this problem. In buffer pool mechanism, a specific memory block is allocated to NI as a software FIFO. When a message arrives at NI, NI will unpack it and write it to the buffer pool with the message tag.

After enough data accumulating in the buffer or after fixed length of time, an interrupt will be triggered and CPU will deal with the data stored in the buffer pool. According to the message tag, CPU will store each message to it receive buffer.

Buffer pool mechanism can effectively reduce the usage of interruption and improve the communication performance.

However, it is not fit for on-chip system. It is because: (1) a mass of additional memory are required by buffer pool, which result in high on-chip memory cost; (2) when a message is too long to be stored in the buffer pool wholly, the message will be divided into several short ones and CPU needs to deal with these messages one by one, which will reduce the efficiency of DMA and affect the performance.

A LTU-based NI structure combined with the DMA mode is proposed in this paper. Just like buffer pool, this mechanism can reduce the usage of interruption and guarantee high DMA efficiency with hardly any additional cost of on-chip memory.

B. Solutions of LUT-NI   Figure 4.  Solutions diagram of LUT-NI  The solution diagram proposed in this paper is shown in Fig.4. There is a LookUp Table (LUT) in the NI and one Swap Table maintained in the local memory. Each item in the LUT contains a valid bit, a tag segment and address information. The valid bit, used to indicate whether the information in this LUT item is available currently, is set to valid whenever a LUT item is create or swapped in, when a transmission is completed, it will be cleared, and then this LUT item can be re-filled. The tag segment is compared with the tag information in the packet, if matches, the address stored in the item can be brought to configure and start the DMA transfer directly. The address segment holds where the data should be stored in the local memory.

Figure 5.  Flow of swapping LUT item  When the LUT in the NI has a lack of empty item, the item not used temporarily will be swapped out to the Swap Table so as to get space in the LUT. The swap strategy is  Administrator ???    First IN First OUT in this paper and can be developed on the application characteristics. Fig.5 is the flow char of process to swap item in the LUT to the Swap Table when NI get a request to use LUT item.



IV. EVALUATION OF LUT-NI To evaluate the performance and the overhead of LUT-  NI, a testcase in which multiple messages may arrive at the receive end simultaneity, is implemented. The result is shown in Fig.6.

1 2 4 8 16 32 64 128 Length of Message(packet)  La te  nc y(  cy cl  e)  Buffer Pool LUT-NI I/O Access   Figure 6.  Communication performance of different transfer modes  The performance of three transfer modes is approximate when the data amount is small, and the latency of data transmission all increases with the increasing of data length, but the I/O access mode has a much higher growth rate than the other two modes. Compared with the Buffer pool, LUT- NI has a slower growth in latency. The communication overhead of LUT-NI is only 30% of Buffer pool mode, and according to the previous description, the benefit will increase as the length of message grows.

1 2 4 8 16 32 64 128 256 512 Length of Message(packet)  La te  nc y(  cy cl  e)  2 LUT items 3 LUT items 4 LUT items   Figure 7.  Impact of various LUT items on communication performace  LUT in the NI will bring in overhead in hardware design.

Each LUT item consist of 65 bits registers, so the number of LUT items will affect the cost of hardware subsystems. Fig.7  shows the impact on the performance when the number of LUT items changed in the scenario 5 messages arrive at the receive end at the same time. The latency of 2 LUT items increases rapidly when the message is long. Especially, the latency of 3 items is almost the same as that of 4 items, so there is adequate trade-off between the amount of LUT items and the communication performance. LUT works similarly with Cache in the CPU, so the future work is to achieve trade-off between performance and overhead through computation under determined application.



V. CONCLUSION The on-chip communication among various cores is a  key issue for MPSoC. This paper proves that the bottleneck of inter-core communication is the interaction between computing and communicating element through analyzing the communication process. And a novel strategy combining a Lookup Table (LUT) mechanism and DMA mode is proposed. LUT NI reduces the usage of CPU interruption and improves the communication performance through a hardware message address table. The proposed strategy has same performance when the message size is small, and brings better performance than the traditional buffer pool strategy when the message size is big.

REFERENCE   [1] J. Ceng et al., ?MAPS: An Integrated Framework for MPSoC Application Parallelization,? DAC 2008, Anaheim, California, USA,pp.754?759.2008  [2] L. Benini, G. De Micheli, ?Networks on Chips: A New SoC Paradigm,?Computer, Vol. 35, Iss. 1, pp: 70-78, Jan. 2002.

[3] P. Pande, C. Grecu, A. Ivanov, R. Saleh, G. De Micheli, ?Design,Synthesis and Test of Networks on Chip?, IEEE Design and Test of Computers, Vol. 22, No. 5, pp: 404-413, 2005.

[4] Tyrone Tai-On Kwok, Yu-Kwong Kwok. On the Design, Control, and Use of a Reconfigurable Heterogeneous Multi-Core System-on-a- Chip. IEEE International Symposium on Parallel and Distributed Processing, Miami, FL, pp.1-11,April 2008.

[5] Praveen Bhojwani and Rabi N. Mahapatra. ?Core Network Interface Architecture and Latency Constrained On-Chip Communication,? ISQED  2006, pp. 358-363, 2006.

[6] Daewook Kim, Manho Kim and Gerald E. Sobelman. ?NIUGAP: Low Latency Network Interface Architecture with Gray Code for Networks-on-Chip,?ISCAS 2006,pp.3902-3905.2006  [7] Heikki Kariniemi and Jari Nurmi. ?NoC Interface for Fault-Tolerant Message-Passing  Communication on Multiprocessor SoC Platform,? NORCHIP 2009,pp.1-6.2009  [8] Mingyan Yu, Junjie Song, Fangfa Fu, Siyue Sun, and Bo Liu. ?A Fast Timing-Accurate MPSoC HW/SW Co-Simulation Platform based on a Novel Synchronization Scheme,? IMECS 2010,  HongKong, Volume II, pp.1396-1400. March 2010.

[9] D. Walker, and J. Dongarra. MPI: A standard message passing interface. Supercomputer, Jan 1996  [10] Pallas MPI Benchmarks - PMB, Part MPI-1, 2000 [11] Cheng Liu, Liyi Xiao, Fangfa Fu. ?Design and analysis of on-chip  BeiJing, pp.1835-1838.Oct. 2008.

