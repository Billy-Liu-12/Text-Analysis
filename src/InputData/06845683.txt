2014 IEEE Workshop on Electronics, Computer and Applications

Abstract-With the explosive growth in data, using data mining techniques to mine association rules, and then to find  valuable information hidden in big data has become increasingly important. Various existing data mmmg  techniques often through mining frequent itemsets to derive  association rules and access to relevant knowledge, but with  the rapid arrival of the era of big data, Traditional data mining  algorithms have been unable to meet large data's analysis  needs. In view of this, this paper proposes an adaptation to the  big data mining parallel algorithms-MRPrePost. MRPrePost  is a parallel algorithm based on Hadoop platform, which  improves PrePost by way of adding a prefix pattern, and on  this basis into the parallel design ideas, making MRPrePost  algorithm can adapt to mining large data's association rnles.

Experiments show that MRPrePost algorithm is more superior  than PrePost and PFP in terms of performance, and the  stability and scalability of algorithms are better.

Keywords- big data; data mining; PrePost algorithm; parallel h.-ation; MRPrePost algorithm

I. INTRODUCTION  Big Data is the most popular word in the IT industry, which refers to the amount of data involved is huge to not pass the current mainstream software tools, to capture, manage, process, and organize datasets into useful information within a reasonable time. Big Data has 4V characteristics-Volume, Variety, Velocity, and Value. Big data mining techniques to existing data poses a severe technical challenges, effective data analysis for commercial and academic research is increasingly important.

Agrawal [1 ] in 1993 first proposed mining customer transaction database item sets problem, now FIM (frequent itemsets mining) has become an essential part of data mining. Most of the current algorithms can be grouped into two categories[2]: Apriori-like algorithm and FP-growth algorithm. Apriori[ 1 ,3,4,5] by repeatedly scanning the database to prune candidate sets. the main advantage of FP? Growth[6] algorithm is FP-Tree. When faced with large data, these two algorithms are not well adapted to. For the above algorithm, a solution is to consider only the large threshold value, the number of candidates can be reduced, but this will lead mining association rules out inaccurate due to low utilization data[7].

Based on above, this paper propose a data mmmg algorithms based on Hadoop framework parallel called MRPrePost,which is a hybrid of big data mining method. It   Yuelong Zhao?, Saiqin Long Computer Science and Engineering, South China  University of Technology Guangzhou, China  e-mail: zhaolab@126.com  combines the Dis-Eclat[8] basis PrePost[2] algorithm. By uniformly partitioning the search space way instead of the original database, which is a good solution to the problem of mining large data response time bring. In order to verifY the efficiency of the algorithm, it's run on two datasets with PrePost and PFP-Growth, conducted experiments comparing, Experimental results show that, MR-PrePost algorithm than several other mining algorithm is faster.

The next form of paper is organized as follows: The second part is related to work, and the third section details MR-PrePost algorithm, the fourth part is the comparison of experimental results and performance analysis, the fifth part of the paper summarizes

II. RELATION WORK  A. Problem Description Let I = {iv i2, ? ? ?  ,in} be a set items, a transaction is  defined as T = (tid, X) where tid is a transaction identifier and X is a set of items over I. A transaction database D is a set of transactions. Its vertical database D' is a set of pairs that the are composed of an item and the set of transactions CJ that contain the item:  Df = {(ij' Cij = {tidlij EX, (tid,X) ED})}. (1) Cij is also called the cover or tid-list ofij. The support pf  an itemset Y is the number of transactions that contain the item set. Faormally,support(Y) = l{tidlY c:;;; X, (tid, X) E D} I, or in vertical database format support(Y) = 1 n i 'EY Ci? I. An  J J item set is said to be frequent if its support is greater than a given threshold 6, which is called minimum support.

Frequency is a monotonic property w.r.t. set inclusion, meaning that if an itemset is not frequent, none of its supersets are frequent. Similarly, if an itemset is frequent, all of its subsets are frequent.

B. Correlation algorithm analysis The current main association rule mining algorithms are  Apriori, FP-growth, Eclat, MRApriori, PFP-growth, Dis? Eclat, etc.

Apriori[l] is the most classical algorithm, the main idea is to generate k+l-frequent itemsets based on k-candidate itemsets. By traversing the database to statistics candidate collection, then according to support threshold to prune candidate itemsets. The pruning strategy is that if an itemset is not frequent, its superset so is. We can be seen from the algorithm ideas, from k-itemset to k+ l-itemset and uses a    2014 IEEE Workshop on Electronics, Computer and Applications  breadth-first strategy prefix mode. The algorithm is simple, but many times to traverse the database and generate a large number of candidate sets, time and memory overhead will become a bottleneck[9, I 0]. Traversing the operation of the database is distributed to each node, while reducing the candidate set of statistical time, but increases network traffic overhead.

Comparing with Apriori, FP-growth[6] is an improved algorithm. Its main advantage is that only needs to scan the database twice, and construct a compressed data structure - FP-Tree, which reduces the search space, while no candidate set, improved memory utilization. From the algorithm thought,we can see it adopts to depth-first mode policy. However, it constructs a large number of conditions pattern tree when recursive, When faced with large amounts of data, the memory is difficult to accommodate all of the pattern tree[ll], and the tree traversal algorithm whose time complexity is higher. PFP[ll] is based on the Hadoop[l2] parallel algorithms, PFP groups the itemsets, as a condition database divided to each node, each node independently generates the FP-Tree and mines frequent itemsets. PFP reduce the traffic between nodes, increases the degree of polymerization of node. However, algorithm is not efficient if the database is discrete[2]. In addition, grouping strategy relates to the efficiency of the algorithm, a distributed programming difficulty lies in load balancing, distributed algorithm execution time is determined by the maximum execution time of a node, therefore, how to make data uniform distributed related to the algorithm efficiency.

Reference literature[8] mentioned a Round-Robin strategy is basically to achieve load balancing effect, this paper also uses this strategy.

Eclat[13] by depth-first traversal and prefix pattern tree generates frequent item sets, which is also based on if an itemset is not frequent, none of its supersets are frequent.

the time complexity is O(M* N) when Eclat each time adds a prefix, M and N are TD-list length of a prefix sub-tree with the newly added item. The algorithm needs to save D' in memory. Dis-Eclat unlike the previously mentioned algorithm dividing database, but the search space will be allocated to each node, which eliminates the communication between nodes. But as the previous analysis, the time complexity is O(M* N), and each prefix subset must store all records containing the subset.

From the above analysis, the current method has advantages and disadvantages, The advantage of Eclat algorithm is to use a vertical database can quickly calculate support of frequent itemsets. The advantage of FP-growth algorithm is to generate a compressed data structure FP-Tree, and does not produce candidate set in the mining process.

MRPrePost proposed algorithm combines the advantages of both, MRPrePost also established similar to FP-Tree data structure called PPC-Tree, then by PPC-Tree to structure similar to the structure of the vertical TD-list database N-list.

The specific implementation details will be described detail in part III.



III. MRPREPOST ALGORITHM  A. PrePost Algorithm PrePost algorithm presents a data structure named N-list,  which is a modification of the vertical database, storing the association rule mining all the information needed. Pre Post also need to scan the database twice to construct a PPC-Tree, and make use of PPC-Tree to generate the N-list of FIM\' In the mining process, the database does not require re? scanning, only need to intersect the merger N-list, and the complexity of the algorithm is O(m+n), m and n are the length of two N-list, Each element of N-list composed by PrePost-Code, which is called after the sequence encoding the preamble, the composition in the form of ?pre-order, post-order: count?, PrePost-Code is based on the PPC-Tree respectively from the previous order traversal and post order traversal. Fig. 1 show the PPC-Tree, which is similar to FP? Tree, and the construction process is the same with the FP? Tree. but not the same as the composition of the node, PPC? Tree node consists of five components.



I. item-name: represent node name 2. count: represent node count 3. children-list: represent a children collection of the node 4. pre-order: represent order of node when preorder 5. post-order: represent order of node when postorder  Table 1 shows a transaction database, corresponding to Fig.l for PPC-Tree, assuming the minimum support 8 is 3.

ID      Table I. Transaction database  Items Ordered frequent items a, c, g, f c, f, a  e, a, c, b b, c, e, a  e, c, b, i b, c, e  b, f, h b, f  b, f, e, c, d b, c, e, f  (1.2)  (2,1) (9,7)  (3.0)  Figure 1. PPC-Tree corresponding with Table 1  b ----7 ?4, 8): 4)  c ----7 ?1,2): [) - - - ?5,6): 3)  e ----7 ?6,5): 3)  f ----7 ?2,1): I) - - - ?8,4): 1> - - - ?9,7): 1>  a ----7 ?3,0): [) - - - ?7,3): I)  Figure 2. N-list of FIMI  Each k-frequent itemsets Fk corresponds to a N-list, which in ascending order according to the pre-order, at the same time must also be ascending according to post-    2014 IEEE Workshop on Electronics, Computer and Applications  order( concrete proof is in [2]). PPC-Tree's main purpose is to construct N-list liking shown by Fig. 2, then find all the frequent item sets based on N-list. We can then delete the PPC-Tree to reduce memory overhead. The main steps of the PrePost algorithm:

I. Scan transaction database named 0, output the FIM I,  And in descending order according to the number of its support to generate Fl.

2. Scan D again, Select the frequent items in each record and arrange them in the order of F], assuming list of itemsin each record is [piP], p is the first item in the list, P is the rest of the items. Call the function insert tree ([piP], TJ  3. Tree formed on the second step, respectively preorder traversal and postorder traversal, set pre-order and post? order of each node and establish N-list of I-frequent itemsets.

4. Mining frequent itemsets based on N-list using the method liking Apriori Algorithm.

For a detailed description of Step 4, we give an example.

For frequent itemsets bf, by the b, f merger. N-list of b is ?4,8):4>, and N-list of f is ?2,1):1>-?8,4):1>-?9,7):1>, because 4>2, 8> I, this case, f and b are not on the same path, then traverse the next PP-Code of f, we find 4<8 and 8>4, this case b and f are on the same path, so the PP? Code:?4,8): I> added to the N-list of bf. Similarly, since 4 <9 and 8> 7, ?4,8): 1> added to the N-list of bf, in which case the item sets bf N-list: ?4,8): 1>-?4,8): I>, for the same pre-order, post-order to get the merger of PP-Code of bPs N-list structure ?4,8): 2>. Implementation and certification process are in [2].

B. Outline of MRPrePost MRPrePost proposed by this article mainly uses three  MapReduce[14,15] to parallelize PrePost, Fig. 3 describes the specific process.

r-r?---r--r .....

MaP+l ReduceS Generate+ N ?lis.t ...

Map+ 1  Mine<'  ...- _.;:;........::: __  ---.

Reducl ,ub!'ee,"' Figure 3. Flowchart of MRPrePost  Data mining is divided into three stages by MRPrePost algorithm: l. Statistic I-frequent itemsets, similar to the process of word frequency statistics, raw data sets scattered on each worker nodes, each node independently perform   map function, reduce function combined statistical results, and according frequent threshold cropped infrequent items; 2. Using a method similar to building FP-Tree to build PPC? Tree, traverse and generate N-List frequent one set; 3. The search space division, the N-list is distributed to each worker nodes, in order to ensure the cluster load balance, we make use of Round-Robin[8] to act as patition function, each node using the prefix pattern generates independently frequent itemsets.

C. MRPrePost algorithm pseudo-code and analysis Step 1: Block the database level, this process can use the  default file block policy of Hadoop, then the data block is called shard, which is allocated on each worker node. Count the number of items in each shard set in map stage,reduce merges output of map stage, and generates FIM I according frequent support threshold 8, then descends FIM I based on support of FIM I to generate F-list. The algorithm pseudo code shows in Fig. 4.

Input: A transaction dat.abase DB and a minimum support. IW Output: F-list (the set of frequent l -iternsets order by desc) .....

Procedure, Mapper. (key,value = T) +' for each item ai in Ti do .....

Output. (<key=a; . value=l? +' end .....

Procedu.re, Reducer (key=a,. value=S(a,? +' swn = O? .....

for each I in Sea;) do+'  swn += 1 : .....

end*", if(sum>=O) Output (<key=a;, value=sum? ; lIoutput the FIMl+'  tben call function Sort.(FIMl); output the F -list+,  Figure 4. Pseudo code of parallel statistical I-frequent item sets and sort them  Step 2: Each map filters shard based on F-list, for each transaction of shard, sort frequent item based on sequence of F-list and output the result as value. Then, reduce constructs the compressed tree similarly constructing FP-Tree.

Postorder traversal the tree to determine postorder and preorder the tree to determine preorder, then generate N-list of I-frequent itemsets. Pseudo code is shown in Fig. 5 and Fig. 6.

loput: shard and F-list ...

Output: PP C-Tree+' Procedure: Mapper (key, TJ ...

for each T, +-'  select. the frequent iten1s in Ti son out them according to the order of F-list.+-, tbeln +-'  produce a path [PIP] as the value to out.put <key, [PIP]>+-, end for+-'  [pPC-Tree cOD strudion]+-'  Procedu re : Reducer (key, [PIP]) +-' create the root of a PP C -Tree . and label it as '?NULL"+-, for each [PIP]"  call insen_tree([PIP], T,)<, end for+-'  [Function insert_tree?(pIP],T r)]+-'  if Tr has a childNsuch thatN.item-nanle=p.item-namethen+-' increaseN's cownby l;+-'  creat,e a ne,,, node N. ,vith it.s count. init.ialized to 1, and add it. toT schildren-list?+-' if P is nonenIpt.y then+-'  call insert_tree(p,N) recursively_+-, end if+-'  end if +-'  Figure 5. Pseudo code of constructing ppe-Tree    2014 IEEE Workshop on Electronics, Computer and Applications  Inp ut: PPC-tree and Ll, the set of frequent l -itemset-s _ +' OU tput : NL1 . the set of the N -lists of frequent l -itemsets_+' Procedure N -lists_construction(pPC-tree? first. scan PPC-Tree to generate the post-order of each node+-'  then create NL" let NL, [k] be the N -list ofL, [k]-"  for each node N of PPC-tree accessed by pre-order traversal do .....

generare the pre-order of each node+'  if (N.it.em-nam e=L 1 [k ].item-nam e)tb eo+' llsert< CN_pre-order. N _post-order) :N_COW1t> into NLl[k]+-'  end if+-' end for+-'  Figure 6. Pseudo code of generating N-list of l-frequent item sets  Step 3: Save the N-list of I-frequent itemsets in a distributed cache, to be shared by all map.While group N? list using Round-Robin[12], which is a modular arithmetic remainder strategy. Assuming the number of groups is m,the set of N-list of I-frequent item sets is P = {Pi> P2' P3' ... ,pd, Pi represents a N-list of the i-th frequent itemsets, group(pJ = i mod m. This can try to ensure that the cluster loads balance. According to this strategy, assuming m is 3, then: gl = {b --> {< (4,8): 4 >}, f --> {< (2,1): 1 >, < (8,4): 1 >, < (9,7): 1 >}} g2 = {c --> {< (1,2): 1 >, < (5,6): 3 >}, a --> {< (3,0): 1 >, < (7,3): 1 >}} g3 = {e -> {< (6,5): 3 >}. Fig. 7 shows the pseudo code.

Function group(NLl,group_sizey  for i=O 1 0 NLI_sizeO+-' groups[ i % group_sizel _add(N L l [i]}-'  end for ......

Figure 7. Pseudo code of grouping N-list  Input; group_i and shared the NLJ to be saved in distributed cache+-' Output: frequent k-itenls F ...

for each mapper dO+-'  for each NL_l of groups[i] dO+-' call mining_fim_k(NL_l .Lk,NLI. OJ+'  end for .....

end for .....

Function mining_fim_kCNL_k.NL I,O)+-' for i = 0 to NLI dO+-'  if(NL_k.count :;::: IDBI * 6)0-' F=F U Lk'-' if(NL_kcount ? NLJ[i].count}o-'  ASSUllle LJ.:. = XlX2X3 ___ x).., L[i].itern = Xl.-.;-). supp(xk?supP(X1.::+1P-' LlL+l = Lk+L.[i] II LlL+l =XIX2 . ___ Xk:+I ....

Ll:= Lk+l+-'  compare N-list ofNL_kwith N-list. ofNLt[i], ....

if NL_kpreorder<NL I [i1 ?prepost&&NL_k . postorder>NLI[i1 ?postorder ...

tben ...

NL_k+l.N -list. .add?( NLt[iJ_prepost , NLJ [i1.postorder.count ):NLt [i].count.?  end if ...

end If ...

end if ......

end for ....

Figure 8. Pseudo code of mining frequent itemsets  Step 4: This step is a key in the whole algorithm. The pseudo-code shown in Fig. 8. Each map independently depth-first traversals every frequent item in the group assigned, until all frequent item sets with the current prefixes sub-tree are located far. For b in group 1, the current prefix is b, when c and e are added to the prefix sub-tree to generate 2-frequent itemsets {bc,be }(bf and ba are not frequent itemsets). To bc, be prefixed to continue the operation, eventually get all the frequent item sets on b. {b,bc,be,bce}.

In the process of merging the prefix sub-tree, the paper made a modification to the original algorithm, for example, when b and c are combined, original algorithm generate PPCode < (b.preorder, b.postorder): c.count > when the   condition is b.preorder < c.preorder && b.postorder > c.postorder. But, this article will generate PPCode as ? c.preorder, c.postorder): c.count> at the same condition.

As a result of the depth-first and prefix sub-tree policy, we must promise the new item added and the current prefix sub-tree on the same path, necessary and sufficient condition is the new added item and the last item of the current prefix sub-tree are on the same path. This's the reason why we generate PPCode as ?c.preorder, c.postorder): c.count>. Finally, reduce combines output.



IV. EXPERIMENTAL RESULTS AND ANAL YSIS  Three sets of experimental environment for desktop computers equip ubuntu 10.0, one as the master node, the other two as slaver node. The three computers have same configuration, CPU is AMD Athlon dual-core processor, clocked at 2.11GHz, memory size 2G. T10I4D100K and Pumsb act as experimental data. They can be downloaded at http://fimi.ua.ac.be/dataJ, the first is small and the second is large. We compare the runtime of three algorithms Pre Post, PFP and MRPrePost when they are performed on the two datasets. Source code of PFP is provided by machout[16].

-z; 50 ? 40 .? 30 j 20   ....

? ??  ?   ?  Minsupp%   "   Figure 9. Comparison of the three algorithms on Tl014Dl00K   ? 5000 ? 4000 -? 3000 u::: 2000   :=-. ? ?  '"' , ......

1000 1500 3000 4000 5000 7000 Minsupp  I ? MWrePos' I _PFP PrePost  Figure 10. Comparison of the three algorithms on Pumsb  From experimental results shown by Fig. 9 and Fig. 10, we know the runtime will become shorter when support increases. It's evident. Fig. 9 also reflects performance of the parallel algorithm is not as good as PrePost on small dataset. The reason is each node needs to send message to others in clusters, but delay of network bandwidth is unpredictable, so I/O operation occupies main runtime, thus affecting the performance of the algorithm. Contrarily, Pre Post has an advantage of data localization. But when the dataset is large, PrePost at a lower support threshold can not be performed due to memory overflow. The MRPrePost and PFP can still frequent itemsets mining, which is the purpose of PrePost algorithm parallelization, the main purpose of parallelization is to handle large dataset, which can not be    2014 IEEE Workshop on Electronics, Computer and Applications  processed on standalone. The results in Fig. 10 reflects this view.

In addition, we can also know from Fig. 10, MRPrePost and PFP are significantly superior to PrePost, PrePost can not be calculated at a low support, but MRPrePost and PFP still have a good performance. At this time, communication time between the nodes in a distributed cluster is not a major factor, but data processing time. Parallelization is to use multiple processors independently to process small-scale data, so the algorithm superior performance compared to a stand-alone environment. The results also reflect, whether on a large or small datasets, runtime of MRPrePost is shorter than PFP's. Mainly because of sharing cache when MRPrePost conducts a depth-first strategy, which reduces the communication. Another reason is MRPrePost sub-tree to add a prefix entry time is linear, these measures save a lot of running time.

7000 r-?--------------..,  6000 f-!l",-------------------1  co 5000 "'? ] 4000 I----=?-?"i======:::;:::==_---j I---+- MRPrePost I ? 3000 f----------------------J '"  2000 f------------------3  1000 f-------------------3  Nwnber of Mappers  Figure 11. The number of ditlerent map atlects the running time  In order to validatethe scalability of the algorithm, this paper dynamically adjusts the number of map on each node, the experimental result shown in Fig. 11. From the experimental results, With the increasing number of map, less and less time to run the algorithm, but with the increase in the map, the running time is reduced more slowly, tends almost unchanged due to the limited capacity of a single node, the calculation efficiency can not unlimited upgrade, number of map can not upgrade performance when increase to a certain number. Then the number of node should be increased instead of increasing the number of map in a single node. Through the above analysis can be drawn, MRPrePost proposed algorithm has good scalability.



V. CONCLUSION  Big Data mmmg is now a hot research question, MapReduce brings a simple distributed programming. The paper proposes a parallel algorithm based on MapReduce called MRPrePost on the basis of PrePost. and describes in detail the implementation of the algorithm.

In the face of mining large data sets, the parallelization is a good solution, experimental results prove this point.

Through experiments, we compare the performance of Pre Post and MRPrePost, and also with the PFP algorithm horizontal contrast, MRPrePost algorithm is the fastest of the three algorithms. The results meet our expectations, and analyzes the causes of the experimental results in the paper.

Since MRPrePost algorithm is based on MapReduce, so we can simply add nodes to extend the algorithm without the   need for any modifications to the algorithm. It can be concluded, MRPrePost proposed algorithm suitable for large-scale data sets for mining association rules.

