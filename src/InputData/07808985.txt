IEEE SIGNAL PROCESSING LETTERS, VOL. 24, NO. 2, FEBRUARY 2017 191

Abstract?Alternating direction method of multipliers (ADMM) has been well recognized as an efficient optimization approach due to its fast convergence speed and variable decomposition property.

However, in big data networks, the agents may not feedback the variables as the centralized controller expects. In this paper, we model the problem as a Stackelberg game and design a Stackel- berg game based ADMM to deal with the contradiction between the centralized objective of the controller and the individual ob- jectives from the agents. The Stackelberg game based ADMM can converge linearly, which is not dependent on the number of agents.

The case study verifies the fast convergence of our game-based incentive mechanism.

Index Terms?ADMM, big data, game theory, large-scale net- work.



I. INTRODUCTION  THE information exposition of ?big data? today requirescomputational methods fast and scalable in data anal- ysis and optimization problems [1]. An important trend of approaches is to divide the ?data? into different small groups and deal with them separately on multiple agents [2]. Recently, the alternating direction method of multipliers (ADMM) [3] has attracted much attention due to its effectiveness in both fast convergence and variable decomposition for dealing with typ- ical big data problems. ADMM has also been modified to solve many other mathematical optimization problems in more com- plicated and specific forms, such as multiagent ADMM with Gaussian back substitution for more than two variables [4] and proximal Jacobian ADMM on parallel programming [5]. Mean-  Manuscript received November 27, 2016; revised December 27, 2016; ac- cepted December 27, 2016. Date of publication January 5, 2017; date of current version January 20, 2017. This work was supported in part by the National 863 Project under Grant 2015AA01A709 5G, in part by the National Nature Science Foundation of China under Grant U1301255 and Grant 61461136002, in part by the Australian Research Council under Grant DP150104019 and Grant FT120100487, and in part by the U.S. National Science Foundation un- der Grant CPS-1646607, Grant ECCS-1547201, Grant CCF-1456921, Grant CNS-1443917, Grant ECCS-1405121, and Grant NSFC61428101. The asso- ciate editor coordinating the review of this manuscript and approving it for publication was Dr. Ashish Pandharipande.

Z. Zheng and L. Song are with the State Key Laboratory of Advanced Optical Communication Systems and Networks, School of Electronics Engineering and Computer Science, Peking University, Beijing 1000871, China (e-mail: zijie.zheng@pku.edu.cn; lingyang.song@pku.edu.cn).

Z. Han is with the Department of Electrical and Computer Engineering, University of Houston, Houston, TX 77004 USA (e-mail: zhan2@uh.edu).

Color versions of one or more of the figures in this letter are available online at http://ieeexplore.ieee.org.

Fig. 1. ADMM form problem with IOs in mobile crowd sourcing.

while, ADMM and its modified forms have been extensively applied in wireless cellular networks [6] and smart grid [7].

Even though the variable updates of ADMM can be performed separately on a number of agents, the way to do the update is specified by the controller. Hence, ADMM cannot work properly if those agents do not feedback variables as the controller ex- pects. This problem may become even worse when the network grows bigger, where different agents are selfish, and thus, likely to have their own individual objectives (IOs). As shown as an example in Fig. 1, for a mobile crowd sourcing system [8], the server collects location data {xi} = (x?1 ,x?2 , . . . ,x?N )? from mobile devices to achieve a systematic goal as min  ?N i=1 fi(xi),  where each xi is a column vector and (?)? is the transpose operation. Each device denoted by Pi uses GPS and collects location information only when it runs the location-based appli- cation, mathematically expressed as a goal min hi(xi). Obvi- ously, when fi(xi) and hi(xi) have different minimum points, the centralized goal can hardly reach.

To achieve the centralized objective, the controller needs to provide incentives on agents, e.g., payments. With incentive mechanisms, the agents are willing to help achieve the centralized goal instead of their individual ones. An effective approach is the Stackelberg game [9], [10], where the controller is modeled as a leader, and agents act as followers. The leader does not directly optimize {xi}. Instead, it can anticipate that the followers select their best responses {xi} when the leader designs its proper strategy on fixed incentives. As such, with properly designed strategy by the leader, the followers are willing to set the best responses to reach the optimal of point centralized goal, i.e., min  ?N i=1 fi(xi). The linear convergence  for this process is proved in this letter when all objective  See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

192 IEEE SIGNAL PROCESSING LETTERS, VOL. 24, NO. 2, FEBRUARY 2017  functions are strongly convex, and the agents? objectives have a uniform Lipschitz gradient.

The rest of this letter is organized as follows. In Section II, we model the problem as a Stackelberg game. Then in Section III, we present the Stackelberg game based ADMM and provide a case study. We draw the main conclusions and discuss future works in Section IV.



II. PROBLEM FORMULATION  We consider a network with one controller and N agents.

The incentive mechanism is needed when the objective of the controller (  ?N i fi(xi)) and each agent (hi(xi)) can-  not reach the same optimal point. We denotes {x?i } =( (x?1)  ?, (x?2) ?, . . . , (x?N )  ?)? as the optimal point for the con- troller, and {x?i} = (x??1 , x??2 , . . . , x??N )? denotes the optimal point for the agents. The controller needs to design an incentive mechanism such that each agent is willing to change its action to reach {x?}: let each agent optimize an incentive function ?i(hi(xi),?i) instead of hi(xi). Through adjusting the param- eter ?i , the controller can change each xi from x?i to x?i . Hence, the incentive mechanism design problem can be formulated as a Stackelberg game below [10]  LG: {??i } = argmin {?i }  N?  i=1  fi(xi)  FG: x?i = argmin x i  ?i(hi(xi),??i ) ? 1 ? i ? N,  Constraint: N?  i=1  A?i xi = C  (1)  where LG represents the leader?s game, FG is the follower?s game, and  ?N i=1 A  ? i xi = C is a linear global constraint for  both LG and FG, Ai is the weight matrix, which represents the importance of the variable xi for the controller, and C is a column constant vector. The controller is defined as the leader and {?i} =  ( (?1)?, (?2)?, . . . , (?N )?  )? are the variables con-  trolled by the leader, i.e., the leader?s strategy. Each agent is defined as each follower, and xi is the variable controlled by the follower Pi , i.e., follower Pi?s strategy.1 The leader can properly adjust its strategy, {?i} = {??i }, to let the followers solve FG. When {?i} = {??i }, the followers select their best responses as {xi} = {x?i }, which in turn minimize the leader?s objective function in LG.

The objective is to design the incentive functions {?i(hi(xi),?i)} and incentive parameters {??i } to solve the Stackelberg game in (1). In other words, with our designed Stackelberg game based ADMM, the optimal point of the leader?s objective function and each follower?s incentive func- tion are the same. To guarantee the convergence of our Stack- elberg game based ADMM for multiple blocks, we provide some necessary mathematical assumptions to restrict the form of objective functions. Similar assumptions can be found in [11] and [12] to extend ADMM from two blocks to multiple blocks and guarantee the convergence.

Assumption 1: Each part in leader?s objective func- tion, fi(xi), is a strongly convex function [11]. Mathematically,  1For convenience, for the rest of this letter, we use the term ?leader? to represent the controller, and the term ?follower? to represent each agent.

given any xi and x?i , the following inequation is satisfied  fi(x?i) ? fi(xi) + (?x i fi(xi))? (x?i ? xi) + ?  ||x?i ? xi ||22  (2) where ?x i is the first-order derivative vector of fi for vari- able xi , || ? ||2 is the second-order norm, and ? > 0.

Assumption 2: The objective function of each fol- lower, hi(xi), is a strongly convex function. Given any xi and x?i , we can find a ? > 0, where the following inequation is satisfied  hi(x?i) ? hi(xi) + (?x i hi(xi))? (x?i ? xi) + ?  ||x?i ? xi ||22 .

(3) Assumption 3: The objective function of each follower sat-  isfies a uniform Lipschitz gradient condition [12]. There exists ? > 0, for each hi(xi), given any xi 	 x?i , we have  ?x i hi(x?i) ??x i hi(xi) 	 ? (x?i ? xi) (4) where D 	 F indicates that each element of the vector D is less than or equal to that of the vector F. The Lipschitz gradient condition usually indicates that the first order gradient of a function cannot change too fast, where the speed is below ?.



III. STACKELBERG GAME BASED ADMM  In this section, we design a Stackelberg game based ADMM on incentive functions {?i (h(xi),?i)} and incentive parame- ters {?i}, to let the optimal point of each ?i(hi(xi),?i) approx- imate the optimal point of each fi(xi), step by step. We then prove that the approximation can converge at a linear speed.

A. Stackelberg Game based ADMM  In the Stackelberg game based ADMM, at each step k, the leader provides incentive parameter ?ki to follower Pi , and the incentive function ?i(hi(xi),?ki ) is given by  ?i(hi(xi),?ki ) = Li(xi ,?) + ? k i (xi) (5)  where Li(xi ,?) is a segmental Lagrangian function for xi and ?ki (xi) is the purely individual part. Li(xi ,?) and ?  k i (xi) are,  respectively, given as  Li(xi ,?) = fi(xi) ? (?E)?A?i xi (6) ?ki (xi) = hi(xi) ? (?ki )?xi (7)  where ? is a diagonal matrix and E is an unit column vector.

The physical meaning of the incentive function in (5) can be  interpreted as below.

Remark 1: The leader asks follower Pi to optimize Li(xi ,?)  through a payment as (?ki ) ?xi , where ?ki is the payment per unit,  i.e., the price. With the payment, follower Pi helps minimize Li(xi ,?) simultaneously and considers its purely individual part ?ki (xi).

We provide the two-layer iteration process of our Stackelberg game based ADMM in Algorithm 1. At the beginning of the algorithm, the leader transmits the form of each Li(xi ,?) in (6) to each follower Pi . At each step k, given leader?s strategy {?ki }, the leader and the followers at first reach a current optimal point ({xki },?k ) for {?i(hi(xi),?ki )} [to solve FG in (1)], with ADMM as given in the inner loop in Algorithm 1. The ADMM process is given as below, where t is the iteration time for the inner loop.

ZHENG et al.: BRIDGE THE GAP BETWEEN ADMM AND STACKELBERG GAME 193  1) Sequentially follower?s update:  xki (t + 1) = argmin x i  Li(xi ,?k (t)) + ?ki (xi)  + ?   ? ? ? ? ? ?  i?1?  j=1  A?j x k j (t + 1) + A  ? i xi +  N?  j=i+1  A?j x k j (t) ? C  ? ? ? ? ? ?    .

(8)  2) Leader?s dual update:  ?k (t + 1) = ?k (t) ? diag (  ?( N?  i=1  A?i x k i (t + 1) ? C)  )  (9)  where diag(D) is to transmit a vector D to a diagonal matrix.

Based on Assumptions 1 and 2, the ADMM process can converge into an optimal point, ({xki },?k ), according to [11].

After the inner loop, each follower feeds back its marginal cost ?x i hki (xki ) to the leader. The leader then changes its strat- egy as the adjustment of incentive parameters, to set the price ?k+1i as the current marginal cost of each follower, i.e.,  ?k+1i = ?x i hki (xki ). (10) Then, the leader and followers can reach a new optimal point  ({xk+1i },?k+1) for FG in (1) at the next step through ADMM.

The outer loop ends when the Lagrangian function of  the leader satisfies a primal stopping criterion, ||L({xk+1i }, ?k+1) ? L({xki },?k )|| ? ?. The augmented Lagrangian func- tion L({xi},?) is the sum of segmental Lagrangian functions in (6) plus a second-order norm of the constraint in (1)  L({xi},?)= N?  i=1  Li(xi ,?) + (?E)?C + ?   ? ? ? ? ?  N?  i=1  A?i xi ? C ? ? ? ? ?    .

(11)  B. Convergence  In this part, we prove that the Stackelberg game based ADMM is guaranteed to converge linearly into a Stackelberg equilib- rium, i.e., to solve the game in (1). To prove the linear conver- gence, we at first prove an important lemma.

Lemma 1: For any {xki } and ?k , we have ||?{x i }L({xki },?k )||22 ? 2?  ( L({xki },?k ) ? L({x?i },??)  ) .

(12) Proof: Please see Appendix A. ?  Fig. 2. Relation between the iteration time k in Algorithm 1 and the objective function of the leader.

Based on Lemma 1, we can prove that our Stackelberg game based ADMM converges linearly on the outer loop.

Proposition 1: The Stackelberg game based ADMM con- verges linearly. Mathematically, given any small scaler ?0 , when the primal residue of the augmented Lagrangian function is less than ?0 , ||L({xKi },?K ) ? L({x?i },??)|| ? ?0 , the iteration time K has the following upper bound, K ? K0 ? 1?0 , where K0 is a positive constant. More specifically, we can prove that  K is a log-linear function of 1?0 , K = K1 log (  K 2 ?0  ) , where K1  and K2 are both positive constant.

Proof. Please see Appendix B. ? Remark 2: FG is guaranteed to be solved by ADMM (the  inner loop in Algorithm 1) at any step k. The convergence guarantees that the Stackelberg game based ADMM can finally solve LG in (1). Thus, the Stackelberg game is finally solved.

In addition, through the proof in Appendix B, we can find that the iteration time only depends on the selection of the initial point, {x0i } and ?0 . That is to say, the convergence speed for the outer loop in Algorithm 1 does not depend on the number of followers, i.e., N . This indicates that the Stackelberg game based ADMM can converge fast even though the network size is large.

C. Simulation Results  A case study is given in Fig. 2 for a crowd souring net- work with 1 leader and N = 10 followers, where each xi is a scalar as xi , fi(xi) = x2i , hi(xi) = exp(xi), and Ai(1 ? i ? N) and C are random scalars satisfying uniform distributions Ai ? UAi (0, 1) and C ? UC (0, 1). In Fig. 2, the lower line shows the optimal value of the leader?s objective,  ?N i=1 fi(x  ? i ).

The upper horizontal line is the value of the leader?s objec- tive,  ?N i=1 fi(x?i), without the incentive mechanism, i.e., each  follower optimizes its own objective, minxi hi(xi). The mid- dle curve shows the iteration process of the Stackelberg game based ADMM. It can be observed that the Stackelberg game based ADMM can effectively converge into the minimum point of the leader?s objective. Notice that the initial value of the leader?s objective is shown when k = ?1, where equalizes to the value without incentive mechanisms. At step 0, the leader sets all prices {?0i } as zero. The convergence speed takes six steps to reach a point with the stopping criterion ? = 10?4 .

194 IEEE SIGNAL PROCESSING LETTERS, VOL. 24, NO. 2, FEBRUARY 2017

IV. CONCLUSION AND DISCUSSION  In this letter, we have studied the incentive mechanism de- sign for a network with one controller and multiple agents, when considering IOs of the agents. The problem can be mod- eled as a Stackelberg game and we have designed a Stackelberg game based ADMM to satisfy both controller (leader) and each agent (follower). The leader?s strategy as parameters {?i} can be interpreted as the price (Remark 1). In addition, our pro- posed Stackelberg game based ADMM can converge linearly, when all objective functions are strongly convex and each fol- lower?s objective function holds a uniform Lipschitz gradient (Proposition 1). The linear convergence is not dependent on the number of followers (Remark 2). The case study verifies the fast convergence of our incentive mechanism.

APPENDIX A PROOF OF LEMMA 1  According to ADMM, the inner loop in Algorithm 1 can reach a current optimal point, we have the following equations  ? (??E)? (  N?  i=1  A?i x ? i ? C  )  + ?  ||A?i x?i ? C||22 = 0 (13)  ? (?kE)? (  N?  i=1  A?i x k i ? C  )  + ?  ||A?i xki ? C||22 = 0 (14)  ? (?kE)? (  N?  i=1  A?i x ? i ?  N?  i=1  A?i x k i  )  ? N?  i=1  ?(A?i )(A ? i x  k i ? C)(x?i ? xki ) = 0. (15)  Due to the strong convexity of fi(xi), as given in Assumption 1, we have  N?  i=1  fi(x?i ) ? N?  i=1  fi(xki ) + N?  i=1  (?x i f(xki ) )?  (x?i ? xki )  + N?  i=1  ?  ||x?i ? xki ||22 . (16)  We plus (13) on the left side of (16) and plus (14) and (15) on the right side, we can derive that the augmented Lagrangian function in (11) is also strongly convex at the current optimal point ({xki },?k )  L({x?i },??) ? L({xk},?k ) + (?{x i }L({xki },?k )  )?  ? ({x?i } ? {xki } )  + ?  ||{x?i } ? {xki }||22 . (17)  Furthermore, we can derive that  L({x?i },??)  ? min {y i }  L({xk},?k ) + (?{x i }L({xki },?k ) )? ({yi} ? {xki }  )  + ?  ||{yi} ? {xki }||22 .

= L({xki },?k ) ? 2?  ||?{x i }L({xki },?k )||22 . (18)  Thus, we have  ||?{x i }L({xki },?k )||22 ? 2? ( L({xki },?k ) ? L({x?i },??)  ) .

(19)  APPENDIX B PROOF OF THE LINEAR CONVERGENCE  According to ADMM, the inner loop in Algorithm 1 can reach a current optimal point, then we have the following equation for any index i  0 = ?x i Li ( xk+1i ,?  k+1) + ?x i ?k+1i ( xk+1i  )  = ?x i Li ( xk+1i ,?  k+1) + ?x i hi ( xk+1i  ) ??x i hi ( xki  ) .

(20) Then, based on Assumption 3 and Lemma 1, we have the  following inequation:  L({xki },?k )  ? L ({xk+1i },?k+1 )  + (?{x i }L  ({xk+1},?k+1))?  ? ({xki } ? {xk+1i } )  ? L ({xk+1i },?k+1 )  + N?  i=1  (?x i Li ( xk+1i ,?  k+1))? 1 ?  (?x i hki ( xki  ) ??x i hki ( xk+1i  ))  = L ({xk+1i },?k+1  ) +  ? ||?{x i }L  ({xi}k+1 ,?k+1 ) ||22  ? L ({xk+1i },?k+1 )  + 2? ?  ( L  ({xk+1i },?k+1 ) ? L ({x?i },??)  ) .

(21)  We subtract L({x?i },??) on inequation (21), we have L({xki },?k ) ? L({x?i },??)  ? (  1 + 2? ?  ) ( L({xk+1i },?k+1) ? L({x?i },??)  ) .

(22)  This shows the step size between two steps k and k + 1. We denote ? = 1/(1 + 2?? ) < 1, we have  L ({  xk+1i }  ,?k+1 ) ? L ({x?i } ,??)  L ({  xki }  ,?k ) ? L ({x?i } ,??)  = ? < 1. (23)  That is to say, when ?K ( L({x0i },?0) ? L({x?i },??)  ) ? ?0 , ||L({xKi },?K ) ? L({x?i },??)|| ? ?0 is guaranteed, where ({x0i },?0) is the current optimal point at step 0. In particular, we draw the relation between the step K and ?0 as below  K ? log(L({x i },?0) ? L({x?i },??))/?0  log(1/?) . (24)  The log-linear convergence of K indicates that there exists a constant K0 > 0, where K ? K0 ? 1?0 . That is to say, the price model converges linearly.

Furthermore, it can be calculated directly that the rela- tion between ? in the stopping criterion ||L({xki },?k ) ? L({xk?1i },?k?1)|| ? ? in Algorithm 1 and the primal residue ?0 , as ?0 = ?1?? .

