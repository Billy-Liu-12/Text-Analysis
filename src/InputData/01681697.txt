Unsupervised Image Segmentation and Annotation for Content-Based Image Retrieval

Abstract? We propose an unsupervised approach to segment color images and annotate its regions. The annotation process uses a multi-modal thesaurus that is built from a large collection of training images by learning associations between low-level visual features and keywords. Association rules are learned through fuzzy clustering and unsupervised feature selection. We assume that a collection of images is available and that each image is globally annotated. The objective is to extract represen- tative visual profiles that correspond to frequent homogeneous regions, and to associate them with keywords. Our approach has three main steps. First, each image is coarsely segmented into regions, and visual features are extracted from each region.

Second, the regions are categorized using a fuzzy algorithm that performs clustering and feature weighting simultaneously. As a result, we obtain clusters of regions that share subsets of relevant features. Representatives from each cluster and their relevant visual and textual features would be used to build a thesaurus. Third, fuzzy membership functions are used to label new regions based on their proximity to the thesaurus entries.

The proposed approach is trained with a collection of 2,695 images and tested with several different images.



I. INTRODUCTION  The advent of digital libraries has made it necessary to develop automated tools for storing, retrieving, organizing, and mining large multimedia databases. Image data offers unique advantages in that it is relatively easy for human to explore and interpret. However, for computer methods, it poses serious challenges. To manage image databases, Content-Based Image Retrieval (CBIR) emerged as a new research subject [1], [2]. CBIR involves the development of automated methods that are able to recognize the visual features of the images, and to make use of this information in the indexing and retrieval processes.

The performance of most CBIR systems is inherently constrained by the used low-level features, and cannot give satisfactory results when the user?s high level concepts cannot be expressed by low level features. In an attempt to bridge this semantic gap, few approaches that integrate low level visual features and textual keywords have been proposed [3], [4], [5], [6], [7]. For instance, Duygulu et al.[6] treat the problem as a translation of image regions to words, and use the EM algorithm to learn the mapping between visual features and words. Mori et al.[8] use co-occurrence statistics of fixed image grids and words to model the associations. More recently, a cross-media relevance model  Hichem Frigui is with the Department of Computer Engineering and Computer Science, University of Louisville, Louisville, KY 40292, USA (email: h.frigui@louisville.edu).

Joshua Caudill is with the Department of Computer Engineering and Computer Science, University of Louisville, Louisville, KY 40292, USA (email: joshua.caudill@gmail.com).

[7] and Latent Semantic Analysis (LSA) were proposed to learn associations.

In this paper, we propose a different approach to annotate images. Our approach is based on fuzzy clustering and feature discrimination. First, clustering is used to identify representative profiles that correspond to frequent homoge- neous regions. The feature discrimination process, embedded in the clustering, would identify the relevant features in each profile. Second, representatives from each cluster and their relevant visual and textual features are used to build a thesaurus. Finally, fuzzy membership functions are used to label new regions based on their proximity to the thesaurus entries.

The rest of the paper is organized as follows. In section 2, we introduce our data representation model that combines visual features and keywords. In section 3, we describe our simultaneous clustering and feature discrimination algorithm that is used to learn associations and to build a multi-modal thesaurus. Section 4 describes the crisp and fuzzy image annotation processes. Section 5 contains the experimental results, and section 6 contains the summary conclusions.



II. PROPOSED MODEL  We assume that we have a large collection of images and that each image is annotated by few keywords. We do not assume that the annotation is complete or accurate. For instance, the image may contain many objects, and we do not have a one to one correspondence between objects and words.

This scenario is very common as images with annotations are readily available, but images where the regions themselves are labeled are rare and difficult to obtain. Moreover, we do not know which specific visual attributes best describe the keywords. Fig. 1 displays 3 images annotated at the image level. Some keywords, such as ?grass?, can be clearly associated with color features. Others, such as, ?house?, may be associated with shape features. Other words may be associated with any combination of color, texture, and shape features. This information, if it could be learned, would improve the efficiency of image annotation and hybrid searching and browsing.

A. Feature extraction and representation  First, the image is segmented into homogeneous regions based on color and/or texture features. We do not require accurate segmentation as subsequent steps can tolerate miss- ing and over-segmented regions. Then, each region would be described by visual features such as color, texture, shape, and a set of keywords. Let {f (i)j1 , ? ? ? , f  (i) jkj  } be a kj dimensional   Sheraton Vancouver Wall Centre Hotel, Vancouver, BC, Canada July 16-21, 2006     Fig. 1. Examples of image-level annotations that refer to different and not segmented regions in the image.

Fig. 2. Representation of visual and textual features  vector that encodes the jth visual feature set of region Ri of a given image. For the keywords, we use the standard vector space model with term frequencies as features [9].

Let {w1, w2, ? ? ? , wp} be the representation of the keywords describing the given image ( not region-specific). An image that includes n regions (R1, ..., Rn) would be represented by n vectors of the form:  f (i) 11 , ? ? ? , f  (i) 1k1? ?? ?  visual feat 1 of Ri  , ? ? ? , f (i)C1, ? ? ? , f (i) CkC? ?? ?  visual feat C of Ri  , w1, ? ? ? , wp? ?? ? Keywords  , i = 1 ? ? ?n.

Fig. 2 illustrates our image representation approach. We should note here that since the keywords are not specified per region, we duplicate them for each region representation. Our assumption is that, if word w describes a given region Ri, than a subset of its visual features would be present in many instances across the image database. Thus, an association rule among them could be mined. On the other hand, if none of the words describe Ri, then these instances would not be consistent and will not lead to strong associations.

B. Association rules  Association rule mining [10] has been used traditionally in applications such as market basket analysis. It attempts to capture interesting relationships between attributes, thereby enhancing the understandability of the data. Association rule mining has also been applied to image data [11], [12].

However, they have not been fully exploited for the case of  multi-modal data to learn relationships among the different modalities.

Using the above image representation, a large collection of images could be mined to extract associations between the different feature sets. For instance, using a subset of images similar to those in Fig. 1, we can extract association rules of the form: ?If color is green and texture is regular, fine, with dominant orientation at 90o then keyword is grass.? (shape and spatial location features are not relevant). Using this type of rules, we can build a thesaurus to:  ? Auto-annotate: e.g., for a given region, if its color is ?green? and has specific texture properties, we can automatically label it ?grass?.

? Perform hybrid search and browsing: e.g., if the user specifies ?grass? as the keyword for searching, the search will also include the associated visual features.

Several algorithms could be used to extract association rules from the proposed data representation. For instance, the features could be quantized into a finite set of symbols, and standard association rule algorithms (such as [10]) could be used. Similarly, algorithms that can handle continuous attributes (such as [13]) could be directly applied. In this paper, we present a different approach that is based on simultaneous clustering and feature discrimination.



III. CLUSTERING AND FEATURE DISCRIMINATION For many clustering applications, the influence of the features  is generally not equally important. Since irrelevant features can adversely affect the definition of clusters, it is recommended to identify cluster-dependent feature relevance weights. Unfortunately, most existing feature selection/weighting algorithms [14] are not suitable for unsupervised learning (i.e., unlabeled data). Recently, we proposed an algorithm that performs Simultaneous Clustering and Attribute Discrimination (SCAD)[15]. SCAD was designed to search for the optimal clusters? prototypes and the optimal relevance weight for each feature of each cluster. However, for high dimensional data, learning a relevance weight for each feature may lead to over-fitting. To avoid this situation, we use a coarse approach to feature weighting (called SCADc). Instead of learning a weight for each feature, we divide the set of features into logical subsets, and learn a weight for each feature subset.

Let X={xj ? <p|j=1, . . . , N} be a set of N feature vectors.

Let B=(?1, . . . , ?c) represent a C-tuple of prototypes each of which characterizes one of the C clusters. Each ?i consists of a set of parameters depending on the distance measure used. For instance, if the Euclidean distance is used, each ?i corresponds to the cluster?s centroid. Similarly, for the Mahalanobis distance, each ?i would include the centroid and the covariance matrix of the cluster. Let uij represent the membership of xj in cluster ?i [16] such that  uij ? [0, 1], ?i and CX  i=1  uij = 1 ?j. (1)  Assume that the p features have been partitioned into K subsets: FS1, FS2, ? ? ? , FSK , and that each subset, FSs, includes ks features. Let dsij be the partial distance between xj and cluster i using the sth feature subset. Let V = [vis] be the relevance weight for FSs with respect to cluster i. The total distance, Dij , between xj and cluster i is then computed by aggregating the partial distances and their weights. In this paper, we let  D2ij =  KX s=1  vis ` dsij ?2  . (2)  SCADc minimizes  J =  CX i=1  NX j=1  umij  KX s=1  vis ` dsij ?2  +  CX i=1  ?i  KX s=1  v2is, (3)     subject to  vis ? [0, 1] ? i, s; and KX  s=1  vis = 1, ? i. (4)  To optimize J , with respect to V, we use the Lagrange multiplier technique, and obtain  J(?,V) =  CX i=1  NX j=1  (uij) m  KX s=1  vis ` dsij ?2  +  CX i=1  ?i  KX s=1  v2is ? CX  i=1  ?i   KX  s=1  vis ? 1  !

,  where ? = [?1, ? ? ? , ?c]t. Since the rows of V are independent of each other, we can reduce the above optimization problem to the following C independent problems:  Ji(?i,Vi) =  NX j=1  (uij) m  KX s=1  vis ` dsij ?2  +?i  KX s=1  v2is ? ?i   KX  s=1  vis ? 1  !

for i = 1, ? ? ? , C,  where Vi is the ith row of V. By setting the gradient of Ji to zero, we obtain  ?Ji(?i,Vi)  ??i =   KX  s=1  vis ? 1  !

= 0, (5)  and  ?Ji(?i,Vi)  ?vis =  NX j=1  (uij) m `dsij?2 + 2?ivis ? ?i = 0. (6)  Solving (5) and (6) for vik, we obtain  vis =  K +   2?i  NX j=1  (uij) m h D2ij/K ?  ` dsij ?2 i  . (7)  The first term in (7), (1/K), is the default value if all K feature subsets are treated equally, and no discrimination is performed. The second term is a bias that can be either positive or negative. It is positive for compact feature subsets where the partial distance is, on the average, less than the total distance (normalized by the number of feature subsets). If a feature subset is compact, compared to the other subsets, for most of the points that belong to a given cluster (high uij), then it is very relevant for that cluster.

Minimization of J with respect to U yields  uij = 1PC  k=1  ? D2ij/D  kj  ? 1 m?1  . (8)  Minimization of J with respect to the prototype parameters depends on the choice of dsij . Since the partial distances are treated independent of each other (i.e., disjoint feature subsets), and since the second term in (3) does not depend on prototype parameters explicitly, the objective function in (3) can be decomposed into K independent problems:  Js =  CX i=1  NX j=1  umij vis ` dsij ?2  , for s = 1, ? ? ? , K. (9)  Each Js would be optimized with respect to a different set of prototype parameters. For instance, if dsij is an Euclidean distance,  minimization of Js would yield the following update equation for the centers of subset s  csi =  PN j=1 u  m ijx  s jPN  j=1 u m ij  . (10)  SCADc is an iterative algorithm that starts with an initial partition and alternates between the update equations of uij , vis, and csi .



IV. IMAGE ANNOTATION After clustering, we examine each cluster, select the dominant  keywords from the textual feature subset and use them to annotate the representative visual features of the cluster. Formally, let cci , cti , and c  w i be the center of the color, texture, and keyword feature  subsets of cluster i, and let vic, vit, and viw be the feature relevance weights of these subsets. These representative features and their relevance weights are used to build a multi-modal thesaurus to annotate regions as follows. Given a test image, we first segment it into homogeneous regions. Then, for each region, Rk, we extract its color feature, Rck, its texture feature R  t k, and compare it to the  clusters? representatives using  Dik = vic ? dist(Rck, cci ) + vit ? dist(Rtk, cti), i = 1, ? ? ? , C.

Based on the distances Dik and the distribution of the clusters, several ways could be used to annotate Rk and assign a confidence value to each label. In this paper, we describe a simple crisp ap- proach that selects only one word from only the closest cluster. We also present a fuzzy labeling approach that uses fuzzy membership functions.

A. Keyword Weighting The keyword components of the prototypes, cwi , are biased  by more frequent words. Frequent words tend to be present in more clusters, and their feature values may not reflect their actual relevance within the cluster. This is a well-known problem in text document classification and categorization. The standard approach to overcome this bias is to weigh the term frequencies by the inverse document frequencies (IDF) [9]. We define the inverse cluster frequency (ICF ) of word j as  ICF (wj) = log(1 + C  Cj ), (11)  where Cj is the number of clusters that include the word wj with a significant frequency.

To reduce the bias of the more frequent words, the word frequencies in each cluster i, i.e. components of cwi , would be scaled by the ICF using  c?wi = ICF ? cwi . (12)  B. Crisp Annotation Let t be the closest cluster to region Rk, i.e.,  t = arg C  min i=1  Dik. (13)  The crisp annotation approach assigns the most relevant words in c?wi to region Rk.

C. Fuzzy Annotation Each region Rk would be assigned a fuzzy membership degree in  all clusters. We use an FCM-type membership function [16], where the membership of Rk in cluster i is computed using:  ?i(Rk) = 1PC  p=1(Dik/Dpk) 2/(m?1)  . (14)  The confidence value of assigning word wj to region Rk is computed using  Conf(wkj ) =  CX i=1  ?i(Rk)? c?wij (15)

V. EXPERIMENTS The proposed approach is validated using a subset of the Corel  image collection. We used a total of 2,695 images, where each image is labeled by 1 to 7 keywords. The keywords provide global description of the image and are not explicitly associated with specific regions. A total of 73 words were used. First, each image is coarsely segmented by clustering the color distribution.

The Competitive Agglomeration (CA)[17] was used to cluster each image into an optimum number of regions. Segmentation of all the images resulted in 13,753 regions. Second, each region is characterized by one color, one texture, and one textual feature set. The color feature consists of a 64 bin RGB histogram. The texture feature consists of one global 5-Dim edge histogram [18].

The textual feature set consists of a 73-Dim binary vector that indicates the presence/absence of each keyword. Since keywords are not specified at the region level, regions of the same image have the same textual feature set (refer to Fig. 2). Each visual feature set is normalized such that its components sum to 1.

The 13,753 feature vectors with the 3 feature subsets were clustered using SCADc. In this application, finding the optimum number of clusters (C) is not critical as long as it is large enough to avoid lumping different profiles into one cluster. Here, we report the results when C=200. After clustering, we examine each cluster, select the dominant keywords from the textual feature set, and use them to annotate the visual features of the cluster.

SCADc was successful in partitioning the data into clusters of homogeneous regions and identifying the relevant feature sets for each group. Fig. 3 displays some regions that were assigned to one of the clusters. For each region, we show the keywords that were used to annotate the images from which the region was extracted.

As it can be seen, not all words are valid. However, some of the words (in this case ?Sky?) would be more consistent across all the regions of the cluster. Consequently, these words will be the dominant terms in the textual feature set. The visual features of this cluster consist of a bluish color and smooth texture. The above cluster properties would be used to create a hybrid thesaurus.

Fig. 3. Sample regions from the ?Sky? cluster.

Fig. 4 displays samples of clusters? representatives. For each representative, we show the image of the closest region, the color feature components (color histogram), the texture feature components (edge histogram where the 5 components indicate the proportion of horizontal, vertical, diagonal, anti-diagonal, and non- edge pixels in the region), and the dominant components (keywords) of the textual features. We also show the relevance weight of each feature set.

Fig. 5 displays the inverse cluster frequency (ICF) values that are used to weigh the confidence of each word. Some words, such as sky or grass, are present in a large number of clusters, and as a result, they have a relatively low ICF.

Fig. 4. Samples of clusters? representatives with their visual and textual features.

The region annotation process is outlined in Fig. 6. The first column in this figure displays the image to be segmented and annotated, and the second column displays the segmentation results.

In the last column, we display the annotating words obtained using crisp and fuzzy labeling. For each region, we display the 5 words associated with the highest confidence values. As it can be seen, for most regions, the crisp and fuzzy labeling generate similar confidence values for the top ranked word. However, if one is interested in generating multiple annotating words for each region, the fuzzy labeling scheme tends to provide better results. This observation holds for several other images that we have tested.

Fig. 7 displays samples of test images that were segmented and labeled. For each region, we only show one annotating word that has the highest confidence value. In some instances, the second word has a confidence value that is very close to the top one.

In this case, we show both words. As it can be seen, some of these images have a good segmentation and a correct label was assigned to each region (e.g. first row). Other images do not have a good segmentation and many of their objects are fragmented. For instance, for the second image, the sky was split into 3 regions.

Two of these regions were labeled correctly (?Sky?), and one was labeled incorrectly (?Mountain? by the crisp algorithm and ?Snow? by the fuzzy algorithm).

The proposed image annotation approach does not generate the correct labels for all regions. We have analyzed several misclassified regions, and in general there are three main reasons:  ? Bad segmentation: Some segmented regions are not homoge- neous and may include parts of different objects. For instance, the last 2 images in Fig. 7, the ?grass? regions were labeled     Fig. 5. Inverse cluster frequency values for the 73 used keywords  ?Castle?. These regions included small areas (with vertical edges) from the building.

? High correlation: Some of the words are highly correlated as they were used to label many common images. For instance, in our training collection ?Sky? is highly correlated with ?buildings?, ?Plane?, ?Snow?, and ?Mountain?. As a result, some regions may be incorrectly labeled by any of the above words.

? Indiscriminative visual features: Different regions can have similar color and texture features. For instance, for the second image in Fig. 7, the body of the deer has similar color and texture to several ?Rock? and ?Dirt? regions in the training collection.

The above cases could be reduced by using a larger and a more diverse image collection, and by using more discriminative visual and shape features.



VI. CONCLUSIONS We have presented an unsupervised approach that extracts rep-  resentative visual prototypes from large collections of images and annotates them. These labeled prototypes, along with the relevance weight for each feature subset, could be used to build a multi- modal thesaurus that could be used to annotate new images. We have presented a crisp and a fuzzy labeling scheme. The fuzzy labeling is more reliable if one is interested in using multiple words to label each region. Future work will include the construction of  Fig. 6. segmented image with crisp and fuzzy region annotation.

a much larger thesaurus and extension of the labeling process to include the clusters? validity measures.

