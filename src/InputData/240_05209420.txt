A New Algorithm For Frequent Itemsets Mining

Abstract   Frequent itemsets mining plays an important role  in association rules mining. The apriori algorithm and the FP-growth algorithm are the most famous algorithms, existing frequent itemsets mining algorithms are almost improved based on the two algorithms respectively and suffer from many problems when mining massive transctional datasets. In this paper, a new algorithm named APFT is proposed, it combines the Apriori algorithm and FP-tree structure which proposed in FP-growth algorithm. The advantage of APFT is that it dosen?t need to generate conditional pattern bases and sub- conditional pattern tree recursively. And the results of the  experiments show that it works faster than Apriori and almost as fast as FP-growth.

1. Introduction   Association rule mining is one of the most important data mining problems. The purpose of association rule mining is the discovery of association relationship among a set of items. The mining of association rule include two subproblems(1)finding all frequent itemsets that appear more often than a minimum support threshold, and(2)generate association rules using these frequent itemsets. The first subproblem plays an important role in association rules mining.

A number of algorithms for mining frequent itemsets have been proposed after Agrawal first introducing the problem of deriving categorical association rule from transactional databases in [1].These existing algorithms can be categorized into two classes: the candidate generate-and-test approach and the pattern growth approach. The first class algorithms, such as Apriori[1, 2] as well as many  subsequent studies[3,4,5]. In each iteration of the candidate generate-and-test approach, pairs of frequent k-itemsets are joined to form candidate (k+1)-itemsets, then scanned the database to verify their supports. The Apriori algorithm achieves good reduction on the size of candidate sets, however?it takes many scans of the database to check the candidates?s supports as much as the most long length of patterns, so, when there exist a large number of frequent patterns and/or long patterns, candidate generate-and-test approach may suffer a large overhead of I\O. The second class comprises pattern-growth methods, over the past few years, several pattern-growth algorithms have been proposed, such as FP-growth[6] ,Tree-projection[7] ,H-Mine[8] and COFI[9]. A pattern-growth algorithm uses the FP- tree to store the database, instead of generating candidates, it mining the FP-tree recursively by building conditional trees that are of the same order of magnitude in number as the frequent pattern.

Compared to the first class approach, the second approach is more efficient but needs more memory to story the intermediate data structure. This massive creation of conditional trees makes this algorithm not scalable to mine large datasets beyond few millions[6].

Aim to overcome the limitation of the two approaches forementioned, a new method named APFT which combines the Apriori algorithm and FP- tree structure is proposed. At the beginning it constructs  a FP-tree like what FP-growth does,  then to every one of the items in the header table, we find all branches that include the item and generate the candidate itemsets including the item by using of the candidate-generate method as Apriori, at last  scan the corresponding branches to calculate the support of those candidates. The results of our experiments show that it works faster than Apriori and almost as fast as FP-Growth.

The rest of the paper is organized as follows: Section 2, formally introduces the problem and briefly reviews the Apriori method and FP-tree structure.

Global Congress on Intelligent Systems  DOI 10.1109/GCIS.2009.387     Section 3 proposes a new algorithm. Experimental results are presented in Section 4. Section 5 concludes this paper.

2. problem statement   In this section, we first introduce the problem, then review the Apriori method and FP-tree structure.

2.1 Mining frequent itemsets   Let I = {x1, x2, x3 ? xn} be a set of items. An  itemset X that is also called pattern is a subset of I, denoted by X ? I. A transaction TX = (TID, X) is a pair, where X is a pattern and TID is its unique identifier. A transaction TX is said to contain TY if and only if Y ? X. A transaction database, named TDB, is a set of transactions. The number of transactions in DB that contain X is called the support of X. A pattern X is a frequent pattern, if and only if its support is larger than or equal to s, where s is a threshold called minimum support.

Given a transaction database, TDB, and a minimum support threshold, s, the problem of finding the complete set of frequent itemsets is called the frequent itemsets mining problem.

2.2 Apriori algorithm   Agrawal[1,2] firstly proposed the Apriori algorithm,  The Apriori algorithm is the most well known association rule algorithm and is used in most commercial products. The use of support for pruning candidate itemsets is guided by the following principles.

Property 1: If an itemset is frequent, then all of its subsets must also be frequent.

Property 2: If an itemset is infrequent, then all of its supersets must also be infrequent.

The algorithm initially scans the database to count the support of each item. Upon completion of this step, the set of all frequent 1-itemsets, F1, will be known.

Next, the algorithm will iteratively generate new candidate k-itemsets using the frequent (k-1)-itemsets found in the previous iteration. Candidate generation is implemented using a function called Apriori-gen. To count the support of the candidates, the algorithm needs to make an additional scan over the database.

The subset function is used to determine all the candidate itemsets in Ck that are contained in each transaction t. After counting their supports, the algorithm eliminates all candidate itemsets whose support counts are less than minsup. The algorithm terminates when there are no new frequent itemsets  generated .The Apriori algorithm can be written by pseudocode as follows.

Algorithm 1. Apriori[2] Input: data set D, minimum support minsup Output: frequent itemsets L 1.  k=1; 2. {i | i I ?({i}) N minsup}Fk = ? ? ? ? 3.  repeat 4.     k = k+1; 5.     Ck = apriori-gen(Fk-1); 6.     for each transaction t?T do 7.         C t = subset(Ck, t); 8.         for each candidate itemset c?C t do 9.             ?(c)=?(c) 1+ ; 10. {c | c ?(c) N minsup}CFk k= ? ? ? ? ; 11.  until  FK = ? 12.  return  L = ?FK ; Procedure apriori-gen Input: (k-1)-frequent itemsets Output: k-candidate itemsets 1.  for each itemset p ?Fk-1  do 2. for each itemset q ?Fk-1  do 3.        if p.iteml = q.item1?p.item2 = q.item2? ? ? p.item(k-2) = q.item(k-2) and p.item(k-1) < q.item(k-1) 4.                 c = p ? q; 5.                if has_infrequent_subset(c, Fk-1 )  6.                              delete c; 7.                else add c to Ck ; 8.    return  Ck;  2.3. FP-tree   Han et al. developed an efficient algorithm, FP- growth, bases on FP-tree. It mining frequent itemsets without generating candidates, this approach scans the database only twice[6]. The first scan is to find 1- frequent itemset, the second scan is to construct the FP-tree. The FP-tree has sufficient information to mine complete frequent patterns, it consists of a prefix-tree of frequent 1-itemset and a frequent-item header table in which the items are arranged in order of decreasing support value.

Each node in the prefix-tree has three fields: item- name, count, and node-link.

item-name is the name of the item.

count is the number of transactions that consist of  the frequent 1-items on the path from the root to this node.

node-link is the link to the next same item-name node in the FP-tree.

Each entry in the frequent-item header table has two fields: item-name and head of node-link.

item-name is the name of the item.

head of node-link is the link to the first same item-  name node in the prefix-tree.

Algorithm 2. FP-tree construction[3] Input: A transaction database TDB and a minimum support threshold ? .

Output: Its frequent pattern tree, FP-Tree 1. Scan the transaction database DB once. Collect the set of frequent items F and their supports. Sort F in support descending order as L.

2. Create the root of an FP-tree, T, and label it as ?null?, for each transaction in TDB do the following.

Select and sort the frequent items in transaction according to the order of L. Let the sorted frequent item list in transaction be [p|P], where p is the first element and P is the remaining list. Call insert-tree ([p|P,T].

Function insert-tree ([p|P], T) 1. if T has a child N such that N.item-name = p.item- name 2.     then increment N?s count by 1; 3. else do 4.      create a new node N; 5.      N?s count = 1; 6.      N?s parent link be linked to T; 7.    N?s node-link be linked to the nodes with the same item-name via the node-link structure; 8.  if  P is nonempty, Call insert-tree (P, N);  An example of an FP-tree is shown in Figure 1.

This FP-tree is constructed from the TDB shown in Table 1. with minsup = 3. In Figure 1., every node is represented by (item - name : count) . Links to the next same item-name node are represented by dotted arrows.

Table  1. Sample TDB TID Transaction Frequent Items 001 f,a,c,d,g,i,m,p f,c,a,m,p 002 a,b,c,f,l,m,o f,c,a,b,m 003 b,f,h,j,o f,b 004 b,c,k,s,p c,b,p 005 a,f,c,e,l,p,m,n f,c,a,m,p       Figure 1. Example of FP-tree  3. AFPT Algorithm   In order to avoid generating candidates, FP-growth algorithm needs to construct a large of conditional FP- tree, it may let the mining process failure when the database is sparse or there are a lot of frequent patterns that result in the memory requirement overstep the main memory. Therefore, we consider using the apriori method to mining the frequent itemsets basing on the FP-tree, the divide-and-conquer strategy is still adopted by mining process. That  is to say, the compressed FP-tree is patitioned off a set of conditional subtree , each of the conditional subtree associated with a frequent item.If there are n 1-frequent items Ii(i=1,2,?.n), then the FP-tree can be divided into n conditional subtree FPTi (i=1,2,?.n) , and FPTi is the conditional subtree associating with frequent item Ii .Then use the apriori algorithm to mine each conditional subtree, and gain all the frequent itemsets with the first prefix item Ii. The AFPT algorithm includes two steps, the first step is to construct the FP- tree as FP-growth does, the second step is to use of the apriori algorithm to mine the FP-tree. On the second step, it is needed to add an additional node Table, named NTable, each entry in the NTable has two fields: Item-name, and Item-support.

Item-name: the name of the node appears in the FPTi,  Item-support: the number of the node appear with Ii The pseudocode of the APFT algorithm is described  below.

Algorithm 3. APFT Input: FP-tree, minimum support threshold ? Output: all frequent itemset L 1.  L = L1; 2.  for each item Ii in header table, in top down order 3. LIi = Apriori-mining(Ii) ; 4.  return  L = {L?LI1 ?LI2???LI n}; pseudocode Apriori-mining(I i )     1?Find item p in the header table which has the same name with Ii ? 2.   q = p.tablelink; 3.   while q is not null 4.      for each node qi != root on the prefix path of q 5.         if NTable has a entry N such that N.Item-name = q i.item-name 6.     N.Item-support = N.Item-support + q.count; 7.            else 8.    add an entry N to the NTable; 9.    N.Item-name = q i. item-name; 10.    N.Item-support = q.count; 11     q = q.tablelink; 12.    k = 1; 13.    Fk = {j | j?NTable?j.Item-support?minsup} 14.    repeat 15.     k = k + 1; 16.     Ck = apriori-gen(Fk-1) ; 17.             q = p.tablelink; 18.     while q is not null 19.                  find prefix path t of q 20.                  Ct = subset(Ck, t); 21.           for each  c?C t 22.               c.support = c.support + q.count; 23.                   q = q.tablelink; 24.     Fk = {c | c?Ck ? c.support ? minsup} 25.     until Fk = ? 26.   return  LI i = Ii ? F1 ? F2 ??? Fk     // Generate all frequent itemsets which  with  Ii as the prefix item.

To explain the algorithm ,we use an example with the transaction database showed in Table 1. The FP- tree of this database is showed in Figure 1. The mining process begins from the top of the header table, and moves toward the bottom. For the f-node, it has only one prefix path and the prefix path has no other node except root-node, so there is no frequent itemset has the first prefix item with f. For the next c-node in the header table, it has two prefix pathes{f}, and {root}, f.support = c.count = 3 = minsup, there is no other requent item in c?s prefix, so we gain the frequent itemset {cf } with support 3, the process of mining the frequent itemsets with the first prefix item c terminates. Next for the a-node, it has one prefix path: cf, and c.support  = f.support = a.count = 3, so item c and item f are 1-frequent item, generate 2-candicate cf by joining c and f, then generate two-subset of a?s prefix path, here just is cf, and (cf).support=a.count=3=minsup, so itemset cf is frequent, finally ,we get the prequent itemsets {ac:3,af:3,acf:3}. The remain mining processes are similar.

4. Experimental Results   In order to verify the performance of the APFT algorithm, we compare it with Apriori and FP-Growth.

Three algorithms are performed on a computer with a 1.41GHz processor and 512MB memory,running windows xp. The program is developed by Visual C++ 6.0. We present experimental results using three databases. Database bankcard is bank card transactions seized from bank of China. Database mushroom and database T10I4D100K can be found from http://fimi.cs.hesinki.fi/data/. Some characteristics of these databases are showed in Table 2. The experimental results are showed in Figure 2, Figure 3 and  Figure 4 respectively. As show as these Figures, APFT is more super than apriori ,because it dosen?t need to generate 2-candidate itemsets and reduce the search space, and APFT performences almost as fast as FP-growth, but when the minsup is low, AFPT runs faster than FP-growth, because in the case , FP-growth needs  to  construct  a large of conditional subtrees , it is  not only time-consuming but also  high memory cost, APFT dosen?t need to much extra spaces on the mining process, so APFT has a better  space scalability.

Table 2. Database characteristics     Figure 2. Bankcard    Database Items Records Max|T| Avg|T|  Bankcard 27 50905 13 6 Mushroom 119 8124 23 23 T10I4D100K 870 100000 29 10      Figure 3. mushroom   Figure  4. T10I4D100K   5. Conclusion   In this paper, a new algorithm is proposed which combined Apriori algorithm and the FP-Tree structure.

The experimental results shows that this new algorithm works much faster than Apriori and as fast as FP- Growth. It works little faster than FP-Growth when the support threshold is small. The future work is to optimize the technique for counting the support of the candidates and expand it for mining more larger database.

6. Acknowledgments   This work was supported by the National Nature Science Foundation of China (Grant no. 60773126) and the Province Nature Science Foundation of Fujian (Grant no. A0710023) and academician start-up fund (Grant No. X01109) and 985 information technology fund (Grant No. 0000-X07204) in Xiamen University.

7. References  [1] R. Agrawal, T. Imielinski, and A. Swami. Mining association rules between sets of items in large databases. In Proc.1993 ACM-SIGMOD Int. Conf. Management of Data, Washington, D.C., May 1993, pp 207?216   [2]  R. Agrawal and R. Srikant. Fast algorithms for mining association rules. In VLDBY94, pp. 487-499.

[3]  J.S. Park, M.S. Chen, and P.S. Yu. An effective hash- based algorithm for mining association rules. In SIGMOD1995, pp 175-186.

[4]  .J. S. Park, M. S. Chen, and P. S. Yu. An effective hash- based algorithm for mining association rules. Proceedings of Data, San Jose, CA, 1995,pp175-186.

[5]  A. Savasere, E. Omiecinski, and S. Navathe. An efficient algorithm for mining association rules in large databases.

large Database,1995.

[6]  J. Han, J. Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation(PDF), (Slides), Proc. 2000 ACM-SIGMOD Int. May 2000.

[7]  AgarwalR,AggarwalC,Prasad V V V.A treeprojection algorithm forgeneration offrequentitemsets. In Journalof Paralleland Distributed Computing (SpecialIssueon High PerformanceDataMining),2000.

[8]  J. Pei, J. Han, and H. Lu. Hmine: Hyper-structure mining of frequent patterns in large databases. In ICDM, 2001, pp 441?448.

[9]  Mohammad El - Hajj and Osmar R Za?ane. COFI Approach for Mining Frequent Itemsets Revisited?C?? 9th ACM SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery ? DMKD - 04 ?? Paris?France?June 2004.

