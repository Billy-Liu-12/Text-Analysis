Full Recognition of Massive Products Based on Property Set

Abstract? In recent years, with the development of e-commerce, the number of online products increases rapidly. Faced with such a mass of data, we need to establish an efficient unified product standard. However, in terms of the whole e-commerce industry there lacks a unified standard which can coordinate with existing standards. In this paper, we put forward an effective solution to unifying products, which is then applied to business intelligence data analysis. The main work and contributions are as follows: 1) an effective solution to unifying product data; 2) a parallel data mining algorithms which solves the problem of identifying similar products from massive product data; 3) the framework is universal, although our project is based on Taobao?s data cloud, it can also be applied to other e-commerce area.

Keywords-Product Recognition; Unified Product Standard; Parallel Computing;  Clustering Algorithm

I.  INTRODUCTION In recent years, the number of online products increases  fast. As china?s largest online shopping website, Taobao[1] has over a billion of products and the quantity is still growing everyday at an astonishing speed. In the face of such a large number of data, how to guarantee efficient operations for online shopping sites has become a hot issue to be addressed. And product information standardization is one of the key approaches to guaranteeing stable and effective operations of e-commerce.

In industry community, product barcode is usually used to solve the problem of standardization of products, namely GTIN (Global Trade Item Number). The products which have the same GTIN code are consistent. Referring to the solutions of standardization in industry community, e- commerce introduces the concept of barcodes for online products, which can provide standardized information and reduce duplicate data. Some e-commerce sites have realized the products bar-coded. For example, Amazon requires sellers to provide UPC, EAN, ISBN, or ASIN (Amazon's own system for goods unique identifier) when they upload product information. All the information is standardized.

Some large B2C electronic companies in China, such as JingDong, DangDang, have proposed their standards of product unification respectively. In terms of JingDong mall, the products have been classified according to their categories hierarchically, and then a unified standard has been established for the final subclasses in every type of  products. As in the subclass of ?computer, office" - "computer" - "the notebook", the products have to be described uniformly according to the criteria of "brand and model, size and type".

However, most e-commerce sites like Taobao are still lack of an effective mechanism to ensure the standardization of products. Due to the lack of standardization, although many sellers provide the same product (such as LenovoY460), a large number of similar and redundancy descriptions exist. When users search a product, a lot of pages showing the same product owned by different sellers will appear. Users need to inquire related information in each page and compare the prices by themselves, which make the acquisition of information confused and complex.

At present, most of the academic studies for online products concentrate on the hidden relationships between consumer behavior and the consumption [2-7]. However, general data analysis mechanism will struggle to cope with such a large number of data. Even though the association mining algorithms, such as Apriori [8] and FP-Tree [9], are simple and effective, they are still difficult to apply to a large scale of data.

At present, in industry community, the data mining of online products are usually applied to product recommendation. On the research and practical application, Amazon takes the leading position. It combines a variety of service recommendation approaches, such as recommendation based on item similarity [10] and correlation, based on browse/purchase history, and based on collaborative filtering [11]. However, how to identify whether two items are similar or not, and how much is the similarity? For a mass of product data, how to find similar products efficiently? There are a lot of problems in academic and industry communities to be addressed.

In order to solve the data confusion caused by inconsistent identification for the same product, in this paper, we propose an approach to recognizing similar products from massive dataset based on property set, which cluster similar products according to their properties based on the categories, and employs distributed solution to deal with large-scale data. The proposed solution can reduce the repetition rate of the same information of products and the redundancy of data storage, and can also be applied to data  2013 IEEE International Congress on Big Data  DOI 10.1109/BigData.Congress.2013.46     analysis service in business intelligence, such as recommendation, sales statistics and etc.

The rest of this paper is organized as follows: clustering algorithms related to this paper are summarized in section 2.

Section 3 introduces the product clustering algorithm and its parallel solution based on the proposed definition of product similarity. Section 4 is the experiments and results, which shows the effectiveness of the proposed parallel mean-shift algorithm for massive data processing. Section 5 is the conclusion and future work.



II. RELATED WORK The section will review the clustering algorithms in data  mining. The clustering technology is employed in full recognition of products in this paper. There are a lot of general classical algorithms of clustering, and we will introduce the application of several mainstream algorithms.

Traditional clustering methods adopt the Euclidian geometric distance as the measure of similarity. Adopted connection rules include single connection rule, full connection rule, average connection rule and Ward method [12].

Partition clustering algorithms include K-means and a series of its evolving ones. Macqueen put forward K-means algorithm for the first time in 1967[13]. The main idea of the algorithm is to find K centroids c1, c2, ... ck to make the sum of square distances between every data point and its nearest centroid ci minimized. The disadvantage of K-means is that the classification usually ends when a local optimum is achieved, and it is only suitable for classifying numeric data.

Huang and etc. [14] put forward K-modes, a clustering algorithm suitable for classifying attributes in 1998.  The K- modes algorithm uses a simple dissimilarity measure to deal with classified objects, and a frequency-based method to update modes in the clustering process to minimize the cost function. But the K-modes algorithm still has the same problem that the classification ends when a local optimum is achieved.

In 2001, Chaturvedi and etc. [15] put forward a nonparametric clustering method aiming at interval scale data, namely K-modes-CGC algorithm. It is different from most of the existing methods for clustering nominal scale data, K-modes explicitly optimize a loss function based on the L0 form.

In 2004, Ding et al. [16] proposed an algorithm named K-means-CP. The nearest neighbor consistency is an important concept in statistical pattern recognition, and they extend the notion to data clustering. For any data point in a category, the k-nearest neighbor and the k-mutual nearest neighbor are required in the category. They proposed to regard the k-nearest neighbor consistency as an important quality measurement for data clustering as well as an improved algorithm of KNN and KMN.

Another category of clustering algorithms is grid-based clustering. In 2001, Zhao[17] et al. proposed a clustering algorithm based on density isoclines graph of grid (GDILC).

The main idea of GDILC is the data sample distribution depicted by density isoclines graph. GDILC is able to  discover various shapes of clusters. It is a kind of unsupervised clustering algorithm, since it does not need interaction. Ma and etc.[18] proposed a new density- and grid-based type clustering algorithm using the concept of shifting grid in 2004. It divides each dimension of the data space into certain intervals to form a grid structure in the data space. Based on the concept of sliding window, shifting of the whole grid structure is introduced to obtain a more descriptive density profile. Compared with other classical algorithms, the algorithm is efficient because it clusters data in a way of cell rather than in point. Pilevar and etc.[19] proposed a new clustering method called GCHL(a Grid- Clustering algorithm for High-dimensional very Large spatial databases) in 2005. It combines a novel density-grid based clustering with axis-parallel partitioning strategy to identify areas of high density in the input data space. The authors demonstrate the high quality of the obtained clustering solutions, capability of discovering concave/deeper and convex/higher regions, robustness to outlier and noise as well as scalability. Briant and etc.[20] present a new density-based clustering algorithm, ST- DBSCAN, which has three marginal extensions to DBSCAN related to the identification of core objects, noise objects and adjacent clusters. The algorithm is able to discover clusters according to non-spatial, spatial and temporal values of the objects.

Mean-shift [21] is a density-based clustering algorithm. It is an iterative procedure, in which a mean shift value is computed for current point, and then move the point to the mean value, and start moving from the new start point until certain conditions are satisfied. The mean-shift algorithm has been applied in the area of visual tracking and image processing. The algorithm is easy to realize, and the computation amount is small with high real-time response.

Hence in this paper, we apply the Mean-shift algorithm and improve it with parallel framework to realize full recognition of massive online products.



III. FULL RECOGNITION OF MASSIVE PRODUCTS  A. Definition of Product Similarity  Definition 1. Online Product. There are various kinds of information involved in an online product description, for example, the title, detailed introduction, the information of the seller, and the price. Here we only focus on the three most important elements and describe a product as a triple (ID, category, properties). In the triple, ID is the identifier of the product, the same products sold by different sellers will have different IDs while the same products sold by one seller have the same ID; category is the catalog that a product belongs to, for example BlackBerry 9000 belongs to digital appliances and its sub-category cell-phone. Here we only record the last level of categories, i.e. the key category.

The properties are the property set for describing a product, and its definition is given as follows.

Definition 2. Property. Property is the concise and pivotal information for describing a product. Each property is a pair that consists of a property name and corresponding value. A product can be described by the properties such as color, size, shape, and material. For example, the properties for describing BlackBerry 9000 can be (appearance pattern: bar phone) and (memory size: 256M).

Definition 3. Property Set. Property set is the combination of several property pairs, recorded as {(property_name1: property_value1), (property_name2: property_value2), ?, (property_nameN, property_valueN)}. The whole properties constitute the property set of a product. For example, the property set of BlackBerry 9000 is {(brand: BlackBerry), (model: 9000), (appearance pattern: bar phone), (memory size: 256M), (pixel of camera: 320w)}.

Definition 4. Similarity of Property Set. The formula for the similarity between the property sets of two products is shown as formula 1:  Similarity of Property Set =   PrPr  PrPr  cc  cc  opertyoperty  opertyoperty  ?  ?   (1)  In formula 1, the intersection of two property sets is defined as formula 2:  )12(*)( 12PrPr 21 PNPNcc opertyoperty ???? (2)  in which, )( 12PN  is the number of properties that are the same both in property name and property value for two products 1 and 2, )12(PN  is the number of properties that are the same in property name but different in property value for two products 1 and 2, while ? is an adjusting weight which is usually determined by experience and its range is [0,1]. It shows that the same property names have positive effect on property set similarity but the effect is not very much.

In formula 1, the union of two property sets is defined as formula 3:  )21()21()12()( 12PrPr 21 PNPNPNPNcc opertyoperty ????? (3)  in which, the definitions of )( 12PN  and )12(PN  have been  given, while )yx(PN is the number of properties that are contained in the property set of product x but not in that of product y.

Definition 5. Standard Product Unit (SPU). SPU is a set of standard information that can be reused and searched as keywords. Different from a general property set, SPU is the most representative property set of a product. For example, the SPU of BlackBerry 9000 is {(brand: BlackBerry), (model: 9000)}, and the ISBN of a book can be deemed as its SPU.

The products with the same SPU values can be deemed as the same product. However, in Taobao, the SPUs of most products are not easy to retrieve, for example, the food and clothes. It is very hard to find a clear key property set such as brand and model for these products, so the SPU values of these products are missing, and this is why many products in Taobao cannot be classified at present. Since SPU can identify products, we use SPU as the verification standard, and we extract the products with SPU values to test the precision of the products recognition with formula 1.

We do not require the similarity to be exactly 1.0 to identify two products as the same, since our approach is based on rough statistical information rather than the precise GTIN.

We can set a threshold value, and once the similarity is above the value, we can identify the two are the same.

Definition 6. Precision. Precision is N(similarProduct)/N(allProduct), in which N(similarProduct) is the number of products that are similar to a benchmark with a similarity value above the threshold, while N(allProduct) is the number of products that have the same SPU values with the benchmark.

B. Parallel Full Recognition of Massive Products The full recognition of products is to classify products  with high similarity into a cluster while keep products with low similarity into different clusters. The full recognition of products is similar to the clustering in data mining, hence we can refer to clustering algorithm to achieve full recognition of products and limit the number of final clusters and precision by setting the threshold of similarity. Different from the general clustering, each product belongs to a specific category in e-commerce websites, so recognition of products can be processed in each category, rather than identifying products in different categories, so as to reduce computation amount.

The process of full recognition of products can be roughly divided into three steps: 1. preprocess each datum of online products and assign each product to an initial cluster; 2. apply mean-shift algorithm to combine clusters with high similarity repeatedly; 3. for each product, find the nearest centroid of clusters by comparing the similarity between the product and each centroid of clusters, and then mark the product with the centroid. Each step corresponds to an     individual map/reduce operation, and the detailed process is shown as Fig. 1. The parallel solution to each step will be given in the sub-sections.

Mapper1 Input: item For item in itemList:  find its nearest centroid nc if(similarity(item,nc))>threshold  update the property set of the cluster and the centroid  Output: updated cluster  Mapper2 Similar to Mapper1  ? MapperN Similar to Mapper1  Reduce Combine similar clusters and  output all the clusters  Convergent or exceeds maximum iteration number  yes  no  Mapper1 Input: item For item in itemList:  find its nearest centroid nc if(similarity(item,nc))>threshold  update item Output: item with marked category   Figure 1. Process of Full Recognition of Products    B.I  Data Preprocessing and its parallelization  The first step is data preprocessing. We need to eliminate irrelevant information from product data and just keep that will be used in clustering. The information used in clustering mainly includes the ID, category and property set of a product. The map/reduce operation for this step is relatively simple. In the map process, we need to filter the input data and keep the information we need, making the category as the key, the processed product data as value. In the reduce process, the data in the same category will be write into a file.

One thing that needs to note is, the number of categories is more than 10,000, if an individual file is generated for a category, and the number of small files will be too large to be supported by the Hadoop file management system. Hence we can adjust the number of files to a specific range and here we use 300 in the experiment. To achieve it, we can hash the category into the range of 1 to 300. The category is represented as an id in the form of integer, therefore the data in the categories with the same value of categoryid%300 will be written into a same file and the file can be named as the result of categoryid%300. In this way, a file will store product data of several categories, and if we need to search information in a specific category, we can locate the file by the value of categoryid%300. The number of generated files can be adjusted according to the capability of category management system.

B.II  Clustering Generation by Mean-shift and its parallelization  The second step takes the output of 1st step as input and combines the clusters obtained in the 1st step repeatedly. In the map process, the clusters assigned to current machine are updated continuously, and the detailed process is as follows: maintain a list of clusters for each category in current machine, and initialize the lists as null. For each newly scanned cluster namely cluster_1, compute the similarity between the centriod of cluster_1 and that of all the other clusters in the same category. If the similarity is larger than the threshold t1 (0<t1<1, here t1 is set to 0.9), cluster_1 will be merged into the corresponding cluster (assuming is cluster_2), and meanwhile the property set of cluster_1 is merged to that of cluster_2 as well. If the similarity is larger than t2 (set manually and typically t2<t1, here t2 is set to 0.7), the property set of cluster_1 will be added into the referenced property set of cluster_2 and the property set of cluster_2 is added into the referenced property set of cluster_1. Finally, the output is making category as the key, the corresponding cluster list as the value. The process is to combine clusters with high similarity, and combine the property set of a cluster with that of clusters of relatively high similarity. It is in fact the mean value of shift in the mean-shift algorithm.

In the reduce process, the cluster lists for the same category from different machines will be merged together. It checks all the clusters in each category, and follows the same rule of the map process, that is, combining the clusters with similarity larger than t1, and adding the property set of clusters to each other with similarity larger than t2. When all the clusters in a category have been checked, compute the difference between the centroid generated by the referenced property set of each cluster and that generated by the property set, if the difference is not large (usually less than 0.1), the cluster is deemed as convergent. Otherwise, the property set of the cluster will be updated with the referenced property set as a shift, and the ID, category, property set and convergence of the cluster will be write into the file categoryid%300. The un-convergent categories will be shifted and continued with the next iteration until the clusters in the category are all convergent or the number of iterations exceeds the maximum times.

For the convenience of the next step, the centroids of each cluster will be computed as well, so the final output of the 2nd step is cluster id, category and the centroid.

B.III  Marking a Product with Cluster ID  In the third step, compare each product with the centroid of clusters in corresponding category by computing the similarity between the property set of the product and that of the centroid using formula 1. Find the largest similarity above the threshold and then mark the product with the cluster id which the centroid belongs to. If such centroid cannot be found, the cluster id is marked as -1. There is only one map operation needed, which reads the product data, searches the centroids in the same category, and compares     their similarities. The output of the map operation is the product data marked with a cluster id.



IV. EXPERIMENTS In this section, we test the accuracy of our method and  the efficiency of parallel solution by a set of experiments. All experimental data are from Taobao.

A. Setting of Threshold We select several products with SPU values and conduct  a series of experiments to obtain an appropriate threshold value by comparing the precision of product recognition in different threshold. In the experiment, we select 10 benchmarks randomly for each product (such as Nokia 5230) to test the precision and the final result is the average precision. Fig.2 shows the result of some representative products.

Figure 2. The precision of Product Recognition under different similarity  threshold  Fig.2 shows four representative products namely iphone4, ThinkPad E50 (031934C), Nokia5230 and Canon IXUS115.

From Fig.2 we can see that, the precision increases as the threshold increases. But when the threshold reaches 0.9, the precision of Nokia5230 and Canon IXUS115 increases little, while that of iphone4 and ThinkPad E50 (031934C) becomes descending. The result is similar for other products. This is because that in formula 1, the denominator is changed little (according to the realistic result, for most similar products, their similarities are usually above 0.95), while the numerator decreases accordingly (the product with similarity between 0.9 and 0.95, its SPU value may be different from that of benchmarks), hence the precision result decreases.

According to the experiment result, we set the similarity threshold to 0.9, that is, when the similarity of two products is above 0.9, they are deemed as the same one.

B. Time Efficiency In order to observe the change of computing time of our  proposed parallel solution for different data size, we pick up several groups of typical data at random in Taobao?s  hundreds of millions of products to test the time efficiency.

The unit of the data is based on the number of products, respectively 1000, 5000, 10 thousand, 50 thousand, 100 thousand, 500 thousand, 1 million, 2 million, 4 million, 6million, 8 million. Each group of the data set is chosen randomly for six times and each has an independent experiment, after which an average time of each set is computed. Our experimental results are shown in Fig.3.

Figure 3. Time cost of full recognition of products for different data  size   In Fig.3, 1000 products are regarded as a unit. The  experiment tests the time efficiency of the parallel clustering algorithm in different product data sets. In this figure, the unit of y axis which represents time is minute. From the figure we can see that, if the number of products is 1000, the computing time is about 5minutes. With the increase of the quantity of product, the time is also increasing. When the quantity is more than 2,000,000, the time increase begin to slow down. In fact, when the quantity of the products is small, the advantages of parallel computing can hardly be reflected. Using a single machine to deal with 1000 pieces of products may not be bad than parallel computing, because three process of parallel computing need to consume time.

But when the quantity of the product is increasing, the advantage of parallel computing is obvious. In fact, when the quantity of products is more than a certain number, the performance of a single machine is intolerable. Besides, a memory overflow may come into being. In the procedure of 3.1, preparation work and writing file of several Map- Reduce calculation take up a certain time, and the cluster computing itself has only takes a part of time. In addition, with the increase of data size, different products and categories join in, which means more computing resources (distributed computing machines) join in and the distribution of products becomes better partitioned. Therefore, when the number of products increases, the time increase is not proportionate to the quantity of products. When the quantity of the products is 8,000,000, the running time is about 95 minutes.

C. Statistics of Product Recognition In order to test the accuracy of product recognition, we  conduct experiments on dataset with different data sizes, and summarize some related statistics results. We conducted 5 groups of experiments based on category. The numbers of selected categories in 5groups are 2, 4, 8, 16 and 32, respectively. In each group of experiments, a certain number of products to be recognized are chosen randomly from the selected multiple categories. Each group of experiments is conducted 6 times independently, that is, select the same number of products randomly in the same categories for 6 times, and the statistical result is the average value. The result is shown as Table 1.

Table 1. Statistics of Full Recognition of Products for different data size  Number of goods(10,000)  Statistics  1.6 12 100 400 800  Number of categories  2 4 8 16 32  Number of clusters  1312 4487 15500 74415 200540  Average number of  commodites in a cluster  12 27 65 54 40  The largest number of  products in a cluster  5412 10955 25493 35981 72285  The nuber of commodites  with cluster id as -1  5 17 1426 2397 3682    Table 1 shows the number of generated clusters, the cluster with the largest number of products and the number of products which cannot be classified to any cluster after product recognition.  We can see that, when the number of categories and products increases, the number of generated clusters increases as well, while the amplification of products in one cluster is limited, the average number of  products in a cluster becomes descending even when the total number of products reaches 4,000,000. It is because that the number of products increases fast while the number of categories increases slowly, therefore the average number of products in a category is increasing, the possibility of more types of products leads to more clusters to be generated, and the average number of products in a cluster decreases.

The numbers of products with cluster id as -1 in the 5 groups of experiments are not many. Generally speaking, a  product will not be sold by only one seller except some wired ones. In our proposed framework, the clustering of products depends both on the description of a product and the setting of similarity threshold.  When the description of a product is not detailed or the threshold is set too high, some products will not be able to be classified to an appropriate cluster. However, from the experiments we can see that the times of exceptions occurs a little, which can be acceptable.

The selected category is the key impact factor to the largest number of products in a cluster, since products in some category are easy to be sold out while those in some unpopular category are not. For example, the largest number of products in a cluster is 72285 for 8,000,000 products. The statistical results of ranking clusters by the involved products are of application value. In order to show its importance, table 2 shows the top 5 products in the two categories namely cell-phone and notebook.

Table 2. The top 5 clusters with most products in two categories ?cell- phone and notebook?(data size is 8,000,000)  ranking category product  1 Cell-phone Google G3 HTC  2 Nokia 5230  3 Motorola A3100  4 iPhone 4  5 Nokia N82  1 Notebook ThinkPad T420  2 Asus N61  3 Dell Inspiron  4 HPCQ42  5 Lenovo Y470    We count the top 5 clusters with most products in two categories which are cell-phone and notebook. We can see that by ranking the clusters in the cell-phone category according to the number of involved products from large to small, the top 5 clusters are Google G3 HTC, Nokia 5230, Motorola A3100, iPhone4, Nokia N82;  the top 5 clusters in the notebook category are ThinkPad T420, Asus N61, Dell Inspiron, HPCQ42, Lenovo Y470.  We searched these products in Taobao and found that they are indeed bestsellers.

We can understand it since a seller may join into selling a product with a good sales volume. In addition, the experiments show the application of product recognition. For     example, the result of product recognition can be used to count the number of sellers who sell a specific same product, or the sales volume of a certain product by all sellers to show its popularity. From the perspective of data storage, we can generate the property sets of cluster centroids as standard property sets, so that product data can be standardized.

D. Accuracy of Product Recognition In order to test the accuracy of product recognition, we  take samples based on the data of 8,000,000 clustered products. We take a cluster in each of the four categories, and in each cluster three products will be compared by pictures. In order to be more persuasive, we arrange the difficulty of identification of the four categories step by step, from easy to difficult, respectively is Nokia5230, Nike package, wire products for maternal and children and Adidas sport kit.

Figure 4. Nokia 5230 cellphone    Figure 5. Nike Waist Bag    Figure 6. Collision belt for Children      Figure 7. ADIDAS Sports Suit    By comparing four groups of pictures, we can see that in Fig. 4 Nokia5230 is a hot one and easy to be identified as the same product, as these three pictures are similar, only with different shooting angles. Therefore it is easy to recognize.

In Fig. 5, the picture of Nike bag have many other picture elements, using image recognition technology is hard to identify them as the same product, while using product identification is easy. The properties of them are similar so that the identification is not very difficult. In Fig. 6, children collision belt is not a popular product so that the sellers are less than others. Although the pictures are totally different, we can still know that they are the same one.

The first picture in Fig. 7, ADIDAS 2011 sport suit use a female model to show the product. The style is totally different. If we use image recognition technology to identify, these three items will not be regarded as the same one, but actually they are the same. In experimental result, pictures of products which are in the same cluster are fundamentally similar, just with different shooting styles and different parts showed. From these pictures we can see that the effect of product identification is better, as product identification can do the things which image recognition technology cannot do.



V. CONCLUSIONS Nowadays the descriptions and classification of online  products in most e-commerce websites are not uniformed, and the number of online products increases at a high speed, which brings the challenge to recognize massive products in real time. In this paper, we proposed a parallel solution to full product recognition which needs a large amount of computation. Specifically speaking, we define the notion of product similarity based on their property set, and based on the notion, we design a parallel clustering algorithm based on mean-shift and map-reduce framework. From the experiments we show that our proposed solution achieves a high precision of product recognition with good efficiency.

In the future, we aim to address the problem of incremental recognition of a product, that is, the recognition of a newly-added product. In addition, we will study the application of the product recognition mechanism to product recommendation and search optimization.

