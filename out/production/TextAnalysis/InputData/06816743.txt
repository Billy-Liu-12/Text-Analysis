AQUAS: A Quality-Aware Scheduler for NoSQL Data Stores

Abstract? NoSQL key-value data stores provide an attractive solution for big data management. With the help of data parti- tioning and replication, those data stores achieve higher levels of availability, scalability and reliability. Such design choices typically exhibit a tradeoff in which data freshness is sacrificed in favor of reduced access latency. At the replica-level, this tradeoff is primarily shaped by the resource allocation strategies deployed for managing the processing of user queries and replica updates.

In this demonstration, we showcase AQUAS: a quality-aware scheduler for Cassandra, which allows application developers to specify requirements on quality of service (QoS) and quality of data (QoD). AQUAS efficiently allocates the available replica resources to execute the incoming read/write tasks so that to minimize the penalties incurred by violating those requirements.

We demonstrate AQUAS based on our implementation of a microblogging system.



I. INTRODUCTION  NoSQL data stores contribute significantly to the manage- ment of big data. Such data stores are widely adopted by var- ious web applications, ranging from online shopping to social networking. As one of the fundamental design choices, those systems typically rely on replication and weak consistency in order to improve scalability, availability and reliability. Hence, there is an intrinsic trade-off between latency and consistency, which has become a key factor in the design of large-scale data management systems [1]. On the one hand, most NoSQL data stores employ weak consistency to reduce latency, i.e., provide high Quality of Service (QoS). On the other hand, under weak consistency, an end-user might access stale data, i.e., experience low Quality of Data (QoD). Herein, an intuitive question is ?how much data staleness is acceptable??.

Consider an online shopping application such as the one provided by Amazon (see Figure 1). As mentioned in [2], an interactive session between a customer and Amazon?s naviga- tor page would involve issuing several key-based queries to the Dynamo key-value data store. For example, in the page illustrated in Figure 1, three different queries could be issued to read the data values corresponding to the following zones (i.e., fragments): 1) Recommendation list (zone A): the system should display this list as soon as possible, as it entails poten- tial profits. However, it might be acceptable to provide some stale information such as rating and review count. 2) Shopping cart (zone B): different from recommendations, all information  Fig. 1. An Online Shopping Example  in the shopping cart list should be fresh and exact. However, small delays are acceptable and tolerated. 3) Wish list (zone C): it represents moderate expectations in terms of both QoS and QoD. In particular, a customer checking her wish list might tolerate some delays but she is not as committed to wait as someone who has already finished a purchase and waiting for check out. Meanwhile, she also expects to see fresh updates on her wish list items but providing slightly stale items entails no serious consequences (as opposed to purchasing). This example clearly shows that different web queries/applications would have different expectations in terms of QoS and QoD, which emphasizes the need for employing quality-aware query scheduling in those emerging NoSQL data stores.

Recent work attempts to maximize a replica consistency (i.e., QoD) within some pre-specified latency (i.e., QoS) con- straint per request [3]. However, it overlooks the discrepancy between the constraints specified for different requests and their impact on effective resource allocation. The prototype in [4] leverages probabilistic models to analyze the consistency for eventually consistent data stores, but is oblivious to QoS.

In this work, we propose a novel quality-aware scheduler for NoSQL data stores named AQUAS. In particular, we have developed a prototype system, in which AQUAS is imple- mented in the popular Cassandra store [5]. AQUAS allows developers to specify the different QoS and QoD requirements for cassandra-based web applications, typically in the form of bounds on both latency and staleness. Moreover, AQUAS provides efficient resource scheduling strategies for executing the incoming read and write operations, which leverage and     expand our FIT scheduling family [6, 7, 8]. As thousands of requests are issued simultaneously, AQUAS strives to maximize the QoS and QoD as perceived by end users, hence, maximizing user satisfaction and minimizing the penalties incurred due to violating the application requirements. To illus- trate the advantages provided by AQUAS, we demonstrate our prototype system via an interactive microblogging application.



II. AQUAS OVERVIEW In NoSQL data stores, data is typically organized in tables or  tablets of rows where each row is an abstract key-value pair. In particular, Cassandra supports eventual consistency to manage replicas of the same key-value pair. It also provides asyn- chronous replication in which replica updates are propagated in a lazy manner. In other words, the write operations to some replicas are disseminated in the background. Hence, a replica object is accessed by either a foreground read request (i.e., user query) or a background write request (i.e., system update).

Additionally, each Cassandra instance starts and manages a fixed number of threads to execute read or write tasks. In our proposed prototype, AQUAS continuously schedules those foreground reads and background writes at each replica, in order to maximize the user satisfaction in both QoS and QoD.

A. System Goals To quantify QoS, Service Level Agreement (SLA) [2] has  been widely used to specify the expectations and tolerance for latency. Intuitively, SLA acts as a soft deadline where delays beyond the pre-specified SLA incur penalties to the system. AQUAS allows application developers to configure a QoS penalty function of the form f(Ai, lbi, Fi), where Ai is the query arrival time, lbi is a latency bound, and Fi is the finish time. Here, Ai and lbi decide the SLA?s and a common choice for f is a linear piecewise function as in [9].

In addition, we adopt a Freshness Level Agreement (FLA) for specifying QoD in terms of freshness [6]. Similar to SLA, FLA describes the user?s expectations for freshness in serviced data. Generally, providing stale data could reduce delays and satisfy the pre-specified SLAs requirements, whereas updating the requested data before serving it could improve freshness but incur additional delays. Similar to QoS, AQUAS allows application developers to configure QoD penalty function as g(Ri, sbi, Fi), where Ri is the arrival time of first unapplied update and sbi is a staleness bound. This time-based function g mapping the time factors into QoD penalty is especially useful in a distributed environment [7].

In AQUAS, the total combined incurred penalty of a query Qi is Pi = Wi[?if(Ai, lbi, Fi) + (1 ? ?i)g(Ri, sbi, Fi)] where Wi is the query weight and ?i is the QoS fraction.

The weight Wi represents the inter-query importance, whereas ?i and (1 ? ?i) represent the intra-query QoS and QoD preferences, respectively. Hence, for N queries, our objective is to minimize the average total penalty, i.e., 1N  ?N i=1 Pi.

B. System Design Figure 2 shows the user queries and system updates arriving  at each node. Those requests are placed into corresponding  Read Queue  Write Queue  AQUAS  Thread Pool  Cost Estimator  User  Queries  System  Updates  Node  Fig. 2. System Architecture  read and write queues until they are submitted to a thread pool for execution. In our prototype, the thread execution is handled by AQUAS, which is described next.

1) Scheduling: AQUAS employs a suite of strategies for the efficient scheduling of both pending queries and updates at each replica node in a key-value data store. For each scheduling strategy, we follow the popular design principle on the separation of high-level mechanism and low-level policy.

Mechanism: In AQUAS, a mechanism is used to specify the dependency between user queries and pending updates.

In [10], the On-Demand (OD) mechanism couples the exe- cution of the pending system updates with the arriving user queries. In other words, all the data objects read by a certain query are refreshed on demand before the execution of that query. This mechanism is especially well suited for key-value data stores in which each object is accessed by its key leading to a clear relationship between the arriving queries and their corresponding pending updates. Clearly, the OD mechanism above sacrifices QoS to maximize QoD.

However, even when the staleness of a data object violates the QoD requirements of a certain query, applying the pend- ing updates to that object might often be at odds with the processing of user queries. In particular, installing a pending update with a high processing cost could potentially have the negative impact of delaying all pending user queries leading to an overall low QoS (i.e., high tardiness), despite of the high QoD (i.e., high freshness). Motivated by this observation, we investigate a Freshness/Tardiness-aware mechanism called FIT for the scheduling of both queries and updates [8]. Similar to OD, FIT defers refreshing an object until it is requested by a query. However, under FIT, the scheduling policy reasons about the global impact of applying an update in terms of the benefits of processing that update to the query under consideration as well as the other pending queries in the replica. The basic idea is to compare the approximate penalty incurred by skipping updates to that of installing them and choose the alternative with least penalty.

Policy: Different from a mechanism, in AQUAS, a policy is used for deciding the execution order of queries and updates.

In our prototype, we investigate the following policies.

? First Come First Served (FCFS): FCFS queues the     queries according to their arrival time. Hence, it is a fair scheduling policy, but performs very poorly under deadline-based metrics such as SLAs.

? Earliest Deadline First (EDF): EDF organizes the set of pending queries according to their deadlines. This deadline-aware policy provides a close to zero penalty under low to medium system utilization.

? Weight Shortest Job First (WSJF): WSJF gives the query with shorter processing cost a higher priority. WSJF outperforms EDF under high system utilization, in which EDF might exhibit a domino effect [11].

? Adaptive SRPT EDF Transaction Scheduling (ASETS): Motivated by the tradeoff between EDF and WSJF, ASETS [12] integrates deadline-oblivious WSJF with deadline-aware EDF to automatically adapt to workload.

Strategy: The different combinations of low-level policies and high-level mechanisms result in a suite of scheduling strategies employed by AQUAS. As shown in our previous work, the variant of ASETS policy and FIT mechanism provides the best performance under a wide range of workloads [7, 8].

2) Cost Estimation: When AQUAS deploys a cost-aware mechanism (e.g., FIT) or policy (e.g., WSJF), it is necessary to estimate the running time of a read or write operation before its execution. Hence, central to the design of AQUAS, is a light-weight high-accuracy cost estimator. Towards effective cost estimation, it is important to make the distinction between the read and write tasks, since they have different behaviors in terms of disk and memory accesses.

Read Task: In Cassandra, a local valid read operation is executed on a merged view of the sequence of SSTables and the memtable. To estimate that execution time, we adopt a simple approach based on statistical estimation that leverages the monitored execution times of previous queries. In particular, this approach consists of two stages:  ? offline sampling: AQUAS randomly samples some keys in an offline manner and records the cost (i.e., time) needed to retrieve their corresponding objects. In AQUAS, we rely on an embedded Berkley DB for the fast storage and retrieval of those costs.

? online feedback: During handling user queries, AQUAS captures the costs incurred in processing each of the submitted read operations. Similar to the offline stage, those costs are also recorded in Berkley DB, where older values are updated.

For cost estimation, if the cost of an input read operation exists in the database, the estimator reports that cost. Other- wise, it reports an average cost instead. Clearly, the accuracy of estimation is improved with the increase in offline sampling.

However, in AQUAS, sampling is permitted when the system is idle. Meanwhile, the online feedback compensates for that drawback and refines the estimated cost.

Write Task: A local write operation in Cassandra is ap- pended to commit-log on disk and delivered to memtable in memory. The data in memtable does not leave memory until a compaction occurs (e.g., when memtable reaches a  threshold). Hence, estimating the cost of a write operation is relatively simple, since in-memory operations behave linearly.

This observation allows AQUAS to employ a simple linear regression model for estimating the cost of writes. The primary factors shaping that model are data size and the cost of appending onto the commit-log.



III. DEMONSTRATION  Our demonstration is based on a microblogging system that we have built on top of a modified version of Cassandra, in which we have incorporated our proposed AQUAS scheduler.

Like Twitter, each user in our microblogging system has a set of followers and subscribes to a set of followees. Every time a user logs onto her account, she receives the latest microblogs (i.e., tweets) generated by her followees in the form of a list that is ordered by recency. This functionality, usually termed timeline query, typically involves issuing many read queries to the key-value data store so that to retrieve the latest updates generated by each followee. Clearly, a microblogging user expects a very short response time to their timeline query while also expecting to see a list that includes all the recent tweets. Moreover, microblogging applications typically host hundreds of millions of users, which makes answering simultaneous timeline queries a challenging task and requires efficient scheduling to maximize the user satisfaction in both QoS and QoD, as offered by AQUAS.

AQUAS allows application developers to set a latency bound and a staleness bound to specify the QoS and QoD requirements, respectively. Hence, a timeline query is expect- ed to finish execution within the latency bound while also tolerating missing some data that fall within the staleness bound. That is, no penalty is incurred if a query finishes within the latency bound and acquires all the tweets generated beyond the staleness bound. Otherwise, there is a system penalty for violating the QoS or QoD requirement. Here, we choose piecewise linear penalty functions as in [7]. Our configuration interface also allows setting different quality- aware requirements to different users. For example, application developers could set tight latency and staleness bounds for VIP users, whereas general users are provided by relatively relaxed bounds. In such scenario, providing differentiated service is necessary and is easily achieved via our proposed AQUAS.

To illustrate the benefits of AQUAS, in our demonstration scenario we present to the audience the timeline of few representative users while running thousands of users in the background. Moreover, we employ a tweet generator that has the ability to produce new tweets on the fly or to replay them from a stored log file. Figure 3(a) shows one representative user named AQUAS, whereas Figure 3(b) shows the portion of the tweets coming from AQUAS?s followees (as produced by our generator). A tweet (including content, creation time, etc.) is stored in a super column where the tweets generated by the same user are inserted into a single row with the user id as the key. Hence, a timeline query for a certain user is implemented as reading all the tweets generated by that user followees using our modified Cassandra key-value query API.

(a) Timeline of User AQUAS  Two recent tweets from  user Reading are skipped  (b) Generating the Tweets from AQUAS?s Followees  Penalty incurred by querying the  tweets from User Reading  (c) Performance Metadata  Fig. 3. Tweets Displayed with QoS/QoD Requirements in Microblogging System  Ideally, the tweets in Figure 3(a) should be identical to the ones in Figure 3(b). However, there might be some deviations based on the employed scheduler. For instance, when the scheduler skips updates (i.e, tweets) in order to minimize the total penalty incurred due to QoS/QoD requirement violation.

In particular, Figure 3(a) shows a demonstrated setting in which we set a latency bound to 5ms and staleness bound to 1s. Compared to Figure 3(b), two recent tweets from user Reading are temporarily skipped by the scheduler (highlighted within bold frames). To capture the impact of such tradeoffs, we provide a monitoring tool, which allows the audience to gain insights into the perceived user experience.

Figure 3(c) shows a sample dashboard from our monitoring tool, which lists all issued queries together with their perfor- mance metadata (e.g., arrival time, finish time, penalties, etc.).

