Language Models & Topic Models for Personalizing Tag Recommendation

Abstract?More and more content on the Web is generated by users. To organize this information and make it accessible via current search technology, tagging systems have gained tremendous popularity. Especially for multimedia content they allow to annotate resources with keywords (tags) which opens the door for classic text-based information retrieval. To support the user in choosing the right keywords, tag recommendation algorithms have emerged. In this setting, not only the content is decisive for recommending relevant tags but also the user?s preferences.

In this paper we introduce an approach to personalized tag recommendation that combines a probabilistic model of tags from the resource with tags from the user. As models we investigate simple language models as well as Latent Dirichlet Allocation.

Extensive experiments on a real world dataset crawled from a big tagging system show that personalization improves tag recommendation, and our approach significantly outperforms state-of-the-art approaches.

Keywords-Personalization, Data mining, Clustering.



I. INTRODUCTION  The World Wide Web is growing at incredible speed. User generated content is uploaded by millions everyday. Web 2.0 or the Social Web are evolving rapidly. Sharing of user generated content is one of the predominant actions on the Web nowadays. To organize this content and to make it accessible to other users is the main purpose of sites like Flickr1, LastFm2, YouTube3, or Delicious4.

These sites allow users to annotate content with their own keywords (tags), opening up the possibility to retrieve con- tent using traditional keyword search. Especially multimedia content like music, photos, or videos rely on manually added meta information. Adding keywords to content (tagging) is the only feasible way to organize multimedia data at that scale and to make it searchable. These keywords can be freely chosen by a user and are not restricted to any taxonomy. This results in some benefits like flexibility, quick adaption, and easy usability, but has also some drawbacks.

Tagging is considered a categorization process not a classifi- cation process [1]. The underlying meaning has to be evaluated and inferred in the context of other tags and user information.

1Flickr: http://www.flickr.com 2LastFm: http://www.last.fm 3YouTube: http://www.youtube.com 4Delicious: http://delicious.com  Tags can even have no concrete meaning or are only inter- pretable by the user herself. In addition, tags can have various purposes. Some describe the annotated content, some refer to the user (e.g. ?jazz?, ?myHolidays? or ?to read?) [2]. In practice allowing users to freely annotate means that tagging systems contain noise and are rather sparsely populated.

Studies [3], [4] have shown that many users annotating a resource leads to a stable tag distribution for this resource, capturing its characteristics sufficiently. To support users in choosing tags, tag recommendation algorithms have emerged.

For resources already annotated by lots of people this recom- mendation is rather straight forward. The tagging system can provide the most frequent tags assigned to the resource, or look at the tagging history of the user to make a more per- sonalized recommendation. On this note, tag recommendation algorithms can be classified into user-centered and resource- centered ones [5].

In this paper we combine both perspectives to recom- mend personalized tags to users. To this end, we employ a mixture (Section III-D) of simple language models (LM) (Section III-B) and Latent Dirichlet Allocation (LDA) (Sec- tion III-C) to estimate the probability of new tags based on the already assigned tags of a resource and a user, and introduce a principled approach for combining these estimates in Section III-A. The potential advantage of employing LDA is the possibility to recommend tags not previously assigned to the resource or used by the user. This broadens the available vocabulary for tag recommendation. The potential advantage of combining the resource perspective with the user perspec- tive is to filter general tags for a resource with the individual tagging preferences of a user.

In Section IV we evaluate our approach on a real world dataset. We systematically analyze tag recommendation based on resources or users only, assess the possible merits of LDA as opposed to language models, and compare our combined approach to FolkRank [6] as a state-of-the-art personalized tag-recommender. Our evaluation shows that combining ev- idence from the resource and the user improves tag recom- mendation significantly, and that LDA helps, in particular for generalizing from individual tagging practices on resources.

Moreover, our approach achieves significantly better accuracy than state-of-the-art approaches.

DOI 10.1109/WI-IAT.2010.29

II. RELATED WORK In recent years interest in tag recommendation was sparked  within the research community. The gaining importance of tagging systems led to the development of sophisticated tag recommendation algorithms. A popular approach to this end has been Collaborative Filtering, a well known approach for different kinds of recommender systems [7].

Xu et al. [8] describe a way to recommend a few descriptive tags to users by rewarding co-occuring tags that have been assigned by the same user, penalizing co-occuring tags that have been assigned by different users, and boosting tags with high descriptiveness.

An interactive approach in the context of a photo tagging site based on co-occurence is presented in [9]. After the user enters a tag for a new resource, the algorithm recommends tags based on co-occurence of tags for resources which the user or others used together in the past. After each tag the user assigns or selects, the set is narrowed down to make the tags more specific.

Sigurbjo?rnsson and van Zwol [10] also look at co-occurence of tags to recommend tags based on a user defined set of tags.

The co-occuring tags are then ranked and promoted based on e.g. descriptiveness.

Jaeschke et al. [11] compare two variants of collabora- tive filtering and FolkRank [6], a graph based algorithm for recommendations in folksonomies. For collaborative filtering, once the similarity between users on tags, and once the similarity between users on resources is used for recommen- dation. FolkRank uses random walk techniques on the user- resource-tag (URT) graph based on the idea that popular users, resources, and tags can reinforce each other. These algorithms take co-occurrence of tags into account only indirectly, via the URT graph. Our evaluation shows that our approach achieves significantly better accuracy than FolkRank, and even the simple and scalable combination of smoothed language models achieves competitive accuracy.

Symeonidis et al. [12] employ dimensionality reduction to personalized tag recommendation. Whereas [11] operate on the URT graph directly, [12] use generalized techniques of SVD (Singular Value Decomposition) for n-dimensional tensors. The 3-dimensional tensor corresponding to the URT graph is unfolded into 3 matrices, which are reduced by means of SVD individually, and combined again to arrive at a more dense URT tensor approximating the original graph. The algorithm then suggests tags to users, if their weight is above some threshold. Rendle and Schmidt-Thieme [13] introduce two more efficient variants of this approach using canonical decomposition and pairwise interaction tensor factorization.

When content of resources is available, tag recommendation can also be approached as a classification problem, predicting tags from content. A recent approach in this direction is presented in [14]. They cluster the document-term-tag matrix after an approximate dimensionality reduction, and obtain a ranked membership of tags to clusters. Tags for new resources are recommended by classifying the resources into clusters, and ranking the cluster tags accordingly.

Heymann et al. [15] employ association rule mining on the tag sets of resources for collective tag recommendation. The mined association rules have the form T1 ? T2, where T1 and T2 are tag sets. On this basis tags in T2 are recommended, when all tags in T1 are available for the resource, and the confidence for the association rules is above a threshold.

In [16] we have shown that tag recommendation based on LDA achieves significantly better accuracy than this approach, and recommends more specific tags, which are more useful for tag-based search.

A. LDA for Tag Recommendation  Latent Dirichlet Allocation has recently gained some atten- tion for tag recommendation. Xiance et al. [17] and Krestel et al. [16], [18] introduce an approach to collective tag recommendation using LDA. Xiance et al. employ LDA for eliciting topics from the words in documents (blogposts) and from the associated tags, where words and tags form disjoint vocabularies. On this basis they recommend new tags for new documents using their content only. Krestel et al. on the other hand use LDA to infer topics from the available tags of resources and then recommend additional tags from these latent topics. In this paper we extend these approaches for personalized tag recommendation by also taking the personal tagging practices of users into account. Moreover, we show that using a mixture of language models and latent topic mod- els significantly improves the accuracy of tag recommendation.

Bundschus et al. [19] introduce a combination of LDA based on the content and tags of resources and the users having bookmarked a resource. The underlying generative process elicits user specific latent topics from the resource content and seperately from the tags of the resource. The content-based topics and tag-based topics are in a one-to-one correspondence by the user-id. On this basis personalized tag recommendation is realized by first eliciting user specific topics from the resource content, and then using the corresponding tag-based topics for suggesting tags. Our approach does not require content, which may not be available, e.g., for multimedia data, but works exlusively on the tags.

Harvey et al. [20] introduce a similar approach to personal- ized tag recommendation as proposed in this paper on the basis of LDA. Rather than decomposing the joint probability of a tag given the tag assignments for a resource and user via an application of Bayes? rule (see Equation 5), they decompose the joint probability of latent topics given the tag assignments.

On this basis, they introduce an extended Gibbs sampler which draws topics simultaneously from the user and the resource.

This fully generative approach, however, requires some initial tags from the user to a given resource, in order to recommend additional tags. In contrast, our approach can also handle the arguably more realistic setting of suggesting tags for a new bookmark without any initial tags from the user.



III. PERSONALIZED TAG RECOMMENDATION MIXING LANGUAGE MODELS WITH TOPIC MODELS  In this section we present our approach to combine tag recommendation for users with tag recommendation for re- sources. We show that this combination helps to overcome the weaknesses of the individual approaches applied in isolation.

On the one hand we take the user?s interest and tagging preferences into account, and on the other hand we identify suitable tags for a particular resource. For both, user-centered and resource-centered, we investigate the use of two methods, Latent Dirichlet Allocation and language models, for tag recommendation.

A. Goals and Approach  Given a set of resources R, tags T , and users U , the ternary relation X ? R ? T ? U represents the user specific assignment of tags to resources. A bookmark b(r, u) for a resource r ? R and a user u ? U comprises all tags assigned by u to r: b(r, u) = ?t?r,uX5. The goal of personalized tag recommendation is to assist users bookmarking a new resource by reducing the cognitive load by suggesting tags for their bookmark b(r, u). This can be based on other tag assignments to this resource and similar resources, or based on the user and similar users.

To this end, we need to rank possible tags t, given a resource and a user. We rank based on a probabilistic approach. More formally, we estimate the probability P (t|u, r) of a tag t given a resource r and a user u as follows:  P (t | r, u) = P (r, u | t)P (t) P (r, u)  (1)  ? P (r | t)P (u | t)P (t) P (r, u)  (2)  = P (t | r)P (r)  P (t) P (t | u)P (u)  P (t) P (t)  P (r, u) (3)  = P (t | r)P (t | u)  P (t) P (r)P (u) P (r, u)  (4)  ? P (t | r)P (t | u) P (t)  (5)  Equation 1 applies Bayes? rule, Equation 2 splits P (r, u|t) assuming conditional independence of r and u given t, Equation 3 again applies Bayes? rule to P (r|t) and P (u|t), Equation 4 simplifies, and Equation 5 discards the factors P (r), P (u), and P (r, u), which are equal for all tags.

P (t) can be estimated via the relative frequency of tag t in all bookmarks. For estimating P (t|r) and P (t|u) we investigate and combine two approaches. On the one hand, we use simple language models (Section III-B), on the other hand, we use Latent Dirichlet Allocation (Section III-C), in order to also recommend tags for new resources and users, which have only few bookmarks available.

The estimate in Equation 5 gives equal weight to P (t|r) and P (t|u). However, typically there are more tags available  5projection ? and selection ? operate on multisets without removing duplicate tuples  for a particular user u than for a resource r. Thus the estimate for P (t|u) should be weighted more strongly than the estimate for P (t|r). To this end, we smoothen P (t|r) and P (t|u) with the prior probability P (t).

P ?(t | r) ? log2(|r| + 1)P (t | r) + log2(|u| + 1)P (t) (6) P ?(t | u) ? log2(|u| + 1)P (t | u) + log2(|r| + 1)P (t) (7)  where |r| is the number of tags available for a resource r, and |u| is the number of tags available for a user u. When |r| is smaller than |u|, P (t|r) is smoothed more strongly, and thus influences P (t|r, u) less than P (t|u). Note that when P (t|r) is zero, P ?(t|r) is proportional to P (t). Consequently, the combined probability P ?(t|r)?P ?(t|u)/P (t) is effectively proportional to P ?(t|u). Likewise, when a resource has no tags at all, log2(|r|+1) = 0, and the combined probability is again proportional to P ?(t|u).

The combination above is reminiscent of the popular ?Prod- uct of Experts? approach, where our Experts are resources and users. We have also experimented with the popular mixture, which linearly interpolates P (t | r) and P (t | u). But this approach did not achieve competitive results.

B. Language Models  The most straightforward approach to tag recommendation is to simply recommend the most frequent tags for each resource. More formally, the probability for a tag t given a resource r is estimated as:  Plm(t | r) = c(t, r)? ti?r c(ti, r)  (8)  where c(t, r) is the count of tag t in resource r. The probability Plm(t | u) of a user u using tag t is determined in a similar way from all tags the user has assigned.

Note that we do not need to smoothen the language models as usual, because we smoothen P (t|r) and P (t|u) with P (t) via Equations 6 and 7.

C. Latent Dirichlet Allocation  Especially for new resources and users with only few bookmarks, the simple language model does not suffice for tag recommendation, because the tag vocabulary of the already available bookmarks may differ from the preferred tag vocab- ulary of the user. Smoothing with the global tag probability only effectively switches off tags that are not available for a resource or user.

To also recommend topically related tags, we use Latent Dirichlet Allocation (LDA) [21]. The general idea of LDA is based on a simple generative model. Resources and users are modelled as mixtures of latent topics, which in turn consist of a mixture of words. When looking at a resource, for each tag, a user first chooses one of the topics of the resource and then chooses a tag from this topic. Likewise, from the perspective of the user, the user first chooses one of her topics of interest from which she chooses the tag.

More formally, the modeling process of LDA can be de- scribed as finding a mixture of topics z for each resource r,     i.e., P (z | r), with each topic described by tags t following another probability distribution, i.e., P (t | z). This can be formalized as  Plda(t | r) = Z?  z=1  P (t | z)P (z | r) (9)  where Plda(t | r) is the probability of tag t for a given resource r and z ranges over the latent topics of the resource. P (t | z) is the probability of tag t within topic z (see Equation 11).

P (z | r) is the probability of picking a tag from topic z in the resource (see Equation 12). The number of latent topics Z has to be defined in advance and allows to adjust the degree of specialization of the latent topics.

LDA estimates the topic-tag distribution P (t | z) and the resource-topic distribution P (z | r) from an unlabeled corpus of documents using Dirichlet priors for the distributions and a fixed number of topics. Gibbs sampling [22] is one possible approach to this end: It iterates multiple times over each tag ti in resource r, and samples a new topic z for the tag based on the probability P (z|ti, r, z?i) using Equation 10, until the LDA model parameters converge.

P (z | ti, r, z?i) ? (CRZrz + ?) CTZtiz + ?? t C  TZ tz + T?  (10)  CTZ maintains a count of all topic-tag assignments, CRZ  counts the resource-topic assignments, z?i represents all topic- tag and resource-topic assignments except the current as- signment z for tag ti, and ? and ? are the (symmetric) hyperparameters for the Dirichlet priors, serving as smoothing parameters for the counts. Based on the counts the posterior probabilities in Equation 9 can be estimated as follows:  P (t | z) = C TZ tz + ??  ti CTZtij + T?  (11)  P (z | r) = C RZ rz + ??  zi CRZrzi + Z?  (12)  The estimation of Plda(t | u) proceeds in the same way as the estimation of Plda(t | r) by operating on the individual tag sets of users rather than resources.

For resources, the resulting topics reflect a collaborative shared view of the resource, and the tags of the topics reflect a common vocabulary to describe the resource. Table I shows typical examples of resource topics. As can be seen, the topics group typically co-occuring tags, which often will not be used by the same user. E.g., one user may prefer the tag ?photography?, another user may prefer ?photo? or ?photos?.

For users, the resulting topics reflect the topical interests of a user, and the tags of topics reflect the individual tagging vocabulary of the user and similar users. Table II gives exam- ples of user topics. Note that latent topics are not necessarily disjoint. E.g. ?hardware? occurs in the ?mac? topic as well as in the ?do it yourself? topic, but most certainly these two interpretations of ?hardware? are rather disjoint.

Tag Prob. Tag Prob.

news 0.201 flickr 0.344  technology 0.182 photography 0.167 tech 0.118 photos 0.117 blog 0.082 photo 0.093 daily 0.070 tools 0.089 geek 0.067 web2.0 0.045 blogs 0.029 visualization 0.016  community 0.025 images 0.015 internet 0.023 pictures 0.012  computers 0.021 api 0.010 web 0.018 search 0.009  forum 0.018 internet 0.007 computer 0.015 applications 0.005 software 0.013 sharing 0.004  TABLE I TOP TAGS COMPOSING THE LATENT TOPICS ?TECH NEWS? AND ?FLICKR?  BASED ON RESOURCE PROFILES  Tag Prob. Tag Prob.

mac 0.320 diy 0.234 osx 0.215 make 0.099  apple 0.191 hardware 0.084 software 0.170 creativity 0.080  video 0.025 hacks 0.072 quicktime 0.013 electronics 0.070 macintosh 0.012 crafts 0.063  mail 0.012 science 0.046 tv 0.009 mind 0.030  ipod 0.006 theory 0.027 gmail 0.005 photography 0.023  hardware 0.004 engineering 0.019 algorithm 0.003 tutorials 0.017  boot 0.002 language 0.008  TABLE II TOP TAGS COMPOSING THE LATENT TOPICS ?MAC? AND ?DO IT  YOURSELF? BASED ON USER PROFILES  By combining these two perspectives using Equation 5, the resource perspective serves as a selector of the topical content of the resources, while the user perspective takes into account the individual tagging practices of the user.

D. Combining LDA and LM  As Plm(t | r) and Plda(t | r) both constitute (normal- ized) probability distributions, we can combine these two by straightforward linear interpolation (likewise for P (t | u)):  P (t | r) = ? ? Plm(t | r) + (1 ? ?) ? Plda(t | r) (13) We have experimented with a broad range for ?, and achieved consistently good results for ? in the range of [0.2..0.8]. We report results for ? ? {0.0, 0.5, 1.0}, where ? = 0, and ? = 1 practically switch off the estimates based on language models and latent topics respectively. Combined with the smoothing in Equations 6 and 7, we effectively use a two level smoothing of the simple language model Plm(t | r): First by the more general Plda(t | r) and then by the marginal tag probability P (t).



IV. EVALUATION  Evaluating personalized tag recommendation algorithms is not a trivial task. To get precise performance statistics a good way would be to compare two recommendation algorithms in a live tagging system, which allows for a direct user evaluation.

Since this scenario is unfeasable or means interfering with a running system other approaches are prefered.

One popular way for evaluation is to take existing data from a tagging system and conduct tests on a hold-out set of tags, resources, or users [7]. This approach has a promising characteristic: All tags which were used for a resource by a particular user are definitly known. The drawback is that these tags have been added by the user after being suggested by some automatic algorithm within the tagging system. This can bias the tag assignments towards the used tag recommendation algorithm [23]. Another disadvantage is that only a small set of correct, good tags are actually picked by the user making no distinction between totally unsuited tags and suitable recom- mended tags which were not picked by the user for whatever reason. Note that this leads to an underestimation of the actual tag recommendation quality.

To extenuate these disadvantages the test dataset has to be designed thoroughly. The strength of a recommendation algo- rithm can only be judged in comparison with other algorithms run on the same dataset. Thus we need to compare directly state-of-the-art algorithms with the proposed tag recommen- dation algorithm on the same dataset.

Tag distributions for a resource tend to stabilize after around 100 bookmarks [3]. This makes tag recommendation espe- cially challenging for resources having a lot less bookmarks.

This so-called cold start problem gives the most discriminative results for different algorithms.

Before we report our results, we have a detailed look into the used dataset, performance metrics, and the used baseline.

A. Dataset  As a dataset for our evaluations we use a crawl from Delicious provided by Wetzker et. al. [24]. The dataset consists of nearly 1 million users crawled between December 2007 and April 2008. The retrieval process resulted in about 132 million bookmarks or 420 million tag assignments that were posted between September 2003 and December 2007. Almost 7 million different tags are contained in the dataset and about 55 million different urls were annotated.

To do the computations in memory and in a reasonable time we were forced to use only a sample of the whole dataset.

The huge amount of data and the fact that no spam filtering was done also results in a very sparse overlap between tags, resources and users. To get a dense subset of the sampled data we computed p-cores [25] for different levels.

For p = 20 we get enough bookmarks for each resource to split the data based on resources into meaningful training and test sets (90%:10%). The 20-core ensures that each tag, each resource and each user appears at least 20 times in the tag assignments. For the 10% resources in the test set, we only include the bookmarks for the first n users (n ?  1, 3, 5, 7, 10, 20) who annotated a resource into the training set. This results in a setting close to real life situations where users often annotate a resource previously annotated by only a few (n) other users. As soon as a resource is annotated by many users, tag recommendation can exploit the stabilized tag distribution [3] for resources and recommending good tags becomes less challenging.

The proposed setup allows to analyze how well different algorithms can generalize from relatively few tags available for a resource similuating the cold start problem in tagging environments.

Parameter settings were tested on 1/256 of the data. We have five test sets containing 10% of the resources with different numbers of ?known? bookmarks. On this set, the only preprocessing of the tag assignments performed was the decap- italization of the tags. No stemming or other simplifications were applied. More sophisticated preprocessing improve the results but would complicate the evaluation of the algorithms and the comparison of different methods.

B. Evaluation Measures  We use standard information retrieval evaluation metrics to report and compare the performance of the algorithms.

? P@1 ? precision at one: Percentage of test cases where the first recommended tag was actually used by the user to annotate the resource. This is the same as success at one (S@1).

? P@5 ? precision at five: Percentage of tags among the first five recommended tags that where actually used by the user. Averaged over all test cases.

? S@5 ? success at five: Percentage of test cases where at least one of the first five recommended tags was used by the user.

? S@uAVG ? success at user average: Percentage of test cases where at least one of the recommended tags was used by the user. The number of recommended tags is the average number of tags per (other) bookmark for the user.

? P@uAVG ? precision at user average: Percentage of tags among the top n recommended tags that where actually used by the user, where n is again the average number of tags per bookmark.

? R@uAVG ? recall at user average: Percentage of user tags among the top n recommended tags, n as above.

? Fma@5 ? f1 macro average at five: The harmonic mean of averaged precision and recall for the first five recommended tags.

? Fmi@5 ? f1 micro average at five: The averaged harmonic mean of precision and recall for the first five recommended tags.

? MRR ? mean reciprocal rank: The average over all test cases of the multiplicative inverse of the rank of the first correct tag.

Rec. based on P@1 S@5 S@10 S@uAVG P@uAVG R@uAVG P@5 R@5 Fmi@5 Fma@5 MRRUser Resource FR ? 0.271 0.537 0.673 0.442 0.190 0.231 0.168 0.262 0.180 0.205 0.400  LDA ? 0.279 0.571 0.689 0.475 0.194 0.229 0.178 0.262 0.187 0.212 0.407 LM ? 0.284 0.584 0.717 0.482 0.204 0.246 0.187 0.282 0.197 0.225 0.424  LDA&LM ? 0.288 0.596 0.715 0.486 0.208 0.250 0.190 0.287 0.200 0.228 0.428 ? FR 0.488 0.768 0.824 0.657 0.294 0.376 0.281 0.420 0.299 0.337 0.601 ? LDA 0.496 0.815 0.880 0.675 0.328 0.416 0.310 0.460 0.328 0.370 0.635 ? LM 0.493 0.762 0.806 0.661 0.352 0.363 0.335 0.411 0.323 0.369 0.604 ? LDA&LM 0.560 0.826 0.894 0.718 0.358 0.454 0.334 0.492 0.352 0.397 0.678  LM LM 0.547 0.813 0.882 0.726 0.353 0.441 0.319 0.478 0.338 0.382 0.667 LDA LDA 0.561 0.847 0.908 0.738 0.370 0.467 0.336 0.507 0.358 0.404 0.689 LDA LM 0.532 0.812 0.885 0.722 0.340 0.425 0.313 0.467 0.332 0.375 0.653 LM LDA 0.566 0.859 0.924 0.759 0.386 0.488 0.343 0.526 0.368 0.416 0.703  FolkRank 0.570 0.840 0.906 0.734 0.354 0.452 0.325 0.499 0.349 0.393 0.689 LDA&LM LDA&LM 0.610 0.890 0.934 0.795 0.415 0.529 0.372 0.564 0.396 0.448 0.733  TABLE III RESULTS FOR ONE KNOWN BOOKMARK AND DIFFERENT ALGORITHMS  C. Baseline  To get a good estimation of the performance of our tag recommendation algorithms we compare the results with the results from FolkRank [6]. FolkRank (FR) is one of the state- of-the-art tag recommender algorithms. Its recommendations are very accurate but this comes with high computational costs.

In contrast to our approach, FolkRank does not make use of latent topics but relies on a graph representation of the folksonomy.

The basic idea is to adapt PageRank [26] to get scores for tags. A graph G = (V,E) is constructed from the folksonomy F = (U, R, T, X), where the vertices are users, resources, and tags (R?U ?T ) and the edges are co-occurences of tags and users, tags and resources, and users and resources within tag assignments (u, t, r) ? X .

The symmetric characteristic of the graph G would lead to scores biased towards ?popular?, i.e., highly connected entities within the graph when employing the adapted PageRank (ap).

Thus, folkrank uses a differential approach and computes the scores for each node based on the difference between a regular PageRank computation and a ?personalized? PageRank, like, e.g., in [27] using a preference vector.

For tag recommendation this preference vector is highly biased towards two entries: the user and the resource for whom the recommendation is computed [11]. To compare FolkRank with LDA and LM employed only on resources or only on users, we only boost one entry in the preference vector ? the resource or the user in question. This gives either resource- centered or user-centered FolkRank results.

The FolkRank scores are computed iteratively and finally combined:  Rapi+1 = c(?R ap i + ?AR  ap i ) (14)  Rprefi+1 = c(?R pref i + ?AR  pref i + ?P ) (15)  R = Rpref ? Rap (16)  where ?, ?, ? are constants and c is a normalization factor.

A is a row-stochastic version of the adjacency matrix of G.



V. RESULTS  We have systematically applied the described approaches for estimating P (t|r), P (t|u), and P (t|u, r), and compared them to the corresponding results using FolkRank. Table III gives a complete overview for tag recommendation when there is only one bookmark available for the resource.

The first four rows give the results for taking only the user perspective into account, i.e., tags are predicted based on P (t|u) only (or for FolkRank the preference vector is only biased towards the user). We see that generally the probabilistic approach introduced in this paper outperforms FolkRank (FR) w.r.t. all measures. The simple (smoothed) language model approach (LM) slightly outperforms LDA.

The linear interpolation of LM with LDA achieves a very slight improvement over LM and LDA alone. It is also clear that the user perspective in isolation performs worse than the resource perspective (next four rows). This is to be expected.

The mixture of topics that a user is interested in is typically much more diverse than the mixture of topics a particular resource is about. Thus, no matter how the tag probabilities are estimated, just recommending the most likely tags for a user, disregarding the resource, will often go astray.

The general trends for tag recommendation based on re- sources only (second four rows) are slightly different. FR and LM are rather clearly outperformed on all measures, but among them, for some measures FR is better than LM and vice versa. LDA comes on a clear second place, and a very clear winner is is again the linear interpolation of LM and LDA.

It is interesting that LDA outperforms LM on resources, while LM outperforms LDA on users. The strength of LDA is to generalize from the tagging practices of individual users who have assigned tags to a particular resource (such as ?photography?), in order to also include semantically related tags (such as ?photo?). This strength turns out to be a slight weakness for the user perspective, possibly because users tend to stick to a particular vocabulary, and thus the generalization by LDA does not help.

0.6  0.65  0.7  0.75  0.8  1 3 5 7 10 20  M ea  n R  ec ip  ro ca  l R  an k  Known Bookmarks  ?LDA and LM? ?FolkRank?  ?LM?  Fig. 1. Mean reciprocal rank for different numbers of known bookmarks  The next four rows inspect the performance of the individual approaches to estimate the probability of a tag when combined for personalized tag recommendation. The most simple (and scalable) approach by just combining the smoothed language models already achieves significant improvements6 compared to tag recommendation based on LMs for the resource only.

Combining only LDA models for the user and resource yields further improvement, and the best combination of individual models is achieved by using LM for the user perspective and LDA for the resources. This is consistent with the results for user-based (LM best) and resource-based recommendation (LDA best).

Finally, the last two rows compare full FolkRank with a complete combination of user-based recommendations using an interpolation of LDA and LM with resource-based rec- ommendation. This full combination outperforms FolkRank signifantly on all measures. With one exception (S@10, with only 3 % improvement), all relative improvements are in the range of 7 % to 17 % with 11 % average.

Figure 1 compares mean reciprocal rank (MRR) of the main approaches when varying the number of available book- marks between 1 and 20. FR stands for FolkRank, LM for a combination of language models for the user and the resource, LDA and LM for the full combination of language models and LDA on users and resources. The full combination clearly outperforms the other two approaches, but notably the scalable combination of simple language models outperforms FolkRank for more than seven bookmarks. The reason why MRR degrades with all approaches at least for 20 bookmarks is due to the experimental setup. When using 20 bookmarks, much fewer test data are available in the post-core at 20, and the few remaining test data may be the most difficult ones.

Finally, Figure 2 shows the progression of F-Measure de- pending on the number of recommended tags for resources with three known bookmarks. For all approaches, recommend- ing three tags appears to provide the best balance between  6all improvements are significant well beyond a confidence of 0.99 based on a 2-tail paired t-test.

0.2  0.25  0.3  0.35  0.4  0.45  0.5  1 2 3 4 5 6 7 8 9 10 20  F -m  ea su  re  Recommended Tags  ?LDA and LM? ?FolkRank?  ?LM?  Fig. 2. Macro f-measure for different numbers of recommended tags  FolkRank LDA&LM Original tags tags score tags score  from user microformats 0.0138 microformats 50.6 howto 0.0078 howto 15.1  standards 0.0070 tutorial 12.8 tutorial 0.0068 standards 11.5  collection 0.0066 programming 9.6 information 0.0064 reference 8.4  webdev resources 0.0061 semantic 7.2 programming tutorials 0.0060 development 5.3  reference webdev 0.0024 software 4.0 web2.0 tool 0.0021 web 3.4  webdesign development 0.0020 xml 3.2 xhtml programming 0.0018 webdesign 3.0  microformats web 0.0013 code 2.7 tutorial html 0.0011 tool 2.4  code 0.0009 webdev 2.3 software 0.0007 information 2.3 javascript 0.0006 css 2.0  python 0.0005 design 2.0 snippets 0.0004 tips 2.0  optimization 0.0004 tutorials 1.6  TABLE IV RECOMMENDED TAGS FOR USER 800 AND RESOURCE  ?HTTP://WWW.XFRONT.COM/MICROFORMATS/?  recall and precision. This also reflects the tagging behaviour of users who on average assigned 4.3 tags to one resource in our dataset. Again the approach presented in this paper clearly outperforms FolkRank and smoothed language models, with the latter two being more or less on par.

To get an impression of the actual tags recommended, Ta- ble IV gives a randomly picked example of tags recommended by FolkRank and by the approach introduced in this paper.

The correctly predicted tags are in bold. We see that our approach correctly predicts 4 tags in top 6, and 6 correct predictions in the top 20,whereas FolkRank predicts only 4 in top 20. But of course a single example can only provide anecdotal evidence. For tag recommendation both tag sets make intuitively sense and the underestimation of the hold- out strategy can be observed.



VI. CONCLUSION  In this paper we have explored user-centered and resource- centered approaches for personalized tag recommendation. We compared and employed a language modeling approach and an approach based on Latent Dirichlet Allocation. We further- more thoroughly investigated the use of language models and LDA for tag recommendation showing that simple language models built from users and resources yield competitive per- formance while consuming only a fraction of the computa- tional costs compared to more sophisticated algorithms. We showed that the combination of both methods (LDA and LM) tailored to users and resources outperforms state-of-the-art tag recommendation algorithms with respect to a broad variety of performance metrics.

For future work we want to investigate the use of these methods for item or user/community recommendation in the context of tagging systems. It would also be interesting to see whether the behavior of the algorithms changes when applied to a photo or video tagging system instead of a tagging system for web pages. Finally, we also plan to investigate how addi- tional contextual knowledge such as time, location, and current task can be used to further improve tag recommendation.

