Improving ReduceTask Data Locality for Sequential MapReduce Jobs

Abstract?Improving data locality for MapReduce jobs is critical for the performance of large-scale Hadoop clusters, embodying the principle of moving computation close to data for big data platforms. Scheduling tasks in the vicinity of stored data can significantly diminish network traffic, which is crucial for system stability and efficiency. Though the issue on data locality has been investigated extensively for MapTasks, most of the existing schedulers ignore data locality for ReduceTasks when fetching the intermediate data, causing performance degradation.

This problem of reducing the fetching cost for ReduceTasks has been identified recently. However, the proposed solutions are exclusively based on a greedy approach, relying on the intuition to place ReduceTasks to the slots that are closest to the majority of the already generated intermediate data. The consequence is that, in presence of job arrivals and departures, assigning the ReduceTasks of the current job to the nodes with the lowest fetching cost can prevent a subsequent job with even better match of data locality from being launched on the already taken slots. To this end, we formulate a stochastic optimization framework to improve the data locality for ReduceTasks, with the optimal placement policy exhibiting a threshold-based structure.

In order to ease the implementation, we further propose a receding horizon control policy based on the optimal solution under restricted conditions. The improved performance is further validated through simulation experiments and real performance tests on our testbed.



I. INTRODUCTION  MapReduce [1] has become quite popular due to its simplic- ity and flexibility for analyzing large volumes of unstructured data. Most organizations deploy MapReduce clusters with its open-source implementation such as Hadoop [2]. As one of the most popular MapReduce implementations, Hadoop [2] is currently maintained by the Apache Foundation, and supported by leading IT companies such as Yahoo!, Google, Facebook, Amazon, and IBM.

MapReduce jobs need to be scheduled under resource con- straints with performance goals; Fair Scheduler [3] is one of the widely used schedulers. A guiding principle on improving the system performance is to move computation close to data, which has been shown to be effective in enhancing the performance for distributed computing systems [4], [5].

The issue on data locality has been extensively investigated for MapTasks [4]?[9]. However, most of the existing work ignores the fact that the generated intermediate data also need to be fetched by the ReduceTasks from each MapTask, through either network or disk I/O. Though this issue is recently tackled in [10]?[14], the solutions are exclusively based on a greedy approach, i.e., always greedily placing ReduceTasks  to the slots that are closest to the majority of the already generated intermedate data [11]?[14]. The consequence is that, in presence of job arrivals and departures, assigning the ReduceTasks of the current job to the nodes with the lowest fetching cost can prevent a subsequent job with even better match of data locality from being launched on the already taken slots. We relate the ReduceTask assignment problem to a classic sequential stochastic assignment work [15], [16], and formulate a stochastic optimization framework to minimize the fetching cost. This formulation is tangent to other optimization techniques that have been adopted for Hadoop, e.g., Delay Scheduling [5] or deadline-orinented scheduling policies [7], [17], [18]. These techniques can be jointly implemented and thus we avoid the problem with contradicting objectives [19].

A. Background of MapReduce For a Hadoop cluster a JobTracker running on the master  node is responsible for handling complicated data distribution, assigning tasks and fault-tolerance. It divides a MapReduce job into two types of tasks, MapTasks and ReduceTasks, and assigns tasks to the TaskTrackers that reside on all of the slave nodes for parallel data processing.

MapTasks are short and independent, launched in waves to process data splits. MapTasks from different jobs can share the same pool of map slots. Measurements show that the map phase is approximately conducting processor sharing discipline under Fair Scheduler [20]. Differently, ReduceTasks have long execution times that span over the whole process of the map phase, because a ReduceTask needs to fetch the in- termediate data from every MapTask through the copy/shuffle phase. A ReduceTask is launched almost immediately when its MapTasks start. Once it is launched, it stays with the reduce slot until completion. Therefore, the copy/shuffle phase of the ReduceTasks overlaps the map phase when the MapTasks of the same job are in processing. Only after a ReduceTask fetches all the intermediate data can it start the reduce phase.

Hadoop can limit the number of jobs that run simultane- ously. For example, the following Hadoop cluster supports at most 50 jobs at the same time through related configuration  <pool name=admin> <maxRunningJobs> 50 </maxRunningJobs>  </pool>.

In addition, Hadoop needs to configure the number of map and reduce slots for each slave node. Each slot grants a single   2013 Proceedings IEEE INFOCOM     task to run at any time. The JobTracker assigns MapTasks and Reducetasks from jobs in the queue to available map slots and reduce slots separately, one task per slot. For example, the number of ReduceTasks that can run simultaneously can be specified by the following parameter  <name>mapred.tasktracker.reduce.tasks.maximum</name>.

B. Copy/shuffle phase and fetching cost  Analysis by HiTune [21] has demonstrated that the point- to-point movement of intermediate data between MapTasks and ReduceTasks in the copy/shuffle phase is crucial for the performance of Hadoop. So much so, that seperate channels in this phase are even introduced to accelerate the data shuffling, e.g., using high-performance InfiniBand network [22], [23].

Direct communication channels have also been proposed by MapReduce Online [24] to instantly shuffle the intermediate data. There are also a number of suggestions on improving data transfer from algorithmic perspectives [10]?[14]. Regarding the performance on a virtualized environment, it has been shown that carefully placing the virtual machines that run the MapReduce clusters on physical nodes can impact the job execution time and network traffic significantly [10]. However, none of them has investigated the problem of improving copy/shuffle performance for sequential jobs through placing ReduceTasks.

Fig. 1. Hadoop tree topology  To quantify the cost of moving intermediate data, we introduce the concept of fetching cost. Hadoop assumes a tree topology G = (V,E) that comprises a set V of nodes and a set E of edges. A typical example is illustrated in Fig. 1. Denote by h(u, v) the hop distance between node u and v, and by X(u) the size of the intermedidate data stored on node u for a ReduceTask. When transferring the stored data on node u to node v, the network cost is proportional to X(u)h(u, v). Thus we can define the total network cost of transferring all stored intermediate data to a ReduceTask that is placed on node v by C =  ? u?V X(u)h(u, v). If X(u) are evenly distributed  on all the nodes X(u) ? X , then we have  C = X  (? u?V  h(u, v)  ) ,  where ?  u?V h(u, v) ? H(v) is the accumulated hop dis- tances for a node v. Therefore, in the following we will refer to H(v) as an indicator of the fetching cost per bit due to network topology. In general, this cost can also depend on the uneven distribution of the intermediate data. Furthermore,  since the involved switches may have different speeds, we can instead use weighted hop distances accordingly.

For example, in Fig. 1 we can compute the accumulated hop distances for node A and G, H(A) = 13 and H(G) = 19, e.g.,  H(A) = h(A,B) + h(A,C) + h(A,D) + h(A,E)  + h(A,F ) + h(A,G) + h(A,H)  = 1 + 1 + 1 + 2 + 2 + 3 + 3 = 13.

Note that the actual network topology of a Hadoop cluster can be much more complicated, especially when Hadoop nodes run on scattered virtual machines. For instance, when deploying a virtualized Hadoop cluster in a Infrastructure-as- a-Service Cloud environment (e.g., Amazon EC2) where tens of thousands of servers are inter-connected with hierarchical network and virtual machines are randomly chosen for users, the number of hops between any two nodes (a.k.a. virtual machines) can be very different. As a result, the cost of data transferring also varies heavily across different Hadoop nodes. The fetching cost per bit H(v) of a node v essentially provides a measure of the quality so that we can compare the performance of different nodes in a quantitive way. With this being said, a value from a given set can be assigned to a working node by monitoring its performance, e.g., when the topology information is not directly available in a cloud environment.

C. Motivating experiments We illustrate the importance of optimizing the placement of  ReduceTasks for a MapReduce job through real experiments.

First, we measure the intermediate data transferred over net- work when running two different jobs, as depicted in Fig. 2.

It shows that different jobs can generate various volumes of  0 200 400 600 800 1000 1200     x 10   Time (second) Grep program  B yt  es /s  ec on  d  Total network traffic: 4.9 Gbytes      network_in network_out  0 100 200 300 400 500 600 700     x 10   Time (second) Sort program  B yt  es /s  ec on  d  Total network traffic: 20.3 Gbytes      network_in network_out  Fig. 2. Intermediate data transferred over network for two MapReduce jobs  intermediate data. More importantly, the information about the size of the intermediate data is only available when a job has started. A common and accurate approach is to measure the intermediate data generated from the first few finished MapTasks, and use a linear model to estimate the total intermediate data [25]. Precisely, ReduceTasks of a job are launched after 5% of its MapTasks have completed, following the default slowstart configuration in Hadoop. Therefore, when observing the sizes of the intermediate data Xdonei generated from the finished mdone MapTasks, we can estimate the total intermediate data Xi of job i that has m MapTasks according to a linear relationship  Xi = m  mdone Xdonei .

2013 Proceedings IEEE INFOCOM     Hereby we emphasize that before a job touches the input data and processes a few MapTasks, it is very difficult to estimate the total size of the intermediate data even when the job is waiting in the queue, since the user-provided map program can be very unpredictable in terms of how to process the input data and generate the intermediate data.

Fig. 3. Performance variation due to changing ReduceTask placement  Next, we demonstrate that the system performance can degrade dramatically if ReduceTasks are not placed carefully.

We repeat the same sort program two times on 7 slave nodes by placing the ReduceTasks differently. As shown in Fig. 3, these two scenarios result in significant difference in job processing times and I/O rates. Specifically, the bad placement of ReduceTasks shown on the left-hand side in Fig. 3 yields a job processing time by 1.75 times longer than the one on the right-hand side.



II. MODEL DESCRIPTION  This section presents the model for studying the Reduc- eTask data locality based on the observations described in Sections I-B and I-C. The schematic diagram of this model is presented in Fig. 4, and the details are described in the rest part of this section.

Fig. 4. Scheduling model for MapReduce  We assume that the cluster can run at most K number of jobs simultaneously and the cluster has r number of reduce slots indexed by the list (1, 2, ? ? ? , r). Upon the submission of job i, its map workload (consisting of multiple MapTasks) joins the map queue and its ReduceTasks join the reduce queue at the same time, as shown in Fig.4. We are interested in the scenario when any job in service can claim the required number of reduce slots immediately after the map phase of this job receives service. This can ensure an efficient processing of the submitted mapreduce jobs since the copy/shuffle phase effectively overlaps with its map phase. One suffient way is to configure the total number r of reduce slots in the cluster greater than or equal to Kr?, where r? is the maximum number  of ReduceTasks for any job. In practice, there are rules of thumb to guide the configuration of the number of reduce slots such that it can match with the number of map slots and the workload injected to the MapReduce cluster. Let Ai be the time interval between the arrival points of job i and i? 1.

1) MapTasks: Denote by Bi the total workload for the MapTasks of job i, which is measured by the time taken to process the map phase of job i without other jobs present in the system. Under Fair Scheduler, the map phases of every jobs in service get the same share of map slots, as demonstrated in [20], [26]. Since there are at most K jobs in service simultaneously, the map phase can be modeled by the K limited processor sharing discipline.

Definition 1 (K limited processor sharing): For a queue that serves jobs according to their arrival times with at most K jobs simultaneously in service at any time, each job in service is attended with 1/min(n(t),K) fraction of capacity, assuming n(t) ? 1 jobs in queue at time t. If n(t) > K then n(t)?K jobs need to wait in the queue.

2) ReduceTasks: The reduce phase is modeled by a multi- server queue. Job i claims a random Ri, 1 ? Ri ? r? number of reduce slots immediately upon its arrival, and the reduce phase overlaps with the map phase. Denote by Xi the total amount of intermediate data that needs to be fetched by the ReduceTasks of job i. Let Xji , 1 ? j ? Ri be the amount of intermedate data shipped to the jth reducer of job i; clearly?Ri  j=1 X j i = Xi. If X  j i are equal then X  j i = Xi/Ri. Real  trace studies [27] show that the performance bottleneck is in the copy/shuffle phase and the map phase. Therefore, in our model, we assume that the time to process the real reduce function can be neglected for ReduceTasks.

Definition 2: Assume that {Bi}i>??, {Xi}i>?? and {Ri}i>?? are three sequences of mutually independent i.i.d.

random variables that are also independent from all other random variables, with Bi  d = B, Xi  d = X and Ri  d = R.

Remark 1: The assumption that Bi is independent of Xi is to ease the analysis. As shown in Fig. 2 and further demonstrated in Section V-A, they can be correlated. Our simulation experiments will take this into consideration. A beneficial consequence of this correlation is that carefully placing the ReduceTasks can not only decrease the fetching cost but also speed up the job proceessing.



III. MAIN RESULTS  In order to improve the system performance, we want to place ReduceTasks on the nodes such that the total fetching cost is miminized. In this regard, we study two different scenarios. First, we consider the case when the job processing time is much longer than the job arrival interval. We relate this problem to the classic stochastic sequential assignment problem [28]. Second, we investigate a more realistic case when jobs arrive and depart. After a job leaves the system, it will release the occupied reduce slots, which can be further taken by later jobs. Let ?i(j) be the index of the reduce slot where the jth ReduceTask (1 ? j ? Ri) of job i is placed.

2013 Proceedings IEEE INFOCOM     A. Assign K sequential jobs with infinite service time  When the job processing times are very long, e.g., Bi =?, the system can only process K MapReduce jobs. Therefore, we consider the problem of minimizing the fetching cost incurred by the first K jobs with the system initially being empty. Specifically, we want to minimize  E  ? ? K?  i=1  Ri? j=1  Xji H(?i(j))  ? ? . (1)  Under either of the following two assumptions: 1) Xji = Xi/Ri, or 2) {Xji }1?j?Ri are i.i.d. conditional on Ri for each fixed i, the objective (1) is equivalent to  E  ? ? K?  i=1  Xi Ri  Ri? j=1  H(?i(j))  ? ? . (2)  When the ith job arrives, 1 ? i ? K, it sees a num- ber of available reduce slots, which is denoted by the list L(i) = (i1, i2, ? ? ? il(i)) of length l(i) with the corresponding accumulated hop distances Hi(1) ? Hi(2) ? ? ? ? ? Hi(l(i)).

The condition Kr? ? r ensures that l(i) ? (K ? i + 1)r? for all 1 ? i ? K. Therefore, we only need to consider the first (K ? i + 1)r? reduce slots in the list L(i). Equivalently, we can just assume that l(i) = (K ? i+ 1)r?.

Theorem 1: For each 1 ? i ? K with l(i) empty reduce slots, there exist  ? = q0,i ? q1,i ? ? ? ? ? ql(i+1)+1,i = ??, that are independent of Hi(j), such that when qy,i > Xi/Ri ? qy+1,i the optimal placement that minimizes (2) is to assign the Ri number of ReduceTasks to the slots indexed by iy, iy+1, ? ? ? , iy+Ri in the list L(i) when job i arrives.

Lemma 1 (Hardy?s lemma [29]): If X1 ? X2 ? ? ? ? ? Xn and H(1) ? H(2) ? ? ? ? ? H(n) are two sequences of numbers, then for any permutation ?(1), ?(2), ? ? ? , ?(n) of 1, 2, ? ? ? , n, we have  n? i=1  H(i)Xi ? n?  i=1  H(?(i))Xi. (3)  To ease the presentation we define the total expected fetching cost Ci?K for jobs i, ? ? ? ,K, 1 ? i ? K under an optimal ReduceTask placement policy when job i sees the set of available slots with accumulated hop distances Hi(1), Hi(2), ? ? ? , Hi(l(i)). Correspondingly we can define the conditional expected fetching cost Cx,vi?K conditional on the event that Xi = x and Ri = v under an optimal policy.

Proof of Theorem 1: The proof is based on induction, starting from i = K back to i = 1. We proceed with the case i = K. Clearly, the optimal placement is to assign the ReduceTasks of job K to the first RK slots in LK = (K1,K2, ? ? ? ,Kl(K)) that have the smallest accumulated hop distances. Therefore, we can set q0,K = ? and q1,K = ?? since l(K + 1) = 0.

Now, suppose that the results are true for all i > n.

Specifically, there exist numbers {qu,i}1?u?l(i+1)+1 that are independent of Hi(j), 1 ? j ? l(i) for all i > n, 1 ? n ? K, such that the optimal placement is to assign the Ri number of ReduceTasks to the slots indexed by iy, iy+1, ? ? ? , iy+Ri in the list L(i) when q0,y < Xi/Ri ? q0,y+1. Then, when job n arrives, bringing Rn = rn ReduceTasks and Xn = xn intermediate data, we obtain, for any subsequence (nz1 , ? ? ? , nzrn ) of the list L(n),  Cxn,rnn?K =  max (z1,??? ,zrn )?L(n)  ? ?xn  rn  rn? j=1  Hn(zj) + C(n+1)?K  ? ? . (4)  Using the induciton hypothesis, we know that the optimal placement scheme to optimize C(n+1)?K only depends on the order of the accumulated hop distances of the available reduce slots and is independent of their exact values. Hence, under the optimal placement scheme, defining qj,n as the expected value of the size of the intermediate data fetched at the slot with the jth smallest accumulated hop distance in the optimal scheme of C(n+1)?K . Hence,  C(n+1)?K = l(n+1)? j=1  Hn+1(j)qj,n. (5)  Furthermore, since qj,n is independent of the values of the accumulated hop distances Hn(j) and by the induction as- sumption C(n+1)?K reaches its maximum, it follows that, using Hardy?s lemma (Lemma 1),  q1,n ? q2,n ? ? ? ? ? ql(n+1),n. (6) Now, using (4), (5), (6), and applying Hardy?s lemma, we  obtain, if qy,n < xn/rn ? qy+1,n  Cxn,rnn?K = xn rn  y+rn? j=y  Hn(j) +  y?1? j=1  Hn(j)qj,n  +  l(n+1)? j=y+rn+1  Hn(j)qj,n, (7)  which is to assign the ri number of ReduceTasks to the slots indexed by iy, iy+1, ? ? ? , iy+Ri in the list L(n). This completes the induction.

The preceding result reveals the structure of the optimal placement, which is based on thresholds. The following the- orem explains the procedure to calculate these thresholds.

Define F (x) ? P[X/R ? x] and F? (x) ? P[X/R > x].

Theorem 2: Setting q0,n = ?, ql(n+1),n = ?? for n = 1, 2, ? ? ? ,K with q0,K = ? and q1,K = ??, we have the following recursion: ? if i > r?,  qi,n?1 = qi,nF (qi,n)  +  r?? j=1  P[R = j]  (? qi?j,n qi,n  ydF (y) + qi?j,nF? (qi?j,n)  )  2013 Proceedings IEEE INFOCOM     ? if 1 ? i ? r?,  qi,n?1 =  ? ? r??  j=i  P[R = j]  ? ? ? ? qi,n  ydF (y)  + i?1? j=1  P[R = j]  (? qi?j,n qi,n  ydF (y) + qi?j,nF? (qi?j,n)  )  + qi,nF (qi,n)  where ?? ? 0 and ? ? 0 are defined to be 0.

Remark 2: The preceding result can be used to recursively  compute the values qj,i. For instance,  qj,K?1 = P[R ? j] ? ?  ydF (y),  where 1 ? j ? r?.

Proof: We first prove the result with the condition i > r?.

Recall that qj,n is the expected value of the size of the intermediate data fetched at the slot with the jth smallest accumulated hop distance when the reduce placement is under the optimal scheme for jobs n+ 1, ? ? ? ,K.

Conditioning on the value of Xi = u,Ri = v, we are interested in qi,n?1. Applying Hardy?s lemma, we know that the i?th slot in the list L(n ? 1) should be assigned to one ReduceTask of job n? 1 if and only if qi?v,n > u/v ? qi,n.

This is because we need to insert v number of larger values after qi?v,n+1, which covers the i?th position in the list (qj,n)1?j?l(n). If u/v < qi,n, then the i?th position will get an expected network transfer cost qi,n under the optimal policy, since the v number of values will be inserted after the i?th position in (qj,n)1?j?l(n). If u/v > qi?v,n, this expected value is equal to qi?v,n since there will be v values larger than qi?v,n that needs to be inserted in front of qi?v,n.

The other scenario i ? r? involves a corner case that needs to be taken care of separately, and it can be proved using similar arguments.

B. Assign sequential jobs for a stable system  In this section, we study a more realistic scenario when jobs can come and leave. It combines the study of a queueing system with the sequential stochastic assignment for Reduc- eTasks. In order to simplify the analysis, we assume that {Bi} is a sequence of i.i.d. exponential random variables and the jobs arrive according to a Poisson process with rate ?. For a sequence of jobs i,?? < i < ?, denote by Wq(i) the number of jobs in the queue and Ws(i), 0 ? Ws(i) ? K the number of jobs in service observed by job i, respectively.

For job i in service, we use ?(i) = (i1, ? ? ? , iRi) to denote the indexes of the reduce slots occupied by job i where ij ? {1, 2, ? ? ? , r}. Let ?i(j) be the index of the slot where the jth ReduceTask (1 ? j ? Ri) of job i is placed.

Since exponential random variables are memoryless, we can use S(i) = ((?(j), 1 ? j ?Ws(i)) ,Wq(i)) to denote the state of the system observed by job i. An important class of policies is the so-called stationary ones. A policy is said to be stationary if the scheduling taken at any time only  depends on the state of the system at that time, i.e., S(i) for job i. Precisely, a stationary policy is a deterministic function mapping the state information into a control action.

Lemma 2: If ?E[B] < 1 and Xji are i.i.d. conditional on Ri, then under any stationary reduce placement policy, the following limit exists  C? ? lim n??   n  n? i=1  Ri? j=1  Xji H(?i(j)). (8)  Proof: This result is based on the property of renewal processes. A busy period is when the map queue has at least one job in service; an idle period is when the map queue does not have any running jobs. Since the map queue conducts K limited processor sharing (a work-conserving policy), we know that the system alternate between idle Ii and busy Bi periods with the condition ?E[B] < 1, by applying the well- known queueing result for GI/GI/1 queue [30]. Since the arrival follows a Poisson process, {Ii} and {Bi} form two independent i.i.d. random sequences, as shown in Fig. 5.

Fig. 5. Renewal cycle  Denote by Cm the total network transfer cost and by Nm the number of jobs processed in the mth busy period, respectively.

Since the placement policy is stationary, we know that {Cm} and {Nm} are also two i.i.d. random sequences. Therefore, using the law of large numbers, we obtain  lim n??   n  n? i=1  Ri? j=1  Xji H(?i(j)) = limm??  ?m i=1 Cm?m i=1 Nm  = lim m??  ?m i=1 Cm m  m?m i=1 Nm  = E[C1]  E[N1] . (9)  In the following part, we investigate the optimal placement scheme that assigns ReduceTasks on the nodes such that the long term average network transfer cost per job C? is miminized. We first introduce some notation. For a set S, denote by SP its power set, and by SPk the subset of S  P  whose element each exactly contains k number of values. For A ? NP with N = {1, 2, ? ? ? , r}, define H(A) ??a?A H(a) where H(a) is the accumulated hop distances of the slot with index a.

The state Q = ((?(j), 1 ? j ?Ws(i)) ,Wq(i)) of the system observed by job i upon its arrval is from the state space(( ?(j), 1 ? j ? ms,?(j) ? NP  ) ,mq  ) where mq,ms ? N?  {0}. Job i arrives and sees the set L(i) of available reduce slots; if |L(i)| = 0 then job i needs to wait in the queue according to FIFO (first in first out) discipline. For a fixed v ? 1, we sort all the elements in L(i)Pv using the metric H(?) to form a list LL(i) according to increasing order when |L(i)| > 0.

2013 Proceedings IEEE INFOCOM     Theorem 3: If ?E[B] < 1 and Ri = v, then, there exist  ? = q0,i ? q1,i ? ? ? ? ? q|L(i)Pv |+1,i = ??, such that when qy,i > Xi/Ri ? qy+1,i the optimal placement to minimize C? is to assign the Ri number of ReduceTasks to the slots indexed by the yth element in the list LL(i) when job i arrives.

Remark 3: Since all random variables follow exponen- tial distributions, Theorem 3 implies that the process ((?(j), 1 ? j ?Ws(i)) ,Wq(i)) evolves according to a finite state Markov process if the optimal placement scheme (which is a threshold-based stationary policy) is applied.

Proof: Let D be any element in the state space(( ?(j), 1 ? j ? ms,?(j) ? NP  ) ,mq  ) . Since both {Bi} and  {Ai} follow exponential distributions that have the memory- less property, we can define the optimal expected fetching cost C(D) starting from the system state D until the first time when the system becomes empty, i.e., at the end of the busy period.

Since the expected number of jobs arriving during this period has a finite mean under the condition ?E[B] < 1 [30], we know that C(D) is finite and well defined.

When job i arrives, it observes a state Q. Suppose Xi = u,Ri = v, then, the optimal policy is by finding  min A?L(i)Pv  (H(A)u/v + C(A ?Q)) , (10)  where A ? Q means placing ReduceTasks on slots indexed by set A when observing the state Q. The value H(A)u/v +  Fig. 6. Minimum of a class of linear functions  C(A?Q) can be viewed as being evaluated at x = u/v for a linear funciton H(A)x+C(A?Q). Regarding the inteception C(A?Q) and the slope H(A) of these linear functions, we can prove the following result. For any A1, A2 ? L(i)Pv , H(A1) ? H(A2) implies C(A1 ?Q) ? C(A2 ?Q) and vice versa, i.e.,  H(A1) ? H(A2)? C(A1 ?Q) ? C(A2 ?Q). (11) The preceding result is because if H(A1) < H(A2), then exchanging all the indexes in set A1 for the ones in A2 can only increase the total expected fetching cost. The same arguments also apply in the reverse direction. The value in (10) is illustrated in Fig. 6.

Using (11) and applying induction to the number of linear functions, we can finish the proof of this theorem.

In presence of job arrivals and departures, the optimal placement scheme is still a threshold based policy. However, the thesholds now depend on the accumulated hop distances  and the arrival/departure rates, different from the policy cal- culated in Section III-A that only depends on Xi and Ri. This dependency makes the computation of these optimal thresholds difficult. To ease the implementation, we provide a policy based on the receding horizon control in Seciton IV.



IV. RECEDING HORIZON CONTROL POLICY  Section III-B provides a theoretical framework to study the optimal placement policy under restricted assumptions, e.g., {Bi} have been assumed to be i.i.d. exponential random variables. The optimal policy is based on thresholds, which however involve complex computations that are not easy to im- plement. In real MapReduce clusters, the assignment decisions are made under other non-idealized conditions with practical engineering concerns. To this end, we provide a heuristic receding horizon control policy based on the insights obtained from previous analysis. This policy is easy to implement and our simulation results in Section V-B also show significant performance improvement, though it is not optimal in yielding the long term average fetching cost.

The receding horizon control [31] policy is to optimize the ReduceTask placement of the newly arrived job by assuming that in the future only one more job that requires the same number of ReduceTasks will be submitted to the system before any other running job finishes, as illustrated in Fig. 7.

Fig. 7. Receding horizon control policy  The receding horizon control policy is based on solving the optimal policy for K = 2, i.e., a system with only two sequential jobs. Under the assumption that Ai, Bi follow exponential distributions with rate ? and ?, we know that [30] the probability that the first job still has not finished when the second job arrives is equal to p = ?/(? + ?) = ?/(1 + ?) where ? = ?/?. We need to find an optimal policy such that the expected fetching cost for these two jobs can be minimized. We fomulate the following stochastic optimization, by assuming R1 = R2 = R,  C? = minE  ? ? R? j=1  Xj1H(?1(j)) +  R? j=1  Xj2H(?2(j))  ? ? , (12)  where H(1) ? H(2) ? H(2) ? ? ? ? denote the ordered list L of accumulated hop distance values.

There are two reasons why we posulate the condition R1 = R2 = R. First, in practice, the jobs arriving temporally close often exhibit positive correlations. For example, a number of HBase queries, each being a MapReduce job, are submitted to discover the answers related to the same topic; several large batch jobs, each with a different goal, need to provide statistic  2013 Proceedings IEEE INFOCOM     reports on log files collected on the same day. Therefore, in these scenarios the number of ReduceTasks of two jobs arriving sequentially may be close to each other. Second, our receding horiozn control should be easy to implement in the real MapReduce system. We want to avoid the complicated computation introduced by the recursive equations, e.g., shown in Theorem 2. Interestingly, under the assumption R1 = R2 we have a very simple form for the optimal policy that minimizes (12), as characterized by the following theorem.

Theorem 4: The optimal placement policy has a simple threshold-based structure: if X1 ? R1?/(1 + ?)E[X/R], then assign the R1 number of ReduceTasks to the slots indexed by 1, 2, ? ? ? , R1 when job 1 arrives; otherwise to R1 +1, R1 +2, ? ? ? , 2R1. When job 2 arrives, it always takes the best R2 slots from the available ones.

In order to prove the theorem, we need the following lemma.

Lemma 3: For a real-valued close set A and real numbers  a, b, the linear function ax+ by reaches the minimum within the set (x, y) : x + y = n, x ? A only if |x ? y| reaches the maximum.

Proof: The proof is simply by observing that ax+ by = (a ? b)x + bn, which reaches its minimum when x is either the smallest or the largest value in A.

Proof of Theorem 5: Job 1 brings X1 = u,R1 = v upon its arrival. Denote by C?u,v the minimal expected network cost conditional on X1 = u,R1 = v, i.e.,  C?u,v =  E  ? ? R? j=1  Xj1H(?1(j)) +  R? j=1  Xj2H(?2(j)) ???X1 = u,R1 = v  ? ? .

Suppose that we assign job 1 to the nodes indexed by B = {i1, i2, ? ? ? , iv}. When job 2 arrives, it sees job 1 still running in the system with probability p = ?/(? + ?) and sees an empty system with probability ?/(?+?) = 1?p. If job 1 has not finished yet, then job 2 has no other choices and can only choose the slots in BC , where BC is the complement of B in {1, 2, ? ? ? , 2v}. Otherwise, job 2 can allocate its ReduceTasks to the best slots.

Using the afore-mentioned arguments, we obtain  C?u,v = u/v  ? ??  j?B H(j)  ? ?+ pE[X/R]  ? ??  j?BC H(j)  ? ?  + (1? p)E[X/R] ? ? v?  j=1  H(j)  ? ? . (13)  Therefore, applying Lemma 3 we obtain that C?u,v reaches its minimum only if  ??? (?  j?B H(j) ) ? (?  j?Bc H(j) )??? at-  tains its maximum, implying that B can be only equal to {1, 2, ? ? ? , v} or {R+ 1, R+ 2, ? ? ? , 2v}.

Again, applying Hardy?s lemma, we know that the optimal policy is based on the relative relationship between u/v and pE[X/R]: if u/v ? pE[X/R] then B = {1, 2, ? ? ? , v}; otherwise B = {R+1, R+2, ? ? ? , 2v}. This finishes the proof  of the theorem.

Now we can describe the implementation of this placement  policy. Since the policy only involves a single threshold ?/(1 + ?)E[X/R], we need to estimate ? and E[X/R]. In recoganizing the fact that workload statistics is usually time- varying, e.g., depicting the peak and off-peak hours during a day, we resort to an adaptive dynamic control policy that con- stantly updates the estimates of the two required parameters.

Specifically, upon the arrival of a new job i, we observe the number Ni of jobs present in the system, the number Ri of ReduceTasks and the total amount Xi of intermedate data that will be generated by job i. We maintain a list W = (w1, w2, ? ? ? , w? ), wj = (nj , xj , rj), 1 ? j ? ? of size ? to record the latest ? observations (Ni, Xi, Ri). Then, we update the following estimates of the average queue length N and the average intermedate data per ReduceTask X/R,  N =  ?  ?? j=1  nj , X/R =  ?  ?? j=1  xj/rj ,  where N and X/R can be initially set to N1 and X1/R1.

By the well-known queueing result, we know EN = ?/(1?  ?), implying p = ?/(1 + ?) = N/(2N + 1). The assignment policy then follows what has been described in this section, immediately placing the arriving job?s ReduceTasks.



V. MEASUREMENTS AND EXPERIMENTS  This section contains three parts. First, we provide and analyze measurements collected from extensive experiments on Hadoop to see the relationship between job processing times and the sizes of the intermediate data. Second, we use simulation experiments to demonstrate the good performance of receding horizon control compared to random placement policy and the greedy policy. Last, we test the performance of receding horizon control policy on 15 nodes. The result shows great improvement to the job processing times.

A. Measurements  In our theoretical study, we have assume that {Bi} and {Xi} are two i.i.d. sequences. To investigate to what extend this assumption can serve as a good approximation. We collect a large number of measurements through extensive experiments, from which we observe some interesting relationships.

The measurements are collected on 7 slave nodes. Each node has 4 cores (2933MHz, 32KB cache size), 6GB of memory and 72GB of disk space, with configuration 4 map slots and 2 reduce slots per node. We test Grep program running on a file of 10.0G bytes, each line of which contains a randomly generated 10 digit integer. In order to change the amount of intermediate data, we vary the regular expression provided to each MapTask.

Interestingly, we see linear relationships with different slopes between the job processing times and the intermedate data sizes, which highly depend on the types of regular expressions. The three curves in Fig. 8 correspond to the three blocks in Table I, respectively. In the figure, we also compute  2013 Proceedings IEEE INFOCOM     Fig. 8. Relationships between intermedate data size and job processing time  the coefficient of determination (R2) that is used to describe how well a regression line fits a set of data; a value of R2  near 1.0 indicates a good fit.

TABLE I INTERMEDIATE DATA SIZES AND JOB PROCESSING TIMES  Regular Intermedate data Output Time expression (MBytes) (MBytes) (Seconds) [1? 2]? 184.41 0.01 1110 [1? 3]? 189.22 0.27 1129 [1? 4]? 195.23 3.02 1165 [0? 4]? 210.80 17.51 1208 [1? 5]? 194.96 20.49 1178 [0? 5]? 675.90 86.57 1235 [1? 6]? 243.45 99.23 1249 [0? 6]? 2092.84 336.68 1315 [1? 7]? 2221.29 380.42 1337 [0? 7]? 5495.84 1098.06 1490 [1? 8]? 5641.22 1225.81 1533 [1? 3][1? 8]? 3157.23 463.57 795 [1? 4][1? 8]? 3606.56 616.84 884 [1? 5][1? 8]? 4950.25 769.78 1005 [1? 6][1? 8]? 5584.79 922.49 1083 [1? 7][1? 8]? 5914.40 1074.76 1138 ?[0? 2][0? 2][0? 9]? 1621.28 501.03 295 ?[0? 3][5? 6][0? 9]? 1564.35 501.07 295 ?[0? 4][1? 2][0? 9]? 2442.98 667.95 325 ?1[0? 9]? 2787.25 835.00 371 ?[0? 1][0? 7][0? 9]? 5727.24 1336.03 525 ?[0? 1][0? 8][0? 9]? 5891.73 1502.99 558 ?[0? 2][0? 9]? 6939.92 1670.02 615  B. Simulations  To evaluate our analysis-inspired scheduling scheme, we run a set of simulation experiments and compare the performance of our scheme with two baseline schemes: random policy and greedy policy. The random policy randomly assigns Reduc- eTasks to available ReduceTask slots and is oblivious to the cost difference across slots, which essentially models how the current Hadoop schedules ReduceTasks. Compared with the random policy, the greedy policy is heterogeneity-aware, in the sense that it knows the cost of different ReduceTask slots.

It, however, always assigns ReduceTasks to the available slots with the least cost. Our experiments simulates the ReduceTask assignment process, where we set ? = 1 and vary the value of ? to create systems with different load levels. The system has 1000 reduce slots and we feed the system with 50k MapReduce jobs in the experiments. Job arrival intervals and service times follow exponential distributions. The number of ReduceTasks of a MapReduce job varies from 1 to 10, and the  size of intermediate data ranges from 1 to 100. Furthermore, the fetching costs associated with ReduceTask slots are drawn from a uniform distribution [1, 100].

Sa vi  ng (%  ) ov  er G  re ed  y an  d R  an do  m  ?      0.2 0.3 0.4 0.5 0.6       Greedy Random  Fig. 9. Fetching cost reduction compared to random and greedy placement  Fig. 9 shows the simulation results, where the the x-axis shows the ? value for the job arrival rate and the y-axis indicates the percentage of cost saved by our scheme over the baseline schemes, i.e.,  Baseline Cost - Our Cost Baseline Cost  ? 100%.

As we described earlier, the fetching cost of a policy is com- puted as products of per-slot cost and the intermedidate data size of the assigned ReduceTask, summed over all jobs and divided by the total number of jobs. Compared with the ran- dom policy, our receding horizon control policy consistently reduces the cost by more than 90% for different values of ?, indicating the considerable potential in exploring heterogeneity in ReduceTask slots. When compared with the greedy policy, our scheme still delivers up to 22% improvement, espeically for systems with relatively large ? (i.e., highly utilized Hadoop clusters). This is because the greedy scheme always assign ReduceTasks to available slots with the lowest cost, which often causes tasks with small intermediate data to occupy low- cost slots and later tasks with large intermediate data assigned to slots with relatively high costs. Such scearnios occurs more frequently with higher ?, which contributes to the growing saving percentage of our scheme with increasing ?.

C. Experiments  To test the data locality for reducers, we conduct the ex- periment on 15 slave nodes by running Sort jobs on two files, RandomPair1 and RandomPair3 of size 15.0G and 4.3Gbytes, respectively. These two files are geneated using RandomWriter program. We submit two Sort jobs with submission interval 1 minute, where the first job has 64 MapTask and 2 ReduceTasks and the second job has 224 MapTasks and 4 ReduceTasks.

We have assigned an fetching cost per bit that only takes five values 1, 2, 3, 4, 5 to each reduce slot through learning from running many Sort jobs, as mentioned in Section I-B.

Specifically, this cost value is computed by measuring the ratio of the time a ReduceTask spent on a given slot compared with the average reduce time from repeating the same job many times when assigning the ReduceTasks on different nodes.

We plot the mean, minimum and maximum of the job processing times in Fig. 10 by repeating the experiment 5 times. Under Fair Scheduler, the placements of the reducers  2013 Proceedings IEEE INFOCOM     are random, depending on the order of the received heartbeats.

Differently, aware of the heterogenity of the reduce slots, we  sort RandomPair3 sort RandomPair1      Jo b  pr oc  es si  ng ti  m e  (m in  ut es  )      random placement enhanced placement  Fig. 10. Mean and range of the job processing times by repeating 5 times  can improve the performance, as shown by a shorter average and smaller variance for job processing times in Fig. 10.



VI. CONCLUSIONS  We have described an important ReduceTask placement problem in MapReduce clusters where different reduce slots may be associated with different fetching costs caused by environmental factors such as network topologies. We show that ignoring such heterogeneity could introduce substantial overhead which in turn limits the performance of MapReduce jobs. Through theoretical analysis, we propose optimal Re- duceTask assignment schemes that can minimize the expected fetching cost per job for two different service scenarios. As job characteristics information may not be readily available in production environment, we also develop a receding horizon control policy that makes online scheduling decisions based on partially observed workload information. We perform ex- perimental evaluation with both simulation and real system deployment. Our results suggest that the proposed scheduling scheme, even though derived based on a number of theoretical assumptions, improves the performance of Hadoop/MapRe- duce.

