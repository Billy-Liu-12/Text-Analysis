Data Quality: The other Face of Big Data

Abstract?In our Big Data era, data is being generated, collected and analyzed at an unprecedented scale, and data- driven decision making is sweeping through all aspects of society.

Recent studies have shown that poor quality data is prevalent in large databases and on the Web. Since poor quality data can have serious consequences on the results of data analyses, the importance of veracity, the fourth ?V? of big data is increasingly being recognized. In this tutorial, we highlight the substantial challenges that the first three ?V?s, volume, velocity and variety, bring to dealing with veracity in big data. Due to the sheer volume and velocity of data, one needs to understand and (possibly) repair erroneous data in a scalable and timely manner. With the variety of data, often from a diversity of sources, data quality rules cannot be specified a priori; one needs to let the ?data to speak for itself? in order to discover the semantics of the data.

This tutorial presents recent results that are relevant to big data quality management, focusing on the two major dimensions of (i) discovering quality issues from the data itself, and (ii) trading- off accuracy vs efficiency, and identifies a range of open problems for the community.



I. INTRODUCTION  With the huge volume of generated data, the fast velocity of arriving data, and the large variety of heterogeneous data, the quality of data is far from perfect. It has been estimated that erroneous data costs US businesses 600 billion dollars annually [20]. Enterprises typically find data error rate of approximately 1-5%, and for some companies, it is above 30% [26, 57]. In most data warehousing projects, data cleaning accounts for 30-80% of the development time and budget for improving the quality of the data rather than building the system. On the web, 58% of the available documents are XML, among which only one third of XML documents with ac- companying XSD/DTD are valid [47]. 14% of the documents lack well-formedness, a simple error of mismatching tags and missing tags that renders the entire XML-technology useless over these documents. These all highlight the pressing need of data quality management to ensure data in our databases represent the real world entities to which they refer in a consistent, accurate, complete, timely and unique way. There has been increasing demand in industries for developing data quality management systems, aiming to effectively detect and correct errors in the data, and thus to add accuracy and value to business processes. Indeed the market for data quality tools is growing at 16% annually, way over the 7% average forecast for other IT segments [39].

With the advent of big data, data quality management has become more important than ever. Typically, volume, velocity and variety are used to characterize the key properties of big data. But to extract value and make big data operational, the importance of the fourth ?V? of big data, veracity, is  increasingly being recognized. Veracity directly refers to in- consistency and data quality problems. As [63] states, one of the biggest problems with big data is the tendency for errors to snowball. User entry errors, redundancy and corruption all affect the value of data. Without proper data quality management, even minor errors can accumulate resulting in revenue loss, process inefficiency and failure to comply with industry and government regulations (the butterfly effect [61]).

The Big Data era comes with new challenges for data quality management. Due to the sheer volume and velocity of some data (like stock trades, or machine/sensor generated events), one needs to understand or get rid of the erroneous data extremely fast. And as multi-structured data, often from many different sources, is brought together, determining the semantics of data and understanding correlations between attributes becomes a daunting task. In contrast to traditional data quality management, it is impossible to specify all the data semantics beforehand and no global semantics may fit the entire data. We need context-aware data quality rules to detect semantic errors in our data, and better still fix those errors by using the rules. We need to learn interesting and informative rules from the dirty data itself [16, 31, 43?45, 51, 70]. We therefore go from the close-world assumption of database systems to an open world view where rules are learned from data, validated and updated incrementally as more data is gathered and based on the most recent data.

Due to the variety of data sources, these rules may apply only to certain subsets of data [3, 30, 43, 45]. Such conditioning requires that proper metrics be ascertained to find statistically robust rules as opposed to outliers since rules are inferred from dirty data itself [43, 45]. Violation of rules indicates data inconsistency. Based on the applications, either one deals with these inconsistencies without repairing them, or finds ways to repair them [2, 11, 14, 22, 33, 35?37, 40, 50, 53, 59, 60, 64, 68]. Due to inherent noise in discovering rules, repairing must adjust between inconsistent data and inaccurate constraints [6, 17].

These various stages of data quality management: dis- covering rules, checking for inconsistencies, repairing, need to be done in a very scalable manner. This brings in the efficiency vs accuracy trade-off. We may have to live with some approximations since generating optimal results are costly [45, 51, 53]. We need either centralized near-linear time procedures or distributed map-reduce processing to deal with the volume [32, 45, 53]. To tackle the velocity, we need incremental and streaming processing [38, 54, 65, 66].

In this tutorial we explore the challenges of data quality management that arise due to volume, velocity and variety of data in the Big Data era. Specifically, our goal is to     cover two major dimensions of big data quality management: (i) discovering/learning based on data, and (ii) accuracy vs efficiency trade-off under various computing models. We will present the state of the art research in these dimensions for relational, structured and semi-structured data and identify many open problems for future research.



II. INTENDED AUDIENCE  The target audience is anyone with interest in learning data quality challenges in the Big Data environment. We expect the tutorial to appeal to a large portion of the ICDE community:  ? Researchers in the fields of data cleansing, data consolidation, data extraction, data mining, and Web information management.

? Practitioners developing and distributing products in the data cleansing, ETL & data warehousing, and master data management areas.

The assumed level of mathematical sophistication will be that of the typical conference audience.



III. TUTORIAL OUTLINE  Our tutorial is organized as follows.

A. Introduction  ? Motivating Examples for Big Data Quality  ? Different Aspects of Data Quality: consistency, accu- racy, completeness and timeliness  ? Statistical vs Logical Data Quality Management  ? Logical Data Quality Management  ? Dependency Theory  Extension with conditions and similarity Static Analysis Aspects of Big Data: volume, velocity, variety  ? Overview of various Data Quality Tools  We start the tutorial by providing a variety of real world cases. The principles for data quality manage- ment can be broadly classified as statistical/quantitative vs logical/constraint-based. In the former, data is corrected based on statistics over value distributions [21, 49], whereas the latter intends to develop models and inconsistencies are reported as violations of the model [3, 11]. In the tutorial, we mainly focus on logical/constraint-based data quality management. In- tegrity constraints (ICs) have been recently repurposed towards improving the quality of data. Traditional types of ICs such as key constraints, check constraints, functional dependencies (FD) and their extension conditional functional dependencies (CFD), conditional inclusion dependencies, matching depen- dencies etc. [4, 30, 70] have been proposed for data quality management. We review the notion of dependency theory and static analysis [4, 30, 36]. Our main goal is to explore the developments in dependency theory pertaining to the big data challenges of volume, velocity and variety. We provide an overview of available data quality management tools and platforms and outline the additional requirements to tackle these challenges [1, 18, 24, 27, 37, 42].

B. Discovering/Learning Data Quality Semantics  ? Discovering Logical Model  ? Conditional vs Full [E.g. learning CFD vs FD] ? Approximate/Soft vs Exact  Measures of robustness  ? Template based learning: learning pattern tableau  Hold Tableaux for summarization vs Fail Tableaux for outliers  ? Learning keys, FDs, DTD and XSD for semi- structured documents  ? Efficiency vs Accuracy  Centralized near linear time algorithms with approximation guarantees Incrementally generating semantic rules Incrementally maintaining pattern tableaux  Due to the large variety of sources from which data is collected and integrated, for its sheer volume and changing nature, it is impossible to manually specify data quality rules.

Big data comes with a major promise: having more data allows the ?data to speak for itself,? instead of relying on unproven assumptions and weak correlations. The key is to learn the rules from the data itself [15, 16, 31, 51, 70].

The rules need to be learnt efficiently, in near linear time centralized or distributed and since the data itself is dirty, to make the rules robust against outliers, approximations need to be allowed [42, 43, 45, 51]. Rules may apply only to part of the data [31, 44, 45]. For learning rules for semi-structured data, both value and structure need to be taken care of. In this part of the tutorial, we focus on efficient learning of integrity constraints [31], schema [9], DTD [8, 41], whether they apply to the entire database or partially [16, 31], proper measures to obtain statistically robust rules [44], generating them incrementally to cope with the dynamic nature of big data [13]. We also focus on template based learning such as learning pattern tableaux for CFD, sequential dependency, conservation dependency etc. [42, 43, 45]. Approximation plays a crucial role both for finding statistically robust rules and also for designing efficient algorithms.

C. Detecting/ Repairing Inconsistencies  ? Repairing Inconsistencies in Relational Databases  ? Minimal Repairs, Sample of Possible Repairs, Chase based Repairs  ? Repairing Structural Problems in Semi-structured Data  ? Well-formedness, Validity: XML, Web data etc.

? Detecting Inconsistencies in Distributed Data and Streaming Data  ? Validation of streaming XML/ Web documents  Once rules have been specified, inconsistencies in data show up as violations to the rules. There are two main approaches to deal with inconsistencies, either they are re- moved [14, 33, 40, 68] or queries are answered in a consis- tent/approximate manner without repairing the database [22,     50, 59]. In this tutorial, we focus only on the first approach. We discuss how inconsistencies in relational and semi-structured data can be detected incrementally in streaming and distributed fashion [32, 38, 54]. Since rules are discovered based on dirty data, inconsistencies may appear as an effect of faulty rules. Therefore, it is required to detect whether the data is inconsistent or the model is incorrect [6, 17]. Computation of all possible repairs requires to explore a space of solutions of exponential size with respect to the size of the database which is infeasible in practice. Hence conditions are imposed on the computed repairs to restrict the search space. These condi- tions include, e.g., various notions of cost-based minimality, maximum likelihood [2, 11, 14, 67], repairing using record linkage and master data [35?37], sampling based repairs [5] and chase-based algorithm to fine-tune the trade-off between quality and scalability of the repair process [7, 12, 48]. We will contrast among these approaches in terms of efficiency, repair strategies, value and solution preferences. For semi- structured data, we highlight the recent progress on finding top-k repairs and validating them in a scalable manner [53, 60, 64]. Again to ensure efficiency, many of these algorithms require approximation and are computed in a distributed or streaming model [32, 54, 65, 66].

D. Open Problems  Distributed and streaming discovery of data quality se- mantics as well as detection and repairing inconsistencies is a fledgling topic and there remain many open problems in this area. Apart from this, there are several new directions for data quality research to consider such as using master data for repairing partially-closed databases [25], crowdsourced data cleaning, and using value and structure for discovering interesting rules in semi-structured documents and resolving conflicts.



IV. RELATIONSHIP TO PRIOR SEMINARS  This is the first time that this seminar will be presented.

There are some available recent books, tutorials and invited presentations that cover static analysis of CFD and matching dependency [23, 26, 28]. In addition, [23, 26] cover certain topics that are included in this tutorial, such as discovering CFD [31] and repairing violations to CFD [36, 37]. We are the first to discuss how the state-of-the-art techniques on data quality for relational, structured and semi-structured data address the challenges raised by the Big Data environment.

Previous seminars and books did not focus on approximation vs efficiency trade-off, while for discovering/repairing rules, our presentation will cover much beyond CFD on relational data.



V. BIOGRAPHIES  A. Barna Saha  Barna Saha is a researcher at AT&T Labs-Research, where she joined after receiving her Ph.D. from University of Mary- land, College Park in 2011. Previously she received a Masters Degree from Indian Institute of Technology, Kanpur, and received a Bachelors Degree from Jadavpur University in India.

Her research interests include algorithms, optimization and various aspects of databases with emphasis on data quality,  data integration and distributed/uncertain data management.

She is the recipient of Deans Dissertation Fellowship Award, University of Maryland, 2010, for her PhD work on scalable approximation algorithm design for distributed workload man- agement. She also received the Best Paper award for her work on ranking under uncertainty at VLDB 2009.

B. Divesh Srivastava  Divesh Srivastava is the head of the Database Research Department at AT&T Labs-Research. He received his Ph.D.

from the University of Wisconsin, Madison, and his B.Tech.

from the Indian Institute of Technology, Bombay. He is an ACM fellow, on the board of trustees of the VLDB Endowment and an associate editor of the ACM Transactions on Database Systems. His research interests and publications span a variety of topics in data management.

