SLPMiner: An Algorithm for Finding Frequent Sequential Patterns  Using Length-Decreasing Support Constraint'

Abstract  Over the years, a variety of algoriihms forfindingfre- quent sequential patterns in very large sequential databases have been developed. The key feature in most of these al- goriihms is that ihey use a constani suppori constraint to control the inherenrly exponential complexity of ihe prob- lem. In general, patterns thai conrain only a few iiems will tend to be inreresting ifihey have a high support, whereas long patterns can still be interesting even if their support is relatively smnll. Ideally, we desire to have an algorithm thatfinds all the frequentpatrerns whose support decreases as afunciion of their length. In this paper we present an al- gorithm called SLPMiner; thatfinds all sequential patterns thar satisfy a length-decreasing support constraini. Our ex- perimental evaluation shows that SLPMiner achieves up to hvo orders of magnitude of speedup by effectively exploit- ing fhe lengrh-decreasing support constraint, and that ifs runtime increases gradually as the average length of the se- quences (and the discovered frequeni patterns) increases.

1 Introduction  Data mining research during the last years has led to the development of a variety of algorithms for finding frequent sequential patterns in very large sequential databases [7,9, 51. These patterns can be used to find sequential association rules or extract prevalent patterns that exist in the sequences, and have been effectively used in many different domains and applications.

The key feature in most of these algorithms is that they control the inherently exponential complexity of the prob- lem by finding only the patterns that occur in a sufficiently large fraction of the sequences, called the support. A limita-  'This work WBS supported by NSF CCR-99725 19. EIA-9986C42. ACI- 9982274, ACI-0133464 by Army Research Office mntra~t DAIDAAGSS- 98-1-0441, by the DOE ASCI program. and by Army High Performance Computing Research Center contract number DAAH04-95-C-0008.

tion of this paradigm for generating frequent patterns is that it uses a constant support value, irrespective of the length of the discovered patterns. In general, patterns that con- tain only a few items will tend to be interesting if they have a high support, whereas long patterns can still be interest- ing even if their support is relatively small. Unfortunately, if constant-support-based frequent pattern discovery algo- rithms are used to find some of the longer but infrequent patterns, they will end up generating an exponentially large number of short patterns. Ideally, we desire to have an al- gorithm that finds all the frequent patterns whose support decreases as a function of their length. Developing such an algorithm is particularly challenging because the downward closure property of the constant support constraint cannot be used to prune short infrequent patterns.

Recently [6], we introduced the problem of finding fre- quent itemsets whose support satisfies a non-increasing function of their length. An itemset is frequent only if its support is greater than or equal to the minimum support value determined by the length of the itemset. We found a property that an itemset must have in order to support longer itemsets given a length-decreasing support constraint. This property, that we call the smallesi valid extension or SVE for short, enabled us to prune many short itemsets that are irrelevant to finding longer itemsets. We developed an al- gorithm called LPMiner that efficiently finds frequent item- sets given a length-decreasing support constraint by pruning large portion of search space.

In this paper, we extend the problem of finding pat- terns that satisfy a length-decreasing support constraint to the much more challenging problem of finding sequen- tial patterns. We developed an algorithm called SLPMiner that finds all frequent sequential patterns that satisfy a length-decreasing support constraint. SLPMiner follows the database-projection-based approach for frequent pattern generation, that was shown to lead to efficient algorithms, and serves as a platform to evaluate our new three pNn- ing methods based on the SVE property. These pruning methods exploit different aspects of the sequential pattern discovery process and prune either entire sequences, items  0-7695-1754-4/02 $17.00 0 2002 IEEE 418  mailto:cs.umn.edu   within certain sequences, or entire projected databases. Our experimental evaluation shows that SLPMiner achieves up to two orders of magnitude of speedup by effectively ex- ploiting the SVE property, and that its runtime increases gradually as the average length of the sequences (and the discovered patterns) increases.

The rest of this paper is organized as follows. Section 2 provides some background information. Section 3 describes the basic pattern discovery algorithm of SLPMiner and how the length-decreasing support constraint can he exploited to prune the search space of frequent patterns. The experimen- tal results of our algorithm are shown in Section 4, followed by a conclusion in Section 5.

2 Background 2.1 Sequence Model  and Notation The basic sequence model that we will use was introduced by Srikant et al [71 and is defined as follows. Let I = {it, iz, . . . ,in} be the set of all items. An itemset is a sub- set of items. A sequence s = ( t l ,  t z ,  . . . , tl) is an ordered list of itemsets, where tj  5 I for 1 5 j < 1. A sequential database D is a set of sequences. The length of a sequence s is defined to he the number of items in s and denoted as (SI .  Similarly, given an itemset t ,  let It1 denote the number of items in t .  Given a sequential database D, JDI denotes the number of sequences in D. This model can describe a wide range of real data. For example, at a retail shop, cus- tomer transactions can he modeled by this sequence model such that an itemset represents a set of goods (or items) pur- chased by a customer at a visit and a sequence represents an ordered purchased itemsets history of a customer.

Sequence s = ( t l ,  t z , .  . . ,ti) is called a sub-sequence of sequence s' = ( t i ,  t i , .  . . , t a )  (I 5 rn) if there exist 1 integers i t ,  iz, . . . il such that 1 5 il < iz < . . . < ir < m and tj 2 tii ( j  = 1,2, . . . , I ) .  I f s  is a sub-sequence of s', then we write s 2 s' and say sequence s' suppons s. The support of a sequence s in a sequential database D, denoted as U D ( S ) ,  is defined to be lD81/lDl, where D. = { s i [ .  g s, A si E D}. From the definition, it always holds that 0 < Q ( S )  5 1. We use the term sequentialpattern to refer to a sequence when we want to emphasize that the sequence is supported by many sequences in a sequential database.

We assume that we can give a lexicographic ordering on the items in I. Although an itemset is just a set of items without the notion of ordering, it is essential to he able to de- fine an ordering among the items for our algorithm. When we represent the items in an itemset, we order the items ac- cording to the lexicographic ordering and put those ordered items within matched parentheses (). When we represent the items in a sequence, we represent each itemset in this way and mange these itemsets according to the ordering in the sequence within matched angled parentheses ().

2.2 Sequential Pattern Mining with Constant  The problem of finding frequent sequential patterns given a constant minimum support constraint [71 is formally de- fined as follows:  Delkitinn 1 (Sequential Pattern Mining with Constant Support) Given a sequential database D and a minimum support a (0 < a 5 l), find all sequences each of which is supported by at least raID11 sequences in D.

Efficient algorithms for finding frequent itemsets or se- quences [Z, 8, 1, 4, 3, 5, 101 in very large itemset or se- quence databases have been one of the key success stories of data mining research. The key feature in these algorithms is that they control the inherently exponential complexity of the problem by using the downward closure property [7].

This property states that in order for a pattern of length 1 to be frequent, all of its suh-sequences must be frequent as well. As a result, once we find that a sequence of length I is infrequent, we know that any longer sequences that in- clude this particular sequence cannot be frequent, and thus eliminate such sequences from further consideration.

23 Finding Patterns with Length-Decreasing  Recently, we introduced the idea of length-decreasing sup- port constraint I61 that helps us to find long itemsets with low support as well as short itemsets with high support.

A length-decreasing support constraint is given as a func- tion of the itemset length f ( 1 )  such that f ( 1 , )  > f(1a) for any la, l a  satisfying 1. < l b .  The idea of introducing this kind of support constraint is that by using a support func- tion that decreases as the length of the itemset increases, we may be able to find long itemsets that may he of in- terest without generating an exponentially large number of shorter itemsets. We can naturally extend this idea to the sequence model by using the length of the sequence instead of the length of the itemset. Figure 1 shows a typical length- decreasing support constraint. In this example, the support constraint decreases linearly to the minimum value and then stays the same for sequential patterns of longer length. For- mally, the problem of finding this type of patterns is stated as follows:  Definition 2 (Sequential Pattern Mining with Length- Decreasing Support) Given a sequential database D and a length-decreasing support constraint f (1 ) .  where f ( 1 )  is a non-increasingfunction defrned over all the positive in- tegers and always 0 < f ( 1 )  < 1, jind all the sequential patterns each s ofwhich satisjies UD(S)  > f(ls1).

Finding the complete set of frequent sequential patterns that satisfy a length-decreasing support constraint is par- ticularly challenging since we cannot rely solely on the  S u p p o r t  Support     o.owI ........ ........ 1 I IO  Length Of sulUenCc  Figure 1. An example of typical length-decreasing support constraint  downward closure property of the constant support pat- tern mining. Notice that, under a length-decreasing sup- port constraint, a sequence can be frequent even if its sub- sequences are infrequent since the minimum support value decreases as the length of a sequence increases. We must use rninl?l f ( 1 )  as the minimum support value lo apply the downward closure property. which will result in finding an exponentially large number of uninteresting infrequent short patterns.

- Length-deereasing suppnronsminl f ( 1 )  "l lh I I . , , , , ,  8':SVEofa . .  . .

Lengthof sequence  Figure 2. Smallest valid extension (SVE)  A key property regarding sequences whose support de- creases as a function of their length is the following. Given a sequential database D and a particular sequence s E D , if the sequence s is currently infrequent ( U D ( S )  < f(Is1)).

then ~ - ' ( u D ( s ) )  = min({llf(l) 5 U D ( S ) } )  is the mini- mum length that a sequence s' such that s' 3 s must have before it can potentially become frequent. Figure 2 illus- trates this relation graphically. The length of s' is nothing more than the point at which a line parallel to the x-axis at y = U D ( S )  intersects the support curve; here, we essen- tially assume that the best case in which s' exists and it is supported by the same set of sequences as its suh-sequence s. This property is called the smallest valid extension prop- erty or SVE property for short and was initially introduced for the problem of finding itemsets that satisfy a length- decreasing support constraint [61.

3 SLPMiner Algorithm  We developed an algorithm called SLPMiner that finds all the frequent sequential patterns that satisfy a given length- decreasing support constraint. SLPMiner serves as a plat- form to develop and evaluate pruning methods for reduc- ing the complexity of finding this type of patterns. Our de- sign goals for SLPMiner was to make it generic enough so  that any conclusions drawn from our experiments can carry through other database-projection-based sequential pattern mining algorithms [3,5].

This section consists of two main parts. First, we will explain how SLPMiner finds frequent sequential patterns.

Second, we will explain how SLPMiner prunes unnecessary data by using three different pruning methods that exploit the SVE property.

3.1 Sequential Database-Projection-based Algo- rithm  SLPMiner finds frequent sequential patterns using the database-projection-based approach. First, we describe the general idea of the the database-projection-based approach and then discuss about details specific to SLPMiner. The description of the database-projection-based approach is based on [3].

SLPMiner grows sequential patterns by adding an item at a time. It uses a prefix tree that determines which items are to he added to grow each pattern. Each node in the tree represents a frequent sequential pattern with one item added to the end of the sequential pattern that its parent node rep- resents. As a result, if a node represents a sequential pattern p, its parent node represents the length-( Ipl - 1) prefix of p.

For example, if a node represents a pattern ((l), (2,3)), its parentnode represents ((l), (2)).

SLPMiner starts from the root node that represents the null sequence to find all the frequent items in the input database and expands the root node into the child nodes cor- responding to the frequent items. Then it recursively moves to each child node and expands i t  into child nodes that rep- resent frequent sequential patterns.

SLPMiner grows each pattern in two different ways, namely, itemset extension and sequence extension. Item- set extension grows a pattern by adding an item to the last itemset of the pattern, where the added item must be larger than any item in the last itemset of the original pat- tern. For example, ((l), (2)) is extended to ((1); (2,3)) by itemset extension, but cannot be extended to ((l), ( 2 , l ) )  or ((I), (2,2)). Sequence extension grows a pattern by adding an item as a new itemset next to the last itemset of the pat- tern. For example, ((l), (2)) is extended to ((l), (2), (2)) by sequence extension.

Figure 3 shows a sequential database D and its prefix tree that contains all the frequent sequential patterns given minimum support 0.5. Since D contains a total of four se- quences, a pattern is frequent if and only if at least two sequences in D support the pattern. The root of the tree represents the null sequence. At each node of the tree in the figure, its pattern and its supporting sequences in D are depicted together with symbol SE or E on each edge rep- resenting itemset extension or sequence extension respec- tively.

Figure 3. The prefix tree of a sequential database  At each node we need to know the support of each pos- sible extension to see whether it is frequent or not. In prin- ciple, we can count the number of supporting sequences at each node by scanning the input sequential database D.

However, if only a small number of sequences in D support the pattern, scanning the whole database costs too much for a pattern. We can avoid this overhead by scanning a database called pmjected database. which is generally much smaller than the original sequential database D .  The projected database of a sequential pattern p has only those sequences in D that support p .  For example, at the node ((2,3)) in Figure 3, its projected database needs to con- tain only sl, s2,s4 since s3 does not support this pattern.

Furthermore, we can eliminate preceding items in each se- quence that will never be used to extend the current pattern.

For example, at the node ((2)) in Figure 3, we can store sequence sl' = ((2,3)) instead of sl itself in its projected database. Overall, database projection reduces the amount of sequences that need to be processed at each node and enhances efficient pattern discovery.

There are various database-projection-based algorithms for both finding frequent itemsets and finding frequent se- quential patterns [ I ,  3, 4, 51. SLPMiner builds the tree in depth first order and generates a projected database at ev- ery node explicitly to maximize opportunities for applying the various pruning methods. As a result, its overall ap- proach is similar to that used by Prefixspan [51. However, the main difference between them is that SLPMiner gener- ates several projected databases at a time before exploring those generated child nodes, whereas Prefixspan generates and explores one projected database at a time.

3.2 Performance Optimizations  Expanding each node of the tree, SLPMiner performs the following two steps. First, it calculates the support of each item that can be used for itemset extension and each item that can be used for sequence extension by scanning the projected database D' once. Second, SLPMiner projects D' into a projected database for each frequent extension found in the previous step.

Since we want SLPMiner to be able to run against large input sequential databases, the access to the input database  and all projected databases is disk-based. To facilitate this, SLPMiner uses two kinds of buffers: a read-buffer and a write-buffer. The read-buffer is used to load a projected database from disk. If the size of a projected database does not fit in the read-buffer, SLPMiner reads part of the database from disk several times. The write-buffer is used to temporally store several projected databases that are gen- erated at a node by scanning the current projected database once using the read-buffer. There are two conflicting re- quirements concerning how many projected databases we should generate at a time. In order to reduce the number of database scans, we want to generate as many projected databases as possible in one scan. On the other hand, if we keep small buffers for many projected databases simultane- ously within the write-buffer, it will reduce the size of the buffer assigned to each projected database, leading to ex- pensive frequent YO between the write-buffer and disk. In order to balance these two conflicting requirements, SLP- Miner calculates the size of each projected database when calculating the support of every item in the current pro- jected database before it actually generates new projected databases. Then SLPMiner generates projected databases as many as they fit in the write-buffer by one database scan, writes those projected databases on the write-buffer to the disk, and traverses only those generated child nodes in depth first order. This method also facilitates storing each pro- jected database in a chunk rather than fragmented small pieces, which improves and stabilizes disk U0 efficiency dramatically.

Even though the disk U0 of SLPMiner is quite efficient, it is  still a bottle-neck of the total performance. In order to reduce the size of projected database, SLPMiner prunes all items from a projected database if the support is less than rniol2l f ( l )  since such items will never contribute to any frequent sequential patterns.

3.3 Pruning Methods  Given a length-decreasing support constraint, SLPMiner follows the sequential database-projection-based approach explained so far using rnin,>l f ( 1 )  as the constant minimum support constraint. Then SLPMiner outputs sequential pat- terns if their support satisfies the given length-decreasing support constraint. But this algorithm itself does not reduce the number of discovered patterns and will be very ineffi- cient as our experimental results will show. In this subsec- tion, we introduce three pruning methods that exploit the length-decreasing support constraint using the SVE prop- erty.

3.3.1 Sequence Pruning, SP The first pruning method is used to eliminate certain se- quences from the projected databases. Recall from Section 3 that SLPMiner generates a projected database at every     node. Let us assume that we have a projected database D' at a node N that represents a sequential pattern p .  Each se- quence in D' hasp as its prefix. lf p is infrequent, we know from the SVE property that in order for this pattern to grow to something indeed frequent, it must have a length of at least f - ' ( o ~ ( p ) ) .  Now consider a sequence s that is in the projected database at node N, i.e., s E D'. The largest se- quential pattern that s can support is of length /SI + Ipl. Now if [ S I  + Ipl 5 f - ' ( o D ( p ) ) .  then s is too short to support any frequent patterns that have p as prefix. Consequently, s does not need to be considered any further and can he pruned.

We will refer to this pruning method as the sequence prun- ing method or SP for short, which is formally defined as follows:  Definition 3 (Sequence Pruning) Given a length- decreasing support constraint f ( 1 )  and a projected database D' at a node representing a sequential pat- tern p ,  a sequence Y E D' can be pruned fmm D' if f ( ls l  + lPl) > o D ( P ) .

SLPMiner checks if a sequence can he pruned before inserting it onto the write-buffer. We evaluated the com- plexity of this method in comparison with the complexity of inserting a sequence to a projected database. There are three parameters required to prune a sequence: IsI, Ipl, and U D ( ~ ) .  As the length of each sequence is pan of sequence data structure in SLPMiner, it takes a constant time to cal- culate Is1 and [ P I .  As for U D @ ) .  we know this value when we generated the projected database for the pattern p .  Eval- uating function f takes a constant time because SLPMiner has a lookup table that contains all possible ( 1 ,  f ( l ) )  pairs.

Thus, the complexity of this method is just a constant time per inserting a sequence.

33.2 Item Pruning, IP The second pruning method eliminates some items of each sequence in projected databases. Let us assume that we have a projected database D' at a node N that represents sequential pattern p and consider an item i in a sequence s E D'. From the SVE property we know that the item i will contribute to a valid frequent sequential pattern only if  where u p  (i) is the support of item i in D'. This is he- cause of the following. The longest sequential pattern that s can participate in is Is1 + IpI. and we know that, in the suhtree rooted at N .  sequential patterns that extend p with item i have support at most U D . ( ~ ) .  Now, from the SVE property, such sequential patterns must have length at least f - '(UD, (i)) in order to he frequent. As a result, if equation ( I )  does not hold, item i can he pruned from the sequence s. Once item i is pruned, then op(i) and 1.9 decrease, possibly allowing further pruning. Essentially. this pruning  method eliminates some of the infrequent items from the short sequences. We will refer to this method as the item pruning method, or I f  for short, which is formally defined as follows:  Definition 4 (Item Pruning) Given a length-decreasing support constraint f ( 1 )  and a projected database D' at a node representing a sequential pattern p ,  an item i in a sequence s E D' can be pruned from s if Is1 + (pl < f - ' ( U D ' ( i ) ) .

We can implement this pruning method simply as fol- lows: for each projected database D', repeat scanning D' to collect support values of items and scanning D' again to prune items from each sequence until no more items can he pruned. Then, we can project the database into a pro- jected database for each frequent item in the pruned pro- jected database. This algorithm, however, requires multiple scans of the projected database and hence will be too costly.

Instead, we can scan a projected database once to col- lect support values and use those support values for prun- ing items as well as for projecting each sequence. Notice that we are using approximate support values that might he higher than the real values since the support values of some items might decrease during the pruning process. SLP- Miner applies IP before generating a projected sequence s' of s as well as after generating s' just before inserting s' onto the write-buffer. By applying IP before projecting se- quences, we can reduce the computation of projecting se- quences. By applying IF' once again for projected sequence s', we can exploit the reduction of length Is1 - ls'l to prune items in SI furthermore. Pruning items from each sequence is repeated until no more item can be pruned or the sequence becomes short enough to be pruned by SP.

IP can potentially prune larger portion of projected database than SP since it always holds that O D ( P )  2 U D ,  (i) and hence f - ' ( a ~ ( p ) )  5 f - ' ( u p ( i ) ) .  However, the pruning overhead of IP is much larger than that of SP. Let us consider the complexity of pruning items from a sequence s. The worst case is  that only one item is pruned in ev- ery iteration over the items in s. Since this can he repeated as many as the number of items in the sequence, the worst case complexity for one sequence is O(n') where n is the number of items in the sequence.

3.3.3 Structure-based Pruning Given two sequences sl, s2 of the same length k, these two sequences are treated equally under SP and IP. In fact, the two sequences can be quite different from each other. For  same I-sequence ((l)), ((2)). ((3)), and ((4)) hut never support the same k-sequences for k 2 2. From this ohser- vation, we considered ways to split aprojecteddatabase into smaller equivalent classes. By having smaller databases in-  example, ((1,2,3,4)) and ((11, (Z), (31, (4)) support the     stead of one large database, we may be able to reduce the depth of a certain path from the mot to a leaf node of the tree.

As a structure-based pruning, we developed the min-max pruning method. The basic idea of the min-max pruning is to split a projected database D? into two 0: , 0: such that D: and D; contribute to two disjoint sets of frequent sequential patterns. In order to separate D? into such D; and D:, we consider the following two values for each sequence s E D?:  1. a ( s )  = the minimum number of itemsets in frequent sequential patterns that s supports  2. b(s) = the maximum number of itemsets in frequent  These two values define an interval [a(.), b (s ) ] ,  that we call the min-max interval of sequence s. If two sequences s, s? E D? satisfy [a(.), b (s ) ]  n [a(.?), b(s?)]  = 0,  then Y and s? cannot support any common sequential pattern since their min-max intervals are disjoint.

If we have D: and D; satisfy U 8 e ~ ; [ a ( s ) , b ( s ) ]  n U.,,;[a(s), b(s)]  = 0 ,  then D; and D: support distinct sets of frequent sequential patterns. However, this is not possible in general. Instead, D? will be split into three sets A, B ,  C of sequences as shown in Figure 4. More precisely, these three sets are defined for some positive integer k as follows.

A(k) = [sls E D? A b(s) < k} B ( k )  = {sls E D? A a ( s )  2 k}  sequential patterns that s supports  ~ ( k )  = D ? - ( A U B )  A ( k )  and B(k)  support distinct sets of frequent sequen- tial patterns, whereas A ( k )  and C(k)  as well as B(k)  and C(k)  support overlapping sets of frequent sequential pat- terns. From these three sets, we form D; = A(k) U C(k) and D; = B(k)UC(k).  If we mine frequent sequential pat- terns of length up to k - 1 from D; and patterns of length no less than k from 04, we can gain the same patterns as we would from original D?.

1 .  - .  . ~. ~, .-  1 k Min-max interval Figure 4. Min-max intervals of a set of sequences  Through our experiments, we observed IC1 is so close to ID?( that mining 0; and D; defined above will cost  more than mining the original database D?. However, we can prune entire D? if both ID;I and ID;/ are smaller than the minp l  f ( 1 ) .  Furthermore. we can increase this minimum support by the fact that any sequential patterns that the current pattern p can extend to is of length at most mU.rD!(ISI) + Ip/. Now, from the SVE prop- erty, we know that if both ID:I and ID;] are smaller than /-l(maX.EDt((sI) + /PI). then we can eliminate entire D?. Essentially, this means that if we can split a projected database into two subsets each of which is too small to be able to support any frequent sequential pattern, then we can eliminate the entire original projected database. We call this pruning method the min-mar pruning or MP for short, which is formally defined as follows:  Definition 5 (Min-Max Pruning) Given a length- decreasing support constraint f ( l )  and a projected database D? at a node representing a sequential pattern p , entire D? can be pruned there exists a positive integer k such that  We apply MP just after a new projected database D? is generated if the entire sequences in D? is still kept on the write-buffer and if ID?/ 5 1.2f(max.ro,(lSl))lDl. The first condition is necessary to avoid costly disk U 0  and the second condition is necessary to increase the probability of successfully eliminating the projected database. The algo- rithm of MP consists of two parts: the first part to calculate the distribution of the number of sequences over possible min-max intervals and the second part to find k that satis- fies condition (2). The first part requires scanning D? on memory once and finding the min-max interval for each se- quence. For each sequence s, SLPMiner generates a his- togram of itemset size and calculates a ( s )  by summing up itemset sizes from the largest one. The other value b(s) is simply the numberof itemsets ins .  This part requires O(m) where m is the total number of itemsets in D?. The second part uses an n x n upper triangular matrix Q = ( q i j )  where q i j  = I { s ~ Q ( s )  = i A b ( s )  = j A s  E D?}Iandnisthemax- imum number of itemsets in a sequence in D?. Matrix Q is generated during the database scan of the first part. Given matrix Q, we have  k - I  n  lA(k)l +IC(k)l = C C q i j k l  j = i     constraint that decreases linearly with the length of the fre- quent sequential pattern. In particular, the initial value of support was set to 0.001 and it was decreased linearly down to 0.0001 for sequences of up to length [lCllT1/2J.

We ran SPADE 191 to compare runtime values with SLP- Table 1. Parameters for datasets used in our tests Miner. When running SPAbE. we used the depth first  search option, which leads to better performance than the breadth first search option on our datasets. We set the mini- mum support value to he minl>l f(l). Using relations  ? 4.1 Results  (IAQ + 1)1+ IC(k + 1)l) - (IA(k)l+ IC(k)l) = x q k j j=* Tables 2 and 3 show the experimental results that we ob-  L. tained for the DSI and DS2 datasets respectively. Each - C % k r o w  of the tables shows the results obtained for a differ- (IB(k + 1)1 + IC(k + 1)1) - (1B(k)1 + Ic(k)l) =  ,=I  we can calculate IA(k)l+ IC(k)l and IB(k)l+ IC(k)l incre- mentally for all k in O(nz).  So the total complexity of the min-max pruning for one projected database is O(m + n?).

This complexity may be much larger than the runtime re- duction by eliminating projected databases. However, our experimental results show that the min-max pruningmethod generally reduces the total runtime substantially when other pruning methods are not used together.

4 Experimental Results We experimentally evaluated the performance of SLPMiner using a variety of datasets generated by the synthetic se- quence generator that is provided by the IBM Quest group and was used in evaluating the AprioriAll algorithm 171. All of our experiments were performed on Linux workstations with AMD Athlon at 1 SGHz and 2GB of main memory.

All the reported runtime values are in seconds.

We used two classes of datasets DS1 and DS2, each of which contained 25K sequences. For each class we gener- ated different problem instances as follows. For DSI, we varied the average number of itemsets in a sequence from IO to 30 by interval 2, obtaining a total of 1 I datasets, DSI- IO, DS1-12, ??, DS1-30. For DS2, we varied the average number of items in an itemset from 2.5 to 7.0 by interval 0.5, obtaining a total of 10 datasets, DS2-2.5, DS2-3.0, . . ..

DS2-7.0. For DS1-z, we set the average size of maximal potentially frequent sequences to be 212. For DS2-x, we set the average size of maximal potentially frequent itemsets to he x / 2 .  Thus, the dataset contains longer frequent patterns as z increases. The characteristics of these datasets are sum- mariwd in Table 1, where ID1 is the number of sequences, IC1 is the average number of itemsets per sequence, IT1 is the average number of items per itemset, N is the number of items, IS( is the average size of maximal potentially fre- quent sequences, and IT( is  the average size of maximal potentially frequent itemsets.

In all of our experiments, we used a minimum support  ent DS1-x or DS2-2 dataset, specified on the first column.

The column labeled ?SPADE? shows the amount of time taken by SPADE, which includes the runtime of prepro- cessing to transform the input sequential database into the vertical format 191. The column labeled ?None? shows the amount of time taken by SLPMiner using a constant sup- port constraint that corresponds to the smallest support of the support curve, that is 0.0001 for all datasets. The other columns show the amount of time required by SLPMiner that uses the length-decreasing support constraint and a to- tal of five different varieties of pruning methods and their combinations. For example, the column label ? S P  corre- sponds to the pruning scheme that uses only sequence prun- ing, whereas the column labeled ?SP+IP+MP? corresponds to the scheme that uses all the three pruning methods. Note that values with a ?-? correspond to experiments that were aborted because they were taking too long time.

A number of interesting observations can be made from the results in these tables. First, even though SLPMiner without any pruning method is slower than SPADE, the ra- tio of runtime values is stable ranging from 1.9 to 2.7 with average 2.3. This shows that the performance of SLPMiner is comparable to SPADE and good enough as a platform to evaluate our pruning methods.

Second, either one of pruning methods performs better than SLPMiner without any pruning method. In particu- lar, SP, IP, SP+lP, and SP+IP+MP have almost the same speedup. For DSI, the speedup by SP is about 1.76 times faster for DS 1-10.7.6 1 times faster for DS1-16, and 14 1.16 times faster for DSI-22. Similar trends can be observed for DS2, in which the performance of SLPMiner with SP is 1.76 times faster for DS2-2.5,8.78 times faster for DS2-3.5, and 296.59 times faster for DS2-5.0.

Third, SP pruning alone can achieve almost the best per- formance among all the other tested combinations. This was counter-intuitive for us since we expected SP+IP would be much better than SP or IP alone. On the other hand, this result shows that many other sequential pattern mining al- gorithms can exploit the SVE property by using SP since it     Table 2. Comparison of pruning methods using DSI

I. ".,i? -  Table 3. Comparison of pruning methods using DS2  is easy to implement. For example, it is straight-forward to implement SP in Prefixspan [5] ,  for both its disk-based pro- jection and pseudo-projection. Even SPADE [91, which has no explicit sequence representation during pattern mining, can use SP by adding the length of sequence to each record in the vertical database representation.

Fourth, among the three pruning methods, SP leads to the largest runtime reduction, IP leads to the second largest runtime reduction, and MP achieves the smallest reduction.

The problem with MP is the overhead of splitting a database into two subsets. Even so, it seems surprising lo gain such a great speedup by MP alone. This shows a large part of the runtime of SLPMiner with no pruning method is accounted for by many small projected databases that never contribute to any frequent patterns. As for SP and IP, SP is slightly better than IP because IP and SP prune almost the same amount of projected databases for those datasets hut IP has much larger overhead than SP.

Fifth, the runtime with three pruning methods increases gradually as the average length of the sequences (and the discovered patterns) increases, whereas the runtime of SLP- Miner without any pruning increases exponentially.

