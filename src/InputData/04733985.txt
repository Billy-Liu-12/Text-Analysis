Bounding and Estimating Association Rule Support from Clusters on Binary Data

Abstract  The theoretical relationship between association rules and machine learning techniques needs to be studied in more depth. This article studies the use of clustering as a model for association rule mining. The clustering model is exploited to bound and estimate association rule support and confidence. We first study the efficient computation of the clustering model with K-means; we show the sufficient statistics for clustering on binary data sets is the linear sum of points. We then prove itemset support can be bounded and estimated from the model. Finally, we show support bounds fulfill the set downward closure property. Exper- iments study model accuracy and algorithm speed, paying particular attention to error behavior in support estimation.

Given a sufficiently large number of clusters, the model be- comes fairly accurate to approximate support. However, as the minimum support threshold decreases accuracy also decreases. The model is fairly accurate to discover a large fraction of frequent itemsets at different support levels. The model is compared against a traditional association rule al- gorithm to mine frequent itemsets, exhibiting better perfor- mance at low support levels. Time complexity to compute the binary cluster model is linear on data set size, whereas the dimensionality of transaction data sets has marginal im- pact on time.

1 Introduction  Association rules [2] are a very popular data mining technique [15, 17], which have produced a significant body of research. There is research on efficient algorithms to discover association rules [3, 32, 16, 38], using item tax- onomies [35], creating itemset summaries [8], incorporat- ing constraints [40], creating summaries [10, 20] and ap- plying them in different domains [9, 11, 29], among other topics. Unfortunately, association rules lack a basic model  to understand their relationship to properties from the data set. In this work, we defend the idea of using clustering [7, 26, 12] as a model to discover association rules.

Association rules is a well studied topic, but most re- search has proposed efficient algorithms [24, 16, 32] or vari- ants like quantitative [36], constrained [25, 40], taxonomy- based [35], among others. Studying their relationship to classical machine learning techniques has received less at- tention. Using a model to discover them, finding ways to summarize them, proposing metrics to approximate them or explaining their validity using alternative techniques are aspects that have not received enough attention. Our point of view is that clusters represent a more fundamental pat- tern than association rules, from which we can generate them. The mathematical relationship between clustering and association rules is important and interesting because clustering, on one hand, looks for global patterns produc- ing disjoint subsets of points, whereas association rules un- cover many local patterns referring to overlapping subsets of points. That is, both techniques have different goals.

This is a summary of our contributions. We first in- troduce a one-pass K-means clustering algorithm based on simplified sufficient statistics for binary data to build a model that summarizes a set of transactions as clusters. We prove clusters can be used to bound and approximate asso- ciation rule metrics. Based on the clustering model three association metrics are introduced: lower, upper and av- erage support. These new metrics provide bounds and an estimation for association support. Also, our metrics ex- hibit the subset downward closure property, which allows efficient bottom-up search algorithm that can work in main memory. We derive three metrics for association rule con- fidence reusing support bounds: lower, upper and average confidence. We perform an extensive experimental with real and synthetic data sets. We study error behavior varying the number of clusters and accuracy to discover a large set of frequent itemsets at different support thresholds.

This is an outline of the article. Basic definitions on clus- tering and association rules are presented in Section 2. Sec-   DOI 10.1109/ICDM.Workshops.2008.49    DOI 10.1109/ICDMW.2008.47     tion 3 explains how to exploit a clustering model to bound an estimate itemset support. Section 4 discusses experi- ments studying error on support estimation, speed and scal- ability. Closely related work is presented in Section 5. Sec- tion 6 concludes the article.

2 Definitions  We give a unified set of definitions for association rules [2, 3, 32] and clustering [7, 28]. Items act as dimension subscripts for matrix manipulation.

2.1 Association Rules  Let D = {T1, . . . , Tn} be a set of n transactions on d items, where |Ti| << d (crucial to develop an efficient dis- tance computation [26]) and I = {1, 2, . . . , d} (each inte- ger identifies one item). Each subset X of items s.t. X ? I is called an itemset. An itemset X containing p items is called a p-itemset: X = {i1, i2, . . . , ip}, where il ? I. The probability of appearance for one itemset X ? I, is called support and it is defined as s(X) = |{Ti|X ? Ti}|/n. That is, the fraction of transactions in D that contain X . In an association rule X ? Y , X,Y ? I, and X ? Y = ?. (X is called the antecedent and Y is called the consequent).

X ? Y has a measure of reliability called confidence, given by c(X ? Y ) = s(X ? Y )/s(X). The standard association problem is to find all itemsets that have support above a minimum support threshold ? . The standard prob- lem of mining association rules is to find all rules X ? Y s.t. s(X ? Y ) ? ? and c(X ? Y ) ? ?.

2.2 Clustering  In the context of clusteringD is analyzed as a binary data set with d dimensions. K-means partitions the data set into k disjoint groups based on a Euclidean distance similarity metric. The clustering model consists of matrices W,C,R, which contain the weights, the centroids (means) and the radiuses (variances), respectively for each cluster and a par- tition of D into k subsets, where k is user-specified. Matri- ces C and R are d ? k and W is a k ? 1 matrix. To refer to to the jth cluster we use the j subscript (i.e. Cj , Rj).

Therefore, Cj is the jth cluster centroid, Rj is the jth (di- agonal) variance matrix and Wj is the weight of cluster j.

Subscripts are used as follows. Subscript i is used to scan transactions: i ? {1, 2, . . . , n}. Notice i alone is a sub- script to index transactions, but ij is item j. Subscript j is used to refer to one cluster: j ? {1, 2, . . . , k}. Finally, subscript h is used for dimension: h ? {1, 2, . . . , d}. Let D1, D2, . . . , Dk be the k disjoint subsets from D induced by clusters s.t. Dj ? Dg = ? for j = g. The diag[] is a  Ti Items 1 4 5 2 3 4 3 1 2 4 2 5 1 2 6 3 4 5 7 1 8 3 4  Ti 1 2 3 4 5 1 0 0 0 1 1 2 0 0 1 1 0 3 1 1 0 0 0 4 0 1 0 0 0 5 1 1 0 0 0 6 0 0 1 1 1 7 1 0 0 0 0 8 0 0 1 1 0  Figure 1. Input transactions d = 5, n = 8.

W = [  0.5 0.5  ] C =  ? ?????  0.00 0.75 0.00 0.75 0.75 0.00 1.00 0.00 0.50 0.00  ? ?????  Figure 2. Clustering model; d = 5, k = 2.

transformation operator to transform a vector into a diago- nal matrix or vice versa. The T superscript indicates vector and matrix transposition.

Example 2.1 In Figure 1 we present a small input database D, where d = 5, n = 8. Figure 2 presents a clustering model where k = 2. Finally, Figure 3 presents an intuitive model summary. Items are grouped according to their clus- ter centroid value. The last column shows a basic itemset represented by each cluster, which can be used to derive itemset subsets. Thus the clustering summary can derive all frequent itemsets that can be obtained fromD. Parameter ? is used to filter out long itemsets obtained from each cluster; it will be explained in more detail in Section 3.

3 Bounds on Support Based on Clusters  Clustering is used as a statistical model for association rules. In this section we introduce an efficient algorithm to cluster binary data sets and a family of association rules metrics based on a clustering model.

Cj range for items Itemsets j Wj 0.75-1 0.5-0.75 0.25-0.5 at ? = 0.5 1 0.5 3 4 5 3 4 5 2 0.5 1 2 1 2  Figure 3. Clusters summary and itemsets.

3.1 Clustering Binary Data Sets  Since we are trying to use clusters as a model to dis- cover associations the first task is clustering transactions.

K-means [7, 31, 23, 17] is used to cluster transactions. From a scalability point of view, It is feasible to derive incremen- tal K-means clustering versions [26] that can get a good so- lution in one pass or a few passes over a large data set.

The K-means algorithm used in this work is an im- provement of the incremental K-means algorithm for binary streams [26]. In this section we present the main features that make it suitable to cluster binary data sets and intro- duce useful theoretical results. K-means is significantly ac- celerated with sparse distance computation, sparse matrix addition and simplified sufficient statistics.

The K-means algorithm [26], like other scalable cluster- ing algorithms [7, 44], exploits sufficient statistics, which are summaries of D1, D2, . . . , Dk represented by matrices L,Q,N that contain k sums of points, k sums of squared points and number of points per cluster, respectively. L is a d ? k matrix, such that each column Lj is computed as Lj =  ? xi?Dj xi. Each column of Q is a diagonal matrix  Qj: Qj = ?  xi?Dj xix T i . Finally, matrix N is k ? 1 which  counts the number of points per cluster (i.e. Nj = |Dj |).

The update formulas for W,C,R based on sufficient statis- tics are:  Wj = Nj n  (1)  Cj = Nj  Lj (2)  Rj = Nj  Qj ? 1 N2j  LjL T j (3)  The sufficient statistics can be simplified for clustering binary data. In fact, they are simpler than the sufficient statistics required for clustering numeric data.

Lemma 3.1 Let D1, D2, . . . , Dk be k subsets representing a partition of D. Then the sufficient statistics required for computing C,R,W are only N and L because L = Q.

Proof. Based on the fact that xi is a binary vector: ? xi =?  xix T i . Therefore, L = Q. ?  This result states that we only need to update the model based on Equation (1) and Equation (2). That is, matrix Rj can be obtained from Cj :  Rj = diag[Cj ] ? CjCTj (4) More importantly, storage space for clustering results is  reduced to one half. Therefore, matrix R is not needed to estimate bounds for support and confidence. We can now derive a simple criterion to evaluate cluster quality.

Theorem 3.1 Rlj has a minimum if and only if Clj = 0 or Clj = 1. Also,Rlj has a maximum if and only if Clj = 1/2.

Proof. The result follows from Rj = diag[Cj ] ? CjCTj and the first derivative conditions for entry Rlj to have a maximum. ?  Theorem 3.1 states that it is best Cj entries are close to either 0 or 1, with the ideal case when Clj = 0 or Clj = 1.

3.2 One Pass K-means for Binary Data  When D is a sparse matrix and d is high, Euclidean dis- tance is expensive to compute. In a typical binary (transac- tion) data set only a few dimensions have non-zero values.

Therefore, we precompute a distance from every Cj to the null vector 0?. To that purpose, we define the following k- dimensional vector: ?j = ?(0?, Cj). Let ?ij = ?(xi, Cj).

Then  ?ij = ?j + d?  l=1,(xi)l ?=0 ((xi)l ? Clj)2 ? C2lj . (5)  This optimized distance computation decreases time complexity, but it does not affect quality of results. A simi- lar idea is applied to update L,Q matrices in a sparse fash- ion after a transaction cluster membership is determined.

Sparse matrix addition updates only the closest cluster di- mensions where the point dimension value is 1.

Based on the improvements described in the previous section we present a one pass K-means algorithm. This is a high-level description. The input is the set of transactions D = {T1, T2, . . . , Tn}. and k, the desired number of clus- ters as defined in Section 2. The output is ? = {W,C}, describing the model and a partitioning of D into k sub- sets. A constant ? is used to seed C based on d and k. The global statistics ? and ? can be quickly computed from a sample. Standard deviations are computed as ?ll =  ? ?ll.

The cluster membership step is executed for every transac- tion (n times). Since this is the most frequently executed step the optimization for sparse matrix operations is essen- tial to achieve good performance. These sparse operations include sparse distance computation for ? and sparse matrix addition to update L. ?ij is efficiently computed using ?j .

The M step is periodically executed every n/? transactions (? times). W,C are updated using Eq. (1) and Eq. (2).

Dimensions (items) are ranked within each cluster by their value in Cj to make output easier to understand.

3.3 Bounds on Support  The following theoretical results are important to use the model to obtain bounds on association support. The support     Input: D = {T1, T2, . . . , Tn} and k.

Output ? = {W, C} ?? 1/(dk), ? ? log(n) FOR j = 1 TO k DO /* Initialize */  Cj ? ?? ?r diag[?], Wj ? 1/k ?j = ?(0?, Cj) = C  T j Cj  Lj ? Cj , Nj ? 1 ENDFOR  FOR i = 1 TO n DO FOR j = 1 TO k DO /* distances */ ?j ? ?(Ti, Cj) /* sparse operation */  ENDFOR Let J be s.t. ?J ? ?j , j = 1 . . . k LJ ?MJ + xi /* sparse operation */ NJ ? NJ + 1 IF( i mod (n/?) = 0 ) THEN /* update model */  FOR j = 1 TO k DO Cj ? Mj/Nj Wj ? Nj/  ?k J=1  NJ ?j ? CTj Cj  ENDFOR ENDIF  ENDFOR  Figure 4. One pass K-means for binary data.

of an itemset in one cluster (s(X,Dj)) must be less or equal than the minimum support of any item, as shown in [32].

Lemma 3.2 Let Dj be one cluster from D. Let X = {i1, i2, . . . , ip} be some p-itemset. Let Cj be the mean of transactions that belong to cluster j. Then s(X,Dj) ? min((Cj)i1 , (Cj)i2 , . . . , (Cj)ip).

Proof. (sketch) Induction on p. ?  The following theorem links clustering results with asso- ciations. Let the upper bound of s(X) be defined as follows:  u(X) = k?  j=1  Wjmin((Cj)i1 , (Cj)i2 , . . . , (Cj)ip). (6)  Theorem 3.2 Let D be a set of n transactions. Let C,W be the matrices containing the means and weights respec- tively on some clustering of D into k subsets of transac- tions D1, D2, . . . , Dk. Let X be any p-itemset given by X = {i1, i2, . . . , ip}. Then s(X) ? u(X).

Proof. (sketch) By Lemma 3.2 we can sum k inequalities corresponding to each subset Dj , s(X,Dj) ? u(X,Dj) to get s(X) ? u(X) on D. ?  Theorem 3.2 asserts C and W can be used to mine all potential frequent itemsets, thereby eliminating the need to scan D. Nevertheless, the theorem does not guarantee that  only true frequent itemsets are above the minimum support threshold. We may find some false-positive itemsets whose exact support is actually lower than that indicated by the model. The following result, based on Theorem 3.2, is ap- plied to prune the lattice search space if associations are being mined from the clustering model using u(). Now we introduce a lower bound for the support of an association.

l(X) = k?  j=1  Wjmax(0, 1 + p?  l=1  ((Cj)il ? 1)), (7)  where max() is used to get a non-negative lower bound per cluster.

Theorem 3.3 Let X,C,W, {D1, . . . , Dk} be as stated in Theorem 3.2. Let X be a p-itemset X = {i1, i2, . . . , ip}.

Then l(X) ? s(X).

Proof. (sketch) The sum in Equation 7 computes a lower bound on the fraction of transactions that do not contain X for cluster j. ?  Since u() and l() are guaranteed bounds on the exact support of any association their average is a good estimator:  a(X) = l(X) + u(X)  . (8)  3.4 Downward Closure Property  The following result proves our metrics preserve the downward closure property (e.g. a subset of a frequent item- set is frequent).

Theorem 3.4 Let X be a p-itemset and let Y ? X , Y = {h1, . . . , hq}, q < p. Then: (1) l(X) ? l(Y ), (2) u(X) ? u(Y ), (3) a(X) ? a(Y ).

Proof. The proof follows by considering the extra items in X , which may decrease the bounds on Y . ?  Theorem 3.4 guarantees a complete set of associations with respect to any of the metrics using the standard bottom- up search algorithm. This result generalizes bottom-up search algorithms to work with a clustering model instead of the data set.

As explained above, upper support (u()) is safe in the sense that it produces a superset of all frequent itemsets, but it may generate infrequent (false positive) itemsets. In gen- eral, a subset of Cj entries close to zero indicate itemsets that are unlikely to be frequent. To solve such limitation, we propose to use a ? parameter which is used to filter out itemsets unlikely to be above ? (i.e. not frequent): clusters whose upper support for one specific itemset fall below ?     do not contribute to the support estimation used for pruning.

Therefore, we get an adjusted upper support for pruning be- low ? . The spectrum for ? is [0,1]. At one extreme ? = 0 does not eliminate any itemset: all frequent itemsets are dis- covered, but there are additional itemsets whose support is actually below the ? threshold. At the other extreme, ? = 1 allows only dimensions with zero variance in each cluster to participate in itemset generation; in this case all itemsets that are generated are frequent (there are zero false posi- tives), but there may be many itemsets whose support was actually above the ? threshold (there are false negatives).

4 Experimental Evaluation  We first discuss our experimental setup and data sets.

Experiments study two major aspects. First, we study rela- tive error on support estimation and model accuracy to dis- cover frequent itemsets varying the number of clusters and the minimum support threshold. Second, we evaluate speed and scalability.

4.1 Setup  Experiments were performed on a workstation with a 2.4 GHz CPU, 2 GB of memory and 160 GB on disk. The al- gorithms were programmed in the C++ language. The data sets were stored as text files. The clustering model matrices were stored on a binary file after their computation. The model was loaded into main memory before mining fre- quent itemsets. Each experiment was repeated five times and average measurements are reported. In general, elapsed times are reported in seconds.

4.2 Data Sets  Experiments use real and synthetic data sets. The real data sets were obtained from the UCI Machine Learning Repository [4] and are widely used in the research literature on association rule mining. Synthetic (transaction) data sets were created running the well-known IBM transaction data generator [3]. All data sets were transformed and converted to transaction format (itemset representation), as required by our proposal. Table 1 provides a summary of data sets.

We now explain how synthetic data sets were generated with the IBM transaction data generator [3]. This generator has seven parameters (indicating our notation in parenthe- sis): number of transactions D (n), number of items (d), number of patterns P (frequent itemsets), average transac- tion length T, average pattern length I, average correlation corr and average rule confidence conf . Test files are named after the parameters with which they were created. The standard way [3, 15] is to use T, I and D to label files since those are the three most common parameters to change and  Table 1. Data sets.

Data set type d n Chess real 75 3196 Mushroom real 127 8124 Votes real 17 435 T10I4D100k synthetic 100 100k T10I4D1M synthetic 100 1000k  the ones which play a more important role in performance (e.g. T10I4D100k). The transaction files we used had defaults D=1M (n=1000k), d = 100, T=10, I=4, P=100, corr = 0.75 and conf = 0.75.

4.3 Clustering Model Accuracy  Deafult Parameters Setting  Each data set has a different optimal setting for k. There- fore, there is no default value for k. On the other hand, we set ? = 0.2; Recall ? is used to filter out itemsets unlikely to be frequent.

Relative error on support was used to measure accu- racy. For an itemset X it was computed as es(X) = |a(X)? s(X)|/|s(X)|, where average support a(X) is ob- tained from the clustering model and s(X) is the exact sup- port value, as computed from D.

We focus on analyzing error for support estimation. The problem of choosing the best k is called model selection [23, 17]. A low k produces a simple, but probably inaccu- rate model, a high k will produce a more complex, but accu- rate model. We made a few runs to determine a good k for the real data sets, considering that when k is too high some clusters tend to have almost zero points. Therefore, we set k high enough so that relative error reached less than 10% and we did not get zero-weight clusters. In general, there was a slight variation in the cluster model quality from run to run.

We selected the run that produced best clustering results out of five runs in order to estimate support bounds. This is a reasonable approach because in practice a data mining user picks the best model.

Error varying the number of clusters  Figure 5 analyzes error behavior varying k. The left graph shows error behavior for real data sets at ? = 0.2, whereas the right graph analyzes error for the synthetic data set for two ? values. We discuss results for real data sets.

As can be seen error rapidly decreases as k grows. The trend indicates asymptotic behavior. Despite the difference in data set sizes and dimensionality in all cases error goes below 5% when k reaches 80. On the other hand, error is      0.05  0.1  0.15  0.2  0.25  0.3  0 20 40 60 80 100  E rr  o r  k  Real data set  Chess Mushroom  Votes   0.02  0.04  0.06  0.08  0.1  0.12  0.14  0.16  0.18  0 100 200 300 400 500 600 700 800  E rr  o r  k  Synthetic data set  tau=0.10 tau=0.05  Figure 5. Support estimation error increasing k.

high when k is low. We now turn our attention to the syn- thetic data set, where we built models with much higher k.

In this case error shows a slower rate of decrease. The trend also seems asymptotic. We can also see there is a clear gap between ? = 0.05 and ? = 0.10; error roughly grows 100% when ? decreases by 50%. Interestingly enough, results are much better for real data sets than for the synthetic data set.

Error decreasing minimum support  Figure 6 shows error growth as ? decreases. The left graph analyzes error for real data sets. We can see error growth shows different behavior for each data set. Error growth is slow for the Votes data set. Error grows almost linearly for the Chess data set. Error grows fast for the Mushroom data set. The trend indicates we need to build more accurate models with higher k for Mushroom, proba- bly not for Chess and not necessary for the Votes data set.

The right graph analyzes error growth for the synthetic data set. The first model at k = 800 is twice as accurate as k = 400. We can see error grows linearly for high ? values, but it start growing faster at ? = 0.1. The trend indicates the growth is not linear, but it does not seem exponential.

4.4 Comparing Speed and Scalability  We first compare our proposal versus the standard algo- rithm to mine association rules. We then study time com- plexity and scalability.

Comparing clustering and A-priori  Table 2 compares the efficiency of the model with the standard A-priori algorithm. We must stress our proposal does not intend to substitute fast association rule algorithms  Table 2. Comparing model and A-priori.

? from model clustering+model A-priori  0.20 1 1672 24 0.15 1 1672 43 0.10 1 1672 156 0.05 3 1674 645 0.02 11 1683 3347 0.01 36 1708 14806  [32, 16], but we include these comparisons to provide a rel- ative performance benchmark. We used the synthetic data with n = 1M . The clustering model used had the number of clusters set to k = 100. These times include the time to compute exact support on a final pass. The first column varies ? (the minimum support threshold). The second col- umn shows the time to discover frequent itemsets from the model, excluding the time to compute the model. The third column adds the time to compute the clustering model and the time to mine frequent itemsets. Finally, the fourth col- umn shows the time for the traditional algorithm. As can be seen, the clustering model represents an efficient mech- anism to produce all frequent itemsets, assuming the model is already tuned and stored. That is, we assume the model is computed a few times or even once. The second column shows clustering the data set takes most of the time. In this case, the standard algorithm is faster at high support levels, but the model becomes faster at low support levels. Notice the third column represents a pessimistic case in which the model is recomputed every time. Finally, we can see the A-priori algorithm suffers scalability problems due to the exponential growth of patterns. The basic reason the model is faster is because it uncovers long itemsets and it is effi- ciently manipulated in main memory.

0.01  0.02  0.03  0.04  0.05  0.06  0.07  0.08  0.09  00.10.20.30.40.50.6  E rr  o r  tau  Real data set  Chess Mushroom  Votes   0.02  0.04  0.06  0.08  0.1  0.12  0.14  0.16  00.050.10.150.2  E rr  o r  tau  Synthetic data set  k=400 k=800  Figure 6. Support estimation error decreasing ? .

0 200 400 600 800 1000 1200 1400 1600  T im  e i n s  e c o n d s  n x 1000  Data set size  d= 100 d=1000         0 20 40 60 80 100 120 140 160  T im  e i n s  e c o n d s  k  Number of clusters  d= 100 n=100k d=1000 n=100k  Figure 7. Time complexity for clustering large data sets with K-means.

Time Complexity and Speed  We now evaluate scalability and speed with large, high dimensional, data sets to only compute the models, as shown in Figure 7. The plotted times include the time to store models on disk, but exclude the time to mine frequent itemsets. We experimentally prove: (1) Time complexity to compute models is linear on data set size. (2) Sparse vector and matrix computations yield efficient algorithms, whose accuracy was studied before. (3) Dimensionality has min- imal impact on speed, assuming average transaction size T is small. Transactions are clustered with Incremental K- means [26], introduced on Section 3. Large transaction files were created with the IBM synthetic data generator [3] hav- ing defaults n = 1M, T=10, I=4.

Figure 7 shows time complexity to compute the cluster- ing model. The first plot, on the left, shows time growth to build the clustering with a data set with one million records (T10I4D1M). As can be seen times grow linearly as n in- creases, highlighting the algorithms efficiency. On the other hand, notice d has marginal impact on time when it is in- creased 10-fold on both models due to optimized sparse matrix computations. The second plot, on the right, in Fig- ure 7 shows time complexity to compute clustering models increasing k on T10I4D100k. Remember k is the main pa- rameter to control support estimation accuracy. In a similar manner to the previous experiments, times are plotted for two high dimensionalities, d = 100 and d = 1, 000. As can be seen time complexity is linear on k, whereas time is practically independent from d. Therefore, our methods are competitive both on accuracy and time performance.

4.5 Summary  The clustering model provides several advantages. It is a descriptive model of the data set. It enables support esti- mation and it can be processed in main memory. It requires the user to specify the number of clusters as main input pa- rameter, but it does not require support thresholds. More importantly, clusters can help discovering long itemsets ap- pearing at very low support levels.

We now discuss accuracy. In general, the number of clusters is the most important model characteristic to im- prove accuracy. A higher number of clusters generally pro- duces tighter bounds and therefore more accurate support estimations. The clustering model quality has a direct re- lationship to support estimation error. We introduced a pa- rameter to improve accuracy when mining frequent itemsets from the model; this parameter eliminate ?spurious? item- sets unlikely to be frequent. The clustering model is rea- sonably accurate on a wide spectrum of support values, but accuracy decreases as support decreases.

We conclude with a summary on time complexity and  efficiency. When the clustering model is available, it is a significantly faster mechanism than the A-priori algorithm to search for frequent itemsets. Decreasing support impacts performance due to the rapid combinatorial growth on the number of itemsets. In general, the clustering model is much smaller than a large data set: O(dk) < O(dn). A clustering model can be computed in linear time with re- spect to data set size. In typical transaction data sets dimen- sionality has marginal impact on time.

5 Related Work  There is a lot of research work on scalable clustering [1, 30, 28] and efficient association mining [24, 16, 40], but little has been done finding relationships between as- sociation rules and other data mining techniques. Sufficient statistics are essential to accelerate clustering [7, 30, 28].

Clustering binary data is related to clustering categorical data and binary streams [26]. The k-modes algorithm is proposed in [19]; this algorithm is a variant of K-means, but using only frequency counting on 1/1 matches. ROCK is an algorithm that groups points according to their com- mon neighbors (links) in a hierarchical manner [14]. CAC- TUS is a graph-based algorithm that clusters frequent cat- egorical values using point summaries. These approaches are different from ours since they are not distance-based.

Also, ROCK is a hierarchical algorithm. One interesting aspect discussed in [14] is the error propagation when using a distance-based algorithm to cluster binary data in a hier- archical manner. Nevertheless, K-means is not hierarchical.

Using improved computations for text clustering, given the sparse nature of matrices, has been used before [6]. There is criticism on using distance similarity metrics for binary data [12], but in our case we have proven K-means can pro- vide reasonable results by filtering out most itemsets which are probably infrequent.

Research on association rules is extensive [15]. Most ap- proaches concentrate on speeding up the association genera- tion phase [16, 1]. Some of them use data structures that can help frequency counting for itemsets like the hash-tree, the FP-tree [16] or heaps [18]. Others resort to statistical tech- niques like sampling [38], statistical pruning [24]. In [34] global association support is bounded and approximated for data streams with the support of recent and old item- sets; this approach relies on discrete algorithms for efficient frequency computation instead of using machine learning models like our proposal. Our intent is not to beat those more efficient algorithms, but to show association rules can be mined from a clustering model instead of the transaction data set. In [5] it is shown that according to several pro- posed interest metrics the most interesting rules tend to be close to a support/confidence border. Reference [43] proves several instances of mining maximal frequent itemsets (a     constrained frequent itemset search) are NP-hard and they are at least #P-hard, meaning they will remain intractable even if P=NP. This work gives evidence it is not a good idea to mine all frequent itemsets above a support threshold since the output size is combinatorial. In [13] the authors derive a bound on the number of candidate itemsets given the current set of frequent itemsets, when using a level-wise algorithm.

Covers and bases [37, 21] are an alternative to summarize association rules using a combinatorial approach instead of a model. Clusters have some resemblance to bases in the sense that each cluster can be used to derive all subsets from a maximal itemset. The model represents an approximate cover for all potential associations.

We now discuss closely related work on establishing re- lationships between association rules and other data min- ing techniques. Preliminary results on using clusters to get lower and upper bounds for support is given in [27]. In gen- eral, there is a tradeoff between rules with high support and rules with high confidence [33]; this work proposes an al- gorithm that mines the best rules under a Bayesian model.

There has been work on clustering transactions from item- sets [41]. However, this approach goes in the opposite di- rection: it first mines associations and from them tries to get clusters. Clustering association rules, rather than transac- tions, once they are mined, is analyzed in [22]. The output is a summary of association rules. The approach is different from ours since this proposal works with the original data set whereas ours produces a model of the data set. In [42] the idea of mining frequent itemsets with error tolerance is introduced. This approach is related to ours since the er- ror is somewhat similar to the bounds we propose. Their algorithm can be used as a means to cluster transactions or perform estimation of query selectivity. In [39] the authors explore the idea of building approximate models for associ- ations to see how they change over time.

6 Conclusions  This article proposed to use clusters on binary data sets to bound and estimate association rule support and confi- dence. The sufficient statistics for clustering binary data are simpler than those required for numeric data sets and con- sist only of the sum of binary points (transactions). Each cluster represents a long itemset from which shorter item- sets can be easily derived. The clustering model on high dimensional binary data sets is computed with efficient op- erations on sparse matrices, skipping zeroes. We first pre- sented lower and upper bounds on support, whose average estimates actual support. Model-based support metrics obey the well-known downward closure property. Experiments measured accuracy focusing on relative error in support es- timations and efficiency with real and synthetic data sets. A clustering model is accurate to estimate support when using  a sufficiently high number of clusters. When the number of clusters increases accuracy increases. On the other hand, as the minimum support threshold decreases accuracy also decreases, but at a different rate depending on the data set.

The error on support estimation slowly increases as itemset length increases. The model is fairly accurate to discover a large set of frequent itemsets at multiple support levels.

Clustering is faster than A-priori to mine frequent itemsets, without considering the time to compute the model. Adding the time to compute the model, clustering is slower than A- priori, at high support levels, but faster at low support levels.

The clustering model can be built in linear time on data size.

Sparse matrix operations enable fast computation with high dimensional transaction data sets.

There exist important research issues. We want to ana- lytically understand the relationship between the clustering model and the error on support estimation. We need to de- termine an optimal number of clusters given a maximum error level. Correlation analysis and PCA represent a next step after the clustering model, but the challenges are up- dating much larger matrices and dealing with numerical is- sues. We plan to incorporate constraints based on domain knowledge into the search process. Our algorithm can be optimized to discover and periodically refresh a set of asso- ciation rules on streaming data sets.

