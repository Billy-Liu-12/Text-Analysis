Perturbed Gibbs Samplers for Generating Large-Scale Privacy-Safe Synthetic

Abstract?This paper introduces a non-parametric data syn- thesizing algorithm to generate privacy-safe ?realistic but not real? synthetic health data. Our goal is to provide a systematic mechanism that guarantees an adequate and controllable level of privacy while substantially improving on the utility of public use data, compared to current practices by CMS, OSHPD and other agencies. The proposed algorithm synthesizes artificial records while preserving the statistical characteristics of the original data to the extent possible. The risk from ?database linking attack? is quantified by either an l-diversified or an ?- differentially perturbed data generation process. Moreover its algorithmic performance is optimized using Locality-Sensitive Hashing and parallel computation techniques to yield a linear- time algorithm that is suitable for Big Data Health applications.

We synthesize a public Medicare claim dataset using the proposed algorithm, and demonstrate multiple data mining ap- plications and statistical analyses using the data. The synthetic dataset delivers results that are substantially identical to those obtained from the original dataset, without revealing the actual records.



I. INTRODUCTION  Synthetic data, generated from a certain random pro-  cess, can address disclosure limitation issues in public  use health data. Many health datasets contain privacy-  sensitive and sometimes confidential information such as  disease, payment, and treatment records. Revealing such  health information is clearly disagreeable to many, and raises  severe ethical and fiduciary issues. Instead, when created  carefully, synthetic data can provide the required statistical  information for various analyses in the healthcare domain  without revealing person-specific data or person?s identity.

Developing appropriate algorithms to generate synthetic data  is critical to meeting the growing need for well-grounded  health informatics research.

Using traditional ?parametric model-based? synthesizers,  however, provides only a partial solution to such objectives  [1], as it introduces two open-ended issues, namely model  selection problem and unquantifiable privacy-risk. The com-  plexity of a synthetic model limits the answerable range of  research questions; for example, a linear model synthetic  data cannot address identifying quadratic relationships be-  tween covariates. However, most data mining research is  based on retrospective analysis, where the research questions  are posed post hoc and may be determined during the data  exploration process. Thus, this type of synthetic data are  not perfectly suited for data mining applications. Moreover,  popular privacy metrics such as k-anonymity [2], l-diversity  [3], or ?-differential privacy [4] cannot be directly applied  to such model-based synthesizers [5]. Recent reports on  adversarial privacy attacks [6], [7] suggest that rigorous  characterization of such risk is critical in data publishing.

These difficulties usually result in ?overly complicated?  synthetic processes when publishing a privacy-sensitive  dataset. As an illustrative example, Centers for Medicare  and Medicaid Services (CMS) recently published synthetic  Medicare data for 5% of the US population [8] ? Data  Entrepreneurs? Synthetic Public Use File (DE-SynPUF). To  publish such data, the original data were processed with six  different Statistical Disclosure Limitation (SDL) techniques:  1) Variable Reduction (feature selection), 2) Suppression (k-  anonymity), 3) Substitution (data swapping), 4) Imputation,  5) Data Perturbation (on event times), and 6) Coarsening  (binning). These processes usually require extensive domain-  knowledge, hence they are very difficult to automate. Fur-  thermore, this chain of perturbations substantially changes  the characteristics of the original data, to the extent that  CMS explicitly states ?As a result, the DE-SynPUF has  very limited inferential research utility... That is, analyses  using the DE-SynPUF to draw inferences about Medicare  beneficiaries, providers, or the Medicare program will be  misleading and often incorrect [8, page 16]?.

Non-parametric synthetic data can be a remedy for the  model selection issue. Moreover, if the generative process  of such data adheres to a certain state of the art privacy  metric, then the risk from synthetic data can be well  characterized. However, such non-parametric schemes are  rarely adopted in generating synthetic data in practice, due  to its computational cost, and non-intuitive connections to  privacy metrics. In this paper, we analyze the definition of  privacy in the healthcare context, and derive its connection  to probabilistic generative processes. We propose a novel  and practical algorithm for generating synthetic data. The  proposed algorithm utilizes parallel computation and Lo-  cality Sensitive Hashing (LSH) techniques [9] to address  the computational challenges. This data type is dominant in  many health records. Furthermore, for many numeric fields,   DOI 10.1109/ICHI.2013.76    DOI 10.1109/ICHI.2013.76    DOI 10.1109/ICHI.2013.76     binning, also known as histogramming or quantization, can  be appropriately applied to transform data into categorical  format, supporting the generality of our framework.



II. RELATED WORK  A wide variety of privacy oriented mechanisms have  been proposed in the past decade, including differential  privacy, k-anonymity and l-diversity [10]. We note that  while differential privacy is perhaps the currently most  popular approach in the academic community, it has not been  adopted by healthcare agencies, who rely on a combination  of approaches such as described in [8]. Though for brevity  we have focussed on l-diversity and differential privacy,  our framework can work with a variety of mechanisms,  including combinations, making it more likely to be adopted.

There are two main bodies of related work regarding the  technical parts of this paper: ?statistical privacy-preserving  disclosure? and ?non-parametric data synthesizing? meth-  ods. We first visit Statistical Disclosure Limitation (SDL)  techniques, and then briefly cover Markov Chain Monte  Carlo (MCMC) to describe data synthesizing methods.

SDL can be cast into three categories [11]: data-swapping,  data recoding, and multiple-imputation. In data-swapping  [12], sensitive row values are swapped with other row values.

Although this method can protect sensitive information from  simple adversarial attacks, data swapping distorts the joint  distribution of the original data. Data recoding is also known  as feature suppression or aggregation. The masking process  is usually based on the ?k-anonymity? principle that ensures  any entry in data is indistinguishable with other k entries.

Data recoding can be viewed as a lossy coding, and finding  the utility-optimal coding is an NP-hard problem. Multiple-  imputation (MI) [13] utilizes MCMC algorithms to obtain  random samples from posterior distributions. MI typically  uses parametrized posterior distributions, thus this approach  suffers from the same model-based synthesizer issues: model  selection and unquantifiable privacy-risk issues.

MCMC methods generally refer to a class of algorithms  that draw samples from non-trivial probability distributions  through Markov chain simulations. Some special cases  are Metropolis-Hastings (MH) algorithm, Gibbs sampler,  Slice sampling, and Multiple-try Metropolis. Among many  choices, this paper uses Gibbs sampler for two reasons.

First, Gibbs sampler only requires conditional distributions  and no other tuning parameters. This property is crucial in  our approach, since our goal is to control privacy-risks by  ?deliberately? perturbing conditional distributions. Note that  all these alternatives try to estimate the true joint distribution,  while we take a novel approach of deliberately perturbing the  sampling process to meet privacy requirements while staying  close to the true distribution. Second, unlike MH algorithm,  Gibbs sampler does not reject samples, thus Gibbs sampler  is usually more efficient than MH algorithm when drawing  high-dimensional samples. For this reason, Gibbs sampler is  sometimes viewed as a special case of MH algorithm with  no rejection.



III. PRELIMINARIES  The original table and synthetic table are denoted as D and S, respectively. n and m represent the number of rows and columns in table D, and x = (x1, x2, ... , xm) is a row of the table. x\j is a row with the value of xj undetermined  e.g. x\1 = (?, x2, ... , xm). Random variables are expressed using capital letters, for example X for a scalar, and X for a  vector. Pr(X) is a true probability distribution of the original data, and Pre(X | D) represents an empirical probability mass function given data D.

A categorical dataset D with indistinguishable rows, i.e. without any ID?s, can be completely described by a  contingency table. Thus, with enough number of samples,  the contingency table can be said as the most precise  and non-parametric approximation for the underlying joint  distribution. We exchangeably use the notations for the  normalized contingency table1 and the empirical distribution  Pre(X | D).

The difference between the empirical and the true distri-  bution decays as the number of data points increase [14].

The empirical distribution approaches the true distribution  exponentially fast as N increases. In practice, this empirical  distribution can be treated as the underlying true distribution  when the size of the data is huge.

Obtaining a full contingency table is usually intractable  especially with a large number of attributes. However, we  can mimic sampling from a full contingency table using  Gibbs sampling. Gibbs sampling is a prevalent estimation or  sampling technique, when sampling from a joint distribution  is intractable. A synthetic sample xs from a joint distribution  Pre(X | D) can be Gibbs-sampled as follows:  x1 ? Pre(X1 | x\1,D)  x2 ? Pre(X2 | x\2,D)  ...

xm ? Pre(Xm | x\m,D)  where Pre(Xi | x\i,D) is a conditional frequency table derived from data D. When the above cycle has reached an enough number iterations (burn-in period), the last sample  x is equivalent to a random sample from the joint distri-  bution. It is important to note that, in Gibbs sampling, the  information about the joint distribution is distributed across  its conditional distributions.

This brute-force Gibbs synthesizer, however, has three  critical issues: a lack of convergence guarantee, compu-  tational inefficiency, and loose privacy guarantee. For a  Gibbs sampler to converge, the Markov chain in a Gibbs  1Normalized by the total row counts.

sampler needs to be irreducible and aperiodic. In the brute-  force Gibbs synthesizer, the Markov chain of the empirical  conditional distributions needs to be verified to satisfy both  conditions. Moreover, this brute-force Gibbs synthesizer is  computationally expensive. The number of distinct condi-  tional distributions exponentially grows with the number  of features, thus pre-computing Pre(Xi | X\i,D) is not desirable. Estimating Pre(Xi | X\i,D) on the fly is not a smart choice, since the estimation needs a linear scan for  every Gibbs iteration. Finally, synthetic samples from an  empirical distribution always can always be found in the  original data, as the support of an empirical distribution is  the same as the support of the original data. In other words,  the synthetic data are ?realistic and real?, not ?realistic but  not real?.



IV. PEGS ALGORITHM  We now present a Perturbed Gibbs Sampler (PeGS). PeGS  is illustrated in Algorithm 1. The PeGS algorithm consists  of three steps: estimation, perturbation, and sampling steps.

Algorithm 1 PeGS Algorithm  (Step 1) Estimate Pe(Xi | g(X\i),D) (Step 2) Perturb Pe ? Q (Step 3) Gibbs-sample synthetic data from the perturbed  conditional distributions  A. Step 1: Estimation  Instead of Pre(Xi | X\i,D), PeGS estimates an approx- imated distribution, Pe(Xi | g(X\i),D), where g(X\i) is a hash function such as MinHashing [15] and Locality-  Sensitive Hashing (LSH) [9], [16]. The hash function is  employed to reduce the number of distinct conditional dis-  tributions and to access the conditional distributions faster.

We perform a fast (approximate) nearest neighbor search  using LSH. In this paper, we choose to use an LSH family  F defined for a metric space M = (x,Hamming), but we note that other distance metrics are also applicable. Thus,  even for a really large-scale database with high-dimensional  features, PeGS can sample from its (approximated) condi-  tional distributions. Note that there is a trade-off between  the exact conditional distribution and the required memory  size.

B. Step 2: Perturbation  Two types of perturbation methods are illustrated: l-  diversity and ?-differential privacy. Although we borrow the  privacy metric names, we note that the actual definitions  in PeGS are slightly different from the original definitions.

For example, the original l-diversity is typically achieved  by suppressing or generalizing features, not synthesizing  samples. Moreover, the original ?-differential privacy is  based on the context of statistical databases, not publishing  synthetic data.

We first describe an l-diversified perturbation technique.

The obtained conditional distributions are perturbed to meet  a prescribed entropy l-diversity criterion for privacy:  Definition 1 (Entropy l-diversity [3]). A probability mass  function Q is ?synthetic l-diverse? if  ?E[logQ] = ? ? x  Q(x) log  Q(x) ? C ? log l  where 0 < log l < 1 and C = log  |X|.

We perturb the approximated probability distribution  Pe(Xi | g(X\i),D) to satisfy the l-diversity principle as follows:  Q?(Xi | g(X\i)) = (1? ?)Pe(Xi | g(X\i),D) + ?U  where U is a uniform distribution over the range of Xi.

As we want to keep the statistical properties of the original  data to the extent possible, we choose the minimum ? that  satisfies the condition:  ?? = argmin ?  Q?(Xi | g(X\i)) ? C log l  Q(Xi | g(X\i)) = Q??(Xi | g(X\i))  Next, we show an ?-differential perturbation technique.

Definition 2 (Exponential Gibbs Sampler). A probability  mass function Q is ??-differentially perturbed? if  Q?(x) ? exp(? ?N ? P(x))  where ? > 0 and N is a number of samples that are used to estimate P(x).

In the ?-differential perturbation mechanism, a smaller  value of ? means a larger amount of perturbation. Q?(x) approaches to P(x) as ? moves toward infinity. In PeGS, this synthetic ?-differentially perturbation mechanism can be  implemented as follows:  Q?(xi | g(X\i)) = exp(?NPe(xi | g(X\i),D))? zi exp(?NPe(zi | g(X\i),D))  This perturbation mechanism guarantees ?-differential pri-  vacy in a following sense. For two data sets differing at most  one row, D and D?, we have:  sup D,D?  Q?(xi | g(X\i),D)  Q?(xi | g(X\i),D ?)  < exp(?)  C. Step 3: Sampling  PeGS samples are generated through:  x1 ? Q(X1 | g(x\1))  x2 ? Q(X2 | g(x\2))  ...

xm ? Q(Xm | g(x\m))     This Markov chain converges to a unique stationary distri-  bution, Q(X) if ? > 0, see Theorem 1.

Theorem 1 (Existence of Q(X)). If Q(Xi|X\i) > 0 (positivity condition), then the Markov chain of a perturbed  Gibbs sampler is irreducible, thus there exists a unique  stationary distribution Q(X).

Proof: The Markov chain is recurrent with positivity  condition, thus it has a unique stationary distribution Q(X).

The difference between Pre(X) and Q(X) is upper- bounded by the parameter log l and ? depending on per- turbation methods, and hence the difference between the  true distribution Pr(X) and the PeGS-distribution Q(X) is also upper-bounded. Note that the perturbed Gibbs sampler  generates artificial samples that are not in the original data.

For these artificial samples, we find the nearest neighboring  perturbed conditional distribution as follows:  Q(Xi | g(X ann \i )) s.t. X  ann \i ? X\i  using LSH, then use this approximated nearest neighbor  distribution in the Gibbs sampling iterations.

D. Additional Step: Parallelization  The PeGS Algorithm is embarrassingly parallelizable with  sub-sampling. Algorithm 2 illustrates the para-PeGS algo-  rithm. If each process in para-PeGS satisfies the desired  privacy definitions, the collection of synthetic data will also  meet the desired level of privacy guarantees.

Algorithm 2 para-PeGS Algorithm  (Step 0) Partition D into {Dp} (Step 1, 2, 3) PeGS with Dp  (Step 4) Combine and mix synthetic data {Sp}

V. PRIVACY ANALYSES  The ?realistic but not real? principle can be achieved  through rigorous analyses on the definition of ?privacy? in  the healthcare context. Unlike other scientific measurement  datasets, healthcare datasets in general contain the informa-  tion about real people, such as patients and physicians. Thus,  the definition of data privacy in healthcare datasets can be  narrowed down. We define the goal of privacy protection in  the healthcare domain is to protect the identity of the people,  and to protect their sensitive information.

The attacks on published datasets are either identity  attacks (identifying ?who?) or feature attacks (identifying  ?what?). Although many other types of attacks may be  threats to uncover private information, such attacks are not  directly related to our definition of privacy protection. For  example, an attacker can link two datasets and find out new  information for a specific row, without knowing a person?s  identity. In this case, as the attacker has no clue about the  Figure 1. A link attack example. Two datasets can be linked based on Age and ZIP code. Sensitive values such as Diabetes and STD can be revealed using the new linked variables Name and BMI.

identity of the records, this attack does not infringe on  privacy by our definition. Again, it is possible that even  such information can be useful for later identity and feature  attacks (transitive attacks). We note that the uncertainty level  of such information may decrease as more linkable datasets  become available.

In this paper, we analyze the effect of database linking  on feature attacks. Consider a dataset with two features:  X (non-sensitive) and Y (sensitive). When publishing the  dataset, a typical strategy is to noise Y to be Y? . However, if  another dataset is linked with the original dataset based on  X , the linked information may reduce the uncertainty about  the sensitive field. Figure 1 illustrates this linking scenario.

Theorem 2 illustrates this information gain for attackers by  linking two datasets:  Theorem 2 (Link Gain). Suppose that two datasets DZ = {Z,X} and DY = {(Y? , X)} are linked based on X , then:  Pr(Y? = Y | X,Z)  Pr(Y? = Y | X) =  Pr(Z | X, Y? = Y )  Pr(Z | X) = ? (1)  where ? is the ?link gain ratio? by linking two datasets.

Proof: Equation (1) is a direct result by applying Bayes?  theorem.

Theorem 2 states two competing objectives when publish-  ing data: ? needs to be low and Pr(Y? = Y | X) needs to be low. As can be seen, ? increases as Pr(Y? = Y | X) decreases, and Pr(Y? = Y | X) increases as ? decreases.

Furthermore, ? can be arbitrarily large when Pr(Z | X) ? 0, where we have no control over Z and DZ . Thus, lowering ? is very difficult. PeGS addresses this issue by perturbing every variable in a dataset i.e. including linking variables.

In other words, instead of directly lowering ?, we inject uncertainty on linking.

The perturbation step in PeGS can be interpreted from the  information theory context. The mutual information between  Xi and X\i is as follows:  I(Xi;X\i) = H(Xi)?H(Xi|X\i) (2)     where I(Xi;X\i) is Shannon Mutual Information between Xi and X\i. Uniformly smoothing Pre(Xi | X\i,D) increases the conditional entropy H(Xi|X\i), decreasing I(Xi;X\i) as a result. Thus, PeGS weakens the link be- tween Xi and X\i by increasing the conditional entropy.

On the other hand, traditional data publishing methods  such as k-anonimity and l-diversity decreases the entropy  of H(Xi) by generalization or suppression of the feature values. Although their approaches are different, the common  goal of such privacy mechanisms is reducing the mutual  information between features.



VI. EMPIRICAL STUDIES  In this section, we demonstrate the impact of PeGS al-  gorithm using Medicare Claims records for both descriptive  analysis and predictive modeling. Centers for Medicare and  Medicaid Services (CMS) provides several public-use data  files (PUF), such as inpatient claims, line-items, drug events,  etc. For our experiment, we use ?BSA Inpatient Claims  PUF?2 describing Basic Stand Alone Inpatient records.

The data file contains seven variables: ID, Gender, Age,  DRG (drug code), ICD-9 (procedure code), Length (the  length of stay), and Amount (payment), and has 15K rows.

Age, Length, and Amount variables are originally numeric  records, but CMS has categorized them into five quantiles.

Note that data re-coding methods such as k-anonymity  and l-diversity cannot be directly compared to PeGS, as  their feature granularities are different. To the best of our  knowledge, PeGS is the first privacy-safe data synthesizer,  which adheres to the rigorous privacy metrics, ?synthetic  l-diversity? and ??-differential perturbation?.

A. Marginal Distribution  Figure 2 and 3 shows marginal histograms of drug and  ICD-9 procedure codes over different levels of privacy  metrics log l and ?. Histograms from the original data are shown in the top left cells. As can be seen, the histograms  with lower values of log l and higher values of ? are similar to those of the original data. Note that, in an l-diversified  PeGS, log l = 0 represents no perturbation on conditional distributions. Also, an ?-differential PeGS becomes a non-  perturbed PeGS as ? approaches to infinity. As can be  seen, the histograms when log l = 0.5 and ? = 0.5 are very close to a uniform distribution. With this amount of  perturbation, any particular record is not distinguishable  from other records (privacy-safe), but the utility of the  data also decreases. The parameters log l and ? should be determined considering this trade-off relationship between  ?utility and privacy?.

B. Effect on Predictive Models  Privacy preserving synthetic data can be publicly dis-  closed to answer various types of data mining research  2http://www.cms.gov/Research-Statistics-Data-and-Systems/Statistics- Trends-and-Reports/BSAPUFS/Inpatient Claims.html  Original Data log l = 0.1 log l = 0.2 log l = 0.5  diff eps = 10 diff eps = 5 diff eps = 1 diff eps = 0.5          Figure 2. Marginal Histogram of Drug Codes vs. log l and ?.

Original Data log l = 0.1 log l = 0.2 log l = 0.5  diff eps = 10 diff eps = 5 diff eps = 1 diff eps = 0.5            Figure 3. Marginal Histogram of ICD-9 Procedure Codes vs. log l and ?.

ldiv diff  1.7  1.8  1.9  2.0   .0   .1  .1   .2  In fi n it y   0 5 1  M S  E  lambda 0.001 0.01 0.1  Figure 4. MSE vs. log l, ? and ? (color coded, regularization parameter).

?ldiv? and ?diff? represent l-diversity and ?-differential privacy, respec- tively.

0 0.05 0.1 0.15 0.2  ?2    ld iv  ?9 ?6 ?3 ?9 ?6 ?3 ?9 ?6 ?3 ?9 ?6 ?3 ?9 ?6 ?3  log?  ?  IP_CLM_DAYS_CD  IP_CLM_ICD9_PRCDR_CD=NA  Infinity 50 10 5 1  ?2    d iff  ?10.0 ?7.5 ?5.0 ?2.5 ?10.0 ?7.5 ?5.0 ?2.5 ?10.0 ?7.5 ?5.0 ?2.5 ?10.0 ?7.5 ?5.0 ?2.5 ?10.0 ?7.5 ?5.0 ?2.5  log?  ?  IP_CLM_DAYS_CD  Figure 5. Coefficient values vs. log l, ? and ?. Each ? is color-coded. ?IP CLM DAYS CD? (days) is positively correlated with the target variable.

?IP CLM ICD9 PRCDR CD=NA? is negatively correlated with the target, and we also can check this fact from the l-diversified PeGS data.

questions. If the statistical properties of the original data  are well preserved in the PeGS-generated data, then the data  mining results from the synthetic data should be significantly  identical to those from the original data. We demonstrate il-  lustrative results of simple predictive modeling by arbitrarily  setting Amount as a dependent variable:  AMT? ?? ? y: dependent variable  ? Age + Gender + DRG + ICD9 + DAYS? ?? ? X: independent variables  Dummy coding is applied to the categorical variables such  as DRG and ICD9, resulting in 352 dependent variables.

We apply the Lasso regression [17] to deal with this high  dimensionality, then measure Mean Square Errors (MSE)  using the original data:  ?? = min ? ?ysynth ?Xsynth??  2 + ????  MSE = ?yorig ?Xorig? ??2  Figure 4 shows the measured MSE values over different ?,  log l, and ? values. The regularization parameter ? is color- coded. As can be seen, MSE values increase with the log l value, but their deviation from the original MSE is negligible  compared to the spread of values at each privacy setting,  log l. For ?-differentially perturbed data, we can observe that the measured MSE values decreases as ? approaches  to infinity.

Next we check the interpretation of the models obtained  from the perturbed data. Figure 5 shows the coefficients  values ?? over log l, ?, and ? values. Intuitively, the amount spent in a hospital (the target variable) is proportional to  the days spent in a hospital. From the original data, we can  observe a strong correlation between these two variables. As  a result, LASSO regression produces a non-zero coefficient  value for this independent variable (days) when a strong  regularization parameter is imposed (e.g. log ? > ?3). As can be seen in Figure 5, the perturbed data produce non-  zero coefficients for the ?day? variable. Although we also  can see some differences in other non-significant coefficient  values, the perturbed data preserve the overall coefficient  characteristics of the original data.



VII. DISCUSSION  We proposed a novel data synthesizer that protects sen-  sitive information by adhering to a prescribed l-diversity  and ?-differential privacy metrics. The proposed algorithm,  PeGS, scales linearly with respect to the size of the data.

Furthermore, PeGS is a non-parametric and non-model  based technique. This property assures that a wide range of  data mining algorithms can be applied without considering  the generative process of the synthetic data. Many health  datasets are not available because of privacy restrictions such  as HIPAA privacy regulation. We believe that the proposed  solution can be an alternative way to facilitate collaborative  healthcare research efforts.

