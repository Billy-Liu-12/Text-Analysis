F-Miner: A New Frequent Itemsets Mining Algorithm?

Abstract  In this paper, we present a novel algorithm, called F- Miner, to mine the complete set of frequent itemsets by pat- tern growth. The F-Miner algorithm uses two new com- pact data structures, Ascending FP-Tree (AFP-Tree) and Frequent Pattern Forest (FP-Forest), to represent the con- ditional databases. When we construct an AFP-Tree, the items in frequent 1-itemset are ordered in frequency ascend- ing order. The AFP-Tree structure is traversed in top-down depth-first order. The root of the AFP-Tree is not ?null?, but an item which can identify this tree. AFP-Tree has a one- dimensional array which stores the counts of every tree- node?s item except root-node. In F-Miner, we need many AFP-Trees to store a conditional database; these trees con- struct one forest, called FP-Forest. We test our algorithm versus several other algorithms on real world datasets, such as BMS-POS. The experimental results show that our al- gorithm is an efficient algorithm on both sparse and dense databases.

1. Introduction  Frequent itemset mining is a fundamental and essential problem in data mining and can be used in many data min- ing applications. These applications include association rules analysis, correlations analysis, classification, cluster- ing, outlier analysis and many other important discovery tasks. Since the pioneering work in [1], the problem of efficiently generating frequent itemsets has been an active research topic.

?Supported by Natural Science Foundation of Gansu Province Project 3ZS051-A25-035 and Gansu Province Meteorological Administration In- novation Foundation Project (2005).

The most popular frequent itemset mining algorithm seems to be the Apriori algorithm [2]. This algorithm also forms the foundation of most known algorithms. It uses an anti-monotone property stating that if a k-itemset is frequent, all of its (k-1)-itemsets must be frequent. Us- ing the fundamental property can reduce the computational cost of candidate frequent itemset generation dramatically.

However, in the cases of extremely large input sets with big frequent 1-itemset, the Apriori algorithm still suffers from two main problems of repeated I/O scanning and high computational cost. One major hurdle observed with most real datasets is the sheer size of the candidate frequent 2- itemsets and 3-itemsets.

One innovative approach of discovering frequent item- sets in transactional databases, FP-Growth, was proposed by J.Han et al. in [4]. This algorithm creates a compact tree-structure, FP-Tree, representing frequent patterns, that alleviates the multi-scan problem and improves the candi- date itemset generation. The algorithm requires only two full I/O scans of the dataset to build the prefix tree in main memory and then mines this structure directly. The authors of this algorithm report that their algorithm is faster than the Apriori algorithm. Mining the FP-Tree structure is done re- cursively by building conditional trees that are of the same order of magnitude in number as the frequent patterns. This massive creation of conditional trees makes this algorithm not scalable to mine large datasets beyond few millions.

In this paper, we present a novel algorithm, called F- Miner, to mine the complete set of frequent itemsets by pattern growth. The F-Miner algorithm uses two new data structures, Ascending FP-Tree (AFP-Tree) and Frequent Pattern Forest (FP-Forest). AFP-Tree is a variation of FP- Tree; FP-Forest is constructed by many AFP-Trees. We test our algorithm versus several other algorithms on real world datasets. The experimental results show that our algorithm  0-7695-2645-4/06 $20.00  ? 2006    is an efficient algorithm on both sparse and dense databases.

The rest of the paper is organized as follows: In Section  2, we define relevant terms used in frequent itemset min- ing. In Section 3, we describe the design and construction of AFP-Tree and FP-Forest. Section 4 contains a descrip- tion of the F-Miner algorithm and the experimental results are reported in Section 5. We conclude with a summary in Section 6.

2. Preliminaries  We define the basic terms needed for describing frequent itemsets mining.

Definition 1: Let I= {i1, i2, i3, ..., in} be a set of items, and D be a set of transactions, where a transaction T is a subset of I (T ?I). Each transaction is identified by a TID.

An itemset X is a subset of I (X ?I), and an itemset of length k is called a k-itemset. The support of an itemset X is the number of transactions in D that contains X. If the support of an itemset is greater than or equal to a given sup- port threshold s, it is called a frequent itemset or frequent pattern otherwise it is infrequent.

The problem of frequent itemset mining is to find the complete set of frequent patterns in a given transaction database with respect to a given support threshold s.

3 AFP-Tree and FP-Forest: design and con- struction  The prefix tree was first used for frequent itemset mining in the TreeProjection algorithm [3] that outperformed Apri- ori. Han et al. employed a variant of the prefix tree named FP-Tree for their FP-Growth algorithm in [4]. Experiments showed that FP-Growth performed better than Apriori and TreeProjection. FP-Growth algorithm opened up the new way to mine the frequent pattern effectively. However, its time and space efficiency are not high enough, still need to be further improved. A variant of FP-Tree named COFI- Tree was proposed in [12]. The difference between COFI- Tree and FP-Tree is that COFI-Tree maintains bi-directional links allowing bottom-up scanning as well, but the num- ber of nodes is the same as in FP-Tree. AFOPT is a sim- ple while compact data structure to store the conditional databases, was proposed in [7]. The AFOPT structure is traversed in top-down depth-first order; experiment results show that the combination of the top-down traversal strat- egy and the ascending frequency order achieves significant performance improvement over previous works. In [5], the authors propose a novel array-based technique that greatly reduces the need to traverse FP-Tree, thus obtaining signifi- cantly improved performance for FP-Tree based algorithms.

Furthermore, the authors present a new algorithm for min- ing complete frequent itemsets, called FP-Growth*. This  algorithm uses the FP-Tree data structure in combination with the array technique efficiently, and incorporates vari- ous optimization techniques.

In this paper, we present two new data structures, As- cending FP-Tree (AFP-Tree) and Frequent Pattern Forest (FP-Forest) to store the conditional databases. AFP-Tree is a variation of FP-Tree, and FP-Forest is constructed by many AFP-Trees. When we construct an AFP-Tree, the items in frequent 1-itemsets are ordered in frequency as- cending order. The AFP-Tree structure is traversed in top- down depth-first order. The root of the AFP-Tree is not ?null?, but an item which can identify this tree. AFP-Tree has a one-dimensional array which stores the counts of ev- ery tree-node?s item except root-node. The one-dimensional array of the AFP-Tree is like but not completely like the array of the FP-Growth*, which is two-dimensional, used to store the support of the 2-itemset. Given a transaction database, we need many AFP-Trees to store it; these trees construct one forest, called FP-Forest.

Table 1. Dataset  TID Transactions 100 I1, I2, I5 200 I2, I4 300 I2, I3 400 I1, I2, I4 500 I1, I3 600 I2, I3 700 I1, I3 800 I1, I2, I3, I5 900 I1, I2, I3  We use an example to illustrate the construction of the AFP-Tree and FP-Forest structure from the original database. Given a transaction database D as shown in ta- ble 1 and minimum support threshold s=2, we can construct the AFP-Trees and FP-Forest structure as follows.

First, a scan of transaction database derives a list of fre- quent items, L= I5:2, I4:2, I1:6, I3:6, I2:7 (the number after ?:? indicates the support), in which items are ordered in fre- quency ascending order. This ordering is important since each path of a tree will follow this order. The length of L is 5.

Second, an empty FP-Forest is constructed. The length of L is 5, so we construct 5 AFP-Trees which included in the FP-Forest. The roots of the AFP-Trees are filled by I5, I4, I1, I3, I2 in turns, and the 5 AFP-Trees are labeled with TI5, TI4, TI1, TI3, TI2, and the array of the 5 AFP-Trees are named AI5, AI4, AI1, AI3, AI2. In the second database scan, transaction 100 becomes F1= I5, I1, I2. Notice that the frequent items in the transaction are listed according to the order in the list of frequent items L. The first item of  0-7695-2645-4/06 $20.00  ? 2006    F1 is I5, and then we insert the other items I1, I2 into the AFP-Tree TI5, at the same time the counts of I1, I2 in AI5 are increased by 1. Transaction 200 becomes F2= I4, I2. I4 is the first item, so we insert I2 into the TI2, and the count of I2 in AI2 is increased by 1. After all the transactions are inserted, the whole AFP-Trees and the FP-Forest are shown in figure 1. In figure 1, the length of the AI2 is zero, so we don?t draw it.

AI4  AI5  I4  I1  I3  I2         I1  I3  I2    I3  I2 2 I2  AI3 AI1  TI5           TI4           TI1        TI3       TI2 I2:7 I1:6  I3:3  I2:1  I3:6  I2:2  I5:2  I1:2  I2:1 I3:1  I2:1  I4:2  I1:1 I2:1  I2:1  FP-Forest  Figure 1. AFP-Tree and FP-Forest Construc- tion  Procedure: BuildFP-Forest(D, s)  Input: transaction database D; minimum support s  Output: FP-Forest and AFP-Trees  Method:  1:  L = find_frequent_1-itemsets(D);  // Constructed Forest with L.size AFP-Trees  2:  CreateFP-Forest(Forest, L.size);  3:  for each transaction t D  4:  {  // F is listed according to the order in the L  5:    F = find_frequent_items(t, L);  // TF[1] is a AFP-Tree, its root is F[1]  6:    FP-Forest.insert_tree(TF[1], F);  7:  }  Figure 2. Procedure for constructing FP- Forest  Figure 2 gives the procedure for constructing FP-Forest.

In line 1, L is the frequent 1-itemset of transaction database,  in which items are ordered in frequency ascending order. In line 2, FP-Forest is constructed, it has L.size AFP-Trees, the order of the AFP-Trees are according to the order in the L.

Line 5 finds the frequent items list F, items in F is listed according to the order in the L. In line 6, F is inserted into an AFP-Tree; the root of the AFP-Tree is the first item in F.

4 F-Miner algorithm  In this section, we describe the novel F-Miner algorithm for mining complete frequent patterns using the AFP-Tree and FP-Forest.

4.1 Description of F-Miner  F-Miner uses the new data structure, AFP-Tree and FP- Forest, to mine the complete set of frequent itemset by pat- tern growth. The AFP-Tree structure is traversed in top- down depth-first order.

Algorithm: F-Miner(Forest, s)  Input: FP-Forest structure Forest, minimum support s  Output: all frequrnt itemsets  // according to the order in the L  1: foreach AFP-Tree T in Forest {  2:   output T.root.item Forest.baseItem  with support=T.root.count;  3:   if T contains a single-frequent-branch P  then  4:     foreach subpath Y of P  // Y contain the root  5:       output Y Forest.baseItem  with support=smallest count of nodes in Y;  6:   else FP-Growth+(T, s, Forest.baseItem);  7:   PushRight(T, Forest); // rightward merge operator  8:   Free(T);  9: }  Figure 3. F-Miner algorithm  Figure 3 gives algorithm F-Miner for mining all fre- quent itemsets from a FP-Forest structure. Before calling F-Miner, we already constructed a FP-Forest structure For- est. Line 1 mines one AFP-Tree structure T of Forest in turns, and line 2 outputs a frequent itemset, which con- tains item in root of T and base item of Forest with support equals the count of T.root. In Line 3, if T contains a single- frequent-branch, then line 4, 5 forms all subpaths which must contain the root and outputs all frequent itemsets con- taining these subpaths and base item of Forest. Else, line 6  0-7695-2645-4/06 $20.00  ? 2006    calls FPGrowth+ method to mine this AFP-Tree structure T. Line 7 calls PushRight method to merge T into T?s right AFPTree(s). The main purpose of this method is to prevent the losing of information. In line 8, the memory space used by T and T?Array is deallocated.

Algorithm: FP-Growth+(T, s, base)  Input: AFP-Tree structure T; minimum support s;  Base Item base  Function: Mining frequent itemsets on one AFP-Tree  Method:  1: L = find_frequent_1_itemsets(T.Array);  2: if L.size == 1 then  3:   output L[1].item T.root.item base  with support = L[1].count;  4: else if L.size > 1 then {  // constructing conditional FP-Forest  5:   BuildFP-Forest(ConForest, T, L);  6:   F-Miner(ConForest, s);  7: }  Figure 4. FP-Growth+ algorithm      ConForest  (2) constructs  ConForest  (4) merges TI5  into TI1  (3) executes F-Miner  on ConForest  (1) finds frequent  1-itemset L  I4  I1  I3  I2      AI5  I1  I2  2   2 L  I2  AI5I1    I3  I2  AI1  I5:2  I1:2  I2:1 I3:1  I2:1  {I5, I1:2}  {I5, I1, I2:2}  {I5, I2:2}  TI2I1   TI5I2  I2:2 I1:2  I2:2 I1:6  I3:4 I2:1  I2:2  TI5  Figure 5. Example for F-Miner  Figure 4 gives algorithm FP-Growth+ to mine all fre- quent itemsets of an AFP-Tree. Line 1 counts the frequent 1-itemset L of AFP-Tree structure T from T?Array, and items in L are ordered in frequency ascending order. In line 2, if length of L equals 1, then line 3 outputs a frequent  itemset which contains L[1].item, T.root.item and base item.

Else, if length of L greater than 1, line 5 constructs a con- ditional FP-Forest structure ConForest based on T, line 6 calls F-Miner to mine ConForest.

Figure 5, we give an example for using F-Miner on AFP- Tree TI5 which was given in Figure 1.

4.2 Single frequent branch  Definition 2: Traversing an AFP-Tree top-down, when comes to some node N, if the count of N is less than the minimum support threshold s, while the count of N?s parent node is greater than or equal to the minimum support thresh- old s, and all the ancestor nodes of N have only one child node, then we can say the AFP-Tree contains one single- frequent-branch, the branch goes from the root of the AFP- Tree to the parent node of N.

a:5  b:4  c:3  d:2  e:1  f:1  f:1  a  a:5  b:4  c:3  d:2  e:1  f:1  b  a:5  b:4  c:3  d:2  e:1  e:1  c  Figure 6. Three FP-Trees  Figure 6 gives three AFP-Trees structure, but we don?t draw corresponding arrays. Let minimum support threshold s equal to 2. According to definition 2, we can find out that only AFP-Tree b contains a single-frequent-branch, this branch is a, b, c, d, while AFP-Tree a, c does not contain single-frequent-branch.

If an AFP-Tree contains a single-frequent-branch P, then mining this tree will become very easy. We can enumerate all the combinations of the subpaths (which must contain the root node) of P with the support set to the minimum count of the items contained in the subpath. This is because each such subpath is distinct and appears the same number of times as the minimum occurrence frequency among the items in the subpath which is equal to the support of the last item in the subpath. For AFP-Tree b in figure 6, it contains a single-frequent-branch a, b, c, d. We can enumerate all the combinations of the subpath of this branch: ab, ac, ad, abc, abd, acd and abcd, for other node e and f, we no longer consider.

0-7695-2645-4/06 $20.00  ? 2006    4.3 High performance memory manager  In the execution process of F-Miner algorithm, each re- cursion must involve the construction and release of AFP- Tree, frequently allocate and deallocate the memory in this process, will cause the decline of algorithm?s efficiency.

In order to solve this problem, we designed the High Per- formance Memory Manager (HPMM) that is responsible for the allocation and deallocation of the memory in F- Miner algorithm. In the process of construction and release of AFP-Tree, the base memory unit is an AFP-Tree node, records as: AFPnode.

In the F-Miner implementation, the allocation and deal- location frequency of AFPnode is extremely high. We can use the HPMM to store memory entrance of AFPnode that has already been released but not returned to the system memory heap. So when we need AFPnode memory next time, it can be obtained from HPMM directly. In this way, the efficiency of F-Miner can be speeded up. Figure 7 is the test result of HPMM and SDMM (System Dynamic Mem- ory Manager. We can see that HPMM is very efficient.

Figure 7. Test result of HPMM  5 Experimental analysis  In this section, we report the result of the experiments to evaluate performance of F-Miner. All testing was per- formed on a 2.8 GHz Pentium PC with 1 GB RAM running MS Windows XP SP2. The source codes were compiled using GCC 3.3.1. The runtime include both CPU time and I/O time.

We compared F-Miner with other high efficient al- gorithms: AFOPT and FP-Growth*. AFOPT and FP- Growth* are currently two very fast available algorithm  based on pattern growth approach and employed variants of the prefix tree. The source codes of AFOPT and FP- Growth* were downloaded from the web site of FIMI work- shop (http://fimi.cs.helsinki.fi/).

Table 2 shows some statistical information about the datasets used for performance study. All the datasets were downloaded from the web site of FIMI workshop. The fourth column is average transaction length. These statis- tics provide some rough description of the density of the datasets. BMS-WebView-1 and BMS-POS are two real world datasets from two e-commerce web sites. They are very sparse. Accidents and connect are dense datasets, acci- dents is a real traffic accidents dataset, and connect was a game dataset, prepared by Roberto Bayardo from the UCI datasets.

Table 2. Dataset for experimental  DataSets #Trans #Items AvgTL accidents 340,183 468 33.81 connect 67,557 130 43  BMS-POS 515,597 1,675 6.53 BMS-WebView-1 59,602 497 2.51  Figure 8 shows the performance comparisons among F- Miner, AFOPT and FP-Growth* on four datasets.

Figure 8 (a) shows the time of the three algorithms run- ning on dataset accidents. We see that F-Miner is the best algorithm for this dataset. When the minimum support goes down below 40%, the speed of F-Miner is much higher than AFOPT and FP-Growth*.

Figure 8 (b) shows the time for running the three algo- rithms on dataset connect. F-Miner is again the best. When the support is higher, the speed of three algorithms is simi- lar; otherwise the speed of F-Miner is fastest.

Figure 8 (c) gives the time for running the three algo- rithms on dataset BMS-POS. The minimum support is very low. AFOPT is the slowest algorithm for this dataset. The time of F-Miner is less than FP-Growth*.

Figure 8 (d) gives the time for running the three algo- rithms on dataset BMS-WebView-1. The minimum support is much lower. The time of these three algorithms all is few and close to a horizontal line. F-Miner is always the fastest.

From figure 8 we can see that the performance of F- Miner algorithm is much better than that of AFOPT and FP-Growth* that have been thought of as the effective al- gorithms at present no matter on dense data sets or sparse data sets.

6 Conclusions  This paper has proposed two new data structure named AFP-Tree and FP-Forest to represent the conditional  0-7695-2645-4/06 $20.00  ? 2006    Figure 8. Experiment result  databases. We have used the AFP-Tree and FP-Forest in our new algorithm named F-Miner for mining complete fre- quent itemsets. We have compared the performance of F- Miner against AFOPT and FP-Growth*. The results show that the performance of F-Miner algorithm generally out- performs others.

