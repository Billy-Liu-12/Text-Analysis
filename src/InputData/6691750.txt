Large-scale Restricted Boltzmann Machines on

Abstract?Recent works on deep belief network (DBNs) have shown that applying large-scale unsupervised feature learning model can dramatically improve the performance of the appli- cations in many fields. Training billions of parameters in these models such as restricted boltzmann machines (RBMs) appears to be computational challenging for modern CPUs. Graphical Processing Units (GPUs) has been employed in many large- scale deep learning models for performance enhancement due to its massively parallel computing capability. Unfortunately, the limited device memory of GPUs imposes a restriction on the size of the model trained on a single GPU. Multi-GPUs approaches, on the other hand, suffer from inefficient communication and economic cost. In this paper, we proposed a novel memory efficient algorithm on single GPU that can train large-scale RBMs without size restriction and preserve the performance gain of GPU parallel computation. Particularly, the experiments demonstrated that our approach used 75% less memory storage at the cost of only 10% performance loss in training large-scale RBMs with billions of parameters.

Keywords?GPU; RBM; deep learning; parallel; high perfor- mance computing

I. INTRODUCTION  Neural network topics regain the attentions of researchers due to the tremendous success of deep learning models in many machine learning applications [1]. For example, deep learning has great empirical results in image object recognition tasks [2]?[5]. The inputs for visual related applications are usually the image pixel which typically is of huge amount especially dealing with high resolution images. Moreover, many works [5]?[7] attempt to improve performance by introducing more parameters in the learning model. Large input and large model inevitable lead to computational challenges and researchers have proposed efficient models [6], [8] to address this problem.

Another alternative is to leverage the powerful computation architectures such as distributed system [9] and GPUs [6], [10]?[12]. This work focused on the GPU approach.

Training a DBN requires training multiple RBMs which is computational intensive for large models with millions and billions of parameters. GPU algorithms for efficient training of RBMs have been proposed [6], [13]. However, algorithms in [6], [13] didn?t intend to handle RBMs that can?t fit into the device memory of a single GPU. Other related works targeted on large-scale networks in a locally connected structure using multiple GPUs either in one server node [4] or across nodes of a cluster [14] in which inter-GPU communications must  Fig. 1: Restricted Boltzmann Machine  be seriously considered. Besides the limited device memory, another major bottle neck for GPU programming is the mem- ory transfer between host(CPU) and device(GPU) required by the heterogeneous programming model of GPU. Therefore, the inadequacy of the device memory and extra cost of memory transfers introduce significant overhead that limits GPUs to only modestly sized problems. In this work, we proposed a new memory efficient implementation of RBMs on one single GPU which is capable of training RBMs with extremely large number of parameters with much less memory usage on GPU?s device memory. Our implementation also exploits the concurrent kernel execution and operation overlapping to minimize the overhead of the memory manipulations and achieved satisfactory performance in experiments.

The remainder of this paper is organized as follows.

Section II provides background knowledge about RBM and contrastive divergence algorithm. Section III starts with the difficulties of implementation followed by the details of our approach addressing these difficulties. The experiment results are presented in Section IV. Finally, Section V summarizes our contribution in this work and gives the possible improvement in future.



II. BACKGROUNDS  A. Restricted oltzmann Machines  Restricted boltzmann machines (RBMs) are the key compo- nents of the deep belief networks (DBNs) which was proposed in [15] with a greedy learning algorithm. A RBM contains two sets of stochastic, binary units: visible and hidden units. Any unit in one set is connected to all units in the other set and       has no connection with units in the same set, as shown in Fig. 1 where m and n are number of visible and hidden units.

A weight wij is assigned to each connected unit pair (vi, hj) and every unit has a associated bias term ai and bi for vi and hj respectively.

Let v ? Rm be vector representation for vi?s, h ? Rn for hj?s, a ? Rm for ai?s, b ? Rn for bj?s and W ? Rm?n the matrix form of {wij}. The RBMs define the energy function of a given a joint configuration (v, h) as:  E(v, h) = ?aT v? bT h? aTWb and the corresponding joint distribution as:  P (v,h) = e?E(v,h)  Z  where Z is the normalization term:  Z = ?  v,h  e?E(v,h)  So the probability of a particular configuration of visible units is given by suming up the probabilities conditioning on all possible hidden unit configurations:  P (v) = ?  h  P (v, h)  Given a assignment of visible units v, hidden unit hj is randomly set to 1 (versus 0) with the conditional probability:  P (hj = 1 | v) = sigmoid(bj + ?  i  wijvi) (1)  Similarly, given h:  P (vi = 1 | h) = sigmoid(ai + ?  j  wijhj) (2)  To solve the max-likelihood optimization problem:  argmax W,a,b  ?  v?training data log P (v)  calculate the gradient of the log-likelihood:  ?log P (v)  ?wij = ?vihj?data ? ?vihj?model (3)  where ???data and ???model refer to the expectation under the training data distribution and the true model distribution respectively.

. ontrastive Divergence  It is expensive to calculate the ???model term by repeating alternating Gibbs sampling on hidden and visible units for a large number of iterations as in equation (1) and (2).

Contrastive Divergence (CD) [16] addressed this problem via two tricks:  ? The training data is fed to the RBM as the initial visible units since it is expected to be closer to the true distribution than random selected input.

? CD stops after only k iterations of Gibbs sampling (CD-k) instead of waiting for convergence.

With v(0) initialized with training data, we could rewrite eq.

(1) and (2) in a iterative way:  h(t) ? sigmoid(b(t) +WT v(t)) v(t+1) ? sigmoid(a(t) +Wh(t))  where h(t), v(t) a(t) and b(t) represent the value of v,h, a and b after the t iterations, the operator ? means performing a element-wise Gibbs sampling on the following matrix operand.

In this way, the new update rule for weights given by CD-k is:  ?wij = ?(?v(0)i h(0)j ? ? ?v(k)i h(k)j ?) Similarly for bias update:  ?ai = ?(?v(0)i ? ? ?v(k)i ?) ?bj = ?(?h(0)j ? ? ?h(k)j ?)  where ? is the learning rate.

Although larger k is expected to approximate the original gradient better, CD-1 (CD-k with k = 1) has been proven great success in practice such as [17] and we also implemented CD- 1 in our work. Furthermore we perform one model update over a small subset of training data (mini-batch) instead of every single training case. This enables the matrix operations in sig- moid function evaluation which improves the efficiency of the learning process. In batch processing mode, normalization is required that the final v(t) and h(t) are calculated by averaging all individual v(t)?s and h(t)?s of every single training case in current mini-batch. Let matrix V (t) ? Rm?l be the batch representation of v(t), H(t) ? Rn?l of h(t) where l is the size of the mini-batch, the CD-1 algorithm can be described as shown in Fig. 2. For simplicity, we use the same notation a and b to denote the matrices consisting of duplicate column vectors of a?s and b?s.

for all mini-batches in training data do V (0) ? current mini-batch ? Alternating sampling H(0) ? sigmoid(b+WTV (0)) V (1) ? sigmoid(a+WH(0)) H(1) ? sigmoid(b+WTV (1))  v(0) ? row-wise average(V (0)) ? Normalization h(0) ? row-wise average(H(0)) v(1) ? row-wise average(V (1)) h(1) ? row-wise average(H(1))  W ?W + ?(v(0)h(0)T ? v(1)h(1)T ) ? Model Update a? a+ ?(v(0) ? v(1)) b? b+ ?(h(0) ? h(1))  end for  Fig. 2: CD-1 Algorithm

III. GPU IMPLEMENTATION  Our training algorithm for large-scale RBMs was imple- mented on NVIDIA GPU. GPU was originally designed for graphic rendering with massive parallelism. In 2006, a parallel computing platform and programming model for NVIDIA GPUs named CUDA was introduced aiming to harness the     Fig. 3: H(t) = WTV (t)  Fig. 4: V (1) = ?  i WiH (0)  computing power of GPUs to general purpose computation [18]. CUDA also enables programmers without any knowledge about graphic APIs to write C/C++ code for high performance scientific computation by using NVIDIA GPUs. However, GPU implementation of large-scale RBM faces some difficul- ties:  ? Memory size. Large number of parameters doesn?t fit in limited device memory. For example, a considerable 4G memory can accommodate up to 1 billion floating- point parameters which is of modest size in deep learning problems.

? Memory transfer. GPU can only operate on data in device memory explicitly or implicitly. That means all input has to be copied to device memory before any computation happens and copied back to host after computation ends. Due to the limited connection speed of the PCI-E bus, memory copy between host and device is usually time consuming for large-scale problems and thus neutralizes the performance gain of GPU computing.

Our approach use the following techniques to address the two challenges above.

A. Memory Slicing with Minimal Transfer  With the fact that large size of mini-batch (l) hurts the overall efficiency of learning as stated in [19], we can assume l << m,n for typical large-scale RBMs. Therefore, the weight matrix Wm?n dominates the memory storage required in model training and it is also safe to assume device memory is capable to store all other data except W . So our strategy is to split W into slices Wi ? Rm?n? where n? << n such that every single slice can fit in the device memory and will be copied to GPU as needed for calculations of other  variables. Specifically, H(0), as well as H(1), is calculated according to block matrix multiplication rule that each block Hi ? sigmoid(b+WTi V ), as shown in Fig. 3 which ignores the sigmoid and sampling for the sake of simplicity. We could ap- ply the same trick for V (1) calculation by splitting W in slices W  ? i ? Rn?m  ? and transferring them one by one on demand.

However, we chose an alternative that avoids the undesirable memory transfer of the entire W for the second time. Our solution is based on the block matrix multiplication rule as  well that V (1) ? sigmoid(a + ?i WiH(0)i ). In other words, a partial result V  ? i = WiH  (0) i is calculated immediately after  H (0) i and all V  ? i ?s finally add up to V  (1), as shown in Fig. 4.

In this way, V (1) calculation is done along with H(0) using the Wi?s that are already loaded and the second transfer of W is no longer needed. Furthermore, reusing Wi to calculate V (1) eliminates the expensive synchronization of GPU threads since all threads have to wait for the full completion of H(0) if  V (1) is calculated by blocks as V (1) i ? sigmoid(a+WiH(0)).

Similarly, h(0) and h(1) calculation can also be performed  piece by piece with H (0) i and H  (1) i calculated. After h  (1) is updated, we can proceed to Wi update immediately as ?Wi = ?(v  (0) i h  (0)T i ?v(1)i h(1)Ti ) where v(t)i and h(t)i are corresponding  pieces of v(t) and h(t) required for updating the current Wi.

This saves us another transfer of W and finally leads to a two- scan fashion for our CUDA version of CD-1 which we named as cuCD-1. Detail of cuCD-1 is described in Fig 5.

1: for all mini-batches(V (0)?s) in training data do 2: Host-to-Device transfer of current mini-batch(V (0)) 3: v(0) ? row-wise average(V (0)) 4:  5: for all slices Wi of W do 6: Host-to-Device transfer of Wi 7: H  (0) i ? sigmoid(b+WTi V (0))  8: h (0) i ? row-wise average(H(0)i )  9: V ? i = WiH  (0) i  10: end for 11:  12: V (1) ? sigmoid(a+?i V ? i )  13: v(1) ? row-wise average(V (1)) 14:  15: for all slices Wi of W do 16: Host-to-Device transfer of Wi 17: H  (1) i ? sigmoid(b+WTi V (1))  18: h (1) i ? row-wise average(H(1)i )  19: Wi ?Wi + ?(v(0)i h(0)Ti ? v(1)i h(1)Ti ) 20: ai ? ai + ?(v(0)i ? v(1)i ) 21: bi ? bi + ?(h(0)i ? h(1)i ) 22: Device-to-Host transfer of updated Wi 23: end for 24:  25: Device-to-Host transfer of updated a and b 26: end for  Fig. 5: cuCD-1 Algorithm     Fig. 6: Stream Concurrency of GPU  . oncurrent ecution  CuCD-1 enables large-scale RBM training on GPU by memory slicing with a major overhead caused by memory transfers. For example, update of every Wi requires two host- to-device memory transfers for calculation (line 6 and 16 in Fig. 5) and one device-to-host transfer (line 22) for result return. GPU simply keeps idling when these memory transfers occur. To eliminate this inefficient idle state, recent GPUs of compute capability 2.x or higher allow concurrency of data transfers and kernel executions by using stream which is a sequence of operations that execute in issue-order on GPU. In general the concurrency is achieved by launching multiple streams and overlapping CUDA operations in dif- ferent streams. Fig. 6 from [20] gives a example of stream concurrency that every stream of the total 4 performs CUDA operations in a typical order: host-to-device memory copy (HD), two kernels (K) and device-to-host memory copy (DH).

Each row is the operation sequence in one stream and opera- tions in one column belongs to different streams that may run concurrently. In our algorithm, potential concurrency exits in the two for-loops. With multiple streams launched and each responsible for a subset of Wi?s, the concurrency may occur in the following cases:  ? Kernel execution in one stream may run concurrently with memory transfer operation in another. For ex-  ample, H (0) i calculation kernel (line 7) and Host-to-  Device transfer of Wj (line 6).

? Kernels in different streams may run concurrently as long as the computing resources on GPU allows. For  example, V ? i (line 9)and H  (0) j (line 7) calculation  kernels.

? Host-to-device and device-to-host memory copies in different streams can be performed concurrently. For example, Host-to-Device transfer of Wi (line 16) and Device-to-Host transfer of Wj (line 22).

However, this multi-stream approach costs an extra syn- chronization of streams because each stream calculates in- dividual V  ? i ?s and needs to synchronize with each other to  produce the summation ?  i V ? i for V  (1) calculation. It?s too expensive to implement a classic mutex using CUDA events [20] for exclusively update of V (1) by each stream. In our implementation, each stream first calculates a local sum of  all V ? i ?s produced by itself and finally all the local sums add  up to V (1). It improves the performance at the cost of extra memory space for local sum in O(s? l?m) bytes where s is the number of streams launched. We assume this is affordable because current GPUs only allows up to 16 concurrent kernels [21] and hence typically s ? 16.

TABLE I: Memory Storage Requirement of cuCD-1  Variable Wi?s a, b V (0,1) H(0,1) v(0,1) h(0,1) V  ? i ?s  Memory sn?m m + n 2lm 2ln 2m 2n slm  TABLE II: Performance of cuCD-1 with 1/8 Memory Usage  Model CPU cuCD-1(1/64) 1/32 1/16 1/8 1/4  Time 125381s 10675s 5416s 2941s 1724s 1125s  Speed-up 1 11.7 23.1 42.6 72.7 111.4

IV. EXPERIMENT AND DISCUSSION  We tested cuCD-1 on a NVIDIA Tesla C2075 with 6GB device memory. The matrix operation parts are performed using the NVIDIA CUDA Basic Linear Algebra Subroutines (cuBLAS) library [22]. We used random inputs since cuCD-1 mainly focus on memory efficiency and performance.

A. Memory fficiency  CuCD-1 removes the device memory size limitation and minimize memory transfer for efficiency. Table I lists all the variables stored in device memory and their sizes as well.

Typically, s, l are small constants that s, l << m,n and s, l < n?. So cuCD-1 works in the memory complexity of O(n?m). Since n? is adjustable depending on s and the total device memory size, cuCD-1 is capable of training RBM of extremely large size.

We first executed cuCD-1 in a memory efficient mode to train the testing RBM presented in [6] and compared the performance with the CPU implementation also in [6] which used a highly optimized multi-thread linear algebra package GotoBLAS [23]. Since [6] didn?t intend to address the large- scale problem, we only pick the largest testing case which is of the size 4096?11008. Although all the parameters of this RBM can fit in the device memory of our GPU, we forced different memory usage constraints (eg. keep up to 1/8 of the W in device memory) and apply the cuCD-1 for performance test. We vary the memory usage for storing W from 1/64 to 1/4 of the entire W with fixed number of streams s = 4.

Table II shows the training time of CPU implementation and cuCD-1 with different memory usage constraints denoted in the first row (eg. 1/4 means only up to 1/4 of the weight matrix W can be stored in device memory at any point of cuCD- 1 execution). Same configuration used in [6] about number of training examples (1 million) and mini-batch size (192) is applied for this experiment and rest ones of this section as well. The training time of the CPU implementation is taken from [6] which is tested on a Dual-core CPU @3.16GHz.

CuCD-1 begins to obtain remarkable speed-up (20?) over CPU implementation with only 1/32 memory usage in total and 1/128 for each stream. The more we loosen the memory constraint, the higher speed-up cuCD-1 achieves (eg. over 100 when storing 1/4 W in device memory).

. Performance Trade-off  CuCD-1 relies on memory slicing technique requiring a second transfer of W . In addition, stream synchronization and scheduling will introduce more overhead. All these new features of cuCD-1 may cause lower performance comparing     TABLE III: Performance of regCuCD-1 and cuCD-1 with 1/4 Memory Usage Constraint  # of parameters 32M 64M 128M 256M 512M 1B 2B  regCuCD-1 656s 1132s 2219s 4161s 8051s 15822s N/A  cuCD-1 2987s 3263s 4019s 5692s 9603s 17568s 47974s  Ppres 22% 34.7% 55.2% 73.1% 83.8% 90.1% N/A  to regular GPU implementations that assumes device memory can store everything and hence needs neither memory transfer nor streams. We implemented such a regular GPU CD-1 training algorithm named regCuCD-1 for comparison with cuCD-1 to quantify the performance decrease that cuCD-1 trades for memory efficiency. With fixed m = 32768, we compared performance of reCD-1 and cuCD-1 on RBMs of different sizes by varying n. We defined a new variable named performance preservation  Ppres = training time of regCuCD-1  training time of cuCD-1 ? 100%  to measure how close the performance of cuCD-1 is to regCuCD-1. We imposed a memory usage constraint of 1/4 and launched 4 streams. Table III shows the training time and the corresponding Ppres. As the number of parameter grows, the performance gap between cuCD-1 and regCuCD- 1 is diminishing, say only 10% difference in the 1-billion case. In other words, overhead stated above about cuCD-1 is being neutralized by performance gain of concurrency as the size of the model increases. Furthermore, regCuCD-1 is unable to train the 2-billion RBM because 8G memory space required (2 ? 109 floats ? 4 bytes/float) exceeds our GPU?s device memory capacity. On the other hand, cuCD-1 still works as expected and is supposed to handle RBMs with even more parameters.

. Parameter Tuning  The relations of the parameters in cuCD-1 can be expressed by:  s? slice size? n? sizeof(float) ? dev mem where dev mem is device memory available for store Wi?s and slice size is the number of columns of each Wi. We ex- plored different combinations of these parameters. Assuming a fixed amount of device memory, which is a common case given a particular GPU, an optimal combination of s and slice size is desirable. In this experiment, we assume the device memory limits us to load only a half of a weight matrix W 4096?36000.

Multiple configurations satisfying s ? slice size = 36002 are valid such as 3?6000, 4?4500 and 6?3000. We tried a set of configurations and draw a conclusion from the result in Fig. 7 that neither large slice with few streams nor small slice with numerous streams is a good choice. In practice, it is reasonable to try s starting from 2 until performance drops.

In the last experiment, we fixed size of Wi (slice size = 1024) and tested cuCD-1 on a RBM with W 4096?32768 varying the number of streams, as shown in Fig. 8. Two interesting things we found here: First, one stream performed much worse than multiple streams. The performance gap between single and two streams is larger than the memory usage difference is expected to cause. We regard this as the proof of concurrency  Fig. 7: Performance of different configurations of s and slice size given s ? slice size = 18000 on a RBM of 4096?36000  Fig. 8: Performance of different stream numbers with fixed slice size = 1024 on a RBM of 4096?32768  benefit because using one single stream forces everything synchronized with no operation overlapping. Second, training time kept dropping and reached the minimal at s = 7 which stores less than 1/4 of W (7?1024 < 327684 ) in device memory.

In other words, using more memory by launching additional streams after a critical point doesn?t help in performance but introduces more overhead such as stream synchronization and scheduling. Our explanation is that 7 concurrent streams gen- erate enough operation overlapping that can eliminate almost all the idling of GPU. In this case, the computing resource is almost fully occupied when s = 5 as shown in Fig. 8.

Same experiment on RBMs of other size present the similar performance pattern which suggest smart picks of s could achieve comparable performance but use much less memory usage such as s = 2 in this case.



V. CONCLUSION  We proposed a novel GPU algorithm cuCD-1 for large- scale RBM training on a single GPU. The major contribution of cuCD-1 is that it removes the memory limitation of a     single GPU for training large-scale RBMs and maintains a comparable high performance by leverage concurrency. Once the GPU is fully occupied with enough concurrent operations, multi-GPU may be necessary to maintain the scalability. Even in a multi-GPU framework, cuCD-1 is still valuable as it increases the number of parameters every single GPU handles and hence reduce the communication among GPUs. Integrating cuCD-1 into multi-GPU framework is exactly one possible direction of future work.

