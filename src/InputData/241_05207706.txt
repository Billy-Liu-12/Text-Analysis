A query optimization based on semantic user focus

Abstract?Based on the semantic query expansion, we propose a novel approach to re-rank the query result that enables users to retrieve the required documents quickly. Our technique is different from others in that (1) Ontology entries are added to the query by disambiguating word senses. (2) We propose a new algorithm to mine frequent patterns, which is used to construct semantic user focus. (3) It utilizes semantic user focus (SUF) to re-rank documents. Finally, the proposed approach experimentally demonstrates significantly improved retrieval precision.

Keywords-Data Mining; SUF; ontology; query expansion;

I.  INTRODUCTION A main task of an information retrieval (IR) is to find a set  of documents containing information needed by searchers in the given indexed database(s). At present, IR technology can be divided into three types: text retrieval, data retrieval and knowledge retrieval [1]. Among these, knowledge retrieval emphasis the match based on knowledge and semantic that can guarantee the precise ratio and recall ratio, and become research?s focus.

To increase the precise ratio, lots of researches endeavor in how apply data mining into query expansion. Lungswang et al [2] introduced a well-known association rule algorithm, Apriori algorithm, to solve a query expansion problem in TREC competition, but this algorithm is time consuming to the large document sets. Qin et al. [3] designed a heuristic model to help the user of search engines come up with an effective query set by finding the potentially useful relationships among terms, called term association rules, in text corpus. Wei et al. [4] proposed an automatic query expansion technique by mining association between terms.

The technique used for the discovery of the association is association rules mining which makes use of co-occurrence frequency as well as the confidence and direction of the association rules. But he only uses terms and did not utilize ontology.

Query expansion based on ontology technology is begun with Voorhees [5] in 1994, who utilities the concepts of ontology to expansion, and added the synonyms and subclasses in the initial query. Hence -forth, the study of query expansion based on ontology focus on two aspects: based on structure and based on annotation approach. Maki [6] approached query expansion and the similarity calculation based on the structure of ontology, which include the number, length and relationship of the path, categories of notes and so on. Navigli [7] proposed a query expansion based on  annotation. The technique assumed that the similar concepts have the similar meanings, and then calculated the similarity through the statistics of the common words or phrase in the annotations.  However, with the rise of layers, the number of extend terms and useless terms is increased rapidly.

This paper makes the following contributions: (1) The method utilizes ontology to disambiguate the word sense and to find the synonyms, which increases the recall ratio and precision ratio. (2)We use SUF to re-rank the query documents which help users to find required documents quickly. (3) According to the characters of the query documents, we propose a novel and simple algorithm, which finds frequent patterns by scanning the documents only once, and can incrementally, mines frequent patterns if new items are inserted.



II. THE QUERY EXPANSION SYSTEM In this section, we describe the query expansion system. In  Section 2.1, we present the procedures of the system workflow.

In Section 2.2, we discuss the ontology used in our method.

Finally, Section 2.3 we introduce how the phase3 works and the new algorithm that is used to mine frequent patterns.

A. The system workflow The procedures of the semantic query expansion system  workflow are illustrated in Fig.1. It consists of three major steps: (1) Expand initial query keyword with ontology. (2) Find SUF from query documents. (3) Apply SUF to re-rank the documents.

We use the Lemur IR system as a backend engine for query expand in that Lemur is robust and achieves high accuracy in terms of precision [8].Lemur is designed to facilitate research in language modeling and information retrieval. We assume that each retrieval document can be seen as a transaction while each separate word inside can also be seen as items, repress- ented by wordset, and then propose novel and simple algorithm  to mine frequent patterns.

The outline of the approach describe in Fig.1 is as follows: Phase1: Process user query and keyword-based retrieval: We remove stopwords and stem the terms from the user query, getting the query keywords. At the initial stage of the overall document retrieval process, due to the only information we require about the target answer set are query keywords, it is used to obtain the initial documents.

Phase 2: Expand query with ontology: In order to find the appropriate expanded concepts, we apply initial documents to calculate the relevance between original query and concepts in  2009 First International Workshop on Database Technology and Applications  DOI 10.1109/DBTA.2009.46  2009 First International Workshop on Database Technology and Applications  DOI 10.1109/DBTA.2009.46     the ontology base, and then choose the concept with the highest relevancy value to query expand.

Phase 3: Re-rank query documents: Applying a novel data mining algorithm, we find a set of frequent patterns, and generate SUF by merging them. According to the SUF, we re- rank query documents to reach semantic optimization.

Phase 4: Submit query to search engine    Figure 1.  .Procedures of the system workflow  B. Ontology WordNet is adopted as the ontology for query expansion.

WordNet is an online lexical reference system in which English nouns, verbs, adjectives and adverbs are organized into synonym sets, each representing one underlying lexical concept. Different relations link the synonym sets in WordNet [9]. We traverse WordNet to find out the best entries for the query keywords to be expanded.

A challenging problem with WordNet is that there are multiple sense of given a keyword. To tackle this problem, we introduce a Word-sense disambiguation technique, which is based on relevancy between QK and WordNet phrases. In WordNet, a group of synonyms with the same meaning compose a ?synset?.  The synsets are linked to each other through relationships such as hyponyms, hypernyms and holonyms. If no synset are found for the given phrase, we traverse down in the synset list to find the sysnet. For multiple synsets, we compare the query keyword Qi and concept C in the synset Si and by the relevancy function co_degree(C, Qi).

( ) ( ) ( )  co _ d eg ree(C ,Q ) lg ( ( , ) ( ) / lg ( ) ( , ) , ,  ( ) m in (1, lg / / 5)  i i  i i D  c  co C Q id f C n co C Q tf C d tf Q d  id f C N N  = =  = ?  Where n is the number of the top documents S in the initial retrieval documents D={d1,d2,?,dn}, and N is the number of the initial retrieval documents D. co(C, Qi) is the  co- occurrences  number between C and Qi in the documents S.

tf(x,di) is the occurrence number of x in the document di. Nc is the number of the documents which C appear in it. Finally, the concept with the highest relevancy value is chosen and synonyms Si= {S1, S2?Sl} from the synset are added for query expansion.

C. Re-rank the query document set Due to the huge documents acquired by the phase 2, we introduce SUF to re-rank the query document set, which can achieve semantic optimization and help users to find the required documents quickly,  The specific  procedures of phase3 workflow is illustrated in Fig.2, which contains five steps, and we discusses the steps as following .

Figure 2.  .Procedures of the phase3 workflow  On the retrieved document set, we parse each document into sentences and apply natural language processing techniques to select terms and phrases. Four rules are used to select the terms: (1) It cannot be a proper name. (2) It cannot begin or end with a stop word. (3) It is limited to a certain maximum length. (4) It is not the expand words.

After preprocessing document set, we acquire important terms in each document, which is used to construct a mapping table. Firstly, we scan the terms in each document to acquire items  to construct the mapping table, where the left columns stores the identity of the transaction and the right column stores the corresponding items. Secondly, the feature sets is calculated for each items, which is Term Frequency*Inverse    Preprocess document set  Acquire frequent models  Acquire frequent items, construct a mapping table  Construct SUF  Re-rank documents set  A set of retrieve documents  Expand query with ontology WordNet  Apply semantic user focus to re- rank document  Submit query to search engine  A set of retrieval documents  The user-provided initial query  Process initial query and keyword-based retrieval  A set of initial retrieval documents     Document Frequency (TF*IDF). Thirdly, the frequent items I are extracted based on TF*IDF [10].

According to the query document set?s character, we proposes a novel and simple algorithm, which finds frequent patterns by scanning the documents set only once, and can incrementally mine frequent pattern if new items are inserted.

Definition 1. Assume that there is a document set ? consisting of a set of documents, and that there are n frequent items I in ?, I is represented as the set {I1,I2,?,In}.

Definition 2.  Assume that there is a document set ? with I={I1,I1, ? ,In}, 21 2P = { , , , }i i i i nP P P  is defined as the pattern that connect Si in the synonyms Si={S1,S2,?Sl} and the other  frequent items.

We provide details of algorithm which is used to acquire frequent patterns, as shown in Table 1. The approach utility the expand synonyms S={S1,S2,?Sl} and the frequent items I={I1,I2,?,In}  to construct patterns 21 2P = { , , , }i i i i nP P P .After scanning all the patterns, it filters the frequent patterns according to the value of  ?and then reports them.

Subroutine1: AcquirePattern (S, I) Input: =Expand synonyms S, Frequent items I Output: = Pattern 21 2P = { , , , }i i i inP P P R1:  For i=1 to S // S  is the number of elements in set S R2:  let P[i] [0] =Si; R3:  End for: R7:  let t=1, k=0 R5:  For i =1 to S R6:      For j = 1 to ( 1 ) / 2I I + R7:       If  k?0, then  P[i][j]=I(t)+P[i][k], j++, k--; R8:       else k=j-1, t++; //add pattern into pattern set R9:        End for R10:  i++; R11:  End for Subroutine2: AcquireFrequencePattern (P, ?) Input:= Pattern P and ? Output: =Frequent pattern set PF, Frequency F  R1: For i=1 to S   R2:    For j = 1 to ( 1 ) / 2I I +  R3:      For k=1 to [ ][ ]P i j // [ ][ ]P i j  is the length of the pattern P[i][j] R4:   let t: =MinFrequence (p[i][j]) //MinFrequence (p[i][j])  is the minimal frequency of  the items which is in the p[i][j] R5:      End for R6:     If t??1 then FP[i][k]=P[i][j], t[i][k]=t, k++?//check whether the frequency of P[i][j] reaches the boundary condition?1, add the frequent patterns into frequent pattern set and add the frequency into the frequent set R7:    End for R8   i++? R9:  End for  TABLE I.  THE PSEUDO CODE OF THE PROPOSED ALGORITHM  To evaluate the performance of the proposed algorithm, we compare it with Apriori and FP-growth algorithms, which respectively are the most famous ones in the candidate generate-and-test and pattern-growth approaches. We find that the storage complexity of the proposed ones is comparable to the FP-growth algorithm. But the time complexity of the proposed approach is better than that of FP-growth and Apriori algorithms. Because we need to process large document set and the classic approaches? speed is slow for it, so the proposed ones is useful.

Owing to the redundancy from short frequent patterns, we unite the frequent patterns to construct semantic user focus (SUF). If N/M ??2, we can untie them into one pattern, where N is the number of duplicate concepts, M is the major number of two concepts, and and?2 is the boundary condition.

The document set is ranked by the match number between SUF and document keywords, and we give priority to the document that has a larger match number.



III. EXPERIMENT In the following experiment, we analyze the number of  SUF and then measure the effectiveness of the proposed approach, in which ?Chelsea? is used as query keyword.

One hundred retrieval documents are chose as the document set to mine frequent patterns, and then find 20 frequent patterns. In the process of the construction of SUF, we analyze the relationship between the average precision and the number of SUF. Fig3. shows that the best optimized result is got when the number is two, and the effectiveness is declined along with the increase of the number, that is because redundant SUF will make the semantic description disperse.

0%  10%  20%  30%  40%  50%  60%  70%  1 2 3 4 5  Semantic user focuses  a ve ra ge  pr ec i si on   Figure 3.  the influence of SUF to average precision  When there are two semantic user focuses, we calculate the result of the semantic optimization. It acquires the 5.4% improvement on average as the table 2, which can meet user?s requirement fully. Because users are interested in the previous pages generally, and this approach can adjust the high related query result to the front, so it enable the user to find the required document quickly and is of great value.

precision document  Semantic query  retrieval optimization  10 82.60% 85.7 %( +3.1%) 20 71.30% 77 %( +5.7%) 30 58.40% 62.7 %( +4.3%)     40 47.10% 54.5 %( +7.4%) 50 36.40% 42.9 %( +6.5%)  average 61.84% 67.24 %( +5.4%)  TABLE II.  THE RESULT OF THE QUERY OPTIMIZATION

IV. CONCLUSION We proposed a novel effective technique to query  documents, which different from other query expansion techniques in the following aspects. First, a retrieval documents are re-ranked through the semantic user focus.

Second, we propose the relevance -based word sense disambiguation technique that is able to find the target concept, and find additional useful documents through the related synonyms. Third, a novel and simple algorithm is used to find a frequent pattern, which is better than Apriori and FP-growth algorithms in the time complexity. In the end, we conducted a series of experiments to calculate the optimum number of the semantic user focuses, and find our technique averagely acquire 5.4% improvement on the retrieval performance.

As future studies, we will apply frequent patterns to extract entity relations, which can be used to query expansion.

