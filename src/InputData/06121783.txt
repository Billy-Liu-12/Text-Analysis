Frequent Absence and Presence Itemset   for Negative Association Rule Mining

Abstract ? Negative association rule (NAR) mining has  created more attention recently due to the knowledge and discovery of the interestingness of the pattern of the negative association rules and the challenges during the mining process.

Pattern from negative association rules are considered to be unique and unexpected compared to positive rules. Negative association rules are useful in analysis for decision making in identifying the items which conflict with each other or the items that complement each other. However, negative association rules mining still have their own issues such as mining space and good quality of negative association rules.  In this paper, we provide the preliminaries of basic concepts of negative association rule. We proposed an enhancement in Apriori algorithm for mining negative association rule from frequent absence and presence (FAP) itemset.  Prominent literature will be discussed to further understand negative association rule mining.

Keywords; negative association rule (NAR), Apriori, frequent  absence and presence (FAP) itemset.



I.  INTRODUCTION Data mining is the process to discover the hidden  knowledge from huge data and therefore accurate decision and action can be made or taken. Various tasks can be implemented in data mining such as association rules, clustering, classification, prediction, and regression.

Association rule task is interested in finding the relationship between items in the dataset. Instead of predicting an unknown value, we want to uncover interesting facts about the relationships between the known values. In association rules, these patterns take the form of rules show the co- occurrence of attributes.

Agrawal [1] introduced the concept of association rule mining, which is then attracted many researchers not only in machine learning but also other domains like medical, financial and share market. It can be explained in the form of rule A B. Items in the left-hand-side of the rule are named as antecedent while items in the right-hand side are named as consequent [2]. Many algorithms have been developed to implement association rule mining such as Apriori, FP Tree, TreeProjection and PRICES.  Apriori is the most common algorithm in mining association rule and have been through much advancement in both the algorithm and measures.

In the beginning, association rule mining only explores positive relationship. Typical association rules are positive  relationships that imply purchase of one item or itemset with the purchase of another item or itemset. In 1997, Brin et al.

[6] discovered the correlation between antecedent and consequent in the association rules. The correlation can be positive, negative or independent. Negative association rules (NAR) indicate that there is a negative relationship between items in the association rules. In other words, negative relationship implies the presence of items by the absence of other items in the same transactions. For example, ?customers that buy Coffee do not buy Tea?. We called rules of the form A ?C as NAR which contains a negation of an item. Since then, NAR mining attracted serious attention from researchers.



II. MOTIVATION Positive association rules (PAR) cannot be denied as an  important factor for decision making but it does not reject the fact that NAR also plays significant roles in decision making.

Something that is of the opposite of the normal pattern or knowledge is interesting compared to strong positive rules which are predictable and common [10]. Therefore, NAR is beneficial in detecting abnormal knowledge like fraud and intrusion. Furthermore, NAR is able to provide more complete knowledge together with the knowledge from PAR in ensuring better analysis and decision. Analysis process in decision making involves two factors which are positive factor and negative factor. To make better analysis for decision making both factors need to be scrutinized.

Therefore, the analysis will be more comprehensive and complete. In several domains like medical, NAR can reduce the time for diagnosis process.  For example, when a patient who complains of headache does not have a throbbing pain, migraine should not be suspected with a high probability [18]. NAR is useful in identifying items which conflict with each other or items that complement each other.

Unfortunately, NAR mining is not an easy task. It leads to explosive amounts of rules that will be discovered and when added to it, the majority of them are uninteresting.

Furthermore the ratio of the average number of possible items and the number of possible items with negation is huge. The total number of generated positive and negative association rules is 4(3m-2m+1+1) of which 3m-2m+1+1, roughly three quarters are negative association rules [7].

Therefore efficient algorithm and measures are crucial in     discovering NAR in order to keep the number of discovered rules low and to improve the quality of the mined rules.



III. CONCEPT AND DEFINITION Let I = {i1, i2,...,im} be a set of n distinct items. Let D be a set of transaction D = {t1, t2, t3, ? tn}, tj? I, where T represents the transaction for a set of items. Let A = {i1, i2, i3, ? ik}, A? I, called an itemsets, a set of items in I.

Association rule is in the form  A B whereas A? I, B? I and A  B = ?.  Every rule generated has their own measures, support (supp) and confidence (conf). The calculation of support is the frequency of transaction in D containing A and B. It is also known as probability, P(AUB).

The value of confidence is the percentage of transaction in D containing A and B and also known as conditional probability, P(B|A). Each positive association rules A B can have three forms of negative association rules 1) A ?B; 2) ?A B; 3) ?A ?B. By extending the definition in support-confidence framework, NAR discovery seeks rules from previous example, A B, by calculating the values of support and confidence as below:  Supp(?A) =1 ? Supp (A)    (1) Conf(?A) = 1 ? Conf(A)    (2) Supp(A ?B)= Supp(A) Supp (A B)  (3) Conf(A ?B)=1 Conf (A B)   (4) Supp(?A B)= Supp(B) Supp (B A)  (5) Conf(?A B)=(P (B) (1  Conf (B A)))/1  P(A) (6)  As in [3], the definition of generalized NAR is a rule that  contains a negation of an item whether its antecedent or its consequent that can be formed by a conjunction of presence or absence of terms. An example for such rule would be as follows: A^?B^?C^D E^?F. From the literature, there is no algorithm that can produce such type of rule, due to the difficulty of developing such algorithm. It is well known that the generation of candidate in the association rule mining process is an expensive one. It would be necessary not only to consider all items in a transaction, but also all possible items absent from the transaction. There could be a considerable exponential growth in the candidate generation phase. This is especially true in datasets with highly correlated attributes. Therefore [3] restricted their NAR to confined NAR, where the entire antecedent or consequent must be a conjunction of negated attributes or a conjunction of non-negated attributes.

However, in this paper we will investigate this problem and build an algorithm in order to generate a generalized NAR. Furthermore, the calculations of support and confidence in (1) until (6) are not applicable anymore in this type of rule. Each item in itemset needs to be scanned to the database to obtain the frequency of absence and presence of the support value.



IV. RELATED WORK NAR mining has been addressed in a number of  publications as reflected in Table I. The most common  algorithm for implementing NAR mining is Apriori but there are several researchers who adopted other methods such as Fuzzy [16] and Rough Set [13]. There are two phases in Apriori, the candidate generation phase and rule generation phase and also known as the support-confidence framework.

This framework uses threshold values to filter the weak and uninteresting rules. Threshold value for candidate generation phase is the minimum support whereas rule generation phase is the minimum confidence. In candidate generation phase, itemsets which are greater than the minimum support are called frequent itemsets while itemsets less than the minimum support are labeled as infrequent itemsets. For NAR mining, the candidates can be generated from infrequent [19] or frequent [3] itemsets.

TABLE I. SUMMARIES OF NAR PUBLICATIONS.

Author Algorithm Threshold/ Measurements  Application/ Dataset  Yuan et al.

[20]  - Support, confidence, salience measure and locality of similarity  TV cable  Wu et al. [19] MBP Support, confidence, interest, CPIR  Synthetic  Antonie and Zaiane [3]  - Support, confidence, correlation coefficient, interest  Reuters text categorisation  Cornelis et al.

[7]  PNAR Support, confidence and partition of itemset  Synthetic (market- basket)  Koh and Pears [12]  MINR Fisher exact test, chance 7 UCI data  Zhu and Xu [23]  PNAR Support, confidence, correlation, different support, and different confidence  Synthetic  Jiang et al.

[11]  WPNMS Multiple weighted support, weighted confidence and correlation  Man made  Ma et al. [13] RGCA Classification accuracy ratio and coverage ratio  UCI (Lens, Iris, Balloons and Wine)  Ben-Gang and Li [5]  Free-PNP Node count Mushroom  Bala [4] - Support, confidence and lift  Retail  Zhao et al.

[22]  WNRIF Weighted support, weighted confidence and correlation  Man made  Teng et al.

[17]  SRM Support, confidence, chi- square test correlation coefficient, and violation ratio  Synthetic  Hussain et al.

[10]  - Support, confidence, Relative measure  Mushroom  Taniar et al [16]  - Support, confidence, Exceptional interval  UCI data (Intrusion Detection)   NAR is the hidden patterns among itemsets patterns that  have infrequent but high-correlation [19]. Negative patterns are important in applications that tell us which of the items rarely occur together whereas in [3] interested in correlation between items, if correlation is negative, NAR are generated     when their confidence is high enough. However, if the correlation is positive, a positive association rule is generated. If the support is not adequate, a negative association rule is generated when its confidence and support are high. However, if the frequent itemset is an option, very minimum support is needed. As a result the searching space will be exponential. Several researches employed additional measures to mine the interesting negative association rules as listed in Table 1. Additional measures are essential in pruning strategy because the searching space for mining association rules is greater especially in NAR mining. There were also few researchers who hybrid other methods with Apriori such as taxonomy [20] and entropy [10] for better results.

Majority of researchers mining positive and negative association rules together because the mining procedure is similar [6,8,9,10,11,19]. Furthermore the comparison can be made between positive association rule and negative association rule. The knowledge from negative association rule is needed to complement knowledge from positive association rule [4,17,20]. The dataset that has been used varied but most researchers used man made data or synthetic data and only three researchers from Table I utilized UCI datasets. Selection of the dataset is important in order to obtain better results and analysis. Market-basket dataset is the most suitable type of data to represent NAR. Dataset with high correlation and complexity surely affects the mining process and its results.



V. FREQUENT ABSENCE AND PRESENCE ITEMSET As we discussed in the previous section, there are two  types of itemsets in association rule mining; infrequent itemset and frequent itemset. The calculation of support for itemset is based on the presence of the particular itemset in the transaction. However in this paper, we proposed the calculation of support from the absence and also the presence of item or itemset. Therefore, the frequent absence and presence itemset (FAP) will be generated at the end of the candidate generation phase. As a result, optimum number of strong and interesting NAR will be discovered during the rule generation phase.

A. Our Algorithm We now present the algorithm that was implemented. We  look at how the frequent itemset is generated in Apriori as denoted in the algorithm below. This algorithm will mine both positive and negative association rules. Only candidate itemsets including absence itemsets which calculated support above the minimum support value will be extended.

Algorithm: Frequent Itemset (including absence items) Input: D:Transactional Database, ms:minimum support Output: l:1-frequent itemsets Method: (1)   l??, f?? (2)   scan the database and find the set of items (f) (3)   for all items f in F do (4)      for all transaction t ? D do (5)      if t contains f then  (6)          countP++; (7)      else (8)          countN++; (9)   if countP ms or countN ms (10)    l1?l1 {f} (11) return l1  The generated 1-frequent itemsets will be used in the candidate generation phase as in the algorithm below.

Algorithm: Candidate generation (including absence items) Input: D:Transactional Database, ms:minimum support Output: l:k-frequent itemsets Method: (1) (2)   for (k = 2; lk-1 ?; k++) do (3)      Ck= lk-1 l1 (4)      foreach i ? Ck (5)        for each item in i (6)           if item is negative item then (7)              if item not contains in t then (6)                 count++; (7)           else (8)             count++; (9)        if count ms (10)         lk?lk {i} (11) return lk   In the rule generation, we adopted similar steps from the traditional Apriori. All the rules which are greater than the minimum confidence will be extracted. The rules will consist of positive and negative association rules. In negative association rule, we excluded the rules in the form ?A ?B.

The number of the form ?A ?B is very large, and these rules are pure negative itemsets and are usually of little use in real application [23]. For example, we assume that the database DB in a supermarket contains n transactions. Now we are concern of the sale of tea (t) and coffee (c). Suppose we mine the rule of the form ?t  ?c, which means customers not to buy tea and coffee in a transaction, the result is not useful to our market basket analysis so we adopt a pruning strategy that will not consider the part negative association rules of the form ?A ?B to improve mining efficiency. Importantly, the search space can be significantly reduced by this pruning strategy.

B. An Example For better comprehension of our algorithm, we conducted  a test with a small dataset as an example to illustrate the mining process. Table II consists of transactional dataset whereas Table III is a frequent itemset that been discovered.

Lk denoted all the frequent k-itemset. Table III lists all the generated frequent itemsets including positive and negative with the minimum support = 0.4. The candidate generation discontinued at L3 because there were no more frequent itemsets which are greater than the minimum support. Later, based on the frequent itemsets L3, association rules were generated including positive rules and negative rules with the     given minimum confidence = 0.5 as in the right-most column from Table IV. If we only consider the positive frequent itemset, item E will not be considered for NAR mining. As the result, the generated rules will miss the strongest items in NAR. Therefore, we are far from the aim of NAR mining, where we should focus on the absence of the strongest items or itemsets.

TABLE II. TRANSACTION DATA  TID Items 1 A,B,D 2 A,B,C,D 3 B,D 4 B,C,D,E 5 A,C,E 6 B,D,F 7 A,E,F 8 C,F 9 B,C,F  10 A,B,C,D,F   TABLE III. FREQUENT ITEMSET  L1 Supp A 5 ?A 5 B 7 C 6 ?C 4 D 6 ?D 4 ?E 7 F 5 ?F 5    L2 Supp ?A^B  4 ?A^?E 4 B^C 4 B^D 6 B^?E 6 B^?F 4 C^?E 4 D^?E 5 D^?F 4 ?E^F 4    L3 Supp B^D^?E 5 B^D^?F 4     This example demonstrates that the number of frequent  absence itemsets is dominant compared to frequent presence itemsets. There are only two frequent positive itemsets in L2 whereas there are eight frequent negative itemsets. When these frequent itemsets went through the rules generation phase with the minimum confidence = 0.8, 16 rules were generated, 14 of them are NAR and the balance are PAR.

Furthermore, the generated NAR are strong and believed to be interesting as in Table IV.  Those rules are important and should be considered in the analysis process to ensure better decision. Unfortunately, the mining space was increased tremendously because of the frequent absence itemsets, if we look at L2, there are 10 frequent absence itemsets and only 2 frequent presence itemsets. Even though the mining space is greater when we consider negative frequent item but the knowledge hidden is far more crucial when it comes to critical data. Therefore pruning strategy is serving an important role in mining negative association rules and the main focus for researchers.

We used similar examples in Table II to compare rules from the only frequent positive itemset, and FAP itemset (our algorithm) as listed in Table IV. The table denotes that our algorithm has successfully generated additional six rules (as bold in the table) in which several of them are higher in confidence and support values (in the curly brackets) compared to the rules from frequent positive itemset. For better evaluation, we also generated rules from infrequent  itemset but the number of rules is huge, 53. Due to the limited space, the list of rules is not displayed in this paper.

As a result, the FAP itemset is proven to have successfully generated optimum number of interesting NAR compared to the common frequent itemset or infrequent itemset.

TABLE IV. ASSOCIATION RULES  Frequent +ve Frequent +ve & -ve D->B {100, 6} B^?F->D {100,5} D^?F->B {100,4} B->D {85,6} ?A->B {80,4} ?F->B^D {80,4} ?F->D {80,4} ?F->B^D {80,4}  D->B {100, 6} B^?F->D {100,5} D^?F->B {100,4} B->D {85,6} ?A->B {80,4} ?F->B^D {80,4} ?F->D {80,4} ?F->B^D {80,4} D^?E->B {100,5} B->?E {85,6} ?E->B {85,6} D->?E {83,5} D->B^?E {83,5} B^D->?E{83,5} ?F->B {80,4} F->?E {80,4}  C. Measures of Interestingness The literature has shown that the interestingness of  measure is an important area in the association of rule mining. Hence, the support and confidence are insufficient for mining interesting rules. The advancement in measures has taken much consideration from researchers. Many additional measures such as the interest, correlation, chi- square, Piatetsky Shapiro, odd ratios, and collective strength were introduced to generate better association rules [8]. The objective of these measures is to eliminate misleading rules and focus on the interesting rules by setting a certain value.

Thus, they made their decision according to the rules more quickly. In this paper we will use the selective measures in our experiment to prove the quality of the rules generated.

Those measures are succeeding to achieve theirs? objectives and value added to the traditional measures which are support and confidence.

1) Interest (I) [6]: To find the dependence of items, in order to determine the cause of correlation. The further the value is from 1, the more the dependence. Note that the dependence applies to a single cell of a contingency table, while correlation applies to the entire table. Interest values above 1 indicate positive dependence, while those below 1 indicate negative dependence. Interest values = 1 indicate that the two sets are independent. The interest measure is preferable as it directly captures correlation, as opposed to confidence which considers directional implication (and treats the absence and presence of attributes non-uniformly).

Interest(I)=   (7)   2) Collective Strength (CS) [2]: The collective strength of an itemset is defined to be a number between 0 to . A value of 0 indicates perfect negative correlation, while a value of  indicates perfectly positive correlation. A value     of 1 indicates the break-even point , corresponding to an itemset present at the expected value. An itemset is said to be in violation of a transaction, if some of the items are present in the transaction, and others are not. Thus, the concept of violation denotes how many times a customer may buy at least some of the items in the itemset, but may not buy the rest of the items.

CS=  x  (8)   3) Piatetsky Shapiro (PS) [8]: This measure has three principles that should be obeyed by good measure (F):  a) F = 0 if A and B are statistically independent, that is, P(AB) = P(A)P(B).

b) F monotonically increases with P(AB) when P(A) and P(B) remain the same.

c)  F monotonically decreases with P(A) (or P(B)) when P(AB) and P(B) (or P(A)) remain the same.

PS = P(AB)  P(A)P(B)  (9)   From the example in Table II, rules were generated as  displayed in Table V together with their interestingness measures. The measures shows that the NAR is as interesting as PAR. Even though PAR which is D B and B D is still have the highest values in most of the measures but in some rules, for example C B and B C are less interesting compared to NAR, D B^?E, ?A B, F ?E and ?F B^D in terms of the confidence, support, and interest.

TABLE V. RULES WITH MEASURES  Rules Conf Supp Interest Collective Strength  Piatetsky Shapiro  D->B 100 6 1.4286 1.2778 0.1800 D^?E->B 100 5 1.4286 1.4390 0.2060 B^?F->D 100 4 1.6667 1.4167 0.1900 D^?F->B 100 4 1.4286 1.1351 0.1900  B->D 85 6 1.4286 1.2778 0.1800 B->?E 85 6 1.2245 2.8966 0.1100 ?E->B 85 6 1.2245 2.8966 0.1100 D->?E 83 5 1.1905 1.9877 0.0800  D->B^?E 83 5 1.3889 1.2727 0.2060 B^D->?E 83 5 1.1905 1.3810 0.2060 B^?E->D 83 5 1.3889 1.2727 0.2060  ?A->B 80 4 1.1429 1.5000 0.0500 F->?E 80 4 1.1429 1.5000 0.0500 ?F->B 80 4 1.1429 1.5000 0.0500 ?F->D 80 4 1.3333 2.3333 0.1000  ?F->B^D 80 4 1.3333 1.5556 0.1900

VI. EXPERIMENTAL RESULTS For evaluation purpose, we have conducted several  experiments on six different datasets from the UCI Machine Learning Repository. Table VI outlines the results from our implementation of FAP itemsets compared to the frequent positive itemsets and infrequent itemsets. Each row of the table shows the results from the dataset named in the left- most column. For this experiment, the minimum support is 0.5 and minimum confidence is 0.9.

FAP itemsets effectively generate optimum number of rules including NAR compared to others. The number of rules has drastic reduction than the infrequent itemsets but more than the frequent positive itemsets. Number of the FIS for FAP itemsets is parallel with the number of rules as projected in Figure 1. The search space can be significantly reduced if the extracted FIS is decreased. The number of FAP also highly depends on the dimensionality of particular dataset. For example in the Glass dataset, even though it has less number of attributes compared to the Hepatitis and Heart Diseases dataset but it has more categorical values for each attribute. Therefore the number of FAP and rules for Glass dataset is the highest followed by the Heart Diseases and Hepatitis dataset.  By setting the NAR mining from infrequent itemsets we may be flooded with many trivial rules. We would need wade through the rules to find those that may be of our interest. However by setting the mining from only frequent positive itemset we may miss out other useful rules. Therefore mining from FAP itemsets is estimated to discover the interesting NAR.

TABLE VI. EXPERIMENT WITH UCI DATASET  Dataset Infrequent Positive Frequent Positive FAP Rules FIS Time Rules FIS Time Rules FIS Time  Lenses 1370 2811 17 2 4 9 10 16 10 Monks 2043 26431 2174  - - - 12 42 15 Iris 3041 2440 41 - 1 7 338 106 13 Hayes Roth 9046 16989 1081  - - - 211 155 9 Heart Disease  21728 66002 12552 - 3 6 213 199 18  Hepatitis 13763 25564 2367 229 202 11 2792 1848 158 Glass 173803 170563 107638 123 4 14 7580 2288 243   We also evaluated the interestingness of rules generated  with the selected measures from previous section by using Lenses dataset as an example. In order to achieve better understanding, we divided the rules according to their length and type of rules (NAR and PAR) as in Figure 2. Rules with the length of two items are likely to be more interesting because they have higher values in confidence, support and Piatetsky Shapiro whereas three length itemsets has better results in interest and collective strength compared to others.

Most importantly, NAR (dash line) is dominant in all measures compared to PAR (continuous line).



VII. CONCLUSION Challenges in mining negative association rules are still  there to motivate new researchers, which are the exponential mining space and the usefulness of negative association rules. The works of this paper proposed some modification in Apriori algorithm to generate NAR based on frequent absence and presence itemset. Relevant interestingness measures were adopted to prove that the generated rules are also interesting and strong. For future work, application with critical domains such as network and web will be interesting and beneficial.

