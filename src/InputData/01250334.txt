New Parallel Algorithms for Frequent Itemset Mining in Very Large Databases

Abstract  Frequent itemset mining is a classic problem in data mining. It is a non-supervised process which concerns in finding frequent patterns (or itemsets) hidden in large vol- umes of data in order to produce compact summaries or models of the database. These models are typically used to generate association rules, but recently they have also been used in far reaching domains like e-commerce and bio-informatics. Because databases are increasing in terms of both dimension (number of attributes) and size (num- ber of records), one of the main issues in a frequent item- set mining algorithm is the ability to analyze very large databases. Sequential algorithms do not have this ability, especially in terms of run-time performance, for such very large databases. Therefore, we must rely on high perfor- mance parallel and distributed computing. We present new parallel algorithms for frequent itemset mining. Their effi- ciency is proven through a series of experiments on different parallel environments, that range from shared-memory mul- tiprocessors machines to a set of SMP clusters connected to- gether through a high speed network. We also briefly discuss an application of our algorithms to the analysis of large databases collected by a Brazilian web portal.

1. Introduction  Our ability to collect and store data is fair outpacing our ability to analyze it. This phenomenon is mainly ex- plained by the gap between advances in computing power and storage capacity technologies. Moore?s law states that computing power doubles approximately every 18 months, while the corresponding law for advances in storage capac- ity technology is even more impressive ? storage capac- ity doubles approximately every 9 months [5]. As a conse- quence, the value of data is no longer in ?how much of it you have? or neither in ?how fast can you gather it?, but in how quickly and how effectively can the data be reduced and explored. Thus, implementation of non-trivial data min-  ing ideas in high performance parallel and distributed com- puting environments is becoming crucial. Clearly, such par- allel computing environments greatly increase our ability to analyze data, since the computing power is increased.

However, these environments also pose several challenges to data mining, such as high communication and synchro- nization overhead, data skewness, and workload balancing.

In this paper we present new parallel algorithms for a key data mining task: the discovery of frequent itemsets.

This task has a simple problem statement: to find the set of all subsets of items (or attributes) that frequently occur to- gether in database transactions (or records). Although the frequent itemset mining task has a simple statement, it is both CPU and I/O intensive, mostly because of the high di- mension and large size of the databases involved in the pro- cess. Several efficient sequential algorithms were proposed in the literature [2, 12, 9]. There are also some parallel algo- rithms [1, 7, 6, 13, 10]. However, these algorithms need sev- eral rounds of communication, incurring in serious synchro- nization and I/O overhead. Our new algorithms need only one round of communication while searching for the fre- quent itemsets and one reduction operation to generate the correct model. Further, they can make use of both shared- and distributed-memory advantages and deal with both high dimension and large size problems. We evaluated our algo- rithms through a broad series of experiments using different parallel environments.

2. Definitions and Preliminaries  DEFINITION 1. ?ITEMSETS? For any set ? , its size is the number of elements in ? . Let ? denote the set of ? natu- ral numbers ?1, 2, ? ? ?, ??. Each ? ? ? is called an item. A non-empty subset of ? is called an itemset. The power set of ?, denoted by ????, is the set of all possible subsets of ?. An itemset of size ?, ? = ???, ??, ? ? ?, ??? is called a ?-itemset (for convenience we drop set notation and denote ? as ???? ? ? ? ??). For ? , ? ? ???? we say that ? con- tains ? if ? ? ? . A set (of itemsets) 	 ? ???? is called an itemset collection, and 	 is a ??????? collection if no  Proceedings of the 15th Symposium on Computer Architecture and High Performance Computing (SBAC-PAD?03)     itemset in it contains another: ? , ? ? ?, and ? ?? ? im- plies ? ? ? .

DEFINITION 2. ?TRANSACTIONS? A transaction ? ? is an itemset, where ? is a natural number called the transaction identifier or ???. A transaction database ? = ?? ?, ? ?, ? ? ?, ? ??, is a finite set of transactions, with size 	?	 = ?.

The absolute support of an itemset ? in ? is the number of transactions in ? that contain ? , given as ??? ??? = ?? ? ? ? 	? ? ??	. The (relative) support of an item- set ? in ? is the fraction of transactions in ? that contain ? , given as, ??? ??? = ??? ?????? .

DEFINITION 3. ?FREQUENT AND MAXIMAL FREQUENT ITEMSETS? An itemset ? is frequent if ??? ???? ????, where ???? is a user-specified minimum-support threshold, with 0 ? ???? ? 1. A collection of frequent itemsets is de- noted as ????????. A frequent itemset ? ? ???????? is ? ??	? if it has no frequent superset. A collection of maximal frequent itemsets is denoted as ?????????.

????????? is a ?????? collection on ?.

LEMMA 1. [2] Any subset of a frequent itemset is frequent: ? ? ???????? and ? ? implies ? ? ????????.

Thus, by definition a frequent itemset must be subset of at least one maximal frequent itemset. ?  LEMMA 2. [12] ????????? is the smallest collection of itemsets from which ???????? can be inferred.?  PROBLEM 1. ?MINING FREQUENT ITEMSETS? Given ???? and a transaction database ?, the problem of min- ing frequent itemsets is to find ????????.

EXAMPLE 1. Let us consider the example in Figure 1, where ? = ?1,2,3,4,5? and the figure shows ? and ????.

Suppose ???? = 0.4 (40%). ??????? is composed by the shaded and bold itemsets, while ???????? is composed only by the bold itemsets. Note that 	????????	 is much smaller than 	???????	.

A naive approach to find ??????? is to first compute ??? ??? for each ?? ????, and then return only those that ??? ??? ? 0.4. This approach is inappropriate because:  ? If 	 ? 	 (the dimension) is high, then 	 ???? 	 is huge (i.e., ????).

? If 	 ? 	 (the size) is large, then computing ??? ??? for all ? ? ???? is infeasible.

By applying lemma 1 we can greatly improve the search for ???????. In Figure 1, once we know that the item- set ?34? is not frequent, we do not need to generate any of its supersets, since they must also be not frequent. This sim- ple pruning trick was first used in [2] and it greatly reduces the number of candidate itemsets generated. Several differ- ent searches can be applied by using this pruning trick.

Figure 1. Frequent Itemset Mining Example.

Basic Algorithm: Our algorithm employs a backtrack search to find ???????? and ?????????. A solu- tion is represented as an itemset ? = ? ?? ?? ? ? ??. Each item ? is chosen from a finite possible set, ?? . Ini- tially ? is empty; it is extended one item at a time, as the search proceeds. The size of ? is the same as the depth of the corresponding itemset in the search tree. Given a ?-candidate itemset ? ? = ? ?? ?? ? ? ? ? ????, the possi- ble values for the next item ? comes from a subset ??  ?? called the combine set. Each iteration tries extend- ing ? ? with every item in ??. An extension is valid if the resulting itemset ? ??? is frequent. If ? ??? is fre- quent and it is no subset of any already known maximal frequent itemset, then ? ??? is a maximal frequent item- set. The next step is to extract the new possible set of ex- tensions, ????, which consists only of elements in ?? that follow . The new combine set, ???? consists of those el- ements in the possible set that produce a frequent itemset when used to extend ? ???. Any item not in the com- bine set refers to a pruned subtree. The algorithm per- forms a depth-first traversal of the search space. When the search finishes, ???????? and ????????? were found. The computation of ??? ??? is based on the as- sociativity of subsets. Let ???? ? be the ?????? of ? in ? (the set of ???? in ? in which ? has occurred), and thus, 	 ???? ? 	 = ??? ???. According to [12], ???? ? can be obtained by intersecting the tidsets of two sub- sets of ? . For example, in Figure 1, ?????	? = ?1, 5, 9, 10? and ????? ? = ?1, 4, 5, 8, 9, 10?. Conse- quently, ????? ? = ?????	? ? ????? ? = ?1, 5, 9, 10?.

????? ?	 = ???? ??? = 4, and thus ???? ??? = 0.4.

EXAMPLE 2. Figure 2 shows how the algorithm works. We used sequence numbers above each itemset to facilitate the understanding of the backtrack search. First the algorithm process the items, and then it starts a depth-first search for frequent itemsets. For example, for sequence number ? = 13, the itemset being processed is ?124?. Therefore, the depth is ? = 3, and ?? = ?? = ?5?. When ? = 15, the al- gorithm visits the itemset ?124? again, but now?? = ? and  Proceedings of the 15th Symposium on Computer Architecture and High Performance Computing (SBAC-PAD?03)     consequently ?124? ? ?????????. Although an itemset ? can be visited more than once, ??? ??? is computed only in the first visit.

Figure 2. Basic Algorithm.

3. New Parallel Algorithms  In this section we propose several different parallel al- gorithms. First we present the design of our parallel ap- proaches, and then we show how to efficiently implement these approaches using different parallel environments.

In our parallel and distributed scenario,? is divided into ? partitions, ??? ??? ? ? ? ? ??. Each partition ?? is assigned to a node ? ?. A given node ? ? is composed by one or more processor units, one memory unit, and one database ??. A set of nodes is called a cluster. Our scenario is composed by a set of inter-connected clusters.

3.1. Algorithm Design  3.1.1. The Data Distribution Approach In this approach all nodes generate the same set of candidate itemsets, and each node ? ? can thus independently get ??? ? ??? for all candidates ? . A naive method to get ??? ??? would be to perform a communication round (and synchronization) between all nodes for each itemset ? generated. Then we could easily check if ??? ???? ????. This method is ob- viously inappropriate, since it performs several rounds of communication. Another method would be each node ? ? generating all ? ? 	? ? within its partition ??, and then only one sum-reduction operation between all nodes is nec- essary to get ??? ??? ? ? ? 	? ?. This approach is also inappropriate, since processing all ? ? 	? ? is infeasible.

We overcame all these problems and developed an ap- proach which needs only one round of communication and prunes the search space for frequent itemsets (i.e., our ap- proach processes much fewer itemsets than ?	? ??). We first present the basic theoretical foundation of our method.

LEMMA 3. For a given itemset ? , if ??? ???? ????, then ?? ? ??? ???? ? ????.

PROOF: If ??? ???? ? ?????? ? ??? ? ? ? ? ??, then ??? ???? ????, since  ?? ??? ? ?? ??? ? ?. ?  LEMMA 4.

?? ???????  ???? ??? is an upper bound for ??????????, and therefore it determines all itemsets ? such that ??? ???? ????.

PROOF: If ??? ???? ????, then ? must be frequent in at least one partition of?. If ? is frequent in some partition ?? (? ? ? ? ?), then it can be inferred by ???????? ???, and consequently by  ?? ???????  ???? ???. ? The Data Distribution approach has four distinct phases:  PHASE 1. ?FIRST ASYNCHRONOUS PHASE? Each node ? ? reads its local partition ??, and constructs the tidsets for all items in ??. Then, each node performs our basic algo- rithm on its partition. At the end of the search each node ? ? will know both ?????????? and ???????????.

PHASE 2. ?COMMUNICATION PHASE? Each node ? ? has to exchange ???????? ??? with every other node, so that at the end of the communication operation, each node can compute  ?? ???????  ???? ???. Since only ???????? ????? ? ??? ?? ? ? ? ? ?? is exchanged, the com- munication overhead is minimized (note that by lemma 2 ???????? ??? is the smallest frequent itemset collec- tion from which ??????? ??? can be inferred).

PHASE 3. ?SECOND ASYNCHRONOUS PHASE? Now that all nodes know  ?? ???????  ???? ???, by lemma 4 they can find?????????, without further synchronization. Now each node performs a top-down enumeration of itemsets, as follows. Each itemset present in  ?? ???????  ???? ??? is broken into ? subsets of size ?? ? ??. The support of this itemset can be computed by intersecting the tidsets of each item present in this itemset. This process iterates generat- ing smaller subsets and computing their supports until there are no more subsets to be checked. At the end of this phase, each node will have the same set of candidate itemsets, since the enumeration is based on the same upper bound.

PHASE 4. ?REDUCTION PHASE? After all nodes finish the top-down enumeration, a sum-reduction operation is per- formed to find ??? ??? for each candidate itemset ? (i.e.,  ??? ??? = ?  ?  ??? ??????? ??? . Next step is to remove those ?  with ??? ??? ?????, and then ????????? was found.

EXAMPLE 3. Figure 3 shows an example of the Data Dis- tribution approach. The database ? in Figure 1 was di- vided into two partitions, ?? and ??. In the first phase, ? ? and ? ? independently apply the basic algorithm on ?? and ??, respectively. After both nodes finish the first phase, ? ? will know ??????? ??? and ???????? ???, while ? ? will know ??????? ??? and ???????? ???. In the sec- ond phase, they exchange their maximal frequent collec- tions, so that both nodes will know  ?? ???????  ???? ??? =  Proceedings of the 15th Symposium on Computer Architecture and High Performance Computing (SBAC-PAD?03)     ?1235, 1245?. In the third phase, each node performs the top-down enumeration. ? ? process itemsets ?1245, 124, 145, 245?, while ? ? does not process any further item- set (since ???????? ??? =  ? ?  ??? ???????? ???). In the  fourth phase, ? ? and ? ? exchange ??? ? ??? and ??? ? ??? of each itemset ? , so that they know all ? with ??? ??? ? ????. For instance, ?????? ??? = 1, ?????? ??? = 3, and ???????? = ?  ?? = 0.4.

Figure 3. Data Distribution Example.

3.1.2. The Candidate Distribution Approach The basic idea of this approach is to separate the candidates in dis- joint sets. Each set is assigned to a node, so that it can asyn- chronously process its tasks. There are four distinct phases:  PHASE 1. ?FIRST ASYNCHRONOUS PHASE? Each node ? ? reads its local partition ??, and constructs the tidsets for all items in ??.

PHASE 2. ?COMMUNICATION AND ASSIGNMENT PHASE? Each node has to exchange the local tidsets with every other node. After each node knows the tidsets of the items in all partitions, we start the assignment of the tasks to the nodes.

Note that each item corresponds to a different backtrack tree, which corresponds to a disjoint set of candidate item- sets. The way the backtrack trees are assigned to different nodes depends on the implementation (we will discuss more about this in the next section). After the assignment, there is no dependence among the nodes.

PHASE 3. ?SECOND ASYNCHRONOUS PHASE? Each node proceeds the search for frequent itemsets within the back- track trees assigned to it, using our basic algorithm. Since the dependence among the processors was decoupled in the previous phase, there is no need for further synchroniza- tion, and the costly communication of tidsets (in the previ- ous phase) is amortized in this phase.

PHASE 4. ?REDUCTION PHASE? Finally, a join-reduction operation is performed and ????????? was found.

EXAMPLE 4. Figure 4 shows an example of the Candidate Distribution approach. In the first phase, ? ? reads ?? and constructs ??????, ??????, ??????, ??????, and ??????. At the same time, ? ? reads ?? and constructs ??????, ??????, ??????,??????, and??????. In the second phase, both nodes exchange its local tidsets, and then the backtrack tree for item ?1? is assigned to ? ?, while backtrack trees for items ?2,3,4? are asigned to? ?. In the third phase, each node ap- plies the basic algorithm on the backtrack trees assigned to it (i.e.,? ? has processed the white and bold itemsets, while ? ? has processed the shaded itemsets).

Figure 4. Candidate Distribution Example.

3.2. Algorithm Implementation  Two dominant paradigms for using multiple proces- sors have emerged: distributed- and shared-memory.

The performance-optimization objectives for distributed- memory implementations are different from those of shared-memory implementations. In the distributed- memory paradigm synchronization is implicit in commu- nication, so the goal becomes communication optimiza- tion. In the shared-memory paradigm, synchronization stems from locks and barriers, and the goal is to mini- mize these points. A third, very popular, paradigm com- bines the best of these two paradigms. Clusters of SMPs are part of this third paradigm. The physical mem- ory is distributed among the nodes, but each processor within each node has free access to the entire mem- ory of the node. Clusters of SMPs necessitate a hierarchical parallelism implementation, with shared-memory prim- itives used in a node and message passing used among the nodes. A fourth paradigm has also emerged, which al- lows a number of clusters to be connected together through a high speed network to work like a single super-computer.

3.2.1. The Shared-Memory Implementation  Proceedings of the 15th Symposium on Computer Architecture and High Performance Computing (SBAC-PAD?03)     Cache Locality, False Sharing, and Synchronization: Our algorithms avoid the use of complex structures, which may have poor locality. In fact, they use simple intersection op- erations to determine the frequent itemsets. This feature en- ables the algorithms to have good cache locality. Also, a problem unique to shared-memory systems is false sharing, which occurs when two different shared variables are lo- cated in the same cache block, causing the block to be ex- changed between the processors even though the processors are accessing different variables. A simple solution would be to place unrelated data that might be accessed simultane- ously on separate cache lines. While this simple approach will eliminate false sharing, it will result in unacceptable memory space overhead, and most importantly, a significant loss in cache locality [8]. Another technique for eliminating false sharing is called Privatization [3]. It involves mak- ing a private copy of the data that will be used locally, so that operations on that data do not cause false sharing. This technique allows us to keep a local array of frequent item- sets per processor. Each processor can use its local array during the search for frequent itemsets. Privatization elimi- nates false sharing completely, and there is no need of any kind of synchronization among the processors. We imple- mented the Candidate Distribution approach using this tech- nique. As explained earlier, the main idea is to assign dis- tinct backtrack trees to distinct processors. Each backtrack tree corresponds to a disjoint set of itemsets, and by using the privatization technique, there is no dependence among the processors. The backtrack trees are assigned to the pro- cessors in a bag of tasks approach, that is, given a bag of backtrack trees to be processed, each processor takes one tree, and as soon as it finishes the search for frequent item- sets on this tree, it takes another tree from the bag. When the bag is empty, we have found ?????????. We observed that the amount of work associated with a given backtrack tree is generally proportional to the support of the generat- ing item (or root). We sorted the backtrack trees in the bag in descending order of support of their roots, so that big- ger tasks will be processed earlier.

3.2.2. The Distributed-Memory Implementation We implemented both Data and Candidate Distribution ap- proaches using the distributed-memory paradigm. While data distribution deals with the large size problem, candi- date distribution deals with the high dimension problem.

The distributed-memory implementation of the Candi- date Distribution approach is similar to the shared-memory implementation, that is, different backtrack trees are as- signed to different nodes. Next we will describe how the Data Distribution approach was implemented.

Reducing the Large Size Problem: One step in the Data Distribution approach deserves special attention: the top- down enumeration. It is very important to implement this  step in an efficient way, otherwise one node can replicate work, computing ??? ???? for the same itemset ? more than once. As mentioned before, we can do this by simply inter- secting the tidsets of all items that compose ? . However, this approach is not efficient because it would need ??? in- tersections (if the size of ? is ?). To solve this problem, we store the intermediate tidsets in a hash-table (whose key is the itemset). For example, suppose we must find ?????? ???.

We first perform the operation ?????? ? ?????? = ???????, and then ??????? ? ?????? = ????????, and ?????? ??? = ??????????. After ??????? is processed, it is stored in the hash-table, so that we do not need to process??????? again.

3.2.3. The Hierarchical Implementation  Using both Shared- and Distributed-Memory: We imple- mented a hierarchical version of the Candidate Distribution approach, where different backtrack trees are assigned to different nodes, and inside each node, its backtrack trees are assigned to its processors. This approach greatly reduces the amount of communication performed. We also imple- mented a hybrid approach, which combines data distribu- tion among the nodes and candidate distribution inside each node. This hybrid approach reduces data skewness, since there are fewer partitions (although bigger) to be processed.

3.2.4. The Massively Parallel Implementation  Balancing Communication and Data Skewness: A very im- portant issue is how to reduce communication and data skewness. Unfortunately, we cannot reduce both of them at the same time (if communication is reduced, data skew- ness is increased, and vice-versa). Therefore, we must rely on how to balance these two metrics. This is especially important in massively parallel environments, because if we perform only candidate distribution, then the amount of communication will be too large. Otherwise, if we per- form only data distribution, then there will be a large num- ber of partitions, and data skewness will be too high. What is needed is a way to use both Data and Candidate Distri- bution approaches. We implemented this hybrid approach in such a way that data is distributed among the clusters and candidates are distributed among the nodes within each cluster. This choice reduces the communication among the clusters. Note that Data Distribution approach needs much less communication than Candidate Distribution approach, since only  ? ?  ??? ???????? ??? is transfered among the  clusters. Further, the number of partitions will be always the same as the number of clusters involved in the mining process (which is much smaller than the number of proces- sors), reducing data skewness.

4. Experimental Evaluation  Our experimental evaluation was carried out on two 8 node PENTIUM processor clusters. In one of the clusters all  Proceedings of the 15th Symposium on Computer Architecture and High Performance Computing (SBAC-PAD?03)     nodes have two processors. Each node has 1GB of main memory and 120GB of disk space. Writes have a latency of 1.2 ?secs, with transfer bandwidth of 100MB/sec. All nodes inside a cluster are interconnected through a high- speed network, the ???????. The clusters are intercon- nected through an optic-fiber. We have implemented the parallel algorithms using the MPI message-passing library (MPICH), and POSIX PTHREADS.

We used a real database for evaluating the performance of our algorithms. The WPORTAL database is generated from the click-stream log of a large Brazilian web portal.

We scanned the log and produced a transaction file (i.e.,?), where each transaction is a session of access to the site by a client. Each item in the transaction is a web request, but not all web requests were turned into items; to become an item, the request must have three properties: (1) the request method is GET; (2) the request status is OK; and (3) the file type is HTML. A session starts with a request that satis- fies the above properties, and ends when there has been no click from the client for 30 minutes. WPORTAL has 3,183 items comprised in 7,786,137 transactions. In each experi- ment, WPORTAL was divided into ? partitions, where ? de- pends on the number of processors (or clusters) employed.

Our evaluation is based on two basic parameters: number of processors and minimum support. Thus, for each mini- mum support employed, we performed multiple executions of the algorithms, where each execution employs a differ- ent number of processors. Further, we employed four dif- ferent metrics in our evaluation:  Execution Time: It is the total elapsed time spent for min- ing ?. Timings are based on wall clock time.

Communication: It is the total amount of bytes transfered among the nodes during the mining operation.

Parallel Efficiency: It is given by ?????? ??  ? , where ? is  the execution time obtained when using ? processors, and ? ? is the execution time obtained when using ??? processors. For instance, if ? = 100 secs and ? ? = 50 secs, then the parallel efficiency is 1 (100%).

Data Skewness: This metric quantifies how even (or un- even) the frequent itemsets are distributed among the partitions. Intuitivelly, if the frequent item- sets are evenly distributed among the partitions, then??  ?????? ??? ??? (i.e., the upper bound) is close  to ???	??? ??. We used the well established no- tion of entropy [4] to develop a suitable measure for data skewness. For a random variable, the en- tropy is a measure of the non-uniformity of its prob- ability distribution. Suppose ?? ? ?? ? ? ? ? ? ?? = ?. In this case, the value ?? ??? =  ??? ???? ??? ??? can be re-  garded as the probability of occurrence of ? in ??. We first define the skewness of an itemset ? as ?? =  ????????? ?????? , where ?? =  ?? ????? ??? ?  ????? ????. Then the data skewness metric can be de- fined as a weighted sum of the skewness of all item- sets, that is, ? =  ? ??	 ???????? ????? , where ??  = ??? ???? ??????????? ??????  . Please, refer to [4] for fur-  ther explanations and insights about this metric.

In the first series of experiments we used the 8 node dual processor cluster. In all graphs, Data Distribution (Dis- tributed) refers to the distributed-memory implementation of Data Distribution approach (? is divided into ? parti- tions, where ? is the number of processors), Candidate Dis- tribution (Distributed) refers to the distributed-memory im- plementation of Candidate Distribution approach (? parti- tions), Data Distribution (Hierarchical) refers to the hier- archical implementation of the Data Distribution approach (?? partitions, since there are 2 processors per node), and Candidate Distribution (Hierarchical) refers to the hier- archical implementation of the Candidate Distribution ap- proach (?? partitions). The first experiment we conducted was to empirically verify the advantages of our parallel al- gorithms in terms of execution times. We varied the num- ber of processors (1 to 16) and the minimum support (the number of frequent itemsets generated varied from 91,874 (for 	??? ? ????) to 283,186 (for 	??? ? ?????)), and compared the execution times for each parallel algorithm.

Figure 5 shows the execution times obtained for different parallel configurations. In all cases the distributed imple- mentations are slightly better when the number of proces- sors is less than 8. If more processors are used, the hier- archical implementations become better, since communi- cation (in the Candidate Distribution approach) and data skewness (in the Data Distribution approach) become too high in the pure distributed implementations. Further, the efficiency increases when we reduce the minimum support, since the asynchronous phase becomes more relevant [10].

The Data Distribution approaches seem to be the best ones.

Figure 6 shows the results obtained in a similar experiment, but now the appraised metric is the amount of communi- cation performed by each algorithm. The Candidate Dis- tribution approaches require approximatelly 3-4 orders of magnitude more communication than the Data Distribution approaches. Further, the hierarchical implementations save communication requirements by a factor of 5.

In the next experiment, we investigated how data skew- ness increases as we vary the number of processors (and consequently, partitions) using the Data Distribution ap- proach (note that the Candidate Distribution approaches do not generate data skewness, since the data is communicated among the processors). As showed in Figure 7, data skew- ness is higher for smaller minimum supports. Also, as ex- pected, data skewness is higher for the pure distributed im- plementations, since there are two times more partitions in this case. In the last experiment of this series, we inves-  Proceedings of the 15th Symposium on Computer Architecture and High Performance Computing (SBAC-PAD?03)     tigated how data skewness affects parallel efficiency. Fig- ure 8 shows that the effect of data skewness are worse for the pure distributed implementations. Although data skew- ness increases as we reduce the minimum support, paral- lel efficiency increases for lower minimum support values (since the asynchronous phase becomes more relevant).

2  4  6  8  10  12  14  16  Ex ec  ut io  n tim  e (s  ec s)  Number of processors  WPortal - 0.01  Data Distribution (Distributed) Candidate Distribution (Distributed)  Data Distribution (Hierarchical) Candidate Distribution (Hierarchical)             2  4  6  8  10  12  14  16  Ex ec  ut io  n tim  e (s  ec s)  Number of processors  WPortal - 0.005  Data Distribution (Distributed) Candidate Distribution (Distributed)  Data Distribution (Hierarchical) Candidate Distribution (Hierarchical)  Figure 5. Total Execution Times as a function of number of processors (SMP cluster).

The other series of experiments employed the two clus- ters, but to make ?equal? the computational power of both clusters, we used only 4 dual nodes of the first cluster. Our objective is to evaluate our algorithms in terms of execution times and speedup. We varied the number of processors and the minimum support value, and in each parallel configura- tion employed, each cluster used the same number of pro- cessors (i.e., 2 processors mean that cluster 1 and cluster 2 used 1 processor). Figure 9 shows the execution times ob- tained. For ???? = 0.01 we observed a speedup number of 11/16, while for ???? = 0.005 the speedup number reached almost 14/16. These numbers shows that combining both Data and Distribution approaches in the inter-cluster par- allel environment can be an efficient approach, since it re- duces communication among the clusters, and reduces data skewness, since there are only two logical partitions.

1e+06  1e+07  1e+08  1e+09  2  4  6  8  10  12  14  16  C om  m un  ic at  io n  (b yt  es )  Number of processors  WPortal - 0.01  Data Distribution (Distributed) Candidate Distribution (Distributed)  Data Distribution (Hierarchical) Candidate Distribution (Hierarchical)  Figure 6. Communication as a function of number of processors.

0.05  0.1  0.15  0.2  0.25  0.3  2  4  6  8  10  12  14  16  D at  a Sk  ew ne  ss  Number of processors  WPortal - Data Distribution  Distributed (0.01) Hierarchical (0.01) Distributed (0.005)  Hierarchical (0.005)  Figure 7. Data Skewness as a function of number of processors.

5. Related Work  Several algorithms for mining frequent itemsets were proposed in the literature [2, 12, 11]. APRIORI [2] was the first efficient algorithm and it forms the core of almost all current algorithms. During the first pass over ? the sup- port of each item is counted. The frequent items are used to generated candidate 2-itemsets. ? is scanned again to ob- tain the support of all candidate 2-itemsets, and the frequent ones are selected to generate candidate 3-itemsets. This it- erative process is repeated for ? = 3,4,. . . , until there are no more frequent ?-itemsets to be found. Clearly, APRI- ORI needs ? passes over ?, incurring in high I/O over- head. In the parallel case, APRIORI based algorithms do a reduction operation at the end of each pass, thus incur- ring also in high synchronization cost. Three different par- allel implementations of the APRIORI algorithm on IBM- SP2, a distributed-memory machine, were presented in [1].

In [10] the authors showed the impact in synchronization  Proceedings of the 15th Symposium on Computer Architecture and High Performance Computing (SBAC-PAD?03)     0.75  0.8  0.85  0.9  0.95   0  0.05  0.1  0.15  0.2  0.25  Pa ra  lle l E  ffi ci  en cy  Data Skewness  WPortal - Data Distribution  Distributed (0.01) Hierarchical (0.01) Distributed (0.005)  Hierarchical (0.005)  Figure 8. Parallel Efficiency as a function of Data Skewness.

2  4  6  8  10  12  14  16  Ex ec  ut io  n tim  e (s  ec s)  Number of processors  WPortal  0.01 0.005  Figure 9. Total Execution Times as a function of number of processors (2 clusters).

overhead due to fine-grained parallel approaches. The paral- lel algorithms NPA, SPA, HPA, and HPA-ELD, proposed in [7] are similar to those in [1]. HPA-ELD is the best among NPA, SPA, and HPA, because it reduces commu- nication by replicating candidates with high support on all processors. ECLAT, an algorithm presented in [12], needs only two passes over ? and decomposes the search-space for frequent itemsets in disjoint sub-spaces. In [13] several ECLAT-based parallel algorithms were presented. Our ba- sic algorithm need only one access to ?. In the parallel case, our algorithms need only one communication round.

Also, complementar experiments show that our basic algo- rithm generates a smaller number of candidates than both APRIORI- and ECLAT-based algorithms.

6. Conclusions  The huge size of the available databases and their high dimension make data mining applications very computa-  tionally demanding, to an extent that high performance par- allel computing is fast becoming an essential component of the solution. In fact, data mining applications are poised to become the dominant consumers of supercomputing and high performance systems in the near future. There is a practical necessity to develop effective and efficient paral- lel algorithms for data mining. In this paper we presented several parallel algorithms for frequent itemset mining, a fundamental data mining task. Key issues such as load bal- ancing, communication reduction, attention to data skew- ness, improving cache locality and reducing false sharing, were all addressed. Our parallel algorithms need only one access to the disk-resident database and are based on a novel and efficient backtrack algorithm (which was also presented in this paper). Also, our algorithms are the first ones able to deal with both large size and high dimension problems.

The algorithms were evaluated on different parallel environ- ments, and showed to be very efficient. We also presented a possible application: mining large web logs to better under- stand web patterns. We intend to continue our work by dis- tributing the databases in a widely distributed scenario, and developing proper algorithms to deal with the challenges imposed by this scenario.

