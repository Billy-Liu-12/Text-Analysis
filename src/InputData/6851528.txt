PQ KEY-A NOVEL KEY GENERATION ALGORITHM  FOR PROCESSING BIG DATA USING QUANTUM

Abstract  This paper proposes combining the concepts of quantum computing and big data. The idea of using the concept of quantum computing for processing the big data is suggested. Quantum computing is the advanced computing technology where we think ahead of our conventional binary bits. This is based on the quantum physics which states that a material can be at more than one place at a time. This concept, when used in our conventional computing systems, will have the third bit called the superimposed bit that can be in two states at a time. The term big data is used to refer to the huge amounts of data collected. The data being collected is increasing exponentially. According to a study conducted by McKinsey Global Institute, the data volume is growing 40% every year; and with the increasing number of users and research areas, one can expect much more growth in the data volume. With this kind of growing data, the need arises for faster processing and analysis methods giving better performance to the end users and the analytics. In this paper, a new algorithm, named as PQ-Key, is proposed for processing the big data using quantum computing.

1. Introduction   1.1. Big data  With the evolution of internet, the ever growing data is the major area of concern. The data is being generated at a very rapid rate every second and even every millisecond.

The evolution of our internet universe, the sources and the frequency of the data being generated varies and is ever growing. The term big data is used to refer to this ever growing data of various types, captured from various sources. Big data is the future of the internet. Every day the number of users is increasing and with this the data also increases. A simple example that can be considered here is the social networking sites like Face book, Twitter etc. whereas the data input is very high and the type varies we have the images, videos, text etc.  The big data is defined in terms of the three characteristics; Variety, Velocity and Volume. Variety speaks of the different types of data available. Conventional structured data  types, and our current unstructured and semi structured data types. Volume speaks of the amount or quantity of the data. Velocity speaks about the frequency or the rate at which the data is being generated, the rate being very high. The sources of data can be many such as, sensor data, Social networking data, Online survey data, online shopping data etc.

Figure1. Big data   With this ever growing volume and variety the need of the big data has increased and along with this, the need for a better way of processing the big data has become a major area of concern. The better and faster the big data is processed and better the analysis. The data that has to be considered ?big? is completely organization specific, depends on the capacity and processing powers of the organization. For some organization some GB?s of data may be big data and for some may not be. It is entirely organization/institution specific. Our main concern in any case is the better processing of the big data. Unless and until the captured big data is processed, the data does not make any sense. We have the Map Reduce and Hadoop currently in place to process the big data.  Processing of the big data is a pain area taken into account the size and the type of the data under consideration.

1.2 Quantum computing  Quantum computing is based on quantum physics.

Quantum physics is a branch of science that deals with discrete, indivisible units of energy called quanta as described by the Quantum theory. A key feature of quantum physics is the ability of the quantum wave function to exist in multiple states at the same time. It    uses the concept of quantum parallelism. Quantum parallelism is the method in which a quantum computer is able to perform two computations simultaneously. The term was coined by physicist David Deutsch, so as to distinguish it from classical parallel computation in standard computers. In a quantum computer, a single quantum processor is able to perform multiple computations on its own by utilizing the fact that the qubit exists in multiple states simultaneously  This gives a quantum computer much greater raw computation ability than a traditional computer. Quantum parallelism arises from the ability of a quantum memory register to exist in a super position of base states. In the quantum computing model we have a basic unit of information called the ?quantum bit? or ?qubit?. A qubit can be represented as a vector in a two dimensional complex Hilbert space where ?0> and ? 1> form a basis in the space.

The difference between qubits and bits is that a qubit can be in a state other than ? 0> or ? 1> whereas a bit has only one state, either 0 or 1. It is also possible to form linear combination of states, often called superposition.

The traditional computers use the binary bits for processing. The binary bits can be in either 0 or 1 state at a time. In quantum computing the bit called the ?qubit? can be in a superimposed state; meaning to state that they can be in either of the state at the same time. This ?qubit? thus provides huge performance benefits over the traditional computing. By allowing the computation to be performed on different values at once, the single unit in itself can behave as parallel processor.

1.3 Qubit Representation  The representation of qubits is shown in below Figure 2.

Single qubit can be in 2 states at the same time, 2 qubits in 4 states, 3 qubits in 8 states and so on.  With this kind of multiple states at the same time, the processing power will be relatively very high and behave like parallel processing.

Binary bit Qubit   0 or 1  Figure2. Qubit representation  1.4 Quantum computer  A full-fledged quantum computer is still under development. However the D-Wave company has successfully developed the prototype for 512 qubit quantum computer. Recently Google and NASA have opted for this quantum computer for better processing of the data and the research is under process [2].

Consider the following example of representing 2 and 3 in traditional binary and qubit. Here we can clearly observe the advantage of qubit. The most significant bit is constant and the rest bits vary in the binary bits, the same being considered to be in superimposed state in case of qubits. Author has organized paper as follows section (1) presents the introduction. Section (2) presents the big data processing. Section (3) describes the methodology and.

Section (4) presents the defiance and section (5) describes the conclusion and followed by reference.

Number Binary Qubit 2 010 3 011  Figure3. Data represents in quantum computer  2. The Big data processing   Big data as discussed earlier is used to refer to the data that is relatively very huge and is of the various types.

The present data growth rate is exponentially very high and this is growth rate is never going to be reduced and the data types are above the conventional standard types and along with the conventional structured data types we have the unstructured data type and semi structured data types. The big data is defined in terms of the three main characteristics: Volume, Velocity and Variety. Volume speaks of the size of the data, Velocity speaks of the frequency with which the data is generated and Variety speaks of the types of data. Big data is already in place, meaning to say we are already living in the world of big data, data being generated from various sources such as sensors, surveys, online sites, social networking sites and the list goes on. The data processing has always been an area of concern. With the evolution of data to big data, the traditional processing methods are turning vague.

With big data we need much more sophisticated methodologies. If we consider the example of online survey of some shopping sites, we need to process the input data to understand the consumer requirements. One more example that can be considered is the online voting system, where our main concern would be to identify the duplicate or fraudulent votes. Big data comes with security issues as well; which is not a point of concern here in this paper. Here we mainly focus on improving the processing ways.  To list a few companies that have already started working on big data, we have Oracle, IBM, Sas etc. We have the NoSQL of the oracle which is used for storing the big data. Hadoop and MapReduce open source of the Apache Foundation for processing the big data.  The data can only be processed sequentially in the above processing.

2.1 Current scenario of processing  We can consider three stages; Acquire, Organize/Process and Analyze of the big data. For Acquring the big data, the database popularly used is NoSql. NoSql is the model developed by Oracle, highly suitable for the unstructured    data types. Organizing the big data is being handled by the Apache Hadoop. Hadoop is the technology widely used to organize and process the big data; this uses the concept of cluster ? Hadoop File system (HDFS).  We have the MadReduce programming model that is used at large scale to process datasets. We have Google Map Reduce and Apache Map Reduce. In simple terms Map Reduce is based on the divide and conquer concept. The processes can be discussed under two phases; Map phase and Reduce phase. Map phase is responsible for dividing the problem under consideration into modules that can be easily passed onto to reduce phase for processing. Reduce phase is responsible for merging the processed results and producing the final result.

With quantum computing the processing can be made random instead of the regular sequential processing and thus better performance. We will be able to handle the parallel processing to a greater extent. Here we consider the single file system and two processors.

3. Methodology   The Methodology explains the key generation algorithm that will generate unique key and pass it to the two processors. The unique key will determine the superimposed qubit. Based on the superimposed qubit the processor will determine the state and hence determine the files to be processed. Since we have two processors under consideration, the processing speed will be improved to a very great extent and the processing will happen simultaneously on unique files. The processing will not happen on a file twice. The superimposed qubit will drive this feature. The key generation alogirthm will ensure that the key generated, and the superimposed qubit generated is unique that is being passed to the two processor. In this way we can speed up the processing and results in shorter span of time. Let us consider the example below of three bits. First let us discuss the key generation algorithm.

3.1 Key generation algorithm  The algorithm will be responsible for generating unique keys.  By keys here, we are speaking about the bits or rather qubits. The algorithm will identify and uniquely generate the key ? the stable bit and the superimposed bit.

This key will be passed on to the processor ? quantum computing processor which will be capable to being in the superimposed state at the same time and act as parallel processing. To keep the algorithm generic, we can consider passing the number of processors and the bits as input. Next let us understand the processing by quantum processor.

3.2 Processing by quantum computer  Quantum processor as discussed earlier will be processing quantum bits which can be in superimposed state. Once the processor receives the input from the key generation algorithm, it can then decode and decide its state and  perform the processing on the multiple states at a time returning the result back in reduced time. Since the single processor itself behaves as parallel processor due to the superimposed state of the bits, the processing time here can be expected to be reduced to a greater extent when compared to the traditional computers, where we require multiple processors and parallel processing to be set up.

As shown in the figure 5, the processor will be capable of processing the multiple bits at the same time.

Figure4. Applying Quantum Processing   Here in the below image we can see that the Processor 1 will process the first four bits at the time and Processor 2 will process the last four bits at the time. The processing will be unique.

Figure5. Key generation algorithm.

Key generation algorithm  P2  0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1  P1        4. The Defiances   The major challenges are as follows.

4.1 Quantum computer  Simulating the quantum computing is one of the biggest challenges. Since the quantum computer is not developed on full-fledged scale, simulating is the only option we have and simulation itself is a difficult task to accomplish, keeping in consideration the superimposed states of the qubits. For simulation we can make use of tools like Network Simulator or Matlab.

4.2 Big data  Require a huge file system to simulate and make a better comparison of the processing speed.

4.3 Key Generation algorithm  The algorithm should be capable of generating unique keys. We cannot use random function to generate keys as there are chances of repetition.

5. Conclusion   Once the quantum computing is used to process big data, the analytics and the processing will provide better and greater meaning. This can be made use in almost all the areas ? Analyzing customer data, Online surveys, Satellite data, Sensor data, Banking data etc. Quantum computing and big data in itself is one of the most challenging areas.

The understanding and exploiting these areas will prove to be a great achievement in the current internet universe, which is all about processing the data.

