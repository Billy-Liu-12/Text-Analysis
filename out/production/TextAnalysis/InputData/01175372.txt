'DISCOVERY OF EMERGING PATTERNS FROM NEAREST NEIGHBORS

Abstract:"' In this paper, we propose a scalable classifier that uses  jumping emerging patterns (JEPs), which are combinations of values that occnr in one class. The original classifier, DeEF%, is an instance-bad classifier that operates on all instances in real-time. It diswvers maximal patterns that occur thmughont the entire database and idenwies JEPs by wing these patterns. The necessary wmpotational effort, though, is likely to increase when DeEPs is applied to a large database.

Our proposed classifier operates on the nearest neigbbon of  a test instance. This reduction of instances improves scalability as the database volume increases. Moreover, our classifier imposes a restriction regarding JEPs discovery, so that it excludes patterns that cannot be identified as either correct JEPs or JEPs caused by the maximal patterns missing from nearest neighbors. These probably incorrect JEPs are specialized with additional items and participate in class determination. Our classifier performs significantly faster with these two enhancements, while it remains as accurate as the origiaal classifier.

1 Introduction  Classification is a critical issue in machine leaning research. In general, classification is aimed at classifying instances in a set of test data based on knowledge learned from a set of training data. Through extensive research, various classification methodologies have been developed [4s  DeEPs '' is a classifier that uses emerging patterns(EPs) to perform classification. EPs are, informally, combinations of values whose frequency of occurrence changes significantly between different classes. While association rule mining 'I' aims to discover patterns regarding any item that occurs dependently in a database, DeEPs is .intended for classification; DeEPs mainly discovers patterns that occur only in a certain class without occumng in other classes and uses these patterns for classification. These is a special type of EPs that are called jumping emerging patterns (JEPs).

DeEPs is an instance-based algorithm [*I in that it discovers EPs only after a-test instance is supplied for classification. Instance-based hyssifiers 'quire  no pre- learning, which is advantageous when these classifiers are applied to a target whose learning concept changes frequently. These classifiers, though, need to maintain many training instances in memory for use in its delayed  IO, 61  decision making, which requires a long time since these classifiers have to operate on these instances in real-time.

In this paper, we propose a classifier that is scalable with respect to database volume by employing the pattern discovery methods of DeEPs. The proposed classifier operates only on nearest neighbors, which is a group of instances within a certain distance from the current test instance. Since our classifier reduces its target instances in its early stage, it requires less processing time. DeEPs uses m i m l  panerns in its discovery phase, which patterns usually contain many items. Since the nearest neighbors also have many items in common with the current test instance, a large portion of these important patterns are likely to be available in the neighborhood of the current test instance.

However, all maximal patterns are needed to obtain logically correct JEPs. The correctness of JEPs means that a discovered JEP occurs only in its class. When some maximal patterns are not among the nearest neighbors, we will fail to discover some correct JEPs and misidentify some patterns as JEPs. These emrs  may lower accuracy.

To overcome this problem, we impose a restriction on which patterns will be discovered. This restriction eliminates a pattern that cannot be determined whether it is a correct JEP or a JEP merely caused by the maximal pat- tern missing from the nearest neighbors. This elimination is enabled by constraining the possibly incorrect pattern with additional items that commonly occur in the nearest neighbors.

The remainder of this paper is organized as follows.

Section 2 describes DeEPs and introduces necessary concept of borders. Section 3 describes the problem that arises from missing maximal patterns and proposes an algorithm to overcome this problem. Section 4 reports the experimental results carried out to compare the performance of the proposed classifier and DeEPs. Section 5 concludes this paper.

2 DeEPs  Emerging patterns are informally defined as patterns whose frequency of occurrence changes significantly between classes. In the remainder of this paper, we consider a special type of EPs called jumping emerging patterns (JEPs). JEPs are extreme type of EPs in that they occur in  0-7803-7508-4f02l$17.00 @ZOO2 IEEE   mailto:flab.fujitsu.co.jp    just one class and never in the other classes. Table 1 repre- sents an example database.

database. We say that a pattern is maximal if there is no pattern that is a superset of the pattern. A pattern is minimal  Since the JEPs in Table 2 occur only in a single class and .1 if there is no pattern that is a subset of the pattern. Table 3 presents the maximal patterns of class P, m R p ,  and those of class N, m R n .

never in any other, they are the JEPs inhisdatabase.

Table 2. Example of Jumping Emerging Patterns 1 JEPs of class I [sunny, mild, I  JEPs of class (sunny, high] (Sunny, mild, high 1 m, (sunny, high,  2.1 Classification Algorithm  Before we describe the original classification algorithms, we will explain the necessary terminology and definition.

A value that occurs in a database or in a test instance is called an item. Both patterns and instances are represented in the form of an item set, which is a collection of items.

A collection of item sets is succinctly represented in the form of a border as defined below 15'.

Definition 1 An ordered pair <LR> is called a border,  L the left-hand bound of this border and R the right-hand bound, if (a) each of L and R is an anti-chain - a collection of sets in which any two elements X and Y satisfy X g Y and Y g X , and (b) each element of L is a subset of some element in R and each element of R is a superset of some element in L.

A bonier represents a collection of item Sets containing any item set 2 satisfying X 0 Z 0 Y for any X E L and Y E R. For example, < ( ( l l ,  (2,311, ((1.2.31. (2,3,411> represents a collection of item sets:((l), (1,2], [1,31, (1,2,31. 12.31. (2,3,411.

DeEPs performs classification in three stages.

2.1.1 Reduction  Given a test instance, training instances are reduced by removing items irrelevant to the test instance.

In the example given in Table 1, given a test, T =  [sunny, mild, high, true), the removed items are represented in italic style. The result of this reduction is a sparse database.

Maximal patterns are then selected from the sparse  Table 1. Weather Conditions Class P(suitable for activity)  outlook temperature humidity windy overcast hor high false rain mild rain cool overcasf cool  rain mild s u ~ y  mild overcast mild overcmr hat  sunny cool  false false m e false false m e m e false  Table 3. Example of Maximal Patterns I maxR I (sunny, mild, I  . .  I I [sunny, mild, I high)  1 (&Id, high, true] I In some cases, there may be continuous attributes in a  datahase. Suppose that an attribute, ai, is a continuous valued attribute and its domain is 10, 11 and that the current test instance is T whose value of ai is xi.

Given a training instance, S, the reduced version of S contains xi if ai value of S is in [xi - a  , xi + a I. The default value of a is 0.12.

2.1.2 JEP discovery  This stage effects JEP discovery by using the maximal patterns obtained in the previous stage. An example of JEPs is already presented in Table 2. Formally, JEPs are represented in the form of border. <epLp,epRp> and <epLn,epRn> represents, respectively, all patterns occurring only in class P and class N '81.

<epLp,epRp> = <[ Q ],maxRp> - <( Q ] , m R n >  , <epLn,epRn> = <( Q ) , m R n >  - <( 6 I , m R p > .

DeEPs uses a collection of patterns, SEP=epLp U epLn, which is called necessq EPs "I, to determine what class to classify. An example SEP is ((sunny, mild, true), [sunny, high] 1.

This JEP discovery is achieved by using the two algorithms, JEPPRODUCER and BORDERDIFF ''. 'I. In brief, they generate all patterns that occur in just one class by operating on the maximal patterns, while a naive algorithm might enumerate all item sets and check them if they occur in a single class.

id Saturday Morning Activity Class N(not suitable)  outlook temperature humidity windy S U M Y  hot high false S U M Y  hot high true rain cool normal true sunny mild high false rain mild high true      2.1.3 Score Evaluation  In the third stage, DeEPs determines to which class it classifies the current kst instance. This decision is made through compact summations for each class.

compactScore(C) = countSc (SEP) / ISCl.

SC represents the records of class C and lSCl represents its size. countSc(SEP) is thenumber of instances in SC that contain one or more necessary EPs in SEP. The test instance is classified to the class with the highest compact score of all classes. A majority rule is used to break ties.

For the example given in Table 1, the compact scores for classes P and N would be  compactScore(P) = 1B = 0.1 1 , compactScore(N) = 3/5 = 0.6.

DeEPs assigns the current test to class N, consequently.

,  2.2 Multiple Classes  The problem that arises with two classes has been .explained and here we briefly explain an example with three classes. S1, S2 and S3 are, respectively, the instance sets for classes 1, 2, and 3. As in the problem with two classes, DeEPs first applies the reduction phase and then discovers the EPs for each class with respect to S1 from (S2  U S3), S2 from (S3 U Sl), and S3 from (S1 U S2).

3 Emerging Patterns from Nearest Neighbors  Our classifier uses only nearest neighbors for JEP discovery. We define nearest neighbors as the instances whose distance to the current test instance is within a certain threshold [91.

Dennitinn 2 d-Nearest Neighbors (d-NN), 0, is a set of instances whose distances to a test instances, T, are .less than or equal to a certainftied distance, d.

D =  { XI IX-Zl 5 d ) .

We currently employ city-block distance in determining d- NN. ,.

Given a test instance, the reduction phase removes values irrelevant to the test instance. The resulting sparse database is rkpsented with <LP,Rp> and &,Rn>. Rp and Rn represent, respectively, the maximal patterns of classes P and N in d-NN, and Lp and Ln, respectively, the minimal7patterns of classes P and N. Generally, the population of d-NN is much smaller than that of all iiistances. JEP discovery requires less processing time if it operates on a small  number of maximal patterns. On the other hand, many maximal patterns are among d-NN because each of d-NN instance matches to the test instance  with many items while maximal patterns likely to contain many items. As a result, d-NN should not considerably affect the accuracy with a moderate distance threshold.

However, d-NN does not 'always contain all maximal patterns in the entire database. The missing maximal patterns may affect the JEPs. The following example describes this situation. Suppose the current test instance is T = { 1,2,3,41. The sparse database of all instances and d- NN are, respectively, represented by <I 4 ],marRp> U < I  ] , m R n >  and <Lp,Rp> U <L.n,Rn>.

<( 41, m R p > =  <I 4 I, {11,2,31, 12,41)>, <{ 41, maxRn>= <{ 41. 111,3,41, {211>.

< L P ~ P > =  <1{1.21. 12,411, {{1,2,31, 12,41)>, <Ln,Rn> = <{[1,4]], [[1,3,4]]>.

In this example, the distance threshold is set to 2. The maximal pattern, (21, is missing from Rn because its distance to 1 1,2,3,4) is 3. borderP and border'P represent, respectively, collections of JEPs in all instances and d-NN.

borderP=<[4],marRp>-<(4],mMRn> ~  border'P=<( 41, R p > - < (  4],Rn> =<{{1,21, 11,31, 12,4I'l, {11,2,31. t2,411,>; .

= <{1211. I l l ,  2.31, 42,411>.

borderP is correct in thatit is the-set of JEPs that occur in a single class while hrder'P .contains two kinds of errors caused by the missing W i m a l  pattern. One is that there is a pattern that the correz? JEPs include, while JEPs in d-NN exclude; borderP includes { 1,3],'while border'P does not. [1,3} c& never be discovered as long as we consider d-NN. This is ~'because,..the missing maximal pattern, 121, is necegpy + i s c o v e r  (1 ,3)  since this undiscovered E P  is-fabtaii%ed by eliminating 2 from [1,2,3), which is a maximal pattern in class P. The other is that there is a pattern &at,the'correct JEPs exclude, while JEPs in d-NN includ+border'P includes (2,3), while borderP does not. Note that three 2-length patterns containing {2) are possible; they are [1,2], [1,3] and (2,3),.:and also note that the first two are included in the corredt JEPs. Therefore; {2,3] is the pattern that is missed from@ correct JEPs.

It is certainly true that 12) is not in class N if we investigate d-NN, since all instances of class N are in <[{1,4]], {{1,3,4])> as long as we consider d-NN.

However, one cannot determine whether the missing pattern is truly out of class or is merely missing from d- NN.

3.1  . '  Enhancement with the Left Bound  The problem was caused by the fact that we determined a pattern as a discovered JEP, although it might merely be missing from d-NN. To eliminate this probably-incorrect pattern, we impose a restriction on a discovered JEP that it      should be.a superset of a pattern that occur in both of the two classes.

In the example above, the problem is that 12) is determined as a discovered JEP, although it can not be determined whether it is correct or not. Note that 1) and {4) occur in both class P and class N. By constraining with these additional items, (2)  is specialized to (1,2) and (2,4), which are two of the three elements in the left bound of collect JEPs.

Therefore, our algorithm would firstly separate the pattern space by, using the set of pattems that commonly occur in both of the two classes. Then it would merge these patterns discovered in each subspke. The definition of the restricted JEPs in d-NN is givenbelow.

Definition 3 JEPs in d-NN are defined as a union of borders, each of which represents a collection of JEPs discovered in each subspace that is separated by ci as a basis.

k border",=u(<(cj],R~ > - < { c i ] , R :  >),(I)  i=l  R'p= [ X I X  E R p ,  ci 0 X )  , R'n = [ YI Y E Rn, ci 0 Y )  , < [ Q ) , [ c i  ,..., c i ) > = < [ Q ) L p > n < ( + ) & ~ > .

The set of ci is ,ob&ed by using the INTER- SECOPERATION algorithm [''. We employ the set of ci as a basis for partitioning because of the two reasons; one is that all d-NN instances belong to at least one subspace when they are partiti.oued by using this basis. This feature gives every d-NN instance a chance to satisfy a JEP. The other is that it is a necessary condition for a pattern to be a JEP that is a superset of ci. This proposition is m e  because all patterns that are subsets of ci always occur both in class P and class N. This feature is aimed to maximally eliminates the area any pattern from which cannot be determined whether it is correct or incorrect. ,The elements in both Rp and Rn are grouped into R p  and Rn by using ci.

Note that a maximal pattern may belong to more than two groups. JEPs are discovered ,in each partition using the formula, <(c i ) ,Rp> - <[ci),Rn>. This border representing JEPs is obtained by using JEPPRODUCER with a slight modificatiou; the modified algorithm strips ci from all elements in R'p and R'n at the beginning its p k e s s  and attaches ci to all elements of the resulting border at the end of its process. Finally, all JEPs from every group are merged.

3.2 The Proposed Algorithm  The outline of our' classification algorithm is similar to DeEPs: fmt, reduction of the values: second, JEP discovery; third, score evaluation and class determination.

We call our classifier DeEPsLB. LB indicates this classifier is enhanced using left bound of border representing d-NN.

In the reduction phase, DeEPsLB ,similarly removes irrelevant values in a database with the exception that it takes care of the d-NN. DeEPsLB firstly obtains a set of k- nearest neighbors (k-NN), which are the top k nearest instances by the city-block distance. Then it determines,d to be the distance to the farthest one in the k-NNs. The optimal k value is determined in ,the initialization .phase, which is explained later. The JEP discovery phase was specialized for d-NN in the previous sections. DeEPsLB obtains the maximal, Rp and Rn, and, minimal, Lp and Ln, patterns among d-NN. Then JEPPRODUCERLB performs discovery of the JEPs in d-NN.' The concrete procedure of JEPPRODUCERLB, which is straightforwardly imple- menting Eq. 1, is presented below.

1: JEPPRODUCERLB(<Lp.Rp>, <Dt,Rn>) 2: epLp= [ 1, epRp= [ )  4 INTERSECOPERATION(<[ & ],Lp>, <( 6 ),Ln>) 5:  for c in CL do 6: rp= ( x - c l  c Ox,  x E Rp) 7 :  m =  [ y - , c  I c . 0  y. y E Rn) 8: <el. er> = 10  e L = [ x U c I x E e l ) 11: e R = [ y  U c l y E  er) 12 epLp= epLp U eL 13: epRp= epLp U eR 1 4  remove all Y E epLp that are not minimal 15: return <epLp, epRp>  3: <[ @ 1, cL> =  JEPPRODUCER(<[ + 1, rp>, <[ Q 1, rn>)  The score evaluation phase differs from the compact summation. We employ a simple majority vote: score(0  = counrSc(SEP").

DeEPsLB assigns the current test to the class with the highest score. Tie breaking is identical to that for DeEPs: a majority rule by populations.

Finally, we explain the initialization phase, which is processed only once during the classifier lifetime. The optimal k, which is a parameter necessary in addition to a , is determined in this stage. A cross-validation like procedure determines the optimal k; for each k value in the fixed values, ( 5 ,  10, 15, 201, DeEPsLB is supplied with 10% of training instances as a test set and classifies them with the remaining 90% of training instances. The k value with the highest accuracy is the optimal value.

4 Experimental Results  We report the performance of our classifier (DeEPsLB) in comparison to the original classifier (DeEPs). We employ accuracy and processing time as measures to evaluate performance of these two classifiers. We have carried out experimeuts by using several of the databases from the UCI repository of machine learning databases 13'.

- 5.00 ~ 9.51  Our experiments were carried out on a 2.0 GHz Pentium IV PC, with 1 GB of RAM. The program was implemented with JAVA language.

Accuracy and processing time are' measured by a ten- fold cross validation; supplied the 10% of instances as a test set, classifiers execute classification using the.

remaining 90% as training instances. Each of the exclusive tenrfold test set is randomly collected from the original database and the same split of the data were used for the two classification algorithms.

When there are continuous attributes in a ,  database, values of these attributes in training instances are normalized into [0, I ]  using the formula x-midmax-min, where x is a continuous value of an attribute, and [min,max] is the domain of the attribute. Tbh process is executed at a loading time in the initialization phase of classifiers. The same scaling method is also applied to test.instances.

In the initialization phase, DeEPs determines the optimal a value and DeEPsLB uses the same optimal a .

The optimal a value is determined among the fixed values of [0.02, 0.05, 0.08, 0.10, 0.15, 0.20) by using the explained cross-validation l i e  method, which is also employed in determining the optimal k value.

4.1 Accuracy and Speed  5.89 100.00 93.62 27.09 98.12 99.06  Table 4.

data  20.15 53.96 19.22 21.85 32.58.

19.87 17.75 28.65 29.82  958.9.2  214.9.6, iris pima  wine 178.13.3  CIX 690.15.2  92.64 94.42 67.76 94.00 69.01 70.09 94.38 85.07 85.65  We carried out experiments on 13 databases and Table 4 shows our experimental results.

Column 1 Name of database.

Column 2 Property of database. They are the number of instances, attributes, and classes.

Column 3 Averaged optimd a value.

Column 4 Averaged optimal k value.

Cnlumn 5 Size of d-NN on which DeEPsLB performs classification.

Column 6-7 Accuracy of DeEPsLB and DeEPs.

Column 8-9 Processing time of DeEPsLB and DeEPs.

Column 10 The rate of improvement in processing time.

DeEPsLB was superior for five databases and inferior for eight. There were four databases (soy-small, glass, pima, vehicle) where the difference exceeded 3%, which are described in bold face. Among these four, DeEPsLB was superior for three databases and inferior for one.

Regarding to the average values, DeEPsLB was superior to DeEPs, although the difference was small. Note that we limited the target instances used for JEP discovery and the optimal a value is determined by DeEPs.

Processing times differed significantly between the two classifiers. F6r all databases, our classifier was faster. The processing time with DeEPs was at least 1.49 (tic) and up to 176.94 (vehicle) times of that needed with DeEPsLB.

With regard to the average, DeEPsLB is 53.25 times faster than DeEPs.

0.33 0.07 1.00 2.92 0.73  'he accuracy and speed of DeEPsLB in cor P-etem I accuracy(%)  1.64 5.01 0.27 4.10 2.70 2.69  516.21 176.52 13.10 17.94  - 0.121 0.031 0.054 0.051 0.061 0.059 0.070 0.089  8.47 10.49 8.95  10.50 12.51 13.99 11.01 12.99 13.51  94>7 1 95.71 59.81 94.61 66.78 95.51 84.49 85.94  parison to DeEPs timelmsl I rate I +I DeEPsL.6 DeEPs  93.94 140.91 y::; 1 3.07 I 2.23 I  1.68 I 19.31 I 11.47 1 1.86 21.51 11.58  I hunaarian 1294:13:2 I 0.093 I 11.51 I 25.54 I 82.31 I 82.99 I 0.75 I 2.35 I 3.12 1 - Z W  1 101.16.7 1 0.105 1 6.24 1 10.44 1 94.06 1 93.07 0.40 1 3.96 10.00  average - - - - 86.73 I 86.07 I 1.05 I 56.02 53.25  5 Conclusions  We have proposed a classifier that enhances DeEPs with respect to scalability as the database volume increases.

This classifier operates only on nearest neighbors to discover JEPs. Since the number of nearest neighbors is much smaller than the number of all instances, this reduction decreases the processing time. JEP discovery can also be enhanced by imposing a restriction on its process. We have proposed an algorithm that excludes patterns that cannot be identified as correct or incorrect  JEPs. These patterns are specialized with additional items and they are used for classification. The experiments was carried out and we found there was no significant difference in accuracy, but our classifier was much faster.

