Text Document Categorization by Term Association

Abstract  A good rexr classifer is a classifier that eflcienrly car- egorizes large sers of rexr documenrs in a reasonable rime frame and with an acceptable accuracy, and rhar provides classification rules rhar are human readable for possible fine-tuning. If rhe rraining of the classifer is also quick, rhis could become in some application domains a good as- ser for the classiJer: Many rechniques and algorithms for automatic rexr caregorizarion have been devised. According to published lirerarure, some are more accurare rhan others.

and some provide more inrerprerable classifcarion models rhan orhers. However, none can combine all the beneficial properties enumerared above. In  rhis paper, we present a novel approach for auromaric rexr caregorizarion that bor- rows from marker basker analysis techniques using associ- arion rule mining in rhe data-miningfield. We focus on WO majorpmblems: (1)finding the besr r e m  associarion rules in a rexrual darabase by generaring and pruning; and (2) using rhe rules IO build a text classifer: Our rexr carego- rization merhod proves ro be eflicienr and effecrive, and er- perimenrs on well-known collecrions show rhar rke classifer performs well. In  addition, training as well as classificarion are borh fasr and rhe generared rules are human readable.

1 Introduction  Automatic text categorization has always been an impor- tant application and research topic since the inception of digital documents. Today, text categorization is a necessity due to the very large amount of text documents that we have to deal with daily. A text categorization system can he used in indexing documents to assist information retrieval tasks as well as in  classifying e-mails, memos or web pages in a yahoo-like manner. Needless lo say, automatic text catego- rization is essential.

The text classification task can he defined as assigning category labels to new documents based on the knowledge gained in a classification system at the training stage. In the training phase we are given a set of documents with class  Osrnar R. Zai'ane University of Alberta, Canada  zaiane@cs.ualberta.ca  labels attached. and a classification system is built using a learning method. Classification is an important task in both data mining and machine learning communities, however, most of the learning approaches in text categorization are coming from machine learning research.

Recent studies in the data mining community proposed new methods for classification employing association rule mining [ 12, 131. These associative classifiers have proven to be powerful and achieve high accuracy. However, they were only implemented and tested on small numerical datasets from the UCI archives [19].

In this work we present a new classification method for text that takes advantage of association rule mining in the learning phase and makes the following contributions: First, a new rechnique for  rexr caregorizarion rhar makes no as- sumprion of r e m  independence is proposed. This method proves ro perform as well as orher merhods in the lirerarure.

Second, it is fasr during borh training and caregorizarion phases. Third, rhe classifierrhar is built using ourappmach can be read, understood and modijied by humans. Exper- iments show that the effectiveness of the classifier can he improved by manually fine tuning the classification rules generated during the training phase. The resulting classi- fier is able to perform both single-class classification. by which each document is assigned a unique class label, and multiple-class classification, by which a document could be classified in many classes simultaneously. Our experiments are performed on text databases, however, this doesn't limit the use of our classifier to text documents. It can he ap- plied in addition to images or any other database that can be modelled as a transactional database [2].

The remainder of the paper is organized as follows: Sec- tion 2 gives an overview of related work in automatic text categorization and in association rule mining. In Section 3 we introduce our new text categorization approach. Experi- mental results are described in Section 4 along with the per- formance of our system compared to known systems. We summarize our research and discuss some future work di- rections in Section 5 .

0-7695-1754-4102 $17.00 0 2002 IEEE 19    2 Related work  Many text classifiers have been proposed in the litera- ture using machine learning techniques. probabilistic mod- els, etc. They often differ in the approach adopted: decision trees, ndve-Bayes, rule induction, neural networks. nearest neighbors, and lately, suppon vector machines. Although many approaches have been proposed, automaled text cate- gorization is still a major area of research primarily because the effectiveness of current automated text classifiers is not faultless and still needs improvement.

A classifier is built by applying a learning method to a training set of objects. This model is further used to predict thelabelstonewincomingobjects. With all theeffon in this domain there is still place for improvement and a great deal of attention is paid to developing highly accurate classifiers.

The use of association rule mining for building classifi- cation models is very new. Recent studies have proposed the use of association rules in building effective classifiers for numerical data. These classification systems discover the strongest association rules in the database and use them to build a categorizer.

In the following subsections a more detailed overview of the related work is presented from both domains that merge in our research: text categorization and association rule mining.

2.1 Text categorization  The automatic text classification problem can be defined as building a classification model to assign one or more pre- defined classes to new documents. Text categorization re- search has a long history, starting in the early 1960s. Nowa- days, with all the textual information on the Web or in com- panies' intranets, text categorization has revived and there is more demand for effective and efficient classification mod- els.

Most of the research in text categorization comes from the machine learning and information retrieval communi- ties. Rocchio's algorithm 181 is the classical method in in- formation retrieval, being used in routing and filtering doc- uments. Researchers tackled the text categorization prob- lem in many ways. Classifiers based on probabilistic mod- els have been proposed starting with the first presented in literature by Maron in 1961 and continuing with n ~ v e - Bayes [IO] that proved to perform well. ID3 and C4.5 are well-known packages whose cores are making use of deci- sion trees to build automatic classifiers [5 ,  6 .  91. K-nearest neighbor (k-NN) is another technique used in text catego- rization [ZO]. Another method to construct a text categoriza- tion system is by an inductive rule learning method. This type of classifiers is represented by a set of rules in disjunc- tive normal form that best cover the training set [14, 11.31.

As reported in [ 181 the use of bigrams improved text cate- gorization accuracy as opposed to unigrams use. In addi- tion, in the last decade neural networks and suppon vector machines (SVM) were used in text categorization and they provedto be powerful tools [16.21.9].

2.2 Association Rule Mining  2.2.1 Association Rules Generation  Association rule mining is a data mining task that discov- ers relationships among items in a transactional database.

Association rules have been extensively studied in the lit- erature. The efficient discovery of such rules has been a major focus in the data mining research community. From the original apriori  algorithm [I] there has been a remark- able number of variants and improvements culminated by the publication the FP-Tree growth algorithm [71. How- ever. most popular algorithms designed for the discovery of all types of association rules, are apriori-based.

Formally, association rules are defined as follows: Let I = { i l , i z ,  ... im} be a set of items. Let V be a set of transactions. where each transaction T i s  a set of items such that T I. Each transaction is associated with a unique identifier TID. A transaction T is said to contain X. a set of items in I, if X C T. An associarion ride is an impli- cation of the form "X + Y", where X C I, Y G Z. and X n Y  = @ . T h e r u l e X  + Yhasasupporrsinthetransac- tion set V ifs% of the transactions in 'D contain X U Y. In other words, the support of the rule is the probability that X and Y hold together among all the possible presented cases.

It is said that the rule X j Y holds in the transaction set 2, with confidence c if c% of transactions in U that contain X also contain Y.  In other words, the confidence of the rule is the conditional probability that the consequent Y is true under the condition of the antecedent X .  The problem of discovering all association rules from a set of transactions V consists of generating the rules that have a sirpporr and confidence greater than given thresholds. These rules are called srmng rules.

The main idea behind apriori algorithm is to scan the transactional database searching for k-itemsers (k items be- longing to the set of items I). As the name of the algorithm suggests. it uses prior knowledge for discovering frequent itemsets in the database. The algorithm employs an iter- ative search and uses k-itemsets discovered to find (k+l)- itemsets. The frequent itemsets are those that have the sup- pon higher than a minimum threshold.

2.2.2 Associative classifiers  Besides the classification methods described above, re- cently, and parallel to our work on associative text catego- rization. a new method that builds associative general clas-    Figure 1. Construction phases for an association-rule-based text categorizer  sifiers has been proposed. In this case the learning method is represented by the association rule mining. The main idea behind this approach is to discover strong patterns that are associated with the class labels. The next step is to take ad- vantage of these patterns such that a classifier is built and new objects are categorized in the proper classes.

WO such models were presented in the literature: CMAR [12] and CBA [13]. Although both of them proved to be effective and achieve high accuracy on relatively small UCI datasets [19]. they have some Limitations. Both models perform only single-class classification and were not im- plemented for text categorization. In many applications, however, and in text categorization in particular, multiple- class classification is required. In our paper we try to over- come this limitation and construct an associative classifica- tion model that allows single and multiple-class categoriza- tions of text documents based on term co-frequency counts (i.e. a probabilistic technique that doesn?t assume term in- dependence).

3 Building an Associative Text Classifier  In this paper we present a method to build a categoriza- tion system that merges association rule mining task with the classification problem. This model is graphically pre- sented in Figure 1.

Given a data collection, a number of steps are followed until the classification model is found. Data preprocess- ing represents the first step. At this stage cleaning tech- niques can be applied such as stopwords removal, stem- ming or term pruning according to the TFKDF values (term frequencylinverse document frequency). The next step in building the associative classifier is the generation of as- sociation rules using an apriori-based algorithm. Once the entire set of rules has been generated an important step is  to apply some pruning techniques for reducing the set of association rules found in the text corpora. The last stage in this process is represented by the use of the associa- tion rules set in the prediction of classes for new docu- ments. The first three steps belong to the training pro- cess while the last one represents the testing (or classifi- cation) phase. More details on the process are given in the subsections below. If a document D; is assigned to a set of categories G = {cl ,cz,  ... c,} and after word pruning the set of terms T = { t l , t z ,  . . A n }  is retained, the following transaction is used to model the document: D, : {cl,cp ,._. cm,tl,tp, ... t,,} and the association rules are discovered from such transactions representing all docu- ments in  the collection. The association rules are, however, constrained in that the antecedent has to he a conjunction of terms from T, while the consequent of the rule has to a member of C.

3.1 Data Collection Preprocessing  In our approach, we model text documents as transac- tions where items are words or phrases from the document as well as the categories to which the document belongs. as described above. A data cleaning phase is required to weed out those words that are of no interest in building the asso- ciative classifier. We consider stopwording and term prun- ing as well as the transformation of documents into trans- actions as a pre-processing phase. Stopword removal and term pruning is done according to the TFnDF values and a given list of stopwords. We have opted to selectively turn on and off stopwording depending upon the data set to catego- rize, It is only after the terms are selected from the cleansed documents that the transactions are formed. The subsequent phase consists of discovering association rules from the set of cleansed transactions.

3.2 Association Rule Generation  In our algorithm, as we shall see in this section, we take advantage of the apriori algorithm to discover frequent tern-sets in documents. Eventually, these frequent item- sets associated with text categories represent the discrimi- nate features among the documents in the collection. The association rules discovered in this stage of the process are further processed to build the associative classifier.

Using the apriori algorithm on our transactions repre- senting the documents would generate a very large number of association rules, most of them irrelevant for classifica- tion. We use an apriori-based algorithm that is guided by the constraints on the rules we want to discover. Since we are building a classifier, we are interested in rules that indicate a category label, rules with a consequent being a category label. In other words, given the document model described     above, we are interested in rules of the form T + c, where T c T and c; C C. To discover these interesting rules ef- ficiently we push the rule shape constraint in  the candidate generation phase of the apriori algorithm in order to retain only the suitable candidate itemsets. Moreover, at the phase for rule generation from all the frequent k-itemsets, we use the rule shape constraint again to prune those rules that are of no use in our classification.

There are two approaches that we have considered in building an associative text classifier. The first one ARC- AC (Association Rule-based Classifier with All Categories) [22] is to extract association rules from the entire training set following the constraints discussed above. As a result of discrepancies among the categories in  a text collection of a real-world application, we discovered that it is diffi- cult to handle some categories that have different charac- teristics (small categories, overlapping categories or some categories having documents that are more correlated than others). As a result we propose a second solution ARC-BC (Associative Rule-based Classifier By Category) that solves such problems. In this approach we consider each set of documents belonging to one category as a separate text col- lection to generate association rules from. If a document belongs to more than one category this document will be present in  each set associated with the categories that the document falls into. The ARC-BC algorithm is described in more detail below.

Algorithm ARC-BC Find association rules on the training set ofthe text collection when the text corpora is divided in subsets by category  Input A set of documents (D) of the form D; : ( c i ; t ~ , l 2 :  .Am} where e. is the category attached to the docu- ment and 1, xe the selected terms for the document; A minimum support threshold; A minimum confidence threshold:  Output A set of association rules of the form t 1 A 1 2  A.. . A t ,  3 ct where c, i s  the category and t j  is a term;  Method:  ( I ) (2) (3) (4) C, - (F,-I w Ft-1) (5) C, + C, - { c  1 (i - 1) item-set of c (6)  D, - FilterTub/e(D,-1, KI) (7) fareach document d in  'D, do { (8) (9) (10) } ( 1 1 )  } (12) (13) 1 (14) S e t s e U i { c E F , l i > l } (15) R=O (16) (17)  CI + {Candidate I term-setsand their suppml} FI - (Frequent 1 term-sets and their support} for (i - 2; R - ,  # 0 i - i + 1) do(  F + - I }  foreach c i n  Ci do { c.support - c.support + CounNc, d )  F, + { c  E C, I c.support > U }  fareach itemset I in Sets do { R - R+ { I  + Cat}  (18) }  In ARC-BC algorithm step (2) generates the frequent 1- itemset. In steps (3-13) all the k-frequent itemsets are gen- erated and merged with the category in C1. Steps (16-18) generate the association rules. The document space is re- duced in each itereation by eliminating the transactions that do not contain any of the frequent itemsets. This step is done by FilterTable('D,-1, F,-I)  function.

Table 1 presents a set of rules that are discovered in the text collection. Such rules are composing the classifier. Al- though the rules are human readable and understandable if the amount of rules generated is too large it is time consum- ing to read the set of rules for further tuning of the system.

This problem leads us to the next subsection where prun- ing methods are presented. Although the rules are similar to those produced using a rule-based induced system, the approach is different. In addition, the number of words be- longing to the antecedent could he large (in our experiments up to IO words), while in some studies with rule-based in- duced systems. the mles generated have only one or a pair of words as antecedent [3].

3.3 Prun ing  the Set of Association Rules  The number of rules that can he generated in the asso- ciation rule mining phase could he very large. There are two issues that must be addressed in this case. One of them is that such a huge amount of rules could contain noisy in- formation which would mislead the classification process.

Another is that a huge set of rules would make the classifi- cation time longer. This could be an imprtant problem in applications where fast responses are required.

The pruning methods that we study in this paper are the following: eliminate the specific rules and keep only those that are more general and with high confidence, and prune unnecessary rules by database coverage. Let us introduce the notions used in this subsection by the following defini- tions:  Definition 1 Being given two rules TI * C and 2'2 + C we say that the first rule is more general if TI  The first step of this process is to order the set of rules.

This is done accordingly to the following ordering defini- tion.

Definition 2 Being given two rules R l  and Rz.  R I  is higher ranked than Rz if:  (1) RI  has higher confidence than Rz (2) if the confidences are equal, supp(R1) must exceed  supp(Rz) (3) both confidences and support are equal, but R1 has  less attributes in  left hand side than R2 With the set of association rules sorted, the goal is to  select a subset that will build an efficient and effective clas- sifier. In our approach we attempt to select a high quality subset of rules by selecting those rules that are general and  Tz.

agriculture A depmment A grain * corn assistance A bank A england A market A money * inlmesl acute A coronary A function A left A ventricular arnbvlatoly A ischemia A myocardial  myocardial-infarction coronary-disease  Table 1. Examples of assoc ia t ion  rules composing t h e  classifier.

have high confidence. The most significant subset of rules is finally selected by applying the database coverage. The algorithm for building this set of rules is described below.

Algorithm Pruning the set of association rules Input The set of association rules that were found in the  association rule mining phase (S) and the training text col- lection (D)  Output A set of rules used in the classification process Method  sort the rules according to Definition 1 foreach rule in the set S  find all those rules that are more specific according to (Definition 2) prune those that have lower confidence  a new set of rules S? is generated foreach rule R in the set S?  go over D and find those transactions that are covered by the rule R if R classifies correctly at least one transaction  remove those cases that were covered by R select R  3.4 Prediction of Classes Associated with New Documents  The set of rules that were selected after the pruning phase represent the actual classifier. This categorizer will be used to predict to which classes new documents are attached.

Given a new document, the classification process searches in this set of rules for finding those classes that are the clos- est to be attached with the document presented for cate- gorization. This subsection discusses the approach for la- belling new documents based on the set of association rules that forms the classifier.

A trivial solution would be to attach to the new document the class that has the most rules matching this new docu- ment or the class associated with the first rule that apply to the new object. However, in the text categorization domain, multi-class categorization is an important and challenging problem that needs to be solved. In our approach we give a solution to this problem by introducing the dominance fac- tor. By employing this variable we allow our system to as- sign more than one category. The dominance factor 6 is the proportion of rules of the most dominant category in the  applicable rules for a document to classify. Given a docu- ment to classify, the terms in the document would yield a list of applicable rules. If the applicable rules are grouped by category in their consequent part and the groups are or- dered by the sum of rules? confidences, the ordered groups would indicate the most significant categories that should be attached to the document to be classified. We call this order category dominance, hence the dominance factor 6.

The dominance factor allows us to select among the candi- date categories only the most significant. When 6 is set to a certain percentage a threshold is computed as the sum of rules? confidences for the most domina1 category times the value of the dominance facrnr. Then, only those categories that exceed this threshold are selected. TakeKClasses(S.6) function selects the most k significant classes in the classi- fication algorithm.

The next algorithm describes the classification of a new document.

Algorithm Classification of a new object Input A new object to be classified 0; The associative  classifier (ARC); The dominance factor 6; The confidence threshold r;  Output Categories attached to the new object Method:  (1) (2) (3) (4) if (count == 1) ( 5 ) (6)  S - S u r (7)  (9) elseexit (10) divide S in subsets by category: 4 ,  Sz ... S,, (11) foreachsubset S1,S ,... S,, (12) sum the confidences of rules and divide by  the number of rules in S, (13) if i t  is single class classification (14) put the new document in the class that has  the highest confidence sum (15) else /*multi-class classification*/ (16) TakeKClasses(S.6) (17)  s - 0 /*set of rules that match o*/ foreach rule r in ARC (the sorted set of rules)  if (T c o) { count++ }  fr.conf + r.conf /*keep the first rule confidence*/  else if (r.conf > fr.conf-r) (8) s c s u r  assign these k classes to the new document     4 Experimental Results and Performance Study  4.1 Text Corpora  In order to be able to objectively evaluate our algorithm vis-a-vis other approaches, like other researchers in the field of automatic text categorization, we used the Reuters-21578 text collection [ 151 as benchmarks. This text database is de- scribed below. Text collections for experiments are usually split into two parts: one part for training or building the classifier and a second part for testing the effectiveness of the system.

There are many splits of the Reuters collection: we chose to use the ModApte version. This split leads to a corpus of 12,202 documents consisting of 9,603 training documents and 3,299 testing documents. There are 135 topics to which documents are assigned. However, only 93 of them have more than one document in the training set and 82 of the categories have less than 100 documents [22]. Obviously, the performances in the categories with just a few docu- ments would be very low, especially for those that do not even have a document in the training set. Among the doc- uments there are some that have no topic assigned to them.

We chose to ignore such documents since no knowledge can be derived from them. Finally we decided to test our clas- sifiers on the ten most populated categories with the largest number of documents assigned to them in the training set.

Other researchers have used the same strategy [ 171, which constrained us to do the same for the sake of comparison.

By retaining only the ten most populated categories we have 6488 training documents and 2545 testing documents. On these documents we performed stopword elimination but no stemming.

4.2 Experimental Results  On this data set we tested our classification system ARC- BC on a Pentium 111 700MHz dual processor machine run- ning Linux. Several measurements have been used in previ- ous studies for evaluation. Some measures, as well as those used in our evaluation, can be defined in terms of precision and recall. The formulae for precision and recall are given below: R = & and P = 5. The terms used to express precision and recall are given in the contingency table Table 2.

For evaluating the effectiveness of our system, we used the breakeven points. The breakeven point is the point at which precision equals recall and i t  is obtained as reported in [4].

When dealing with multiple classes there are two pos- sible ways of averaging these measures, namely, macro- average and micro-average. In the macro-averaging, one  Category col  classifier Yes assignments No  human assignments Yes I NO a I b c 1 d  Table 2. Contingency table for category car  HEY Xlthl",  pN"1"I rn/'  m" d*.L,,,  upli"i >"WW' ~"px"* &%I h=l,l is, &VI &111 *;U, &I,, &,O &U,  "?,I X l h  x n  X I "  Xl, I(" X I , ,  " e /  Xlll *i ,?.* 70J h X 1  , s o  7"s n l ?  7 8 1  , V I  ,I,, i",, "?I 111 ,"* 7 2 ,  i n ,  ,114 U ,  Table 4 (the results for the other classification systems are reported as given in [9]) shows a comparison between our ARC-BC classifier and other well-known methods. The measures used are precisiodrecall-breakeven point, micro- average and macro-average on ten most populated Reuters categories. Our system proves to perform well as compared to the other methods. It outperforms most of the conven- tional methods. but i t  does not perform better than SVM. In addition to these results, our system has two more features.

First, it is very fast in both training and testing phases (see Table 6). The times reported are for all training and testing documents. Second, i t  produces readable and understand- able rules that can be easily modified by humans (see Table I) .  Table 5 reports the improvements in the response of the system when human tuning was applied. The support wds set to 20% which made corn and wheat categories to per- form very poor. By reading the rules we noticed that by adding 4 more rules for each of these categories the perfor-     most known classifiers  ARC-BC suppc2036 6=90 rm-scdbcov BEP I initial set of NleS I manual tuned set of rules %." I I I  micm-avg 1 84.14 I M.62 mscmavg 1 63.55 74.41  Table 5. Micro-average PrecisionlRecall- breakeven point for ten most populated Reuters categories - manual tuning of the classifier  mances improved as presented in Table 5.

A comparison between the pruning methods is given in  Table 7. By applying the pruning methods the accuracy of the classifier is not improved. However, the reduction in number of rules represents a step further in manually or au- tomatically tuning of the system.

5 Conclusion and Future Work  This paper introduced a new technique for text catego- rization. It employs the use of association rules. Our study provides evidence that association rule mining can he used for the construction of fast and effective classifiers for auto- matic text categorization. We have presented an association rule-based algorithm for building the classifier: ARC-BC that considers categories one at a time. The algorithm as- sume a transaction-based model for the mining documenl  I suppon 11 training I testing 1 IOW 11 18 I 3 15% I1 9 I 2  Table 6. Training and testing time (in sec- onds) with respect to the support threshold for Reuters-21570 dataset  set.

The experimental results show that the association rule- based classifier performs well and its effectiveness is com- parable to most well-known text classifiers. One major ad- vantage of the association rule-based classifier is its rela- tively fast training time. Moreover, the rules generated are understandable and can easily be manually updated or ad- justed if necessary. The maintenance of the classifier is straight forward. In the case of ARC-BC, when new doc- uments are presented for retraining. only the concerned cat- egories are adjusted and the rules could be incrementally updated.

The introduction of the dominance factor 6 allowed multi-class categorization. However, other feature selection techniques, such as latent semantic analysis could improve the results by giving an insight on the discriminative fea- ture among classes. We are working on reducing the num- ber of features, thus better discrimination among classes is expected. Currently the discovered rules consider the pres- ence of terms in documents to categorize. We are studying possibilities to take into account the absence of terms in the classification rules as well.

Table 4. PrecisionlRecall-breakeven point on ten most populated Reuters categories for ARC-BC and     I BEP II ARC-BC with 6 = 50 and rupp=15% 1 [IO] D. Lewis. Niive (bayes) at forty: The independence assump tion in information retrieval. In 10th European Conference on Machine Learning (ECML-98). pages 4-15. 1998.

[ I  I] H. Li and K. Yamanishi. Text classification using esc-based stochastic decision lists. In Xrh ACM Inremarional Confer- ence on Informarion and Knowledge Management(C1KM- 99). pages I22 -130. Kansas City.USA, 1999.

[I21 W. Li, I. Han. and 1. Pei. CMAR: Accurate and efficient c!assi!kation based on multiple class-association rules. In IEEE Inrernarional Conference on Dara Mining (ICDM'OIA San Jose, California. November 29-Dwmber 2 2001.

[I31 B. Liu, W. Hsu, and Y. Ma. Integrating classification and as- sociation rule mining. In ACMlnr. Coni on Knowledge Dis- covery and Dara Mining (SIGKDD'9XA pages 8046,  New York City, NY, August 1998.

[I41 1. Moulinier and J . 4 .  Ganascia. Applying an existing machine learning algorithm to text Categorization. In S.Wemter, E.Riloff. and GScheler. editors, Connecfionisl srarisrica1,and symbolic approaches 10 learning for naru- ral language processing. Springer Verlag, Heidelberg, Ger- many, 1996. Lecture Notes for Computer Science series, number IMO.

[I51 The reuters-21578 text categorization test collection.

http://www.research.att.co1nl~lewis/reuters2 1578.html.

[I61 M. Ruiz and P. Srinivasan. Neural networks for text catego- rization. In 22nd ACM SIGIR /nlernariOMl Conference on Informarion Rerrieval, pages 281-282, Berkeley, CA, USA, August 1999.

[I71 F. Sebastiani. Machine learning in automated text cate- gorization. Technical Report IEI-B4-31-1999, Consiglio Nazionale delle Ricerche, Pisa. Italy, 1999.

[I81 C. M. Tan, Y. F. Wang, and C. D. Lee. The use of bigrams to enhance text categorization. Jour- nal of Information Processing and Management, 2002.

http://www.cs.ucsb.edu/ yfwang/paperdig&m.pif.

[I91 University of California irvine knowledge discovery in databases archive. http://kdd.ics.uci.edd.

[20] Y. Yang. An evaluation of statistical approaches to text cat- egorization. Technical Report CMU-CS-97-127, Camegie mellon University, April 1997.

(211 Y.Yang and X.Liu. A re-examination of text categorization methods. In 22nd ACM Inremarional Conference on Re- search and Developmenr in Informarion Retrieval (SIGIR- 99). pages 42-49, Berkeley.US, 1999.

[22] 0. R. Zaiane and M.-L. Antonie. Classifying text documents by associating terms with text categories. In Thirreenrh Aus- tralasian Darabase Conference (ADC'OZ), pages 215-222, Melbourne. Australia, lanuan, 2002.

