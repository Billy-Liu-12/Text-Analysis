Mining First-order Knowledge Bases for Association Rules

Abstract  Data mining from relational databases has recently become a popular way of discovering hidden knowledge. Methods such as association rules, chi square rules, ratio rules, implication rules, etc. that have been proposed in several contexts o?er complimentary choices in rule induction in this model. Other than inductive and abductive logic programming, research into data mining from knowledge bases has been almost non-existent, because contemporary methods involve inherent procedurality which is di?cult to cast into the declarativity of knowledge base systems. In this paper, we propose a logic-based technique for association rule mining from declarative knowledge which does not rely on procedural concepts such as candidate generation.

This development is signi?cant as this empowers the users with the capability to explore knowledge bases by mining association rules in a declarative and ad hoc fashion.

1 Introduction  In recent years, mining association rules has become a popular way of discovering hidden knowledge from large databases. Most e?orts have focused on developing novel algorithms and data structures to aid e?cient computation of such rules. Despite major e?orts, the computational complexity of even the best known methods remains high. While several e?cient algorithms for association rule mining have been proposed [1, 6, 20, 15, 9, 19, 14, 21], overall e?ciency is still a major issue, specially for other kinds of rule induction such as ratio rules [8] and chi square rules [4].

While many forms of rule inductions are interesting, association rules were found to be appealing because of their simplicity and intuitiveness. In this paradigm, the rule mining process is divided into two distinct steps { discovering frequent item sets and generating rules. There are practical reasons behind this two step process, such as e?ciency and computability. To explain these two issues, we need to briey summarize the concept of association rules.

Let I = fi1; i2; : : : ; img be a set of item identi?ers. Let T be a transaction table such that every tuple in T is a pair, called the transaction, of the form htid; Xi such that tid is a unique transaction ID and X ? I is a set of item identi?ers (or items). A transaction is usually identi?ed by its transaction ID tid, and said to contain the item set X. An association rule is an implication of the form  X ! Y , where X;Y ? I, and X \ Y = ;. Association rules are assigned a support (written as ?) and con?dence (written as ?) measure, and denoted X ! Y h?; ?i. The rule X ! Y has a support ?, denoted sup(X ! Y ), in the transaction table T if ?% of the transactions in T contain X [ Y . In other words, sup(X ! Y ) ? sup(X [ Y ) =  ? = jftjt2T ^X[Y?t[I]gj  jT j , where I ? I is a set of items.

On the other hand, the rule X ! Y is said to have a con?dence ?, denoted con(X ! Y ), in the transaction table T if ?% of the transactions in T that contain X also contains Y . So, the con?dence of a rule is given by  con(X ! Y ) = ? = sup(X[Y ) sup(X)  .

Given a transaction table T , the problem of mining association rules is to generate a set of quadruples R (a table) of the form hX;Y; ?; ?i such that X;Y ? I, X \Y = ;, ? ? ?m, and ? ? ?m, where ?m and ?m are user supplied minimum support and con?dence thresholds, respectively.

The clarity of the de?nitions and the simplicity of the problem is actually deceptive. As mentioned before, to be able to compute the rules R, we must ?rst compute the frequent item sets. A set of itemsX is called a frequent item set if its support ? is greater than the minimum support ?m. Given T , all frequent item sets can be determined by essentially scanning the database. Unfortunately, this process of scanning is by no means easy. Algorithms such as apriori [1] for ?nding frequent item sets make several passes over T . Each pass uses the frequent item sets generated in the previous pass as a seed to generate new frequent item sets.

The problem with such algorithms is that they generate a large set of candidate frequent item sets [1, 6], and then prune the ones that are not likely to become a seed for the next iteration. These candidate sets are usually stored in hash trees, accounting for a signi?cant overhead. For a set of items jIj = n, the number of candidate frequent item sets are about jP(I)j = ?nk=1C  n  k . Essentially, it is expensive to generate and test such a large set of candidate frequent item sets. For example, for a set of items X ? I, and m = jXj, apriori must generate 2m candidate sets, and for m = 100, 2100 ? 1030, which is huge. This is an inherent cost of candidate set generation, no matter which implementation is chosen [6].

2 Properties of Transaction Tables  In this section, we identify some of the basic properties shared by all transaction tables. We explain these     properties using a synthetic transaction table in relational data model [22]. In the next section, we will introduce the ?rst-order knowledge base solution to the association rule mining problem.

Let I be a set of items, P(I) be all possible item sets, T be a set of identi?ers, and ?m be a threshold. Then an item set table S with scheme fTid, Itemsg is given by  S ? T ?P(I)  such that m = jSj. An item set table S is admissible if for every tuple t 2 S, and for every subset s 2 P(t[Items]), there exists a tuple t0 2 S such that s = t0[Items]. In other words, every possible subset of items in a tuple is also a member of S. The frequency table of an admissible item set table S can be obtained as1  F =items Gcount(?)(tidG(S))  which has the scheme fItems, Countg. A frequent item set table Ff is a set of tuples that satis?es the count threshold property as follows.

Ff = ?Count=m??m(F )  Ff satis?es some additional interesting properties. Suppose I = t[Items] is an item set for any tuple t 2 Ff . Then, for any X;Y ? I, there exists t1 and t2 in Ff such that t1[Items] = X, t2[Items] = Y , t1[Count] ? t[Count], t2[Count] ? t[Count], and t[Count] ? min(t1[Count]; t2[Count]). The converse, however, is not true. That is, for any two tuples t1 and t2 in Ff , it is not necessarily true that there exists a tuple t 2 Ff such that t[Items] = t1[Items][t2[Items]. But if such a t exists then the relation t[Count] ? min(t1[Count]; t2[Count]) is always true. Such a relationship is called anti-transitive.

The goal of the ?rst stage of apriori like algorithms has been to generate the frequent item set table described above from a transaction table T . Note that a transaction table, as de?ned above, in reality is not admissible. But the ?rst stage of apriori mimics admissibility by constructing the k item sets at every kth iteration step.

Once the frequent item set table is available, the association rule table R can be computed as2  R = ? Ff1  :Items;Ff2 :ItemsnFf1 :Items;  F f1  :Count  m ;  F f2  :Count  F f1  :Count  (  ?Ff1 :Items?Ff2 :Items(Ff1 ? Ff2))  This expression, however, produces all possible rules, some of which are even redundant. For example, let a !

bh  sab  m ; sab  sa i and ab ! ch  sabc  m ; sabc  sab i be two rules discovered  from Ff , where sX and m represent respectively the frequency of an item set X in the item set table (i.e., t 2 Ff , and t[Count] = sX  m ), and number of transactions in the  item set table S. Then it is also the case that R contains another rule (transitive implication) a ! bch  sabc  m ; sabc  sa i.

Notice that this last rule is a logical consequence of the ?rst two rules that can be derived using the following inference rule, where X;Y; Z ? I are item sets.

X ! Y h sX[Y m  ; sX[Y sX i X [ Y ! Zh sX[Y [Z  m ; sX[Y [Z  sX[Y i  X ! Y [ Zh sX[Y [Z m  ; sX[Y [Z  sX i  1Recall that G is the traditional group by relational algebra  operator, whereas count is the aggregate function as described  in [22].

2Assuming that two copies of Ff are available as Ff1 and Ff2 .

Written di?erently, using only symbols for support (?) and con?dence (?), the inference rule reads as follows.

X ! Y h?1; ?1i X [ Y ! Zh?2; ?2i  X ! Y [ Zh?2; ?1 ? ?2i  We formalize this idea in the following de?nition.

De?nition 2.1 Let X;Y; Z ? I be a set of items. Also let the rules X ! Y h?1; ?1i, X [ Y ! Zh?2; ?2i and X ! Y [ Zh?3; ?3i hold. Then, X ! Y [ Zh?3; ?3i is an anti-transitive rule.

De?nition 2.2 Let r = X ! Y h?; ?i be a rule, and ?m and ?m respectively be the minimum support and con?dence requirements. Then r is redundant if it is derivable from other rules, or if ? < ?m or ? < ?m.

Lemma 2.1 Let ?m and ?m be minimum support and con?dence thresholds respectively, and let the rules X !

Y h?1; ?1i, and X [ Y ! Zh?2; ?2i hold. Then, the rule X ! Y [ Zh?3; ?3i holds such that ?3 = ?2 ? ?m, and ?3 = ?1 ? ?2 ? min(?2; ?1).

Proof Sketch: By showing that X and Y [ Z is an anti- transitive chain and any violation is a contradiction of the inference rule shown above.

Notice that ?3 = ?1 ? ?2 could be less then the con?dence threshold ?m, even though ?1 ? ?m and ?2 ? ?m. In other words, ?1 ? ?m ^ ?2 ? ?m 6) ?1 ? ?2 ? ?m.

Lemma 2.2 Let r = X ! Y h?; ?i be any rule. r is redundant if it is anti-transitive.

Proof Sketch: By showing that there exist at least two other rules that imply r, and then by de?nition, the rule is redundant.

Since in the frequent item set table, every item set is a member of a chain that di?ers by only one element, the following modi?cation for R will compute the rules that satis?es given support and con?dence thresholds and avoids generating all such redundant rules3.

R = ?Conf??m (?r(Ant;Cons;Sup;Conf)(  ? Ff1  :Items;Ff2 :ItemsnFf1 :Items;  F f1  :Count  m ;  F f2  :Count  F f1  :Count  (  ?Ff1 :Items?Ff2 :Items^(jFf2 :Itemsj?jFf1 :Itemsj=1) (  Ff1 ? Ff2))))  2.1 The Challenge  The preceding discussion was aimed to demonstrate that a relational computation of association rules is possible.

However, we used an explicit generation of the power set of the items in I to be able to compute the frequent item set  3? is a relation renaming operator de?ned in [22]. ? changes  the name of a relation and optionally, changes the names of the  attributes in the corresponding columns, the real focus of our  interest. Also notice that this expression is not optimized but  we assume that a relational optimizer can easily identify this  opportunity and execute a suitable optimized expression.

table Ff from the item set table S. This is a huge space overhead, and consequently, imposes substantial computa- tional burden on the method. Furthermore, we required that the item set table S be admissible,another signi?cant restriction on the input transaction table. These are just some of the di?culties faced when a logical characterization of data mining is considered. The procedurality involved acts as the major bottleneck. So, the challenge is to admit any arbitrary transaction table, yet be able to compute the association rules \without" explicit generation of candidate item sets from a ?rst-order knowledge base, and compute the relation R as introduced before using existing logical constructs and machineries.

3 An Illustrative Example from  Point of Sale Transaction  Knowledge bases  The concept of association rules in knowledge bases can be best explained using a so called transaction knowledge base. We present below an example of such a database, and explain on intuitive grounds the implications of its knowledge content with regard to association rules.

Consider the ?rst-order knowledge base T shown in ?gure 1 that represents six transactions over the item set fa, b, c, d, eg.

trans(t1; a): trans(t1; b): trans(t1; c):  trans(t2; a):  trans(t3; a): trans(t3; d):  trans(t4; a):  trans(t5; a): trans(t5; b): trans(t5; c):  trans(t6; b): trans(t6; c): trans(t6; e):  Figure 1: Transaction Database T.

Alternatively, we can think of T as the set of group facts in ?gure 2 once we aggregate the items on transaction numbers. From the group facts, it is easy to see that the frequency of the set fa; b; cg is .33 (33%) as it appears in th of the transactions. Similarly, the frequency of fag  is .33, fa; dg is .16, and of fb; c; eg is .16. A closer look will show that the frequency of the set fag must be an additional .33 and .16 as it is a subset of the set fa; b; cg and fa; dg which appears .33 and .16 times respectively. An interesting but not obvious item set is fb; cg which appears exactly .50 times. Notice that this set is an intersection of the item sets fa; b; cg and fb; c; eg which appears .33 and .16 times respectively, yielding the frequency :33+ :16 ? :50 for fb; cg. These observations and their cumulative e?ects are captured respectively through the set of facts in freq and cand in ?gure 2.

Suppose we are interested in considering only those transactions that contain sets of items shared by a large set of transactions in the knowledge base, i.e., the support threshold. If we take 20% to be our support threshold, that means, we will consider only those item sets that appear in 20% of the transactions in T. We can then make the assertions in large shown below, i.e., essentially the large item sets in T in association rule terminology. From the facts large, the only two association rules we can compute with greater than 40% con?dence is that a ! bc and bc ! a, i.e., a ! bch0:33; 0:40i and bc ! ah0:33; 0:66i,  group(t1; fa; b; cg): freq(fa; b; cg; :33): cand(fa; b; cg; :33):  group(t2; fag): freq(fa; dg; :16): cand(fa; dg; :16):  group(t3; fa; dg): freq(fag; :33): cand(fag; :82):  group(t4; fag): freq(fag; :33): cand(fb; c; eg; :16):  group(t5; fa; b; cg): freq(fag; :16): cand(fb; cg; :50):  group(t6; fb; c; eg): freq(fb; c; eg; :16): freq(fb; cg; :50):  Figure 2: Mining Process from T { generating possible  large item sets.

as shown below too as the facts rules(fag; fb; cg; 0:33; 0:40) and rules(fb; cg; fag; 0:33; 0:66)  large(fa; b; cg; :33): rules(fag; fb; cg; 0:33; 0:40):  large(fag; :82): rules(fb; cg; fag; 0:33; 0:66):  large(fb; cg; :50):  Figure 3: Large item sets and association rules implied  by T corresponding to support and con?dence at least  equal to 20% and 40% respectively.

The above two rules serve as important reminders that X ! Y hs1; c1i, and Y ! Xhs2; c2i 6) c1 = c2, and that X ! Y hs1; c1i, and Y ! Xhs2; c2i ) s1 = s2.

4 Computing Association Rules  from First-order Knowledge bases  The challenge now is to ?nd a logic program that can compute the facts large and consequently the facts rules.

The pioneering work on association rules [1] proposed an iterative method that generates the item sets of cardinality 1 through jIj, where I is the set of all items in K in every pass k over K. These sets are called the candidate item sets Ck, that are checked against K to see if they appear at least ?m (the minimum support threshold) number of times.

Generating and testing these candidate item sets is an immensely expensive and inherently procedural task. Such a procedural method, called the FP-tree, was proposed in [6] which avoids generating candidates by creating a complex data structure.

Our goal here on the other hand is to develop a declar- ative means of computing the association rules without candidate generation so that we can mine large ?rst-order knowledge bases through deductive methods. It turns out that such an abstraction for declarative computation of as- sociation rules already exists. All we require is the avail- ability of aggregate functions. Such functions are available in most logic based languages such as Prolog, and most de- ductive database systems such as LDL [5], CORAL [17], XSB [18] and RelationLog [10].

In ?gure 4 we present the entire program called RULES needed to compute association rules from any knowledge baseK. The predicate trans need not be a stored knowledge base fact. In fact, it can be an intentional predicate itself, de?ned in terms of any extensional predicate. We can verify that the query  rules(X;Y; S; C) will produce the rules presented in the preceding section if we apply the program RULES on the trans facts in ?gure 1 (assuming 20% support     r1 : group(T;< I >) trans(T; I):  r2 : total(count(< T >)) group(T; I):  r3 : inh(I; count(< T >)=N) group(T; I); total(N):  r4 : inh(I1 \ I2; 0) inh(I1; C1); inh(I2; C2); I1 6? I2;  I2 6? I1; I1 \ I2 6= ;:  r5 : freq(I1; C2) inh(I1; C1); inh(I2; C2); I1 ? I2:  r6 : cand(I; sum(< C >)) freq(I;C):  r7 : large(I;C) cand(I;C); C ? ?m:  r8 : rules(X;Y nX;SY ; C) large(X;SX); large(Y;SY );  X ? Y;C = SY =SX ; C ? ?m;:large(Z;SZ);  Z ? Y;X ? Z:  Figure 4: Deductive association rule mining system  RULES.

and 40% con?dence), thereby establishing the correctness of the program on intuitive grounds. Before we present a more formal treatment of the concepts that serve as the backbone of this abstract program, we present an informal explanation of the rules below.

The rule r1 essentially aggregates items in a transaction predicate so that we can refer to all the items pertaining to one single transaction. This will also facilitate set operations involving item sets. The second rule r2 gives us the total number of transactions in the knowledge base so that we can calculate the support of item sets and subsequently, the support and con?dences of generated rules. Rule r3 helps group by item sets in group facts, and helps count the support by computing the proportion of transactions in which a particular item set appears over N number of total transactions. Rule r4 (in conjunction with r3) computes the meet irreducible elements of the trans database item sets with support count zero (see section 8 for the rationale). It does so for only those item sets that are not related by a subset superset relationship. Rule r5 says, if an item set I in inh is a subset of another item set J in inh, then the item set I must inherit the support of J . Rule r6 generates the sum total of all supports corresponding to each item set in K. For any given minimum support threshold ?m, rule r7 computes the large item sets by ?ltering the item sets in freq whose support fall below the threshold. Finally, rule r8 generates all the rules for a given minimum con?dence ?m from the large item sets. Note that this rule generates all the non-redundant rules stipulated by de?nition 2.2.

5 Theoretical Basis of the Program  RULES  In order to appreciate the novelty of the method proposed in the preceding section, we will present its theoretical basis using an example, as a full treatment is not possible within the space constraints. Consider a transaction knowledge base K over a set of items I = fa; b; c; d; eg. In the ensuing discussion and the examples we present, let us assume that for any item set I ? I, and two natural numbers t and c, Itc denotes the fact that I appears in exactly t transactions in K, and that I also appears as a subset of other transactions n number of times such that c = n + t. t is called the transaction count and c is called the total count of item set

I. Any item set I having its total count c at least equal to some support threshold ?m is called a large item set.

Using this method, we can thus represent any transaction knowledge base in a lattice structure L as shown in ?gure 5 representing an arbitrary K.

abcde  abde acde bcdeabceabcd  abc abd acd bcd abe ace bce ade bde cde  ab ac bc ad bd cd ae be ce de  b c d e  null  a   3 0 1 2 2 5 2 3 4 4  6 0 0 3 1 0 2 5 0 0  9 3 5 2 10 0 1 2 37  12 0 8 2 0  11 6 7 10 4 4 6 10 5 6  22 16 23 15 21 12 12 15 10 15  47 51 38 35 28  0 level 1  level 2  level 3  level 4  level 5  level 6  h  Figure 5: Lattice representation of knowledge base K  over item set I = fa; b; c; d; eg.

In this lattice, the top element is the set of all items, I, and the bottom element is the null set. For any item set I, the height of the lattice is h = jIj+1. In our example, it is h = 5+1 = 6. For any node v at a level l in lattice L, (i) v will have a degree l?1 which is equal to the number of items in v, (ii) v will have h? l parents called the degree-1 super sets of v, and ?nally (iii) v will also have l ? 1 children, called the degree-1 subsets of v. Ancestors at a level j of a node v at level l is called a degree-k, k = j ? l, supersets of v in general. Likewise, the descendants are called the degree-k subsets of v. The fact that v is a descendant of u, is denoted by v ? u. Notice that such ancestors (or descendants) always di?er with v by k items.

The nodes in L also satisfy additional interesting prop- erties. For example, a node v at level l di?ers from its child u at level l ? 1 by exactly 1 element, and that u ? v. For any two children u and w of a node v, v = u [ w. For any two nodes utucu and v  tv cc  at any level l, their join is de-  ?ned as (u \ v) tj cj , and the meet as (u [ v)tmcm , such that  cj ? min(cu; cv) and cm ? max(cu; cv). However, we do not de?ne meet and join of nodes u and v across levels (l and l0 such that l 6= l0) although it is possible to do so, and if u \ v = ; even if u and v are at the same level l. For example nodes ab and ce do not have a meet or join.

De?nition 5.1 (Total Count) Let vtc be a node in L at  level l, h be the height of L, and vksup be the set of degree-k supersets of v such that 1 ? k ? (h ? l). Then the total count c of v, is given by c = ?v = t+?  u ts cs 2[h?l  k=1 v k sup  ts.

For example, the total count of abcd is ?abcd = tabcd + tabcde = 3 + 2 = 5, and the total count of acd is ?acd = tacd + tabcd + tacde + tabcde = 0 + 3 + 2 + 2 = 7, and so on.

De?nition 5.2 (Large Item Sets) Let vtc be a node in L, ?v be its total count, ?null be the total number of transactions in K, and ?m be the support threshold. Then vtc is a large item set if  ?v  ?null ? ?m.

In ?gure 5, the nodes below the dotted line, called the l-envelop, are large item sets if we insist that ?m = 0:20. In     contrast, the ones above the line are not. Also note that if a node v is a large item set, all its descendants are too.

Conversely, a node v cannot be a large item set if any of its degree-k subset (descendant) is not. This is referred to as the so called anti-monotonicity property of transaction knowledge bases. Although we are interested in ?nding the l-envelop in a lattice, there may be many nodes below the l-envelop that are redundant because they do not carry any useful information. Our goal is to ?nd the sub lattice under the l-envelop without the redundant nodes.

De?nition 5.3 (Redundant Nodes) Let utucu and v tv cv be  two nodes such that utucu is a degree-1 subset of v tv cv . Then,  the node utucu is redundant in L if cu = cv, i.e., cu is computable solely from cv.

De?nition 5.4 (Transaction and Virtual Nodes) A node vtc in L is called a transaction node if t > 0, i.e., the fact hv; ti is derivable from K. It is called a virtual node other- wise.

In a given lattice L, the nodes below the l-envelop satisfy the following condition. For any node v below l-envelop, either v is a transaction node, or there exists a node u such that v ? u, and u is a transaction node. This observation indicates that we need not compute the entire lattice L to be able to compute the large item sets towards ?nding the association rules. Hence, we de?ne the concepts of a sub- lattice and K-mapping of a knowledge base K as follows.

De?nition 5.5 (Sub-lattice of L) Let L be an item set lattice and S be a subset of all the nodes in L. Then the sub-lattice L(S) of L is the partial lattice involving only the nodes in S which is obtained from L by removing all the nodes v along with their edges such that there exists no u in S for which v ? u holds.

For example, the sub-lattice for the knowledge base in ?gure 1 is shown in ?gure 6 along with the l-envelope corresponding to a support threshold of 20%.

abc bce  ab ac bc ce  b c e  null  a  2 1  0 0 0 0  2 0 0 0 0  2 1  2 2 3 1 0  5 3 3 1 1   be 0  d  ad 11  Figure 6: K-mapping of the knowledge base T of ?gure  1.

De?nition 5.6 (K-Mapping) Let K be a transaction knowledge base over a set of items I, and L be the corresponding item set lattice. Let I be the distinct set of items appearing in K. Then the K-mapping, denoted LK, is the sub-lattice L(I).

Note that the K-mapping of the example T in section 3 is shown in ?gure 6. In this K-mapping, the nodes marked with a solid rectangle are the nodes in T, and the nodes marked with dotted ellipses are redundant. The nodes below the dotted line are the large item sets. Notice that the node bc is a large item set but is not a member of T, while ad and bce are, yet they are not included in the set of large item sets of T. We are assuming here a support threshold of 20%. So, basically, we would like to compute only the nodes abc, bc, and a fromT. The following results guarantee that the proposed computations are correct, intended and su?cient.

Lemma 5.1 Let LK be a K-mapping for a transaction knowledge base. Let i and j be two nodes in LK such that j is a degree-1 subset of node i. Then the total count of node j is always higher than node i if both i and j are transaction nodes, i.e., ?j ? ?i.

Proof Sketch: By showing that j is a subset of i, and that being transaction nodes, j must have non-zero transaction count. So, the transaction count of i + j must be greater than the transaction count of i alone, which by de?nition is the total count of j.

Corollary 5.1 Let LK be a K-mapping for a transaction knowledge base. Let i and j be two nodes in LK such that j is a degree-1 subset of node i. Then node j is redundant if their total counts are identical, i.e., ?i = ?j , and node j is virtual.

Proof Sketch: By showing that non-redundancy is an impossibility.

Corollary 5.2 Let LK be a K-mapping for a transaction knowledge base. Let i and j be two nodes in LK such that j is a degree-1 subset of node i. Then the total count of node j is always at least equal to the total count of i if node j is virtual, i.e., ?j ? ?i.

Proof Sketch: Follows directly from the lemma 5.1.

Corollary 5.3 Let LK be a K-mapping for a transaction knowledge base. Let j be a virtual node in LK. Then, j is non-redundant if j is a least common descendant of any two transaction nodes in LK.

Proof Sketch: This follows from that fact that the total count of j will be higher than the two transaction nodes as it will inherit the counts from these two transaction nodes.

The above lemma and its corollaries suggest that it is safe to further prune theK-mapping of a knowledge base to remove all redundant virtual nodes. Recall that only those virtual nodes for which there exists a parent node that has an identical total count are redundant, and that a virtual node always has a parent. Needless to say, all transaction nodes are non-redundant although some of themmay not be a large item set. The following lemma essentially captures the anti-monotonicity property of frequent item sets in the literature in terms of total counts in our framework.

Lemma 5.2 Let LK be a signature mapping for a trans- action knowledge base. Let i and j1 : : : jk be nodes in LK such that jms are degree-1 supersets of node i. Then ?i ? ?  k  m=1?jm .

Proof Sketch: By showing that the total count of a child node is always at least equal to the parent in the item set lattice, and hence the sum total should also be the same for all parents.

6 Su?ciency and Correctness of  RULES  We now proceed to show that the rules presented in the program RULES are su?cient and correctly compute the large item sets and thus, the association rules. For any transaction knowledge base K, we know that it is su?cient to compute the total counts of all transaction nodes in K.

From lemma 5.1 and corollary 5.1, we also know that we can avoid computing virtual nodes that are redundant, and from corollaries 5.2 and 5.3, we know that we must compute the non-redundant virtual nodes that do not appear in K.

First, we compute the total counts of all transaction nodes as follows.

Through rule r1, we compute the group facts so that we can refer to a transaction as a set of items (instead of a item at a time). The frequency (the proportion, to be exact) of each distinct item v in group, i.e., the parameter t of vtc, is computed through rule r3. This is accomplished by taking a count of transaction IDs for a group of transactions that have exactly the same item sets. Then, rule r4 makes it possible to inherit the transaction counts of superset transaction nodes to a transaction node for which they are supersets to be subsequently summed by rule r6.

Lemma 6.1 For any knowledge base K, the rules r1 through r3, together with rules r5 and r6 compute the total count of all transaction nodes.

Proof Sketch: By showing that the rules actually collect all the subsets (inclusive of the superset node) of a transaction node for which there is another transaction node in K, and carries the superset's transaction count.

This, when grouped together and summed, gives the total count of that node.

Unfortunately, computing the signature counts of trans- action nodes is not su?cient as demonstrated in the exam- ple in ?gure 6 because there may be non-redundant virtual nodes that are of interest to us. This scenario will occur only when, for any pair of transaction nodes, the least com- mon descendant is not a transaction node (e.g. ?gure 6).

Such nodes cannot be redundant by corollary 5.3. Further- more, these two transaction nodes must not be related by degree-k subsets relation. These observations follow from lemma 5.1, and corollaries 5.1 through 5.3.

Since these non-redundant virtual nodes do not appear in the knowledge base K, we must compute them explicitly.

The expression in rule r5 computes the total count of non- redundant virtual nodes by ?nding the intersection of a pair of transaction nodes such that they are not related by the ? relationship.

Lemma 6.2 For any knowledge baseK, the rules r3 and r4 along with r6 compute the total count of all non-redundant virtual nodes.

Proof Sketch: By showing that the expression actually collects all the non-redundant virtual nodes, and then from lemma 6.1 the proof follows.

The following theorem follows immediately.

Theorem 6.1 Let K be a transaction knowledge base and ?m be the minimum support threshold. Then rules r1 through r8 correctly compute all the non-redundant association rules entailed by K.

To prove this theorem, we need yet another theorem (below) that establishes the fact that the set of large item sets is a subset of all transaction nodes and non-redundant virtual nodes in the K-mapping of K, which is actually not too di?cult to prove.

Theorem 6.2 (Su?ciency) LetK be a transaction knowl- edge base and ?m be the minimum support threshold. Then the set of transaction nodes and non-redundant virtual nodes are su?cient to ?nd the non-redundant large item sets of K.

7 Multiset Processing  Though we normally expect the system RULES to work ?ne, it breaks down in most practical cases. This situation can be explained with the example in ?gure 1. Recall that we expect the system to compute the freq facts shown in the leftmost column in the ?gure 7 below from the facts group using rules r3 through r5. Since a bottom-up deductive database will only keep distinct facts (set semantics), it will produce the set shown in the middle column of the ?gure 7. In the process, the system will throw away the boxed fact in the ?rst column, and thus, ultimately produce the boxed cand fact shown in the rightmost column reecting an incorrect computation of support for the item set fag.

freq(fa; b; cg; :33): freq(fa; b; cg; :33): cand(fa; b; cg; :33): freq(fa; dg; :16): freq(fa; dg; :16): cand(fa; dg; :16):  freq(fag; :33): freq(fag; :33): cand(fag; :49):  freq(fag; :33): freq(fag; :16): cand(fb; c; eg; :16):  freq(fag; :16): freq(fb; cg; :50): cand(fb; cg; :50):  freq(fb; c; eg; :16):  freq(fb; cg; :50):  Figure 7: Incorrect execution due to set semantics of  Datalog.

To remedy this system peculiarity, we can modify the RULES system as follows. The critical observation here is that every freq fact contributes towards the support count of item sets and hence, none of them are redundant even though as a predicate they may be identical to other predicates. The issue now is to force the database system to treat facts as multisets (bag) before we start processing rule r5 rather than a set (adopt a multiset semantics - at least simulate it). Since the freq facts are derived from unique transactions in the knowledge base K, we can exploit this fact and utilize a system de?ned or interpreted function to generate a new id every time a freq fact is created, and include it as an argument of freq as shown in ?gure 8.

In this way, we are able to force the inclusion of every derivation as each predicate will be unique (due to the inclusion of a unique id as an argument) and later count their contributions. To accomplish this goal, we modify the     r05 : freq(genid(); I1; C2) inh(I1; C1); inh(I2; C2); I1 ? I2:  r06 : cand(I; sum(< C >)) freq(P; I;C):  Figure 8: Modi?ed rules that simulates multiset  semantics in Datalog.

rules r5 through r6 to obtain rules r 5 through r  6 as shown  in ?gure 8. In these rules, we have used a system de?ned function called genid() that returns a unique identi?er every time it is called.

Notice that the rules r3 and r4 in RULES are the only rules that are recursive and that they are \safe".

Furthermore, it is important that we maintain a set semantics while we complete processing these two rules because we need unique derivations of the meet irreducible elements in inh.

The modi?ed system RULES now behaves as expected and computes the correct support for item sets in any knowledge base K, including our example knowledge base T. It is important to note here that the arti?cial ?x we have proposed above to simulate multiset operation in set based framework through the use of genid() function is not necessary in many systems including CORAL and RelationLog deductive database systems.

For example, CORAL supports multiset relations (bags) through @multiset declaration. Finally, grouping using set valued terms are also allowed in CORAL and RelationLog.

8 Why the System Works  The reader may have noticed that the RULES system did not rely on generating candidate item sets in the way apriori has to. Unlike apriori, it also does not rely on a level wise computation. Instead, it uses a few critical observations that many systems fail to notice4. We summarize below two critical observations that we exploit in our system.

These observations follow from the formal properties of transaction knowledge bases that we have presented in section 5, and section 2.

? Item sets that are large can be computed from the database in two principal ways. Either they appear as transactions in the knowledge base, or they are computable from the transactions as follows:  Item sets in the transaction table that are not related by a subset superset relationship intersect with each other to produce intersection (virtual) nodes in the item set lattice (meets). These intersection nodes in turn intersect until they become meet irreducible elements.

Only a subset of these intersection nodes will be large item sets. These elements can be generated from the knowledge base by computing the least ?xpoint of the pairwise intersection of the elements in the transaction knowledge base. Hence, there is no need to generate  4Zaki, and several others, also have made similar observations  in their work on closed sets and concept lattices. But there are  important di?erences between our observations and the manner  in which we utilize these observations. His observations and  techniques rely on a search based algorithm for CHARM [25]  which is essential in order to compute the so called closed sets,  and thus, have to be completely procedural.

any candidate item sets arti?cially (as the way apriori does).

? All other possible item sets are either not large item sets, or are redundant and can be computed from the other large item sets found in the two types of sets computed as above.

These observations can be intuitively understood from the example below. Consider another knowledge base T' as shown in ?gure 9.

trans(t1; a): trans(t1; b): trans(t1; c):  trans(t2; a): trans(t2; b): trans(t2; d):  trans(t3; d): trans(t3; e):  trans(t4; a): trans(t4; c): trans(t4; d):  trans(t5; a): trans(t5; b): trans(t5; c):  Figure 9: A new knowledge base T'.

Application of rules r1 and r3 will produce the group and inh facts shown in ?gure 10.

group(t1; fa; b; cg): inh(fa; b; cg; :40):  group(t2; fa; b; dg): inh(fa; b; dg; :20):  group(t3; fd; eg): inh(fd; eg; :20):  group(t4; fa; c; dg): inh(fa; c; dg; :20):  group(t5; fa; b; cg):  Figure 10: Execution trace of T'.

Following the conventions of lattice building in previous sections, we construct the K-mapping in ?gure 11 for the item set lattice corresponding to the example knowledge base T' in ?gure 9.

Notice that in ?gure 11, node ab03 is an intersection of nodes abc22 and abd  1, which inherits the transaction count  of all its ancestors (2 from abc and 1 from abd) to record its total count as 3. Notice that its transaction count is still zero, as it is a virtual node (not appearing in T', and because it was created through an intersection). Recall that the total count of a virtual node cannot be less than any of its parents' from which it was created. In fact, it is always higher than its parents' count (refer to lemma 5.2).

de 11ac  acd 11  ad 02  3c  2d  abc 22 abd  0 bd 01 cd 1ab   null  3b  1e  4a  bc 2  acd 11  ad 02  3c  2d  transaction nodes  Intersection of transaction nodes Redundant nodes  Meet irreducible elementl-envelope 40%  l-envelope 60%  Figure 11: K-mapping of the knowledge base T'.

Furthermore, the intersection of the transaction nodes abc22, abd  1 and acd  1 could not cross the level of 2 item  sets (i.e., ab, ac and ad) as intersections always produce the meet, always the largest possible common subset of     the parents. In particular, the intersection of these three nodes cannot yield a04. To produce a  4, we need to take  another round of intersection of the new intersection nodes produced in the ?rst round (from T'). It turns out that  a04 is a meet irreducible element in the K-mapping of T', and hence no further intersection involving a04 is required.

In general, we need to compute the least ?xpoint of the pairwise intersection process to compute all the intersection nodes, and stop only when the set generated at the ?nal stage are all meet irreducible elements.

Such a least ?xpoint computation will only generate nodes ab, ac, ad, a and d (with abd and acd actually in the ?rst round). In particular, the least ?xpoint will never compute the nodes with b, c and bc as shown under the so called l-envelope in ?gure 11. Recall that every node under this envelope is a large item set. Consequently, the l-envelope in this example assumes a 40% support for large item sets. But notice that the node b03 has an identical total count with one of its non-redundant parent, ab03. Hence, b   is a redundant node and thus, not computing or generating this node does not result in the loss of any information (because we can infer b03 from ab  3, in case we need to).

Since we do not have to generate the redundant large item sets, the RULES systems works just ?ne. But if we wish to create all the large item sets similar to apriori, we must add another rule to achieve this goal as we do not explicitly compute them as a view d large. The addition of the following rule which essentially copies the count of a large item set I to all its subsets X if X does not exist as a large item set already, will do the trick.

r9 : d large(X;C) large(I;C); X ? I;:large(X;C2)  There is a subtle issue that we would like to point out here. Consider the K-mapping shown in ?gure 12 corresponding to another knowledge base T" (not shown) from which we have removed all the redundant nodes and shown only the transaction and intersection (virtual) nodes. At a ?rst glance, one may think that it is possible to compute the total count of nodes (or item sets) in a level wise manner, and save time by not redoing certain computations. For example, consider computing the total count of node abc and recall that initially, the node abc will read as abc20 (). Assume that we compute abc  3 from  abc20 and abcd 1 by adding the total count of abc and abcd.

Recall that abc20 reects the fact that abc appears twice in the database, whereas abc23 represents the fact that abc appears twice as a database transaction, and appears once (3-2=1) as a sub item in another transaction, i.e., abcd. Let us assume, for a moment, that we compute the total count of every node in this fashion starting from node abcd in a level wise fashion { compute the total count of each node by adding the total counts of all its parents.

Now for the third level (from the top), to compute the node counts for ac, we add the total count of its parents (3+2) giving 5. But as can be seen from the ?gure count, 5 is not really accurate. This discrepancy resulted because we added total counts of \parents", instead of the transaction counts of \ancestors" to compute the total count of the node ac. Notice that the count corresponding to ac in abcd was accounted for twice in node ac via two distinct branches, as shown. Similarly, if we continue with the same scheme, we will compute a011 for node a instead of a  5 (which in reality  is the correct total count for a).

1ac 1ac  a1a1  a1 a1  a1  abd 12  null  5a  1abcd  abc 23 acd  ab 04 ac 4 ad   Figure 12: K-mapping of a new database T" showing  incorrect inheritance of transaction count if total count  of parents are used to compute total count of lower level  nodes (instead of transaction count).

Our rule system worked correctly because we either inherited the transaction counts in the freq rules from a node that is related via subset-superset relationship, or by ?rst generating the intersection node once initializing the transaction count to zero (rule r4), and using this  intersection node to inherit the transaction counts, which now is in a subset-superset relationship with its ancestors.

Finally, we added the transaction counts with a grouping operation followed by a count operation, which by de?nition is the total count for any node.

8.1 Breaking the Barrier of Procedurality  We would like to highlight here that the three observations we have made early in this section were critical in devel- oping a model theoretic and declarative characterization of the large item set computing process, as it did not depend on procedural concepts such as candidate generation. The observation, that we only need to generate and test the in- tersection nodes, helped us visualize the process as a sort of Cartesian product of the knowledge base with itself, and compare each transaction tuple with the other tuples in the knowledge base and see if they were unrelated by subset- superset relationships. Recall that such pairs are potential contributors to an intersection node. The least ?xpoint of the intersection process helped because we know that we have computed all the meet irreducible elements by now and no other intersection nodes exists.

There are several works that have investigated the issue of declarative association rule mining using SQL [7, 23, 19, 16, 11]. Most of these works, specially [23, 19] attempt to simulate apriori in SQL giving rise to a complicated and awkward method. They do not exploit the inherent declarative properties of transaction databases as we have identi?ed in this paper. The inherent procedurality of their proposed expressions appears to be a major bottleneck.

While it is obviously possible to develop operators that hide the complexity of these expressions, the system nonetheless is awkward, unnatural and procedural which may have e?ciency related drawbacks. Furthermore, by specifying the semantics in procedural terms, they compromise the query optimization aspects of the system. The reason for this loss of opportunity is the fact that the process has already been coded into the declarativity of SQL, and thus database system must now consider only local optimization     of the query expression without having the global view of the intention. There is a big chance that the encoded procedure may not be the best way to compute the rules depending on the database instance. Furthermore, as we understand it, their proposals require potentially large number of name generation for relations and attributes.

The names that are needed are usually database dependent and thus possibly cannot be gathered at query time. An additional process needs to be completed to gather those variables before actual computations can begin5.

9 Optimization Issues  While it was intellectually challenging to develop a declar- ative expression for association rule mining from deductive databases, there are several open issues with great promises for resolution. In the worst case, the least ?xpoint needs to generate n2 tuples in the ?rst pass alone when the database size is n. Theoretically, this can happen only when each transaction in the database produces an intersection node, and when they are not related by subset-superset relation- ship. In the second pass, we need to do n4 computations, and so on. The question now is, can we avoid generat- ing, and perhaps scanning, some of these combinations as they will not lead to useful intersections. For example, the node b03 in ?gure 11 is redundant. A signi?cant di?erence with apriori like systems is that our system generates all the item sets top down (in the lattice) without taking their candidacy as a large item set into consideration. Apriori on the other hand does not generate any node if their sub- sets are not large item sets themselves, and thereby prunes a large set of nodes. Optimization techniques that exploit this so called \anti-monotonicity" property of item set lat- tices similar to apriori could make all the di?erence in our setup. The key issue would be how we push the selection threshold (minimum support) inside the top down compu- tation of the nodes in the lattice in our method.

For the moment, and for the sake of this discussion, let us consider a higher support threshold of 60% for the database T' of ?gure 9. Now the l-envelope will be the one shown in lighter dashed lines in ?gure 11, and the nodes under this line will be the large item sets. Notice that now we have to discard nodes ad20 and d  2 too. This raises the question, is  it possible to utilize the support and con?dence thresholds provided in the query and prune candidates for intersection any further. Ideas similar to magic sets transformation [3, 24] may be borrowed to address this issue. The only problem is that pruning of any node depends on its support count which may come at a later stage. By then all nodes may already have been computed and thus pushing selection conditions inside aggregate operator may become non-trivial.

Special data structures and indexes may also aid in developing faster methods to compute e?cient intersection joins that we have utilized in this paper. We leave these questions as open issues that should be taken up in the future. Fortunately though, there has been a vast body of research in optimizing Datalog programs including recursive programs (such as the one we have used in this paper), and hence, the new questions and research  5Recall that their proposal requires one to express the  mining problem to the system using several queries and update  statements that utilizes information about the database contents  to achieve its functionality.

challenges that this proposal raises for declarative mining may exploit some of these advances.

Needless to emphasize, a declarative method, preferably a formal one, is desirable because once we understand the functioning of the system, we will then be able to select appropriate procedures depending on the instances to compute the semantics of the program which we know is intended once we establish the equivalence of declarative and procedural semantics of the system. Fortunately, we have numerous procedural methods for computing association rules which complement each other in terms of speed and database instances. In fact, that is what declarative systems (or declarativity) buy us { a choice for the most e?cient and accurate processing possible.

10 Conclusion  Our primary goal for this paper has been to demonstrate that mining association rules from any ?rst-order knowl- edge base is possible in a declarative way, without help from any special tools or machinery, and that we can now have a very intuitive and simple program to do so. We have shown that it is indeed possible to mine declarative knowledge by exploiting the existing machinery supported by contempo- rary inference engines in programming languages (e.g., Pro- log) or knowledge base systems (e.g., RelationLog, XSB, LDL, CORAL). All we require is that the engine be able to support set valued terms, grouping, aggregate functions, and set relational operators for comparison, functionalities which most of these systems currently support. We have also demonstrated that our formalism is grounded on a more mathematical foundation with formal properties on which the semantics of the RULES system rely.

We have also raised several open issues related to e?ciency and query optimization which should be our next order of business. As future research, we plan to develop optimization techniques for mining queries that require non-trivial look ahead and pruning techniques in aggregate functions. The developments presented here also have other signi?cant implications. For example, it is now possible to compute chi square rules [4] using the building blocks provided by our system. Declarative computation of chi square rules, to our knowledge, has never been attempted for the many procedural concepts the computation of chi square method relies on. In a separate work [2] we show that the counting method proposed in this paper can be e?ectively utilized to generate the expectations needed in order to compute such rules rather easily. These are some of the issues we plan to address in the near future.

The motivation, importance, and the need for integrating data mining technology with relational databases has been addressed in several articles such as [12, 13]. They convincingly argue that without such integration, data mining technology may not ?nd itself in a viable position in the years to come. To be a successful and feasible tool for the analysis of business data in relational databases, such technology must be made available as part of database engines and as part of its declarative query language.

Our proposal for declarative mining bears merit since it sheds light on how ?rst order databases can be mined in a declarative and procedure independent way so that the optimization issues can be delegated to the underlying database engine.

Once such arguments are accepted, several systems     related issues become prime candidates for immediate attention. For example, traditionally database systems supported declarative querying without the necessity to care about the procedurality of the queries. In this paper, we have actually demonstrated that association rule mining can be viewed as a Datalog query. It is immediate that a direct mapping from the Datalog expressions presented in this paper to SQL can be developed with no problem at all. We can then rely on e?cient database processing of the query in an optimized fashion. Hence, we come close to the essence of the visions expressed by the leading database researchers and practioners [12, 13].

