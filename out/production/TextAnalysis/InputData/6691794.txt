Big Data for Business Managers - Bridging the gap  between Potential and Value

Abstract ? Given the surge of interest in research, publication and application on Big Data over the last few years, the potential of Big Data seems to be well-established now across businesses. However, in most of the business implementations Big Data still seem to be struggling to deliver the promised value (ROI). Such results despite using the market leading Big Data solutions and talented deployment team are forcing the business managers to think what needs to be done differently.

This paper lays down the framework for business managers to understand Big Data processes. Besides providing a business overview of Big Data core components, the paper presents several questions that the managers must ask to assess the effectiveness of their Big Data processes.

This paper is based on the analysis of several Big Data projects that never delivered and comparison against successful ones. The hypothesis is developed based on public information and is proposed as the first step for business managers keen on effectively leveraging Big Data.

Keywords?Big Data, Analytics, Business Managers, Failure, Critical Errors, Potential, Value

I. INTRODUCTION   Under the pressure of economic uncertainty, innovative  disruptions (not just in product/marketing but also in business models) and increasing competition, businesses are aggressively looking for the competitive edge. Big Data provides great potential through data-driven decisions, unprecedented insights, ability to measure and monitor factors that were previously considered abstract (such as customer satisfaction), discovering new sales opportunities and better understanding of profitability across products/customers [1].

However, of the several businesses that have invested in Big Data, a significant section stays aloof of positive returns on their investment. Most often, such failures are attributed to the analytics tool or the IT team.

While the Big Data success stories guide us on how to do it correctly, the not so popular failures also provide valuable lessons ? what does not work? ? and persuades us to analyze the gap between potential and delivering value.



II. BIG DATA DILEMMA   Due to the popularity of Big Data, data is being viewed as  an invaluable asset. Most of the big companies now have some department to process Big Data and provide insights.

Procuring investment for Big Data isn?t so challenging now as it was a few years ago. However, establishing the short-term and long-term value of such investments continues to be a huge  challenge that only a few firms have met. In the absence of long-term vision of firms regarding Big Data, short term benefits are rarely sustainable.

The business managers across the world are facing the quintessential dilemma:  How to ensure that Big Data projects deliver their true value and provide sustainable benefits?



III. BIG DATA PROCESS FOR BUSINESS MANAGERS   In order to resolve the Big Data dilemma, managers need to  understand that Big Data ownership can no more be left merely to statisticians or business intelligence unit. Deriving the maximum value from analytics would need configuring and customizing your analytics implementation to meet your business goals. Business domain understanding and technology solutions need to work hand-in-hand to deliver effective analytics solutions [2].

It is a common practice that business users prefer to think of Analytics as a black box, and that they are better-off leaving "under the hood" details of this "black box" with the IT team.

This gap is often the reason of mismatch between requirement and implementation, and subsequently, a series of quick-fix duct-tape solutions providing temporary relief from a growing disaster. Analytics is not an IT solution to your data problem, rather it is a paradigm to manage your data and transform it into valuable insights.

The next section provides a balanced description of the several steps involved in analytics and what you need to know about them. As a business manager, you do not need to know all the technical details but you do need to understand the key steps and concepts involved in the analytics process.

A good understanding of analytics will help you use the reports generated by analytics in a better way, while providing better directions to analytics team to meet your requirements.

A. LOOKING ?UNDER THE HOOD?  The analytics process is iterative and interactive, consisting of the nine steps as shown in Fig.1. The process starts with determining the Analytics goals and ?ends? with the implementation of knowledge discovered. Then the loop is closed and next iterations start where results from subsequent steps are used to improve the performance of prior steps.

1) Developing an understanding of the application domain: This initial step is primarily a managerial process. It is very important to clearly understand the business context and clearly identify Analytics goals. While some firms are      looking for new information to harness, others are looking for verifying their existing approach against numbers.

Knowing what users really need usually requires that Analytics professionals go beyond what those people actually say. All the later steps would involve some or other trade-offs.

Taking the right decisions for these trade-offs often require a very solid business understanding.

Fig.1 Typical components and process flow of a Big Data  implementation [3]   2) Selecting data and building input dataset: Once you have defined the goals, you need to determine the relevant data that will be required for your analysis. This step may sound deceivingly simple, but remember, ?garbage in, garbage out?. While we rarely get data devoid of any issues such as incompleteness, the right decisions here can strongly mitigate such data issues. If you are setting up your analytics infrastructure, prefer to select only the most relevant data, so that the Analytics processes can be focused and optimized.

3) Preprocessing and cleansing: In this step we resolve as many input-data issues as we can to enhance data reliability and simplify later processes. It includes data cleansing such as handling missing values, identifying and removing noise or outliers, etc. The complexity of this step can vary largely from simple assumptions to building prediction models to best- guessing the missing values in critically required input-data.

4) Transforming data: Data coming from different sources is usually not in a compatible format. So, we transform data to one consistent format that could be easily processed by the later stages. This might include transforming qualitative data into quantitative form and attribute transformation.

5) Data Mining: Data mining is the process of extracting patterns from data. Based on the business goals, suitable data mining algorithms such as classification, regression or clustering are applied on the input data to extract patterns that are generally not obvious. The selection of the specific method to be used for searching patterns is based on trade-offs such as precision versus understandability. Most of the data mining methods are adaptive, i.e. they improve their performance as they process more data. Once a data mining method has been selected, its control parameters need to be adjusted to obtain the best possible results for one?s business situation.

6) Evaluating and interpreting patterns: In this step we evaluate and interpret the mined patterns (rules, reliability, etc.) with respect to the goals defined in the first step. This step focuses on the comprehensibility and usefulness of the Analytics setup.

7) Visualization and feedback: Visualization methods help represent these patterns in an intuitive way for the non- technical audience. Using advanced visualization technology, you can also set up interactive dashboards that dynamically tune the control parameters of Analytics processes and/or help you understand the potential impact of your decisions (predictive analytics).

B. CRITICAL ERRORS ? HOW TO AVOID THEM  With the basic understanding of the key phases of Big Data, business managers need to focus on the critical errors across these phases and how to avoid them.

1) Developing an understanding of the application  domain: People often consider this to be the ?zero step? and do it in a hurry. This is the most critical step, not only from the perspective of goal-setting but also making the right decisions during trade-offs. Thus, ensure that all relevant business parties are involved in this discussion and that the decisions are well communicated to everyone.

It is advised to ensure having answered the following questions clearly before moving to the next step:  ? Is there a consensus on business goals for analytics amongst the parties involved and impacted?

? Have decisions regarding Analytics scope/design/ implementation been taken and well-communicated?

? Do the Analytics goals clearly identify Critical Success Factors(CSFs) and performance metrics?

? Has the learning from any previous Analytics experience (of business unit or company or the industry) been incorporated?

? Have the roles and responsibilities been clearly identified and communicated?

2) Selecting data and building input dataset: When it comes to selecting the right data, people are often possibility- driven rather than being need-driven, i.e. they will consciously or unconsciously prefer data that is easily available rather than focusing on the data most important with respect to business goals. While availability is an important aspect of input data, relevance is way more important than availability. Another common tendency is to non-judiciously prefer quantitative data over qualitative one. Some key data such as consumer sentiments is often qualitative (for example, set of consumer feedback comments) yet very important.

It is also important to understand the inherent limitations of data collection methods. For example, it makes little sense to report the consumer sentiment index calculated from the text mining of feedback forms to be 83.746 out of 100. In such cases, some sort of hit-and-trial testing is required to identify and validate the tolerance (error %) involved in the calculation of this metric. Say the tolerance level here is ?1%, so, the value of the metric should be reported as 84 out of 100.

If multiple data sources are available for the same information, do not yield to the temptation of picking one and ignoring all other data sources. Data often comes with lot of quality issues and using multiple sources for the same information can be of tremendous help in monitoring data quality and using the best data source. Besides, it boosts reliability of the system by incorporating flexibility across multiple input data sources.

3) Preprocessing and cleansing: Casual ignorance at this step often leads to errors that are very hard to troubleshoot at a later stage. For example, it is a common approach to fill the missing values for any field by zeroes. While this may seem obvious but think of what it could mean if the field was let?s say, heart beat rate. A value of zero could mean a dead patient and might raise a false critical alert.

All data has some inherent error. It is important to track this error for all data through all stages. Quite often it is observed that in absence of proper error tracking end-result of Analytics processes are reported in misleadingly high accuracy.

4) Transforming Data: The data coming from multiple sources needs to be merged carefully. Often, the units used across these sources are different, and need to be trasnformed to the same unit. Otherwise, we will end up with an incompatible data set on which no matter how good data mining we do, we will be stuck with meaningless results.

Also, data standards must be set up and followed consistently.

5) Data Mining: Whenever choosing a data mining approach or algorithm, understanding its limitations is equally important as understanding what wonders it can do for you.

These limitations become particularly hard to identify and quantify when your approach is a hybrid of different algorithms, which often seems to be the case.

Since data mining is the first step where you start making sense from data, it might be your first chance to understand the quality of your input data. Common data quality issues include improper sampling and insufficient/incomplete data.

6) Evaluating and interpreting patterns: Data Mining can generate myriad kinds of reports, not all of which will be useful. The important insights are obtained by studying the patterns discovered through data mining. Some patterns such as outliers and seasonality could be easy to spot, where others like correlation and association might need analytical tools.

While reading such graphs and reports, it is important to keep the scale in mind, different metrics may have been reported on different scales.

7) Visualization and feedback: Despite the vast array of visualization technologies, it is often observed that Analytics reports are misinterpreted. This is often because the reports are drafted by Analytics experts with little understanding of the audience ? Business Managers. What might seem obvious to an Analytics expert while looking at a graph might not even cross the mind of a typical Business Manager. Consistent use of standard visualization approaches can help understand dense analytics reports quickly, followed by either suitable business actions or request for further analytical information.

Most common errors in reading analytics visual reports include ignoring the labels, over-estimating the accuracy of data and hasty analysis of multi-dimensional data.



IV. LEVERAGING INSIGHTS TO MAXIMIZE ROI   To some extent, doing Analytics seems to be an obvious thing these days, until you are confronted with the tough question about "Analytics ROI" (Return On Investment). While the abundance of analytics success stories in the market might get you an easy buy for small, isolated Analytics implementation, for an organization-wide roll-out you would definitely need to do a detailed cost-benefit analysis. This analysis is not just helpful for the CFO but also for the Analytics team. When dealing with so much data and multiple complex analytical processes it is easy to get distracted from your business goals.

An "Analytics ROI" framework provides continuous direction as well as performance assessment [4].

It is critically important to note here that "Analytics" will provide opportunities to improve profitability, but will not do it itself. Analytics provides you a better understanding and measurement of your performance. At the end of the day, it is upon you to leverage this insight to improve your bottom-line.

If you fail to leverage these insights properly and promptly, Analytics might rather hurt your bottom-line through the cost of Analytics investment.

Besides, it is also vital to understand that Analytics is an evolutionary paradigm and not a revolutionary one. You learn and improve over iterations. The key is to be swift, flexible and adaptive. Thus, a rapid pursuit of successive short term goals can be very effective in staying focused on identifying and removing errors across the Analytics implementation [5].

SUMMARY   While the need for Big Data in business is clearly evident, business managers continue to struggle to successfully capture value from Big Data initiatives. They need to develop a basic understanding of Big Data processes and core components. It is also vital to understand the critical errors across various phases that are easy to overlook, yet have severe impact on the quality of end-results.

Business managers must understand that no matter which commercial Analytics tool they select, it can rarely be a sustainable competitive advantage. However the way they understand their business and can use the Analytics tools efficiently and effectively will definitely be a sustainable competitive leverage.

