Study on Association Rules Mining Based Chinese Text Representation*

Abstract   In this paper, the problem of text representation in the process of text mining is mainly discussed. The paper focuses on how to simplify the text model in advance of the construction of term-by-document matrix. By using association rules mining method to find the highly correlative words to form words-set, the vocabulary set is decreased effectively, which leads to the text model?s simplification directly. During this process, some incremental update problems of text representation are also introduced. In the end, a simulation case validate that the method is not only efficient but also helpful to the further text clustering.

1. Introduction   For mining large document collections, it is necessary to pre-process the text documents and store the information in a data structure, which, also known as text representation or text modeling is more appropriate for further processing than a plain text file.

Vector Space Model (VSM) is currently the predominant approach and more effective in text structuring. But very often VSM of document text data lead to high dimensional matrix representation. Low- rank factorization methods, such as principal component analysis (PCA) and non-negative matrix factorization (NMF), are good choices to enable user to work with reduced dimensional models, and also facilitate more efficient statistical classification, clustering and organization of data. But all these methods ignore the semantic relationship between different words, which is more important in dealing with text data. So it will be a good idea to focus on the method considering the semantic information adequately with dimension reduction.

Association rules mining [1] can do it. In this paper, we use association rules mining method to mine the  words-set, which is comprised of highly correlative words. Then, using words-sets to substitute the correlative words, we can get a lower dimensional data set, based on which we can construct a simple VSM.

We simplify the problem at the phrase of text presentation. Of course, this method can?t substitute further dimension reduction, but it does simplify the model to a certain extent.

2. Association rules mining   The general association rules are regularly evaluated by support and confidence.The following is a formal statement of association rules [2]: Let I= {i1,i2,?im} be a set of items. Let D be a set of transactions, where each transaction T represent a set of items such that T?I.  Associated with each transaction is a unique identifier, called its TID. A transaction T contains X, a set of items in I, if X?T.

An association rule is a meaning of the form ?X?Y?, where X, Y?I, and X?Y=?. The rule X?Y has a support s in the transaction set D means s% of the transactions in the D contain X?Y. In other words, the support of the rule stands for the probability that X and Y are concurrent among all the possible presented cases. The rule X?Y hold in the transaction set D with confidence c if c% of transactions in D that contain X also contain Y.

Given a set of transactions D, the problem of mining association rules is to generate all association rules that have support and confidence greater that the user-specified minimum support (called minsup ) and minimum confidence (called minconf) respectively.

Association rules mining is normally a two-step process, where in the first step frequent item-sets are discovered and in the second step association rules are derived from the frequent item-sets. Here, the first step is more important, on which plenty of researches are   DOI 10.1109/ICINIS.2008.117       focused. The most popular algorithms are Apriori[2]and Fp-Growth[3].

3. Text representation  3.1. Text preprocessing   Text representation is the basic step of text mining, by which, a text document is represented by a set of words, i.e. a text document is described based on the set of words contained in it (bag-of-words representation). In order to be able to define at least the importance of a word within a given document, usually a vector representation is used, where for each word a numerical ?importance? value is stored.

Chinese text representation is different from English, because in case of Chinese document text, there is no obvious space between Chinese words. So the first step needs word-segmentation. We use the word segmentation software (ICTCLAS), developed by Chinese Academy of Science, to make Chinese word segmentation. Then we get a complete vocabulary set W of document corpus after stop-words removal, W= {w1,w2,?,wm}. Let matrix V be the set to record each word?s occurrence state in each document. Here, Vij is defined as the ith word?s occurrence times in document j.

Different from the traditional VSM, we don?t construct term-by-document matrix immediately, instead, we use association rules mining method to deal with the matrix V first.

3.2. Association rules analysis   In order to use association rules to analyze the word set, we consider the matrix V as a transaction data set.

Just like in the transaction database, a transaction includes many items; here a column denotes a document, many rows denote many words in this document. So we map the matrix V into a transaction matrix T. In order to illustrate this mapping process, a simple example is shown in table 1 and table 20.A problem worthy to be noted is that we must map the records denoting words? more than one times occurrence into multiple records in the new transaction matrix. In other words, all the elements in matrix T are 0 or 1, which represent a word?s occurrence or not in a document respectively. Here, we have the equations (1):   sum(T1+T2)=D1; T3=D2;   (1) sum(T4+T5+T6)=D3.

Table 1. Matrix V recording  each word?s occurrence times in documents   D1 D2 D3  W1 2 0 1 W2 0 1 3 W3 2 0 1 W4 1 1 0   Table 2. Transaction Matrix T recording word?s  occurrence or not in transactions T1 T2 T3 T4 T5 T6  W1 1 1 0 1 0 0 W2 0 0 1 1 1 1 W3 1 1 0 1 0 1 W4 1 1 0 0 0 0   For convenience, we use apriori algorithm [2] to  find the frequent item-sets based on the matrix T. Then we can get the association rules like W1? W2.

Here, the concept of association rule is what keywords will occur simultaneously in a document, the following expression can show the co-occurrence support and confidence values of item ???? (beijing) and item ????? (Olympic): ???? (beijing) ? ?? ? ? ? (Olympic)[support=1.0%,confidence=70%], where support value means that there are 1% of documents having ? ?? ?(beijing) and ? ??? ?(Olympic) concurrently, and confidence value means that there are 70% of documents emerging ??? ?(beijing) emerge ???? ?(Olympic) at the same time.

On condition that the minconf is high enough, we can be sure that the rules we find are effective, and we can merge these items that comprise rules into words- set one by one. Then the vocabulary set W?s dimension will be decreased, and we get the new vocabulary set W?.

3.3?Incremental update   Sometimes we encounter the problem that some new documents are added just after we have constructed the transaction matrix T, when we meet an awkward choice: if we start from scratch, then all the calculations before are wasted; so we name such problem as incremental update problem of constructing transaction matrix T. In this section, we focus on resolving such problem.

With new documents? adding into the data corpus set, the process of incremental update is composed of three steps.

3.3.1. Incremental update of matrix V. When new documents are added, what we need to do first is word segmentation to the new documents. Then we need to analyze if there are new words added to the vocabulary set W. We have two different conditions:  A) No new words added:  In this situation, it?s easier to update the matrix V to V+, and also transaction matrix T to T+. Suppose that the number of added documents is dn, dn columns are added in V+:     11 1 1(n 1) 1(n dn)  1 n m(n+1) m(n dn)  V V V V (2)  V V V V  n  m m  V V + +  +  +  ? ? ? ?? = ? ? ? ? ? ?  ?    B) Some new words added: If there are new words  added into W after new documents adding, the situation becomes a little complicated, which means that matrix V will be extended not only on columns but also rows, just as shown in (3).

11 1 1(n 1) 1(n dn)  1 n m(n+1) m(n dn)  (m+1)(n+1) (m+1)(n dn)  (m+wn)(n+1) (m+wn)(n+dn)  V V V V  V V V V (3)  0       0  V   V  0       0  V V  n  m m V V  + +  ++  +  ? ? ? ? ? ? ? ? ? ?? = ? ? ? ? ? ? ? ? ? ?  ?     3.3.2. Increment update of matrix T. We can map the dn columns in V+ into several columns in transaction matrix T, then we get T+. The process is similar to what above mentioned in section B. All the elements added in matrix T+ are 0 or 1 like other columns in T. And we can also get equations of these dn new columns like (3).

3.3.3. Incremental update of Association Rules generation. After we get the updated transaction matrix T+, the problem has transformed to an incremental update of Association Rules mining. Many different algorithms [6] have been developed for that task. We use the methods in [6] for reference in our process.

3.4. Text presentation   With the simplified vocabulary set W?, using the weighted term-frequency vector to represent each document, we construct the term-by-document matrix X.

11 1   x x X (4)  x x  n  m mn  ? ? ? ?= ? ? ? ? ? ?  ?   The term-frequency vector Xi of document di is defined as  T i 1i 2i miX [x ,x ,...,x ] (5)=  During constructing the matrix X, there are many ways to do the feature selection, but we use the most popular method TF/IDF (term-frequency/inverse document frequency). The specific methods are shown in (8):  ji ji j  nx t log( ) (6) idf  = ?  where tji, idfj, n denote the term frequency of word wj?W in document di, the number of documents containing word fj, and the total number of documents in the corpus, respectively. In addition, Xi is normalized to unit Euclidean length. Here, we use Xi as the ith column.

4. Case study   In order to validate the effectiveness, we select a simple document set as our data corpus: 10 abstracts of Chinese science literature from magazines relative to Computer Science. These literatures are about ???? (security), ????? (database) and ???? (software).

We first deal with the document set by the way above mentioned. The process is shown in Figure 1.

The process?s Input are: document sets, minsup, minconf. Here, the minsup we selected is 1.5%, while the minconf we selected is 70% to ensure the effectiveness of generated association rules. After the whole process, the Output we obtained is term-by- document matrix X.

Results indicate that by using association rules mining before data modeling, the vocabulary set decreased 8.63%, which means that in a large data set with 10000 records, association rules mining help delete 863 words. Thus the matrix V and T are also simplified accordingly. During the process of generating association rules, Some highly correlative words-set like ????-???(computer security), ?? ??-???(database management) were found.

Figure 1. Association rules mining based text representation process  Furthermore, We use the non-negative matrix factorization (NMF) [5] methods to cluster W? and W to validate the effectiveness of the methods. In order to get results more accurately, we do the experiment three times with and without association rules mining process, and use the metric AC [4]defined by  ( )  D / (7) n  i i  AC n? =  = ?  to assess the accuracy of clustering. Where ( )Di?  is set to 1 if Di is classified correctly, and set to 0 otherwise, and n is the number of documents in the collection.

The results are shown in table 3.The last line in the table is the average accuracy values.

From the table we can see that the accuracy of clustering isn?t influenced after the word-merged according to association rules mined, i.e. using association rules mining can simplify the data matrix without decreasing the clustering results. Instead, simplified matrix enhances the efficiency of data analysis and leads to clustering accuracy?s increase to some extent.

Table 3?NMF clustering result with and  without association rules mining   5. Summary and future work   In conclusion, the paper gives a try to simplify the problem of text representation before real feature selection and text modeling. Association rules mining helps to solve such problem well.

During this process of text representation, the incremental update problems of matrix V and transaction matrix T are very interesting directions. We separated these problems into three steps in this paper.

In fact, if we can give these steps an integrated analysis, maybe we can get even better results, which we will focus on for further study.

6. References  [1] Chiang, D.-A. et al., The Chinese text categorization  system with association rule and category priority, Expert Systems with Applications.2007.pp.1-9  [2] Rakesh Agrawal,et al. Mining association rules between sets of items in large databases. In: Proceedings of the Management of Data. Washington DC, 1993.pp.207-216.

[3] Han J W, Pei J, Yin Y W, and Mao R Y. Mining frequent patterns without candidate generation: a frequent-pattern tree approach. Data Mining and Knowledge Discovery.

2004, pp.53-87.

[4] Wei Xu, Xin Liu, Yihong Gong.. Document Clustering Based On Non-negative Matrix Factorization. Proc. of SIGIR?03, Toronto, CA. 2003, pp. 267-273.

[5] Lee D D, Seung H S. Algorithms for non-negative matrix factorization. Advances in Neural Information Processing Systems. vol.13, 2001, pp. 556?562.

[6] Ma, X. L., Tang, S. W., Yang, D. Q., and Du, X. P.

Towards Efficient Re-mining of frequent patterns upon threshold changes. Lecture Notes in Computer Science 2419. Meng X, Su J,Wang Y (eds.). Springer-Verlag.

2002. pp.80-88.

Times Construct X after association rules mining, AC value  Construct X without association rules mining, AC  value 1 85.1% 85.2% 2 83.5% 83.4% 3 86.4% 87.2%  Average 85% 85.27%  Word-segmentation, Construct W  Word occurrence times statistics,  Map the matrix V into transaction matrix T  Incremental update process, construct V  Add new documents  Using Apriori algorithm, Minsup, and minconf  to generate association rules.

Merge words according to association rules to simplify the W,then construct W?.

