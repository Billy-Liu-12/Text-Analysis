Influence Maximization for Big Data Through Entropy Ranking and Min-Cut

Abstract?As Big Data becomes prevalent, the traditional models from Data Mining or Data Analysis, although very efficient, lack the speed necessary to process problems with data sets in the range of million samples. Therefore, the need for designing more efficient and faster algorithms for these new types of problems. Specifically, from the field of social network analysis, we have the influence maximization problem. This is a problem with many possible applications in advertising, marketing, social studies, etc, where we have representations of influences by large scale graphs. Even though, the optimal solution of this problem, the minimum set of graph nodes which can influence a maximum set of nodes, is a NP-Hard problem, it is possible to devise an approximated solution to the problem. In this paper, we have proposed a novel algorithm for influence maximization analysis.

This algorithm consist in two phases: the first one is an entropy based node ranking where entropy ranking is used to determine node importance in a directed weighted influence graph. The second phase computes the minimum cut using a novel metric.

To test the propose algorithm, experiments were performed in several popular data sets to evaluate performance and the seed quality over the influences.

Keywords?Influnce maximization, entropy, minimum cut

I. INTRODUCTION  Large scale data processing analysis has brought us new challenges with respect to algorithmic support. There are sev- eral problems, like finding statistics in data streams, clustering of information in data streams, classification and learning [1], [2], [3], where the need of light and efficient algorithm is necessary. For example, in pattern recognition there are algorithms [4] where the top-k most important patterns are found instead of all of them.

Specifically in the new world of social networks, being able to identify sets of users able to influence the rest of the social network has become highly important and necessary. For example, Influence Maximization (IM) is a classic problem in social analysis, and it has become of great interest in data mining, machine learning and optimization. The original problem was formalized by Domingos and Richardson in [5], and it has the following structure: Given a social network (a directed graph) G, we need to find m nodes (call seeds) who influence the maximum set of network nodes. Because IM is NP-Hard problem [6], most of related works [6], [7], [8], [9] are focused in trying to approximate the optimal solution.

Quality results in the IM problem depends of finding the best seed nodes, and there are two kind of seeds nodes, static and extra seed nodes [7].

Static seed nodes are those which are well know. For example, if we represent a series of twitter accounts, which talk about a famous pop music singer, as a graph. It is possible to identify fan club leaders as static nodes, and the rest of the followers as nodes being influenced by the leaders. The second type of seed nodes are related to nodes who potentially can influence others to change their opinion. These type of nodes are called extra seed nodes. The focus of this work is to try to find these extra seed nodes to define a solution of the IM problem.

Even though, several methods [10] [6] [11] [8] [12] allow to process large amounts of information, they lack of effective techniques for seed discovery. Therefore, it is necessary to have more effective methods to obtain faster and more complex answers. For example, Liu et al. [7] uses a greedy technique based on min-cuts that allows finding the most influential nodes by computing min-cuts for every node to be a potential extra seed node. A drawback of this technique, when applied to a large amount of data, is its combinatorial nature.

IM has been recently re-stated as a Big Data problem for marketing purposing in social networks. For example, traditional greedy techniques based in Kempe et al. [6] were adopted to get faster results using parallelizing algorithms [11].

In another example, Li et al. [12] were facing large datasets problems, for which they decide to introduce the ?conformity? term in the network. This term means ?the inclination of one node to be influenced by others?.

In this work, the proposed algorithm needs to calculate these two indexes, influence and conformity. Then, using these indexes, it gets a series of graph partitions to determine local seed nodes because after all influence and conformity are local properties.

Then, the initial problem is to find the extra seed nodes.

Then, nodes need to be classified in order to determine the extra seed nodes.

A popular method is the use of ranking algorithms [13] for this task. For example, there are traditional ranking algorithms using the degree of the nodes, as the populars PageRank [14] and HITS [15], for web page classification. Another    method [16] uses ranking for labeled directed graph for hyper- linked document ranking. There are other methods, as Dirichlet PageRank [17], designed to avoid cyclical structures to identify e.g. spammer pages in web map.

To process large datasets for IM, we propose a two-phase algorithm and a bounded greedy algorithm in order to reduce computational time and to obtain good approximations to the IM problem. In the first phase, we rank the graph nodes using an entropy ranking [18] to find ?a hidden organizational struc- ture to select interesting prominent members?. This entropy ranking does not take in account the node?s degree which makes this ranking more effective for finding extra seed nodes.

The influence entropy is calculated by removing the node from the graph, and calculating its effect. The second phase uses a well know min-cut in a local fashion to speed up the computational time.

This idea of using the min-cut is inspired by the Ising model [19]. This model was originally used for ferromagnetic material behavior analysis in mechanical statistics [20]. In Social Network Analysis (SNA), the Ising model is used to detect community structures [21]. In this type of problems, the computation of the min-cut for calculating the Ising model leads to a ground state, which is a configuration with a state of low energy. In an equivalent way, in social networks, the ground state is a configuration with low number of conflicts.

Finally, the proposed method utilizes a greedy pruning method taken from IM traditional methods [15] [8] to obtain the extra seed nodes. These extra seed nodes are choose from the most ranked nodes, which are also attractor seeds.

Finally, in order to prove the proposed algorithm, we select a series of datasets to build weighted graphs. These datasets are from twitter trending topics, hashtags, user mentions and retweets to represent the users influence. In addition, we include random graphs for baseline comparison.

The paper is organized as follows. In Section II, we formally explain the problem, and the required background to understand the proposed algorithm. In Section III, we describe the proposed algorithm. In Section IV, we explain how the algorithm was implemented. In Section V, a series of experiments are described, and the results are reported including comparison with other algorithms. In this section, we show the complexity time improvement due to the two phase algorithm and the bounded greedy algorithm. Finally, in the conclusion, we describe some ideas for future improvements.



II. PROBLEM DESCRIPTION AND BACKGROUND  A. Graph Construction  Graphs are a very used technique for data representation, specially in SNA [22] [23]. In our case, the graphs are used to represent influences between social network members. For this, we define a Social Network as a graph G = ?V,E?, in which every user is a graph node (V ), every link (E) between nodes represents an influence. Links are defined as E ? {(i, j) ? V? V : i ?= j }, and we define influence in one way or direction.

In other words, if there is a influence from i to j, there is not necessarily the same influence from j to i. Actually, this is the natural way to form user?s link (followers) in twitter. Matrix weight W represents degrees of influence between nodes, W :  {n}? {n} ? N where n = |V |, Wij means link weight from node i to node j and i ?= j.

For twitter data representation, graphs are used to represent twitter followers where users are nodes and links are follow relationships. For example, if a user a follows user b there are a link from a to b. Follow relationships do not necessary reflects influence because, for example, user a can follow user b but if b never writes a tweet, there is no influence. For influence representation, we can build it using retweets and mention graphs. This means that if user a retweets user?s b tweets, there is an influence from b to a. The same happens for tweet mentions. In this way, the weight matrix Wij is built by summing the number of times user j mentions or retweets user i those ideas are based in McKelvey and Menczer work [22].

Influence graphs are constructed to represent specific users opinion. Given an opinion set ?, every opinion ? ? ? belongs to a node ?i (the node?s i opinion). To fit the notation to the min-cut problem model, we say, if node i has a positive opinion, then, it is represented by ?i ? N+. When it has negative opinion, then ?i ? N? as is used in [7]. For example, we could use an opinion set to indicate if a user knows or does not know a new product.

B. Entropy Based Ranking  The node ranking problem is wildly studied in web search, document classification, citation analysis, anomaly detection, etc., and many of the proposed solutions use graph represen- tation [9], [16], [18].

The most basic algorithm is the Degree Ranking (DG), it was mentioned in Kempe et. al [6] as a simple strategy to find extra seed nodes. In this algorithm, given the degree value of each node, the nodes are sorted by degree values with a expected complexity of O(n log n), if quicksort is used. For example, if we apply this for twitter followers, the node with most in-degree is the most popular node because in-degree represents the number of followers. In-degree is represented by a function indegree(i) and the out-degree is represented by function outdegree(i). In this algorithm, high out-degree values could represent spammers.

Although, most of the ranking methods are based in metrics related with high node degree, this metric does not necessarily implies ?importance?. This can be seen in (Fig. 1), where there are nodes linking two communities, both with low indegree(v) = 3 and low outdegree(v) = 1.

This is the reason why we decide to adopt graph entropy [18]. For this, we are using the entropy definition in [24], which has the following expression (Eq. (1)).

H(G,P ) = min x?SatableSet(G)  n?  v?V pvlog(pv) (1)  This equation involves finding the Stable Set (SS). This set is the node collection L such that there is no edge between L members. It is well know, that finding SS is NP-complete problem. To overcome this problem Shetty et al. [18] proposed to use the equation (Eq. (2)) to compute graph entropy. In    Figure 1. The graph represents twitter user interactions which are formed with hastag #israel. The brown nodes have low in-degrees and out-degrees, but are important because they link two big communities.

Data was taken from Indiana University Truthy project [22].

addition, they also proposed an algorithm to determine node ?effect?. This metric evaluates the graph when a node is present and re-calculates the evaluation when the node has been removed. The node?s effect is calculated as follows:  1) Compute node entropy E(v).

2) Compute graph entropy without node to be evaluate  EN(v).

3) Calculate the effect with effect function (Eq. (3)).

H(G,P ) = n?  v?V pvlog(1/pv) (2)  effect(v) = EN(v)/log(EN(v)/E(v)) (3)  If these three steps are applied ?v ? V , all possible effects are calculated. Then, the nodes can be sorted in descen- dant order using the ranking/effect generated by the entropy.

Furthermore, entropy could be calculated at different levels (l). Here, level number is related to the direct and indirect influences between nodes. For example, l = 1 represents direct influence, i.e. node v0 directly influences node v1. For l > 1, we have indirect influences, i.e. for example, node v0 influences node v1, and this in turn influences v2. This means that there is an indirect influence of level two from v0 to v2.

How the probability graph distribution P is calculated depends on analysis level. For l = n, we need to count every single path of size n for each node v. Probability of node v is equal to all paths of length l, which contain v, divided by the total number of graph paths of length l (?l).

pv0 = ( ?  (v0,v1)?E ? ? ?  ?  (vl?1,vl)?E Wl?1,l)/?l ?v ? G, l > 0  (4)  A final note, it does not make sense to analyze l with large values because influence is lost as you increase level?s value.

C. Min-cuts for Influence Maximization  Min-cuts are mainly used in social network analysis for graph reconstruction [21]. Suppose that we have a directed weighted graph G that represents social network?s influences between users V by edges E, weight matrix W , and we know all members opinion about certain subject s. If all opinions are of two types, positive or negative, then we have two sets, N+ for positive opinion nodes, and N? for negative opinion nodes. In addition, N+ and N? are disjoint sets. Therefore, the min-cut can be defined as a function that:  c(N+, N?) = min (i,j)?N+?N?  ? Wij (5)  Now, suppose we want to reconstruct the graph?s node opinion set, and we know some specific opinions represented by subsets ?+ ? N+ representing positive opinion, and ?? ? N? representing negative opinion. These subsets are called seed nodes, and they have the characteristic that they never change their opinion no matter what. We can use these seed sets and a min-cut algorithm [25] to obtain the original opinion sets, N ?+ and N ??, which are known as source and sink sets. In addition, if we compare N+ and N ?+ subsets obtained from the min-cut algorithm, from the original graph opinions, we can get an error e, defined as the number of nodes in N+ which are misclassified. In other words, they are in N ?? after min-cut. This error e can be reduced, if we increase ?+ size [21].

Then, we can use min-cuts for IM, with a dependable seed sets, to infer the other member opinions [7] in a very accurate manner. This final inference is determined depending on which side of the min-cut the node is. In order to use the min-cut algorithms in the IM problem, we need to specify, in addition to seed sets, the source s and sink t nodes.

The computation of the min-cut is accomplished by using a flow algorithm [25]. For example, if we use the Edmonds- Karp algorithm, the running time is in O(V E2). This is highly recommended for sparse graphs as the ones generated by social networks. Finally, we only need to determine the source and sink nodes. There are two strategies to determine these nodes:  ? The first strategy takes the most representative node belonging to a community. This can be done if we have some knowledge about the graph. For example, in the karate club problem [21], if we do not choose as a source/sink the most representative nodes, and select them randomly, the results may give us a very different result. For this selection, we can use ranking algorithms to determine them, we can select one node with high values in the positive opinion side and one for the negative opinion side.

Figure 2. The first graph represents the source and sink election by high degree in their communities. The second graph represents the extra node placement for source and sink nodes.

? In the second strategy, we create extra source and sink nodes. These nodes do not represent any real graph member, it is only used to have a starting node and a ending node. This technique is used in flow problems (Fig. 2).

Once we have s and t, we need to link them with static seeds nodes, with s ? N+ and t ? N?. For this, we use the following directed weights:  ? Wsv+ = ?, ?v+ ? ?+.

? Wv?t = ?, ?v? ? ??  Where set ? is defined as ? = ?+ ? ??. These rules are from multi-source/multi-sink flow problem. In addition, they are used to not allow N+ nodes does not go to the N? subset.

