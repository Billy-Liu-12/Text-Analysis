Research on Strong-association Rule Based Web Application Vulnerability Detection

Abstract?With the increase of the web applications in information society, web application software security become more and more important. Recent investigations show that web application vulnerabilities have become the largest security threat. Websense security report shows that in the first half of year 2008 above 75% of the most popular web site have utilized by the hackers to run malicious code.

Detecting and solving vulnerability is the effective way to enhance web security. In this paper we focus on the regression test in web vulnerability detection, and present a strong-association rule based algorithm to make the detection more efficient. In the first step we traverse the whole web site to get the web page collection. And then, in the regression test, we make the association between the pages and expand the pages to a collection set. The set will used in the following iterate traverse. And we define the relational grade to describe the association. Finally, we do the experiment on our target web site which contains the known vulnerabilities such as XSS and SQL injection, and the result shows that the algorithm can detect almost all the pages that may contains vulnerabilities in the target web site.

Keywords- Web security testing; Web vulnerability; Strong association Rule

I. INTRODUCTION With the continuous development of web technology,  there?s more and more web site in the internet that have different types and subjects. The attack of the site reports also a common occurrence. The web security became more and more crucial. The current system for web security issues there are two solutions. One of them is detect and control of data in the network to prevent malicious attacks. This technology is known as Intrusion Detection. Firewall is one of the products using intrusion detection technology. Another is to detect the system itself, so that prevent the hacker?s attack. The more perfect the web system is, the more difficult the intruders find the vulnerabilities. The first type of protective measures can easily be broken. So if we want to protect the web system security, we should improve the system itself. This paper discusses the way to improve the security of the web system in this point. The solution for improve the security of the system is scanning and vulnerability-based detection. Using this method, the vulnerabilities which have been found can be more effective detected, but for unknown vulnerabilities is weak. Besides, this way is mainly to find the vulnerabilities in the computer system, not for the web application.

There is an important problems in this method. That is how to find the web pages which have the vulnerabilities.

To solve the first problem, we can analysis the logic relations of the web pages. Web pages contain hyper links?active links?active interactive units, which contain lots of logical relations. These relations is the right thing hackers look for, because logical relation means that there could be data stream between pages.

This paper focuses on the discussion of the vulnerability detection.First, we need to collect the pages in a web application. HITS is the classical algorithm based on hyperlinks to get the important pages of a web site. It distinguishes the importance of links according to weight. But pagerank based on weight has nothing to do with initial parameter selection [6], which will be demonstrated in section 2. This collection we define as Base Set. Using base set, we make the association between the pages in the set. In the section 3, we introduce the strong association rule we use. With the relationship we find, we can get the pages which contains vulnerability. We define these pages as Risk Pages. And then we get the pages which linked by the Risk Pages, and make the association between the new pages and the Risk Pages. The process above is iterative, and can be seen as the regression test for ensure the web application security. In the section 4, we introduce the target web site and discuss the result. In the section 5, we make the conclusion and the future work.



II. OVERVIEW OF HITS ALGORITHM When one page links to another page, it means that  there exists some logical relation between them. Some links from different pages but to one page could display the importance of this page. So lots of link information can be used as an important resource to find content correlation.

HITS algorithm mines the link structure of the Web and discovers the thematically related Web communities [1, 2] that consist of ?authorities? and ?hubs?. Authorities are the central Web pages in the context of particular query topics. For a wide range of topics, the strongest authorities consciously do not link to one another. Thus, they can only be connected by an intermediate layer of relatively anonymous hub pages, which link in a correlated way to a thematically related set of authorities.

These two types of Web pages are extracted by iteration that consists of following two operations.

q q p  p yx  ?  = ? ? q p q  p y x  ?  = ? (1) For a page p, the weight of Xp is updated to be the sum  of Yp over all pages q that link to p: where the notation q?p indicates that q links to p. In a strictly dual fashion, the weight of Yp is updated to be to the sum of Xp.

_____________________________    Therefore, authorities and hubs exhibit what could be called mutually reinforcing relationships: a good hub points to many good authorities, and a good authority is pointed to by many good hubs?

HITS algorithm gives every page two weight: authorities weight and hub weight. Finally it outputs a series of pages with highest weight. Kleinberg [3] demonstrates that this algorithm convergence. Other papers [4], [5] also have description of this algorithm.

Firstly, HITS gets start set which consists of pages through initial URL and then generate base set on basis of it. The base set includes pages pointed by start set and others pointing to start set. The rest of algorithm mainly works in the base set. We define Xp and Yp as non-negative integers for every page in the base set. If the value of Xp is big, the page will be considered a good authority; if the value of Yp is big, the page will be considered a good hub.

We can mark pages by integers like {1,2,?,n} and define their n?n Adjacency Matrix. If Pi points to Pj, then the value in matrix of (i, j) is 1, else 0. Similarly, we define authority weight and hub weight as x=(x1, x2, ?, xn), y=(y1, y2, ?, yn), then the matrix form of formula (1) is:  Tx A y? , y Ax? (2) Stretching out formula (2) we can get:  ( )T T Tx A y A Ax A A x? ? ? , ( )T Ty Ax AA y AA y? ? ? (3)  Therefore, vector x, y can be deduced by formula (3).

According to Linear Algebra theory, Iterative sequences will convergence at Eigenvector by standardization. In other words, hub weight and authority weight are intrinsic property of page. So it has nothing to do with initial vector and selected parameter [6].

Define 1: Base Set is the collection of the web pages which are the result export by HITS algorithm.

Using the Base Set, we can make the association between the web pages, and find the pages which contain vulnerabilities.



III. ASSOCIATION RULE EXTRACTION IN WEB APPLICATION  In client side web page is actually HTML document, which can be got and read by user easily. When we collect all HTML documents of one web application displaying in the client side, all of them constitute document database. HTML document is actually general text plus special tag. These tags are keywords for analyzing HTML document. We can find out some association rules in web pages. This type of analysis collects keywords displaying together and then find their relation.

Admittedly, keywords in HTML should be collected; some keywords in webpage document also should be paid attention such as username, password, search, upload, download etc. Because there is an association between these text and interactive forms corresponding to some keywords. Since main source where hackers get information is information in webpage, it is important to find relationship between web application page and hacker attack.

Association analysis deals with etyma and then removes useless words. After pre-processing, we could find a series of keywords. Finally, association rule [7-9] could be used as extracting high-related rule. In database, every document could be viewed as a transaction, and keywords in document could be viewed as items. So database could be displayed as follows: {document_id, set_of_keywords}  A. Association Rule Association rules mining can he stated as follows: Let  1 2( , ,..., )mI i i i= be a set of items.

Let 1 2( , ,..., ,...., )j nD T T T T= , the task-relevant data, be a set of transactions in a database, where each transaction  jT (j=1?2?? ?n) such that jT I? . Each transaction is assigned an identifier, called TID. Let A he a set of items, a transaction T is said to contain A if and only if A I? . An association rule is an implication of the form A B? where A I? ? B I? and A B? =? . The rule A B? holds in the transaction  set D with support s, where s is the percentage of transactions in D that contain A B (i.e., both A and B).? This is taken to be the probability P(A B). The rule has? confidence c in the transaction set D if c is the percentage of transactions in D containing A that also contain B. This is taken to be the conditional probability, P (B|A). That is, confidence(A->B)= P (B|A)=support(A B)? ? support(A)=c , support(A->B)=P(A B)=s.?  The popular association rules Mining is to mine strong association rules that satisfy the user-specified both minimum support threshold and confidence threshold.

That is, minconfidence and minsupport.

If support(X)?minsupport, X is frequent itemsets.

Frequent k-itemsets is always marked as LK. If support(A->B)?minsupport and confidence(A->B)?minconfidence ? A->B is strong correlation. Several Theorems are introduced as follows:  (1)If A?B?support(A)?support(B).

(2)If A?B and A is non-frequent itemset?then B is  non-frequent itemset.

(3) If A?B and B is frequent itemset?then A is  frequent itemset.

B. Apriori algorithm Apriori algorithm is an influential algorithm for mining  frequent itemsets for Boolean association rules, employs an iterative approach known as a Level-wise search, where k-itemsets are used to explore (k+l)-itemsets. First the set of frequent 1-itemsets is found. This set is denoted L1,then L1 is used to find L2, the set of frequent 2-itemsets, which is used to find L3, and so on, until no more frequent k-itemsets can be found. The finding of each Lk requires one full scan of the database. The problem of mining association rules can be decomposed into two major steps: 1) Find out all frequent itemsets; 2) Use the frequent itemsets to generate the strong rules.

Once all frequent itemsets from transactions in a database D have been found, it is straightforward to generate strong association rules from them, where strong association rules satisfy both minimum support and    minimum confidence. In the meantime, these association rules should satisfy the minimum weighted interestingness.

C. Improved Apriori algorithm APL Based on the Apriori algorithm, an improved algorithm  for association rule mining named APL is proposed.

Based on a strategy of transaction database fuzzy pruning, APL reduces the size of the database and improves the efficiency of the algorithm. It doesn't like the Apriori which fixes a new candidate after each complete database scanning. It can add new candidate at any start point. It dynamically values the support of all the counted itemsets.

If all the subsets of an itemset are frequent, the itemsets will be as a new candidate. Based on the definition of the frequent itemset, if itemset I is not frequent, namely P (I) < minsupport, if we add item A into I, the result itemsets (I U A) can not be more frequent than I, so I U A is not frequent.

Theorem 1: Any item set in Lk must be the super set of a certain item set in Lk-1.

Theorem 2: If a transaction does not contain any item set in Lk-1, then deleting this transaction will not affect the calculation of Lj (j > k)  The conclusion of Theorem 1 and Theorem 2 is that a transaction contains no item set in Lk-1, that is to say when it is 0, the transaction can be deleted from the database. Regarding the "0" here, can it be further changed? If the 0 can be replaced by a bigger number, then it will be faster to prune the database obviously.

Based on the thinking above, we extend theorem 1 and theorem 2, and put forward more theorems below:  Theorem 3: Any item set in Lk must be the super set of k certain item sets in Lk-1.

Theorem 4: If a transaction contains less than k item sets in Lk-1, then deleting this transaction will not affect the calculation of Lj (j > k)  Theorem 3 and 4 guaranteed this conclusion: during every time calculating the support of Lk, if a transaction contains less than k+1 item sets in Lk, mark this transaction a delete and consider no more in the scans later. Compared with theorem 1 and 2, evidently theorem 3 and 4 can delete more invalid transactions.

The improved algorithm, APL deletes more invalid transactions, further reduces the records which will be scanned next time, and improves the efficiency of the algorithm.

D. Strong association in Web Application Vulnerability Detection  From hackers? view, it is their final purpose to find weakness in web application. They will look for useful information continuously and then do attack on purpose by this information.

HITS algorithm outputs a series of high-weight pages by inputting initial URL. Through response from server, these URL requests get pages in the form of document.

Each document is stored in database, which provides data for analysis. By association rule algorithm we can get a frequent itemsets. But its performance is not good enough.

So we combine APL and feedback from database to enhance performance. Through several cycle of optimizing, it is quicker and better. Finally, this strong association will be used to detect vulnerabilities.

Define 2: Risk Page is the page contains the vulnerabilities potentially, and are found by the strong-association rule.

In HTML document there are links, filename, input field?hidden field?selection field of form. We locate these data mainly by HTML tags. For example, find <href> to locate links and find <form> to locate several kinds of field in forms. In all kinds of data, the most important ones are dynamic forms and links.

Algorithm: Use the mediate results of database scanning last time to prune the database. It deletes more transactions, prunes the database with larger scale, and further reduces the time needed by the next scan.

Input: Transaction database D, minimum support threshold minsupport Output: All the frequent item sets in the database D.

L1= {large 1-itemsets} return L1 for(k=2; Lk-1?null; k++){  Ck=Apriori-gen (Lk-1); //the item set that new generated Forall transations t D {?  If (Ldelete=0) { Lt=subset (Lk, t); //the candidate item set  contained in t Forall candidates l Lt? l.count++; If (|Lt |<k+l) {  t.delete=1; //make a delete mark}}} Lk={l k| l.count > minsupport } ?  return L = kLk; //return all the frequent item sets? Define 3: Relational Grade is the measurement to  measure the similarity between the web pages.

Using Relational Grade we can judge a page whether  similar to a Risk Page or not. According to the output of algorithm, it is easy to find out that hackers attack web application vulnerability when interactive. We can deduce strong relation between interactive properties and attack type, which provides support for traversing on purpose.



IV. THE RESULT OF THE RESEARCH To validate the method, we developed a target web site  which contains the known vulnerabilities such as XSS, SQL injection and so on. The result is as figure 2 shows.

In the result, initial url is http://192.168.112.112/test which is the url of the target web site. First, we use HITS algorithm to get the base set. We get the all pages in the system. The whole url list is show in the figure 2. Using the base set, we make the association between the pages, and find some pages contain the vulnerabilities we design in the target site, such as SQL injection. The collection of the web pages is shown in the figure 3. These Risk Pages contains links to other pages, which may contains the same vulnerabilities. So we gather the pages linked by the Risk Page and make the association between them. By this way, we found another pages include SQL injection vulnerabilities. So we expand the collection for the next search and compare. The figure 4 describe the expand collection. In the figure, we can see that almost all the pages contain the SQL injection vulnerabilities in the target web site have been searched by this method. So we can say that this method can make the vulnerability detect more effective and have a higher accuracy.

Figure 1. Traverse Model  Besides to using the target system to validate the method, we also test some web sites in our university.

The result is in the table below.

TABLE I. RESULT FOR UNIVERSITY WEB SITES  Web Sites Vulnerabilities(Amount)  Some College Site SQL Injection (13) Cross Site Scripting (2)  Meal Order Site SQL Injection (3) Organic Chemistry Lab Site  SQL Injection (5) Cross Site Scripting (3)  Movie Online Cross Site Scripting (4) PHPSESSID session fixation (2)  Information Build Site Cross Site Scripting (2) SQL Injection (1)  The results in the table show that using the method we present in this paper, we can find the vulnerabilities hind in the web site.



V. CONCLUSION AND FURTHER WORK This paper presents the research on the strong  association rule based web application vulnerability detection, which improve the efficiency in the vulnerability detection. The method gets a set of web pages by HITS algorithm. This set provides dataset for Apriori algorithm. Strong association between interactive properties could found the pages contain vulnerabilities.

The process is iterative.

Admittedly, it is necessary to design a reasonable way to store data which was got from page documents. How to search easily, how to take less space and how to avoid redundancy is our future work.

AKNOWLEDGMENT  The research work here is sponsored by Tianjin Science and Technology Committee under contract 08ZCKFGX01100 and 06YFJMJC0003.

