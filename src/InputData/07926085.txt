Distributed Learning of Human Mobility Patterns From Cellular Network Data

Abstract?The advent of ubiquitous mobile devices has pro- vided us with an abundant spatio-temporal data source that helps us understand human mobility. The big data generated from mobile devices can be distributed at different locations and it is always infeasible to aggregate the data from multiple data collection centers into one location due to communication and privacy considerations. This paper studies human mobility patterns by learning data-adaptive representations for cellular network data that are distributed across a set of interconnected nodes. It proposes a distributed algorithm, termed cloud NN- K-SVD, for collaboratively learning a sparsifying dictionary (i.e., overcomplete basis) from the data without exchanging data samples between different nodes. The effectiveness of cloud NN- K-SVD is demonstrated through experiments on anonymized Call Detail Records from Columbus, OH.



I. INTRODUCTION  Modeling and understanding human movement patterns is of great importance in a number of fields including urban planning, epidemiology, and telecommunications. In urban planning, the design of transportation networks involves under- standing of traffic volumes on commuting routes [1]. Likewise, the geographic spread of infectious diseases is often dependent on multiscale human mobility [2]. Furthermore, knowledge of the behavior of populations allows companies and govern- ments to optimize mobile networks.

Location information from wireless cellular networks offers opportunities to the study of human mobility. The data used for mobility modeling originate from anonymized Call Detail Records (CDRs) recorded at the spatial resolution of the deployed cell phone towers (or antennas). These mobile phone location measurements are aggregated on internal servers before any further analysis.1 In contrast to other location sources such as GPS, these data are coarse both in space and time [3], leading to impossibility of inferring complete trajectories. The ever-growing ubiquity of cellular phones is pushing network operators toward generation of massive amounts of data. In addition, the data can be geographically distributed across many data centers with limitations on the transfer of raw data due to privacy concerns. It is therefore important to develop efficient approaches for collaborative learning of human mobility patterns in this ?big, distributed data? regime.

1No personally identifiable information (PII) was gathered or used in conducting this study which was in compliance with AT&T?s privacy policy.

Sparse representation over redundant dictionaries has drawn extensive attractions and has been shown to be highly effective for many information processing tasks such as image denois- ing [4], image classification [5], [6], and novel document detection [7]. The basic premise is that natural signals can be well approximated using sparse linear combinations of a few vectors (also called atoms) in some overcomplete basis (or so-called dictionary). Concretely, consider a signal y ? Rm, we say y admits a sparse representation over a dictionary D ? Rm?K consisting of K atoms, if we can approximate y as y ? D? and the number of nonzero entries in ? ? RK is small compared to K. Intuitively, dictionary learning for sparse representation corresponds to learning a union of low- dimensional subspaces underlying data of interest. While data- adaptive dictionary learning dates back more than a decade [8]?[11], many of these works have been focused on learning a dictionary from the data available at a central location. In recent years, the interest of developing efficient algorithms for distributed dictionary learning has increased because of our move toward an era of data explosion [12], [13].

In this paper, we propose a new distributed dictionary learning algorithm, which we refer to as cloud NN-K-SVD, for learning human mobility patterns from anonymized CDRs that are distributed across a number of geographically distant data centers/nodes. The proposed algorithm is a distributed variant of the existing centralized NN-K-SVD algorithm [9] and each node learns a dictionary through computations on its local data and communications with its neighboring nodes. We impose non-negativity on both the dictionary atoms and sparse coef- ficients in our dictionary learning framework, which renders the interpretability of the resulting dictionaries for mobility patterns and facilitates their further analysis by human experts.

Cloud NN-K-SVD can also be regarded as an extension of the cloud K-SVD algorithm [13], and both these works rely on the classical power method for eigenanalysis [14] and consensus averaging [15]. To the best of our knowledge, the work pre- sented here is the first one that explores the use of dictionary learning for characterizing human mobility patterns. In order to demonstrate the validity of our proposed dictionary learning method, we carry out numerical experiments on CDRs from Columbus (OH) area. The results of these experiments show the benefit of collaborative dictionary learning in comparison to local dictionary learning.

Notational Convention: We use non-bold letters to represent scalars/sets, bold lower-case letters to denote vectors, and bold upper-case letters to denote matrices. The i-th element of a vector v is denoted by v(i) and the (i, j)-th element of a matrix A is denoted by ai,j . The i-th column and j-th row of a matrix A are denoted by ai and aj,T , respectively. We use 0 to denote the zero vector of appropriate dimension. Given a set ?, [A]:,? (resp., [v]?) denotes the submatrix of A (resp., subvector of v) corresponding to the columns of A (resp., entries of v) indexed by ?. Superscript (?)T denotes the transpose operation and ? ? ?0 counts the number of nonzero entries in a vector.

Finally, ?v?2 denotes the `2 norm of a vector v and ?A?F denotes the Frobenius norm of a matrix A.



II. PROBLEM FORMULATION  In this paper, we consider a network with P distributed nodes according to an undirected graph G(?, E), where ? = {1, . . . , P} denotes the nodes and E describes links among the nodes with (p, q) ? E if there exists a connection between node p and q. Here, each node corresponds to a data center, which can store hundreds of millions of anonymized CDRs, perform computations independently and exchange informa- tion with its immediate neighbors. We assume the graph G is a connected graph.

In order to characterize the human mobility patterns in a geographic region using cellular network data, we use the locations of cell sites with which a phone is associated as the approximated locations of the phone. We first convert the geolocations (latitude, longitude) of the cell sites into slippy tile indexes at a given zoom level h such that every cell site in the region of interest is associated with one tile2. To facilitate our analysis, we assume that for every anonymized user, all the CDRs associated with this user are collected in one node/data center of the network. We focus on analyzing the patterns of daily travel. Specifically, for every anonymized user whose CDRs are stored in a data center, we create an m-dimensional binary feature vector y ? Rm to indicate the incidence between the user and the slippy tiles in one day (i.e., the vector has ones in the entries where this user has ?passed through? the corresponding slippy tiles and zeros everywhere else), where m is the number of slippy tiles in the region. This feature vector describes the travel path of this user and will be regarded as one data sample in the dictionary learning.

Next, consider a collection of m-dimensional training data distributed across these P nodes, where the p-th node has Np local data samples (which corresponds to Np anonymized users), given by a matrix Yp = [yp,1,yp,2, . . . ,yp,Np ] ? Rm?Np . We can express all the data samples using a single matrix Y = [Y1,Y2, . . . ,YP ] ? Rm?N , where N =?P  p=1Np denotes the total number of samples distributed across these P nodes. The basic premise in this paper is that all the distributed data samples lie near a union of s- dimensional subspaces with s? m. To be specific, assuming the distributed data Y are available at a fusion center, the  2https://wiki.openstreetmap.org/wiki/Slippy map tilenames  problem of learning a (centralized) dictionary can be expressed as the following optimization problem [9]:  (D,?) = arg min D,??0  ?Y ?D??2F s.t. ?i, ??i?0 ? s. (1)  Here, D ? Rm?K is an overcomplete dictionary that consists of K unit `2-norm columns (i.e., K > m) and ? = [?1, . . . ,?N ] ? RK?N is the sparse coefficient matrix. In words, (1) aims to learn a dictionary D such that every path yi can be decomposed into no more than s travel routes, where each route corresponds to an atom in the dictionary D. For maintaining the interpretability of path-route associations, we require both D and ? to be non-negative. This problem is non-convex in (D,?) and a natural approach for solving it is to alternate between solving (1) for D using a fixed ? and then solving (1) for ? using a fixed D [9].

However, due to the extremely high storage and commu- nication costs of big data and privacy concerns, it is always prohibitive to gather the distributed data Y to a central loca- tion. In this regard, we are interested in learning a collection of dictionaries {Dp}p?? through local computations and com- munications within the network such that the performance of these collaborative dictionaries approximates the performance of a dictionary D learned from Y in a centralized manner.

In the following section, we present a decentralized variant of the dictionary learning method proposed in [9].



III. PROPOSED ALGORITHM  In this section, we first review the centralized NN-K-SVD algorithm [9], which is followed by our proposed distributed dictionary learning algorithm, termed cloud NN-K-SVD.

A. Review of Centralized NN-K-SVD Algorithm  The NN-K-SVD algorithm starts with a random dictionary D, and solves (1) by iterating between sparse coding step and dictionary update stage [9]. Specifically, when the dictionary D is fixed, the sparse coding amounts to solving ? as follows:  ?i, ?i = arg min ??RK ,??0  ?yi ?D??22 s.t. ???0 ? s. (2)  This problem can be solved by either convexifying (2) [16] or using greedy algorithms [17]. Afterward, given a fixed ?, the dictionary update step in NN-K-SVD involves sequentially updating one atom dk, k = 1, . . . ,K, at a time, while keeping all other atoms in the dictionary fixed. In order to update dk, we first compute the error matrix Ek = Y ?  ? ` 6=k d`?`,T ,  and let ?k = {i : 1 ? i ? N, ?k,T (i) 6= 0}. Then the problem of updating dk can be expressed as the following positive rank-one optimization problem:  (dk, [?k,T ] T ?k  ) = arg min u,v?0,?u?22=1  ?E?k ? uvT ?2F , (3)  where E?k = [Ek]:,?k is the reduced error matrix by keeping the columns of Ek indexed by ?k only. In order to find starting points for u and v, we apply singular value decomposition (SVD) on E?k and then cancel out the negative entries. More precisely, we initialize u and v by setting u = a and v = ?b,    where a and b are the dominant left and right singular vectors of E?k, respectively, while ? denotes the largest singular value of E?k. In order to make both u and v positive, we set the negative entries of u and v to be zeros by performing the following operations: u = u? [u ? 0] and v = v ? [v ? 0], where ? denotes the element-wise product between two vec- tors. After this, we adopt an alternate minimization approach [18], which involves iteratively updating u for a fixed v using (4) and then updating v for a fixed u using (5) [9]:  u = E?kv  vTv , u = u? [u ? 0], (4)  v = E?Tk u  uTu , v = v ? [v ? 0]. (5)  This process is repeated until convergence is achieved. Finally, we set dk = u?u?2 and [?k,T ]?k = ?u?2v  T . After finishing the update of all the K atoms in D, NN-K-SVD [9] then moves to the sparse coding step and this two-stage iterative process continues until a stopping criterion is satisfied.

B. Distributed Dictionary Learning Using Cloud NN-K-SVD  We now introduce our proposed distributed dictionary learn- ing algorithm based on NN-K-SVD. Similar to the centralized NN-K-SVD, our distributed algorithm starts with a common random dictionary Dinit for all the nodes. In the sparse coding stage, we propose that each node p computes the sparse coefficients of its local data Yp using the local dictionary Dp without collaborating with other nodes by solving  ?i, ?p,i = arg min ??RK ,??0  ?yp,i ?Dp??22 s.t. ???0 ? s, (6)  where yp,i and ?p,i denote the i-th sample and its correspond- ing coefficient vector at node p, respectively. In this paper, we propose to use Nonnegative Matching Pursuit (NMP) [17] to solve (6) because of its greedy nature.

The main challenge in our distributed dictionary learn- ing problem lies in the update of the dictionary atoms. As described in Section III-A, in order to update dk in the centralized setting, we need to compute the dominant left and right singular vectors of the reduced error matrix E?k. However, in the distributed scenario, this error matrix is distributed across the network and each node p has its own reduced error matrix E?p,k computed from its local data. Mathematically speaking, we can express the distributed error matrix as E?k = [E?1,k, E?2,k, . . . , E?P,k], where E?p,k = [Ep,k]:,?p,k with Ep,k = Yp ?  ? ` 6=k dp,`?p,`,T and ?p,k = {i : 1 ? i ?  Np, ?p,k,T (i) 6= 0}. Here, dp,k denotes the k-th atom of the dictionary Dp and ?p,k,T denotes the k-th row of the sparse coefficient matrix ?p at node p. In order to estimate the dominant left singular vector of E?k (again denoted by a) over the network, we perform distributed power method described in [13] by using the distributed error matrices E?p,k?s, and we denote the collection of the estimates of a at different nodes by {up}Pp=1. We omit the details here in the interest of space.

We again use ? and b to denote the largest singular value and dominant right singular vector of E?k, respectively. Note that  Algorithm 1 Cloud NN-K-SVD for dictionary learning Input: Distributed data Y1,Y2, . . . ,YP , parameters K and s, and a doubly-stochastic matrix W.

Initialize: Generate a random dictionary Dinit ? Rm?K and set Dp = Dinit, p = 1, . . . , P .

1: while stopping rule do 2: The p-th site locally solves (Sparse Coding) ?i, ?p,i = arg min??0 ?yp,i ?Dp??22 s.t. ???0 ? s.

3: for k = 1 to K (Dictionary Update) do 4: ?p, Ep,k = Yp?  ? ` 6=k dp,`?p,`,T , ?p,k = {i : 1 ? i ?  Np, ?p,k,T (i) 6= 0} and E?p,k = [Ep,k]:,?p,k .

5: Fp = E?p,kE?  T p,k.

6: Generate zinit randomly, set tb = 0 and ?p, z(0)p = zinit.

7: while stopping rule (Distributed Power Method) do 8: tb = tb + 1, tc = 0 and ?p, ?(0)p = Fpz(tb?1)p .

9: while stopping rule (Consensus Averaging) do  10: tc = tc + 1, ?p, ?(tc)p = ?  q?Np wp,q? (tc?1) q .

11: end while 12: ?p, z(tb)p = ?(tc)p /??(tc)p ?2.

13: end while 14: ?p, up = z(tb)p and vp = E?Tp,kup.

15: ?p, up = up ? [up ? 0], vp = vp ? [vp ? 0].

16: while stopping rule do 17: ?p, cp = E?p,kvp and rp = vTp vp.

18: tc = 0 and ?p, ?(0)p = cp and ?  (0) p = rp.

19: while stopping rule (Consensus Averaging) do 20: tc = tc + 1, ?p, ?(tc)p =  ? q?Np wp,q?  (tc?1) q and  ? (tc) p =  ? q?Np wp,q?  (tc?1) q .

21: end while 22: ?p, up = ?(tc)p /?  (tc) p , up = up ? [up ? 0].

23: ?p, vp = E?Tp,kup uTp up  , vp = vp ? [vp ? 0].

24: end while 25: ?p, dp,k = up?up?2 and [?p,k,T ]?p,k = ?up?2v  T p .

26: end for 27: end while Output: A collection of dictionaries {Dp ? Rm?K}Pp=1.

E?Tk a = [E?1,k, E?2,k, . . . , E?P,k] Ta = ?b and we can write the  variable v in (3) as vT = [vT1 ,v T 2 , . . . ,v  T P ] in the distributed  setting, where vp is the variable corresponding to the reduced coefficient vector [?p,k,T ]T?p,k . It then follows that once we have up?s available at different nodes, we can initialize vp?s by setting vp = E?Tp,kup. In order to make the initial up?s and vp?s positive, we again perform the following operations:  ?p, up = up ? [up ? 0], vp = vp ? [vp ? 0]. (7)  Once we have the initial vectors up?s and vp?s, we move onto the stage where we iteratively update up?s and vp?s using alternate minimization. Notice that in the distributed setting,  we can write the first equation in (4) as u = ?P  p=1 E?p,kvp?P p=1 v  T p vp  . In  this manner, we first compute each cp = E?p,kvp and rp = vTp vp locally, and then make use of the consensus averaging    (a) (b) (c)  Fig. 1. Comparison between centralized K-SVD and NN-K-SVD for learning the human mobility patterns. (a) represents a test vector/path on May 7, 2016.

The associated atoms of this test vector for K-SVD and NN-K-SVD are shown in (b) and (c), respectively.

(a) (b)  (c)  Fig. 2. Comparison between centralized K-SVD and NN-K-SVD for learning the human mobility patterns. (a) represents a test vector/path on May 12, 2016.

The associated atoms of this test vector for K-SVD and NN-K-SVD are shown in (b) and (c), respectively.

method [15] to compute the summation of these individual vectors/scalars over the network. To be specific, we first design a doubly-stochastic matrix W according to the topology of the network graph G [15]. Next, each node is initialized with a vector ?(0)p = cp (we also set ?  (0) p = rp, since the computation  of ?P  p=1 rp has the same principle, we focus on describing the procedure of computing  ?P p=1 cp here) and we define ?  (0) =  [? (0) 1 , . . . ,?  (0) P ]  T ? RP?m. We also let Np = {q : (p, q) ? E} be the set of neighbors of node p. In each iteration, each node updates its local vector through the communications with its neighbors as follows: ?(tc)p =  ? q?Np wp,q?  (tc?1) q , where tc  denotes the iteration number. This iteration can be written in vector form as ?(tc) = Wtc?(0) and it has been shown in [15] that limtc???  (tc) p = (?  (0))T1/P for all p?s, where 1 ? RP is a vector of all ones. But in practice, we can only perform a finite number of iterations for consensus averaging, which we denote by Tc, and the averaging result gets better as Tc grows.

In our experiments, the consensus iterations Tc is set to be 10. After finishing consensus iterations for both cp?s and rp?s, each node p then updates up by setting up = ?(Tc)p /?  (Tc) p  and up = up ? [up ? 0]. Once we have the updated up?s available at different nodes, we then simply update vp?s locally  as vp = E?Tp,kup uTp up  and vp = vp ? [vp ? 0]. The update of up?s and vp?s is repeated until a convergence criteria is met.

Finally, we set dp,k =  up ?up?2 and [?p,k,T ]?p,k = ?up?2v  T p .

This concludes our discussion of the dictionary update step.

A complete description of the resulting algorithm, which we term cloud NN-K-SVD, is given in Algorithm 1.



IV. EXPERIMENTAL RESULTS In this section, we present the results of human mobility  characterization using centralized/distributed dictionary learn- ing. We focus on analyzing the records of Columbus, OH and we work with three days of data: May 7, 2016 (Saturday), May 11, 2016 (Wednesday), and May 12, 2016 (Thursday).

These mobile phone location measurements are anonymized and the locations are discretized to a grid of slippy tiles, each corresponds to a 600m ? 600m area. All the analysis is conducted on internal AT&T servers. There are 175 slippy tiles/grids in the region of interest. We exclude the users who ?passed through? less than 20 slippy tiles on weekdays and 9 slippy tiles on weekends since there is little to no mobility of these users. This leaves us with roughly 16000 anonymous users for analysis in each of these three days. Afterwards, we create m = 175-dimensional binary feature vectors for all the these users as described in Section II and these vectors are used in the experiments. From all the retained samples, we randomly select 12000 samples for training and the remaining samples (roughly 4000 samples) are used for testing purposes.

This random selection is repeated five times. All the training samples are normalized to have unit `2-norms.

We first examine the performance of learning the mobil- ity patterns using centralized dictionary learning methods.

We perform centralized K-SVD/NN-K-SVD with parameters: m = 175, N = 12000, K = 200, and the sparsity level s is set to be 3 for the data on weekends and 4 for the data on weekdays. Then for a test sample y, we apply Orthogonal Matching Pursuit (OMP) [19] and NMP [17] to compute its sparse representation coefficient vector ? in terms of the    TABLE I RELATIVE REPRESENTATION ERRORS OF TEST DATA FOR DIFFERENT DICTIONARY LEARNING APPROACHES  centralized K-SVD local K-SVD cloud K-SVD centralized NN-K-SVD local NN-K-SVD cloud NN-K-SVD May 7 0.240 0.301 0.283 0.237 0.274 0.238 May 11 0.252 0.325 0.277 0.246 0.279 0.246 May 12 0.252 0.326 0.278 0.247 0.280 0.247  (a) (b) (c)  (d) (e)  Fig. 3. Comparison between local NN-K-SVD and cloud NN-K-SVD for learning the human mobility patterns. (a) represents a test vector/path on May 7, 2016. The associated atoms of this test vector for local NN-K-SVD at node 4 and node 8 are shown in (b) and (c), respectively. The associated atoms of this test vector for cloud NN-K-SVD at node 4 and node 8 are shown in (d) and (e), respectively.

learned dictionary by K-SVD and NN-K-SVD, respectively.

The relative representation error with respect to y can be defined as e = ?y?D??   ?y?22 , and we use the mean of relative  representation errors of test data to quantitatively evaluate the performance of dictionary learning. As shown in Table I, the relative error of test data using the learned dictionary by NN- K-SVD is slightly less than the one computed using the learned dictionary by K-SVD for all the three days. The visualization of a test path on May 7 and its associated routes/atoms for K-SVD/NN-K-SVD with s = 3 is represented in Fig. 1.

The blue dots in the figures correspond to the center location of the slippy tiles, and the dots with red cross in Fig. 1(a) correspond to the ?active? slippy tiles of this test path and the dots with red cross in each of the plots in Fig. 1(b) and Fig. 1(c) correspond to the slippy tiles whose respective entries in the associated atoms have absolute values greater than 0.1. By comparing Fig. 1(b) with Fig. 1(c), we can see the third associated atom for K-SVD has only few ?active? slippy tiles and has large overlap with the first associated one, whereas the three associated atoms for NN-K-SVD do not have much overlap. The main reason for the overlap between the associated atoms is that K-SVD allows negative dictionary atoms and sparse coefficients, and some learned atoms are only used to reduce the representation error without interpretability.

We also show a test path on May 12 and its associated routes/atoms with s = 4 in Fig. 2, from which we observe that the third associated atom for K-SVD has large overlap with the first associated one. These results confirm the superiority of applying nonnegative dictionary learning methods for learning the human mobility patterns.

Next, we study the performance of distributed dictionary learning on human mobility data. For each set of training and test data, we randomly generate a network with P = 10  different nodes using an Erdo?s-Re?nyi graph with probability 0.5, and each node has 1200 training samples. The generation of the network is also repeated five times. We perform both cloud NN-K-SVD and cloud K-SVD [13] for collaborative dic- tionary learning, in contrast with localized dictionary learning approaches, where each node learns a local dictionary from its local data using K-SVD and NN-K-SVD (which we term local K-SVD and local NN-K-SVD, respectively). For a test sample y, we compute its relative representation error at node p using the dictionary Dp as ep =  ?y?Dp?p?22 ?y?22  , where ?p is computed using OMP for cloud/local K-SVD and NMP for cloud/local NN-K-SVD, respectively. Table I summarizes the mean of the relative representation errors of the test data for all the nodes, from which we provide the evidence that cloud NN-K-SVD outperforms the local NN-K-SVD in terms of smaller relative errors of the test data, and the performance of cloud NN-K-SVD is very close to the one for centralized NN-K-SVD. In order to further compare cloud NN-K-SVD with local NN-K-SVD, we again show some test examples and their associated atoms for cloud NN-K-SVD and local NN-K-SVD at some nodes in Fig. 3 and Fig. 4. As can be seen from Fig. 3, the first and the third associated atom for local NN-K-SVD at node 4 (Fig. 3(b)) look similar to the first and the second associated atom at node 8 (Fig. 3(c)), respectively. However, the second associated atom for local NN-K-SVD at node 4 is different from the third associated atom at node 8. In contrast, it can be inferred from Fig. 3(d) and Fig. 3(e) that the associated atoms for cloud NN-K- SVD at node 4 and 8 are very similar and the consistency is another main advantage of using collaborative dictionary learning. We can also observe a similar phenomenon in Fig. 4.

Furthermore, some atoms learned by local NN-K-SVD do not have continuous patterns (e.g., the second associated atom at    (a) (b)  (c)  (d)  (e)  Fig. 4. Comparison between local NN-K-SVD and cloud NN-K-SVD for learning the human mobility patterns. (a) represents a test vector/path on May 11, 2016. The associated atoms of this test vector for local NN-K-SVD at node 3 and node 6 are shown in (b) and (c), respectively. The associated atoms of this test vector for cloud NN-K-SVD at node 3 and node 6 are shown in (d) and (e), respectively.

node 6 in Fig. 4(c)) and these atoms are difficult to interpret.

Therefore, our proposed method outperforms the local NN-K- SVD in learning the human mobility patterns.



V. CONCLUSION  In this paper, we have proposed a new distributed dictionary learning algorithm, termed cloud NN-K-SVD, for collabo- ratively learning the dictionary from massive data that are distributed across interconnected nodes in the network. The efficacy of the proposed algorithm is demonstrated through numerical experiments on anonymized Call Detail Records with an application of human mobility characterization.

