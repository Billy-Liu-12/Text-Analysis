Classification of Pancreas Tumor Dataset Using  Adaptive Weighted k Nearest Neighbor Algorithm

Abstract?k nearest neighbor algorithm is a widely used classifier.  It benefits from distances among features to classify the data. Classifiers based on distance metrics are affected from irrelevant or redundant features. Especially, it is valid for big datasets. So, some of features can be weighted with higher coefficients to reduce the effect of irrelevant or redundant features. We suggest adaptive weighted k nearest neighbor algorithm to increase classification accuracy. This algorithm uses t test which is one of the feature selection to weight features.

Classification accuracy is increased from 74.14% to 86.57% for k=3 neighbors and Euclidean distance metric thanks to the proposed method.

Keywords?t test, Euclidean distance, Manhattan distance, classifier, weighted k nearest neighbor

I. INTRODUCTION Training process is one of the fundamental problems in data  classification [1]. The aim of the training process is to obtain higher classification accuracies in the short time.

Classifying algorithms based on distances are simple and effective methods. These algorithms classify datasets by examining distances among features. Neighbor distances among samples are considered to classify a test sample.  Each feature is evaluated by equal importance to find nearest neighbor [2].  However, this situation is not practically valid, because each feature does not have same importance on dataset. For this reason, different information about features should be evaluated for classifiers based on distance metrics.

It is very important to decide to discriminatory powers of features. While features which have high discrimination values can increase the success rate of classification, features with lower discrimination values can adversely affect the success.

Therefore features are weighted depending on their discriminatory powers [3].

Classifiers based on distance metrics are also negatively affected on high dimensional dataset [4].

The rest of this paper is organized as follows. In section II, we summarize some of the studies about weighted k nearest neighbor. k nearest neighbor, t test and the proposed method are explained in section III. In section IV, experimental results of weighted k nearest neighbor are reported.



II. RELATED WORKS k nearest neighbor is a simple and old algorithm. It is  affected from distance metrics. Basic k nearest neighbor does not examine discriminatory power of features. All features have equal importance. Researchers who study about features? importance rank suggested weighted k nearest neighbor method. Garcia et al. [5] suggested weighted k nearest neighbor method based on mutual information. In this study, performance of classifier was improved with mutual information. Researchers imputed missing data on benchmark datasets thanks to this approach.

Pan et al. [6] suggested a new weighted classifier method to increase classification accuracy for microarray gene expression data. The suggested method uses the combination of a k nearest neighbor based on weighted majority vote and a local support vector machine approach. Features are weighted by genetic algorithm to find the best weights for kNN. Also, they used EIN-ring neighborhood search to improve performance of genetic algorithm. kNN may not work well in narrow problems. Because of that, they suggested local support vector machines. Instead of nonlinear programming search, they used piecewise segment hyper plane based on local support vectors to obtain maximum decision line. The effect of noises was decreased 10% thanks to the proposed method. Parades et al.

[7] developed a new weighted dissimilarity measure. The aim of algorithm is to improve performance of k nearest neighbor classifier. Class-dependent weighting (CDW) benefited from fractional programming to identify the best weights. Dataset which are obtained from the UCI Repository of Machine Learning Databases are used to evaluate classification accuracy of CDW. The satisfactory results are achieved in 10 of 13 dataset compared to unweighted and weighted k nearest neighbor methods. Ling et al. [8] offered a novel method depending on weighted features to achieve higher performance of classifier. They used Support Vector Regression to compute the weights. Missing data are imputed with weighted k-nearest neighbor algorithm. It is tested from three evaluation metrics; Mean Square Error, Mean Absolute Error and Mean Absolute Percentage Error. This approach decreased error rate for these three metrics.

Tan [9] suggested weighted k nearest neighbor algorithm for unbalanced text corpus. The algorithm used Term     Frequency ? Inverse Document Frequency to find weight coefficients of features. While the algorithm assigns a bigger weight for neighbors from small classes, it assigns a smaller weight for neighbors contained in large category. F1 measure is used to evaluate performance of classifier. F1 measure benefited from values of recall and precision. The proposed method yields better performance compared to basic k nearest neighbor method. Liao et al. [10] suggested two weighting techniques; frequency weighting and term frequency ? inverse document frequency. These techniques are used for intrusion detection. Known and novel attacks are tested in DARPA dataset and success rate was found as 91.7%. Liu et al. [11] suggested class confidence based weighted (CCW) kNN algorithms for imbalanced datasets. The multiplicative-inverse and the additive-inverse were used to calculate distance weight of the algorithm. However, these methods are not enough alone for imbalanced datasets. For this reason, authors applied class confidence weighted to achieve higher classification accuracy.

They benefited from mixture modeling and Bayesian Networks to identify CCW weights. Classifier performance is evaluated on 31 data sets and compared to other kNN algorithms.



III. METHODS  A. k Nearest Neighbor k Nearest Neighbor is a simple and effective classifier. It  consists of two stages; training and test. It examines distance from each test sample to training samples to classify each test sample. In this situation, there are two important parameters; to identify k value and distance metric. It must be selected odd k values for dataset which has even classes. Distance metric is other important parameter. There are various distance metrics; Manhattan, Euclidean, City Block and Correlation [12]. We applied Manhattan distance metric and Euclidean distance metric. Equations of these metrics are seen in (1), (2). While x, y represent test and training sample, n represents number of features.

( ) n  Euclidean i i i  d x y =  = ??            (1)    ( ) n  Manhattan i i i  d abs x y =  = ??                          (2)  B. t Test Statistical methods are simple and fast [13]. t test which  measures information about features as individuals is a statistical method.  It is usually used for feature selection.  t test values are obtained for each feature. The values are sorted according to their t test score. While the highest value represents the most powerful feature, lower values represent irrelevant or less important feature. Thanks to the t test, we can identify more important features. These features affect directly classification accuracy.

t test method is appropriate for dataset which has two classes. t test values are calculated as looking through sample values belonging to features for each class. Each feature is evaluated independently. t test does not evaluate any  correlation among features. It can be a disadvantage of the method to classify dataset. However, it is very successful to detect outliers and irrelevant features [14].  Equation of t test is seen in (3).

2 2  | | ( )  ( ( ) ( ) ) / ( ) i i  i  i i i i i i  u u t x  n n n n? ?  + ?  + + ? ? + ?  ? =  + + (3)  iu + and iu  ?  are arithmetic means belonging to classes.

i? + and i?  ?  are standard deviations of classes and n represents number of samples in the dataset.

C. The Proposed Method Although kNN is a powerful classifier algorithm, it  evaluates equally all of features in dataset. However, this case is not valid for big datasets, because dataset can contain redundant or irrelevant features. And these features can adversely affect classification accuracy. To solve this problem, features which have more discriminatory power are weighted with higher values. So, the effect of the redundant or irrelevant features can be reduced to obtain higher classification accuracy.

We combined t test and k nearest neighbor in our approach to increase classification accuracy. t test sorts features depending on their discriminatory power. So, we can weight features which have higher discriminatory power, with higher values. Redundant or irrelevant features can be weighted with lower values. We generate a new representing space by weighting [15]. We used two different distance metrics; Manhattan distance and Euclidean distance. Equation of these weighted distance metrics are seen in (4), (5).

2( )  ( ) n  e w Euclidean i i i  i  d w x y =  = ??           (4)  ( )  ( ) n  e w Manhattan i i i  i  d w abs x y =  = ??                   (5)  While w represents weight values, e represents exponent of t test. According to our approach, we benefit from t test values of each feature to weight features. In this wise, features which have higher discriminatory power are multiplied by higher weights. We took different exponents of t test values to obtain the best classification accuracy. Flow chart of weighted k nearest neighbor algorithm is seen in Fig. 1.



IV. EXPERIMENTAL RESULTS  A. Dataset In this paper, we used pancreatic ductal adenocarcinoma  tumor dataset [16]. The dataset was published in July, 2012 and updated in March, 2013. It could be downloaded from www.ncbi.nlm.nih.gov address. This dataset has two classes.

While one of classes represents ductal adenocarcinoma tumor tissue, the other class represents adjacent non tumor tissue.

Each class has 45 matching pair samples (total 90 samples).

Dataset  Calculate t test value for each  feature and sort  Multiply value of each feature with exponent of its t  test value  kNN classifier  Stop  Finish  Exponent=Exponent+1   Fig. 1. Flowchart of the adaptive weighted kNN  The dataset has normally 33297 genes. However, some genes do not have any value. Because of that, some genes (4428 genes) have been eliminated. 456 genes also contain same values for all samples. Also these genes have been eliminated not to affect classification accuracy positively or negatively. Consequently, we have worked with 28413 genes.

B. Results Table I shows us kNN classification results. We used two  different distance metrics, Manhattan and Euclidean, to classify data. While 20 of 90 samples are used to train the dataset in our experiments, 70 of 90 samples are used to test. We used 10 different random indices to obtain more reliable classification results. We also benefited from three k values; k=1, k=3, k=5.

Their averages are seen in Table I. According to the results, it is obtained better classification results with Euclidean Distance and k=5 neighbors.

TABLE I.  KNN CLASSIFICATION RESULTS  Manhattan Distance Euclidean Distance k=1 68,43 71,43 k=3 73,14 74,14 k=5 76,29 76,86  Although kNN classifier is powerful, it evaluates equally all of features for classification. Thus, we applied weighted kNN algorithm to classify the dataset. The weighted kNN results are seen in Fig. 2 and 3. After each iteration, we  increased exponentially t test value to find the best weights.

The worst results are achieved with k=1 neighbor by using both Manhattan and Euclidean distance. Euclidean distance is more successful compared to Manhattan distance. According to the results, 86.57% classification accuracy is achieved with exponent=5 and k=3 neighbors for Euclidean distance metric.

For Manhattan distance metric, 86.42% classification accuracy are obtained with exponent=10 and k=5 neighbors. The best classification accuracy results are seen for k=1, k=3, k=5 in Table II.

60,00  65,00  70,00  75,00  80,00  85,00  90,00  95,00  100,00  1 2 3 4 5 6 7 8 9 10  Weighted kNN (Euclidean Distance)  k=1 k=3 k=5  C la  ss ifi  ca tio  n A  cc ur  ac y  Exponent   Fig. 2. Weighted kNN results for different exponent values  60,00  65,00  70,00  75,00  80,00  85,00  90,00  95,00  100,00  1 2 3 4 5 6 7 8 9 10  Weighted kNN (Manhattan Distance)  k=1 k=3 k=5  C la  ss ifi  ca tio  n A  cc ur  ac y  Exponent    Fig. 3. Weighted kNN results for different exponent values    TABLE II.  WEIGHTED KNN CLASSIFICATION RESULTS  Manhattan Distance Euclidean Distance  k=1 78.42% (exponent=10)  81.86% (exponent=10)  k=3 86.00% (exponent=10)  86.57% (exponent=9)  k=5 86.42% (exponent=10)  86.57% (exponent=5)   The proposed method also allows to dimensionality reduction, because it sorts features depending on t test values.

Features which are higher t test value have better discriminating characteristics. In this wise, if we want to reduce dimension of the dataset, we can select features which have the highest t test values. So, time complexity can be reduced. Fig. 4 shows us results according to different dimensions of the dataset. Even if we select different dimensions of the dataset (more or less), classification accuracy is changed quite a few.

Because features which have higher t test value are weighted according to their exponential t test values. As shown in Figure 4, better classification accuracy results are obtained for exponent=5, because powerful features are weighted with higher values. However, classification accuracies are not increased after exponent value of 6, because the effects of irrelevant features are minimized on classification.



V. CONCLUSION Datasets which contain a great number of features have  redundant or irrelevant features. These features can cause to incorrect classification. k nearest neighbor  considers equally all of features without exception. For this reason, firstly, we sort features depending on their discriminatory power. Then features are weighted depending on their t test values according to our approach. While t test value uses normally to select feature selection, we benefit from t test to improve classification accuracy of k nearest neighbor.

According to results, classification accuracy is increased from 74.14% to 86.57% using Euclidean distance and k=3. Fig.

4 also shows that it is quite a few change in classification accuracy, if the exponent value or weighted value increases.

The proposed method also offers dimensionality reduction thanks to t test. In this wise, time complexity can be reduced.

