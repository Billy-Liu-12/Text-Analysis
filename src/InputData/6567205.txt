

Big Data, Little Decisions: Tightening the Loop between  Data Crunching and Human Expertise   Zack Bennett  LexisNexis  Dayton, Ohio USA  zack.bennett@lexisnexis.com  Marc G. L'Heureux  LexisNexis  Dayton, Ohio USA  marc.lheureux@lexisnexis.com        EXTENDED ABSTRACT    This presentation is a case study examining how  LexisNexis uses scaled active learning on the HPCC Systems environment to focus manual topical annotations on critical documents pulled from a large corpus. The active learning system uses natural language processing and machine learning techniques to identify and present ?next best? training set candidates to legal editors, combining massive parallel processing with expert human analysis to improve classifier accuracy while minimizing human effort.

Keywords- active learning, annotations, large corpus,  machine learning, text classification, training  set  Classification is the task of mapping a document d in the object space D to zero or more classes in a predefined set C, by  way of some function .: CDf Classification technologies serve important roles within enterprise systems, wherein objects are placed into predefined classes based on their observable and describable characteristics. Automated classification systems have been employed to solve problems such as recommending movies based on a user?s favorite actors [1], separating spam from legitimate email on the basis of included links [2], and interpreting spoken language [3]. At LexisNexis, we primarily apply classification to text documents, so we will use ?object? and ?document? interchangeably.

We can divide classification systems broadly into two groups: rule-based (RB) systems, and machine learning (ML) systems.

The earliest classification systems were RB. To create a classifier for a topic in a RB system, a human creates a Boolean query ? the rule ? to retrieve documents from the search space.

Each class in the predefined set of classes has an associated rule, and any document that matches the rule?s query belongs to that class.

Recent research has placed great focus on ML methods. To create a supervised ML classifier, humans compile training sets, which are collections of documents in the search space paired with annotations about which of the classes the  documents belong to. Formally, a training set is a set of ordered pairs in D?C. A training algorithm appropriate to the classification algorithm then creates a model, usually a parameterization of a standard evaluation algorithm, which plays the part of f in the description above. Semi-supervised and unsupervised ML are other techniques not described here, which vary from supervised ML in the amount of annotation information provided by a humans.

In our experience at LexisNexis, RB systems perform adequately and display very good precision (a measurement of the ratio of documents correctly classified as belonging to a class c to documents incorrectly classified as belonging to class c), but suffer a deficiency in recall (which is the ratio of documents classified to a class c to the number of documents in the object space which actually belong to class c). Writing rules is a time intensive process: expert rule-writers at LexisNexis can write two rules per day, and a rule can grow arbitrarily large as humans augment it to account for shortcomings.

With the adoption of our HPCC Systems parallel computing platform, we hoped to modernize our classification by adopting state-of-the-art ML techniques. After experimentation with different classification algorithms, we selected support vector machines (SVM), described by [4], and integrated a third-party C++ implementation of that algorithm into HPCC as a plug-in.

In converting to more-modern ML classification systems running on HPCC, we found serious shortcomings in traditional approaches to training the SVM. Compiling training sets for a topic manually still requires humans to formulate Boolean searches that retrieve documents that exhibit the total diversity of language relating to that topic. The size of the training set necessary to build an acceptable classifier varies with the topic, and humans cannot determine the necessary size any easier than computers can. Additionally, the rationale for including any single document in the training set may only be clear in the context of the whole set, making maintenance of training sets very difficult. Moreover, classifiers trained on manually-compiled training sets performed very poorly, displaying very high recall at the cost of near-zero precision.

We also tried bootstrapping SVM with annotation results from LexisNexis? previously-implemented RB classification system. We sampled the RB classification results, and used this sample as a training set for the SVM training algorithm.

Though this allowed us to completely remove the need for human intervention in ML training, these classifiers also performed poorly, perhaps due to the non-trivial recall errors in our RB system.

Active Learning (AL) is a supervised ML technique, described in [5], which minimizes cost associated with creating training sets. It also reduces noise and redundancy in the training set, which we have found degrades the quality of ML models. AL works by iteratively building a provisional classifier based on human annotations, and selecting the documents in an un-annotated pool of candidate training documents that are most confusing to that classifier. Humans review the selected documents and assign annotations, and these newly-annotated documents are added to the training set for the next provisional classifier. Cycles cease when some well-defined, deterministic criteria is met, such as a na?ve criterion limiting training set size or the number of cycles, or those described in [6] and [7]. At LexisNexis, HPCC enables the massively parallel processing necessary to deal with classifying millions of documents in a single AL cycle, which is required to find the optimal documents to present to human annotators.

Using AL, we reduce the amount of time necessary to  create training sets to as little as one hour per topic, by reducing  the number of documents humans must review and annotate.

Additionally, we have found models trained with AL achieve  results comparable or superior to our RB system in  considerably less time than it takes to create a rule. We find  this gain in productivity coupled with potential gains in  precision- and recall-oriented performance over the entire topic  space promising for future development of our classification  systems on platforms like HPCC.

