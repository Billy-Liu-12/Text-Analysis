

Proceedings of The on Industrial Technology, 1996  Design of a Neural-Fuzzy Controller  Based on Fuzzy Differential Competitive Learning  Ge Ming Institute of Industrial Process Chtrol  Hangzhou, 3 10027, CHINA  Abstraa- In this paper, a novel neural-fuzzy cuntroller based on fuzzy difFeretrtial competitive learning is proposed. Since one of the most impoitant parts is the generation of the fuzzy rules in the design of the fuzzy control system, a fast leaming algorithm-Fuzzy Merentid COntpeMive Leaming(FDCL) for the generation of the rules is applied in the fuzzy control system The FDCL algorithm adopts a principle of leam according to how well it wins. Unlike the previous competitive learning algorithm such as ciiSp competitive learning algorithms where only one neuron will win a d leam at each competition step every neuron in the neural network based on FDCL algciithm will along with its different distance to the input pattern and leams the pa- accordingly. compared with the ordmary competitive learning algorithm, the propod FDCL algorithm has the various distinguishing features. The FDCL algorithm is implanted in the neural network based fuzzy system and the network adopted is fuzzy associative memory system(FAMS) which simulates the knowledge 7 ?on and inference procas by using fuzzy notation and by asoociation in neural networks. In FAMS the fuzzy rules will be generated by clustering the input- output training data through the FDCL paradigm. By using the FDCL algorithm the neural network can highly refine knowledge and represead the expert elgxxience.

I . Introduction  During the past decade, fuzzy logic control has been widely used in many various fields such as industrial process control. Fuzzy logic control is usually based on a set of fuzzymles that sum up expert?s common sense and experience. But sometimes people maybe find it is difficult to get adequate fuzzy rules, especially when certain complicated dynamic processes are comemed. Now there are many approaches to obtain the fuzzy rules of some specrfc processes[l][2][3] and the fuzzy logic system designed based on these methods can also been successfully applied, however, these ways are quite problem dependent, i.e., a method may work well for one problem but is not suited for another problem. The other drawback of current fuzzy logic control is that there is no systematic procedure for the design of fuzzy logic control system and thus make it very difficult for people to analyze the properties of the fuzzy logic system. In recent years, the deal neural network has been widely combined with the fuzzy logic system for it?s learning capability and parallel structure. But in these research[4][5] on the neural-based fuzzy logic control system some problems exist: (1) The fuzzy rules identified by the neural networks are hard to mderstand because these rules are implicitly acquired in the networks; (2) The learning of the networks is time consuming; (3) Some informating concerning the process are mising or not used in the identification of the fuzzy rules.

This paper presents a new kind of clustering meth&  Sun YiuXian Institute of Industrial Process Control  Hangzhou, 3 10027, CHINA  Fuzzy-Differential-Competitive-learning(FJlCL) to obtain the fuzzy rules based on fuzzy associative memories(FAM) system. The FAM system provided by Kosko, which integrates neural and fuzzy logic and some learning laws, is used to learn the causal structure of the system[6]. In the designing of the FAM system , it is most important to select some efficient learning law to clustering the fuzzy rules. The clustering methods we proposed here is based on a kind of competitive learning and it?s effectiveness is verified through the control of the inverted pendulum system.

In Section 11, we propose a four step proceduae for generating fuzzy rules from sampled input-output data pairs by?the FDCL method. In section m, the new clustering method is applied to an inverted pendulum control system and this approach is compared with the FAM system using other clustering methods such as the Differential-Competitiveompetitive-learning method. Conclusions are giyen in SectionIV.

11 . Fuzzy-Differential-Competitive-Learning-  The goal of the merential competitive learning is to cluster or categorize the training paneins into some representative groups so that patterns within a cluster are more similar to each other than patterns belonging to different clusters. The differential competitive learning has been suggested as an alternative approach to various sophisticated problems such as the control of nonlinear, time-varying, ill-defined systems. The model is usually associated with a layered neural feedforwad network in which hidden neurons compete according to some sort of distance metric, usually the Euclidean one, to learn the current input pattern .?&. If the jth neuron wins, it?s parametric vector%, is updated additively by some propoition of the difference vector & -6,. The differential competitive learning dgorithm is described below and then we will make some modification which lead to the gew version of the competitive learning algorithm named F.Jzzy-D~erentia-~o~~titive-  Algorithm  Learning (FDCL) algorithm.

Differential Competitive Learning law @CL):  mirJ = I,AS,(Y,)[X - 4  0-7803-3 104-4 697    or: mJ(t + l ) = m J ( f ) +  c t ~ ~ , ( y , ( t ) ) [ ~ ( t ) -  mdt)]  where ct is the learning rate and AS (y (t)) denotes the  time change of the jth neuron's competitive signal s (y (t )) in the competition field Fy :  (1) mi(t + 1) 7 mi(t) $ z * J  In practice we often use only the sign of the signal difference (2) or sgn( Ay,), the sign of the activation ~Werence[6]. Kosko has pointed out that the Fy neuronal activations y, can be updated with an additive model:  ( y j  (t + '))= (y J (I))+ gsZ (.l)my (t)+ Sk(yk)wkJ (3) z=l  1 = 1  The fixed competition matrix W defines a symmetric lateral inhibition topology within Fy . In the simplest case wy=-l and wJl=l for distinct i andj .

The differential competitive learning adaptively quantities the input pattern space R" and it has been proved that the competitive synaptic vectors mJ capl cowkrge exponentially quickly to pattemclass centroid [6]. The clustering algorithm using the differentid learning law is described below: 1.Set the number of competing neurons and initialize the neuron's synaptic vectors: mz(0)=x(i),i=l,.. .,m.

2. For random sample x(t),find the closest("winning') synaptic vedtor mJ{t):  (4)  where llxll is the distances between competing neurons, there are many different distance metric can be employed and the most common distance metric is the squared Euclidean norm:  3. Update the winning neuron's parametric vector using the differential competitive learning law (1).

Although there are many successful applications using the DCL algorithm, the exclusive learning mech,?nism has two drawbacks : one is the neuron underutilizing problem [7][8], and the other is that the information concerning the closeness of input patterns and competing neurons is wasted during the winner-take-all training process because the winner takes all the responsibility for learning the current input pattern in the DCL algorithm. The differential competitive learning law can be rewritten as other form: mJ ( ' + ') = m J (  ')+ Ct 1, A SI (y J (  '))(,(') - m J (t)]  0 d 1 ' 1  (6) If Z = J  I , =  As shown in (6), the indicator function 4 is an crisp(hard) function so that in the competing process only  one neuron will win and learn the current training pattern.

Obviously, the concept of win in this setting is a crisp one and has a very clear-cut boundary. By considering win as a fuzzy set, every neuron to a certain degree wins, depending on its distance to the current training pattern.

Therefore it has to learn according to its win membership during the competition. In this way , we can replace the none-fuzzy indicator function with a fuzzy indicator function, that means, the indicator function is a fuzzy scaling function s m n g  the sign and magnitude of the difference vector 9  Below we will derive the fuzzy differential competitive learning via minimization of an objective function. Let us study a collection of n patterns constituting vectors in the p-dimensional space of real numbers, namely X I ,  ~2 ,..., xn CAP . We assume that the structure contains c clusters.

The Objecting function will be introduced as the following sum:  (7)  SllbJected to:  i U i k = l  Vk; ,=I  urt E[0,11 Vk;z n  O < x u , k < n  Vk; k=l  where : u,k E [0,1] denotes the grade of membership of the k-th  pattern in the i-th cluster.

The parameter m is used to control the influence of  intermediate membership values on the performance index; l<m< 03 . The function 6 Ik is usually defined as the distances between the k-th pattern and the generic structure of the clusters which involves both points and linear varieties:  where:  The function Dlk involves dzk and adds also a linear variety of dimension r expressed by a scalar prokct < * , - >. This variety goes through the vector mJ and is spanned by the collection of r linearly independent vectors sy[lO]. The role of the parameter g is to keep the balance between these two components(d,k and Dlk ).

By applying the gradient descent method to the objective function J,  a fuzzy differential competitive learning law can be obtained:    ( 9 )  Since the derivation of these formulae is the same as that of the fuzzy clustering algorithm that has already been studied in [lo ] so the procedure of this derivation is not discussed in detail. From the derived algorithm, it can be seen that every neuron is responsible for learning the current training pattem rather than only a winning neuron has the responsibility. It is already mentioned in the front of this section that normal Werentid competitive learning has two disadvantages and the disadvantages maybe lead to the failure of the learning, however, the fuzzy differential competitive learning will reduce the probability of the learning failure and its effectheness will be demonstrated in the section ID. From the equation (9) a d  (10) we can see that the membership U,k is changed with the distance between the neuron and the pattern, that is, the closer the neuron to the pattern, the larger its win membership UIk is and the more it has to learn. However, the far-away neurons has also the chance to update its synaptic vector and can be moved to some pattern regions.

Therefore the problem of the old differential competitive learning that it waste the idormation concerning the closeness of input patterns and competing neurons is solved. In the section ID, we will integrate this new clustering algorithm with the FAM system and develop a fuzzy contrcl system.

Although it has been proved that the synaptic vectors converge to decision-class centroids that correspond to local of the sampled but unknown probability density function Ax) through the Werentid competitive learning [6], the hzzy differential comwtive learning is not yet proved and wil l  \re studied. In the following discus- sion we will only study the centroid theorem because it is very important for the fuzzy clustering process and the others are discussed sidarly by Ksoko[6].

Suppose that the decision classes D1, D2, ..., 4 partition R? into k classes: Rn=D1UD2 ... U Dk (1 1) D l n D 2 z o  i f i * j  (12) Centroid defines the deterministic ?center of mass? of pattem class Dj 161:  (13) J4Asj(y/)xpodx a= - I,P(X)d.

The centroid theorem will be first proved that if a fuzzy differential competitive learning system converges, it con- verges to the centroid of the sampled decision class.

Centroid theorem:  where q is the centorid of the sampled decision class.

Prob (mi=@ =1 at equilibrium (14)  proo$ Suppose thejth neuron in F y  wins the activation  , * 699 *  competition during the training interval and the jth syna@.ic vector aj codes for decision cl&s Dp The stochastic fuzzy differential competitive learning equations is adopted in the proof for In practice we actually use the random process for studymg. The stochastic fuzzy differential competitive learnihg law (9) can be rewritten as: i,= ~ , A s , ( ~ , k - m , ] + n ,  (15) where {nj) represents a zero-mean Gaussian white-noise random process. 4 is defined in (9).

Suppose the synaptic vector mj has reached equilibrium: mj = (16)  which holds probability one. Take expectatiops of both sides of (15), use the zero-mm property of the noise process, eliminate the synaptic vector hj with the stochastic fuzzy Werentid competitive leaning (14) and expand to give : ?: qk] = l p s , ( Y , p  -mJ)P(x )dx+E[4  = S,ASr(Y,)(X -&Jo)dx = J q A s j ( y j k P ( + - L , d x F  = JqAs~(ylZYP(x)mc- 4,P(.)d.

19Asi(Y,Pdodx 19d+  =O  :. mj= Thus the theorem is proved.

III . Application to the Inverted-Pendulum System  In this section, we wil l  give an simulation of the control of the Inverted Pendulum System with the Fuzzy- DitTerential-Competitive-~ng-Algorithm(F?DCL) and the normal Differential-Competitive-Learning-Algorithm @CL).

Controlling of the Inverted Pendulum System is a hard task becapse ~e pendulum system is a non-linear system for which there are no traditional control system design methods. Ksoko has proposed the fuzzy control strategy for the pendulum system with the DCL algorithm. Now we use the FDCL algorithm to control the same problem and compared it with the DCL algorithm.

The Inverted Pendulum System is composed of a rigid pole and a cart on which the pole is hinged. The cart moves on the rail tracks to its right or left, depending on ,the force exeFed on the cart. The control aim is to balance the pole starting from nonzero conditions by supplying appropriate force the cart. The dynamics of the Inverted Pendulum System are characterized by four state variables: 8 (angle of the pole with respect to vertical axis), 8 ?(angular velocity of the pole), z (position of the cart on  the track), and z?(vel0City of the cart). The relationship between these variables is determined by two second-order Merentid equations described by [9].



IV.CONCLUSIONS '  m+m We use the equations to generate the desired Input-  Output data pairs( 8 ,  8 '; F)( F is the force that balance of the pendulum system). Because the detailed process of controlling 'the pendulum system and the constructing of the FAM@uzzy Aslsociative Memory) system is similar to the previouS methd proposed by Ksoko except for the implanted learning algorithms, thus we only give the results of simulation of these two laming algorithms(FDCL and DCL). Figure1 shows the results of the two Merent control methods.

h g l e 5   - 5 0 1  2 3 4 5 . 6  7  Titne(Sec)  ( a ) 10 , I  -30 I 0 1 2 3 4 5 6 7    Force5   - 5 L- O 1 2  3 4- 5 6 7  Time(Sec) i ( c )  Fig. 1. (a) Pole angle, @) pole angular velocity, (c) balance force. (Solid, dashed curves with regard to difFerent leaming methods: DcL, mL, respectively.)  In this paper, we have proposed a Fuzzy-Differential- Competitive-Leaming-Algorithm(FDCL) and implanted it in the F A M  system for co"cting a Neural-Fuzzy controller. By comparing of the results of the two simulations with two learning algorithms we can find that there are some impravements by using the FDCL algorithm.

V . REFERENCE  1. E.M. Scharf and N.J. Mandic, "The application of a fuzzy controller to the control of a multidegree-freedom robot arm," Industrial Application of Fuzzy Control, M.Sugeno, Ed. Amsterdam: North Holland, 1985, pp. 41- 62.

2. S .  Shao, "Fuzzy self-organizing controller and its application for dynam~c processes," Fuzzy Sets Syst.,  3. R Tanscheit and E.M. Scharf, *'experiments with the use of a rule-based self-organizing controller for robetics applications," Fuzzy Sets Syst., Vo1.26, pp. 195-214, 1988.

4. A.G. Barto, RS. Sutton, and C.W. Anderson, "Neuronlike adaptive elements that can solve difficult leaming control problems," JEEE Trans. Syst., Man, Cybern., Vol. SMC-13,nOS. pp.834-847, 1983.

5. F.C. Chen, "Back-propagation neural network for nonlinear self-tuning adaptive control," Proc. IEEE Intelligent Machine, pp. 274-279, 1989.

6. B. Kosko, Neural Networks and Fuzzy Systems.

Englewd Cliffs, NJ: Rentice-Hall, 1992.

7. Ahalt, S.C., Krishnamurthy, A.K.. Chen, P..& Melton, D.E., "Competitive Learning algorithms for vector quantization, " Neural Networks, 3,  pp.277-290, 1990.

8. Grossberg, S., "Adaptive pattern classification and universal reaxling: 1. Parallel development and coding of neural feature detectors," Biological Cybemetics, vol. 23,  9. A.G.Barto, RS.Sutton, and C.W.Anderson, "Neronlike adaptive elements @at can solve difficult leanking control problems," IEEE Trans. Syst. Man. Cybern, vol. SMC-13, no. 5, pp.834-846, 1983.

10. Y.Yoshinari, W.pedrycz, and K. Hirota, "Construction of Fuzzy models through clustering techniques," Fuzzy Sets and Systems, Vo1.54, pp. 157-165, 1993.

V01.26, p ~ .  151-164, 1988.

~121-134.

