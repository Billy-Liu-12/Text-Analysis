CTC - Correlating Tree Patterns for Classification

Abstract  We present CTC, a new approach to structural classifica- tion. It uses the predictive power of tree patterns correlating with the class values, combining state-of-the-art tree min- ing with sophisticated pruning techniques to find the k most discriminative pattern in a dataset. In contrast to existing methods, CTC uses no heuristics and the only parameters to be chosen by the user are the maximum size of the rule set and a single, statistically well founded cut-off value. The experiments show that CTC classifiers achieve good accu- racies while the induced models are smaller than those of existing approaches, facilitating comprehensibility.

1. Introduction  Classification is one of the most important data mining tasks. Whereas traditional approaches have focused on flat representations, using feature vectors or attribute-value rep- resentations, there has recently been a lot of interest in more expressive representations, such as sequences, trees, and graphs [4, 5, 1, 12, 3]. Motivations for this interest include drug design, since molecules can be represented as graphs or sequences. Classification of such data paves the way to- wards drug design on the screen instead of extensive experi- ments in the lab. Regarding documents, XML, essentially a tree-structured representation, is becoming ever more pop- ular. Classification in this context allows for more efficient dealing with large amounts of electronic documents.

Existing approaches to classify structured data can be categorized into various categories, namely propositional- ization approaches [5, 1], association rule approaches [12], and integrated techniques [11, 9, 3]. They differ largely in the way they derive structural features for discriminating between examples belonging to the different classes. They share the need for the user to set parameters influencing the feature derivation and the use of heuristics. Also, the first two approaches typically produce large feature sets, making  classifiers rather difficult to interpret.

In this work we present CTC, an approach situated be-  tween the association rule technique and integrated systems.

It is motivated by recent results on finding correlated pat- terns, allowing to find the k best, i.e. most discriminating, features according to a convex maximization criterion such as ?2 [8]. Rather than generating the complete set of pat- terns satisfying a given criterion and post-processing them, or searching for good features in a heuristic manner, CTC computes the set of k best patterns by employing a branch- and-bound search. Only two parameters have to be speci- fied, decoupling the success of the classifier from decisions about parameters influencing the search process.

The paper is organized as follows: in Section 2 we de- scribe earlier work on the topic and relate it to our approach; in Section 3, we discuss technical aspects of our method and outline our algorithm; in Section 4, the experimental evalu- ation is explained and results are discussed. We conclude in Section 5 and point to future work directions.

2. Related work  Structural classification has been done with different techniques. Firstly, there are several propositionalization approaches, e.g. [5] and [1]. While details may differ, the basic mechanism in these approaches is to first mine all pat- terns that are unexpected according to some measure (typ- ically frequency). Using these, instances are transformed into bitstring representation and classifiers trained. These approaches can show excellent performance and have ac- cess to the whole spectrum of machine learning techniques but there are also problems: The decision which patterns to consider meaningful, e.g. frequent ones, will have an effect on the quality of the model. The resulting feature set can be very large, requiring pruning of some kind. Finally, inter- pretation of the resulting model is not easy, especially if the classifier is non-symbolic, e.g. a SVM.

A second group of approaches is similar to the associa- tive classification approach [7]. Again, unexpected patterns       are mined, each of them associated with the class value. An example is Zaki et al.?s XRULES classifier. Each pattern is considered as predicting its class. Usually, the result- ing rule set has to be post-processed and/or a conflict res- olution technique employed. As in the propositionalization techniques, the choice of mining constraints is not straight- forward and the resolution technique can strongly influence performance, as has been shown e.g. in [10, 13]. Addition- ally, the resulting classifier often consists of thousands of rules, making interpretation by the user again difficult.

Finally, there exist integrated techniques that do not mine all patterns, but generate features during classifier construc- tion. Since structural data can be represented in predi- cate logic, techniques such as FOIL [11] and PROGOL [9] can be used for the task of structural classification. While ILP approaches are elegant and powerful, working on large datasets can be too computationally expensive. Approaches such as DT-GBI [3] construct the features used by graph- mining. Most integrated approaches have in common that feature induction is done in a heuristic way, e.g. using beam search. The user sets the parameters governing this search such as beam size and maximum number of literals per rule in FOIL and beam size, maximum number of specializations per node, and possibly minimum frequency in DT-GBI.

In contrast, only two parameters have to be specified for CTC. The first one is the maximum rule size, giving the user an intuitive way to decide on the complexity of the resulting model. The second one, the cut-off value below which to not consider rules interesting anymore, is optional.

By setting this value the user can enforce the significance of included rules. By basing it on e.g. the p-values for the ?2-distribution, the user has a well-founded guide-line for choosing this value.

3. Methodology  In this section we sketch the pattern matching notion used by the CTC approach, discuss upper bound calculation, the main component of the principled search for the most discriminating pattern, and formulate the algorithm itself.

3.1. Matching embedded trees  Several types of structured data exist, such as graphs, trees and sequences. In this paper we will focus on tree structured data, like XML, only. Thus, we need a notion for matching tree structured data. We use tree embedding to compare our approach with Zaki et al.?s technique. Due to space constraints we cannot give a formal definition here and refer the reader to [12].

This particular notion is more flexible than simple sub- trees and the mining process is still efficient. In general, other matching notions (see [4]) and even different repre- sentations could be used with CTC. This includes not only  Table 1. A contingency cable c1 c2  T yT xT ? yT xT ?T m ? yT n ? m ? (xT ? yT ) n ? xT  m n ? m n y  c x  Threshold  f(k)  k  f(c)  f(l)  l  x ,y  c ,c ?(x ?y )  y ,y  x0,0  y T c ,cu u  l l  T x  TTTT  x ?y ,0T  y  T  TT  (A) Convex function (B) Convex hull of ??s domain  Figure 1. Convex function and convex hull of the set of possible ?x?T , y?T ?  other notions of matching trees, but also graphs, sequences etc., since the general principles of our approach apply to all domains.

3.2. Correlation measures  A correlation measure compares the expected frequency of the joint occurence of a pattern and a certain class value to the observed frequency. If the resulting value is larger than a given threshold the deviation is considered statisti- cally significant enough to assume a causal relationship be- tween the pattern and the class.

We organize the observed frequencies in a contingency table, cf. Table 1. Since xT and yT are sufficient for cal- culating the value of a correlation measure on this table, we view these measures as real-valued functions on N2 for the remainder of this paper. While we focus on binary class problems a multi-class problem can be tackled by choosing the right measure and training round-robin classifiers.

Since correlation measures are not (anti-)monotone, di- rected search becomes more difficult. But if they are con- vex, an upper bound for the score of future patterns can be calculated, making it possible to prune the search tree.

3.3. Convexity and upper bounds  Convex functions like ?2 and Information Gain (see [8] for a proof) take their extreme values at the points forming the convex hull of their domain D. Consider the graph of f(x) in Figure 1(A), with D := [k, l] which also makes those points the convex hull. Obviously, f(k) and f(l) are locally maximal, and f(l) the global maximum. Given the current value of the function at f(c) , evaluating f at k and l allows to check whether it is possible for any future value of c to put the value of f over the threshold.

For the two-dimensional case, the extreme values are reached at the vertices of the enclosing polygon (such as the parallelogram in Figure 1(B) in our case). This par- allelogram encloses all possible tuples ?x?T , y?T ? that cor-       respond to occurence counts of specializations of the cur- rent pattern T . The upper bound on a measure ?(T ?) is ub?(T ) = max{?(yT , yT ), ?(xT ? yT , 0)}, since ?0, 0? and ?xT , yT ? represent uninteresting patterns. For an in- depth discussion of upper bound calculation we refer the reader to [8, 13]  3.4. The CTC algorithm  Given k and ?user , the CTC algorithm (Algorithm 1) constructs an ordered list of at most k best-scoring rules that exceed at least ?user . Starting from the most general (empty) tree pattern all tree patterns are canonically enu- merated. Since correlation measures are neither monotone nor anti-monotone the upper bound is used to prune, i.e.

only patterns whose upper bound exceeds ?user and the kth- best significance score seen so far are specialized.

Algorithm 1 The CTC algorithm ? - correlation measure, ?user - cut-off value, k - maximum size of rule set 1: S = ? 2: ENUMERATEK-BESTSUBTREES(S,?, ?user , ?) 3: return S  ENUMERATEK-BESTSUBTREES(S,t, ?, ?) 1: for all canonical expansion t? of t do 2: if ?(t?) ? ? then 3: S = S ? {t?} 4: if |S| > k then 5: S = S \ arg mins?S ?(s) 6: ? = mins?S ?(s) 7: if ub?(t?) ? ? then 8: ENUMERATEK-BESTSUBTREES(S,t?, ?, ?)  Once the k best patterns have been found, each pattern is treated as a rule predicting the more frequent class among the training instances covered by it. In case of a tie, the majority class in the dataset is predicted.

3.5. Classification strategies  The computed ruleset can be used in different ways in the classification process. A first, simple strategy often refered to as decision list (DL) uses the first rule from the ordered ruleset matching an instance for classification. A second strategy called majority vote (MV) collects all rules and - in the least complex version - predicts the class that is pre- dicted by the majority of the matching rules. Since it is counter-intuitive that rules of different strength are given the same weight, we also evaluate two discounting strategies from the field of associative patterns [6, 12]. One is the av- erage strength method (AvgStr), introduced by Zaki et al. in [12]. For each class, the strength, e.g. confidence, w.r.t. this class of the rules matching the instance is added up and nor- malized by dividing by the number of rules. The class with highest average strength is predicted or the majority class if no class achieves higher than default strength. Finally, the  weighted ?2 heuristic (WChi), introduced by Han et al. [6] discounts the ?2 value for each rule against the maximum ?2 value that rule could have attained. In all cases the ma- jority class is predicted if no rule matches the instance to be classified.

4. Experimental evaluation  For the experimental evaluation, we compared our ap- proach to Zaki et al.?s XRULES [12] and TREE2, an inte- grated approach introduced in [2] based on similar princi- ples as CTC, on the XML data used in [12].

The XML data used in our experiments are log files from web-site visitors? sessions. They are separated into three weeks (CSLOG1, 2, and 3) and each session is classified based on whether the visitor came either from an .edu do- main or from any other domain. For the mining process we set k = 1000 and ?user = 3.84, the 90%-p value for the ?2 distribution. For the comparison with TREE2 we built decision trees with the same cut-off value. In each setting we used one set of data for training and another one for test- ing. Following Zaki?s notation, CSLOGx-y denotes that we trained on set x and tested on set y.

The results are summarized in Table 2. Each column is labeled with the setting and reports the error rate for the classifier and its complexity. Complexity for XRULES and CTC are given in number of rules, for TREE2 in number of inner nodes. The mining step itself gave rise to less than 1000 rules for CTC on each dataset, ranging from 497 on CSLOG2 to 981 on CSLOG12. To arrive at the subset of rules used in the classifier, whose size is reported in Table 2, the n best rules mined were evaluated on a validation set (half the test set), with n ranging from 10 to the maximum number mined with increments of ten. A graph showing the effects of using a subset of the rules mined is shown in Fig- ure 2. It is interesting that less than 100 rules cause high error rates and increasing n past 200 decreases the qual- ity of the classifier again. The resulting classifier was then evaluated on the other half of the test set.

As expected, TREE2 has the least complex models since it creates features on-demand. It is, however, out- performed by XRULES and the first three CTC versions (MV,DL,AvgStr) except for the first setting. XRULES? models are two orders of magnitude larger than CTC?s while not being significantly better than the first three of the CTC versions on the 5% level for all settings. XRULES shows better performance than CTC using the Weighted Chi heuris- tic on the settings CSLOG2-3 and CSLOG12-3.

5. Conclusion and future work  In this work, we presented CTC, a rule-based approach to structural classification. Using an optimal branch-and- bound search, the algorithm finds the k most discriminating       0.165  0.17  0.175  0.18  0.185  0.19  0  50  100  150  200  250  300  350  400  450  500  550  600  E rr  or R  at e  on T  es ts  et  Size of Rule Set  MV DL  AvgStr WChi  Figure 2. Error rates for different classifica- tion strategies for the CSLOG1-2 setting  Table 2. Error rates and complexity for XRULES, different CTC strategies, and TREE2  Setting CSLOG1-2 CSLOG2-3 CSLOG12-3 CSLOG3-1 XRULES 17.01 - 28911 15.39 - 19098 14.70 - 29098 16.19 - 31661 CTCMV 16.77 - 130 16.05 - 150 15.73 - 170 16.50 - 220 CTCDL 16.69 - 130 16.10 - 150 15.76 - 170 16.23 - 220 CTCAvgStr 16.69 - 130 16.08 - 150 15.71 - 170 16.37 - 220 CTCW Chi 16.99 - 130 17.17 - 150 16.47 - 170 16.47 - 220 TREE2 17.53 - 66 18.09 - 57 17.42 - 103 18.69 - 60  patterns in a data set and uses them for prediction. This al- lows the user to separate the success of the classifier from decision about the search process, unlike in approaches that use heuristics. Basing the criterion for inclusion in the rule set on statistically well founded measures rather than ar- bitrary thresholds whose meaning is somewhat ambiguous gives the user better guidance for selecting this parameter. It also alleviates the main problem of the support-confidence framework, namely the generation of very large rule sets that are incomprehensible to the user and possibly include uninformative rules w.r.t. classification.

As the experiments show, CTC classifiers are effec- tive while being less complex than existing rule-based ap- proaches. By having users supply a parameter restricting the maximal size of the induced rule set, we give them the opportunity to build models that they still consider compre- hensible. Furthermore, evaluating the subset of the induced rule set consisting of the l highest-ranking rules on a vali- dation set and selecting the l giving the best results offers a straight-forward way of tuning the classifier?s performance.

So far, we have restricted ourselves to a single represen- tation, trees, a single measure, and evaluated four possible classification strategies. Future work will include evaluat- ing other correlation measures and applying our approach to different representations. Finally, selecting the subset of rules to actually use in the classifier is done heuristically so far. To base model construction on optimal search seems to be a promising research direction.

