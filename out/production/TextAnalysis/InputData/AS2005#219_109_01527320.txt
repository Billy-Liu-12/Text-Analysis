MINING ASSOCIATION RULES WITH NEW MEASURE CRITERIA

Abstract: Nowadays, association rules mining from large databases  is an active research field of data mining motivated by many application areas. However, there are some problems in the strong association rules mining based on support-confidence framework. Firstly, there are a great number of redundant association rules generated, then it is difficult for user to find the interesting ones in them; secondly, the correlation between attributes of given application areas is ignored. Therefore new measure criteria, which are Chi-Squared test and cover, should be introduced to association rules mining, and the more important aspect is the use of Chi-Squared test to reduce the amount of rules. In the paper, the Chi-Squared test and cover of measure criteria would be used to association rules mining for removing the itemsets, which are statistic independent, while frequent itemsets or rules are generated.

Thus the number of patterns (including itemsets and rules) itemsets decreases, and it is easy for user to capture the more interesting association rules. The experimental results demonstrate that the Chi-Squared test is effective on reducing the amount of patterns via merging support and cover constrain. Pattern selection based on Chi-Squared test can eliminate some irrelevant attributes, and the efficiency and veracity of mining association rules are improved.

Keywords: Data mining; Association rule; Apriori; Chi-Squared test  1. Introduction  Nowadays, association rules[1] mining is an important task of data mining, which describes potential relationships among data items in databases, the main idea of which was firstly proposed by R. Agrawal et al. in 1993, shortly after then realized by the well-known Apriori algorithm[2], which was an influential algorithm for mining frequent itemsets for Boolean association rules. Usually most of other algorithms are improved on the basis of the Apriori algorithm. According to the common measure criteria of association rules mining, we can mine those strong association rules, which should satisfy two conditions: support and confidence.

However, mining association rules by the common  measure criteria, there are some shortcomings. In the paper, the Chi-Squared statistic method and cover are introduced to association rule mining, which help to reduce the amount of patterns and mine the more interesting association rules, then the efficiency and veracity of mining association rules are improved.

2. The formal statement of association rule  In order to grasp the following work more accurately, we will firstly review the formal model of association rule.

Association rules mining can be stated as follows: Let I={i1, i2, ? in} be a set of items. Let D, the task-relevant data, be a set of transactions, where each transaction T is a set of items satisfying T?I. The quantity of each item bought in the same transaction is not considered. Each transaction is assigned an identifier, called TID. Let A be a set of items, a transaction T is said to contain A if and only if A?T. An association rule is an implication of the form A?B, where A?I, B?I, and A?B=?. The rule A?B holds in the transaction set D with support s, where s is the percentage of transactions in D that contains A?B (i.e., both A and B).

This is taken to be the probability P (A?B). The rule A?B has confidence c in the transaction set D, where c is the percentage of transactions in D containing A that also contains B. This is taken to be the conditional probability, P (B|A). That is, Support (A?B) =P (A?B)=s, Confidence (A?B) = P (B|A) =s/Support (A)=c.

The common association rules mining is to mine strong association rules that satisfy both the user-specified minimum support threshold and confidence threshold.

3. Association rules mining based on new measure criteria  According to the common measure criteria, support and confidence, the redundant, unwanted or even false strong association rules are likely to be generated. Thus it is difficult for user to find interesting association rules, and the common association rule mining should be improved.

We think the main reason for shortages mentioned above is the correlation of attributes of a given application areas is ignored. So the Chi-Squared test of measure criterion should be introduced to association rules mining since the Chi-Squared test could remove irrelevant itemsets and rules which support are more higher, and the cover of association rule makes it easy for users to reduce the amount of association rules, then the efficiency and veracity of mining rules are improved.

Based on what mentioned in above, we could apply the support-confidence framework to market basket data from a retail store. We may capture rules that satisfy both the user-specified minimum support threshold and minimum confidence threshold. However, some of these rules are uninteresting, maybe some of these rules are implicated in other rules which already exist; some of them could be generated from user?s domain knowledge directly, instead of applying any mining approach to source datasets; even some are unwanted or misleading.

Let?s consider an example. Given a 2?2 contingency table for binary variable as shown in Table 1 to describe the purchase of milk and coffee.

Table 1.: A 2? 2 contingency table for milk and coffee c c  m 2000(a) 500(b) 2500  m  7000(c) 500(d) 7500  9000 1000 10000(N)  When we apply the approach based-on support and confidence framework to the dataset with the user-specified minimum support threshold & confidence threshold are 0.05&0.6 respectively, we can capture the association rule m c, which support is 0.2 and confidence is 0.8, for example. Of course then, we can say that increasing the number of people who buy milk may promote the number of people buying coffee.

?  However, the priori probability of buying coffee is 0.9, which is larger than posteriori probability at the condition of knowing to buy tea. That is to say, the rule by itself may omit some important information or even mislead. In fact, the truth is that the support of buying coffee is very high, and it is such very high support result in the sequel that the rule m?c was mined, but not because there are any strong relationships between milk and coffee. In some case, the higher the support of the antecedent or consequent of the rule are, the more likely such misleading may occur. One way of solving the problem is calculating the Chi-square value between the milk and coffee via the formula 1(the formal statement of this formula will be introduced in section 3.1):   ))()()((  )( 22 dbcadcba  bcadN ++++  ?=?           (1)  The Chi-Squared value is 3.7,lower than a cutoff value 3.84 (for example at the 0.05 significance level). So it is independent between milk and coffee, and the itemset {milk, coffee} should be removed from frequent itemsets used as generating rules.

3.1. The Chi-Squared test  In theory[6],the Chi-Squared test is used to measure the degree of independence among different attributes by comparing their observed results with the expected value of occurrence under the assumption of independence. Note the Chi-Squared statistic has an upward closure property, i.e. if an itemset C passes the Chi-Squared test, so will every superset of C. This property will help to prune the exponential search space.

Let?s only consider the binomial case which suit the market basket dataset fine. We can define the vector X={X1 , ? , Xk} denote k-independent, binomial distributed random variable; define a contingency table CT, which is k-dimensional array indexed by {0,1}k, called R.

The cell CT(r) in the table is a count of the number of trial, out of N independent trials, where the event r={X1=r1,?,Xk=rk} occurs. The statistic  could be calculate via formula 2:  2?  ? ?  ?= Rr rE  rErO )(  ))()(( 22?  ~         (2) )1(2??  In the formulation, O(r) is the observed value of event r, E(r) is the expected value of r. When r is rj, E(r)=E(ri)=O(ri), otherwise E(r)=n?E(r1)/n???E(rk)/n, and the degree of freedom is 1.

The Chi-Squared test can be decomposed into two step:  (1) Calculate the value of the Chi-Squared statistic by formula 2.

(2) Compare the  value with the cutoff value that determined by significance level  2? ? , which reflects  the feature of a given application areas and the user?s subject judgment to it. If the Chi-Squared value is lower than the cutoff value at corresponding significance level we have no excuse to reject the independence assumption, so it is justified to remove the association patterns (including itemsets and rules). Note there is no definite formal formula used to choose ? , because there are various factors affecting the ? .

3.2. The cover of association rule  Similar to the definition of association rule in Section 2, it said that the rule A?B has confidence c in a transaction set D if c is the percentage of transactions in D containing A that also contains B. Here, we define that the association rule A?B has cover k in the transaction set D if k is the percentage of transactions in D containing B that also contains A. This is taken to be the conditional probability P(A|B). That is, Cover(A?B) = P(A|B) = Support (B?A)/ Support (B)=k.

We introduce the cover to association rule mining with the purpose for cutting down the amount of association rules to a half. Since the Cover of the association rule A?B can keep the information of Confidence of the association rule B? A, we do not need to mine the association rule B ? A, then the redundant association rules can be reduced a lot.

From definition of Support, Confidence, Chi-Squared test, Cove, the property of association rule mining can be evaluated from different perspectives.

Support is necessary because it represents the statistical significance of a patterns; Confidence tells us that the association rule hold true at how much probability; Chi-Squared value measures the degree of independence between different attributes; Cover will help us reduce the amount of association rules to a half. In a word, these parameters will be helpful for us to mine the interesting and useful association rules.

4. An example  The approach based on Apriori algorithm has become one of the most common data mining methods, which was proposed by Agrawal et al., the aim of which is finding interesting relations among the attributes in large databases.

Many other improved methods have been proposed to reduce the number of scans and candidates by hashing, sampling, partitioning, dynamic itemset counting, parallel computing etc. but the essential of them have not changed.

Aprori is an influential algorithm for mining frequent itemsets for Boolean association rules, employs an iterative approach known as a Level-wise search, where k-itemsets are used to explore (k+1)-itemsets. First the set of frequent 1-itemsets is found. This set is denoted L1, then L1 is used to find L2, the set of frequent 2-itemsets, which is used to find L3, and so on, until no more frequent k-itemsets can be found. The finding of each Lk requires one full scan of the database. The problem of mining association rules can be decomposed into two major subproblems: 1) Find out all frequent itemsets, 2) Use the frequent itemsets to generate  the strong rules. Once all frequent itemsets from transactions in a database D have been found, it is straightforward to generate strong association rules from them, where strong association rules satisfy both minimum support and minimum confidence threshold. In the experiment, the results demonstrate the Chi-Squared test merging support constrain is proper and effective.

4.1. Generating the frequent itemsets  From the definition mentioned above, we can mine association rule by Apriori algorithm, taking a typical example shown in Table 2. There are 10 transactions and 5 items in all, that is N=10; the minimum support threshold is 3/10.

Scan D for count of each candidate itemsets of C1 = { {I1}, {I2}, {I3}, {I4}, {I5} } , the support of each one is 8/10, 8/10, 7/10, 3/10, 3/10 respectively, remove those candidate itemsets which support are lower than the minimum support threshold 3/10, we can easily obtain frequent itemsets L1 = { {I1}, {I2}, {I3}, {I4}, {I5} }; generate candidate itemsets C2 = { {I1I2}, {I1I3}, {I1I4}, {I1I5}, {I2I3}, {I2I4}, {I2I5}, {I3I4}, {I3I5}, {I4I5} }, scan D for count of each candidate itemset, which support are 6/10, 6/10, 2/10, 3/10, 5/10, 3/10, 3/10, 1/10, 2/10, 1/10, respectively, remove those itemsets which support are lower than 3/10, we can easily get L2 = {{I1I2}, {I1I3}, {I1I5}, {I2I3}, {I2I4}, {I2I5} }; generate candidate itemsets C3 = {{I1I2I3}, {I1I2I5}, {I1I3I5}, {I2I3 I4}, {I2I3I5}, {I2I4I5}}, scan D for count of each candidate itemset, which support are 4/10, 3/10, 2/10, 1/10, 2/10, 1/10 respectively, then we can gain the frequent itemsets L3 = { {I1I2I3}, {I1I2I5} }, then C4 = { {I1I2I3I5} },with support is 2/10 lower than minimum support threshold and the finding process for frequent itemset will be finished.

Table 2.

TID Items  T1 I1I2I5  T2 I1I2I3  T3 I2I4  T4 I1I2I4  T5 I1I3  T6 I2I3  T7 I1I3  T8 I1I2I3I5  T9 I1I2I3  T10 I1I2I3I4I5  4.2. The using of Test in mining association rules 2?  Once the frequent itemsets from transactions in the database D have been found, we can generate association rules from them. For example, we can get the rule I1? I3 (There may have many rules like this, which lead the number of rule is large, but some of them may unwanted for specific users.) As we mentioned above, such rules may      be captured by user directly via analysis to a given application areas. So we should remove the frequent itemset {I1I3} to reduce search space for generating rules.

To the rule I1? I3, the Chi-Squared value of attribute I1&I3 is 0.47, lower than the cutoff value 3.84(when we specify the significance level with 0.05), i.e. there is significant independent between I1 and I3, so the rule will not appear in rules set because we can removed the frequent itemset {I1I3} via Chi-Squared test just after it is generated.

From the results, compared with the common association rules mining, we can know that some unwanted patterns (itemsets and rules) could be removed by the means of merging Chi-Squared test into support and cover constrain. We can reach the conclusion that the Chi-Squared test may reduce some uninteresting patters, and the cover of association rules make it easy for user to reduce the amount of association rules, then the efficiency and veracity of association rules mining are improved[5].

5. Conclusions  In the paper, we have introduced association rules mining based on considering of correlation of attributes via Chi-Squared test. As we know, Chi-Squared test may eliminate some independent itemsets and the cover of association rules make it easy for users to reduce the amount of association rules, then the efficiency and pertinence of mining association rules are improved, in the meantime the searching space of the algorithm has been reduced. Therefore we can say it is more effective and veracious to merging Chi-Squared test into support constrain and cover measure while mining association rules from large databases.

Acknowledgements  This paper is supported by the Natural Science Foundation of the AnHui Education Department (China) under Grant No. 2005KJ312ZC .

