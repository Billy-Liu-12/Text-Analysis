

Abstract - Software production has become a focus  of major economic activities all over the world. This paper presents an evaluation of 63 software projects in a big Canadian bank using Data Envelopment Analysis (DEA).

DEA model was developed to measure software project efficiency focusing on the factors that affect software productivity and suggestions were offered on how DEA could be used to improve their software production metrics.  Finally, potential management uses of these DEA results were presented.

Keywords ? data envelopment analysis, software project  productivity, efficiency

I.  INTRODUCTION   Software production has become a focus of major economic activities all over the world. However, this activity is hard to measure in a conventional sense because participants in the industry can not agree on what is to be measured, how and what standards may be for any of these efforts. When the efficient standards are not available, organizations, especially service organizations, turn to comparative efficiency analysis (CEA) to measure their performance. CEA compares the current performance with historical data, other organizations and opinions to determine if the unit is producing efficiently.

Efficiency measurements naturally evoke the concept of ratios of outputs to inputs.  Usually different ratios are calculated to focus on different aspects of the operations.

In addition, service industries also rely on parametric methods to measure the project efficiency. There are ten primary parametric models. They are System Development Corporation (SDC) model, Wolverton model, SLIM model, Doty model, the RCA PRICE S Model, the IBM-FSD model, the 1977 Boeing Model, the 1979 GRC model, the Bailey-Basili Meta-Model, and COCOMO model. Most of these parametric models focus on cost estimation and determination of factors affecting cost and productivity. Data Envelopment Analysis (DEA), a relatively new quantitative technique that is a cornerstone of the service productivity management program, is used to establish the best practice group of units and to determine which units are inefficient compared to the best practice groups and the magnitude of inefficiency present. A number of applications of DEA to software production have been reported in the literature.

Banker and Kemerer [1] applied DEA models to estimate the Most Productive Scale Size (MPSS) for different data sets. They used single?input (Effort) single?output (Source Lines of the Code) production functions which allowed for both increasing and decreasing returns to scale. Later, Banker et. al [2] used a single-input (Effort) two-output (Function Points (FP) and Source Lines of Code) model to study the effects of project characteristics on the different phases of the software maintenance process. Some environmental variables were also included in this model. In Elam?s model [3], a quality attribute of the software as an output was considered, an important issue that is often not included because good data is not available. Paradi et. al [4] used a model with one input and three outputs. The single input, project cost, measured the effort and included labour, overhead, computer charges and other costs. The three outputs were size (Function Points - FP), quality (Defects) and duration (Days) which represented time to market considerations.

This paper presents a DEA evaluation of 63 software projects in a big Canadian bank. The rest of the paper is organized as follows. Section 2 gives a brief review of DEA. Section 3 provides the models and methodology utilized in this paper. Section 4 presents our findings and shows the factors that affect software productivity.

Finally, our conclusions are presented in section 5.



II. DEA BASICS   Production process can be defined as a process that can turn a set of resources into desirable outcomes by production units. During this process, efficiency is used to measure how well a production unit is performing in utilizing its resources to generate the derived outcomes.

Each of the various DEA models seeks to determine which of the n decision making units (DMUs) define an envelopment surface that represents the best practice, referred to as the empirical production function or the efficient frontier. Units that lie on the surface are deemed efficient in DEA while those units that do not, are termed inefficient.  DEA provides a comprehensive analysis of relative efficiencies for multiple input-multiple output situations by evaluating each DMU and measuring its performance relative to an envelopment surface composed of other DMUs. Those DMUs forming the efficiency reference set are known as the peer group for the inefficient units. As the inefficient units are projected onto the envelopment surface, the efficient units closest to the  A DEA Evaluation of Software Project Efficiency    Zijiang Yang1, J. C. Paradi2 1 School of Information Technology, York University, Toronto, Canada, M3J 1P3  zyang@mathstat.yorku.ca 2 Centre for Management of Technology and Entrepreneurship, University of Toronto, Toronto, Canada, M5S 3E5  paradi@mie.utoronto.ca        projection and whose linear combination comprises this virtual unit form the peer group for that particular DMU.

The targets defined by the efficient projections give an indication of how this DMU can improve to be efficient.

Consider n DMUs to be evaluated, DMUj (j=1,2?n) consumes amounts Xj ={xij} of inputs (i=1, 2, ?, m) and produces amounts Yj ={yrj} of outputs (r=1 ,?, s ). The efficiency of a particular DMU0 can be obtained from the following linear programs (input-oriented BCC model [5]).

0,s  ,    s-Y               ..

(1)        110    min ,,,    ??+ =  =???  =+  ???+??= ?+  s  sXX  Yts  ssz ss  ?  ?  ??  ?  ??? ??    The above equation is called the envelopment form, or primal form. The multiplier form or dual form is given as:    free  u  1-   1                        ..

(2)                           max   T  T  ,,   ?  ?  ???  ??  ?+?  =  +=  ?  ??  ?  ? ??  T  T  T  T  v  uXvY  Xvts  uYw    Performing a DEA analysis actually requires the solution of n linear programming problems of the above form, one for each DMU. The (scalar) variable ? is the proportional reduction to be applied to all inputs of DMU0 (the DMU being evaluated) to move it onto the frontier.

This reduction is applied simultaneously to all inputs and results in a radial movement toward the envelopment surface. The vector ? indicates the contribution of the efficient DMUs to the peer group that forms the reference set for the DMU under evaluation. Their magnitude indicates the degree to which the characteristics of the efficient DMUs are used to construct the virtual DMU on the frontier to which the inefficient one is projected.  The slack variables (s+ and s-) are variables added to the linear programming model in order to convert inequality constraints to equality constraints.  Theoretically, the presence of the non-Archimedean ? in the primal objective function effectively allows the minimization over ? to preempt the optimization involving the slacks [5].

The dual problem yields an alternative geometric interpretation.  In the dual form, ? and ? are the vectors of input and output weights. Efficiency is measured as a function of these weights. Each DMU is then allowed to choose weights that maximize its efficiency, provided that  the set of weights yield efficiency scores that do not exceed unity, for all DMUs. The variable u is the measure of scale efficiency. Assuming that (x0, y0) is on the efficient frontier the following conditions identify the situation for returns to scale at this point [5][6].

? Increasing returns-to-scale prevails at (x0, y0) if and only if u0*< 0 for all optimal solutions.

? Decreasing returns-to-scale prevails at (x0, y0) if and only if u0* > 0 for all optimal solutions.

? Constant returns-to-scale prevails at (x0, y0) if and only if u0* = 0 for all optimal solutions.

Some researchers argued that the above technique  could only handle the case of unique optimal solutions.

Sueyoshi provides a remedy when there exist alternative optimal solutions. For a detailed discussion, the reader is referred to [7].

The optimal value, z0* (= w0*), yields an efficiency rating that measures the distance that a particular DMU being rated lies from the frontier. A DMU is technically efficient if and only if w0* = z0* = 1. In other words, if a DMU is efficient, the following two conditions must be satisfied:  1. The optimal value ?* = 1; 2. All slack variables are zero.

The objective function values obtained partition the set of DMUs into two sets:  ? DMUs for which z0*=w0*=1 are efficient and determine the envelopment surface  ? DMUs for which z0* < 1 are inefficient and are enveloped by the surface.

If the convexity constraint ( 11 =? ) in (1) and the  variable u0 in (2) are removed, the feasible region is enlarged, which results in the reduction in the number of efficient DMUs, and all DMUs are operating at constant returns to scale. The resulting model is referred to as the CCR model. The reader is advised to consult the textbook by Cooper, Seiford and Tone [6] for a comprehensive treatment of DEA theory and application methodology.



III. MODELS AND METHDOLOGY   Partial production ratios are widely used measures for assessing performance in software project development mainly because they are easy to calculate and, ratio analysis only provides qualitative answers.  But the interpretation of such information is difficult and often subjective.  In contrast, this paper utilizes the dummy variable 1 as the sole input for the DEA model. The outputs are two production ratios:   ? R1= Project Size/ Total Cost ? R2=Project Size/ Project Duration  Thus, the two production ratios will be integrated and  no weight is specified. The DEA model will select the weights for the two ratios automatically so that the overall efficiency can be achieved. The diagram for the DEA  Proceedings of the 2009 IEEE IEEM       model is shown below. Output orientation is selected for this research in order to maximize the production efficiency.

Figure 1. The DEA model

IV. RESULTS AND DISCUSSION   In this analysis, 63 software projects in a big  Canadian bank are evaluated. Summary statistics for the inputs and outputs are reported in Table 1.

Table 1. SUMMARY STATISTICS OF OUTPUTS AND INPUTS  Max Min Average Standard Deviation  Inputs Dummy variable 1 1 1 0 Outputs R1=Total Cost /SLOC 9.26 0.07 2.63 2.29 R2=Duration per 100SLOC 6.14 0.02 1.09 1.35   The degree of correlation between variables is an  important issue that has great impact on the robustness of the DEA model. Thus, a correlation analysis is imperative to establish appropriate inputs and outputs. On the one hand, if very high correlations are found between an input variable and any other input variable (or between an output variable and any of the other output variables), this input or output variable may be thought of as a proxy of the other variables. Therefore, this input (or output) could be excluded from the model. On the other hand, if an input variable has very low correlation with all the output variables (or an output variable has very low correlation with all the input variables), it may indicate that this variable does not fit the model. Since the DEA model only has one dummy input variable, correlation analysis was done for the output variables.  The correlation coefficient between the two output variables is 0.62. This is a reasonable validation of my DEA models.

The output oriented BCC model is run and Table 2 summarizes the results for the model.

TABLE 2 DEA RESULTS BCC Average Score 0.32 Standard Deviation 0.27 Maximum Efficiency Score 1.00 Minimum Efficiency Score 0.01 Number (and %) of Efficient DMUs  2 (3.2%)  # Efficient DMUs exhibiting IRS   # Efficient DMUs exhibiting CRS   # Efficient DMUs exhibiting DRS    From this table, we can conclude that BCC model  identified 32% technical efficiency on average. There is no scale efficiency at work in the analysis.

The efficiency score distribution is given in Figure 2.

Efficiency Score Distribution    <0.1 0.1- 0.2  0.2- 0.3  0.3- 0.4  0.4- 0.5  0.5- 0.6  0.6- 0.7  0.7- 0.8  0.8- 0.9  0.9-1   Figure 2 Efficiency score distribution   The distribution of efficiency scores is skewed  towards the lower efficiency scores. Generally speaking, this model identified significant potential savings for the 63 involved software projects from the indicated performance improvements. The average efficiency score of 0.32 implies that the studied bank could use about 68 percent less resources to produce the same amount of outputs from a theoretical point of view. In practice, the saving will almost certainly be substantially less.

Nevertheless, this indicates that significant potential savings can be achieved for the examined company.

It is very important to present results in an easy to understand manner. Thus, we split the branches into groups based on the efficiency scores in order to find further insights [8]:  ? The robustly efficient units that will appear in many reference sets and are likely to remain efficient unless there were major shifts in their fortunes.

? The weakly efficient units that will typically appear in only one or two reference sets and may well drop below 1.0 if there was even a small drop in the value of an output variable (or a small increase in the value of an input variable).

? The marginally inefficient units will have an efficiency rating in excess of 0.8 (but less than 1.0) and  Software project  Input    Outputs   Project Size/ Total Cost Project Size/ Project Duration  Proceedings of the 2009 IEEE IEEM       could raise their score towards 1.0 with a relatively small amount of improvement in their operating results.

? Medium inefficient units that have an efficiency score between 0.5 and 0.8.

? Distinctively inefficient units. If the efficiency score of a unit is less than 0.5, then this unit would have significant difficulties making themselves efficient in the short term.

The units in the robustly efficient group could be examples for those inefficient units, as they manage their resources better.

Figure 3 shows the number of projects in each category and figure 4 shows the distribution of projects.

Number of DMUs         robustly efficient  weakly efficient  marginally inefficient  Medium inefficient  Distinctively inefficient   Figure 3 Number of units in each category     Projects Distribution  robustly efficient weakly efficient marginally inefficient Medium inefficient Distinctively inefficient   Figure 4 Projects distribution   Careful analysis of the above result suggests that the  49 distinctly inefficient units (74%) are too many for a real-life situation. To investigate this abnormality, it is most likely that the efficient units may have something unusual about one ore more of their outputs. Thus, setting targets may be unrealistic for other projects to reach.

Hence, the distances between the inefficient units and the frontier are unusually large. In order to further investigate this issue, the two efficient projects on the frontier were removed from the analysis, which is called ?peeling off the frontier?. The DEA model was run again to see what effect this has in the 49 distinctly inefficient DMUs. Table 3 summarized the new results.

TABLE 3 DEA RESULTS AFTER PEELING OFF THE FRONTIER   BCC Average Score 0.32 Standard Deviation 0.26 Maximum Efficiency Score  1.00  Minimum Efficiency Score  0.01  Number (and %) of Efficient DMUs  3 (4.9%)  # Efficient DMUs exhibiting IRS   # Efficient DMUs exhibiting CRS   # Efficient DMUs exhibiting DRS    After we peeled off the frontier, the efficiency scores  of the remaining projects remain almost the same. This suggests that the original frontier could be the real target for the inefficient projects.

DEA results also highlight the reasons for both favorable and poor use/production of resources/outcomes involved in the unit?s performance ? factors that contributed to or detracted from the DMUs? efficiency rating.  Nevertheless, one of the most powerful pieces of information that is obtained by a DEA analysis is the set of target values for those assessed as inefficient. The reference set provides strong indications of what type and amounts of inputs and outputs are needed to make the inefficient units efficient. Since input oriented DEA models are used in the analysis, there will be target input values that the inefficient units could use to achieve an efficiency score of 1.0. Table 4 provides DEA efficiency scores and reference sets for some of the DMUs in this analysis. For example, in order for Project 3 to become efficient, based on the reference sets DMU3 should improve its output to Y60*0.65 + Y66*0.35.

TABLE 4. DEA EFFICIENCY SCORES AND REFERENCE SETS    DMU Score Reference set (lambda)  1 0.13 60 1  2 0.63 66 1  3 0.17 60 0.65 66 0.35  4 0.09 66 1  5 0.65 60 0.58 66 0.42  6 0.01 60 0.98 66 0.02  7 0.16 66 1  8 0.05 60 0.58 66 0.42  10 0.12 60 1  11 0.48 60 0.75 66 0.25        Proceedings of the 2009 IEEE IEEM

V.  CONCLUSION   This paper adopts DEA to evaluate the software project performance in a big Canadian bank. The results identify the significant amount of improvement for the involved software project.  Special emphasis was placed on how to present the DEA results to management so as to provide more guidance to them on what to manage and how to accomplish the changes. Finally, recommendations to management?s use of DEA results were given.

