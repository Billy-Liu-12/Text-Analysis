Understanding the Impacts of Solid-State Storage on the Hadoop Performance

Abstract?The superior I/O performance of solid-state stor- age (e.g., solid-state drives) makes it become an attractive replacement for the traditional magnetic storage (e.g., hard- disk drives). More and more storage systems start to integrate solid-state storage into their architecture. To understand the impacts of solid-state storage on the performance of Hadoop applications, we consider a hybrid Hadoop storage system consisting of both HDDs and SSDs, and conduct a series of experiments to evaluate the Hadoop performance under various system configurations. We find that the Hadoop per- formance can be increased almost linearly with the increasing fraction of SSDs in the storage system. The improvement is more significant for a larger dataset size. In addition, the performance of Hadoop applications running on SSD-dominant storage systems is insensitive to the variations of block size and buffer size, which significantly differs from HDD-dominant storage systems. By increasing the fraction of SSDs, there is no need for the Hadoop operators to consider how to carefully tune block size and buffer size to achieve the optimal performance.

Our findings also indicate that the upgrade of the hadoop storage system can be achieved by increasing the capacity of SSDs linearly according to the scale of the applications.

Keywords-Solid-state storage; Hybrid storage system; Hadoop performance; Hadoop configuration

I. INTRODUCTION  Due to the solid-state property, an SSD can provide better I/O performance compared with a traditional HDD.

Recently, a large number of Internet service providers (such as Amazon, Facebook, and Dropbox) start to replace HDDs in their data centers with SSDs [6]. However, the performance improvement is at the cost of high monetary investment, which prevents a complete replacement of HDDs in current data centers. Instead, the solution of hybrid storage systems, which consist of both HDDs and SSDs, become very promising and can achieve SSD-like performance with only a certain amount of SSD integration [1][4][5].

As a popular big data analytics framework, Hadoop [8] is widely used for large-scale data storage and processing.

There is few work to study how the Hadoop performance will be impacted by the integration of SSDs. Previous work [4][7] mainly focused on the performance of traditional database systems (i.e., SQL) on hybrid storage systems.

However, the I/O patterns of Hadoop applications are very different from that of traditional SQL-based applications.

In this paper, we aim to investigate the performance of Hadoop applications running on hybrid storage systems.

Unlike previous work[2][3][9][10], which studied how to improve the performance of a single SSD, we consider the Hadoop performance from the perspective of a whole storage system. In our experiments, we increase the fraction of SSDs in the storage system gradually, and use the benchmark [11] to evaluate the performance of Hadoop applications. We vary configurations of multiple Hadoop parameters (such as block size, buffer size, etc) and check how to configure the system to achieve the maximum performance in a right way. The main contribution of our work can be summarized as below:  ? We build a small-scale distributed testbed with multiple physical servers, and run Hadoop applications on the testbed. We conducted a series of experiments on pure storage systems (with only HDDs or SSDs) to see the impacts of different system parameters on the perfor- mance. The tunable system parameters include dataset size, block size, buffer size, etc. We find that a linear performance improvement can be achieved with the increase of dataset size. The superior I/O performance of SSDs makes the configuration much easier than that of pure HDD storage systems. There is no need to tune the complicated tradeoff encountered when seeking the optimal configurations of block size and buffer size for pure HDD storage systems.

? We test the Hadoop performance under various frac- tions of SSD storage in the system. The results show that the Hadoop performance can be increased linearly with the increasing fraction of SSDs. Our findings indicate that a better approach for the upgrade of the Hadoop storage system is to increase the percentage of SSDs linearly according to the scale of the applications.

? We conduct a series of experiments to evaluate the impacts of other system parameters when running the Hadoop application on hybrid storage systems. We find that the configuration of block size and buffer size for the HDD-dominant (SSD-dominant) storage system is similar to the case of pure HDD (SSD) storage system.

The rest of this paper is organized as follows. The experiment framework is introduced in Section II. We show   DOI 10.1109/CBD.2013.39     all the experiment results in Section III. In Section IV, we provide discussions on our experimental results. Section V concludes our work.



II. EXPERIMENT FRAMEWORK  A. Hardware Architecture  To run Hadoop applications, we build a small-scale server cluster, which consists of 5 commodity computers. One server acts as the master node and the other four servers act as slave nodes in the Hadoop environment. The detailed configurations of four slave nodes in the testbed are listed in Table I.

These five nodes are connected via a fast-Ethernet switch with a network transmission capacity of 100Mbps. Figure 2 shows the topology of the cluster testbed.

The operations of NameNode and JobTracker are executed on the master node, while each slave node takes the duties of both DataNode and TaskTracker. The storage devices of each slave node consists of an HDD and an SSD. The storage ca- pacity of each SSD is 128GB. The operating system of each slave node is running on the HDD. When running Hadoop applications, HDD and SSD can be used interchangeably to store HDFS files and MapReduce intermediate files. The usage of storage devices can be controlled by configuring multiple parameters, including dfs.data.dir, mapred.local.dir, mapred.system.dir and mapred.temp.dir. In a hybrid storage system which consists of HDDs and SSDs, the percentage of SSDs can be set by the above approach. The architecture of the hybrid storage system is illustrated in Fig. 1. The storage devices used by HDFS and local file system are controlled by Hadoop storage configurations.

B. Benchmark  Although Hadoop has become a powerful tool to design large-scale data processing applications, its fine-grained per- formance analysis remains complicated. There are a set of representative benchmark programs having been designed to characterize the behaviors of Hadoop. For example, Sort, which aims to sort a randomly generated dataset; Grep, which can count the number of matches of a regex in the  Table I HARDWARE AND SOFTWARE CONFIGURATIONS OF SLAVES  Slave CPU Memory Slave1 Intel Core i3 3.10GHz 4GB DRAM Slave2 Intel Core i3 3.10GHz 4GB DRAM Slave3 Intel Core i3 3.10GHz 4GB DRAM Slave4 Intel Core i3 3.20GHz 4GB DRAM  Slave Hard Disk Drive Solid State Drive Slave1 SATA III 7200RPM SanDisk SDSSDHP128G Slave2 SATA III 7200RPM SanDisk SDSSDP128G Slave3 SATA III 7200RPM SanDisk SDSSDP128G Slave4 SATA II 7200RPM SanDisk SDSSDP128G  Slave Operation System JDK Version Hadoop Version All CentOS 6.3 JDK 1.6.0 31 Hadoop 1.0.4  input dataset; and WordCount, which is designed to count the word appearance frequencies, etc. In our experiments, we take the Sort program as the benchmark program with the following reasons, (i)the Sort program is a representative of most real-world MapReduce jobs, in which data is shuffled from one representation to another. (ii)The objective of our work is to investigate the impacts of storage system on the performance of Hadoop applications. It is preferable to select I/O intensive benchmark programs. The execution of the Sort program will incur a large volume of I/O operations.

We aim to seek an appropriate design of storage systems to handle incurred I/O operations to guarantee the application performance.

The benchmark suite Hibench [11], designed by Intel team, is used to evaluate the performance of Hadoop applica- tions in our experiments. Before running the Sort benchmark program, the RandomWriter program is executed to generate a random input dataset being written into HDFS. The Sort program will read the input dataset from HDFS, and write it back to HDFS after being sorted.



III. PERFORMANCE RESULTS  In our experiments, we evaluate the performance of Hadoop applications over pure storage systems and hybrid storage systems. In pure storage system, either HDDs or SSDs can be used as storage devices. Through the exper- iments over pure storage systems, we can find the upper bound of performance improvement that SSDs can bring to Hadoop applications. By testing the performance of Hadoop applications running over hybrid storage system, which consists of both HDDs and SSDs, we can reveal whether  Hadoop MapReduce  HDFS LocalFile System  Hadoop Storage Configurations  Input And Output File I/O  Intermediate File I/O  HDD SSD  Figure 1. Architecture of the hybrid storage system.

Slave Node  Slave Node  Slave Node Slave Node  Master Node  Commodity Computer  Switch  Figure 2. Topology of the server cluster.

Hadoop can exploit SSDs in a smart manner to highlight the advantages of SSDs when configuring the storage system with heterogeneous storage devices.

A. Pure Storage Systems  In this subsection, we conduct a series of experiments over pure storage systems. Although SSDs can improve the performance of a single I/O operation, it is still not well explored how the volume of I/O operations affects the efficiency of SSDs. Thus, we adjust the load of I/O operations in various ways, such as varying the size of input dataset and modifying I/O performance related Hadoop parameters. We aim to provide guidelines for application designers on how to configure system parameters in a right manner to guarantee application performance.

1) Varying DataSet Size: We configure the Ran- domWriter program to generate a set of datasets with differ- ent sizes. Each dataset consists of 40 files. The dataset size is varied from 10GB to 70GB. Considering the processing capacity of the Hadoop cluster, the dataset size is controlled to be no larger than 70GB. The size of each file in one dataset is determined by the size of the dataset, e.g., the size of a file in a 20GB dataset is 20GB/40 = 500MB.

As for the Hadoop execution environment configuration, we set the replication number of HDFS to 2. And the number of Reduce tasks per job is set to 95% of the Reducing capacity of the cluster, so that all Reduce tasks can be run in a single wave even if 5% of Reduce tasks fail accidently. In our experiments, the number of Reduce tasks is set to 11, thus the output data will be partitioned into 11 separate files.

Fig. 3(a) illustrates the execution time of the Sort program running over pure storage systems. With the increase of dataset size, the performance improvement of SSDs becomes more significant. If the dataset size is no greater than 30GB, similar performance can be achieved by HDDs and SSDs.

When the dataset size is small, HDDs can also handle I/O operations efficiently. Thus, SSDs can hardly improve application performance when processing small datasets.

The reduction of execution time over SSDs can be up to 30% compared with that over HDDs when the dataset size reaches 70GB. Moreover, the execution time over SSDs increases almost linearly with the increase of dataset size. Larger dataset size will incur higher increase rate of execution time over HDDs. Thus, SSDs are more scalable to handle the increase of the scale of the dataset.

Table II THREE PARAMETERS FOR PERFORMANCE TUNING  parameter alias in this paper description dfs.block.size block size the block size of HDFS  io.sort.mb buffer size the amount of memory buffer for sorting files in the Mapping phase of each Map task  io.sort.factor stream size the number of streams for merging files  2) Modifying Hadoop Parameters: The configuration of Hadoop consists of a set of parameters associated with two separate components, HDFS and MapReduce framework.

We focus on the parameters which can affect the I/O operations during the execution of Sort program. Table II lists 3 parameters whose configurations will have direct impacts on the behaviors of I/O operations. The physical definitions of these parameters are summarized as below:  ? Block size dfs.block.size: block is the basic unit of data stored in HDFS. It is set to 64MB in default.

In the Merge-Sort phase, each block in the buffer will be sorted. Larger block size may require external sorting, which will incur more I/O operations. Smaller block size will result in larger number of blocks and Map tasks, which increases the Map task processing overhead.

? Buffer size io.sort.mb: buffer size indicates the total amount of memory used for Sorting operations in the Mapping phase of each Map task. It is set to 100MB in default. Intuitively, a larger buffer size can reduce the number of I/O operations during the Sorting operations.

? Stream size io.sort.factor: Stream size defines the num- ber of streams that a Map task or Reduce task can merge at once, which is set to 10 in default. Larger stream size can speedup the Merging operations.

We modify one parameter while keeping the default configurations of other parameters in one experiment. The dataset size is set to 50GB. To achieve better load balancing among physical servers, we set the number of Reduce tasks per job to 175% of the Reducing capacity of the cluster. We also set both mapred.map.tasks.speculative.execution and mapred.reduce.tasks.speculative.execution to false [12], in order to avoid re-executing a task speculatively which will degrade the application performance exceptionally.

At first, we run the Sort program over pure storage systems with various block sizes. Fig. 3(b) illustrates the execution time of the Sort program when varying the block size from 16MB to 512MB. HDDs and SSDs act differ- ently to the variation of block size. When the block size is smaller than buffer size (100MB), the execution time can be reduced with the increase of block size. The block size determines the number of Map tasks which equals to the ratio of dataset size and block size. Note that, each Map task processes one input split with the same size of a block. Smaller block size will result in larger number of Map tasks, which leads to higher processing overhead for a Hadoop cluster. For instance, when the block size is 16MB, the number of Map tasks is 3000. A small scale of cluster with only 5 nodes needs a long processing time to handle such a large number of tasks. When the size of block size is larger than the buffer size, Map tasks will incur additional I/O operations to complete Sorting and Merging operations. Thus, we can see the increase of     execution time over HDDs when increasing the block size from 64MB to 128MB. While the execution time over SSDs will have small variations when the block size is larger than buffer size. The efficient I/O performance can handle the additional I/O operations well. Larger block is a better choice when running Hadoop applications over pure SSDs storage systems.

Fig. 3(c) shows the execution time of the Sort program with various buffer sizes. The execution time over SSDs are much less sensitive to the variation of buffer size than that over HDDs. The basic usage conception of the buffer is that: (i)The output records of the Mapping phase are stored in the memory buffer. Around 5% of the memory buffer size (the percentage is specified by the parameter io.sort.record.recent) is reserved to track records storing boundaries, while the rest of the memory buffer is used to store these output records. (ii) When either of the two mem- ory buffer parts reaches a pre-determined threshold which is specified by the parameter io.sort.splill.percent with a default value of 80%, then Sorting operations will be executed for the current data records in the memory buffer and these records will be flushed to the disks as a spill file after being sorted. Merging operations are needed to integrate all spill files into a single file if the number of spill files is more than one. From our experiment, we can see that the increase of buffer size does no good to performance when the buffer size is no larger than the block size. Intuitively, a smaller buffer size will incur a higher performance in the spilling phase, while more spill files which can increase the complexity of Merging operations. Since I/O operations occur in both the spilling phase and merging phase, the choice of buffer size should tradeoff the performance in the two phases when the buffer size is smaller than the block size. However, the increase of buffer size can improve the performance when the buffer size is larger than the block size. In this case, I/O operations incurred by external sorting can be avoided.

Overall, when the configuration of memory devices is not elastic, the pure SSD storage system is a better choice to reduce the configuration complexities of memory size such as the complicated tradeoff encountered in the pure HDDs storage system.

Fig. 3(d) gives the performance of the Sort program running over pure Storage systems with different stream size configurations. The parameter stream size determines the number of spill files being merged once after the Mapping phase. Similarly, the records which are processed in the Reducing phase and stored in the memory buffer will be merged into a single disk file when a user-specified buffer usage threshold is reached. The value of the stream size can also affect the performance of Merging operations in the Reducing phase. Intuitively, a larger value of stream size can speed up the Merging phase because more spill files can be merged concurrently. However, a larger stream size can possibly increase the load of I/O operations. For  instance, with the increase of the stream size, more I/O operations are needed to read large number of spill files concurrently. In this case, a lot of random read operations may occur, which will lead to bad performance if the seek latency of the storage device is high. From Fig. 3(d), the increase of stream size benefits the performance over both the pure storage systems. Due to the powerful I/O operations provided by SSDs, the variation of stream size can hardly affect the application performance over the pure SSDs storage system. The variation of execution time over SSDs is within 200s in our experiments. Thus, a reasonably large stream size can be chosen to improve the performance of Hadoop applications as long as the load of I/O operations on disks is not overloaded.

Through our experiments over pure storage systems, we find that pure SSDs storage system is more scalable to the variations of system configurations. Specifically, larger dataset size can help SSDs to bring more performance improvements. Moreover, SSDs can mitigate the compli- cated tradeoffs encountered when configuring buffer size and block size.

B. Hybrid Storage Systems  Due to the high monetary cost of SSDs, the complete re- placement of HDDs with SSDs is not preferable for the eco- nomical consideration. The constructors of storage systems tend to explore how much performance improvement can be achieved when integrating a certain percentage of SSDs storage into a pure HDD storage system. The performance properties of such hybrid storage systems can guide the construction and configuration of the storage system. In this subsection, we aim to investigate the performance properties of hybrid storage systems from various angles. At first, we run a series of experiments to verify the performance improvements when increasing the integration percentage of SSDs. Then, we try to find the optimal configuration of Hadoop applications when varying the integration percent- age of SSDs.

1) Varying SSD Percentage: To investigate the impacts of SSD integration percentage, we set all other system parameters (such as block size, buffer size, and stream size) to default values. We set the SSD integration percentage to 0%, 25%, 50%, 75% and 100% in our experiments. Fig. 4(a) shows the performance with different percentages of SSDs.

The increasing percentage of SSDs can reduce the execution of the Sort program linearly.

In Fig. 4(b), the average execution time in the Mapping phase and Reducing phase are illustrated. The performance reduction in the Reducing phase dominates overall perfor- mance improvements.

2) Modifying Hadoop Parameters: Fig. 5(a) shows the performance of the Sort program running over different hybrid storage systems when varying block size. The inte- gration percentage of SSDs is denoted by fSSD. When the     10 20 30 40 50 60 70              Size of Data Set (GB)  E xe  cu tio  n T  im e  (s )  Pure HDD Pure SSD  (a) Performance under different dataset sizes  16 32 64 128 256 512            Block Size (MB)  E xe  cu tio  n T  im e  (s )  Pure HDD  Pure SSD  (b) Performance with different block size  20 40 60 80 100 120        Buffer Size (MB)  E xe  cu tio  n T  im e  (s )  Pure HDD Pure SSD  (c) Performance with different buffer size  2 4 6 8 10 12             Stream Size  E xe  cu tio  n T  im e  (s )  Pure HDD Pure SSD  (d) Performance with different stream Size  Figure 3. Performance over pure storage systems with various parameters  0% 25% 50% 75% 100%           SSD Percentage  E xe  cu tio  n T  im e  (s )  (a) Performance of Hadoop job  0% 25% 50% 75% 100% 8.5   9.5   10.5   11.5  SSD Percentage  A ve  ra ge  M ap  E xe  cu tio  n T  im e  (s )         A ve  ra ge  R ed  uc e  E xe  cu tio  n T  im e  (s )Map Task  Reduce Task  (b) Performance of Mapping phase and Reducing phase  Figure 4. Performance with different percentage of SSDs  block size is smaller than the buffer size, the performance can be improved because the number of Mapping tasks is decreased with the increase of block size. If the block size is larger than the buffer size, the performance properties depend on the value of fSSD. The execution time varia- tion is more stable when fSSD is not smaller than 50%.

Overall, more SSD integration can improve the application performance. Compared with the case of fSSD = 0%, the performance improvement is 3.8%, 15.8%, 23.6% and 39.5% when fSSD equals to 25%, 50%, 75% and 100% respectively and the block size is 512MB. When fSSD ? 50%, the performance of the Sort program approaches to that over pure HDDs storage system, while it approaches to that over pure SSDs storage system when fSSD ? 75%.

According to the integration percentage of SSDs, we can classify hybrid storage system into two categories, HDD- dominant hybrid storage system and SSD-dominant hybrid storage system. The classification boundary possibly de- pends on the execution environment of Hadoop applications.

For SSD-dominant hybrid storage system, a larger block  size can provide more performance improvements and the performance becomes stable when the block size is larger than the buffer size. The choice of block size for an HDD- dominant hybrid storage system should tradeoff the task processing overhead and I/O operation overhead.

Fig. 5(b) depicts the impacts of buffer size variations on the performance of the Sort program running over hybrid storage systems. The variation of the performance over SSD- dominant hybrid storage system approaches to the case of pure SSDs storage system closely, while the variation of the performance over HDD-dominant hybrid storage system is similar to that under the case of pure HDDs storage system.

The increase of SSD percentage can provide performance in most cases. For instance, compared with the case of fSSD = 0%, the performance improvement is 2.5%, 3.6%, 7.8% and 10.6% when the buffer size is 100MB.

Fig. 5(c) illustrates the execution time of the Sort program running over hybrid storage systems when varying the value of stream size. A larger stream size can provide more performance improvement under different hybrid storage systems. The increasing percentage of SSDs can provide performance improvement under various stream sizes.

Through our experiments over hybrid storage systems, we find that the optimal configuration block size and buffer size for SSD-dominant (HDD-dominant) hybrid storage system can follow the concepts derived from the results in the case of pure SSD (HDD) storage system.



IV. DISCUSSIONS  Our experimental study can provide guidelines for the design and configuration of a Hadoop storage system:  ? A larger dataset size drives a higher usage of SSDs. Al- most linear performance improvement can be achieved by SSDs with the increase of the dataset size and the integration percentage of SSDs. The performance improvement brought from the use of SSDs can scale with the load of data processing.

? If the percentage of SSD capacity is not so large, that is, the storage system is HDD-dominant, the block size should be configured to be close to the buffer size. The tradeoff between task processing overhead and I/O operation overhead should be considered when     16 32 64 128 256 512            Block Size (MB)  E xe  cu tio  n T  im e  (s )  f SSD  =0%  f SSD  =25%  f SSD  =50%  f SSD  =75%  f SSD  =100%  (a) Performance with different block size  20 40 60 80 100 120          Buffer Size (MB)  E xe  cu tio  n T  im e  (s )  f SSD  =0%  f SSD  =25%  f SSD  =50%  f SSD  =75%  f SSD  =100%  (b) Performance with different buffer size  2 4 6 8 10 12             Stream Size  E xe  cu tio  n T  im e  (s )  f SSD  =0%  f SSD  =25%  f SSD  =50%  f SSD  =75%  f SSD  =100%  (c) Performance with different stream size  Figure 5. Performance over hybrid storage systems with various parameters  configuring the block size according to the properties of the dataset. The complexity incurred by the tradeoff can be mitigated by integrating more SSD capacity into the storage system. The performance of Hadoop applications running over an SSD-dominant storage system is less sensitive to the variation of block size when the block size is larger than the buffer size.

? The configuration buffer size for HDD-dominant stor- age systems should consider the tradeoff between the performance in the spilling phase and merging phase, which is pretty complicated since the number of tasks is large. Due to the efficient I/O performance provided by SSDs, the performance of Hadoop application is stable when running applications over SSD-dominant storage systems, which can still provide good performance under small memory buffer size.



V. CONCLUSIONS In this work, we investigate the performance properties  of Hadoop applications on hybrid storage systems config- ured with both SSDs and HDDs. We conduct a series of experiments over pure storage systems and hybrid storage systems. We find that a linear performance improvement can be achieved by SSDs with the increase of dataset size and the integration percentage of SSDs. The integration of SSDs can mitigate the consideration of complicated tradeoffs encountered when seeking the optimal configurations of block size and buffer size for traditional HDD-dominant storage systems. In the future, we would like to consider the design of hybrid cache systems integrated with SSDs to make the utilization of SSDs smarter.

