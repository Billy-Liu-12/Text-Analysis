

Design Patterns for Internet-Scale Services Jinquan Dai  Intel China Software Center, Shanghai, P.R.China  jason.dai@intel.com

I. INTRODUCTION  There are dramatic differences between delivering software  as services in the cloud for millions to use, versus distributing  software as bits for millions to run on their PCs.

First, ?the data center is the computer?. Services are  developed to run on distributed data centers for high  scalability, high availability and low latency. The data centers  have a scale-out shared-nothing architecture (made up of  thousands of independent, unreliable servers) and are  distributed in geography. In addition, there is also a move to  many small, cheap, independent and flaky data centers for  cost and energy effectiveness. Therefore, the service needs to  be programmed to have its data set to be partitioned and  spread around independent servers, and to be replicated across  independent servers and data centers.

Second, ?the application is always offline?. Despite  increasingly ubiquitous wireless connectivity, clients will be  occasionally disconnected. In addition, due to increasingly  ubiquitous service compositions and data integrations (e.g.,  SOA, mashup and widget), remote services or data will be  occasionally unavailable to the applications (either in the  cloud or on the clients). Consequently, the application needs  to be programmed to work with an assumption that it is  always offline, by making progress based on its local states as  much as possible, and by interacting with remote partners  asynchronously to reconcile their states.

Therefore, Internet-scale services need new design patterns  and programming models for the partitioned data set with  many copies that are changed independently. This is a huge  software challenge. Big websites spend 70% of their efforts on  undifferentiated heavy lifting (e.g., partitioning, replication  and scaling) versus 30% on differentiated value (feature)  creation. This talk will review the challenges for Internet-scale  services and some of the emerging solutions to address those  challenges, based on our experience in building Internet-scale  service platforms as well as the industry best practices.



II. DATA MODEL  Traditionally, enterprise systems store their data in  RDBMS, which however is a very inefficient solution for  Internet-scale services. This is because that the complex  management, general querying and transaction support of  RDBMS is overkill for most services, and that distributed  RDBMS is usually limited in scalability and availability.

Therefore, Internet-scale services need a new data model to  deal with its unique challenges (e.g., horizontal partitioning,  dynamic schemas, absence of distributed transactions and  incremental scaling). Typically the new data model comprises  a collection of uniquely keyed elements [1], in which a single  element is guaranteed to reside on a single machine (ignoring  replications), and service applications can only perform  atomic transactions within a single element.



III. COMPUTING MODEL  Service-oriented architecture (SOA) is usually used to  distribute the complex, stateful processing of Internet-scale  services across many computers. However, adopting SOA as a  design pattern and programming model presents a few  important implications and new challenges to the services. In  practice, most services use at-lease-once message delivery  (e.g., resending messages indefinitely until acknowledged).

Consequently, the service must tolerate duplicated messages  and the out-of-order arrival of messages, and therefore it is  essential to design the service to be idempotent, commutative  and associative. In addition, it is more preferable to have  different services decoupled and integrated via asynchronous  interactions (e.g., event driven architecture).



IV. DISTRIBUTED AGREEMENT  When distributed applications attempt to make decisions  across multiple computers, they have to cope with the  uncertainty (race conditions and partial failures). In traditional  enterprise systems, distributed transactions can be used and  the uncertainty is managed by the transaction manager. Due to  the absence of distributed transactions, Internet-scale services  must manage the uncertainty in the business logic. This is  usually achieved through fine-grained workflow-style  solutions, such as two-way tentative/confirming/cancelling  operations [1], and asynchronous recovery processing.



V. CONSISTENCY MODEL  In Internet-scale services, the partitioned data set (i.e.,  elements) is replicated across different computers. Traditional  distributed database systems provide transactional (AICD)  consistency for the replicated data using techniques such as  two-phase commit. However, as determined by the CAP  theorem, out of three properties of a system (data consistency,  system availability and tolerance to network partition), one  can only achieve two at any given time. Most Internet-scale  services focus more on availability and usually work with the  BASE model ? basically available, soft state and eventually  consistent. Consequently, Internet-scale services must manage  the replicated data using the weak, eventual consistency model.

