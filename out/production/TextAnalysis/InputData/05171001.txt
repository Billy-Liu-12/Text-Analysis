A Set-Covering-Based Approach for Overlapping Resource Selection   in Distributed Information Retrieval

Abstract   Resource selection, also called server selection,  collection selection or database selection, is a foundational problem in distributed information retrieval (DIR). This paper introduces a set-covering-based algorithm for resource selection in DIR, with consideration of overlapping extent between resources.

Give different document with different weight according to its position in merged results for question Q. Only results that have not appeared in some earlier selected resource are focused on in later selected resources. The score of each resource is decided by the total weights of those merged results included in, and only the resource with max score is selected in each selecting step. So, the selecting order is the actual rank of selected resources which are used to search the question Q?, which is similar to question Q. The approach saves big searching time due to overlapping between databases and, at the same time, enhances user's recall rate and precision.

1. Introduction   With the rapid growth of the internet, digital libraries and other information source, data items are spreading across all the worldwide in special distributed systems with heterogeneous data structure. How to quickly present what a user needs from the ?information ocean? with lower cost, higher precision and higher recall from the distributed information resources is a challenging issue.

Within the information retrieval community, the problem of retrieving data items from a set of collections/databases (DBs) that are distributed in different servers is referred to as distributed information retrieval (DIR). The problem of DIR can be broken down to three major sub-problems as the following steps:  (1) Resource selection, also called collection selection, sever selection and database selection, the first step, is to choose the order in which the resources will be searched or alternately to choose a subset of the resources charge for search.

(2) Sending the query and collecting the results, the second step, is to forward the user?s query to the selection resources. After each resource retrieving a group of documents in individual text database with individual server, the DIR system collects the individual results from each of the selected resources. It can be illustrated in figure 1.

DB 1DB 1 DB 2DB 2 DB 3DB 3 DB 4DB 4 DB nDB n. . . .

. . . . . .?

Information Need  Figure 1   Problem of database selection in DIR   (3) Results merging, the finally step, is to merge all those results retrieved into a single coherent list to be presented to the user.

A great deal of related work has been done on these individual sub-problems. This paper just focuses on the sub-problem resource selection, which is a key issue and is the first step in a process that continues with search at the distributed resource and sending the query, collecting and merging the results. The primary goal in the collection selection step is to select as small a set of collections as possible to send a query to with as high a recall rate as possible. Several methods of resource selection have been proposed with pros and cons.

In this paper we introduce a novel improved set- covering-based greedy algorithm for overlapping resource selection in distributed information retrieval. The article is organized as follows: firstly, related work is previewed.

Secondly, the methodology and the algorithm employed are described. Finally, the presented work and future research are discussed.

2. Related work   2009 World Congress on Computer Science and Information Engineering  DOI 10.1109/CSIE.2009.702   2009 World Congress on Computer Science and Information Engineering  DOI 10.1109/CSIE.2009.702     To be famous methods, some resource selection algorithms use the document frequency of a term in a database as the most important evidence for the database usefulness. The Collection-Retrieval-Inference (CORI) [1] applies inference networks for collection selection, using DF (document frequency, the number of times a documents in occurs in a collection) and CF (the number of collections containing the document), which is a modification of an INQUERY document ranking method.

Some other approaches, such as decision-theoretic approach [2], consider word counts and merely utilize the summarized similarity of documents as a usefulness measure. The Cue-Validity-Variance (CVV) [3] collection ranking method is based on a combination of document frequency and cue validity variance of query terms. Better discriminate between collections, higher CVV terms have and therefore more contribution to the score. Gravano et al. proposed GlOSS, the Glossary-of- Servers Server, as an approach to the resource selection for the Boolean IR model and it is generalized as gGlOSS to be used for any information retrieval model. The GlOSS /g GlOSS [4] ranks collections according to their goodness for the submitted query, and to do so, it estimates the number of documents in each collection having similarities to the query greater than a predefined threshold, and it then sums the similarities in order to get the collection score. It has been reported that the gGlOSS ideal ranks do not estimate relevance-based ranks well and that CORI is a uniformly better estimator of relevance-based ranks than gGlOSS [5]-[8].

On the other hand, some resource selection methods based on language model have been claimed to perform better than CORI [9]-[12]. They work with a database as a ?virtual document? by concatenation all its documents and simulate document retrieval. Additionally, methods based on pseudo-relevance feedback combined the statistic of term frequency and language model [13]. Peers ranking and final documents ranking are refined with use of the pseudo-relevance feedback from the best peer in the preliminary peers ranking.

Moreover, a multi-object mode for resource selection considering a document?s relevance to the given query, time, monetary cost and chance of getting document duplicates from resources simultaneously [14]. Central- rank-based approach [15] proposed a new collection- selection method based on the ranking of downloaded sample documents for collection selection in uncooperative distributed information retrieval, with the assumption that all collections are using the same retrieval model with equal effectiveness. French et al. proposed that mapping users queries into the controlled indexing vocabulary and employing the controlled indexing vocabulary to achieve performance gain for resource selection [16]. Most of the previous researches are ground on statistics and reckon without the overlap of resources  in DIR, which affect efficiency and effective of the computation heavily.

In this paper, we take overlapping extent between databases into account and propose a novel technique for collection selection in DIR based on set-covering, neither on document frequency nor on language model, which will increase both the efficiency and the effective of distributed information retrieval.

3. The improved set-covering greedy algorithm for resource selection  Previous techniques mostly fall into two classes: based on document/term frequency statistics (example is CORI and GlOSS) and based on language model. This paper focuses on how to create the smallest possible series collections to ensure a high recall rate as far as possible.

3.1. Definition of variations and functions   Firstly, some definitions are shown in table 1.

Notationn Meaning M The total number of search resource  including one of answers to a given query Ci The ith search resource, i=1,2,?,M C The set of servers known to a search broker,  C=(C 1, C 2,?, CM  ) m The total number of selected resources Cj? The jth selected search server, sorted by  relevance,  j=1,2,?,m SCj? The score of Cj?,  j=1,2,?,m C? Subset of C selected to answer a query ,  C?=(C1?,C2?,?,Cm? ) n The number of top n documents in the  broker?s merged search results kCi The kth document that appears in resource Ci kCj? The kth document that appears in resource Cj? ?  Here ? is a constant weighting parameter. In  this paper, we let ?=1 Table 1: Notations and meanings  The contribution of the kth document that appears in  different resources has the same useful score  k ? , just is  marked in form with  ( )ikC ?  (i=1 to M) in search resource  and '  ( )jkC  ?  (j=1 to m) in selected resource respectively.

Especially, considering duplicated ratio of documents in different resources, all the documents? contribution in the later selected resource is ignored if they have appeared in  previous selected resource. The formula  '  ( )  l  i j j  kC kC ?  =  ?? means  only the kth document that has not appeared in some earlier selected resource is focused on. That is to say, one document cannot be used to score in two or more     resources when selected. So scores of selected collections C1?, C2?,?, Ci?,... Cm? can be calculated as the following equations:  '  1 1  ( )max  nM  i k i  SC kC ?= =  = ? (1)  '  '1 1    ( ) max  nM  l l i k  i j j  SC kC kC ?  ? = =  =  = ?  ? ?  2 l m? ?           (2) Figure 2 presents the process of this paper?s idea.

'  1 1  ( )max  nM  i k i  SC kC ?= =  = ?  '  '1 1    ( ) max  nM  l l i k  i j j  SC kC kC ?  ? = =  =  = ?  ? ?  2 l m? ?  Figure 2 The process of the paper?s idea  3.2. Description of the algorithm   In practice, if we can select those all m collections  successfully, the recall rate will reach one hundred percent for top n documents in the broker?s merged search results, which are actually distributed in M unknown search collections. For the convenience of description, we let n=10 and M=5. The case means that 10 documents are distributed randomly in 5 different databases with different overlapping extent. Furthermore, we assume there be collections C1 ={1,2,3,4}, C2 ={2,3,7,8}, C3 ={1,5,6,7}, C4 ={4,5,6,9}, C5 ={9,10}. For example, here C5 = {9, 10} means that only top 9th document and top 10th document fall in search resource C5. Then we describe the algorithm as the following steps and show intuitionisticly in figure 3.

Firstly, we select C1? (the most important resource should be selected at first time) according to equation 1.

Step1: SC1?=max{1+1/2+1/3+1/4, 1/2+1/3+1/7+1/8, 1+1/5+1/6+1/7, 1/4+1/5+1/6+1/9, 1/9+1/10}={1+1/2+1/3+1/4}. That means the firstly- selected resource C1? is C1.

After first step, we begin to consider the document duplicated ration in resources. The contribution of document that has ever appeared in some previously selected database will be ignored when computing the score of later database, as shown in equation 2. Then we get the scores of selected resources one by one.

Step2: SC2?=max{1/2+1/3+1/7+1/8, 1+1/5+1/6+1/7, 1/4+1/5+1/6+1/9, 1/9+1/10}={1+1/5+1/6+1/7}. That means the secondly-selected database C2? is C3.

Step3: SC3?=max {1/2+1/3+1/7+1/8, 1/4+1/5+1/6+1/9, 1/9+1/10} = {1/9+1/10}. That means the thirdly-selected collection C3? is C5.

Step4: SC4?=max {1/2+1/3+1/7+1/8, 1/4+1/5+1/6+1/9} = {1/2+1/3+1/7+1/8}. That means the fourthly-selected resource C4? is C2.

Step5: SC5?=max {1/4+1/5+1/6+1/9} =0. That means all the data in collection C4 are overlapped by the former selected database. So, collection C4 is not selected.  Then we get m=4.

Figure 3   Illustration of the algorithm   The novel method here indicates that not the  collections including more wanted documents is more likely to preference, but the set of collections which are complementary better each other is pre-selected. The pro of the technique is that it uses set-covering theory to optimize the collections selection considering the overlap between them, rather than uses document frequency statistics or language model, cons of which are well avoided.

4. Experimental methodology and result   Experiments have been down here to demonstrate efficiency and effectiveness of the method, and the result depicts the details clearly that selected resources how to cover the top n documents complementarily when considered the overlap between collections.

Let the top n documents in merged results be ranked in row matrix, with each item representing a document, and     different color means different selected collection. Given a set of top n documents in merged results, we can find the optimum m selected resources which cover all the top n documents with least overlapping rate from M search resources. In the experiment, we chose top 100 documents merged in results (let n=100), marked with number 1, 2,?., and 100 with the corresponding score of 1, 1/2, ?, 1/100 (let ?=1) respectively, and assume that there be only 60 databases for a query to be searched in (let M=60). The algorithm using C++ pseudo code is shown in figure 4.

Figure 4.  The set-covering-based greedy algorithm for  overlapping resources selection.

Additionally, Figure 5 depicts the details clearly that selected resources how to complementarily cover the top 100 documents which have been merged in results using the same results merging method with equal effectiveness.

In this random experiment, 24 resources, as shown in color bar, are selected from 60 search resources, and every selected resource marked with different color. Here different color means different score. For example, Blue (the color of j=1 in colorbar) as shown in the first square represents the firstly selected search server C1? with max score?which covers the 1st, 9th, 11st, 33rd, 35th, 46th, 53rd, 55th, 62nd, 64th, 82nd, 83rd, 86th, 87th, 91st, 94th, and 96th documents as illustrated in Figure 5. Black (the color of j=11 in colorbar ) presents 11st selected database, which only covers 19th document except of superposition in the  former 18 selected databases. Kelly (the color of j=24 in colorbar) presents the 24th selected database. Without regard to the documents overlapped with 23 databases which have been selected, it covers the 89th document and the 98th document.

Figure 5. The case that all 24 selected resources cover  the top 100 documents   5?Conclusion  The previous resource selection algorithms are mostly  based either on document term frequency statistics or on language model. In this paper, we have introduced a novel resource selection for DIR environment with overlapping database. We have theoretically formulated that our proposed method can outperform the current document frequency statistics or language model techniques in some way. The method, on basis of set-covering rather than statistics, has a higher recall rate and precision. In addition, after considering the overlapping extent between databases to search in, the improved set-covering greedy algorithm for resource selection in DIR performs in selecting as small a set of collections as possible with as high effectiveness and efficiency as we can.

Experiments reported in the paper are based on the assumption that n documents are the top n documents that have been merged in results, using the same results merging method with equal effectiveness. However, in practice, different results merging methods cause different top n documents. We plan to extend our experiments on resource selection with different result merging methods.

Besides, the most proper values for ? have not been investigated and will be explored in our further research.

Finally, we will do experiments on the same test bed to experimentally clarify the more effectiveness and efficiency of our method than those existing resource selection algorithms.

Acknowledgement The authors are indebted to the anonymous reviewers for their valuable comments. This research has been partially funded by Innovation     Foundation of Jiangsu University for PhD Graduates (contract number is CX08B_18x) and Natural Science Foundation of Jiangsu Province (contract number is BK2006073).

