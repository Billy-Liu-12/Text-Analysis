Association Rule Mining Using Graph and Clustering Technique

Abstract? Mining association rules is an essential task for knowledge discovery. From a large amount of data, potentially useful information may be discovered. Association rules are used to discover the relationships of items or attributes among huge data. These rules can be effective in uncovering unknown relationships, providing results that can be the basis of forecast and decision.  Past transaction data can be analyzed to discover customer behaviors such that  the quality of business decision can be improved. The approach of mining association rules focuses on discovering large item sets, which are groups of items   that appear together in an adequate number of transactions. The proposed method focuses on a combined approach to generate association rules from a large database of customer transactions. It also helps in identifying rarely occurring events. The proposed algorithm will outperform other algorithms which need to make multiple passes over the database.



I. INTRODUCTION Data mining has high applicability in the retail industry,  banking, insurance industries, and market-basket analysis [2]. Transaction database in some applications can be very large. For example Hedberg (1995) quoted that Wal-Mart kept about 20 million sales transactions per day. Such data requires sophisticated analysis. As pointed out by Blischok (1995), a major task of talented merchants is to pick the profit generating items and discard the losing items. It may be simple enough to sort items by their profit and do the selection. However, this ignores a very important aspect in market analysis ? the cross-selling effect. There can be items that do not generate much profit by themselves but they are the catalysts for the sales of other profitable items. Recently, some researchers (Kleinberg, Papadimitriou & Raghavan, 1998) suggest that concepts of association rules can be used in the item selection problem with the consideration of relationships among items.

Data mining play an important part in shopping basket data analysis, product clustering, catalog design and store layout[2]. In order to support this analysis, a sufficient amount of transactions needs to be collected and stored in a database. From this collection frequent itemset are identified and association among rules are generated. Any association rule will hold if its support and confidence are equal to or  greater than the user specified minimum support(S) and confidence(C).

The conventional algorithm for association rule discovery suffer from bottleneck such as they use complex candidate generation process that uses most of the time, space and memory. Another bottleneck is the multiple scan of the database. So new algorithms need to be designed with some modifications or improvements.

A proposed algorithm employs both graph and clustering technique to discover association rules. It reduces the database scans and eliminates some candidate item sets which cannot be frequent.



II. BACKGROUND The literature review is done to get an insight of the  association rule mining algorithms in data mining. It is necessary to identify various methodologies that could possibly used to identify the relationships among itemsets of various transactions in database.

A. Basic Concepts & Basic Association Rules Algorithms Let I=I1, I2, ? , Im be a set of m distinct attributes, T be  transaction that contains a set of items such that T  I, D be a database with different transaction records Ts. An association rule is an implication in the form of X Y, where X, Y  I are sets of items called itemsets, and X ? Y = . X is called antecedent while Y is called consequent, the rule means X implies Y. Since the database is large and users concern about only those frequently purchased items, usually thresholds of support and confidence are predefined by users to drop those rules that are not so interesting or useful. The two thresholds are called minimal support and minimal confidence respectively. Support(s) of an association rule is defined as the percentage/fraction of records that contain X  Y to the total number of records in the database.

Confidence(C) of an association rule is defined as the percentage/fraction of the number of transactions that contain X  Y to the total number of records that contain X.

Confidence is a measure of strength of the association rules.

B.  Apriori Algorithm Apriori is a classic algorithm of association rule  mining[6][7]. Apriori is designed to operate on large database containing transactions (for example, collections of   DOI 10.1109/CICN.2012.53     items bought by customers, or details of a website frequentation). As is common in association rule mining, given a set of itemsets (for instance, sets of retail transactions, each listing individual items purchased), the algorithm attempts to find subsets which are common to at least a minimum number C of the itemsets. Apriori uses a "bottom up" approach, where frequent subsets are extended one item at a time (a step known as candidate generation), and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found.

However there are two bottlenecks of the Apriori algorithm. One is the complex candidate generation process that uses most of the time, space and memory. Another bottleneck is the multiple scan of the database. Based on Apriori algorithm, many new algorithms were designed with some modifications or improvements.

C. FP-Tree FP-Growth, tree based method to find frequent pattern is  proposed [8][9][10], which   is   an extended prefix-tree structure for storing compressed, crucial information about frequent patterns, and developed an efficient  FP-tree  based mining  method. It is another milestone in the development of association rule mining, which breaks the main bottlenecks of the Apriori. The frequent itemsets are generated with only two passes over the database and without any candidate generation process. FP-tree is an extended prefix-tree structure storing crucial, quantitative information about frequent patterns. Only frequent length-1 items will have nodes in the tree, and the tree nodes are arranged in such a way that more frequently occurring nodes will have better  chances of sharing nodes than less frequently occurring nodes will have better  chances of sharing nodes than less frequently occurring ones. FP-Tree scales much better than Apriori because as the support threshold goes down, the number as well as the length of frequent itemsets increase dramatically. The candidate sets that Apriori must handle become extremely large, and the pattern matching with a lot of candidates by searching through the transactions becomes very expensive. The frequent patterns generation process includes two sub processes: constructing the FT-Tree, and generating frequent patterns from the FP-Tree.

D. Graph based Association Rule Mining A graph-based approach [11] is used to generate various  types of association rules. This approach scans the database once to construct an association graph and then traverses the graph to generate all large itemsets.

A uniform data mining framework to discover various types of association rules is given:  ? Numbering phase: In this phase, all items are assigned an integer number.

? Large item generation phase: This phase generates large items and records related Information. A large  item is an item whose support is no less than a user specified minimum support.

? Association graph construction phase: This phase constructs an association graph to indicate the associations between large items.

? Association pattern generation phase: This phase generates all association patterns by traversing the constructed association graph.

? Association rule generation phase: The association rules can be generated directly according to the corresponding association patterns.

This method focuses on the association pattern generation, because after generating the association patterns, the association rules can be generated from the corresponding association patterns.

E. Cluster Based Association Rule Mining Tsay and Chiang proposed an efficient cluster based  association rule mining method (CBAR)[12] for discovering the large itemsets. The CBAR method creates cluster tables by scanning the database once, and then clustering the transaction records to the k-th cluster table, where the length of a record is k. Moreover, the large itemsets are generated by contrasts with the partial cluster tables. This method not only prunes considerable amounts of data reducing the time needed to perform data scans and requiring less contrast, but also ensures the correctness of the mined results.

The performance is dramatically decreased in the process of many association rule algorithms. This is due to the fact that a database is repeatedly scanned to contrast each candidate itemset with the whole database level by level in the process of mining association rules. The main characteristics of CBAR are it requires a single scan of the transaction database, followed by contrasts with the partial cluster tables. This not only prunes considerable amounts of data reducing the time needed to perform data scans and requiring less contrast, but also ensures the correctness of the mined results.



III. PROPOSED METHOD Through the study of literature survey has identified  different association rule mining techniques; still there is a need to find the time efficient method to mine strong association rules from large database. Hence the proposed method is introduced to reduce the database scans.

A. Algorithm The algorithm scans the database once to build a graph of  items and a clustering-table. This scan is enough to find frequent 1-itemsets and frequent 2-itemsets. There is no need to generate candidate 2-itemsets and hence no need to scan the database to discover frequent 2-itemsets. After that, GCARM works iteratively starting from frequent 2-itemsets (F2) in the sense that frequent itemsets that are discovered in one iteration will be used as the basis to generate candidate itemsets in the next iteration.

The Build-Graph algorithm build a complete undirected graph  G  =  (V,  E)  using  all  transaction  in  the  database.

Initially, the graph G is the subgraph of the first transaction.

For each transaction t in the database, the algorithm build a complete undirected subgraph Gt  = (Vt, Et), where Vt  is the set of all items in t and Et  is the set of all edges between every 2- subset itemsets in t. A counter is associated with each vertex or edge that stores the occurrences of that vertex or edge and is initialized to 1. After building the subgraph Gt, a new version of graph G is created by merging G and Gt. If there are any similar vertices and edges between Gt and G, their counters are summed up.

GCARM (int minsup)  G  C1  = {set of all items} For all transactions t D do  Build_Graph(G, t);  Create_Cluster(t, length(t)); Parse_Graph(G); For (k = 3; Ck ; k++) do For every itemset P Ck do Wp = Calculate_weight(P,k) If Wp > minsup then Add P to Fk If Fk ? Ck+1 ?   Fk U Fk  // Union Fk with Fk to generate ck+1, Ck+2,  Build-Graph (graph G, transaction t)  For each item i t do V [G] = V[G] U  {i} Count[i] = 1;  For each 2-subset itemset e t do E [G] E[G] {e}  Count[e] = 1; If (there are similar ertices and edges)  Merge (vertices and edges);   In the clustering step, each transaction is clustered to the k-th cluster, where the length of a transaction is k. Cluster the transaction records by length and store each transaction record into the cluster table.

Create_Cluster(transaction t, int n)  // add entry in the cluster table (n)  For each item i t do  Cluster-table[i][n]=1;   The function Parse_Graph searches the graph to find frequent 1- itemsets and Candidate 2-itemsets. The function Parse_Graph traverses each vertex and edge in the graph, if the counter of a vertex is greater than or equal the minimum support then the corresponding item is inserted into the set of frequent  1-itemsets  (F1)  and all the edges in the graph represents association between 2 items which gives candidate itemset (C2)    Parse_Graph(graph G) for each vertex v  V[G] do  If (count[v] ? minsup) then F1   F1 {v};  for each edge e  E[G]do C2   C2 {e};   Now algorithm parses each candidate itmesets and  calcaulates weight (Wp) of each itemset P. Association between two items considering Wp can also be represented in a matrix format as shown in figure 3.5. If this weight Wp is more than minimum support then we add this itemset (P) to frequent itemset (Fk).  Then we perform Union operation on Fk with Fk to generate candidate itemset Ck+1 and so on.

The algorithm repeated until Ck results empty.

Calculate_weight(P,k)  Count (P)= Get count of occurrence of P from Cluster_table  // for k=2, this is count of edge from graph  Fot each subset item S  P do Psum = 0  // Get count of individual items and add it   Psum = Count [occurance(S1) OR occurance(S2)? OR occurance(P) Wp = Count (P) / Psum  Return Wp   Once the frequent itemsets have been found, generating interesting association rules is a straightforward step.

Interesting association rules are the ones that satisfy the minimum   support   and   the   minimum   confidence.

Generate-Rule (float minconf) For all frequent itemset f do  For all nonempty subset s of f do If (support(s)/support (f) > minconf) then  Add "s ? (f-s)" to the set of rules B. An Example  We provide an example to further explain the application of proposed algorithm. There are 20 transactions in the database Table 1.

Part 1 of the algorithm scans the database once to build a graph of items and a clustering-table. This scan is enough to find frequent 1-itemsets and Candidate 2-itemsets.

TABLE I.  EXAMPLE TRANSACTION DATABASE    TID ITEMS TID ITEMS TID ITEMS T1 A,B,C T8 B,C,E T15 B,C,D T2 B T9 A,B,C,D T16 A,D,E T3 A,E T10 D T17 B,C T4 A,C,D,E T11 A,B,C T18 E T5 A,C T12 C,E T19 A,C,D T6 A,C,E T13 B,C,D,E T20 A,B,C,D T7 C T14 C,D     Fig 3.1 shows final graph built after reading all the transactions for one transaction. Final graph can directly be referred to get the frequent-1 Itemset, whereas edge can be used to get the candidate set of frequent-2 Itemsets.

Figure 3.1  : Final graph G  For above example based on the built graph, cluster table can be formed as shown in Table 2 and Table 3. Cluster the transaction records by length and store each transaction record into the cluster table. Using Boolean annotation to denote the appearance and disappearance of each item.

There are four cluster tables, named  Cluster_Table(k),where 1 ? k ? 4. For above example the cluster table is formed as shown in Table 2 and Table 3.

TABLE II.  CLUSTER -1 AND CLUSTER-2  TABLE III.  CLUSTER -3 AND CLUSTER-4   The function Parse_Graph searches the graph to find  frequent 1-itemsets and Candidate 2-itemsets. For given example following metrics is presented which gives edge count to find candidate-2 itemset.

Figure 3.2 : Original metric  Function Calculate_Weight computes weight of each itemset of Ck in order to find frequent-2 itemset. To calculate weight between two items i and j of individual itemset following method is used in the proposed algorithm.

Wp= Count of Occurrence of items ( i, j) / Count of occurrence of(i ) OR  Count of occurrence of  ( j ) OR  Count of occurrence of  ( i , j ).

Now to find frequent-2 itemset the appropriate support is considered and for that the association value indicated in above matrix is re-calculated to get strong frequent-2 itemset. The modified metric is shown in fig. 3.3.

Figure 3.3 :   Modified metric  Candidate-2 itemset for above example will be {AB, AC, AD, AE, BC, BD, BE, CD, CD, DE}. Considering minimum support as 30% and based on above matrix frequent-2 itemset will be resulted as {AC, AD, BC, CD}.  After joining above frequent-2 itemset (F2) with (F2) we get candidate-3 itemset as below {ACD, ABC, and BCD}.

In order to generate frequent-3 itemset, it is necessary to compute the number of occurrence of each candidate in the cluster-3. Again the entire cluster table is scanned to find the number of occurrence of every item of every subset of candidate itemset. Consider the min support as 20%, for given example freguent-3 itemset (F3) is {ACD, ABC, BCD}.

Generate the Candidate-4 itemsets (C4 ) by combining the items of F3 in order to generate candidate 4-itemsets that are labeled C4. The itemset of C4 is {A, B, C, D}. The minimum support (MinSup) is specified as 15%. The candidate itemset {A, B, C, D} occurs only twice in the Cluster_Table(4) therefore its support is 10%, less than the minimum support 15%. So F4=NULL, and so is C5 and the algorithm terminates.

Once the frequent itemsets have been found, generating interesting association rules is a straightforward step.

Interesting association rules are the ones that satisfy the minimum   support   and   the   minimum   confidence.

Itm Cluster-1 Cluster-2 A 0 0 0 0 1 1 0 0 0 B 1 0 0 0 0 0 0 0 1 C 0 1 0 0 0 1 1 1 1 D 0 0 1 0 0 0 0 1 0 E 0 0 0 1 1 0 1 0 0  Itm Cluster-3 Cluster-4 A 1 1 0 1 0 1 1 1 1 0 1 B 1 0 1 1 1 0 0 0 1 1 1 C 1 1 1 1 1 0 1 1 1 1 1 D 0 0 0 0 1 1 1 1 1 1 1 E 0 1 1 0 0 1 0 1 0 1 0  Items A B C D E A 0 4 8 5 4 B 4 0 8 4 2 C 8 8 0 7 5 D 5 4 7 0 3 E 4 2 5 3 0  Items A B C D E A 0 26% 47% 35% 28% B 26% 0 50% 28% 13% C 47% 50% 0 41% 27% D 35% 28% 41% 0 21% E 28% 13% 27% 21% 0  D         B9  E8  C1  A1      For given example for frequent item subset {ACD} associations rules will be generated by given algorithm are as follows,  R1: A^C?D    ACD/AC      4/8 50% R2: A^D?C    ACD/AD 4/5 80% R3: C^D?A     ACD/CD   4/7 57% R4: A?C^D    ACD/A 4/10 40% R5: C?A^D   ACD/C 4/15 26% R6: D?A^C   ACD/D 4/9  44%   From above results it is clear that rule R2 is strongest  among all association rules. Similarly for remaining frequent itemset the association can be found.

IV. EXPERIMENTAL DATA.

We are using dataset of WEKA tool to verify the  correctness of the algorithm. Results will be compared with the traditional cluster based as well as graph based association rule mining method. To verify the results the proposed method is applied on the datasets of different size.

Such as dataset containing 500, 1000, 2000, 3000, 4000 records with the varying items.



V. PERFORMANCE COMPARISON We have applied this algorithm on different size of data  Such as 500, 1000, 1500, 2000, 2500, and 3000 to ensure the correctness of the algorithm. The results of this algorithm are compared with the cluster and graph based association rule mining algorithm.

The performance of proposed algorithm with respect to no of scans is shown in Fig. 5.1.

0 500 1000 1500 2000 2500 3000 3500  No. of Records  No . o  f s ca  ns Graph Based Cluster based Proposed Algorithm   Figure 5.1:    No. of scans required for different sizes of dataset  Processing time required for every method is calculated and analysed and the results are obtained as shown in fig. 5.2.

These results are based on the parameter such as processor speed, clock rate, main-memory size etc.

0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  0 500 1000 1500 2000 2500 3000 3500  No. of Records  Pr oc  es si  ng T  im e  (S ec  .)  Graph Based Method Cluster Based Method Proposed Method   Figure 5.2:     Processing Time required for different sizes of dataset

VI. CONCLUSION Many association rule mining techniques are available  and they are still evolving to get faster and accurate results.

Every technique is an improvement over other but the drawback of existing methods are, they take more time to scan the huge database again and again. The only intention behind this study is to find the efficient technique to mine strong association rule among the itemsets of large database in short span of time. The proposed method is also useful to identify rarely occurring events in case of specialized application of Data Mining.



VII. REFERENCES [1] Wael Ahmad AlZoubi,Khairuddin Omar, Azuraliza Abu Bakar, ?An  Efficient Mining of Transactional data using Graph Based Technique?, IEEE, 3rd Conference on Data Mining and Optimization(DMO), 28-29 June 2011, Selangor, Malaysia  [2] M. S. Chen, J. Han, P. S. Yu. "Data mining: An overview from a database perspective". IEEE Trans. Knowledge and Data Engineering, pp.866-883, 1996.

[3] David A. Matsa, ?Competition and Product Quality in the Supermarket Industry?July 6, 2009.

[4]  Agrawal, R., Imielinski, T., and Swami, A. N., ?Mining association rules between sets of items in large databases?, In Proceedings of the ACM  Special Interest Group on Management of pp.207-216,1993.

[5]  Qiankun Zhao, Sourav S. Bhowmick, ?Association Rule Mining: A Survey?,Technical Report, CAIS, Nanyang Technological University, Singapore,  2003.

[6] Sotiris Kotsiantis, Dimitris Kanellopoulos, ?Association Rules Mining: A Recent Overview?, GESTS International Transactions on Computer Science and Engineering, Vol.32 (1),  pp.71-82,2006.

[7] R. Agrawal, R. Srikant, ?Fast algorithm for mining association rules in large  databases?,  Proceedings  of   International  Conference  on VLDB,  pp. 487?499,1994  [8] Jiawei Han ,  Jian Pei ,  Yiwen Yin ,  Runying Mao, ?Mining Frequent Patterns without   Candidate   Generation:   A   Frequent- Pattern   Tree Approach?, Data Mining and Knowledge Discovery, 8, pp53?87, 2004.

[9] Han, J. and Pei, J. ?Mining frequent patterns by pattern-growth: methodology and implications?, ACM SIGKDD Explorations Newsletter 2, 2, 14-20, 2000.

[10] Agarwal, R., Aggarwal, C., and Prasad, V.V.V. ?A tree projection algorithm for generation of frequent itemsets?, Journal of Parallel and Distributed Computing, pp.350?371, 2001.

[11] Show-Jane Yen and Arbee L.P. Chen, ?A Graph-Based  Approach for Discovering Various Types of Association Rules?, IEEE Transactions On Knowledge And Data Engineering, Vol. 13, No. 5, Pp. 839 ? 845, 2001.

[12] Yuh-Jiuan Tsay, Jiunn-Yann Chiang, ?CBAR: An  efficient method for mining association rules?, Knowledge-Based Systems 18 (2005) 99?105.

[13] M. A. Jabbar, Dr.Priti Chandra, B.L.Deekshatulu,?Cluster Based Association Rule Mining For Heart Attack Prediction?, Journal of Theoretical and Applied Information Technology, Vol. No.2, pp. 196- 201, 2011.

[14] Jiang Xiuying, ?Data mining based decision support system for Research and Development (ICCRD),pp.259-261,2011.

