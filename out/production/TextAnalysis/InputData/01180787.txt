Mining Association Rules in Text Databases Using Multipass with

Abstract  In this paper, we propose a new algorithm named Multipass with Inverted Hashing and Pruning (MI HP) for mining association rules between words in text databases. The characteristics of text databases are quite different from those of retail transaction databases, and existing mining algorithms cannot han- dle text databases efficiently because of the large num- ber of itemsets (i.e., words) that need to be counted.

Two well-known mining algorithms, the Apriori algo- rithm [1] and the Direct Hashing and Pruning (DHP) algorithm [8], are evaluated in the context of min- ing text databases, and are compared with the pro- posed MI HP algorithm. It has been shown that the MI HP algorithm has better performance for large text databases.

Keywords: association rules, text database, inverted  hashing, performance analysis.

1 Introduction  Mining association rules in transaction databases has been demonstrated to be useful and technically feasible in several application areas [2] , particularly in retail sales. Let I = {il,i2, ...,im} be a set of items.

Let V be a set of transactions, where each transaction T is a set of items, such that T ~ I. An associa- tion rule is an implication of the form X => Y, where X C I, y C I, and X n y = <I>. The association rule X => y holds in the database V with confidence c if c% of transactions in V that contain X also contain Y. The rule X => y has support s if s% of transac- tions in V contain X U Y. Mining association rules is to find all association rules that have support and  -confidence greater than or equal to the user-specified minimum support ( called minsup) and minimum con- fidence ( called mincon/) , respectively [1] .For exam- pIe, beer and disposable diapers are items such that  beer =>- diapers is an association rule mined from the database if the co-occurrence rate of beer and dispos- able diapers (in the same transaction) is not less than minsup and the occurrence rate of diapers in the trans- actions containing beer is not less than minconf.

The first step in the discovery of association rules is to find each set of items ( called itemset) that have co-occurrence rate above the minimum support. An itemset with at least the minimum support is called a large itemset or a frequent items et. In this paper , the term frequent itemset will be used. The size of an itemset represents the number of items contained in the itemset, and an itemset containing k items will be called a k-itemset. For example, {beer, disposable diapers} can be a frequent 2-itemset. Finding all fre- quent itemsets is a very resource consuming task and has received a considerable amount of research effort in recent years. The second step of forming the asso- ciation rules from the frequent itemsets is straightfor- ward as described in [1]: For every frequent itemset f , find all non-empty subsets of f. For every such subset a, generate a rule of the form a =>- (1 -a) if the ratio of support(1) to support(a) is at least minconf.

The association rules mined from point-of-sale (pas) transaction databases can be used to predict the purchase behavior of customers. In the case of text databases, there are several uses of mined associ- ation rules. The association rules for text can be used for building a statistical thesaurus. Consider the case that we have an association rule B =>- C, where B and C are words. A search for documents containing C can be expanded by including B. This expansion will allow for finding documents using C that do not contain C as a term. ..A closely related use is Latent Semantic Indexing, where documents are considered close to each other if they share a sufficient number of associations [5] .Latent Semantic Indexing can be used to retrieve documents that do not have any terms in common with the original text search expression by  .This research was supported in part by LexisNexis and NCR.

adding documents to the query result set that are close  to the documents in the original query result set.

The word frequency distribution of a text database  can be very different from the item frequency distri-  bution of a sales transaction database. Additionally,  the number of unique words in a text database is sig-  nificantly larger than the number of unique items in a  transaction database. Finally, the number of unique  words in a typical document is much larger than the  number of unique items in a transaction. These differ-  ences make the existing algorithms, such as Apripri [1]  and Direct Hashing and Pruning (DHP) [8], ineff~ctive  in mining association rules in the text databases.

A new algorithm suitable for mining association  rules in text databases is proposed in this paper. This  algorithm is named Multipass with Inverted HaJshing  and Pruning (MI HP), and is described in Section 3.

The results of performance analysis are discussed in  Section 4. The new algorithm demonstrated s~gnifi-  cantly better performance than Apriori and DHP al-  gorithms for large text databases.

2 Text Databases i  Mining associations rules between words inl con-  trolled vocabularies has been done in [3, 4] .Doc-  uments labeled with a controlled vocabulary were  mined for association rules. Generalized association  rules can also be mined when the controlled votabu-  lary has a thesaurus by substituting broadening terms.

The number of controlled vocabulary terms a~plied  to a particular document is much smaller tham the  number of unique words in the same document. Us-  ing controlled vocabularies can improve precisiQn for  many kinds of searching. However, when the terms of  interest are new, so that they are not in the cont~olled  vocabulary, both precision and recall are adversely af-  fected. This paper is concerned with finding associa-  tions between words in text documents without using  a controlled vocabulary.

The word distribution characteristics of text data  present some scalability challenges to the algo-  rithms that are typically used in mining transaction  databases. A sample of text documents was drawn  from the 1996 TREC (Text Research Collection)![II].

The sample consisted of the April 1990 Wall Street  Journal articles that were in the TREC. There were  3,568 articles and 47, 189 unique words. Most of those .

words occur in only a few of the documents. Some of  the key distribution statistics are: !

I  48.2% of the words occur in only one document; i  13.4% of the words occur in only two documentsj  7.0% of the words occur in only three documents!.

The mean number of unique words in a document, was  207, with a standard deviation of 174.2 words. In this sample, only 6.8% of the words occurred in more than 1% of the documents. A single day sample of January 21, 1990 was taken as well. In that sample, there were 9,830 unique words, and 78.3% of the words occurred in three or fewer documents.

The characteristics of this word distribution have profound implications for the effic.iency of association rule mining algorithms. The most important impli- cations are: (1) the large number of items and com- binations of items that need to be counted; and (2) the large number of items in each document in the database.

It is commonly recognized in the information re- trieval community that words that appear uniformly in a text database have little value in differentiating documents, and further those words occur in a sub- stantial number of documents [9] .It is reasonable to expect that frequent itemsets composed of highly fre- quent words (typically considered to be words with oc- currence rates above 20%) would also have little value.

Therefore, text database miners need to work with itemsets composed of words that are not too frequent, but are frequent enough. The range of minimum and maximum support suitable for word association min- ing is not known at this time. However, it is clear that word association mining will require using min- imum support levels that are significantly lower than the ones typically used for pas transaction databases.

The relatively low minimum support required for text database mining exacerbate the problems caused by the word frequency distribution. In text docu- ments, the preponderance of the words are of mod- erate or low frequency and these words are precisely the words of interest for finding frequent itemsets.

3 Multipass with Inverted Hashing  and Pruning (MI HP) Algorithm The new Multipass with Inverted Hashing and  Pruning (MI HP) Algorithm is a combination of the Multipass-Aprirori (M-Apriori) algorithm [6] and the Inverted Hashing and Pruning (IHP) algorithm [7] that we proposed. They are described below in Sec- tion 3.1 and Section 3.2, respectively.

Due to M-Apriori, we can reduce the the required memory space by partitioning the frequent l-itemsets and processing each partition separately. At the same time, by using IHP we can prune some of the candidate itemsets generated for each pass efficiently.

3.1 Multipass-Apriori (M-Apriori) Algo-  rithm The Multipass-Apriori (M-Apriori) algorithm for  mining association rules is direct descendent of the     6 frequent items in 3 partitionsApriori Algorithm [I]. Apriori is not suitable foIt min- ing frequent itemsets in text databases because of the high memory space requirement for counting t~e oc- currences of large number of potential frequent iitem- sets. The Multipass approach directly reducEjs the required memory space by partitioning the frequent I-itemsets, and processes each partition separ,ately.

Each partition of items contains a fraction of the set of all items in the database, so that the memory space required for counting the occurrences of the s~ts of items within a partition will be much less thap the case of counting the occurrences of the sets of ~ll the items in the database.

The M-Apriori algorithm is described as follows:  1. Count the occurrences of each item in the  database to find the frequent l-itemsets.

2. Partition the frequent l-itemsets into p partitions,  Pl,P2,...,Pp. !

Figure 1:  M-Apriori Partitioning a set of 6 frequent items for  3. Use Apriori algorithm to find all the frequent itemsets whose member items are in each parti- tion, in the order of Pp, Pp-I , ..., PI, by scanning the database. When partition P p is processed, we can find all the frequent itemsets whose member items are in Pp. When the next partition !Pp-I is processed, we can find all the frequent iteIbsets whose member items are in Pp-I and Pp.This is because, when Pp-I is processed, the ite$ls in Pp-I are extended with the frequent itemsets we found from P p and then counted. This procJdure is continued until PI is processed.

Assume, without loss of generality, that the fre- quent l-itemsets (or, simply items) are ordered lex- ically. The frequent items are partitioned into pi par- titions, P1,P2, ...,Pp, such that for every i < j, ~very item a E Pi is less than every item b E Pj. ~hus the itemsets under consideration for some partiti(j)n Pi have a particular range of item prefixes. Notice,that if the partitions have the same number of items~ the potential number of itemsets that will be formed by extending a lexically lower ordered partition will be larger than the potential number of itemsets from a lexically higher ordered partition.

Since the frequent items are ordered lexically, it is important to process the partitions in sequence from the highest ordered one to the lowest ordered lone.

This processing order is required to support the drun- ing of candidate itemsets based on the subset clQsure property of frequent itemsets. Figure 1 shows art ex- ample of partitions, where items 0, 1, 2, 3, 4, and 5  are frequent l-itemsets and are partitioned into PI, P2, and P3.

When P3 is being processed, we consider only the candidate itemsets whose members are in {2,3,4,5}, not including O and 1. Thus, the number of candi- dates will be smaller compared to the case of consid- ering all the frequent l-itemsets. Similarly, when P2 is processed, item 0 is not considered and many of the candidate itemsets can be pruned. Therefore, it re- quires less memory space during the processing as one partition is processed at a time.

Suppose that {2, 3} is the only frequent 2-itemset found as a result of processing the frequent items in partition P3. When the next partition P2 is being processed, item 1 in P2 is extended with each of the items in P3 first. As a result we can find some frequent 2-itemsets including item 1. Let's assume that {1, 2}, {1, 3}, and {1, 5} are found frequent. Then, {1,2} and {1,3} are joined into {1,2,3}, and we need to check if {2,3} is also frequent to determine whether {1, 2, 3} is a candidate 3-itemset or not. Since {2, 3} was found frequent when P3 was processed, {1,2,3} becomes a candidate 3-~temset. This explains why we need to process the l~t partition of frequent items first in the multipass algorithm. On the other hand, {1, 2,5} and {1,3,5} are not candidate 3-itemsets because {2,5} and {3, 5} were not found frequent.

In practice, if the estimated number of candidate itemsets to be generated is small after processing a     Items  B cA D E F G  Entr  Till Hash Tables  Figure 3: Till Hash Tables of frequent .itemsets  certain number of partitions, then we can merge the remaining partitions into a single partition, so that the number of database scannings will be reduced.

3.2 Inverted Hashing and Pruning (IHP)  Algorithm Inverted Hashing and Pruning (IHP) is analogous  to the Direct Hashing and Pruning (DHP) [8] in the sense that both use hash tables to prune some of the candidate itemsets. In DHP, during the k-th pass on the database, every (k + 1)-itemset within each trans- action is hashed into a hash table, and if the count of the hash value of a (k+ 1)-itemset is less than the min- imum support, it is not considered as a candidate in the next pass. In IHP, for each item, the transaction identifiers (Tills) of the transactions that contain the item are hashed to a hash table associated with the item, which is named Till Hash Table (THT) of the item. When an item occurs in a transaction, the Till of this transaction is hashed to an entry of the THT of the item, and the entry stores the number of trans- actions whose Tills are hashed to that entry. Thus, the THT of each item can be generated as we count the occurrences of each item during the first pass on the database. After the first pass, we can remove the THTs of the items which are not contained in the set of frequent l-itemsets, Fl, and the THTs of the frequent l-itemsets can be used to prune some of the candidate 2-itemsets. In general, after each pass k ?: 1, we can remove the THT of each item that is not a member of any frequent k-itemset, and the remaining THTs can prune some of the candidate (k + 1)-itemsets.

Consider a transaction database with seven items; A, B, C, D, E, F, and G. Figure 2 shows the THTs of these items at the end of the first pass. In our exam- pie, each THT has five entries for illustration purpose.

Here we can see that item D occurred in five transac- tions. There were two Tills hashed to 0, one Till hashed to 1, and two Tills hashed to 4.

Items  B c D E F G  O  Entries 7  If the minimum support count is 7, we can remove the THTs of the items B, D, E, and F as shown in Fig- ure 3. Only the items A, C, and G are frequent and are used to determine G2, the set of candidate 2-itemsets.

Based on the Apriori algorithm, {A,C}, {A,G}, and {C,G} are generated as candidate 2-itemsets by paring the frequent l-itemsets. However, in IHP, we can elim- inate {A,G} from consideration by using the THTs of A and G. Item A occurs in 12 transactions, and item G occurs in 19 transactions. However, according to their THTs, they can co-occur in at most 6 transac- tions. Item G occurs in 5 transactions whose Tills are hashed to 0, and item A occurs in no transactions that have such Tills. Thus) none of those 5 transactions that contain G also contains A. Item A occurs in 3 transactions whose Tills are hashed to 1 and item G occurs in 6 transactions with those Tills. So, in the set of transactions whose Tills are hashed to 1, items A and G can co-occur at most 3 times. The other THT entries corresponding to the Till hash values of 2, 3, and 4 can be examined similarly, and we can determine items A and G can co-occur in at most 6 transactions, which is below the minimum support level.

In general, for a candidate k-itemset, we can esti- mate its maximum possible support count by adding the minimum count of the k items at each entry of their THTs. If the maximum possible support count is less than the required minimum support, it is pruned from the candidate set.

3.3 MI HP Algorithm  The details of the MI HP algorithm are presented below:  1) 2) 3) 4)  Till Hash Tables  Database = set of transactions; Items = set of items; transaction = (TID, {x I x E Items}}; Comment: Read the transactions and count the  occurrences of each item and create a Till Hash Table (THT) for each item  Figure 2: Till Hash Tables at the end of the first pass  O  ies 2       the (k -1)-itemsets in Fk-l assuming that the items are lexically ordered in each itemset [1]. For exam- ple, if F2 includes {A, B} and {A, C}, then {A, B, C} is a potential candidate 3-itemset. Then the po- tential candidate k-itemsets are pruned in line 24 by using the property that all the (k -1)-subsets of a frequent k-itemset should be frequent (k -1)-itemsets [1]. This property is subset closure property of the fre- quent itemset. Thus, for {A, B, C} to be a candidate 3-itemset, {B, C} also should be a frequent 2-itemset.

To count the occurrences of the candidate itemsets ef- ficiently as the transactions are scanned, they can be stored in a hash tree, where the hash value of each item occupies a level in the tree [1] .

GetMaxPossibleCaunt(x) returns the maximum number of transactions that may contain k-itemset x by using the THTs of the k items in x. Let's de- note the k items in x by x[I],x[2],...,x[k]. Then GetM axPossibleCaunt(x ) can be defined as follows:  using a hash function h 5) foreach transaction t E Database do begin 6) foreach item x in t do begin 7) x.count + +; 8) x.THT[(h(t.TID)] + +; 9) end 10) end 11) Comment: Fl is a set of frequent l-itemsets 12) Fl = {x E Items I x.caunt/IDatabasel ~ mfnsup}; 13) Partition Fl into p partitions, Pl, P2, ..., Pp 14) Comment: Process the partitions in the order of  Pp,Pp-l,...,Pl 15) for (m = p;m > O;m --) do begin 16) Comment: Find Fk, the set of frequent  k-itemsets, k ~ 2, whose members are in partitions Pm, Pm+l , .., , Pp  17) for (k = 2; Fk-l ~ 4>; k + + ) do begin 18) Comment: Initialize Fk before Pp  is processed if m = p then Fk = 4>; Comment: Ck is the set of candida~e  k-itemsets whose members are in  Pm,Pm+l,...,Pp Comment: Fk-l * Fk-l is the natuIial join  of Fk-l and Fk-l on the first k -2 items Ck = Fk-l * Fk-l ; foreach k-itemset x E Ck do if 3y I y = (k- 1)-subset of x and y jt Fk-l  then remove x from Ck; Comment: Prune the candidate k-itlemsets  using the THTs foreach k-itemset x E Ck do  if GetMaxPossibleCaunt(x)/IDatabasel < minsup then remove x fro~ Ck;  Comment: Scan the transactions to! count the occurrences of candidate k-itehtsets  i foreach transaction t E Database dQ begin  foreach k-itemset x in t do if x E Ck then x.caunt + +;  19)  20)  21)  GetM axPossibleCO'Unt(itemset x)  begin k = size(x); MaxPossibleCount = 0; for (j = O;j < size(THT);j + +) do  MaxPossibleCount += min(x[I].THT[j]  x[2].THT[j], ...,x[k].THT[j]); return (MaxPossibleCount) ;  end  22) 23) 24) 25) 26)  27)  28)  29) 30)  31) 32) 33) 34) 35)  end  Fk = Fk U {x E Gk I x.count/IDatabasel ~  minsup};  36) end 37) end 38) Answer = Uk Fk ;  The formation of the set of candidate itemsetsi can be done effectively when the items in each itemset are stored in a lexical order, and itemsets are also lexi~ally ordered. As specified in line 22, candidate k-item$ets, for k ~ 2, are obtained by performing the na~ural join operation Fk-l * Fk-l on the first k -2 ite~s of  size(x) represents the number of items in the item- set x and size(T HT) represents the number of entries in the THT .

For further performance improvement, the MI HP algorithm can be used together with the transaction trimming and pruning method proposed as a part of the DHP algorithm [8]. The concept of the transaction trimming and pruning is as follows: During the k-th pass on the database, if an item is not a member of at least k candidate k-itemsets within a transaction, it can be removed from the transaction for the next  pass (transaction trimming), because it cannot be a member of a candidate (k + l)-itemset. Moreover, if a transaction doesn't have at least k + 1 candidate k- itemsets, it can be removed from the database (trans- action, pruning) , because it cannot have a candidate (k + l):itemset.

Transaction trimming and pruning is synergistic with the candidate pruning by MI HP. The reduction of candidate k-itemsets for the k-th pass, k ~ 2, will result in additional items to be trimmed during the  pass.

300000T  " -0; " E " "' 200000- i: ~ I 0- Ie ...I  ~ 100000~.0 E I " z  .1-ltemsets  D 2-itemsets  ra 3-itemsets  m 4-itemsets-  3.00% 2.75% 250% 225% 200% 1.75%  Minimum Support  Figure 4: Number of frequent itemsets  port levels upon the execution time of the five miners.

There are three distinct performance groups visible.

The slowest miner is DHP. The distribution of words in text documents and the large document sizes do not play to the strength of DHP, because the over- head of direct hashing for each pass on the database offsets the benefit of pruning candidate itemsets. A variation of DHP that uses the direct hashing only for the second pass performs about as well as Apriori.

The middle group is Apriori and M-Apriori. Note that M-Apriori is slightly faster than Apriori at the lower minimum support levels due to the improvement from the multipass approach. The faster group is composed of IHP and MI HP. The difference in performance be- tween the two groups is due to the combination of re- ducing the number of candidate itemsets and the pro- cessing efficiency gained through transaction trimming and pruning. Again, we see at the lower thresholds, the MI HP algorithm is faster than the IHP algorithm.

It is important to recall that these runs were memory constrained such that the JVM memory for objects was limited to 512 Mbytes. Since the main memory size was also 512 Mbytes, there was no virtual mem- ory paging affecting the execution times. Only MI HP was able to complete at the 1.75% minimum support level. The Apriori, M-Apriori, and IHP miners were run at the 1.75% minimum support level, but were terminated after running 100,000 seconds.

For MI HP, the effect of the partition size ( of fre- quent items) was examined in a series of runs with the minimum support level of 2.5% and partition sizes of 25,50, 100, 250, and 500 frequent items. IpFigure 6 we see that a partition size of about 100 frequent items is appropriate. This implies that multipass is effective only up to the point where the efficiency of working with a smaller number of candidate itemsets for each pass overcomes the overhead of additional scanning of the database.

4 Performance Analysis of MI HP  Some performance tests have been done with MI HP. The first objective was to assess whether the multipass approach combined with Inverted Hashing and Pruning (IHP) would improve the performance.

The second objective was to assess the scalability of the MI HP miner. To meet these objectives, we stud- ied the performance of five miners, Apriori, DHP, M- Apriori, IHP, and MI HP. All these miners were derived from the same code base. All of the test runs were made on a machine with a 400 MHz Pentium pro- cessor and 512 Mbytes of memory. All miners were written in Java, and the IBM JVM 1.1.8 was used.

For all of the tests, the JVM memory for ob- jects was constrained (via the mx parameter) to 456 Mbytes. The initial heap size was set to 456 Mbytes (via the ms parameter) to control the effect of heap growth overhead on the performance. The partition size used for the M-Apriori and MI HP was 100 fre- quent items, and the hash table size for MI HP and IHP was 500 entries. The hash table size for DHP was 500,000 entries. The minimum support ranged from 3% to 1.75% in the runs comparing the miners. For the test of scalability at various document collection sizes, 1.75% minimum support level was used.

The minimum support values for the comparison test runs was selected such that only 456 Mbytes of memory would be required. The rationale for this choice is to study the performance of the algorithms without the additional complication of accounting for the impact of paging upon the performance. At the 1.75% minimum support level, only MI HP was able to fit within the 456 Mbytes constraint. The memory constraint was lifted to 512 Mbytes, and the Apriori, M-Apriori, and IHP miners were allowed to run for 100,000 seconds each before they were terminated.

We did not use a stop-word list, but instead used a maximum support threshold of 20% to remove the common words. This threshold removed about 500 words from consideration. We also did not perform any stemming of the words. We did however monocase the words.

The comparison runs were made against the April 1990 Wall Street Journal articles. There were 3,568 documents in 11.7 Mbytes, and 47,188 unique words were in the collectio~. The number of frequent item- sets for each of the' minimum support levels can be seen in Figure 4. At the 2% minimum support level, there were total 131, 793 frequent itemsets of all sizes.

At the 1.75% minimum support level, this jumped to 242,569 frequent itemsets.

In Figure 5, we see that the effect of various sup-  f - ~~~ 'ljji;     *Apriori + M-Apriori  ~DHP  BIHP .MI HP  "  "  ~ !:!- " E F  Figure 5: Comparison of execution times Figure 7: Effect of Till hash table size on the candi- dates  *IHP  +MI HP  Figure 6: Effect of multipass partition size in M[HP  Figure 8: Effect of Till hash table size on the execu- tion time  The effect of the Till hash table size was examined in a series of runs with a minimum support leViel of 2.5% and Till hash table sizes of 10, 25, 50, 100, 250, and 500 entries. Figure 7 shows that there is a sharp reduction in the number of candidate 2-itemsets as the number of Till hash table entries increases up to 250, but only a modest decrease after that. In Figure 8 we see that the change in the size of Till hash table, and hence the change in the number of candidate itemsets, has little effect upon the total execution time until it becomes as large as 500 entries. At this point, the effect of candidate pruning clearly offsets the overhead of using the Till hash tables.

The above results suggest that the MI HP algorithm is more effective than Apriori, DHP, M-Apriori, and IHP algorithms for mining frequent itemsets from text documents. The next tests were conducted to e'i'alu- ate how well MI HP scaled in terms of the number of documents to be processed and the number of frequent itemsets mined.

There were four document collections used for the scalability test: a two-week collection (from 3/2/1992 to 3/13/1992) of 1,739 documents; a one-month col- lection (April 1990) of 3,568 documents; a two-mC)nth  collection (September and October of 1991) of 7,361 documents; and the largest collection was for three months (January, February, and March in 1992) con- taining 10,163 documents. The minimum support level was 1.75% for all test runs.

In Figure 9 we see that the time per frequent item- set mined increases proportionally with the number of documents in the collection. This suggests that MI HP will scale linearly with respect to the number of doc- uments processed. In Figure 10 we see that the time per frequent itemset mined decreases as the minimum support level decreases. The time per candidate item- set processed was constant at about 1 msec. The de- crease in time per frequent itemset mined is due to the increase in the number of candidate itemsets that be- come frequent itemsets as the minimum support level ,rlecreases.

5 Conclusions The main conclusions that can be drawn from this  study are centered around the nature of the text databases and the use of the mined association rules.

The distribution of words in text document collections  Minimum Support     Figure 10: Time per frequent itemset mined (;3,568 documents)  and the number of unique words in a document hlake the problem of finding frequent itemsets (i.e., s~ts of words) in text databases very different from the c~e of traditional point-of-sale transaction databases. i This difference motivated us to develop a new MI HP (iMul-  ,  tipass with Inverted Hashing and Pruning) algoltithm  for text databases.

Our performance analyses show that the mult~pass  approach can be effective with text databases. i The  key performance factor appears to be the reduction in the amount of required memory space. The ~ulti- pass approach reduces the number of objects in rf1em- ory during each pass by partitioning the frequetlt 1- itemsets, and processes each partition separately,  Moreover, Inverted Hashing and Pruning (IHP) can prune some, of the candidate itemsets generated for" each pass on the database efficiently. Since a lot of Till hash tables are pruned before we count the otcur- rences of the candidate 2-itemsets, most of the ~em-  I ory used for holding the Till hash tables in thelfirst pass on the database is available to hold the c~ndi- date 2-itemsets for the second and subsequent p~ses.

In large part, this effect is a result of the distribution  of word occurrences discussed in Section 2. The large  number of words with very low occurrence rates re-  suits in a situation that the preponderance of the Till  hash tables generated in the first pass are pruned prior  to the initiation of the second pass.

