Classifying Text Documents using Unconventional Representation

Abstract? Classification of text documents is one of the most common themes in the field of machine learning. Although a text document expresses a wide range of information, but it lacks the imposed structure of tradition database. Thus, unstructured data, particularly free running text data has to be transferred into a structured data. Hence, in this paper we represent the text document unconventionally by making use of symbolic data analysis concepts. We propose a new method of representing documents based on clustering of term frequency vectors.  Term frequency vectors of each cluster are used to form a symbolic representation by the use of Mean and Standard Deviation. Further, term frequency vectors are used in the form a interval valued features. To cluster the term frequency vectors, we make use of Single Linkage, Complete Linkage, Average Linkage, K-Means and Fuzzy C-Means clustering algorithms. To corroborate the efficacy of the proposed model we conducted extensive experimentations on standard datasets like 20 Newsgroup Large, 20 Mini Newsgroup, Vehicles Wikipedia datasets and our own created datasets like Google Newsgroup and Research Article Abstracts. Experimental results reveal that the proposed model gives better results when compared to the state of the art techniques. In addition, as the method is based on a simple matching scheme, it requires a negligible time.

Keywords- Classification; Text Documents; Representation; Clustering Algorithms.



I.  INTRODUCTION Text classification is one of the important research issues  in the field of text mining, where the documents are classified with a supervised knowledge. Based on a likelihood of the training set, a new document is classified.

The task of text classification is to assign a boolean value to each pair  ( , ) ,j id k D K  where ' 'D  is the domain of documents and ' 'K  is a set of predefined categories. The task is to approximate the true function : 1,0D K by  means of a function ^  : {1,0}D K such that ^  and  coincide as much as possible. The function ^  is called a classifier. A classifier can be built by training it systematically using a set of training documents D , where all of the documents belonging to D  are labeled according to K [1], [2]. The major challenges and difficulties that arise in the problem of text classification are: High dimensionality (thousands of features), variable length, content and quality  of the documents, sparse distribution of terms in documents, loss of correlation between adjacent words and understanding complex semantics of terms in a document [3]. To tackle these problems a number of methods have been reported in literature for effective classification of text documents. Many representation schemes like binary representation [4], ontology [5], N-Grams [6], multiword terms as vector [7], UNL [8] are proposed as text representation schemes for effective text classification. Also, in [9], [10] a new representation model for the web documents are proposed. Recently, [11], [12] used bayes formula to vectorize a document according to a probability distribution reflecting the probable categories that the document may belongs to. Although many representation models for the text document are available in literature, the Vector Space Model (VSM) is one of the most popular and widely used models for document representation [13].

Unfortunately, the major limitation of the VSM is the loss of correlation and context, of each term which are very important in understanding the document. To deal with these problems, Latent Semantic Indexing (LSI) was proposed by [14]. The LSI is optimal in the sense of preserving the global geometric structure of a document space. However, it might not be optimal in the sense of discrimination [14]. Thus, to discover the discriminating structure of a document space, a Locality Preserving Indexing (LPI) is proposed in [15]. An assumption behind LPI is that, close inputs should have similar documents. However, the computational complexity of LPI is very expensive. Thus it is almost infeasible to apply LPI over very large dataset. Hence to reduce the computational complexity of LPI; Regularized Locality Preserving Indexing (RLPI) has been proposed in [16]. The RLPI is being significantly faster obtains similar or better results when compared to LPI. This makes the RLPI an efficient and effective data preprocessing method for large scale text classification [16].

However the RLPI fails to preserve the intraclass variations among the documents of the different classes.

Also in case of the RLPI, we need to select the number of dimensions m to be optimal. Unfortunately we cannot say that the selected m value is optimal. Also, in conventional supervised classification an inductive learner is first trained on a training set, and then it is used to classify a testing set, about which it has no prior knowledge. However, for the classifier it would be ideal to have the information about the distribution of the testing samples before it classifies them.

Thus in this paper, to deal with the problem of learning from     training sets of different sizes, we exploited the information derived from clusters of the term frequency vectors of documents.

Clustering has been used in the literature of text classification as an alternative representation scheme for text documents. Several approaches of clustering have been proposed. Given a classification problem, the training and testing documents are both clustered before the classification step. Further, these clusters are used to exploit the association between index terms and documents [17]. In [18], words are clustered into groups based on distribution of class labels associated with each word. Information bottle neck method is used to find a word cluster that preserves the information about the categories. These clusters are used to represent the documents in a lower dimensional feature space and na?ve bayes classifiers are applied [19]. Also, in [20] information bottleneck is used to generate a document representation in a word cluster space instead of word space, where words are viewed as distributions over document categories. [20] proposed an information theoretic divisive algorithm for word clustering and applied it to text classification. Classification is done using word clusters instead of simple words for document representation. Two dimensional clustering algorithms are used to classify text documents in [21]. In this method, words/terms are clustered in order to avoid the data sparseness problem. In [22] clustering algorithm is applied on labeled and unlabeled data, and introduces new features extracted from those clusters to the patterns in the labeled and unlabeled data. The clustering based text classification approach in [23] first clusters the labeled and unlabeled data. Some of the unlabeled data are then labeled based on the clusters obtained.

All in all, the above mentioned clustering based classification algorithms work on conventional word frequency vector. Conventionally the feature vectors of term document matrix (very sparse and very high dimensional feature vector describing a document) are used to represent the class. Later, this matrix is used to train the system using different classifiers for classification. Generally, the term document matrix contains the frequency of occurrences of terms and the values of the term frequency vary from document to document in the same class. Hence to preserve these variations, we propose a new interval representation for each document. We have made an initial attempt in giving an interval representation by using maximum and minimum values of term frequency vectors for the documents [3]. In this paper, we are using mean and standard deviations to give the interval valued representation for documents. Thus, the variations of term frequencies of document within the class are assimilated in the form of interval representation.

Moreover conventional data analysis may not be able to preserve intraclass variations but unconventional data analysis such as symbolic data analysis will provide methods for effective representations preserving intraclass variations.

The recent developments in the area of symbolic data analysis have proven that the real life objects can be better described by the use of symbolic data, which are extensions of classical crisp data [25].

Thus these issues motivated us to use symbolic data rather than using a conventional classical crisp data to represent a document. To preserve the intraclass variations we create multiple clusters for each class. Term frequency vectors of documents of each cluster are used to form an interval valued feature vector. With this backdrop, in our previous work [3], we made an initial attempt towards application of symbolic data concepts for text document representation. In this paper the same work is extended towards creating multiple representatives per class using clustering before symbolic representation.

The rest of the paper is organized as follows: A detailed literature survey and the limitations of the existing models are presented in section 1. The working principle of the proposed method is presented in section 2. Details of dataset used, experimental settings and results are presented in section 3. The paper is concluded along with future works in section 4.



II. PROPOSED METHOD The proposed method has two stages: (A) Cluster based  representation of documents and (B) Document classification stage.

A. Cluster based representation In the proposed method, initially documents are  represented by term document matrix. The obtained term document matrix is of very high dimension and huge sparse matrix. We employ regularized locality preserving indexing (RLPI) dimensionality reduction technique, to reduce to a lower dimension. Unfortunately, the RLPI features of documents of a class have considerable intraclass variations.

Thus, we propose to have an effective representation by capturing these variations through clustering and representing each cluster by an interval valued feature vector called symbolic feature vector as follows.

The training documents of each class are first clustered on RLPI features. Let 1 2 3, , ,..., nD D D D be a set of n documents of a document cluster say jC ;  1,2,3,...,j N ( N denotes the number of clusters) and let M be the number of classes and let  1 2 3, , ,...,i i i i imF f f f f be the set of m features characterizing the document iD of a cluster jC . Let jk ;  1,2,...,k m be the mean of the thk feature values obtained from all n documents of a cluster jC .

i.e.,   1 n jk ik  i f  n (1)   Similarly, let jk ; 1,2,...,k m  be the standard deviation of  the thk  feature values obtained from all the n documents of the cluster jC .

i.e.,    1 n jk ik jk  i f  n (2)   Now, we recommend capturing intraclass variations in each  thk  feature values of the thj cluster in the form of an  interval valued features ,jk jkf f , where, jk jk jkf ;  jk jk jkf . Thus, each interval ,jk jkf f  representation depends upon mean and its standard deviation of respective individual features of a cluster. The interval  ,jk jkf f represents the upper and lower limits of feature value of a document cluster in the knowledgebase. Now, the reference document for a cluster jC , is formed by representing each feature 1,2,...,k m  in the form of an interval and it is given by  1 1 2 2, , , ,..., ,j j j j j jm jmRF f f f f f f        (3)  Where, 1,2,...,j N  represents the number of clusters of documents samples of a class. It shall be noted that unlike conventional feature vector, this is a interval valued features and this symbolic feature vector is stored in the knowledgebase as a representative of the thj cluster. Thus the knowledgebase has N number of symbolic vectors representing clusters corresponding to a class.

B. Document Classification The document classification proposed in this work  considers a test document, which is described by a set of m feature values of type crisp and compares it with the corresponding interval type feature values of the respective cluster stored in the knowledge base. Let,  1 2, ,...,i i i imF f f f be a m dimensional feature vector describing a test document.

Let jRF be the interval valued symbolic feature vector of thj cluster of thl class. Now, each thm feature value of the  test document is compared with the corresponding interval in jRF to examine whether the feature value of the test document lies within the corresponding interval. The number of features of a test document, which fall inside the corresponding interval, is defined to be the degree of belongingness. We make use of Belongingness Count cB as a measure of degree of belongingness for the test document to decide its class label.

, ,  m  c ik jk jk k  B C f f f  and        (4)    , ,ik jk jkC f f f 1; ; 0ik jk ik jkif f f and f f Else   The crisp value of a test document falling into its respective feature interval of the reference class contributes a value 1 towards cB and there will be no contribution from other features which fall outside the interval. Similarly, we compute the cB value for all clusters of remaining classes and the class label of the cluster which has highest cB  value will be assigned to the test document as its label. Further, there may be chances of having the same cB value for two or more classes. Under such circumstances we recommend to resolve the conflict by making use of the similarity measure proposed by [24], to measure the similarity between the test feature vector qR and the  thj class representative jR as given by,  _ ( , ) ( ,[ , ])  m  q j ql jl jl l  Total Sim R R Sim f f f   (5)  Here, [ , ]jl jlf f represents the thl feature interval of the  thj class, and , ,ql jl jlSim f f f 1; ,ql jl ql jlif f f and f f           (6)   max 1 1 ,1 1ql jl ql jlOtherwise f f f f   Similarity values of the test document with all k classes  are computed and then the test document is classified to be a member of the class for which it has a highest similarity value.



III. EXPERIMENTAL SETUP  A. Datasets To test the efficacy of the proposed model, we have used  the following five datasets. The first dataset is standard 20 Newsgroup Large. It contains 20000 documents categorized into 20 classes. The second dataset is a standard 20 Mini Newsgroup dataset which contains about 2000 documents evenly divided among 20 Usenet discussion groups. This dataset is a subset of 20 Newsgroups which contains 20,000 documents. In 20 Mini Newsgroup, each class contains 100 documents in 20 classes which are randomly picked from original dataset. The third dataset consists of vehicle characteristics extracted from wikipedia pages (Vehicles - Wikipedia). The dataset contains 4 categories that have low degrees of similarity. The dataset contains four categories of vehicles: Aircraft, Boats, Cars and Trains. All the four categories are easily differentiated and every category has a set of unique key words. The fourth dataset is constructed by a text corpus of 1000 documents that are downloaded from Google-Newsgroup. Each class contains 100 documents     belonging to 10 different classes (Business, Cricket, Music, Electronics, Biofuels, Biometrics, Astronomy, Health, Video Processing and Text Mining). The fifth dataset is a collection of Research Article Abstracts. All these research articles are downloaded from the scientific web portals. We have collected 1000 documents from 10 different classes. Each class contains 100 documents.

B. Experimentation In this section, we present the results of the experiments  conducted to demonstrate the effectiveness of the proposed method on all the five datasets viz., 20 Newsgroup Large, 20 Mini Newsgroup, Vehicles Wikipedia, Google Newsgroup and Research Article Abstracts. During experimentation, we conducted two different sets of experiments. In the first set of experiments, we used 50% of the documents of each class of a dataset to create training set and the remaining 50% of the documents for testing purpose. On the other hand, in the second set of experiments, the numbers of training and testing documents are in the ratio 60:40. Both experiments are repeated 5 times by choosing the training samples randomly. As measures of goodness of the proposed method, we computed classification accuracy. The minimum, maximum and the average value of the classification accuracy of all the 5 trials are presented in Table 1 to Table 5.

For both the experiments, we have randomly selected the training documents to create the symbolic feature vectors for each class. While conducting the experimentation we have varied the number of features m (empirically) selected through RLPI from 1 to 30 dimensions. For each obtained dimension we create cluster based interval representation as explained in section 2.1. At this juncture, we used Single Linkage, Complete Linkage, Average Linkage, K Means and Fuzzy C Means (FCM) clustering algorithms to create a cluster based symbolic representation. However, it can be observed from the Table 1 to Table 5, Fuzzy C Means (FCM) clustering algorithm achieved a better results (for 3 clusters). The reason behind this is FCM has its ability to discover cluster among the data, even when the boundaries among the data are overlapping. Also, FCM based techniques have the advantage over the conventional statistical techniques like NN classifier, maximum likelihood estimate etc, because its distribution is free and no knowledge about the distribution of data is required [26]. A comparative analysis of the proposed method with other state of the art techniques using standard classifiers and benchmark datasets is given in Table 6. The number 3 in the bracket indicates the number of clusters selected (empirically selected). From the Table 6 it is analyzed that the proposed method achieves better classification accuracy.



IV. CONCLUSION In this paper, a simple and efficient symbolic text classification is presented. A text document is represented by the use of symbolic features. Term frequency vectors of each cluster are used to form a symbolic representation by the use of interval valued features. To check the  effectiveness and robustness of the proposed method, extensive experimentation is conducted on various datasets.

The experimental results reveal that the proposed method outperforms the other existing methods. With the successful application of concepts of symbolic data to text classification, it is our firm belief that this work will open up new avenues to exploit the concepts of symbolic data for effective text representation.

In future, our research will emphasize in enhancing the ability and performance of our model by considering other parameters to effectively capture the variations between the classes, which in turn improves the classification accuracy.

Along with this, we have a plan of exploiting other similarity/dissimilarity measures, selection of dynamic threshold value and studying the classification accuracy for the varying dimensions. Besides we are also targeting towards the study of complexity issues of the proposed model with the existing representation models.

