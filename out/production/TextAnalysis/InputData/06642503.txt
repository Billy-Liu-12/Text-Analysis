Probability-Based Incremental Association Rule Discovery   Using the Normal Approximation

Abstract  An incremental association rules mining is one of an association rule mining research topics which finds the relation between set of item in dynamic databases. As data grows up rapidly, the co-occurrence itemset which discovered in the previous mining may be changed and the association rule will be change consequently.

Incremental association rule mining research attempts to maintain that rules. Probability-based algorithm, one of an incremental algorithm, applied the principle of Bernoulli trial to predict expected frequent itemsets for reducing collected border itemsets and a number of times to rescan the original database. However, the numerical problem will occur when the algorithm deals with a large database. To manipulate with this problem, the improved probability-based incremental association rule discovery using normal approximation to estimate the probability of occurrence of expected frequent itemset is introduced in this paper. In addition, the confidence interval is applied to ensure that the collecting of expected frequent itemsets is properly kept.

Keywords: Data Mining, Incremental Association Rule Discovery, Normal Approximation  1. Introduction Association rule mining is well known as one of a core  task of data mining in knowledge discovery in databases process. It was first introduced by Agrawal et al. [1] in a pilot study of the market basket analysis which found simultaneous bought itemset. It shows the perspective of information hidden in large databases in the form of if antecedent then consequent (X ? Y) such as customers who buy beers they must also buy diapers. The process of basic rule discovery operation is defined as follows.

Firstly, counting co-occurrence itemsets in entire databases, secondly, itemsets which is greater than or equal to minimum support threshold is collected to create an association rule. Finally, rule which is greater than or equal to minimum confidence threshold will be a valid  extracted association rule. From that research, the first market basket analysis, the problem statement of an association rule mining is commonly defined as follows.

Let I = {I1,I2,?,Im} be a set of literal items. DB is a database which contains transactions. Each transaction T is a set of items where T ??I. Given X is an item and X ??I. Each transaction contains X if and only if X ??T. Let X and Y are items where X ??I, Y ??I and X ??Y??. Set of items, itemsets, is called a frequent itemset or large itemset if and only if its support count, calculated from a number of transactions in DB that contain X ??Y, is greater than or equal to support threshold s%.  Each frequent itemset can be made the association rule if and only if it is satisfied by confidence threshold c% which calculates from a number of transactions in DB that contain X and also contain Y. Both s% and c% are liberally defined by users.

The number of researchers has studied extensively about association rule mining research area in many issues. Incremental association rule discovery is one of those issues which maintain rules when new transactions are appended to an original database. In fact, data has grown rapidly; thus, when a new set of transactions, called increment database, are inserted into the original database, some rules from the previous mining may be invalid. This problem has been motivating many researchers to propose several rules maintaining algorithms.

A basic and simple method for solving this problem is to rescan entire databases with Apriori algorithm [2] to get new itemsets. However, this method is time- consuming and inefficiency. By reducing a number of times to scan databases, several algorithms are proposed such as Sliding Windows Filtering (SWF) [3], Negative Border (NBd) [4], probability-based incremental association rule discovery [5] and so on. This research also proposes in this direction.

For the probability-based incremental association rule discovery algorithm, it needs only one time to scan the whole original database and works by using the principles  432IEEE IRI 2013, August 14-16, 2013, San Francisco, California, USA    of Bernoulli trials to predict the expected frequent itemsets, i.e., the infrequent itemset which can possibly be a frequent itemset. However, numerical difficulty in computing probabilities occurs when the algorithm deals with a large database. To manipulate with this problem, the improved probability-based incremental association rule discovery using normal approximation to estimate the probability of occurrence of expected frequent itemset is introduced in this paper. In addition, a statistical confidence interval is applied to ensure that the collecting of expected frequent itemsets is properly kept.

The other parts of this paper are organized as follows.

The literatures of an association rule mining and incremental association rule mining are reviewed in section 2. The problem statement of an incremental association rule mining and the probability-based incremental association rule discovery algorithm are detailed in section 3. Probability-based incremental association rule discovery using the normal approximation and the experiment are presented in section 4 and 5 respectively. The summary and conclusion is described briefly in section 6.

2. Related Work In 1993, association rule discovery was first proposed  by Agrawal et al. [1] in a pilot study in market basket analysis which found the relationship between the buying items in a retail transaction database. Next year, Apriori [2], the most popular algorithm of association rule mining, was issued. Apriori is normally divided into 2 major steps: finding frequent itemsets (sometimes called large itemsets) and generating rules. After Apriori was revealed, there are many researchers propose algorithms in this field.

In the real world, databases are dynamic. The database size has been enlarging because the new set of transactions is continuously inserted into the original database. When the new increment database, the new set of transactions, is inserted in to the original database, the old existing rule may be invalid. It is easy to rerun Apriori in whole database (consists of original database and increment database) to get the updated rules if and only if both databases and a number of item are small. In fact, the database is big and possibly bigger in over time, rerunning Apriori is not the suitable way to do because too much time is consumed. Thus, the incremental association rule discovery, one of the association rule discovery issue, is studied extensively to maintain association rules in dynamic databases.

Fast UPdate algorithm (FUP) [6] was proposed to maintain association rules in dynamic databases. It works by using frequent itemsets from previous mining in the  original database compares with frequent itemsets in the increment database. For each iteration, a frequent itemset in the increment database which is not a frequent itemset in the original database will be rescanned in the original database and updates its support count. From the FUP experiment result, even though it can save the computational time but it still needs to rescan an original database k times when new frequent k-itemsets are found.

This is the disadvantage of FUP.

Sliding Windows Filtering (SWF) [3] was proposed to reduce a number of rescanning times of an original database by dividing both original database and increment database into several partitions, and processing from the first partition to the last partition. There are 2 major procedures: preprocessing procedure and incremental procedure. Two new ideas are proposed in SWF: all 1- tiemsets are assumed to frequent itemsets and candidate k?3 itemsets are obtained from Ck-1 * Ck-1, these ideas can decrease a number of candidate itemsets. This algorithm is a good work for both deleted and inserted database. In addition, SWF requires only one time to rescan an original database.

Negative Border algorithm (NBd) [4] was proposed to reduce the number of rescanning times of an original database by collecting both frequent itemsets and border itemsets (itemset which is not frequent itemsets but its proper subsets are frequent itemset). This algorithm is successful for reducing the number of rescanning times but a large number of border itemsets have to collect.

Thus this Negative Border consumes a large amount of memory. Moreover, in the worst case, Negative Border algorithm needs to rescan an original database several times when new frequent itemsets are discovered in an updated database.

To solve with the collecting of massive border itemsets problem, Tsai et al. [7], Hong et al. [8], and Amornchewin and Kreesuradej [5] presented algorithms which are not only reduce a number of collecting border itemsets but also decrease a rescanning time. These algorithms keep both frequent itemsets and expected frequent itemsets, i.e., itemsets which are able to be frequent itemset. However, each researcher gives the quite different method to select expected frequent itemsets. Tsai et al. determined a new threshold, called tolerance degree. It collaborates with support threshold to select expected frequent itemsets. As Hong et al. assigned the 2 news thresholds, upper support and lower support threshold, to select expect frequent itemsets.

Amornchewin and Kreesuradej applied the probability theory (the principle of Bernoulli trial) and defined a new threshold, probpl, to predict expected frequent itemsets.

Our proposed algorithm was based on this direction.

As mentioned to the previous study [5], we observed that the binomial probability has a numerical problem when factorial is computed with a large value, i.e., factorial of a large value cannot fit to the size of an integer variable. To solve this problem, the probability of occurrence of expected frequent itemset estimation using normal approximation is introduced in section 4.

3. Incremental Association Rule Mining In this section, an incremental association rule mining  concept is introduced concisely. The problem statement of an incremental association rule mining is described in subsection 3.1 and the probability-based incremental association rule discovery algorithm is reviewed in subsection 3.2.

3.1 Problem Statement of the Incremental Association Rule Mining  Generally, dynamic database is categorized databases into 2 types: an original database and an increment database. The original database is the old database which old transactions are collected. The increment database is the new database which a new group of transactions are inserted into the original database. When a new increment database is merged to the original database, the association rule from the previous mining may have been changed. The problem statement for an incremental association rule discovery is normally defined as follows.

Let DB is an original database which is a collection of old transactions. db is an increment database which is a collection of new transactions. Then, UD is an updated database which is the database after merging DB and db together, i.e., UD = DB ? db. The number of transactions in a database is called the size of the database. Thus, the size of DB, db and UD are |DB|, |db| and |UD| = |DB|+|db| respectively. The notation of the incremental association rule mining problem statement is defined in table 1.

Basically, the minimum support threshold, s, is assumed to be a constant number. Before the updating activity has begun, FDB is a frequent itemsets of DB if and only if support count of itemset X, 	DB, is greater than or  equal to s |DB|, i.e., 	DB ? s |DB|. After the updating activity has ended, FUD is called a frequent itemsets of UD if and only if 	UD ? s |DB|.

As mentioned to Tsai et al. [8], when an original database and increment database are merged, an itemset, i.e., X, can possibly become to 4 cases:  Case 1 : X is a frequent itemset in both DB and UD Case 2 : X is a frequent itemset in DB and an infrequent  itemset in UD Case 3 : X is an infrequent itemset in DB and a frequent  itemset in UD Case 4 : X is an infrequent itemset in both DB and UD  From all cases mentioned above, the first two cases are easily discovered frequent itemsets in an updated database since their support count are exactly known.

Accordingly, the updating tasks in these 2 cases are negligible tasks. In the fourth case, it does not necessarily keep attention because it does not need to rescan an original database. The most difficult task of these 4 cases is the third case because it needs to rescan an original database to obtain the support count of itemsets in the updating tasks. The rescanning an original database is the really big problem because a lot of I/O operations are required.

3.2 Probability-based Incremental Association Rule Discovery Algorithm  Probability-based incremental association rule discovery algorithm [5] was proposed by Amornchewin and Kreesuradej in 2009. The main idea of this algorithm is predicting expected frequent itemsets using the principle of Bernoulli trials. The expected frequent itemsets are collected during finding the frequent itemsets of an original database. This predicted expected frequent itemsets can help the algorithm to reduce the number of times to rescan the original database when the new set of transactions is appended to the original database. In this subsection, the necessary concept of probability-based algorithm is briefly described.

For probability-based incremental association rule discovery algorithm, the process of inserting m transactions into an original database of n transactions can be considered as (n+m) Bernoulli trials. Each itemset has its probability of occurrence in a transaction, denoted by p. Then the probability of the occurrence of itemset in (n+m) transactions, denoted by P(X), can be found by the following equation:  xmnx pp x  mn XP ???  ?  ? ?? ?  ? ? ? )1.(.)(  (1)  where p is the probability of an itemset appearing in a transaction, m is a number of new transactions, and n is a  Table 1 The notation the incremental association rule mining problem statement  notation meaning  DB an original database db an increment database UD an updated database  DB kF ,  UD kF frequent k-itemset of DB, and UD respectively  DB x	 ,  UD x	 a support count of itemset X in DB  and UD respectively     number of transactions of an original database.

Thus, if k is a minimum support count after inserting  new transactions into an original database, the probability of an itemset to be a frequent itemset in an updated database can be obtained as the following equation:  itemitem kxPkxP )(1)( ????  (2)  According to eq.1, itemkxP )( ? can be found as the following equations:  xmnx k  x item ppx  mn kxP ??  ?  ?  ? ?  ? ?? ?  ? ? ?? ? )1.(.)(   (3)  Here, an expected frequent itemset is an itemset that is not a frequent itemset but has its probability to be a frequent itemset greater than Probpl. Probpl is a constant threshold specified by users. Probpl indicates the minimum confidence level that a promising frequent itemset will be a frequent itemset after inserting new transaction into an original database. The higher Probpl is set, the lesser expected frequent itemsets are kept. As results, the algorithm may need more a number of rescanning times in the original database when the algorithm performs the discovering new frequent itemset task.

Probability-based incremental association rule discovery algorithm assumes that the statistics of old transactions obtained from previous mining can be utilized for approximating that of new transactions.

Therefore, the algorithm uses support count of itemsets obtained from mining the original database to approximate the probability of itemsets when new transactions are inserted into the original database. Thus, the probability of occurrence itemset in (3), i.e., p, can be approximate as the following equation:  || ),(  DB DBitemsetcp ?  (4)  where ),( DBitemsetc is the support count of the itemsets obtained from the original database.

However, in practice, the probability of itemset to be a frequent itemset in an updated database, itemkxP )( ? , which obtained from binomial probability as eq. 3 suffer from a numerical problem when factorial is computed with a large value, i.e., the factorial of a large value cannot fit to the size of an integer variable.

For any positive integers of n, the factorial of n, i.e. n!, is the product of all integers from 1 to n. Thus, the value of n! increases swiftly with a large value of  n. For a data type in a computer language, a long integer data type, which occupies 64 bits, has 263-1 range of value. This range can only store 20! However, the binomial probability as eq. 3 usually involves in computing the factorial of n that greater than 20!   Therefore, the long  integer type does not have enough memory to store the factorial result. Such that problem, some probabilities of itemsets cannot be computed directly as eq. 3. In the previous work, these probabilities of itemsets are approximated by a linear extrapolation in order to avoid the problem. This gives some error of probability values.

To deal with this problem, probability-based incremental association rule discovery using the normal approximation is presented in the next section.

4. Probability-based Incremental Association Rule Discovery Using Normal Approximation  As Amornchewin and Kreesuradej work [5], the process of inserting m transactions into an original database of n transactions can be considered as (n+m) Bernoulli trials. Then, the probability of the occurrence of itemset in (n+m) transactions, denoted by P(X), can also be obtained by eq. 3. Unlike the previous work, the probability of occurrence of itemset, i.e. P(X), is calculated by using normal approximation theory in this work.

4.1 Normal Approximation to the Binomial  According to Bernoulli trial, it is a random experiment that one of two outcomes is occurred: success and failure.

Let p is a probability of success and q = 1- p is a probability of failure. The probability mass function (p.m.f.) of X can be written as  xx ppxf ??? 1)1()(  , )(1 )(0  successx failurex  ? ?  (5)  Generally, Bernoulli trial is considered as the special case of binomial distribution because Binomial experiment is an experiment that consists of several repeated independent Bernoulli trials [10].

Definition1: [9]?Let X is the number of observed success in n Bernoulli trials, then the possible values of X are 0,1,2,?,n. If  x successes occur, where x = 0,1,2,?,n, then n-x failures occurs The number of ways of selecting x positions for the x success in the n trials is?  )!(!

!

xnx n  x n  ? ? ?  ? ?? ?  ?  (6)  Definition2: [9] ?Since the trials are independent and the probabilities of success and failure on each trial are, respectively, p and q=1-p, the probability of each of these ways is xnx pp ?? )1( . Thus, f(x), the p.m.f of X, is the sum of  probabilities of the ?  ? ?? ?  ? x n mutually exclusive events; that is,  xnx pp x n  xf ?? ?  ? ?? ?  ? ? )1()(  , x=0,1,2,?,n (7)     These probabilities are called binomial probabilities and the random variable X is said to have a binomial distribution.?  The cumulative probability P(X<k) is calculated by  ? ?  ?  ?? ?  ? ?? ?  ? ??   )1()(  k  x  xnx pp x n  kXP  (8)  Thus, P(X?k) can be calculated by  ? ?  ?  ?? ?  ? ?? ?  ? ???   )1(1)(  k  x  xnx pp x n  kXP  (9)  According to probability-based algorithm, an expected frequent itemset is predicted by applying eq. 9. In practice, the binomial probability has a numerical problem when factorial is computed with a large n, i.e., a factorial value of a large n cannot fit to the size of an integer variable.

Definition3:[11] ?When n, a number of trials, is large and p, the probability of success, is not too far from ?.

Figure 1 shows the histograms of binomial distribution with p=1/2 and n = 2,5,10 and 25, and it can be seen that with increasing n these distribution approach the symmetrical bell-shaped pattern of a normal curve?  Figure 1. Binomial distribution with p = ? and n = 2, 5, 10 and 25  When n is large enough, the normal distribution with np?? and )1( pnp ??? can be used to approximate to the  binomial distribution, when np>5 and n(1-p) >5 [11].

However, a continuity correction must be applied when the binomial distribution, which is a discrete distribution, is approximated by the normal distribution.

Here, given np?? , )1( pnp ??? and X be a number of success, the area under the curve to the left of x is defined as  ? ??  ?  ? ?? ?  ? ? ?  ?? k  x  ekXP   1)( ?  ?  ?? . (10)  By using the continuity correction, the number of successes, i.e. k, is decreased by 0.5. Accordingly, the probability success k times can be derived as the  following equation  dxekXP k  k  k  k  x  ?? ?  ?  ?  ?   ?  ? ?? ?  ? ? ?  ???   5.0  5.0 1 2  11)( ?  ?  ?? (11)  Therefore, eq. 11 can be derived to the following equation  dxekXPobEF k x  X ? ?  ?   ?  ? ?? ?  ? ?? ????  5.0  5.0  1 2   1)(Pr ?  ?  ?? , (12)  where np?? , )1( pnp ??? , n is a number of transaction of updated database and p is the probability of occurrence itemsets. For the proposed algorithm, eq. 12 is used to calculate the probability of an expected itemset instead of eq. 3.

Since the estimation of the probability of an expected itemset based on eq. 12 is the point estimation, the actual probability of an expected itemset may vary from the estimation of the probability of an expected itemset.

Therefore, the proposed algorithm also introduces a probability tolerance threshold to deal with the difference between the estimated probability and the actual probability. Probability tolerance threshold ensures that an expected frequent itemset is tolerably kept. Probability tolerance threshold is based on a statistical confidence interval.

Typically, the confidence interval of the probability of an occurrence of an itemset, i.e., p, can be defined as following:  n ppZpp )1(2/  ? ?? ? . (13)  According to eq. 13, there are two threshold values:  an upper threshold and a lower threshold. To ensures that an expected frequent itemset is tolerably kept, Probability Tolerance Threshold of an itemset (pttX) is defined as  n ppZpptt X  )1( 2/  ? ?? ?  (14)  Here, pttx in eq. 14 is used in eq. 12 instead of p. Then, an itemset is an expected frequent itemset if its probability of an itemset is equal to or greater than probpl. These expected frequent itemsets are kept in order to reduce the number of times to rescan an original database. The proposed algorithm is presented in the next section.

4.2 Probability-based Incremental Association Rule Discovery Algorithm Using the Normal Approximation  The proposed algorithm, called Probability-Based Incremental Association Rule Discovery Algorithm Using The Normal Approximation, is based on the probability- based incremental association rule discovery algorithm.

There are 2 main phases: original mining and incremental     mining phase. The notation used in this 2 main phases is defined in table 2.

Figure 2. Original Mining Phase Algorithm  For the first iteration (k = 1) of original database phase as shown in figure 2, itemsets are scanned to an original database and obtained their support count. Then, frequent itemsets are found as line 3. The remaining infrequent itemset are computed to obtain two values: the probability tolerance threshold of an itemset (pttX) and the probability of an expected itemset (ProbEFX) as line 5 and 6, respectively. After that, the support count of the minimum of ProbEFX, i.e., DB? , is obtained from  line 7. This  DB? indicates the possible minimum support count of expected frequent itemset. Next, an infrequent itemsets  which its support count is less than s*|DB| and greater than or equal to DB? is collected to the set of expected frequent itemset as line 8. In addition, candidate 1- itemsets are obtained from line 9.

For the second iteration and beyond (k ? 2), the apriori_gen concept is used to generate candidate k- itemsets. Unlike apriori_gen, the candidate k-itemset of the proposed algorithm is generated by ( DBkF 1? ? DBkEF 1? )* ( DBkF 1? ? DBkEF 1? ) as line 13. After the candidate k-itemsets are obtained, the other steps is performed as the first iteration as line 14 and Ck>=2 does not collect. These steps of k?2 iteration are operated until candidate k-itemset cannot generate. When the original mining phase is ended, all outputs of this phase are DBkF , DBkEF , DBC1 , DB? as line 17. These out puts are used in the next phase.

Figure 3. Incremental Mining Phase Algorithm  The incremental mining phase, there are 2 main tasks in this phase: k=1 iteration and k? 2 iteration.

For the iteration of k=1, an increment database, db, is scanned and updated the support count of itemsets as shown in line 2. Then frequent 1-itemsets of updated  Algorithm1: Original Mining Phase Input: DB, |db|, Probpl, s, Z Output: DBkF ,  DB kEF ,  DBC1 , DB?  1 k =1 2  scan DB for all X? Ck and obtain DBx  3 DBkF ={X | DB x	 ? s |DB|}  4  for {X | DBx	 < s |DB|} 5   calculate pttx   //using equation (14) 6   calculate probability of expected itemset X (ProbEFX) //using equation (12) 7   DB? = min( DBx	 |ProbEFx ? Probpl)  8   DBkEF = {X | s |DB| > DB x	 ?  DB? }  9   DBC1 = {X| DB x	 <  DB? } 10  end 11 k =2 12  while | DBkF ?  DB kEF | > 1  13 Ck = ( DBkF 1? ? DB  kEF 1? )*( DB  kF 1? ? DB  kEF 1? ) 14 repeat line 2-8 15 k++ 16 end while loop 17 Return DBkF ,  DB kEF ,  DBC1 , DB?  Algorithm2: Incremental Mining Phase Input: DB, db, Probpl, s, DBkF ,  DB kEF ,  DBC1 , DB? ,Z  Output: UDkF , UD kEF ,  UDC1 , UD?  1 k=1 2  scan db and updating support count to obtain  UDx  3 UDkF ={X | UD x	 ? s |UD|}  4  for {X | UDx	 < s |UD|} 5   calculate pttx   //using equation (14) 6   calculate probability of expected itemset X (ProbEFX) //using equation (12) 7   UD? = min( DBx	 |ProbEFx ? Probpl)  8   UDkEF = {X | s |UD| > UD x	 ?  UD? }  9   UDC1 = {X| UD x	 <  UD? } 10  end 11 for k = 2 12  while UDkF 1? ? ? 13   Generating Candidate itemset ( ) // call algorithm 3 14   repeat line 2-8 15   for X? newkC   //  new kC is obtained from line 12  16   Temp_scanDB = {X | ( dbx	 +( DB? -1)) ? s |UD|}  17  end 18   k++ 19  end while loop 20 end 21 if Temp_scanDB ? ? 22  Rescanning Original Database to obtain new UDkF and  UD kEF  23 endif 24 clear Temp_scanDB 25 Return UDkF ,  UD kEF ,  UDC1 , UD?  Table 2 The notation of probability-based incremental association rule discovery using the normal approximation algorithm  notation meaning  DB kF ,  db kF ,  UD kF  frequent k-itemset of DB, db and UD respectively  DB kEF ,  UD kEF expected frequent k-itemset of DB and UD respectively  DB x	 ,  db x	 ,  UD x	 a support count of itemset X in DB , db and UD respectively  DBC1 , UDC1 Candidate 1-itemset of DB and UD  s a minimum support threshold UDDB ?? , support count of minimum probability of  itemset in DB and UD respectively Z confidence interval  Probpl probability value threshold defined by user     database, UDkF , are found as line 3. For itemsets which are not become UDkF , they are calculated the Probability tolerance threshold of an itemset (pttX) and the probability of an expected itemset (ProbEFX) as eq.14 and eq.12.

After UD? is calculated as line 7, an expected frequent itemset ( UDkEF ) and candidate 1-itemset ( UDC1 ) of an updated database is obtained as line 8 and 9, respectively.

For the iteration of k ? 2, in generally, this task is performed as the first iteration. However, in this iteration, there are 2 added steps: generating candidate itemset and rescanning original database in line 13 and 22, respectively.

The generating candidate itemset is detailed in figure 4.

Generating candidate 2-itemset and candidate k?2 itemset is different. For candidate 2-itemset, it is generated by  UDF1 * UDF1 as line 2, while candidate k?2 itemset is  generated by dbkF 1? * dbkF 1? as line 8 when dbkF 1? is obtained from itemset which its support count is greater than or equal to s*|db|. To ensure that no candidate itemsets are lost, itemset which is a member of ( DBkF 1? ? DBkEF 1? ) is also brought to consider as line 5 and 6. In addition, the new candidate k-itemset ( newkC ) is obtained from line 3 and 9.

This newkC  are itemsets which their support count is only known in db but not in DB. These new candidate itemsets are reused in line 15 of figure 3 to extract Temp_scanDB.

Figure 4. Generating Candidate Itemset  The new candidate itemsets ( newkC ) will be collected to Temp_scanDB if and only if the support count of  new kC plus ( DB? -1), where DB? obtained from the original  mining phase, is greater than or equal to s*|UD|. This process is shown in line 16 of figure 3 for pruning newkC which impossibly be a frequent itemset or an expected frequent itemset of the updated database.

For the final step of an incremental mining phase, Temp_scanDB is rescanned to an original database to update support count and find new frequent itemset and expected frequent itemset of an updated database as shown in line 21-23 of figure 3.  Both new frequent itemset and new expected frequent itemset are merged to the set of frequent itemset and expected frequent itemset which are found before. This guarantees that there is no remaining frequent itemset and expected frequent itemset to found absolutely.

5. Experiments  The objective of the proposed algorithm in this paper is to solve the numerical problem occurs with binomial distribution. Moreover, the confidence interval is introduced to increase the probability value of an itemset to be a frequent itemset in an updated database. The hypothesis is the proposed algorithm can reduce the number of Temp_scanDB itemsets which is rescanned to the original database.

To evaluate the efficiency of our algorithm, probability-based incremental association rule discovery using the normal approximation, this algorithm is implemented and tested on a PC with 2.93 GHz Intel Core i7, and the main memory is 3 GB. The experiment is tested with a synthetic dataset which are generated by synthetic technique proposed by Agrawal [1]. The synthetic dataset is T10I4D100K with the original database of 10,000 transactions. A number of transactions of increment database appended to an original database are 3,000 and 5,000 respectively.

The experiment is conducted with 0.005% minimum support threshold and Probpl = 0.3. The average of execution time compared with Apriori, FUP and probability-based algorithm are demonstrated in table 3 and 4 respectively.

?  Table 3. Average of Execution time for adding 3,000 transactions  Algorithm Apriori FUP Probability-based  Prob-based using  Normal Approximati  on Execution time  (sec) 61,452.012  34,951.310  16,252.670  14,656.836   a number of frequent itemset 1,105 1,105 1,105 1,105  a number of collected expected frequent itemsets  - - 40 115  maximum frequent itemset  (size of k) F5 F5 F5 F5  a number of Temp_scanDB - - 95 31  Algorithm3: Generating Candidate Itemset Input: dbkC ,  UDF1 , DB  kF , DB  kEF ,s ,|db|  Output: dbkC , new kC  1 if k = 2 2 dbC2 =  UDF1 * UDF1  3 newC2 = {X? dbC2 |X  (  DBF2 ? DBEF2 )}  4 else if  k ? 3 5 X = DBkF 1? ?  DB kEF 1? ?  db kC 1?  6 UDx	 = DB x	 +  db x  7 dbkF 1? ={X| UD x	 ?s*|db|}  8 dbkC = db  kF 1? * db  kF 1? 9 newkC = {X?  db kC |X  (  DB kF ?  DB kEF )}  10 end if 11 Return dbkC ,  new kC     The experiment results from table 3 and 4 display that the execution time of the proposed algorithm, the probability-based using the normal approximation algorithm, is concisely better than Apriori and FUP algorithm because Apriori needs to rerun a whole database (the merging between the original database and increment database) and FUP needs k-times (as size of k) to rescan the original database. While the proposed algorithm needs only one time to rescan the original database.

For comparing between the proposed algorithm and probability-based algorithm, the execution time of the proposed algorithm is slightly better than probability-base algorithm. A number of collected expected frequent itemset the proposed algorithm is greater than the probability-based algorithm. This affects to the number of Temp_scanDB, that is the number of Temp_scanDB of the proposed algorithm is less the than probability-based algorithm. These experiment results demonstrate that our hypothesis is accepted, i.e., the proposed algorithm can decrease the number of Temp_scanDB. As a result, the proposed algorithm can reduce a time-consuming of rescanning the original database.

6. Conclusion  In this paper, we propose the incremental association rule algorithm, probability-based incremental association rule discovery using the normal approximation algorithm, which estimates the probability of occurrence of expected frequent itemset using normal approximation. The proposed algorithm can deal with the numerical problem occurred when large n factorial is computed by binomial distribution. In addition, we introduce the increasing probability value of an itemset to be a frequent itemset in an updated database using confidence interval. The  experiment results show the slightly better execution time when compare with the probability-based algorithm and the decreasing of a number of Temp_scanDB. However, we encounter with trade off a number of Temp_scanDB against a number of collected expected frequent itemset.

However, the common limitation of the proposed algorithm and probability-based algorithm is the fixed size of the increment database. That is, both algorithms need to know the exact number of the increment database size for computing the probability value of the co- occurrence itemset in the updated database. Practically, the increment database may be continually added to the original database with various sizes. Thus, the future research is looking for the approach to deal with the fixed increment database size problem.

7. References  [1] R Agrawal, T Imielinski, A Swami, ?A mining association rules between sets of items in large database,? in proceeding of the ACM SIGMOD Int?l Conf. on Management of Data (ACM SIGMOD?93), Washington, USA, May 1993, pp.207-216.

[2] R Agrawal, R Srikant, ?Fast algorithm for mining association rules,? in Proc. 20th  Int. Conf. Very Large Databases (VLDB?94), Santiago, Chile, September 12-15, 1994, pp.487-499.

[3] C. H. Lee, C. R. Lin, and M. S. Chen, ?Sliding-window Filtering: An Efficient Algorithm for Incremental Mining,? Proceeding of the Knowledge Management (CIKM?01), November 2001. pp 263- 270.

[4] H Toivonen, ? Sampling large database for association rules,? In Database (VLDB?96), September 1996, pp. 134-145.

[5] R Amornchewin, Kreesuradej Worapoj, ?Mining Dynamic Databases using Probability-based Incremental association Rule Discovery Algorithm,? Journal of Universal Computer Science, Vol.15, No.12, April 2009, pp. 2409-2428.

[6] D W Cheng, J Han, V T Ng, C Y Wong, ?Maintenance of Discovered Association Rules in Large Databases: An incremental Engineering, 1996, pp.106-114.

[7] S M Tsai Paulry, Lee Chin-Chong, L P Chen Arbee, ?An Efficient Approach for incremental Association Rule Mining,? Proceedings of the third Pacific-Asia Conference on methodologies for Knowledge Discovery and Data Mining, Lecture Notes in Computer Science, Vol. 1574 archive, 1999.

[8] T P Hong, C Y Wang, Y H Tao, ?A new incremental data minting algorithm using pre-large itemsets,? Journal of Intelligent Data Analysis, Vol.5, No.2, 2001, pp.111-129.

[9] V. H Robert, A.T Elliot, ?2.4 Bernoulli trials and the binomial distribution,? Probability and Statistical Inference. 8th edi, Pearson Education, Inc, New Jersey, 2010, pp. 78-86.

[10] J. K Larry, ?4.4 The Binomial Distribution,? Exploring Statistics: A Modern introduction to Data Analysis and Inferernce, 2nd edi., Integre Technical Publishing company, Inc.; G&S Typesetters, Inc, 1998, pp.265-275.

[11] E. F John, M. P Benjamin, ?7.4 The normal approximation to the binomial distribution,? Statistics: A first course, 8th edi., Pearson Education, Inc, New Jersey, 2004, pp. 256-260.

