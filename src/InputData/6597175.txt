Secure Outsourcing of Network Flow Data Analysis  PositionPaper

Abstract?In this paper, we identify a new and challenging application for the growing field of research on data anonymization and secure outsourcing of storage and computations to the cloud. Network flow data analysis is of high importance for network monitoring and management.

Network monitoring applications reveal new challenges not yet addressed in the secure outsourcing literature. The secure and verifiable outsourcing of computation on anonymized network flow records provides a practical tool for network operators in order to harness the cloud benefits, which untapped until now because of privacy concerns. We present representative use- cases and problems, and identify sample related work that can be utilized for developing an effective solution.

Keywords-IP Flow; Cloud Computing; Secure outsourcing; Anonymization

I. INTRODUCTION Monitoring of nowadays networks requires increasing  storage capacity and computation resources. For example, a university network that has a 10 Gbps optical Internet connection with an average load of 650 Mbps and peaks up to 1.0 Gbps, exports several hundred million flows per day [3].

Due to this big data, many monitoring systems have already shifted from deep packet inspection to the aggregated flow data level. Still, the scalability of current network monitoring solutions is questioned. Moreover, small and midsize business may not have the expertise and resources to concentrate on monitoring networks. Therefore, they like to get it as a service.

In a collaborative setting, different organizations may need to do value-added network data analytics across multiple networks while preserving their privacy. On the other hand, cloud computing has become a successful new paradigm in the IT industry. Virtualized infrastructures, platforms, and applications are remotely and dynamically provisioned over the Internet.

Cloud computing presents promising opportunities for network operators in order to achieve large-scale network monitoring at low cost and minimal management effort. However, a wide adoption of outsourcing to the cloud is impeded by the privacy and the security of the corporation and individual users of the monitored network. Service Level Agreements (SLA) and building a trust relationship with the cloud provider solves this problem only partially. First, the cloud may be subject to intrusions from insiders (e.g. a disloyal system administrator) or outsiders (e.g. third-party malwares),which risks to compromise the data and the computations. Second, the cloud providers would like to avoid being liable upon lost or compromise of sensitive information. Therefore, developing techniques for secure and privacy-preserving outsourcing of IP data flow  storage and computations allows harnessing the benefits of the cloud. Little works have been developed in this direction [6].

Prior to outsourcing, sensitive information such as IP addresses, which may reveal the identity of the user or its location, must be anonymized. Other fields of the IP flows may reveal indirectly the identity of a user. For example, the timing of an IP flow record correlated to the source port or the destination port being used may uniquely identify a user.

Nevertheless, the utility of the anonymized data should be preserved. Network monitoring requires continuous evaluation of statistical functions over the flows or their attributes. This may range from simple statistical variables (e.g. average and standard deviation of the number of requests per hour to the HTTP server) to more complex and composed variables related to anomaly or intrusion detection (e.g. evaluating cause-effect relationships between two groups of flows). In this context, an anonymization technique introduces a tradeoff between the risk of information leakage and the utility for statistical analysis. For example, Burkhart et al. [4] evaluates the risk-utility tradeoff for the IP truncation anonymization technique. The first research question we highlight in this abstract consists on finding a suitable anonymization technique for optimizing the risk-utility tradeoff.

From another side, the inputs and the results of statistical computations being outsourced are of equal importance and their protection is required. The network operators may not tolerate revealing the traffic matrix and the traffic patterns of the monitored network even though the source and destination IPs are anonymous. That is, the numerical fields within the flows may need to be protected as well, for instance the number of exchanged bytes and the number of exchanged packets. The number of flows itself forms a times series and may need to be hidden. Hence, our second research question is: How to securely delegate the evaluation of the statistical functions to the cloud without revealing the input variables and the results?

In the rest of this paper, we present somehow representative use-cases and problems of network monitoring and highlight research works related to anonymization and secure outsourcing that are useful for building a solution for answering the aforementioned questions.



II. IP FLOW ANONYMIZATION Anonymization is defined as the deletion or transformation  of information that is considered sensitive and that could be used to reveal the identity of entities involved in a communication [1]. This is an important tool for privacy protection within network monitoring infrastructures. IP flow anonymization aims at preventing the adversary from asserting that endpoint X contacted endpoint Y at time T while guaranteeing at the same time a relevant IP Flow data analysis.

2013 IEEE International Congress on Big Data  DOI 10.1109/BigData.Congress.2013.71     Research on network trace anonymization techniques and attacks against them is ongoing [1] [2][10]. There is an increasing evidence that anonymization applied to network trace or flow data on its own is insufficient for many data protection applications. Examples of IP address anonymization are truncation, reverse truncation, permutation, and direct substitution [1].



III. EXAMPLES OF COMPUTATIONS ON NETWORK FLOWS FOR MONITORING AND RELEVANT WORKS FROM SECURE  OUTSOURCING Ntop [7] is a well-known tool for network monitoring and in  particular flow-based traffic Monitoring. Some of the functionalities of Ntop are 1) sort network traffic according to many protocols 2) show network traffic sorted according to various criteria 3) analyse IP traffic and sort it according to the source/destination and 4) display IP Traffic Subnet matrix (who?s talking to who). In secure outsourcing to the cloud, these statistics must be evaluated without revealing the actual collected network traffic. Atallah et. al. address secure outsourcing of sorting next to other scientific computations in [9].Secure Multi-Party computation (SMC) is a set of distributed algorithms that allow different parties evaluating a function without revealing their private inputs, and sharing the results to one or more of the participants. SEPIA [11] is a project for generic SMC, processing high-volume input data. It uses Shamir's secret sharing scheme and is secure in the honest-but- curious adversary model.

In [6] a secure two-party computation for privacy-preserving cooperative statistical analysis is presented. The authors propose two secure protocols for scalar vector product: the first is based on oblivious transfer and the second is based on a permutation protocol using homomorphic encryption. The authors compare their schemes to the general result of garbled circuits and increase their security by implying hard to guess combinatory.

The authors build on the scalar vector product protocol for securely computing the mean value, the standard deviation, the correlation coefficient and the linear regression coefficients of a data set. This paper is particularly relevant to our problem because these statistics are often used in profiling of network data. Two settings of cooperation are studied: homogeneous and heterogeneous. In the homogeneous setting each party has the complete set of only one feature. In the heterogeneous setting each party has a subset of all the features. Still, SMC settings are different than those of secure outsourcing because, in the latter, one party has the data and resorts to the computing power of the second party.

Finally, we admit that the user is able to accomplish computations if their complexity is linear to the size of the input data (e.g. the average number of flows per hour). We focus more on computations that are polynomial to the size of the input data. For example the correlation matrix between two sets of features during a given period of time takes a quadratic time.

Similar to this example is biometric computations which have been addressed in [8]. Another problem is how to privately conduct aggregate queries over a set of flows which are securely stored at the cloud. An example of such a query is the number of flows respecting a specific condition as given by the user (e.g.

number of bytes exchanged is smaller than a threshold).



IV. CONCLUSION AND FUTURE WORKS A significant amount of work has been addressing  obfuscation and anonymization of raw packet data, and to some  extent log files of network devices. The impediment to these works is principally the complexity and the diversity of the input data. Network flows however are more suitable for analysis because of their simple format. To the best of our knowledge, there has been no work to take advantage of the body of work on secure outsourcing of computations for flow- based network monitoring and intrusion detection. In this paper we motivated the application of anonymization and secure outsourcing techniques to the network flow data, which is of practical importance for network operators having limited resources or unwilling to procure their own storage and computation infrastructure. In future work, we aim to present a solution for secure and verifiable outsourcing of network flow data to the cloud covering the anonymity and the privacy of the network users while optimizing the utility for secure and verifiable statistical and machine learning computations. We envisage this task will identify new algorithmic challenges and the solution will be effective and practical for network operators.

