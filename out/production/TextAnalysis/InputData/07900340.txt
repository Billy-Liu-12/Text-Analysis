Incorporation of Efficient Second-Order Solvers Into Latent Factor Models for Accurate

Abstract?Generating highly accurate predictions for missing quality-of-service (QoS) data is an important issue. Latent fac- tor (LF)-based QoS-predictors have proven to be effective in dealing with it. However, they are based on first-order solvers that cannot well address their target problem that is inherently bilinear and nonconvex, thereby leaving a significant opportunity for accuracy improvement. This paper proposes to incorporate an efficient second-order solver into them to raise their accuracy.

To do so, we adopt the principle of Hessian-free optimization and successfully avoid the direct manipulation of a Hessian matrix, by employing the efficiently obtainable product between its Gauss?Newton approximation and an arbitrary vector. Thus, the second-order information is innovatively integrated into them.

Experimental results on two industrial QoS datasets indicate that compared with the state-of-the-art predictors, the newly proposed one achieves significantly higher prediction accuracy at the expense of affordable computational burden. Hence, it is especially suitable for industrial applications requiring high prediction accuracy of unknown QoS data.

Manuscript received June 4, 2015; revised April 1, 2016 and February 17, 2017; accepted March 11, 2017. This work was supported in part by the Pioneer Hundred Talents Program of Chinese Academy of Sciences, in part by the International Joint Project through the Royal Society of the U.K. and the National Natural Science Foundation of China under Grant 61611130209, in part by the Young Scientist Foundation of Chongqing under Grant cstc2014kjrc-qnrc40005, in part by the National Natural Science Foundation of China under Grants 61370150, 61433014, and 61402198, and in part by the Fundamental Research Funds for the Central Universities under Grants 106112016CDJXY180005 and CDJZR12180012. This paper was rec- ommended by Associate Editor P. Tino. (Xin Luo and Shuai Li contributed equally to this work.) (Corresponding authors: Xin Luo; MengChu Zhou)

X. Luo is with the Chongqing Key Laboratory of Big Data and Intelligent Computing, Chongqing Institute of Green and Intelligent Technology, Chinese Academy of Sciences, Chongqing 400714, China, and also with the College of Computer Science and Engineering, Shenzhen University, Shenzhen 518060, China (e-mail: luoxin21@cigit.ac.cn).

M. Zhou is with the Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ 07102 USA and also with the Renewable Energy Research Group, King Abdulaziz University, Jeddah, Saudi Arabia (e-mail: zhou@njit.edu).

S. Li, Z.-H. You, and H. Leung are with the Department of Computing, Hong Kong Polytechnic University, Hong Kong 999077 (e- mail: shuaili@polyu.edu.hk; zhuhong.you@polyu.edu.hk; hareton.leung@ polyu.edu.hk).

Y. Xia and Q. Zhu are with the College of Computer Science and the Chongqing Key Laboratory of Software Theory and Technology, Chongqing University, Chongqing 400044, China (e-mail: xiayunni@hotmail.com; qszhu@cqu.edu.cn).

Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.

Index Terms?Big data, latent factor model, missing data prediction, quality-of-service (QoS), second-order solver, service computing sparse matrices, Web service.



I. INTRODUCTION  INDUSTIRAL applications based on service-oriented archi-tectures take Web services as the basic components [1]?[4], whose most nonfunctionality is reflected by quality-of- service (QoS). Meanwhile, with the exploration of the World Wide Web, more and more Web services with similar functionality become available. Under such circumstances, QoS becomes vital for users who want to build an effi- cient and reliable system. Many QoS-based approaches to service composition [50], [51] and selection [5]?[7], [49] are proposed in such contexts. For them, the QoS data are fundamentally important.

It is well known that QoS data can be measured at both server and user sides [1]?[12]. The server-side QoS data, e.g., price and popularity, are provided by the service-providers.

However, the user-side QoS data, e.g., response-time and throughput, are highly different among users depending on many factors, such as network conditions and invoking envi- ronment. A straightforward approach to acquiring this part of QoS data is by taking real-world service evaluations [1]?[8], but it has two main issues [4]?[12]. First, since most com- mercial services are not free to invoke, these tests lead to high expense. Second, once the target set of candidate ser- vices becomes too large, it takes a long time to evaluate them all.

Consequently, QoS-based approaches encounter a dilemma: to obtain sufficient user-side QoS data requires frequent ser- vice evaluations which are expensive and time-consuming, while insufficient data lead to loss in performance. As a result, a critical problem of QoS prediction arises, i.e., how to accurately predict user-side QoS data?

Motivated by the success of user-collaboration in E-commerce, researchers propose to address the problem of QoS prediction in a similar way, i.e., encouraging users to share their historical QoS data on Web-services with similar functionality, and then predict the unknown QoS data with col- laborative filtering (CF)-based approaches [6]?[16]. Although  See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

mailto:luoxin21@cigit.ac.cn mailto:zhou@njit.edu mailto:shuaili@polyu.edu.hk mailto:zhuhong.you@polyu.edu.hk mailto:hareton.leung@polyu.edu.hk mailto:hareton.leung@polyu.edu.hk mailto:xiayunni@hotmail.com mailto:qszhu@cqu.edu.cn http://ieeexplore.ieee.org http://www.ieee.org/publications_standards/publications/rights/index.html   This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

a user cannot invoke all candidate services, reliable predic- tions for corresponding unknown QoS data can be generated based on the other users? former experience [6]?[12]. Hence, with a CF-based approach, the problem of QoS prediction is transformed into missing data prediction, where the key is to predict unknown QoS data based on known ones.

Various CF-based QoS-predictors are recently proposed [6]?[14]. Among them, the latent factor (LF)- based predictors prove to be highly accurate [11]?[15]. An LF-based predictor first models the historical QoS data into a user-service (U-S) matrix, where each row denotes a speci- fied user, each column denotes a specified service, and each entry denotes the historical QoS record by a specified user on a specified service. Since in real applications each user usually touches a finite subset of candidate services, the resulting U-S matrix is filled with numerous unknown entries.

But based on the known ones, an LF-based predictor is able to predict these missing entries, and the resulting predictions can assist the effective implementation of QoS-aware service composition and selection.

Naturally, the prediction accuracy of an LF-based predictor is vital: with reliable predictions, a QoS-based approach can achieve high performance. Hence, highly accurate LF-based QoS-predictors are highly desired. Building an LF model requires one to solve a bilinear and nonconvex optimiza- tion problem. Since second-order approaches have been proven to be effective in addressing nonconvex optimization problems [29]?[43], this paper proposes a highly accurate second-order LF-based QoS-predictor with acceptable com- putational cost for the first time. By integrating the second- order information into its optimization process, we expect to achieve higher prediction accuracy than existing first-order models [11]?[15]. By innovatively introducing the principle of Hessian-free optimization [30]?[44] into it, we aim to decrease its computational cost for industrial usage.

Section II states the problem. Section III presents our method. Section IV gives experimental results and related discussions. Section V summarizes related works. Finally, Section VI concludes this paper.



II. PROBLEM STATEMENT  A. CF-Based QoS Prediction  For predicting unknown QoS data, a CF-based approach encourages users to share their historical QoS data on simi- lar services, as depicted in Fig. 1. Through such a strategy, it considers the QoS-prediction problems on the whole user and service sets. The known QoS data with respect to each specific QoS metric, e.g., response-time, are put into a U-S QoS matrix where each row, column, and entry correspond to a specified user, service, and U-S QoS record, respectively.

Due to users? limited service invocations, this matrix is filled with a large amount of missing data. Hence, a CF-based QoS- predictor addresses the QoS-prediction problem by performing missing data estimation in U-S matrices, as shown in Fig. 1.

In comparison with relatively fixed QoS data at the ser- vice side, user-sensed QoS data may vary significantly among  Fig. 1. CF-based approach to QoS prediction. Note that in matrices, ?+? denotes historical QoS data shared by users, and ?*? denotes prediction based on known data, respectively.

users. Consequently, a CF-based approach focuses on predict- ing unknown user-side QoS data based on known ones shared by users. Among CF-based QoS-predictors, LF-based ones are highly effective. Next, we give their formal definitions.

B. LF-Based QoS-Predictor  For an LF-based QoS-predictor, a U-S matrix is the funda- mental data source.

Definition 1: Given a user set U and a service set S, a U-S matrix Q is a |U| ? |S| matrix consisting of the historical records of QoS values by U on S where a known entry qu,s denotes a QoS record by user u on service s.

Since each user invokes a limited number of services only, Q is often incomplete. Denoting its known and validation entry sets as ? and ?, respectively, we define an LF-based QoS- predictor as follows.

Definition 2: Given Q, an LF-based QoS-predictor consists of LFs built on ?; they form the rank-d approximation Q? to Q, generating an estimate q?u,s for each instance (u, s) ? ? such that (  ? (u,s)?? |qu,s ? q?u,s|abs) is minimized.

It is common to model the desired LFs in Q? into separate LF matrices, i.e., a user LF matrix P|U|?d and a service LF matrix Q|S?d| [6]?[15] with d denoting the dimension of the LF space. Note that each row vector in P/Q is actually the LF vector of each user/service. Therefore, we obtain the following equation:  Q? = PQT ? q?u,s = pu,? ( qs,? )T =  d?  k=1 pu,kqs,k (1)  where pu,? and qs,? denote the uth and ith row vector of P and Q, respectively. Moreover, as discussed in [15]?[28], (1) can be extended with linear biases for more stable model convergence and prediction accuracy, leading to  q?u,s = pu,1 + qs,1 + d?  k=2 pu,kqs,k (2)    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

LUO et al.: INCORPORATION OF EFFICIENT SECOND-ORDER SOLVERS INTO LF MODELS FOR ACCURATE PREDICTION 3  where pu,1 and qs,1 are treated as the linear biases correspond- ing to user u and service s, respectively.

The multiparameter-dependent form (2) of Q? was frequently adopted in prior studies [11]?[29]. Nonetheless, a second- order solver requires computing the objective function?s second-order partial derivative with respect to all involved LFs. Under such circumstances, with multiple parameters, i.e., P and Q, the inference and resulting equations become very complex. In lieu of this, we choose to model the desired LFs in P and Q into a uniform, (|U| + |S|) ? d-dimensional LF vector X as in the following definition.

Definition 3: Given U and S, an LF vector X is a (|U| + |S|)?d-dimensional vector, where d decides the dimension of the LF space. Both user u ? U and service s ? S correspond to d-dimensional subvectors of X, which are denoted by x(u) and x(s), respectively.

Thus, each entry of Q? can be formulated by the following function with respect to LFs in X:  q?u,s = x(u)1 + x(s)1 + d?  k=2 x(u)kx(s)k (3)  where x(u)k and x(s)k denote the kth element of x(u) and x(s), respectively. These desired LFs are obtained through minimiz- ing a loss function measuring the difference between ? and corresponding entries in Q?, e.g., the Euclid distance  arg min X  E(X)  = 1  ?  (u,s)??  ?  ?  (  qu,s ? x(u)1 ? x(s)1 ? d?  k=2 x(u)kx(s)k  )2  + ? d?  j=1  ( x2(u)j + x2(s)j  ) ?  ? (4)  where ? ?d  j=1(x2(u)j + x2(s)j) is the Tikhonov regularizing term, and ? denotes the controlling constant, respectively. Note that the regularized Euclid distance (4) can also be replaced with other objective functions without impairing the generality of the proposed method.

C. Second-Order Approach  To optimize X in (4), a Newton-type second-order method seeks for its training increment ?X by solving the following linear system:  HE(X)?X + ?E(X) = 0 (5) where ?E(X) denotes the gradient of E with respect to X, and HE(x) denotes the Hessian matrix of E with respect to X, respectively.

Note that HE(x) is an |X| ? |X| matrix, i.e., it is of size (|U|+|S|)2?d2. Naturally, we can directly solve (5) as follows:  ?X = ?(HE(X))?1 ? ?E(X) (6) but the computational cost comes to ?((|U| + |S|)3 ? d3) due to the necessity to inverse HE(x). Meanwhile, its stor- age complexity is ?(|U| + |S|)2 ? d2) owing to the necessity  to cache HE(x). With huge user/service counts which are commonly seen in real applications, the straightforward solu- tion (6) is very inefficient in both computation and storage.

Next, we illustrate how to implement a practically deployable second-order LF model for QoS-prediction.



III. HIGHLY ACCURATE SECOND-ORDER QOS-PREDICTOR  A. Base Components  Note that the greatest obstacle preventing (5) from practical usage is its direct dependence on HE(x). In other words, once we can solve (5) without the direct involvement of HE(x), it is promising to decrease the complexity in both compu- tation and storage. This is the principle of the Hessian-free optimization [30]?[44].

First, it is widely accepted that the linear system (5) can be solved via a conjugate gradient decent (CGD) solver [33]?[36], [45], which is an iterative solver avoiding inversing the Hessian matrix HE(x). With CGD, (5) is solved relying on the product between HE(x) and some vector v, i.e., ? = HE(x)v, rather than on HE(x) directly. Thus, the compu- tational complexity is decreased to ?((|U| + |S|)2d2) because of computing HE(x)v only, much lower than that of (6). Yet it is still hard to resolve with large |U| and |S|.

Based on CGD, a Hessian-free optimization-based approach takes advantage of the fact that not HE(x) but ? is nec- essary during the iterative process. Given an arbitrary vec- tor v, ? actually denotes E?s directional derivate along v?s direction [33]?[36]. Thus, the following equation holds:  ? = HE(X)v = ?(?E(X + ?v)) ??  ? ? ? ? ??0  = R(?E(X)) (7)  where R(?E(X)) means taking the R-operation [33]?[36] on the gradient ?E(X). Note that the R-operation is actually a spe- cial case of the reverse-mode of algorithmic differentiation which enables the exact computation of an arbitrary directional derivate [33]?[36]. Thus, ? is computed as follows:  ? = (  R  ( ?E(X)  x1  )  , . . . , R  ( ?E(X)  x(|U|+|S|)?d  ))  (8)  by noting the following characteristics of R-operation [33]?[36]:  1) R(X) = v; 2) R(f (X) + g(X)) = R(f (X)) + R(g(X)); 3) R(f (X) ? g(X)) = R(f (X)) ? R(g(X)). (9)  With (8) and (9), it is possible to solve (5) with CGD under affordable computational burden. Nonetheless, CGD requires HE(X) to be positive semidefinite, or the negative-curvature problems [33]?[35], [37], [38] may arise to disturb the solving process. Next, we present the details of our model.

B. Employing Gauss?Newton Approximation to HE(X)  According to [33]?[35], [37], and [38], the negative curva- ture problems can be avoided by replacing HE(x) in (5) with    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

its Gauss?Newton approximation, i.e., GE(x), which is always positive semidefinite. Consequently, (5) is changed into  GE(X)?X + ?E(X) = 0. (10) For applying CGD to (10), the product between GE(x) and an arbitrary vector v, i.e., ?G = GE(x)v, is demanded. Hence, the first step is to derive GE(X). To do so, we need a function F(X) to map input X into a |?|-dimensional output vector, to transform (4) into a convex function with respect to X. Yet this is not easy in our case, due to the involvement of the Tikhonov regularizing term.

To address it, we first consider removing the regularizing term from (4), resulting in a simpler objective function  arg min X  D(X) = 1  ?  (u,s)??  (  qu,s ? x(u)1 ? x(s)1 ? d?  k=2 x(u)kx(s)k  )2  (11)  whose Hessian matrix is HD(X). Then we let Z = F : R(|U|+|I|)?d ? R|?| be the following function of X:  z(u,s) = F(u,s)(X) = x(u)1 + x(s)1 + d?  k=2 x(u)kx(s)k (12)  where z(u,s) denotes the element of Z corresponding to instance qu,s ? ?, and F(u,s)(X) denotes the related mapping function.

With (12), we transform (11) into the following function:  D(Z) = 1  ?  (u,s)??  ( qu,s ? zu,s  )2 (13)  Note that the second-order derivative of (13) regarding Z is the identity matrix I. Hence, given arbitrary v, we derive the product between v and GD(X), as follows [33]?[35], [46]:  ?D = GD(X)v = JF(X)TJF(X)v (14) where JF(X) denotes the Jacobian matrix of F with respect to

X. Note that JF(X)v in (14) is equal to the directional derivative of F(X) in the direction of v [33]?[36]. So similar to (7), the following equation holds:  JF(X)v = ?(?F(X + ?v)) ??  ? ? ? ? ??0  = (  R ( F(u,s)(X)  )? ? (u,s)??  ) .

(15)  By applying (9) to (15), we obtain  JF(X)v = ?  ?v(u)1 + v(s)1 + d?  k=2  ( v(u)kx(s)k + x(u)kv(s)k  ) ? ? ? ? ? (u,s)??  ?  ?.

(16)  Meanwhile, from the definition of Jacobian matrices, we have  JF(X) = (  ?F(u,s)(X)  ?X  ? ? ? ? (u,s)??  )  . (17)  By combing (16) and (17), we formulate ?D as follows:  ?u ? U, i = 1  ?D(u)1 = ?  s??(u)  (  v(u)1 + v(s)1 + d?  k=2  ( v(u)kx(s)k + x(u)kv(s)k  ) )  ?u ? U, i = 2 ? d  ?D(u)i = ?  s??(u)  (  x(s)i  (  v(u)1 + v(s)1 + d?  k=2  ( v(u)kx(s)k + x(u)kv(s)k  ) ))  ?s ? S, i = 1  ?D(s)1 = ?  u??(s)  (  v(u)1 + v(s)1 + d?  k=2  ( v(u)kx(s)k + x(u)kv(s)k  ) )  ?s ? S, i = 2 ? d  ?D(s)i = ?  u??(s)  (  x(u)i  (  v(u)1 + v(s)1 + d?  k=2  ( v(u)kx(s)k + x(u)kv(s)k  ) ))  (18)  where ?D(u) and ?D(s) denote the subvectors in ?D corre- sponding to user u and service s, and ?D(u)i, and ?D(s)i denote their ith elements, respectively.

On the other hand, similar to (8), it is easy to derive  HD(X)v = (  R  ( ?D(X)  x1  )  , . . . , R  ( ?D(X)  x(|U|+|S|)?d  ))  . (19)  Then we obtain the following residual:  HE(X)v ? HD(X)v = (  R  ( ?E(X)  x1  )  ? R (  ?D(X)  x1  )  , . . . , R  ( ?E(X)  x(|U|+|S|)?d  )  ? R (  ?D(X)  x(|U|+|S|)?d  ))  = (?1, . . . , ?(|U|+|S|)?d )  ? {  for user u, k = 1 ? d : ?(u)k = ?v(u)k|?(u)| for service s, k = 1 ? d : ?(s)k = ?v(s)k|?(s)| (20)  which depends on the difference between E(X) and D(X), i.e., the Tikhonov regularizing term. Hence, it is reasonable to infer that the regularization terms act on this residual. With this intuition, we derive ?G = GE(x)v as follows: ?G = GE(X)v ? GD(X)v + (HE(X)v ? HD(X)v) ?u ? U, i = 1  ?G(u)1 = ?  s??(u)  (  v(u)1 + v(s)1 + d?  k=2  ( v(u)kx(s)k + x(u)kv(s)k  ) )  + ?v(u)1|?(u)| ?u ? U, i = 2 ? d  ?G(u)i = ?  s??(u)  (  x(s)i  (  v(u)1 + v(s)1 + d?  k=2  ( v(u)kx(s)k + x(u)kv(s)k  ) ))  + ?v(u)i|?(u)| ?s ? S, i = 1  ?G(s)1 = ?  u??(s)  (  v(u)1 + v(s)1 + d?  k=2  ( v(u)kx(s)k + x(u)kv(s)k  ) )  + ?v(s)1|?(s)|    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

LUO et al.: INCORPORATION OF EFFICIENT SECOND-ORDER SOLVERS INTO LF MODELS FOR ACCURATE PREDICTION 5  ?s ? S, i = 2 ? d  ?G(s)i = ?  u??(s)  (  x(u)i  (  v(u)1 + v(s)1 + d?  k=2  ( v(u)kx(s)k + x(u)kv(s)k  ) ))  + ?v(s)i|?(s)|. (21)  Note that for arbitrary v, we have  vT GE(X)v = vT GD(X)v + vT(HE(X)v ? HD(X)v)  = vT GD(X)v + ? ?  ? ?  u,k  v2(u)k|?(u)| + ?  s,k  v2(s)k|?(s)| ?  ?  (22)  where GD(X) is always positive semidefinite [33]?[35], [46].

Therefore, given nonzero v, we have vTGE(X)v > 0, indicat- ing that we guarantee the positive definiteness of GE(X) after making the approximations in (21). With it, a CGD solver can always converge well on (10). This is also validated by our experimental results later on.

C. Output Model  To avoid overshooting during the optimization, we append damping terms to (10), i.e., replacing GE(X) with A = GE(X)+ ? ?I, where ? is a positive constant and I is an identity matrix, as frequently adopted in prior research [30]?[44]. Thus, we have  { A?X + ?E(X) = 0 A = GE(X) + ? ? I. (23)  Then, the actual matrix-vector product used during the CGD process is given by  ?A = A ? v = (GE(X) + ? ? I)v = ?G + ? ? v. (24) After solving the linear system (24) with CGD, the obtained  increment ?X actually means updating the LF vector X along the direction of ?X with a step-size at one, which may also lead to overshoot. So we further introduce a tunable step-size, i.e., after obtaining ?X in the tth iteration, we perform:  Xt+1 = Xt + ??X (25) where ? is the step-size constant in the range of (0, 1].

Based on the above inferences, the proposed second-order optimization-based LF (SOLF) model for QoS-prediction is formulated as follows.

1) The objective of SOLF is given by:  arg min X  E(X) = 1  ?  (u,s)??  ?  ?  (  qu,s ? d?  k=1 x(u)kx(s)k  )2  + ? (  d?  k=1 x2(u)k +  d?  k=1 x2(i)k  )?  ?.

2) The linear system is given by:  A ? ?X + ?E(X) = (GE(X) + ? ? I) ? ?X + ?E(X) = 0.

3) The involved matrix-vector product is given by:  ?u ? U, i = 1  ?A(u)1 = ?  s??(u)  (  v(u)1 + v(s)1 + d?  k=2  ( v(u)kx(s)k + x(u)kv(s)k  ) )  + v(u)1(?|?(u)| + ? ) ?u ? U, i = 2 ? d  ?A(u)i = ?  s??(u)  (  x(s)i  (  v(u)1 + v(s)1 + d?  k=2  ( v(u)kx(s)k + x(u)kv(s)k  ) ))  + v(u)i(?|?(u)| + ? ) ?s ? S, i = 1  ?A(s)1 = ?  u??(s)  (  v(u)1 + v(s)1 + d?  k=2  ( v(u)kx(s)k + x(u)kv(s)k  ) )  + v(s)1(?|?(s)| + ? ) ?s ? S, i = 2 ? d  ?A(s)i = ?  u??(s)  (  x(u)i  (  v(u)1 + v(s)1 + d?  k=2  ( v(u)kx(s)k + x(u)kv(s)k  ) ))  + v(s)i(?|?(s)| + ? ).

4) The update rule is given by:  Xt+1 = Xt + ??X. (26)  D. Complexity Analysis  With (26), it is easy to design an algorithm implementing SOLF, which mainly depends on a CGD process computing ?X for each iteration, as recorded in Algorithm 1.

SOLF?s computational complexity mainly depends on the computation of ?E(X), and the CGD process for ?X.

From (4), we derive ?E(X) as follows: ?u ? U, i = 1 ?E(X)  ?x(u)1 = ?  ?  s??(u)  (  qu,s ? x(u)1 ? x(s)1 ? d?  k=2 x(u)kx(s)k  )  + ?|?(u)|x(u)1 ?u ? U, i = 2 ? d ?E(X)  ?x(u)i = ?  ?  s??(u) x(s)i  (  qu,s ? x(u)1 ? x(s)1 ? d?  k=2 x(u)kx(s)k  )  + ?|?(u)|x(u)i ?s ? S, i = 1 ?E(X)  ?x(s)1 = ?  ?  u??(s)  (  qu,s ? x(u)1 ? x(s)1 ? d?  k=2 x(u)kx(s)k  )  + ?|?(s)|x(s)1 ?s ? S, i = 2 ? d ?E(X)  ?x(s)i = ?  ?  u??(s) x(u)i  (  qu,s ? x(u)1 ? x(s)1 ? d?  k=2 x(u)kx(s)k  )  + ?|?(s)|x(s)i. (27) With (27), an efficient procedure can be taken to com-  pute ?E(X), as in Algorithm 2. It simply traverse on ?, and calculate the effect of each known instance qu,s ? ? on ?E(X). Given max(|U|, |S|) << |?| which is the usual    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Algorithm 1 SOLF Input: ? Initialize X(|U|+|S|)d = Random Initialize m = 0, M = max number of training rounds while not converge & t < T do  compute ?E(X) ?X = CGD(?, X,?E(X)) X = X + ? ? ?X t = t + 1  end while Output: LF vector X  Algorithm 2 Gradient (GRAD) Input: ?, X Operation Cost Initialize ?E(|U|+|S|)d = 0 ?((|U| + |S|)d) for (u, s) ? ? ?|?|  a = x(u)1 + x(s)1 ?(1) for i = 2 to d ?(d ? 1)  a = x(u)i?x(s)i ?(1) end for ? e = qu,s ? a ?(1) ?E(u)1 = ?E(u)1 ? e + ? ? x(u)1 ?(1)?E(s)1 = ?E(s)1 ? e + ? ? x(s)1 ?(1) for i = 2 to d ?(d ? 1)  ?E(u)i = ?E(u)i ? x(s)i ? e + ? ? x(u)i ?(1)?E(s)i = ?E(s)i ? x(u)i ? e + ? ? x(s)i ?(1) end for ?  end for ? Output: gradient vector ?E  case in practice, its computational complexity is TGRAD = ? (|?| ? d).

With a well-designed algorithm [46], the computational complexity of the CGD process comes to ? (TMULT ? N), where TMULT is the cost of computing the desired Hessian- vector product, and N denotes the iteration count to make the CGD process converge, respectively. Hence, in SOLF, its complexity mainly depends on the computation of ?A. Based on (26), we design the following MULT procedure to deal with this task.

As described in Algorithm 3, we compute the desired matrix-vector product with a single traverse on ?. Its com- putational complexity is  TMULT = ?(3|?| ? d + 2(|U| + |S|) ? d) ? ?(|?| ? d) (28) where the last step is also based on the precondition that max(|U|, |S|) << |?|.

Based on the inference above, we conclude that the com- putational complexity of Algorithm 1 is as follows:  THSOLF = ?(M(TGRAD + N ? TMULT)) = ?(M(|?| ? d + N ? |?| ? d)) ? ?(M ? N ? d ? |?|). (29)  Given that the maximum training round count M, CGD iteration count N and LF dimension d are all positive con- stants in practice, this complexity is actually linear with respect to |?|.

Meanwhile, as shown above, during the whole process, only vectors with the length of |?| and (|U|+ |S|)?d are involved  Algorithm 3 Multiplication (MULT) Input: ?, X, v Operation Cost  Initialize ?(|U|+|S|)dA = 0 ?((|U| + |S|) ? d) for (u, s) ? ? ?|?|  c = v(u)1 + v(s)1 ?(1) for i = 2 to d ?(d ? 1)  c = c + v(u)i ? x(s)i + x(u)i ? v(s)i ?(1) end for ? ?A(u)1 = ?A(u)1 + c ?(1) ?A(s)1 = ?A(s)1 + c ?(1) for i = 2 to d ?(d ? 1)  ?A(u)i = ?A(u)i + c ? x(s)i ?(1) ?A(s)i = ?A(s)i + c ? x(u)i ?(1)  end for ? end for ? for u ? U ?|U|  ?A(u)1 = ?A(u)1 + v(u)1(? ? |?(u)| + ? ) ?(1) for i = 2 to d ?(d ? 1)  ?A(u)i = ?A(u)i + v(u)i(? ? |?(u)| + ? ) ?(1) end for ?  end for ? for s ? S ?|S|  ?A(s)1 = ?A(s)1 + v(s)1(? ? |?(s)| + ? ) ?(1) for i = 2 to d ?(d ? 1)  ?A(s)i = ?A(s)i + v(s)i(? ? |?(s)| + ? ) ?(1) end for ?  end for ?  Output: matrix-vector product ?A  in SOLF. Thus, its storage complexity is ?(max{|?|, (|U| + |S|) ? d}), which is easy to accommodate. Next, we carry out experiments to test its performance.



IV. EXPERIMENTAL RESULTS  A. General Settings  1) Datasets: Two datasets are adopted in the following experiments; both are collected by the WS-Dream system which is also the first large-scale system adopting the CF approach to QoS prediction [7], [8], [10], [11], [14]. The first one, i.e., D1, is the response-time dataset, which contains 1 873 838 response-time data by 339 users on 5825 real- world Web-services. The second dataset, i.e., D2, consists of 1 831 253 throughput data by 339 users on 5825 real-world Web-services. Note that these two WS-Dream datasets are the largest QoS datasets publicly available, and are commonly adopted in many pioneering research regarding the problem of QoS-prediction [7]?[15].

On both datasets, we have designed different test cases for validating the performance of each involved model under dif- ferent data densities, as shown in Table I. The column ?Train: Test? means the ratio of training data to testing ones; e.g., 5%:95% denotes that 5% of the given data are chosen ran- domly and employed as training data, to predict the remaining 95% of the given data. This process is repeated for ten times to obtain more objective results. Naturally, the testing data are not involved in the training process.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

LUO et al.: INCORPORATION OF EFFICIENT SECOND-ORDER SOLVERS INTO LF MODELS FOR ACCURATE PREDICTION 7  TABLE I DETAILED SETTINGS OF THE TEST CASES  The reason for such experimental design is twofold.

1) For a CF-based QoS predictor, the data density of the  target U-S matrix will naturally affect its prediction accuracy since more known data provide more infor- mation regarding user/service behavior. Therefore, it is necessary to test a model with such density variation.

2) In real applications, the QoS data are collected chrono- logically; however, the time stamps are not provided in both experimental datasets. Hence, we reasonably assume that the probabilities for a record to arrive ear- lier or later than the others are the same, and build the training/testing datasets at random. Moreover, since the output will also be affected by the training-testing set- tings, we have also adopted a repeating mechanism, i.e., for each test case, we have ten different runs, where each run generates different training/testing datasets with the same split ratio.

2) Evaluation Metrics: In this paper, we focus on the accu- racy of generated QoS-predictions, since it can directly reflect whether or not the model has captured essential characteristics of the given data. We adopt the mean absolute error (MAE) and root mean squared error (RMSE) [47], [48] as the accuracy metrics:  MAE = ?  (u,s)??  ? ?qu,s ? q?u,s  ? ? /|?|  RMSE = ? ?  (u,s)??  ( qu,s ? q?u,s  )2/|?| (30)  where ? denotes the testing dataset, and naturally ??? = ?.

Note that as indicated in prior works [6]?[16], [19]?[29], MAE and RMSE are commonly adopted to evaluate the prediction accuracy of a CF model. We have recorded the time to train all tested models to evaluate their efficiency.

3) Experimental Environments: All experiments are con- ducted on a PC with a 2.5 GHz i5 CPU and 32 GB RAM. All involved models are implemented in JAVA SE 7U60.

B. Hyper-Parameter Test  Although SOLF can be implemented efficiently as described in Section III-D, it has several hyper-parameters that con- trol its training process. Hence, it is necessary to conduct parameter-sensitivity tests to see if they have strong effect on its performance. Table II summarizes them and their test scales. Note that the CGD iteration threshold N, SOLF  TABLE II OUTLINE OF THE HYPER-PARAMETER TEST  iteration threshold T and LF space dimension d are fixed at 500, 1000, and 20, respectively.

First, SOLF relies on a CGD process to solve the training increment ?X in each iteration. During such a process, two hyper parameters are involved.

1) The relative tolerance coefficient, i.e., ? in Table II, takes charge in controlling the precision when solving the target linear system.

2) The CGD iteration count N. In our experiment, we set N = 500 since the CGD process barely exceeds it across the experiments. So, we draw tests with the value of ? changing to see its effect; its tested scale is [10?7, 0.1].

Meanwhile, ? and ? are frequently seen in SOLF?s training process. ? controls the Tikhonov regularization, which inte- grates the l2 norm of X to prevent it from overfitting the generalized error. ? controls the damping effect in (26). With it the update direction is actually between the pure second- order direction and the steepest-gradient direction. Note that during the optimization process the second-order direction can be sometimes aggressive. It will not only enable high searching speed but also cause overshooting, thereby resulting in accu- racy loss. Therefore, a damping term as in (26) is required to avoid such situations. The tested scales are [0.1, 1] for ?, and [10?4, 105] for ? .

The information in ?X is twofold: 1) update direction and 2) fixed step-size at one. Hence, we introduce a controlling parameter ? into the update process as in (26), for obtaining the finely tuned results. Its tested scale is [0.1, 1].

LF space dimension d and training iteration count M of SOLF are common adjustable parameters in LF models, and we set d = 20 and M = 1000, as commonly used [11]?[28].

Here, we present the parameter-sensitivity tests on case D2.3 with RMSE only. Note that similar situations are observed on the others. To show the effect of different hyper- parameters, we choose ? = 0.1, ? = 0.1, ? = 1, and ? = 0.1 as the base point, and tested the performance by tuning one hyper-parameter and fixing the other three. In this experi- ment, we care about their effect on SOLF?s prediction accuracy and efficiency. Hence, we have recorded SOLF?s RMSE and consumed time as they change.

The results are depicted in Fig. 2. From it we see that each hyper parameter affects the performance of SOLF:  1) As ? varies from 0.1 to 1.0, we find an improvement in SOLF?s prediction accuracy, as depicted in Fig. 2(a).

For instance, with ? = 0.1 and 1.0, its RMSE is 45.04 and 40.69, which indicate an improvement in prediction accuracy at 9.66%. However, this improvement is at the cost of more computing time, as shown in Fig. 3(a). With ? = 0.1 and 1.0, the consumed time is, respectively,    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Fig. 2. RMSE of SOLF with hyper-parameters ?, ? , ?, and ? changing (the base point is ? = 0.1, ? = 1.0, ? = 0.1, and ? = 0.1; when testing one parameter, the other three are fixed on their base values). (a) ? from 0.1 to 1.0. (b) ? from 10?4 to 100. (c) ? from 0.1 to 1.0. (d) ? from 10?7 to 0.1.

1.19?104 and 4.33?104 ms. Thus, the consumed time with ? = 1.0 is about 3.64 times as much as that with ? = 0.1. This is because ? controls the Tikhonov regu- larizing effect. By amplifying this effect with bigger ?, the training process of the desired LFs in X is taken more carefully by keeping a balance between minimizing the generalized loss and the l2 norm of X.

2) Different conclusions are drawn regarding the effect of ?. As shown in Fig. 2(c), with ? increasing from 0.1 to 1.0, we see a drastic increase in RMSE. The RMSE with ? = 0.1 and 1.0 are, respectively, 45.04 and 51.03, which indicate a 13.30% decrease in accuracy. However, large ? leads to fast convergence. The consumed time with ? = 0.1 and 1.0 are, respectively, 1.19 ? 104 and 1.95 ? 103, meaning that time cost by enlarging ? from 0.1 to 1.0 is decreased at 83.61%. It is well-known that in first-order solvers like gradient descent, the step-size is very important in ensuring a finely tuned search- ing process, making the model converge with a good solution [11]?[28]. However, in second-order solvers, such as the Newton-type methods [30]?[43], it is not common to employ this coefficient. This is partially because in the latter, the update increment, e.g., ?X in SOLF, is figured out through solving the second- order Taylor-approximation of the objective function, and is presumed to be the optimal update [30]?[43].

However, in our problem the situation is a bit differ- ent. First, the objective function is nonconvex in X, and thus either first-order or second-order solvers can achieve a local optimum only. Second, what we care is not only to make the model converge at a local opti- mum, but also to achieve a best possible local optimum.

In other words, the obtained model is expected to gener- ate predictions for missing-entries in Q with the highest accuracy. Thus, to integrate ? into SOLF is necessary to obtain a finely tuned second-order searching process, as shown by Fig. 2(c).

3) The situation is more complex when testing the effect of ? . As shown in Figs. 2(b) and 3(b), as ? increases from 10?4 to 0.1, we observe nearly no change in accu- racy or time of SOLF. However, as ? increases from 0.1 to 1000, the performance gap becomes larger; both accuracy and time consumption of SOLF increase. For instance, with ? = 0.1 and 1000, its RMSE is, respec- tively, 45.57 and 39.86; the gap is about 12.53%. On the other hand, its consumed time is, respectively, 1.15?104  and 3.46 ? 104; and the increase in computational bur- den is also obvious. Note that as in (23) and (26), ? controls the effect of the damping term, which actually affects the searching direction of SOLF. With ? > 0 the search is taken not along the pure second-order direction, but another one between the second-order and first-order directions. So, with small ? it goes more toward the second-order direction, and vice versa. Therefore, at the beginning of our test, i.e., ? ? 0.1, we observe no performance change since the damping effect is too small to deflect the model from the second-order direc- tion. As ? gets larger, i.e., in [0.1, 1000], the training direction goes more toward the first-order direction, resulting in increase in accuracy and time. However, does this mean that to search along the pure first- order direction can enable even higher accuracy? The answer is no. As shown in Fig. 2(b), with ?>1000, the RMSE rises again. Actually, with too large ? , SOLF is transformed into a first-order model with a search direction along the deepest gradient of the objective function. The decrease in RMSE indicates that second- order information is indeed vital to achieve the highest prediction accuracy. Moreover, as shown in Fig. 3(b), with ?>1000, the time of SOLF increases drastically.

For instance, with ? = 1000 and 100 000, its consumed time is, respectively, 3.46?104 and 2.91?105 ms. This huge difference in time indicates that it is necessary to update LFs along a proper direction combining second- order and first-order information, for high prediction accuracy and efficiency.

4) It is not surprising that as ? increases, the time cost of SOLF decreases, as shown in Fig. 3(d). This is because ? controls the effect of relative tolerance in CGD; larger ? allows one to solve the linear system with lower precision, i.e., making the linear system in (23) not strictly equal to zero. Consequently, fewer itera- tions are taken by the CGD process, thereby resulting in less consumed time. Interestingly it is possible to improve the prediction accuracy of SOLF by adopting larger ? . As depicted in Fig. 2(d), when ? is in the scale of [10?7, 10?4], SOLF?s accuracy appears stable.

As ? increases from 10?4 to 10?3, its RMSE slightly increases. This is reasonable since large ? results in low CGD precision. However, with ? increasing from 10?3 to 10?1, we find a sudden drop in RMSE. One pos- sible reason for this is that the objective function is    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

LUO et al.: INCORPORATION OF EFFICIENT SECOND-ORDER SOLVERS INTO LF MODELS FOR ACCURATE PREDICTION 9  Fig. 3. Consumed time of SOLF with hyper-parameters ?, ? , ?, and ? changing (the base point is ? = 0.1, ? = 1.0, ? = 0.1, and ? = 0.1; when testing one parameter, the other three are fixed on their base values). (a) ? from 0.1 to 1.0. (b) ? from 10?4 to 100. (c) ? from 0.1 to 1.0. (d) ? from 10?7 to 0.1.

TABLE III DETAILS OF TESTED MODELS  User and item Pearson correlation coefficient model  Neighborhood improved matrix factorization  nonconvex; by setting ? too small, the output model overfits the training data. This issue surely calls for our further investigations. Therefore, in SOLF it is vital to choose ? wisely for both high accuracy and efficiency.

For instance, with ? = 10?7, SOLF achieves the RMSE at 54.03 in 4.48 ? 105 ms. But with ? = 0.1, its RMSE is 45.04, and the consumed time is 1.19?104 only. This indicates a 16.64% improvement in accuracy as well as a 97.34% decrease in the training time. Can we con- sistently improve the performance of SOLF by keeping ? increasing? Unfortunately, as ? grows too big, e.g., around two on case D2.3, SOLF cannot converge. This is because with too much decrease in precision, the CGD process cannot solve the linear system correctly, not to mention optimally.

C. Comparison Against State-of-the-Art CF-Based Predictors  In this experiment, we have compared SOLF against four state-of-the-art CF-based QoS-predictors to see if it can obtain advantage in performance. They are summarized in Table III.

Their experimental settings are as follows.

1) For user and item Pearson correlation coeffi-  cient (UIPCC), the number of the nearest neighbors and the balancing coefficient directly decide the performance. So we tuned them together on all cases following [8].

2) Dimension of the LF space, i.e., d, is set at 20. To elimi- nate the effect of the random initial guess, each model is initialized with the same randomly generated LF arrays.

TABLE IV RMSE OF TESTED MODELS ON ALL CASES  TABLE V CONSUMED TIME CORRESPONDING TO TABLE IV (SECONDS)  3) For probabilistic matrix factorization (PMF), the momentum is set at 0.9, and we have tuned its step-size following [20].

4) Weighted non-negative matrix factorization model (WNMF) has no hyper-parameters.

5) For non-negative latent factor (NLF), the regularizing coefficient is tuned on each case following [15], [16].

6) Neighborhood improved matrix factorizations (NIMF?s) balance coefficient, the number of Top-K users and regu- larization coefficient are set at 0.4, 10 and 0.001 accord- ing to [14].

7) For SOLF, the hyper parameters are set as ? = 0.1, ? = 100, ? = 0.1, and ? = 0.1 on D1, and ? = 1, ? = 100, ? = 0.1, and ? = 0.1 on D2.

Tables IV and VI record their RMSE and MAE on all cases.

Tables V and VII record the time to train them to obtain the results in Tables IV and VI. From these results, we have the following findings.

1) Compared with its peers, SOLF has significant advan- tage in prediction accuracy. As shown in Tables IV and VI, on all cases it is able to achieve the low- est prediction error. Meanwhile, this accuracy gap is quite obvious. For example, on case D1.1, the MAE of SOLF is 0.5321, 1.74% lower than the MAE at    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

TABLE VI MAE OF TESTED MODELS ON ALL CASES (SECONDS)  0.5415 by NIMF, 2.15% lower than 0.5438 by NLF, 4.68% lower than 0.5582 by PMF, 10.10% lower than 0.5919 by WNMF, and 21.31% lower than 0.6762 by UIPCC. In terms of RMSE on D1.1, it achieves 1.318, which is 3.23%, 6.26%, 6.99%, 6.33%, and 14.86% than the MAE at 1.362, 1.406, 1.417, 1.407, and 1.548 by NIMF, NLF, PMF, WNMF, and UIPCC, respectively.

On D2, SOLF can obtain even more significant advan- tage in accuracy. For instance, on case D2.1, its MAE is 18.37; compared with that achieved by its peers the improvement is at 5.02%, 14.32%, 18.57%, 33.63%, and 35.99%, respectively. In terms of RMSE, it achieves 52.78, which is 4.72%, 10.02%, 12.09%, 33.31%, and 16.49% lower than that by its peers.

2) Although we design a rather efficient algorithm to imple- ment SOLF?s second-order training, it consumes more time than the most efficient first-order models. For exam- ple, on case D1.1, SOLF spends 12.4 s, which is 4 and 2 times as much as that consumed by PMF and NLF, respectively. Similar results can also be observed on the other cases, as recorded in Tables V and VII.

This phenomenon, however, is explainable. Note that SOLF solves the linear system in (26) via CGD. Thus, its training process is actually taken by a two-layered loop, where the first layer is SOLF and the second one is CGD. Commonly, dozens to hundreds of iterations are taken in CGD to solve the linear system, and each iteration requires traversing on |?|. Hence, it is reason- able that SOLF costs more time than PMF and NLF, which require a single layered loop only. When com- pared with WNMF, SOLF is faster on D2 and case D1.1, but slower on D1.2?D1.4. Generally, the time costs of these two models are comparable. Since the time complexity of UIPCC is quadratic with respect to user and service counts, it consumes much more time than its peers including SOLF. For NIMF, it also requires computing the user similarity for integrating the neigh- borhood information into it. Hence, its time cost is much higher than the other tested LF-based models.

To summarize, when compared with its peers, SOLF achieves significant advantage in prediction accuracy because of its novel use of the second-order information. However, it takes more time than the most efficient first-order mod- els. Hence, it is suitable for industrial applications seeking for the most accurate QoS predictions with the tolerance of high computational burden.

TABLE VII CONSUMED TIME CORRESPONDING TO TABLE VI (MILLISECONDS)

V. RELATED WORKS AND DISCUSSION  A. Virtues of LF-Based QoS-Predictors  For QoS-based approaches to service selection and composition [5]?[7], [49]?[51], the user-side QoS data are highly important for building a robust and efficient system.

They can be obtained through real-world service evaluations; however, they are expensive and time-consuming as the can- didate service count increases. Hence, CF-based approaches to QoS prediction are proposed [6]?[15] for enabling reliable QoS predictions based on historical QoS data. Among them, LF-based models have proven to be highly accurate.

The idea of LF-based QoS-predictors originates from matrix-factorization (MF) techniques [16]?[28]. They work by building the low-rank approximation to a given target matrix that is sparse [11]?[15]. Given a sparse U-S matrix, they map both users and services into the same LF space, denoting them with LF vectors. Then, a generalized loss function is designed on the known data of the target U-S matrix regarding the desired LFs. Finally, the resultant loss function is optimized with respect to these LFs [11]?[28], which are subsequently employed to predict unknown user-side QoS data. Different from traditional MF models, LF-based QoS-predictors are especially designed for incomplete U-S matrices to address their sparsity nature and predict their unknown entries.

An LF-based QoS-predictor consists of the user and service LFs only. When building a low-rank approximation to a U-S matrix, the dimension of its LF space can be set low without impairing its accuracy, i.e., each user/service corresponds to a small number of LFs. Hence, its storage complexity is linear with respect to user/service counts [11]?[28], which is easy to accommodate in practice. Moreover, with a well-designed loss function, it is able to achieve very competitive prediction accuracy [11]?[15], which is vital for QoS-based approaches.

Moreover, an LF-based QoS-predictor can become more accurate by extending its input data source.

Zhang et al. [11], [12] proposed to integrate the time interval as an additional factor into an LF-based QoS-predictor, thereby improving its prediction accuracy. Lo et al. [13] proposed an extended LF-based model by considering the location information in each historical QoS record. Zheng et al. [14] proposed an LF-based QoS-predictor with the consideration of neighborhood information. Luo et al. [15], [16] proposed to ensemble a set of diversified non-negative LF models to achieve a highly accurate QoS-predictor. These models have proven to be able to make more accurate QoS-predictions than their counter algorithms without the mentioned considerations.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

LUO et al.: INCORPORATION OF EFFICIENT SECOND-ORDER SOLVERS INTO LF MODELS FOR ACCURATE PREDICTION 11  B. Performance of the Proposed SOLF Model  To build an LF-based QoS predictor on a sparse U-S matrix is a bi-linear and nonconvex problem. Hence, the performance of such a model relies heavily on the optimization process with respect to its LFs. In spite of their high accuracy, the existing LF models all adopt first-order solvers, e.g., gradient-decent- based solvers or their extensions [11]?[28] to deal with the optimization task. As proven by pioneering research [29]?[43], when addressing nonconvex optimization problems, second- order solvers are usually more effective because of their use of the second-order information.

For instance, the work [29] adopted the Newton method to factorize sparse matrices directly, achieving more accu- rate predictions for their missing entries. Such approaches, however, require computing the full Hessian and its inverse, which is extremely time-consuming. In industrial applications, the user/service counts are usually huge, thereby leading to numerous LFs [11]?[28]. Since the size of a Hessian matrix is quadratic with respect to the number of LFs in our case, it is impractical to employ it directly to implement second-order optimization in an LF-based QoS-predictor.

The proposed SOLF integrates the second-order information into its optimization process with affordable computational burden following the principle of Hessian-free optimization.

As indicated by the experimental results, its accuracy is sig- nificantly higher than state-of-the-art first-order models. Its time cost is higher than that of the most efficient LF mod- els, but comparable with that of the rest rival models in the experiments.

C. Validity of the Experimental Results  The performance of LF-based QoS-predictors mainly relies on the following factors: 1) hyper parameters; 2) initial hypothesis; and 3) datasets. In our experiments, the hyper parameters of involved models are carefully tuned on each testing case to eliminate the performance gap caused by hyper parameter settings. The involved LF models are initialized with the same randomly generated LF arrays to eliminate the performance gap caused by initial hypothesis. The experiments are conducted on the largest QoS datasets which are publicly available [7]?[15]. For each testing case, the random train-test data split is repeated for ten times. By doing so, we aim at minimizing the performance gap caused by experimental data.

Although the WS-Dream datasets are the largest QoS datasets which are publicly available [7]?[15], more objective results are expected with even larger dataset. This data collec- tion and analysis tasks are included in our future plan.

D. Possible Industrial Usage of SOLF  Since the time cost of SOLF is higher than that of the most efficient first-order models, it is not suitable for applications with real-time demands. However, since it is able to achieve highly accurate predictions for unknown QoS data, it can act as an offline module in applications seeking for high prediction accuracy. One typical case of such applications is the QoS- based service recommendation system as proposed in [8]?[12], which: 1) gathers user?s historical QoS data collaboratively;  Fig. 4. Deployment of SOLF in real applications.

2) trains a QoS-predictor offline; 3) makes predictions for unknown QoS data based on known ones; and 4) recommends the most suitable services to users according to their QoS demands and the generated QoS predictions. In such appli- cations, SOLF is able to improve the QoS recommendation owing to its high prediction accuracy.

E. Deployment of SOLF in Real Applications  Section III-D presents the detailed algorithm design for SOLF, which enables the implementation of SOLF in common programming languages. In real applications, SOLF serves as a decision assistant, focusing on assisting service-oriented applications to make service selection based on its historical QoS data. Fig. 4 gives the deployment of SOLF in service- oriented applications. SOLF and real applications are loosely coupled. As the latter generate QoS data, an SOLF model can be built to generate unknown predictions. Then a service- oriented application takes advantage of such predictions to select the suitable services from the service set.

F. Future Works  The accuracy gain by SOLF is achieved on the QoS data only, without any additional information. As mentioned in [9]?[15], the accuracy of a QoS-predictor can be further enhanced by fusing additional information [9]?[14], or aggre- gating a set of diversified base models [15]. Such strategies can also be applied to SOLF for additional performance gain. Moreover, it is also highly interesting to investigate the applications of SOLF in various data analysis tasks [52]?[57].



VI. CONCLUSION  This work aims at proposing a highly accurate second-order LF model to address the problem of QoS-prediction, which was never seen in the prior investigations. Traditional Newton- type methods are straightforward and easy to implement. Yet they suffer unaffordable complexity because of their use of a Hessian matrix. The proposed model applies the principle of Hessian-free optimization to a second-order training pro- cess, thereby resulting in LFs with second-order information but only limited computational burden. By applying them to predict missing QoS data, we find that SOLF is able to achieve significant advantage in prediction accuracy. Yet its efficiency should be further improved.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

