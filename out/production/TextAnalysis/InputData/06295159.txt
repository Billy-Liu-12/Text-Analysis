Supported by ?the Fundamental Research Funds for the Central Universities?   NO: XDJK2011C076

Abstract?Data mining deepens the data analysis, also is able to mine the interesting mode hiding in mass data automatically. As a new data analysis technology, data mining makes full use of the modern software technology and computer scientific knowledge, has the extremely important research significance, provides for researchers of various fields with a new intelligence means to realize and use data. In data mining, association rule learning is a popular and well researched method for discovering interesting relations between variables in large databases. This paper makes a series of data analysis and research on the application of data mining technology in the students? scores. This paper use Apriori algorithm of association rules to analyze the intrinsic link among various courses, dig out the precedence relationship and association of students? learning courses, reveal the teaching regularities and problems from large amount of data, as well as to provide a strong basis for reasonable course-setting .

Index Terms?Data Mining; Association Rule;Frequent Itemsets;

I.  INTRODUCTION With the rapid development of modern scientific  technology and the continuous improvement of education reform, information technology was widely used in education, and the associated data and information will increase endlessly.

Education information is all kinds of education data which have been accumulated by education workers during the long- term teaching practice and management. How to make use of the data more effectively, develope all kinds of teaching research scientifically, make teaching management decisions, etc, becomes a very important research subject. At this time, the function of data mining technology is displayed. With the purpose of solving the problem of ?Rich Data, Poor Knowledge?, data mining technology, a new data analysis with obvious advantages in mass data analysis ,whose essence is a procedure to extract implicit, previously unknown, and potentially useful information, also a powerful tool to help people to find out knowledge and research regularities from mass data.



II. DATA MINING  A. Summary of Data Mining With the wide application of database technology, the  development of various database systems, the data has been accumulated largely in the computer at unprecedented speed, so people can analyse and research these data. Because the  database system only provides some simple processing function such as data management and stored data query, but it is not easy to analyze and use the hidden information deeply from mass data. So, the main problems the people facing are no longer the lack of information to use, but how to effectively use the data from the vast data like the ocean. Consequently, people hope that there is a way to deal with the complicated data, and to find out valuable information or knowledge to service for decision-making, reducing the workload. So, the data mining technology came into being in this context.

The international definition of data mining is: Data mining is the extraction of implicit, previously unknown from application data of abundant, incomplete, obscure, random, but it is the process that have potentially useful information and knowledge [1] [2].Data mining can be operated on any type of stored information with abundant data sources, such as relational database, data warehouse, text and multimedia database, transaction database, www, etc. At present, the data mining technology has been widely used in market basket analysis, financial risk prediction, telecommunication, molecular biology, genetic engineering research, the discovery of access mode to Internet site, and information retrieval, etc [3].

B. The Process of Data Mining Knowledge Discovery in Database (KDD) is a  complicated process that mining effective, novel, potentially useful and final understandable mode from database. Data Mining (DM) is the core of knowledge discovery and an important step, which use special algorithm to extract mode from the data [4], as shown in figure 1 below.

Database Data Warehouse Knowledge  Discovery  Data Cleaning Selection and Transformation Data Mining Assessment  and Representation  Data Preprocessing  mode  Data Mining Outcome Evaluation  Specific Data  Database Data Warehouse Knowledge  Discovery  Data Cleaning Selection and Transformation Data Mining Assessment  and Representation  Data Preprocessing  mode  Data Mining Outcome Evaluation  Specific Data   Figure 1: Data Mining is a Step of the KDD   Computer Science & Education (ICCSE 2012) July 14-17, 2012. Melbourne, Australia   ThP4.33        The process of KDD is made up of following steps: 1) Data Cleaning: To improving data quality by  eliminating inaccurate, defective incomplete, inconsistent tuples from the original data set [5].

2) Data Integration: The data used for Knowledge Discovery may come from multiple fields and systems, and it is necessary to extract relevant information from various data sources to build new data sets, as well as to eliminate redundancy of attribute.

3) Data Selection: To retrieve and analyze data related to the task from a database, and abandon data unrelated to data mining.

4) Data Transformation: Data have been transformed or united into suitable form for mining.

The above four steps called data preprocessing which accounts for the largest proportion in the whole process of data mining, usually 60%[6], and Data Mining, Knowledge Explanation & Evaluation account for 10%, 30% separately.

Data preprocessing can improve the quality of the data, and the accuracy and performance of subsequent mining process.

5) Data Mining: To make sure the task or goal of mining firstly, and then decide use which kind of mining algorithm to implement the data mining operation, and to extract data mode. Two factors shall be taken into consideration while selecting algorithm: one is different data have different characteristics; another is the requirements of user or actual operation system should be met.

6) Mode Assessment: The explanation and evaluation of those results. To analyze the extractive mode at data mining stage, eliminate the redundant or irrelevant mode. If there is no mode in accordance with the requirements of users, it will return to the previous stage, such as data reselection, adopting new method on data transformation, setting new parameter values, even using other data mining algorithms.

7) Knowledge Representation: It provide for users with the mining knowledge through visualization and knowledge representation technology. Data mining will face users ultimately, so visualization should be used in found mode as far as possible, or the result should be converted into other means which easily accessible to the users.



III. ASSOCIATION RULES Association means the regularity among the values of two  or more data items, which can set up simple and practical association rules of these data items [7]. The purpose of the association analysis is to dig out the meaningful relationship hidden in data, to detect hidden mode not found before automatically, and provide a very important, valuable information or knowledge for decision maker. The most famous mining method of association rules is Apriori algorithm, an effective method of mining association rules from the large scale commercial data put forward by American scholars r. Agrawal and others.

A. Related Concept of Association Rules 1) Item: As for a data sheet, each field has one or more  different values. Each value of fields is an item [8].

2) Itemsets: The sets of items mean itemsets. K -Itemsets means the itemset with K itemsets; K is the number of items in itemsets. Maximum itemsets is a set that consist of all the items, generally is represented by symbols ?I?.

3) Affair: Affair is a set of itemsets and is a subset of itemsets. Set of affairs is called affairs sets, generally is represented by symbols ?D?. Each affair has a unique identity, written as TID. Supposing X is a itemset, T is a affair, and TX ? , then T include X, marked as TX ? .

4) Association Rules: Association Rules are implications like as X?Y, in which IX ? , IY ? , and ?=YX ? .

Generally, the following two parameters are used to descripte attributes of association rules:  a) Support: In transaction database D, the support of rules X?Y is the ratio between the affairs number of X,Y and total affairs number which contained in affair sets, marked as support(X?Y),namely support(X?Y)=|{T: DTTYX ?? ,? }|/|D|.The support describes the probability of X itemsets and Y itemsets appear in all affairs at the same time.

b) Reliability: Reliability is often termed Confidence.

In affair sets, the confidence of rules X?Y is the ratio between the affairs number of X,Y and the affairs number of X ,marked as confidence(X?Y), namely confidence(X?Y)=|{T: DTTYX ?? ,? }| |{ DTTXT ?? , }|. The reliability expresses the probability of X itemsets and Y itemsets appear in D affair at the same time.

In order to find out meaningful association rules, two threshold values should be given: minimum support and minimum confidence. The former is the minimum support that must be met by association rules specified by users, which shows the lowest level itemsets should be met in statistics sense; the latter is the minimum confidence that must be met by association rules specified by users, which reflects minimum reliability should be met by association rules.

Without factoring in support and confidence of association rules, there will be too much association rules in transaction database, however, people usually have interested in association rules which can meet certain support and confidence. Therefore, a given threshold of support and confidence can limit output the quantity of association rules of the data mining system, and to provide users with a meaningful association rules as far as possible.

5) Frequent itemset: If the support of an itemset is equal or greater than the threshold value of support, this itemset is called frequent itemsets. Frequency set with k items is called k-frequency set, or frequent k-itemsets.

B. Apriori Algorithm The basic idea of the Apriori algorithm is to break the  design of association rules mining algorithm down into two steps [9]:  1) To find out all itemsets whose support is greater than the minimum support, namely find out frequent itemsets;  2) To use frequent itemsets found out in the first step to generate anticipant rules.

ThP4.33        The second step only considers the situation that there is only one item on the right side of the rules. Given a frequent itemsets:Y=I1,I2,?,Ik, k ? 2,Ij?I, and then there are at most k rules of items which come from set { I1,I2,?,Ik}. These rules such as :I1,I2,?,Ii-1,Ii+1,?,Ik?Ii,1 ? i ? k. In these rules, only those rules whose confidence is greater than that specified by users can be remained.

In order to generate all frequent itemsets, Apriori algorithm uses a recursive method. The pseudo code of the recursion algorithm can be expressed as:  Ck: It is a set that make up of all candidate k-itemset Lk: It is a set that make up of all frequent k-itemset L1= {It is a set that make up of all frequent l-itemset}; For(k=2 Lk-1?? k++)do begin Utilize Lk-1 to generate Ck For each transaction in database do Increment the count of all candidates in Ck that are  contained in t Lk=candidates in Ck with min_support End Return= kkL? The algorithm generates set L1of frequent 1-itemset, and  then generates set L2 of frequent 2-itemset, but the algorithm will not stop until there is some r value causes the Lr empty.

When the number of recurrence is k, set Ck of candidate k- itemset is generated in which the items is used to generate candidate itemset of frequent itemset, and the last frequency itemsets Lk must be a subset of the Ck[10].

If there are m frequent itemsets in Lk-1, Ck will get m(m+1)/2 itemsets from Ck is got through Lk-1?s self join, in which there are few frequent k-itemsets. Therefore, it is necessary to clip after the generation of Ck. The clip strategy is introduced into algorithm based on this property: One itemset is frequent itemset if and only if it?s all subset is frequent itemset. So, if one (k-1) subset of some candidate itemset in Ck is not belong to Lk-1, this itemset should be clipped without consideration. This clip process can reduce cost while calculating the support of all candidate itemsets.

C. Association Rules Generated from Frequent Itemsets 1) According to the given minimum support (min_sup),  finding out all frequent itemsets in the database to be explored; 2) According to the given minimum confidence  (min_conf), strong association rules generate from frequent itemsets. The theory basic of the generate of strong association  is: on condition that Confidence(X?Y) = D(X)support DY)(Xsupport  ? ??  (1) And if each non-empty subset of set l meet  conf l  min_ D(s)support D)(support  ? ? ?  (2) Then strong association rules s ? (l-s) will be generated.

Because of the rules are generated by the frequent itemsets, each rule automatically meet minimum support.



IV. PRACTICAL APPLICATION OF ASSOCIATION RULES IN TEACHING  The following content mainly research on how to mine data of Students? scores, generate association rules with mutual influence among courses by Apriori algorithm, find out the potential relationship among courses, analyze which course is the indispensable preparation for others, make clear the importance of the basic courses, provide favorable basis to courses arrangements for teaching management departments, consequently to promote teaching work scientifically, improve the teaching effect and quality, help students master knowledge more comprehensively.

A. Data Preprocessing Scores of basic courses and specialized courses of  computer-major students are sampled at random to analyze the 29 students? scores of 6 courses. After cleaning up the records of lacking of scores, absence from examination and breaking of discipline, there are 27 records remaining. Next, implement data discretization about each score [11],and we should divide the 27 records remaining into three categories: good (greater than 80) set as 1, normal (less than 80 but greater than 65) set as 2, and bad (less than 65) set as 3. Finally, we make each course name into characters, and name after the initials of course names, in order to facilitate the following evaluation and analysis. The scores viewgraph of students? basic courses and specialized courses as follows:  Table1 Viewgraph of Students Scores  Student Number Advanced Mathematics  C Programming  Language  Information Technology Introduction  Database Principles  and Applications  ASP Java Script  262005080605001 90 73 84 69 85 75  262005080605002 60 50 60 37 50 60  262005080605004 34 48 76 36 72 76  262005080605005 43 60 67 60 64 65  262005080605006 50 69 72 60 70 62  262005080605007 64 48 63 60 54 64  262005080605008 86 73 88 69 76 68  262005080605010 100 64 78 69 78 64  ?? ?? ?? ?? ?? ?? ??  Table 1 can be regarded as an affairs set D, then | D | = 27. According to the experience of the past experiments, support is designated to be 5, then the minimum support min_sup = 0.2, and the minimum confidence min_conf = 0.3, the results was shown in following Table 2 after preprocessing the data in Table1:  Table 2: Viewgraph after the First Preprocessing Student Number M C I D A J  262005080605001 1 2 1 2 1 2  262005080605002 3 3 3 3 3 3  262005080605004 3 3 2 3 2 2  262005080605005 3 3 2 3 3 2  262005080605006 3 2 2 3 2 3   ThP4.33        262005080605007 3 3 3 3 3 3  262005080605008 1 2 1 2 2 2  262005080605010 1 3 2 2 2 3  ?? ?? ?? ?? ?? ?? ??  The next step is to inductive solving for each course score of each student, designate actual presence grade with?1?and others designate with?0?,the result as Table 3 shows.

Table 3: Viewgraph after the Second Preprocessing Student Number M1 M2 M3 C1 C2 C3 I1 I2 I3 D1 D2 D3 A1 A2 A3 J1 J2 J3   1 0 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0   0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1   0 0 1 0 0 1 0 1 0 0 0 1 0 1 0 0 1 0   0 0 1 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0   0 0 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 1   0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1   1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 0   1 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1  ?? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?  To count M1 ~ J3 in Table 4 respectively, the item whose count value is greater than support count 5 shall be reserved, otherwise abandoned, so frequent 1-itemset shown as Table 5 is generated. Next, candidate frequent 2-itemset shown as Table 6 is generated from frequent 1-itemset, and then count the support of each candidate itemset. The itemset, whose count value is greater than 5 shall be reserved, otherwise abandoned, so frequent 2-itemset shown as Table 7 is generated. Finally, according to the frequent 2-itemset and properties of Apriori, all non-empty subset of frequent itemsets must be frequent itemset too, and frequent 3-itemset is generated shown as Table 9.

B. Mining Process      Table 6 Find out candidate frequent 2-  itemset:136 items  Itemset Support Count  {M1,C1} 4  {M1,C2} 6  {M1,C3} 1  ?? ??  {M2,C1} 1  {M2,C2} 4  ?? ??  {M3,A1} 1  {I3,D3} 8  ?? ??  {I1,J2} 5  {D2,A1} 5  {D2,J1} 7  ?? ??  {D3,A2} 6  {A2,J2} 5  {A3 J3} 4  ?? ??    Table 7 frequent 2- itemset:37 items  Itemset Support Count  {M1,C2} 6  ?? ??  {M2,D2} 5  ?? ??  {M3,C3} 5  {C1,J1} 5  ?? ??  {C2,I1} 8  {C3,I2} 5  ?? ??  {I1,D2} 7  ?? ??  {I2,A2} 7  {D2,J2} 5  ?? ??  {D3,A3} 5  {A2,J1} 5  ?? ??  Find out C2  Candidate itemset C2 is generated after L1?s self join  To compare support count of candidate itemset and minimum support count, and then find out L2  Table 4 candidate frequent  1-itemset  Itemset Support Count  M1 11  M2 7  M3 9  C1 5  C2 14  C3 8  I1 10  I2 12  I3 5  D1 3  D2 13  D3 11  A1 7  A2 14  A3 6  J1 10  J2 9 J3 8  Table 5 frequent 1-itemset  Itemset Support Count  M1 11  M2 7  M3 9  C1 5  C2 14  C3 8  I1 10  I2 12  I3 5  D2 13  D3 11  A1 7  A2 14  A3 6  J1 10  J2 9  J3 8    To compare support count of candidate itemset and minimum support count, and then find out L1  Find out frequent 1- itemset: 17 items  To scan the Table 3 preprocessed, and count each candidate itemset  Find out C1   ThP4.33         128 association rules can be generated from the above 46  frequent itemsets. According to minimum confidence, part of the association rules as follows:  {M1, C2}, support is 6, the rule is: M1?C2, confidence=6/11=0.55=55%  C2?M1, confidence=6/14=0.43=43% {M1, I1}, support is 8, the rule is:  M1?I1, confidence=8/11=0.73=73% I1?M1, confidence=8/10=0.8=80% {M2, D2}, support is 5, the rule is:  M1?J1, confidence=5/7=0.71=71% J1?M1, confidence=5/13=0.38=38%  {M3, C3}, support is 5, the rule is: M3?C3, confidence=5/9=56% J1?M1, confidence=5/8=63%  ?? ?? {C1, J1}, support is 5, the rule is:  C1?J1, confidence=5/5=100% J1?C1, confidence=5/10=50%  {C2, D2}, support is 7, the rule is: C2?D2, confidence=7/14=50% D2?C2, confidence=7/13=54%  {C3, I2}, support is 7, the rule is: C3?I2, confidence=5/8=63% I2?C3, confidence=5/12=42%  {I1, A2}, support is 5, the rule is: I1?A2, confidence=6/10=60% A2?I1, confidence=6/14=43%  {I1, J1}, support is 5, the rule is: I1?J1, confidence=5/10=50% J1?I1, confidence=5/10=50%  ?? ?? {I2, D2}, support is 5, the rule is:  I2?D2, confidence=5/12=42% D2?I2, confidence=5/13=38%  {I2, A2}, support is 7, the rule is: I2?A2, confidence=7/12=58% A2?I2, confidence=7/14=50%  {D2, A1}, support is 5, the rule is: D2?A1, confidence=5/13=38% A1?D2, confidence=5/7=71%  {D3, A3}, support is 5, the rule is: D3?A3, confidence=5/11=45% A3?D3, confidence=5/6=83%  {A2, J2}, support is 5, the rule is: A2?J2, confidence=5/14=36% J2?A2, confidence=5/9=56%  ?? ?? {M1, C2, I1}, support is 6, the rule is:  M1 C2?I1, confidence=6/6=100% M1 I1?C2, confidence=6/8=75% I1 C2?M1, confidence=6/8=75% M1?C2 I1, confidence=6/11=55% I1?C2 M1, confidence=6/10=60% C2?M1 I1, confidence=6/14=43%  {M3, D3, J3}, support is 5, the rule is: M3 D3?J3, confidence=5/8=63% M3 J3?D3, confidence=5/5=100% D3 J3?M3, confidence=5/7=71% M3?D3 J3, confidence=5/9=56% D3?M3 J3, confidence=5/11=45% J3?M3 D3, confidence=5/8=63%  {C2, D2, J2}, support is 5, the rule is: D2 C2?J2, confidence=5/7=71% D2 J2?C2, confidence=5/5=100% J2 C2?D2, confidence=5/7=71% C2?D2 J2, confidence=5/14=36% D2?C2 J2, confidence=5/13=38% J2?C2 D2, confidence=5/9=56%  ?? ??  C. Explaination and Evaluation For the above association rules, we realize that if a  student does well in M course and C course, he must do well in I Course, because M1 C2 ? I1,confidence=6/6=100%, namely confidence reaches 100%; if a student does well in M course and I course, his A course or D course can not be bad; if a student doesn?t do well in M course and C course, his D course can be very bad usually, because the confidence is 100%; if a student does well in I course, his A course and J course must good too; if a student does well in C course and I course, his D course must be good, etc. To sum up, mastering M course and C course is a precondition for doing well in D course; the scores of A course and J course are greatly affected by I course. Thus it can be seen that basic courses is very important to the study of subsequent specialized courses.

Table 8 candidate frequent 3-itemset:  Itemset Support  Count  {M1,C2,I1} 6  {M1,C2,D2} 4  ?? ??  {M3,C3,D3} 5  {M3,I2,D3} 6  ?? ??  {C2,I1,D2} 6  {C3,I2,D3} 3  ?? ??  {I1,D2,J2} 4  {I2,D2,A2} 3  ?? ??  {D2,A2,J1} 4  ?? ??    Table 9 frequent 3- itemset:9  Itemset Support Count  {M1,C2,I1} 6  {M1,I1,D2} 5  {M1,I1,A2} 5  {M3,C3,D3} 5  {M3,I2,D3} 6  {M3,D3,J3} 5  {C2,I1,D2} 6  {C2,I1,J2} 5  {C2,D2,J2}    Candidate itemset C3 is generated after L2?s self join   To compare support count of candidate itemset and minimum support count, and then find out L3   ThP4.33        In addition, there are some important correlation between the courses, for example, if a student does poorly in M course and D course, he must get poor grades in J course; if a student does well in C course and J course, his I course can not be bad.

So, J course, and D course should not be arranged together at the same time, avoiding the negative influence that students can?t apprehend; improving the study of a course will help to study D course. Therefore, there are potential regularities among the courses, which are valuable to arrange courses for the department of teaching. The results mined is important for the department of teaching to easily obtain the related information among courses, to make decisions and to arrange courses reasonably, which is beneficial to student learning.

.CONCLUSION In this paper, association analysis technology has been  implemented in student?s scores, and the experimental effect is significant. We should use other mining technology, such as classification decision tree, clustering analysis, predict, statistics analysis in the future research, to dig out the valuable regularities or pattern hidden in students? scores data, to provide students, teachers and teaching management staff with different levels of data support, which playing a scientific guiding significance in teaching process and educational administration management.

