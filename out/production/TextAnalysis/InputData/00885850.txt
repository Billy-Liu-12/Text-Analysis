Tmng-Pei Hong, Ching-Yao Wang and Yu-Hui Tao

Abstract  In this paper; we propose the concept of pre-large itemsets and design a novel eficient incremental mining algorithm based on it. Pre-large itemsets are defined using two support thresholds, a lower support threshold and an upper support threshold, to reduce rescanning the original databases and to save maintenance costs. The proposed algorithm doesn 't need to rescan the original database until a number of transactions have come. If the size of the database is growing large6 then the allowed number of new transactions will be larger too. Therefore, along with the growth of the database, our proposed approach is increasingly eficient. This characteristic is especially useful for real applications.

1. Introduction  Finding association rules from transaction databases are the most commonly seen in data mining [ 1][2][3][4][7][8][ lo][ 11][ 13][ 141. In the past, many algorithms for mining association rules from transactions were proposed, most of which were executed in a level-wise process. Also, these algorithms usually assumed that the database was static and focused on batch mining. In real-world applications, however, new records are usually inserted into a database, and designing a mining algorithm that can maintain association rules as a database grows is thus important and critical.

When new records &e added into the database, the original association rules may become invalid, or some new implicitly valid rules may appear in the whole updated database [5][6][9][ 121. In this situation, the conventional batch mining algorithms must re-process the whole updated database to find the final association rules. Cheung and his co- workers thus proposed an incremental mining algorithm, called FUP (Fast UPdate algorithm) [5], to incrementally maintain the mined association rules. Although the FUP algorithm can indeed improve the mining performance for incrementally growing databases, original databases still need to be  scanned when necessary.

In this paper, we propose the: concept of pre-large itemsets and design a novel efficient incremental mining algorithm based on it. A pre-large itemset is not truly large, but is promising to be large in the future. Pre-large itemsets act like buffer in the incremental mining process a.nd are used to reduce the movement of an itemset directly from large to small and vice verse. The d,atabase re-scanning in FUP can thus by totally avoided in our proposed algorithm.

2. Maintenance of Association Rules  Considering the original datalme and newly inserted transactions, an itemset may have the following four cases:  Case 1: An itemset is large in both the original database and the newly inserted transactions.

Case 2: An itemset is large in the original database, but is not large in the newly inserted transactions.

Case 3: An itemset is not large in the original database, but is large in the newly inserted transactnons.

Case 4: An itemset is not large in both the original databasc: and the newly inserted transactions.

Among the above four cases, case I and case 4 will not affect the final associa.tion rules. Case 2 may possibly remove existing association rules, and on the contrary, case 3 may possibly add new association rules.

3. Review of the IFUP Algorithm Cheung et al. proposed the FUP algorithm to incrementally maintain the association rules from newly inserted transactions [5][6]. Using FUP, large itemsets with their counts in last runs are recorded for later usage in maintenance. As new transactions  0-7803-6400-7/00/$10.00 02000 EEE    Fourth ZntrmptiotlpI Conference on knowledfe-Based InteUimt EnPineering Systems 0 Allied Techdogm,  Avg-l* Sept 2000, Brigh&n,UK nine cases illustrated in Figure 1. are added, FUP fust scans them to gknerGe  candidate 1-itemsets (only for these transactions), and then compares these itemsets with the previous ones. FUP partitions candidate 1-itemsets into two parts according to whether they are large for the origiml database. If a candidate 1-itemset from the newly inserted transactions is also among the large 1-itemsets from the original database, its new total count for the whole updated database can easily be.

calculated from its current count and previous count.

On the contrary, if a candidate 1-itemset from the newly inserted transactions does not exist among the large 1-itemsets from the original database, two cases may possibly occur. If this candidate 1-itemset is not large for the new transactions, then it cannot be large for the whole updated database. Otherwise, if this candidate 1-itemset is large for the new transactions but not among the original large 1- itemsets, the original database must be re-scanned to determine whether it is actually large or not for the whole updated database. Using the above different processing tactics, FUP can thus find all large 1- itemsets for the whole updated database. After that, candidate 2-itemsets from the newly inserted transactions are formed and the same procedure is repeated for finding all large 2-itemsets. This procedure is run until all large itemsets are found.

Fup can thus efficiently handle cases 1, 2 and 4 when compared with conventional batch mining algorithms. It must however reprocess the original database for managing case 3.

4. Pre-large Itemsets  Although the FUP algorithm focuses on the newly inserted transactions and thus saves much processing time in incrementally maintaining the rules, it must still scan the original database to handle case 3 where a candidate itemsets is large for new transactions but not recorded in the previous large itemsets for the original database. Thus, if case 3 can be effectively handled, the maintaining time can be further reduced.

In this paper, we prcpose the concept of pre-large itemsets to solve the above problem. A pre-large itemset is not truly large, but is promising to be large in the future. Two support thresholds, a lower support threshold and an upper support threshold, are used to achieve this concept. The upper support threshold is the same as that in the conventional mining algorithms. On the contrary, the lower support threshold defines the lowest support ratio for an itemset to be pre-large. Pre-large itemsets act like buffer in the incremental mining process and are used to reduce the movement of an itemset directly from large to small and vice verse. Considering the original database and newly inserted transactions under two thresholds, an itemset may thus have the  Figure 1 : Nine cases existing in the original database and new transactions  Among nine cases, cases 1, 5,  6, 8 and 9 will not affect the final association rules. Cases 2 and 3 may possibly remove existing association rules, and on the contrary, cases 4 and 7 may possibly add new association rules. IF we keep all large and pre-large itemsets with their counts in the previous pass, then cases 2, 3 and case 4 can be easily handled. Also, in the maintaining phase, the ratio of the number of new transactions over the number of old transactions is usually very small. This phenomenon is more apparent especially when the database is growing larger. An itemset in case 7 cannot possibly be large for the whole updated database as long as the number of transaction is small when compared to the number of transactions in the original database. This point will be proven later.

5. Notation  The notation used in this paper is defined as follows.

D : the original database; T : the set of new transactions; U : the whole updated database, i.e.. Du T; d : the number of records in D; t : the number of records in T; S, : the lower support threshold for pre-large  Su : the upper support threshold for large itemsets,  ,L: : the set of large k-itemsets from D; L: : the set of large k-itemsets from r; ,$' : the set of large k-itemsets from I/; p p  : the set of pre-large k-itemsets from D; p,' : the set of pre-large k-itemsets from T; py : the set of pre-large k-itemsets from U; C, : the set of all candidate k-itemsets from T; I : an itemset; SD(I) : the count of I in D; ST([) : the count of I in T; ,$"(I) : the count of I in U.

itemsets;  s u  >SI;  6. Theoretical Foundation     As mentioned above, if the number of new newly comng transactions since the last re-scan of transactions is small when compared to the number of transactions in the original database, an itemset which is small (neither large nor pre-large) in the original database but is large in the newly inserted transactions cannot possibly be large for the whole updated database. This will be stated in the following theorem.

Theorem 1: let S, and S,, respectively be the lower and the upper support thresholds, and let d and t respectively be the numbers of the original and the  new transactions. If t I ('U - Sl)d , then an itemset  which is small (neither large nor pre-large) in the original database but is large in the newly inserted transactions is not large for the whole updated database.

b  1 - s u  7. The Proposed Maintenance Algorithm  According to the above discussion, an efficient maintenance algorithm can be designed for incrementally inserted transactions. The iarge and pre-large itemsets with their counts in last runs are recorded for later usage in maintenance. As new transactions are added, the proposed algorithm first scans them to generate candidate 1-itemsets (only for these transactions), and then compares these itemsets with the previously kept large and pre-large 1-itemsets. It partitions candidate 1-itemsets into three parts according to whether they are large or pre-large for the original database. If a candidate 1- itemset from the newly inserted transactions is also among the large or pre-large 1-itemsets from the original database, its new total count for the whole updated database can easily be calculated from its current count and previous count Whether an originally large or pre-large itemset is still large or pre-large after new transactions are inserted is decided from its new support ratio. On the contrary, if a candidate 1-itemset from the newly inserted transactions does not exist among the large or pre- large 1-itemsets from the original database, then it is absolutely not large for the whole updated database as long as the number of newly inserted transactions is within the safety threshold. In this situation, no action is needed. When the transactions are incrementally added and the total number of new transactions is beyond the safety threshold, the original database is re-scanned to find new pre-large itemsets in a way similar to the FW algorithm.

Using the above different processing tactics, the proposed algorithm can thus find all large 1-itemsets for the whole updated database. This procedure is run until all large itemsets are found. The details of the proposed maintenance algorithm are described as follows. A variable c is used to record the number of  the original database.

The proposed maintenance algorithm: INPUT: A lower support threshold SI, an upper  support threshold S,, i i  set of large itemsets and pre-large itemsets in the original database consisting of (d+c) transactions, and a set of t  new transactions.

OUTPUT: A set of final association rules for the  STEP1:  STEP;?:  STEP3:  STEP4:  STEP5 :  updated database.

Calculate the safety number f of new transactions according to theorem 1 as follows:  (Su-S/)d .

= [TZT]  Set k =1, where k records the number of items in itemsets currently being processed.

Find all candidate k-itemsets C, and their counts from the new transactions.

Divide the candidate k-itemsets into three parts according to whether they are large or pre large in the original database.

For each itemset I in the originally large k- itemsets L:, do the following substeps:  Substeps-1: Set the new count S"(I) = S'(Ij+ f(l).

SubstepS-2: If S'(I)/(d+t+c) 2Su,  then assign I as a  large itemset, set f(r> = S"(I) and keep I with p(l).

else, if S"(I)/(d+t+c) 2 SI, then assi n 1 as a pre-large iitemset, set P(0 = s ( I ) and keep I with y{i), otherwise, neglect I .

%  STEP6: For each itemset ir in the originally pre- large itemset p;?. do the following substeps:  Substep6- 1: Substep6-2:  s i t  the new count s"(I) = s'(I)+ f(r>.

If S'(Z)/(d+t+c:) 2 S,, then assign I as a large itemset, $;et s"(0 = S'(1) and keep I with s"(l), else, if S'(I)/(tl+t+c) 2 s,, then assi n I B as a pre-large itemset, set f'(0 = s ( I ) and keep 1 with S"(0, otherwise, neglect I.

STEW: For each itemset I in the candidate itemsets that is not in the originally large itemsets L; or pre-large itemsets &D, do the following substep!;:  Substep7-1: If I is in the large itemsets L; or pre- large itemsets p; from the new transactions, then put it in the rescan- set R, which is used when rescanning in Step 8 is nlecessary.

Substep7-2: If I is small for the new transactions, then do nothing.

STEPS: If t +c I f  or R is null, then do nothing; otherwise, rescari the original database to determine whether the itemsets in the   http://BrightOn.UK   Fourth International CO m e  on knowledge-Based Intelligent Engineering Systems 6 Allied Technologtcs. 3U? A@* sept 2000 Bri hton UK rescan-set &e large or pre-large. and C.Y.Wong, ?Maintenance? of discovered  STEP9: Form candidate (k+l)-itemsets Ck+, from finally large and pre-large k-itemsets (L$!uP,?) that appear in the new transactions.

STEP1O:Set k = k+l.

STEP1l:Repeat STEPS 4 to 10 until no new large or  STEP12:Modify the association rules according to  STEP13:If t +c >j then set d=d+t+c and set c=O;  pre-large itemsets are found.

the modified large itemsets.

otherwise, set c=t+c.

8. Conclusion  In this paper, we have proposed the concept of pre- large itemsets, and designed a novel efficient incremental mining algorithm based on it. Using two user-specified upper and low support thresholds, the pre-large itemsets act as a gap to avoid small itemsets becoming large in the updated database when transactions are newly inserted. The proposed algorithm can effectively handle the case that itemsets are small in an original database but large in newly inserted transactions, although it needs additional storage space to record the pre-large itemsets.

