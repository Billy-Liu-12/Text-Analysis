

Abstract ? Maximizing return on investment is a major focus for all corporations.  Information is seen as the means by which to do so.  This information is used to track performance and to improve the financial results primarily by optimizing the use of company assets.  The ability and speed with which it is possible to gather information and distribute it with current technology to the organization is constantly increasing and, in fact, has surpassed the ability of industry to accept and utilize it.  Today, production operators are overwhelmed with data as a result of an improved ability to monitor assets.  Intelligent motor protection and intelligent instrumentation and condition monitoring systems often provide more than 32 pieces of information per device all with related alarms.  Often operators are not equipped to understand or act on this information.

Production companies need to leverage subject matter expertise for this purpose by locating their engineering staff in regional hubs.  These engineers need to be equipped with sufficient knowledge to be able to interpret and take appropriate action to deal with warnings and alarms generated by these intelligent devices.

The information available can be useful in finding ways to increase production, reduce unplanned maintenance and ultimately reduce down time.  However, finding the information in real time, or getting useful information in real time without significant non-productive time spent making the data useful is a big challenge.

This paper will introduce Cloud Technology as an economical method of gaining visualization and reporting to condition based data.  It will then discuss the use of Cloud Technology to empower engineering resources with live data available in a secure format accessible through web browser technology.  We shall cover the approach to asset optimization and the benefits found in several Cloud Services pilots and projects.

Index Terms ? Asset optimization, cloud technology

I.  INTRODUCTION   When heavy industry firms are viewed on a global basis, world class operations achieve an Overall Equipment Effectiveness (OEE) Score of 91 percent.  Historically, the Oil and Gas industry lags this score by ten points or more (Aberdeen Group ?Operational Risk Management? October 2011).  OEE is the quotient of quality, availability and efficiency scores.  Of these, availability seems to impact the Oil and Gas industry score to the greatest extent.  A deeper  look into root causes of availability scores in Oil and Gas leads to rotating assets as the root cause of failures in 70% of instances of lost production or unplanned downtime. Given this the industry struggles with failure of critical assets, but there are ways to help drive effectiveness scores higher in order to achieve operational efficiency objectives.

Pursuit of offshore reserves in the next decade will involve complex extraction methods and extensive use of subsea extraction technologies.  Extraction of the next wave of reserves will require complex facilities both on the ocean floor, as well as in the form of more traditional surface production facilities.  Facing the growing complexity of production systems, oil and gas operating companies are struggling to successfully operate these complex facilities with a retiring pool of subject matter expertise.

The industry?s reply to this phenomena is significant use of intelligent devices each delivering useful data.  Intelligent breakers, protective relays, measurement relays, process instrumentation and condition monitoring systems to gather useful information.  With sophisticated motor controllers such as adjustable speed drives (ASD) which have literally hundreds of parameters available to be monitored, it is easy to see that the amount of information which can be captured is staggering.

In fact, managing and interpretation of this information database now becomes the issue.

Organizations struggle to bring the data into process control systems at great cost and then present all of the data to engineers each time an event occurs.  Engineers then spend time mining the data, building spreadsheets and creating useful information out of the pile.  From this they can then start to work on the problem or problems.  Unfortunately, this work requires experience with the asset and with the data.

Replacing industry and organizational knowledge is a challenge driving production companies to explore locating subject matter expertise in hubs where resources are leveraged across several assets.  The challenge comes about in two areas.  First, the production systems ? both on the surface and on the sea floor ? require complex production systems and subsystems with many components.  As a result, the automation systems that operate these facilities are extremely complex, often requiring operators to interface with more than 200,000 tags of data and resulting alarms.

Second, placing subject matter experts in regional hubs creates security risks that must be addressed in the design of automation systems.  Enabling 24-hour live-data access for remotely located subject matter expertise requires establishing tunnels in internet technology to empower monitoring from remote offices, tablet PCs and mobile       devices.  A sound security strategy is required as a part of this enablement.



II.  FINANCIAL IMPACT ? WHERE COMPANY CFO?S BELIEVE OPERATIONS SHOULD LOOK FOR EXCELLENCE  In his 2011 report titled ?Operations Risk Management,? Nuris Ismael of the Aberdeen Group cites his findings from polls of the CFO?s of many industries.  According to the study, the top issue facing operators today is the need to maximize return on assets (Fig. 1).

Fig 1. Pressure on Operators   This report also reveals that CFOs of Oil and Gas  enterprises are concerned with reliability of assets and lifecycle costs.  Failure of assets was the risk cited as having the greatest potential impact on achieving operational excellence (Fig. 2).

Fig 2. Risks Having the Biggest Impact  Oil and Gas-focused companies desire higher efficiency.

As facilities grow more complex, the pressures to capture return on investments increase and the need to achieve excellence grows.  However, operation of production systems continues to create sources of risk by impacting asset reliability and creating production downtime.



III.  RISK OF LARGE SYSTEMS FOR COMPLEX PRODUCTION FACILITIES   As stated previously, an engineer?s solution to this  challenge is to measure as much as possible.  Whenever discussions about reliability and availability occur invariably someone asks for more data resulting in more things getting measured in the next project.  Unfortunately, this drives complexity of systems.  Once data exists there is a desire to store it, to share it across disciplines and to mine it when events occur.  The standard solution for this is to stack the system and move all of the data in a Distributed Control System (MAC, ICSS many acronyms exist for this).   When data is required all of the data is presented to anyone asking for it.

Driving operational efficiencies in this approach can be a challenge for many reasons   ? Operations personnel are not completely equipped to deal with the complexity of automation systems and can become overwhelmed with data.

? Traditional approaches of replicating P&ID documents on computer screens create overwhelming operations methods and overwhelming alarming schemes.

? Risks in the form of automation system response times do not align with possible process subsystem interaction times when undesirable events occur.

? Automation systems must be optimized to consider the interaction of subsystems allowing designed hazardous event barriers to work as planned while preventing propagation of unsafe events between subsystems.

? As processing equipment for fluid handling moves to the sea floor in greater frequency, automation processors for the equipment must also move to the sea floor.

The industry will need to manage these complex facilities with a shrinking pool of subject matter expertise.  The risk to operational efficiencies as a result of shared subject matter expertise is driven by the amount of data each facility generates.  How are these resources presented with the facts?  Looking at a simple motor protective relay, there are 32 pieces of information contained in the relay.  Finding the truth is a challenge as not everyone looking at the information contained in the relay needs to see all 32 pieces of information.  In reality a production operator needs to know that the motor is on and healthy.  A maintenance person might need to see efficiency and current data.  Faults need to be presented when events occur. Tools that enable data presentation via Internet access are beneficial, but these tools must present data in a usable format that empowers subject matter expertise to quickly identify where they can provide the most impact.  Additionally, these tools must utilize methods that meet both capital and operating expense budgets.

Figure 3 demonstrates the view of the Oil Industry versus Best in Class operations (Aberdeen Group ?Asset Performance Management Driving Excellence through Reliability Strategies? November 2008).  Oil and Gas CFO?s       believe their DCS systems drive advanced analytics, but these systems fail when it comes to collaboration.  The information presented in the DCS is overwhelming when supplied to people not equipped to deal with it.  The result is lower reliability scores, which contribute to an overall reduction in overall equipment effectiveness (OEE) scores.  Receiving live data empowers people in facilities engineering to help improve asset reliability.

Fig 3. Actions Taken to Improve Reliability

IV.  OPERATIONS PERFORMANCE MANAGEMENT FOR IMPROVED RELIABILITY     Traditional methods continue to impact downtime.  In short, if engineering or maintenance has to build a spreadsheet, hunt down data, manipulate it, and then sort through it to find the truth, the time lost in taking action will lead to increased downtime.  Role-based visualization and reporting, paired with the automation of non-productive work, creates an environment that drives collaboration and improves performance.

Improved visibility with a risk based approach to minimize asset failure is a possible answer to drive asset performance.

It is not complete unless the systems put in place empower Total Productive Maintenance where each person in the enterprise plays a part in maintenance as each is equipped to do.  Implementing Total Productive Maintenance begins with empowering centralized subject matter expertise with live data and giving them the means to collaborate with operations and maintenance personnel in real time.  It involves providing each individual working with the asset the exact information required for their role in the time required to keep the asset healthy.  Senior maintenance and facilities engineering personnel are best positioned to utilize data ? condition-based combined with historical ? to drive reliability improvement in the asset.  Collaboration tools drive the knowledge sharing that in turn create an environment where less experience employees gain from the knowledge of the older employees better equipping the younger engineers, operations and maintenance personnel to optimally engage with the asset as older workers retire.  This combination of data utilization, asset performance management and knowledge sharing leads to an optimized asset through total Operations Performance Management.

The path to Operations Performance Management is one of time and pace.  Creating collaborative zones and systems for analytics and work flow automation can involve significant IT investment.  Resources to execute the systems are often scarce leading to barriers in building the systems that would enable an enterprise to move to Performance Management of operations.

Today, technology exists to empower workflow automation and collaboration using simple Web tools.  Simple browser access allows personnel collaboration using smart phones, tablet PCs and personal computers.  The question is, does an enterprise build the technology within their firewalls or does it take advantage of cloud computing technology to drive collaboration as a paid service?

The answer requires a broader view of data usage and data strategies.  To achieve reliability and efficiency targets leveraging available data, Oil and Gas operating companies must develop data strategies that address a number of considerations.  Is the data required for collaboration also governed by regulatory or investor expectations?  If that is the case, an enterprise should likely consider holding the data within their corporate structure and investing in infrastructure to store and present it.  Capital budgets and IT resources can limit the ability to build the systems required to achieve the desired collaboration ? challenging the pursuit of reliability and efficiency targets.  Purchasing the applications as a cloud- based service can speed up the path to collaboration with a pay-as-you-go model.



V.  THE EMERGENCE OF CLOUD COMPUTING  Cloud computing is rising technology that has quickly appeared on the horizon of Chief Information Officers (CIO) across many industries.  Fig 4 from Gartner?s Technology Update of 2011 indicates cloud computing as technology that today is high on the priority of CIO?s but was not on their horizon five years ago.

Fig 4. Gartner?s Survey of CIO?s on Important Technologies  Fig. 5 provides a view of the key components of cloud computing. On the left side of this chart, the classic model of software installed on enterprise-owned infrastructure is demonstrated ? infrastructure is owned, operating systems are owned and applications are developed to provide user access and interface to data. With each subsequent column moving left to right, less infrastructure is owned by the enterprise interacting with the data. One example that follows       in this paper has been developed using Microsoft?s Azure cloud computing infrastructure.

Cloud technology provides a vehicle to manipulate and manage data.  Purchasing software as a service enables an organization to move faster into the realm of operational excellence through better utilization of existing data.

Purchasing operations management tools as a service avoids using capital budgets and specifically circumvents issues of funding across assets.  Services are paid for as they are used with operating budgets.   Utilizing cloud computing to share operating conditional data creates an environment for operational improvement. ?Private clouds? leveraging the same technologies can be deployed if an enterprise has considerations that require all of the data to remain within their enterprise (regulatory compliance, investor compliance, etc.) In developing strategies that utilize data to drive collaboration between disciplines, an enterprise needs to develop a data strategy.  Some of the data such as, safety records, testing records and flow records are regulated and as such have mandated storage.  Other types of data have investor compliance storage and security requirements.  In the case of investor or regulatory data an enterprise still desires collaborative tools but likely needs to hold the data within its company structure.    Looking at Figure 5 again, collaborative tools and dash boarding are also available in traditional methods of a capital project, purchased software and application development operating within the enterprise data structures.  The next two sections of this paper will address examples of a traditional infrastructure approach and a cloud computing approach used to drive effectiveness in an organization.

Fig. 5 Cloud Computing Taxonomy

VI.  AUTOMATING WORKFLOW WITHIN THE FIREWALL    Cloud technology enables the manipulation of data and automation of non-productive tasks within standard server architectures owned and operated by the enterprise utilizing the data. Hypervisor environments allow multiple applications to run on traditional cluster technology in data centers.

Applications exist which permit the accumulation of data from disparate sources with federation then utilized to create role based visualization and automation of manual tasks (e.g.

production allocation, well test verification, regulatory system testing documentation etc.).  Figure 6 demonstrates a Well Test validation accomplished utilizing cloud technology within an operating company?s firewall.  Figure 7 then demonstrates the role view for an operator resulting. Technology helps to significantly reduce non-productive operations while presenting the data in a way that drives personnel to focus on areas where they can impact the business to improve efficiencies. Figures 9 and 10 show an example of workflow automation and the resulting dashboard presented to personnel containing role-rich information designed to help improve the effectiveness of the individuals working on the asset.

Fig. 6 Well Test Work Flow       Fig. 7 Well Data presented to Operator        This is an example of over 65 workflows automated on an FPSO off the coast of Nigeria where the operator claimed significant benefit from implementation of tools to help drive efficiencies. In SPE 127691 the operator claimed 50,000 bbl per day benefit from improved collaboration. In this example, the operating company invested in infrastructure and application packages. All data remains within the company firewall.



VII.  MOVING APPLICATIONS TO THE CLOUD    Automation systems can require both significant capital investments to build as well as ongoing operating expenses to maintain the infrastructure.  In many cases, collaboration is desired, but securing the investment is difficult due to organizational barriers in funding capital projects across assets or simply because the resources do not exist.

Leveraging cloud computing foundations, applications can be built within the cloud infrastructure and provided as a service.

Collaborating in this fashion is possible within constraints of operating budgets using a fund-as-built model.  In most cases, the cost of the tools required to gain efficiency benefits are significantly lower than the cost of building traditional systems.

The following example of a Dallas-based manufacturer of pumping systems used in mud-handling systems and fracturing systems for drilling of non-conventional wells will be used to illustrate. We will call the company XYZ.  Customers of XYZ send trucks into the field where they provide services to oil and gas enterprises developing oil and gas fields in land- based operations. The pumping systems have a high degree of local automation, but pump operations personnel are not equipped to make decisions regarding when to stop pumping and perform maintenance. For example, the trucks require regular maintenance, and some of the filters require replacement as often as every eight hours. Combine the need for regular maintenance with customers looking over the operator?s shoulders, and occasional mistakes are made with valve positioning and/or running the truck until it breaks in an attempt to please an anxious customer.  This drives significant maintenance costs, and downtime is a big problem.

XYZ sought to empower the management of their customers to help ease this problem by raising the supervisory horizon through Web access to critical truck data.  They also looked to ease their customers? supply chain challenges by empowering the purchase of consumables for the systems with the same tool.  Utilizing cloud technology, a set of work flow rules were developed as an application in Microsoft?s Azure environment.

The Windows Azure environment provides a cloud computing infrastructure hosted in Microsoft?s data centers. It runs Microsoft?s fabric layer and is hosted in a cluster at Microsoft's datacenters that manages computing and storage resources of the computers. The data center provisions the resources (or a subset of them) to applications running on top of Windows Azure. The system developed is shown in Fig. 8. A cloud gateway was developed that takes data from a programmable logic controller and transfers it to the cloud.  As communications in this industry are often unreliable, the gateway was developed with the necessary ?handshaking? and ?store and forward? features to provide data integrity and accurate data flow to the cloud when communications were  available.  In this example a private cellular network served as a tunnel used to permit up to 4000 systems to interact with the cloud.

Fig. 8 XYZ Cloud Architecture     Applications and workflow were then developed to present critical pump information to XYZ?s customers over their existing technology to interact using simple browser technology.  The workflows indicated the location of the truck, maintenance conditions, truck operating conditions and fracturing batch campaign data for the given operation. Fig. 9 and 10 are examples of these dashboards provided.

Fig. 9 Pump Detail           Fig.10 Summary Page  XYZ?s customers benefited significantly from the solution. In the first 90 days of operation, remote supervision eliminated four unplanned trips to the field and prevented one major breakdown ? remote expertise identified a problem and was able to fix it prior to catastrophic failure of the equipment.



VIII.   TURBO-EXPANDER APPLICATION  Another example of utilizing the cloud is a turbo-  expander application at Genalta which is located at Judy Creek, Alberta, Canada.  This application was driven by the customer, a machine OEM that provides turbo- expander machines for the Oil & Gas industry. The arrangement between the supplier and the end user is that OEM provides their equipment as well as entering into a performance contract with the end user for 5 to 10 years, Once a baseline has been established, performance will be monitored (i.e. power generation, etc.) to determine the bonus the OEM is to receive.

Cloud dashboards were created for the purpose of monitoring performance criteria, alarm notifications and other key operating parameters so as to manage the machine life cycle with the objective being to increase revenue by minimizing abnormal events and preventing failures and downtime. In order to provide a cloud infrastructure which met their needs, the operating system was built around Microsoft Windows Azure.

Fig.11 Turbo-expander Application  This system is currently running and collecting data from an on premise historian server.  See Fig 11 for an example of a screen shot used in the application.



IX.  CUSTOMER SUPPORT   An evolution of the technology and approach used in  the previous 2 examples is an innovative way to provide customer support.  As mentioned earlier in the paper, one of the problems faced today is limited subject matter expertise, the location of these resources and the speed in which it can be brought into play to resolve issues should they arise.  Using cloud services, it is now possible to bring the necessary data in a desired and readily understandable format to these technical resources in remote locations.  These resources can be either internal to the organization or external.  In this case, leveraging the support group dedicated to a medium voltage drive which was the key asset at site.

While this was done specifically for this application, the value of this approach to increase productivity and minimize downtime has led to this approach being at implemented at numerous other sites as well.  This has led to an optional support program available to all end users which is known as Virtual Support Engineer (VSE).

The amount of data gathered and the degree of support which can be provided is scalable allowing with the level and degree of external involvement to be determined by the end user.  The support provided can either be reactive (involving troubleshooting and diagnostics) or proactive where a greater level of surveillance and management of the asset are covered as well.

.

Fig.12 Surveillance and monitoring        For example, remote monitoring can be as simple as tracking  alarms and faults, abnormal conditions such as high temperatures or taken to monitoring performance and trends through historical dashboards.  The assets and the degree to which they can be monitored is only limited by the capability of the intelligent device involved.



X.  FURTHER ITEMS RELATED TO THE CLOUD   The use of cloud services will potentially bring further benefits to end users.  The use of the cloud will be beneficial to all users but most particularly smaller users in terms of disaster recovery since critical data can now be stored off site with minimal investment being required.  This avoids major expenditures such as data centers previously required to back up critical data, costs which are particularly prohibitive for smaller organizations.  More data can be safeguarded rather than having companies backup the bare minimum amount of data gambling that it is sufficient to minimize the impact to their organization. The cloud makes it possible for anyone to have a disaster recovery solution.

Going further with the recovery concept, storing data obtained from the operating equipment provides an alternative backup means for information stored on the device.  An example of this situation is that in the event of a major failure involving damage to the operating equipment, the damage may be so severe as to damage the equipment so that information which would be beneficial in troubleshooting the problem may be lost on the device itself.  A parallel record of key parameters to that recorded on the asset itself can be maintained in the cloud providing a backup to this information.

One of the challenges pertaining to the use of the cloud relates to security.  The IT department for many organizations will have concerns and details to work out related to insuring the security of their information and their assets when information is taken beyond their firewall.  To address this concern, there are measures which can be taken including limiting the exchange of data to read only in order to prevent any possible external influence which would affect the operation of the assets involved.

As mentioned earlier, for information to be beneficial, it needs to be sorted and formatted in such a manner as to make it user friendly and readily utilized by the organization.  Ultimately, this information will be seen by upper management who will base decisions on this input and recommendations which will be put forwarded based on it so accuracy is important to all parties including the manufacturer of the assets themselves.  The creation of dashboards and other graphic interfaces to interpret this data will lead to more consistent evaluation as it removes the imperfect element of human translation.  A good example of this relates to large adjustable speed drives which have sophisticated protection capability which permits the equipment to take protective action not only in those cases where it may be a problem with the ASD itself but also in cases where external factors such as air conditioning problems (needed to cool the drive) which may limit the operating range and cause what are what are perceived as nuisance alarms and trips, operator error where the equipment is utilized incorrectly which can lead to trips such as cases where the equipment may be overloaded due to an incorrect sequence of operation, etc.  The latter cases are often left as being drive trips which can lead to the  interpretation that the ASD itself has a reliability issue when in fact the better alternatives may be to provide training to the personnel involved or automate the process.

XI.  CONCLUSION   Operational Excellence seems an elusive goal for Oil and Gas operating companies. The foundation for achieving these goals begins with proper planning, design and application of automation systems. Applying the systems in a way that reduces operator fatigue and empowers operators to perform the required tasks is the beginning of the path towards improved system reliability. Driving to world-class performance standards requires moving the supervisory horizon and establishing a collaborative environment that allows subject matter experts, maintenance personnel and operations personnel to collaborate utilizing data available from the asset. Cloud computing should be considered as a means of achieving these goals.

XII.  REFERENCES  [1] ISMAEL, N. Operational Risk Management, The Aberdeen Group, October 2011  [2] Mehul Shah & Matthew Littlefield Asset Performance Management; Driving Excellence through a Reliability Strategy, The Aberdeen Group, November 2008  [3] Sankaran et al SPE 127691, Realizing Value from Implementing i-Field in a Greenfield Deepwater Facility of the Coast of Nigeria, 2010  [4] Vision for an Upstream Referenced Architecture, Microsoft,  [5] Halliburton?s Asset Observer 2011        XIII.  VITA  Antonio Martinez his BS degree in Mechanical  Engineering from in 1981.  Since that time, he has obtained a MS degree in Electrical Engineering from USC in 1989 and a MBA from Pepperdine University in 1995.

He has worked 18 years for the largest municipal electrical and water utility in the US prior to taking his current position at Chevron where he has been for the past 13 years.  Tony has extensive experience in power system support, commissioning, and maintenance and is currently working on a major project in Nigeria.

Eric Fidler received his degree from The Georgia Institute of Technology in Mechanical Engineering.  Since 1983 has worked in various positions of increasing responsibility in the Oil & Gas Industry related to applied automation and electrical control systems.  He has helped customers succeed in the USA, Nigeria, China, Indonesia, Singapore, Malaysia, Thailand, Vietnam and Norway through applying knowledge of systems, controls and work process automation.  Mr. Fidler joined Intech Process Automation in 2012 and serves as President of the global organization.

Richard Paes received his degree in electrical/electronic engineering technology from Conestoga College, in Kitchener, Ontario Canada in 1981.  He currently holds the position of power technical consultant for the Oil & Gas Global Account team residing in Calgary, Alberta. His primary roles include the application of various power products including medium voltage drives. He is very active in IEEE with current positions being chair of the IEEE1566 Large Drive standard and secretary of the Marine subcommittee.  Mr. Paes is a P.L.

(Eng) in the Province of Alberta, CET in the Province of Ontario as well as a registered Project Management Professional (PMP) with the PMI Institute   Janet Flores received her degree from Valparaiso  University in Mechanical Engineering in Indiana, USA. Since then she has been working for Rockwell Automation in their corporate offices in Milwaukee, WI, at their Drive Headquarters location in Mequon, WI, in their Gulf Coast Regional offices in Houston, TX. and now holds a Global Account Manager position in their Global Industry Sales Group.  Ms. Flores is a member of IEEE and IAS and has been the Registration Chairperson for IEEE PCIC in Houston during the 1990 and 2003 conferences and on the local committee for IEEE PCIC 2010. She is past Chair for the Refining Subcommittee of PCIC, currently Chair of the Electrochemical Subcommittee of PCIC and serves as IAS Liason for IEEE Women In Engineering (WIE).

