Empirical Co-occurrence Rate Networks For Sequence Labeling

Abstract?Structured prediction has wide applications in many areas. Powerful and popular models for structured pre- diction have been developed. Despite the successes, they suffer from some known problems: (i) Hidden Markov models are generative models which suffer from the mismatch problem. Also it is difficult to incorporate overlapping, non-independent features into a hidden Markov model explicitly. (ii) Conditional Markov models suffer from the label bias problem. (iii) Conditional Random Fields (CRFs) overcome the label bias problem by global normalization. But the global normalization of CRFs can be expensive which prevents CRFs from applying to big data.

In this paper, we propose the Empirical Co-occurrence Rate Networks (ECRNs) for sequence labeling. ECRNs are discrim- inative models, so ECRNs overcome the problems of HMMs.

ECRNs are also immune to the label bias problem even though they are locally normalized. To make the estimation of ECRNs as fast as possible, we simply use the empirical distributions as the estimation of parameters. Experiments on two real-world NLP tasks show that ECRNs reduce the training time radically while obtain competitive accuracy to the state-of-the-art models.



I. INTRODUCTION  Structured prediction has many important applications in natural language processing [1], computer vision [2], [3], bioinformatics [4], [5] and other areas. For example, in nat- ural language processing, part-of-speech (POS) tagging [6] is a typical structured prediction task. The input of a POS tagger is a sentence which is treated as a sequence of words and the output is a sequence of POS tags assigned to each word in the sentence. Named entity recognition (NER) [7] is another important application in information extraction which transforms a sequence of words into a sequence of NER tags which identify people, organizations, locations or other named entities. In other applications, the structure of outputs can be more complex than sequences, e.g., for a syntactic parser, the output is a parse tree which is tree-structured.

Structured prediction attracts a lot of research interests and has been studied extensively in NLP and other areas. Many powerful models, such as Hidden Markov Models (HMMs) [8], Conditional Markov Models (CMMs) [9], [10], and Con- ditional Random Fields (CRFs) [11], have been proposed. They are widely applied to practical applications in different areas.

Despite the great successes achieved by these models, they are not flawless. They suffer from some known problems.

Hidden Markov Models are generative models whose objective function at training time is different from the objective function at decoding time. This is known as the mismatch problem [10]. At training time, HMMs optimize a joint probability,  but at decoding time they try to find a sequence of tags which maximizes a conditional probability. Also it is difficult to incorporate overlapping, non-dependent features explicitly into a hidden Markov model. Conditional Markov models were proposed to overcome the drawbacks of hidden Markov models. Conditional Markov Models are discriminative models in which the objective functions are consistent with each other at training and decoding time. But conditional Markov models are affected by the label bias problem [11], [12]. Then Conditional Random Fields were proposed, which avoid the label bias problem. But the training of conditional random fields can be very expensive [13].

In this paper, we propose the Empirical Co-occurrence Rate Networks (ECRNs) for predicting structured outputs.

ECRNs avoid the problems of the existing models. ECRNs are discriminative models. In a discriminative model, the objective functions are consistent at training and decoding time. And also it is easy to craft overlapping, non-independent features into ECRNs explicitly. We also show that ECRNs avoid the label bias problem naturally even though they are locally normalized. To make the training of ECRNs as fast as possible, we simply use the empirical distributions as the estimation of parameters. This results in very efficient training of ECRNs.

Experiments on two real-world datasets show that ECRNs reduce the training time radically while obtain competitive results to the state-of-the-art models.

The rest of this paper is organized as follows. In Section II, we review the existing popular models, such as HMMs, CMMs and CRFs. We also illustrate their known problems.

Section III is devoted to our model: Empirical Co-occurrence Rate Networks (ECRNs). In Section IV, we prove ECRNs do not suffer from the label bias problem by experiments on a simulated dataset. Also in this section we describe experiments on two real-world datasets. Conclusions and future work follow in the last two sections.



II. RELATED WORK  In this section, we review some popular models (HMMs, CMMs, CRFs) for structured prediction. In Section III, we will introduce our model ECRNs. These models differ in some important characteristics, such as conditioning (generative or discriminative), graph structure (directed or undirected) and factorization. Table I summarizes the important characteristics of these models.

A. Hidden Markov Models   DOI 10.1109/ICMLA.2013.23    DOI 10.1109/ICMLA.2013.23    DOI 10.1109/ICMLA.2013.23    DOI 10.1109/ICMLA.2013.23     TABLE I: Characteristics of Models  HMM CMM CRF ECRN Conditioninga Gen. Dis. Dis. Dis.

Normalizationb Loc. Loc. Glo. Loc.

Trainingc Fast Fast Slow Fast Directionalityd Dir. Dir. Un. Un.

LBPe No Yes No No agenerative or discriminative blocal or global cwhether training is fast or slow ddirected or undirected ewhether affected by the label bias problem  Fig. 1: Hidden Markov Models  1) Graph Structure: Figure 1 shows a first order HMM, where S = [s1, s2, ..., sn] is the state sequence (or tag sequence) and O = [o1, o2, ..., on] is the observation sequence (or word sequence). For example, in part-of-speech tagging, S are the POS tags to be predicted and O are the words in a sentence. In graphical models, the graph structure encodes independence relations between nodes. Based on these inde- pendence relations, we can factorize a joint probability into small factors.

2) Factorization: The factorization of directed models is based on the mathematical concept of conditional probability.

HMMs are generative models which factorize a joint proba- bility as follows:  p(S,O) = p(s1) n?  i=1  p(oi|si) n?1?  j=1  p(sj+1|sj). (1)  3) Known Issues: There are two known drawbacks of HMMs [13]. The first drawback is the mismatch problem which stems from their generative nature. At training time, HMMs optimize a joint probability p(S,O), but at decoding time we want a tag sequence which maximizes the conditional probability p(S|O). As P (S,O) = p(S|O)p(O), where p(O) is the distribution of observations, HMMs just pay unnecessary efforts to model p(O). Klein et al. [14] show models with consistent objective functions at training and decoding time perform better than mismatch models. The second drawback is HMMs have difficulty to incorporate overlapping, non- independent features explicitly [10]. This can be observed from the factors p(oi|si) in Equation 1. oi is assumed independent of other observations conditioned by si.

B. Conditional Markov Models  1) Graph Structure: Figure 2 shows a first order CMM.

Maximum entropy markov models (MEMMs) [10] are typical CMMs which train the model using the maximum entropy framework.

Fig. 2: Conditional Markov Models  2) Factorization: CMMs are discriminative models which factorize a conditional joint probability as follows:  p(S|O) = p(s1|O) n?1?  i=1  p(si+1|si, O). (2)  3) The Label Bias Problem: Due to their discrimina- tive nature, CMMs do not suffer from the mismatch prob- lem of HMMs and they can easily craft overlapping, non- independence features explicitly. But CMMs are affected by the Label Bias Problem [6], [11], [12], [15] which stems from the nature of their factorization. The factors p(si+1|si, O) are local conditional probabilities with respect to s. These local conditional probabilities prefer the si with fewer outgoing transitions. The extreme case is when si has only one possible outgoing transition si+1, then p(si+1|si, O) is always 1 no matter what oi+1 is. That is oi+1 is not used for predict- ing si+1. We use the following example to illustrate this problem. Suppose the training dataset consists of 21 training instances including 11 of (rib : XIB), 9 of (rob : YOB) and 1 of (rob : XIB), where {r, o, i, b} are observations and {X, Y, O, I, B} are tags. At test time, we want to predict the tags for the observation sequence (rob). Obviously, the correct tags for (rob) should be (YOB) rather than (XIB). Because there are 9 of (rob : YOB) and only 1 of (rob : XIB). But according to Equation 2, p(YOB|rob) = p(Y|r)p(O|Y, o)p(B|O, b) = 21 ? 1 ? 1 = 921 , which is smaller than p(XIB|rob) = p(X|r)p(I|X, o)p(B|I, b) = 1221 ? 1 ? 1 = 1221 . So CMMs will mislabel (rob) as (XIB). The reason is that (X) has only one outgoing transition (I). This constrains p(I|X, o) to be 1 even though p(I|o) is very small. That is (o) is not used for prediction its tag. The tag of (o) totally depends on the previous tag. From this example, we can see that the local conditional probabilities p(si+1|si, O) cause the label bias problem.

C. Conditional Random Fields  Fig. 3: Conditional Random Fields  1) Graph Structure: Figure 3 shows a linear-chain CRF.

CRFs [11] are discriminative and undirected graphical models.

2) Factorization: The factorization for undirected models are based on the Hemmersley-Clifford Theorem. According to this theorem, a linear-chain CRF can be factorized as follows:  p(S|O) = 1 Z(O)  n?1?  i=1  ?(si, si+1, O) n?  j=1  ?(sj , O),  where Z(O) is the global normalization which ensures? S p(S|O) = 1. ? and ? are non-negative factors defined over  pairwise and unary cliques, respectively. Unlike local models, such as HMM and CMM whose factors are probabilities, the factors of CRFs, ? and ?, have no probabilistic interpreta- tions1. So they cannot be locally normalized.

3) Known Issues: The global normalization makes CRFs unaffected by the Label Bias Problem. A known problem of CRFs is the training of CRFs for large-scale datasets can be slow. On linear-chain CRFs, the time complexity of the standard training method for CRFs is quadratic in the size of the tag set, linear in the number of features and almost quadratic in the size of the training sample [13], [16]. Approximate techniques [17]?[19] have been proposed for reducing the training time, but they are not guaranteed to converge or still take considerable time. Also advanced optimization techniques, such as stochastic gradient descent [20] and average perception [21], have been applied to accel- erate convergence rate. Normally, they reduce the number of iterations, but they cannot reduce the time complexity of one iteration. Also sometimes they oscillate when getting close to the optimum and we need to pre-set the maximum number of iterations to stop the iterative process.



III. EMPIRICAL CO-OCCURRENCE RATE NETWORKS  A. Graph Structure  ECRNs are discriminative, undirected models. A linear- chain ECRN has the same graph structure as Figure 3. But the factorization of ECRNs is different from that of CRFs.

B. Co-occurrence Rate Factorization  Co-occurrence Rate (CR) is the exponential function of Pointwise Mutual Information (PMI) [22]. PMI was first introduced into NLP community by Church and Hanks [23].

It instantiates Mutual Information [24] to specific events and was originally defined between two variables which can be extended to multiple variables [25], [26]. Copula is a similar concept in statistics [27]. To the best of our knowledge, we are the first to apply CR to factorize undirected graphs.

Definition 1 (CR and Conditional CR).

CR(X1; ...;Xn) = p(X1, ..., Xn)  p(X1)...p(Xn) ,  CR(X1; ...;Xn | Y ) = p(X1, ..., Xn | Y ) p(X1 | Y )...p(Xn | Y ) .

According to this definition, if there is only one variable, then CR(X) = p(X)/p(X) = 1. For convenience, CR(?) = 1.

CR is a proper quantity for measuring compatibility: (i) If 0 ?  1Sometimes they are intuitively explained as the compatibility between nodes in cliques. But the notion compatibility has no formal definition.

CR < 1, events occur repulsively; (ii) If CR = 1, events occur independently; (iii) If CR > 1, events occur attractively. We distinguish the following two notations:  CR(X1;X2;X3) = p(X1, X2, X3)  p(X1)p(X2)p(X3) ,  CR(X1;X2X3) = p(X1, X2, X3)  p(X1)p(X2, X3) .

The first denotes CR between three random variables: X1, X2 and X3. By contrast, the second denotes CR between two random variables: X1 and a joint variable X2X3. We have the following two simple but very useful theorems.

Theorem 1 (Partition Theorem).

CR(X1; ..;Xk;Xk+1; ..;Xn)  = CR(X1; ..;Xk)CR(Xk+1; ..;Xn)CR(X1..Xk;Xk+1..Xn).

Proof.

CR(X1; ..;Xk)CR(Xk+1; ..;Xn)CR(X1..Xk;Xk+1..Xn)  = p(X1, .., Xk)?k  i=1 p(Xi)  p(Xk+1, .., Xn)?n j=k+1 p(Xj)  p(X1, .., Xk, Xk+1, .., Xn)  p(X1, .., Xk)p(Xk+1, .., Xn)  = p(X1, .., Xk, Xk+1, .., Xn)?n  i=1 p(Xi)  = CR(X1; ..;Xk;Xk+1; ..;Xn). ?  Theorem 2 (Conditional Independence Theorem). If X ? Y | Z, then we have CR(X;Y Z) = CR(X;Z).

X ? Y | Z means X is independent of Y conditioned by Z.

Proof: X ? Y | Z ? p(X,Y |Z) = p(X|Z)p(Y |Z).

p(X,Y, Z) = p(Z)p(X,Y |Z) = p(Z)p(X|Z)p(Y |Z) = p(Z)  p(X,Z)  p(Z)  p(Y, Z)  p(Z) = p(X,Z)p(Y, Z)/p(Z).

So CR(X;Y Z) = p(X,Y,Z) p(X)p(Y,Z)  = CR(X;Z).

It is easy to prove that Theorem 1 and Theorem 2 also apply to Conditional CR. There are more nice theorems of CR [28].

Even we can obtain the factors of the Hemmersley-Clifford Theorem and Junction Tree by CR (Section 4 and 5 in [28]).

With Theorem 1 and Theorem 2, the linear-chain undi- rected graph in Figure 3 can be factorized as:  p(s1, .., sn|O) = n?  j=2  CR(sj?1; sj |O) n?  i=1  p(si|O). (3)  This factorization can be obtained by CR as follows:     p(s1, .., sn|O) = CR(s1; ..; sn|O) n?  i=1  p(si|O) (4)  = CR(s1|O)CR(s1; s2..sn|O)CR(s2; ..; sn|O) (5) n?  i=1  p(si|O)  = CR(s1; s2|O)CR(s2; ..; sn|O) n?  i=1  p(si|O) (6)  ...

= n?  j=2  CR(sj?1; sj |O) n?  i=1  p(si|O).

Equation 4 is obtained by Definition 1. Equation 5 is based on Theorem 1. We obtain Equation 6 because CR(s1|O) = 1 and s1 ? s2..sn | s2. Following Theorem 2, we have CR(s1; s2..sn|O) = CR(s1; s2|O). By repeating this process we can obtain the final result.

CR, e.g. CR(x; y) = p(x,y)p(x)p(y) , is a symmetric concept which fits the symmetric nature of undirected graphs. By contrast, as we know the factorization of directed graphs (Bayesian networks) are based on the conditional probability. Conditional probability, e.g. p(x|y) = p(x,y)p(y) , is an asymmetric concept which fits the asymmetric nature of directed graphs.

C. Unaffected By The Label Bias Problem  ECRNs avoid the label bias problem due to the nature of their factorization. In Section II-B3, we can see that the label bias problem stems from the local conditional probabilities p(s|s?, O), where s? is the previous tag and s is the current tag. But in the factorization of ECRNs (Equation 3), there is no such local conditional probabili- ties. The factors in Equation 3 are local joint probabilities CR(s?; s|O) = p(s?,s|O)p(s?|O)p(s|O) and unary probabilities p(s|O).

In a local joint probability p(s?, s|O), both o? and o can be used for predicting s. If p(s|o) is very small, p(s?, s|O) is also very small. That is s? cannot dominate the prediction of s. The current observation o also matters. So ECRns avoid the label bias problem naturally. We further check this by the example given in Section II-B3. According to Equa- tion 3, p(YOB|rob) = CR(Y; O|ro)CR(O; B|ob)p(Y|r)p(O|o)p(B|b) =  9/10 9/21?9/10 ? 9/109/10?1 ? 921 ? 910 ? 1 = 910 , which is bigger than p(XIB|rob) = CR(X; I|ro)CR(I; B|ob)p(X|r)p(I|o)p(B|b) =  1/10 12/21?1/10 ? 1/101/10?1 ? 1221 ? 110 ? 1 = 110 . So ECRNs will make the correct prediction YOB for rob. This is confirmed by experiments in Section IV-A.

D. Parameter Estimation  Traditionally, we can use the Maximum Entropy framework (MaxEnt) [10] to train the parameters of a graphical model.

In MaxEnt, the probability is given by an exponential function of features. The constraints of MaxEnt require the expected value of each feature in the estimated distribution be equal to the empirical values in the training dataset. This is equivalent to maximizing some log likelihood function. The maximization can be done by optimization algorithms, such as Limited- memory BFGS.

To make the parameter estimation as fast as possible, also there are some very interesting challenges of applying MaxEnt to CRNs (Section III-D1), we leave MaxEnt train- ing of CRNs as future work. Instead, we simply use the empirical distributions to estimate the parameters. From the factorization of ECRNs (Equation 3), we can see that we need to estimate two kinds of parameters: the unary probability p(si|O) and the CR(sj?1; sj |O). Since CR(sj?1; sj |O) can be calculated through the unary and pairwise probabilities as CR(sj?1; sj |O) = p(sj?1,sj |O)p(sj?1|O)p(sj?1|O) , we can estimate the unary p(si|O) and pairwise probabilities p(sj?1, sj |O) instead of CR(sj?1; sj |O). We simply estimate these probabilities directly by the frequencies of patterns in the training dataset as follows:  p?(si|O) = #(si, oi)? si #(si, oi)  ,  p?(si, si+1|O) = #(si, si+1, oi, oi+1)? sisi+1  #(si, si+1, oi, oi+1) ,  where #(si, oi) is the times (or frequency) of the pattern (si, oi) appears in the training dataset.

At decoding time, we may encounter oi or oi+1 which is a unknown word2. The formulas above cannot be applied to unknown words, because the denominator is equal to 0 due to #(s, oi) is 0 for any unknown word. In this case, we simply use the feature f(oi) to replace the oi itself in the patterns (si, f(oi)) as follows.

p?(si|O) = #(si, f(oi))? si #(si, f(oi))  ,  p?(si, si+1|O) = #(si, si+1, f(oi), f(oi+1))? sisi+1  #(si, si+1, f(oi), f(oi+1)) ,  where oi and oi+1 are unknown words. Since the pattern (si, f(oi)) has been seen in the training data, the denominator cannot be equal to 0. It is important that when we calculate CR using pairwise and unary probabilities, the same features should be used to estimate these three probabilities. That is in CR(s?; s|O) = p(s?,s|O)p(s?|O)p(s|O) , the three factors on right hand side are conditioned by the same features. In other words, CR should be treated as a whole. Otherwise, the accuracy decreases.

1) Challenges of MaxEnt Training of CRNs: In Equation 3, the unary probabilities can be normalized locally, but unfor- tunately the CR factors cannot3. The normalization conditions (or log-partition function in log form) play critical role in esti- mation [29]. They are the constraints to compute the moments of the distribution. Without local normalizations, CR factors can not be directly estimated. A promising way to obtain CR estimations is to train the pairwise and unary probabilities in a CR factors separately using MaxEnt at training time, and at decoding time CR can be calculated by pairwise and unary probabilities.

Another option may be to transform Equation 3 into the following form and maximize the joint probability:  p(s1, .., sn|O) = ?n  j=2 p(sj?1, sj |O) ?n?1  i=2 p(si|O) .

2Unknown words are the words which do not appear in the training dataset.

3Not really cannot. It is very interesting to use the empirical? x,y CR(x; y|O) as the normalization of CR(x; y|O) in future.

Unfortunately, even the unary and pairwise factors can be locally normalized, in our preliminary experiment, this method did not work. We guess the reason is p(s?, s|O) and p(s?|O) or p(s|O) are not independent factors (this is obvious) and the objective function seems not convex. If we maximize the joint probability, p(s?, s|O) is maximized but p(s?|O) or p(s|O) is minimized. Then the estimated moments of these distributions deviate far from the empirical moments and this method failed.

The failure of this method tells us to treat the CR factor as a whole is important because CR factors are independent of unary probabilities. This will be explored further in future.

The Decoder of ECRNs can be efficiently implemented using the traditional Viterbi algorithm.



IV. EXPERIMENTS  We adopt MALLET version 0.4 [30] as the implementation of MEMM, CRF++ version 0.57 [31] as the implementation of standard training of CRFs. ECRNs were implemented by us in Java. For piecewise (PW) [19] training of CRFs, we adopt the MALLET version 2.0 as the implementation.

A. The Label Bias Problem  We test LBP on simulated data following [11]. The sim- ulated data were generate as follows. There are five types of tags: {R1, R2, I, O,B} and four types of observations: {r, i, o, b}. The designated observation for both R1 and R2 is r, for I it is i, for O it is o and for B it is b. We generate the paired sequences from two tag sequences: [R1, I, B] and [R2, O,B]. Each tag emits the designated observation with probability of 29/32 and each of other three observations with probability 1/32. For training, we generate 1000 pairs for each tag sequence, so totally the size of training dataset is 2000. For testing, we generate 250 pairs for each tag sequence, so totally the size of testing dataset is 500. We run the experiment for 10 rounds and report the average per-token accuracy (#CorrectTags#AllTags ) in Table II.

TABLE II: Accuracy For Label Bias Problem  ECRN CRF++ PW MEMMs 95.8 95.9 96.0 66.6  The experimental results show that ECRN, piecewise train- ing (PW) and the standard training (CRF++) of CRFs are all unaffected by the label bias problem. But MEMMs suffer from this problem because its accuracy is significantly lower than other models. This experiments confirm our discussions in Section III-C and Section II-B3.

B. Part-of-speech Tagging Experiment  We use the Brown Corpus [32] for part-of-speech tagging experiments. The raw data were preprocessed by excluding the incomplete sentences which are not ending with a punctuation.

This results in 34,623 sentences. The size of the tag space is 252. Following [11], we introduce features and parameters for each tag-word pair and tag-tag pair. We also use the same spelling features as those used by [11]:  1) (f1) Whether a token begins with a number or upper case letter.

2) (f2) Whether a token contains a hyphen.

3) (f3) Whether a token ends in one of the following  suffixes: -ing, -ogy, -ed, -s, -ly, -ion, -tion, -ity, -ies.

TABLE III: Accuracy On POS Tagging  Overall Known Unknown Time (Sec.) CRF++ 95.4 96.1 71.7 4,571,807 ECRN 95.6 96.9 70.5 3.9  Table III lists the per-token accuracy obtained by CRF++ and ECRN on the part-of-speech tagging dataset. The overall accuracy gives the per-token accuracy obtained by models cov- ering all words including known and unknown words. Known and unknown accuracy show the per-token accuracy only considering known or unknown words. From the experimental results, ECRN is much faster than the traditional training (CRF++) of CRFs, and achieves better results on known words.

Results also show that CRF++ outperforms ECRN on unknown words by 1.2 percent. On the overall accuracy, ECRN and CRF++ perform almost the same well. Because we simply use the empirical distribution to estimate the parameters. The training of ECRN is just by counting the frequencies in the training data without iterative optimization.

C. Named Entity Recognition  We use the the Dutch part of CoNLL-2002 NER Corpus4 as our experimental dataset. There are three files in this corpus: ned.train (13,221) for training, ned.testa (2,305) for development and ned.testb (4,211) for testing.

The size of the tag space is 9. We use the same features as those described in the part-of-speech tagging experiment. The results are listed in Table IV.

TABLE IV: Accuracy On NER  Overall Known Unknown Time (Sec.) CRF++ 96.13 98.2 77.4 794 ECRN 96.23 98.8 73.7 1.3  On this dataset, ECRN obtains better results on overall and known word accuracy. But on unknown words, CRF++ perfomes significantly better than ECRN. This is the trade-off between speed and accuracy. The results also show ECRN is much faster than CRF++. The experimental results on NER dataset are consistent with the result on POS tagging dataset.



V. CONCLUSION  The existing models for structured prediction have their own drawbacks. We proposed the empirical Co-occurrence Rate Networks (ECRNs) for predicting structured outputs.

ECRNs avoid the problems with the existing models. ECRNs are discriminative, local models which are unaffected by the label bias problem. ECRNs can be trained very fast by simply using the empirical distribution to estimate the parameters.

Experiments on two real-world NLP datasets show that ECRNs speeds up the training radically and obtains competitive results to state-of-the-art models. ECRNs can be very useful for practitioners on big data.

4http://www.cnts.ua.ac.be/conll2002/ner/

VI. FUTURE WORK  As discussed in Section III-D1, MaxEnt training of CRNs is very interesting. Also in this paper, we did not try global features for training ECRN. Even Equation 3 shows CRNs can be conditioned by global features (the big O), we still need experimental evidence to support this. More comprehensive comparisons between models will be done including statistical tests. Moreover, we focus on linear-chain graphs in this paper.

CR factorization can also be applied to tree-structured and cyclic graphs [28]. We will explore this direction. We will also study other important models for structured prediction, such as structured SVMs [33], [34] which minimize large-margin risks and apply factorization to kernel representations (ker- nel decomposition), exemplar-based methods [35], constrained conditional models [36] and so on. It is very interesting to apply CR factorization to kernel representations and minimize large-margin risks.

