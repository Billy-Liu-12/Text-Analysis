Research of Data Mining Based on Apriori algorithm  in Cutting Database

Abstract?Cutting data mining is an important method to increase efficiency discover hidden knowledges in cutting database, and provide guidance for cutting decisions.This paper analyze the Apriori algorithm for association rules mining, and make some improvement for this algorithm based on the features of cutting database. Apriori algorithm is improved to mine association rules in cutting database. The results show that the Apriori algorithm can be efficiently used in cutting data mining, and improved algorithm can achieve expected effect better than traditional algorithm.

Keywords-data mining; association rules mining; cutting database; Apriori algorithm

I.  INTRODUCTION Cutting database app ared after the combination of  machining technique and database technique which can provide effective data information support for machining process. we can get higher machining efficiency and economic efficiency if the reasonable and optimized cutting data provided by cutting database is used [1].However, it is a pity that it can?t derive enough more useful information so as to  provide guidance for actual machining although large amount of cutting parameters, processing knowledges, empirical formulas have been stored.

In this sense, it can only be called static cutting database, which result in ?explosive data, however lack of knowledge?. So it is important to adopt data mining technique for cutting database and discover hidden knowledges from this cutting database. In this paper, on studying the properties of cutting database, Apriori algorithm is improved before using into cutting data mining and the final results shows that some useful knowledge can be get from these large amount of cutting data.



II. ASSOCIATION RULES MINING AND APRIORI ALGORITHM Association rules mining is one of the most important  research methods in data mining which can obtain some useful knowledge to describe the association between different valuable data items out of a great amount of datas. A lot of algorithms about mining association rules have been presented recently, among which Apriori algorithm is one of typical algorithm. It was introduced in 1993 by Agrawal as a powerful method used to find regularities in data trend [2]. Apriori algorithm is a width-first search arithmetic in which recursive method was adopted. A basic property of ?every subset of a frequent item sets is still  frequent item set, and every superset of a non-frequent item set is not a frequent item set [3]? has  been used in Apriori algorithm to discover all of the frequent item sets. Details and formulas are given as follows:  C1={candidate 1-itemsets};  L1={c C1/c.count>=minimum support};  For (k=2; Lk-1 ; k++) {  Ck=sc_candidate(Lk-1);  For each transaction t D {  Ct=count_support(Ck,t);  For all candidates c Ct {c.count++}  }  Lk={c Ck/c.count>=minimum support};  }  Return L=UkLk;  The notations and definitions are given as follows:  1) D=set of transactions; each transaction t is included in D;  2) Lk= set of large k-item sets(set of items having minimum support);  3) Ck= set of candidate k-item sets(items to be counted);  4) Function of  sc_candidate is aimed at using Lk-1 to get Ck;  5) Function of count_support is aimed at using Ck and t to get all of the candidate k-itemsets that included in t.

The Apriori algorithm finds only the frequent item sets. In order to find the associations rules in the database , we must apply the following improved algorithm [4],that is:  For each frequent item set Lk {  Generate all non-empty subsets of  Lk;  For each non-empty subset Ls of Lk  {  Output-rule: Ls Lk if support(Lk)/support(Ls)>=min-confidence}  }  The Above steps can be concluded to the following two points: (1) The support of every frequent item sets should be larger than the minimum support threshold, and (2) The  This paper is sponsored by Chinese National Science Fund (50805100) and Chinese National Science & Technology Supporting Program    confidence of every association rules that derived from frequent item sets should be larger than the minimum confidence threshold.



III. THE APPLICATION OF IMPROVED APRIORI ALGORITHM IN CUTTING DATABASE  Since the Apriori algorithm was proposed, many scholars have carried on a series of researches and applications.

Meanwhile, some improvements have been made to reduce the time cost of scanning the database [5] and save the time for searching frequent item sets [6] which occur when the traditional Apriori algorithm is used. But there are still some problems in the various kinds of applications more or less. As the main target of algorithm Apriori is to find the frequent item sets in which the support and the confidence are larger than the minimum threshold respectively. If the support is too small, many useless rules will be obtained, but if the support is too large, there would be some rules missed such as the rules with high confidence and not large enough support. In the cutting database, the support of cutting data for difficult-to-machine material and high-speed cutting is always not very large, but in practice, the rules of that kind is requisite due to a lack of experience. In that case, the minimum support and the minimum confidence need to be set according to various circumstances. The method of this paper is to set the minimum support, the maximum support, the minimum confidence, the maximum confidence based on the principle so that no important rules can be missed and the useless rules can be reduced according to the limited scale. The steps are listed as follows:  1) Finding out the frequent item sets in which the support is larger than the minimum support.

2) Choosing the frequent item sets primarily which satisfy the  rule (the attribute of judgment in the left and the attribute of the result in the right) as the rough rule.

3) Judging the confidence of the roughing sets. If the  confidence is smaller than the maximum confidence and larger than the minimum confidence, judge the roughing sets and pick out the roughing rule minimum confidence of which is larger than maximum support as the eventual output rule; If the confidence is bigger than the maximum confidence, then the rule is outputted as the eventual rule.

In order to make the process clearly, the flow chart of improved algorithm is showed in Fig. 1.



IV. DATA MINING EXAMPLE OF CUTTING DATABASE  A. Data Sources Parts of milling data are stored in this database, some of  which are from experimental data [7], and the others are from the usual cutting data handbook of metal material written by companies such as CPMEC[8].

B. Selection of Mining Data Attributes and its Discretization Cutting conditions such as cutting speed Vc, feed rate f and  cutting depth ap are influenced by milling methods, hardness of work-piece material, heat-resistance temperature of tool  material, cutting precision, machine, cutting coolant, heat treatment conditions of work-piece material, tool diameter and so on. Among the 3 basics of cutting conditions, cutting depth is determined by working allowance and surface quality, which can be neglected during the mining process. Cutting speed and  Start  Find frequent itemsets which suppot>minim  um support  Choose rules which confidence>minim  un confidence  Confidence>maximum confidence?

Output as final rules  Yes  No Compare support to minimum support  Support>maximu m support  Yes   Figure 1.  Flow chart of improved algorithm  feed rate are onditions in  C. Data Mining are exported from cutting database as XLS  file,  be included in the final results. Oppositely, if a lower minimum  key parameters to evaluate cutting c machining center. In this paper, cutting precision, work-piece material and the other 4 influenced factors are selected as regular conditioning attribute, while cutting speed and feed rate are selected as regular determined attribute. In order to extract the regular conveniently, continuous data should be discretized before data mining. Its dividing range and discretized result are shown in TABL . Additionally, the value of each attribute is reflected 1 to 44 respectively so as to facilitate compiling of mining program.

Partial datas then 33 data groups are intercepted from it. In order to  mine by MATLAB, all the datas have to be discretized and reflected to 1 to 44 as TABL . The selection of the minimum support threshold and the minimum confidence threshold play an important role in whole process of mining. If just one minimum support threshold as 0.1 and one maximum confidence as 0.6 thresholds are defined as traditional algorithm, we can get the result as TABL . As shown in TABL , although the rules with high support can be got from the mining results, some important rules would be missed at the same time. For example, the support of frequent item set for stainless steel in mining data is not high enough, but the association rule got from it can reach 100%, which also has to    TABLE I. DECARTELIZATION  TABLE  FOR DATA  PROPERTIES  Machining Classification of Cutting Sp )  Feed Ra /z)Precision Work piece Material  Classification of Milling Diameter of Hardness of Work Cutter Material Method Cutter(mm) piece(HBS) eed(m/min te(mm  p ) s1( l) 1(Roughing Carbon stee c1(Tool steel) x1(face milling) d1(0-4) h1(125-175) v1(20-25) f1(0-0.02)  p2(Semi- finishing) s2(Alloy steel)  c2(High speed steel)  x2 l f(Peripheramilling) d2(4-8) h2(175-225) v2(25-30) 2(0.02-0.04)  p ) s  3(Finishing 3(Stainless steel) c3(Carbide)  d3(8-12) h3(225-275) v3(30-35) f3(0.04-0.06)  s4(Cast steel) c4(Ceramic)  d4(12-16) h4(275-325) v4(35-40) f4(0.06-0.08)  s5(Aluminum alloy) c5(Diamond)  d5(16-20) h5(325 above) v5(40-100) f5(0.08-0.10)  S6(Ti ) tanium alloy   d6(20-30)  v6(100-150) f6(0.1-0.15)  d7(30-40)  v7(150 above) f7(0 15 .above)  d8(40-50)  d  9(50 above)  TABLE II. MINING  WITH  TRADITIONAL  APRIORI  ALGORITHM  Number Rules Support confidence  1 p1 & s2 & & d4 f5 c2 & x1 18.18% 100%  2 p1 & s2 & c2 & x1 & d5 f5 18.18% 100%  3 p1 & s2 & c2 & x1 & d7 f6 18.18% 85.71%  4 p1 & s2 & c2 & x1 & h1 f6 12.12% 60%  5 p1 & s2 & c2 & x1 & h2 f6 12.12% 60%  6 p1 6  & s2 & c2 & x1 & d6 & h1 f 15.15% 100%  7 p  1 & s2 & c2 & x1 & d6 & h3 v1 &  f6  12.12% 100%    pport threshold is chosen, a lot of useless rules will derive.

TABLE III. MINING  WITH  IMPROVED  APRIORI  ALGORITHM  D. Result Analysis TABL that the results got from the  impr  ses, the maximum feed rate for  m

