AFOPT Algorithm for Multi-level Databases

Abstract  With the widespread computerization in business, government, and science, the efficient and effective discovery of interesting information from large databases becomes essential. Previous studies on data mining have been focused on the discovery of knowledge at single conceptual level, either at the primitive level or at a rather high conceptual level.

This paper presents an algorithm based on the AFOPT algorithm for multi-level databases that uses the benefits of multileveled databases, by using the information gained by studying items from one concept level for the study of the items from the following concept levels.

1. Introduction  Finding frequent itemsets and building accurate and efficient classifiers for large databases is one of the essential tasks of data mining and machine learning research.

The aim of data mining is the discovery of patterns within data stored in databases. Mining for association rules is a data mining method that lends itself to formulating conditional statements such as ?if customers buy product x then they also buy product y?.

Extensive efforts have been devoted to developing efficient algorithms for mining frequent patterns.

Following the pioneering work [4], a couple of algorithms adopting the candidate generate-and-test approach are proposed. Apriori algorithm [3] is the first algorithm which uses the Apriori property to prune the search space.

Some researchers have noticed the long pattern problem and suggested mining only frequent closed patterns [6],[5] or only maximal frequent patterns [1]:  -A frequent pattern is closed if all of its proper supersets are less frequent than it.

-A frequent pattern is maximal if none of its proper superset is frequent.

Most of the previous studies use an Apriori-like approach, which is based on an anti-monotone Apriori heuristic [2]: if any length k pattern is not frequent in the database, its length (k +1) super-pattern can not be frequent. The Apriori heuristic achieves good performance gain by reducing the size of candidate sets. However, in situations with prolific frequent patterns, long patterns, or quite low minimum support thresholds, an Apriori-like algorithm may still suffer because it is costly to handle a huge number of candidate sets and it is tedious to repeatedly scan the database and check a large set of candidates by pattern matching, especially true for mining long patterns.

AFOPT?s efficiency of mining is achieved with three techniques [5]: (1) a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans, (2) FP-tree-based mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets (3) a partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for mining confined patterns in conditional databases, which dramatically reduces the search space.

It can be stated as follows: Let I = {i1, i2, ?. , in} be a set of items and D = {t1, t2, . . . , tN } be a transaction database, where ti (i  [1, N]) is a transaction and ti

I. Every subset of I is called an itemset. If an itemset contains k items, then it is called a k-itemset. The support of an itemset l in D is defined as the percentage of transactions in D containing l. If the support of an itemset exceeds a user-specified minimum support threshold, then the itemset is called a frequent itemset.

2. Multi-level Databases  With widespread applications of computers and automated data collection tools, massive amount of  Proceedings of the Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC?05)    transaction data have been collected and stored in databases. In this paper, we first introduce the conceptual hierarchy, a hierarchical organization of the data in databases. The problem of finding single level rules in databases has been studied extensively in [3].

Mining knowledge at multiple levels is both practical and desirable, and thus is an interesting research direction.  Studies on mining association rules have evolved from techniques for discovery of functional dependencies, strong rules, classification rules, etc. to disk based efficient methods for mining association rules in large sets of transaction data.

However, previous work has been focused on mining association rules at single conceptual level. There are applications that need to find associations at multiple conceptual levels. For example, besides finding that 70% of customers that purchase milk may also purchase bread, it could be informative to also show that 65% of customers that purchase wheat bread may also purchase 2% milk.

Multilevel databases use hierarchy-information encoded transaction table, in the transaction table each item is encoded as a sequence of digits (Table 1).

Figure 1. Hierarchy-based multi-level database Table 1, contains category codes and a description  for each code (category or item) which is only needed for the final display. The codes and the descriptions for Table 1 are extracted from Figure 1.

Table 1. Multi-level items Code Description  0 All Electronics 1 Computer 2 Printers 3 Scanners 4 Monitors 5 Keyboards  1.1 Desktop 1.2 Laptop 2.1 Compaq 2.2 HP 3.1 Mustek 4.1 Philips 5.1 Tech  1.1.1 Athlon 1.1.2 Pentium  We assume that a multi-level database contains: a) an item data set which contains the  description of each item in I in the form of (ij; description), where ij is the  product?s  code.

b) a transaction data set, defined as above D. A transaction is in the format of (TID, {ij}); Where TID is a transaction identifier and ij I is an item from the item data set.

Table 2. Transactions from database TID Items T1 3.1.1     1.2.2     2.2.3     2.1.1    1.1.4   1.1.2 T2 2.1.2     4.1.1     1.2.2    1.1.2     3.1.1 T3 2.1.2     4.1.2     5.1.3    1.1.2 T4 1.2.3     1.1.2     3.1.1    4.1.1 T5 1.1.2     3.1.1     1.2.3    1.2.2  3. AFOPT for Multi-level Databases  To get the information needed for the k concept level there are 2 methods:  1. directly use AFOPT on the requested concept level [4]  2. if information from a level lower than k is available, information for the k level can be obtained from l-level (l>k) AFOPT without scanning the database, or by scanning just a few transactions  The algorithm is obtained by extending the AFOPT algorithm for multi-level databases. By using the AFOPT algorithm no candidate sets are being generated, thus the top-down progressive method doesn?t really help. The information gained at a high concept level is insufficient for the following concept levels. Thus, a new scan of the database is required to get the missing information.

This way, to get information from the lowest concept level will require at least n+1 scans of the database. In conclusion, this method is inefficient when using pattern algorithms. Still, when getting information from the lowest level, the AFOPT algorithm, directly used on the last concept level, is faster than candidate generation and testing algorithms for multi-level.

Like in AFOPT, this algorithm first traverses the original database to find frequent items or frequent abstract levels. These are sorted in ascending order and sort them in ascending frequency order.

Then the original database is scanned the second time to construct an AFOPT structure to represent the conditional databases of the frequent items.  First, a header is constructed, using only the items that pass the minimum support threshold, and which are ordered in an ascending order, unlike in the FP-Growth algorithm where the items are in descending frequency order.

Proceedings of the Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC?05)    Table 3. Items Table 4. Frequent items  The conditional database of an item i includes all the transactions containing item i, with infrequent items and those items before i removed from each transaction. Arrays are used to store single branches in the AFOPT structure to save space and construction cost. Each node in the AFOPT structure contains three pieces of information: an item id, the count of the itemset corresponding to the path from the root to the node, and the pointers pointing to the children of the node.

Figure 2. AFOP tree.

In step 1 and step 2, the counts for each individual  item is gathered; and the complete information about the transaction database is compressed in the AFOPT structure. They are realized at the cost of two database scans. From now on, the pattern generation process is started by recursively visiting the AFOPT structure.

Note that no more costly database operations will be involved.

This method allows us to obtain information for level k when we have already obtained the results for level l which is a lower concept level than k.

The FP-Tree obtained for level l contains almost all information to build the FP-Tree required for level k, the only information missing is the one from the items that did not meet minimum support. These items may affect the k-level FP-Tree.

Step 1. Create New k-level Header:  The new k-level FP-Tree needs a header which has k-level items.  We can obtain it using information from section 2. This header is partial and may be completed at step 2.

Figure 3. Constructing k-header Step 2. Complete the headers:  We need to complete the l-header we already have, with the missing items. To do this, we have to read the missing nodes to the header, which is quite easy if the header was saved before the pruning step, or the items were just hidden instead of being removed.

Table 5&6. Completing headers  For each added item to the l-level header we add the corresponding item to the k-level header, and get its support. If, in the k-level it doesn?t meet minimum support, it is pruned; its corresponding items from the l-header are also pruned (even though it was just added). After this step we have the 2 headers from the k and l levels.

Step 3. Complete l-level tree: We also have to read to the tree the transactions that contain the added nodes. This means accessing the database, however only a few transactions are needed, as opposed to 2 full scans of D when using method I.

Table 7: Re-Added transactions TID Items T1 2.1.1    1.1.4     1.2.2    3.1.1    1.1.2 T2 Not re-added T3 4.1.2     2.1.2    1.1.2 T4 Not re-added T5 Not re-added  In Figure 5, notice that item 5.1.3 and 2.2.2 were not added (or were added but removed) because their corresponding k-level items (item 5.1 and 2.2) do not meet minimum support. Also, the transactions which contained these items were not added to the tree.

Item Sup 3.1.1 4 1.2.2 3 2.2.3 1 2.1.1 1 1.1.4 1 1.1.2 5 2.1.2 2 4.1.1 2 4.1.2 1 5.1.3 1 1.2.3 2  Item Sup 4.1.1 2 2.1.2 2 1.2.3 2 1.2.2 3 3.1.1 4 1.1.2 5 2.1.1 1 2.2.3 1 4.1.2 1 1.1.4 1 5.1.3 1  l-Level Sup 2.1.1 1 2.2.3 1 4.1.2 1 1.1.4 1 5.1.3 1  --------- ------- 4.1.1 2 2.1.2 2 1.2.3 2 1.2.2 3 3.1.1 4 1.1.2 5  k-Level Sup 2.1 3 2.2 1 4.1 3 1.1 5 5.1 1  ------- ------- 4.1 2.1 1.2 1.2 3.1 1.1  Proceedings of the Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC?05)    Figure 5. Completing l-level FP-Tree Step 4. Reorder k-level header:  The k-level tree header was completed at step 2 and contains item IDs corresponding to concept level k.

They might need to be reordered by support. To find the missing support of the items from level k we use a recursive algorithm on the l-level FP-Tree:  Table 8. Ordered k-header k-Level Sup  2.1 3 4.1 3 3.1 4 1.2 4 1.1 5  Starting from the root and going downwards: 1. the support for category 1 (for example?) will  be the support of the first-found item that is part of category 1 (item 1.1 or 1.1.1 etc.)  2. if we get to a crossing (and first item was not found) we apply these steps for each branch and the support will be the sum of the result of each branch  Step 5. Create k-level tree: Starting from each terminal item of the l-level tree  we follow their conditional patterns and extract all the k-level items that appear and their support. If a k-item appears more than once we consider it only once but sum their supports. We create an itemset (see Table 2) which is added to the new k-level tree just like a transaction would be added to an FP-Tree.

In the end a k-level FP-Tree is obtained (Figure 6), from which, by using the FP-Growth algorithm, we can obtain the frequent itemsets from the first concept level.

Figure 6. K-level FP-Tree  Although it is rare in practice, if no items were pruned from the l-level FP-Tree, then steps 1 & 3 are not required, and the database is not accessed at all.

Thus, if we have the FP-Tree for the lowest concept level with no pruned items, then we can obtain information for any concept level without accessing the database. This is useful to know when we need information for more than one concept level  4. Performance Study  Items in the FP-tree are sorted in descending frequency order and these branches are traversed bottom-up, therefore the items in these branches (resulted from the traversal) are sorted in ascending frequency order. If these branches are used to build a prefix-tree, it is exactly the AFOPT structure.

We traverse the AFOPT structure in top-down depth-first order. Each node is visited exactly once.

The total number of node visits of the FP-Growth algorithm is equal to the total length of its branches.

The total number of nodes visit of the AFOPT algorithm is equal to the size of the AFOPT structure, which is smaller than the total length of the nfp branches. Therefore the AFOPT algorithm needs less traversal cost than the FP-Growth algorithm  As a result, if we apply the AFOPT and FP-Growth algorithms directly on level 3 of a database (method 1) to obtain the pattern trees, after the database scanning (which lasts the same for both algorithms), we get the following result (Figure 7).

Figure 7. AFOPT vs. FP-Growth If information from a lower level is available (saved on either a disk or in memory), we can use method 2, and thus, there won?t be any need to scan the database entirely:  It is possible that all the conditional databases of a frequent itemset?s frequent extensions can be represented by a single branch using FP-tree structure, but the AFOPT structure requires multiple branches. In this case, both algorithms do not need to construct new conditional databases, but the AFOPT algorithm needs more traversal cost. However, the possibility that this  Proceedings of the Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC?05)    situation happens is much lower than the possibility that a single conditional database can be represented by a single branch.

Figure 8. AFOPT (method 2) For the AFOPT algorithm, additional traversal cost  is caused by the push right step. If we did not have this step we would get a conditional database which consists of multiple subtrees. The number of subtrees constituting the conditional database is exponential to the number of items before that item in worst case.

While the number of merging operations needed is equal to the number of items before that item in worst case. To save the traversal cost, it is better to perform the merging operation.

The items are sorted in descending frequency order in FP-tree, which improves the possibility of prefix sharing. Hence FP-tree is more compact than the AFOPT structure. Applying the algorithm at section 3 on an AFOPT structure will be about the same as applying the transformation on a FP-Tree, because the same (maximal) frequent patterns are extracted, the only difference being the order (reversed).  On the other hand, as stated earlier, the AFOPT algorithm needs less traversal cost than the FP-growth algorithm.

Thus, the use of the AFOPT algorithm is more recommended.

Experiments were conducted on a 1.6 Ghz Athlon XP with 256 MB memory running Microsoft Windows XP Professional using a real database of 50000 entries in 6000 transactions. All codes were done in Java and compiled using the Eclipse Platform.

5. Conclusions  General frequent pattern mining algorithms focus on mining at single level. This way only strong association rules between items will be discovered. For multi-level frequent pattern mining, mining algorithms have to be extended.

Mining knowledge at multiple levels is both practical and desirable and makes possible the usage of the second method.

The importance of multi-level frequent patterns goes far beyond market-basket analysis because they serve as an estimation of joint probabilities of events, and therefore are useful whenever such estimation is required.

The AFOPT algorithm traverses the trees in top- down depth-first order, and the items in the prefix-trees are sorted in ascending frequency order.

The combination of these two methods is more efficient than the combination of the bottom-up traversal strategy and descending frequency order, which is adopted by the FP-Growth algorithm. Both of the above two combinations are much more efficient than the other two combinations ? the combination of the top-down traversal strategy and the descending frequency order, and the combination of the bottom-up traversal strategy and the ascending frequency order.

Using method 2 to obtain a AFOPT tree, only a few items will be added and a full database scan will not be required. Considering the fact that database scanning is slow, this is a great advantage.

6. References  [1] Agarwal, R.C., Aggarwal, C.C., and Prasad, V.V.V., ?Depth first generation of long patterns?, In Proceedings of the 6th ACM SIGKDD Conference,  ACM Press, pp. 108? 116, 2000.

[2] Agrawal R., Mannila H., Srikant R., Toivonen H., and Verkamo A.I., ?Fast discovery of association rules?, In Advances in Knowledge Discovery and Data Mining, pp.

307?328, 1996.

[3] Agrawal R. and Strikant R., ?Fast algorithms for mining association rules?, In Proc. 1994 Int. Conf. Very Large Data Bases (VLDB?95), Santiago, Chile, pp. 487-499, 1994.

[4] Liu, G., Lu, H., Lou, W., Xu, Y. and Xu Yu, J., ?Efficient Mining of Frequent Patterns Using Ascending Frequency Ordered Prefix-Tree?, Data Mining and Knowledge Discovery, 9, 249?274, 2004.

[5]  Liu, G., Lu, H., Lou, W., Xu, Y. and Xu Yu, J., ?Ascending Frequency Ordered Prefix-tree: Efficient Mining of Frequent Patterns? In Proc. of KDD Conf., 2003.

[6] Pasquier, N., Bastide, Y., Taouil, R., and Lakhal, L., ?Discovering frequent closed itemsets for association rules?, In Proceedings of 7th ICDT Conference, Springer, 1999, pp.

398?416.

