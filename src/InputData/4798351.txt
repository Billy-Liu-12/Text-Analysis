2008 IEEE Region 10 Colloquium and the Third ICIIS, Kharagpur, INDIA December 8-10.

Abstract?When data objects that are the subject of analysis using machine learning techniques are described by a large  number of feature (i.e. the data is high dimension) it is often  beneficial to reduce the dimension of the data. Dimensionality  reduction (DR) can be beneficial not only reasons of  computational efficiency but also because it can improve the  accuracy of the analysis. Now we have tried to introduce a novel  transform to achieve dimensionality reduction. This paper  summarizes survey on feature selection and extraction from high-  dimensionality data sets using genetic algorithm. The feature  selection process can be considered a problem of global  combinatorial optimization in machine learning, which reduces  the number of features, removes irrelevant, noisy and redundant  data, to obtain the accuracy and saves the computation time and  simplifies the result. We are trying to develop GA-based  approach utilizing a feedback linkage between feature evaluation  and association rule. That is we carry out feature selection  simultaneously with association rule mining, through ?genetic  learning and evolution.?   Keywords- Feature Selection, Genetic algorithms, Multi-objective Optimization.



I. INTRODUCTION  The growing glut of data in the worlds of science, business and government create an urgent need for a new generation of automated and intelligent tools and techniques which can analysis, summarize and extract ?knowledge? from raw data.

The great challenge for data mining comes from huge database of noisy, high dimensionality data.

Data mining algorithms like ?Association Rule Mining? (ARM) [1, 2] perform an exhaustive search to find all rules satisfying some constraints. Hence, the number of discovered rules from database can be very large. Based on the earlier works, it is clear that it is difficult to identify the most effective rule. Therefore, in many applications, the size of the dataset is so large that learning might not work well before removing the unwanted features. Dimensionality reduction [3, 4, 5] attempts to remove irrelevant features according to two basic criteria: (i) the accuracy does not significantly decrease and (ii) the resulting concept, given only the values for the  selected features, is as close as possible to the original concept, given all the features. The dimensionality reduction methods find the best feature subset in terms of some evaluating function among the possible 2N (where N is the number of features) subsets.

One of the main challenges in association rule mining is to identifying frequent itemsets in very large transaction databases that comprise millions of transactions and items.

Consequently, some recent research has focused on designing efficient algorithm for this purpose (Toivonen 1996; Webb 2000). The main limitation of these approaches, however, is that multiple passes over the database are requires.

Feature selection (FS) [6] involves the production of a new set of features from the original features in the data, through the application of some mapping. The feature selection process can be considered a problem of global combinational optimization in machine learning which is used to select a ?better? subset that can be describe data from an original dataset. Feature selection algorithms take an alternative approach to dimensionality reduction by locating the ?best? minimum subset of the original features, rather than transforming the data to an entirely new set of dimensions.

Classical data mining algorithms that require one or more computationally intensive passes over the entire database can be prohibitively slow and this problem will only become worse in future. We have to focus on identifying relevant data that can help to reduce the workload of processing huge amount of data as well as increase the accuracy for sub- sequent data mining tasks. Dimensionality reduction techniques are often applied as a data pre-processing step or as part of the data analysis to simplify the data model. This typically involves the identification of a suitable low- dimensional representation for the original high dimensional data set.

The motivation for dimensionality reduction can be summarized as follows: ? The identification of a reduced set of features that are predictive of outcomes can be very useful from a knowledge discovery perspective.

2008 IEEE Region 10 Colloquium and the Third ICIIS, Kharagpur, INDIA December 8-10.

? For many learning algorithms, the training and/or classification time increases directly with the number of features.

Existing algorithms try to measure the quality of generated rule by considering only one evaluation criterion, i.e., confidence factor or predictive accuracy. This criterion evaluates the rule depending on the number of occurrence of the rule in the entire database. More the number of occurrences better is the rule. The generated rule may have a large number of attributes involved in the rule thereby making it difficult to understand. If the generated rules are not understandable to the user, the user will never use them.

Again, since more importance is given to those rules, satisfying number of records, these algorithms may extract some rules from the data that can be easily predicted by the user. It would have been better for the user, if the algorithms can generate some of those rules that are actually hidden inside the data.



II. RELATED WORKS  A. Association Rule Mining  Let I = {I1, I2??, Im} be a set of m distinct attributes, also called literals. Ai = r is an item, where r is a domain value is attribute, Ai in a relation, R ( A1, ?, An). A is an itemset if it is a subset of I. D =. { ti , ti+1 ,????, tn) is a set of transactions, called the transaction (tid, t-itemset) .

A transaction t contains an itemset A if and only if, for all  items i?A, i is in t-itemset. An itemset A in a transaction database D has a support, denoted as Supp (A) (we also use p (A) to stand for Supp (A)), that is the ratio of transactions in D contain A. Supp(A) = |A(t)| / |D|, Where A(t) = {t in D/t contains A}.

An itemset A in a transaction database D is called a large (frequent) itemset if its support is equal to, or greater than, a threshold of minimal support (minsupp), which is given by users or experts.

An association rule is an expression of the form IF A THEN C  (or A -> C), A?C =?, where A and C are sets of items. The meaning of this expression is that transactions of the databases, which contain A, tend to contain C. Each association rule has two quality measurements: support and confidence, defined as:  1. The support of a rule A->C is the support of A?C,  where A?C means both A and C occur at the same time.

2. The confidence or predictive accuracy [2] of a rule  A->C is conf (A->C) as the ratio: |(A?C)(t)| / |A(t) or  Supp(A?C) / Supp(A).

That is, support = frequencies of occurring patterns; confidence = strength of implication.

Support-confidence framework (Agrawal et al. 1993): Let I be  the set of items in database D, A, C?I be itemset, A?C =?, p(A) is not zero and p(C) is not zero. Minimal support  minsupp) and minimal confidence (minconf) are given by users or exparts. Then A->C is a valid rule if  1. Supp(A?C) is greater or equal to minsupp, 2. conf(A->C) is greater or equal to minconf.

Mining association rules can be broken down into the following two sub-problems:  1. Generating al itemsets that have support greater than, or equal to, the user specified minimal support. That is, generating all large itemsets.

2. Generating all the rules that have minimum confidence.

B. Genetic Algorithm  Genetic Algorithm (GA) [7] was developed by Holland in 1970. This incorporates Darwinian evolutionary theory with sexual reproduction. GA is stochastic search algorithm modeled on the process of natural selection, which underlines biological evolution. GA has been successfully applied in many search, optimization, and machine learning problems.

GA process in an iteration manner by generating new populations of strings from old ones. Every string is the encoded binary, real etc., version of a candidate solution. An evaluation function associates a fitness measure to every string indicating its fitness for the problem. Standard GA apply genetic operators such selection, crossover and mutation on an initially random population in order to compute a whole generation of new strings.

? Selection deals with the probabilistic survival of the fittest, in that more fit chromosomes are chosen to survive. Where fitness is a comparable measure of how well a chromosome solves the problem at hand.

? Crossover takes individual chromosomes from P combines them to form new ones.

? Mutation alters the new solutions so as to add stochasticity in the search for better solutions.

This process has the advantage of progressing through the possible search space of the problem in an efficient, although somewhat random manner, and often leads to high-quality solutions in a relatively short space of time.



III. MULTI-OBJECTIVE OPTIMIZATION TECHNIQUE  Although it is known that genetic algorithm is good at searching for undetermined solutions, it is still rare to see that genetic algorithm is used to mine association rules. We are going to further investigate the possibility of applying genetic algorithm to the association rules mining in the following sections. At the time of taking a decision, the solution that seems to be fit better depending on the circumstances can be chosen from the set of these candidate solutions. A solution, say a, is said to be dominated by another solution, say b, if and only if the solution b is better or equal with respect to all the corresponding objectives of the solution a, and b is strictly better in at least one objective. Here the solution b is called a non-dominated solution. So it will be helpful for the decision- maker, if we can find a set of such non-dominated solutions.

IEEE Kharagpur Section & IEEE Sri Lanka Section    2008 IEEE Region 10 Colloquium and the Third ICIIS, Kharagpur, INDIA December 8-10.

There are some difficulties to use the standard multi-objective GA [8] for association rule mining problems. In case of rule mining problems, we need to store a set of better rules found from the database. If we follow the standard genetic operations only, then the final population may not contain some rules that are better and were generated at some intermediate generations. It is better to keep these rules. For this task, a separate population is used. In this population no genetic operation is performed. It will simply contain only the non- dominated chromosomes of the previous generation. The user can fix the size of this population. At the end of first generation, it will contain the non-dominated chromosomes of the first generation. After the next generation, it will contain those chromosomes, which are non-dominated among the current population as well as among the non-dominated solutions till the previous generation.

In Multiple-Objective Genetic Algorithm (MOGA) the chromosomes are selected (using standard selection scheme, e.g. roulette wheel selection) using the fitness value. Fitness value is calculated using their ranks, which are calculated from the non-dominance property of the chromosomes. The ranking step tries to find the non-dominated solutions, and those solutions are ranked as one. Among the rest of the chromosomes, if pi individuals dominate a chromosome then its rank is assigned as 1 + pi. This process continues till all the chromosomes are ranked. Then fitness is assigned to the chromosomes such that the chromosomes having the smallest rank get the highest fitness and the chromosomes having the same rank gets the same fitness. After assigning the fitness to the chromosomes, selection, replacement, crossover and mutation operators are applied to get a new set of chromosomes, as in standard GA In the present work we have used two another measures of the rules like comprehensibility [9] and interestingness [10], in addition to predictive accuracy. Using these three measures, some previously unknown, easily understandable rules can be generated. It is very difficult to quantify understandability or comprehensibility. A careful study of an association rule will infer that if the number of conditions involved in the antecedent part is less, the rule is more comprehensible. To reflect this behavior, an expression was derived as comp = N ? (number of conditions in the antecedent part). This expression serves well for the classification rule generation where the number of attributes in the consequent part is always one.

Since, in the association rules, the consequent part may contain more than one attribute; this expression is not suitable for the association rule mining. We require an expression where the number of attributes involved in both the parts of the rule has some effect. The following expression can be used to quantify the comprehensibility of an association rule,   Comprehensibility = log (1 + |C| / ( |D| - |A| )) x (1 / |A| ) Here, |C| and |A| are the number of attributes involved in the consequent part and the antecedent part, respectively and |D| is the total number of records in the database..

Since association rule mining is a part of data mining process that extracts some hidden information, it should extract only those rules that have a comparatively less occurrence in the entire database. Such a surprising rule may be more interesting to the users; which again is difficult to quantify. For classification rules it can be defined by information gain theoretic measures. This way of measuring interestingness for the association rules will become computationally inefficient.

For finding interestingness the data set is to be divided based on each attribute present in the consequent part. Since a number of attributes can appear in the consequent part and they are not pre-defined, this approach may not be feasible for association rule mining. The following expression can be used to defined as interestingness of an association rule,  Interestingness = Supp (A)* [(1? Supp(C)) /(1?Supp(AUC))]* [Supp (AU C) / Supp (A)] *[Supp (AU C) /Supp(C)].

This expression contains two parts. The first part, Supp (A)* [(1? Supp(C)) /(1? Supp(AUC))], compares the probability that A appears without C if they were dependent with the actual frequency of the appearance of A. The remaining part measures the difference of A and C appearing together in the data set and what would be expected if A and C where statistically dependent.



IV. PROPOSED WORK  In the present work we have tried to solve the association rule mining problem with a Pareto based genetic algorithm. In the Michigan approach [11] where each chromosome represents a separate rule. In the original Michigan approach we have to encode the antecedent and consequent parts separately; and thus this may be an efficient way from the point of space utilization since we have to store the empty conditions as we do not known a priori which attributes will appear in which part. So we will follow a new approach that is better than this approach from the point of storage requirement. With each attribute we associate two extra tag bits. If these two bits are 00 then the attribute next to these two bits appears in the antecedent part and if it is 11 then the attribute appears in the consequent part. And the other two combinations, 01 and 10 will indicate the absence of the attribute in either of these parts. So the rule AEF->BC will look like 00A 11B 11C 01D 00E 00F. In this way we can handle variable length rules with more storage efficiency, adding only an overhead of 2k bits, where k is the number of attributes in the database. The next step is to find a suitable scheme for encoding/decoding the rules to/from binary chromosomes. Since the positions of attributes are fixed, we need not store the name of the attributes. We have to encode the values of different attribute in the chromosome only.

Here the chromosomes are selected (using standard selection scheme, e.g. roulette wheel selection) using the fitness value.

2008 IEEE Region 10 Colloquium and the Third ICIIS, Kharagpur, INDIA December 8-10.

Fitness value is calculated using their ranks, which are calculated from the non-dominance property of the chromosomes. The ranking step tries to find the non- dominated solutions, and those solutions are ranked as one.

Among the rest of the chromosomes, if pi individuals dominate a chromosome then its rank is assigned as 1 + pi .

This process continues till all the chromosomes are ranked.

Then fitness is assigned to the chromosomes such that the chromosomes having the smallest rank gets the highest fitness and the chromosomes having the same rank gets the same fitness. After assigning the fitness to the chromosomes, selection, replacement, crossover and mutation operators are applied to get a new set of chromosomes, as in standard GA.

Our approach works as follows:  1. Load a sample of records from the database that fits in the memory.

2. Generate N chromosomes randomly.

3. Decode them to get the values of the different  attributes.

4. Scan the loaded sample to find the support of  antecedent part, consequent part and the rule.

5. Find the confidence, comprehensibility and  interestingness values.

6. Rank the chromosomes depending on the non-  dominance property.

7. Assign fitness to the chromosomes using the ranks,  as mentioned earlier.

8. Select the chromosomes, for next generation, by  roulette wheel selection scheme using the fitness calculated in Step 7.

9. Replace all chromosomes of the old population by the chromosomes selected in Step 8.

10. Perform multi-point crossover and mutation on these new individuals.

11. If the desired number of generations is not completed, then go to Step 3.

12. Decode the chromosomes in the final stored population, and get the generated rules.

13. Select chromosomes based on accuracy, comprehensibility and interestingness.



V. IMPLEMENTATION & RESULT  The proposed technique has been implemented on different data sets with satisfactory results. Here we present the results on one such data (http://archive.ics.uci.edu/ml/machine- learning-databases/lungcancer/lung-cancer.data) set having 39 attributes and 138 records. Crossover and mutation probabilities were taken respectively as 0.84 and 0.02; 4 point crossover operator was used and the population size was kept fixed as 40. Number of generations was fixed as 300. Best four rules which were selected based on accuracy, comprehensibility and interestingness, are put in the following table.

N.B.: [A, C, I stand for accuracy, comprehensibility and interestingness respectively and {1->0} stands for attribute no.

1 having value 0.]   Rule   Antecedent Part   Consequent Part   A value   C value   I value         {4>3}, {18->0}, {28->3}, {38->0}     {1->1},{5->2}, {9->0},{12->3}, {15->0},{17->1}, {21->1},{24->2}, {26>2},{29->0}, {31->2},{34->3}      0.2391      0.0603      0.0145          {12->1}, {13->2}, {15->0}, {22->1}, {24->3}     {1->3},{7->2}, {8->0},{16>3}, {17->1},{20->1}, {21->2},{29->3}, {30->1},{32->1}, {33->3},{34->2}, {35->0},{37->2}, {38->1}      0.2273      0.0575      0.0167             {1->0}, {3->2}, {5->0}, {7->1}, {12->3}, {16->2}, {21->3}     {2->0},{4->2}, {9->1},{13->1}, {15->0},{17->2}, {20->3},{24->2}, {25->3},{28->1}, {30->0},{31->0}, {32->0},{36->2}, {38->1},{39->3}      0.2129      0.0418      0.0173            {6->2}, {13->0}, {22->3}, {25->1}, {31->1}, {38->2}, {39->0}     {3->0},{4->1}, {8->3},{16->1}, {21->2},{26->2}, {29->0},{30->3}, {35->0},{36->3}      0.2074      0.0275      0.0187    2008 IEEE Region 10 Colloquium and the Third ICIIS, Kharagpur, INDIA December 8-10.

Our main goal is to reduce the dimension of a dataset. Again we know that data mining technique (with genetic algorithm) does not give the best solution, it gives optimal solution. Here we have selected one rule with highest accuracy value, one rule with highest interestingness and two rules with highest two comprehensibility values. Here, basically we concentrate with comprehensibility because it is very needed to understand the rule for reduction the dimension. If generated rules are not understandable to the user, the user will never use them.

If rule no. 1 is selected, then we can see that only sixteen attributes are involved in this rule. So, here dimension (number of attributes) is reduced from thirty-nine to sixteen.

Although, there are different values (as in records) for these attributes. It will also helpful when we want to make market basket database for the dataset. The size of the market basket database will be very less after reducing the dimension (number of attributes).



VI.      CONCLUSION & FUTURE WORK  Since its introduction (Agrawal et al. 1993), association rule mining has become an important research area in data mining.

This work covers a broad spectrum of topics, including efficient algorithms for mining association rules (Agrawal et al. 1993; Park et al. 1997; Zhang and Zhang 2002), strongly collective itemsets (Aggarawal and Yu 1998), probability ratio measure (Wu et al. 2002), one-step based association rule mining (Webb 2000), and FP-tree based association rule mining (Han et al. 2000). Though these models are effective and efficient for identifying association rules, they are usually based on database-dependent minimum support. It is difficult or users or even experts to appropriately specify the minimum support as a threshold.

The use of a multi-objective evolutionary framework for association rule mining offers a tremendous flexibility to exploit in further work. In this present work, we have used a Pareto based genetic algorithm to solve the multi-objective rule mining problem using three measures?? comprehensibility, interestingness and the predictive accuracy.

We adopted a variant of the Michigan approach to represent the rules as chromosomes, where each chromosome represents a separate rule. The approach can be worked with numerical valued attributes as well as categorical attributes.

The proposed dimensionality reduction approach using association rule mining is only evaluated based on the three criterion of how relevant the selected features are to the whole dataset. To have a more compact evaluation of the approach, further research should be performed on its completeness to the later data mining process.

