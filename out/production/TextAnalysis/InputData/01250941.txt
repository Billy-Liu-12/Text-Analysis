Association Rule Mining in Peer-to-Peer Systems ?

Abstract  We extend the problem of association rule mining ? a key data mining problem ? to systems in which the database is partitioned among a very large number of computers that are dispersed over a wide area. Such com- puting systems include GRID computing platforms, feder- ated database systems, and peer-to-peer computing envi- ronments. The scale of these systems poses several difficul- ties, such as the impracticality of global communications and global synchronization, dynamic topology changes of the network, on-the-fly data updates, the need to share re- sources with other applications, and the frequent failure and recovery of resources.

We present an algorithm by which every node in the  system can reach the exact solution, as if it were given the combined database. The algorithm is entirely asyn- chronous, imposes very little communication overhead, transparently tolerates network topology changes and node failures, and quickly adjusts to changes in the data as they occur. Simulation of up to 10,000 nodes show that the algorithm is local: all rules, except for those whose confidence is about equal to the confidence threshold, are discovered using information gathered from a very small vicinity, whose size is independent of the size of the system.

1 Introduction  The problem of association rule mining (ARM) in large transactional databases was first introduced in 1993 [1].

The input to the ARM problem is a database in which ob- jects are grouped by context. An example would be a list of items grouped by the transaction in which they were bought. The objective of ARM is to find sets of objects which tend to associate with one another. Given two dis- tinct sets of objects, ? and ? , we say ? is associated with  ? if the appearance of ? in a certain context usually im- ? This work was supported in part by Microsoft Academic Foundation.

plies that ? will appear in that context as well. The output of an ARM algorithm is a list of all the association rules that appear frequently in the database and for which the association is confident.

ARM has been the focus of great interest among data  mining researchers and practitioners. It is today widely accepted to be one of the key problems in the data mining field. Over the years many variations were described for ARM, and a wide range of applications were developed.

The overwhelming majority of these deal with sequential ARM algorithms. Distributed association rule mining (D- ARM) was defined in [2], not long after the definition of ARM, and was also the subject of much research (see, for example, [2, 5, 15, 7, 8]).

In recent years, database systems have undergonemajor  changes. Databases are now detached from the computing servers and have become distributed in most cases. The natural extension of these two changes is the development of federated databases ? systems which connect many dif- ferent databases and present a single database image. The trend toward ever more distributed databases goes hand in hand with an ongoing trend in large organizations to- ward ever greater integration of data. For example, health maintenance organizations (HMOs) envision their medical records, which are stored in thousands of clinics, as one database. This integrated view of the data is imperative for essential data analysis applications ranging from epidemic control, ailment and treatment pattern discovery, and the detection of medical fraud or misconduct. Similar exam- ples of this imperative are common in other fields, includ- ing credit card companies, large retail networks, and more.

An especially interesting example for large scale dis-  tributed databases are peer-to-peer systems. These sys- tems include GRID computing environments such as Con- dor [10] (20,000 computers), specific area computing sys- tems such as SETI@home [12] (1,800,000 computers) or UnitedDevices [14] (2,200,000 computers), general pur- pose peer-to-peer platforms such as Entropia [6] (60,000 peers), and file sharing networks such as Kazza (1.8 mil- lion peers). Like any other system, large scale distributed systems maintain and produce operational data. However,      in contrast to other systems, that data is distributed so widely that it will usually not be feasible to collect it for central processing. It must be processed in place by dis- tributed algorithms suitable to this kind of computing en- vironment.

Consider, for example, mining user preferences over  the Kazza file sharing network. The files shared through Kazza are usually rich media files such as songs and videos. participants in the network reveal the files they store on their computers to the system and gain access to files shared by their peers in return. Obviously, this database may contain interesting knowledge which is hard to come by using other means. It may be discovered, for instance, that people who download The Matrix also look for songs by Madonna. Such knowledge can then be ex- ploited in a variety of ways, much like the well known data mining example stating that ?customers who purchase di- apers also buy beer?.

The large-scale distributed association rule mining  (LSD-ARM) problem is very different from the D-ARM problem, because a database that is composed of thousands of partitions is very different from a small scale distributed database. The scale of these systems introduces a plethora of new problemswhich have not yet been addressed by any ARM algorithm. The first such problem is that in a system that large there can be no global synchronization. This has two important consequences for any algorithm proposed for the problem: The first is that the nodes must act inde- pendently of one another; hence their progress is specu- lative, and intermediate results may be overturned as new data arrives. The second is that there is no point in time in which the algorithm is known to have finished; thus, nodes have no way of knowing that the information they possess is final and accurate. At each point in time, new informa- tion can arrive from a far-away branch of the system and overturn the node?s picture of the correct result. The best that can be done in these circumstances is for each node to maintain an assumption of the correct result and update it whenever new data arrives. Algorithms that behave this way are called anytime algorithms.

Another problem is that global communication is costly  in large scale distributed systems. This means that for all practical purposes the nodes should compute the result through local negotiation. Each node can only be famil- iar with a small set of other nodes ? its immediate neigh- bors. It is by exchanging information with their immediate neighbors concerning their local databases that nodes in- vestigate the combined, global database.

A further complication comes from the dynamic nature  of large scale systems. If the mean time between failures of a single node is 20,000 hours1, a system consisting of  1This figure is accepted for hardware; for software the estimate is usually a lot lower.

100,000 nodes could easily fail five times per hour. More- over, many such systems are purposely designed to sup- port the dynamic departure of nodes. This is because a system that is based on utilizing free resources on non- dedicated machines should be able to withstand sched- uled shutdowns for maintenance, accidental turnoffs, or an abrupt decrease in availability when the user comes back from lunch. The problem is that whenever a node departs, the database on that node may disappear with it, chang- ing the global database and the result of the computation.

A similar problem occurs when nodes join the system in mid-computation.

Obviously none of the distributed ARM algorithms de-  veloped for small-scale distributed systems can manage a system with the aforementioned features. These algo- rithms focus on achieving parallelization induced speed- ups. They use basic operators, such as broadcast, global synchronization, and a centralized coordinator, none of which can be managed in large-scale distributed systems.

To the best of our knowledge, no D-ARM algorithm presented so far acknowledges the possibility of failure.

Some relevant work was done in the context of incre- mental ARM, e.g., [13], and similar algorithms. In these works the set of rules is adjusted following changes in the database. However, we know of no parallelizations for those algorithms even for small-scale distributed systems.

In this paper we describe an algorithm which solves  LSD-ARM. Our first contribution is the inference that the distributed association rule mining problem is reducible to the well-studied problem of distributed majority votes.

Building on this inference, we develop an algorithmwhich combines sequential association rule mining, executed lo- cally at each node, with a majority voting protocol to dis- cover, at each node, all of the association rules that exist in the combined database. During the execution of the al- gorithm, which in a dynamic system may never actually terminate, each node maintains an ad hoc solution. If the system remains static, then the ad hoc solution of most nodes will quickly converge toward an exact solution. This is the same solution that would be reached by a sequen- tial ARM algorithm had all the databases been collected and processed. If the static period is long enough, then all nodes will reach this solution. However, in a dynamic system, where nodes dynamically join or depart and the data changes over time, the changes are quickly and lo- cally adjusted to, and the solution continues to converge. It is worth mentioning that no previous ARM algorithm was proposed which mines rules (not itemsets) on the fly. This contribution may affect other kinds of ARM algorithms, especially those intended for data streams [9].

The majority voting protocol, which is at the crux of  our algorithm, is in itself a significant contribution. It re- quires no synchronization between the computing nodes.

Each node communicates only with its immediate neigh- bors. Moreover, the protocol is local: in the overwhelming majority of cases, each node computes the majority ? i.e., identifies the correct rules ? based upon information arriv- ing from a very small surrounding environment. Local- ity implies that the algorithm is scalable to very large net- works. Another outcome of the algorithm?s locality is that the communication load it produces is small and roughly uniform, thus making it suitable for non-dedicated envi- ronments.

2 Problem Definition  The association rule mining (ARM) problem is tradi- tionally defined as follows: Let ? ? ? ? ? ? ? ? ? ? ? ? ? be the items in a certain domain. An itemset is some sub- set ? ? ? . A transaction ? is also a subset of ? associ- ated with a unique transaction identifier ? ? ? . A database  ? ! is a list that contains " ? ! " transactions. Given an itemset ? and a database ? ! , $ & ( ( + , ? . ? ? ! 2 is the number of transactions in ? ! which contain all the items of ? , and 3 , 5 6 . ? ? ! 2 ? $ & ( ( + , ? . ? ? ! 2 = " ? ! " .

For some frequency threshold @ B D ? F 3 , 5 6 B K , we say that an itemset ? is frequent in a database ? ! if  3 , 5 6 . ? ? ! 2 Q D ? F 3 , 5 6 and infrequent otherwise.

For two distinct frequent itemsets ? and S , and a con- fidence threshold @ B D ? F V + F W B K , we say the rule ? Z S is confident in ? ! if 3 , 5 6 . ? ] S ? ! 2 Q  D ? F V + F W ` 3 , 5 6 . ? ? ! 2 . We will call confident rules between frequent itemsets correct and the remaining rules false. The solution for the ARM problem is a c ? ! e , the list of all the correct rules in the given database.

When the database is dynamically updated, that is,  transactions are added to it or deleted from it over time, we denote ? ! g the database at time ? . Now consider that the database is also partitioned among an unknown number of share nothing machines (nodes); we denote the partition of node & at time ? ? ! ig . Given an infrastructure through which those machines may communicate data, we denote  c & e g the group of machines reachable from & at time ? . We will assume symmetry, i.e., j l c & e g m & l c j e g . Never- theless, c & e g may or may not include all of the machines.

The solution to the large-scale distributed association rule mining (LSD-ARM) problem for node & at time ? is the set of rules which are correct in the combined databases of the machines in c & e g ; we denote this solution a c & e g .

Since both c & e g and ? ! og of each j l c & e g are free to  vary with time ? so does a c & e g . It is thus imperative that & calculate not only the eventual solution but also approx- imated, ad hoc, solutions. We term qa c & e g the ad hoc solu- tion of node & at time ? . It is common practice to measure the performance of an anytime algorithm according to its  recall: " r t i v x y zr t i v x "" r t i v x " , and its precision: " r t i v x y zr t i v x "" zr t i v x "  .

We require that if both c & e g and ? ! og of each j l c & e g remain static long enough, then the approximated solution  qa c & e g will converge to a c & e g . In other words, both the precision and the recall of & converge to a hundred percent.

Throughout this work we make some simplifying as-  sumptions. We assume that node connectivity is a forest ? for each & and j there could be either one route from & to j or none. Trees in the forest may split, join, grow, and shrink, as a result of node crash and recovery, or depar- ture and join. We assume the failure model of computers is fail-stop, and that a node is informed of changes in the status of adjacent nodes.

3 An ARM Algorithm for Large-Scale Dis- tributed Systems  As previously described, our algorithm is comprised of two rather independent components: Each node exe- cutes a sequential ARM algorithm which traverses the lo- cal database and maintains the current result. Addition- ally, each node participates in a distributed majority voting protocol which makes certain that all nodes that are reach- able from one another converge toward the correct result according to their combined databases. We will begin by describing the protocol and then proceed to show how the full algorithm is derived from it.

3.1 LSD-Majority Protocol  It has been shown in [11] that a distributed ARM algo- rithm can be viewed as a decision problem in which the participating nodes must decide whether or not each item- set is frequent. However, the algorithm described in that work extensively uses broadcast and global synchroniza- tion; hence it is only suitable for small-scale distributed systems. We present here an entirely different majority voting protocol ? LSD-Majority ? which works well for large-scale distributed systems. In the interest of clarity we describe the protocol assuming the data at each node is a single bit. We will later show how the protocol can easily be generalized for frequency counts.

As in LSD-ARM, the purpose of LSD-Majority is to  ensure that each node converges toward the correct major- ity. Since the majority problem is binary, we measure the recall as the proportion of nodes & whose ad hoc solution is one when the majority in c & e g is of set bits, or zero when the majority in c & e g is of unset bits. The protocol dictates how nodes react when the data changes, a message is re- ceived, or a neighboring node is reported to have detached or joined.

The nodes communicate by sending messages that con- tain two integers: ? ? ? ? ? , which stands for the number of bits this message reports, and 	 ? ? which is the number of those bits which are equal to one. Each node ? will record, for every neighbor  , the last message it sent to  ?? 	 ? ? ? ? ? ? ? ? ? ? ? ? ? ? and the last message it received from ? ? 	 ? ? ? ? ? ? ? ? ? ? ? ? ? . Node ? calculates the following two functions of these messages and its own local bit:  ? ? ? 	 ? ?  ? ? ! # % 	 ? ? ? ? ' ) + ? ? ?  ? ? ! # % ? ? ? ? ? ? ? .? ? ? ? 	 ? ? ? ? ? 	 ? ? ? ? ' ) 0 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 2 Here 3 ? is the set of edges colliding with ? , 	 ? is the value of the local bit, and ? ? is, for now, one. ? ? measures the number of access set bits ? has been informed of. ? ? ? measures the number of access set bits ? and  have last reported to one another. Each time 	 ? changes, a message is received, or a node connects to  or disconnects from  ,? ? is recalculated; ? ? ? is recalculated each time a mes- sage is sent to or received from  .

Algorithm 1 LSD-Majority Input for node ? : The set of edges that collide with it 3 ? , A bit 	 ? and the majority ratio ) .

Output: The algorithm never terminates. Nevertheless, at each point in time if  ? ? 8 : then the output is ; , otherwise it is : .

Definitions:  ? ? ? 	 ? ? = ? ? ! # % 	 ? ? ? ? ') @ ? ? ? = ? ? ! # % ? ? ? ? ? ? ? B , ? ? ? ? 	 ? ? ? ? ? 	 ? ? ? ? ') 0 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 2 Initialization: For each  ? E 3 ? set 	 ? ? ? ? , ? ? ? ? ? ? ? ,	 ? ? ? ? , ? ? ? ? ? ? ? to : . Set ? ? ? ; .

On edge  ? recovery : Add  ? to 3 ? . Set 	 ? ? ? ? ,? ? ? ? ? ? ? , 	 ? ? ? ? , ? ? ? ? ? ? ? to : .

On failure of edge  ? E 3 ? : Remove  ? from 3 ? .

On message ? 	 ? ? ? ? ? ? ? ? ? received over edge  ? : Set	 ? ? ? ? ? 	 ? ? , ? ? ? ? ? ? ? ? ? ? ? ? ? On change in 	 ? , edge failure or recovery, or the receiv- ing of a message: For each  ? E 3 ? If ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? : and ? ? 8 : or ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? O : and either ? ? ? Q : and? ? O ? ? ? or ? ? ? 8 : and ? ? Q ? ? ? Set 	 ? ? ? ? ? 	 ? ?  V ? XY ? ? ! # % 	 ? ?  V ? and ? ? ? ? ? ? ? ? ? ? ?  V ? XY ? ? ! # % ? ? ? ? ?  V ? Send ? 	 ? ? ? ? ? ? ? ? ? ? ? ? ? over  ? to  Each node performs the protocol independently with each of its immediate neighbors. Node ? coordinates  its majority decision with node  by maintaining the same  ? ? ? value (note that ? ? ? ? ? ? ? ) and making certain that  ? ? ? will not mislead  into believing that the global majority is larger than it actually is. As long as? ? 8 ? ? ? 8 : and ? ? 8 ? ? ? 8 : , there is no need for ? and  to exchange data. They both calculate the majority of the bits to be set; thus, the majority in their combined data must be of set bits. If, on the other hand,? ? ? O ? ? , then  might mistakenly calculate ? ? 8 : because it has not received the updated data from ? . Thus, in this case the protocol dictates that ? send  a message,^ 	 ? ? = V ? XY ? ? ! # % 	 ? ? V ? ? ? ? ? = V ? XY ? ? ! # % ? ? ? ? ? V ? _ .

Note that after this message is sent,  ? ? ? ? ? ? .

The opposite case is almost the same. Again, if : O? ? ? 8 ? ? and : O ? ? ? 8 ? ? , then no messages are  exchanged. However, when ? ? O ? ? ? , the protocol dic-  tates that ? send  a message calculated the same way. The only difference is that when no messages were sent or re- ceived,  knows, by default, that ? ? Q : and ? knows that? ? Q : . Thus, unless ? ? 8 : , ? does not send mes- sages to  because the majority bits in their combined data cannot be set.

The pseudocode of the LSD-Majority protocol is given  in Algorithm 1. It is easy to see that when the protocol dictates that no node needs to send any message, either? ? 8 : for all nodes  E h ? i j , or ? ? Q : for all of them. If there is disagreement in h ? i j , then there must be disagreement between two immediate neighbors, in which case at least one node  must send data, which will cause? ? ? ? ? ? ? ? ? ? ? ? ? ? ? to increase. This number is bounded by the size of h ? i j ; hence, the protocol always reaches con- sensus in a static state. It is less trivial to show that the conclusion they arrive at is the correct one. This proof is too long to include in this context.

In order to generalize LSD-Majority for frequency  counts, ? ? need only to be set to the size of the local database and 	 ? to the local support of an itemset. Then, if we substitute l n ? q r t u for ) , the resulting protocol will decide whether an itemset is frequent or infrequent inh ? i j . Deciding whether a rule v w y is confident is also straightforward using this protocol: ? ? should now count in the local database the number of transactions that in- clude v , 	 ? should count the number of these transactions that include both v and y , and ) should be replaced withl n ? z ? ? { .

Deciding whether a rule is correct or false requires that  each node run two instances of the protocol: one to decide whether the rule is frequent and the other to decide whether it is confident. Note, however, that for all rules of the form| w ~  | , only one instance of the protocol should be performed to decide whether ~ is frequent.

The strength of the protocol lies in its behavior when      the average of the bits over ? ? ? ? is somewhat different than the majority threshold ? . Defining the significance of the input as  ? ? ? ? ? ?  ? ? ? ? ? ?  ? ? ? ? ? ? , we will show in section 4.1 that  even a minor significance, on the scale of ? ? ? ? , is suffi- cient for making a correct decision using data from just a small number of nodes. In other words, even a minor significance is sufficient for the algorithm to become lo- cal. Another strength of the protocol is that during static periods, most of the nodes will make the correct major- ity decision very quickly. These two features make LSD- Majority especially well-suited for LSD-ARM, in which the overwhelming majority of the candidates are far from significant.

3.2 Majority-Rule Algorithm  LSD-Majority efficiently decides whether a candidate rule is correct or false. It remains to show how candi- dates are generated and how they are counted in the lo- cal database. The full algorithm must satisfy two require- ments: First, each node must take into account not only the local data, but also data brought to it by LSD-Majority, as this data may indicate that additional rules are correct and thus further candidates should be generated. Second, un- like other algorithms, which produce rules after they have finished discovering all itemsets, an algorithmwhich never really finishes discovering all itemsets must generate rules on the fly. Therefore the candidates it uses must be rules, not itemsets. We now present an algorithm ? Majority- Rule ? which satisfies both requirements.

The first requirement is rather easy to satisfy. We sim-  ply increment the counters of each rule according to the data received. Additionally, we employ a candidate gen- eration approach that is not levelwise: as in the DIC al- gorithm [4], we periodically consider all the correct rules, regardless of when they were discovered, and attempt to use them for generating new candidates.

The second requirement, mining rules directly rather  than mining itemsets first and producing rules when the al- gorithm terminates, has not, to the best of our knowledge, been addressed in the literature. To satisfy this requirement we generalize the candidate generation procedure of Apri- ori [3]. Apriori generates candidate itemsets in two ways: Initially, it generates candidate itemsets of size ? : ? ! # for every ! % ' . Later, candidates of size ( ) ? are generated by finding pairs of frequent itemsets of size ( that differ by only the last item ? * , ? ! . # and * , ? ! 2 # ? and validating that all of the subsets of * , ? ! . 6 ! 2 # are also frequent be- fore making that itemset a candidate. In this way, Apriori generates the minimal candidate set which must be gener- ated by any deterministic algorithm.

In the case of Majority-Rule, the dynamic nature of the  Algorithm 2Majority-Rule Input for node ? : The set of edges that collide with it 8 : .

The local database ; = : . > ! A C D F G , > ! A I J A K , > Initialization: Set I L ? N P Q ? ! # S K J D V W W ! % ' # For each D % I set D ? ] ? _ a D ? c J ? A g a ? , and D ? ? a  > ! A C D F G For each D % I and every j ? % 8 : set D ? ] ? _ : m a  D ? c J ? A g : m a D ? ] ? _ m : a D ? c J ? A g m : a ? Upon receiving ? D ? ! s 6 ] ? _ 6 c J ? A g # from a neighbor j If  D a N * Q y S z% I add it to I . If c | a N P Q * , y S z% I add it too.

Set D ? ] ? _ m : a ] ? _ , D ? c J ? A g m : a c J ? A g On edge j ? recovery: Add j ? to 8 : . For all D % I set  D ? ] ? _ : m a D ? c J ? A g : m a D ? ] ? _ m : a D ? c J ? A g m : a ? On failure of edge j ? % 8 : : Remove j ? from 8 : .

Main: Repeat the following for ever Read the next transaction ? ? . If it is the last one in ; = : iterate back to the first one.

For every D a N * Q y S % I which was generated after this transaction was last read If * ? ? increase D ? c J ? A g If * , y ? ? increase D ? ] ? _ Once every > transactions Set ?? ? ? ? ? a the set of rules D a N * Q y S % I such that  ? : ? D ? ? ? and for D | a N P Q * , y S ? : ? D | ? ? ? For every D a N * Q y S % ?? ? ? ? ? , such that * a P  and ! % * if D | a N X ? ? ! # Q ? ! # S z% I insert D | into I with D | ? ] ? _ a D | ? c J ? A g a ? , D | ? ? a > ! A I J A K and  D | ? ! s a unique rule id For each c . a N * Q y , ? ! . # S 6 c 2 a  N * Q y , ? ! 2 # S % ?? ? ? ? ? such that ! . ? ! 2 , if c ? a N * Q y , ? ! . 6 ! 2 # S z% I and ? ! ? % y ? D ? a  N * Q y , ? ! . 6 ! 2 # ? ? ! ? # S % ?? ? ? ? ? , add D ? to I with D ? ? ] ? _ a D ? ? c J ? A g a ? , D ? ? ? a D . ? ? , and  D ? ? ! s a unique rule id For each D a N * Q y S % I and for every j ? % 8 : If D ? c J ? A g : m ) D ? c J ? A g m : a ? and ? : ? D ? ? ? or D ? c J ? A g : m ) D ? c J ? A g m : ? ? and either ? : m ? D ? ? ?  and ? : ? D ? ? ? : m ? D ? or  ? : m ? ? and ? : ? D ? ? ? : m ? D ? Set D ? ] ? _ : m a D ? ] ? _ ) ?  ? : ?? m : ? ? D ? ] ? _ ? : and  D ? c J ? A g : m a D ? c J ? A g ) ? ? : ?? m : ? ?  D ? c J ? A g ? :  Send ? D ? ! s 6 D ? ] ? _ : m 6 D ? c J ? A g : m # over j ? to j      system means that it is never certain whether an itemset is frequent or a rule is correct. Thus, it is impossible to guar- antee that no superfluous candidates are generated. Nev- ertheless, at any point during execution ? , it is worthwhile to use the ad hoc set of rules, ?? ? ? ? ? , to try and limit the number of candidate rules. Our candidate generation cri- terion is thus a generalization of Apriori?s criterion. Each node generates initial candidate rules of the form 	 ?  ? ? for each ? ? ? . Then, for each rule 	 ? ? ? ?? ? ? ? ? , it generates ? ?  ? ? ?  ? ? candidate rules for all ? ? ? .

In addition to these initial candidate rules, the node will look for pairs of rules in ? ? ? ? ? which have the same left- hand side, and right-hand sides that differ only in the last item ? ? ? ' )  ? + ? and ? ? ' )  ? . ? . The node will verify that the rules ? ? ' )  ? + 5 ? . ? ?  ? 7 ? , for every ? 7 ? ' , are also correct, and then generate the can- didate ? ? ' )  ? + 5 ? . ? . It can be inductively proved that if ?? ? ? ? ? contains only correct rules, then no superfluous candidate rules are ever generated using this method.

The rest of Majority-Rule is straightforward. When-  ever a candidate is generated, the node will begin to count its support and confidence in the local database. At the same time, the node will also begin two instances of LSD- Majority, one for the candidate?s frequency and one for its confidence, and these will determine whether this rule is globally correct. Since each node runs multiple instances of LSD-Majority concurrently, messages must carry, in ad- dition to = ? ? and @ A ? B ? , the identification of the rule it refers to, D E ? F . We will denote G H I D K and G H L I D K the re- sult of the previously defined functions when they refer to the counters and N of candidate D . Finally, D E N is the ma- jority threshold that applies to D . We set D E N to O ? B S D U V for rules with an empty left-hand side and to O ? B Y A B Z for all other rules.

The pseudocode of Majority-Rule is detailed in Algo-  rithm 2.

4 Experimental Results  To evaluate Majority-Rule?s performance, we imple- mented a simulator capable of running thousands of simu- lated computers. We simulated 1600 such computers, con- nected in a random tree overlaid on a [ ] _ [ ] grid. We also implemented a simulator for a stand-alone instance of the LSD-Majority protocol and ran simulations of up to 10,000 nodes on a ` ] ] _ ` ] ] grid. The simulations were run in lock-step, not because the algorithm requires that the computers work in locked-step ? the algorithm poses no such limitations ? but rather because properties such as convergence and locality are best demonstrated when all processors have the same speed and all messages are de- livered in unit time.

We used synthetic databases generated by the standard  tool from the IBM-quest data mining group [3]. We gener- ated three synthetic databases ? T5.I2, T10.I4 and T20.I6 ? where the number after T is the average transaction length and the number after I is the average pattern length. The combined size of each of the three databases is 10,000,000 transactions. Other than the number of transactions the change we made from the defaults was reducing the num- ber of patterns. This was reduced so as to increase the pro- portion of correct rules from one in ten-thousands to one in a hundred candidates. Because our algorithm performs better for false rules than for correct ones this change does not impair out the validity of the results.

4.1 Locality of LSD-Majority andMajority-Rule  The LSD-Majority protocol, and consequently the Majority-Rule algorithm, are local algorithms in the sense that a correct decision will usually only require that a small subset of the data is gathered. We measure the locality of an algorithm by the average and maximum size of the en- vironment of nodes. The environment is defined in LSD- Majority as the number of input bits received by the node and in Majority-Rule as the percent of the global database reported to the node, until system stabilization. Our exper- iments with LSD-Majority show that its locality strongly  depends on the significance of the input: d e g h  i j k m e  n o d e g h i j k q  e r ` .

Figure 1(a) describes the results of a simulation of  10,000 nodes in a random tree over a grid, with various percentages of set input bits at the nodes. It shows that when the significance is s ] E ` (i.e., [ w x or w w x of the nodes have set input bits), the protocol already has good locality: the maximal environment is about 1200 nodes and the average size a little over 300. If the percentage of set input bits is closer to the threshold, a large portion of the data would have to be collected in order to find the majority. In the worst possible case, when the number of set input bits is equal to the number of unset input bits plus one, at least one node would have to collect all of the in- put bits before the solution could be reached. On the other hand, if the percentage of set input bits is further from the threshold, then the average environment size becomes neg- ligible. In many cases different regions of the grid may not exchange any messages at all. In Figure 1(c) these results repeat themselves for Majority-Rule.

Further analysis ? Figure 1(c) ? show that the size of a  node?s environment depends on the significance in a small region around the nodes. I.e., if the inputs of nodes are independent of one another then the environment size will be random. This makes our algorithms fair: nodes? perfor- mance is not determined by its connectivity or location but rather by the data.

10 20 30 40 45 50 55 60 70 80 90  E nv  ir on  m en  t s iz  e  Percentage of set bits  (a). Worst, average and best environment sizes  20 40 60 80 100           100 (b). 40% set bits             -0.1 -0.05 0 0.05 0.1  P er  ce nt  o f D  B g  at he  re d  Rule significance  (c). Locality of T20.I6 on a 40 by 40 grid, vs. rule significance  Worst-case Average  Figure 1. The locality of LSD-Majority (a) and of Majority-Rule (c) depends of the significance. The distribution of environment sizes (b) depends on the local significance and is hence random.

4.2 Convergence and Cost of Majority-Rule  In addition to locality, the other two important char- acteristics of Majority-Rule are its convergence rate and communication cost. We measure convergence by cal- culating the recall ? the percentage of rules uncovered ? and precision ? the percentage of correct rules among all rules assumed correct ? vis-a-vis the number of transac- tions scanned. Figure 2 describes the convergence of the recall (a) and of the precision (b). In (c) the convergence of stand-alone LSD-Majority is given, for various percent- ages of set input bits.

To understand the convergence of Majority-Rule, one  must look at how the candidate generation and the major- ity voting interact. Rules which are very significant are expected to be generated early and agreed upon fast. The same holds for false candidates with extremely low sig- nificance. They too are generated early, because they are usually generated due to noise, which subsides rapidly as a greater portion of the local database is scanned; the con- vergence of LSD-Majority will be as quick for them as for rules with high significance. This leaves us with the group of marginal candidates, those that are very near to the threshold; these marginal candidates are hard to agree upon, and in some cases, if one of their subsets is also marginal, they may only be generated after the algorithm has been working for a long time. We remark that marginal candidates are as difficult for other algorithms as they are for Majority-Rule. For instance, DIC may suffer from the same problem: if all rules were marginal, then the number of database scans would be as large as that of Apriori.

An interesting feature of LSD-Majority convergence is  that the number of nodes that assume a majority of set bits always increases in the first few rounds. This would result in a sharp reduction in accuracy in the case of a majority of unset bits, and an overshot, above the otherwise expo- nential convergence, in the case of a majority of set bits.

This occurs because our protocol operates in expanding wavefronts, convincing more and more nodes that there is a certain majority, and then retreating with many nodes be- ing convinced that the majority is the opposite. Since we assume by default a majority of zeros, the first wavefront that expands would always be about a majority of ones.

Interestingly enough, the same pattern can be seen in the convergence of Majority-Rule (more clearly for the preci- sion than for the recall).

Figure 3 presents the the communication cost of LSD-  Majority vis-a-vis the percentage of set input bits and of Majority-Rule vis-a-vis rule significance. For rules that are very near the threshold, a lot of communication is re- quired, on the scale of the grid diameter. For significant rules the communication load is about ten messages per rule per node. However, for false candidates the commu- nication load drops very fast to nearly no messages at all.

It is important to keep in mind that we denote every pair of integers we send a message. In a realistic scenario, a mes- sage will contain up to ? ? ? ? bytes, or about ? ? ? integer pairs.

5 Conclusions  We have described a new distributed majority vote pro- tocol ? LSD-Majority?which we incorporated as part of an algorithm ? Majority-Rule ? that mines association rules on distributed systems of unlimited size. We have shown that the key quality of our algorithm is its locality ? the fact that information need not travel far on the network for the correct solution to be reached. We have also shown that the locality of Majority-Rule translates into fast convergence of the result and low communication demands. Commu- nication is also very efficient, at least for candidate rules which turn out not to be correct. Since the overwhelm- ing majority of the candidates usually turn out this way, the communication load of Majority-Rule depends mainly       0.2  0.4  0.6  0.8   0.01 0.1 1 10 100  R ec  al l  Number of database scans  (a). Average recall for T5.I2, T10.I4 and T20.I6  T5 T10 T20   0.2  0.4  0.6  0.8   0.01 0.1 1 10 100  P re  ci si  on  Number of database scans  (b). Precision for T5.I2, T10.I4 and T20.I6  T5 T10 T20  Pe rc  en t o  f N od  es       10 100 10001  10% 20% 30% 40% 45% 48% 52% 55% 60% 70% 80% 90%  (c). Convergence on 10,000 nodes, vs. percentages of set bits  Steps  Figure 2. Convergence of the recall and precision of Majority-Rule, and of LSD-Majority.

-1 -0.5 0 0.5 1  A ve  ra ge  n um  be r  of p  er n  od e  m es  sa ge  s  Rule significance  (a). Per node messages regarding a rule vs. rule significance           10 20 30 40 45 50 55 60 70 80 90  N um  be r  of m  es sa  ge s  Percentage of set bits  (b). Max, avg, and min per node messages vs. percentage of set bits  Figure 3. Communication characteristics of Majority-Rule (a), and of LSD-Majority (b). Each mes- sage here is a pair of integers.

on the size of the output ? the number of correct rules.

That number is controllable via user supplied parameters, namely ? ? ? ? ? ? and ? ? ? ? ? ? ? .

