An Efficient Approach to Discovering Knowledge from  Large Databases*

Abstract in the transaction. Because the amount of these trans- In this paper, we study two problems: mining as-  sociation rules and mining sequential patterns in a large database of customer transactions. The prob- lem of mining association rules focuses on discover- ing large i t emse t s  where a large itemset is a group of items which appear together in a sufficient number of transactions; while the problem of mining sequential patterns focuses on discovering large sequences where a large sequence is an ordered list of sets of items which appear in a sufficient number of transactions.

We present efficient graph-based algorithms to solve these problems. The algorithms construct an associa- t i o n  graph to indicate the associations between items and then traverse the graph to generate large item- sets and large sequences, respectively. Our algorithms need to scan the database only once. Empirical eval- uations show that our algorithms outperform other algorithms which need to make multiple passes over the database.

1 Introduction From a large amount of data, potentially useful in-  formation may be discovered. Techniques have been proposed to find knowledge (or rules) from databases [l, 2, 3,  6,  9, 10, 15, 17, 21, 22, 241. The knowledge discovered can be used to  answer cooperative queries [7, 141, handle null values [20] and facilitate semantic query optimization [8, 12, 17, 18, 19, 231.

Data mining has also high applicability in retail in- dustry. The effective management of business is signif- icantly dependent on the quality of its decision mak- ing. It is therefore important to analyze past trans- action data to discover customer purchasing behav- ior and improve the quality of business decision. In order to support this analysis, a sufficient amount of transaction items needs to be collected and stored in a database. A transaction in the database typically con- sists of customer identifier, transaction date (or trans- action time) and the set of items (itemset) purchased  *This work was partially supported by the Republic of China National Science Council under Contract No. NSC 86-2213-E- 007-009.

action data is very large, an efficient algorithm needs to be devised for discovering useful information em- bedded in the transaction data.

In this paper, we study two problems: mining asso- ciation rules and mining sequential patterns in a large database of customer transactions. The problem of mining association rules over customer transactions was introduced in [a]. An association rule describes the association among items in which when some items are purchased in a transaction, others are purchased too.

In order to  find association rules, we need to dis- cover the itemsets which occur often enough within transactions. The first step to find association rules is therefore to identify all itemsets that are contained in a sufficient number of transactions above a certain minimum threshold. After discovering all such item- sets, the association rules can be generated as follows [a]: If the discovered itemset Y = 1112 ... I k ,  k 2 2, all rules that reference items from the set { 11,12, . .. , I k } can be generated. The antecedent of each of these rules is a subset X of Y ,  and the consequent is Y - X.

The rule X ==+ Y - X holds in the database D of transactions with confidence f a c t o r  c if at  least c% of the transactions in D that contain X also contain Y - X. An example of such an association rule is "95% of transactions in which coffee and sugar are purchased, milk is purchased too." The form of this rule is ')coffee, sugar milk." The antecedent of this rule consists of coffee and sugar and the conse- quent consists of milk alone. The percentage 95% is the confidence factor of the rule.

The following definitions are adopted from [a ] .  A transaction supports an itemset 2, if Z is contained in the transaction. The support  for  a n  atemset is defined as the ratio of the total number of transactions which support this itemset to the total number of transac- tions in D. Hence, the major work of mining associa- tion rules is to find all itemsets that satisfy a certain user-specified manamum support .  Each such itemset is referred to as large z temset .  An itemset of length IC is called a k-itemset and a large itemset of length k a large k-itemset.

The problem of mining sequential patterns in a  0-8186-7475-X/96 $5.00 0 1996 IEEE    large database of customer transactions was intro- duced in [5]. An example of such a pattern is that cus- tomers buy books about ?basic computer concepts,? and then about ?programming language,? and then about ?system programming.?  The problem is stated as follows [5]: A sequence is an ordered list of itemsets. A sequence s is de- noted as < SI, sa, ..., s, >, where sj is an itemset. The items in sj represent that these iterns were bought to- gether. A sequence < a1,a2,  ..., a,  > is contained in another sequence < b l ,  b 2 ,  ..., b, > if there exist in- tegers i l  < i 2  < ... < i,, 1 < i k  < m, such that a1 bi, ,  ..., a ,  C bin.  In a set of sequences, a se- quence s is m a x i m a l  i f s  is not contained in any other sequence. All the transactions of a customer, ordered by increasing transaction-time is a customer-sequence.

A customer supports a sequence s if s is contained in the customer-sequence for this customer. T h e  sup- port for a sequence is defined as the fraction of total customers who support this sequence. Each sequence satisfying a certain minimum support threshold is a large sequence. A sequence of length k is called a k- sequence and a large sequence of length k a large k- sequence. The problem of mining sequential patterns [5] is to find the maximal large sequences among all large sequences.

Various algorithms [2, 4, 5, 11, 13, 161 have been proposed to discover large itemsets or sequential pat- terns. These a1 orithms generate candidate k-itemsets (k-sequences) ?IC 2 1) for large IC-itemsets (lare ti- sequences), scan each transaction in a database to count the supports of these candidate k-itemsets ( k - sequences) and find all large k-itemsets (k-sequences) in the kth iteration based on a pre-determined min- imum support. However, because the size of the database can be very large, it is very costly to scan the database to count supports for candidate itemsets in each iteration. Hence, the key issue to improve the performance of large itemset and large sequence dis- covery is to reduce the number of candidates and the amount of data that has to be scanned in each itera- tion.

In this paper, we propose two algorithms, DLG (Di- rect Large itemset Generation) and DSG (Direct Se- quential pattern Generation), for efficient large item- set generation and efficient sequential pattern gen- eration, respectively, which are significantly different from previous approaches [a, 4, 5, 11, 13, 161. DLG and DSG are very efficient for finding large itemsets and sequential patterns, respectively, because they need not generate candidates and need to scan the database only once. The algorithms DLG and DSG construct an association graph to indicate the asso- ciations between items, and then traverse the graph to generate large itemsets and sequential patterns, re- spectively.

The rest of this paper is organized as follows: Sec- tion 2 describes the related work. The algorithm DLG proposed for generating large itemsets and the experi- mental results for the performance evaluation are pre- sented in Section 3.  Section 4 describes the algorithm DSG for sequential pattern generation and evalutes the performance of DSG. Finally, we conclude this pa-  per and present directions for future research in Sec- tion 5.

2 Related Work An algorithm for finding all association rules, called  AIS algorithm, was presented in [a] .  AIS generates candidate itemsets and counts their supports as the database is scaned in each iteration. After reading a transaction, it is determined which of the large item- sets found in the previous iteration are contained in this transaction. During a database scan, new candi- date itemsets are generated by extending these large itemsets with other items in the transaction, and sup- port information is collected to evaluate which of the candidates actually are large. However, it was found [4] that the problem with AIS is that it generates too many candidates that later turn out to be small.

Hence, the AIS algorithm is rather inefficient.

The Apriori and AprioriTid algorithms [4] generate the candidate itemsets to be counted in an iteration by using only the itemsets found large in the previ- ous iteration without considering the transactions in the database. This results in generation of a smaller number of candidate itemsets. However, for each can- didate itemset, it needs to count its appearances in all transactions. In the Apriori algorithm, each iteration requires one pass over the database. In the Apriori- Tid algorithm, the database is not scanned after the first iteration. Rather, the transaction-id and candi- date k-itemsets which were present in each transaction are generated in each iteration. This is used to count supports for candidate k + 1-itemsets during the next iteration. It was found that in the initial stages, Apri- ori is more efficient than AprioriTid, since there are too many candidate k-itemsets to be tracked during the early stages of the process. A hybrid a1 orithm of the two algorithms was also proposed in f4], and shown to lead to better performance in general.

Park, Chen and Yu [16] pointed out that the key issue to improve the performance of large itemsets dis- covery is the initial candidate set generation, espe- cially for the candidate 2-itemsets, and the amount of data that has to be scanned during large item- set generation in each iteration. They utilized hash method to reduce the number of candidate 2-itemsets generation and employed pruning techniques to pro- gressively trim the transaction database. The prun- ing techniques are described as follows: an item in a transaction can be trimmed if it  does not appear in a t  least IC of the candidate k-itemsets in the kth it- eration. However, in order to reduce the number of candidate 2-itemsets1 the overhead for building hash table is large. Moreover, in order to trim the database, it is necessary to scan each transaction in the database to determine which of the candidate itemsets are con- tained in the transaction.

Agrawal and Srikant [5] presented two algorithms called AprioriAll and AprioriSome for mining sequen- tial patterns in a large database of customer trans- actions. These two algorithms make multiple passes over the data. In each pass, they use a seed set to generate candidate sequences, and count supports for     each candidate sequence. At the end of the pass, it is determined which of the candidate sequences are actu- ally large. These large candidates become the seed for the next pass. The two algorithms generate too many candidate sequences to be counted, and the database needs t.0 be scaned repeatedly.

In [16], the DHP algorithm is shown to provide the best performance for large itemsets generation. Hence, DHP is used as the base algorithm to compare with our algorithm DLG. The analysis and experimental results are shown in Section 3.3. AprioriAll and Apri- oriSome have the similar performance, which is shown in [5]. We take AprioriAll algorithm to compare with our algorithm DSG. The analysis and the experimen- tal results are shown in Section 4.3.

3 Association Rule Discovery In this section, we present the algorithm DLG for  efficient large itemset generation. There are three phases in the DLG algorithm: The first phase is the large 1-itemset generation phase which generates large items (large 1-itemsets) and records related informa- tion. The second phase is the graph construction phase which constructs an association graph to  indicate the associations between large items. In this phase, large 2-it,emset can also be generated. The last phase is the large itemset generation phase which generates large k-itemsets ( I C  > 2) based on the constructed associa- tion graph.

In the previous approaches [a, 4, 11, 13, la] ,  they all need to sort the items in each transaction in their lexicographic order. However, our approach need not to sort the items in each transaction.

3.1 Association graph construction Before performing the DLG algorithm, each item is  assigned an integer number. Suppose item i represents the item whose item number is i .  In the first phase, algorithm DLG scans the database once to count the support and build a bit vector for each item. The length of each bit vector is the number of transactions in the database. If an item appears in the i th trans- action, the ith bit of the bit vector associated with this item is set to 1. Otherwise, the ith bit of the bit vector is set to 0. The bit vector associated with item i is denoted as BV,. The number of 1?s in BK is equal to the number of transactions which support the item i ,  that is, the support for the item i .

For example, consider the database in Table 1.

Each record is a <TID, Itemset> pair, where TID is the identifier of the corresponding transaction, and Itemset records the items purchased in the transac- tion.

Table 1: A database of transactions  Assume that the minimum support is 2 transac- tions. In the large 1-itemset generation phase, the large items found in the database shown in Table 1 are items 1, 2 ,  3 and 5, and BVl, BV2, BV3 and SV, are ( lolo) ,  (Olll), (1110) and (Olll), respectively.

Property 1. The support for the itemset { i l , i z , ..., i k }  is the number of 1?s in BV,, A BV,, A ... A B x , , where the notation ?A? is a logical AND operation.

After the first phase, the database need not be scanned again. In the graph construction phase, DLG constructs an association graph to indicate the asso- ciations between items. For the association graph, if the number of 1?s in BV, A BV, (i < j )  is no less than the minimum support, a directed edge from item i to item j is constructed. Also, itemset { i , j }  is a large 2-itemset. The association graph for the above exam- ple is shown in Figure 1, and the large 2-itemsets are {1,3}, (2~31, {2,5} and { 3 , 5 } .

(1010)  I I  (0111) 2 -/ 5 (0111)  (1 110)  Figure 1: The association graph and the bit vector associated with each large item for Table 1  3.2 Large itemset generation The large k-itemsets (k > 2) are generated based on  the association graph constructed in the second phase.

The data structure used to  implement the association graph is a linked list.

The large 2-itemsets La is found in the graph con- struction phase. In the large itemset generation phase, the DLG algorithm generates large k-itemsets LI,  ( k  > 2). For each large k-itemset in LI,  (k 2 a), the last item of the k-itemset is used to extend the itemset into k + 1-itemsets. Suppose { i I , i Z ,  ..., i k }  is a large k-itemset. If there is a directed edge from item i k to item U ,  then the itemset { i l ,  i a ,  ..., i k }  is extended into k + 1-itemset { i l , i 2 ,  ..., i k , ~ } .  The itemset { i l , i ?> .,,,i~,u} is a large k +  1-itemset if the number of 1?s in BV,, ABV,, A ... A BV,, A BV, is no less than the mini- mum support. If no large IC-itemsets can be generated, the DLG algorithm terminates.

For example, consider the database in Table 1. In the second phase, the large 2-itemsets L2 = {{1,3},  2,3 , {2,5}, {3,5}} is generated. For large 2-itemset t l  2,3 , there is a directed edge from the last item 3 of the itemset {2,3} to item 5 .  Hence, the 2-itemset {2,3} is extended into 3-itemset {2,3,5}. The number of 1?s in BV2ABV3ABV5 (i.e., (0110)) is 2. Hence, the     3-itemset {2,3,5} is a large 3-itemset, since the num- ber of 1's in its bit vector is no less than the minimum support. The DLG algorithm terminates because no large 4-itemsets can be further generated. After com- pleting the DLG algorithm on Table 1, the large item- sets are {1,3}, {2,3}, {2,5}, {3,5} and {2,3,5}. The DLG algorithm for each phase is shown as follows: \* Large l-itemset generation phase *\ forall items i do  f o r ( j = l ; j < N ; j + + )  dobegin \* N is the number of transactions in database D *\  set all bits of SV, to 0;  forall items i in the j t h  transaction do begin i.count++; set the j t h  bit of BK to 1;  end end  forall items i in database D do begin L1 = 4;  if i.count 2 minsup then \* minsup is the minimum support threshold *\  L1 = L1 U {ij; end  \* Graph construction phase *\ if L1 # 4 then begin  forall large l-itemsets 1 E L1 do  Lz = 4 for every two large items i, j ( i  < j )  do begin  allocate a node for Item[l] and Item[l].link=NULL;  if (the number of 1's in SV, A By) 2 minsup then begin \* create an directed edge from i to j in the association graph *\  allocate a node p; p.linle = Item[l,].link; p.Item = l b ; Item[l,].link = p ;  \* generate large 2-itemsets *\ L2 = L2U {{i,j} j ;  end end  end  \* Large itemset generation phase *\ le = 2; while Lk # 4 do begin  Lk+1 7 4; forall itemsets (il i z  ... i k )  E Lk do begin  pointer = Item[ik].linle; while pointer # NULL do begin  U = pointer.Item; if (number of 1's in Bx, A ... A B K L ASV,) 2 minsup then  L k + l  = Llc+lu {{ i l  ,..., & , U } } ; pointer = pointer.link;  end end k = k + l  end  3.3 Experimental results To assess the performance of the DLG algorithm  for large itemset generation] we perform several ex- periments on Sun SPARC/10 workstation. The exper- iments show that the DLG algorithm is very efficient for large itemset generation] because it takes only one database scan to generate large itemsets. We first de- scribe how the datasets are generated for the perfor- mance evaluation. We then compare the performance of DLG and DHP [16] by performing experiments on the generated datasets. Finally, we demonstrate the scale-up properties of the DLG algorithm.

3.3.1 generation of synthetic data  The synthetic database of sales transactions is gener- ated to  evaluate the performance of the algorithms.

The method to generate synthetic transactions is sim- ilar to the one used in [4]. The parameters used in our experiments are shown in Table 2.

Average size of the potentially large itemsets  Number of large itemsets Average size of the transactions  Maximum size of the transactions  Table 2: The parameters  We first generate a set L of the potentially large itemsets, and then assign a large itemset picked up from L to a transaction. The size of each potentially large itemset is between 1 and \ M I \ .  The probabil- ities for sizes 1, 2 ,  ... and \ M I ]  are obtained by a Possion distribution with mean equal to 111. These probabilities are normalized such that the sum of these probabilities is 1. For example, suppose average size III of the large itemsets is 3 and maximum size [MI1 of the large itemsets is 5. According to the Possion distribution with mean 111 the probabilities for sizes 1, 2, 3, 4 and 5 are 0.17, 0.26. 0.26, 0.19 and 0.12, respectively, after the normalization process. These probabilities are then accumulated such that each size falls in a range, which is shown in Table 3. For each potentially large itemset, we generate a random real number which is between 0 and 1 to determine the size of the potentially large itemset.

I f I 0.18 - 0.43 1 0.44 - 0.69  Table 3: The probabilities for the sizes of itemsets  The number of the potentially large itemsets in L is set to ILI. Items in the first large itemset are cho- sen randomly. Some fraction of items in subsequent     large itemsets are chosen from the previously gener- ated large itemset. For each item in the previous large itemset, we flip a coin to decide whether the item will be retained in the current large itemset. The remain- ing items in the large itemset are picked at random.

After generating the set L of large itemsets, we then generate transactions in the database. The size of each transaction is picked from a Poisson distribution with mean equal to ITI, and the size is between 1 and IMTI.

The method to determine the size of a transaction is the same as the method to determine the size of a large itemset. For a transaction, we randomly choose a large itemset from L to fit in the transaction and assign it to the transaction. The remaining items of the first transaction are chosen randomly. The frac- tion of the remaining items of the subsequent transac- tion are chosen from the previously generated trans- action. For each item in the previous transaction, we also flip a coin to decide whether the item is retained in the transaction. After choosing the items from a large itemset and from the previous transaction, the remaining items in the transaction are picked at ran- dom. The same as [4], we also use a corruption level during the transaction generation to model the phe- nomenon that all the items in a large itemset are not always bought together. Each transaction is stored in a file system with the form of <transaction identifier, the number of items, items>.

We generate datasets by setting N = 1000 and IL( = 2000. We choose three values for ITI: 5, 10 and 20, and the corresponding lMTl = 10, 20 and 40, respectively. We choose two values for (11: 3 and 5, and the corresponding (MI1  = 5 and 10, respectively. The number ID1 of transac- tions is set to 100,000. We use T a . M T z . l b . M I y to mean that a = ITI, z = IMTI, b = 111 and y = / M I / .  We generate the following datasets for the experiments: T5. M T  10 . I 3  . M l 5 ,  T 10 .MT20 . I3  . M l 5 , T1O.MT20.15.Ml10 and T20.MT40. I3.M I5 .

JLlI(ILi -1 I logical AND operations on bit vectors to construct association graph and generate large 2- itemsets. DHP needs to generate candidate 2-itemsets and prune these candidate 2-itemsets using the hash table created in the first pass. Besides, DHP needs to scan database to count support for candidate 2- itemsets and trim the database DB to generate a re- duced database. These jobs needed by DHP are more costly than these logical operations performed by DLG in the second pass.

In the kth ( k  > 2) pass, DLG extends each large k - 1-itemset into k-itemsets according to  the asso- ciation graph and performs logical AND operations.

Suppose on the average, each node (item) has Q out- degrees in the association graph. DLG performs ( k  - 1) x ( L I , - ~ (  x q logical AND operations to find all large k-itemsets. Hence, as the minimum support decreases, the number of logical AND operations per- formed increases because the two values ILk-11  and q increase. In the kth pass, DHP generates candidate k-itemsets Gk from large k - 1-itemsets L k - 1 .  Af- ter generating Ck, DHP scans each transaction in the database DBI,  to count supports for these candidate k-itemsets and trim the database DBk to generate an- other reduced database D B I , + ~ .  Hence, the execution time of DHP depends on the number of generated can- didate itemsets and the amount of data that has to be scanned.

3.3.2  Figure 2 shows the relative execution time for DHP [16] and DLG, using the four synthetic datasets de- scribed in Section 3.3.1. In these experiments, the hash table size lHal used in DHP is set to $ x C y , which was found to have better overall performance in [ls], where N is the number of items. Suppose there are ID1 transactions in database D B  and m items in each transaction on the average. In the kth pass, the large k-itemsets LI,  is generated. For the first pass, DLG and DHP both need to scan each transaction in DB to count support for each item. By the way, DLG records the bit vectors for each item. However, DHP needs to take extra overhead to combine every two items to form a 2-itemset in each transaction. Totally, there are /Dl x CF combinations needed. For each combination, DHP uses the hash function to locate the 2-itemset in the hash table. Hence, DHP takes much more time than DLG in the first pass.

Suppose there are l L k l  large itemsets generated in the kth pass. In the second pass, DLG performs  comparison of DLG and DHP Table 4: Comparisons of DLG and DHP  We perform an exper- iment on dataset TlO.MT20.15.MIlO with minimum support 0.75%. The experimental results are shown in Table 4, where MI,  denotes the number of transac- tions in DBk,  and mk denotes the number of items in each transaction on the average.

In this experiment, there are 238 nodes and 687 edges in the association graph. Hence, on the average, the out-degrees ofeach node is 3.  Table 4 shows that in each pass, the number of logical AND operations per- formed by DLG is much less than the size of database scanned and the number of candidate itemsets gen- erated by DHP. Hence, DHP takes much more time than DLG for large itemset generation. Figure 2 shows that the DLG algorithm outperforms the DHP algo- rithm significantly, and the performance gap increases as the minimum support decreases because the num- ber of candidate itemsets and the number of database scans increases for DHP.

0.5 1 1.5 2 2.5 3 3.5 Mini" Support l % l  Figure 2: Relative Execution Time   0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  T5.MT10.13.MI5 + TlO.MT20.13.MI5 -+-  TI0 .MTZO .I5 .MI10 D--  ,.B'  100 200 300 400 500 600 700 800 900 1000 Number of Transactions (in '000s)  Figure 3:  Scale-up: Number of Transactions  3.3.3 discussions for DLG  The memory space needed for performing DLG is dominated by the bit vectors. Since the len th of each bit vector is the number of transactions fD1 in D B ,  there are N x ID1 bits needed, where N is the number of items. In our experiments, N = lo3 and ID1 = lo5. Hence, lo3 x lo5 bits (12.5MB) are needed to store all bit vectors.

Figure 3 shows how DLG scales up as the number of transactions is increased from 10,000 to 100,000 trans- actions. We use the three datasets T5.MT10.13.MI5, TlO.MT20.13.MI5 and TlO.MT20.15.MI10, and set the minimum support to  1%. As shown, the execu- tion times of DLG increase linearly as the database sizes increase, because the number of large itemsets increases.

Next, we examine how DLG scales up as the num-  T5.MT10.13.MI5 + TlO.MT20.13.MI5 +--  T10.MT2O.I5.MI10 0-  0.1 0.2 1000 k z l = = - l  2000 3000 4000 5000 6000 7000 8000 9000 10000 Number of Itemsets  Figure 4: Scale-up: Number of Items  ber of items increases from 100,000 to 1,000,000 for the three datasets T5.MTlO.I3.MI5, TlO.MT20.13.MI5 and TlO.MT20.15.MIlO. The minimum support is set to 1% for this experiment, and the results are shown in Figure 4. The execution times decrease slightly, because the number of large itemsets de- creases as we increase the number of items.

4 Sequential Pattern Discovery  In this section, we present the algorithm DSG for ef- ficient sequential pattern generation. In [5], the prob- lem of mining sequential patterns is splitted into the following phases: 1. Sort phase, 2.  Large itemset phase, 3. Transformation phase, 4. Sequence phase and 5. Maximal phase. The Sort phase is to con- vert the original transaction database into a database of customer-sequences. The customer-sequence is a list of itemsets which are ordered by increasing transaction-times. The Large itemset phase is to find all large itemsets (or large 1-sequences). The Transfor- mation phase is to transform each original customer- sequence into a transformed customer-sequence which is an ordered list of large itemsets.

The Sequence phase and the Maximal phase are the main portions for mining sequential patterns. In these two phases, we propose an algorithm DSG to generate sequential patterns, which needs only one database scan. The DSG algorithm is also splitted into two phases: The first phase is the graph constructzon phase which constructs an association graph to indi- cate the associations between large itemsets (or large 1-sequences) and records related information. In this phase, large 2-sequences can also be generated. The second phase is the sequentaal pattern generataon phase which generates large k-sequences (k > 2) based on the constructed association graph and finds maximal large sequences (or sequential patterns).

A 4.1 Association graph construction After completing the Transformation phase, we are  given a database of transformed customer-sequences.

In the graph construction phase, DSG algorithm scans each customer-sequence in the database to combine every two large itemsets to  generate a 2-sequence and count support for the 2-sequence. For each Zsequence, the set of identifiers of the customer-sequences where the 2-sequence appears is recorded. When the sup- port for a 2-sequence achieves the minimum support threshold] the DSG algorithm creates a directed edge from the first itemset to  the second itemset in the 2- sequence.

In the following, 3, denotes the set of customer identifiers of the customer-sequences where sequence s appears. The cardinality of 3s is equal to the number of customer-sequences which support the sequence s , that is, the support for the sequence s.

C1L) I Csequence 1 I ABCD  Table 5: A database of customer-sequences  For example, Table 5 is a database of customer- sequences after completing the Transformation phase.

Each record is a <CID, Csequence> pair, where CID is the customer identifier of the corresponding customer- sequence] and Csequence is the customer-sequence.

Csequence is a list of large itemsets which are ordered by increasing transaction-times. A large itemset is de- noted by an alphabet.

Assume the minimum support is 2 customer- sequences. After scanning the database of customer- sequences in Table 5, the association graph and the recorded information are shown in Figure 5, where the set of numbers on each edge X Y  is the set of customer identifiers of the customer-sequences where the large 2-sequence < X , Y  > appears. After completing the graph construction phase, the large 2-sequences are  <CID> and <C,E>, and the set S<A,B> of cus- tomer identifiers of the customer-sequences where the 2-sequence <A,B> appears is (1, 3}, and so on.

4.2 Sequential pattern generation In this section, we describe how to generate large  k-sequence ( k  > 2) based on the association graph and the recorded information] and further to find se- quential patterns. The large 2-sequences LS2 is found in the graph construction phase. In the sequential pattern generation phase, the DSG algorithm gener- ates large k-sequences LSk ( k  > 2). For each large k-sequence in LSk ( k  2 a), the last itemset of the k-sequence is used to extend the sequence into k + 1- sequences.

i  <A,B>, <A,C>, <A,D>, <B,D>, <B,E>, <C,B>,  D  Figure 5: The association graph and the set of iden- tifiers of the customer-sequences where each large 2- sequence appears for Table 5  Property 2. The support for the k-sequence < SI,  s 2 ,  ..., Sk > is the cardinality of the set 3<sl,sz> fl s < s , , s , >  n ... n s < s k - - l , s * > .

Suppose < SI ,  s a 1  ...] s k  > is a large k-sequence. If there is a directed edge from itemset s k  to itemset U , the sequence < s1 sa, ..., sk > is extended into k + 1- sequence < SI., s2, ..., s k ,  v >. The k + 1-sequence < SI,  s2, ..., Sk, U > is a large k+l-sequence if the support for the k + 1-sequence is no less than the minimum support. If no large k-sequence can be generated, the DSG algorithm terminates.

After finding all large sequences LS, the large se- quences which are subsequences of the other large se- quences are deleted from LS.  The remaining large sequences are maximal large sequences, that is, se- quential patterns.

For example, consider the database in Table 5 and the association graph in Figure 5. For large 2-sequence <C,B>, there is a directed edge from the last itemset B of the sequence <C,B> to itemset E. Hence, the 2-sequence <C,B> can be extended into 3-sequence <C,B,E>, and the set S<C,B,E> can be obtained by performing set intersection on sets S<C,B> and  set S<B,E> is {2,5}, the set S.<C,B,E> is {2,5}. The 3- sequence <CIBIE> is a large 3-sequence, since the car- dinality of set ~ < c , B , E >  is no less than the minimum support. After completing the DSG algorithm on Ta- ble 5, the sequential patterns are <A,B>, <AIC>, <A,D>, <B,D>, <CID> and <C,B,E>. The DSG algorithm for each phase is shown as follows:  \* Graph construction phase *\ LS1 = { large 1-sequences } \* result of the Large itemset phase *\ if LS1 # 4 then begin  forall large 1-sequences 1 E LS1 do  c c  \  c s<B,E>. Because the set S<C,B> is {2,3,5} and the  allocate a node for I temset[ /]  and Itemset[l] . link=NULL  forall permutation 1,1, where 1, and E, are selected from LS1 do  S<I,,I,> = 4 \* 3 < l z , l y >  records the set of identifiers of the customer-sequences where I ,  I, appears *\     LS2 = (b for (i = 1; i 5 N ;  i + +) do begin \* N is the number of customer-sequences in database D *\  scan the ith customer-sequence c I ,  = the set of all itemsets in c forall combination l akb  do begin \* 1, and l b  are selected from I ,  *\  c s < l a , l b >  = I m < r a , l b >  {i) \* record related information *\ if < l a ,  / b  > .count 5 minsup then  if < I,, l b  > .count = minsup then begin < l a , l b  > .count + +  \* generate large %sequences *\ CreateEdge(l,, j b ) \* create an edge from 1, to / b in the association graph *\  Ls2=Ls2u{<l , ,bb  >}  end end  end end CreateEdge(l,, l b )  allocate a node p p.link = Item[l,].link p.Item = l b Item[l,].link = p  \* Sequential pattern generation phase *\ k = 2 while Lsk # 4 do begin  LSk+l = 4 forall sequences < SI, s2, ..., sk >E Lsk do begin  pointer = Itemset[sk].link while pointer # NULL do begin  v = pointer.Itemset if (the cardinality of set 3<sl,sz>n  pointer = pointerlink  c a<,,,,,> n ... n 3<sk,su>) 2 minsup then LSk+l = LSk+l U {< s1, sa, '", Sk, v >>  end end k = k + 1  end Answer = Maximal sequences in U k  LSk  4.3 Experimental results  To evaluate the performance of the DSG algorithm for sequential pattern generation, we also perform sev- eral experiments on Sun SPARC/10 workstation. The experiments show that the DSG algorithm is very ef- ficient for sequential pattern generation, because it takes one database scan to construct an association graph and the large sequences are generated based on the association graph directly. We first generate datasets for the experiments, and then compare the performance between DSG and AprioriAll by perform- ing experiments on the generated datasets. The scale- up properties of the DSG algorithm are also demon- strated.

4.3.1 generation of synthetic data  The method to generate synthetic datasets is sim- ilar to  the one used in DLG algorithm. The dif- ference between the two methods is described be- low. For the generated dataset used in DLG algo- rithm, the items in each transaction are generated in their lexicographic order. However, for the generated dataset used in DSG algorithm, the itemsets in each customer-sequence are generated in an arbitrary or- der. Each transaction is stored in a file system with the form of <customer-sequence identifier, the num- ber of itemsets, itemsets>. The parameters used in the experiments are as shown in Table 2 with some modifications. In Table 2, the term "transactions" is changed to " customer-sequences," the term "item- sets" is changed to "sequences," the term "items" is changed to "itemsets," and the notation " L",  "T" and "MT" are changed to "LS," "C" and "MC,"  respec- tively. The number [Dl of customer-sequences is set to  100,000. We also set N =lo00 and lLSl =2000 for the generated datasets, and generate the four datasets: C5.MC10.13.MI5, C1O.MC20.13.MI5, ClO.MC20.15.MI10 and C20.MC40.I3.MI5 in the experiments.

4.3.2  Figure 6 shows the relative execution time for Aprio- riAll algorithm [5] and DSG algorithm over various minimum supports, ranging from 0.5% to 3.5%.

Suppose there are M customer-sequences in the database and m itemsets in each customer-sequence on the average. In the kth pass, the set of large k- sequences Lsk is generated.

In the second pass, AprioriAll uses LS1 to gen- erate 2 x CFsl' candidate 2-sequences CS2. More- over, AprioriAll scans the database to combine every two sequences to form a 2-sequence in each customer- sequence. Totally, there are M x CF combinations needed. For each combination, AprioriAll searches for the candidate 2-sequences in CS2 to determine whether the combination is in CS2 for large 2-sequence generation. However, when ILSlI is large, 2 x Crsl' becomes an extremely large number. It is very costly to determine large 2-sequences from a large number of candidate 2-sequences. In this pass, DSG scans each customer-sequence in the database to combine every two sequences to form a 2-sequence and count sup- port for the 2-sequence to determine whether the 2- sequence is large. Because AprioriAll needs to search for a large amount of candidate 2-sequences, DSG out- performs AprioriAll in this pass.

In the kth pass (k > a) ,  AprioriAll generates candi- date k-sequences based on large k- 1-sequences LSk-1 and scans the database to  count supports for the can- didate k-sequences for large k-sequence generation.

AprioriAll needs to  combine every k sequences to form a k-sequence in each customer-sequence, and totally, M x Cp combinations are needed. For each combina- tion, AprioriAll searches for candidate k-sequences in CSk to determine whether the k-sequence is in CSk for  comparison of AprioriAll and DSG     0 . 5  1 1 5  2 2.5 3 3.5 Minimum Support 1%)  Figure 6: Relative Execution Times  large k-sequence generation. Hence, as the minimum support decreases, the execution time of AprioriAll in- creases because the candidate sequence generated in- creases and the number of database scans increases.

For DSG algorithm, the large k-sequences (k > 2) can be generated by extending large k - 1-sequences into k-sequences based on the association graph and performing set intersections on the related informa- tion. Suppose on the average, the number of out- degrees of each node itemset) in the association raph is q .  In the kth i k > 2) pass, DSG performs  A-1) x q x ILSk-1 I set intersections to find all large k- sequences. Hence, as the minimum support decreases, the number of set intersections performed increases, because the values q and ILSk-11 increases. However, DSG need not generate candidate k-sequences (k 2 1) nor scan the database for large k-sequence generation  Since the number of set intersections performed for DSG is much less than the size of database scanned and the number of candidate itemsets generated for AprioriAll, DSG outperforms the AprioriAll for var- ious minimum supports. Figure 6 shows that the performance gap increases as the minimum support decreases because the number of candidate itemsets generated by AprioriAll increases and the number of database scans also increases.

( k  5 2).

4.3.3 discussions for DSG  The main memory space needed for performing DSG is to store customer identifiers on each edge in the association graph. Suppose there are 1 edges in the association graph and on the average, the cardinality of the set of customer identifiers on each edge is k .

1 x k customer identifiers need to be stored.

Figure 7 shows how DSG scales up as the num- ber of customer-sequences increases from 10,000 to 100,000 customer-sequences. We use the three datasets C5.MC10.13.MI5, ClO.MC20.13.MI5 and  I 100 200 300 400 500 600 700 eoo 900  1000  Number of Customers (in ' O O O s i  Figure 7: Scale-up: Number of Customers  0.3 ' I 1000 2000 3000 4000 5000  6000 7000  8000 9000  10000  Number of Itemsets  Figure 8: Scale-up: Number of Itemsets  ClO.MC20.15.MI10, and set the minimum support to 1.5%. As shown, the execution time of DSG in- creases linearly as the database size increases, because the number of large sequences increases.

Next, we investigate the scale up as we increase the number of itemsets from 1,000 to 10,000 for the three datasets C5.MCIO.I3.MI5, CIO.MC20.13.MI5 and ClO.MC20.15.M110. The minimum support is set to 1.5% for this experiment. Figure 8 shows the results.

When the number of itemsets increases, the execution time decreases slightly, because the number of large sequences decreases.

5 Conclusion and Future Work We study two problems: mining association rules  and mining sequential patterns in a large database of customer transactions. The problems of mining asso- ciation rules and mining sequential patterns focuses     on discovering large itemsets and discovering large se- quences, respectively.

We present two algorithms, DLG and DSG which need only one database scan, for efficient large itemset generation and efficient sequential pattern generation, respectively. These two algorithms construct an as- sociation graph to indicate the associations between items and then traverse the graph to generate large itemsets and large sequences, respectively.

We compare DLG and DSG algorithms to the pre- viously known algorithms, DHP [16] and AprioriAll [5] , respectively. The experimental results show that DLG and DSG outperform DHP and AprioriAll, re- spectively. When the minimum support decreases, the performance gap increases because the number of candidate itemsets (candidate sequences) generated by DHP (AprioriAll) increases and the number of database scans also increases.

We demonstrate that the execution time of these two algorithms increases linearly as the database size increases, and the execution time decreases slightly as the number of items (itemsets) increases.

For our graph-based approach, the related informa- tion may not fit in the main memory when the size of the database is very large. In the future, we shall de- velop a mining algorithm based on our graph-based approach, such that in a very large database environ- ment, the mining algorithm can also be run in the main memory. We shall consider mining various dif- ferent relationships among data in a large database of customer transactions, such as is-a relationships and part-of relationships. We shall also apply our graph- based approach on different applications, such as doc- ument retrieval and resource discovery.

