Quantitative Association Rules Based on Half-Spaces: An Optimization Approach

Abstract  We tackle the problem of finding association rules for quantitative data. Whereas most of the previous approaches operate on hyperrectangles, we propose a representation based on half-spaces. Consequently, the left-hand side and right-hand side of an association rule does not contain a conjunction of items or intervals, but a weighted sum of variables tested against a threshold. Since the downward closure property does not hold for such rules, we propose an optimization setting for finding locally optimal rules. A simple gradient descent algorithm optimizes a parameter- ized score function, where iterations optimizing the first sep- arating hyperplane alternate with iterations optimizing the second. Experiments with two real-world data sets show that the approach finds non-random patterns and scales up well. We therefore propose quantitative association rules based on half-spaces as an interesting new class of patterns with a high potential for applications.

1 Introduction  Soon after the introduction of association rules for item- sets, researchers began to realize that association rules would also be useful for quantitative data [5]. Most of the generalizations and extensions of association rules to quan- titative data either require a discretization of the numerical attributes [5, 7] or a characterization of the numerical at- tributes in the right-hand side by their means and standard deviations [1, 6].

The discretization process, however, leads to a loss of information in the data set. In the following we present a novel approach that works directly on the continuous data, without the need for any discretization or the calculation of statistical moments. It derives quantitative association rules  of the form ?if the weighted sum of some variables is greater than a threshold, then a different weighted sum of variables is with high probability greater than a second threshold?.

For instance, consider a table with wind strength, temper- ature and the wind chill index. Approaches so far applied to this data would approximate the relationship among the variables by a bundle of quantitative association rules. In contrast, the approach proposed here would find a weighted sum of wind strength and temperature on the left-hand side and the wind-chill index on the right-hand side. Thus, it allows for the discovery of non-axis-parallel regularities and can account for cumulative effects of several variables.

Since the downward closure property frequently used in conventional association rule mining does not hold for this type of rule, we cast the problem of finding such rules as an optimization problem. The aim is to find rules that are lo- cally optimal with respect to a parameterized score function.

Consequently, the user can adjust the parameters of the pre- sented algorithm to obtain association rules that match her individual interests. For instance, it is possible to specify target values for certain parameters, such that the algorithm attempts to find rules near the target (penalizing rules that are too far off), while simultaneously optimizing the rules? confidence. The whole framework is very flexible in sev- eral directions and can easily be adapted to incorporate user constraints. In summary, the paper has two main contribu- tions: firstly, the representation of quantitative association rules based on half spaces, and secondly, the optimization setting for finding such rules.

2 Quantitative Association Rules Based on Half-Spaces  As outlined above, the aim of this paper is to extend the association rule framework to quantitative data. In general, an association rule is an implication of the form ?if the left-  0-7695-2142-8/04 $ 20.00 IEEE    2 0 2 4          (a) 2 0 2 4           (b)  Figure 1. Two non-perpendicular hyperplanes ? and ? (a), and two perpendicular hyper- planes ? and ? (b).

hand side condition is true for an instance, then, with high probability, a right-hand side condition is also true?. In the traditional setting, the conditions on the right-hand side and left-hand side are based on hyperrectangles of discrete at- tributes. To extend association rules to continuous data, we therefore need to decide which kind of ?conditions? the quantitative association rules should be based on.

Of course, there are lots of different ways to impose con- ditions on numerical data. At the core we would expect from a useful condition that it separates the instance space in two subspaces, the space of instances that meets the con- dition, and the one that does not. The border between those two subspaces can then be conveniently expressed by some separation function. For numerical data, it makes sense to select a smooth separation function to minimize the error that is caused by random noise or measurement errors in the data. In this paper we will focus on hyperplanes, a par- ticularly simple, but powerful class of separation functions.

From a geometrical perspective, a hyperplane ? is given by a vector ?? and an intercept ?0. An instance x is then as- signed to one half-space, if the dot product ?? ? x + ?0 is positive and to the other half-space, if it is negative. In fig- ure 1 (b), the one-dimensional hyperplane ? (i.e. a line) separates the two-dimensional space into two half-spaces, one left of ?, the other right of ?.

In the case of association rules, the use of hyperplanes as conditions boils down to testing a weighted sum of variables against a threshold; i.e. an instance x in an n-dimensional space meets the condition ? ? ?n+1, if ?1x1 + ?2x2 + ? ? ? + ?nxn ? ??0 With this, one could build an associ- ation rule such as x1 ? 31 ? 0.9x5 + 1.2x6 ? 250. In a particular medical application this association rule might be interpreted as ?if the body mass index is greater than or equal to 31, then the weighted sum of the systolic and dias- tolic blood pressure is greater than or equal to 250?.

Of course, it is quite easy to generate a large number of trivial association rules with high confidence. For example,  the association rule 1.5x1 ? 5 ? 2x1 ? 4 has confidence 100%, but does not give any new insight. More generally, situations like the one in figure 1 (a) are problematic: we have two hyperplanes ? and ? in a two-dimensional space, that define an association rule ?1x1 + ?2x2 ? ??0 ? ?1x1 + ?2x2 ? ??0. The problem is that ? and ? are highly correlated. If an instance is left of the ? hyperplane, it is very likely to be left of the ? hyperplane as well, sim- ply because the space that is right of ?, but left of ? is much smaller than the space left of ? and left of ?1. For our pur- poses it is therefore essential, that ? and ? are uncorrelated, i.e. they have to be perpendicular as in figure 1 (b).

3 Quantitative Association Rule Mining  The main problem with finding good quantitative asso- ciation rules is that the space of rules is uncountably infi- nite and therefore not suited to an enumeration strategy. In particular, the downward closure property does not hold for such rules, and thus we have to abandon the idea of generat- ing the complete set of solutions. However, we can adopt an optimization approach, where the user can specify clearly the sort of rules she is looking for, and the algorithm returns locally optimal solutions. While this may seem unusual for association rule mining, it is common practice in other ar- eas, for instance clustering (e.g, K-means clustering) and Bayesian learning (e.g., the EM algorithm).

In the following we describe one particular algorithm for mining quantitative association rules in this setting. First, we define a score function to assess the ?interestingness? of an association rule. Then, we sketch a simple optimization algorithm searching for association rules with a low score.

Before we go into further detail, however, we need to intro- duce the basic setting and some notational conventions.

For mining quantitative association rules we are given a data set X containing m instances. Each instance is given as a vector of n real values, i.e. x ? ?n, so that X ? ?n. We are now looking for association rules that are defined by two hyperplanes ? := (?0, ?1, . . . , ?n)T and ? := (?0, ?1, . . . , ?n)T . The ? hyperplane specifies the condition on the left-hand side of the association rule, the ? hyperplane specifies the right-hand side. Both hyperplanes are given in Hessian normal form: the ?0 value of a hyper- plane ? is the intercept, i.e. the hyperplane?s distance to the origin. The direction vector ?? := (?1, . . . , ?n)T specifies the slope of the hyperplane. The hyperplanes ? and ? are perpendicular, if ??T ?? = 0.

Due to space constraints we are not able to give the exact scoring function and the algorithm here and have to refer to  1Of course, in a strict mathematical sense it does not make sense to compare the ?sizes? of subspaces, because all subspaces are infinite any- way. A more formal justification would demand that the resulting proba- bility distributions are independent for uniform data.

0-7695-2142-8/04 $ 20.00 IEEE    2 0 2 4          -  +  Figure 2. Confidence is optimal if the distri- bution is uneven left of ?, while contrast is optimal if it is even right of ?.

the long version of the paper [4] for details.

First of all, we are mainly interested in association rules  with high confidence, i.e. the fraction of instances in X , that fulfill both conditions ? and ? divided by the fraction of instances that fulfill only the ? condition should be as high as possible. Figure 2 illustrates this idea: If an instance x is located left of ? and below ?, it contributes to a high confidence score. If it is located left of ?, but above ?, it decreases the confidence measure. The following function is minimal for high confidence:  l(?, ?, X) := ? ?  x?X ?(?(?, x)) ? (2?(?(?, x)) ? 1)  where ?(x) := 11+e?x is the well-known sigmoid function and ?(?, x) is the signed distance of instance x to the hy- perplane ?. Since we use the sigmoid function instead of the sharp step function, l is differentiable and puts lower weights to instances in the vicinity of the hyperplane, there- with compensating for noise in the data.

A second criterion for the interestingness of an associ- ation rule is its coverage. The coverage is simply the frac- tion of instances in the data set that satisfy the left-hand side condition. Unfortunately, the coverage of interesting rules is not clear a priori. If the coverage of an association rule is very large, the rule is true for almost the whole data set.

Such rules often express trivial dependencies in the data.

On the other hand, if the coverage of a rule is very small, the pattern describes a very local phenomenon that might just be a random fluctuation instead of a structural prop- erty of the underlying data. Thus, the coverage values for interesting association rules are somewhere in between, de- pending on the data at hand and the knowledge about the data. In practice, the desired coverage (or equivalently, the support) is often determined empirically. Similarly to the confidence score, we design a function c(?, g, t, X) that is minimal if the coverage of ? is near to the user-specified target value t. The g parameter determines how coverage should be weighted relative to confidence.

The confidence and coverage scores determine what the  optimization algorithm is looking for on the left side of ? in figure 2. Just like in traditional association rule mining, there is no constraint regulating the distribution of instances on the right side of ?. For quantitative association rule min- ing, this can be a problem: one can simply move the ? hyperplane upwards until it is located above all instances.

While this achieves maximal confidence, the resulting asso- ciation rule is not very interesting, because the right-hand side condition is true for all instances anyway. One way to overcome this problem is to regulate the distribution of in- stances that are right of ? with regard to ?. We call this cri- terion contrast. The rationale is that the ?contrast? between the instance above and below ? should be as low as possi- ble on the right side of ? in figure 2. Again, we formalize this criterion using a function r(?, ?, X), that is minimal for low contrast settings.

For humans who have to interpret the resulting associa- tion rules, there is one more pragmatic criterion: the com- ponents of the ? and ? vectors are usually not zero. This means that the resulting association rule contains n addends on both sides of the implication. Usually, the user prefers finding sparse association rules, i.e. rules where most coef- ficients are zero and only the relevant coefficients are given.

Those rules are shorter and thus easier to interpret and val- idate. To account for these pragmatic considerations, one can add a term a(?, h) to penalize non-sparse association rules. The user can adjust the importance of sparsity rela- tive to the other scores using the parameter h.

The final interestingness scoring function simply calcu- lates the sum of those scores:  L(?, ?, g, t, h,X) := l(?, ?, X) + c(?, g, t, X) + r(?, ?, X) + a(?, h) + a(?, h)  A high score indicates that the association rule is uninter- esting with regard to the selected parameter settings, a low score means we found an interesting rule. As the scoring function is continuous, there usually is a whole subspace of ?good? rules and it is easy to modify a rule with a low score to some small extent and obtain a rule with an even lower score. We are therefore aiming at finding association rules with optimally low score, that is, the local optima of the scoring function, subject to the constraint that ??T ?? = 0.

This constrained optimization problem can be tackled using established methods from optimization theory. We take an approach that alternatingly keeps ? fixed while optimizing ? and vice versa. Empirical results in section 4 indicate that only a few iterations are sufficient to find such an optimum.

As any other optimization procedure, this algorithm can get stuck in local optima with comparably high scores. For the sake of simplicity we use random restarts to obtain as- sociation rules with low score. Of course, one can utilize simulated annealing or any other global optimization strat- egy as well. A high sparseness parameter leads to rules that  0-7695-2142-8/04 $ 20.00 IEEE    150 100 50 0 50        Score  Figure 3. Distribution of scores on the original (black) and the permuted (grey) data sets.

have a few large and many small (but non-zero) coefficients.

A post-processing step is required to set the small coeffi- cients to zero while retaining the perpendicularity of ? and ?.

4 Experimental Results  To assess the applicability and feasibility of the de- scribed algorithm, we implemented a version in MATLAB.

For our first experiment, we chose the gene expression data set of Hughes et al. [3]. The data set contains the ex- pression levels of 6316 genes in the yeast genome measured for 300 diverse mutations and chemical treatments of yeast cells. We selected the 50 genes with the largest standard deviation for our experiments. The parameters were set as follows: g = 1.0, t = 0.5, and the sparseness parameter h was set to 0.1, 0.3 and 0.5, respectively.

As expected, contrast and coverage were centered around the target value of 0.5 in the experiments. In the long version of the paper [4], a biological interpretation of five of the best rules is given. To assess the robustness and statistical significance of the rules, we performed a random- ization test, where the values of the columns are permutated randomly to generate a new data set with the same distribu- tion, but no structural relations between the columns. We then ran the algorithm ten times on the permuted data set and noted the best score found. This process was repeated one hundred times to get an estimate of the distribution of scores, that can be expected on random, but similar data.

Figure 3 gives the resulting histograms for the original and the permuted data. The scores for the permuted data are peaked around -30, while the original data features a large number of association rules in the range between -50 and -150. Thus, we can be highly confident that the induced rules describe indeed structural properties of the yeast data set. In practical applications, we would recommend this randomization approach to focus on significant findings.

In order to investigate the scalability of the optimization algorithm with respect to the size of the data set, we exper-  Data Set Overall Number of Runtime per Size Runtime Line Searches Line Search 100 2.6 s 19 0.14 s  1,000 7.4 s 27 0.28 s 5,000 23 s 22 1.0 s 10,000 46.9 s 17 2.7 s 50,000 264 s 18 14.7 s  100,000 623 s 23 27.1 s 300,000 2004 s 23 87.2 s 500,000 3529 s 20 176.5 s  Table 1. Runtimes of the optimization algo- rithm as a function of data set size.

imented with the ?Cover Type? data set containing 581,012 instances from the UCI repository [2]. We removed the dis- crete attributes, leaving ten continuous attributes describing cartographic properties of 30 x 30 meter land cells. We nor- malized the data set, so that each column has a mean of zero and a standard deviation of one. We then applied the opti- mization algorithm on subsets of different size, with the t parameter set to 0.5, g set to 1, and h set to 0.5. The exper- iments were performed on a Pentium IV 2.8GHz machine.

As the runtime of the optimization algorithm depends on the number of line search steps and the runtime per line search, the actual runtime varies for different random restarts.

We therefore give the total runtime, the number of line searches that were performed, and the runtime per line search for the various data set sizes in table 1. The table shows that the number of line search steps remains below thirty for all data set sizes and that the runtime per search step scales favorably with the data set size.

