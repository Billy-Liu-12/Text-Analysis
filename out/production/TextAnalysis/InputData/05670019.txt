PERFICT : Perturbed Frequent Itemset based Classification Technique

Abstract?This paper presents Perturbed Frequent Itemset based Classification Technique (PERFICT), a novel associative classification approach based on perturbed frequent itemsets. Most of the existing associative classifiers work well on transactional data where each record contains a set of boolean items.

They are not very effective in general for relational data that typically contains real valued attributes.

In PERFICT, we handle real attributes by treating items as (attribute,value) pairs, where the value is not the original one, but is perturbed by a small amount and is a range based value. We also propose our own similarity measure which captures the nature of real valued attributes and provide effective weights for the itemsets. The probabilistic contributions of different itemsets is taken into considerations during classification. Some of the applications where such a technique is useful are in signal classification, medical diagnosis and handwriting recognition. Experiments conducted on the UCI Repository datasets show that PERFICT is highly competitive in terms of accuracy in comparison with popular associative classification methods.



I. Introduction  Starting with the seminal work over a decade ago in [1], several associative classification approaches have emerged. Advantages of these approaches include (1) Frequent itemsets capture all dominant relationships between items in a dataset; (2) Efficient itemset mining algorithms exist resulting in highly scalable classifiers; (3) These classifiers naturally handle missing values and outliers as they only deal with statistically significant associations. This property translates well into robust- ness; (4) Extensive experimental studies show that these techniques are less error prone.

Most of the recent associative classifiers such as CBA [1], CMAR [2], MCAR [3], CPAR [4], GARC [5], etc. work well on transactional data where each record contains a set of boolean items. There remains scope for more efficient handling of continuous attributes. More- over, in these algorithms, an overhead of rule weighting and selection of n best rules is required for classification purposes.

In this paper, a novel associative classification proce- dure namely PERturbed Frequent Itemsets based Classi- fication Technique (PERFICT) has been proposed. Our algorithm explicitly and effectively handles real valued  attributes by means of Perturbed Frequent Itemsets (PFI). A new MJ similarity measure is proposed which regulates the selection of the PFIs. The introduction of the similarity measure helps weighing of the PFIs during classification. Rule selection process is not required and the pruned PFIs are used for probabilistic estimate of classes. We propose 3 methods:  1) A naive histogram based approach (Hist PER- FICT).

2) A histogram based approach with the similarity measure (HistSimilar PERFICT).

3) A randomized clustering method including the similarity measure (k-Means PERFICT)  Experimental evaluation of our algorithms on stan- dard UCI datasets show that they perform better against most of the recent state-of-art associative classifiers.

Randomized k-Means PERFICT outperforms HistSim- ilar PERFICT and Hist PERFICT in most cases. Our contributions include:  ? Handling noisy data and the problem of exact matches in an effective manner using the notion of perturbation.

? Introduction of a new MJ similarity measure for weighing and pruning itemsets.

? Use of self-adjusting mincount value for pruning perturbed frequent itemsets.

? Identifying drawbacks of standard discretization method and avoiding it through a preprocessing step.



II. Related Work  Recent state of the art has exploited the paradigm of associative rule mining for solving the problem of classification. However, these associative classifiers suffer certain drawbacks. Though, they provide more rules and information, redundancy involved in the rules increases the cost in terms of orders of time and computation com- plexity during the process of classification. MCAR [3] determines a redundant rule by checking whether it covers instances in training data set or not. GARC [5] brought in the notion of compact set to shrink the rule set by converting the rule set to a compact one. Since the reduction of redundant rules require a brute force technique, it fails to avoid some meaningless searching.

DOI 10.1109/ICTAI.2010.20     Second, as we know, the rule generation is based on fre- quent pattern mining in associative classification, when the size of data set grows, the time cost for frequent pattern mining may increase sharply which may be an inherent limitation of associative classification.

These algorithms have a major drawback: generation of rules by exact matches irrespective of categorical or numeric attributes. This situation causes a problem in most of the real world scenarios because records that contain nearly similar values for a real valued at- tribute should support the same rule. Due to discretized matches, the algorithms do not always generate the required rule. Approaches like [6], [7], [8], [9] perform numeric attribute based optimized rule mining. But its difficult to handle noisy data with them. They use com- putational geometry to determine the areas of confidence and use it as the primary criteria. But similarity of the generated rules with the test record is not emphasized and thus a new similarity measure is required.



III. Basic Concepts and Definitions  Without loss of generality, we assume our in- put data to be a relational table with attributes {A1, A2, A3...An, C}, where C is the class attribute. We use the term item to refer to an attribute value pair (Ai, ai), where ai is the value of an attribute Ai which is not a class attribute. For brevity, we also simply use ai to refer to the item (Ai, ai). Each record in the input relational table then contains a set of items I = {a1, a2, a3...an}. An itemset T is defined as T ? I.

A frequent itemset is an itemset whose support (i.e.

frequency) is greater than a user-specified minimum support threshold. We allow for different thresholds de- pending on the length of itemsets, to account for the fact that itemsets which have larger length naturally have low supports. Let Mink denote the minimum support, where k is the length of the corresponding itemset. Use of frequent itemsets for numeric real-world data is not appropriate as exact matches for attribute values might not exist. Instead, we use the notion of perturbation, a term used to convey the disturbance of a value from its mean position. Perturbation represents the noise in the value of attributes of the items and effectively converts items to ranges. For instance given an itemset T with attribute values av1, av2 and av3, the perturbed frequent itemset PFIT will look like  PFIT = {av1 ? ?1, av2 ? ?2, av3 ? ?3} (1)

IV. The PERFICT Algorithms  The PERFICT algorithms are based on the principle of weighted probabilistic contribution of the Perturbed Frequent Itemsets. One advantage of this procedure over other associative classifiers is that there is no rule generating step in PERFICT algorithms. The basic  Figure 1. Hist PERFICT  Figure 2. HistSimilar PERFICT  concept employed is: Records that share the same PFIs are similar, and those that share longer PFIs are more similar.

Here we outline the general structure of the three PERFICT algorithms. From Figure 1?3, we observe that there are several common steps in the three procedures.

We first explain all the steps taking Hist PERFICT in detail and subsequently explain other steps in HistSim- ilar PERFICT and Randomized K-Means PERFICT.

A. Preprocessing Techniques For associative classifiers, it has been observed that  there is need of preprocessing step where real valued attributes are discretized. In our approach, the concept of perturbation appropriately assigns ranges to these attribute values eliminating the need for discretization.

1) Histogram Construction: A histogram is a fre- quency chart with non-overlapping adjacent intervals calculated upon values of some variable. There are sev- eral kinds of histograms, but two types of histograms were best suited for our approach. They are equi-width histogram and equi-depth histogram. Equi-width his- tograms have all partitions of same size. Equi-depth histograms are based on the concept of equal frequency or equal number of values in each partition.

Equi-depth histograms are more suited for classifica- tion because they capture the intrinsic nature of the random variable or attribute being observed. Moreover, the histogram is not affected by the presence of outliers.

Figure 3. K-Means PERFICT  An Equi-width histogram on the other hand is highly affected by outliers and is a weak choice for the purpose of classification.

There exists an approach in [10] which partitions the values of quantitative attributes into equi-depth intervals. The underlying principle is that of partial com- pleteness. This approach measures the information loss due to formation of rules obtained by considering ranges over partitions of quantitative attributes. However, the proposed approach works differently and prevents any information loss.

B. Transforming the Training Data Set  From Figure 1, it can be seen that the PERFICT algorithms includes transforming the training dataset.

Equi-depth histograms are constructed for each attribute with variable depth value. The standard deviation of each such partition is computed as well. Let us assume there are k attributes apart from the class attribute in the training set. In order to convert the (attribute, value) pair ai to ai ? ?i we need a transformation. To obtain these ranges we use the histogram constructed above.

Each attribute value of a training record is mapped to the corresponding histogram bin using a range query from the hash table of histograms. The attribute value is transformed to original value ? the standard deviation of all the values in mapped bin. The perturbation is defined as the standard deviation of all the attribute values that are initially hashed into that partition.

Let aik represent the value of ith attribute correspond- ing to kth record. The histogram bins for the ith attribute are represented as hi1, hi2, hi3...hip. As we are using equi- depth histograms so each partition has same number of values say n. Let aik maps to hi3.

hi3 = hash(aik) (2)  ?hi3 = n?  j=1  aij n  (3)  ?i3 =  ???? n?  j=1  (aij ? ?hi3)2 (4)  aik = aik ? ?i3 (5)  where ?hi3 represents the mean value for the his- togram bin hi3 and ?i3 represents its standard deviation.

Table I Dataset before transformation  S. No A#1 A#2 A#3 Class  1 v11 v12 v13 C1 2 v21 v22 v23 C2 3 v31 v32 v33 C1  Table II Same Dataset after transformation  S. No A#1 A#2 A#3 Class  1 v11 ? ?11 v12 ? ?12 v13 ? ?13 C1 2 v21 ? ?21 v22 ? ?22 v23 ? ?23 C2 3 v31 ? ?31 v32 ? ?32 v33 ? ?33 C1  It can be observed from Table 2, that each attribute value is replaced by a ?perturbed? range.

1) Issues with discretization: Earlier approaches fol- lowed a simple discretization step for converting real valued attributes to ranges and mapped these ranges to consecutive integers. There are several issues with discretization. If the bin size is kept small, the number of partitions become very high and the ranges obtained would not capture the nature of the dataset effectively.

Alternatively, if the bin size is large, two values of the same attribute positioned at the opposite extremes of the same partition are treated as same, though they might have different contributions.

The introduction of perturbation allows two different values of the same attribute belonging to the same partition to be mapped to different ranges. For example, consider a histogram interval 0 ? 3 say for attribute A1 and standard deviation of 0.25. Consider two values belonging to this partition say 0.5 and 2.5. Let the attribute value for the test record be 0.7. A simple discretization process will map 0.5, 2.5 and 0.7 to the interval 0 ? 3 and replace these values with an integer say 1. In other words, both 0.5 and 2.5 are considered to be equally similar with the test record?s value(0.7).

But by perturbation mechanism, we see that similarity of 0.5?0.25 (here perturbation, ? = 0.25) is greater than 2.5 ? 0.25 as its range is closer and intersecting to the test record?s range(0.7 ? 0.25).

C. Transforming the Test Record The same transformation (as for training dataset) is  applied to test records using training data histograms.

D. Generating Perturbed Frequent Itemsets To obtain PFIs we apply the modified Apriori algo-  rithm mentioned below:  Algorithm 1) Generate 2-itemset 2) Repeat till n-itemset(where n is the number of predictor  attributes)  a) Join Step b) Prune Step c) Record Track Step d) forall candidates Ci,j ,  ? if count(Ci,j) ? minsupport Freq itemset = Freq itemset ? Ci,j  Generating 2-itemset: 1) forall training records r ,  a) foreach pair of attributes ai and aj .

? if test record?s range(ai)) ? r ?s range(ai) ?= ? and test record?s range(aj)) ? r ?s range(aj) ?= ?  Candidatei,j = Candidatei,j ? r 2) forall candidates Ci,j ,  a) if count(Ci,j) ? minsupport Freq itemset = Freq itemset ? Ci,j  The Join Step: 1) forall pairs L1,L2 of Freq itemsetk?1,  a) if L1a1 = L2a1 and L1a2=L2a2 ...

L1ak?2 = L2ak?2 and L1ak?1 < L2ak?1 ? Ck = a1, a2 ... ak?2, L1ak?1, L2ak?1  The Prune Step: 1) forall itemsets c ? Ck,  a) foreach (k - 1) subsets s of c,  ? if s /? Lk?1 delete c from Ck  Record Track Step: 1) forall itemsets c ? Ck,  a) foreach (k-1)-subsets s of c,  ? foreach record r contributing in count of s Increment count(r) by 1  b) forall records r ,  ? if count(r) = k Keep track of record r  While developing the algorithm, we assume that the minimum contributing PFIs are perturbed frequent 2- itemsets. To obtain these itemsets we identify all at- tributes in the training dataset whose value ranges in- tersect with the test record value ranges. Let there be k predictor attributes for training dataset along with the class attribute. The ith attribute is denoted by Ai. So the candidate set is a combination of all possible two itemsets and has the cardinality kC2. This set can be  represented as C2 = {(A1, A2), (A2, A3), ... (Ak?1, Ak)} where C2 refers to length 2 candidate itemset.

A candidate itemset is formed if the range of each attribute of the training record intersects with the cor- responding range of the test record. For instance let the two attributes be A0 and A1 and their values for the jth  training record be aj0 ? ?j0 and aj1 ? ?j1 respectively.

From Figure (4), we can conclude that for both  attribute A0 and A1 the ranged based values of the test record and the training record are intersecting.

Hence the count for the candidate itemset (A0, A1) is incremented by 1 and a track of the training record- id is kept. A single training record may account for multiple candidate itemsets and contributes distinctly in the frequency of each such candidate itemset.

Once the candidate itemsets have been constructed we introduce a small prune step based on the minsupport threshold criteria which is applied on the count for each candidate itemset. This mincount is defined as minsupport  100 ? size(Currentdataset). The minsupport is directly proportional to degree of pruning. For a very high minsupport value very few candidate itemsets will survive. A very low minsupport value limiting to 0 results in no pruning. Importantly, the self-adjusting mincount prevents over fitting.

The size of the Currentdataset is also variable in our procedure. For 2-itemsets, the value of Currentdataset is initialized to the size of training dataset. But, for generating frequent itemsets of length > 2, only distinct records which contribute towards the count of at least one perturbed frequent itemset are included in Current- dataset. A book-keeping strategy for all the record ids contributing in the frequency of each PFI is followed which is highlighted in the record track step. There are some records which do not contribute towards any PFI and are removed. Therefore, the value of Currentdataset does not remain same.

As the length of itemset increases, for example (A0, A1) ? (A0, A1, A2), i.e. from 2 itemsets to 3 item- sets, the size of Currentdataset decreases. The value of mincount adjusts accordingly and reduces. The first iteration to calculate frequent 2-itemsets is the one involving major computation.

The Join Step  The join step is similar to the join step ob- served in the Apriori algorithm. Consider a candidate itemset of length r: Cr,1 = {A1, A2 . . . , Ar?2, P,Q}.

Then, Cr?1,i = {A1, A2 . . . , Ar?2, P} and Cr?1,j = {A1, A2 . . . Ar?2, Q} are frequent itemsets of length r?1.

A Frequent itemset of length r ? 1 implies that r ? 1 predictor attributes obtained from the training records are intersecting with the respective attributes of the test record. While forming candidate itemsets of length r, we     Figure 4. All possible range intersection cases  take any two frequent itemsets of length r ? 1 having exactly r ? 2 overlapping attributes in common. The possibilities of intersection for individual attribute values are shown in Figure 4. The two r ? 1 length frequent itemsets contain a number of records with the same ids mapped to them which percolate to the count of the candidate itemset Cr,1. Let us illustrate:  Consider the candidate itemset C = (A0, A1, A2). It can be easily visualized to be formed from frequent item- sets (A0, A1) and (A0, A2). Here A0 is common to both frequent 2-itemsets. The frequency of C is determined by records which are present in both the frequent itemsets.

The Prune Step  After obtaining the candidate itemsets from the above procedure, we apply a prune step similar to Apriori.

The Record Track Step  There is a need to keep track of the records which contribute toward any frequent itemset because all such records form the Currentdataset for the next iteration (i.e. itemset length r-1 to r). From the pseudo code it can be observed that for the participation of a record in the count of frequent itemset, the record must be contributing in the frequency of each of the subsets of the frequent itemsets. For example let us consider a record r participating for frequent itemset (A0, A1, A2).

Then rid ? (A0, A1)map, rid ? (A0, A2)map and rid ? (A1, A2)map where (Ai, Aj)map represents the map be- tween a frequent itemset and set of all the record ids contributing for that itemset.

E. Naive Probabilistic Estimation  Once we have obtained all possible frequent itemsets the final task is the estimation of the class to which the test record belongs. We devise a formula with two components.

1) For each frequent itemsets (PFIs) of length i ? 2 we keep track of all the records contributing to its count. These records may belong to different classes. For instance let (A0, A1) be the Ith PFI of length 2. Let nI be the number of records participating in the count of this PFI. Then, nI =  ? j?C nIjCj where Cj represents j  th class out of the possible C classes. So contribution of each PFI of length ? 2 is defined as:  Contri(PFIIi) = ?  I?Freq(i)  ? j?C  nIj Ni  Cj (6)  where Ni is the size of dataset for itemsets of length i and PFIIi represents the Ith PFI of length i.

2) The second part is the heuristics based rank asso- ciated with each PFI. We assign different ranks to itemsets of different lengths. However, same weight is associated with the itemsets of similar length. The allocation resembles ones intuition - Greater the length of the PFI more is the similarity between training set and test-record and hence better classification.

Ranki =  ?i p=1 p?max  k=2 max ? k + 1 (7)  where the numerator is sum of all natural numbers from 1 to the length i and the denominator is a normalization constant. Here max represents the maximum number of attributes in the dataset apart from class attribute as the largest PFI can be of max length only. The formula is similar to that used for assigning weights in k-nearest neighbor classification.

Equation 7 is similar to the Laplacian operator men- tioned in [11]. It converges to 1 as the length of PFIs increases. This conforms with the true nature of the problem as records which are highly similar to a given test record are fewer in number and play a major role in predicting its class.

The overall formula for finding the class conditional probability of a record becomes  P (C/R) = max? i=2  Contri(PFIIi) ? Ranki (8)  where P(C/R) contains the contribution from all classes and can also be written as  P (C/R) = a1C1 + a2C2 + ...... + atCt (9)  where ai represents the contribution of all PFIs for class Ci and i varies from 1 to t. This sum of co-efficients ai can be either greater than or less than 1 and is given by S. So to normalize, each co-efficient is divided by S. This process converts the ratio into a probability measure.

We select that class label as the class for the test record whose co-efficient is maximum i.e argmaxi{a1, a2, ...at}.

This technique is used for Hist PERFICT.

The importance of the result lies in the fact that a probabilistic estimation of contribution of all the classes pertaining to a single test record is available at the end.

This can be viewed as an addendum for detailed analysis and confidence towards classification.We now provide some of the problems with Hist PERFICT and the way their rectification leads to HistSimilar PERFICT method.



V. Issues with Hist PERFICT  A. Pruning  The number of itemsets generated by the Apriori algorithm are huge and require a pruning step. In the case of Hist PERFICT we generate all possible PFIs without including an extra pruning step. However, some of the itemsets have high contribution in more than one class which sometimes leads to misclassification. So we need a pruning step to make the classifier more effective.

B. Weights  As mentioned earlier, the rank or weight for different length itemsets are different. However, another major issue with the Hist PERFICT approach is that we assign same rank or weights to PFIs of similar length. This sometimes degrades the accuracy of classification as is evident by the Precision Table (Table 3). We need to construct a metric which can capture the range based nature of the PFIs effectively and provide weights in accordance.

Both these problems are resolved by the means of our proposed MJ similarity metric.



VI. HistSimilar PERFICT  From Figures 1 and 2, we observe that an additional step involving a new similarity calculation is required.

The criteria is presented as follows.

A. MJ Similarity Metric  We define a new similarity measure based on the simple though effective notion of area of overlap. Let us illustrate by an example. Let us assume for a given test record 1st attribute value a1 = 0.5 and 2nd attribute value a2 = 0.6. Then during the transformation of the test record to perturbed range based values we map a1 to a histogram whose standard deviation is 0.3 and a2 to a histogram whose standard deviation is 0.4. So the perturbed value for attribute 1 and 2 becomes a1??1 and a2 ? ?2. Now, let there be a training record r for which r1 = 0.5 and r2 = 0.6. These values also map to the histograms whose deviations are 0.3 and 0.4 respectively.

Perturbed training record values become r1 ? ?1 and r2 ? ?2 where ?i is the standard deviation.

For the attributes 1 and 2 of the record r,  AO = (a1 ? ?1) ? (r1 ? ?1)  2 ? ?1 ? (a2 ? ?2) ? (r2 ? ?2)  2 ? ?2 ?2  (10)  Figure 5. Sample area of Overlap  where AO represents Area of Overlap and the intersec- tion of the range based values for 1st attribute leads to similarity of 0.5 and the intersection of range based values for 2nd attribute leads to similarity of 0.7. The similarity values are normalized by taking into account the ? value of the respective attribute histograms. It can be observed from the formula of area of overlap that only taking into account the dot products of the similarity for each attribute would lead to a decrease in the overall (multiplying one fraction with another fraction). So we introduce an additional multiplicative factor to obtain the overall area of overlap which is dependent on length of the itemset and is defined as (itemsetlength)(itemsetlength). Figure 5 represents the example stated above as an illustration. The formula for Area of Overlap can be generalized for the jth PFI of length k:  AOPFIjk = k?  i=1  (ai ? ?i) ? (ri ? ?i) ?i  ? kk (11)  where ai is the ith attribute value for test record and ri is the ith attribute value for a training record r and ?i is deviation of the histogram partition to which the value maps.

An important constraint imposed on the proposed similarity measure is that PFIs of larger length must be assigned greater weights than PFIs of smaller length.

But sometimes if we directly use the AO based formula, this constrain can be violated. Without loss of generality, let us consider we have a PFI of length 2 (A1, A2) and from that PFI another frequent itemset of length 3 (A1, A2, A3) is obtained. However, if the intersection for the 3rd attribute is very small then Area of Overlap is reduced and so similarity of 3 length PFI can be smaller than that of 2-length PFI. In order to prevent such an opposing situation we add a term 2 ? itemset length to Area of Overlap after taking logarithm (base 10) of the Area of Overlap. We place the constraint     that intersection for each attribute (here ith attribute) must be ? (0.02 ? ?i). By maintaining this criteria the similarity value for 3-itemset ? similarity value for 2- itemset.

MJPFIjk = 2 ? k + log10(AOPFIjk) (12) where MJPFIjk represents the similarity value for the jth PFI of length k with respect to training record r. But there are several such records which are contributing in the count of PFIjk. So we take average of all such values as the similarity measure for the corresponding PFI.

The MJ similarity criteria of intersection ? (0.02??) is included in Generating Perturbed Frequent Itemsets step. The Probabilistic estimation step is modified by including the MJ criteria. In equation (9) we replace the Rank term with the MJ similarity measure term to arrive at the following formula to be used for classification.

P (C/R) = max? i=2  Contri(PFIIi) ? MJPFIIi (13)  This changed probabilistic estimation step is used in HistSimilar PERFICT and Randomized K-means algo- rithm.



VII. Randomized K-Means  A. Disadvantages of Histograms There are certain limitations to the Histogram based  approach for preprocessing the data. The depth of each histogram for each attribute is kept variable. But his- togram construction is a time consuming process. Fur- ther the process of estimation of the best depth for each variable is manual and cannot easily be automated.

Sometimes it happens that the variation in the attribute values is not much and for such an attribute 3 or 4 partitions are sufficient but we end up with more number of bins. This problem can be solved by using a clustering technique for preprocessing.

B. K-Means Approach The K-Means algorithm [12] is a highly popular clus-  tering approach. It can be used very effectively to iden- tify clusters of data which act as partitions for us. For our algorithm, we compute the k-means independently for each attribute and so the data points are all one dimensional. The purpose of this method is to minimize the Squared Sum Error (SSE) i.e distance of each data point from the k-means.



VIII. Results and Analysis  We conducted our experiments over 12 datasets from the UCI repository. These datasets consist of real valued attributes which are generally continuous by nature. We also conducted experiments over two different minsup- port values (1 and 10) for various datasets and selected  the minsupport value for which the accuracy was best for the corresponding dataset. We varied the histogram bins and number of clusters from 3 to 15 and selected the one for which accuracy was best. The 10-fold cross validation accuracy for associative classifiers like CBA, CPAR, CMAR, case based algorithm like PART, decision tress like C4.5 and Ripper along with Naive Bayes algorithm are presented in Table 3. For discretization of real valued attributes of these classifiers we use the entropy based technique same as that used in MLC++ library. Our primary concern is accuracy of the PERFICT classifiers.

Apart from the accuracy measure, we also give a detailed analysis of the effect of MJ criteria for HistSimilar PERFICT and Randomized K-Means Algorithm. We have used the WEKA Toolkit and have implemented the PERFICT algorithms in C++.

A. Performance of PERFICT  We provide a brief analysis of the precision results present in Table 3. The datasets are composed of real valued attributes. Randomized K-Means PERFICT out- performs other algorithms over 8 datasets and is among top 3 for 11 datasets. The performance of K-Means PER- FICT over other algorithms is exceptional for waveform, vehicle and ecoli datasets. The inclusion of MJ similar- ity measure is primarily responsible for high precision results. It helps to prune away the itemsets which are not essential for classification and gives the appropriate weight to each PFI necessary for classification. The reason for high success rate of Randomized K-means is that it captures the noisy nature of attributes. Su- periority of Randomized K-means over HistSimilar and Hist PERFICT is because variable number of points can belong to a cluster as opposed to equi-depth histogram.

The necessity for having a variable for the number of clusters or bins can be seen by the fact that in diabetes dataset where the number of clusters or bins for each attribute is best set to 15 while for the ecoli dataset the number is 3 as there is little variation in value of each feature. For dataset like image, vowel and wine the K-Means PERFICT is among the top 3 classifiers and hence is suitable for tasks like image recognition and handwriting recognition.

B. Effects of MJ Criteria  For both HistSimilar PERFICT and Randomized K- Means approach, the application of MJ criteria is one of the most important steps. We vary the MJ criteria as follows:  MJj = ?j N  (14)  where MJj represents the threshold for the jth dimen- sion. ?j represents the deviation for the partition to which the training value maps for the jth attribute and     Table III Precision Results  Dataset Hist HistSimilar K-Means CBA CMAR CPAR RIPPER J4.8 PART Naive PERFICT PERFICT PERFICT Bayes  breast-w 95.51 96.23 96.47 96.28 96.40 95.00 95.10 94.56 93.84 95.99 diabetes 72.28 75.00 77.10 73.29 75.80 75.10 74.70 73.66 73.27 75.88 ecoli 82.03 82.03 86.67 80.65 81.60 82.01 82.16 84.22 83.63 85.41 glass 71.43 77.62 77.80 73.90 70.10 74.40 69.10 66.82 68.22 48.59 heart 67.03 71.85 79.62 81.47 82.20 82.60 80.70 77.57 79.86 83.49 image 51.42 74.76 75.71? 61.44 63.23 65.63 - 86.60 87.08 69.04 iris 96.67 96.00 97.33 96.47 94.00 94.70 94.00 96.00 94.00 96.00 pima 72.76 75.00 75.26 72.51 75.10 73.80 73.10 73.83 75.26 74.30 vehicle 63.09 65.24 75.23 68.70 68.80 69.50 68.60 72.45 71.51 44.79 vowel 52.93 63.64 70.03? 52.03 56.78 55.46 - 71.22 70.02 63.63 waveform 95.26 98.43 99.56 78.46 83.20 80.90 76.00 75.08 77.42 80.00 wine 94.19 91.70 95.29? 94.96 95.00 95.50 91.60 93.82 93.25 96.62  Figure 6. Accuracy vs Overlap  N represents a natural number. The maximum overlap that can occur is equal to 2? ?j , when the range based value of the training record r completely overlaps with test record values. The MJ criteria is always smaller than 2 of maximum Overlap. We vary N from 1 to 100 to see the effect of various thresholds on accuracy.

Observe in Fig 6 that the maximum variation in accuracy occurs for smaller N. Lesser the value of N, higher is the MJ criteria and more is the pruning of the candidate itemsets. More pruning leads to under-fitting and characteristic of dataset is not captured leading to lower accuracy. However as N increases the MJ criteria is relaxed and optimal accuracy is achieved.



IX. Conclusion  This paper presented a new classification approach developed on the concept of perturbed frequent itemsets and their probabilistic contribution for each class. The approach is computationally more efficient than other discretized rule generating algorithms as CBA, CPAR, CMAR etc. and employs a new MJ similarity measure.

Results obtained are accurate and effective particularly  for noisy datasets like waveform, diabetes and vehicle.

