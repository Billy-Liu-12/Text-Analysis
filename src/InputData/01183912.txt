Adapting classification rule induction to subgroup discovery

Abstract  Rule learning is typically used for solving class$carion and prediction tasks However; learning of classijcarion rules can be adapred also IO subgroup discovery This pa- per shows how this can be achieved by modibing rhe cover- ing algorirhm and the search heurisric, performing pmba- bilisric class9carion of instances, and using an appropriare measure for evaluating rhe results of subgmup discovery Experimenral evaluarion of rhe CN2-SD subgmup discov- ery algorithm on I7 UCI dara sers demonstrates subsranrial reducrion of rhe number of induced rules, increased mle coverage and rule significance, as well as slight impmve- menrs in rerms of rhe area under rhe ROC curve.

1. Introduction  Classical rule learning algorithms were designed to con- S tNCI  classification and prediction rules [16,4,5]. In addi- tion to this area of machine learning, referred to as predic- rive induction, developments in descriprive inducrion have recently gained much attention. These involve mining of association rules (e.g., the APRIORI association rule learn- ing algorithm [I]), subgroup discovery (e.g., the MIDOS subgroup discovery systems [241), and other approaches to non-classificatory induction.

The methodology presented in this paper can be applied to subgroup discovery. As in the MIDOS approach, a sub- group discovery task can be defined as follows: given a pop- ulation of individuals and a property of those individuals we are interested in, find population subgroups that are statis- tically ?most interesting?, e.g., are as large as possible and have the most unusual statistical (distributional) character- istics with respect to the property of interest.

This paper investigates how to adapt classical classifi- cation rule learning approaches to subgroup discovery, by appropriately modifying the heuristics and the covering al- gorithm for learning sets of rules. The proposed modifi-  0-7695-1754-4/02 $17.00 Q 2002 IEEE  cations of classification rule learners can, in principle, be used on top of any rule learner using the covering approach for rule set construction. In  this work we show an upgrade of the well-known CNZ rule learning algorithm [4, 31. Al- ternatively, we could have upgraded RL [15], RIPPER [5], SLIPPER [6] or other more sophisticated classification rule learners. The reason for upgrading CNZ is that other more sophisticated learners include modifications that make them more effective in classification tasks, improving their classi- fication accuracy. Improved classification accuracy is, how- ever, not of ultimate interest for subgroup discovery, whose main goal is to find interesting population subgroups.

We have implemented the new subgroup discovery al- gorithm CNZ-SD in Java and incorporated it in the WEKA data mining environment [23]. The proposed approach per- forms subgroup discovery through the following modifica- tions of the classical rule learning algorithm CN2: (a) in- corporating example weights into the covering algorithm, (b) incorporating example weights into the weighted rela- tive accuracy search heuristic, (c) probabilistic classifica- tion based on the class distribution of covered examples by individual rules, both in the case of unordered rule sets and ordered decision lists, and (d) area under the ROC curve rule set evaluation.

This paper presents the CNZ-SD subgroup discovery al- gorithm, together with its experimental evaluation in se- lected domains of the UCI Repository of Machine Learning Databases [171. The experimental comparison with CN2 demonstrates that the subgroup discovery algorithm CN2- SD produces substantially smaller rule sets, where individ- ual rules have higher coverage and significance. These three factors are important for subgroup discovery: smaller size enables better understanding, higher coverage means larger support, and higher significance means that rules describe discovered subgroups that are significantly different from the entire population. The appropriateness for subgroup dis- covery is confirmed also by slight improvements in terms of the area under the ROC curve.

This paper is organized as follows. In Section 2 the   mailto:Peter.Flach@bristol.ac.uk   background for this work is explained the standard CN2 rule induction algorithm, including the covering algorithm, the weighted relative accuracy heuristic. probabilistic clas- sification and rule evaluation in the ROC space. Sec- tion 3 presents the modified CN2 algorithm, called CN2- SD, adapting CN2 for subgroup discovery. Section 4 presents the experimental evaluation on selected UCI do- mains. Some links to the related work are given in Sec- tion 5.  Section 6 concludes by summarizing the results and presenting plans for further work.

2. Background  This section presents the backgrounds of our work the classical CN2 rule induction algorithm, including the cov- ering algorithm for rule set construction, the standard CN2 heuristics, the weighted relative accuracy heuristic, prob- abilistic classification and the basics of rule evaluation in ROC space.

2.1. The CN2 rule induction algorithm  CN2 is an algorithm for inducing propositional clas- sification rules [4, 31. Induced rules have the form i f Cond then Class, where Cond is a conjunction of fea- tures (attribute values). In this paper we use the notation Class c C a d .

CN2 consists of two main procedures: the search pro- cedure that performs beam search in order to find a single rule and the control procedure that repeatedly executes the search. The search procedure performs beam search us- ing classification accuracy of the N k  as a heuristic func- tion. The accuracy of the propositional classification rule Class - Cmd is equal to the conditional probability of class Class, given that the condition Cond is satisfied: Acc(Class c Cond) = p(Class(Cond). Different pmb- ability estimates, like the Laplace [3] or the m-estimate [Z, 71, can be used in CN2 for estimating the above prob- ability. The standard CN2 algorithm used in this work uses the Laplace estimate.

CN2 can apply a significance test to an induced rule. A rule is considered to be significant if it expresses a regularity unlikely to have occurred by chance. To test significance, CN2 uses the likelihood ratio statistic [4] that measures the difference between the class probability distribution in the set of examples covered by the rule and the class probabil- ity distribution in the set of all training examples. Empiri- cal evaluation in [31 shows that applying a significance test reduces the number of induced rules (at a cost of slightly reduced predictive accuracy).

Two different control procedures are used in CN2: one for inducing an ordered list of rules and the other for the un- ordered case. In both procedures, a default rule (providing  for majority class assignment) is added as the final rule in an induced rule set.

When inducing an ordered list of rules, the search proce- dure lwks  for the best rule, according to the heuristic mea- sure, in the current set of training examples. The rule pre- dicts the most frequent class in the set of examples. covered by the induced rule. Before starting another search itera- tion, all examples covered by the induced rule are removed.

The control procedure invokes a new search, until all the examples are covered.

In the unordered case, the control procedure is iterated, inducing rules for each class in turn. For each induced rule, only covered examples belonging to that class are removed, instead of removing all covered examples, like in the or- dered case. The negative training examples (i.e., examples that belong to other classes) remain and positives are re- moved in order to prevent CN2 finding the same rule again.

2.2. The weighted relative accuracy heuristic  Weighted relative accuracy is a variant of rule accuracy that can be meaningfully applied both in the descriptive and predictive induction framework; in this paper we apply this heuristic for subgroup discovery.

We use the following notation. Let n ( C a d )  stand for the number of instances covered by a rule Class - Cond.

n(C1ass) stand for the number of examples of class Class, and n(Class.Cond) stand forthe number of correctly clas- sified examples (true positives). We use p(Class.Cond) etc. for the corresponding probabilities. In our work, the Laplace estimate is used for probability estimation. Rule accuracy can be expressed as Acc(C1ass t Cond) = p(Class(Cond) = w, Weighted relative accu- racy [14,221, a reformulation of one of the heuristics used in EXPLORA [lo] and MlDOS [241, is defined as follows.

WRAcc(C1ass t Cond) =  p(Cond).(p(ClasslCond) - p(Class)).  ( I ) Like most of the other heuristics used in subgroup dis-  covery systems, weighted relative accuracy consists of two components, providing a tradeoff between rule generality (or the size of a group p(Cond)) and distributional unusu- alness (or relative accuracy, i.e., the difference between rule accuracy p(Class1Cond) and default accuracy p(C1ass)).

This difference can also be seen as the accuracy gain rela- tive to the fixed rule Class c true. The latter rule pre- dicts all instances to satisfy Class; a rule is only interesting if it improves upon this ?default? accuracy. Another way of viewing relative accuracy is that it measures the util- ity of connecting rule body Cond with a given rule head Class. However, it is easy to obtain high relative accu- racy with highly specific rules, i.e., rules with low general- ity p(Cond). To this end, generality is used as a ?weight?,     so that weighted relative accuracy trades off generality of the rule (p(Cond), i.e., rule coverage) and relative accuracy  Io [IO], these quantities are referred to as g (generality), p (rule accuracy) and po (default accuracy), and the func- tion g(p - po) is investigated in the so-called 'p-g-space'.

Klosgen also investigates other tradeoffs reducing the in- fluence of generality, e.g. a ( p  - Po) or f i ( p  - P o ) .

Here we favor weighted relative accuracy because i t  has an intuitive interpretation in the ROC space (see Section 2.4).

2.3. Probabilistic classification  (p(ClasslCond) - p(Class)).

The induced rules can be ordered or unordered. Ordered rules are interpreted as a decision list [I91 in a straight- forward manner: when classifying a new example, the rules are sequentially tried and the first rule that covers the exam- ple is used for prediction.

In the case of unordered rule sets, the distribution of cov- ered training examples among classes is attached to each rule. Rules of the form:  if Cond t hen  Class [ClassDistributim]  are induced, where numbers in the ClassDistribution list denote, for each individual class, how many training exam- ples of this class are covered by the rule. When classify- ing a new example, all rules are tried and those covering the example are collected. If a clash occurs (several rules with different class predictions cover the example), a vot- ing mechanism is used to obtain the final prediction: the class distributions attached to the rules are summed to de- termine the most probable class. If no rule fires, a default rule is invoked which predicts the majority class of uncov- ered training instances.

2.4. ROC analysis for subgroup discovery  A point on the ROC curve (ROC: Rcceiver Operating Characteristic) [IS] shows classifier performance in terms of false a l m  orfalse positive rare FPT = & (plot- ted on the X-axis) that needs to be minimized, and sensi- tivity or true posirive rote TPT = - (plotted on the Y-axis) that needs to be maximized. In the ROC space, an appropriate tradeoff, determined by the expert, can be achieved by applying different algorithms. as well as by different parameter settings of a selected data mining algo- rithm or by taking into the account different misclassifica- tion costs.

The ROC space is appropriate for measuring the suc- cess of subgroup discovery, since rules/subgroups whose TPTIFPT tradeoff is close to the diagonal can be discarded as insignificant. Conversely, significant Nles/subgroups are  those sufficiently distant from the diagonal.' The signifi- cant rules define the points in the ROC space from which a convex hull is constructed. The area under the ROC curve (AUO can he used as a quality measure for comparing the success of different learners or subgroup miners.

Weighted relative accuracy is appropriate to measure the quality of a single subgroup, because it is proportional to the distance from the diagonal in the ROC space. To see this, note first that rule accuracy p(Class1Cond) is propor- tional to the angle between the X-axis and the line connect- ing the origin with the point depicting the rule's TPTIFPT tradeoff. So, for instance, the X-axis has always rule ac- curacy 0 (these are purely negative subgroups), the Y-axis has always rule accuracy 1 (purely positive subgroups), and the diagonal represents subgroups with rule accuracy p(Class), the prior probability of the positive class.

Relative accuracy re-normalizes this such that all points on the diagonal have relative accuracy 0, all points on the Y - axis have relative accuracy 1 - ~ ( C ~ R S S )  = p ( ' l a s s )  (the prior probability of the negative class), and all points on the X-axis have relative accuracy -p(Class). Notice that all points on the diagonal also have WRAcc = 0. In terms of subgroup discovery, the diagonal represents all subgroups with the same target distribution as the whole population; only the generality of these 'average' subgroups increases when moving from left to right along the diagonal. This in- terpretation is slightly different in classifier learning, where the diagonal represents random classifiers that can he con- structed without any training.

More generally, one can show that points with the same WRAcc value lie on straight lines parallel to the diagonal.

In particular, a point on the line TPT = FPT + a, -1 I a 5 1 has WRAcc = ap(Class)p(Class).  Thus, given a fixed class distribution, WRAccis proportional lo the veni- cal (or horizontal) distance a between the line parallel to the diagonal on which the point lies. and the diagonal. In fact, the quantity TPT - FPT would he an alternative quality measure for subgroups, with the additional advantage that we can use it to compare subgroups from populations with different class distributions. However, in this paper we are only concerned with comparing subgroups from the same population, and we prefer WRAcc because it is a familiar measure in subgroup discovery.

3. The CN2-SD subgroup discovery algorithm  The main modifications of the CN2 algorithm, making it appropriate for subgroup discovery, involve the imple- mentation of the weighted covering algorithm, incorpora- tion of example weights into the weighted relative accuracy heuristic, probabilistic classification also in the case of the  -  'Any of (hose subgroups may be the 'best' according to some expen- defined optrating conditions.

?ordered? induction algorithm. and the area under the ROC curve rule set evaluation.

3.1. Weighted covering algorithm  Subgroup discovery is, in general, aimed at discovering interesting properties of subgroups of the entire population.

If used for subgroup discovery, the problem of standard rule learners like CNZ and RIPPER is in the use ofthe covering algorithm for rule set construction. The main deficiency of the covering algorithm is that only the first few induced rules may be of interest as subgroup descriptors with suf- ficient coverage and significance. Subsequently induced rules are induced from biased example subsets, i.e., subsets including only positive examples not covered by previously induced rules, which inappropriately biases the subgroup discovery process.

As a remedy to this problem we propose to use the weighted covering algorithm, in which the subsequently induced rules allow for discovering interesting subgroup properties of the entire population. The weighted covering algorithm modifies the classical covering algorithm in such a way that covered positive examples are not deleted from the current training set. Instead, in each run of the covering loop. the algorithm stores with each example a count that shows how many times (with how many rules induced so far) the example has been covered so far. Weights derived from these example counts then appear in the computation of WRAcc. Initial weights of all positive examples e j  equal w(ej,O) = 1. We have implemented two approaches:  (a) Multiplicative weights. In the first approach, weights decrease multiplicatively. For a given parameter y < 1, weights of covered positive examples decrease as follows: w(ej, i)  = 7?. where w(ej,i) is the weight of ex- ample e j  being covered i times. Note that the weighted cov- ering algorithm with y = 1 would result in finding the same rule over and over again, whereas with y = 0 the algorithm would perform the same as the standard CN2 algorithm.

(b) Additive weights. In the second approach, weights of covered positive examples decrease according to the for- mulaw(ej, i)  = &.

3.2. Modified WRAcc heuristic with example  weights  The modification of CNZ reported in [ZZ] affected only the heuristic function: weighted relative accuracy was used as search heuristic, instead of the accuracy heuristic of the original CN2, while everything else remained the same. In this work, the heuristic function was further modified to en- able handling example weights, which provide the means to consider different pans of the instance space in each itera- tion of the weighted covering algorithm.

In the WRAcc computation (Equation I )  all probabilities are computed by relative frequencies. An example weight measures how important it is to cover this example in the next iteration. The initial example weight w(ej,O) = 1 means that the example hasn?t been covered by any rule, meaning ?please cover this example, it hasn?t been covered before?, while lower weights mean ?don?t try too hard on this example?. The modified WRAcc measure is then de- fined as follows.

n?(Cond) n?(C1.Cond) n?(C1) WRAcc(C1c  Cond) = - _ _ _ N? ( n?(Cmdl N? )?  ? (2) In this equation, N? is the sum of the weights of all ex- amples, n?(Cond) is the sum of the weights of all covered examples, and n?(C1.Cond) is the sum of the weights of all correctly covered examples.

To add a rule to the generated rule set, the rule with the maximum WRAcc measure is chosen out of those rules in the search space, which are not yet present in the rule set produced so far (all rules in the final rule set are thus dis- tinct, without duplicates).

3.3. Probabilistic classification  Each CN2 rule returns a class distribution in terms of the numbers of examples covered, as distributed over classes.

The CN2 algorithm uses class distribution in classifying un- seen instances only in the case of unordered rule sets, where rules are induced separately for each class. In the case of or- dered decision lists, the first rule that fires provides the clas- sification. In our modified CNZ-SD algorithm, also in the ordered case all applicable rules are taken into the account.

hence the same probabilistic classification is used in both classifiers. This means that the terminology ?ordered? and ?unordered?, which in CN2 distinguished between decision list and rule set induction, has a different meaning in our setting: the ?unordered? algorithm refers to learning classes one by one, while the ?ordered? algorithm refers to finding best rule conditions and assigning the majority class in the head.

3.4. Area under the ROC curve evaluation  In subgroup discovery there are two ways in which a rule learner can give rise to a ROC curve.

(a) AUC-Method-1. The first method treats each rule as a separate subgroup which is plotted in the ROC space with its true and false positive rates. We then calculate the convex hull of this set of points, selecting the subgroups which per- form optimally under a particular range of operating char- acteristics. The area under this ROC convex hull (AUC) indicates the combined quality of the optimal subgroups, in the sense that it does evaluate whether a particular subgroup     has anything to add in the context of all the other subgroups.

However, the method does not take account of any overlap between subgroups. and subgroups not on the convex hull are simply ignored (the existence of many such subgroups may indicate overfitting. as we illustrate in Section 4).

Note that CN2-SD learns rules both for the positive and negative target class: rules for the positive target class rep- resent subgroups with a higher proponion of positives than average, and rules for the negative target class represent subgroups with a lower than average proportion of posi- tives. Consequently, this method constructs two convex hulls, one above the diagonal and one below (see Figure I in Section 4).

(b) AUC-Method-2. The second method employs the combined probabilistic classifications of all subgroups, as indicated below. If we always choose the most likely pre- dicted class, this corresponds to setting a fixed threshold 0.5 on the positive probability: if the positive probability is larger than this threshold we predict positive, else negative.

A ROC curve can be constructed by varying this threshold from I (all predictions negative, corresponding to (0,O) in the ROC space) to 0 (all predictions positive, correspond- ing to (1.1) in the ROC space). This results i n n  + 1 points in the ROC space, where n is the total number of classified examples. Equivalently, we can order all the examples by decreasing the predicted probability of being positive, and tracing the ROC curve by starting in (O,O), stepping up when the example is actually positive and stepping to the right when it is negative, until we reach ( l , l ) ?  The area under this ROC curve indicates the combined quality of all sub- groups (i.e.. the quality of the entire rules set). This method can be used with a test set or in cross-validation, but the resulting curve is not necessarily convex. A detailed de- scription of this method applied to decision tree induction can be found in [ I l l .

Which of the two methods is more appropriate for sub- group discovery is open for debate. The second method seems more appropriate if the discovered subgroups are also used for classification, while the first seems more appropri- ate for the selection and evaluation of a subset of potentially optimal individual subgroups. One advantage of the second method is that it is easier to apply cross-validation. A disad- vantage of the first method is that it ignores redundant sub- groups which may indicate overfitting. In the experimental evaluation in Section 4 we used AUC-Method-2, while the ROC convex hull obtained by AUC-Method-l is only illus- trated in one domain in Figure 1.

'In Lhe c s e  of ties. we make h e  appmpnate number of steps up and to the fight at once, drawing a diagonal line segment.

4. Experimental evaluation  For subgroup discovery, expen's evaluation of results is of ultimate interest. Nevertheless, before applying the proposed approach to a particular problem of interest, we wanted to verify our claims that the mechanisms imple- mented in the CN2-SD algorithm are indeed appropriate for subgroup discovery.

We experimentally evaluated our approach on 17 data sets from the UCI Repository of Machine Learning Databases [171. In Table 1. the selected data sets are sum- marized in terms of the number of attributes, the number of examples, and the percentage of examples of the ma- jority class. These data sets have been widely used in other comparative studies. Since currently our lava r e implementation of CN2 in WEKA does not support con- tinuous attributes and can not handle missing values, all continuous attributes were discretized and data sets that contain no missing values were chosen. The discretiza- tion described in [SI was performed using the WEKA tool 1231. Moreover, in our experiments all of the data sets have two classes, either originally or by selecting one class as 'positive' and joining all the other in the 'negative' class (in Table I ,  the selected positive class is indicated by {ClassName}); this was done for the purpose of enabling [he area under the ROC curve evaluation.

Table 1. Data set characteristics.

8 768 65.1 7 Glar~(hild~uindnon-floa,) 9 214 35.51 8 HCarMWI, I3 270 55.56 9 lonosphire 34 351 @.I  11 Lyrnphlmelasfaus) 18 148 54.72 12 Scrment{hns!duel 19 2310 14.29  10 Itis{ltir-setosl) 4 I s o  33.33  13 S o h  ' 60 208 53.36 14 lic-lac-LoE 9 958 6594 I 5  Vohicle(hun) 18 846 25.77  13 178 39.89 16 Winel l ) 17 k [ m a m m a l )  17 101 40.59  The comparison of CN2-SD with CN2 was performed in 17 UCI domains with AUC-Method-2 evaluation based on IO-fold stratified cross validation. Table 2 compares the CNZ-SD subgroup discovery algorithm with the stan- dard CN2 algorithm (CN2-sfundard. described in [3]) and the CN2 algorithm using WRAcc (CNZ-WRAcc, described in [22]) in terms of area under the ROC curve (AUC).

All these variants of the CN2 algorithm were first re- implemented in the WEKA data mining environment [23],     because the use of the same system makes the comparisons more impartial. Due to space restrictions we only include the results of the unordered algorithms. The results of the CN2-SD algorithm were computed using both the multi- plicative weights (with y = 0.5, 0.7, 0.9) and the additive weights. Results withy = 0.7 are not listed, as they are al- ways between those of 7 = 0.5 and y = 0.9, as expected. All other parameters of the CN2 algorithm were set to their de- fault values (bean-size = 5 ,  significance-threshold = 99%).

Table 2. Area under the ROC curve with stan- dard deviation (AUC * sd) for different vari- ants of the unordered algorithm using 10-fold stratified cross-validation.

3.5 and on CN2-WRAcc with a factor of 2. Note, however, that rules obtained with additive weights and multiplicative weights with high y are highly overlapping, due to the rela- tively modest decrease of example weights.

In addition, there is also a substantial increase in the av- erage likelihood ratio: while the ratios achieved by CNZ- standard are already significant at the 99% level, this is fur- ther pushed up by CNZ-SD with maximum values achieved by additive weights. An interesting question, to be verified with further experiments, is whether the weighted versions of the CN2 algorithm improve the significance of the in- duced subgroups also in the case when.CN2 rules are in- duced without applying the significance test.

In summary, CNZ-SD produces substantially smaller rule sets, where individual rules have higher coverage and sig- nificance.

Table 3. Average size (9, coverage (CVC) and llkelihood ratio (LHR) of rules for different versions of the unordered algorithm induced from the entire data sets.

We also compared the sizes of the rule sets, average rule coverage, and the l i k e l i h d  ratio of rules, computed from the entire data sets (not using cross-validation). Table 3 compares CNZ-SD with CN2-standard and CN2-WRAcc in terms of the size of the rule set (S is the number of rules in a rule set, including the default rule), average rule coverage (CVG is computed as the averaged percentage of covered positive and negative examples per rule), and likelihood ra- tio? per rule.

The experimental results show that CN2-SD achieves im- provements across the board. In terms of AUC, the smallest improvement is achieved by additive weights and slightly better improvements of 3 4 %  are by multiplicative weights.

On the other hand, additive weights result in ahout 2 times less rules on average than multiplicative weights, and 6.5 times less rules than CNZ-standard. Average rule coverage is also optimal for additive weights, improving on the av- erage the coverage of CN2-standard rules with a factor of  ?The likelihood ralio is used in CNZ for testing the significance of Ihs induced tule 141. For Iwo-class problems this stalislic is distributed ap- prorimalely as xz wilh one degree of freedom.

Finally, we illustrate our approach in the ROC space by means of the results on the Australian data set (Fig- ure 1). The solid lines in this graph indicate the ROC curves obtained by CN2-SD and CN2-standard, evaluated with AUC-Method-2, i.e., probabilistic classification with overlapping tules: the top line (squares) for CN2-SD with additive weights, and the bottom line (triangles) for CNZ- standard. CN2-standard finds many more rules than CN2- SD, which leads to overfitting as the ROC curve is mostly below the diagonal.

For illustrative purposes we also include positive and negative convex hulls constructed from individual sub- groups using AUC-Method-l (dotted lines). The points on the X and Y-axes close to the origin are all small, purely positive and negative subgroups found by CN2-standard, that do not contribute to the convex hull (presumably these are the rules that lead to poor performance using probabilis- tic classification). Using AUC-Method-l we can remove     those overly specific subgroups, leading to reasonable posi- tive and negative convex hulls. Notice, however, that CNZ- SD still improves on CN2-standard after removing redun- dant subgroups.

Figure 1. Example ROC curves on the Aus- tralian data set: solid curvesfor AUC-Method- 2, and dotted positive and negative convex hulls for AUC-Method-1; squares for CNZ-SD with additive weights, and triangles for CN2- standard.

5. Related work  Various rule evaluation measures and heuristics have been studied for subgroup discovery [IO, 241. aimed at bal- ancing the size of a group (referred to as factor g) with its distributional unusualness (referred to as factor p). The properties of functions that combine these two factors have been extensively studied (the so-called ?p-g-space?, [lo]).

An alternative measure q = - was proposed in [13], for expert-guided subgroup discovery in the TPIFP space, aimed at minimizing the number of false positives FP, and maximizing true positives TP, guided by generalization pa- rameter par. Besides such ?objective? measures of interest- ingness, some ?subjective? measure of interestingness of a discovered pattern can be taken into the account, such as ac- tionability (?a pattern is interesting if the user can do some- thing with it to his or her advantage?) and unexpectedness (?a pattern is interesting to the user if it is  surprising to the user?) [211.

Instance weights play an important role in boosting [I21 and alternating decision trees [20]. Instance weights have been used also in variants of the covering algorithm imple- mented in rule learning approaches such as SLIPPER [6], RL 1151 and DAIRY 191. A variant of the weighted cover- ing algorithm has been used also in the context of subgroup discovery for rule subset selection [13].

6. Conclusions  We have presented a novel approach to adapting stan- dard classification rule learning to subgroup discovery. To this end we have appropriately adapted the covering algo- rithm, the search heuristics, the probabilistic classification and the performance measure. Experimental results on 17 UCI data sets demonstrate that CNZ-SD produces substan- tially smaller rule sets, where individual rules have higher coverage and significance. These three factors are important for subgroup discovery: smaller size enables better under- standing. higher coverage means larger support, and higher significance means that rules describe discovered subgroups that are significantly different from the entire population.

We have evaluated the results of CN2-SD also in terms of AUC-Method-2 and shown insignificant increase in terms of the area under the ROC curve.

In further work we will evaluate the results also by using AUC-Method-1, where each subgroup establishes a sepa- rate point in the ROC space, and compare the results with the MIDOS subgroup discovery algorithm. We plan to in- vestigate the behavior of CN2-SD also in multi-class prob- lems. An interesting question. to he verified with further ex- periments, is whether the weighted versions of the CN2 al- gorithm improve the significance of the induced subgroups also in the case when CN2 rules are induced without ap- plying the significance test. Finally, we plan to use the CNZ-SD subgroup discovery algorithm for solving practical problems, in which expert evaluations of induced subgroup descriptions is of ultimate interest.

Acknowledgements  Thanks to Dragan Gamberger for joint work on the weighted covering algorithm, and JosC Hernbdez-Orallo and Cesar Ferr-Ramirez for joint work on AUC. The work reported in this paper was supported by the Slovenian Min- istry of Education, Science and Sport. the IST-1999-11495 project Data Mining and Decision Support for Business Competitiveness: A European Virtual Enterprise, and the British Council project Partnership in Science PSP-IS.

Refer en c e s  [ l ]  R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A.I. Verkamo. Fast discovery of association rules. In U.M. Fayyad, G. Piatetski-Shapiro, P. Smyth and R.

Uthurusamy, editors, Advances in Knowledge Discov- ery and Data Mining, 307-328. AAA1 Press. 1996.

[ 2 ]  B. Cestnik. Estimating probabilities: A crucial task in machine learning. In L. Aiello, editor, Proc. of rhe 9rh European Conference on Artificial Inrelligence, 147- 149. Pitman, 1990.

[3] P. Clark and R. Boswell. Rule induction with CNZ Some recent improvements. In Y. Kodratoff, editor, Pmc. of rhe 5rh European Working Session on Learn- ing, 151-163. Springer, 1991.

141 P. Clark and T. Niblett. The CNZ induction algorithm.

Machine Learning, 3(4): 261-283, 1989.

[5 ]  W.W. Cohen. (1995) Fast effective rule induction. In Pmc. of rhe 12th Intermrional Conference on Ma- chine Learning, 115-123. Morgan Kaufmann, 1995.

161 W.W. Cohen and Y. Singer. A simple, fast, and ef- fective rule learner. In Pmc. of AAAI/lAAI, 335- 342. American Association for Artificial Intelligence, 1999.

171 S. Dieroski, B. Cestnik, and 1. Petrovski. (1993) Us- ing the m-estimate in rule induction. Journal of Compuring andlnformarion Technology, 1(1):37 -46, 1993.

[8] U.M. Fayyad and K.B. Irani, K.B. Multi-interval dis- cretisation of continuous-valued attributes for classi- fication learning. In R. Bajcsy, editor. Pmc. of the 13th Inrernarioml Joint Conference on Artificial In- telligence, 1022-1027. Morgan Kaufmann, 1993.

19) D. Hsu, 0. Etzioni and S. Soderland. A redundant cov- ering algorithm applied to text classification. In Pmc.

of the AAA1 Workshop on Learning from Tar Cate- gorization. American Association for Artificial Intel- ligence, 1998.

[IO] W. Klosgen. Explora: A multipattern and multistrat- egy discovery assistant. In U.M. Fayyad, G. Piatetski- Shapiro, P. Smyth and R. Uthurusamy. editors, Ad- vances in Knowledge Discovery and Data Mining.

249-271. MITPress, 1996.

1111 C. Fem-Ramirez, P.A. Flach, and 1. Hemandez- Orallo. Learning decision trees using the area under the ROC curve. In Pmc. of the 19th Internarional Conference on Machine Learning, 139-146. Morgan Kaufmann, 2002.

[I21 Y. Freund and R.E. Shapire. Experiments with a new boosting algorithm. In Pmc. of the 13th International Conference on Machine Learning, 148-156. Morgan Kaufmann, 1996.

[13] D. Gamberger and N. LavraE. Descriptive induction through subgroup discovery: A case study in a medi- cal domain. In Pmc. of the 19th Internarional Confer- ence on Machine Learning, 163-170. Morgan Kauf- mann, 2002.

[I41 N. Lavraf, P. Flach, and B. Zupan. Rule evaluation measures: A unifying view. In Pmc. of the 9th Inter- national Workshop on Inducrive Logic Pmgramming, 74-185. Springer, 1999.

[I51 Y. Lee, B.G. Buchanan, and J.M. Aronis. Knowledge- based learning in exploratory science: Learning rules to predict rodent carcinogenicity. Machine Learning, 30: 217-240, 1998.

[I61 R.S. Michalski, 1. Mozeti?, I. Hong, and N. LavraE.

The multi-purpose incremental learning system AQl5 and its testing application on three medical domains.

In Pmc. 5th National Conference on Artificial Inrelli- gence, 104-1045. Morgan Kaufmann, 1986.

[I71 P.M. Murphy and D.W. Aha. UCI repos- itory of -chine learning databases [http://www.ics.uci .edurmleamiMLRepository.html].

Irvine, CA: University of California, Department of Information and Computer Science, 1994.

[I81 F. Provost andT. Fawcett. Robust classification forim- precise environments. Machine Learning, 42(3): 203- 231,2001.

[I91 R.L. Rivest. Learning decision lists. Machine Learn- ing, 2(3): 229-246, 1987.

[201 R.E. Schapire and Y. Singer. Improved boosting algo- rithms using confidence-rated predictions. In Pmc. of the 11th Conference on Computational Learning The- ory, 80-91. ACM Press, 1998.

[21] A. Silbenchatz and A. Tuehilin. On subjective mea- sures of interestingness in knowledge discovery. In Pmc. of the Isr Internarional Conference on Knowl- edge Discovery and Data Mining, 275-281, 1995.

1221 L. Todorovski, P. Flach, and N. Lavraf. Predictive performance of weighted relative accuracy. In D.A.

Zighed. 1. Komorowski, and J. Zytkow, editors. Pmc.

of the 4rh European Conference on Principles of Data Mining and Knowledge Discovery, 255-264. Springer, 2000.

[231 I.H. Witten and E. Frank. Data Mining: PracricalMa- chine Learning Tools and Techniques wirh Java Imple- mentations. Morgan Kaufmann, 1999.

[24] S. Wrobel. An algorithm for multi-relational discov- ery of subgroups. In Pmc. of the 1st European Sym- posium on Principles of Data Mining and Knowledge Discovery. 78-87. Springer. 1997.

