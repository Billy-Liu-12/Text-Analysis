978-1-4244-5934-6/10/$26.00 2010 IEEE                                 1418

AbstractMining frequent itemset is an important research  topic in association rule area. There are two main kinds of  Algorithm: Apriori Algorithm and FP- growth Algorithm  and their varieties. Generating candidate itemset of Apriori  and traversing tree nodes of FP- growth affect the efficiency  of data mining. This paper puts forward the new simplified  algorithm: eliminating and plotting blocks to the matrix  with simply counting rows and columns, thus, to find out  maximal frequent itemset. The experiment results show that  the algorithm can improve mining efficiency.

Index Termsdata mining; association rule; maximal  frequent patterns; support threshold

I. INTRODUCTION  Mining association rule is a very important research  topic in the field of data mining. Finding frequent patterns  is a key technology in association rule. R. Agrawal et al in  1993 first proposed the famous Apriori algorithm based  on frequent itemset[1, 2], then  most algorithms are  based  on the algorithm for improvement or derivative variant of  the algorithm [6 , 7]. These algorithms are based on  recursive statistics, shear and generate frequent itemset.

The frequent patterns generated at the process require to  scan data sets several times and produce many candidates.

This will greatly reduce the efficiency of algorithms.

In view of Apriori algorithm shortcoming, J. Han et al  proposed a new FP-growth algorithm with the data  structure of the tree without candidate generation [3].

Although the experimental analysis shows that FP-growth  algorithm is faster than improved Apriori algorithm  [3],the data structure of FP-tree and looking for    conditional pattern base require a lot of time. This will  affect the efficiency of FP-growth algorithm. Since then,  although there are some improved algorithms [4, 5], these  ones have not changed basic idea on FP-growth algorithm.

This paper puts forward to a new algorithm: using  matrix data structure to preserve transactional data and  eliminating and divided into sub-matrix to matrix with  simply counting rows and columns, thus, finding out  maximal frequent itemset. In this way, it can greatly  simplify complexity and enhance the efficiency.



II. THE BASIC CONCEPTS  A. Frequent itemset and maximal frequent itemset  Let I = (I1, I2, ... ,Im) be a set of items. M is the total  number of items. Let D, the task-relevant data, be a set of  database transactions where each transaction T is a set of  items such that T?I. Each transaction is associated with an  identifier, called TID. Let A be a set of items. A  transaction T is said to contain A if and only if A?T. A set  of items is referred to as an itemset. An itemset that  contains k items is a k-itemset. Itemset A in the  transaction database D appearing in D, the total number of  accounts for the percentage of affairs, called the itemset  support. The occurrence frequency of an itemset is the  number of transactions that contain the itemset. This is  also known, simply, as the frequency, support count, or  count of the itemset. The itemset support is sometimes  referred to as relative support, whereas the occurrence  frequency is called the absolute support.

Definition 1 (Frequent Itemset): If the relative support  of an itemset I satisfies a prespecified minimum support  threshold (i.e., the absolute support of I satisfies the  corresponding minimum support count threshold), then I  is a frequent itemset. The set of frequent k-itemset is  commonly denoted by Lk. Mining frequent itemset is in  transaction database to identify all frequent itemset and  their actual support.

Definition 2 (maximal frequent itemset): If the frequent  itemset L supersets of all of them are non-frequent  itemset, then called L for the maximal frequent itemset, or  maximal frequent patterns, recorded as MFI (Maximal  Frequent Itemset).

B.Associated matrix data structure    Different data structure will affect the description of  the problem and the efficiency of mining algorithm. Here,  using the data structure of associated matrix to save the  transaction database D. The structure of associated matrix  is as follows: a row vector expresses a transaction in the  transaction database D. If there are total of M different  items in the transaction database D, a row vector is the  length of M. If the first i item in the transaction emerges  the place of the first i sets to 1, otherwise to 0. For an  example, there is abcdef of six items, a current transaction  for the ace, and then the current row vector is (101010). It  is as shown in table 1.

TABLE I.  ACE ROW VECTOR  a b c d e f  1 0 1 0 1 0  Let there are M different items in all transactions and  N transactions, the associated matrix is an N rows and M  columns matrix. The structural matrix that the different  transaction under the same item sets to the same column is  convenience to find frequent itemset. At the same time,    using "1" and " 0 "to express a certain item in a  transaction appears or not, so that the maximal frequent  itemset can be identified through the counting method of  the row and column.

C.The numerical nature of rows and columns in  associated matrix  The columns and rows on the associate matrix can be  counted. Counting the number of 1 on the columns of  matrix gets P1, P2, ?, Pm (m for the total number of  items); Counting the number of 1 on the rows of matrix  gets Q1, Q2, ?, Qn (n is the number of total transactions).

The value of Pi and Qi contains important information  related with frequent itemset, thus maximal frequent  itemset can found out with calculating Pi and Qi. The  following defines Pi and Qi and discusses their nature.

Definition 3 (one-itemset support Pi): Pi counting the  number of 1 on the ith column of associated matrix is  called as one-itemset support or as the greatest possible  support of one item.

In fact ? it is actual support when the minimum    support threshold equals to 1. When the minimum support  threshold min_s ? 1, it is not the actual support threshold.

So it is called as possible support. Pi reflected the  relationship between all transactions on an item. Through  calculating Pi the associated matrix can be simplified and  the important columns can be selected out in advance.

In the matrix, each column represents one item, and  then the number of column is on behalf of the number of  items. The number of column is related with the length of  maximal frequent itemset. So the number of column is a  certain sense. When they meet certain conditions for each  column, the number of column is the length of maximal  frequent itemset.

Definition 4 (one-transaction frequent itemset Qi):  Qi counting the number of 1 on the ith row of  associated matrix is called as one-transaction frequent  itemset. Qi reflected the relationship between all items on  a transaction. Through calculating Qi the associated  matrix can be simplified and the important rows can be  selected out in advance.

Since the algorithm is to find maximal frequent  itemset, thus the size of Qi is on behalf of the importance  of this row creating the maximal frequent itemset. The  larger is Qi, the larger is the possibility of that this row  create maximal frequent itemset. So we may use the  largest Qi to divide matrix or chose rows.

In the matrix, each row is on behalf of a transaction,  then, the number of row is on behalf of the number of  transaction. It is related with minimum support.

According to the definition of minimum support, we  can see that the minimum support threshold can be used to  help determine the number of rows to choose.



III. ALGORITHM DESIGN  A. Algorithm description  The counting algorithm has the following eight steps:  Step 1: Setting up an associated matrix. Scanning  transaction database D once sets up an associated matrix:  setting 0 or 1 of row vector in accordance with a  given order for each transaction.

Step 2: Calculating all one-itemset support Pi and all  one-transaction frequent itemset Qi. The one-itemset  support can be gotten by counting the column value with    1 and the one-transaction frequent itemsets Qi can be  gotten by counting the row with 1.

Step 3: Using elimination method to remove the rows  and columns of the associated matrix according to  following conditions.

There are four conditions.

The first condition: the one-itemset support Pi is less  than the minimum support threshold. The ith column  can be removed because not satisfying the minimum  support threshold.

The second condition: the one-transaction frequent  itemset Qi equals to 0.

The third condition: the one-itemset support Pi equals  to the number of the row.

The forth condition: one-transaction frequent itemset  Qi equals to the number of the column.

The first two conditions can used to eliminate the  columns and the rows of the non-frequent itemset.

If the rows and columns are met with the latter two  conditions, the rows and columns can be selected out in  advance in order to improve the efficiency of algorithm.

After the rows are selected out in advance, the standard  eliminating columns will be changed. It should be one that  minimum support threshold subtracts the number of rows  selected out in advance. When the number of rows  selected in advance is equal to the minimum support  threshold, these rows make up the maximal frequent  itemset.

The elimination method is as follows: First of all,  eliminating the columns which Pi is less than the  minimum support threshold. Then, all Qi of rows are  calculated. The rows which Qi is equal to 0 are  eliminated. Repeat above process until the rows and  columns can not be further eliminated.

In fact, step 3 is one simplifying associated matrix.

Step 4: Qi queue. Qi queue is by descending value of  Qi. This step is to find the largest value of Qi.

Step 5: Divided into sub-matrixes. If the value of all Qi  equals to the number of columns on the matrix, then stop,  otherwise, the matrix is divided into sub-matrixes.

The method is as follows.

At first, the first sub-matrix is divided out according to    the largest Qi. This sub-matrix is composed of the  corresponding rows with the 1 on the columns of Qi.

Then, continuing division from remaining sub-matrix.

Similarly, in the left sub-matrix, all Qi is calculated  according to the largest Qi, until all the rows and columns  are covered.

If there exists across the rows on a division, the  columns of these rows with 1 form a new sub-matrix.

If there is more than the largest Qi, the Qi selected is  one of Qi from top to bottom. The method is selecting the  rows that at least one of their columns includes ones in the  row of the largest Qi.

When divided into sub-matrixes, it is important that the  union set of rows and columns in all sub-matrixes shall  include rows and columns in the original matrix.

Note: the itemset of sub-matrixes after intersection  operation must be empty. Otherwise, the intersection of  the rows is divided into a new sub-matrix.

The following is an example of division of matrix. The  initial matrix is as shown in table 2. Three sub-matrixes  after the division are as shown in table 3, 4 and 5. Table 5  is the new sub-matrix composed by the intersection of the  rows.

TABLE II.  INITIAL MATRIX  I1 I2 I3 I4 I5 I6  1 1 1    3  1 1 1 3  1 1     2  1 1  2  1   1 2  2 2 2 2 2 2  TABLE III.  SUB- MATRIX 1  I1 I2 I3  1 1 1 3  1 1  2  1 1  2 2 2  TABLE IV.  SUB-MATRIX2  TABLE V.  SUB-MATRIX3                      Step 6: Using elimination method in step 3 to the sub-  matrixes and Qi queue  by decreasing order until no new  sub-matrix can be divided out.

Step 7: Selecting the combination of rows in the sub-  matrixes.

According to the minimum support threshold and rows  and columns selected in advance in step 3, the number of  row which Qi is more than minimum support threshold  will be selected out. Selected rows will be combined. The  number of possible combinations is  r  nC : n is the number  of rows selected; r is the minimum support threshold.

Because there are rows or columns selected in advance  in step 3, it is necessary to take into account this factor.

There are four cases discussed.

The case 1: there is no selected rows and columns in  advance.

Because no selected rows and columns in advance, this  case can be dealt with according to normal conditions.

The case 2: there is rows selected in advance.

Because there are the rows selected in advance, the  number of rows to be selected can be reduced. The  number of possible combinations is  r t  n tC  ?

? ?t is the number  of rows selected in advance. In this way, computation can  be reduced to improve the algorithm efficiency.

The case 3: there is columns selected in advance.

Because there are the columns selected in advance,  these columns become the part of the maximal frequent  itemset. These columns do not change the method. In the    end, these columns are only added to maximal frequent  itemset selected.

The case 4: there are selected rows and columns in  advance.

The case 4 is the combination of the case 2 and the  case 3.

According to the case 2, we can use the way to reduce  the number of rows. By the case 3, we can use the way to  add the columns selected in advance to the maximal  frequent itemset, together with the composition of  maximal frequent itemset.

Step 8: Searching for maximal frequent itemset. In  accordance with the selected combination of rows, all of  one-itemset supports Pi are computed out. The maximal  frequent itemset is composed of itemset which the one-  itemset supports Pi equals to minimum support threshold.

The number of the items in the itemset is the maximal  frequent itemset length.

I4 I5 I6  1 1 1 3  1 1  2  1 1  2 2 2  I3 I6  1  1  1 1  1 1 2  2 2    B.an example  To explain the use of the above algorithm, the  following will illustrate an example.

Transaction database D is as shown in table 6. Let  minimum support threshold be min_s = 2.

TABLE VI.  TRANSACTION DATABASE D                          Step 1: Setting up an associated matrix. It is as shown  in table 7.

TABLE VII.  ASSOCIATED MATRIX                          Step 2: Calculating the one-itemset support Pi and  one-transaction frequent itemset Qj. All Pi are (6,7,6,1,2).

All Qj are  (3,1,2,3,2,2,2,4,3).

Step 3: Using elimination method to remove the rows  and columns of the associated matrix according to certain  conditions. Here are items for the I4 column and  transaction for T800 row to be eliminated. The I4 column  is eliminated by the first condition. The T800 row is  eliminated by the forth condition. All Pi are (5,6,5,1). All  Qj are (3,1,2,2,2,2,2,3).A compressed association matrix  is as shown in table 8.

TABLE VIII.  COMPRESSED ASSOCIATION MATRIX  TID I1 I2 I3 I5  T100 1 1  1 3  T200  1   1  T300  1 1  2  T400 1 1   2  T500 1  1  2  T600  1 1  2  T700 1  1  2  T900 1 1 1  3  5 6 5 1    Step 4: Qi queue.

Step 5: Divided into sub-matrix. According to the first  row, the matrix can divided into the two sub-matrixes.

They are as shown in table 9 and 10.

TABLE IX.  FIRST SUB-MATRIX                        TABLE X.  SECOND SUB-MATRIX                  Step 6: Using elimination method to the sub-matrixes  and Qi queue by descending order.

From table 8, T100 row may be eliminated by forth  condition. It is as shown in table 11.

TABLE XI.   FIRST SUB-MATRIX                      As the two rows have been elected, and to meet the  minimum support threshold, and therefore, Table 8 Sub-    Matrix task has been finished.

From table 10, T900 row may be eliminated by forth  condition. It is as shown in table 12.

TABLE XII.  SECOND SUB-MATRIX                TID Commodity ID  T100 I1,I2,I5  T200 I2  T300 I2,I3  T400 I1,I2,I4  T500 I1,I3  T600 I2,I3  T700 I1,I3  T800 I1,I2,I3,I5  T900 I1,I2,I3  TID I1 I2 I3 I4 I5  T100 1 1   1  T200  1  T300  1 1  T400 1 1  1  T500 1  1  T600  1 1  T700 1  1  T800 1 1 1  1  T900 1 1 1  6 7 6 1 2  TID I1 I2 I5  T100 1 1 1 3  T200  1  1  T300  1  1  T400 1 1  2  T500 1   1  T600  1  1  T700 1   1  T900 1 1  2  5 6 1    TID I1 I2 I3  T300  1 1 2  T500 1  1 2  T600  1 1 2  T700 1  1 2  T900 1 1 1 3  3 3 5  TID I1 I2 I5  T200  1  1  T300  1  1  T400 1 1  2  T500 1   1  T600  1  1  T700 1   1  T900 1 1  2  4 5 0  TID I1 I2 I3  T300  1 1 2  T500 1  1 2  T600  1 1 2  T700 1  1 2  2 2 4    As the two rows have been elected, and to meet the  minimum support threshold, and therefore, Table 9 Sub-  Matrix task has been finished.

Step 7: Selecting the combination of rows.

Two rows of T100 and T800 make up new sub-matrix  to search for maximal frequent itemset. It is as shown in  table 13.

TABLE XIII.  THE COMBINATION OF T100 AND T800  Two rows of T800 and T900 make up new sub-matrix  to search for maximal frequent itemset. It is as shown in  table 14.

TABLE XIV.  THE COMBINATION OF T800 AND T900            Step 8: Searching for maximal frequent itemset.

The first maximal frequent itemset is (I1,I2,I5) from  Table 12.

The second maximal frequent itemset is (I1,I2,I3) from  Table 14.



IV. ALGORITHM PERFORMANCE COMPARISON  All the experiments are performed on a AMD Athlon  64*2 Dual core processor 3800+ PC machine with 1G  main memory, running on Microsoft Windows XP. All the  programs are written in Microsoft Visual C++6.0.

Experimental data is generated randomly by  programming. And the number of items is 39.

Experiment result shows that the efficiency of the  counting algorithm is higher in comparison with Apriori  algorithm and FP-tree algorithm. It is as shown in Figure  1.

Figure 1.  Relationship of the support threshold with time consumption

V. CONCLUSION  The core of association mining is mining frequent  itemset. Directly searching for maximal frequent itemset  can increase efficiency of association mining algorithm.

This paper presents a new mining algorithm looking for  Maximal Frequent Itemset. The algorithm with simply  counting the value of rows and columns on associated  matrix can find out the maximal frequent itemset and  greatly simplifies the complexity of association mining  algorithm. Analysis and experiments show that this  algorithm has obvious advantages.

ACKNOWLEDGEMENTS    The paper is supported by the Special Funds for Key  Program of the China No. 2009ZX01039-002-001-04,  2009ZX03001-016, 2009ZX03004-005.

