

Abstract? POWER8? delivers a data-optimized design suited for analytics, cognitive workloads, and today?s exploding data sizes.   The design point results in a 2.5x performance gain over its predecessor, POWER7+?, for many workloads.    In addition, POWER8 delivers the efficiency demanded by cloud computing models and also represents a first step toward creating an open ecosystem for server innovation.



I. INTRODUCTION eeting the needs of evolving server workloads requires new system capabilities.   Data volume is growing exponentially, and ?Big Data? problems can require the  use of exabytes of data within a single application.   Moreover, this mass of ever-growing data is typically unstructured.

Scanning, organizing, and ultimately analyzing large unstructured data-sets requires large memory capacities with high bandwidth, high compute resources, and strong thread performance to address serial code segments.

At the same time, delivering significant performance gains is increasingly difficult.   While technology continues to scale to smaller dimensions, extracting performance gains requires higher levels of innovation, and increasing complexity drives greater resource investments.

In the face of these challenges, POWER8? delivers a data- optimized design suited for analytics and cognitive workloads.

In addition, POWER8 provides the efficiency demanded by cloud computing models and also represents a first step toward creating an open ecosystem for server innovation.



II. DATA OPTIMIZED DESIGN The POWER8? processor [1, 2] is implemented in IBM?s 22nm SOI technology [3]. The 649mm2 die, shown in Figure 1, includes 12 enhanced 8-way multithreaded cores with private l2 caches, a 96MB high-bandwidth eDRAM L3 cache, an on-chip SMP fabric, cryptography and compression accelerators, memory controllers that connect to up to 8 memory buffers, seven SMP links, and 32 lanes of integrated PCI Gen 3 IO.

Figure 1:   The POWER8 Processor   The design point results in a 2.5x performance gain over its predecessor, POWER7+?, for many workloads.    All aspects of the design including the technology, core design, cache structure, and nest architecture are optimized for large datasets, analytics, and cognitive workloads.

A. Technology POWER8 leverages IBM?s 22nm SOI technology with embedded DRAM and deep trench capacitance.   The technology density enables over 4.2 billion transistors and 31.5?F of decoupling capacitance.  15,283 pads provide power, ground, and signal IO.   Three thin-oxide transistor Vt?s are used for power/performance tuning, and thick-oxide devices enable high-voltage I/O and analog designs.

The 15-layer hierarchical metal stack (BEOL) with multiple high performance signal wire planes is specifically architected for high-frequency and low-latency design.   The metal stack enables the creation of high-bandwidth super-highways within the chip that transport up to 3.6TB/s of data between cores, caches, and processor I/O.   As shown in Figure 2, the BEOL contains 5-80nm, 2-144nm, 3-288nm, and 3-640nm pitch layers as well as 2-2400nm ultra thick metal (UTM) pitch layers for low-resistance distribution of power and clocks.

The Power8TM Processor:   Designed for Big Data, Analytics, and Cloud Environments  Joshua Friedrich1, Hung Le1, William Starke1, Jeff Stuechli1, Balaram Sinharoy4, Eric J. Fluhr1, Daniel Dreps1, Victor Zyuban2, Gregory Still3, Christopher Gonzalez2, David Hogenmiller1, Frank Malgioglio4, Ryan Nett1,  Ruchir Puri2, Phillip Restle2, David Shan1, Zeynep Toprak Deniz2, Dieter Wendel5, Matt Ziegler2, Dave Victor1  1 IBM STG, Austin, TX    2 IBM T. J. Watson, Yorktown Heights, NY,   3 IBM STG, Raleigh, NC 4 IBM STG, Poughkeepsie, NY      5 IBM STG, Boeblingen, Germany  M        Figure 2:   POWER8?s Metal Stack   3 specialized SRAM cells enable performance versus density tradeoffs in the cache designs:   (1) 0.160um2 6T with split- wordline for banked read high-performance SRAMs, (2) 0.144um2 6T for density-optimized SRAMs, (3) 0.192um2 8T for two-port SRAMs.   Additionally, ground-rule clean CAM and register file cells enable efficient architecture support, such as the 8 read, 6 write GPR cell.   Most importantly, the embedded DRAM, with a cell size of 0.026um2, provides an area and power efficient implementation of the 96MB L3 cache needed to store large datasets on-die.

B. Core While most multi-core processors focus on core count  increase at roughly constant core performance, the POWER8 core [4], shown in Figure 3, has an enhanced micro- architecture that doubles POWER7?s per-core throughput.

Eight-way simultaneous multi-threading allows eight independent software threads to efficiently share the core resources.   Instruction level parallelism is also exploited with increased dispatch and execution bandwidth.   In a given cycle, the core can fetch, decode, and dispatch up to eight instructions and issue and execute up to ten instructions. There are 16 execution pipelines within the core: 2 fixed-point, 2 load-store, 2 load, 4 double-precision (8 single-precision) floating-point pipelines, 2 fully symmetric vector units for VMX and VSX instructions, 1 crypto, 1 branch, 1 condition register logical, and 1 decimal floating-point pipeline.

To accommodate the large footprint of big data and analytics applications, the POWER8 core has twice as much L1 data cache per core, twice as many ports from the data cache for higher read/write throughput, and four times as much translation (TLB) footprint, compared to POWER7.

Single-thread performance is improved by > 1.5x in POWER8TM with deeper out-of-order execution, better branch prediction, reduced latencies to key resources such as caches and TLBs, and advanced prefetching with greater application software control. In single-thread mode, almost all the resources of the highly-parallel SMT8 core are used by the single thread.

Figure 3:   POWER8 Core Floorplan  C. Cache The POWER8 cache hierarchy [5] consists of an L1 instruction cache, a store-thru L1 data cache, a private, store- in L2 cache, and a large, local L3 cache region built from eDRAM that is part of the chip?s shared L3 cache.

POWER8?s cache hierarchy has been architected for big-data workloads by doubling POWER7?s capacity and bandwidth as shown in Error! Reference source not found..   The L1 data cache increased from 32KB to 64KB, the L2 grew from 256KB to 512KB per core, and the L3 local region increased from 4MB to 8MB.  To enable the larger cache size, the 22nm eDRAM physical design implemented a 2-level bitline hierarchy with 66 cells per bitline to provide a 30% subarray area reduction at constant technology versus the prior 3-level, 34-cell-per-bitline design.

In spite of the increased size, the latencies to each level of cache remain roughly the same as in POWER7.  POWER8 also introduces up to 128MB of shared L4 memory cache per  UTM  5-        UTM  3-  2-  3-8x      processor chip in the attached memory buffer chips  The POWER8 cache hierarchy is plumbed end-to-end for data bandwidth.   Double-wide dataflows extend through the L2 load and store paths, L2 cache arrays, local L3 region read/write paths and cache arrays, and through the on-chip interconnect to the memory read/write interfaces.

TABLE I.  POWER8 CACHE ATTRIBUTES  POWER 7 POWER 8 L1 data capacity / core 32kB 64kB L2 capacity / core 256kB 512kB L3 capacity / core 4MB 8 MB L4 capacity / chip N/A 128 MB L2 bandwidth / core @ 4GHz  256 GB/s 512 GB/s L3 bandwidth / core  @ 4GHz 128 GB/s 256 GB/s Total L3 Interconnect bandwidth 512 GB/s 1382 GB/s    D. Nest POWER8 maintains a balanced system and optimizes for large dataset workloads by scaling the nest architecture and IO bandwidths with the increased compute capacity.   The end to end data and coherence bandwidth of the system is more than 2x POWER7.

Each of the 12 cores connects to the constant frequency fabric via a 192GByte/s asynchronous data interface.   Eight differential memory interfaces supply 230GByte/s sustained memory data bandwidth (>2x POWER7) to the processor from a new memory buffer chip called Centaur (shown in Figure 4).   The processor to buffer interface speed increased from 6.4Gb/s to 9.4Gb/s and the overall memory latency was reduced by roughly 20%.   In addition, Centaur contains 16 MB of eDRAM cache along with an improved DRAM scheduler with support for prefetch and write optimizations.

The scheduling enhancements and L4 cache enable over 90% processor  to memory channel efficiency.

Figure 4:   Centaur Memory Buffer    The on-node and off-node SMP buses enable 494 GByte/s of chip to chip communication that includes data, command, control, error correction, and sparing.  32 lanes of integrated PCIe Gen-3 with 3 PCI Host Bridges (PHBs) provide 64GB/s of IO bandwidth and achieve a DMA read latency less than 1/3 of what POWER7 could achieve with a discrete IO hub chip.

To support POWER8?s 7.6Tbit/s of raw off chip bandwidth (> 160% POWER7?), circuit innovations were needed to reduce I/O power.   The memory interface is 5pJ/bit with a 20dB reach at nyquist.  It invokes all equalization in the receiver, uses decision feedback equalization (DFE-1), and leverages a low-power 4.8GHz CMOS resonant clock distribution.   As a result, it runs 1.5X faster than POWER7+?s memory interface while consuming 50% less power.  All differential interfaces use T-coils for return loss management and deep trench (DT) capacitors for noise control.  The socket to socket interface is single-ended running 4.8Gbit/s (1.5x faster than POWER7+) and enables low-latency, synchronous data transfer.



III. HIGH-EFFICIENCY DESIGN FOR CLOUD ENVIRONMENTS  Servers increasingly operate in cloud environments and hyper- scale data centers where efficiency is an essential metric.   As a result, POWER8 focuses on optimizing for cloud virtualization and energy efficiency.

Cloud partitions often have a small number of threads and, therefore, may not be able to fully utilize all eight threads on the POWER8 core.   To accommodate many small cloud partitions and efficiently use the core?s high throughput, the POWER8 core can be put in a ?split core mode? where four partitions run on the core at the same time, with at most 2 threads per partition.

To increase energy efficiency and maintain POWER7+?s power dissipation in spite of its large increase in performance, POWER8 invested significantly in power management.

The core+L2 & L3 voltage are partitioned into independent power-gating regions using 5 header columns.   A charge pump creates a near zero-power idle state by raising the header gate voltage for >1000x leakage reduction over active mode.

A new On-Chip Controller (OCC) built from an embedded PowerPC? core with 512KB of SRAM runs real-time control firmware to respond to workload variations by adjusting the per-core frequency and voltage based on activity, thermal, voltage, and current sensors.  The on-die OCC achieves ~ 100X speed up in response to workload changes over POWER7+ and enables reaction under the timescale of a typical OS timeslice.   It also allows for multi-socket, scalable systems to be supported as the number of OCCs scales with the number of processor sockets.

POWER8 is also capable of running each core at a unique operating voltage using an internal voltage regulator.

Optimizing both voltage and frequency for workload variation enables ~ 50% increased power savings at ? frequency versus optimizing frequency only.  The distributed regulator includes a central voltage controller (VREGC) which receives a sense line from the grid, a code representing the target voltage, and an external high precision reference voltage. To minimize the error between target and output voltages, VREGC transmits a 7-bit up/down correction code (UPDN) to 64 charge-pump based microregulators (UREGs) which are distributed throughout the 5 header columns and control the headers using a mix of fast-switching and RC filtered control lines.   Figure 5 shows the regulator structure and its power saving benefit.

Figure 5:   Internal Voltage Regulator & Benefit   Other key power reduction techniques such as cache architecture, Vt and device size tuning, and PHY optimization achieved >18% chip power reduction.  Low and high frequency resonant clocking modes for the core mesh saved an additional 4% of chip power.   Each core clock sector includes programmable strength clock buffers, two identical inductors built from the 3um-thick UTM, two mode-switches, and a common large decoupling capacitor node.  Automatic changes between non-resonant, low-frequency resonant, and high- frequency resonant modes during functional operation are supported with no impact to performance through a gradual 15-step process to turn-on the switches connecting the LC tank to the mesh.



IV. OPEN ECOSYSTEM ENABLEMENT Since server workloads will continue to evolve, the POWER8 team wanted to establish an open standard that enabled ongoing off-processor innovation and adaptation in POWER8 systems from a variety of industry sources.   In addition, many workloads can benefit from encapsulating specific algorithms in hardware to support the general purpose cores for a heterogeneous computing solution.

To meet these needs, POWER8 introduces the Coherent Accelerator Processor Interface (CAPI) [6].   CAPI provides the capability for off-chip accelerators to participate in the system memory coherence protocol as a peer of other caches in the system and to use effective addresses to reference data  structures just as an application running on the cores does.

These accelerators can be plugged into PCIe slots and implemented in field-programmable gate arrays (FPGAs) or application-specific integrated circuit (ASIC) chips.

A block diagram of the CAPI hardware is shown in Figure 6.

The Coherent Accelerator Processor Proxy (CAPP) and PCIe Host Bridge (PHB) on the POWER8 processor chip act as memory coherence, data transfer, interrupt, and address translation agents on the SMP interconnect fabric on behalf of the POWER Service Layer (PSL) and Accelerator Function Units (AFU) which reside in an FPGA or ASIC connected to the processor chip by up to 16 lanes of a PCIe Gen3 link.

Address translation is provided by a memory management unit (MMU) in the PSL.   The PSL may also generate interrupts on behalf of AFUs to signal AFU completion, or to signal a system service when a translation fault occurs.

Figure 6:   CAPI Block Diagram

V. CONCLUSION POWER8 delivers a design optimized for big-data, analytics, and cognitive workloads while supporting efficient cloud operating environments and introducing the CAPI framework for off-chip innovation and acceleration.

