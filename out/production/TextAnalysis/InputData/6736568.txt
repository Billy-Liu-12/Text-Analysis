Fifty-first Annual Allerton Conference  Allerton House, UIUC, Illinois, USA

Abstract-A plethora of emerging Big Data applications re? quire processing and analyzing streams of data to extract valu? able information in real-time. For this, chains of classifiers which can detect various concepts need to be constructed in real-time.

In this paper, we propose online distributed algorithms which can learn how to construct the optimal classifier chain in order to maximize the stream mining performance (Le. mining accuracy minus cost) based on the dynamically-changing data character? istics. The proposed solution does not require the distributed local classifiers to exchange any information when learning at runtime. Moreover, our algorithm requires only limited feedback of the mining performance to enable the learning of the optimal classifier chain. We model the learning problem of the optimal classifier chain at run-time as a multi-player multi-armed bandit problem with limited feedback. To our best knowledge, this paper is the first that applies bandit techniques to stream mining prob? lems. However, existing bandit algorithms are inefficient in the considered scenario due to the fact that each component classifier learns its optimal classification functions using only the aggregate overall reward without knowing its own individual reward and without exchanging information with other classifiers. We prove that the proposed algorithms achieve logarithmic learning regret uniformly over time and hence, they are order optimal. Therefore, the long-term time average performance loss tends to zero. We also design learning algorithms whose regret is linear in the number of classification functions. This is much smaller than the regret results which can be obtained using existing bandit algorithms that scale linearly in the number of classifier chains and exponentially in the number of classification functions.



I. INTRODUCTION  Recent years have witnessed the emergence of a pletho? ra of online Big Data applications, such as social media analysis, video surveillance, network security monitoring and etc., which require processing and analyzing streams of raw data to extract valuable information in real-time [1]. Due to privacy issues and proprietary access to information, databases and resources [2] as well as the high computational burdens to fulfill a complex stream processing task, tasks are often decomposed into smaller pieces of subtasks which are assigned to a chain of classifiers implemented on different distributed entities, each of which being responsible for solving one subtask [3][4]. The processing results of these decomposed processing tasks are then aggregated and synthesized to fulfill desired stream mining objectives. For instance, in the video event detection problem [5], rather than directly detecting the event of interest, a set of basic concepts are first detected. The detection results of these concepts are then used to determine the presence of the event of interest. This decomposition  O Parade Gatherin/ ?monstray O  Urban / 0----0 o ? ? Road 0 0 "-.,.O? Convoy ? O  Roadside Bomb  Flag?burning  Protest  Unknown  Fig. I. An Example of Task Decomposition.

has merits that transcend the scalability, reliability and low complexity of large-scale, real-time Big Data stream mining systems. (See Figure. 1)  In this paper, we design online learning algorithms to optimize the classifier chains for Big Data stream mining problems. Unlike existing solutions that require the a priori knowledge of classification functions' performance (i.e. accu? racy and cost) for various types of data characteristics, which is impossible to obtain in practice, our algorithm is able to learn their performance over time and select the optimal classifier configuration. In the considered setting, each classifier in the chain is in charge of a classification subtask (e.g. classification for a specific concept of the data) the results of which are synthesized to obtain the final classification outcome. Each component classifier maintains a number of classification functions from which it can choose the suitable one to use depending on the input data instances. The classification functions have different accuracies and costs when processing different input data sources. Importantly, these accuracies and costs are unknown and may vary over time and hence, they need to be learned online. Hence, to determine the optimal chain of classifiers, the classifiers will need to learn from past data instances and the mining performance which they obtained from a certain configuration in the past.

Learning how to design optimal classifier chains is a challenging problem [10]-[13]. First, the input data stream characteristics can be time-varying and thus, classifier chains require frequent reconfiguration to ensure acceptable classi? fication performance. Classifier chains which are optimized offline are often not able to track the changes in the statistical     characteristics of the data over time. Therefore, an efficient solution requires learning the optimal classifier chain online.

Second, classifier chains are often implemented on different distributed entities due to computational and resource con? straints [10]. Joint optimization between autonomous entities can be a very difficult problem due to legal, proprietary or technical restrictions [2]. Moreover, since the data streams need to be processed in real-time, classifier chains need to minimize their end-to-end delay and thus, they need to be optimized in a distributed way without a centralized entity that can coordinate classifiers' learning process at runtime [11][13]. Third, analytics of individual classifiers may have complex relationships and hence, individual classifiers may have only limited feedback of the classification performance (e.g. the feedback information involves only the classification performance of the overall task but not the subtasks). There? fore, classifiers need to learn in a cooperative, yet distributed manner.

We model the classifier chain learning problem as a multi? player multi-armed bandit problem with limited feedback and design online learning algorithms that address all the above mentioned challenges. The proposed algorithms learn the performance of classifiers in real-time, requiring only one pass of the data set, thereby minimizing the processing delay and the memory requirements of the classifiers. The proposed al? gorithms do not require any runtime coordination by a central entity of distributed classifiers' learning problem and therefore, the communication overhead among classifiers is minimized.

Also, our algorithms learn solely based on the mining quality of the overall task but not that of subtasks, thereby minimizing the feedback information. Most importantly, we are able to analytically bound the finite time performance of our proposed learning algorithms. Finally, we can prove that the convergence rate to the optimal classifier chain is significantly faster than that obtained by existing bandit algorithms in our considered setting.



II. REL ATED WORKS  A key research challenge [6] in Big Data stream min? ing systems is the management of limited system resources while providing desired application performance. Most of the existing approaches on resource-constrained stream mining problems rely on load-shedding [7][8][9], where algorithms determine a discard policy given the observed data character? istics. However, load-shedding may lead to suboptimal end? to-end performance when the data discarded at one stage is essential for a downstream (later stage) classifier. One way to overcome this limitation is to determine how the avail? able data should be processed given the underlying resource allocation instead of to decide on which data to process, as in load-shedding based approaches. In this regard, how to optimally configure the classifier chains are extensively studied in [10]-[13]. In [10], the authors assume that the classifiers' performance is known a priori and determine the configuration of the multiple classifiers by solving an offline optimization problem given a fixed topology. A distributed  [7][8][9] [to] [11][l3] [12] This work End?to?end No Yes Yes Yes Yes consideration  Classifier performance No No No Yes Yes unknown Ontine approach Yes No Yes Yes Yes  Distributed approach No No Yes No Yes  Optimal performance No Yes No No Yes  TABLE I COMPARISON WITH EXISTING WORKS ON STREAM MINING  iterative approach based on reinforcement learning techniques is proposed in [11] to optimize the classifiers' configuration given that the classifier chain topology is fixed and classifiers' performance is known. However, these offline approaches do not adapt to the dynamically changing data characteristics when the classifiers' performance is unknown and may change depending on the characteristics of the different data streams which need to be processed over time. When stream dynamics are initially unknown, a Markov model-based solution for learning the optimal rule for choosing algorithms to recon? figure the classifiers is developed in [12]. However, it requires the state information of the environment and the stream mining utility which is not available in most practical application.

Moreover, for the proposed distributed implementation it does not guarantee that the optimal solution can be learned. Cen? tralized and decentralized online solutions are proposed in [13] to tackle the problem of joint optimizing of the ordering of the chain and the configurations of classifiers. However, it also assumes that the classifiers' performance is known a priori and requires significant communication among classifiers. Even though different orders of classifiers can lead to varying expected processing delay when individual classifiers have the option to discard the data instances (i.e. not forward to the subsequent classifiers for processing), they do not change the mining accuracy [13]. In this paper, we focus on optimizing the classifier chain given the order and view the classifier ordering problem as an important future research topic. We rigorously design learning algorithms without a priori knowledge of classifiers' performance and prove that not only the learned classifier chain converges to the optimal solution but also the time-average performance loss converges to zero rapidly.

This result, which, to our best knowledge, is the first in the Big Data stream mining literature, is important to ensure good mining performance during the learning process. It is especially important when the data characteristics are changing which may not allow the algorithms to determine the optimal classifier chain using a priori knowledge. Table I summarizes the comparison with existing works on stream mining.

We formulate the classifier chain learning problem as a multi-player multi-armed bandit problem with limited feed? back. Literature on multi-armed bandit problems can be traced back to [14] which studies a Bayesian formulation and requires priors over the unknown distributions. In our paper, such infor-     mation is not needed. A general policy is presented in [15] that  achieves asymptotically logarithmic regret in time given that  the rewards from each arm are drawn from an i.i.d. process.

It also shows that no policy can do better than rl(K ln n) (i.e.

linear in the number of arms and logarithmic in time) and  therefore, this policy is order optimal in terms of time. In  [16], upper confidence bound (UCB) algorithms are presented  which are proved to achieve logarithmic regret uniformly over  time, rather than only asymptotically. These policies are shown  to be order optimal when the arm rewards are generated  independently of each other. When the rewards are generated  by a Markov process, algorithms with logarithmic regret with  respect to the best static policy are proposed in [20] and [21].

However, all of these algorithms intrinsically assume that the  reward process of each arm is independent and hence, they do  not exploit any correlation that might be present between the  rewards of different arms.

Another interesting bandit problem, in which the goal is  to exploit the correlation between the rewards, is the com?  binatorial bandit problem. In this problem, the player/agent  chooses an action vector and receives a reward which depends  on some linear or non-linear combination of the individual  rewards of the actions. In a combinatorial bandit problem the  set of arms grow exponentially with the dimension of the  action vector, thus standard bandit policies like the one in  [16] will have a large regret. The idea in these problems is to  exploit the correlations between the rewards of different arms  to improve the learning rate thereby reducing the regret. Since  the classification performance depends on the joint selection  of the classifiers of different segments of the classification  chain, our problem falls into the combinatorial bandit problem  category. In [17], combinatorial bandi ts with linear rewards are  proposed, where the total reward is a linear combination of the  rewards of the individual actions. Authors in [18] address the  decentralized versions of the problem proposed in [17], where  distributed players operate on the same set of arms. Our prob?  lem differs from these works in that, even though distributed  players need to play simultaneously in each time, the sets of  arms that they operate on are different from each other, thereby  leading to disjoint information sets for players. Moreover, the  reward that is revealed comes from the aggregate effect of  their selected arms and hence, players' learning problems are  coupled. In [19], the authors also consider a setting where  multiple players play arms from their own sets. However, the  effect of choosing a particular action is linear on the individual  reward (i.e. proportional to the action the player selects) and  the individual rewards are revealed to all players when players  choose non-zero actions. In our problem, individual rewards  are not revealed at all times. Players have to learn their optimal  actions based only on the feedback about the overall reward.

Other bandit problems with linear reward models are studied  in [20][21][22]. These consider the case where only the overall  reward of the action profile is revealed but not the individual  rewards of each action. However, we do not restrict to linear  reward models and moreover, our algorithm is able to provide  a better regret result in our setting compared to the prior work.

(15]-(17] (18](23](24] (19] (20](21](22] This work Decentralized No Yes No No Yes  Combinatorial with No No No No Yes nonlinear rewards Combinatorial with No No Yes Yes Yes lin ear rewards  Only overall N/A N/A No Yes Yes reward revealed  TABLE II COMPARISON WITH EXISTING WORKS ON BANDITS  Classifier 1 Classifier 2 Classifier M  Source Stream  CI".sifieation .... __ ??c?? __ J Classification result  of concept 1 Classification result of concept 2  Overall claSSification result  Fig. 2. Classifier chain for real-time stream mining.

Classification result of conceptM  Table II summarizes the comparison with existing works on  bandits.



III. SYS TEM M ODEL A. Chains of classifiers  We consider a distributed Big Data stream mining system  consisting of M = {I, 2, ... , M} classifiers. These classifiers are cascaded according to a predetermined order and hence,  the raw stream data goes through these classifiers in sequence.

For notational simplicity, we assume that classifier m + 1 is cascaded after classifier m. An illustrative classifier chain system is provided in Figure 2.

Time is divided into discrete periods. In each period n, there is one data instance x ( n) entering the system. Each data in? stance has a set of concepts y( n) = (Yl (n), ... , YM( n)) where Ym(n) E Ym. These concepts are unknown and require the mining by the classifiers. Concepts Yl(n), ... ,YM(n) jointly determine an unknown ground truth label z(n) E Z according to a deterministic function  J : Yl x ... X YM -+ Z. (1) For example, if the objective is to determine whether the data  instance belongs to a category of interest where Ym(n) = M em E Ym, then J is J(y(n)) = I1 I(Ym(n) = em) m=l where 1(-) is an indicator function. The task of classifier m E M is to make a classification with respect to concept m, denoted by Ym (n). Synthesizing the classification res ults     of all classifiers, denoted by y (n) = (iiI (n), ... , Y M (n) ) , yields the final classification of the label z( n) = a(y( n)).

Each classifier m focuses on the classification problem with respect to concept m and maintains a set of classification func? tions Fm = Um,l, ... , fm,Km} where fm,k : X -+ ?(Ym).

These classification functions can be operating points as in [10]-[13] or more sophisticated feature extraction functions, e.g. SIFT and advanced SIFT for image processing problems [25]. We also note that classifiers can be of different types. For analytical simplicity, we assume Km = K, Vm. In each period n, classifier m picks a classification function am(n) E Fm to classify x( n). Therefore a( n) = (a l (n), ... , aM( n)) represents the classifier chain that is selected in period n.

B. Accuracy, cost and reward Each classification function fm,k of a classifier m has an  unknown accuracy 7r(fm,k) E [ 0, 1]. The accuracy represents the probability that an input data instance will be classified cor? rectly with respect to concept m. Calling upon a classification function fm,k also incurs some (communication/computation) cost. The expected cost is denoted by d(fm,k) which is also unknown.

The accuracy 7r(a) of a classifier chain a depends on the accuracies of its component classifiers. Let 7r( a) = GO" (7r(ad, ... , 7r(aM)) where GO" depends on the deterministic function a. The cost by calling on the classifier chain a is also a function of the costs of individual classifiers. Let the expected cost be d(a) = H(d(ad, ... , d(aM )).

By selecting different classifier chains in different periods, the system obtains a reward that depends on the classification outcome and the incurred cost. Define the reward r( n) in period n as  r(n) = l(z(n) = z(n)) -d(n), (2)  where d( n) is the total cost incurred in period n. Let the expected reward of a classifier chain a be p,(a) = 7r(a) -d(a).

C. Optimal policy with complete information The goal of the designer is to determine an algorithm ?  that selects a classifier chain a( n) in each period n that maximizes the expected reward lE{r(n)}. We summarize the event timeline in each period n below:  ? Each classifier m picks a classification function am (n) E F m to use in this period. Hence, a( n) represents the classifier chain in period n.

? A data instance x( n) enters the system and goes through the classifier chain, yielding the classified concepts y( n) = (VI (n), ... , YM (n)). The final classification result is generated as z(n) = a(Y(n)).

? At the end of the period n, the realized overall reward r(n) according to the true label z(n) and the overall cost d( n) is revealed. Note that classifiers only observe this overall reward but not their own accuracies or costs.

If the accuracies and expected costs of each classification function of each classifier are known, then the optimal solution  is given by (e.g. using the methods in [10][11])  a* = argmax{p,(a)}.

a  (3)  That is, in each time period, the same classifier chain that maximizes the expected reward is selected. However, since the accuracies and the expected costs are unknown, the classifiers have to learn the optimal classification functions over time using the past instances and classification outcomes.

D. The regret of learning We define the regret as a performance measure of the  learning algorithm ? used by the classifiers. Simply, the regret is the performance loss incurred due to the unknown system dynamics. Regret of a learning algorithm ? is defined with respect to the best classifier chain given in (3) and given by  n R(n) = np,(a*) -lE L r(t). (4)  t=l  Regret gives the convergence rate of the total expected reward of the learning algorithm to the value of the optimal solution given in (3). Any algorithm whose regret is logarith? mic in time, i.e. R(n) = O(ln n), will have zero time-average regret when time goes to infinity. However, the constant that multiplies the logarithmic term In n also has a significant impact on the time-average regret over a finite time horizon even though this impact will be eliminated when time goes to infinity. For example, suppose the optimal reward is 1, if the constant is larger than T, then the time average regret up to time T will be larger 1 which gives too loose a bound.

Therefore, this constant should be as small as possible to ensure small finite-time regret.



IV. LE ARNING A L GORI T HMS  In this section, we propose two distributed learning algo? rithms with logarithmic regret (i.e. R(n) = O(ln n)). The first one applies to the general overall reward function but has a large constant that multiplies In n (i.e O(KM In n)). When the problem exhibits certain properties (given by Assumption 1), the second algorithm can significantly reduce the regret compared to the first one (i.e. O(Mlnn)).

Before we describe the learning algorithms, we introduce some notations to facilitate the analysis. We denote ?a = p,(a*) -p,(a) as the difference of the expected overall reward of a classifier chain and that of the optimal classifier chain a*. p,(f;;", a_m) -p'(fm,k' a_m) is the reward difference of a suboptimal classification function fm,k and that of the optimal classification function f;;" by classifier m given the fixed choices of other classifiers a_m. We further let ?:ik = min{p,(f;;", Lm) - p'(fm k, a_m)}, ?:in = min ?:ik a_m ' im,k=l=f:n 1 and ? min = min ??in. ? min is an important parameter m which determines how accurately we can differentiate the best classifier chain and the second best classifier chain and hence, it determines the learning speed. Similarly, we can take maximum to get ?:?k = max{p,(f;;", a_m)-P,(fm,kl a_m)}, a_1n     ? max = max ? max and ? max = max ??ax. ? max m im,k#i,";, m,k m is important in that it determines the maximum regret (i.e.

performance loss) by choosing an suboptimal classification function of a classifier in the chain. Hence, the maximum performance loss by choosing an suboptimal classifier chain is at most M ? max. Finally, we denote D = max { max r( nla) -a min r(nla)} as the bounded dynamic range of the reward for any classifier chain where r( nla) is the overall reward random variable given the classifier chain a.

A. Global learning algorithm  Since the impact of one classification function on the overall reward may vary significantly when it is cascaded with different classification functions of other classifiers, we have to explore every possible classifier chain for sufficiently many times to learn the joint impact of the classification functions in a chain on the final reward. Because the number of all possible classifier chains is large (i.e. KM), this leads to a large constant that multiplies the logarithmic regret term In n.

Our algorithm consists of two phases: exploration and exploitation.

(1) Exploration phase: An exploration phase consists of K M periods. In these periods, all possible classifier chains are selected once in turn. This order is predetermined and hence, all classifiers know the choices of other classifiers at runtime. For example, the exploration order can be loaded into the memory of each classifier at the initialization phase of the algorithm, or classifiers can jointly decide on which exploration order to follow at the initialization phase. This allows us to minimize the communication overhead among classifiers at runtime. Let r( a) record the estimated average reward of the classifier chain a. Using the realized rewards, r( a) is updated at the end of the exploration phase.

(2) Exploitation phase: The length of an exploitation phase changes over time. Each classifier maintains the number of times that it has gone through the exploration phase by the end of time n - 1 , denote by N(n). Let ((n) = Ainn be a deterministic function of where A is a constant parameter.

? If N(n) ? ((n), then the classifiers start a new explo? ration phase starting from time n .

? If N(n) > ((n), then each classifier m selects am(n) such that a(n) = argmaxr(a).

a The regret of this global learning algorithm is established  in the following theorem.

Theorem 1. If the parameter A > 2 (c,!2in ) 2 , then the expected regret of the global learning algorithm after any  number n periods is bounded as follows  R(n) ? A(KM _l)M?max Inn + (KM -1 + B)M?max, (5)  where  B = f>-4 ( "';';'" r (6) t=l  Proof See Appendix A. ? Theorem 1 shows that the global learning algorithm can  achieve the logarithmic regret. This implies that as time n -+ 00, the expected time-average regret tends to 0, i.e.

lim lE{R(n)} = O.

n-+oo n (7)  Note that even if a learning algorithm can learn the optimal classifier chain when time goes to infinity, it may not be able to ensure that the time-average regret is zero. Instead, our pro? posed algorithm guarantees that the performance loss incurred during the learning process is zero when time goes to infinity.

Moreover, our learning algorithm achieves O(KM In n) regret which is the lowest possible regret that can be obtained.

However, since the impact of individual classifiers on the overall reward can be coupled in a complex way, the algorithm has to explore every possible combination of classification functions to learn its performance. This leads to a large constant that multiplies In n which is on the order of K M.

Note that instead of using the proposed global learning algorithm, other learning algorithms such as UeB-type al? gorithms [16] can also be used. In this case, each classifier chain is regarded as a single arm. However, these algorithms do not improve the performance in terms of regret order, i.e. their regret is also O(KM In n). Moreover, they require coordinating the choices of classification functions by local classifiers at runtime which is undesirable in the distributed implementation. Since in our proposed algorithm the choices of classifier chains in the exploration phases are predeter? mined, classifiers do not need to exchange this information at runtime and hence, it has the advantage to reduce the communication overhead and can be more easily implemented in a distributed way.

B. Local learning algorithm In general the overall reward r(a can depend on a in a  complex way. However, if the impact of individual classifiers on the overall reward exhibits some special property, then we can design more efficient algorithms to learn the optimal classifier chain. In this subsection, we propose an efficient learning algorithm when the following assumption holds. For instance, the overall expected reward p,(a) is increasing in individual expected reward p,(am) = 7r(am) -d(am). This is true in many practical systems [12].

Assumption 1. (Monotone Contribution) There exists a func?  tion 9 such that p,(a) is increasing in g(7r(am), d(am)),V'm.

Assumption 1 implies that the optimal classification func? tion of one classifier remains the same regardless of the choic? es of classification functions by other classifiers. Therefore, instead of learning the accuracy and cost of its own classi? fication functions exactly, each classifier only needs to learn the differences between the rewards of its own classification functions, i.e., the relative reward of its own classification functions, to jointly learn how to select the optimal classifier chain.

Our new algorithm also consists of two phases: exploration and exploitation.

(1) Exploration phase: An exploration phase consists of K M periods and is further divided into M subphases with equal length of K periods. Each subphase will be dedicated to the learning problem of one classifier. For classifier m, in the kth period of the ith subphase:  ? If i = m, it chooses am(n) = fm,k.

? If i f= m, it chooses am(n) = arg max f(fm,k).

jm,kEFm Using the realized rewards in its own subphase, a classifier  updates the reward f(fm,k), Vfm,k E Fm at the end of the exploration phase.

(2) Exploitation phase: The length of an exploitation phase changes over time. Each classifier maintains the number of times that it has gone through the exploration phase by the end of time n -1, denote by N(n). Let ((n) = Ainn be a deterministic function of n where A is a constant parameter.

? If N(n) ? ((n), then the classifiers start a new explo? ration phase starting from time n.

? If N(n) > ((n), then each classifier m selects am(n) = arg max f(fm,k).

fm,kEFm Theorem 2 establishes the regret of the local learning  algorithm.

Theorem 2. If the parameter A > 2 C?!2in)2 , then the expected regret of the local learning algorithm after any  number n periods is bounded as follows R(n) ? A(K -l)M ?max In n +[(K -1) + 2K(M -l)e! ( t:.";;inf + 2BlM?max,  where  B = f>-4( t:.,,;;"'f t=l  (8)  (9)  Proof See Appendix B. ? Theorem 2 proves that the local learning algorithm also  achieves the logarithmic regret and hence, as time n -+ 00, the expected time-average regret also becomes zero, i.e.

lim lE{R(n)} = O.

n-+= n (10)  However, since Assumption 1 allows the classifiers to learn their optimal classification functions using only the relative re? wards, the constant that multiplies In n is significantly reduced in the local learning algorithm. In particular, the constant is reduced by approximately KAL" This improvement is enabled by the learning pattern of our proposed algorithm. In particular, even though the choices of classification functions by other classifiers may change over time and are unknown to classifier m, they do not change within an exploration subphase for classifier m. Therefore, taking average of the realized rewards in the past still gives sufficient information of the relative reward of each classification function of classifier m. This regret result (i.e. O(K In n)) is significantly better than conventional multi-arm bandit solutions which show that  -- Local Learning (Proposed) - - - Safe Experimentation  0,25 UCB1 Random Policy  ? ar 0.2 a: ? ? 0.15  - - --- - --- - - ---- - -- --- - - a: OJ ? ? 0.1 1\  0.05 r ? --??----------?  X 10'  Fig. 3. Regret P erformance Comparison.

the regret is linear in the number of arms (in our problem, it is the number of possible classifier chains, i.e. KM).



V. NUMERI C AL RESULT S  In this section, we provide numerical results for our pro? posed algorithms. We consider a problem where the objective is to determine a label z ( n) E lR in each period which is a linear combination of a set of concept values. That is, z(n) = hyT(n) where h is a known coefficient vector. For instance, if h = (11M, ... , 11M), then the label z(n) is the average of the concept values. Each classifier m maintains a set of K classification functions with unknown accuracies and computation costs. It selects one of them each time to estimate  Ym(n). The final overall classification result z(n) is obtained by combining the concept estimations.

Since the considered problem exhibits the monotone con? tribution property, we adopt the proposed local learning algo? rithm and compare against the widely-studied UeB 1 algorithm [15] and the Safe Experimentation learning algorithm adopted in [10][11]. To provide the worst case performance, we also implement a random policy which randomly selects a classifier chain in each period. Figure 3 shows the average regret of these four algorithms when M = 4, K = 3. The curves are generated by averaging over 100 experiments. Since the UeB 1 treats every classifier chain as an arm, the convergence speed is very slow. Because the Safe Experimentation algorithm requires the accurate knowledge of classification functions' performance, it performs poorly when such information is available in the considered setting. By exploiting the monotone contribution property, the local learning algorithm significantly outperforms the safe experimentation algorithm and UeB 1 in terms of much lower regret (performance loss). In Table III, we further show the time-average relative regret of UeB 1 and the proposed local learning algorithm after 105 periods for varying number of classifiers. The number of classification functions of each classifier is fixed to be K = 3. As we can see, the     M= 1 M=2 M=3 M=4 M=5  OCBl 0.0026 0.0148 0.0546 0.1249 0.1548 Proposed 0.0025 0.0030 0.0031 0.0073 0.0166  TABLE III REGRET PERFORMANCE FOR VARYING NUMBER OF CLASSIFIERS.

UCBI Local Learning Message Exchange OeM) 0  Memory Requirement O(KM) O(KM) Regret O(KM Inn) O(Klnn)  TABLE IV IMPLEMENTATION COMPLEXIT Y AND REGRET COMPARISON.

performance gain increases significantly with the increase of the number of classifiers since the arm space of the OCB 1 algorithm has to increase exponentially with the M thereby leading to a very slow convergence rate when M is large.

Finally, Table. IV compares the implementation complexity and learning regret of UCB 1 and the local learning algorithm.

V I. CONCLUSION  In this paper, we proposed online distributed algorithms to learn the optimal classifier chains for real-time Big Data stream mining applications. To our best knowledge, this paper is the first that formulates this problem as a bandit problem and provides the first analytical results on the learning performance for such Big Data applications. The learning regret of the proposed algorithm is linear in the number of classification functions which is much smaller than the regret results that can be obtained by existing bandit algorithms that scale linearly in the number of classifier chains (and hence, it scales exponentially in the number of classification functions).

ApPENDIX A PROOF OF THEOREM 1  We first prove that after L exploration phases, the proba? bility that a non-optimal classifier chain a T a* is selected in an exploitation slot is at most 2e -? ( -%'-) . A non-optimal classifier a i- a* is selected in an exploitation slot only if r(a) > r(a*). Notice that  P(r(a) < r(a*)) > P(r(a*) > p,(a*) -0.5?a) x P(r(a) < p,(a) + 0.5?a).

(11)  Therefore,  <  <  P(r(a) > r(a*)) = 1-P(r(a) < r(a*)) 1 -P(r(a*) > p,(a*) -0.5?a)  x P(r(a) < p,(a) + 0.5?a) 1 -(1 -P(r(a*) < p,(a*) -0.5?a))  x (1 -P(r(a) > p,(a) + 0.5?a)) P(r(a*) < p,(a*) -0.5?a) + P(r(a) > p,(a) + 0.5?a).

(12)  By Hoeffding's inequality,  P(r(a*) < p,(a*) -0.5?a) L(L:>a)2 (13) = P(r(a) > p,(a) + 0.5?a) < e-2""D .

Therefore,  L (?)2 P(r(a) > r(a*)) < 2e-2 D . (14)  By construction, at any time n, at most (( n) + 1 exploration phases have been experienced. Hence, at most KM (((n) + 1) exploration slots have been experienced. For these slots, the regret is at most  (((n)+I) L ?a=A L ?alnn+ L ?a (15) a#a* a#a* a#a*  ::::;A(KM -1)M?maxlnn+(KM _1)M?max. (16)  At any time t < n when it is an exploitation slot, the expected regret by choosing a non-optimal classifier chain a i- a* is at most  2?ae-H?a) 2((t) < 2?ae-H?a)  2Alnn = 2?aC4(?at (17)  The expected regret in the exploitation slots up to time n is at most  n A(L:? 2 n A(L:? 2 2: 2 max ?aC 2 "H' < 2M ? max 2: C 2 "H' t=l a t=l  CX) A(L:? 2 2M?max 2: C2 Da < 2BM?max.

t=l  (18)  Combining the regret in the exploration periods and the exploitation periods, the total expected regret is at most  A(KM -1)M?maxlnn+(KM _1+B)M?max (19)  ApPENDIX B PROOF OF THEOREM 2  Lemma 1. After exploration phases, the probability that a  non-optimal classification function fm,k i- f:n is selected by L (L:>:?:k ) 2  classifier m is at most 2e -2 -D - .

Proof A non-optimal classification function fm,k i- f:n is selected if r(fm,k) > r(f:n). Let b?m be the classification functions selected by other classifiers at the Lth exploration phase, then  1 L lEr(fm,k) = L L p'(fm,k' b?m)' (20) 1=1  Since p,(f:n, b_m) -p'(fm,k' b_m) ;::: ???k' \fb_m,  (21)  Because the realized reward is bounded, according to Ho? effding's inequality, we have  [ (L:>fl"k )2 P(r(fm,k) > r(f:n)) < 2e -"f ';; (22)  ?     Proof of Theorem 2: At any time n, at most ((n) + 1 ex? ploration phases have been experienced. In the [th exploration phase and the mth subphase, the regret caused by classifier m is at most I: ??ak' By Lemma 1, the regret caused by  fm,ki-!;;' ,  classifier i =I m is at most i-I 1 ( t.m"' )2  K?max2e -"2 -D?  , (23)  The regret caused in the exploration phases up to the Lth exploration phase is at most,  t f ( I: .

???k+ I: K?iaxeJ  ( t.:?inr)  l=1 m=1 fm,k#f;;, ,#m < L(K _l)M?max  + f K(M _ l)M ?max2e _I;' ( t.:;;in r l=1  < L(K _ l)M ?max + 2K(M _ l)M ?maxe? ( t.:;;in ) 2  (24)  Since at any time n, at most (( n) + 1 exploration phases have been experienced, the regret in the exploration phases is at most  A(K -l)M ?max In n +[(K - 1) + 2K(M _ l)e? ( t.S',in r]M ?max. (25)  At any time t < n when it is an exploitation period, the expected regret by choosing a non-optimal classification function fm,k =I f:n for classifier m is at most (by Lemma 1)  _ ?(n) t.m,k ( min )2  [4] C. Olston, J. Jiang, and J. Widom, "Adaptive filters for continuous queries over distributed data streams;' in ACM SIGMOD 2003.

[5] Y. Jiang, S. Bhattacharya, S. Chang, and M. Shah, "High-level event recognition in unconstrained videos," Int. J. Multimed. Info. Retr., Nov.

[6] M. Gaber, A. Zaslavsky, and S. Krishnaswamy, "Resource-aware knowl? edge discovery in data streams," in Proc. lstlnt. Workshop on Knowledge Discovery in Data Streams, 2004.

[7] B. Babcock, S. Babu, M. Datar, and R. Motwani, "Chain: operator scheduling for memory minimization in data stream systems," in ACM SIGMOD, 2003.

[8] N. Tatbul, U. Cetintemel, S. Zdonik, M. Cherniack, and M. Stonebraker, "Load shedding in a data stream manager," in Proc. 29th Int. Con! Very Large Data Bases (V LDB), 2003.

[9] Y. Chi, P. S. Yu, H. Wang, and R. R. Muntz, "Loadstar: a load shedding scheme for classifying data streams," in Proc. IEEE Int. Con! Data Mining (ICDM), 2004.

[10] F. Fu, D. Turaga, O. Verscheure, and M. van der Schaar, "Configuring competing classifier chains in distributed stream mining systems;' IEEE Journal of Selected Topics in Signal Processing, vol. 1, no. 4, Dec 2007.

[II] B. Foo and M. van der Schaar, "A distributed approach for optimizing cascaded classifier topologies in real-time stream mining systems," IEEE Trans. Image Process., vol. 19, no. II, pp. 3035-3048, Nov. 2010.

[12] B. Foo and M. van der Schaar, "A rules-based approach for configuring chains of classifiers in real-time stream mining systems," EURASIP Journal on Advances in Signal Processing, vol. 2009, Article ID 975640, 2009.

[13] R. Ducasse, D. Turaga, and M. van der Schaar, "Adaptive topologic optimization for large-scale stream mining," IEEE Journal of Selected Topics in Signal Processing, vol. 4, no. 3, June 2010.

[14] P. Whittle, "Multi-armed bandits and the Gittins index;' Journal of the Royal Statistical Society, Series B (Methodological), 1980.

[15] T. Lai and H. Robbins, "Asymptotically efficient adaptive allocation rules;' Adv. Appl. Math., vol. 6, no. I, 1985.

[16] P. Auer, N. Cesa-Bianchi, and P. Fischer, "Finite-time analysis of the multiarmed bandit problem," Mach. Learn., vol. 47, no. 2-3, 2002.

[17] Y. Anantharam, P. Varaiya, and J. Walrand, "Asymptotically efficient allocation rules for the multiarmed bandit problem with multiple plays - Part I: I. l. D. reward;' IEEE Trans. Autom. Control, vol. AC-32, no. II, 1987.

[18] A. Anandkumar, N. Michael, A. K. Tang, and A. Swami, "Distributed algorithms for learning and cognitive medium access with logarithmic 2?max 2 D m,k e ( t.min )2 ( t.min )2 (26) regret," IEEE 1. Sel. Areas Commun., vol. 29, no. 4, Apr. 2011. A In '11- 1n,k A rn,k  < 2?maXe --2- --D- = 2?maxt - 2 -D-  m,k m,k The expected regret in the exploitation slots up to time n is  at most  (27)  Combining the regret in the exploration periods and ex? ploitation periods, the total regret is at most  A(K -l)M ?max In n +[(K -1) + 2K(M -l)eH t.';in r + 2B]M ?max.

