VARIABLE PRECISION ROUGH SET MODEL BASED DATASET  PARTITION AND ASSOCIATION RULE MINING

Abstract: Discovery association rule is one of the most important tasks  in data mining. Many efficient algorithms have heen proposed in literature. In this paper, a method of dataset-partition using conceptual hierarchy and Variable Precision Rough Set Model is presented. Algorithm of mining association rule using this technique is designed, and asynchronous algorithm is proposed, too. The efficiency of algorithm and the factors that affect the efficiency of algorithm are analyzed hy mining association rule in dataset artificial generated. The result of the experiment proves efficiency of the algorithm.

Keywords: Data mining; Association rule; Conceptual hierarchy  1 Introduction  At present, data mining has become the focus of research in the field such as Artificial Intelligence, Database, and statistics, The main techniques used in data mining include Classification, Cluster, Association Rule, Bayes Net and Server Vector Machine. These techniques have been used in Web mining, failure diagnosis, invasion detection, and so on. Among these techniques, association rule mining attracts most interests of researches in data mining, because of its successful applications in the business of supermarkets. "1'51.

Rough set theory, introduced by Zdaislaw Pawlak in the early 1980s, is a new mathematical tool to deal with vagueness and uncertainty. Since then, this theory has generated a great deal of interests among logicians as well as among researchers dealing with machine learning and data mining. And this approach has been successfully applied in many areas. The Variable Precision Rough Set Model is the extension of Pawlak's Rough set theory, and it can be used to solve the problem of classification while there is no functional or uncertain relation among attributions describing objects. ['I.

In this paper, based on Variable Precision Rough Set Model and association rule mining, we propose a method to partition the dataset of association rule mining Gith conceptual hierarchy among' items of dataset. And the algorithm of association rule mining using this technique, DPARMA (Dataset Partition Association Rule Mining  Algorithm), is presented. The asynchronous algorithm of association rule mining using this technique is presented, too. From the results of the test, we have analyzed the efficiency of the algorithm and the main factor that affects the efficiency of the algorithm.

2 Variable Precision Rough Set Model based Dataset Partition  Among the algorithms of association rule mining, the algorithm that partitions the dataset (PD algorithm) is important and has high efficiency, and it's the base of asynchronous algorithms and distributed algorithms [31. PD algorithm partitions the dataset into data-blocks, searches frequent itemsets in each data-block, and finds frequent itemsets in the whole dataset[''. Methods of dataset partition the most important factor that affects the efficiency of PD algorithm. So, designing new efficient algorithm of data partition is the first important problem of PD algorithms needed to be solved.

2.1 Conceptual hierarchy  Definition 1:The dataset of association rule mining is denoted by D. D=[tl,tz ,..., tk ,..., t"], tt={il,iz ,..., ii: ..., &,I ( lG6n)  is a transaction in dataset. Element ij(llj+p) in tk IS an item. The set of all item in D is denoted by I and I=(ii,iz, ..., i,J(misthenumberof iteminD).

Definition 2:Classes used to classify items in D is a general-item. The set of all general-items are denoted by I, and Ip=[il,iz, ..., i,,)(mg is the number of general-items).

Definition 3:If isIuI,, i'el,, and i belongs to the class denoted by i', then i' is the generalization of i, denoted with i'=g(i), and i is the specialization of i', denoted with i=s(i').

If there is no general-item i" in I, satisfy i'=g(i") and i'=g(i), then, i' is the direct generalization of I, denoted by i'=dg(i) and i is the direct specialization of i', denoted by i=ds(i').

Definition 4Conceptual hierarchy is directed nncycle graph L=<V,E>, and V=IuI,.If i,, i,(lSr,s<(m+mg), ir,i& V)satisfy i?dg(iJ, then there is an arc from i, to i, in L.

Defmition 5:In L=<V,E>, the hierarchy of i, (lSS(m+mg) ,ir= V) denoted by CN(iJ. CN(i,)=l(if the  0-7803-7508-4/02/$17.00 @ZOO2 IEEE  21 75  mailto:qdwang@263.net    number of arcs from i, zero), and if i,,i, (lL,s<(m+mg), i& V) satisfy ipdg(i,), then CN(iJ= CN(i,)+l.

Definition 6The hierarchy number of k < V , E >  is denoted by MCN(L), and  MCN(L)= max CN(i ) ie V  Definition 7:In L=<V,E>, let L(n)=( i I (isIuI,)~(CN(i)=n)] be the n Conceptual Layer of L.

From definition 7, we can conclude that L(I)=I in L=<V,E>.

2.2  Definition 8 : U is a set(the universe), XdJ(X&), Y@(Y a), the error classification rate of X relative to Y is denoted by c(X,Y), and  Variable Precision Rough Set Model  1- I XnY I I I x I ,  I x I >O IxI=o I 0, c(X,Y)=  where I I denotes cardinality of a set .

Definition 9 : An approximation space is a pair (U,R) where R is an equivalence relation defined on U. U / R={Elr& ,...,En] is a set of the equivalence classes of the relation R. Let p be a real number within the range OSpd.5 and set X(XrU), the p lower approximation of X is  R p  = u [ E E U I R I c ( E , X ) < p )  %= U { E E  U / R l c ( E , X )  S p ] -  p upper approximation of X is  R, is called the positive region of X, too. It's the set of objects in U that can be classified into X with error classification rate not greater than p.

2.3 Partition dataset  -  In all kinds of applications of association rule mining, there's usually concephlal hierarchy among items in dataset, 161, and we can use it to partition transactions in dataset. The conceptual hierarchy L < V ,  E> in dataset D reflects the process of generalization of items in D, and different items in D can be generalized to different conceptual layers.

Usually, generalization g(i) of any item or general-item i in L is unique, namely we can classify all items and general- items precisely. Let conceptual layer L(n)={il,iz, ..., i,] (l=amCN(L)), and set set-i,=(i lie L(l),i=s(i)](lGQ), then set-il, set-iz, ..., set-i, is a partition of I. Let R be the equivalence relation that U / R={set-i,, setLiz ,..., set-ir), we can use d e s  listed below to partition transactions in dataset D.

Definition lO(ru1es of transaction partition): Let L=<V,  ,E> be the conceptual hierarchy in dataset D, conceptual layer L(n)=(il,iz, ..., i,)(l<nSMCN(L)), set-i,=(i I ie L(l),i= s(i)](lGQ), and a real number p within the range O<p<O.S.

For any transaction T(TE D) , it can be pdt ioned  into one  data-blockamongD1. Dz- ___. D,, Df, (1) If Lower approximation of T, R B ( T ) =  u{set-i E D/Rlc(set-i , T ) s  p )  ,Satisfy -  R B  (T)  = Iset-i = l(1 5 s 5 r )  I- I and  c(T,set-i,) 5 p then transaction T i s  partitioned into data-block D,.

(2)For all set-il, set-i2, .. ., set-i,, Rg (7') not satisfy -  R B ( T )  =lse-i 8 1 = l ( l < s 5 r )  c(T,set-i 8 )  < p  I- I and  then transaction T is partitioned into data-block Df.

Algorithm DP(dataset partition) is listed below.

Algorithm 1 : DP algorithm Input: dataset D, set-il, setLi2. ..., set_ir,n. B, Output: Partition of dataset D D 1 ~  D2- '_..-  Dr\ w: ( I )Dl=0.  D2=0. .... h 0 ~  W=0: (2)foNeach transanion TED) ( 3 1 (4) for(each set is  E (set-il. set~i2.  .. ., set-ir)) (5) (6) DFDSU(TJ: (7)if(Taodeachoneofset~iI,set_lZ. .... setirdon't satisfy  (8) W = W u I T l :  if(T and setLis satisfy NI~S of transaction pi i t ion )  N k s  O f m a C t i O "  @tiO")  3 DPARMA Algorithm  DPARMA algorithm mines all association rules in dataset D by mining data-blocks D1.  Dz. ..., D,. Df step by step. Let data-block D.E ( DI, Dz. ..., D,, Df], and set of items in D, is ID,  , then IDS = set-i,u( ID,  - set-i,).

Definition 11 (rules of itemset partition): Let any Xyet-is, and any X'G( ID,  - set-is). If itemset XuX' is in data-block D,, it's classified using rules below:  and I ~ I ~ 1 - B * ( m a x ( l s e t - i , I , l s e t _ i , l  P ...., Iset-i ,I~) , then  itemset XuX' can only be included in data-blocks D, and Df, and it is classifid into itemset class I.

(2) Otherwise, itemset X u X  can be included in data- blocks D1, Dz\ _..- D, and D ~ ,  and it is classified into itemset class n.

Theorem 1 : Let 1 T, I denote the average length of all transactions in data-block D,, n=p* IT, 1 ,  n'=(I-P)* 1 T, 1 , max-s= g  -*(-I Iset_ill,(sCt-i2l ,._.. l * e t - i , l ) ) ' I - P      max i= B * (i is no-negative integer). The number of  itemsets in data-block D, which belongs to itemset class I is not greater than nl,  1 - B  l i  jlmar-z  The number of itemsets in data-block D, which belongs to itemset class U is not meater than nn.

I < i < W _ S mar_i<jrn'  From definition 10 we get max-sxnax-i, and from equation (I) and equation (2) in theorem 1 we get nl>nll.

That is, the number of itemsets belonging to itemset I is greater than the number of itemsets belonging to itemset II in data-block D, and the support of these itemsets can he counted by only scanning data-blocks D, and Df.

DPARMA algorithm makes full use of it to reduce the number of candidate itemsets, find all frequent itemset as fast as possible and improve the efficiency of finding frequent itemsets in dataset D.

(1) Scan data-block DI: Partition all possible itemsets in data-block D, into itemset class I and itemset class U, denoted by C1, Cy respectively. Scad data-block DI and connt the support of all itemsets in C l u  Cl, in this data- block. Scan data-block ,Df and count the support of all itemsets in CI in this data-block. So we can find frequent itemset of whole dataset D in CI with definition 11 and the set of these frequent itemsets denoted by F1.

(2) Scan data-block D, ( 2 9 3 ) :  Partition all possible itemsets in data-block D, into itemset class I and itemset class E, denoted by C,, C,, respectively. Scan data-block D, and count the support of all itemsets in C,uC,,uC,.

Iutl., .uC,, in this data-block. Scan data-block Df and connt the support of all itemsets in C, in this data-block. So we can find frequent itemset of whole dataset D in C, with definition 11 and the set of these frequent itemsets denoted by F,.

lDr*I ID1  (3) Scan data-block Df: (i) If - 2 minsupport, then  count the support of all itemsets in C i u  ... kl,. Scan each data-block once more to count the support of itemsets in C f u  ...uC1.. During the scanning of this time, we can find some frequent itemsets when finishing scanning one data- block. For example, when finishing scanning data-block Ds(I<sG-I), we can find frequent itemsets in C,,+Iy. All frequent itemsets in C f u  ...MI, denoted by Fi. Now all frequent itemsets in dataset D have been found. (ii) If  - IDJ > minsupport, then count the support of all itemsets ID1  in C f u  ...U?,,. At the same time, find all frequent itemsets of data-block D< but not in C f u  ... uCI,, and these itemsets denoted by Cf. Scan each data-block once more to count  the support of itemsets in CpuCfu  ... uCI,. We can find frequent itemsets of whole dataset D like (i). All frequent itemsets in C p u G u . .  .uCI, are denoted by Fp.

(4) Let F be the set of all frequent itemsets of whole dataset D, then F = F l u F ~ u  ... uF# or F=F,uF,u ... uFfuFp DPARMA algorithm is listed in algorithm 2 as follows.

Algorithm 2: DPARMA algorithm  D,- Di. . . .~  D,, DA minsupport, minconlidence.

(1) F=0, (2) Scan dam-block DI to decide set CI, CI.. Count suppon of itemsets in CI and Cy and decide set F,; (3) F=Fu Fl;s=2: (4) While(sSr) (5 )  I (6) of iternsets in C,uC,-&,~tu ... Ucr and decide set Fa; (7) F=FuF,;s++: (8) I (9) Scan data-block DA count suppon of itemseIs in CW...Uc,. or ccvcrJ...Ucr: (1O)Scan all dam-block once more to decide F+ or Fp: (1 I)F=FuFt 01 FuF+:  (13)R=generate_mle(F): (14)Renun(R):  Input: dam-blocks generated by DP algorithm  Output: set of all association mles R.

Scan datahlock D, to decide set C,, C,,. Count suppon  ( W I  4 Parallel Algorithm based DPARMA Algorithm  DPARMA algorithm can be redesigned to parallel algorithm easily.

(1) Assign data-blocks DI,Dz ,..., D, to processor Pl,Pz ,..., P, respectively, and data-block Di to each processor.

(2) Processor Pa(lSsG) partitions all possible itemsets in data-block D, into itemset class I and itemset class 11, denoted by C,, C,. respectively. Find all frequent itemsets of data-block D< but not in C,uC,,, and these itemsets are denoted by Ci. Scan data-block D, and count the support of all itemsets in C,uC,uCf in this data-block. Scan data- block Df and count the support of all itemsets in C, in this data-block. So we can find frequent itemsets of whole dataset D in C, with definition 11 and the set of these frequent itemsets of whole dataset D are denoted by F,.

(3) Processor P3(lSsSr) broadcasts the support of itemsets in C ,M,uC< to other processors.

(4) Processor PS(KsG) gets the support of itemsets in C l u  ...uC, uC,,uC< from other processors, scans data- block Di and counts the support of all itemsets in Cf in this data-block. According to minsupport, Processor P, finds frequent itemsets in C l u  ... uC,uC.,uCf and these frequent itemsets are denoted by F. F is the set of all frequent itemsets in the whole dataset D.

(5 )  Each processor generates rule separately.

5 Performance  To study the effectiveness and efficiency of DPARMA     algorithm; we implement it and other algorithms of association rule mining in C++ and test them in a PC with a CPU clock rate of 1.5'3, 256 megabytes of main memory, and under the Microsoft Windows2000 operating system.

5.1 Synthetic Dataset Generation  The dataset used in the test is generated, using a randomized item set .generation algorithm similar to the algorithm described in [4], and we set parameter N to 512 and &1=1000.

Table 1. Generated dataset  In order to study the effectiveness and efficiency of DPARMA algorithm, especially when p or Df are different, we generate the same size data-block DI,D2 ,..,,Dr according to p firstly, then generate data-block D$, and combine these data-block to dataset D.

Generate data-block DI,D2 ,..., Dr: partition set of items I into 11, Iz, ..., I,, and these sets of items are the sets of items in data-block DI, DZ , ..., Dr respectively. In order to generate a transaction T' in data-block D,(lSsQ), a transaction T is generated from set of items I, firstly. Then, a(0SaSpxlTI) items from sets of items IluI~u ... U 13.1u 18+,u ... u1, are chosen randomly to form itemset X. Finally, let T=TuX.

Generate data-block Df: transactions in data-block D+ are generated from sets of items I ,uIzu ... uI,.

Generate conceptual hierarchy in dataset: we generate the same conceptual hierarchy L for all dataset, and MCN(L)=5, I L(1) I =512, 1 L(2) I =128, I L(3) I =32, I L(4) 1=8, 1 L(5) I =2, each general-item in conceptual layer L(2), L(3), L(4) and L(5) have four direct specialization item.

Dataset in Table 2 is generated by the method provided above.

Table'2. Datasets used to test DPARMA algorithm  daraset  5.2 Comparing DPARMA algorithm with other algorithms of association rule mining  We use dataset in Table 1 to test AIS algorithm, Apriori algorithm, DIC algorithm(partition dataset into 10 data- block freely), and DPARMA algorithm(parti6on dataset into 10 data-block freely). The result is given in Figure I .

We use dataset in Table 2 to test AIS algorithm, Apriori algorithm), DIC algorithm (partition dataset into 10 data- block freely), and DPARMA algorithm (partition dataset with DP algorithm, p=0.2, choose conceptual layer L(3)).

The result is given in Figure 2.

Fig.1. Comparing with other algorithms (Using dataset in Table 1, minsupport=l%)  dataset  Fig.2. Comparing with other algorithms (Using dataset in Table I, minsupport=l%)  ~  5.3 Main factors affecting DPARMA  In order to test the effect of p to DPARMA algorithm, we generate dataset DlOkT1618 with different p. The result of testing is in Figure 3(choose conceptual layer L(3)). The result of the test that different conceptual layer is chosen is in Figure 4(dataset is DIOkT1618, p=0.2).

Fig.3. Effect of p to DPARMA algorithm (Using dataset DlOkT1618, minsupport=l%)  0 0. I 0. 2 0. 3 0.4 0. 5  9 i 7 R     Fig. 4. Effect of conceptual hierarchy (Using dataset 10kT1618, minsupport=l%)  6 Conclusions  This paper has proposed a new algorithm of association rule mining. From the result of the performance test, we know that the efficiency of this algorithm is decided by several factors, such as p, conceptual layer chosen, etc..

That is what we should pay more attention to when we using it to the real world.

