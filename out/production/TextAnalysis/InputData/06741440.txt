Denoising Predictive Sparse Decomposition

Abstract?Recent years have witnessed the great success of sparse coding in many areas, including data mining, machine learning, and computer vision. Sparse coding provides a class of unsupervised algorithms for learning a set of over-complete basis functions, allowing to reconstruct the original signal by linearly combining a small subset of the bases. A shortcoming of most existing sparse coding algorithms is that they need to do some sort of iterative minimization to inference the sparse representations for test points, which means that it?s not conve- nient for these algorithms to perform out-of-sample extension. By additionally training a non-linear regressor that maps input to sparse representation during the training procedure, predictive sparse decomposition (PSD) can naturally be used for out-of- sample extension. Hence, PSD has recently become one of the most famous learning algorithms for sparse coding. However, when the training set is not large enough to capture the variations of the sample, PSD may not achieve satisfactory performance in real applications. In this paper, we propose a novel model, called denoising PSD (DPSD), for robust sparse coding. Experiments on real visual object recognition tasks show that DPSD can dramatically outperform PSD in real applications.

Keywords?Sparse coding, denoising, predictive sparse decomposi- tion, object recognition

I. INTRODUCTION  Sparse coding has been proved to be a useful framework for object recognition, which is one of the most challenging tasks in computer vision. It can be used to build a deep hierarchy of features and is one of the building blocks for deep learning [7], [2], [6]. Sparse coding approximates the input, in terms of a sparse linear combination of over-complete basis functions or the so-called dictionary. The resulting sparse representations are more likely to be separated in the high- dimensional space. This mechanism is seen as a resemblance of orientation selective cells in the primary visual cortex V1 [20], [23]. Also, it can produce localized dictionary when applied to other fields like speech and video [14], [21]. In [30], sparse coding is used to do human action recognition task.

Histograms of sparse codes are computed for object detection in [24]. Algorithms to solve convolutional sparse coding are proposed in [11] and [4]. In [3], multipath is used to learn a set of multi-layer sparse coding architectures.

Due to the wide application in many areas, a lot of algorithms have been proposed to solve the sparse coding problem [5], [1], [13], [19], [26], [16]. In [1], K-SVD is applied for dictionary design by adapting the dictionary to a set of training samples. Matching pursuit (MP) [17] and orthogonal matching pursuit (OMP) [27] are greedy pursuit  algorithms that select the basis functions sequentially. In [13], a feature-sign search algorithm is proposed to learn the sparse representations efficiently.

One shortcoming associated with most existing sparse coding algorithms is that they need to do some sort of iterative minimization algorithm to inference the sparse representations for test points, which means that it?s not convenient for these algorithms to perform out-of-sample extension. Predictive s- parse decomposition (PSD) [11], [9], [10], [8] is a recently proposed algorithm which can naturally be used for out-of- sample extension by additionally training a non-linear regres- sor that maps input to sparse representation during the training procedure. After training, PSD only need to do a simple feed- forward prediction to obtain the sparse representations for test points. Hence, PSD has recently become one of the most famous learning algorithms for sparse coding. However, when the training set is not large enough to capture the variations of the sample, PSD may not achieve satisfactory performance in real applications.

In this paper, we propose a novel model, called denoising PSD (DPSD), for robust sparse coding. Experiments on real- world visual object recognition tasks show that DPSD can achieve better performance in real applications.

The following content of this paper is organized as follows.

Section II formulates the sparse coding problem and describes the PSD algorithm. Then we elaborate our DPSD algorithm in Section III. Section IV illustrates the superiority of our DPSD algorithm over the normal PSD algorithm on real visual object recognition tasks. Finally, section V concludes our work and describes our future work.



II. PREDICTIVE SPARSE DECOMPOSITION (PSD)  A. Standard Sparse Coding  The standard formulation of sparse coding is to minimize the following objective function:  L(X,Z;D) = 1 ?X ?DZ?22 + ??Z?1, (1)  where ? is the regularization hyper-parameter, X ? Rn is the given input, Z ? Rm is the sparse representation we want to find, and matrix D ? Rn?m is a set of basis functions with m > n for over-complete representation. However, without any constraint on D, one can easily obtain a trivial solution by increasing D and decreasing Z by a same constant, which reduces the second term in (1) while keeping the first term     fixed. So the columns of D are usually required to have unit L2 norm.

From (1), it is easy to find that given a new test point, we need to optimize the objective function in (1) to get the sparse representation of this test point even if the basis functions D have been learned. Typical algorithms use iterative methods such as gradient descent for optimization. Hence, it is not easy for these standard sparse coding algorithms to perform out-of- sample extension.

B. PSD  PSD [11], [9], [10], [8] trains a feed-forward non-linear regressor F (X; ?) parameterized by ? to directly map the input X to its sparse representation Z, which can easily complete the out-of-sample extension.

The new objective function is as follows:  L(X,Z;D,?) = 1 ?X ?DZ?22 + ??Z?1  + ??Z ? F (X; ?)?22, (2)  where the hyper-parameters ? and ? are regularization weights, and F (X; ?) is usually defined as follows:  F (X;G,W,B) = G tanh(WX +B) (3)  parameterized by the filter matrix W ? Rm?n, the bias vector B ? Rm, and a diagonal matrix G ? Rm?m. tanh is the element-wise hyperbolic tangent function, G is used to scale the limited range of tanh, and ? = {G,W,B}. PSD tries to find a solution that can be approximated by a non-linear regressor to achieve both stability and fast inference. As with sparse coding described by (1), the columns of D need to be normalized.



III. DENOISING PREDICTIVE SPARSE DECOMPOSITION (DPSD)  One problem with PSD is that PSD may not achieve satisfactory performance in real applications when the training set is not large enough to capture the variations of the sample.

In this paper, we propose a novel algorithm, called Denoising Predictive Sparse Decomposition (DPSD), which can learn a sparse representation that is robust to noise in the input and produce the sparse representation directly.

A. Model  It is known that a dictionary D that can capture the structure of the input, will also be adept at removing noise from input [22]. To make the learned representation robust to partial corruption of the input pattern, we modified the encoder of PSD to map a corrupted version of the input. First we do a corrupting process: adding a noise to each input X by means of a stochastic mapping X? ? q(X?|X). Then the corrupted input X? is mapped by the regressor, and the reconstructed input DZ is measured in terms of distance with the original input X . The point is that we want to learn the data distribution from out of  ?X ? X??2 DZ  X  X? G tanh(WX? +B) ?Z ? Z??2  Z  |Zj |  ? ?  j |Zj |  Fig. 1. Illustration of our DPSD algorithm.

the noise. The objective function is changed as follows:  L(X,Z;D,?) = 1 ?X ?DZ?22 + ??Z?1  + ??Z ? F (X?; ?)?22 (4)  =  ?X ?DZ?22 + ??Z?1  + ??Z ?G tanh(WX? +B)?22. (5)  The whole structure of the DPSD algorithm is shown in Fig.1, where X? = DZ and Z? = G tanh(WX? +B).

Theorem 1: For small Gaussian noise, the corruption step of our DPSD is approximately equivalent to introducing a contractive term to the original PSD objective.

Proof: For small ?, add a Gaussian noise ? ? N (0, ?2I), to input X , we get X? = X+?. As can be easily seen, we only change the third term of PSD?s objective function (2). Now we consider the expectation of this term after adding noise to the input1:  E(?Z ? F (X?)?22) = E(?Z ? F (X + ?)?22)  ? E(?Z ? (F (X) + ?F ?X  ?)?22)  = E(?(Z ? F (X))? ?F ?X  ??22)  = E(?Z ? F (X)?22)  + E((Z ? F (X))(? ?F ?X  ?)T )  + E((Z ? F (X))T (? ?F ?X  ?))  + E(? ?F ?X  ?22???22).

Since ? ? N (0, ?2I), E(?) = 0, and E(???22) = ?2, we get that:  E(?Z ? F (X?)?22) ? E(?Z ? F (X)?22) + ?2? ?F  ?X ?2F .

Now we can see that adding a small Gaussian noise to input X is equivalent to adding a scaled Frobenius norm of the encoder function?s jacobian matrix to the original objective function.

As discussed in contractive auto-encoder (CAE) [25], the introduced contractive term penalizes the sensitivity to the  1For clarity, we omit the parameters G,W ,B of F (X;G,W,B) in the following proof and write F (X;G,W,B) as F (X)     input, which has the same effect as applying the denoising principle. However, adding a noise is much easier than com- puting the Frobenius norm of the encoder function?s jacobian matrix in terms of time complexity.

Denoising auto-encoder (DAE) [28], [29] uses similar denoising principle to achieve better performance than normal auto-encoder. However, DAE cannot achieve sparse results.

On the contrary, our DPSD algorithm can get the sparse representation that is more robust to noise.

B. Learning  We use alternative optimization to train our DPSD model.

The optimization procedure of the parameter learning phase in DPSD consists of two steps. When all the parameters except Z are given, minimizing the loss of (5) with respect to Z reduces to solving a L1 regularized least squares problem that is convex in Z. On the other hand, once Z is given, the problem reduces to a least squares problem with quadratic constraints that is also convex in the parameters and can be learned with a coordinate gradient descent algorithm. The whole learning procedure of our DPSD model is briefly described in Algorithm 1. As the total loss (5) is reduced in each step of the algorithm, G,W,B will be converged after several iterations. And the complexity of our algorithm is almost the same as PSD, for the computational cost of the additional corruption process at the beginning of each iteration can be ignored in comparison with other steps.

Algorithm 1 Learning algorithm for DPSD 1: Input: X ? Rn 2: Initialization: randomly initialize D,G,W,B, initialize Z  by the output of (3).

3: repeat 4: Get a corrupted input X? using X? ? q(X?|X) for input  X .

5: Keep the parameters (D,G,W ,B) constant. Use gradient  descent to get the optimal Z value.

6: Using the Z value from the previous step, update pa-  rameters (D,G,W ,B) using stochastic gradient descent.

7: Re-scale the columns of D to unit L2 norm.

8: until G,W,B has converged 9: Output: G,W,B  C. Inference  After we have learned the parameters G,W and B, we can easily perform inference for any new input Xt. More specifically, to get the sparse representation Zt of Xt, we only need to do the following straightforward mapping without any iterative computation like [13]:  Zt = G tanh(WXt +B). (6)

IV. EXPERIMENTS  In the experiments, we consider two stochastic mappings.

One is the isotropic Gaussian noise: X? = X + ?, ? ? N (0, ?2I). The other is binary masking noise: for each com- ponent of the input, set its value to 0 according to a Bernoulli distribution with probability p. In our experiment, we call the value of ? and p noise level.

A. Datasets and Evaluation Scheme  Three datasets are used in our experiments. They are Berkeley dataset [18], MNIST dataset [12], and Caltech 101 dataset [15].

The Berkeley dataset2 includes natural images for segmen- tation. We will randomly sample patches of different sizes from the Berkeley dataset, then convert each image patch to gray- scale values and normalize it to zero mean and unit standard deviation. Some example images from this dataset are shown in Fig.2(a).

The MNIST dataset3 has a training set of 60,000 hand- written digit images and a test set of size 10,000. The digits in the image have been size-normalized and centered and all the images are gray-scale and of size 28?28. We show some example images in Fig.2(b).

The Caltech 101 dataset4 has 102 generic object categories including the background class. Each category has about 40 to 800 color images and each image is about 300?200 pixels.

Fig.2(c) shows some samples from Caltech 101. We pre- process the images to be gray-scale and of size 143?143 in the same way as proposed by [10].

Similar to the experimental settings in PSD [9], we use Berkeley dataset to measure the average signal to noise ra- tio (SNR):  SNR = 10 log 10( ?2Signal ?2Noise  ), (7)  where Noise = Signal ? Approximation and ?2 denotes the variance, between the regressor output and the sparse representation Z. Furthermore, we use the MNIST and Caltech 101 datasets for visual object recognition evaluation.

B. SNR Comparison  To compare the representation ability between our DPSD algorithm and traditional PSD together with measuring the similarity between the sparse representation Z (called Optimal) and the output of the regressor Z? = G tanh(WX?+B) (called Predictor), we calculate SNR over random patches from the Berkeley dataset. That is to say, Optimal denotes the Signal in (7), and Predictor denotes the Approximation in (7). We randomly pick 20,000 9?9 and 12?12 patches respectively from the images of the Berkeley dataset. We use 9?9 patches to learn sparse codes with 64 units and use 12?12 patches to learn sparse representations with 256 units. In this experiment, we set both ? and ? to 1.

The results are shown in Table I, where we can see that DPSD achieves much higher SNR than PSD no matter which kind of noise is used. Because higher SNR means better approximation to the signal, we can say that DPSD can achieve better performance than PSD.

Next, we measure how different noise levels influence the Optimal/Predictor SNR of our DPSD algorithm. In Fig.3, we calculate SNR on the 9?9 image patches of Berkeley  2http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/ 3http://yann.lecun.com/exdb/mnist/ 4http://www.vision.caltech.edu/Image Datasets/Caltech101/     (a) Berkeley  (b) MNIST  (c) Caltech 101 Fig. 2. Example images from the datasets  TABLE I. SNR COMPARISON BETWEEN DPSD AND PSD.

Signal to Noise Ratio(SNR) Algorithm 9?9 patches 12?12 patches  Gaussian Bernoulli Gaussian Bernoulli DPSD 9.38 9.55 6.65 6.85 PSD 8.3 6.00  dataset. In Fig.3(a), the blue line indicates the SNR of PSD algorithm, and the red line corresponds to the SNR of our DPSD algorithm with different Gaussian noise. We can see that when the standard deviation of the Gaussian noise ? is small (less than 0.6), our DPSD algorithm has higher SNR value than PSD, and achieves the highest value when ? is 0.3.

When the value of ? becomes larger, the value of SNR drops quickly, which makes sense as adding too much noise to the input can change the data distribution to a totally different one.

Fig.3(b), shows similar curves in which we add a Bernoulli noise. We get the best SNR value when the p value of Bernoulli distribution is 0.1 in Fig.3 (b). Similar results can be achieved with 12?12 patches, which are shown in Fig. 4.

C. Recognition Accuracy On Caltech-101  We randomly sample 50,000 9?9 patches from the pre- processed images of the Caltech 101 dataset and use these patches to train our DPSD algorithm to learn sparse codes with 64 units. To measure the accuracy, we use an unsuper- vised model architecture identical to the one proposed in [8].

0 1 2 3 4 5 6 7 8 9 10            standard deviation of the Gaussian noise  S N  R  SNR over 20000 Berkeley 9x9 patch samples  DPSD PSD  (a) Gaussian Noise  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 ?4  ?2        noise level  S N  R  SNR over 20000 Berkeley 9x9 patch samples  DPSD PSD  (b) Bernoulli Noise  Fig. 3. SNR comparison for different noise levels over 20000 9?9 image patches  Through the filter bank layer, we use our DPSD algorithm convolutionally to get 64 feature maps of size 135?135. Then we apply abs rectification to the feature maps and do local contrast normalization using a Gaussian window kernel of size 9?9. After that, we do average pooling with a boxcar filter of size 10?10, and down-sampling of size 5?5, which produces 64 feature maps of size 26?26. A simple logistic regression classifier is used to recognize the objects in the images.

We randomly choose 30 images per class for training and up to 30 images per class for testing. We use a validation set to find the optimal noise level (? = 0.7). The recognition accuracy of DPSD on the test set is 55.5%, while PSD?s recognition accuracy is 53.2%. Hence, we can find that DPSD performs better than PSD when proper noise is added.

Then we measure the recognition accuracy on the same testing set but use different number of images per category for the training set. We randomly sample 5, 10, 15, 20, 25 images per category from the previous training set respectively and calculate the recognition accuracy on the same testing set as the previous experiment. In this experiment, we set both ? and ? to 1 and use a Gaussian noise with ? being 0.5. The results are shown in Fig. 5, from which we can see that DPSD     0 1 2 3 4 5 6 7 8 9 10         standard deviation of the Gaussian noise  S N  R  SNR over 20000 Berkeley 12x12 patch samples  DPSD PSD  (a) Gaussian Noise  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 ?6  ?4  ?2       noise level  S N  R  SNR over 20000 Berkeley 12x12 patch samples  DPSD PSD  (b) Bernoulli Noise  Fig. 4. SNR comparison for different noise levels over 20000 12?12 image patches  always performs better than PSD for the different number of images per category in the training set and the advantage is more obvious for the case with smaller number of training images per category.

5 10 15 20 25 30        image number per category for training set  re co  gn iti  on a  cc ur  ac y  (% )  PSD DPSD  Fig. 5. Recognition accuracy with different number of training images per category over Caltech 101 dataset.

D. Recognition Accuracy On MNIST  MNIST dataset contains a training set of 60,000 images and a test set of size 10,000 images. After obtaining the sparse representations of 256 units, we feed the feature vectors to a logistic regression classifier.

From Algorithm 1, we can find that several iterations are needed for our algorithm to achieve satisfactory performance.

We illustrate this property of DPSD in Fig. 6. In this experi- ment, we set both ? and ? to 1 and use a Gaussian noise with ? being 0.5. We can find that after about 30 iterations, our DPSD algorithm can achieve a good performance, and our DPSD can significantly outperform the traditional PSD algorithm.

0 5 10 15 20 25 30 4.5   5.5   6.5   7.5   iteration  er ro  r ra  te (%  )  error rates over MNIST dataset for different iterations  DPSD PSD  Fig. 6. Error rates for different iterations over MNIST dataset.



V. CONCLUSION AND FUTURE WORK  Sparse coding has been proved to be very effective in many real applications, especially in computer vision and object recognition. In this paper, we have proposed a novel sparse coding algorithm, called denoising predictive sparse decom- position (DPSD), for object representation and recognition, which has approximately the same complexity as PSD but is more robust to noise. Experiments on three real-world datasets, Berkeley dataset, MNIST dataset and Caltech 101 dataset, show that our DPSD algorithm can achieve very promising performance in real applications.

In the future, we plan to extend our proposed model to use topographic filter [10] and convolutional filter [11], and then further stack them to form a deep architecture [8].



VI. ACKNOWLEDGEMENTS  This work is supported by the NSFC (No. 61100125), the 863 Program of China (No. 2012AA011003), and the Program for Changjiang Scholars and Innovative Research Team in University of China (IRT1158, PCSIRT).

