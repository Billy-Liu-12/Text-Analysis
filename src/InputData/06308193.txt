

Associative Classification Using an Immune Optimization Algorithm    Zhang Lei and Meng lingrui Hou chunjie School of Electronic Information Engineering School of Electronic Information Engineering Henan University of Science and Technology Henan University of Science and Technology  Luoyang, Henan Province,China Luoyang, Henan Province,China leizhang87@163.com   Abstract -Associative classification algorithms which are based on association rules have performed well compared with other classification approaches. However a fundamental limitation with these classification algorithms is that the search space of candidate rules is very large and the processes of rule discovery and rule selection are conducted separately. This paper proposes an algorithm based on immune optimization mechanism for optimizing associative classification rules. In the proposed algorithm the rule search process and the rule selection process are integrated in a more reasonable way in the optimization process of associative rules, thus it has the capability of dealing with complex search space of association rules while still ensuring that the resultant set of association rules is appropriate for associative classification. The performance evaluation results have shown that the proposed algorithm has achieved good runtime and accuracy performance for categorical and text datasets in comparison with conventional associative classification algorithms.

Index Terms - classification; association rules; associative classification; immune optimization

I.  INTRODUCTION  As one of the most fundamental data mining tasks, classification has been extensively studied and various types of classification methods such as decision tree [1], neural network [2] and rough set theory [3] have been proposed to solve the classification problems in various fields. Since the introduction of association rule mining [4], many association- based (or related) classifiers have been proposed [5-8]. The Associative Classification (AC) approach was first introduced in [6] with the Classification Based on Associations (CBA) algorithm. Associative classification takes advantage of association rule mining in the rule discovery process in extracting high quality rules that can accurately generalize the training dataset. This approach has achieved high accuracy in comparison with other classification approaches. However, one of the main problems of using association rule mining in AC approaches is the exhaustive search in a very large search space of possible rules. Therefore, the rule search process is computationally expensive [9], especially with small support threshold values which are very important for building accurate associative classifiers from large training datasets. In addition, the two different processes on rule discovery and  classification are poorly integrated in conventional AC approaches as these processes are conducted separately [10].

Thus the overfitting/underfitting problem for a classification model is generally ignored in association rule discovery process for AC. This will subsequently affect the predictive accuracy of the classification process for the test datasets.

Artificial immune system (AIS) is inspired by the natural immune system. The powerful information processing capabilities of the natural immune system such as feature extraction, pattern recognition, learning, memory, and its distributive nature provide rich metaphors for its artificial counterpart. Specifically, three immunological principles are primarily used in AIS methods [11]. These include the immune network theory, negative selection mechanism, and clonal selection principle, etc. Artificial immune systems and algorithms are often applied to a variety of optimization problems, such as function optimization and combinatorial optimization.

In this paper, we mainly apply the immune network theory and clonal selection principle for optimizing associative classification rules. It follows the population-based search model of evolutionary algorithms that have the capability of dealing with a complex search space. The optimization process for searching a set of high confidence association rules is mainly inspired by the clonal selection principle. Instead of searching for association rules using conventional association rule mining algorithms and then selecting the most suitable subset for the classification process, the proposed algorithm searches for the most suitable subset of association rules for the classification process directly in an evolutionary manner. We can avoid searching association rules exhaustively while still ensuring that the resultant set of association rules is appropriate for AC.

Therefore, the computational complexity of rule search space can be reduced greatly. In addition the diversity of the individuals in the population is also considered inspired by immune network theory. Thus the proposed algorithm can get a diverse number of local optimum, which are the candidate rules to form an effective association classifier. The performance evaluation results for some common datasets have shown that the proposed algorithm has achieved good runtime and accuracy performance in comparison with conventional associative classification algorithms.



II. RELEVANT WORK   Proceeding of the IEEE  Zhengzhou, China, August 2012     The natural immune system is seen as a complex of cells, molecules, and organs that protect organisms against infections [12]. Artificial immune systems (AIS) are a new computational intelligence approach inspired by theoretical immunology, observed immune functions, principles and mechanisms [13]. Three immunological principles including the immune network theory, negative selection mechanism and clonal selection principle are most frequently adopted in AIS. AIS appear to offer powerful and robust information processing capabilities for solving complex problems. AIS have been applied to a wide variety of domain areas, such as pattern recognition and classification [14-15], optimization[16], computer security [17], etc. The book by Dasgupta [12] is the first book in artificial immune systems that covers various computational aspects of the immune system and their applications. De Castro and Timmis provide a definition for an artificial immune system, design and implementation guidelines and an extensive survey of applications in their book [13].

Since the introduction of association rule mining, many association-based (or related) classifiers have been proposed [5-8]. The Associative classification approach was first introduced in [6] with the classification based on associations (CBA) algorithm. In CBA, all class association rules are extracted from the available training dataset. This classifier builder uses a brute-force, exhaustive global search, and yields better results than the decision tree building algorithm C4.5 [6]. CMAR is another typical association based algorithm [18]. CMAR differs from CBA in its frequent itemset mining strategy and its classifier construction. It adopts a variant of the FP-growth algorithm to obtain and efficiently store rules for classification in a tree structure. Instead of relying on a single rule to classify data, CMAR considers sets of related rules, taking into account that the most confident rule might not always be the best choice for classifying data.

These approaches adopt efficient association rule mining algorithms, such as Apriori [19] and FP-growth [20] etc., to first mine a large number of high-confidence rules satisfying a user-specified minimum support and confidence thresholds and then use various sequential-covering-based schemes to select from them a set of high-quality rules to be used for classification. Generally, these kind of association-based methods are more accurate than traditional decision tree building algorithms because they are based on global knowledge. However, the drawback of these approaches is that the number of initial rules is usually extremely large, significantly increasing the rule discovery and selection time.



III. NOTATIONS AND DEFINITIONS  A.  Association classification rule In this research, each dataset has already been separated  into training and test sets. A training dataset TrDB is a set of training instances, where each training instance, denoted as a triple (Tn, X, Cn), where and Tn denote the unique training instance identifier, X contains a set of items (X?I, I presents the complete set of items including in the whole dataset); Cn denote a class identifier which presents a possible class label  in dataset. (Cn?{C1, C2 ,?Ck}). A training dataset TeDB is in the same form as the training dataset and is used to evaluate the performance of a classifier.

An association classification rule has the following form:  :  : ,j jC Ci j Y YR Y C sup conf?     1  Where Y is itemset called the body or the condition of the rule Ri and Cj is the head or the consequence of the rule Ri which contains a single class label  Two measures  the degree of support and the degre e of confidence  are used to define an associative class  ification rule  j C Ysup and  jC Ycon are the degree of supp  ort and the degree of confidence of rule Ri respectively   Let Ysup represent the number of transactions in t  he dataset that contain the itemset Y  Also  let j C Ysup d  enote the number of transactions in the dataset contai ning both the itemset Y and associated with the class l  abel Cj  The confidence of Ri jC  Ycon is defined as the f ollowing form.

j j  C C Y Y  Y  supcon sup  =       2  We say a transaction contains itemset Y  if all the item in the itemset Y are included in this transaction   Given a minimum support threshold, minsup, an itemset Y is frequent if supY minsup. For the given training dataset TrDB, our approach first computes the frequent items by scanning TrDB once, and sorts them to get a list of frequent items (denoted by f?list ) B. Association Classification  Associative classification approaches are similar to other rule based classification methods. They all discover classification rules from the given training dataset firstly and then obtain a classifier by making full use of these rules.

Generally, the design of an associative classification approach consists of three major stages, i.e., rule discovery, rule selection and classification.

In the rule discovery stage, the main task is to mine or search the complete set of association rules from a given training dataset. These association rules are called class association rules which are divided into different classes.

In the rule selection stage, the association rules discovered by the rule discovery process are evaluates based on the classification performance for the training dataset and selecting the most suitable subset, which gives the best accuracy, to form a classifier. In General, common AC approaches are based on the confidence measure to select rules. The main idea of these approaches is that rules with higher confidence would probably give higher prediction accuracy.

In the final stage the obtained set association classification rules will be used to determine the class label of the unseen data in test dataset. In general, when a classifier has higher     classification accuracy for training dataset, it also has better classification performance for test dataset.

In the proposed algorithm, the rule search process and the rule selection process are implemented simultaneously in the evolution of the immune cell population. After the classification model has been built, it can be used to classify a new test instance. We adopt the same classification method as in the HARMONY algorithm to classify a new instance [29].

Suppose the classification rule set, RS, has been built. we first computes a score CV of a new instance tei for the rules of each class label in the RS and predicts the class label or a set of class labels if the underlying classification is a multi-class multi-label problem (i.e., each instance can be associated with several class labels). The score CV for a certain group of rules is defined as the sum of the confidences of the covering rules.

For a multi-class single-label classification problem, we simply choose the class label with the highest score as the predicted class label. While for a multi-class multi-label problem, the class label of a new instance can be predicted as follows. Given a user-specified factor ? , let the highest score be CVmax for an instance tei, then any class label whose corresponding score is no smaller than maxCV ??  is a predicted class label for this instance.



IV. MINING ASSOCIATION CLASSIFICATION RULES BASED ON IMMUNE OPTIMIZATION PRINCIPLES  In the proposed algorithm, the problem of mining a set of association rules for AC is considered as an optimization process inspired by immune optimization principles. The optimization goal is to obtain a classifier with good accuracy performance both for training and test sets. The optimization process searches for association rules in an evolutionary manner which is based on the clonal selection principle and immune memory mechanism. The good solutions in the optimization process are retained inspired by immune memory mechanism. The clonal selection principle, a form of natural selection, is used to explain the basic features of an adaptive immune response to an antigenic stimulus. It establishes the idea that only those cells (B cells) that recognize the antigens are selected to proliferate. The selected cells are subject to an affinity maturation process, which improves their affinity to the selective antigens.

In addition the diversity of the individuals in the current population is also considered inspired by immune network theory. This is reflected in the cloning process and memory cells selection process. Following the population-based search model, the proposed algorithm can reach a diverse number of local optimal solutions, which are the candidate rules to form an effective association classifier.

The implementation process of the proposed algorithm is as follows. Firstly, for the given training dataset TrDB, our approach first computes the frequent items by scanning TrDB once, and sorts them to get a list of frequent items (denoted by f_list ). The minimum support threshold is used to filter out specific rules from the population in each generation. Next, the confidence values of the rules are used for clonal selection.

The population is cloned, mutated and diversified, and then  the better rules in the population are moved to the memory population based on the confidence constraint and their diversity.

1? Rule Selection: This rule selection process is aimed at eliminates the rules with support values below the support and the confidence thresholds and at the same time selects some high affinity rules for cloning operation. Firstly the support and the confidence values of the rules in the population P are computed, and then the rules with low support and confidence values are eliminated. A proportional of antibodies are selected and will be performed cloning operation based on their affinity. In the proposed algorithm, the confidence measure of each rule is considered as its affinity which is defined as follows.

Definition 1 (affinity/fitness). Let us denote the corresponding association rule for an antibody Abi in the population P is Ri. The affinity/fitness of antibody Abi is defined as the degree of confidence of association rule Ri.

2? Cloning Operation: The cloning process is carried out such that the clone rate of a rule is directly proportional to its affinity and inversely proportional to the density of the association rule. The density of an association rule in the current population reflects whether the number of rules similar to this rule is many enough. The density of a association rule Ri density(Ri) is defined as follows.

Definition 2 (cover). The instance in the dataset which can be classified correctly by an association rule Ri. We call the instance is covered by the association rule Ri. The number of instances which is covered by the association rule Ri is denoted by Covern(Ri).

Definition 3 (similarity degree). When the association rules Ri and Rj are the same class. The simdegree between the association rule Ri and Rj in the population P is defined as follows.

( , ) ( ) ( )i j i j  SCovern SCovernsimdegree R R Covern R Covern R  = ?    ?3?  Where SCovern denotes the number of instances which is both covered simultaneously by the association rule Ri and Rj in the population P. When the simdegree between the rule Ri and another rule Rj is above the specified threshold we call the association rules Ri and Rj are similar to each other.

Definition 4 (density). Let us denote ( )isimrn R  as the number of association rules in the current population which is similar to Ri, the density of the association rules Ri is defined as follows.

1          if ( ) ( )  0          others i d  i  simrn R th density R  ?? = ? ?  (4)  When the density of an association rules Ri is 1, this means there have been many similar cells with this rule in the current population and we should control the number of clones in order to keep the diversity of rules in the current population.

The density of rules was used to keep the diversity of the population P.

Then we can determine the number of clone for each antibody selected to clone. The cells with higher confidence value implies closer to the optimal association classification rule, thus produce more clone. The cells with higher density     value implies that there have been many similar cells in the current population, thus its number of clones should be smaller in order to reduce the possibility of sinking into local optima.

Let us denote the number of clones produced for a rule Ri as CloneNum(Ri). When the density of an association rules Ri is 1, then its number of clones is directly set to the minimum otherwise the number of clones is proportional to its affinity.

The number of clones for the rule Ri CloneNum(Ri) is computed as shown in (5).

min max min max  ( )( ) ( ) ii Affinity RCloneNum R NumC NumC NumC  Af = + ? ?  (5)                                   (5)  Where maxNumC and minNumC are the maximum and minimum number of clones for an antibody respectively and Afmax is the maximum fitness of the current antibody population and Affinty(Ri) is the affinity of the association rule Ri.

3? Maturation: In the clonal selection algorithm, the mutation rate of a cell is inversely proportional to the affinity of the cell. It gives the chance for each low affinity cell to ??mutate?? more in order to improve its affinity. In the proposed algorithm, the mutation rate is equal to ??one item?? for every rule. That is, when a rule is mutated, the newly produced rules will differ from the parent rule only by one item, either by adding one item or deleting one item.

When adding one new item, the item is randomly selected from the list of frequent items (f_list), and the Ci is determined according to the rule which covers more instances (when Ci is not same this rule is discarded). When deleting one item, the item is in the same way randomly selected from the body of the rule, the class label keeps the same.

4? Memory Selection: The memory selection process adds some new higher confidence rules into the memory pool.

At the same time some redundant rules are deleted from the memory population.

Definition 5 (containing). Given two association classification rules with the same class label R1: X?ci and R2: Y ?ci, if the following equation (6) is satisfied then we say R2 contains R1 or R1 is contained by R2.

sup sup & sup sup  i ic c X Y  X Y  Y X? ?                          (6)  Suppose an association classification rule Ri: Y?ci in the population P, all the association rules in the memory population M can be pruned without degrading the classification performance that are contained by the rule Ri and the rule Ri is added into M.  Similarly, all the association rules can be pruned which are contained by some association rules in the memory population M and are replaced by one new one- item rule produced randomly.

5? Termination test: At the end of each generation, the coverage measure is calculated based on the memory rules to decide whether the process should continue with another generation or be terminated. The stopping condition in this algorithm is the redefined maximum number of iteration and the coverage constraints. When the algorithm terminates, the  resultant association rules will be extracted from the memory cell population and form the resultant association classification rule set. Otherwise the evolutionary process is continued for the next generation.

Definition 6 (coverage). The concept coverage reflects the classification ability of an association rule set. The definition coverage is defined as the number of instance in the dataset which can be classified correctly by an association rule set denoted as RS.

The coverage constraints are that the number of instances covered by current association rule set in the memory population surpasses the predetermined threshold.



V. EMPIRICAL RESULTS  In this section, our algorithm was implemented on a 2.0 GHz Pentium PC with 2GB of memory. We compared our algorithm with some well-known classifiers on both categorical and text datasets. Firstly we evaluated the proposed algorithm on the five categorical datasets from the UCI Machine Learning Repository [21] in comparison with FOIL [22] and HARMONY [23], which are two well-known algorithms for classifying categorical data. Foil is a rule induction-based algorithm using the foil gain to determine how each rule would be extended. HARMONY follows an instance-centric framework and mines the covering rules with the highest confidence for each instance.

In our experiment, we have selected five common datasets from the UCI databases to measure the classification performance of the proposed algorithm. Table 1 lists the properties of the five categorical datasets. Here, each item corresponds to a value of a nominal attribute, or an interval of value of a continuous attribute.

Table 1 Selected UCI datasets characteristics  Data set Attribute # instances # items # classes Adult 48842 131 2 Chess 28056 66 18 letter 20000 106 26 nursery 12960 32 5 penDigits 10992 90 10  In the implementation of the proposed algorithm, the size of the population P was set to 100 and the coverage threshold was set to 98%, and the absolute minimum support was fixed at 50 for these five UCI datasets. Table 2 shows the performance comparison results in terms of the 10-fold cross validation accuracy. All the results are the best accuracy achieved in each dataset for the three algorithms. From these results we can see that ARMBIS has better accuracy than both FOIL and HARMONY for four datasets among the five selected UCI datasets. And only in the penDigits dataset the FOIL and HARMONY algorithms have the better classification accuracy. Thus we can get the conclusion that our algorithm can achieve comparable accuracy to the two approaches FOIL and HARMONY.

Table 2 Comparison results of classification accuracy for categorical datasets  Data set Attribute FOIL HARMONY ARMBIS Adult 82.5% 81.9% 85.9%     Data set Attribute FOIL HARMONY ARMBIS Chess 42.6% 44.8% 58.2% letter 57.5% 76.8% 77.6% nursery 91.3% 92.8% 96.9% penDigits 88.0% 96.2% 87.8%  In addition, Table 3 shows the performance comparison results of runtime on the three algorithms using the five datasets. Similarly all the results are the shortest runtime (in seconds) achieved in each dataset for the three algorithms. We can see that the runtime of ARMBIS have a distinct advantage over FOIL and are comparable to HARMONY. In the nursery and chess dataset the runtime of HARMONY are faster than the proposed algorithm. Because the results of the FOIL and HARMONY algorithms were achieved by implementing in JAVA and testing on different machines, these times only provide a relative computational requirement of the different schemes. The reason is that ARMBIS is a stochastic optimization algorithm and if the stopping condition is not satisfied the algorithm will continue the iteration until the maximum number of iteration. However the proposed algorithm usually can achieve the better classification accuracy in such datasets.

We also a textual dataset called Reuters-R10 which is extracted from the popularly used Reuters-21578 text categorization collection dataset [24].  The training and test set of this database are split according to the standard ?ModeApte Split?. After preprocessing it contains totally 8575 distinct terms, 9603 training documents and 3299 test documents. The 10 largest categories of Reuters-21578 dataset are usually called Reuters-R10 dataset.

These 10 largest categories contain 6488 training documents and 2545 test documents. Some training documents are associated with multiple category labels and each one of these documents is transformed to multiple documents, each one with a distinct label.

Table 3 Comparison results of runtime (in seconds) for categorical datasets  Data set Attribute  FOIL HARMONY ARMBIS Adult 10251.0 1395.5 972.6 Chess 10122.8 11.34 698.6 letter 4365.6 778.9 512.2 nursery 73.1 6.2 298.7 penDigits 821.1 82.6 152.9  Table 4 lists the distribution of documents per class is the following for the Reuters-R10 dataset which has already been separated into training and test sets. The Reuters-R10 dataset processed contains 7193 training documents, and 2787 test documents. In this textual dataset, each word is considered as an item. And ?# train docs?, ?# test docs?and ?total # docs? refer to the number of training documents, testing documents and all documents respectively.

Table 4 Reuters-R10 dataset Class # train docs # test docs Total # docs acq 1650 719 2369 corn 181 56 237 crude 389 189 578 earn 2877 1087 3964 grain 433 149 582  interest 347 131 478 money-fx 538 179 717  ship 197 89 286 trade 369 118 487 wheat 212 71 283 Total 7193 2787 9980  For a multi-class multi-label database the breakeven point of precision and recall, which is defined as the point at which precision is equal to the recall, is often used to measure the classification performance of different classifiers. We also used the breakeven point of precision and recall to measure the classifier performance for the Reuters-R10 text dataset. In our experiment the proposed algorithm were compared with other three well-known approaches for classifying text data, which are the SVM method, Decision-trees and HARMONY [29]. The results for Decision-Trees and SVM method were obtained from [25]. Table 5 shows the comparison results of these four approaches. The micro-avg is the overall breakeven performance over all 10 categories. We first found the overall breakeven point in terms of all the top 10 categories by adjusting the dominant factor, and then reported the average of precision and recall for each category as their corresponding breakeven performance.

From Table 5 we can see that ARMBIS performs best for 6 categories among the 10 categories. In this experiment we used the absolute support threshold 80. SVM approach performs achieves the best performance for 3 categories, crude, grain and interest. Decision-tree approach also performs best for the category wheat. From the experiment results we can see that our algorithm can achieve better classification performance compared with other 3 well-known approaches.

Table 5 Comparison results of classification accuracy for Reuters-R10 dataset  Categories SVM Decision tree  Harmony ARMBIS  acq 93.6 89.7 95.3 95.8 corn 90.3 91.8 78.2 92.1 crude 88.9 85.0 85.7 86.8 earn 98.0 97.8 98.1 98.2 grain 94.6 85.0 91.8 92.2  interest 77.7 67.1 77.3 76.6 money-fx 74.5 66.2 80.5 81.1  ship 85.6 74.2 86.9 87.6 trade 75.9 72.5 88.4 89.8 wheat 91.8 92.5 62.8 90.9  Micro-avg 92.0 88.4 92.0 92.7

VI. DISCUSSION  In the last section, we gave the quantitative comparison results for both the categorical and text datasets. From the experimental results we can see that the proposed algorithm can achieve higher classification accuracy in comparison with the other classification approaches. Usually when the user- specified support threshold is decreased the associative classification approaches will achieve the better classification performance. However, the time of rule discovery and selection for associative classification will also increase exponentially. On the other hand, this will cause the overfitting problem.

Instead of searching for association rules using conventional association rule mining algorithms and then selecting the most suitable subset for the classification     process, the proposed algorithm searches for the most suitable subset of association rules for the classification process directly in an evolutionary manner. We can avoid searching association rules exhaustively while still ensuring that the resultant set of association rules is appropriate for AC.

Therefore, the computational complexity of rule search space can be reduced greatly and the rule discovery and selection time can be decreased significantly.

To achieve an effective AC, it needs not only to mine the highest association rules, but also needs to integrate these association rules into a whole effective classifier. It is not necessary and practical to mine all the possible highest confidence association rules and form a classification rule set.

In our algorithm the resultant association rules are extracted from the memory cell population and form the resultant association classification rule set. In fact the memory association rule set is optimized as a whole. In this optimization process both the affinity and the diversity of the individuals in the population are considered. Therefore the proposed algorithm can reach a diverse number of local optimal solutions, which are the candidate rules to form an effective association classifier. One advantage of our algorithm the rule search process and the rule selection process are integrated in a more reasonable and systematic way in the evolution of the immune cell population.



VII. CONCLUSION In this paper, we have proposed an algorithm based on immune optimization mechanism for searching association rules for associative classification. The proposed algorithm has the capability of dealing with complex search space of association rules. Moreover, we can avoid searching association rules exhaustively while still ensuring that the resultant set of association rules is appropriate for associative classification. In addition it enables the integration between rule discovery and rule selection processes in a more reasonable way which makes this approach adaptable to the experimental dataset in comparison with the conventional associative classification approaches. The performance evaluation results for both categorical and text datasets have shown that the proposed algorithm has achieved good runtime and accuracy performance in comparison with conventional associative classification algorithms.

