Online Algorithms for Uploading Deferrable Big Data to The Cloud

Abstract?This work studies how to minimize the bandwidth cost for uploading deferral big data to a cloud computing platform, for processing by a MapReduce framework, assuming the Internet service provider (ISP) adopts the MAX contract pricing scheme. We first analyze the single ISP case and then generalize to the MapReduce framework over a cloud platform.

In the former, we design a Heuristic Smoothing algorithm whose worst-case competitive ratio is proved to fall between 2?1/(D+1) and 2(1 ? 1/e), where D is the maximum tolerable delay. In the latter, we employ the Heuristic Smoothing algorithm as a building block, and design an efficient distributed randomized online algorithm, achieving a constant expected competitive ratio.

The Heuristic Smoothing algorithm is shown to outperform the best known algorithm in the literature through both theoretical analysis and empirical studies. The efficacy of the randomized online algorithm is also verified through simulation studies.



I. INTRODUCTION  Cloud computing is emerging as a new computing paradigm that enables prompt and on-demand access to computing resources. As exemplified in Amazon EC2 [1] and Linode [2], cloud providers invest substantially into their data centre infrastructure, providing a virtually unlimited ?sea? of CPU, RAM and bandwidth resources to cloud users, often assisted by virtualization technologies. The elastic and on-demand nature of cloud computing assists cloud users to meet their dynamic and fluctuating demands with minimal management overhead, while the cloud ecosystem as a whole achieves economies of scale through cost amortization.

Typical computing jobs hosted in the cloud include large  scale web applications [3] and big data analytics [4]. In such data-intensive applications, a large volume of information (up to terabytes or even petabytes) is periodically transmitted between the user location and the cloud, through the public Internet. Parallel to utility bill reduction in data centres (com- putation cost control), bandwidth charge minimization (com- munication cost control) now represents a major challenge in the cloud computing paradigm [5], [6], [7], where a small fraction of improvement in efficiency translates into millions of dollars in annual savings across the world [8].

Commercial Internet access, particularly the transfer of big  data, is nowadays routinely priced by the Internet service  This work is supported in part by the Natural Sciences and Engineering Research Council of Canada (NSERC), and grants from Hong Kong RGC under the contracts HKU 717812 and HKU 718513.

providers (ISPs) through a percentile charge model, a dramatic departure from the more intuitive total-volume based charge model as in residential utility billing or the flat-rate charge model as in personal Internet and telephone billing [5], [9], [7], [10]. Specifically, in a ?-th percentile charge scheme, the ISP divides the charge period, e.g., 30 days, into small intervals of equal fixed length, e.g., 5 minutes. Statistical logs summarize traffic volumes witnessed in different time intervals, sorted in ascending order. The traffic volume of the ?-th percentile interval is chosen as the charge volume.

For example, under the 95th-percentile charge scheme, the cost is proportional to the traffic volume sent in the 8208- th (95%? 30? 24? 60/5 = 8208) interval in the sorted list [9], [7], [10]. The MAX contract model is simply the 100- th percentile charge scheme. Such percentile charge models are perhaps less surprising when one considers the fact that infrastructure provisioning cost is more closely related to peak instead of average demand.

Due to both its new algorithmic implications and its  economic significance in practice, this interesting percentile charge model has soon spawned a series of studies. Most of these endeavours examine cost saving strategies and op- portunities through careful traffic scheduling, multihoming (subscribing to multiple ISPs), and inter-ISP traffic shifting.

However, they model the cost minimization problem with a critical, although sometimes implicit, assumption that all data generated at the user location have to be uploaded to the cloud immediately, without any delay [9], [10]. Consequently, the solution space is restricted to traffic smoothing in the spatial domain only.

Real-world big data applications reveal a different picture, in  which a reasonable amount of uploading delay (often specified in service level agreement, or SLA) is tolerable by the cloud user, providing a golden time window for traffic smoothing in the temporal domain, which can substantially slash peak traffic volumes and hence communication cost. An example lies in astronomical data from observatories, which are periodically generated at huge volumes but require no urgent attention.

Another well-known example is human genome analyses [4], where data are also ?big? but not time-sensitive.

The main challenge of effective temporal domain smoothing  lies in the uncertainly in future data arrivals. Therefore a practical cost minimization solution is inherently an online algorithm, making periodical optimization decisions based on  IEEE INFOCOM 2014 - IEEE Conference on Computer Communications     hitherto input. It is again, surprising, to discover that the on- line cost minimization for deferrable upload under percentile charging, even when defined over a single link from one source to one receiver only, is still highly non-trivial, exhibiting a rich combinatorial structure, yet never studied before in the literature of either computer networking or theoretical computer science (with an only exception below) [5].

The only study of the online cost minimization problem  under percentile charges that we are aware of is a recent work of Golubchik et al. [5], which focuses exclusively on the single point-to-point link case. The online algorithm they present, referred to as Simple Smoothing here, is extremely simple, and involves evenly smoothing every input across its window of tolerable delay for upload. Nonetheless, this seemingly straightforward algorithm is proven to approach the offline optimum within a small constant under the MAX model. In this work, we first design our own online algorithm for a single link, also adopting the MAX model, in preparation for the MapReduce data processing case. Based on the insight that Simple Smoothing ignores valuable information including the maximum volume recorded so far and the current amount of backlogged data and their deadlines, we tailor a more sophis- ticated solution, which incorporates a few heuristic smoothing ideas and is hence referred to as Heuristic Smoothing. We prove that Heuristic Smoothing always guarantees a compet- itive ratio no worse than that of Simple Smoothing, under any possible data arrival pattern. Theoretical analysis shows that Heuristic Smoothing can achieve a worst-case competitive ratio between 2? 1D+1 and 2(1? 1e ), where D is the tolerable delay.

We further extend the single link case to a cloud scenario  where multiple ISPs are employed to transfer big data dynami- cally for processing using a MapReduce-like framework. Data are routed from the cloud user to mappers and then reducers, both residing in potentially different data centres of the cloud [6]. We apply Heuristic Smoothing as a plug-in module for designing a distributed and randomized online algorithm with very low computational complexity. The competitive ratio guaranteed by the randomized online algorithm increases from that of Heuristic Smoothing by a small constant factor.

Extensive evaluations are conducted to investigate the per-  formance of the proposed online algorithms. The results show that Heuristic Smoothing performs much better than Immedi- ate Transfer (ITA), a straightforward algorithm that ignores temporal smoothing. Meanwhile Heuristic Smoothing also achieves smaller competitive ratios than Simple Smoothing does. In most cases tested, the observed competitive ratio of Heuristic Smoothing is smaller than 1.5, better than the theoretical upper bound, and relatively close to the offline optimum. Such superior performance is attributed to less abrupt responses to highly volatile traffic demand. Empirical studies for the cloud scenario further verify the efficacy of the randomized cost reduction algorithm, in terms of both scalability and competitive ratio.

In the rest of this paper, we discuss related work in Sec. II,  and introduce the system model in Sec. III. Heuristic Smooth-  ing and the randomized algorithm for the cloud scenario are designed and analyzed in Sec. IV and Sec. V, respectively.

Evaluation results are in Sec. VI. Sec. VII concludes the paper.



II. RELATED WORK  Similar to deferring data upload to minimize the peak band- width demand, there have been studies on scheduling CPU tasks to minimize the maximum CPU speed, that is closely related to the power consumption. Yao et al. [11] initially provide an optimal offline algorithm, the YDS algorithm, to optimally minimize power consumption by scaling CPU speed under the assumption that the former is a convex function of the latter. Bansal et al. [12] further propose the BKP algorithm with a competitive ratio of e, for minimizing the maximum speed when facing arbitrary inputs with different delay requirements, and arbitrary workload patterns.

Towards new challenges brought by the proliferation of  multi-core processors, Albers et al. [13] design an online algorithm for multi-processor job scheduling without inter- process job migration. Bingham et al. [14] and Angel et al. [15] further propose polynomial-time offline optimal al- gorithms, with migration of jobs considered. Greiner et al.

[16] generalize a c-competitive online algorithm for a single processor into a randomized cB?-competitive online algorithm for multiple processors, where B? is the ?-th Bell number.

Different from the MAX traffic charge model in this work, they focus on the total volume based energy charges computed by integrating instantaneous power consumption over time.

In recent years, data centre workload scheduling with dead-  line constraints has been extensively studied in the cloud computing literature. Gupta et al. [17] analyze the energy minimization problem in a data center when available dead- line information of the workload may be used to defer job execution for reduced energy consumption. Yao et al. [18] tackle the power reduction problem with deferrable workloads in date centers using the Lyapunov optimization approach, for approximate time averaged optimization.

A few studies exist on the transfer of big data to the cloud.

Cho et al. [19] design a static cost-aware planning system for transferring large amounts of data to the cloud provider via both the Internet and courier services. Considering a dynamic transfer scheme where data is produced dynamically, Zhang et al. [6] propose two online algorithms to minimize the total transfer cost. Different from this work, they assume mandatory immediate data upload, and adopt a total volume based charge model instead of the percentile charge model.

Goldenberg et al. [9] study the multihoming problem under  95-percentile traffic charges. Grothey et al. [10] investigate a similar problem through a stochastic dynamic programming approach. They both leverage ISP subscription switching for traffic engineering, so that the charge volume is minimized.

However, data traffic in their studies cannot be deferred. Adler et al. [20] focus on careful routing of data traffic between two types of ISPs (Average contract, Maximum contract) to pursue the optimal online solution, leading to an online optimization problem similar to the classic ski-rental problem. Golubchik  IEEE INFOCOM 2014 - IEEE Conference on Computer Communications     et al. [5] study the minimization of transmission cost by exploiting a small tolerable delay when ISPs adopt a 95- percentile or MAX charge model, focusing on a single link only, and proposing the Simple Smoothing algorithm.



III. SYSTEM MODEL  We consider a cloud user who generates large amounts of data dynamically over time, required for transfer into a cloud or a federation of clouds for processing using a MapReduce- like framework. The mappers and reducers may reside in geographically dispersed data centres. The big data in question can tolerate bounded upload delays specified in their SLA.

A. The MapReduce Framework  MapReduce, initially unveiled by Dean and Ghemawat [21], is a programming model targeting at efficiently processing large datasets in parallel. A typical MapReduce application includes two functions map and reduce, both written by the users. Map processes input key/value pairs, and produces a set of intermediate key/value pairs. The MapReduce library combines all intermediate values associated with the same intermediate key I and then passes them to the reduce func- tion. Reduce then merges these values associated with the intermediate key I to produce smaller sets of values.

There are four stages in the MapReduce framework: push-  ing, mapping, shuffling, and reducing. The user transfers workloads to the mappers during the pushing stage. The mappers process them during the mapping stage, and deliver the processed data to the reducers during the shuffling stage.

Finally the reducers produce the results in the reducing stage.

In a distributed system, mapping and reducing stages can happen at different locations. The system will deliver all in- termediate data from mappers to reducers during the shuffling stage, and the cloud providers may charge for inter-datacentre traffic during the shuffling stage. Recent studies [22], [23] suggest that the relation between intermediate data size and original data size depends closely on the specific application.

For applications such as n-gram models, intermediate data size is much bigger, and the bandwidth cost charged by the cloud provider cannot be neglected. We use ? to denote the ratio of original data size to intermediate data size.

B. Cost Minimization for MapReduce Applications  We model a cloud user producing a large volume of data every hour, as exemplified by astronomical observatories [6].

As shown in Fig. 1, the data location is multi-homed with multiple ISPs, for communicating with data centers. Through the infrastructure provided by ISP i, data can be uploaded to a corresponding data centre DCi. Each ISP has its own traffic charge model and pricing function.

After arrival at the data centers, the uploaded data will be  processed using a MapReduce-like framework. Intermediate data need to be transferred among data centers in the shuffling stage. Towards a general model, we again assume that multiple ISPs are employed by the cloud to communicate among its distributed data centers, e.g., ISP A for communicating  between DC1 and DC2, and ISP B for communicating between DC1 and DC3. If two inter-DC connections are covered by the same ISP, it can be equivalently viewed as two ISPs with identical traffic charge models.

User Location  DC 1'  DC 2'  DC 3'  DC 1  DC 2  DC 3  Data Sources Mappers Reducers  Fig. 1. An illustration of the network for deferrable data upload under the MapReduce framework.

The system runs in a time-slotted fashion. Each time slot is 5 minutes. The charge period is a month (30 days). M and R denote the set of mappers and the set of reducers, respectively. Since each mapper is associated with a unique ISP in the first stage, we employ m ? M to represent the ISP used to connect the user to mapper m. All mappers use the same hash function to map the intermediate keys to reducers [23]. The upload delay is defined as the duration between when data are generated to when they are transmitted to the mappers. We focus on uniform delays, i.e., all jobs have the same maximum tolerable delay D, which is reasonable assuming data generated at the same user location are of similar nature and importance. We use Wt to represent each workload released at the user location in time slot t. Let xmd,t be a decision variable indicating the portion ofWt assigned to mapper m at time slot t+d. The cost of ISP m is indicated by fm(Vm), where Vm is the maximum traffic that goes through ISP m at time slot t.

To ensure all workload is uploaded into the cloud, we have:  0 ? xmd,t ? 1, ?m ?M. (1)  ? m  D? d=0  xmd,t = 1, ?t. (2)  Given the maximum tolerable uploading delay D, the traffic V tm between the user and mapper m is:  V tm =  D? d=0  Wt?dx m d,t?d, ?m ?M. (3)  Let Vm be the maximum traffic volume of ISP m, which will be used in the calculation of bandwidth cost. Vm satisfies:  Vm ? V tm ? 0, ?t. (4)  We assume that ISPs in the first stage, connecting user to mappers, employ the same charging function fm; and ISPs in the second stage from mappers to reducers use the same charging function fm,r. Both charging functions fm and fm,r are non-decreasing and convex. We further assume that the first stage is non-splittable, i.e., each workload is uploaded  IEEE INFOCOM 2014 - IEEE Conference on Computer Communications     through one ISP only.

The user decides to deliver the workload to mapper m in  time slot t. Assume it takes a unit time to transmit data via ISPs. LetM t+1m denote the total data size at mapper m in time slot t + 1. M t+1m can be calculated as the summation of all transmitted workloads at time slot t:  M t+1m =  D? d=0  Wt?dx m d,t?d, ?m ?M.

Assume the mappers take 1 time slot to process a received workload. Therefore the mappers will transfer data to the reducer in time slot t+2. Let T t+2m,r be the traffic from mapper m to reducer r is in time slot t+ 2:  V t+2m,r = ?M t+1 m y  t+2 m,r, ?m ?M, r ? R. (5)  The maximum traffic volume of the ISP (m, r), Vm,r, satisfies:  Vm,r ? V t+2m,r ? 0, ?t. (6)  Notice that the MapReduce framework partitions the output pairs (key/value) of mappers to reducers using hash functions.

All values for the same key are always reduced at the same reducer no matter which mapper it comes from. Furthermore, we assume that data generated in the data locations are uniformly mixed, therefore we have:  yt+2m,r = zr, ?m ?M, r ? R. (7)  This equation also implies that the superscript of yt+2m,r can be ignored. Now we can formulate the overall traffic cost minimization problem for the cloud user, under the MAX contract charge model:  minimize ? m  fm(Vm) + ? m,r  fm,r(Vm,r) (8)  subject to:  Vm ? V tm ? 0, ?t,m (8a) Vm,r ? V tm,r ? 0, ?t,m, r (8b)  D? d=0  xd,tm = nm, ?t,m (8c) ? m  nm = 1, (8d)  0 ? xd,tm ? 1, nm ? {0, 1}, ?m (8e).

where V tm and V t m,r are defined in Eqn. (3) and Eqn. (5),  respectively. nm is a binary variable indicating whether ISP m is employed or not.

For ease of reference, notations are summarized l in Tab. I.



IV. THE SINGLE ISP CASE We first investigate the basic case that includes one mapper  and one reducer only, co-located in the same data center, with no bandwidth cost between the pairs. Given a MAX charge model at the ISP, the algorithm tries to exploit the allowable delay by scheduling the traffic to the best time slot within the allowed time window, for reducing the charge volume.

This can be illustrated through a toy example: in t = 1, a  TABLE I NOTATION  Symbol Definition D the maximum delay from the time data is generated to the time  the data location begins to transmit it to the mappers.

M the set of mappers.

R the set of reducers. Some mapper and reducer may be in the same  location, i.e., M ?R ?= ?.

Wt the workload released in user location at time slot t.

xmd,t the portion of the workload Wt that is assigned to mapper m at  time slot t+ d.

? the ratio of the size of output of a mapper to the size of its input.

ytm,r the portion of the output of mapperm that is transmitted to reducer  r at time slot t.

zr the portion of the key space mapped into reducer r.

V tm the total traffic that goes through ISP m at time slot t.

fm(y) the cost of ISP m for the input y.

job (100MB, max delay = 9 time slots) is released; in the following time slots, no jobs are released. If the algorithm smooths the traffic across the 10 time slots, the charge volume can be reduced to 10MB/5min, from 100MB/5min if immediate transmission is adopted.

A. The Primal & Dual Cost Minimization LPs  We can drop the location index (m, r) in this basic scenario of one mapper and one reducer locating in the same data centre. Note that the charging function f is a non-decreasing function of the maximum traffic volume. Minimizing the maximum traffic volume therefore implies minimizing the bandwidth cost. Consequently, the cost minimization problem in our basic single ISP scenario can be formulated into the following (primal) linear program (LP):  minimize V (9)  subject to: min{D,t?1}?  d=0  Wt?dxd,t?d ? V, ?t ? T (9a)  D? d=0  xd,t = 1, ?t ? TD (9b)  xd,t ? 0, V ? 0, ?d ? D, t ? TD, (9c)  where T = [1, T ], TD = [1, T ?D], D = [0, D] and xd,t = 0, ?t > T ?D, ?d ? D Introducing dual variable y and z to constraints (9a) and  (9b) respectively, we formulate the corresponding dual LP:  maximize T?D? t=1  zt (10)  subject to: T?  t=1  yt ? 1 (10a)  zt ?Wtyt+d ? 0, ?t ? TD, d ? D (10b) yt ? 0, ?t ? T (10c)  zt unconstrainted, ?t ? TD (10d)  IEEE INFOCOM 2014 - IEEE Conference on Computer Communications     The input begins with W1 and ends with WT?D, and WT?D+1 = 0, ...,WT = 0 is padded to the tail of the input.

We use P and D to denote feasible solutions to the primal and dual LPs, respectively.

The optimization in (9) is a standard linear program. For  an offline optimal solution, one can simply solve (9) using a standard LP solution algorithm such as the simplex method or the interior-point method.

B. Online algorithms  The simplest online solution in the basic one ISP scenario is the immediate transfer algorithm (ITA). Once a new job arrives, ITA transfers it to mappers immediately without any delay. Next we analyze the competitive ratio of ITA, as compared to the offline optimum.

Theorem 1. ITA is (D + 1)-competitive.

Proof: Consider the following input: (W, 0, 0, 0, 0, ....). ITA will process it immediately with bandwidth cost: W . However the offline optimal algorithm will divide the workload into small pieces:W/(D+1),W/(D+1), ...W/(D+1), 0, 0, 0, ...), feasible within the deadline D, with maximum traffic volume W/(D + 1).

Competitive ratio ? ? W W/(D + 1)  = D + 1  We hence obtain a lower bound on the competitive ratio of ITA, D+ 1. Next we prove D+ 1 is also an upper-bound.

Without exploiting any delays, ITA provides a feasible solution to the primal problem, which is denoted as PITA.

PITA = max t  Wt  Now we design a feasible solution to the dual problem as follows (assume ? = argmaxtWt):  yt =  { 1/(D + 1) if t = ?, ..., ? +D 0 otherwise  zt =  { 1/(D + 1)Wt if t = ? 0 otherwise  D = 1 D + 1  W?  So the competitive ratio is:  Competitive ratio ? = PITA OPT  ? PITAD = D + 1  Remarks: if D = 0, i.e. jobs are not deferrable, the offline op- timal algorithm degrades into ITA, agreeing with the theorem, which claims ITA is 1-competitive (D + 1 = 1).

ITA is apparently not ideal, and may lead to high peak  traffic and high bandwidth cost as compared with the offline optimum. Golubchik et al. [5] design a cost-aware algorithm that strikes to spread out bandwidth demand by utilizing all possible delays, referred to as the Simple Smoothing Algo- rithm. Upon receiving a new workload, Simple Smoothing evenly divides it into D+1 parts, and processes them one by one in the current time slot and the following D time slots, as shown in Algorithm 1.

Algorithm 1 The Simple Smoothing Algorithm [5] 1: for ? = 1 to T ?D do 2: for d = 0 to D do 3: xd,? = 1/(D + 1) 4: end for 5: end for  Theorem 2. [5] The competitive ratio of Simple Smoothing is 2? 1D+1 .

Theorem 2 can be proven through weak LP duality, i.e.,  using a feasible dual as the lower bound of the offline optimal.

Simple Smoothing is very simple, but guarantees a worst  case competitive ratio smaller than 2. Nonetheless, there is still room for further improvements, since Simple Smoothing ignores available information such as the hitherto maximum traffic volume transmitted, and the current ?pressure? from backlogged traffic and their deadlines. Such an observation motivated our design of the more sophisticated Heuristic Smoothing algorithm for the case D ? 1, as shown in Algorithm 2. Here T is the charge period, ? is the current time slot, and Hd is the total volume of data that have been buffered for d time slots.

Algorithm 2 The Heuristic Smoothing Algorithm 1: Vmax = 0 2: W? = 0, ?? = T ?D + 1, ..., T ; 3: Hd = 0, ?d = 1, ..., D; 4: for ? = 1 to T do 5: V? = min  { W? +  ?D d=1 Hd,max{Vmax, W?D+1 +  ?D d=1 Hd D }  }  6: if Vmax < V? then 7: Vmax = V? ; 8: end if 9: Transfer the traffic following Earliest Deadline First (EDF) strategy;  10: Update Hd, ?d = 1, ..., D; 11: end for  Theorem 3. The competitive ratio of Heuristic Smoothing is lower bounded by 2(1? 1e ).

Proof: Consider the following input: (W,W, ...W, 0, ..., 0) whose first D + 1 time slots are W .

The traffic demand V increases until time slot D + 1.

VD+1 = W  D + 1 +  W  D + 1 +  (D ? 1)W (D + 1)D  + ...+ (D ? 1)D?1W (D + 1)DD?1  = W  D + 1 (1 +D(1? (1? 1  D )D))  We can find a feasible primal solution which yields the charge volume D+12D+1W . This primal solution is an upper bound of the offline optimum. Therefore the lower bound of the competitive ratio ? ? 2D+1VD+1(D+1) =  2D+1 (D+1)2 (1 +  D(1 ? (1 ? 1D )D)) ? 2(1 ? 1e ) as D ? +?. Notice that  IEEE INFOCOM 2014 - IEEE Conference on Computer Communications     2D+1 (D+1)2 (1 + D(1 ? (1 ? 1D )D)) is a decreasing function for D ? [1,+?), we further have ? ? 2(1? 1e ).

Theorem 4. The competitive ratio of Heuristic Smoothing is upper-bounded by 2? 1D+1 .

Proof: We take the Simple Smoothing algorithm (Algo- rithm. 2) as a benchmark, and we prove that Psmooth ? Pheuristic, where Pheuristic is the charged volume produced by Algorithm 3.

Algorithm 3 will only increase the traffic demand when  W? D+1 +  ?D d=1 Hd/D exceeds Vmax. Therefore, we rearrange  Hd to compute the maximum traffic demand. Let  Vt+D = (Wt+D D + 1  + Wt+D?1 D + 1  + (D ? 1)Wt+D?2  (D + 1)D + ...+  (D ? 1)D?1Wt (D + 1)DD?1  ) Then Pheuristic = maxt Vt+D . Let ? = argmaxt Vt+D, and  we have  Psmooth = max t  t+D? i=t  Wt D + 1  ? ?+D? i=?  W? D + 1  ? W?+D D + 1  + W?+D?1 D + 1  + (D ? 1)W?+D?2  (D + 1)D  + ...+ (D ? 1)D?1W? (D + 1)DD?1  = Pheuristic Since the simple smoothing algorithm is  2 ? 1D+1?competitive, the competitive ratio of Algorithm 3 cannot be worse than 2? 1D+1 .

From the proof above, we have following corollary.

Corollary 1. For any given input, the charge volume resulting from Heuristic Smoothing is always equal to or smaller than that of Simple Smoothing.

Algorithm Complexity. All three online algorithms discussed have moderate time complexity, making them light-weight for practical applications. More specifically, ITA, Simple Smoothing and Heuristic Smoothing have a time complexity of O(T ?D), O((T ?D)D), and O(TD), respectively.



V. CLOUD SCENARIO  In this section, we apply the algorithms designed for the single ISP case to the cloud scenario, which utilizes a MapReduce-like framework for processing big data. Define Cost1 =  ? m fm(Vm), Cost2 =  ? m,r fm,r(Vm,r), and adopt  power charge functions by letting fm(x) = fm,r(x) = x?, ? > 1.

A. Algorithm Design  The two-phase MapReduce cost optimization problem is defined in (8), and is a discrete optimization with integer variables. Consequently, an offline solution that solves such an  integer program has a high computational complexity, further motivating the design of an efficient online solution.

A native online algorithm selects a fixed mapper and sched-  ules the traffic on the corresponding ISP using the Simple Smoothing Algorithm.

Theorem 5. The competitive ratio of the native online algo- rithm is lower bounded by |M |??1, where |M | is the number of mappers.

Proof: Consider the input (W, ? ? ? ,W, 0, ? ? ? , 0) whose first D + 1 items are W . We can verify that the charge volume is DW 2D+1 . The corresponding cost is (  DW 2D+1 )  ?+ ?  r(?zr DW 2D+1 )  ?.

Next we consider a more intelligent algorithm that as-  signs the j-th workload to the mapper (j mod |M |). This algorithm acts as the upper bound of the offline optimum.

Its charge volume is DW(2D+1)|M | . The corresponding cost is |M |( DW(2D+1)|M | )? + |M |  ? r(?zr  DW (2D+1)|M | )  ?. Therefore,  Competitive ratio  ? ( DW2D+1 )  ? + ?  r(?zr DW 2D+1 )  ?  |M |( DW(2D+1)|M | )? + |M | ?  r(?zr DW  (2D+1)|M | ) ?  = |M |??1  We next present a distributed randomized online algorithm for (8). For each workload, the user chooses ISPs uniformly at random to transfer the data to a randomly selected mapper.

Formally, let WA be the randomized workload assignment allocating each workload to mappers. For each selected ISP, the user runs Heuristic Smoothing to guide one-stage traffic deferral and transmission, as shown in Algorithm 3.

Algorithm 3 Randomized Uploading Scheme 1: Generate a randomized workload assignment WA which allocates each workload to a randomly selected mapper.

2: For each ISP m, apply the single ISP algorithm, e.g., Algorithm 2 to schedule the traffic.

We analyze Algorithm 3 by building a connection between the uploading scheme ? and the randomized workload as- signment WA. We combine ? and WA to a new uploading scheme ?WA. Let t0 = 1 < t1 ? ? ? < te = T . During each interval [ti, ti+1), each ISP is employed to transfer at most one workload in the uploading scheme ?. If a workload is processed in [ti, ti+1), then it cannot be finished before ti+1.

Due to the MAX charge model, the transfer speed for workload w in [ti, ti+1) is a single speed, say vi,w. If workload w is not processed in [ti, ti+1), we set vi,w = 0. Therefore, for any given i, there are at most |M | values of vi,w ?= 0.

Assume there are n workloads, forming a set W. Let  ?m = {w|all workloads assigned to ISP m} ? W. In scheme ?WA, the user transfers data at speed of  ? w??m vi,w in time  interval [ti, ti+1). Let ?n(?m) be the probability that exactly the workloads ?m are allocated to ISP m.

?n(?m) = (  |M | ) |?m|(1? 1|M | )  n?|?m|  IEEE INFOCOM 2014 - IEEE Conference on Computer Communications     We next define function ?n(x) where x ? Rn \ {0}:  ?n(x) = |M | ?  ?m?W ?n(?m)(  ? w??m  xw) ?/  n? w=1  x?w  Lemma 1. Given any uploading scheme ? and a randomized workload assignment WA, we have a randomized uploading scheme ?WA, which satisfies:  E(Cost1(?WA) + Cost2(?WA))  ? max x  ?|M|(x)(Cost2(?) + Cost1(?))  Proof: Since the traffic pattern in ISP (m, r), ?r is exactly the same as ISPm, we only consider one stage. Let us consider scheme ? first. In the first stage, the cost is:  Cost1(?) = ? m?M  max i,w  (vmi,w) ? ? max  i ??|M|(v  ? i,w)  where vmi,w indicates the transfer speed in ISP m during [ti, ti+1) for workload w. ??|M |(v  ? i,w) is the sum of the largest  |M | values of v?i,w when given i. The inequality holds because there are at most |M | non-zero speeds for any given duration [ti, ti+1). We next have the cost of the second stage:  Cost2(?) = ? m  ? r  max i,w  (?zrv m i,w)  ?  = ?? ? r  z?r ? m  max i,w  (vmi,w) ?  ? ?? ? r  z?r max i  ??|M|(v ? i,w)  The cost of the first stage in ?WA is:  E(Cost1(?WA)) = ? m?M  ? ?WAm ?W  ?n(? WA m )max  i (  ? w??WAm  vi,w) ?  = |M |max i  ? ?WAm ?W  ?n(? WA m )(  ? w??WAm  vi,w) ?  The second equality above holds because the assignment is uniformly random. Similarly, The cost of the second stage in ?WA is:  E(Cost2(?WA))  = |M | ?  ?WAm ?P ?n(?  WA m )  ? r  max i  (zr ?  w??WAm  ?vi,w) ?  = |M |?? ? r  z?r max i  ? ?WAm ?W  ?n(? WA m )(  ? w??WAm  vi,w) ?  Again because for any [ti, ti+1), there are at most |M | values of vi,w ?= 0. We have  |M |??WAm ?W ?n(?WAm )( ?  w??WAm vi,w) ?  ??|M |(v ? i,w)  = |M |??WAm ?W ?n(?WAm )(  ? w??WAm vi,w)  ?  ?n w=1(v  ? i,w)  = ?n(v) = ?|M |(v?)  where v? is an |M |-dimensional subvector of v ? Rn \ {0}, which contains all non-zero transfer speeds in [ti, ti+1).

Therefore, the ratio for the first stage is:  E(Cost1(?WA))  Cost1(?)  ? |M |??WAm ?W ?(?WAm )maxi(  ? w??WAm vi,w)  ?  maxi ??|M|(v ? i,w)  ? |M |??WAm ?W ?(?WAm )(  ? w??WAm vi  ?,w) ?  ??|M|(v ? i?,w)  ? max x  ?|M|(x)  where i? = argmaxi( ?  w??WAm vi,w) ?. Similarly, the ratio  for the second stage is also bounded by maxx ?|M |(x), i.e., E(Cost2(?WA))  Cost2(?) ? maxx ?|M |(x). This proves Lemma 1.

Let S(?, j) be the j-th Stirling number for ? elements, defined as the number of partitions of a set of size ? into j subsets [24]. Let B? be the ?-th Bell number, defined as the number of partitions of a set of size ? [24]. The Bell number is relatively small when ? is small: B1 = 1, B2 = 2, B3 = 5, B4 = 15. The definitions also imply:  ?? j  S(?, j) = B?  The following lemma is proven by Greiner et al. [16].

Lemma 2. [16] ?? ? N and ? ? |M |, maxx ?|M |(x) =?? j=1 S(?, j)  |M |!

|M |j(|M |?j)! .

Theorem 6. Given a ?-competitive algorithm with respect to cost for the single ISP case, then the randomized online algorithm is ?B???-competitive in expectation.

Proof: Let ?? be the optimal uploading scheme, the cor- responding randomized uploading scheme is ??WA. The algo- rithm we use is ?WA. Since the workloads in ??WA and ?WA are the same, we have:  E(Cost1(?WA)) ? ?E(Cost1(??WA)) (11)  since the algorithm is ?-competitive. Similarly,  E(Cost2(?WA)) ? ?E(Cost2(??WA)) (12)  since the traffic pattern in ISP (m, r), ?r is exactly the same as in ISP m.

Lemma 1 implies:  E(Cost1(? ? WA) + Cost2(?  ? WA))  ? max x  ?|M|(x)(Cost1(? ?) + Cost2(?  ?)) (13)  Since ?|M |(x) is a monotonically increasing function of ?, we use ??? as an upper bound of ? > 1, obtaining a corresponding upper bound of ?|M |(x). Combining Eqn. (11) (12) and (13) as well as Lemma 2, we have the following expected cost of the randomized online algorithm:  IEEE INFOCOM 2014 - IEEE Conference on Computer Communications     E(Cost1(?WA) + Cost2(?WA))  ? ?E(Cost1(??WA) + Cost2(??WA)) ? ?max  x ?|M|(x)(Cost1(?  ?) + Cost2(? ?))  = ?  ???? j=1  S(???, j) |M |!|M |j(|M | ? j)! (Cost1(? ?) + Cost2(?  ?))  ? ? ???? j=1  S(???, j)(Cost1(??) + Cost2(??))  ? ?B???OPT  Remark: For a single link, we can employ Heuristic Smooth- ing, whose competitive ratio is smaller than 2 with respect to maximum traffic volume. Then the competitive ratio of Algorithm 2 is 2? in cost. Thus Algorithm 3 is 2?B?- competitive in expectation. When ? = 2, the competitive ratio is 8, a constant regardless of the number of mappers.



VI. PERFORMANCE EVALUATION We have implemented Simple Smoothing, Heuristic  Smoothing, as well as the randomized online algorithm, for performance evaluation through simulation studies. The de- fault input Wt is generated uniformly at random, as shown in Fig. 2, where all data are normalized, i.e., scaled down by maxt Wt. We assume there are 5 mappers at different locations, and 5 reducers at different locations. We choose ? = 2, thus the charge function fm(x) = fm,r(x) = x2.

A. The Single ISP Case First we compare Heuristic Smoothing with Simple Smooth-  ing. The two algorithms are executed under a delay require- ment D = 5. Fig. 3 illustrates the traffic volume scheduled at each time slot. Compared with Simple Smoothing, Heuristic Smoothing results in a maximum traffic volume this is about 28% smaller. Heuristic Smoothing tries to exploit the available delay to average the traffic and is less sensitive to the fluctua- tion of traffic demand, as compared with Simple Smoothing.

For example, at around t = 10, the traffic of Simple Smoothing increases abruptly due to high traffic demand in the input; around t = 40, it goes down due to low traffic demand. In comparison, Heuristic Smoothing results in more even traffic distributions around t = 10 and t = 40.

Next we examine how the tolerable delay affects the perfor-  mance of the proposed online algorithms. We execute Simple Smoothing, Heuristic Smoothing and ITA against a variety of delays ranging from D = 0 to D = 24. We also compute the offline optimum as a benchmark. The observed competitive ratios are shown in Fig. 4. The results suggest that both Simple Smoothing and Heuristic Smoothing perform much better than ITA. Heuristic Smoothing also beats Simple Smoothing, by a smaller margin. Heuristic Smoothing approaches the offline optimum rather closely; the observed competitive ratios are always below 1.5 and usually around 1.2, much better than the theoretically proven upper bound in Theorem 4.

Heuristic Smoothing is further evaluated under other ran-  dom inputs, including Poisson distribution in Fig. 5, Gaussian  distribution in Fig. 6 and a specifically designed random input in Fig. 7. All results verify that Heuristic Smoothing works best among the three online cost minimization algorithms.

B. The Cloud Scenario  We implemented the randomized algorithm in Algorithm 3 and the native algorithm in Sec. V-A. They are evaluated under three types of inputs: uniform distribution, Poisson distribution and Gaussian distribution. We compare the costs of the two algorithms using these inputs, as shown in Fig. 8, Fig. 9 and Fig. 10, respectively. We observe that the randomized algorithm achieves much lower cost than the native algorithm, in particular with longer tolerable delays. For example, Fig. 8 shows that the randomized algorithm saves approximately 45% cost as compared with the native algorithm whenD = 5, and it saves more than 68% when D = 10. This suggests that longer tolerable delays provide the randomized algorithm more space of maneuver, leading to more evident cost reduce.

We further investigate the influence of ?, the ratio of original  data size to the intermediate data size. Results are shown in Fig. 11. When D is small, a large ? causes a rather high cost.

However when a large D is used, e.g., D = 40, even a large ? only produces a relatively small cost.

20 0     0.2  0.4  0.6  0.8  Delay window size ?  N or  m al  iz e  C os  t  Fig. 11. Relationship between traffic cost and parameters D, ?.



VII. CONCLUSION  ISPs now charge big data applications with a new, interest- ing percentile based model, leading to new online algorithm design problems for minimizing the traffic cost paid for uploading big data to the cloud. We studied two scenarios for such online algorithm design in this work. A Heuristic Smoothing algorithm is proposed in the single link case, with proven better performance than the best alternative in the liter- ature, and a smaller competitive ratio below 2. A randomized online algorithm is designed for the MapReduce framework, achieving a constant competitive ratio by employing Heuristic Smoothing as a building module. We have focused on MAX charge rules, and leave similar online algorithm design for 95-percentile charge rules as future work.

