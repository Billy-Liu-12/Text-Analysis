1Mining Condensed and Lossless Association Rules by Pruning Redundancy

Abstract   There are excessive and disordered rules generated by traditional approaches of association rule mining, many of which are redundant, so that they are difficult for users to understand and make use of. Han et al pointed out the bottleneck of association rules mining is not on whether we can derive the complete set of rules under certain constraints efficiently but on whether we can derive a compact but high quality set of rules. To solve this problem, a new method was represented, which is based on statistics and probability to get a condensed rules set by removing redundant rules. Our set of rules is more concise, more intelligible and easier to manage and use than others. Especially, the condensed set is lossless so that its switch to original rules-set can be realized. It?s important because the switch keeps the information complete. Experiments on some datasets show that the number of rules in rules-set has been reduced greatly.

1. Introduction   Association rule mining, an important technique in data mining, has been widely applied in industries of retailing, insurance and banking, etc. The main task of association rule discovery is to extract frequent itemsets from data and to generate association rules from these frequent itemsets. Let D  be a transaction dataset,  { }1 2, , , nD t t t= ? , the element ji ? j=1,2,?,p?  in transaction kt  is called ?item?. 1 2{ , , , }nI i i i= ?  is a set of all the items in D , if X I? , 1 2{ , , }kX i i i= ? , X is named k -itemset. The frequency of X  in D  is named support, denoted as ( )supp X . Given a support threshold minsup, X  is a frequent itemset if  ( )supp X minsup? .An association rule has the form of X Y?  , where X I? , Y I?  and X Y? = ? , X  is called antecedent, and Y  is called consequent. The  conditional probability of X Y?  is named confidence, ( ) ( ) / ( )conf X Y supp X Y supp X? = ? . Given  a confidence threshold minconf, X Y?  is a strong rule if ( )conf X Y minconf? ?  and  ( )supp X Y minsup? ? , where ( ) ( )supp X Y supp X Y? = ? . The task of  association rule mining is finding all strong rules in D .

The two representative algorithms are Apriori and FP-  Growth. The Apriori algorithm needs to scan dataset for many times, and produces a lot of candidate itemset during the mining, so it will take a long time to get rules by Apriori [1]. Han et al presented the FP-growth algorithm which uses tree-based pattern and scans the dataset just twice and no candidate itemsets are produced [2]. Over the past decade, a variety of algorithms have been developed to address the issue of efficiency of association rule mining. Most of them are variations of Apriori [3], or, of FP-Growth [4].

A serious problem in association rule discovery from a large dataset is the fact that the number of association rules is huge. A major challenge of reducing the number of rules, which is a good way to solve the problem, is to keep the completeness, that is, although the number of rules is cut down, the information in the representation should be lossless. The method presented in this paper does a good job in the two aspects.

The remainder of this paper is organized as follows. In Section 2, we present the related work. In Section 3, the method of mining lossless rules-set is introduced in detail.

A comparison between our approach and other approaches is presented In Section 4 our experiments compared the performance of our algorithm with that of previous algorithms on synthetic datasets and real-life datasets, with respect to the number of rules and the elapsed running time. Conclusions and future work are described in the last section.

2. Related Work   There has been some work in pruning discovered association rules. The first type is pruning by forming structural rule covers [5]. A rule :r X Y?  is removed if another rule ' : ' 'r X Y? exists, where 'X X= ,  'Y Y?  and ( ) ( ')Tidlist r Tidlist r= . Kryszkiewicz proposed the concept of representative association rules which are the smallest set of rules that cover all association rules satisfying minconf. The FastGenRR   DOI 10.1109/FSKD.2008.8    DOI 10.1109/FSKD.2008.8     algorithm was proposed to efficiently compute representative association rules [6].

As a new type of association rules mining method, closed frequent itemsets mining and maximal frequent itemsets mining were proposed. Pasquier et al propose rule inferencing through the use of Galois closed sets to produce the smallest covers, or basis [7]. Two new basis that discover rules of minimal antecedent and maximal consequent is proposed by Bastide et al [8]. Zaki proposes a variation to this in which a redundant rule is one for which a more general rule exists with the same support and confidence, where a rule r  is more general than 'r , if  'r  can be generated from r  by appending items to either the antecedent or consequent [9]. A new method called Basic Association Rules(BAR) suggests that the minimal set of association rules are analogous to minimal functional dependencies, a set of inference rules based on restricted conditional probability distribution that addresses Armstrong?s axioms [10], the GenBR algorithm results in the generation of Basic Association Rules which are non- redundant canonical rules (single consequent).

All the redundant rules can be inferred by some operator.

The above works devote to reducing the number of rules or to describing rules in another way, but they did not address three questions completely. Firstly, some rules are discarded when defined as ?redundant rule?, but the decrease of the number of rule is not enough, that is, some ?non-redundant rules? can still be derived from other rules. Secondly, some rules are discarded by user- specified constraints or objective metrics of interestingness, these constraints or metrics which depend on applications or users? domain knowledge lead to the loss of generality. Thirdly, a rule can?t be derived from the existing rules, or its support and the confidence can?t be calculated, if it has been discarded. Our method focuses on getting a lossless rules-set, which is more concise, more intelligible and easier to manage and use.

3. Mining Condensed and Lossless Rules   In order to reduce the number of rules, one of the most effective ways is identifying and pruning redundant rules.

Meanwhile, these pruned redundant rules should be able to be derived from preserved rules to keep lossless.

3.1. Non-Redundant Rules   The definition of ?redundancy? for association rules  has varied in previous approaches. The two definitions in [7,8] have a common base that a rule r is redundant if another rule r1 exists whose form is more general and information is more abundant, where the form of a rule means the structure of its antecedent and consequent, the information that a rule contains means its interestingness measures values, in general, the support and the confidence. According to above discussion, the new definition of redundant rule is presented as follows:  Definition 1: An association rule :r X Y?  is a redundant rule if there exists another association rule  ' : ' 'r X Y?  with ( ') ( )conf r conf r? , 'X X? and 'Y Y? .

This new definition is more suitable for applications in real world. For example, A ?B and C are items in database, given r AB C= ? , 'r A C= ? . 'r  is more general than r  in form, on this condition, the truth that the confidence of 'r  is equal to or bigger than r indicates that the conditional probability of 'r  is equal to or higher than r  in the condition that the antecedent of  'r  is less than r. Obviously, 'r  provides more information than r , so r  is a redundant rule.

Definition 2: Given a rule :r X Y? , ' : 'r X Y? where 'X X? . 'r  is a conditional cover rule (CCR) of r  if ( ') ( )conf r conf r? .

According to Definition 1, if rule r  has conditional cover rule, r  is a redundant rule.

Definition 3: A rule :r X Y?  is a minimal conditional cover rule (MCCR) if no conditional cover rule of it exists.

Table 1. A binary dataset  A   B   C   D   E  1    0   1   1    1 0    0   1   1    1 1    0   1   1    1 1    1   1   1    0 0    1   0   1    1 1    1   0   1    0      Figure 1. The computation of MCCRs  Example 1. Suppose we have the dataset shown in Table 1, minsup is 0.3 and minconf is 0.6. The maximal frequent itemsets is { }{ , , , },{ , , }A C D E A B D . The Figure 1 shows how to compute all MCCRs.

Figure 1 shows the search space used to find MCCRs by computing a itemset of { }, , ,A C D E . We only consider the rules of consequent containing only one item, that is, a canonical rule. For example, along the branch of ACD E?  , AC , CD  and AD  are maximal subsets  of ACD . If the confidence of a child is equal to or bigger than the confidence of its parent and both confidences are at least as great as minconf, then in Figure 1, they are connected with a bold arrow, such as ACD E?  to AC E? , both confidence are  2/3 , ( ) ( )confidence AC E confidence C E? < ? .

so  C E?  is a MCCR. In Figure 1 all MCCRs are shown as bold.

3.2. Mining Lossless Rules-set   Users may be interested in some redundant rules because these redundant rules are strong rules after all.

Definition 4:  Given a rules-set rS , if all strong rule and its support and confidence, either belong to rS  or be able to be derived from rS , rS  is a lossless rules-set.

According to Section 3.1, those rules in which the number of items in consequent is more than 1, called as noncanonical rules, and those canonical rules which are not minimal conditional cover rules, won?t appear in the result. The two kinds of rules should be able to derive from lossless rules-set. It is proved that all noncanonical rules can be derived from canonical rules [10]. We must solve the problem of deriving those canonical rules which are not minimal conditional cover rules.

Definition 5: Given a rule :r XY Z? , where X , Y , Z  are three disjoint itemsets and 1Z ? , taking Y away from the antecedent of r  to get a new rule  ' :r X Z?  is called as an antecedent reduction of r .

Definition 6: Let X , Y  and Z  be three disjoint  itemsets, the three combination rules of them: 1:r XZ Y? , 2 :r XY Z? , 3 :r YZ X? , are  called as triad similar rules.

Lemma 1: The ratios of the confidence of triad similar  rules to their respective antecedent reductions are equal, that is, Given 1:r X Y? , 2 :r Z Y? ,  3 :r X Z? , 4 :r Y Z? , 5 :r Z X? , 6 :r Y X? , 7 :r XZ Y? , 8 :r XY Z? , 9 :r YZ X?  ( 7) / ( 1) ( 8) / ( 3)conf r conf r conf r conf r=       (1) ( 8) / ( 4) ( 9) / ( 6)conf r conf r conf r conf r=      (2) ( 7) / ( 2) ( 9) / ( 5)conf r conf r conf r conf r=      (3)  Proof:  ?  ( 7) ( 3) ( ) ( ) ( ) ( ) ( )  conf r conf r supp XYZ supp XZ conf X YZ supp XZ supp X  ?  = ? = ?   and ( 8) ( 1) ( ) ( ) ( ) ( ) ( )  conf r conf r supp XYZ supp XY conf X YZ supp XY supp X  ?  = ? = ?   ? ( 7) ( 3) ( 8) ( 1)conf r conf r conf r conf r? = ? ? ( 7) / ( 1) ( 8) / ( 3)conf r conf r conf r conf r=  Equation (2) and (3) can be proved in a same way.

Lemma 1 indicates that there are some relevancies  among triad similar rules. We have Corollary 1.

Corollary 1: A redundant rule can be derived from three rules, one is its conditional cover rule, the second is another redundant rule which is one of triad similar rules to it, and the third is the conditional cover rule of the second one.

Proof:  According to Equation (1) and (3), we have ( 9) ( 2) ( 8) ( 1)( 7)  ( 5) ( 3) conf r conf r conf r conf rconf r  conf r conf r = = (4)  Equation (4) indicates that 7r , which comprises triad similar rules with 8r  and 9r , can be derived from { }2, 5, 9r r r  or { }1, 3, 8r r r , we just need to prove that if 5r  is a conditional cover rule of 9r , then 2r  is a conditional cover rule of 7r , and if 3r  is a conditional cover rule of 8r , then 1r  is a conditional cover rule of  7r . According to the definition of conditional cover rule, if 5r  is a conditional cover rule of 9r , then  ( 9) / ( 5) 1conf r conf r ? and ( 7) / ( 2)conf r conf r = ( 9) / ( 5) 1conf r conf r ? , so  2r  is a conditional cover rule of 7r . Similarly, we can prove that if 3r  is a conditional cover rule of 8r , then  1r  is a conditional cover rule of 7r .

Lemma 2: Given :r X Y?  and ' :r XZ Y? , we  have ( ') ( ) 1conf r conf r= =  if ( ) 1conf r = .

Proof:  ? ( ) 1conf X Y? = , ? ( )X Z X Y? ? ? ? ( ) ( )supp X Y Z supp X Z? ? = ? ? ( ') ( ) / ( ) 1conf r supp XYZ supp XZ= = We divide the lossless rules-set into three subsets.

Exact rules set. Consisting of all the MCCRs whose  confidence is 1, denoted as PR.

Canonical rules set. Consisting of all the MCCRs  whose confidence is less than 1, denoted as CR.

Redundant rules set. Consisting of the necessary  redundant rules in order to keep lossless, denoted as RR.

Example 2.  we give another example based on  Example 1 to show the lossless rules-set. Firstly, we compute the MCCRs of { }, , ,A C D E , all MCCRS is  , , , , , , , , , , C E D E A C D C E C C A D A A D C D E D AE C  ? ? ? ? ? ?? ? ? ?? ? ? ? ?? ?  { }, , ,PR A D C D E D AE C= ? ? ? ? , , ,  , , , C E D E A C D C  CR E C C A D A  ? ? ? ?? ? = ? ?? ? ?? ?    The original RR includes all redundant rules.

, , ,  , , , , , , , , ,  ACD E AC E CD E ACE D AC D AE D CE D ADE C AD C DE C CDE A CD A CE A  ? ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ?? ? ? ??? ?    According to Corollary 1 and Lemma 2, for every triad similar rules, we can prune two of three rules, and the redundant rules whose confidence equal 1 should be pruned, as a result we have  , , , ,  ACD E AC E AD C RR  DE C AE D ? ? ?? ?  = ? ?? ?? ?   Do the same computation to another maximal frequent itemset { }, ,A B D  and join the two resulting sets. The final result is as follows:  { }, , , ,PR A D C D E D B D AE C= ? ? ? ? ? , , ,  , , , C E D E A C D C  CR E C C A D A  ? ? ? ?? ? = ? ?? ? ?? ?  , , , , , ACD E AC E AD C  RR DE C AE D BD A  ? ? ?? ? = ? ?? ? ?? ?     3.3. The GenLRR algorithm   We propose a new algorithm named GenLRR. After all frequent itemsets being generated, our algorithm generates the lossless rules-set. GenLRR consists of two steps. Firstly, it generates the maximal frequent itemsets.

Secondly, the algorithm computes each maximal frequent itemsets, all the results are joined to generate the conclusive lossless rules-set.

Algorithm GenLRR(S) Input: S is a set of all frequent itemsets.

Output: rules-set which consists of three subsets.

Begin  PR,  CR, RR  = ? mS = GenMI(S)  Foreach , 2mI S I? ? Begin CRR, MCCR = GenMCCR(I)  NewPR , NewCR = GenPR(MCCR) NewRR = GenRR(CRR)  PR = NewPR ? PR CR = NewCR ? CR RR = NewRR ? RR  End Return PR, CR, RR  End The other algorithms are ommited.

4. Comparison and experimental results Table 2. The overall comparison between GenLRR and other approaches  Characteristic \Algorithm FastGenRules FastGenRR GenBR GenLRR  #Rules All Fewer Fewer least  derive a redundant rule  ? ? ?  calculate a rule?s support    ? lossless  calculate a rule?s confidence   ? ?   The overall comparison between our approach and  three previous approaches: FastGenRules [3], FastGenRR [9] and GenBR [10], is shown in Table 2.

We compared the two algorithms with regard to the number of rules generated under different mincof  on two famous datasets: Chess is a real and dense dataset; T10I4D100K is a sparse and synthetic dataset. All experiments described below were performed on a 2.0 MHz Intel Pentium Notebook Computer with 1 GB of memory, running Win XP. Algorithms were coded in C#.

Fig. 2  Chess, sup:80%, #rules   Fig. 3  T10I4D100K, sup:0.1%, #rules  The results in Fig. 2 and 3 indicate that GenLRR generates fewer rules than GenBR on both dense dataset and sparse dataset. Especially, the number of rules got by GenLRR is far less than GenBR regardless of RR.

5. Conclusions and Future Work  This paper has demonstrated in a formal way, supported with experiments on several datasets, a new association rules mining method. We divide the rules into three subsets. The grouped rules are more concise, more intelligible and easier to use. Another major advantage of  GenLRR is it is lossless, all pruned rules can be derived out, including their support and confidence. We have developed the GenLRR algorithms for mining lossless rules-set. We will develop the deriving algorithm in future. Moreover, the form of rules-set needs further discussion. A succinct and effective form is needed too.

6. References  [1] R. Agrawal, T. Imielinski, and A. Swami. Mining association rules between sets of items in large databases.

In SIGMOD, 1993. 207-216.

[2] J. Han, J. Pei, Y. Yin, Mining Frequent Patterns without Candidate Generation, Proc. ACM-SIGMOD Int?l Conf. Management of Data, May 2000. 1-12.

[3] R. Agrawal, R. Srikant. Fast algorithms for mining association rules in large databases. In 20th International Conference on Very Large Database. 1994. 487-499  [4] K. Wang, L. Tang, J. Han, and J. Liu, Top down FP-growth for association rule mining. In Proceedings of the 6th Pacific Asia Conference on Knowledge Discovery and Data Mining, Taipei, Taiwan. Vol. 2336. Springer, Berlin, Germany, 2002. 334?340.

[5] H. Toivonen, M. Klemettinen, P. Ronkainen, K.

Hatonen, and H. Mannila. Pruning and grouping discovered association rules. In Proc. ECML-95 Workshop on Statistics, Machine Learning, and Knowledge Discovery in Database, April 1995. 47-52.

[6] M. Kryszkiewicz. Representative association rules and minimum condition maximum consequence association rules. In Proc. PKDD?98. Nantes, France, 1998. 361-369.

[7] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal.

Closed set based discovery of small covers for association rules. Proceedings of the 15th Conference on Advanced Databases. Springer. Bordeaux, France. 1999. 361?381.

[8] Y. Bastide, N. Pasquier, R. Taouil, G. Stumme, and L. Lakhal. Mining minimal non-redundant association rules using frequent closed itemsets. Proceeding of the 2000: 972-986.

[9] M. J. Zaki, Mining Non-Redundant Association Rules. Data Mining and Knowledge Discovery, 2004.

223?248.

[10] G. Li, H. J. Hamilton, Basic association rules.

Data Mining (SDM?04). Orlando, FL. 2004.166-177.

