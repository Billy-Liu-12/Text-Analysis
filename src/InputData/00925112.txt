A Parallel Genetic Algorithm for Rule Mining

Abstract  Rule mining consists of discovering valid and useful rules in large databases. As other data mining tasks, it is known to be time-consuming and I/O intensive. Evolu- tionary algorithms and parallelism are two important ways to deal with that performance problem. In this paper, we propose a parallel genetic algorithm for rule discovery, namelyPGA?RM . We evaluated it on the Nursery School public domain data set available from the UCI Repository of Machine Learning databases. The results show that PGA?RM is efficient and allows to discover high quality rules.

1 Introduction  Rule mining in large databases is one of the most studied tasks in data mining [1, 3, 10, 4]. It consists of discovering rules having the form IF C THEN P, where C and P rep- resent respectively the condition and the prediction of the rule. Basically, C and P are conjunctions of terms. The terms can be simply attributes. For instance, let us con- sider a commercial database, where the attributes represent products. The rule IF (X and Y ) THEN Z means that a customer who buys the products X and Y are likely to buy the product Z. In [7], such rules are referred to as boolean association rules. Indeed, the semantics of the rule remains the same if one replaces X, Y and Z with respectively the terms X = 1, Y = 1 and Z = 1, and assigns the value 0 to all the other products. Generally, in practice the attributes have quantitative values (e.g. age) or categorical values (e.g.

matrimonial status). Rules with such attributes are referred to in [7] as quantitative association rules. In this paper, we focus on the the problem of finding in a very large database a set of small interesting quantitative rules with categorical attributes. Moreover, the prediction part of each rule must have only one term. The attribute of this term is called goal  attribute, and it belongs to a user-specified list of attributes.

Such a problem is referred to in [2] as the dependence mod- eling problem.

As the number of rules that are candidates for extraction is known to be exponential [10], we propose a parallel ge- netic algorithm (GA) to perform the rule discovery. Each chromosome in the population encodes a candidate rule.

Initially, they are generated randomly and partitioned into sub-populations. All the individuals of each sub-population have the same goal attribute in their prediction part. Conse- quently, the genetic operators are only applied on individ- uals belonging to the same sub-population. Three kinds of operators are used by the GA: the classical crossover, mu- tation operators and a removal operator. Th last one can be performed on one individual with a probability propor- tional to its length. Its role is to reduce the length of the rules as the objective is to discover small interesting rules.

The operators are similar to those proposed in [2] but their combination is different.

On the other hand, the fitness function is derived from the information theory: the entropy function. The function includes the two basic known metrics: the support and the confidence factor [1]. A major experimental study is done on the fitness function that allows its improvement.

A parallel version of the GA is also proposed. The paral- lel GA is SPMD-based where the master assigns the sub- populations to the processors in a Round-Robin scheme.

Each worker performs the GA on its sub-populations and returns the found rules to the master. The database is repli- cated on the processors. The parallel GA has been evalu- ated on an Ethernet network of SGI workstations using the ?Nursery school? public domain database.

This paper is organized as follows. Section 2 presents a formulation of the rule mining problem and the GA. In Sec- tion 3, we propose a parallel version (PGA-RM) of the GA.

Section 4 describes and discusses an experimental evalua- tion of the PGA-RM. Finally, some concluding remarks and futur focus points are given.

0-7695-0990-8/01/$10.00 (C) 2001 IEEE    2 The Rule Mining Genetic Algorithm  2.1 Problem Formulation  The problem consists of discovering rules IF C THEN P, where C and P are respectively the condition and the pre- diction of the rule. The condition is a short conjunction of terms term1 ^ term2 ^ ::: ^ termn. Each term has the form: < attribute = value >, where attribute and value are respectively a categorical attribute of the database and a value belonging to its domain. The prediction is a single term with a goal attribute belonging to a set of attributes specified by the user in a meta-data file.

In order to solve the problem with a GA, a population of candidate rules has to be generated. Each chromosome of the population encodes one candidate rule. As theC part of the rules contains different numbers of terms the chromo- somes are variable-length. From implementation point of view, each chromosome is represented by a dynamic array.

Each tile of the array contains a term (an attribute name and its value). The problem now consists of evolving the indi- viduals of the propulation to obtain a set of best i.e. small in- teresting rules. The next section presents the components of the GA (operators and fitness function) allowing to achieve that objective.

2.2 The Genetic Algorithm  2.2.1 Initial population  The population is divided into a user-specified number of sub-populations. The size (number of individuals) of each sub-population is a parameter of the GA. The individuals are initially generated randomly. It means that first, the number of terms of each chromosome is randomly chosen between 1 and a user-specified maximum number. Second, the attribute of each term is selected randomly from the set of attributes of the database. Third, the value of each at- tribute is randomly chosen from its domain.

2.2.2 Fitness function  The interest (quality) of individuals (candidate rules) is evaluated by mean of a fitness (interestingness) function.

Several functions have been defined in the literature. They can be basic [1] such as the support of a rule (the percentage of database instances satisfying the C part of the rule) and the confidence factor (the percentage of database instances satisfying the implication IF C THEN P). Fitness functions can also be more complex such as the Jmeasure function defined in [6] or the PS metric proposed in [5]. Complex metrics are generally combinations of basic ones. For more details on interestingness metrics the reader can find an in- teresting study in [3, 8]. In this work, we have considered  the metric proposed in [2]. It is derived from information theory. The fitness function F is the following:  F (rule) = w1:(G) + w2:(Npu  NT )  w1 + w2 (1)  where:  G = jCj  N (b:log(  b  a )) (2)  jCj: number of database instances satisfying the C part of the rule.

jP j: number of database instances satisfying the P part of the rule.

jC&P j: number of database instances satisfying at the same time the C and P parts of the rule.

N : size (number of instances) of the database.

a = jP j  N and b = jC&P j  jCj .

Npu: number of potentially useful attributes of theC part of the rule. An attribute is said potentially useful if it appears in theC part of at least one individual of the sub-population.

w1 and w2: two user-specified parameters chosen between 0 and 1.

The fitness function is composed of two terms weighted by w1 and w2. The first term G is a metric defined in [9]. It is a reduced version of the Jmeasure function (variant of the entropy function) proposed in [6]. The jCj  N factor rep-  resents the support of the rule. It allows to select frequent rules. The b factor is the confidence of the rule. It permits to favour strong (valid or interesting) rules. The confidence factor is not sufficient to evaluate the validity of the rule.

Indeed, assume that a given rule has a confidence factor of 50% and at the same time 100% of the instances contain P i.e. a = jP j  N = 1. That means instances characterized  by P are not only those having the C characteristics. The interestingness of that rule is thus questioned. The log( b  a )  factor allows to avoid such problem. Furthermore, the sec- ond term of the fitness function is added in [2] to solve the following problem. In the early generations the value of C is generally near to zero. As a consequence, the G term is small and thus the value of the fitness function is very small.

This leads to a very slow convergence to the best solution.

The value of w1 must be greater than that of w2 in order to limit the influence of the second term.

2.2.3 Genetic operators  All the individuals of each sub-population have the same goal attribute in their P part. Consequently, the genetic operators are only applied on individuals belonging to the same sub-population. Three kinds of operators are utilized by the GA: crossover, mutation and removal. First, the crossover operator has two versions. If two individuals X  0-7695-0990-8/01/$10.00 (C) 2001 IEEE    and Y have one or several common attribute(s) in their C parts one attribute is randomly selected. The value of the attribute in X is exchanged with its contrepart in Y . Con- versely, if X and Y have no common attribute one term is selected randomly from X and inserted in Y with a proba- bility inversely proportional to the length of Y . One may re- call that the GA searches small rules. The similar operation is performed to insert one term of Y into X. Second, the mutation operator consists of replacing either one attribute by another one or one attribute value by another one. Third, the removal operation can be performed on one individual with a probability proportional to its length. Its role is to reduce the length of the rules as the objective is to discover small interesting rules. The operators are similar to those proposed in [2] but their combination is different.

3 Parallelization  It is known that sequential rule mining is an exponen- tial process [10] either in terms of computing time and in- put/output operations. The parallelization of the GA is thus required. It depends strongly on the partitioning and the placement of the candidate rules (sub-populations) and the database. The sub-populations can be either centralized on one processor or distributed among several processors. On the other hand, the database can be centralized, replicated or distributed. Table 1 summerizes the different possible com- binations and the advantages (+) and drawbacks (-) of each configuration. Computing tasks represent mainly the appli- cation of the genetic operators. I/O operations are mainly performed during the calculation of the fitness function.

In this paper, we implemented the version (Distributed sub-populations, Replicated database) (see Figure 1). It is SPMD-based where the master assigns the sub-populations to the processors composing the parallel machine in a Round-Robin scheme. Each worker performs the GA on its sub-populations and returns the found rules to the mas- ter. The database is replicated on each processor. One can note here that the number of sub-populations has not to be necessary equal to the number of processors. Indeed, two sub-populations having the same goal attribute can be al- located to two different processors. As the individuals in the sub-populations have different condition parts the rules discovered by the two processors are often different. As a consequence, the probability to discover interesting rules is greater.

4 Experimentation  4.1 The Database and GA parameters  We have experimented the PGA-RM on the Nursery School public domain database  ...

Master  Worker 1  ...

Sub-population 1  Instances  Database  ...

Worker N  Sub-population N  Instances  Database  Small interesting rules  Small interesting rules  Figure 1. The Parallel Genetic Algorithm  get from UCI repository of Machine learning databases (http://www.ics..uci.edu/AI/ML/Machine-Learning.html). It is an instance of a hierarchical model allowing the classifi- cation of candidats to a nursery. The final decision of ac- ceptance/rejection of a candidate takes into account three criteria :  ? The job of parents,  ? The structure and financial situation of the family,  ? The social and health situation of the family.

The Nursery database contains 12960 instances and 9 at- tributes (see Table 2).

In our experimentation, we have done the following choices. First, from the database point of view three at- tributes constitute the goal attributes: Recommendation, So- cial et Finance. Second, concerning the GA, the popula- tion is composed of 150 individuals. The maximum number MAXTERM of attributes of each rule is fixed as egal to 5 (the goal attribute is included).

4.2 Results and discussion  The experimentation platform is an Ethernet network (10Mbps) of 8 Silicon Graphics (SGI5/IRIX) workstations (a 100MHz MIPS R4600 processor, 32MB of RAM and 549MB of disk). The PGA-RM is coded with C-PVM 3.3.11. The objectives of our experimentation are the fol- lowing:  ? Studying and probably enhancing the fitness function with adaptive weights.

0-7695-0990-8/01/$10.00 (C) 2001 IEEE    Table 1. Different parallel versions of the GA  DBn Sub-populations Centralized Distributed  Centralized No parallelism + Parallel computing - Throttling situation with I/O  Replicated + Parallel I/O + Parallel computing and I/O - Loss of disk space - Loss of disk space  Partitioned + Parallel I/O + Parallel computing and I/O  Table 2. Structure of the Nursery School database  Attribute name Attribute values 1 parents usual, pretentious, great pret 2 has nurs proper, less proper, improper, critical, very crit 3 form complete, completed, incomplete, foster 4 children 1, 2, 3, more 5 housing convenient, less conv, critical 6 finance convenient, inconv 7 social nonprob, slightly prob, problematic 8 health recommended, priority, not recom 9 recommendation recommend, priority, not recom, very recom, spec prior  ? Improving the algorithm termination criterion in order to reduce the execution time without decreasing the quality of extracted rules.

? Studying the quality (interestingness) of discovered rules on the basis of the confidence factor (CF). We recall that the CF of a given rule r : IF C THEN P is defined as: CF (r) = jC&P jjCj .

? Discussing the parallelization of the proposed GA.

In [2], the parameters w1 and w2 are set respectively to 0:6 and 0:4. Recall that the second term has been introduced because the term G is near to zero at the early generations.

As a consequence, its influence must decrease as a func- tion of the generation number. To take that into account we observed in our experiments the evolution of Npu  NT mean of  the individuals as a function of the generation number. The figure 2 shows the results obtained witn 10 runs of the al- gorithm. The second term remains constant after the sixth generation.

On the basis of this result, we modified the fitness func- tion as the following. At the generation number g, the value of the function is:  Fnew(rule) =  ( G+w:(  Npu  NT )  1+w if g ? N  G else (3)  where: w = N?g N  .

0.2  0.4  0.6  0.8   2 4 6 8 10 12 14 16 18 20  N pu  m ea  n  G  Evolution of Npu mean as a function of the generation number  Npu mean G  Figure 2. Evolution of Npu NT  mean as a function of the generation number  The parameter N represents the number of generations after which Npu  NT remains constant. In our experiments, we  set it to 10 (quite greater that 6). The parameter g designates the current generation number. The main advantage of this new version of the fitness function is that it contains one parameter less than the fitness (1). However, from execution efficiency point of view the experimental results show that it has no significant improvement compared with 1.

0-7695-0990-8/01/$10.00 (C) 2001 IEEE    The second point of concern of our experimental study is the algorithm termination criterion. In [2], it is defined as the number of generations. In their experiments, the authors set the total number of generations to 100. The problem is that if the best solution is found before that limit the work performed later is useless. That is why we have proposed a more interesting termination criterion. Basically, it con- sists of stopping the execution as soon as it is observed that there is no rule quality improvement. Formally, the execu- tion stops at the generation number g (g 6= 0) if the follow- ing condition is satisfied:  (g =MaximumGeneration) or (jFg ? Fg??j ? w1:?) (4)  where MaximumGeneration is a fixed maximum num- ber of generations set to 50 in our experiments. Fg and Fg?? are respectively the values of the fitness function at the generations respectively number g and g ? ?. The pa- rameter ? designates a fixed number of generations i.e. a checkpoint, set to 6 (determined in Figure 2). The factor ? is empirically set to 0:0001 on only the basis of the G term of the fitness function (1). The second term is not considered in the determination of ? because as it was observed in Fig- ure 2 it remains constant after 6 generations. We included the condition (4) in our experiments. The results show that the condition allows an average relative CPU time gain of 35:6%. The average gain is computed by the formula:  Gain = T ? Tc T  where T and Tc designate the average execution time re- spectively obtained without the termination condition and with the termination condition on 10 runs. Their values obtained on the platform described before are respectively 386:989 seconds and 249:053 seconds. These time values include the time past in saving into files the extracted rules and their associated statistics. Furthermore, for the 10 runs the algorithm including the condition terminates at the 31th  generation in average. The generation number of the algo- rithm termination is between 10 and 50.

The third aspect concerned by the experimenatal study is the quality of discovederd rules. One remarks that the quality of the mined rules remains the same with or without the termination condition. In both versions, several rules of confidence CF = 1 are extracted. One notes that the best rule reported in [2] has a confidence of 0:9922. For exam- ple, the following rule is discovered by our two versions.

IF (parents=usual) && (healt=not_recom) THEN (recommendation=not_recom).

For that rule CFtrain = CFtest = 1, where CFtrain and CFtest are respectively its confidence factor computed on the training set and its confidence factor calculated on the  test set. The training set contains all the instances (sample) of the database utilized by the PGA-RM to compute the fit- ness function. It represents the 20% first instances of the database. The test set is the rest of the database. The role of CFtest is to check the validity of the interestingness of the discovered rules. Recall that in this paper we aim at finding valid and general rules (nuggets). That means it is a matter of finding rules with a high confidence factor. The results show that the PGA-RM is able to discover rules such as the one quoted above.

The last evaluation criterion is the speed-up of the paral- lel execution of the PGA-RM. The algorithm is based on the SPMD paradigm and the database is replicated on the work- stations before execution . As a consequence, no commu- nication is peformed neither between farmers nor between the farmer and the workers, the speed-up is thus linear.

5 Conclusion and Future Work  In this paper, we have presented and evaluated a parallel genetic algorithm for rule mining (PGA-RM). The problem consists of discovering in very large databases interesting i.e. small general valid rules IF C THEN P. The predic- tion part is specified by the user as a list of goal attributes.

The initial population is generated randomly and divided into several sub-populations. Each of them contains indi- viduals having one same goal attribute but variable-length condition parts. Adapted genetic operators are applied on individuals of the same sub-population during reproduc- tion. The operators are defined in [2] but are combined differently in their application. The fitness function has ba- sically been proposed in [6] (Jmeasure) and improved in [9] (J1measure=G). As the early experiments of [2] using J1measure showed that its value is near to zero at the early generations, the authors added a second term Npu  NT to solve  that problem. The two terms are weighted by fixed parame- ters. Our experiments on the public domain Nursery School data set have shown that the added term remains constant after 6 generations. Thus, we proposed a new version with only one adaptive parameter. The fitness function omits the second term after a certain number of generations.

Furthermore, unlike in [2] we defined an algorithm ter- mination criterion. Its basic principle is if there is no sig- nificant improvement (fitness improvement) during a fixed number of generations the execution stops. The fitness improvement parameter is fixed by considering only the J1measure because, as it is said, the second term becomes early constant. The results obtained by including the termi- nation criterion in PGA-RM show that an average relative gain of 35:6% can be achieved in terms of execution time.

Moreover, the quality of extracted rules remains the same.

Indeed, rules of confidence factor of 100% are discovered.

In the future, we will experiment our algorithm on other  0-7695-0990-8/01/$10.00 (C) 2001 IEEE    larger databases for several reasons. First, we need to con- firm its capacity of discovering interesting rules. Second, we suspect that the fact that the second term of the fitness function becomes quickly constant is due to the database. It seems that the attributes are likely to be potentially useful as all their possible instanciations (values) are present into the data set. Other data sets must be used to check our intu- ition. Finally, in order to evaluate the interest of the paral- lelism large databases must be utilized. Our actual parallel version is based on the replication of the database, which is very costly in terms of disk space and copying time. We plan to define a new version based on the distribution of the data set among several processors.

