Partitioning large data to scale up lattice-based algorithm

Abstract  Concept lattice is an effective tool and platform for data analysis and knowledge discovery such as classification or association rules mining. The lattice algorithm to build for- mal concepts and concept lattice plays an essential role in the application of concept lattice. We propose a new effi- cient scalable lattice-based algorithm: ScalingNextClosure to decompose the search space of any huge data in some partitions, and then generate independently concepts (or closed itemsets) in each partition. The experimental results show the efficiency of this algorithm.

1. Introduction  Concept lattice structure [6, 8] has shown to be an ef- fective tool for data analysis and knowledge discovery. It has been applied to machine learning, data mining and in- formation retrieval, etc. Concept lattice can derive concep- tual structures from data. It studies how objects can be hier- archically grouped together according to their common at- tributes. It can generate formal concepts from the data to reveal the relations between objects and attributes. So con- cept lattice is a natural framework for data mining. Its char- acteristics are very suitable for data mining. For example, a closed itemset (or the intent of a formal concept) is a max- imal itemset for association rules [11]. So the problem of finding frequent itemsets from data for association rules can be reduced to finding frequent closed itemsets with closed itemset lattice or concept lattice.

The lattice algorithm to build formal concepts and con- cept lattice plays an essential role in the application of con- cept lattice. Several algorithms were proposed to generate concepts or concept lattices of a data context, for exam- ple: Bordat [2], Ganter (NextClosure algorithm) [5], Chein [3], Norris [9], Godin [7] and Nourine [10], etc. Experimen- tal comparisons of performance of existing algorithms show that NextClosure algorithm is the best for large and dense data [8, 4]. But the problem is that it still takes very high  expensive time cost to deal with huge data. So in this pa- per, we propose a new efficient lattice-based algorithm Scal- ingNextClosure that decomposes the search space of any huge data in some partitions, and then generates indepen- dently concepts or closed itemsets in each partition.

The new algorithm is a kind of decomposition algorithm of concept lattice. All existing decomposition algorithms for generating concept lattices use an approach of context decomposition, that are different from ours. Our new algo- rithm uses a new method to decompose the search space. It can freely decompose the search space in any set of parti- tions if there are concepts in each partition, and then gener- ate them independently in each partition. So this algorithm can be used to analyze huge data and to generate formal concepts. Moreover for this algorithm, each partition only shares the same source data (data context) with other par- titions. ScalingNextClosure algorithm shows good scalabil- ity and can be easily used for parallel, distributed network and partial computing.

The experimental comparison with NextClosure algo- rithm shows that our algorithm (only sequential computing) is better for large data. Our algorithm succeeds in comput- ing some large data in worst case that are impossible to be computed with another algorithm.

The rest of this paper is organized as follows. The ba- sic notion of concept lattice will be described in the next section. The new algorithm ScalingNextClosure will be pre- sented in section 3. In section 4, the performance of the new algorithm will be shown. The paper ends with a short con- clusion in section 5.

2. Concept lattice  The theoretical foundation of concept lattice relies on the mathematical lattice theory [1, 6]. Concept lattice is used to represent the order relation of concepts. Concept lattice de- scribes the character of the set pair: intent and extent of con- cept.

Definition 2.1 Data context is defined by a triple ???? ???, where ? and M are two sets, and R is a rela-      tion between ? and ? . The elements of G are called ob- jects or transactions, while the elements of M are called attributes or items.

Definition 2.2 Given a subset ? ? ? of objects from a data context ???? ???, we define an operator that pro- duces the set ?? of their common attributes for every set ? ? ? of objects to know which attributes from M are com- mon to all these objects:  ?? ?? ?? ?? ? ??? for all ? ? ??.

Dually, we define ? ? for subset of attributes ? ?? , ? ?  denotes the set consisting of those objects in ? that have all the attributes from ?:  ?? ?? ?? ? ? ? ??? for all ? ? ??.

These two operators are called the Galois connection  for ???? ???. These operators are used to determine a for- mal concept.

So if ? is an attribute subset, then? ? is an object subset, and then ?? ??? is an attribute subset. We have: ? ? ? ? ??? ? ? . Correspondingly, for object subset ?, we have: ? ? ?? ??? ? ?.

Thus we define two closure operators as ? ? ? ?? for set ? and ?? ??? for set ?.

Definition 2.3 A formal concept of the data context ??????? is a pair ????? with ? ? ?, ? ? ??? ? ??  and ? ? ??. A is called extent, B is called intent.

Definition 2.4 If ???, ??? and ???, ??? are concepts, ?? ? ?? ?or ?? ? ???, then we say that there is a hi- erarchical order between ???, ??? and ???, ???.

All concepts with the hierarchical order of concepts form a complete lattice called concept lattice.

3. ScalingNextClosure algorithm  Analyzing all concepts of a data context and their search space, we define the Ordered data context, and analyze the property of the ordered data context in order to decompose the search space.

Definition 3.1 A data context is called ordered data con- text if the attributes are ordered by number of objects of each attribute from the smallest to the biggest one (In other words: the smallest attribute is in first column, and the biggest one is in the last column), and the attributes with the same objects are merged as one attribute.

Proposition 3.1 An ordered data context has the same con- cept lattice as the data context.

Proof : By the definition of ordered data context, G, M and R aren?t changed, so the data context does not change by the preprocessing step. There is a unique concept lattice for any given context[6].

It?s the precondition of ScalingNextClosure algorithm to transform a data context to the ordered data context. It is easy to generate the ordered data context.

We propose a new method to divide the search space of concepts into partitions. For each partition, we can find all concepts in it. This is the principle of our ScalingNextClo- sure algorithm. In following section, we will present the main idea of ScalingNextClosure algorithm and explain why and how we can divide the search space into partitions and then generate concepts in each partition.

3.1. The search space for concepts  Intent and extent of a concept are bijection, so we can only study the search space of intent or extent of concepts instead of search space for concepts. In fact, any concept intent is a attribute subset of ? , so all subsets of ? are elements of the search space. So the size of search space for enumeration of all concept intents is ??. This search space can be considered as the fold of some attribute sub- sets of ? . For example, we consider that the search space is formed by ? ??, where ? ?? is all subsets of ??, . . . , ???, ???, ???, ?? that include ?. Each ? ? is a search sub-space. The whole search space will be decom- posed according to the situations of such ? ??. Here we define Folding set and Folding search sub-space in order to decompose the search space.

Definition 3.2 The attribute set ? of a data context ???? ??? is ??, . . . , ?, . . . , ???, ???, ??, an at- tribute ? ? ? , the set ??? is called folding set of ?, where  ??? ?? ?? ?? ? for all ? ??? ? ? ? ? ??  In other words, the folding set of ? is the set of ????? ???? ? ? ? ? ???? ??.

For example, the folding set of ? is ?. For attribute ???, its folding set is ????? ???? ??.

Definition 3.3 An attribute joins respectively with all sub- sets of its folding set to generate the new attribute sub- sets, these new attribute subsets form a search sub-space of concepts that is called folding search sub-space of an at- tribute(F3S).

For example, F3S of ??? is: ?????; ????,??. F3S of ? is: all subsets of ??,. . . ,???,???,?? that include ?.

Proposition 3.2 For a data context ???? ???, 	? ? ? , the number of attributes of M is ?, if the number of objects of attribute ? is ?, then the folding search sub-space ?for concepts? of ? is the minimum of ?? and ????.

Proof : 	? ?? . The folding set of ? is ????, ???, ? ? ?, ???, ???, ??, it has ? ? attributes. So the size of      subset of the folding set of ?? is ????, ?? can be assembled with ???? attribute subsets to form new attribute subsets.

On the other hand, if the number of objects of attribute ?? is n, we have ?? object subsets corresponding to ??. For any concept, it?s a bijection between concept and correspond- ing object set. So there are at most ?? concepts that include attribute ??. Thus the folding search sub-space of ?? is the minimum of ?? and ????.

According to this proposition, we order the data context with the number of objects of each attribute. In practice, this arrangement can remarkably reduce the search space for real data.

In the definition of ordered data context, we need to merge the attributes with exactly the same objects as one at- tribute. The important reason for this is that we need to com- pletely ensure that there are concepts in the folding search sub-space of an attribute. It?s one important precondition of the following proposition.

Proposition 3.3 For an ordered data context, it exists con- cepts in the folding search sub-space of an attribute.

Proof : For an ordered data context ???? ???, ??? ? ? and ?? ???? ? ? ? ?? , we have ????  ? ?? ???? ?, so ????  ??  is in the folding search sub-space of attribute ??, otherwise ????? ? ? ? ??? ????  ?  ? ???? ?.

This property of ordered data context allows us to find partitions that include some search sub-space.

3.2. A scalable algorithm: ScalingNextClosure  We propose a new algorithm ScalingNextClosure which decomposes the search space and builds all concepts of each search sub-space. For each search sub-space, we use the same method (NextClosure algorithm) to generate the con- cepts. So we can generate all concepts of each search sub- space in parallel, as the search sub-spaces are independent.

ScalingNextClosure algorithm has two steps: determin- ing the partitions (see Algorithm 1) and generating all con- cepts of each partition (see Algorithm 2).

For the first step of the algorithm (determining the par- titions), we can decide the size of partition by a param- eter ?	 of our algorithm according to the size of data and our needs. For the real data, we can give a value of ?	 (? ? ?	 ? ?). ?	 is used to determine the position of the beginning and the end of each partition.

We choose some attributes of ordered data context to form an order set 	 . If the number of the elements of is , we have ??? ? ??? ? ? ? ? ? ??? ? ? ? ? ? ??? .

we denote [??? , ?????[ is the search space from attribute ??? to attribute ????? for ordered data context. From ????? (????? is the first subset of [??? , ????? [), we generate the next concepts until ???????, so we can find all concepts be- tween [??? , ????? [.

Algorithm 1 The first step of ScalingNextClosure algo- rithm: determining the partitions  1: input a parameter ?	 ?? ? ?	 ? ?? 2: generate the ordered data context (saving the order of  attributes for ordered data context in an array) 3: output the order of attributes of the ordered data context 4: ? = cardinal of the attribute set of the ordered data con-  text 5: ?? ?? ? 6: ? ?? ? 7: while ??? ?? ?? do ?determining partition? 8: ? ?? 9: 	? ?? ??  10: output 	? 11: ?? ?? ????? ??	 ? 12: end while 13: ?? ? // is the number of the partitions  All concepts (non-empty) of data context are included in  ?  ?????  ??? ? ????? 			??? ?  We use all 	? to form the partitions [??? , ????? [ and [??? ), where ? ? ? ? . Here 	? means the position of an attribute of the ordered data context, and we use it to represent the attribute ??? of the ordered data context; ??? doesn?t represent the attribute of data context. When we search the concepts, ? isn?t considered.

For each partition, we compute the next concepts from ????? to ???????. There is no relation between each par- tition. The partitions only share the same source data. We can deal with any partition independently. So we can apply this algorithm for parallel, distributed and network comput- ing.

We use the principle of NextClosure algorithm to gener- ate concepts in each partition. The principle of NextClo- sure algorithm uses the characteristic vector, which rep- resents arbitrary subsets ? of ? , to enumerate all con-  Algorithm 2 The ScalingNextClosure algorithm to find all concepts in each partition  1: input the order of attributes of the ordered data context 2: input 	? and 	??? //input the partition 3: ? ????? 4: ??? ????? 5: ???? ?? ????? 6: while ? ????? do 7: ? generate the next closure of ? for the ordered  data context 8: if ??? ? ? when searching the next closure then 9: ???? ?? ????  10: end if 11: end while      cepts for data context ???? ???. Attribute subset ? ?? , ? ? ???? ??? ? ? ? ? ??? ? ? ? ? ????? ???, ? ? ??? is the clo- sure operator. The lectically smallest attribute subset is ???.

The NextClosure algorithm proved that if we know an arbi- trary attribute subset ?, the next concept (the smallest one of all concepts that is larger than ?) with respect to the lex- icographical order is ?? ?? , where ? is defined by ?? ?? ? ?? ? ???? ??? ? ? ? ? ????? ? ???????  ? ? ? and ?? ? ? , ?? being the largest element of ? with ? ? ?? ?? by lexicographical order.

In other words, for ?? ? ?	?, from the largest element to smaller one of ?	? , we calculate ?? ??, until we find the first time ? ? ?? ??, then ?? ?? is the next concept.

Here we show an example of using ScalingNextClo- sure algorithm to find all concepts: First, we need not to generate a data file for ordered data context, the order of attributes is only stored in the main memory. The or- dered attribute set of the ordered data context for this ex- ample is: ????????????????. And then, we give a value of the parameter to determine the partitions, for exam- ple, ?	 ? ???. We use ScalingNextClosure algorithm to get 4 partitions: [??, ??[, [??, ??[, [??, ??[ and [??).

In the end, we find all concept intents in each partition.

Ordered attributes: ????????????????.

?	 ? ???  ?? ????  ????  ??  ? ?? ?  ????  ?? ??  ? ?? ?  ????  ?? ?? ?? ??  ? ?? ?  ????   ? ? ??, ? ? ??, ? ? ??, ? ? ?? The partitions and their search space: [??, ??[: ??, ??, ????, ??, ????, ????, ??????, . . . , ????????.

[??, ??[: ??, ????, ????, ??????, . . . , ??, ????, . . . , ????????????.

[??, ??[: ??, ????, ????, . . . , ??????????????.

[??) : ??, ????, ????, ??????,. . . , ????????????????.

3.3. ScalingNextClosure for worst case data  For the worst case, a different technique can be used to generate partitions in order to avoid a great unbalance of partitions in terms of number of concepts. The worst case appears when the sizes of ? and ? are equal to ?, and each attribute is verified by ? ? different objects, each object possesses ? ? different attributes. We can rede- compose the search sub-space of an attribute into partitions so that it?s easy to deal with for each partition according to the number of concepts per partition. The aim is to de- crease the complexity of each partition.

4. Experimental results  We have implemented our algorithm in Java. Preliminary results of our implementation on a PIII450 computer with 128Mo RAM show that our algorithm has efficient perfor- mance. It can deal with huge data, and the total time of com- puting of all partitions for large data is lower than that of the NextClosure algorithm.

We have tested our algorithm with the datasets of the UCI repository. The comparison with NextClosure algo- rithm shows that our algorithm (only sequential comput- ing) is better for large and dense data. The experimental re- sults show that the algorithm has high performance for very large data. The total time cost of all partitions is remarkably lower than that of NextClosure algorithm. For example (see Figure 1), for the large data of UCI agaricus with 8124 ob- jects and 124 attributes, NextClosure?s time cost is 60 times higher than that of ScalingNextClosure. Some data have a large amount of attributes, and it?s very hard to deal with them with NextClosure, but ScalingNextClosure is very ef- ficient in this case.

The 11 data contexts (??? ): 1: soybean-small (47,79), 2: SPECT (187,23), 3: flare2 (1066,32), 4: audiology(26,110), 5: breast (699,110), 6: lung-cancer (32,228), 7: promoters (106,228), 8: soybean- large (307,133), 9: dermatology (366,130), 10: nursery (12960,31), 11: agaricus (8124,124)  Figure 1. Performance of comparison for Next Closure and ScalingNextClosure algorithms on UCI data. The time cost(in milliseconds) is represented by LN(Time).

For our experiments, we have used ScalingNextClosure algorithm to generate various different partitions for some      datasets. For example, we can build different partitions ac- cording to the size of data. Figure 2 shows the results of comparisons with different values of parameter ?? . Given a bigger value, we can build more partitions. Varying differ- ent values of?? can affect the result of our algorithm, as it is the case with Agaricus dataset. How many partitions and what partition should we create for the best performance?

We will try to find an answer to this question in our future work.

Figure 2. Performance of comparison on dif- ferent values of parameter ?? with 4 test data.

We have tested our algorithm in the worst case, and it succeeds in computing some large data that were impossi- ble to be computed with other algorithms. For example: the worst case with 30 attributes is very hard to compute with other algorithms [4]. Using ScalingNextClosure algorithm, we have generated all concepts for worst case data sets with 24, 25, 30, 35 and 50 attributes.

5. Conclusion  As an effective tool of data analysis and knowledge dis- covery, concept lattice has been applied for classification and association rules mining, etc. It?s the essential task to generate concepts and concept lattice in the application of concept lattice. The existing lattice algorithms are difficult to deal with large data. The search space of concepts is very large for large data. It?s a hard problem to determine all the concepts of large data.

We study the search space of concepts in order to par- tition the search space to scale up lattice algorithm. In this paper, a new efficient scalable lattice-based algorithm, Scal- ingNextClosure is proposed for creating the partitions of the search space and building concepts in each partition. Scal- ingNextClosure is different from other existing decomposi-  tion algorithms that generate concept lattice using the ap- proach of context decomposition [12], which is based on an incremental approach.

The experimental results show that ScalingNextClo- sure algorithm is very suitable and scalable to deal with large data. For the ongoing research, we will paral- lelize ScalingNextClosure in order to improve its perfor- mance. Futhermore, we will extend our method to classifi- cation and association rules mining.

Acknowledgements  We are grateful to anonymous reviewers for helpful com- ments and to Corine Zimny for english proofreading. This research benefits from the support of the region Nord/Pas de calais.

