Affinity-Based Probabilistic Reasoning and Document Clustering on the WW W

Abstract  The World Wide Web (WWW) has become one of the fastest growing applications on the Internet today. More and more information sources have linked online through WWu: butjinding informution on the WWW is also a great challenge. For most of the users, the information retrieved is not well organized and the access time is considered high on the W c u r r e n t l y .  Therefore, there is a need to develop a good mechanism to organize and manage the tremen- dous size and various kinds of information to facilitate the functionality of a search engine for information retrieval on the wwu! In response to such a demand, we propose a Markov Model Mediator ( M M M )  mechanism which em- ploys the afinity-based data mining techniques to organize and manage the information sources so that the most rel- evant documents are clustered together to achieve higher recall and precision values for information retrieval on the m  1 Introduction  Since its introduction in the early 1990s, the World Wide Web (WWW) has become an important means of providing and accessing information around the world. For those in- formation providers, they simply put the information on the Web servers. For the users, they can access information by requesting the servers to send the information via the Web browsers. Though the WWW provides such a convenient way for putting and getting information, the information re- trieved is not well organized and the access time is consid-  ered high on the WWW for most of the users.

With the increasing number of information sources on  the WWW, the need to develop a good mechanism to or- ganize and manage the tremendous size and various kinds of information for information retrieval becomes important.

One technique to search information from the WWW is by keyword-based querying. Keyword searching has been an immediate and efficient way to specify and find related in- formation that the user inquires. However, since the WWW is a completely open environment, people can use any syn- onyms and/or abbreviations in their information sources.

Potentially, a large amount of information is retrieved us- ing keyword matching, but the retrieval precision is low due to inappropriate match of keywords. In addition, the recent increase in popularity of the WWW has led to a consider- able increase in the amount of traffic over the Internet which makes finding information on the WWW time-consuming.

Imagine that given a query, a Web search engine pro- duces hundreds of hits that represent documents supposedly relevant to the query and only a small portion of the located documents is actually relevant to the query. It takes so much time for the user to wait for the browser to display such a huge list of documents and to browse through the list. If those documents can be organized into clusters with respect to their degrees of relevance to the issued queries, the time to browse through the documents for the user can be re- duced significantly since higher recall and precision values for information retrieval can be achieved. Towards this de- mand, the data mining techniques which can discover qual- itative and quantitative patterns from the query usage pat- terns on the WWW can be beneficial.

Data mining is a process to extract nontrivial, implicit, previously unknown and potentially useful information  0-7695-0792-1/00 $10.00 0 2000 IEEE  mailto:miami.edu mailto:chens@cs.fiu.edu   from data. Data mining involves data analysis techniques which develop methods for extracting valuable knowledge from the huge repositories of data - most of which will re- main unseen by humans. Three of the most common meth- ods to mine data are association rules [ 121 [ 131, data clas- sification [3] [8] and data clustering [ 5 ]  1161. Association rules discover the co-occurrence associations among data.

Data classification is the process that classifies a set of data into different classes according to some common properties and classification models. Finally, data clustering groups physical or abstract objects into disjoint sets that are similar in some respect.

Many data clustering strategies have been proposed in the literature. Methods that rely on the designers to give hints on what objects are related require the domain knowl- edge of the designers [2] [9]. Syntactic methods such as depth first and breadth first, determine a clustering strat- egy based solely on the static structure of the database [7].

The disadvantages of this strategy are that it ignores the ac- tual access patterns and the queries might not traverse the database according to the static structure. The third type of methods gather the statistics of the access patterns and par- tition the objects based on the statistics 1141. Other strate- gies such as the placement tree clustering method in [ l ] and the decomposition-based simulated annealing cluster- ing method [6] combine two or all of the above strategies.

In this paper, the Markov Model Mediator ( M M M )  mech- anism is proposed to organize and manage the documents for information retrieval on the WWW. The large number of documents is represented as a browsing graph with each document represented as a node in the browsing graph. The connectivity of the nodes in the browsing graph is deter- mined by the structural relationships between two docu- ments. Two nodes are connected in the browsing graph only if the two corresponding documents have structurally equiv- alent terms. A set of data, i.e., the usage patterns and access frequencies of the queries issued on the WWW, together with the structure of the document are used to generate the training traces for the proposed affinity-based data mining process. The MMM mechanism employs the affinity-based data mining techniques - document clustering and proba- bilistic reasoning. By analyzing the statistics of the query usage patterns from a set of historical data, probabilistic reasoning derives sets of probability distributions for the MMMs. The probability distributions are required in the proposed stochastic process which leads to document clus- tering. Document clustering groups the tremendous amount of documents into a set of clusters with respect to their de- grees of relevance to the queries. The idea is to organize the documents such that the user can get the closely related links for the query in the minimal amount of time. The MMM mechanism can be incorporated into a Web search engine to facilitate the functionality of the search engine.

Instead of producing and displaying a huge list of unorga- nized documents to the user, the search engine can display the documents in clusters. Documents in the same cluster are potentially closely related to a certain application do- main. In addition, since queries tend to access information from the closely related documents, document clustering can improve recall by effective1 y matching queries against clusters and retrieving clusters with the highest similarity measure.

The organization of this paper is as follows. Section 2 briefly overviews the MMM mechanism. In section 3, the affinity-based data mining process which includes prob- abilistic reasoning and a stochastic process for document clustering is introduced. Conclusions are presented in sec- tion 4.

2 Markov Model Mediator (MMM) Mecha- nism  The Markov Model Mediator ( M M M )  mechanism adopts both the Markov Model framework and the mediator con- cept. A Markov model is a well-researched mathematical construct which consists of a nuimber of states connected by transitions. A mediator is defined as a program that collects information from one or more sources, processes and com- bines it, and exports the resulting information [15]. In our previous study, we proposed the use of the MMM mecha- nism to facilitate the functionality of a multimedia database management system (MMDBMS) [lo, 111. In this paper, the functions of the MMM mechanism are extended to organize the documents into clusters with respect to the degrees of relevance for efficient information retrieval on the WWW.

Each document on the WWW is modeled by an MMM.

A document can be a web page for a company which sells computer hardware. Each document consists of a set of terms which are the components, of a computer such as a ter- minal, a mouse, and a keyboard. Each term has a set of at- tributes like price, size, color, ecc. An MMM is represented by a 6-tuple X = ( S ,  F, A, U,n, Q) where S is a set of terms called states; F is a set of attributes; A is the state transition probability distribution; is the observation symbol proba- bility distribution; II is the initial state distribution; and Q is a set of multimedia augmented transition networks (ATNs).

The multimedia ATN is based on the augmented transition network (ATN) model and is used as a semantic model for multimedia presentations, mull imedia database searching, and multimedia browsing [4].

The structure of the terms (in S) in a document is mod- eled by the sequence of the MMM states. Each term has its own set of attributes (in F). The states are connected by directed arcs (transitions) which contain probabilistic and other data used to determine which state should be selected next. A, U, and II are the probability distributions for an     Table 1. The query usage patterns.

0 ni = number of terms in document di  MMM and will be mined in the affinity-based data mining process.

3 Affinity-Based Data Mining Process  3.1 Relative Affinity Measures  We use the relative affinity measurements to indicate how frequently two terms are accessed together. Two doc- uments whose terms are accessed together more frequently are said to have a higher relative affinity relationship. Re- alistically, the applications cannot be expected to specify these affinity values. Therefore, formulas to calculate these relative affinity values need to be defined.

We will use the following simple document system as an example to illustrate how the MMM mechanism together with the affinity-based data mining techniques are used for document clustering. Assume there are four documents d l , d 2 ,  d 3 ,  and dq. Each document has its own set of terms with each term denoted by t i , j ,  where i represents the ith document (i.e., d i )  and j represents the j t h  term in that doc- ument. For example, dl  has two terms that are denoted by t1,l and t l , 2 .  Let the total number of distinct attributes in the system be 22. Moreover, assume a set of query usage patterns and access frequencies with ten queries is used to generate the training traces. Table 1 shows the usage pat- terns of the terms versus the sample queries. If a term was accessed by a certain query, then the corresponding entity has a value 1. For example, the term t 1 , 2  has been accessed by queries q 2 ,  q 3 ,  4 5 ,  q 7 ,  and 49. The access frequencies of the sample queries are shown in Table 2. For example, the access frequency of query q 2  is 50.

Let Q = {1,2, . . ., lo} be the set of sample queries that ran on the documents d l ,  d2, . . . , d 4  with the term set OC = { 1,2,  . . ., 13) in the system. Define the variables:  0 usem,k = usage pattern of term m with respect to query q k  per time period  1 if term m is accessed by query Qk { 0 otherwise 0 accessk = access frequency of query qk per time pe-  riod  0 a f f n a , n  = affinity measure of terms m and n  Table 2. The query access frequencies.

3.2 Probabilistic Reasoning Method  3.2.1 State Transition Probability Distribution  Two documents whose terms are accessed together more frequently are said to have a higher relative affinity rela- tionship. Accordingly, in terms of the state transition prob- ability in a Markov Chain, if two documents have a higher relative affinity relationship, the probability that a traversal choice to node n given the current node is in m (or vice versa) should be higher. Therefore, the conditional proba- bility am,n is the (m, n)th entity of the state transition prob- ability distribution A. Define     affTn. ,  = the joint probability  which refers to the fraction of the relative affinity of terms m and n in a document with respect to the total relative affinity for all the terms in a document  fm = E, fm,, = the marginal probability am,, = fm,n= the conditional probability which refers to the state transition probability for an MMM  f m l n  = a f f= ,n  f m  Table 3 gives the constructed state transition probability distribution for d l .

Table 3. The state transition probability distri- bution A for d l .  For example, the transition goes from state 1 (term t l , l )  to state 2 (term t1,2) is 0.2955.

state  3.2.2 Observation Symbol Probability Distribution  The observation symbol probability denotes the probabil- ity of observing an output symbol from a state. Here, the observed output symbols represent the attributes and the states represent the terms. Since a term has one or more attributes and an attribute can appear in multiple terms, the observation symbol probabilities shows the probabilities an attribute is observed from a set of terms.

A temporary matrix B B  whose columns are the terms in a document and rows are all the distinct attributes in the environment is assigned a value 1 or 0 for each entity of BB. It indicates whether an attribute appears in a term of the document.

0 otherwise  if attribute s appears in term t  After B B  is constructed, the observation symbol probabil- ity distribution U can be obtained via normalizing BB per column. In other words, the sum of the probabilities which the attributes are observed from a given term should be 1.

Table 4 is the constructed observation symbol probability distribution for d l .  From this table, we can obtain informa- tion such as the probability that attribute 3 is observed given the current state is 1 ( t 1 , l )  in d l  is 1/4.

3.2.3 Initial State Probability Distribution  Since the information from the training traces is available, the preference of the initial states for queries can be ob- tained. For any term m E di (the ith document), the initial  Table 4. B for d l .

t i q i  n i n l  mT& 21 0 0 22 0 0  state probability is defined as the fraction of the number of occurrences of term m with respect to the total number of occurrences for all the member terms in document di from the training traces.

Hence, the four initial state probability distributions for documents dl  to dq can be determined by using Equation 2.

For example, II1 = [A $1 for dl .

3.3 Similarity Measure of Two Documents - A  Stochastic Process  A similarity value measures how well two documents together match the observations generated by the sample queries. The similarity value is formulated under the assumptions that the observation set Ok is conditionally independent given X and Y ,  and the sets X E di and Y E d j  are conditionally independent given di and d j .  Let iVk = k1 + lc2, OS be a set cf all observation sets, and S ( d 2 ,  d j )  be the similarity measure between documents di and dj . The similarity values are computed for the pairs of documents that are connected in the browsing graph.

where  0 Ok = (01,. . .  ON^} is an observation set with the at- tributes belonging to di and d j  and generated by query q k     x = {XI ,  . . . ,Xk1} is a set of terms belonging to d i  in Ok  Ok Y = { y l ,  . . . , ykz} is a set of terms belonging to d j  in  Ai, Bi, and IIi are the state transition probability dis- tribution, the observation symbol probability distribu- tion, and the initial state probability distribution for document d i ,  respectively. These probability distribu- tions are formulated using the proposed probabilistic reasoning method.

P(Ok I X ,  Y ;  d i ,  d j )  = the probability of occurrence of Ok given X E di and Y E dj = ntLl Bi(o, 1 s,) n:L,,+l I Y v - k l ) P ( X ,  Y ;  di, d j )  = the joint probability of X E di and  E d j  n:Lz I x U - l )  ni(sl)  n r L k l + 2 Aj(Yjv-kl I Yu-k l -1 )  nj(Y1) F ( N k )  = an adjusting factor which is used because the lengths of the observation sets are variable = I O N k  Pi,j d l d2  Table 6. The branch probabilities transformed from the similarity values (in Table 5). For example, the branch probability from d l  to d4 is 0.5166.

d l  d2 d3 d4 - 0.4834 0 0.5166  0.7222 - 0.2778 0 d3  d4 0 0.5116 - 0.4884  0.7443 0 0.2557 -  The similarity values in Table 5 are then transformed into the branch probability Pi,j for nodes i and j (as shown in Table 6) for the browsing graph. The transformation is ex- ecuted by normalizing the similarity values per row to in- dicate the branch probabilities from a specific node (docu- ment) to all its accessible nodes (documents). For example, d l  has similarity value 18.93 with dz and 20.23 with d3,  and these two similarity values are transformed into the branch probabilities 0.4834 and 0.5166. After the branch probabil-  Table 5. The similarity values for pairs of doc- uments. For example, the similarity value be- tween dl and d4 is 20.23.

ities are obtained, the stationary probabilities for the four documents can be computed by using Equation 3. Table 7 lists the stationary probabilities for the four documents.

I  d4 I 20.23 1 0 I 6.95 1 - The resulting similarity values for the documents are  shown in Table 5. As can be seen from Table 5,  the similar- ity values are symmetric which means S ( d i ,  d j )  is equal to S ( d j ,  d i ) .  When two documents have no structurally equiv- alent terms, the similarity value between them is 0 because there is no connectivity between these two nodes (docu- ments) in the browsing graph.

3.4 Document Clustering Strategy  The similarity values calculated from the stochastic pro- cess are transformed into the branch probabilities among the nodes (documents) in the browsing graph. Then the station- ary probability 4i for each node i in  the browsing graph can be obtained from the branch probabilities. The stationary probability qbi denotes the relative frequency of accessing node i (document d i )  in the long run.

i i j = 1 , 2 ,  (3)  Table 7. The stationary probability.

document I dl  I dz I d3 I d4 4 I 0.3667 I 0.2455 I 0.1333 I 0.2545  Our document clustering strategy is traversal based and greedy. Documents are partitioned with the order of their stationary probabilities. The document which has the largest stationary probability is selected to start a document cluster. While there is room in the current document clus- ter, all documents accessible in terms of the browsing graph from the current member documents of the document clus- ter are considered. The document which has the largest stationary probability is selected and the process continues until the document cluster fills up. At this point, the next un-partitioned document from the sorted list starts a new document cluster, and the whole process is repeated until no un-partitioned documents remain. In this example, if the number of documents per cluster is two, then there will be two clusters. The first cluster consists of documents dl and d4; while the second cluster has documents dZ and d3.

The time complexity for this document clustering strategy is O(d1og d ) ,  where d is the number of documents.

4 Conclusions  In this paper, we introduced a mathematically sound framework, Markov model mediator (MMM) mechanism, to facilitate the functionality of a Web search engine for fast information retrieval on the WWW. The MMM mechanism employs the proposed affinity-based data mining process that includes probabilistic reasoning and document cluster- ing. The probabilistic reasoning derives the sets of probabil- ity distributions for the MMMs and are used in the proposed stochastic process for document clustering. A document clustering strategy based on the MMM mechanism is pro- posed to partition the documents into a set of beneficial doc- ument clusters. Since a document cluster consists of several related documents which are usually required for queries in the same application domain, document clustering can improve recall by effectively matching queries against clus- ters. More experiments to compare our proposed MMM mechanism with other clustering approaches will be con- ducted in the near future.

