A New Perspec Anhad Mathur

Abstract -- Big data is data that excee  capacity of traditional database systems.

voluminous, moves too fast, or is impossib by the structures of existing database arc there must be an alternative way to process  This paper outlines the fundamental as along with its opportunities and challen builds on some of the most recent findin data science. It does not aim to cover the e challenges nor to offer definitive answ addresses, but to provide as a refere reflection and discussion.

Keywords -- Big Data, Ameliorate,  Elucidation, Heterogeneity, Data Acces Scalability, Warehouse

I. INTRODUCTION   ?The growth of data is never ending?.

states that, we consume more bytes on t minutes than grains of rice in a year i.e. 4 the number will be going to increase expo day and a study say the amount of dat 2015 will be thrice of what we are consu so to handle that big amount of data technology and architecture is required Data.

The term ?Big Data? was coined in 19 in 2008. Till now, the Big Data ph restricted to only research field but no analysis and phenomenon is requ commercial fields like retail, m manufacturing, financial services as al reflect our modern society.

Basically, Big Data is not just a tech phenomenon that how the retrieval and can be made more effective, and we nee algorithms which can manage and conv imperfect, complex and machine-gener records, web-log files, sensor data) information so that an effective automate be made on the real-time based calcula velocity is very high at any instance of tim to all this complexity is Big Data.

Walmart [1] which has more than  customer transactions every minute, wh into databases estimated to contain terabytes of data ? the equivalent of information contained in all the books in of Congress.

Similarly, Amazon.com [2] an e-com handles millions of back-end operation well as queries from more than half a m  ctive to Data Processing: Big   h Sihag mputer Science itute of Technology Rajasthan  @gmail.com  Er. Gaurav Bagaria Dept. Of Computer Science  Vivekananda Institute of Technology Jaipur, Rajasthan  bagaria_gaurav@yahoo.co.in ds the processing The data is too  ble to be managed hitectures. Hence  s these data.

spects of Big Data nges. The paper gs in the field of  entire spectrum of wers to those it ence for further  Semantics, Data ssion, Metadata,  N  . A recent survey the internet in 30 40 petabytes and onentially day by ta we require in uming today and  a new kind of d known as Big  970s but got pace henomenon was ow    Big Data uired in many  mobile services, ll these services  hnology but is a d storage of data ed to create such vert unstructured, rated data (data into actionable  ed conclusion can ation as the data me. The solution  n 16 thousand hich is imported  more than 42 f 2.5 times the n the US Library  mmerce website, ns every day, as  million third-party  sellers. The core technology that kee is Linux-based and as of 2005 they h largest Linux [3] databases, with capac TB, and 24.7 TB.

According to McKinsey datasets whose size are beyond th database software tools to record, analyze. Big Data has no exact defin should be in order to be considere Hence there is a requirement of manage Big Data. Big Data technolo as advanced data extracting te architectures are designed such that data can be managed economically regulating the different characteristics   What is Big Data?

Big Data is a term used for manag  datasets which is difficult to be m database management tools or traditi applications.

Basically, Big Data is considered rather it is a phenomenon which repre utilizing this volume of data, and als organizations who seek to ameliorate

II. CHARACTERIS   There are three important properti  not just only about the vastness o includes variety and velocity of data forms the 3 V?s of Big Data.

g Data Shalini Rajawat  Dept. Of Computer Science Vivekananda Institute of Technology  Jaipur, Rajasthan shalinirajawat19@gmail.com  eps Amazon running had the world?s three cities of 7.8 TB, 18.5  y, Big Data refers to he ability of typical  store, manage and nition that how big it ed in this category.

new technology to  ogies can be defined echnology and its  the values from the y and efficiently by s of datasets.

ging large amount of managed by on-hand  onal data processing  as a technology, but esents a challenge in o an opportunity for their effectiveness.

STICS  ies of Big Data. It is of data but it also . All these attributes       Here, the generalized term for the ?big? which represents the size of data term as some small organizations are like gigabytes or terabytes of data in compari organization that have several petabytes data to handle. The only prediction th about the data is that it is certain that increase day by day and it?s volume is in size of organization. Till now, there i companies to store all kind of datasets li financial data, environmental data, an analytics data and today many of the or their datasets in the range of terabytes like petabytes and Exabyte are not so far.

Data can be obtained from a variety o  can be of different types i.e. structured, and unstructured. With the rise in escalation of sensors, social media and n devices the data has become more comp it includes unstructured and semi-structur     Structured data: In this type, the data  a relational scheme i.e. a prevailing datab and columns. The data consistency an allows it to retrieve usable information b basic queries, based on the parameter needs of the organization.

Semi-structured data: In this type, th form of structured datasets that does not particular schema. It has a variable sch that purely depends on the generatin category of data sets includes data obtai from hierarchies of records and fields social media and weblogs.

Unstructured data: This category co scattered data and is most complex to b this type, one of the general problem ar proper index to data tables for its analys Examples include images, video an multimedia files.



III. PHASES OF BIG DA   word volume is a. It is a relative ely to have some son to big global  s and Exabyte of hat can be made is only going to  ndependent of the s a tendency in ike medical data, nd statistic and rganizations have  but soon words   of sources which , semi-structured the technology, etworking, smart lex because now red data.

a is clustered into base having rows nd configuration by responding to and operational  he data is in the t have a fixed or  hematic designed ng source. This ined or inherited of data such as  ontains the most be processed. In rises is to assign sis and querying.

nd many other  ATA  The Big Data includes the follo processing pipelining model:     A. Data Accession and Recor  It is mandatory to have a source to  it does not emerge out of a vacuum.

from a source. For example, we can our real world where we sense and around us i.e. the presence of air and heart rate of a person, to the big ex and simulations performed by the activities produces up to millions of t day. For Example: Hubble telescope biggest telescopes in the world gen amount of raw data.

B. Information Pulling and Fi  "It's not information overload. I The raw data we collect from the  recording phase is not in a format Therefore, we cannot leave it in this f still analyze it further. For example, records in a hospital, structured transcribed dictations from phys measurements that possibly have Hence, we require an information ex can generate the required information can express it in a structured form.

Doing this task completely and co biggest continuing technical challeng the data may be in different forms as only images but in future may inclu data is highly application dependent extract from a picture analyzer is to what we want to pull out of an MRI.

C. Data Integration, Am  Representation :-  Due to this non uniformity and fl  sufficient only to record it and send  owing phases in its    ding:-  o record Big Data as The data is recorded consider the case of  d observe the things smell if present in it, xperimental analysis scientists, all these terabytes of data per which is one of the  nerates a very huge  ltering:-  It's filter failure." e data accession and  ready for analysis.

form and we have to collection of health data from sensors, sicians and other  some uncertainty.

xtraction process that n from the source and  orrectly is one of the es. We can note that  s in today it includes ude video and such i.e. what we want to otally different from  malgamation, and  lood of data, it?s not it into a warehouse.

A set of scientific experiments is suitable example. If we have a bundle of datasets in the warehouse, then it is impossible to find, reuse, and utilize any of such data. But if we have appropriate metadata, there can be a chance, but even then, challenges will persist due to the differences in experimental results and data record structures.

Data analysis is lot more challenging than only being locating, understanding, recognizing and referring data.

For a very large scale analysis, all of these processes has to be done in an automated approach.

Hence, this will require a different set of syntax and semantics which are in such forms that a machine readable and machine resolvable. There is a powerful art of work in integration of datasets. Although some additional work is needed to achieve error-free automated problem solutions.

D. Query Processing, Data Modeling, and  Analysis:-  Techniques for mining and retrieval of BIG DATA  are basically different from our conventional statistical analysis of small scale samples. Big data is generally rowdy, inter-related, signified and untrustworthy. In spite of that, rowdy big data could be more valuable than those small samples because some basic statistical data is obtained from recurrent system.

Mining[4] Big Data requires filtered, integrated, trustworthy, effectively accessible data, scalable algorithms and environments which are suitable for performing Big Data Computations. On the other hand mining Big Data can also be used to improve the standards and trustworthiness of the data.

At the other hand, data mining can also be used to improve the standard and truthfulness of the data, interpret its semantics, and provide tools for intelligent querying.

E.  Data Elucidation and Interpretation :-  One of the main task in the processing pipelining of  Big Data is to make analysis in such a form that it can easily be understandable in terms of the user and if an effective Interpretation about the data is cannot be made by the user than it is of limited values. Ultimately, decision making devices and algorithms had to interpret the result which includes various stages like examining all the possibilities and assumptions made and to retract the analysis.

Furthermore, there may be many possible sources of error like all models almost have some kind of assumptions, computer systems and programs can have bugs, and results can be based on wrong and erroneous data. Therefore, for all these reasons, no user will give authority to the computer system and instead he/she will try to understand and verify the results through some other processes. The computer system must take care of that and should make it easy for the user to do so. This is one of the biggest challenges with Big Data due to its  complexity since there are many crucial assumptions made behind the data recorded. Even analytical pipelines often involve many steps, again by considering many possibilities and assumptions built in.



IV. CHALLENGES IN ANALYSIS OF BIG DATA  Big Data is providing new opportunity to modern  society and scientists as it promises to handle large amount of heterogeneous data but due to high dimensionality and massive sample size, it also have to face some unique statistical and computational challenges which includes storage bottleneck scalability noise accumulation incidental endogeneity spurious correlation and other measurement errors. Some of the challenges are discussed here:   A.  Heterogeneity, Diversity and Incompleteness:-  The data and information consume by humans possess  a lot of heterogeneity [5] but is comfortably tolerable.

Actually, the nuance and richness of natural language provides a great valuable in-depth.

But this is not the case with machine algorithms as they expect cannot understand nuance and expect homogeneous data. Therefore, for the data to be effectively processed by these algorithms it should be carefully structured. For example, we can consider the case of patient in a hospital or consumer purchasing goods from a shop. We can create different records for different aspects of the user as in case of a patient we can create one record for laboratory test, one record for hospital stay, or one more record for lifetime for all time interaction of this customer with the hospital. Now, here we can observe that leaning the first design, all other medical procedures and tests per records would be distinct for each patient. All the three designs discussed above are less structured and conversely have successively greater variety.

The basic requirement of a (traditional) data analysis system is to have data with a well-defined structure.

Computer systems work most efficiently if they to store multiple items of same size and structure and hence this field requires further work.

B. Scalability :-  The first important thing about the Big Data is its size  and that?s why it is called so. To manage large and voluminous amount of data is a challenging problem and issue for many years. In the past, this problem was eradicated by following the Moore?s law which states that the frequency of processor will get doubled in every two years but now this is shifted toward a whole new scenario of cloud computing in which the whole system is in the form of distributed cluster. This technique of resource sharing now requires new ways of deducing how to execute and run the data processing jobs.

One more dramatic shift that is going underway is the change in the form of traditional input-output subsystem.

From last few decades, HDDs (hard disk drives) were used to store data. HDDs had some disadvantages like slower random input=output performances but now these devices are increasingly replaced by solid state drives today, or other technologies like PCM ( Phase Change Memory) are around corner. All these newer technologies requires a new thinking of how to design storage subsystems for data processing as these technologies do not have the same large spread in performance between the random and sequential I/O performance in comparison with the older HDDs techniques. Implying all this changing storage subsystem potentially require to touch the every aspect of processing data including database design,  query scheduling processing algorithms, recovery methods and concurrency control methods   C. Timeliness :-  When the larger data sets are to be processed by a  system, it requires more time to analyze them as the larger data sets increases complexity which in turn increases time complexity because the flip side of size is speed. The design of a system which is likely to have a greater speed of handling larger data sets is more suitable for these technologies.

In many situations result of analysis is required immediately. For example, consider a case of fraudulent credit transaction which should be flagged before the actual transaction is completed to prevent the transition from taking place. Now, as we know a full analysis of a user?s purchase history will not be feasible at real-time.

Instead, the system needs to develop a partial result about the user and the card so that an effective conclusion can be made to arrive at quick determination. Hence, the system should be made in such a way that it possesses flexibility in computation.

D.  Privacy :-  The data privacy is another big concern and one  which continuously hikes in background of Big data .In the electronic records of health ,strict laws are there deciding what can be and what cannot be done.

Particularly in US all regulations are less forceful for other data. However great public fear is there regarding use of personal data inappropriately and peculiarly data linking from multiple users [3]. Privacy management is both a sociological and a proficient problem, which must be addressed collectively from both perspectives realizing the big data promise.

Consider an example, data collected from services based on location. These brand new architectures will need a user who will share his/her location with provider of the service, resulting in obvious privacy pertain.

Hiding only the identity without hiding the location will not accurately address these privacy pertains. A location based server or an attacker can deduct the identity of the source of query from its respective location information.

For example the information of user's location can be determined through many stationary connection points.

After sometime, user just leave "a trail of packet crumbs" which may be related to a certain residence or location of office and thereby used to find the identity of user.

Several different types of private information like religious preference or health problems can also be unveiled by just observing unknown user's movement usage pattern and movement over time. Generally, Barab?si et al [6], demonstrated that there is a strong and close correlation between people?s movement pattern and their identities. Note that concealing a location of user is much more challenging than concealing his/her identity.

This is due to services based on location, the location of the user is required for a successful access to data or collection of data, while the user's identity is not essential.

E.  Human collaboration :-  In spite of enormous advances made in computational  analysis, there are many patterns remaining which are not detected by computer algorithms but easily detected by human beings Indeed. CAPTCHAs ruins incisively this fact to inform human web users isolated from computer programs. Ideally Big data analysis will not be all computational instead it is explicitly created to have human in the loop. The new visual analytics sub-field is seeking to do this and at least with respect to the analysis and modelling phase in the pipeline. At all stages of the analysis pipeline there is similar value to input of human.

In today's complicated world, it usually takes multiple known experts from various different domains to actually understand what is going on. The analysis system for big data must support shared exploration of results and input from multiple human experts. All these multiple experts can be separated in time and space when it is too costly to collect and assemble a whole team in one room. Also the data system has to support their collaboration and accept this scattered expert input.



V. CONCLUSION   We are entering in an era of Big Data. With the help  of large scale data that are available nowadays, there are great opportunities in making faster advances in different scientific fields to enhance and enrich many organizations. Nevertheless, there are many challenges which are already discussed in this paper must be considered before realizing these opportunities. The challenges must include not just only the salient issues, but also Heterogeneity, Diversity, Incompleteness, Scalability, Timeliness, and Human collaboration, at all levels of analysis flow from data accession to result explanation. Listed challenges are very common in large application domain. Hence, these are non-cost-effective to understand in reference of single domain. Also, these challenges are not able to be addressed naturally by the products of the new era of industrialization. We are in great favor and support for further research in realizing     these technical challenges, in order to get all the benefits of BIG DATA.



VI. REFERENCES   [1] "Data, data everywhere". The Economist. 25  February 2010.

[2] Layton, Julia. "Amazon Technology".

money.howstuffworks.com.

[3] ?Top ten Big Data Security and Privacy  Challenges?. Cloud Security Alliance. November 2012.

[4] Principles of Knowledge discovery in databases.

By Osmar R. Zaiane.

[5] Challenges and Opportunities with Big Data 2011-1. Cyber Centre Technical Report, Purdue  University.

