A Feature Dependent Method for Opinion Mining and Classification

Abstract:  Mining the web for customer opinion on different products is both a useful, as well as challenging task.

Previous approaches to customer review classification included document level, sentence and clause level sentiment analysis and feature based opinion summarization. In this paper, we present a feature driven opinion summarization method, where the term ?driven? is employed to describe the concept-to-detail (product class to product-specific characteristics) approach we took. For each product class we first automatically extract general features (characteristics describing any product, such as price, size, design), for each product we then extract specific features (as picture resolution in the case of a digital camera) and feature attributes (adjectives grading the characteristics, as for example high or low for price, small or big for size and modern or faddy for design).

Further on, we assign a polarity (positive or negative) to each of the feature attributes using a previously annotated corpus and Support Vector Machines Sequential Minimal Optimization[1] machine learning with the Normalized Google Distance[2]. We show how the method presented is employed to build a feature-driven opinion summarization system that is presently working in English and Spanish. In order to detect the product category, we use a modified system for person names classification. The raw review text is split into sentences and depending on the product class detected, only the phrases containing the specific product features are selected for further processing. The phrases extracted undergo a process of anaphora resolution, Named Entity Recognition and syntactic parsing. Applying syntactic dependency and part of speech patterns, we extract pairs containing the feature and the polarity of the feature attribute the customer associates to the feature in the review. Eventually, we statistically summarize the polarity of the opinions different customers expressed about the product on the web as percentages of positive and negative opinions about each of the product features. We show the results and improvements over baseline, together with a discussion on the strong and weak points of the method and the directions for future work.

Keywords:  Opinion mining; summarization; Normalized Google Distance; SVM machine learning      1. Introduction  Presently, the consumer market is flooded with products of the most varied sorts, each being advertised as better, cheaper, more resistant, easy to use and fault free. But is all what is advertised actually true? Certainly, all companies will claim that the products they make are the best. In practice, however, each product will behave better or worse depending on the user?s necessities and level of expertise, will have certain capabilities or not depending on the price and product class. Moreover, for most users, the choice will depend on reasons such as lower price, brand name or ratio between price and performance. Therefore, how can a person that is faced with the harsh decision of having to choose among tens of products of the same type, with the same features, finally take a rational decision? Since the world wide web contains a large quantity of product reviews, in specialized review sites and user blogs, a good solution seems to be that of lending an ear to this ?word-of-mouth? on the web [3], weighing the pros and cons of the different products and finally buying the one satisfying the personal needs and expectations.

The clear advantage in having at hand the volume of data present on the Internet nowadays is that one is apt to obtain almost objective information on the products that he/she is planning to buy. This process can be accomplished by viewing different opinions that people who have previously bought the product in question have on it, based on the experience in using it. Such people can comment on the reasons motivating their choice of purchase and their present attitude towards that decision.

Thus, besides the objective information concerning price and product capabilities, a prospect buyer can also have access to the subjective information about a product.

However, a high volume of information is also bound to bring difficulty in sifting through it. The ideal situation is that in which one is able to read all available user reviews and create his/her opinion, depending on the feature(s) of interest and the value or ratio of the feature attributes. The main problem then becomes the time spent in reviewing all available data and the language barrier - the fact that product reviews are written in different languages. The solution is obvious - a system that automatically analyzes and extracts the values of the        features for a given product, independent of the language the customer review is written in. Such a system can then present the potential buyer with percentages of positive and negative opinions expressed about each of the product features and possibly make suggestions based on buyer preferences. What follows is a description of such a system that presently works on Spanish and English. However, the algorithm is language independent and can be extended to work in any language where similar NLP resources and tools as the ones used herein exist. In the approach proposed, we concentrated on two main problems. The first one was that of discovering the features that will be quantified.

As previously noticed in [3], features are implicit or explicit. To this respect, apart from a general class of features (and their corresponding attributes), that are applicable to all products, we propose a method to discover product specific features and feature attributes using knowledge from WordNet and ConceptNet.

The second problem we address is that of quantifying the features in a product-dependent manner, since, for example, small for the size of a digital camera is a positive fact, whereas for an LCD display it is a rather negative one. We accomplish this by classifying the feature attributes using positive and negative examples from a corpus of customer opinions that was polarity annotated depending on the product category and SMO SVM machine learning [1] with the Normalized Google Distance [2]. We will illustrate the manner in which we solved the above mentioned problems with examples and discuss on the issues raised at each step by using different methods, tools and resources.

This paper is organized as follows: in section 2, we briefly present the previous work done in the area of customer opinion mining and the motivation for introducing our method. Further on, in section 3, we describe the architecture of the system we built for customer opinion mining, together with the NLP resources and tools we employed. In the following section, we describe the method to assign polarity to feature attributes. In section 5, we present the manner in which we summarized the polarity of the features depending on the polarity of feature attributes used in the reviews. Further on, in section 6, we discuss on the methods to evaluate a system implementing the method presented and we show the evaluation results obtained when applying our system to a set of previously manually annotated texts containing customer reviews in English and Spanish. Finally, we conclude on the method used and present the lines for future investigation and improvement.

2. Background  Previous work in customer review classification  includes document level sentiment classification using unsupervised methods [4], machine learning techniques [5], scoring of features [6], using PMI, syntactic relations and other attributes with SVM [7], sentiment classification considering rating scales [5], supervised and unsupervised methods [8] and semisupervised learning [9]. Research in classification at a document level included sentiment classification of reviews [10], sentiment classification on customer feedback data [11], comparative experiments [12]. Other research has been conducted in analyzing sentiment at a sentence level using bootstrapping techniques [13], considering gradable adjectives [14], semisupervised learning with the initial training set identified by some strong patterns and then applying NB or self-training [15], finding strength of opinions [16] sum up orientations of opinion words in a sentence (or within some word window) [17], [18], determining the semantic orientation of words and phrases [19], identifying opinion holders [20], comparative sentence and relation extraction and feature-based opinion mining and summarization [4].

The approach we use is grounded on the feature-based opinion summarization paradigm, whose theoretical background can be found in [3]. Relevant research done in feature-driven opinion summarization can be found in [4] or [5]. However, present research has not included the discovery of implicit features and furthermore, it has left the problem of explicit features dependent on the mentioning of these features in the individual user reviews or not. The method we propose is language and customer-review independent. It extracts a set of general product features, finds product specific features and feature attributes and is thus applicable to all possible reviews in a product class. What follows is a description of the steps performed in order to obtain the features for each product class and of the manner in which input text is processed to obtain the opinion expressed by customers on them.

3. System Architecture  Our method consists of two distinct steps: preprocessing and main processing, each containing a series of sub modules and using different language tools and resources.

A. Preprocessing  In our approach, we start from the following  scenario: a user enters a query about a product that he/she is interested to buy. The search engine will retrieve a series of documents containing the product name, in different languages. Further on, two parallel operations are performed: the first one uses a language identifier software to filter and obtain two categories - one containing the reviews in English and the other the        reviews in Spanish. The second operation implies a modified version of the system described in [22] for the classification of person names. We use this system in order to determine the category the product queried belongs to. Once the product category is determined, we proceed to extracting the product specific features and feature attributes. This is accomplished using WordNet and ConceptNet and the corresponding mapping to Spanish using EuroWordNet. Apart from the product specific class of features and feature attributes, we consider a core of features and feature attributes that are product-independent and whose importance determines their frequent occurrence in customer reviews.

1) Product-independent features and feature attributes:  There are a series of features that are product independent and that are important to any prospective buyer. We consider these as forming a core of product features. For each of these concepts, we retrieve from WordNet the synonyms which have the same Relevant Domain [23], the hyponyms of the concepts and their synonyms and attributes, respectively.

2) Using WordNet to extract product specific features and feature attributes: Once the product category has been identified, we use WordNet to extract the product specific features and feature attributes. We should notice that, contrary to the observation made in [5], once we establish the product, its corresponding term in WordNet is sense disambiguated and thus the obtained meronyms, synonyms and corresponding attributes are no longer ambiguous. Moreover, the terms obtained in this manner, should they appear in customer reviews, have the meant meaning. We accomplish this in the following steps:  1) For the term defining the product category, we search its synonyms in WordNet [24].

2) We eliminate the synonyms that do not have the same top relevant domain [23] as the term defining the product category  3) For the term defining the product, as well as each for each of the remaining synonyms, we obtain their meronyms from in WordNet, which constitute the parts forming the product.

4) Since WordNet does not contain much detail on the components of most of new technological products, we use ConceptNet [25] to complete the process of determining the specific product features. We explain the manner in which we use ConceptNet in the following section.

After performing the steps described above, we conclude the process of obtaining the possible terms that a customer buying a product will comment on. The final step consists in finding the attributes of the features discovered by applying the ?has attributes? relation in WordNet to each of the nouns representing product features. In the case of nouns which have no term  associated by the ?has attribute? relation, we add as attribute features the concepts found in ConceptNet under the OUT relations PropertyOf and CapableOf. In case the concepts added are adjectives, we further add their synonyms and antonyms from WordNet. As result we have for example, in the case of ?photo?, the parts ?raster? and ?pixel? with the attributes ?blurry?, ?clarity?, ?sharp?.

3) Using ConceptNet to extract product specific features and feature attributes:  ConceptNet [25] contains relations such as CapableOf, ConceptuallyRelatedTo, IsA, LocationOf etc.

In order to obtain additional features for the product in question, we add the concepts that are related to the term representing the concept with terms related in onceptNet by the OUT relations UsedFor and CapableOf and the IN relations PartOf and UsedFor. For example, for the product ?camera?, the OUT UsedFor and CapableOf relations that will added are ?take picture?, ?take photograph?, ?photography?, ?create image?, ?record image? and for the IN PartOf and UsedFor relations ?shutter?, ?viewfinder?, ?flash?, ?tripod?.

4) Mapping concepts using EuroWordNet: We employ EuroWordNet and map the features and  feature attributes, both from the main core of words, as well as the product specific ones that were previously discovered for English, independent of the sense number, taking into account only the preservation of the relevant domain. Certainly, we are aware of the noise introduced by this mapping, however in the preliminary research we found that the concepts introduced that had no relation to the product queried did not appear in the user product reviews.

5) Discovering overlooked product features: The majority of product features we have identified so far are parts constituting products. However, there remains a class of undiscovered features that are indirectly related to the product. These are the features of the product constituting parts, such as battery life, picture resolution, auto mode. Further, we propose to extract these overlooked product features by determining bigrams made up of target words constituting features and other words in a corpus of customer reviews. In the case of digital cameras, for example, we considered a corpus of 200 customer reviews on which we ran Pedersens Ngram Statistics Package [28] to determine target co-occurrences of the features identified so far. As measure for term association, we use the Pointwise Mutual Information score.

In this manner, we discover bigram features such as ?battery life?, ?mode settings? and ?screen resolution?.

B. Main Processing  The main processing in our system is done in  parallel for English and Spanish. In the next section, we        will briefly describe the steps followed in processing the initial input containing the customer reviews in the two considered language and offer as output the summarized opinions on the features considered. We part from the reviews filtered according to language. For each of the two language considered, we used a specialized tool for anaphora resolution - JavaRAP for English and SUPAR [26] for Spanish. Further on, we separate the text into sentences and use a Named Entity Recognizer to spot names of products, brands or shops. Using the lists of general features and feature attributes, product-specific features and feature attributes, we extract from the set of sentences contained in the text only those containing at least one of the terms found in the lists.

1) Anaphora resolution: In order to solve the anaphoric references on the product features and feature attributes, we employ two anaphora resolution tools- JavaRAP for English and SUPAR for Spanish. Using these tools, we replace the anaphoric references with their corresponding referents and obtain a text in which the terms constituting product features could be found.

Using JavaRAP, we obtain a version of the text in which pronouns and lexical references are resolved. For example, the text: ??I bought this camera about a week ago,and so far have found it very very simple to use, takes good quality pics for what I use it for (outings with friends/family, special events). It is great that it already comes w/ a rechargeable battery that seems to last quite a while...??, by resolving the anaphoric pronominal reference, becomes ??I bought this camera about a week ago, and so far have found <this camera > very very simple to use, takes good quality pics for what I use <this camera > for (outings with friends/family, special events). It is great that <this camera> already comes w/ a rechargeable battery that seems to last quite a while...??.

SUPAR (Slot Unification Parser for Anaphora Resolution). The architecture of SUPAR contains, among others, a module solving the linguistic problems (pronoun anaphora, element extraposition, ellipsis, etc.).

We use SUPAR in the same manner as JavaRAP, to solve the anaphora for Spanish.

2) Sentence chunking and NER: Further on, we split the text of the customer review into sentences and identify the named entities in the text. Splitting the text into sentences prevents us from processing sentences that have no importance as far as product features that a possible customer could be interested in are concerned.

We use LingPipe to split the customer reviews in English into sentences and identify the named entities referring to products of the same category as the product queried. In this manner, we can be sure that we identify sentences referring to the product queried, even the reference is done by making use of the name of another product. For example, in the text ?For a little less, I could have bought the Nikon Coolpix, but it is worth the  extra money.?, anaphora resolution replaces <it> with <Nikon Coolpix> and this step will replace it with <camera>. We employ FreeLing in order to split the customer reviews in Spanish into sentences and identify the named entities referring to products of the same category as the product queried.

3) Sentence extraction: Having completed the feature and feature attributes identification phase, we proceed to extracting for further processing only the sentences that contain the terms referring to the product, product features or feature attributes. In this manner, we avoid further processing of text that is of no importance to the task we wish to accomplish. For example, sentences of the type ?I work in the home appliances sector.? will not be taken into account in further processing. Certainly, at the overall level of review impact, such a sentence might be of great importance to a reader, since it proves the expertise of the opinion given in the review. However, for the problems we wish to solve by using this method, such a sentence is of no importance.

4) Sentence parsing: Each of the sentences that are filtered by the previous step are parsed in order to obtain the sentence structure and component dependencies. In order to accomplish this, we use Minipar [27] for English and FreeLing for Spanish. This step is necessary in order to be able to extract the values of the features mentioned based on the dependency between the attributes identified and the feature they determine.

5) Feature value extraction: Further on, we extract features and feature attributes from each of the identified sentences, using the following rules:  1) We introduce the following categories of context polarity shifters, in which we split the modifiers and modal operators in two categories - positive and negative:  - negation: no, not, never etc.

- modifiers: positive (extremely, very, totally etc.)  and negative (hardly, less, possibly etc.) - modal operators: positive (must, has) and negative (if, would, could etc.)  2) For each identified feature that is found in a sentence, we search for a corresponding feature attribute that determines it. Further on, we search to see if the feature attribute is determined by any of the defined modifiers. We consider a variable we name valueOfModifier, with a default value of -1, that will account for the existence of a positive or negative modifier of the feature attribute. In the affirmative case, we assign a value of 1 if the modifier is positive and a value of 0 if the modifier is negative. If no modifier exists, we consider the default value of the variable. We extract triplets of the form (feature, attributeFeature, valueOfModifier). In order to accomplish this, we use the syntactic dependency structure of the phrase, we determine all attribute features that determine the given        feature (in the case of Minipar, they are the ones connected by the ?mod? and ?pred? relations).

3) If a feature attribute is found without determining a feature, we consider it to implicitly evoke the feature that it is associated with in the feature collection previously built for the product. ?The camera is small and sleek.? becomes (camera, small, -1) and (camera, sleek, -1), which is then transformed by assigning the value ?small? to the ?size? feature and the value ?sleek? to the ?design? feature.

4. Assigning polarity to feature attributes  In order to assign polarity to each of the identified feature attributes of a product, we employ SMO SVM machine learning and the Normalized Google Distance (NGD). The main advantage in using this type of polarity assignment is that NGD is language independent and offers a measure of semantic similarity taking into account the meaning given to words in all texts indexed by Google from the world wide web.

The set of anchors contains the terms {featureName, happy, unsatisfied, nice, small, buy}, that have possible connection to all possible classes of products and whose polarity is known. Further on, we build the classes of positive and negative examples for each of the feature attributes considered. From the corpus of annotated customer reviews, we consider all positive and negative terms associated to the considered attribute features. We then complete the lists of positive and negative terms with their WordNet synonyms. Since the number of positive and negative examples must be equal, we will consider from each of the categories a number of elements equal to the size of the smallest set among the two, with a size of at least 10 and less or equal with 20.

We give as example the classification of the feature attribute ?tiny?, for the ?size? feature. The set of positive feature attributes considered contains 15 terms such as (big, broad, bulky, massive, voluminous, large-scale etc.) and the set of negative feature attributes considered is composed as opposed examples, such as (small, petite, pocket-sized, little, etc.). We use the anchor words to convert each of the 30 training words to 6-dimensional training vectors defined as v(j,i) = NGD(wi,aj), where aj with j ranging from 1 to 6 are the anchors and wi, with i from 1 to 30 are the words from the positive and negative categories. After obtaining the total 180 values for the vectors, we use SMO SVM to learn to distinguish the product specific nuances. For each of the new feature attributes we wish to classify, we calculate a new value of the vector vNew(j,word)=NGD(word, aj), with j ranging from 1 to 6 and classify it using the same anchors and trained SVM model. In the example considered, we had the following results (we specify between brackets the word to which the scores refer to:  (small)1.52,1.87,0.82,1.75,1.92,1.93,positive (little)1.44,1.84,0.80,1.64,2.11,1.85,positive (big) 2.27, 1.19, 0.86, 1.55, 1.16, 1.77, negative (bulky) 1.33, 1.17, 0.92, 1.13, 1.12,1.16, negative The vector corresponding to the ?tiny? attribute  feature is: (tiny) 1.51, 1.41, 0.82, 1.32, 1.60, 1.36.

This vector was classified by SVM as positive,  using the training set specified above. The precision value in the classifications we made was between 0.72 and 0.80, with a kappa value above 0.45.

5. Summarization of feature polarity  For each of the features identified, we compute its polarity depending on the polarity of the feature attribute that it is determined by and the polarity of the context modifier the feature attribute is determined by, in case such a modifier exists. Finally, we statistically summarize the polarity of the feature attributes, as ratio between the number of positive quantifications and the total number of quantifications made in the considered reviews to that specific feature and as ratio between the number of negative quantifications and the total number of quantifications made in all processed reviews. The formulas can be summarized in:   Fpos(i)= #pos_feature_attributes(i)/#feature_attributes(i)  Fneg(i) =#neg_feature_attributes(i)/#feature attributes(i)   The results shown are triplets of the form  (feature, % Positive Opinions, % Negative Opinions).

6. Discussion and evaluation  For the evaluation of the system, we annotated a corpus of 50 customer reviews for each language, collected from sites as amazon.com, newegg.com, dealsdirect.com, ciao.es, shopmania.es, testfreaks.es and quesabesde.com. The corpus was annotated at the level of feature attributes, by the following scheme: <attribute> [name of attribute] <feature> [feature it determines] </feature> <value> [positive / negative] </value> </attribute>.

It is difficult to evaluate the performance of such a system, since we must take into consideration both the accuracy in extracting the features that reviews comment on, as well as the correct assignation of identified feature attributes to the positive or negative category. Therefore, we introduced three formulas for computing the system performance. The formula used in measuring the accuracy of the system represented the normalized sum of the ratios between the number of identified positive feature attributes and the number of existing positive attributes and the ratio of identified negative feature and the total number of negative feature attributes for each of        the considered features existing in the text. Secondly, we compute the Feature Identification Precision (FIP) as ratio between the number of features correctly identified from the features identified and the number of identified features. Thirdly, we compute the Feature Identification Recall (FIR) as the number of correctly identified features from the features identified and the number of correctly identified features. The results obtained are summarized in Table I. We show the scores for each of the two languages considered separately and the combined score when using both systems for assigning polarity to feature attributes of a product. In the last column, we present a baseline, computed as average of using the same formulas, but taking into consideration, for each feature, only the feature attributes we considered as training examples for our method. We can notice how the use of NGD helped the system acquire significant new knowledge about the polarity of feature attributes.

Eng Sp Combined Baseline  Eng Baseline Sp   SA 0.82 0.80 0.81 0.21 0.19 FIP 0.80 0.78 0.79 0.20 0.20 FIR 0.79 0.79 0.79 0.40 0.40  Table 1. System results  There are many aspects to be taken into  consideration when evaluating a system identifying features, opinion on features and summarizing the polarity of features. First of all, customers reviewing products on the web frequently use informal language, disregard spelling rules and punctuation marks. At times, phrases are pure enumerations of terms, containing no subject or predicate. In this case, when there is no detectable dependency structure between components, an alternative method should be employed, such as verifying if the terms appearing near the feature within a window of specified size are frequently used in other contexts with relation to the feature. Secondly, there are many issues regarding the accuracy of each of the tools and language resources employed and a certain probability of error in each of the methods used.

7. Conclusions and future work  In this paper we presented a method to extract, for a given product, the features that could be commented upon in a customer review. Further, we have shown a method to acquire the feature attributes on which a customer can comment in a review. Moreover, we presented a method to extract and assign polarity to these product features and statistically summarize the polarity they are given in the review texts in English and Spanish.

The method for polarity assignment is largely language  independent (it only requires the use of a small number of training examples) and the entire system can be implemented in any language for which similar resources and tools as the ones used for the presented system exist.

The main advantage obtained by using this method is that one is able to extract and correctly classify the polarity of feature attributes, in a product dependent manner. Furthermore, the features in texts are that are identified are correct and the percentage of identification is high. Not lastly, we employ a measure of word similarity that is in itself based on the ?word-of-mouth? on the web. The main disadvantage consists in the fact that SVM learning and classification is dependent on the NGD scores obtained with a set of anchors that must previously be established. This remains a rather subjective matter. Also, the polarity given in the training set determines the polarity given to new terms, such that ?large? in the context of ?display? will be trained as positive and in the case of ?size? as negative. However, there are many issues that must be addressed in systems identifying customer opinions on different products on the web. The most important one is that concerning the informal language style, which makes the identification of words and dependencies in phrases sometimes impossible.

Future work includes the development of a method to extend the list of product-dependent features and feature attributes, alternate methodologies for polarity assignation to product dependent feature attributes and finally, the application of a textual entailment system to verify the quality of the feature extracted and the assigned polarity.

8. Acknowledgements  This research has been partially funded by the European Union under the project QALLME number FP6 IST-033860 and by the Spanish Ministry of Science and Technology under the project TEXT-MESS number TIN2006-15265-C06-01.

