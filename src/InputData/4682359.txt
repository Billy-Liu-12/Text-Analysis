Mining Databases by Means of an Incremental Association Rule Learner

Abstract   This paper presents an enhanced algorithm for  mining incremental updates in large databases. Our  paper shows that the algorithm performs significantly  faster than the approach of mining the whole updated  database from scratch. We first present our Enhanced  Algorithm (PEA), a previously implemented algorithm  for mining association rules in large databases.  Next,  we introduce an Incremental Algorithm (PIA) for  mining incremental updates to a database while  efficiently updating the discovered association rules.

We present scale up experiments that show how the  PIA outperforms the PEA.

1. Problem Description   The extensive quantity and diverse nature of  knowledge and data that needs to be stored in a  database requires the development of specialized tools  for: (a) data storage and access; (b) ensuring data  integrity; (c) data maintenance; (d) data analysis; and  (e) effective use of stored knowledge.  Moreover,  methods and tools for intelligent data analysis are  helpful in narrowing the gap between data gathering  and data analysis.

Knowledge-Discovery in Databases (KDD) is the  process of automatically searching large volumes of  data gathered by different data sources to look for  patterns using tools such as classification, clustering,  and association rule learners.

In commerce, association rule learners are used to  discover elements that co-occur frequently within a  data set consisting of multiple independent selections  of elements (such as purchasing transactions), and to  discover rules, such as implication or correlation,  which relate co-occurring elements: ?if a customer  purchases product A, how likely is he to purchase  product B?" and "What products will a customer buy if  he buys products C and D?" are answered by  association-finding algorithms. In commerce, this is  also known as market basket analysis. As with most  data mining techniques, the task is to reduce a  potentially huge amount of information to a small,  understandable set of statistically supported  statements. The problem is not only to discover the  rules but to do this process incrementally as data and  knowledge is added to the database.  Maintenance of  association rules is therefore an important KDD  problem of existing database algorithms.

The problem of association rule mining was first  introduced by Apriori [9] to discover associations  between items from a static set of transactions (a static  set of transactions comprises transactions that are not  subject to regular modification like addition or  deletion). Many interesting works have been published  to address this problem [1, 3-7, and 10], While Apriori  is regarded as highly competitive, our alternative  implementation of association rule mining, PEA [8],  reduced the I/O and computational requirement  compared to [9].

Incremental mining was first introduced in [2] to  address the problem of updating the association rules  as new transactions become added to a database.

The proposed algorithm, FUP [2], stores the counts  of all the frequent itemsets found in a previous mining  operation and iteratively discovers (k + 1) itemsets in  the same way as the Apriori algorithm.

In recent years, several algorithms have been  proposed, including the MAAP algorithm [11] to solve  the problem of incremental mining. The problem with  these algorithms is that they calculate maximum  frequent itemsets by the Apriori algorithm framework,  Apriori-like algorithms tend to suffer from three  inherent problems:  (1) The occurrence of a potentially huge set of  generated candidate itemsets making it impractical to  investigate the issues of frequency (when an itemset is  a candidate it must be tested regardless of its  frequency).

(2) The need for certain hypothesis to prune the  candidate itemsets for computational savings.

DOI 10.1109/ICCIT.2008.211    DOI 10.1109/ICCIT.2008.211     (3) The need for multiple scans of databases which  increases I/O time and computational costs.

These problems motivate our search for new  algorithms that can address these weaknesses.  This  paper demonstrates how an association rule learner  may achieve incremental updates.

2. Research Objectives   Our aim is to achieve the following goals:  1. Minimizing the set of generated candidate itemsets.

2. Maximizing the set of pruned candidate itemsets.

3. Counting the supports of candidate itemsets over the database in less time  4. Reducing I/O time by reducing the number of database scans over the database.

If these goals are met then the speed of the execution  of the algorithm will increase, in addition the I/O and  computational costs would be significantly reduced  relative to those of standard Apriori. The next sections  discuss how we went about trying to achieve these  goals.

3. Description of the Algorithms   The algorithms that we describe in this section were  implemented using PowerBuilderTM (Sybase Product)  on an OracleTM database and can be applied on any  other database and therefore have the advantage that  they can cope with general purpose and large real-  world database transaction problems.

[I] The Enhanced Algorithm (PEA):   The previously implemented algorithm PEA [8] has  demonstrably improved performance over the Apriori  algorithm. An important result of our approach is that  we achieve I/O improvement over Apriori; this was  comprehensively demonstrated in our previous  publication [8].

[II] The Incremental Algorithm (PIA):   Formal Problem Description  The association mining task can be stated as  follows: Let I={1,2, .., n}  be a set of n distinct  attributes, also called items, and let D be the input  database. Typically D is arranged as a set of  transactions, where each transaction T has a unique  identifier TID and contains a set of items such that T?

I. A set of items X? I is called an itemset. For an  itemset X, we denote its corresponding Tid-list as the  set of all TID's that contain X as a subset. The support  of an itemset X is the percentage of transactions in D  in which X occurs as a subset. An itemset is a frequent  itemset if its support ? Minsupp where Minsupp is a  user-specified minimum support threshold.

An association rule is an expression A ? B where  A, B are itemsets. The confidence of the rule is the  conditional probability that a transaction contains B  given that it contains A. A rule is confident if the  confidence is greater than Minconf where Minconf is a  user specified minimum confidence threshold.

Finding frequent itemsets is computationally and  I/O intensive. Let |I| = m is the number of items. The  search space for enumeration of all frequent itemsets is  2m which is exponential in m, thus we need to  investigate each of these items for each transaction in  the database. This high computational cost may be  acceptable when the database is static, but not in  domains with evolving data, since the itemset  enumeration process will be frequently repeated. In  this paper we only deal with how to efficiently mine  frequent itemsets in evolving databases.

Table 1 summarizes the notation used in the  remainder of this section. Given DB, db, |DB|, |db|,  minsup and LkDB, the problem of updating  association rules is to find the set LkDB +db of large  itemsets in DB + db.

Notation Definition  DB The set of old transactions  db The set of new transactions  DB + db The total set of transactions  |A| The number of transactions in the  transaction database A  minsup Minimum support threshold  X A set of items (i.e., one itemset)  Support A(X) Support of X in the set of transactions A  Tid-list A (X) Transaction list of X in the set of  transactions A  CKA Set of candidate k-itemsets in a set of  transactions A  LKA Set of large k-itemsets in a set of  transactions A  PruneSet Set of large itemsets in DB that have 0  support in db  Unchecked Set of large k-itemsets in DB that are not  counted in db  Table 1: Notation Used in the algorithm       The Incremental Algorithm Description  In this section, we explain how the algorithm  works, and the optimizations it employs. The algorithm  is presented in Pseudo Code 1. Inputs to the algorithm  are DB, db, LkDB (along with their supports in DB),  |DB|, |db|, and minsup. The output of the algorithm is  LkDB+db, the set of large itemsets in DB + db. We can  break down the algorithm into three steps as identified  below:    1. Counting 1-itemsets in db and creating a Tid-list  for each item in db.

2. Checking the large itemsets in DB whose items are  absent in db and their supersets for largeness in  DB+db.

3. Checking the large itemsets in db for largeness in  DB+db    Algorithm (DB, db, LDB, |DB|, |db|, minsup);  1 C1db = all 1-itemsets in db whose support  > 0  2 InitialPruneSet = L1DB ?  C db  3 initial_pruning(InitialPruneSet) (See Pseudo Code 2)  4 k=1  5 while Ckdb ? ? and L k DB ? ?  do begin  6 Unchecked = LkDB  7  for all X ?  Ckdb  do  8  if X is small in db and X is large in DB then  9 remove X from UncheckedSet  10 if X is small in DB+ db then  11 remove all supersets of X from LDB  12 else  13 add X to LDB+db  14 end  15 else if X is large both in db and DB then begin  16 remove X from UncheckedSet  17 add X to LDB+db and L k db  18 end  19 else if X is large in db but small in DB then begin  20 find support DB(X) using Tid-lists  21 if X is large in DB + db then  22 add X to LDB+db and L k  db  23 end  24 for all X ? UncheckedSet do begin  25 find supportdb(X) using Tid-lists  26 if X is small in DB +db then  27 remove all supersets of X from LDB  28 if X is large in DB + db then  29 add X to LDB+db  30 end  31 k=k+1  32 C = generate_candidate(Lk-1db) (See Pseudo Code 3)  33 end  Pseudo Code 1: Update of frequent Itemsets    Initial_Pruning (Initial_PruneSet)  1 while PruneSet ? ? do begin  2 X = first element of PruneSet  3 if X is small in DB + db then  4 remove X and all its supersets from LDB and  PruneSet  5 else  6 begin  7 add the supersets of X in LDB to the PruneSet  8 add X to LDB+db  and remove X from LDB  9 end  10 remove X from PruneSet  11 end  Pseudo Code 2: Initial Pruning Algorithm    Generate_Candidate(Lk-1db);  1 Ckdb = ?  2 for all itemsets X ? Lk-1db and Y ? L k-1  db do  3 if (X1 = Y1) and ?. and  (Xk_2= Yk_2)  and    (Xk_1<  Yk_1)    then begin  4 C= X1 X2 ? Xk-1 Yk-1     5 if all subsets S of C is an element of Lk-1db then  begin  6 Tid-list db (C) = Tid-list db (X)   ?   Tid-list db (Y)  7 support db(C) = |Tid-list db (C) |  8 add C to  Ckdb  9 end  10 end  Pseudo Code 3: Candidate Generation Algorithm  As proposed in the previous section, efficient  updates of large itemsets can be achieved by  considering the set of previously discovered rules to  effortlessly replace certain old and large itemsets with  the addition of new ones. The algorithm reduces  (prunes) the set of candidate itemsets, and this is  achieved by the following heuristics:  1. If a candidate itemset is large in DB, and it is also large in db, then it is large in the updated Database  (DB+ db)  2. Similarly if a candidate itemset is small in DB, and it is also small in db, then it is small in the updated  Database.

3. If a candidate itemset is large in DB, and it doesn't exist in db we compare the support of this itemset to  minsup(DB + db) , if its support was greater then  this itemset is pruned and is removed from the set of  candidate large Itemsets  4. If a candidate itemset is large in DB, and it doesn't exist in db and if  the support of this itemset is less  than minsup(DB + db), this itemset is pruned and is  removed from the set of candidate large Itemsets  5. If a candidate itemset is large in DB, and it doesn't exist in db and if  the support of this itemset is less  than minsup(DB + db), all the supersets of this  itemset are pruned and are removed from the set of  candidate large Itemsets  6. If a candidate itemset is large in DB, and it doesn't exist in db and if  the support of this itemset is  greater than minsup(DB + db), this itemset is  removed from the set of candidate large Itemsets into  the set of large itemsets in the updated database.

7. If a candidate itemset is large in DB, and it doesn't exist in db and if the support of this itemset is greater  than minsup(DB + db), all the supersets of this  itemset must be checked against minsup(DB + db), if  their support were less, then they are pruned with all  their supersets, else they are added to large itemsets  and their supersets are also checked, and so on.

These heuristics rely on knowledge about standard  (existing) pruning rules such as:  1. Supersets of small itemsets are small 2. Subsets of large itemsets are large  These heuristics (our proposed rules) can  potentially lead to significant I/O and computational  savings and relatively faster response time in dynamic  databases (databases which are frequently updated)  rather than rediscovering all the patterns by scanning  the entire old and new databases.

4. Supporting Experiments   The implementation of the rule association  algorithms for knowledge discovery was undertaken  on a medical database at the international medical  center hospital in Cairo to compare the efficiency of  PEA and PIA running on the same platform and  environment. The algorithms tested and results are  described in the next section.

Methods were developed to extract data from the  huge database and transform it into the required (TID,  itemset) form then loading it into our data warehouse.

The Incremental Algorithm and the Enhanced  Algorithm were applied to the processed data.

To assist comparison of both algorithms these were  implemented on the same hardware (identical  computers in processor and memory) with the same  operating system (Windows 2003 with the minimum  services installed), no additional processes were  running in the background, no scheduled programs  were running.

To minimize the influence of any spurious external  criterion that might have interfered with the calculation  of CPU time, the experiment was implemented several  times and the readings were recorded, and when  comparing the readings which were approximately the  same, the averages of the readings were recorded for  statistical analysis for both of the algorithms.

Figure-2  The Enhanced Algorithm (PEA)     The previous figure shows how PIA is applied on  the whole database and how the frequent itemsets are  generated and how rules can be extracted.

Figure-3  The Incremental Algorithm (PIA)  PIA is implemented on the initial database. After  adding new transactions PIA is implemented again on  the incremental transactions only rather than applying  it on the whole database. This figure shows the  implementation of PIA and how the frequent itemsets  are generated and how rules are extracted.

Figure-4  Comparing the results  The previous figure shows that the two algorithms  have the same results in terms of the frequent itemsets  and the rules generated but not in terms of time.

Figure-5  Measuring Time at different DB sizes at fixed  increment ratio for both algorithms.

Fig. 5 shows that PIA outperforms PEA since time  needed to discover the frequent itemsets is much  smaller than time needed to rerun PEA on the whole  database. It shows also that when the database size  increases the difference in time between PEA and PIA  increases.

Figure-6  Generated Candidates at different DB sizes for both  algorithms  Fig. 6 shows that the number of pruned candidates  in case of the PIA is greater than that in PEA which  indicates that PIA is more efficient.

Figure-7  Pruned Candidates at different DB sizes for both  algorithms    Fig. 7 confirms the same conclusion of Fig. 6 as the  number of generated candidates in case of  PIA is less  than that in PEA, this helps in decreasing time to test if  the generated candidate itemsets is likely to be  frequent itemsets thus decreasing the time for the  execution of the algorithm.

5. Research Outcome   The experiments provided the following evidence  that the aforementioned weaknesses in association rule  learners can be overcome:  ? The algorithm reduced the number of generated candidate itemsets as illustrated  in Fig. 6.

? The number of pruned itemsets in PIA was larger than in PEA which helped to  decrement the time needed for itemset  investigation for frequency of itemsets as  illustrated in Fig. 7.

? The need for multiple scans of databases is reduced because the algorithm scans the  database once only.

These heuristics that were described in section 3  (results presented in the previous section) can yield  significant I/O and computational savings and  relatively faster response time in dynamic databases as  opposed to rediscovering all of the patterns by the  scanning of the entire databases (old and new).

We conclude that the improvement in time for the  PIA can help many researchers in applying the  algorithm in many other fields to extract interesting  rules that can advance the state of the art, because  many researchers and many experts would not be  satisfied to wait for a long time for an algorithm to be  applied on the whole huge databases particularly if the  previously discovered knowledge is not considered.

Acknowledgements   We would like to thank the international medical  center hospital in Cairo for making these research  experiments possible.

9. References  [1] A. Savasere, E. Omiecinski, and S. Navathe. "An  efficient algorithm for mining association rules in large  databases". In Proc. of the 21th International  Conference on Very Large Data Bases, pages 432{444,  September 1995.

[2] D. Cheung, J. Han, V. Ng, and C.Y. Wong.

"Maintenance of discovered association rules in large  databases: An incremental updating technique". In Data  Engineering, pages 106-114, February 1996.

[3] J. Han and J. Pei. "Mining frequent patterns by pattern- growth: Methodology and implications". In ACM  SIGKDD Explorations (Special Issue on Scaleble Data  Mining Algorithms), December 2000.

[4] J. Han, J. Pei, B. Mortazavi-Asl, Q. Chen, U. Dayal, and M.-C. Hsu. "Freespan: Frequent pattern-projected  sequential pattern mining". In Knowledge Discovery  and Data Mining, pages: 355-359, August 2000.

[5] J. Lin and M.H. Dumham. "Mining association rules: Anti-skew algorithms". In Proc. of 1998 Int'l Conf. on  Data Engineering, pages 486-493, 1998.

[6] J. Park, M.-S Chen, and P.S Yu. "Using a hash-based method with transaction trimming for mining   and Data Engineering, volume 9, pages 813-825,  October 1997.

[7] J. Pei, J. Han, and L.V.S. Lakshmanan. "Mining frequent itemsets with convertible constraints". In Proc.

of 2001 Int. Conf. on Data Engineering, 2001.

[8] Medhat M. M. Fakhry, Walid Adly Atteya, "An Enhanced Algorithm for Mining Asociation Rules",   and Information Systems - Cairo - Egypt, june 2002.

[9] R. Agrawal, T. Imielinski, and A. Swami. "Mining association rules between sets of items in large  databases". In Proc. of ACMSIGMOD, pages 207-216,  May 1993.

[10] R. C. Agarwal, C. C. Aggarwal, V.V.V. Prasad. "A tree projection algorithm for generation of frequent  itemsets". Journal of Parallel and Distributed  Computing, Volume 61, Number 3, pages 350-371,  March 2001.

[11] Z. Zhou and C.I. Ezeife. "A low-scan incremental association rule maintenance method". In Proc. of the  14th Canadian Conference on Artificial Intelligence,  June 2001.

