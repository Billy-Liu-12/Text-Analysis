Mining Association Rules from Data with Missing Values by Database Partitioning and Merging

Abstract  Often, real world applications contain many missing val- ues. In mining association rules from real datasets, treating missing values is an important problem. In this paper, we propose a pattern-growth based algorithm for mining asso- ciation rules from data with missing values. No data impu- tations are performed. Each association rule is evaluated using all the data records with which attributes of it are not missing values. Our algorithm partitions the database so that the data record with which the same attributes con- tain missing values is assigned to the same database parti- tion, and the algorithm mines association rules by combin- ing these database partitions. We propose methods of re- ducing processing workload: estimating the upper bound of global support using local supports, reutilizing part of the constructed tree structure, and merging redundant database partitions. Our performance study shows that our algo- rithm is efficient and can always find all association rules.

1. Introduction  Mining association rules within a large database is a rep- resentative problem in data mining. Several effective algo- rithms such as candidate generation approaches [1] and pat- tern growth approaches [3, 4], have been proposed. How- ever, these algorithms have not dealt with missing values, and they cannot find correct association rules from data with missing values. The missing values are inevitable in real ap- plications:  ? Medical data: The kind of input data differ with each patient, since the suspected disease or condition, de- tails of consultation, and medical therapy differ with each patient. Thus, many missing values appear in medical data. Genotype data also contains many miss- ing values. The locus cannot be analyzed for various reasons, such as the condition of the specimen mate- rial, neighboring sequence of analyzing locus position, and the condition of analyzer.

? Basket data of retail businesses: Basket data would  seem not to contain any missing values. A basket data record consists of a combination of bought items. Dif- ferent basket data records contain different combina- tions of items because different customers bought dif- ferent items. We cannot consider an item that is not contained in a basket data record as a missing value.

It is simply an item that was not purchased by a cus- tomer. However, there are cases when missing values have to be taken into consideration in basket data. The types of items being sold changes at various times. The items that are not sold cannot be interpreted as not hav- ing been purchased. These items cannot be judged as having been purchased or not. When we mine asso- ciation rules between items A and B, we have to use basket data records for the period when both items A and B are sold. Additionally, the items being sold dif- fer with each shop. The basket data records of the shop where item A is not sold are equal to a dataset where the attribute value of attribute A is a missing value.

The correct association rules cannot be mined when we apply traditional association rule mining algorithms by dis- regarding missing values. Therefore, missing values are removed in preprocessing. The most common preprocess- ing technique for treating missing values is data imputation (filling in the missing values), such as mean, multiple impu- tation, and expectation maximization (EM) [2]. Whichever method is selected, the imputed data are biased. Another method is record deletion. A record with missing values is simply deleted from the database altogether. The drawbacks of this method are that information included in the deleted records is lost, and the sample size becomes smaller.

The problem of mining association rules from data with missing values has been reported [6, 5, 7]. The the idea that to cut a database into several valid databases for each rule, a valid database must not have any missing values, was intro- duced, and the robust association rule mining (RAR) algo- rithm for mining association rules from valid database was proposed [6]. The RAR algorithm can mine correct associ- ation rules. However, its performance is not good because it is based on the Apriori algorithm [1]. Furthermore, the RAR algorithm is not always able to find all the association  International Workshop on Component-Based Software Engineering, Software Architecture and Reuse (ICIS-COMSAR?06)    rules, which is a drawback of this algorithm. ?AR algorithm for generating approximate rules (?AR rules) by imputing missing values was proposed [5]. A a method to compute the upper and lower bounds of rules under all possible as- signments of missing values was proposed [7]. These two methods mine approximate association rules by estimation.

In this paper, we propose a pattern-growth based algo- rithm for mining association rules from data with missing values, called ?Association rules from data with Missing values by Database Partitioning and Merging (AMDPM)?, In the same way as [6], each association rule is evalu- ated using all the data records with which attributes of it are not missing values. It consists of evaluating a rule only with known values, that is, ignoring missing values.

The AMDPM algorithm divides the database into partitions where the records in which the same attributes contain miss- ing values are assigned to the same database partition, and it counts the local support of itemsets for each database partition. The association rules are derived by combining some database partitions. The AMDPM algorithm effec- tively reduces processing workload by: 1) estimating the upper bound of global support using local supports, 2) con- structing and reutilizing part of a tree structure, and 3) merg- ing redundant database partitions. The experimental results show the effectiveness of this algorithm.

The rest of this paper is organized as follows: In the next section, we explain the problem of mining association rules from data with missing values. In section 3, we propose our algorithm. Performance evaluations are given in section 4.

Section 5 concludes the paper.

2. Data and association rule with missing val- ues  2.1. Problem definition  First we introduce some basic concepts of associa- tion rules from data with missing values. Let A = {A1, A2, . . . , Am} be a set of all attributes. A set of attributes is called an attribute pattern. A pair comprising an attribute Ai and an attribute value a j is called an item and denoted as {Ai : a j}. Let I be a set of all items. A set of items is called an itemset. Let D = {r1, r2, . . . , rn} be a set of all records, where each record has an associated unique iden- tifier and a set of attribute values. When an attribute value is NULL, we call it a missing value. For example, Table 1 presents a database with five attributes (A,B,C,D,E) of which the attribute values are a0,a1,a2 for A, b0,b1,b2 for B, c0,c1,c2 for C, d0,d1,d2 for D and e0,e1 for E. Missing values appear in all of the records except ID 1, 7, 14, 17, and 20. For example, the record for ID 2 has two missing values at attributes A and E. A record r is disabled for item- set X if r contains missing values for at least one attribute value of an item in X. These records are called disabled records. We denote Dis(X) the set of disabled records for X, DX the subset of D containing X, and |D| the number of records in D. The valid records of an itemset X, which can  Table 1. Database ID A B C D E 1 a0 b0 c1 d1 e0 2 b2 c1 d1 3 b0 c1 d0 4 b0 c1 d2 e0 5 b0 c1 d0 6 a2 d1 7 a2 b1 c2 d1 e1 8 a0 c0 d1 9 a0 b0 c0 d1  10 b1 c2 d1 e0  ID A B C D E 11 a2 d0 12 b1 c0 d0 13 a0 c0 d1 14 a1 b0 c0 d0 e0 15 c0 d2 16 b0 c1 d0 17 a1 b1 c0 d2 e1 18 b0 c1 d0 19 b1 c2 d1 20 a1 b1 c2 d1 e0  be utilized to evaluate association rules derived from X, is VR(X) = D ? Dis(X). For example, disabled records for itemset {B:b1,C:c1,D:d1} are records ID 6, 8, 11, 13, and 15, and valid records for itemset {A:a0,B:b0,C:c0,D:d0} are records ID 1, 7, 9, 14, 17 and 20. An itemset X has support s, if s% of records in VR(X) contain X; here, we denote  s 100 = S upp(X). The number of records containing X is called the support count of X.

An association rule is an implication of the form X ? Y with X, Y ? I, X ? Y = ?. A rule is evaluated by support (Supp), confidence (Conf) and representativity (Repr). The support of rule X ? Y is S upp(XY)(= |DXY ||D|?|Dis(XY)| ). The confidence of the rule X ? Y means that c% of records in VR(XY) that contain X also contain Y, which can be writ- ten as the ratio |DXY ||DX |?|Dis(Y)?DX | . To avoid mining association rules from a small VR that are over-specified rules, another evaluation value called representativity is defined; the rep- resentativity of itemset X is calculated by |VR(X)||D| .

The problem of mining association rules from data with missing value D is to find all the rules that satisfy a user- specified minimum support (MinSupp), minimum confi- dence (MinConf) and minimum representativity (MinRepr).

This problem can be decomposed into two sub-problems:  Step 1: Find all itemsets that have support above the user- specified MinSupp and MinRepr. Itemsets that have support above the user-specified minimum support are called frequent itemsets. The items contained in a fre- quent itemset are called frequent items.

Step 2: For each itemset found in Step 1, derive all rules that have more than the user-specified minimum con- fidence as follows: for itemset X and Y (Y ? X = ?), if |DXY ||DX |?|Dis(Y)?DX | > MinCon f , then the association rule X ? Y is derived.

2.2. Conventional algorithms  Here, we explain the RAR algorithm [6] for finding all frequent itemsets from data with missing values. The main idea of the RAR algorithm is that each itemset holds a set of ID lists of disabled records. Since the RAR algorithm is based on the Apriori algorithm, it finds frequent itemsets for each pass. At the first database scan, the RAR algorithm generates an ID list of disabled records for each item. At  International Workshop on Component-Based Software Engineering, Software Architecture and Reuse (ICIS-COMSAR?06)    each pass, it generates a set of candidate itemsets from the frequent itemsets found at the previous pass. An item which is not frequent at pass k is not considered after pass (k + 1).

The ID list of disabled records of itemset X is the union of the ID list of disabled records for each item in X. The RAR algorithm then scans all the records to obtain the count sup- ports of the candidate itemsets, and determines the frequent itemsets. The RAR algorithm reduces the number of gener- ated candidate itemsets by using the itemset?s down-closed property: if an itemset is frequent, all its sub-itemsets have to be frequent. However, this property is not always con- sistent for data with missing values. For example, assume that the number of records in the entire database is 15, the minimum support is 0.35, the support count of itemset X is 5 with 1 disabled record, and the support count of itemset XY is 4 with 5 disabled records. In this case, the supports of X and XY are 0.29 and 0.40, respectively. X does not satisfy the minimum support, but XY does. Since the RAR algo- rithm does not find itemsets that contain X, it cannot find XY. Thus, it is not always possible for the RAR algorithm to find all association rules. Although the RAR algorithm is based on the Apriori algorithm, the method of holding an ID list of disabled records for each itemset can be applied to the pattern growth approaches such as the FP-growth al- gorithm [3] and the AFOPT (ascending frequency ordered prefix-tree) algorithm [4]. The pattern growth approaches find frequent itemsets without candidate generation. The basic idea of the pattern growth approach is to grow an item- set from its prefix. It constructs a conditional database for each frequent itemset X; then the support counting for the itemsets that have X as the prefix is performed only on X?s conditional database. The FP-growth algorithm first com- presses the database into an FP-tree, but retains the item- set association information at the same time. It then di- vides the FP-tree into a set of conditional databases, each of which is associated with one frequent item, and it mines each such database separately. The AFOPT algorithm uses the ascending frequency ordered prefix-tree to represent the conditional databases, and the tree is traversed top-down. It will first mining on the first item?s conditional database. Af- ter the mining of first item is finished, all of its subtrees are merged with its siblings, called the push-right step, and the second subtree of the root becomes the first subtree of the root. Then, the mining process of the first item is contained.

The AFOPT algorithm is more effective than the FP-growth algorithm [4]. However, there is a drawback in that there is no guarantee that all frequent itemsets will be mined.

These conventional pattern growth approaches can mine all association rules by using the minimum support count on the minimum number of records as the pruning itemset threshold. Since the minimum number of records that sat- isfies MinRepr is (|D| ? MinRepr), the minimum number of minimum support count is (|D| ?MinRepe?MinS upp).

This value is the minimum number of support count of item- sets that have the potential to derive association rules that satisfy minimum thresholds. Thus, with the conventional  pattern growth approaches it is possible to mine all rules by using (|D ? MinRepe ? MinS upp) as a threshold of the itemset pruning instead of MinSupp. Since it is the same as reducing the threshold of itemset pruning, the number of processing itemsets increases. Moreover, excessive support counting occurs for itemsets that cannot derive association rules. Thus, there is the possible disadvantage of a higher processing cost. However, it is possible to mine all associa- tion rules.

3. Proposed algorithm  In this section, we describe our algorithm, AMDPM (Association rule mining from data with Missing values by Database Partition and Merger), for mining association rules from data with missing values. The AMDPM algo- rithm adopts a pattern growth approach, and handles par- titioned databases. It consists of two processes: database partitioning and association rule mining.

3.1. Database partitioning process  This process generates database partitions. The database is divided into partitions so that each partition consists of records that have the same missing values in the same attri- butions. The association rule mining process utilizes these database partitions. When an attribute pattern is Z, we de- note DP(Z) as the database partition that consists of records containing no missing value of attributes in Z. Here, the support and the support count in a database partition are called local support and local support count, respectively.

An itemset whose local support is greater than MinSupp is called a local frequent itemset. Furthermore, the support and the support count in an entire database are called global support and global support count, respectively. An itemset whose global support is greater than MinSupp is called a global frequent itemset. The procedure of this process is as follows: Step 1. Assign records to each database partition: A record  is read from the database and is assigned to the asso- ciated database partition. At the same time, the occur- rence of each item and the number of records contain- ing no missing values for each attribute are counted.

Step 2. Detect exclude-attributes: If an attribute Ai does not satisfy MinRepr, we do not need to find itemsets containing items in Ai. When the number of records containing no missing values of Ai is less than (|D| ? MinRepr), Ai is inserted into the exclude-attribute list.

Step 3. Detect exclude-items: If an item I j does not satisfy MinSupp and MinRepr together, we do not need to find itemsets containing I j. When the global support count of I j is less than (|D| ? MinS upp ? MinRepr), I j is inserted into the exclude-item list. An attribute Ai is inserted in the exclude-attribute list, when all items of Ai are in the exclude-item list.

Step 4. Merge redundant database partitions: All at- tributes in the exclude-attribute list are deleted from  International Workshop on Component-Based Software Engineering, Software Architecture and Reuse (ICIS-COMSAR?06)    each database partition. The database partitions that consist of identical attribute patterns are merged into one database partition, and the local support counts of each item in the merged database partitions are up- dated.

Step 5. Decide the processing order of each attribute: The processing order of each attribute in the associ- ation rule mining process is determined. To reduce the amount of data in the main memory, attributes are ar- ranged in ascending order of the number of database partitions they are contained in.

The database partitioning process generates database partitions and deletes unnecessary attributes from them.

Since Repr of an attribute is calculable from the number of records containing no missing values, it is possible to detect the attributes that cannot satisfy MinRepr (Step 2). More- over, as described in Section 2.2, the item whose support count is less than the minimum support count in the mini- mum number of records cannot be contained in association rules that satisfy all minimum thresholds. Therefore, it is not necessary to take into consideration items whose sup- port count is less than (|D|?MinS upp?MinRepr) (Step 3).

These attributes are deleted from database partitions since they are unnecessary in the association rule mining process.

When an unnecessary attribute is deleted from the database partitions, database partitions with an equal attribute pattern may occur. By merging such database partitions into one database partition, we can reduce the number of partitions to be processed (Step 4).

A database partition-based algorithm, called ?Partition algorithm?, was proposed [8]. Partition algorithm divides the database into database partitions so that each database partition can fit in the main memory, and it finds local frequent itemsets for each database partition. After all the partitions have been processed, all the local frequent itemsets become candidate itemsets for the entire database.

This algorithm is one of the methods for handling very large databases. It does not take missing values into account and cannot be applied to data with missing values.

On the contrary, the AMDPM algorithm focuses on mining association rules from data with missing values, and the method of dividing the database is different. Additionally, the AMDPM algorithm does not count all the local frequent itemsets, as will be described in detail in section 3.2. By estimating the upper bound of global support, the AMDPM algorithm reduces the amount of counting records for some itemsets.

Example: Here, we show an example using the database in Table 1 with MinSupp 0.25, MinConf 0.70 and MinRepr 0.50. First, each record is assigned to the associated database partitions. For example, the record of ID 2 is assigned to DP(B,C,D), since this record has missing values at attribute A and E. After all records are processed, seven database partitions DP(A,B,C,D,E)={1,7,14,17}, DP(A,B,C,D)={9}, DP(B,C,D,E)={4,10}, DP(A,C,D)={8,13}, DP(A,D)={6,11}, DP(B,C,D)={2,3,5,12,16,18,19}, and DP(C,D) ={15} are generated. Here, the number of records containing  no missing values for each attribute and the support count of each item are counted. Next, the exclude-attributes and items are detected. In this example, attribute E is an exclude-attribute since Repr of attribute E is 0.35. Since the number of records in the entire database is 20, MinSupp is 0.25 and MinRepr is 0.50, items whose global count is less than 3 become exclude-items.

In this example, item B:b2 is inserted to exclude-item list.

Then, exclude-attributes are deleted from database partitions and redundant database partitions are merged. When attribute E is deleted from DP(B,C,D,E), all the records in DP(B,C,D,E) are added to DP(B,C,D). After deleting all the exclude attributes, the database partitions consist of five database partitions.

3.2. Association rule mining process  Next, we describe the association rule mining process.

The association rules are mined by using the database parti- tions. This process mines association rules with items of an attribute by rotation. To determine the number of support counts, the AMDPM algorithm uses the prefix-tree struc- ture similar to AFOPT [4] and traverses it using a top-down strategy. The root node of the prefix-tree is empty(NULL).

Each node in the prefix-tree contains three types of infor- mation: the item, the support count array of the itemset cor- responding to the path from the root to the node for each database partition, and the pointers pointing to the node?s children and parent. The nodes in the prefix-tree are ordered based on the processing order of attributes. Here, we de- fine semi-minimum support (SemiMinSupp), whose value is less or equal to MinSupp, and an item whose local sup- port is greater than SemiMinSupp is a local semi-frequent item.

The AMDPM algorithm calculates the global support of an itemset from the local support count of database parti- tions. The global support GS of an itemset X is calculated by the following equation.

GS (X) =  ?m i=1 LC(X; DP(Zi)) ?m  i=1 RC(DP(Zi)) (1)  Here, database partitions that contain all attributes in X are DP(Z1), . . . ,DP(Zm); LC(X; DP(Zi)) is the local sup- port count of X on DP(Zi), and RC(DP(Zi)) is the number of records in DP(Zi). GS (X) is calculable by the local support count of X and the number of records of database partitions containing all attributes of X. However, GS (X) cannot be calculated when the local support counts of X are unknown in some database partitions. In this case, the upper bound of global support UGS can be calculated by the following equation.

UGS (X) = ?n  i=1 LC(X; DP(Zi)) + ?m  j=n+1 min{LC(X?; DP(Z j))} ?m  i=1 RC(DP(Zi)) (2)  Here, database partitions that contain all attributes in X are DP(Z1), . . . ,DP(Zm); database partitions in which the lo- cal support count of X is known are DP(Z1), . . . ,DP(Zn); database partitions in which the local support count of X  International Workshop on Component-Based Software Engineering, Software Architecture and Reuse (ICIS-COMSAR?06)    is unknown are DP(Z(n + 1)), . . . ,DP(Zm); and any sub- itemset of X is X?. Since LC(X; DP(Z j)) is less than or equal to any LC(X?; DP(Z j)), UGS (X) calculated by equation (2) is less than or equal to GS (X). If all of min{LC(X?; DP(Z j))} = 0, UGS (X) is equal to GS (X).

We show the procedure for mining association rules con- taining items of attribute Ai.

Step 1. Construct a prefix-tree: The database partitions containing attribute Ai are read to construct the prefix- tree for counting the local support counts of itemsets with Ai. By reading records from each database par- tition, the prefix-tree grows. Assume that record r is read from database partition DP(Z). The local frequent items and local semi-frequent items are extracted from r, and exclude-items are removed and sorted by the processing order of attributes. The remaining items r? are used for building the prefix-tree. The prefix- tree generation for i-th item Ii in r? is as follows: the current node has a child node of item Ii, the current node moves to it and increments the support count of database partition DP(Z). When the current node does not have a child node of Ii, a new node for the cur- rent node?s child node of item Ii is created, the current node moves to the created node, the support count of database partition DP(Z) is set to 1. Additionally, the AMDPM algorithm counts the local support count of combinations of two items, each containing one item of attribute Ai. This information is utilized when calcu- lating the upper bound of global support in the process of finding global frequent itemsets.

Step 2. Find global itemsets: For each item Ip of attribute Ai, the subtree of rooted at Ip is traversed, and the lo- cal supports of all items in it are counted. When an item Iq satisfies MinRepr and is a local frequent item- set at at least one database partition, Iq?s GS or UGS is calculated. If GS is calculated and satisfies MinSupp, itemset {Ip, Iq} is inserted into the global frequent item- set list. If UGS is calculated and satisfies MinSupp, {Ip, Iq} may become a global frequent itemset. {Ip, Iq} and all database partitions in which the local support of {Ip, Iq} is unknown are inserted into the additional counting itemset list. Then the subtree of rooted at Ip is traversed again to build a new prefix-tree that contains only the items satisfying SemiMinSupp at at least one database partition. The above procedure is repeated on a new prefix-tree.

Step 3. Count additional counting itemsets: By read- ing the records from database partitions in the addi- tional counting itemset list, nodes corresponding to ad- ditional counting itemsets are created, or their support count arrays are updated. For each node that is cre- ated or updated, the local support counts of additional counting itemsets are counted by traversing the path from the updated node to the root node. Just as in Step 2, the global support of each additional counting item- sets is calculated, and itemsets whose global support is  greater or equal to MinSupp are inserted into the global frequent itemset list.

Step 4. Reconstruct the prefix-tree: None of the attribute Ai nodes are required following this process, since all the itemsets with Ai have already been mined. The Ai nodes are deleted from the prefix-tree. All of these are child nodes of the root node. The subtree of a child node of a deleted node is merged with siblings.

Step 5. Count sub-itemsets: By traversing the subtree of all the root node children, the local support counts of all items in it are counted. Here, only the itemsets that are sub-itemsets of the global frequent itemsets mined in Step 3 and 4.

Step 6. Derive association rules: For each global frequent itemset, association rules are derived in the same way as when using an ordinary association rule mining al- gorithm.

Step 7. Reutilization of constructed prefix-tree and merge redundant database partitions: There are three types of methods for this process.

(a) Reutilization of partial prefix-pattern tree  (AMDPM-R): This method reuse the con- structed prefix-tree. In subsequent processes, this prefix-tree is reused.

(b) Merger of database partitions (AMDPM-M): The prefix-tree is deleted since this method does not reutilize the constructed prefix-tree. This method deletes attribute Ai from database partitions and merges the database partitions with the same at- tribute pattern.

(c) Hybrid of the two methods above (AMDPM- H): For database partitions with a small num- ber of records, attribute Ai is deleted, and it is merged with a database partition with the same attribute pattern. For database partitions with a large number of records, the constructed prefix- tree of these database partitions is reused. In the AMDPM-H algorithm, we define a small database partition as a database partition whose local minimum support count is 1. It is expected that both the effects of partial prefix-tree reuti- lization and database partition merger can be at- tained.

The AMDPM algorithm selects suitable items for every database partition and builds the prefix tree using them. The kinds of items that constitute the prefix tree differ with each database partition. By avoiding construction of unnecessary nodes, we can expect to reduce the prefix-tree construction cost. Moreover, the information for calculating correct sup- ports of all itemsets may not be contained in the prefix-tree built in the stage of Step 1. In that case, UBS of the item- sets is calculated. Since an itemset whose upper bound of support satisfies MinSupp may become a global frequent itemset, the additional support counting process is required when there is insufficient information. However, when the  International Workshop on Component-Based Software Engineering, Software Architecture and Reuse (ICIS-COMSAR?06)    Figure 1. Prefix-tree for attribute B  Figure 2.

Conditional  database for item B:b0 Figure 3. Reconstructed prefix-tree  Figure 4. Frefix-tree for attribute A (AMDPM-R)  Figure 5. Reutilizing prefix-tree (AMDPM-H)  upper bound does not satisfy the minimum support, the sup- port counting of the itemset is unnecessary. In this case, ex- cessive count processing is avoidable, and we can expect to reduce the support counting cost.

Example: We show an example of the association rule mining process using the database in Table 1. Here, SemiMinSupp is set to 0.20. In this example, we show the process for mining associ- ation rules containing items of attribute B. First, the prefix-tree is constructed by reading records from database partitions contain- ing attribute B, that is, DP(A,B,C,D) and DP(B,C,D). The prefix- tree is constructed without item A:a2, B:b2, C:c1, D:d0, D:d2 for DP(A,B,C,D), B:b2, C:c0, D:d2 for DP(B,C,D). Item B:b2 is an exclude-item, and the other items do not satisfy SemiMinSupp on each database partition. Additionally, all itemsets which are combination of the items containing an item of attribute B are counted. After all database partitions containing attribute B are processed, the prefix-tree in Figure 1 is constructed. Next, the sup- port counting is performed by constructing conditional database for each item of attribute B. For mining association rules con- taining item B:b0, the prefix-tree is traversed. By traversing the subtree rooted at B:b0, GS or UGS of all items in it are calcu- lated. For item D:d1, the local support count in DP(A,B,C,D) and DP(B,C,D) are 2 and 0. In this case, GS is calculated as 0.13 (= 2+06+9 ) and itemset {B:b0,D:d1} is pruned. For item C:c1, the local support count in DP(B,C,D) is 5, but that in DP(A,B,C,D) is unknown. In this case, UGS is calculated as 0.40 (= 1+56+9 ).

This calculated value satisfies MinSupp, itemset B:b0,C:c1 may become global frequent itemset. {B:b0,C:c1} at database partition DP(A,B,C,D) is inserted to additional counting itemset list. Then, conditional database is constructed. By picking up items which  satisfy SemiMinSupp at at least one database partition from the subtree rooted at B:b0, new prefix-tree is constructed. In this case a prefix-tree shown in Figure 2 is constructed. Then, by traversing subtree rooted at itemset B:b0,C:c1, the support count of itemset {B:b0,C:c1,D:d0} is counted. By this way, all itemsets containing item of attribute B are checked. Next, additional counting process is performed. In order to count the local support count of itemset {B:b0,C:c1,D:d0} in DP(A,B,C,D), node of item C:c1 and D:d0 is inserted and/or updated, and they are linked. By traversing the linked nodes and its antecedent nodes, all itemsets whose GS is recalculated are checked. Since all global frequent itemsets con- taining an item of attribute B have mined, all nodes of attribute B are deleted from the prefix-tree. From the prefix-tree, the nodes of item B:b0 and B:b1 are deleted, the subtree of child nodes of them are merged, and it is decomposed into subtrees for database parti- tion DP(A,B,C,D) and DP(B,C,D), the subtrees in Figure 3 is con- structed. Then, the support counting for sub-itemsets is performed, and association rules are derived. From the global frequent item- set B:b1,C:c2,D:d1, association rule {B:b1,C:c2}?{D:d1} (Conf 1.00, Supp 0.27, Repr 0.75) is derived. At this point, the min- ing association rule containing attribute B was completed. The reutilization of the prefix-tree and/or merger of database partitions are performed. In AMDPM-R, the constructed prefix-tree is reuti- lized. For example, the association rule mining process of attribute A construct the prefix-tree from the subtree of DP(A,B,C,D) and records in DP(A,C,D) and DP(A,D) (Figure 4). On the contrary, merger of database partitions is performed in the AMDPM-M al- gorithm. By deleting attribute B from all database partitions and merging redundant database partitions into one database parti- tions, three database partitions DP(A,C,D)={1,7,8,9,13,14,17,20}, DP(A,D)={6,11}, DP(C,D)={2,3,4,5,10,12,15,16,18,19} are gen-  International Workshop on Component-Based Software Engineering, Software Architecture and Reuse (ICIS-COMSAR?06)    Connect-4 T10I4D100K         10  20  30  40  50  60  70  E xe  cu tio  n tim  e (s  ec )  % of missing values (%)  AMDPM-R (Unif) AMDPM-M (Unif) AMDPM-H (Unif) AMDPM-R (Exp) AMDPM-M (Exp) AMDPM-H (Exp)        10  20  30  40  50  60  70  E xe  cu tio  n tim  e (s  ec )  % of missing values (%)  AMDPM-R (Unif) AMDPM-M (Unif) AMDPM-H (Unif) AMDPM-R (Exp) AMDPM-M (Exp) AMDPM-H (Exp)  (a) With varying the ratio of missing values  Connect-4 T10I4D100K         30  35  40  45  50  55  60  E xe  cu tio  n tim  e (s  ec )  Minimum support value (%)  AMDPM-R (Unif) AMDPM-M (Unif) AMDPM-H (Unif) AMDPM-R (Exp) AMDPM-M (Exp) AMDPM-H (Exp)        0.02  0.03  0.04  0.05  0.06  0.07  0.08  0.09  0.1  E xe  cu tio  n tim  e (s  ec )  Minimum support value (%)  AMDPM-R (Unif) AMDPM-M (Unif) AMDPM-H (Unif) AMDPM-R (Exp) AMDPM-M (Exp) AMDPM-H (Exp)  (b) With varying MinSupp   0.2  0.4  0.6  0.8   1.2  20  25  30  35  40  45  50  E xe  cu tio  n tim  e (r  el at  iv e  ra tio  )  Semi-minimum support value (%)  Connect-4 (Unif) Connect-4 (Exp)   0.2  0.4  0.6  0.8   1.2  20  25  30  35  40  45  50  E xe  cu tio  n tim  e (r  el at  iv e  ra tio  )  Semi-minimum support value (%)  T10I4D100K (Unif) T10I4D100K (Exp)  (c) AMDPM-H with varying SemiMinSupp  Connect-4 T10I4D100K         0  2000  4000  6000  8000  10000  E xe  cu tio  n tim  e (s  ec )  # of database partitions  Rule gen (Unif) Rule gen (Exp)  DB partition (Unif) DB partition (Exp)         0  10000  20000  30000  40000  50000  E xe  cu tio  n tim  e (s  ec )  # of database partitions  Rule gen (Unif) Rule gen (Exp)  DB partition (Unif) DB partition (Exp)  (d) AMDPM-H with varying # of database partitions Figure 6. Execution time  erated. Since the AMDPM-M algorithm does not reutilize prefix- tree, all constructed prefix-tree are deleted. In the AMDPM-H al- gorithm, both reconstruction of prefix-tree and merger of database partitions are performed. In this example, the AMDPM-R algo- rithm is employed for DP(B,C,D) (Figure 5), and the AMDPM- M algorithm is employed for DP(A,B,C,D), four database partitions DP(A,C,D)={1,7,8,9,13,14,17,20}, DP(A,D)={6,11}, DP(B,C,D)={2,3,4,5,10,12,16,18,19}, DP(C,D)={15} are gener- ated.

4. Performance evaluations  We implemented our algorithm on a Linux machine (2.4GHz Xeon processor, 4GB RAM, and 36GB local disk drive), and conducted performance evaluations. Table 2 contains sev- eral datasets used in this performance study. Connect-4 is a dense dataset obtained from the UCI Machine Learning Repository (http://www.ics.uci.edu/?mlearn/MLRepository.html).

T10I4D100K is a synthetic dataset generated by the procedure in [1], and is very sparse. Table 2 lists some statistical information about the datasets. The last column is the average number of items in the records. We append missing values to these datasets as fol- lows: 1) A set of missing attribute patterns is generated by select- ing attributes at random. The number of missing attribute patterns is equal to the number of database partitions. 2) For each record, a missing attribute pattern is selected and is applied to it. The at- tribute values included in the selected missing attribute pattern are changed into missing values. By changing the ratio of missing val- ues for each attribute, the number of missing attribute patterns, and the distribution of selecting missing attribute patterns, we gener- ate several types of datasets. Here, we denote the dataset where the missing attribute patterns are applied by selected uniform distribu- tion as ?(Unif)? and that of exponential distribution as ?(Exp)?. In all experiments, we set MinConf at 70%, and MinRepr at half of the ratio of the records not containing missing values.

Figure 6 shows the execution time. The ratio of missing values is set to 25% in (b), (c), (d), MinSupp is set to 50% for Connect-4 and 0.05% for T10I4D100K in (a), (c), (d), SemiMinSupp is set to 50% on Connect 4 and 0.05% for T10I4D100K in (a), (b), (d), and the number of database partitions is set to 5000 for Connect- 4 and 10000 for T10I4D100K in (a), (b), (c). The performance  Table 2. Data sets Data sets # of records # of attributes # of items AveRItems  Connect-4 67,557 43 129 43 T10I4D100k 100,000 1,000 1,000 10  of the AMDPM algorithm decreases as the ratio of missing val- ues increases. When the database contains several missing values, the support of each itemset increases, and the number of process- ing itemsets increases. Furthermore, the performance of AMDPM is affected by SemiMinSupp setting. When SemiMinSupp is low, the number of nodes in the prefix-tree and the cost of traversing in- crease, but the number of additional counting itemsets decreases.

When SemiMinSupp is high, the cost of traversing decreases, but the number of additional counting itemsets increases. Since the effect of the SemiMinSupp setting is data dominant, it is difficult to define an optimal value. Empirically, we can attain good per- formance when setting SemiMinSupp at about 35 to  5 of MinSupp.

In the AMDPM algorithm, the AMDPM-H algorithm attains the best performance. Since AMDPM-H uses the method of reutiliz- ing a partial prefix-tree and merging database partitions, it benefits from the effects of both methods. AMDPM reduces the process- ing cost by reutilizing the constructed prefix-tree and/or merging redundant database partitions. The effects depend on the number of records in the database partition. It is presumed that the prefix- tree construction cost of a database partition with many records is large, and that of a database partition with only a few records is small. The kinds of items influence the prefix-tree construction cost. When there are few records in a database partition, the num- ber of items that satisfy SemiMinSupp increases. For example, assume that the database partition contains five records with Min- Supp 0.1. All the items appearing in it have to be used to construct the prefix-tree. In this case, the number of nodes of the prefix- tree increases, and the support count cost also increases. To im- prove performance, it is necessary not only to avoid constructing a prefix-tree for a database partition with many records but also to reduce the kinds of items in the prefix-tree. Additionally, the num- ber of database partitions affects the performance of AMDPM. The number of treating database partitions varies with the processing order of each attribute. The execution time of AMDPM-H with the processing order of each attribute in ascending order of the num- ber of database partitions they each are contained in is 39.4 sec on T10I4D100K (Unif) with MinSupp 0.05%, SemiMinSupp 0.04%.

International Workshop on Component-Based Software Engineering, Software Architecture and Reuse (ICIS-COMSAR?06)    Table 3. Performance comparison (Execution time (sec) / Ratio of mined rules (%))  % of Missing values 5% 25% 50% 5% 25% 50% MinRepr 47.5% 37.5% 25% 47.5% 37.5% 25%  Connect-4 (Unif) with MinSupp 50% Connect-4 (Exp) with MinSupp 50% AMDPM-H 45.8 / 100 43.0 / 100 38.6 / 100 42.1 / 100 38.2 / 100 35.2 / 100  RAR 72.5 / 99.1 68.4 / 96.6 59.1 / 91.2 72.1 / 98.2 67.9 / 94.6 57.5 / 88.1 RAR+ 152.9 / 100 176.3 / 100 238.6 / 100 152.1 / 100 176.7 / 100 237.1 / 100  Extd. AFOPT 20.8 / 99.1 20.1 / 96.6 18.4 / 91.2 21.2 / 98.2 19.9 / 94.6 18.5 / 88.1 Extd. AFOPT+ 33.3 / 100 37.0 / 100 47.2 / 100 35.1 / 100 38.9 / 100 49.0 / 100  T10I4D100K (Unif) MinSupp 0.025% T10I4D100K (Exp) MinSupp 0.025% AMDPM-H 43.0 / 100 39.5 / 100 34.1 / 100 34.9 / 100 33.6 / 100 29.4 / 100  RAR 58.8 / 98.6 58.4 / 95.0 54.0 / 86.7 58.8 / 98.2 58.5 / 93.1 54.1 / 85.8 RAR+ 109.1 / 100 113.7 / 100 116.2 / 100 109.1 / 100 113.8 / 100 117.8 / 100  Extd. AFOPT 17.1 / 98.6 16.8 / 95.0 16.3 / 86.7 17.3 / 98.2 16.9 / 93.1 16.3 / 85.8 Extd. AFOPT+ 27.6 / 100 33.7 / 100 40.5 / 100 27.6 / 100 33.8 / 100 40.3 / 100  And, the execution time in descending order and in random order are 46.2 sec and 42.1 sec, respectively. AMDPM-H holds down the number of treating database partitions by the processing order for each attribute and by merging redundant database partitions.

Table 3 lists the execution time and the ratio of mined rules with varying ratios of missing values. The number of database par- titions is set to 5000 and 10000 for Connect-4 and T10I4D100K, respectively. With the AMDPM-H algorithm, SemiMinSupp is set to 50% and 0.05% for Connect-4 and T10I4D100K, respec- tively. These results show that the AMDPM-H algorithm can always find all rules. The ratio of mined rules decreases as the ratio of missing values increases, and it depends on the ra- tio of missing values for each attribute. This tendency is high with sparse datasets. Here, the results of RAR and AFOPT are shown for comparison. As described in section 2.2, AFOPT is ex- tended to hold a disabled record ID list. Extended AFOPT where (|D| ? MinRepe ? MinS upp) is used for the threshold of itemset pruning are denoted by AFOPT+. Also, in conventional pattern growth approach, it is possible to mine all association rules by set- ting the threshold of itemset pruning to (|D|?MinRepe?MinS upp.

However, the number of kinds of processing items increases, and the execution time is longer. This tendency is high when the ra- tio of missing values is high and MinRepr is low. The execution time of conventional algorithms increases exponentially as Min- Supp decreases. When the ratio of missing values is low, we can set a high MinRepr. In this case, there is only a slight increase in the incremental processing cost when taking missing values into account. However, few association rules are mined if MinRepr is set to high at a high ratio of missing values. In order to find interesting association rules, we have to set to low the MinRepr for dataset with a high ratio of missing values. Consequently, the performance degrades rapidly. When the ratio of missing values is very low and MinRepr is high, the execution time of AMDPM- H becomes longer than that of conventional pattern growth ap- proaches. There is little point in considering missing values when the ratio of missing values is very low. However, AMDPM-H out- performs other algorithms when the ratio of missing values is high and MinRepr is low, and it is beneficial to consider missing values.

5. Conclusion Missing values are a natural phenomenon in real datasets. If  we mine association rules in complete disregard of missing values, mistaken rules are derived. Thus, the problem of treating missing values is important. In this paper, we proposed the AMDPM algo-  rithm for mining association rules from data with missing values, and examined its effectiveness through performance evaluations.

The AMDPM algorithm does not perform data imputations, and it utilizes all records not containing missing values for each asso- ciation rule. Our approach is to divide the database into database partitions, to count the support of itemsets for each database par- tition, and to mine association rules by combining some of the database partitions. Unnecessary record counting is avoided by estimating global support from the local support counts of local frequent itemsets, and the processing cost is reduced by reutilizing a constructed partial prefix-tree structure and merging redundant database partitions. Experimental results showed the AMDPM al- gorithm can always find all association rules satisfying minimum thresholds, and it can attain good performance by reutilizing trees and merging redundant database partitions. In our future work, we plan to extend our algorithm to distributed environments.

