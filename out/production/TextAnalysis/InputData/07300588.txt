Frequent Itemsets Mining on Weighted Uncertain Data

Abstract- Mining frequent itemsets from datasets is a well studied problem. Several variations of this problem have also been  investigated in the literature. Two such variations deal with  datasets with weights and datasets with uncertainty. There are  many applications where the data are both weighted and uncertain.

Mining from such datasets has not been studied before. In this  paper we initiate the study of frequent itemsets mining from  weighted uncertain data. In particular, we propose two algorithms  called HWUAPRlORl and VWUFIM for mining frequent itemsets  from weighted uncertain data. We evaluate the performance of the  proposed algorithms on various datasets.

Keywords- Data mining, Algorithms, Frequent itemsets, Apriori algorithm, weighted frequent pattern mining algorithms,  Uncertain databases.



I. INTRODUCTION  The problem of association rules mining can be stated as  follows. Let I = {iv i2, ? ? ?  , im} be a set of possible items. The input is a dataset D = {tl' t2, ... , tn} where ti is a transaction consisting of a subset of 1. Any subset of I is also called an itemset. A k-itemset is an itemset with k elements. An  association rule is nothing but an implication of the from X => Y, where X c I, Y c I, and X n Y = 0. From among all such possible implications, only some of them could be of  interest to us. Rules of importance are normally characterized  with two parameters: minsup which is defined as the fraction of transactions that contain both X and Yand minconf which is defined as the number of transactions in which both X and Yoccur divided by the number of transactions in which X occurs. An item set is said to be frequent if its support is at least minsup.

Generally speaking, association rules mining algorithms  run in two phases. All the frequent itemsets are identified in the first phase, while the second phase generates association rules using the frequent item sets in the first phase. Since the  second phase is straightforward compared to the first phase,  most work has focused on reducing the number of database  scans in order to identify the frequent item sets.

One of the first papers to study the association rules  mining problem was by Agarwal et al. [1]. Since then many algorithms have been proposed for generating rules based on two user-specified thresholds, namely, minimum support and  minimum confidence. While the minimum support threshold  is applied to find all the frequent item sets in a database, the minimum confidence is used for generating rules. Agarwal et al. [2] proposed the Apriori algorithm for rules mining.

Apriori uses a breadth-first search to count the supports for itemsets. It minimizes the number of passes through the  database by adopting the downward closure property that  states that any subset of a frequent item set should also be frequent. A major drawback of Apriori is the complexity of  generating candidate item sets that could involve multiple passes through the database. A series of algorithms followed  Apriori that seek to improve the performance. For example, the algorithm of Han et al. [3], called FP-growth, adopts a divide-and-conquer concept. FP-growth is faster than the  Apriori algorithm since it finds the frequent patterns without generating candidate itemsets. This algorithm runs in two  steps: first it builds a compact data structure called FP-tree in only two passes through the database. Then, it uses the  information in the FP-tree to extract frequent itemsets by traversing it.

Let I = {il' i2, ... , im} be the set of all possible items in a dataset. There are many domains such as social networks,  sensor networks [8], protein-protein interaction analysis [9]  and inaccurate surveys where the data are uncertain. For any  transaction in an uncertain dataset, there is a probability  vector P = {Pl' P2' ... , Pm}, where Pi is the probability that item i is in the transaction [8]. In uncertain data, the frequency of an item (or an itemset) is a function of the expected number of occurrences of the item (item set) in the  transactions [3, 5, 6]. Recently, a plethora of papers have been  published on mining from uncertain data [4, 5, 6, 7].

Traditional approaches to mining frequent patterns have  suffered from the fact that all the items are treated as equally important. In real applications, each item has a different level  of importance. For example, in retail applications some  products may be much more expensive than the others, and these expensive items may not be present in a large number of transactions. The importance of weight-based pattern mining  approach can be felt in many domains such as, biomedical data analysis where the causes of most diseases are not only one gene but a combination of genes; web traversal pattern  mining where the impact of each web page is different; and so on. Thus, many algorithms [13-15] have been proposed for weighting items based on their significance. Unfortunately,  the downward closure property does not hold for weighted data making it a challenge to develop mining algorithms [1].

To the best of our knowledge, the problem of mining from  uncertain weighted data has not be studied before in the  literature. This paper introduces this important problem and presents novel algorithms.

UApriori is an algorithm for mining from uncertain data that extends the Apriori algorithm. It fmds frequent itemsets     based on expected support [7] in uncertain databases. UFP? growth [8] is similar to the certain FP-growth algorithm. It fIrst builds a compact data structure called the UFP-tree using only two passes over the uncertain database. Then, it  constructs conditional sub-trees and fInds expected support? based frequent item sets from the UFP-tree. Unlike FP?  growth, where the compact FP-tree shrinks different transactions that share common subsets/prefIxes in only one  path, UFP-tree is substantially reduced. Thus, items may share only one node when both their ids and probabilities are  common. Otherwise, even common subsets/prefIxes that  don't share the same probabilities will be represented in two  different nodes/paths. Due to fewer shared nodes and paths, an uncertain database can be viewed as a sparse database. A  lot of redundant computations take place while processing a UFP-tree since the number of conditional sub-trees could be very large. In fact, the performance of the deterministic FP? growth cannot be achieved using UFP-growth [8].

Leung et al. [10] have suggested a straightforward solution by considering each distinct probability as a different item. This solution can be effective only when many items have the same probability which is not the typical case in the  domain of uncertain databases [5]. Agarwal, et al. [5] suggested another solution by creating cluster ranges of  probabilities and for each clustered range the relevant nodes are linked. Followed by this, FP-Tree algorithm is used to generate a close superset of the frequent item sets. They also  suggested another solution by using an extend H-Mine,  namely, U H-Mine.

1.1. Uncertain apriori algorithm (UAPRIORI)  Chui, et al. [11] have introduced the UApriori algorithm that is based on the computation of expected supports. Like  the traditional Apriori, it adapts a )support-based frequent itemsets recursively. In uncertain databases, downward closure property [2] also is satisfIed. So, we still can prune all the supersets of expected support-based infrequent itemsets.

Chui, et al. [11] have also proposed decremental pruning  methods to improve the effIciency of UApriori. The decremental pruning methods are employed to estimate an upper bound on the expected support of an itemset from the beginning, and the traditional Apriori pruning is used when the upper bound is lower than the minimum expected support.

Yet, the traditional Apriori pruning is mainly used in  UApriori since the decremental pruning methods depend on the structure of the datasets.

Unlike the traditional Apriori algorithm in deterministic databases, the performance of UApriori is better than the other mining algorithms in the domain of uncertain data. It is considered as one of the fastest for dense uncertain datasets in general [8].

1.2. Strategies for generating frequent ltemsets:  Many frequent itemset mining algorithms employ the following strategy: I-frequent itemsets are generated fust; followed by this candidate 2-frequent item sets are generated;  for each candidate 2-frequent itemset, its support is calculated to check if it is indeed frequent; after this we have determined  all the 2-frequent itemsets; followed by this we generate candidate 3-frequent itemsets; and so on. Several techniques have been introduced to reduce the computational complexity  for generating the candidate itemsets. An example is the p-I x p-I method. EffIcient data structures have been introduced to reduce the number of comparisons. Examples include hash  tables, FP-trees, etc. [18].

In the p-I x p-I method, a pair of frequent (k-l )-itemsets are merged to get a candidate frequent k-itemset if and only if  their fust k-2 items are equal.

Let A = {al,a2 a3, ... ... . ak-d, and B = {bl,b2 b3, ... ... . bk-d be two frequent itemsets. Then A and B are merged to get a candidate frequent k-itemset if and only if: ai.= bi for all i = 1, ... ... k - 1.

1.3. Weighted frequent pattern mining algorithms  Yun, et al. [16] have proposed an algorithm called WFIM for frequent itemset mining from weighted data. In order to  ensure the downward closure property, a minimum weight  and a weight range are used, where each item has a random  fIxed weight W in a weight range: W min S; W S; Wmax.

An itemset X is a weighted frequent itemset if, and only if,  at least one of the following conditions is not satisfIed:  1. Pruning condition 1: sup (X) < minsup && W(X) < Wmin  2. Pruning condition 2: Wsup(X) < minsup, where Wsup(X) = sup(X) X Wmin -  Here sup(X) stands for the support of X and W(X) is the average weight of the items in X Yun [17] has proposed an algorithm called WIP. He has also defIned the concept of a weighted hyperclique. This is a new measure of weight? confIdence that measures the weight affInity of a pattern and prevents the generation of patterns that have signifIcantly  different weight levels. Weight confIdence of a pattern  X = {Il' 12, 13, ... , 1m} is defIned as the ratio of the minimum weight of items to the maximum weight of items.

Wconf(X) = Minl<i<m {weight({Ii))). Max l,;j,;m (welght((Ij}))  If the weight confIdence of a pattern is greater than or  equal to a minimum weight confIdence, then the pattern is  called a weighted hyperclique pattern.

An itemset X is a weighted frequent itemset if, and only  if, at least one of the following three conditions is not  satisfIed:     l.

2.

3.

Pruning condition 1:  sup(X) X Wmin < minsup Pruning condition 2:  Wconf(X) < min _wconf.

Pruning condition 3:  hJonfidence < min_hconf  where, the h - confidence of a pattern }. d f' d support ((Iv/zh.???.Im)) X = {Iv 1z, 13, ... , 1m IS e me as: . ( ((I'))) . Max '''J"m support J

II. DEFINITIONS  In this section, we present some basic definitions relevant for mining frequent itemsets from an uncertain database as  well as weighted frequent itemsets.

Let 1 = {i" iz, ... , im} be the set of possible items, and let  UDB stand for the uncertain database that includes N transactions, T = {t" tz, ... , tN}' A transaction t, is described by a probability vector {Pi (11)' pJ1z), ... ,pJ1m)}, where Pi (1) represents the probability that the transaction tj contains item 1, for any item 1 and 1 ? i ? N.

? The expected support of an itemset X can be defmed as: esupp(X) = Lf= l nXEX Pi(X),  ? An item set X is said to be frequent if and only if its expected support: esup(X) ? (N * minsup) where minsup is user specified.

? Let w(I) be a positive real number that stands for the weight of an item. It represents the importance of the  item in the transaction database. Then, the weight of an  itemset X is the average weight of items, and is defend Itength (X) Wx  as: W(X) = 1 . length (X) ? Weighted support of an itemset is defined as:  Wsupport(X) = W(X) x Support (X).

? An itemset X is said to be a weighted frequent pattern if  Wsupport(X) ? minsup.



III. PROPOSED METHODS:  In this section we present elegant algorithms for mining  weighted frequent item sets from uncertain databases.

In the WFIM algorithm [16], for the case of certain data, an itemset is defined to be frequent if, at least one of the following pruning conditions is not satisfied:  l. Pruning condition 1: sup (X) < minsup && W(X) < Wmin  2. Pruning condition 2: Wsupport(X) < minsup, where [Wsupport(X) = sup(X) X Wminl  Therefore, in uncertain data, we can say, an itemset is frequent if, at least one of the following pruning conditions is  not satisfied:   l. Pruning (esup(X) <  condition  min1esup && W(X) min_weight)  1 :  <  2. Pruning condition 2: (Wesup(p) [= esup(X) x W(X)] < min2esup)?  We introduce two support parameters min1esup and min2esuP' where min1esup stands for the first pruning condition, and min2esupstands for the second pruning condition. Also, min2esup should be less than min1esup.

Our algorithms for weighted uncertain frequent item set mining are designed using two layouts namely horizontal and  vertical. Horizontal Layout of any dataset keeps the data as transactions where each transaction is an item set. On the other  hand, the vertical layout keeps the data as a list of transactions for each item. The list associated with any item is the list of  transactions in which the item is present. Below we present the horizontal weighted uncertain Apriori algorithm.

3.1. Horizontal Weighted Uncertain apriori (HWUAPRIORl)  Input: A transaction set T with 111=N, A set 1 of items in the transactions, minsup1, minsup2, Weights of the items WI' wz, ... , wm' Minimum weight threshold: min_weight.

Output: All the Uncertain Weighted Frequent Itemsets.

Algorithm: Step 1: Pre-Processing min1 eSlIp= minsup1 x N  . _ miniesup mm2eslIp- N Step 2: /* Find the uncertain weighted frequent items with ( essup(x) ? min1 eSlIp II weight(x) ? min_weight) && ( essup(x) * weight(x) ? min2eslIp) */  1. Scan through the database to find weighted frequent items x that do not satisfY the pruning conditions.

Condition 1: ( essup(x) < min1 eSlIp && weight(x) < min_weight) Condition 2: ( essup(x) * weight(x)< min2eslIp) 2. Update the database by deleting all infrequent items  that satisfY at least one of the pruning conditions.

3. Scan through the database to find uncertain weighted  candidate 2-itemsets x that do not satisfY the pruning conditions.

Condition 1: ( essup(x) < min1 eSlIp && weight(x)< min_weight) Condition 2: ( essup(x) * weight(x)< min2eslIp) 4. Delete all candidate 2-itemsets that satisfY at least one  of the pruning conditions.

5. For all candidate itemsets of size greater than two do: 5.1. Apply F"-I x F"-I method to generate the candidate  itemsets.

5.2. Next, the database is scanned again for matching  each transaction against candidates contained in the hashed buckets.

5.3. The support of candidates are counted, and the two pruning conditions are checked.

5.4. A candidate is a weighted frequent itemset if does not satisfY at least one of the two conditions.

5.5. Prune the weighted infrequent k-itemsets.

6. Generate all the Weighted Frequent ltemsets.

The horizontal UW Apriori algorithm has some drawbacks. First, it merges two frequent (k-l )-itemsets with  (k-2) items common between them to come up with a  candidate k-itemset. This requires O(k) time given that the items are all ordered. Also, the algorithm has to traverse  through the entire database of transactions to figure out if a k? itemset is frequent. Even for very dense databases a typical itemset is present in only a fraction of all the transactions.

Checking the entire database is an overkill. These issues are addressed in the Vertical Weighted Uncertain Frequent  Itemset Mining (VWUFIM) algorithm. Below we provide an overview of the VWUFIM algorithm.

3.2. Vertical WUFIM Algorithm  This algorithm works on a different layout than its horizontal counterpart. We first order the items and transactions. The database is changed in the following form  : Ij ? T(Ij), where Ii is the jth item and T(lj) is a list of transactions that contain ?. We can convert the horizontal layout to vertical layout in just a single scan. A vertical layout removes the shortcomings of the horizontal UW Apriori algorithm in the following way. First, we generate a k? frequent item set by merging a (k-l)-frequent item set with a 1-  frequent item set in ascending order of item IDs. Note that,  this can be in O(k) time. Second, to compute the support for a k-itemset, we perform intersection between transaction lists.

Due to ordering of transactions this intersection has a time  complexity of linear in the total length of the two lists. Below we provide the pseudo code for VUWFIM algorithm.

Input: A database of all transactions.

Total number of transactions, minsupl, minsup2, Weights of the items Minimum weight threshold: min_weight.

Output: All the Uncertain Weighted Frequent ltemsets.

Algorithm:   1. If the dataset is in horizontal format, convert it to vertical format. A vertical format is given by a set of items each associated with a set of transactions the item appears in.

2. Find out the set of I-frequent itemsets in a single pass through the data. We use the same conditions as mentioned in algorithm HWUAPRIORI.

3. Generate k-frequent itemsets.

3.1. Generate candidate k-itemsets using the Fk-1 X  Flmethod.

3.2. If A is a list of transactions that a frequent (k-l)?  itemset x occurs in and B is a list of transactions in which a frequent item y occurs in, then the support for the candidate k-itemset z obtained from x and y can be computed by intersecting A and B. Apply the pruning conditions to check if z is frequent.

4. Repeat step3 until all the frequent itemsets are found.

3.3. Complexity Analysis:  Let n be the number of transactions q = average number of items in a transaction m = total number of items p = average fraction of transactions in which any item occurs. We refer to this as the item density. Density is a fraction f [0,1].

l = a generic k-itemset T(l) = a list of transactions that l appears in.

fk= a generic frequent k-itemset.

P< = set of all k-frequent itemsets  Time Complexity Analysis for HWUAPRIORI:  HWUAPRIORI requires O(lpk-I)I*(k)*IPI)1) total time to generate set of all the candidate k-itemsets l from pk-I) and pl). To check whether any l is frequent or not we have to scan through the dataset each time. Hence the total average  time is given by  O[m+ IJ?/l-l)IF(j)IIF(l)ljqn], where k '-l is the number  of items in the largest frequent item set.

Time Complexity Analysis for VWUFIM:  In VWUFIM algorithm step 1 takes O(q * n) time. Each item is allocated a bucket indexed with the same id as the  item. In single scan through the database we can figure out T(f) for all l-itemsets. Step 2 takes a single scan through the set of all l-itemsets. Time complexity for this step is Oem).

Step 3 is most crucial in the algorithm because this step is  iterated. Step 3a takes O(k) time for a (k-l)-frequent itemset.

Total time for all the k-l item sets in the set is given by O(lpk- 1)1 *(k-l)*lpl) I). Step 3b requires an expected time of O(pqn).

Hence the total runtime for algorithm VW FI M is given  by O[m + IJ?/l- l)IF(j)IIF(l)ljpqn].

Note that pq :s q as p ( [0,1]. Hence VWUFIM is faster than HWUAPRIORI in practice. Although the ratio of runtime is  expected to vary based on other parameters provided by the users. In Section IV we show tables that display experimental results supporting our theoretical analysis.



IV. EXPERIMENTAL RESULTS  In this section, we present some experimental results on  HWUAPRIORI and VWUFIM. HWUAPRIORI and VWUFIM are implemented and compiled using Microsoft's Visual Studio c++ 2013 and Java respectively. Following is the execution environment:  ? Intel(R) Core(TM) i7 3 AOGHz PC ? 8GB Main memory ? Operating System is Microsoft Windows 7.

For testing the performance of HWUAPRIORI and VW U F 1M over weighted uncertain databases, we have used:  A sparse dataset consisting of 9982 transactions with 336  different items, and the size of largest itemset is 267 items.

Dense datasets have 10,000 transactions with 992 different  items, and the size of the largest itemset is 267 items.

For each dataset, we have run our algorithms with  different values of minimum weights, and different  configuration parameters: minsupl, and minsup2.

Figure 1 shows the performance of our proposed  algorithms and figure 2 shows a comparison between the algorithms VWUFIM and HWUAPRIORI in terms of run  times. Note that the runtimes can vary based on other  parameters provided by the users.



V. CONCLUSIONS  Many real life problems can be addressed using uncertain association rules mining. Many of these problems come with  items having different weight factors. In this paper we have introduced and studied the prob lem of mining from uncertain  weighted data. To the best of our knowledge we are the first  ones to introduce this problem. We have also proposed two algorithms to tackle this problem. Our algorithm HWUAPRIORI works on weighted uncertain horizontal databases while VWUFIM works on vertical layout for the  same problem. We have presented theoretical and  experimental results for the proposed methods.

