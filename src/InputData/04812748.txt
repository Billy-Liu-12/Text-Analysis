The ARNO Project: Challenges and Experiences in a Large-scale  Industrial Software Migration Project

Abstract: The following paper presents the experience gained from a large scale software migration project.

It describes how the migration strategy was selected and how the project was planned. It emphasizes the importance of keeping the costs as low as possible. It then goes on to outline the methods and techniques used in the migration with special emphasis on the migration process. After that, it deals with the tools used in converting the code, migrating the data and re- implementing the job control. As with all migration projects, testing played a big role and consumed a significant part of the resources. Thus, the testing techniques and tools are given particular emphasis.

The paper ends with a summary of the lessons learned from this project and how they can be applied to other similar projects.

Keywords: ARNO Project, Software Migration, START System, Replacement of mainframe systems, Software reengineering, BS2000, UNIX.

1. Amadeus and the Amadeus Germany (START) System  Amadeus Germany GmbH is Germany?s leading provider of IT solutions for the travel industry. With its Corporate Solutions division, Amadeus Germany also offers full-service business travel solutions to faci- litate efficient corporate travel management. An exten- sive selection of training courses rounds out the portfo- lio. In Germany, 85 percent of all travel agencies run- ning approximately 45,000 PCs deploy the state-of- the-art and highly efficient Amadeus System.

In Germany, the Amadeus System can be used to make bookings with: approximately 500 airlines, more than 75,000 hotels, 22 car rental companies, nearly 200 tour and bus operators, 74 public transport associa- tions, 40 European railway companies, 30 ferry com- panies, 6 insurance companies, 3 event ticketing sys- tems representing more than 1,000 event organizers, as well as 8 cruise lines.

The sole owner of Amadeus Germany is the Ama- deus IT Group SA, a worldwide leading provider of technology and sales/distribution solutions for the tra- vel and tourism industry. More than 90,270 travel  agencies and 29,660 airline ticketing offices ? using over 600,000 PCs ? in more than 217 markets around the globe utilize the company?s network and efficient data center. The START System is the link between provider systems on one side and travel agencies and end customers on the other (see Figure 1). In addition to travel information, booking and reservation requests and confirmations, travel documents and accounting data are provided, and the payment flows between the participating business partners are organized. The Amadeus Germany System is linked to the internation- al Amadeus System described above.

This paper summarizes the process, methods, tools and techniques used in migrating large application systems.

The goal of the project presented, was to migrate all applications on a mainframe platform to UNIX, replac- ing the entire system platform and substantially reduc- ing hardware, software license and infrastructure costs.

The migration project was named ARNO as an acronym for ?Application Relocation to New Operat- ing System?. The special challenge of this project was rooted in the fact that the application systems migrated, interconnect to some 200 external partner systems.

Another challenge was to keep the current systems in operation throughout the migration. It had to be possi- ble to issue new application releases every month dur- ing the course of a project, which spanned several years. Besides converting the applications, the high- performance file handling system had to be replaced by a relational database, the middleware had to be converted and a multitude of job control scripts had to be ported. The systems in question must run virtually 24 hours a day, 7 days a week with a peak processing load of 1500 transactions per second. This same per- formance had to be duplicated in the new environment.

2. Initial Situation 2.1. Continuous Evolution of the Applications  The Amadeus START System originally developed between 1976 and 1979 and went into production in 1979 with a proprietary network system for over 800 ?dumb? terminals, 500 printers and 3 computer links to provider systems. The developers of that time used the then latest software engineering knowledge in the  2009 European Conference on Software Maintenance and Reengineering  DOI 10.1109/CSMR.2009.64   European Conference on Software Maintenance and Reengineering  DOI 10.1109/CSMR.2009.64     course of the implementation [6],[1],[5], making it possible to be evolve the system for another  30 years (see above) as well as to efficiently implement funda- mental changes in the technology of the mainframe, end device and network areas, while still retaining the basic system architecture. The applications ran on a  number of mainframe computer systems deploying the BS2000 operating system. Several UNIX and LINUX satellite systems have since been implemented systems to provide add-on services.

Figure 1: The Amadeus Germany System      Figure 2: The Main Migration Process (MMP)    Travel booking agencies  Companies  Ticket agencies  End customers  Computer cen- ters   Business travel  Travel agents  Privat customers  Corporate cus- tomers  Front-Office / Mid-Office Products  Amadeus Selling Platform  Internet products  Business  Travel Management   Amadeus Germany  Airlines  Hotels 235 chains  54.000 Hotels  Cars 43 providers 29.550 stations  Event organizers 12.000 Events  Deutsche Bahn (German nat. Rail)  Tour- and bus operators   Hotels  Isurance / Creditcards 8 / 5 Providers  Ferry operators  Railways  Public transport 61 providers  Rail 2  providers  Cruise 3 providers  Train and  public tran- sit bus operators   Amadeus     2.2. Reasons for a Migration The START system was implemented on the main  frame using the System Programming Language, a language developed in the early 1970?s to support structured programming at the machine level. With it higher level control commands were combined with lower level IO operations, making it a good language for systems programmers. As time progressed, new and more promising software development tools and languages have emerged, especially in the UNIX envi- ronment. There are also hardly any programmers left who are familiar with the SPL language and young programmers are not inclined to learn it. The fact is that the SPL language was conceived for mainframe systems programming and was never intended to be used as an application language in a distributed envi- ronment. Thus, the language was no longer suitable for modern distributed applications. The time had come to retire it. Besides that the BS2000 operating system was no longer the state of the art and the data management system was antiquated. Something had to be done to ensure the continuity of the service.

3. Procedural Model Used in the Project 3.1. Study  As a first step, the project managers conducted an expert study to determine how the systems could be replaced. Following that, they performed an analysis of the costs of the current system and of the potential new systems as well as an evaluation of the maintenance, further development and operation of the applications in the old and new environments. Three alternate mi- gration strategies were up for debate:  1. New development of the applications with the same scope of functionality  2. Automated conversion of the existing programs, scripts (jobs) and files  3. Development of a cross compiler to generate UNIX object code from SPL sources thus allowing the SPL source text to be retained.

Here are the main criteria for the decision, which was made on the basis of extensive individual evalua- tions: Alternative three would have meant that further development continues in the current language. This was not a solution to the inadequacies of the develop- ment environment, the constraints of the language and the lack of available personnel. The product stakehold- ers calculated the costs of alternative two ? the conver- sion ? to be hardly higher than the costs of developing a cross compiler. The costs of the first alternative ? new development ? were estimated to far exceed the costs that would be incurred by the other two alterna- tives. It would have been tantamount to the develop-  ment of a new system with the same scope of functions ? the disadvantage being, however, that all the bugs inherent in a newly developed system would have had to been detected and corrected. It would have taken years for the new system to attain the same state of stability as the old one. In contrast, conversion errors are local and can be easily located.

Besides, a new development with all its costs and risks is difficult to sell to users who are basically satis- fied with the functionality they have. That was the case here. There was simply no business justification of the new systems [8]. The costs times the risks would have been greater than the calculated benefits. For this rea- son the product stakeholders opted for alternative two ? automated conversion.

Java was ruled out as a target language because the gap to the source language ? SPL ? was too great. De- spite years of research no one has yet to resolve the problem of automatically converting a procedural lan- guage into an object-oriented one. On the other hand, the stakeholders wanted to retain the option of evolv- ing the applications to an object-oriented paradigm.

This left C++ as a compromise solution. The SPL code could be converted over into procedural C, which could be processed by a C++ compiler. Later the code could be objectivized by defining objects within the existing components without having to switch over to another language. PERL was selected as a script lan- guage for the job control because of its greater flexibil- ity. Oracle was selected to be the new database system because it was already a corporate standard. Several applications targeted for migration were already run- ning under the UTM transaction monitor. The same Siemens product is also available under UNIX with the same functionalities. Thus, it was obvious to continue with it rather than having to deal with yet another product. It was considered risky enough to be exchang- ing the programming language, the database system and the process control language without dealing with the transaction monitor [10].

The migration of the SPL source, the SDF script and the existing data files was planned from the begin- ning to be automated. Since there were no such con- version tools on the market, they had to be developed.

Since the development of these tools was a one-time project-specific task and tool development was not a core competency of the user organization it was de- cided early on to outsource this work. However, in view of the expertise and knowledge of the various processes, it was decided to perform the migration work itself in house. This fundamental decision turned out to be a cornerstone for the success of the project [10].

3.2. Project Organization There were many stakeholders in this project - vari-  ous business divisions and legally independent compa- nies - participating in the project. For this reason it was essential to include the top management of the respec- tive companies in the project steering committee, which functioned as a central point of coordination and decision making instance for the project manager. This was the only way to ensure the necessary prioritization over other projects and facilitate short decision-making processes. It also guaranteed the required ?manage- ment awareness?. A major risk was that projects with "visible" benefits to the customer would be squeezed in during the course of the project, thus taking re- sources away from the migration. Experience had shown that this leads to resource conflicts and exces- sive implementation times and is a major reason why long running migration projects are abandoned. It is impossible to freeze a living system during migration.

Changes and corrections must be made. The longer a migration lasts the more the migrating system will change. Thus, a goal is to push through the migration as fast as possible. Even then a well defined change management process is vital to the project success. All changes made to the old code must be replicated in the new code and this in a controlled manner.

3.3. Preparing the Migration (?first renovate, then migrate?)  Prior to converting old software it is wise to first clean it up. This has been argued by Sneed, in respect to his 30 years of reengineering and migration expe- rience [4]. It is obvious that migrating low quality and redundant software will only increase the migration costs and carry the problems over to the new environ- ment. Here, a complete inventory of all program sources, scripts (jobs) and third-party utility programs was taken in order to determine the contents of the migration package and to define the precise scope of the project. To this end, the team identified elements which were no longer required and could be removed from the system, for example ?dead code? and unrefe- renced data. A project to clean up the systems in the BS2000 environment was carried out prior to the con- version. In addition, the team consulted with the prod- uct managers to discuss which products could be taken off the market before the project began. These obsolete or redundant products were then removed from the migration portfolio. This helped to reduce the conver- sion and testing costs, and to spare resources. The mi- gration project acted here as a catalyst for streamlining the product range.

3.4. The Main Migration Process (MMP) For cost reasons, it was decided that the language  conversion tools should not convert the complete lan- guage scope of the respective systems [3]. The conver- sion therefore required an iterative approach. The basis for all conversions was the current source versions (see Figure 2). Each source was converted by the appropri- ate conversion tool, e.g., STC. If no errors occurred, it was included in the repository of generated C++ sources. If errors in the compilation occurred, an anal- ysis was made to determine whether the SPL source should be adjusted or whether the tool should be im- proved. Among other things, this depended on the number of similar ?problem cases?. Once all required objects were converted, they were compiled and linked by the C++ compiler (makepp). If errors occurred dur- ing this operation, the decision was made again whe- ther to adjust the converted source text or the conver- sion tool. If the error occurred only seldom it was cheaper to correct the source. If it occurred often it was worth it to correct the tool. As such, this was truly a cost driven project.

If an object was successfully linked, it was tested. If the test was successful, all the linked components were considered to be migrated. Otherwise, the convert, compile and link process had to be repeated. The same procedure applied to all programs, jobs, files and data interfaces. The modifications to the original objects in BS2000 were then replicated in the converted objects before they were submitted to an overall functional test. Thus, at the time of the final system switchover, the SPL and C++ sources were functionally equivalent.

It was possible to compare the outputs of the one with the outputs of the other (?verification in production?).

The status of each individual source member was rec- orded and monitored in the configuration management system. The specific conversion tool version used to convert an object was also archived for reuse, since modifications to the tool might have rendered already successful conversions to be unsuccessful if they were repeated later.

3.5. The Integrated Test Process (ITP) The test process involved not only testers from the  project team but also persons from the user depart- ments, from product management and the help desk. A test process was set up to document the test results with the aid of the configuration management system.

In the case of errors, the error messages were recorded as RFD?s (?Requests for Development?). The test coordination team maintained a constant overview of the test progress and the status of reported errors. The RFD?s were passed on to the responsible developers for correction. Objects corrected by the migration team     were resubmitted to the test process for regression test- ing.

Automa tic tests  Master RFD per  Master RFD per  Product Manage  Develop ment  Depar tment Mana  Departm ent  Manager (issue  In fo  In fo  Repres entativ  Repres entativ  Automatic / manual Test  O K  To test  To test  Test OK  N.

OKSub  - Sub  -  In fo  In fo  ARNO Test Coordination  Standar dError  Figure 3: The Integrated Test Process (ITP)  3.6. Staff At the start of the project, it was recognized that  both the development and production staff working with the mainframe environment would also have to be ?migrated?, i.e. retrained, in order to retain their appli- cation and production know-how. It was clear that these employees and their know-how were major as- sets to the company which needed to be preserved.

Process knowledge is more important to a user organi- zation than technical knowledge, e.g. the command of a particular programming language or operating sys- tem. This required the staff to be thoroughly trained in maintaining and using the target systems. Numerous informational events as well as the establishment of project WIKIs and BLOGs ensured the compilation and distribution of information. This retraining effort simplified the subsequent re-documentation of the ex- isting systems and gave rise to an important change process within the entire company.

3.7. Involving the Line Organizations Since the operation of the computer center and the  applications was distributed among different company departments, the migration project team had little in- fluence on the system administration and organization of the computer center. The responsibility for the ap- plication systems was only temporarily turned over to the migration team for the duration of the project. The project charter specified that once the migration was completed and validated via system, acceptance and performance tests, the responsibility would be trans- ferred back to the line organization. This also meant that the converted sources had to be handed over to the  responsible development departments. Involving the affected development, maintenance and production departments in the migration project helped to prepare them for their later responsibility. Their staff was given roles in the project which would enable them to carry out similar tasks later in the line organization  4. Technical Implementation 4.1. SPL to C++  As pointed out above, the applications being mi- grated were programmed in SPL, a procedural pro- gramming language to support system level program- ming under the operating system BS2000. The large volume of source code prohibited manual rewriting.

There were altogether 25.000 source members with some 2 million source statements and more than 4 mil- lion lines of code. There was no alternative to an au- tomated conversion process. The target language of choice would have been C because it is the closest to the procedural structure of the SPL and also because it is widely used in the UNIX world. However, since C does not contain equivalent counterparts of all SPL language elements, these had to be provided in the form of a 'SPLCompat' compatibility layer. This meant that not C but C++ was chosen since C++ provides the mechanisms to implement the missing elements.

Among other things, the compatibility layer con- tains the data types that are available in SPL, but for which there are no corresponding types in C, e.g. bit strings and hexadecimal strings. In C++ these types could be implemented as classes for string and bit string processing with a complete set of operators to produce exactly the same result as in SPL. The library also includes the IO operations which in SPL are at the system level. These too could be replicated in technical classes [5].

Using the SPLCompat library, the STC converter (SPL to C++) could generate a UNIX-compatible C++ code. The conversion addressed numerous non-trivial incompatibilities such as the visibility of variables. For each and every incompatibility a mapping rule was defined for mapping that particular SPL constructs to a functionally equivalent C++ construct. The use of these strict mapping rules eliminated the need to ad- dress every individual incompatibility problem. The individual incompatibilities could be resolved with a general solution. This implied that the original SPL source had to be adapted to comply with the general rules. Here again is a case where reengineering is a prerequisite to migration. The SPL code had to first be unified in order to be converted automatically. Com- pliance with the rules in the SPL code was verified before the conversion and deviations were corrected     and tested in the SPL code under BS2000, thus provid- ing the basis for an unambiguous conversion. During the conversion phase, details of these rules had to be adjusted several times. This meant that the SPL source also had to be readjusted.

Another problem arose with respect to the defini- tion of variables and constants created in C++ where the appropriate type could not be defined until their actual usage was determined. This is a problem with all typed languages. Many of these variables and con- stants were also defined in include members, the SPL equivalent of C header files, which were used in many modules. As it turned out the different SPL modules did not use these variables and constants in exactly same way. Each had its own view of them. This led to the generation of different incompatible C++ includes for one and the same SPL include.

The first solution to reducing the number of these multiple definitions was to transform them into a common all encompassing definition. Where that was not possible the developers were compelled to adapt the code of the include members to a common view before generating the corresponding C++ header files.

Here again, reengineering had to precede migration.

The few remaining problems were then manually cor- rected in the target language after the conversion.

Thus, despite all efforts, a fully automated conver- sion of the code was not possible. Even the compre- hensive set of guidelines and the reduction of the lan- guage scope did not enable the creation of unambi- guous conversion rules for all cases. The STC conver- ter generated alerts for the remaining problem cases, which were then manually reviewed and corrected as necessary. This process was managed with a well- organized configuration management system specifi- cally customized for this job. A high degree of automa- tion was achieved for the language conversion process, but it was never 100% (educated guesses said: more than 95%), even though ambitious goals like the elimi- nation of all GO TO branches and the creation of ob- jects with multiple instances were avoided. A fully automated conversion to support these features would have driven up the costs of the projects. The goal here was to make the conversion as cheap and with the least risk as possible. This is a goal of most technical migra- tions in industry.

4.2. JCL to Perl In addition to the programs, a large number of  BS2000 job control procedures had to be migrated to equivalent UNIX scripts. In view of the large number, manual conversion was out of the question. In contrast to the source code of the programs, the existing control procedures were far less homogeneous and less well-  defined. On the one hand, the Job Control Language (JCL) used under BS2000 actually comprises several different languages like ISP and SDF. Furthermore, many procedures included utilities such as ?merge? and 'sort' routines. Other procedures invoked system spe- cific utilities, such as the EDT editor in BS2000. This required special proprietary commands. To make the command code homogeneous and to create a basis for automated conversion, the first step was to preprocess all procedures with the aid of the Fujitsu Siemens SDF-CONV converter to transform them into uniform job procedures in a well-defined subset of the SDF language. In the second step, a somewhat simpler JTP (JCL to Perl) converter was able to convert the prepro- cessed procedures into Perl.

For the most common utilities, equivalent routines were written in Perl. Although there are comparable substitutes for most utilities in the Unix environment, e.g., the 'sort' utility under Unix, the functional scopes of the products are different. This prohibited the sim- ple substitution of the utility calls. They had to be re- written by hand. Conversely, formally defined conver- sion rules enabled the JTP converter to insert the new- ly written Perl utilities in place of the former BS2000 utility calls. Nevertheless, some control procedures still had to be rewritten prior to conversion.  For ex- ample, it was cheaper to rewrite those procedures con- taining the internal procedural language of the BS2000 editor (EDT) before the conversion than to rewrite the procedures containing these elements in UNIX. Ana- logous to the STC procedure, JTP generated a com- plete list of the job control procedures that could not be fully converted. This left it to the developers to ma- nually complete the conversion [3].

4.3. Filehandler to Oracle Data management under BS2000 was handled with  a proprietary, low-level program library called the ?Fi- lehandler?, containing the essential access operations of a database system ? insert, select, fetch, delete, up- date, etc. A conversion of these operations would have been extremely costly, due to their dependence on the host operating system. A new development of the li- brary routines would have been equally expensive.

Some IO routines were written in OS assembler.

After a thorough evaluation, the approach chosen was to deploy the Oracle 10 database on the Siemens host machine. Since the previous Filehandler had a clearly defined call interface, the application logic did not have to be modified. Based on the existing inter- face, a middleware access layer was inserted, whose task it was to translate the previous Filehandler calls into SQL database queries of the Oracle database. This not only required remodeling the functional scope, but     also converting, e.g., Oracle error messages to the cor- responding Filehandler error codes. Since the existing interface was well-defined, a fully automated conver- sion was possible. It was also possible to test the new access layer with the relational database on the main- frame prior to the actual conversion.

The new access layer achieves a clean decoupling of the application layer from the data access layer, thus ensuring data independence of the applications. This encapsulation makes it relatively easy to replace the data storage system at a later date, e.g. when opting for a different database supplier.

4.4. DCAM Monitor to openUTM The mainframe applications encompassed a proprie-  tary middleware layer, the DCAM monitor ? based on the basic communication methods provided by BS2000 (Data Communication Access Method). Since it had been specifically developed to the requirements of the START applications, it was extremely efficient.

To achieve its performance goals the DCAM moni- tor had been implemented using many Assembler level routines built into the SPL code. Besides having to convert the SPL code to C++, it would also have been necessary to translate all of the system calls to their exact equivalents in Unix. This, in turn, would have required an additional middleware layer for the emula- tion of all BS2000 system calls. Not only would this have given rise to a significant amount of additional work and expense, but it would also have led to a BS2000 communication emulator under Unix. This was contrary to the goals of the project which was to free the application software from all dependencies on the BS2000 operating system.

Therefore, the decision was made to deploy a stan- dard middleware, openUTM (Universal Transaction Monitor) from Fujitsu Siemens under both BS2000 and Unix. Fortunately the product was available in both environments. In the first step this transaction monitor was installed under BS2000 and the applica- tions there switched over to it. Thus, the new transac- tion monitor was already in production, even before the migration began. The applications using it could then be converted without changes to the architecture or middleware interfaces and were directly operable under Unix. This platform change was transparent to the communication partners. Consequently, the migra- tion was not burdened by a conversion of the transac- tion framework. There were no additional costs for running it in the target environment. This also made regression testing a lot easier.

5. Migration Tools and their architecture One of the key success factors of ARNO were the  dedicated migration tools, specifically developed for this project. They enabled a high degree of automation, as has been pointed out in previous papers [3].

Of particular importance in choosing the right con- version tool is, whether the programming language of the legacy system should be preserved on the target system or not. In the case of COBOL, it is mostly pre- served on the target system despite of all criticism. In this case only an adaption of the dialect is necessary.

For the conversion process a simple task pattern rec- ognition algorithm based on regular expressions is adequate. [3] describes a tool, which converts different COBOL dialects. Controlled by a graphic Eclipse in- terface the complex pattern recognition algorithms of Perl are used.

However, when converting one programming lan- guage into another, the use of regular expressions falls short.  A complex translation processes requires com- plex translation techniques. Translators are tools, which automatically convert source programs in lan- guage A into source programs in language B. For the ARNO migration project, a tool to translate SPL into C++ had to be developed. This was accomplished us- ing a transformation architecture similar to that of a compiler. This transformation model had already been under development for many years. It is displayed in picture 4 and described in [3].

The architecture of a translator is based on the clas- sic compiler model.. But there are some differences:  1. Preservation of comments: Translators don?t discard comments. They are preserved during the whole conversion process to insert them later into the target code.

2. Preservation of preprocessor information: Ac- cording to the classic compiler model preprocessor instructions are discarded after they have been re- solved. In a translator, they have to be preserved to the end of the conversion process, e.g. macros of the source programs have to appear optionally as macros in the target program and the include instructions are needed during the generation for the partitioning of target sources into several files.

3. Interface between source- and target represen- tation: In translation, there exists a strong separation between the two. The parser provides an attributed syntax tree of the source program. Out of that the con- verter generates an attributed syntax tree of the target program.

4. Postprocessor: Compared to the classic com- piler model this component is unique to a translator.

Among the main tasks of a translator are the division     of the complete attributed syntax tree of the target pro- gram into partial syntax trees, the insertion of prepro- cessor instructions of the target language and the op- tional integration of the program?s comments. These actions are based on syntax trees rather than on files.

5. Generator: The generator writes out source code by traversing separate partial syntax trees. During a conversion to C++ several header files and a main program have to be handled in one pass. Because of maintainability requirements, the formatted output should be configurable. The operation of the postpro- cessor can be described as follows: Within a classic compiler the task of a postprocessor is to execute pre- processor instructions, which results in a single source code file without preprocessor instructions. Preproces- sor instructions contain important information, e.g.

which include files the source program consists of,  which macros appear at which position within the in- clude files and which comments occur at which posi- tions. In contrast to classic compiling this information is preserved during the translation. The preprocessor and scanner insert new tokens into the token stream, which will be read by the parser. After parsing they will be assigned to the nodes of the syntax tree based on the source code position. The converter put them in the position to the corresponding nodes of the target syntax tree. The postprocessor uses this information for the subsequent tasks.

6. Division of the target syntax tree: There comes a partial syntax tree for each include file with semantically equivalent content. For that the target syntax tree must be broken up into partial trees, which are later joined together in the final source.

Figure 4: The Translator architecture     7. Placement of macros: All partial trees result- ing from macros in the source program are annotated in the target syntax tree. The postprocessor compares these partial trees, defines a macro of the target lan- guage and replaces the syntax trees by syntax trees of the corresponding macro calls. The partial syntax tree of the macro is inserted at the corresponding position within the full syntax tree.

8. Reinsertion of comments: Comments are reas- signed to the single partial tree of the target program heuristically as pre- and post-comments. Then they are placed before or after the corresponding target code fragment by the generator.

For the development of the translators in the ARNO project the following, self-developed generator tools (Meta tools) were used as shown in figure 4:  BTRACC: Backtracking Compiler Compiler, a parser generator based on the backtracking process, which can optionally generate parser in C, Java or Perl.

CTree: A tool for the declarative description of syntax tree models.

CBack: A tool for the generation of structured C/C++ code from syntax trees. The formatting of the code can be configured optionally.

Experience has shown that the development effort for translators amounts to 3 to 3.5 man-years. By using the meta-tools described here, the effort was reduced by at least 25 %.

6. Test The explicit objective of this migration project was  to execute the conversion in such a way that would not negatively affect the customers and, ideally, would go     unnoticed by them. Since no new functions were added to the system during the conversion, it had to be veri- fied that the new system would be functionally equiva- lent to the old one [7]. Several tests were required to confirm this. The batch processing tests were compara- tively simple, ?only? requiring verification that the jobs under UNIX and BS2000 generated identical out- put files from identical input files. Of course, the dif- ferent encoding on both systems (EBCDIC in BS2000, Latin-9 under UNIX) had to be taken into account when comparing the files. The online testing presented a greater challenge. Due to the large number of inter- connected partner systems, each with their own data- bases, the comparison of the online application results was much more difficult.

6.1. Test Procedures for Regular Version Re- leases  For the applications running under BS2000, approx- imately twelve versions were created and rolled out per year. Other than the routine error corrections and changes, these updates also included newly developed functions. For testing them, an established test proce- dure had been in place for many decades. In addition to verifying the implementation of the new functions, the tests focused on ensuring that the modifications did not adversely affect the other functions. To this end, simple regression tests were performed. For some areas that were rarely modified and had little interac- tion with new or modified components, the test cover- age was not in-depth enough for the migration project.

For example, negative tests were missing in many cas- es. Particularly for components that were older, only few automatable test cases were available  6.2. Test Automation To ensure adequate coverage of the migrated sys-  tems, test automation was considered essential. The objective was to promptly test each newly migrated application system without having to involve the re- sponsible development departments. To achieve this, more than 500 automated test cases were developed by the existing test teams, making it possible to test sever- al program versions a day, without the need for setting up a new test team for the duration of the project.

The newly created automated tests had another positive effect. They could be reused for the further evolution of the migrated systems. They are now fully integrated into the regular test runs and contribute to assuring the quality of each new release on the new system platform.

6.3. Load Tests To avoid unnecessary costs, the applications in  question were not subjected to regular load tests under  BS2000. Since the consecutive version releases only slightly influenced the response under load, monitoring was limited to behavior in productive operation. The migration to UNIX completely changed the perfor- mance profile of many components. For example, a new database layer was added to the architecture, which giving rise to a completely different behavior under load. Therefore, verification that the required load could be handled was an essential prerequisite for going productive under UNIX.

To assure this, a load test environment had to be set.

The fact, that individual components had to be tested in a complex application network, made the setup ex- tremely difficult. But, a test was defined that realisti- cally simulated the application environment and enabled a long-term test monitoring. This application environment included among other things, components for the simulation of external partner systems. This test required realistic test scenarios that could be repeated any number of times. With some test cases, e.g., book- ing a seat on an airplane, this was difficult to imple- ment. If a seat had been reserved, it was not possible to book it a second time since it was already occupied.

Despite the difficulties incurred, the tests could be conducted successfully, and the application was in operation for more than twelve hours under the re- quired load. During the long-term test, it was also veri- fied that no leaks existed, e.g. when using memory, semaphores or other resources. These tests not only verified compliance with the requirements, they also facilitated the early detection and correction of several bugs which had not become manifest in the develop- ment environments without parallel processing.

6.4. Replay of a Productive Online Session (Message Protocol Test)  Even sophisticated tests do not reveal all bugs and errors. Not every data constellation is considered when designing the test cases. Some bugs only become ma- nifest by a coincidence of circumstances. Therefore, some problems with complex application can only be detected after productive operation has commenced.

In order to further enhance test coverage, a further test was designed. All messages generated by the ap- plications being migrated under BS2000 during pro- ductive operation were recorded and, using an identical time sequence, subsequently sent to the new applica- tions running under UNIX. In this scenario, the linked partner systems were simulated since it was not possi- ble to otherwise integrate them into the test run. The results were then compared and the deviations eva- luated.

At first sight, this test might appear to be simple.

However, the details of it presented tremendous diffi-     culties. For example, prior to each test run, the data- base of the application had to be reset to precisely the state before the start of the recording. Even small devi- ations would result in a vast number of errors.  Fur- thermore, all components involved in the application network had to be simulated. Exact compliance with the recorded time pattern was essential here, as well.

To this end, some of the simulators from the load test were used. The system time had to be synchronized with the test data stream. Otherwise, requests for flights on a certain date would have triggered an error message instead of a positive response, since the de- sired departure date of the original message would have referred to a date in the past.

Comparing the results had similar difficulties. Even for identical test runs, the results did not exactly match, the reason being that on multiprocessor systems, there is no deterministic sequence in which sub-tasks at the micro-level are processed if routines with synchro- nized subtasks are started at the same time. Therefore, incoming messages were not always processed in the same order, which resulted in differences in message logs and databases. Consequently, 24-hour test ses- sions never yielded the exact same final status. An automated analysis to determine whether such differ- ences only arose from these effects proved difficult. In the end, some deviations remained that could only be evaluated manually.

6.5. Test Results The many different tests ? integration testing, func-  tional testing, performance testing and regression test- ing ? resulted in high quality software being rolled out on time. Each alternate test method revealed different types of errors that were not detected by the other tests, making each test method indispensable. Due to the fact that this application had never before been subjected to such in-depth testing, a surprisingly large number of bugs and errors were identified that the application system already contained under BS2000, but which had not become manifest until the migration. Of course, these bugs and errors were also migrated to UNIX along with the entire software. It is typical of migration projects that they reveal many buried errors.

7. Conclusions Within the scope of the ARNO project, many ques-  tions and issues came up which were addressed and discussed by the experts, specialists and knowledge holders involved. The active involvement of users and developers led to improvements in the software devel- opment environment, configuration management, test systematic and the test systems. The software devel- opment and rollout processes were also reviewed and  optimized. These improvements cannot be conclusive- ly assessed yet with regard to reducing costs and facili- tating more rapid software development.

Decisive success factors of this complex and unique project included consistent change management of the project requirements and the modifications to the ob- jects being migrated during the course of the project, utilizing a configuration management system. Other key success factors were the support of top manage- ment in the steering committee, the practical distribu- tion of tasks, including the outsourcing of the tool de- velopment, and a highly motivated project team.

The factor that contributed most to the success of this project was the constraints on the migration. The responsible managers resisted the temptation to get bogged down by technical issues such as restructuring and objectivizing the code. This was not a reengineer- ing project. Reengineering of the original code and data was limited to what was absolutely necessary to enable the conversion. The lesson learned was that it is very important to separate concerns [2]. The concern of migration is to transform a system from one envi- ronment to another with a minimum of side effects. It remained the guiding principle of the ARNO project.

