Logic-in-memory based Big-data Computing by Nonvolatile

Abstract?As one recently introduced non-volatile memory (NVM) device, domain-wall nanowire (or race-track) shows not only potential as future memory, but also computing capacity in big-data processing under unique domain-wall manipulation ability. In this paper, domain-wall nanowire device is studied for a NVM-based big-data computing platform, where all three parts: general purpose logic in LUT, special logic of XOR and data storage are all implemented by domain-wall nanowire devices. As one application, matrix multiplication, which is widely deployed in big-data applications such as machine learning or web searching, is studied in the proposed big-data computing platform. Experiment shows that when compared to CMOS based multi-core platform, the proposed computing platform exhibits 37x higher performance at the cost of 9x silicon area under the same power budget; and 4.2x better performance and 88.77% less power consumption under the same area constraint.



I. INTRODUCTION  The big-data applications have raised the need to rethink  the architecture of the current computing platforms. The data-  oriented applications have significantly increased demands  on memory capacity and bandwidth. The current computing  platform has well-known memory-wall issue with limited  accessing bandwidth but also large leakage power at advanced  CMOS technology nodes, which hinders the scalability for big-  data computing [1], [2], [3], [4], [5], [6], [7], [8], [9], [10],  [11], [12], [13], [14]. As such, a scalable yet power-efficient  memory-oriented computing platform is highly desirable for  the future big-data processing.

There are many recent explorations by the newly discovered  non-volatile memory (NVM) technologies at nano-scale [7],  [8], [9]. Specifically, domain-wall nanowire device [3], [4], [5]  has been recently utilized to perform logic-in-memory based  computation, which may erase the memory-wall of the limited  I/O bandwidth. In this paper, we have explored the study  of domain-wall nanowire based big-data computing platform.

Firstly, the domain-wall nanowire based look-up table (DW-  LUT) is configured to realize general logic functions required  by workload for big-data applications. What is more, the  domain-wall nanowire is also explored in specific logic (DW-  XOR) such as XOR and shift due to the unique domain-wall  shifting property. Because of the low power consumption of  DW-LUT and DW-SL, the proposed computing system can  be rich of computing resources that are specific to big-data  workloads. As a case study, the proposed computing system  is applied to execute large-scale matrix multiplication in a  MapReduce fashion. Compared to the conventional CMOS  based multi-core platform, the proposed computing platform  shows 37x higher performance at the cost of 9x silicon  area under the same power budget; and shows 4.2x better  performance and 88.77% less power consumption when under  the same area constraint.

The rest of the paper is organized as follows. Section II  introduces the overall memory-based computing platform by  domain-wall nanowire. Section III discusses the domain-wall  nanowire operation, domain-wall nanowire based LUT and  XOR logics in details. Section IV shows the case study of  how to apply the proposed computing platform to execute  MapReduce-based matrix multiplication. Experiment results  are presented in Section V with conclusion in Section VI.



II. NVM-BASED BIG-DATA COMPUTING PLATFORM  Non volatileMemory  General purpose reconfigurable  computing engine by NV LUTs  Logic 1  LUT  Logic 2  LUT  Logic N  LUT  ?Logic 3  LUT  Data switch network by NV crossbar  M e m o ry & fu n ct io n se q u e n ce co n tr o ll e r  DW XOR DW adder DW shifter  Accelerators resources by Non volatile DW logic  ????  Data storage and exchange  d a ta b u s  Fig. 1. System overview of the proposed NVM-based logic-in-memory computing platform  As shown in Figure 1, the proposed non-volatile memory  (NVM) based computing platform is composed of three parts.

Firstly, non-volatile domain-wall nanowire based lookup-table  (DW-LUT) is utilized for configuring general logic. In this  part, multiple LUTs are configured as different functions  according to the program to be executed. Conventionally,  program that intends to achieve complex functionality will  be decomposed into basic instructions that the ALU can  take by compiler. Therefore, it needs multiple clock cycles  to be executed due to the decomposition. This compromises     efficiency in order to gain better generality. However, in big-  data applications, programs are usually intensive with a set  of domain-specific functions without generality such that the  coarse granularity can be introduced. With the basic functions  implemented by LUTs, programs can be executed with greatly  augmented execution performance as well as power efficiency.

Secondly, the physics of domain-wall nanowire device is  exploited for special logic (DW-SL). Although most of the  functions are covered by LUTs, some commonly executed  functions such as XOR and shift can be economically imple-  mented by domain-wall nanowire device directly. Thirdly, non-  volatile memory is deployed for the data storage. Obviously,  the three main parts of the proposed computing system are  rich of non-volatile devices, which can be significantly helpful  to achieve high power efficiency and high bandwidth. In the  following, we will explore detailed design by non-volatile  domain-wall nanowire device for each part.



III. DOMAIN-WALL NANOWIRE BASED LOGICS  A. Domain-wall nanowire operations  Domain-wall nanowire, also known as racetrack memory  [3], [4], [5], is a newly introduced non-volatile memory device  in which multiple bits of information are stored in single  ferromagnetic nanowire. As shown in Figure 2(a), each bit is  denoted by the leftward or rightward magnetization direction,  and adjacent bits are separated by domain-walls. By applying  a current through the shift port at the two ends of the nanowire,  all the domain-walls will move left or right at the same velocity  while the domain width of each bit remains unchanged, thus  the stored information is preserved. Such a tape-like operation  will shift all the bits similarly like a shift register.

Magnetic  tunnel  junction  Insulator  Fixed layer  Write/read  port  Shift  port  Contact  Domain  wall  Fig. 2. The diagram of domain-wall nanowire device  In order to access the information stored in the domains, a  strongly magnetized ferromagnetic layer is placed at desired  position of the ferromagnetic nanowire and is separated by  an insulator layer. Such a sandwich-like structure forms a  magnetic-tunnel-junction (MTJ), through which the stored  information can be accessed.

Depending on the alignment of the fixed layer and free layer,  parallel or anti-parallel, the MTJ exhibits different states, high  resistance state or low resistance state. This is called giant  magnetoresistance (GMR) effect. By detecting the resistance  of MTJ junction, the direction of magnetization can be known  thus the stored bit is read out. The write operation is achieved  by current induced magnetization reversal of MTJ free layer.

Based on the current direction, the electron spin will force the  free layer to be parallel or anti-parallel to fixed layer, thus the  free layer magnetization is altered. Because the write and read  operation can only occur in the MTJ junction, thus the bit to  be operated needs to be shifted and aligned with fixed layer.

Shift operation is like a shift register, and the domain-wall  motion direction and velocity can be controlled by the current  direction and amplitude.

B. Special XOR logic  Obviously, the current induced shift operation of domain-  wall nanowire makes it a natural candidate to achieve shift  logic. Figure 3(a) shows the structure of shift logic obtained  by domain-wall nanowire and access transistors. This structure  contains multiple access ports for write and read operation,  and one port for shift operation. Therefore, the shifter can be  assigned with binary value, shifted leftward or rightward, read  out the binary value by manipulating the current through dif-  ferent port. Unlike traditional shift registers, in which the logic  gates built by complementary PMOS and NMOS will bring  the inevitable sub-threshold leakage, the access transistors for  non-volatile devices are able to avoid leakage current.

SHF1SHF1  RD  RDWR1  WR1 SHF2SHF2  WR2  WR2  Operand  A  Operand  B  XOR op  (a)  SHF  SHF WR1  WR1  WR2  WR2  WRn  WRn  Bit_1 Bit_2 Bit_n  (b)  Fig. 3. Logic devised at device level (a) shift logic achieved by multiple bit domain-wall nanowire with access transistors; (b) the XOR logic achieved by two domain-wall nanowires with access transistors.

The GMR-effect can be interpreted as the bitwise-XOR  operation of the magnetization directions of two thin magnetic  layers, where the output is denoted by high or low resistance.

In MTJ structure, however, the XOR-logic will fail as there is  only one operand as variable since the magnetization in fixed  layer is constant. Nevertheless, this problem can be overcome  by the unique domain-wall shift operation in the domain-wall  nanowire device, which enables the possibility of DWL-based  XOR-logic for computing.

Figure 3(b) shows a bitwise-XOR logic implemented by two  domain-wall nanowires. A new read-only-port is constructed    by two free layers and one insulator layer stacked in the  middle, where proposed bitwise-XOR logic can be performed.

Thus, the two operands, denoted as the magnetization direction  in free layer, can both be variables with values assigned  through the MTJ of the according nanowire. To perform  XOR operation, first the operand A and B are written into  left and right MTJ respectively; then A and B are shifted  to the constructed port, followed by a read operation at the  constructed port.

C. General LUT logic  nanowire  nanowire  nanowire  nanowire  nanowire  nanowire  nanowire  nanowire  nanowire  nanowire  nanowire  nanowire  BL BLB  Bit-line Mux  W o  rd -l in  e d  e c o  d e  r  Bit 1 Bit 2 Bit n  Bit nBit 1 Bit 2  Parallel output  Serial output  SHF  SHF  WL  WL  BL  BLB  MTJ  as access port  Data  segment  Reserved  segment  Fig. 4. LUT by domain-wall nanowire array with parallel output and serial output  Figure 4(a) shows the structure of cell in the LUT array. The  access-port lies in the middle of the nanowire, which divides  the nanowire into two segments. The left-half segment of  nanowire is used for data storage while the right-half segment  is reserved for shift-operation in order to avoid information  lost.

Figure 4(b) shows the domain-wall nanowire based LUT  array. The input of the function implemented by LUT is  represented as binary address. The address is fed into word-  line decoder and bit-line mux to find the target domain-wall  nanowire cell, where the multiple-bit result is kept. The LUT  array size depends on the domain, range and precision of the  function to perform.

Based on the way data is organized, the result can be output  in serial manner or parallel manner. In serial output scenario,  the binary result is stored in single domain-wall nanowire that  is able to hold multiple bits of information. Assume each  cell has only one access port and the first bit of result is  initially aligned with access port, the way to output result  is to iteratively readout and shift one bit until the last bit is  output. In parallel output scenario, the multiple-bit result is  distributed into different nanowires. Because each cell has their  own access port, the multiple bits can be output concurrently.

The design complexity of parallel output scheme is that, to  find the relative position of the result within the nanowire, a  variable access time will be introduced. For example, if luckily  the result is stored at first bit of the nanowires, the result can be  readout in one cycle; on the contrary if the result is kept at the  very last bit of the nanowires, it will take tens of cycles to shift  first before the result is output. Therefore, the choice between  serial output and parallel output is the tradeoff between access  latency and design complexity.



IV. CASE STUDY OF BIG-DATA MATRIX MULTIPLICATION  Matrix multiplication is one of the essential functions in  big-data applications like data mining and web searching.

For instance, singular value decomposition (SVD), which can  be used for deep learning in neural networks [15], involves  iterations of matrix multiplication. Google PageRank, which  intends to provide relative importance of billions of web pages  according to searching query, also involves large amount of  matrix multiplication operations [16]. In the following, we will  show how matrix multiplication can be computed in parallel,  and how it can be mapped to the proposed platform.

A. MapReduce-based matrix multiplication  MapReduce [17] is a parallel programming model to ef-  ficiently handle large volume of data. The idea behinds  MapReduce is to break down large task into multiple sub-  tasks, and each sub-task can be independently processed by  different Mapper computing units, where intermediate results are emitted. The intermediate results are then merged together  to form the global results of the original task by the Reducer computing units.

The problem to solve is x = M?v. Suppose M is an n?n matrix, with element in row i and column j denoted by mij , and v is a vector with length of n. Hence, the product vector x also has the length of n, and can be calculated by  xi = n?  j=1  mijvj  The pseudo-code of matrix multiplication in MapReduce  form is demonstrated in Algorithm 1. Matrix M is partitioned  into many blocks, and each Mapper function will take the  entire vector v and one block of matrix M. For each matrix  element mij it emits the key-value pair (i, mijvj). The sum of all the values with same key will make up the matrix-vector  product x. A reducer function simply has to sum all the values associated with a given key i. The summation process can also be executed concurrently by iteratively partial summing and  emitting until one key-value pair left for each key, namely the  (i, xi).

Algorithm 1 Matrix multiplication in MapReduce form  function MAPPER(partitioned matrix p ? M , v) for all elements mij ? p do  emit(i, mijvj) to list li end for  end function  function REDUCER(key q, s ? lq) sum ? 0 for all values rk ? s do  sum ? sum+ rk end for  emit(q, sum) end function  B. Workload mapping  5 2 6 2  8 0 3 1  7 1 4 3  Task queue  compile (0x00, 3, 0x10)  (0x03, 3, 0x10)  (0x06, 3, 0x10)  0x00  0x10  M  v  ?...

stored  ?...

Data segment  Step 3: Map process  DW-LUT  DW-logic  Step 2: fetch data  [5,2,6][2,1,3]  [8,0,3][2,1,3]  [7,1,4][2,1,3]  Step 1: fetch task  (1, 10)  (key, value) pairs  (1, 2)  (1, 18)  controllers  DW-LUT  DW-logic  DW-LUT  DW-logic  Dispatch  atom task  (2, 16)  (2,0)  (2, 9)  (3, 1)  (3, 12)  (3, 14)  DW-LUT  DW-logic  DW-LUT  DW-logic  DW-LUT  DW-logic  Mappers Reducers  Dispatch  atom task  (1, 30)  (2, 25)  (3, 27)  Step 4: Reduce process  [30,25,27] T  Step 5:  Write back  result  Fig. 5. Matrix multiplication mapping to proposed domain-wall nanowire based computing platform  Figure 5 shows how the MapReduce based matrix multi-  plication is mapped into the proposed non-volatile memory  based computing platform. Before execution, the DW-LUTs  are configured to execute integer multiplication. In addition,  the matrix multiplication workload needs to be compiled into  a task queue and stored into predefined region in the memory.

The purpose of compilation is to break down the workload  into smaller tasks that can be executed concurrently. In this  example, the matrix M is partitioned in the unit of rows, so each task only requires dot product of two vectors. Each  task instruction includes information like the entry address of  specific row of matrix M and vector v, as well as length of row n.

When the computation of the matrix multiplication starts,  the controllers, also achieved by reconfigurable LUTs, will  fetch the tasks from the task queue. For each task in the  queue, the controller will fetch its required data according  to the address, and then dispatch the multiplication tasks to  mappers based on the availability. The controllers will keep  fetching tasks, interpreting tasks, and dispatched tasks until the  queue is empty. The DW-LUT in each mapper will compute  the dispatched multiplication tasks, and emit the <key, value> pairs as intermediate results. The <key, value> pairs are further combined by the reducers. Specifically, each reducer  will take two pairs with same key from the intermediate results  pool, and combine the value by addition, and emit a new pair  to the intermediate results pool. The core operation in reduce  process is addition, which is mainly achieved by DW-XOR  based adder. The reduce process works in an iterative manner,  combining two pairs to one pair until the intermediate results  can not be further combined. As such, the final results are  obtained, and the results write back signifies the end of whole  process.



V. NUMERICAL EXPERIMENT RESULTS  A. DW-SL performance  TABLE I PERFORMANCE FOR DOMAIN-WALL NANOWIRE DEVICE AND SPECIAL  LOGICS  domain-wall nanowire device  Operation speed (cycles) energy (pJ)  read 1 1  write 1 0.3  shift 1 1  domain-wall nanowire logic  logic speed (cycles) energy (pJ)  32-bit XOR 5 110  32-bit adder 7 218  32-bit shifter 1?31 50?100  Table I shows the energy cost and speed of domain-wall  nanowire basic operations, as well as the performance of DW-  SL. For the domain-wall nanowire, the MTJ read and write  operation evaluation is obtained from NVM-spice [10], [11],  a SPICE simulator for NVM developed based on simulation  theory in [6], [12], [13]. The current density of 7e10A/m2  is utilized for magnetization switching, and 2.4K? and 1K? resistance values are assumed for anti-parallel state and paral-  lel state for MTJ. The sub-pJ results agree with the reported  measurement data in [18]. The current induced shift data is  extracted from reported measurement data [2].

The performance evaluation of DW-SL is obtained by  decomposing the logic into basic operations and summing up  accordingly. For example, as elaborated in section III-B, the  DW-XOR performs by two write operations, two shift opera-  tions, and one read operation. For shift logic, the indeterminate  range is due to the variable number of bits to shift. Different  from CMOS based logic, DW-SL needs multiple cycles to  operate, thus the latency is expected to be much longer  than its CMOS counterpart. However, computation throughput  outweighs latency in the context of big-data applications where    the data parallelism is high; benefited from the leakage free  property, more DW-SL resources are able to be allocated to  significantly increase the system throughput.

B. DW-LUT performance  700800 )  parallel output serial output leakage power             g e  p o  w e  r (n  W )  e n  e rg  y (p  J)        le a  k a  ge  LUT sizeLUT size  Fig. 6. Power characterization for DW-LUT in different sizes  Figure 6 shows the power characterization of DW-LUT in  different array sizes. To obtain the area, power and speed  of DW-LUT, the memory modeling tool CACTI [19] has  been extended with domain-wall nanowire model in [14]. In  terms of dynamic energy per look-up operation, the parallel  output scenario is much more power efficient than serial output  scenario, and the gap enlarges when array size increases. This  is because more cycles are required to output results in serial  than in parallel, therefore more access operations are involved.

However, the serial scenario is able to avoid the variable  access latency issue, which reduces the design complexity of  the controller. For leakage power, the non-volatility leads to  extremely low leakage power in nW scale, which is negligible  compared with its dynamic power. For volatile SRAM and  DRAM, the leakage power may consume as large as half the  total power especially in advanced technology node [19].

Once the domain, range, and precision of function are  decided, the DW-LUT size can be determined accordingly.

Therefore, the power characterization can be used as a quick  reference to estimate the power profile of specific function  to perform system level design exploration and performance  evaluation.

C. MapReduce-based matrix-multiplication performance  Table II shows the power, area, and performance com-  parison between proposed platform and conventional multi-  core platform. The workload is matrix multiplication in the  scale of million by million, and all matrix elements are 8 bit  integer numbers. In addition, serial output scenario is adopted,  and each nanowire is composed of 32 bits, out of which 16  bits are used for storing results and the other 16 bits are  reserved for shift operation, leading to an LUT array size of  64K for one multiplication operation. The comparison focuses  on the computation resources and is exclusive of memory  components. The 32nm technology node is assumed for both  scenarios.

TABLE II PLATFORM COMPARISON  General settings  Platform multi-core proposed  Technology 32nm  Workload 1M?1M matrix multiplication  Clock rate 3.4GHz 500MHz  Under 100W power budget  Platform multi-core proposed  # of computing resources 8 cores 58716 LUTs+ 5963 adders  Performance 24.48 GOPS 917.44 GOPS  Area 145 mm2 1292 mm2  Under 145 mm2 silicon area budget  Platform multi-core proposed  # of computing resources 8 cores 6591 LUTs+ 669 adders  Performance 24.48 GOPS 102.98 GOPS  Power 100W 11.23W  The multi-core based scenario consists of 8 Xeon cores  where the MapReduce based matrix multiplication is exe-  cuted. The evaluation flow is in two steps. Firstly, gem5  [20] simulator is employed to take MapReduce based matrix  multiplication from Phoenix benchmark suites [21] and to  generate the runtime utilization rate of core components. Next,  the generated statistics is taken by McPAT [22], which is able  to provide core area, power and performance results. For the  domain-wall nanowire based computing platform, the matrix  multiplication is translated into the task list form in section  IV-B. The DW-LUT and DW-SL evaluation is gained from  previous sections, and combined with the operation count, the  performance of proposed platform can be estimated.

The results indicate that, when power constraint is assumed  for both systems, the proposed memory based platform ex-  hibits 37x higher throughput, but at the cost of 9x larger  silicon area. This is because the non-volatile memory based  computing platform has very high power efficiency, thus  more computation resources can be afforded to gain better  performance. In the case if area constraint is adopted, proposed  system shows 4.2x better performance and 88.77% less power  consumption.



VI. CONCLUSION  In this work, a domain-wall nanowire based computing plat-  form is proposed for big-data applications. The non-volatile  memory domain-wall nanowire device is intensively utilized  to implement low power memory, special logic, and also  reconfigurable logic. Enabled by the low power dissipation  with large density, the proposed computing platform exhibits  very low power dissipation and high scalability, which can  provide abundant computing resources to exploit the high  data parallelism. In the case study of matrix multiplication  by MapReduce, experiment results show that when compared  to the conventional CMOS based multi-core platform, the  proposed computing platform shows 37x higher performance  at the cost of 9x silicon area under the same power budget; and    4.2x better performance and 88.77% less power consumption  under the same area constraint.

