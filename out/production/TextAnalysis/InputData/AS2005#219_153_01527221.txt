ONE EXTENDED FORM FOR NEGATIVE ASSOCIATION RULES AND THE  CORRESPONDING MINING ALGORITHM

Abstract: Recently, mining negative association rules has received  some attention and proved to be useful. To the best of our knowledge, three typical forms for negative association rules and three corresponding mining methods have been proposed.

However, the existing forms are not general enough, and can not represent some special cases in the real world. In this paper, an extended form for negative association rules is proposed, and a corresponding mining algorithm is presented.

The proposed mining algorithm is performed on two datasets.

Experimental results show that the algorithm is efficient on simple and sparse datasets when minimum support is high to some degree, and it overcomes some limitations of the previous mining methods. The proposed form will extend related applications of negative association rules to a broader range.

Keywords: Data mining; negative association rules; extended form  1. Introduction  Association rule mining, as an important problem in data mining, was originally proposed in [1]. It is intended to discover implicit and interesting associations among data items in a dataset. Let I={i1,i2,?,im} be a set of literals, which are called items. Let D be a dataset of transactions, where each transaction is a set of items. An association rule is an implication of the form X?Y, where X?I, Y?I, X?Y=? and X, Y are called itemsets. Support (sup) and confidence (conf) of an association rule are defined below:  )()sup()sup( YXpYXYX ?=?=?    (1) )sup()sup()( XYXYXconf ?=?      (2)  where p(X?Y) is the percentage of transactions that contain X and Y in dataset D. Given a datset D, the problem is to find all association rules that satisfy user-specified constraints: minimum support (minsup) and minimum confidence (minconf).

The originally proposed rules are positive association rules without the connective ???, and most of existing work has concentrated on positive association rules. Recently, mining negative association rules has received some attention and proved to be useful.

1.1.  Negative association rules  Logically speaking, a negative association rule is a rule with the connective ???, which corresponds to the absence or negative attribute of an item (itemset). For example, a??b??(c?d)?e is a negative association rule.

The necessity of introducing negative association rules lies in two aspects. From the point of logic, the expressive power of logical expressions constructed only by the two connectives ??? and ??? is not complete. From the point of applications, there exist many cases where negative attributes of items (itemsets) need to be considered.

1.2.  Three existing forms  Negative correlation in association rules was first mentioned by Brin S. et al. in 1997 [2]. Since then, to the best of our knowledge, three typical forms and three corresponding mining methods have been proposed. For convenience, we will denote the three forms by F1, F2, F3, and the corresponding methods by M1, M2, M3 respectively.

The first form F1 [3] is defined as an implication of the form X~?Y, whose intuitive meaning is that X?Y does not hold. The basic idea is: if ?P(Y|X) (expected value) is high to some degree, and P(Y|X) (actual value) is significantly lower, then X~?Y is an interesting negative rule. In M1, the expected value is computed based on taxonomy.

The form F2 was proposed by X. Wu et al. in [4, 5]. It extends the traditional positive form X?Y to a form involving four basic types: X?Y, X??Y, ?X?Y and      ?X??Y, where X?Y is called positive rule and the other three types are called negative rules. In M2, rules are generated based on observations: if minsup is high, sup(X??Y)?minsup would mean that sup(X?Y)?minsup.

Therefore, if minsup is high, negative rules are extracted from infrequent itemsets.

The form F3 [6] can be represented as an implication of the form: X?N?c, where X?I, N?NI, c?I or c?NI (NI={?i1,?i2,?,?im}, which is called a set of negative items). The essence of M3 is to extend GRD (generalized rules discovery) approach [7] from generating positive rules to generating both positive rules and negative ones.

1.3.  Motivation  We note that the three existing forms have some limitations. Clearly, F1 is too vague to precisely describe the actual relationship between itemsets X and Y. Although F2 and F3 further refine F1, they are not general enough. To sum up, none of the three forms can represent such a general form as ?a??b??(c?d)?e?f??(g?h)?, which is used in some cases in the real world sometimes.

One example is prevention or diagnosis of some new infectious diseases such as SARS. Diagnosis of SARS is so complicated that, perhaps, we have to consider three attributes (absence, presence and non-simultaneous presence) of symptoms to identify patients with SARS. In this case, the form of an association rule may be:  )()ss(...

)ss(s...ss...s  1tjitji  2ji1jiji1ii1  suspectedSARS?????  ??????????  +++++  ++++++ (3)  where sk (k=1,2,?,i+j+t+1) is a symptom.

Another example is analysis of some chemical  reactions. If we want to find out how a chemical compound is produced, sometimes, we have to precisely consider conditions of reactions: what substances are present, what substances are absent and what substances are not simultaneously present.

To handle these cases, a general form for negative rules should be proposed, and a corresponding mining algorithm should be devised. This is what this paper aims at.

In addition, the proposed mining algorithm tries to overcome the limitations of the previous mining methods.

The rest of this paper is organized as follows. Section 2 defines an extended form for negative association rules, and what is an interesting extended negative association rule. Section 3 proposes a corresponding mining algorithm for mining rules of the extended form. Experimental results are presented in Section 4. Section 5 contains our conclusions and future work.

2. One extended form for negative association rules  We observe that the general form mentioned above comprises three basic components as follows.

a ? Positive item, denoting the presence of item a.

?b ? Negative item, denoting the absence of item b.

?(c?d) ? Negation of positive itemsets, denoting the  non-simultaneous presence of items c and d (Note: in this paper, ?(c?d)=?(cd)=?{c,d}).

Theoretically, a general form should permit the three  basic components to occur in both the antecedent and consequent of a negative rule. The extended form (EF) that we propose is such a form.

2.1.  Concepts and notation  In this study, the three basic components are referred to as ?pattern?. Note that our use of pattern differs from that of the literature. A set of patterns is denoted by Pa (Pa=I?NI?NS), where I, NI and NS refer to the sets of the three basic components respectively. They are defined as:  I={i1,i2,?,im} (m=1,2,?) NI={?i1,?i2,?,?im} (m=1,2,?) NS={?X | X?I, |X|?2} Thus, pattern is the basic element of EF we propose.

A pattern p is frequent if (1) sup(p)?minsup and (2)  1-sup(p)?minsup if p?NI or p?NS.

k-patternset is denoted by psk, which is defined as:  psk={{p1,?,pk}|?i,j=1,?,k, i?j, pi?Pa, pj?Pa, atoms(pi)? atoms(pj)=?}, where atoms(pi) is the set of atoms appearing in pi. For example, if pi=?(c?d), then atoms(pi)={c, d}. Patternset is denoted by ps (ps=?k psk).

Other related notations are defined as follows.

PS (PSk): a set of patternsets (k-patternsets) L (Lk): a set of frequent patternsets (k-patternsets) NSL: a set of frequent negations of positive itemsets PL(PLk):a set of frequent positive itemsets (k-itemsets) NL1: a set of frequent negative items (NL1?NI)  2.2.  Definitions  Definition 1. (EF) The extended form for negative association rules is an implication as follows:  )()(  },...,,{},...,,{  ......

ConsConsequentAntAntecedent  qqqppp  qqqppp  nn  nn  ?=  ?=  ???????  (4)  where 1?n1,n2<m; ?i?{1,?,n1}, pi?Pa; ?j?{1,?,n2}, qj?Pa; Ant?Cons?PSn (n=n1+n2 is the size of the rule).

The form EF has the following five special cases: Case 1. X?Y when Ant?I, Cons?I Case 2. X??Y when Ant?I, Cons?NS or NI Case 3. ?X?Y when Ant?NS or NI, Cons?I Case 4. ?X??Y when Ant?NS or NI , Cons?NS or NI Case 5. X?N?c when Ant?(I?NI), Cons?I or NI.

Clearly, positive rules, F1, F2 and F3 all fall within the special cases of the form EF.

Definition 2. Rules of the form EF are called extended negative association rules (ENARs)  Definition 3. (An Interesting ENAR) Given minsup, mininterest and minconf, an extended negative association rule r: Ant?Cons is interesting iff  (1) Ant?Cons?Ln (2) interest(r)=sup(Ant?Cons)-sup(Ant)sup(Cons)  ? mininterest (3) conf(r)=[p(Cons|Ant)-p(Cons)]/[1-p(Cons)]  ?minconf  3. Proposed mining algorithm  In this section, we propose an algorithm for mining extended negative association rules (AMENAR).

The main framework of AMENAR is Apriori-like, i.e., the mining problem is decomposed into two subproblems: (1) Finding the set of frequent patternsets L; (2) Generating all interesting ENARs from L. To avoid too many scans of the dataset, we select a highly compact data structure, Patricia trie, to represent dataset D, so that D fits in main memory. We note that L1 is the base for generating L.

According to the definition of L1, before L1 is generated, the set of frequent positive itemsets, PL, must be found. We use the FP-growth algorithm [8] to find PL, since it is an efficient FP-tree-based algorithm for generating frequent positive itemsets. Before moving on to the AMENAR algorithm, we review the Patricia trie, the algorithms Apriori and FP-growth roughly.

3.1.  Patricia trie  The Patricia trie [9] is a simple modification of the FP-tree [8] by combining unary branching nodes. The main reason why we choose the Patricia trie is that its high reduction ratio enables a moderate-sized dataset to fit in main memory. It must be remarked that, in this study, we focus on only those cases where the Patricia trie fits in main memory, and AMENAR is a memory-based algorithm.

3.2.  Apriori and FP-growth  Apriori [10] and its derivatives have become a  standard for discovering association rules. It follows a two-step process to generate association rules: (1) Finding all frequent itemsets; (2) Generating interesting rules from the frequent itemsets.

FP-growth is an efficient FP-tree-based approach for mining positive frequent itemsets without candidate generation. Please refer to [8] for details.

3.3.  The AMENAR algorithm  All interesting ENARs are generated by the following four phases: Phase 1: Construct a Patricia trie for dataset D.

Phase 2: Find the set of positive frequent itemsets PL  by using the FP-growth algorithm; then generate the set of frequent 1-patternsets L1.

Phase 3: Starting from L1, find frequent k-patternsets Lk by using a strategy of candidate-generation-and-test iteratively (L=?kLk).

Phase 4: Generate all interesting ENARs from L.

The four phases are illustrated by Example 1 below.

Example 1. Let dataset D be the first two columns of  Table 1. Let I={a,b,c,d,e,f}, minsup=0.4, mininterest=0.1, minconf=0.5. We examine how AMENAR generates all interesting ENARs from D.

Table 1. Dataset D and ordered frequent items TID Items bought (Ordered) frequent  positive items  f, d, b a, b, c, f b, e, f a, f, c f, b, c c, a a, c, b c, e, a d b  b, f b, c, a, f b, f c, a, f b, c, f c, a b, c, a c, a  b  Phase 1. The patricia trie is constructed as follows: (1) Scan D once and collect the set of frequent positive items F, then sort F in support-descending order as FList; (2)Scan D again and insert the ordered frequent positive items in each transaction into the Patricia trie.

In this example, FList={b:6, c:6, a:5, f:5}. The ordered frequent positive items are shown in the last column of Table 1, and the Patricia trie is shown in Fig.1.

Phase 2. First, find PL by using FP-growth. After PL has been found, NL1 and NSL can be generated directly according to their definitions. In this example, they are:  PL={{b:6},{c:6},{a:5},{f:5},{(bf):4},{(ca):5}} NL1={{?b:4},{?c:4},{?a:5},{?f:5}}      NSL={{?(bf):6},{?(ca):5}} L1=PL1? NL1? NSL   Figure 1. Patricia tire for dataset D   Phase 3. This phase implements two major operations: candidate-generation and test.

For ensuring the correctness of the operations candidate-generation and test, patterns are ordered according to FList: ?p1, p1? ?I, ?p2, p2? ?NI, ?p3, p3? ?NS, let p1>p2>p3; p1 and p1? (p2 and p2?, p3 and p3?) are ordered by their corresponding atoms according to Flist. For example, if p1=b, p1?=a, p2=?b, p2?=?c, p3=?{b,a}, p3?=?{b,f}, then p1>p1?, p2>p2? and p3>p3?. Patternsets are ordered according to their corresponding patterns, e.g, {p1?, p3?}>{p2?, p3} since p1?> p2?.

In essence, the candidate-generation operation is a join operation, i.e., a set of candidate k-patternsets Ck is generated by joining Lk-1 with Lk-1, where members of Lk-1 are joinable if they have (k-2) patterns in common. It must be remarked that candidate positive k-itemsets do not need generating again since PL has been found in Phase 2.

The test operation counts the support of each candidate as soon as a candidate c (c?Ck) is generated. If the support is larger than minsup, then c is added to Lk, or else c is pruned. The key operation is the counting of support, which is illustrated by Example 2 below.

Example 2. Examine the dataset in Table 1 and the Patricia trie in Fig.1. Let minsup=0.3, l1={c, ?b}, l2={c, ?(a?f)}, c?=l1 join l2={c, ?b, ?(a?f)}. sup(l1)=0.3, sup(l2)= 0.4, l1,l2?L2, c??C3. We focus on the counting of sup(c?).

Note that sup(c?)=sup(l1)-count(l)/10,where l={c,a,f, ?b}. Now count(l) needs to be counted. First, we introduce three notations: (1) I-P(l): part I of l; (2) NI-P(l): part NI of l; (3)NS-P(l): part NS of l. In this example, I-P(l)={c,a,f}, NI-P(l)={?b}, NS-P(l)=?. Count(l) is counted as follows: choose the last element of I-P(l) (denoted by LastI-P(l)), f, as a base, then travel each path (from the parent node of f to the root) to identify whether l-{f} is on the path. If l-{f} is  on the path, then f.num is added to count(l). The paths are shown in Fig.1, where path1={b}, path2={a,c,b}, path3={c,b}, path4={a,c}. Header Table Null count(l)=count(path1)+count(path2)+count(path3)  +count(path1)=0+0+0+1=1 b 6 c 6 a 5 f 5  c a  b:6 sup(c?)=0.3-1/10=0.2<minsup. :3 In the above, the counting of sup(c?) is converted into  that of sup(l) until NS-P(l)=? and ???Atoms(NI-P(l)), ?>LastI-P(l). This termination condition ensures that we only need to travel upward from the base.

c:3 f:2 f:1  a:2 In addition, to avoid generating conflicting patternsets  and too many candidates, two candidate-pruning strategies are implemented before sup(c?) is counted.

Candidate-pruning1: A new candidate c? (c??Ck) is  pruned if there exists an infrequent (k-1)-subset in c?.

Candidate-pruning2: A new candidate c? (c??Ck) is  pruned if it conflicts with an existing frequent patternset l (l?Lk? k??k), e.g., c?={b,a,?d}, l={b,a, d}, or c?={b,c,f,?a}, l={b, ?a, ?(c?f)}.

Phase 4. This phase outputs the ENARs that satisfy  minsup, mininterest and minconf. To avoid generating redundant rules, we consider two kinds of redundant rules and adopt two corresponding rule-pruning strategies, which are illustrated by Example 3 as follows.

Example 3. (adapted from [2]) Given a dataset D of 100 transactions, let minsup=0.2, mininterest=0.13, minconf=0.38. We focus on the purchase of tea (t) and coffee (c). Their counts are shown in Table 2. Consider three negative rules:  r1:?c?t [0.42, 0.1384, 0.874] r2: ?t ?c [0.34, 0.1384, 0.874] r3: t??c [0.42, 0.1384, 0.386] The three values behind each rule refer to support,  interest and confidence of the rules respectively.

Table 2. Tea and coffee c  ?c ?row t ?t  22 42 34  2   ?col 56  44 100 Firstly, consider a pair of equivalent rules: r1 and r2.

Logically, r1 is equivalent to r2, therefore one of them should be pruned. Note that interest(r1)=interest(r2) and conf(r1)=conf(r2). Furthermore, it is not difficult to prove that the two equations hold for any pair of equivalent rules.

Therefore the rule with lower support, r2, can be pruned.

Secondly, consider a pair of reverse rules: r1 and r3.

Clearly, sup(r1)=sup(r3) and interest(r1)=interest(r3), and therefore the rule with lower confidence, r3, can be pruned.

Moreover, we observe that, in a pair of reverse rules, the rule with higher antecedent's support has lower confidence (the proof is omitted). Therefore, with higher antecedent?s  path4 f:1  Num Item path1  f:1 path3 Node-Link  path2      support, the rule r3 can be pruned. The above pruning strategies are summarized as follows.

Rule-pruning1: In a pair of equivalent rules, the rule  with lower support is pruned.

Rule-pruning2: In a pair of reverse rules, the rule  whose antecedent has higher support is pruned.

We apply AMENAR, M2 and M3 to dataset D in  example 1. The rules generated are shown in Table 3.

Table 3. Rules found by AMENAR, M2 and M3 AMENAR/EF M2/F2 M3/F3  r1: a?c r2: a??(b?f)?c r3: c??(b?f)?a r4: c?a??(b?f) r5: f?b r6: ?a?b r7: a??(b?f)  Y(r1?,r1??,r1???) N N Miss Y Y Miss  Y(r1?,r1??,r1???) N N N Y Y N  Remark: (1) In Table 3, ?Y? means the rule is included in the results; ?N? means the rule is excluded in the form; ?Miss? means the rule is missed by the method although it is an interesting rule by the definition; (2) Redundant rules r1?: ?c??a, r1??: c?a and r1???: ?a??c are pruned in AMENAR; (3) Some negative rules are missed in M2 since the observations it based on are inaccurate; (3) M1 is not included since it is dependent on taxonomy.

3.4.  Advantages of AMENAR  Compared with M1, M2 and M3, AMENAR has the following advantages: Mining Power: (1) As shown in Table 3, AMENAR is  more powerful than M2 and M3, since it finds not just a subset, but all interesting ENARs; (2) Unlike M1, AMENAR is independent on domain knowledge.

Accuracy: (1) AMENAR is more accurate than M2 and M3 since it filters conflicting rules and two kinds of redundant rules; (2) It will not miss interesting rules.

Space efficiency: (1) Due to the high compactness of the Patricia trie, AMENAR has high space efficiency; (2) Some strategies used in AMENAR further reduce the space cost: (a) In Phase 3, a candidate is tested as soon as it is generated, so it will be discarded at once if it is infrequent; (b) Conflicting candidates and two kinds of redundant rules are pruned.

Time efficiency: (1) I/O cost: only two scans of the dataset are required in AMENAR, while k+1 scans are required in the previous methods; (2) The cost of generation and test: it is related to the characteristics of datasets.

In Section 4, the time efficiency will be analyzed  further by experimental results.

4. Experimental results  To study the effectiveness of the algorithm AMENAR, we have performed AMENAR, M2 and M3 on two synthetic datasets and compared their effectiveness.

4.1.  Experimental design and environment  We use Visual C++ programming language to implement the algorithms. The algorithms are performed on a personal computer of an Intel Pentium processor with a clock rate of 2.6GHz and 512MB main memory.

4.2.  Datasets  The experiments are conducted on two synthetic datasets: one is T10I4D100K, which is a simple and sparse dataset downloaded from [11], and the other is Dtest, which is a complex and dense dataset produced by a synthetic data generation program [10]. The characteristics of the datasets are shown in Table 4.

Table 4. Characteristics of Datasets Dataset  |R| |T| |MaxFP| |r|  T10I4D100K Dtest      Remark: |R|: the number of items that occur in dataset D, |R|=|I|=m. |T|: the average number of items in each transaction. |MaxFP|: the size of maximal frequent positive itemsets. |r|: the number of transactions.

4.3.  Experimental results and analysis  Figure 2 shows the execution time of the three algorithms on T10I4D100K with the variation of minsup.

Figure 2. Execution time on T10I4D100K   When minsup is lower than 2%, AMENAR is slower  than M2 and M3. This is because when minsup is low, too many candidates are generated. When minsup is over 2%,  0  0.5   1  1.5  2   2.5  3  3.5 minsup(%)    Ex ec  ut io  n Ti  m e  (S )  M2 M3 AMENAR      AMENAR is more efficient than M2 and M3, since the number of candidates generated is not too large, and the main overhead is searching the Patricia trie in main memory for frequent patternsets.

Figure 3 shows the execution time of AMENAR, M2 and M3 on dataset Dtest. We see that AMENAR is not efficient on complex and dense datasets with too many items (|R|=18490) and long frequent patternsets (|MaxFP|=10). This is because Phase 2 generates a large number of frequent items, and too many candidates are generated in Phase 3.

Figure 3. Execution time on Dtest   According to our experimental experience, AMENAR  is efficient when (1) |PL1| has a magnitude of ?(102); (2) |MaxFP|<5; (3) minsup is high to some degree.

Furthermore, to examine the scalability of AMENAR against the number of transactions, we conducted it on Dtest with the number of transactions varying.

Experimental results show that it has a linear scalability, which is due to Patricia trie?s linear nature over the number of transactions.

5. Conclusions and future work  In this paper, we have proposed an extended form for negative association rules and a corresponding mining algorithm AMENAR. The extended form is more general and expressive than the previous forms. The algorithm AMENAR is efficient on simple and sparse datasets, and has overcome some limitations of the previous methods.

The extended form and the AMENAR algorithm will extend related applications of negative association rules to a broader range, such as prevention or diagnosis of some new infectious diseases and analysis of some chemical reactions.

However, some problems remain unsolved in our current work. Furthermore, we will do some work below.

Firstly, we will improve the efficiency of AMENAR on complex and dense datasets. One possible way is to find  max-patternsets [12] or closed patternsets [13], instead of all frequent patternsets. Secondly, we will extend the form EF to quantitative, multilevel and multidimensional forms, and propose the corresponding mining methods.

