Big Snapshot Stitching with Scarce Overlap

Abstract?We address certain properties that arise in gigapixel-scale image stitching for snapshot images cap- tured with a novel micro-camera array system, AWARE-2.

This system features a greatly extended field of view and high optical resolution, offering unique sensing capabilities for a host of important applications. However, three simul- taneously arising conditions pose a challenge to existing approaches to image stitching, with regard to the quality of the output image as well as the automation and efficiency of the image composition process. Put simply, they may be described as the sparse, geometrically irregular, and noisy (S.I.N.) overlap amongst the fields of view of the constituent micro-cameras. We introduce a computational pipeline for image stitching under these conditions, which is scalable in terms of complexity and efficiency. With it, we also substantially reduce or eliminate ghosting effects due to misalignment factors, without entailing manual intervention. Our present implementation of the pipeline leverages the combined use of multicore and GPU architectures. We present experimental results with the pipeline on real image data acquired with AWARE-2.



I. INTRODUCTION  A. Image stitching  Digital image stitching offers a means of forming images over an extended field of view (FoV) without sacrificing image resolution. Constituent images may be acquired by a special-purpose camera such as those of planetary rovers [1], orbiting satellites [2], a sky-gazing telescope, a single camera on a robotic mount [3], [4], or a commodity camera with sweep-mode functionality.

Despite great advances in available stitching soft- ware, obtaining high-quality mosaics largely relies on several favorable conditions: the captured scene being almost stationary; overlap between adjacent images be- ing large and not adversely corrupted by noise; and calibrated information on the extrinsic and intrinsic pa- rameters of the image acquisition system [5] being fully or partially accessible. Should no information be avail- able on the extrinsic parameters, one may still recover the spatial geometry for a stationary mosaic, provided that the overlaps are rich in distinctive features [6].

Alternatively, the use of camera arrays [7]?[11] may  relax or remove the stationary-scene requirement, as well as constrain the space of extrinsic parameters.

AWARE-2, a novel micro-camera array developed by Brady et al. [11], allows capturing snapshots at gigapixel scale. The distributed FoVs of simultaneously firing micro-cameras can be combined by means of image stitching to form a single high-resolution image over an extended FoV, while foregoing scene motion or other dynamic effects. Successors to AWARE-2 are expected to provide a cost-effective alternative to very high-end or bulky gigapixel cameras. Such unique imaging capabilities have many potential applications, which include wildlife habitat monitoring [12], celestial exploration [1], and recognition or tracking of moving objects or people in crowded scenes [13]. In the rest of this paper, we focus on image stitching with regard to AWARE-2 characteristics, some of which are shared by its precursors, contemporary peers, as well as successors in the making.

B. The S.I.N. conditions  From a computational viewpoint, the constituent AWARE-2 snapshots can be treated as if shot in succes- sion by a single camera capturing an effectively static scene. An additional advantage is the absence of paral- lax between adjacent shots, owing to the optical design of AWARE-2. Nonetheless, design considerations for maximizing the composite FoV and resolution give rise to three challenging conditions for the stitching process.

These conditions pertain to the sparse, geometrically irregular, and noisy (S.I.N.) overlap amongst the con- stituent FoVs?this is illustrated in fig. I.1: (i) Overlap among constituent images is scarce; any two adjacent images overlap over only a small portion of their combined domain, if at all. (ii) The extrinsic param- eters of each micro-camera are defined by two angular rotations [14] and sensor displacement. These geometric parameters are highly irregular across the set of micro- cameras, as a result of manufacturing deviations from the composite design, as well as camera packing on the dome-like mount. (iii) Image data in regions of overlap are highly noisy, due to adverse vignetting and stray     Fig. I.1: A mosaic of 7 AWARE-2 micro-camera images.

Overlapping regions are highlighted in orange, and challeng- ing cases are circled. Top: overlap is too small. Bottom: the region is very poor in distinctive features.

light effects [14]; this becomes more problematic for regions that are lacking in features.

Existing stitching software solutions are consider- ably challenged by the S.I.N. conditions. Some require the FoV configuration to follow the conventional pattern of a regular grid. On the other hand, grid-free solu- tions rely on significant and distinctive overlap between neighboring images, yielding poor results with sparsely overlapped ones. These problems are further aggravated by the presence of noise and photometric aberrations.

C. The stitching pipeline  Our aim is to obtain appropriate image transforma- tions to effect correct and coherent alignment of the constituent images. Misalignment gives rise to visible geometric and photometric inconsistencies, and often produces ghosting effects?see figs. IV.1a and IV.1c. It stems from deviations in camera location and orientation from the ideal, designed geometry, but also from the inhomogeneous and dynamic nature of camera settings.

Moreover, we are concerned with the computational efficiency and scalability of the stitching process. Based on the above considerations, we introduce a compu- tational pipeline, which does not require any manual intervention?see the diagram of fig. I.2. With it, we have obtained satisfactory results in snapshot stitching with AWARE-2 data.

The underlying idea is as follows: We exploit scene- dependent information (features) together with system- specific information. The latter kind, particularly the distributed camera placement configuration, need not be static, but may be subject to subtle changes over time for reasons such as mechanical or thermal drift [4], [7], [14]. In our stitching approach, we use information from extracted features to refine the system information; and we use the designed or calibrated (by experimental or computational means) geometric configuration to confine and guide the feature-based alignment process,  or to provide a fallback option should scene-dependent information be insufficient.

We introduce two key stages in sections II and III, demonstrate experimental results in section IV, and conclude the paper with section V.



II. MULTIPLE IMAGE ALIGNMENT  The multiple image alignment procedure consists of two basic phases: (i) adjacent images are registered in a pairwise manner, and (ii) the pairwise registration trans- formations are adjusted into a coherent set of simulta- neous transformations. We refer to the latter phase as bundle adjustment, notwithstanding the term generally implies 3D reconstruction, which we circumvent as we are interested in producing 2D snapshot mosaics.

A. Pairwise image registration  Two images Ii and Ij are termed adjacent if they overlap or share a common boundary. Pairwise reg- istration refers, then, to the process of determining a geometric transformation such that, in the absence of noise and photometric variation,  Ii(v) = Ij(Hijv) ?v ? Di ? Dj , (II.1) where v = [x y 1]? is a vector of homogeneous coordinates; Di indicates the domain of image Ii; and Hij is the 3? 3 transformation matrix.

Automatic and robust image registration methods often resort to feature-based approaches. These entail extracting sets of corresponding features in both images and using them as control points, v, for determining the transformation matrix, Hij . This process is complicated by the presence of noise and photometric variation [4], [15], which it must be made robust for. There is a rich body of literature on feature-based image registration? one may refer to [5] for an elaborated overview?and many different types of features to consider. We make use of SIFT keypoints [16], which are robust for a broad class of transformations, albeit computationally more expensive than others [17]; we use the SiftGPU library [18] to compute the keypoints efficiently.

Because of the S.I.N. conditions, we extend the nominal regions of pairwise overlap, Di ? Dj , to com- plement imprecise knowledge of the system geometry, and to avoid unnecessary truncation of the domain of potential features. This extension depends on deviation tolerance settings and is refined through data learning.

Feature matching provides control points to be used in determining the registration transformations. It is realized in two steps: descriptor matching and outlier removal. The former is effected using the approach suggested by Lowe in [16], aided and accelerated by system-specific information. The latter is discussed in the next subsection, and it is central to our pipeline.

Fig. I.2: A simplified diagram of the image stitching process. Computation-intensive modules are highlighted in orange. High- performance operation categories are indicated by the dashed arrows.

Fig. II.1: Minimal FoV overlap may lead to the estimation false-positive registration transformations.

B. PG-RANSAC  The set of matched descriptors may still contain several false matches (or ?outliers?). Such erroneous control points can lead to inconsistent image alignment, or broken mosaics, as shown in fig. II.1. Hence, outliers must be removed. This is typically carried out by the RANSAC algorithm [19] or a variation thereof. We have developed a RANSAC-like algorithm, which we call PG-RANSAC, for robust stitching under the S.I.N. con- ditions. It incorporates the placement geometry of mul- tiple cameras, as per their extrinsic parameter ranges, into the outlier removal process. Thus, PG-RANSAC is an integral part of the registration estimation and adjust- ment process, greatly improving its overall robustness by utilizing system information in tandem with image features.

In order to avoid cases such as that of fig. II.1, we must consider both points and transformations in the outlier removal process, instead of just points. To this end, we augment the ranking function in the RANSAC verification step to make use of the expected placement geometry:  ?? ( d, T (?), ??  ) = f  ( T (?), T (??)  ) ? ?  ( d, T (?)  ) , (II.2)  where d is the feature re-projection distance; T (?) is  a transformation T with parameters ?; ?? refers to the expected parameters; and ?(?, ?) is any RANSAC-style ranking function, normalized to the range [0, 1]. The weight function f(?, ?) penalizes transformations that deviate much from the expected one, and has the form  f(x, x?) = N?  i=1   1 + e??[(xi?x?i)??i] ? 1 1 + e?[(xi?x?i)??i]  ,  (II.3) where N is the number of deviation measurements, such as camera orientation, that are used to validate estimated transformations; x?i is the expected value for the i-th parameter or measurement; and ?i is the error tolerance that corresponds to x?i, which may be related to the camera array manufacturing/mounting process, noise in sensor readings, etc. The weight function for a single measurement is shown graphically in fig. II.2. It features a plateau in the range [(x? ? ?), (x? + ?)], and drops sharply at the boundaries, at a rate dependent on ?.

Incorporating this geometric constraint entails the comparison of estimated transformations against the expected ones. To do this, we generate a set of points to- wards the periphery of the image domain and separately apply the expected and estimated transformations to them. Then, the mean angular and magnitude errors of the motion vectors between the respectively transformed points can be computed [20]; these are used as a measure of deviation of the estimated transformation from the assumed configuration. The set of inliers with respect to this augmented measure is input to the subse- quent bundle adjustment. Should no transformation be estimated by the PG-RANSAC process for a pair of images, which might be the case given the small and noisy nature of overlap between images, then a set of 4 anchor points is generated. These effectively guide the bundle adjustment process to respect the expected transformation for the pair.

C. Bundle adjustment  Bundle adjustment generally pertains to the structure-from-motion problem, which implies recon- struction of the 3D scene [21]. Our objective is, rather, to determine a set of projective transformations, one for    (x?? ?) (x? + ?)   f (x  , x? )  Fig. II.2: Graph of eq. (II.3) for a single deviation parameter.

each constituent image. Still, formulation of this process is analogous to that of a bundle adjustment problem.

Assume that each constituent sub-scene can be cap- tured approximately by a plane, which is reasonable when the distance between the captured scene and the lens is large compared to the focal length of the lens. We may then model the projective relations as homography transformations, and use one of the images as a common reference plane. We form a least-squares (LS) error model for the simultaneous projections:  min {Hi}  ?  Di?Dj ?=?  ?  vk?Mij wij  ??Hivki ?Hjvkj ?? , (II.4)  where Mij denotes the set of matched control points between two adjacent images; vki is the local homoge- neous coordinate vector of the k-th control point in Di; and wij is a weight that normalizes the contribution of each pair to the solution.

We have developed a fast method for minimizing the above error. The LS solution to eq. (II.4) can be obtained through the corresponding system of normal equations.

The related system matrix is a block-wise sparse, with a 3 ? 3 block for each image pair. Note that its size depends solely on the number of constituent images, and not on the number of control points. The block structure corresponds directly to the Laplacian matrix of an adjacency graph where nodes and edges correspond to cameras and regions of FoV overlap, respectively? see fig. II.3. The block-Laplacian matrix can be con- structed easily and in parallel. Next, by choosing a reference plane among the participating image planes, we recast the homogeneous normal equations to a non-homogeneous system, circumventing the null space problem and short-cutting an otherwise lengthy and complex iteration process [21], [22]. The solution to the final, linear system can be obtained directly and efficiently. The fast process is robust because of the PG- RANSAC framework providing reliable control points.



III. IMAGE FUSION  Following the bundle adjustment process, every pixel can be projected onto a specific location of the mo- saic canvas, albeit different pixels might share the same location. Photometric variation between overlapping and adjacent image regions may result in visible seams in the stitched image [23]. In order to fuse the overlap- ping image data, we blend the images in the gradient domain [24], [25]. This technique has the advantage  R2                R2                R  Fig. II.3: A portion of the AWARE-2 camera adjacency graph.

Edges in dashed lines indicate very small overlap. The plane associated with node R is chosen as the reference plane.

of smoothing intensity seams, while maintaining high- frequency information in the source images; moreover, image gradients are invariant to camera sensor bias.

Let I?c be the c-th projected image on the mosaic can- vas. Gradient-domain blending dictates that the blended mosaic, I?, must satisfy  ?I?(v) = ?  D?c?v wc(v)?I?c(v), (III.1)  where wc(v) is a weighting function that we set to be the reciprocal of the pre-calibrated flat-field measure- ment for each pixel v ? D?c, which reflects its apparent gain and dark current [14]. The intensity-domain mosaic is then computed via numerical integration.

We perform the image fusion computations entirely in the GPU. Constituent images are back-projected using bi-linear interpolation, and are subsequently dif- ferentiated in batches of non-overlapping images. Last, we employ the convolution pyramid scheme [26] to effect a fast approximation of the integration operation.



IV. DEMONSTRATION  Our pipeline implementation makes combined use of multicore and GPU architectures. Representative ex- perimental results, obtained with our pipeline on real data acquired with AWARE-2, are shown in fig. IV.1.

The images to the right were produced automatically and processed within half a minute, without overlapping CPU-GPU data transfers and computational processing.



V. ADDITIONAL REMARKS  We have outlined a computational pipeline for image stitching at gigapixel scale, with scarce overlap of the constituent fields of view, and under the presence of noise, photometric variation, as well as other factors that make the image acquisition conditions deviate from the ideal, designed configuration. While existing    (a) (b)  (c) (d) Computational Photography, Seattle, 2012 (bottom floor and architectural surroundings not shown). (a),(c): Results produced by the AWARE-2 compositing pipeline [14], where tone mapping has been applied. (b),(d): Results produced automatically by the alternative pipeline introduced in this paper, without tone mapping. Top row: The displayed scene spans the fields of view of approximately 25 micro-cameras. Bottom row: Detail, zoomed in within the marked windows in the top-row images.

software for image stitching fails or performs poorly with AWARE-2 data, our pipeline in its present imple- mentation succeeded in effecting robust and efficient stitching of high-resolution, ghost-free mosaics. In order to achieve this, we explore and leverage the acquisition conditions in tandem with image features, while using one to refine the other. Further, we exploit advanced algorithm techniques and modern parallel architectures to automate and accelerate the stitching process. While use of a snapshot camera array eliminates scene motion effects, efficiency in the stitching process will become paramount for applications using such camera systems for image or video analysis of dynamically evolving scenes, by enabling the monitoring of many kinds of scientific or social phenomena at fast-changing rates.

The pipeline can be easily ported to other appli- cations, not restricted to snapshots or AWARE-2. This paper is the first written document about its overall structure.1 Due to space constraints, detailed descrip- tions and potential improvements are omitted and will be reported elsewhere.

The authors wish to thank Esteban Vera Rojas, Steve Feller, Daniel Marks, Lars Nyland, Changchang Wu, as well as our anonymouns reviewers.

