2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

Abstract?The proliferation of private clouds that are often underutilized and the tremendous computational potential of these clouds when combined has recently brought forth the idea of volunteer cloud computing (VCC), a computing model where cloud owners contribute underutilized computing and/or storage resources on their clouds to support the execution of applications of other members in the community. This model is particularly suitable to solve big data scientific problems. Scientists in data-intensive scientific fields increasingly recognize that sharing volunteered resources from several clouds is a cost-effective alternative to solve many complex, data- and/or compute-intensive science problems. Despite the promise of the idea of VCC, it still remains at the vision stage at best.

Challenges include the heterogeneity and autonomy of member clouds, access control and security, complex inter-cloud virtual machine scheduling, etc. In this paper, we present CloudFinder, a system that supports the efficient execution of big data workloads on volunteered federated clouds (VFCs). Our evaluation of the system indicates that VFCs are a promising cost-effective approach to enable big data science.

Index Terms?Big data, Cloud federations, Volunteer cloud computing, Workload placement.

F  1 INTRODUCTION  C Loud computing has emerged as an efficient, cost effec-tive alternative that provisions on-demand computing and storage resources. Until recently, organizations have often opted for using commercial clouds, e. g., Amazon Elastic Compute Cloud (EC2), IBM Cloud, Google Compute Engine, Microsoft Cloud, HP Helion Public Cloud, and Rackspace Cloud. As the benefits of cloud computing have now been established and their cost models are better com- prehended, an increasing number of organizations have re- alized that deploying their own on-premises, private clouds is a much more cost-effective alternative to commercial clouds. This obviously incurs acquisition and maintenance costs but, in the long run, these costs are offset after only a few months of exploitation. As a result, many organizations have now deployed private clouds of various sizes and using various commercial and open source private cloud software solutions. These private clouds have proven to be adequate in supporting applications with moderate comput- ing requirements. However, in many cases, a single private cloud may not provide sufficient computing and/or storage resources to support data- and/or compute-intensive appli- cations. At the same time, most private clouds are used at full capacity only during brief periods of time [1].

The proliferation of private clouds that are often un- derutilized and the tremendous computational potential of  ? A. Rezgui, N. Davis, and H. Soliman are with the Department of Computer Science and Engineering, New Mexico Tech, Socorro, NM, 87801.

E-mail: {rezgui@cs, hss@cs, ndav01}@nmt.edu  ? Z. Malik is with the Department of Computer Science, Wayne State University, Detroit, MI, 48202.

E-mail: zaki@wayne.edu  ? B. Medjahed is with the Department of Computer & Information Science, University of Michigan, Dearborn, MI, 48128.

E-mail: brahim@umich.edu  Manuscript received July 13, 2015.

these clouds when combined have recently brought forth the idea of volunteer cloud computing (VCC), a comput- ing model where cloud owners contribute underutilized computing and/or storage resources on their clouds to support the execution of applications of other members in the community. This model is particularly suitable to solve complex, big data problems in many science disciplines such as genomics, astronomy, physics, meteorology, biology, and environmental studies. These and many other science fields are inherently big data disciplines that require the acquisition, transfer, storage, and processing of very large volumes of data. High speed networking technologies have made it possible to transfer massive amounts of data in relatively short times. Also, increasingly affordable storage technologies have made it possible to store very large vol- umes of science data. The major challenge left is to provide processing capacities able to cope with the data-intensive (and often also compute-intensive) nature of many scientific problems.

Scientists increasingly recognize that sharing volun- teered resources from several clouds is a cost-effective alter- native to solve many of these big data science problems. The vision of VCC is based on the premise that, in practice, for mutual benefits, the owners of many private clouds would agree to combine resources on their private clouds to enable the execution of applications that may not be possible using resources on a single cloud.

Volunteer cloud computing usually refers to scenarios where volunteers allow guest virtual machines (VM) to be run on servers in their clouds. Typically, a user prepares a VM that contains a given application and all software packages necessary to run that application. The user then uploads that VM on one of the clouds willing to host new VMs. Examples of systems that have adopted this approach include CernVM [2], BoincVM [3], and Cloud@Home [4], [5], [6]. In this regard, current VCC systems are only a    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2703830, IEEE Transactions on Big Data   virtualization-based extension to traditional grid computing software platforms such as Boinc1 or Condor (now renamed HTCondor)2. We redefine VCC as a computing model that enables entire clouds (and not desktop PCs, workstations, or farms of servers as in traditional grid computing) to volunteer some or all of their computing and/or storage resources. We refer to the infrastructure that is formed when resources on several clouds are combined by the term volunteer cloud federation (VCF). Simply put, a VCF is a software infrastructure that seamlessly combines resources on several (private or commercial) clouds and offers users an abstraction of a single, larger cloud.

Despite the promise of the idea of volunteer cloud computing, it still remains at the vision stage at best [7], [8]. Challenges include the heterogeneity and autonomy of member clouds, unpredictability of resource availability, access control and security, complex inter-cloud virtual ma- chine scheduling , etc. In this paper, we present CloudFinder, a system that can be deployed over a volunteer cloud federation to enable big data science. Our current prototype of the system is deployed on top of GENI, the NSF-funded cloud federation [9]. Using CloudFinder, scientists may run their big data workloads on cloud federations with little or no new investment in computing and storage capacity.

1.1 Paper Organization This paper is organized as follows. We first present the rationale for using volunteer cloud computing to process big data workloads. In Section 3, we present a literature survey where we focus on research in two areas, namely, (i) cloud federations and (ii) volunteer computing in the clouds. In Section 4, we give the details of the CloudFinder system.

In Section 5, we present our experimental evaluation of CloudFinder. In Section 6, we conclude the paper and discuss our ongoing and future work related to CloudFinder.

2 RATIONALE FOR USING VOLUNTEER CLOUD COMPUTING FOR BIG DATA PROCESSING Big data may be defined as both structured and unstruc- tured data that is so large and complex that it requires a pro- cessing capacity not available using conventional database systems or traditional data processing software. Big data is often defined in terms of the three Vs: volume, velocity (rate at which data arrives and speed at which it must be processed), and variety (heterogeneity of data types, representation, and semantic interpretation.)  Typically, big data solutions such as Hadoop [10] / MapReduce [11] achieve reasonable response time by dis- tributing data and processing over many computing nodes and having each node process its local data partition in- dependently. A subsequent data processing phase may be needed to aggregate the partial results of the independent computations and generate the final output of the given application. This big data processing model inherently re- quires a multi-computer parallel/distributed system (e. g., a HPC cluster) to support the concurrent processing of several independent data partitions. A major problem is that  1. https://boinc.berkeley.edu 2. https://research.cs.wisc.edu/htcondor  researchers in big data science often may not have access to data processing capacity able to cope with the requirements of their big data applications.

Cloud computing is an efficient, cost effective alternative that quickly provisions computing capacity adequate for big data processing. Large research institutions now increas- ingly opt for using commercial clouds to run their big data applications. For example, to streamline the processing of satellite images, NASA JPL developed an application that relies on Amazon Web Services (AWS) and uses Polyphony [12], which is a modular workflow orchestration framework designed to streamline the process of leveraging hundreds of nodes on Amazon EC2. In another case, NASA?s Mars Sci- ence Laboratory used Polyphony to process nearly 200,000 Cassin images within a few hours on Amazon Web Services.

Technically, any scientist is able to use commercial clouds to quickly acquire the computing capacity necessary for his/her big data research. High costs, however, remain a major hindrance. For example, in a 2012 cancer research project conducted jointly between Nimbus Discovery and Schro?dinger, the two computational chemistry companies opted to build a 50,000-core supercomputer on the Amazon Elastic Compute Cloud that ran for three hours at a cost of $4,829 per hour [13]. Such amounts are often prohibitive for most universities and research labs where scientists some- times need to run complex models for days or even weeks.

This has been a fundamental barrier that prevents many scientists from conducting big data research. Providing cost- effective approaches for big data management will make it much easier for scientists to engage in big data research which, in turn, will translate into a tremendous impact in terms of scientific discoveries.

2.1 Problem Statement and Approach Overview  As mentioned earlier, big data processing often requires partitioning large data sets into several partitions that are then distributed to several computing nodes that operate on these partitions in parallel. This model assumes that the big data application is to be run on a (generic) distributed computing cluster that already exists. Moreover, program- ming frameworks for big data processing usually assume that the data have already been loaded on the nodes of the computing cluster. As a result, current big data processing frameworks are inherently unable to exploit two important optimization opportunities: (i) forming the computing clus- ter to best suit the features of the given application, and (ii) interleaving the processes of cluster formation, data loading and data processing. We take an orthogonal approach and ask the following two questions: (i) can a computing clus- ter be dynamically composed from volunteered, federated cloud-based resources to run a specific big data workload?

and (ii) if the answer is yes, how can we speed up big data workloads by interleaving the processes of cluster formation, data loading and data processing?

CloudFinder aims at answering the two previous ques- tions. For this, we model the execution of a big data appli- cation as a workflow with three tasks: (i) building a com- puting cluster to support the execution of the application, (ii) loading the input data to the cluster?s nodes, and (iii) running the application on the cluster. The problem then    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2703830, IEEE Transactions on Big Data   becomes to determine the fastest execution of this workflow.

The core of CloudFinder is an optimization algorithm that quickly computes an efficient solution to the following problems: (1) Given a cloud federation (e.g., GENI) that provides access to a large number of computing cluster configurations (number and power of nodes, their proximity to data source(s), their geographic locations, pairwise inter- node bandwidths, etc.) and given a big data workload, compute a cluster that would yield the best performance and (2) find the exact choreography that must take place to achieve that performance level, i.e., how the three tasks of cluster formation, data loading, and processing must be interleaved to achieve the best performance.

To build CloudFinder, we developed an optimization framework that produces the required elements to the so- lutions to the previous two problems, namely:  ? Optimal Cluster Dimensioning: For a given big data workload, the proposed optimization framework first determines the best cluster size (number of nodes) and topology (locations of nodes).

? Determining the Best Choreography: The three tasks of cluster formation, data loading, and data processing may be interleaved in a large number of ways. For example, one scheme could be to start a first node, start transferring data to that first node while start- ing the second node, start transferring data to that second node while starting the third node, etc. Each interleaving alternative will yield different perfor- mance figures. The ultimate goal of CloudFinder is to determine which of these alternatives is the best.

3 LITERATURE SURVEY CloudFinder is a system for big data workload placement on volunteer cloud federations. VCFs are environments that combine the unique and challenging characteristics of both cloud federations and volunteer computing [14], [15]. In this section, we discuss recent research in both of these two areas:  3.1 Cloud Federations To many in the field, cloud federations are the future of cloud computing [16], [17]. They make it possible to combine resources from several clouds to build a virtual cloud with a larger pool of computing and storage resources [18]. Cloud federations are typically introduced as a means to address the economic problems of vendor lock-in and provider integration. In addition, they are an alternative to reduce cost (e.g., through partial outsourcing to more cost- effective regions) and improve performance and disaster re- covery through methods such as co-location and geographic distribution [19], [20], [21]. Research enabling the execution of applications on multiple clouds ranges from work with the simple objective of running a given application using resources on multiple clouds (e.g., MODAClouds [22], [23], [24]) to much more ambitious initiatives aiming at building actual cloud federations.

One of the early projects illustrating the latter trend (i.e., aiming at building actual cloud federations) is RESERVOIR (?Resources and Services Virtualization without Barriers?),  a European Union/IBM cloud computing research project that aims at enabling massive-scale deployment and man- agement of complex IT services both in a computing cloud and across clouds under different administrative domains, IT platforms and geographies [25], [26], [27]. Among other aspects, the project focuses on federations of clouds across vendor boundaries to seamlessly allow the expansion of existing capacity. In [28], the authors present a layered cloud service model of software (SaaS), platform (PaaS), and infrastructure (IaaS) that leverages multiple independent clouds by creating a federation among the providers. The proposed approach is similar to the well-established Open Systems Interconnection (OSI) protocol stack model where different protocols are used at each layer to implement a concrete functionality. Communication occurs between two identical layers or between consecutive layers. In the context of the cloud, the layers (i.e., building blocks of the federa- tion) are SaaS, PaaS and IaaS. In [18], the authors present an architecture where inter-cloud interaction is handeled by a cross-cloud federation manager that includes three agents: Discovery, Matchmaking, and Authentication. In [29], the authors propose a federated cloud management architecture that provides unified access to a federated cloud that ag- gregates multiple heterogeneous IaaS cloud providers. The architecture includes cloud brokers that manage the number and the location of the VMs performing the user?s requests.

In [30], the authors define the Intercloud Federation Frame- work (ICFF) which attempts to address the interoperability and integration issues in provisioning on-demand multi- provider, multi-domain heterogeneous cloud infrastructure services.

In many cases, businesses use cloud federations to im- prove the scalability and availability of applications. In [31], the authors present some architectural elements of Inter- Cloud, a federated cloud computing environment where several instances of an application are deployed on mul- tiple data centers at different geographical locations. The objective is to support dynamic expansion or contraction of capabilities (VMs, services, storage, and database) for handling sudden variations in service demands while main- taining reasonable QoS levels. In [19], the authors focus on how cloud federations can improve the availability of a specific class of applications, namely, batch processing (OLAP) applications. They suggest three possible strategies: (i) redundant deployment (RD), (ii) redundant computation (RC), and (iii) parallel computation (PC). In redundant deployment, the same application logic is deployed with different (usually geographically distributed) providers and requests for service are routed to one of the providers.

In redundant computation, the same application logic is deployed to different clouds. In contrast to RD, though, every request is forwarded to all clouds. Two sub-strategies may then be used. RC with Comparison (RCC) waits for all results and compares them before returning a final result to the client. RC without Comparison (RCwC) returns the first result. In the third federation strategy (PC), the same or very similar application logic is deployed to different providers.

Incoming requests are broken down at bit level so that each cloud processes only a subset of the data. If subsets overlap, PC also allows validation of correctness of the answers like in RCC.

2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2703830, IEEE Transactions on Big Data   In parallel to the research on federating computing resources, researchers also considered federating storage resources. For example, MetaStorage [32] is a federated cloud storage system that can integrate diverse cloud stor- age providers. In [33], the authors present MetaCDN, a system that exploits Storage Cloud resources, creating an integrated overlay network that provides a low cost, high performance Content Delivery Network (CDN) for content creators. MetaCDN removes the complexity of dealing with multiple storage providers by matching and placing users content onto one or many storage providers based on their quality of service, coverage and budget preferences.

3.2 Volunteer Computing in the Clouds  For most researchers in big data science, computing, net- working, and storage resources available on-premises are often insufficient to conduct large scale experiments. Volun- teer computing was introduced to address this problem. It started concretely in January 1996 with the Great Internet Mersenne Prime Search (GIMPS) project. Since then, sub- stantial theoretical and practical research has focused on several aspects of the volunteer computing paradigm. For almost two decades, the term volunteer computing has been synonymous to a distributed computing paradigm where volunteers donate computing, storage, or networking re- sources to support the execution of compute/data-intensive applications. This led to the emergence of many ?commu- nity? testbeds, e.g., Emulab [34], PlanetLab [35], SURAGrid [36], GENI [9], FutureGrid [37], and Open Science Grid (OSG) [38]. These testbeds have been instrumental in en- abling both small and large scale experiments in computing and networking.

With the emergence of cloud computing, a number of researchers have recently started exploring the idea of extending the concept of volunteer computing to the realm of clouds. In [39], the authors present Cloud@Home, a volunteer computing environment built on top of the RESERVOIR framework. They consider a cloud built on off- the-shelf, independent, network-connected resources and devices owned and managed by different users. These users can both sell and/or buy their resources to/from cloud providers or, alternatively, they can share them with other users establishing open interoperable clouds. In [40], the authors present a model for cooperative cloud comput- ing among research institutions and universities using the Virtual Cloud concept [41] which is based on the idea of renting out already rented resources. The proposed model aims at virtualizing an already virtualized infrastructure.

For this, cloud vendors offer low cost cloud services by acquiring underutilized resources from other bigger third party providers.

While some recent research did consider the idea of com- bining volunteer computing and cloud federations, most of the existing literature still revolves around studying the feasibility and viability of volunteer cloud federations and does not contribute models, architectures, resource manage- ment techniques, or programming frameworks for actual VCFs. For example, in [1], the authors discuss the important issue of unreliability and unpredictability of volunteered resources in the context of VCFs. These characteristics make  it difficult to use VCFs to support applications with Service Level Agreements (SLAs). The authors propose a method- ology that addresses the issues of unreliability and unpre- dictability such that cloud software services could be hosted upon volunteered resources. To enable the harnessing of these resources, they rely on autonomic fault management techniques that allow such systems to independently adapt the resources they use based upon their perception of indi- vidual resource reliability.

4 THE CloudFinder SYSTEM  In this section, we describe the CloudFinder System. We first give an overview of GENI, the cloud federation used in CloudFinder. Then, we describe CloudFinder?s architecture.

Next, we present an optimization approach to perform effi- cient placement of big data workloads. Finally, we discuss typical usage scenarios of CloudFinder.

4.1 An Overview of GENI  CloudFinder currently operates on GENI (Global Environ- ment for Network Innovations), an international federated cloud that is available for researchers and students without charge [9]. GENI consists of multiple aggregates. These ag- gregates are defined locations where compute resources are available. Each of these aggregates is managed by an inde- pendent program that regulates how users obtain access to aggregate resources. Volunteers can donate resources to be used as GENI aggregate locations by advertising resources via the GENI API. Permission is required before doing so and machines must meet several requirements (specified in [42].) Once permission has been granted by the GENI managers, it becomes possible to advertise resources on the GENI system.

GENI?s API is automatically updated with added re- sources, and the API allows for other users to request resources and utilize advertised machines. Users request resources via the GENI API using RSpec files. This request is sent to the aggregates which then process the request and create space for the user on that system. If the aggregate finds that the current hardware capacity has been reached, it will send a failure status to the user requesting the resources at the given aggregate location. These resources expire after a set amount of time which the user can renew if they still require the resources at that aggregate.

It is possible to link multiple aggregate resources to- gether using a GENI sub-network. This GENI sub-network links all of the GENI aggregate resources together and provides the networking through which aggregates com- municate. Aggregates are also connected to the Internet and requests to individual aggregates are served using this connection.

There are different types of GENI resources, called racks, which differentiate the type of services that can be expected from the resources. Information on these racks can be found in [43]. GENI allows users to access various types of publicly available resources and it is also possible to contribute resources to the GENI federated cloud provided that those resources meet GENI?s requirements.

2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2703830, IEEE Transactions on Big Data   Fig. 1. CloudFinder?s Architecture  4.2 System Architecture  CloudFinder?s architecture (Figure 1) consists of three ma- jor components: (i) the job submission system, (ii) the job distribution optimization engine, and (iii) the execution controller. Each of these subsystems plays a key role in successfully running a big data workload.

Job Submission System  The job submission system is the entry point for the user.

Currently, this system?s interface is a command line that enables users to provide the system with a Hadoop jar file for program execution as well as a data directory. The information that the user enters here is the task they wish to execute on the federated cloud. Continuing with the user?s task submission would lead to an analysis of their input program and data. Currently, the analysis only takes into account the size of the input data. Once this analysis is complete, information pertaining to the input data is stored and passed on to the job distribution optimization engine.

Job Distribution Optimization Engine  This is the major backbone of the system. This subsystem performs all the necessary calculations required to create an execution plan which is then relayed to the execution controller. The job analysis information obtained from the previous step is used to create an execution plan. This execu- tion plan describes a set of systems and connections between those systems that will optimally execute the user defined task. In order to create this optimized execution plan, the distribution optimizer uses the analysis of the program that will be executed as generated by the submission subsystem.

The analysis made by the submission subsystem is based on the location (IP of the server) of the input data as well as other user-provided elements such as the nature of the job (e.g., compute-intensive, data-intensive), the estimated size of the output (e.g., as a percentage of the input), etc. For example, if the output is expected to be large, the system will give preference to sites close to the user to reduce the networking cost at the end of the computation. In addition, the distribution optimizer uses information on available  machines (obtained from a data file that regularly polls for resource information) and the current execution states of other processes running on the available machines.

Once a defined topology has been created an RSpec file detailing the topology set-up is created, and written to disk.

This RSpec file provides the execution controller with the information required to physically execute the task.

Execution Controller  Once a topology has been described in an RSpec format, the execution controller starts. The execution controller sub- system handles the distribution of the data, reserving the machines, and running the processes. If an error occurs dur- ing this stage (e.g., machine unavailable, system down), then the execution controller re-writes the available machine data file and restarts the job distribution optimization engine subsystem. If all the machines requested are available, then it submits resource requests to servers based on the RSpec file descriptions. The job is then run according to the orders given by the job distribution optimization engine. Once the task is complete, the results are read back into the system and stored in a file system at a location accessible to the user.

Currently, GENI is the only operable federated cloud used in the CloudFinder. A few other systems including Chameleon [44] and CloudLab [45] are still in development and we plan to include them in our system once they are better documented and available for use. The end goal of our work is to provide a simple unified interface for scien- tists and researchers that can be used to take advantage of the processing power of these cloud systems without having to deal with the hassle of reserving resources and running jobs on their own. Adding new machines to any of these systems would, in turn, add potential resources available to CloudFinder. Adding new clouds to the system would be as simple as updating the list of available machines by adding additional locations with which to poll for resources.

More modules can be added if these new systems require different interfaces for resource reservation. CloudFinder also aims at optimizing the usage of the clouds themselves    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2703830, IEEE Transactions on Big Data   by optimizing on processor usage and improving energy efficiency.

4.3 Optimization Approach  An important part of CloudFinder is the job distribution opti- mizer and the way that jobs are optimally placed. There are many variables at play when determining where to locate a task in a cloud environment. If the end goal is solely to minimize execution time, then possible factors could be: fastest processors, most memory, least co-located VM interference, etc. If the goal is to minimize energy usage, then possible factors could be processor utilization, disk I/O requirements, memory swaps, co-located VM interference, etc.

To perform an optimal workload placement, we look at three main factors, data transfer time, available machines at a specified aggregate, and the physical hardware perfor- mance. We first obtain normalized values from aggregate locations for these parameters. We then calculate a weighted average such that more weight is given to the values that are more crucial to the workload at hand. The resulting model gives us a method to calculate the most optimal aggregate on which the current workload is to be executed.

That aggregate is a solution to the equation:  Ag = argmax i  {Wi.Xi} (1)  Where Xi and Wi are vectors for aggregate i that contain the following values:  ? Xi = (Mi, Bi, Pi) is a vector that contains normal- ized values representing the three factors discussed above:  ? Mi represents the normalized value of ma- chine availability. It is calculated by dividing the total number of slots potentially available on an aggregate by the number of slots cur- rently taken. Having a large value shows that the cloud is not busy. For example, if there are 165 slots maximum on a given cloud, and currently 53 are in use, then there is a ratio of 165/53 slots in use. This value is calculated for each aggregate and then normalized with respect to the aggregate with the largest ratio.

? Bi represents the normalized value for data transfer rate. It is calculated by finding the download rate at a given aggregate. Since the GENI API does not make that information readily available, a calculated data transfer rate is determined by downloading a 1 GB file and calculating the time taken to download the file. This gives us the download rate for aggregate i. This value is then normalized with respect to the aggregate with the largest download speed.

? Pi represents the normalized value for proces- sor performance. It is calculated by finding the processor performance of the hardware at a given aggregate. This value is in GHz and rep- resents the computations a given processor can  do per second. For each aggregate, this value is normalized with respect to the aggregate with the largest processor performance.

? Wi = (?i, ?i, ?i) is a vector that contains weighted values representing an estimation of how important each of the three factors is to the completion of the provided task. Determining how important a given factor is to a program?s execution is no easy task but it is possible to estimate the importance of these factors by taking a few things into account. Each of the values are divided by their sum, which creates a weighted average for use in the model given above:  ? ?i represents the amount of time spent per- forming computations. Generally this value is also dependent upon the data size itself as well as the nature of the program that will be run.

This value requires additional input from the user, and this input is the approximate time complexity of the program with respect to the size of the data in bytes. Using this time com- plexity, we use the input data size to calculate a rough estimation of the complexity and then divide by the speed (in GHz) of that given aggregate?s processor. This value provides us a simplistic measurement of the time that will be required to physically execute the input task on the data provided.

? ?i represents the amount of time spent contest- ing resources. This value is the most compli- cated to estimate as finding a way to directly relate execution time with machine availability is no easy task. To simplify the calculation of this value, we assume all resources are contested equally. This means that on a given aggregate, processor usage, memory usage, I/O usage, etc. are all equally contested. As a result, we can just utilize our processor per- formance value ?i to estimate total machine resource usage. Taking ?i, we can multiply this value by the number of slots in use on the considered aggregate to estimate the time taken contesting resources. Hypothetically, ev- ery slot taken can use up ?i processing time before our task completes (each slotted ma- chine gets 1 second of compute time, which means for x slotted machines, we get 1 second of compute time every x seconds.) So, if we multiply our calculated processing time by the number of sloted machines, we get an esti- mate of the time spent contesting resources.

Of course, this assumes there is only one pro- cessor available, but without more detailed information from the hypervisor itself, coming up with a more detailed representation would be difficult.

? ?i represents the amount of time spent trans- ferring data. This value is fairly easy to calcu- late, and is directly related to the download speed of the target aggregate location. First the size of the data that will be transferred is    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2703830, IEEE Transactions on Big Data   calculated. Then, that value is divided by the download rate for a given aggregate site. This gives the amount of time it will take to send the data to the destination. The larger the data size, the more important it will be to minimize the data transfer time, especially if the other two factors are low.

This model chooses which aggregate to execute the input task on using the values calculated above for each aggregate.

The aggregate with the largest Ag value is the aggregate chosen to execute the provided task on.

We tested this model using three aggregates at Texas A&M, University of Central Florida (UCF), and Oakland Scientific Facility (OSF). The goal was to see if the model would select Texas A&M as the best aggregate to execute a big data workload from a site at New Mexico Tech.

When executing the above model using Terasort as the input application, 25 gigabytes of data and 8 worker nodes, Texas A&M was indeed selected as the aggregate to execute the task on. This shows that the model works for our limited aggregate test suite for the most variable data size input.

This was also validated by subsequent testing (presented in the next section) as Texas A&M had the fastest program completion time for the 25GB Terasort execution. We plan to further test this model on a wider set of clouds and refine the model to become applicable to more generic contexts.

4.4 System Usage Scenarios The goal of CloudFinder is to provide a unified interface that leverages multiple clouds for their resources. Ideally, it should be possible for any researcher with a big data code and data to submit it to CloudFinder with minimal hassle and get results back in a timely manner. Of course, individual restrictions based on the cloud service agreements are non- withholding but it should be possible to leverage multiple clouds to execute big data tasks larger than possible on a single cloud.

One potential use case would be for a bioinformatics experiment. Say that a user has written a big data program that analyzes genetic sequences stored in a database and the user wants to run this program using a cloud server.

With CloudFinder, they could upload the program along with the data file they want to use (either a link to the database or a different kind of data mapping) and using a recommended amount of resources, CloudFinder would optimally select a topology to execute their program on. This would require minimal effort on the part of the researcher because they would not have to provide disk images for execution environments, nor would they have to physically set up the machine connections. Using CloudFinder would make the turn-around time for experimental results much shorter.

Another use case would be for a long term physics experiment that analyzes astronomical data as it comes in from a source. Using CloudFinder, the programmer can provide a link to the data that is constantly being updated.

We can then read the incoming streams and store them to a specified location or on the servers we have reserved for usage in this experiment. The program provided would either be running all the time, or the user could upload a  script that sets a flag that triggers program execution. A potential upgrade for CloudFinder would be to allow the user to define time intervals in which the program should be re-run after a certain point. This would be useful for long-term experiments with constantly updating data, such as astronomy experiments. This would allow the researcher to spend more time analyzing the data output rather than fiddling with server set-up and data distribution.

Another example use case would be for a very compli- cated analysis of a large data-set. The analysis of this data set would require a lot of computational power and might take days to finish execution. In order to run this experiment, resources from multiple aggregates have to work in conjunc- tion. The set-up of this system would be complicated and the coordination required from these resources would not be trivial. CloudFinder would greatly reduce the complexity of this job by simply asking the user for a program input and a data input. CloudFinder would handle the data distribution, as well as the processing distribution. We can also observe agents that fail during execution and re-evaluate where to place new aggregate processes so that progress continues to be made in processing the application. These are all things offered by various cloud services such as Amazon.

CloudFinder has similar features but on federated cloud systems while maintaining energy efficiency and processor performance.

CloudFinder could also help computer scientists as well.

For example, a computer scientist may want to test the execution times of their MapRduce program on this kind of network configuration. They can just upload their program and their data, describe their configuration (RSpec file, some kind of GUI, or a text-based system) and CloudFinder would deploy it and give them their results back. This would remove the added steps of physically logging into the machines and executing the tasks. In addition, CloudFinder could optimize on various parameters. For example, if the scientist wants to test their code under a high I/O work- load environment,CloudFinder will determine where to place their VMs so that it best emulates a high I/O environment.

Overall, the goal of CloudFinder is to make the lives of researchers easier by abstracting the process more and providing optimizations to their program performance as well as the overall system performance. The system also leverages multiple federated clouds at once which provides more computational power for researchers.

5 EXPERIMENTS AND ANALYSIS To evaluate the performance of CloudFinder, we performed a number of experiments using the HiBench system. HiBench is a benchmarking system that gives users access to various programs that generate big data workloads [46]. To test our system, we used two big data workloads: one non-iterative (the ?standard? Hadoop Terasort) and one iterative (PageR- ank). We ran our experiments on several GENI aggregates of variable sizes. The Terasort benchmark used in HiBench is the same as the Hadoop Terasort benchmark used by many vendors to determine the computing capabilities of various Hadoop clusters [47], [48]. The PageRank bench- mark implements the algorithm provided by Hadoop using data generated from Web data whose hyperlinks follow the    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2703830, IEEE Transactions on Big Data   Fig. 2. TeraSort Benchmark Performance  Zipf distribution [46]. Together, these benchmarks provide good information to analyze the run-time characteristics of various virtual cluster topologies.

We performed experiments with different Hadoop worker node counts at various GENI aggregate locations.

In Hadoop, there is a master node that handles data distri- bution and workflow, and worker nodes that handle com- putation. Adding more worker nodes should theoretically increase the throughput of a given program which would reduce the execution time. However, adding more worker nodes also increases the amount of communication between the master and workers and simultaneously increases the amount of I/O operations performed. This can lead to a decrease in overall performance depending on the nature of the system. GENI aggregates are clusters of compute resources available at various geographical locations around the world. Each of these aggregate locations is considered a cloud since it allows users to create virtual instances of machines and utilize shared resources. At a given time, it may be possible that many researchers are utilizing the compute resources available at a given aggregate. This may create contention for physical resources. This contention may, in turn, have a negative effect on the performance of applications.

The goal of our experiments is to show that performing the same task at different locations of the cloud federation (with the same virtual resources) will display differences in run-time behavior. This difference in behavior will result from differences in physical hardware as well as the state of contention over physical resources.

5.1 Terasort Benchmark Figure 2 shows the results of executing the Terasort bench- mark at the three GENI aggregate locations: Texas A&M, UCF, and OSF using data sets of size 1GB, 10GB, and 25GB.

These data sets were generated using Teragen and were distributed to the virtual cluster?s Hadoop file system.

From the figures, we can see that the larger the data size, the more inconsistent execution times become between  different aggregate locations. This is probably due to the variability of physical resource contention at each of these aggregate locations. Using 2 workers, the GENI aggregate resources located at Texas A&M finish Terasort execution roughly 40 seconds (15%) faster in comparison to the other aggregate locations. In fact, Terasort, when run on the Texas A&M GENI aggregate, finished execution roughly 40 sec- onds faster independent of the number of workers. A reason that this variability is more visible for larger data sizes is the amount of transfer operations required. When the data size increases significantly, it requires more I/O operations to move that data from disk to memory and across the network to the correct worker locations. This creates more traffic over the network and, in turn, can receive more interference from other virtual processes running at the same time. For smaller data sizes, this interference is negligible but, for large data sizes, it causes a significant reduction in throughput. It is also for this reason that adding more workers does little to reduce the execution time for these data sizes. Adding more workers increases the network traffic during execu- tion. On a busy virtual system, this can cause a drop in throughput because the resources are simply not available when requested. In Figure 2, we can see that, for the largest data size, using 8 workers instead of 4, did not decrease the execution time as much as the change from 2 to 4 did. The most important piece of information to gleam from Figure 2 is that executing the same program at different aggregate locations did have a significant impact on the performance of the program.

Figure 3 shows the data transfer time of sending the data to the various aggregate locations. Of course, sending more data will take more time but the location that data is sent to is important as well. For the data sizes used in the experiment, it is possible to see a difference in the time taken to send the data to a GENI aggregate. For these data sizes the difference is in 10s of seconds but, for very large data sets, the difference could be in thousands of seconds which can have a large impact on the completion time of the submitted jobs. Analyzing the download rates of the    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2703830, IEEE Transactions on Big Data   Fig. 3. Data Transfer Time  various GENI aggregates will be an important factor to take into account depending on the size of the data.

Figure 4 shows the difference in execution time at the OSF GENI aggregate for different virtual machine map- pings. All the previous executions used a XOMedium vir- tual hardware specification which consisted of 1 CPU core, 3 GB of RAM, and 25GB of disk space. The test run in Figure 4 uses 9 XOMedium machines in one configuration, and 1 XOMedium, 4 XOSmall, and 4 XOLarge machines in the other. XOSmall machines utilize 1 CPU core, 1GB of RAM, and 10GB of disk space while XOLarge machines use 2 CPU cores, 6GB of RAM, and 60GB of disk space. Looking at the data from the experiments, we can see that the execution times are relatively similar for the smaller data sizes but, for the 25GB data size, the varied configuration performs around 10% better. This shows that selecting the machine types utilized in an experiment can have a large impact on the execution characteristics of a given big data workload when run on a virtual cluster. This improved performance can be attributed to the enhanced processing power of the large machines which can process the larger data set more efficiently than the 8 medium machines.

5.2 PageRank Benchmark  Figure 5 shows the results of executing the PageRank bench- mark at the three GENI aggregate locations: UCF, OSF, and UH using data sets of size 1GB, 1.3GB, and 1.5GB. The Texas A&M aggregate location was down at the time of PageRank testing so we replaced it by an aggregate at the University of Houston (UH). These data sets were generated using HiBench?s PageRank data generation algorithms and were distributed to the virtual cluster?s Hadoop file system.

The execution patterns for the PageRank benchmark are fairly similar to the patterns exhibited by the Terasort results for the cases of 2 and 4 workers. In those cases, there is an  Fig. 4. Terasort Using Varied VM Sizes  identifiable aggregate that performs better than the other aggregates for most data sizes and that is the UH aggregate.

The outliers for this case are the 2-worker 1GB data set time and the 4-worker 1.3GB data set time. In those cases, the interference posed by the other processes on the system impeded the progress of our task since the MapReduce out- put logs showed no irregularities with the execution of the different processes. In the test case that utilized 8 workers, it is possible to see some significant changes to the previous pattern. The UH aggregate execution time decreased by roughly 20% when increasing the data size from 1GB to 1.3GB. While it is possible that this result might have been from an over-mapping of data to nodes, the results from the other aggregates show that this is most likely not the case.

The aggregates at UFL and OSF showed standard execution patterns where execution time increased when increasing data size. This means that some unknown cause resulted in the UH aggregate execution time decreasing. Observations of the MapReduce output logs showed no irregularities in the execution patterns at UH when running these tests.

Some system components seem to have forced this change.

If other processes were running simultaneously with our application, it is possible that they may have caused some interference. This resource contention could have caused the significant drop in throughput seen by our application. Even more so than in the Terasort case, the PageRank executions showed major variability in throughput when executing on different aggregates at various locations. This variability can be attributed to the performance of individual machines and to traffic at the each aggregate.

Figure 6 shows the difference in execution time at the OSF GENI aggregate for different virtual machine mappings running the PageRank benchmark. Similar to the Terasort case, the PageRank algorithm experienced an improvement in throughput for the topology that utilized 4 large and 4 small virtual machines. Unlike the Terasort case, however, there was great improvement for even the smaller data test    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2703830, IEEE Transactions on Big Data   Fig. 5. PageRank Benchmark Performance  cases. This can primarily be attributed to the complex com- putational requirements of the PageRank algorithm. There is also more variability in the actual improvement between each test case. For the 1.3GB test case, the improvement is less significant than in the other two test cases. This can be attributed to increased network traffic at the time of the test for the 1.3GB test case. The CPU intensive PageRank algorithm is more affected by conflicting I/O operations and the sharing of processor time. This causes the execution  Fig. 6. PageRank Using Varied VM Sizes  patterns to exhibit more variability when the traffic of other processes at the aggregate increases. This experiment shows, in conjunction with the Terasort test, that different VM types can have a large impact on the execution patterns of a given big data workload.

After analyzing the data that resulted from the tests outlined above, we can see that selecting a specific ma- chine configuration at a certain aggregate location can result in a significantly variable throughput time. This variabil- ity is caused by many factors, namely aggregate location, virtual cluster configuration, hardware specifications, and hardware availability. Because of these factors, selecting an optimal location to run a given big data workload is difficult to do manually. The key contribution in CloudFinder is that it takes into account the above information to derive near optimal placement decisions for big data workloads on clouds in a cloud federation.

6 CONCLUSION AND FUTURE WORK 6.1 Conclusion  We designed and implemented CloudFinder, a system that computes efficient execution plans of big data workloads over volunteered cloud federations. The current version operates using the GENI cloud federation but it can be easily updated to operate on any other cloud federation or even on a ?constellation? of several cloud federations.

Our experiments indicate two key results. First, execution times of the same big data workload at different aggre- gate locations of a given federation may vary substantially.

Factors in these differences include significant hardware differences, resource over-subscription, and transfer times.

Second, CloudFinder is able to find efficient placements of big data workloads in large cloud federations.

6.2 Future Work  Volunteer cloud federations are inherently open computing infrastructures where, typically, arbitrary organizations do- nate cloud resources that are made available to users at arbitrary organizations. This obviously translates into two    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2703830, IEEE Transactions on Big Data   fundamental and complex challenges: security and scalabil- ity. Security is challenging because of the very open nature of the cloud federation. In principle, any organizations may join a cloud federation and any user must be able to access resources from the federation. From a security perspective, resource contributors are vulnerable to threats from other resource contributors and from users. Similarly, users are vulnerable to threats from other users and from resource contributors. The cloud federation must provide security mechanisms that protect all parties from all types of threats.

Scalability is also challenging in such a context because the cloud federation must be able to efficiently allocate a fluctuating capacity (computing and storage resources) located at a large number of geographically distant data centers. The cloud federation must be able to allocate this capacity to a large number of big data workloads originating at geographically distant locations. Moreover, the times at which resources are contributed to (or withdrawn from) the cloud federation and the amount of those resources are unknown a priori. The cloud federation must scale up when its capacity grows and gracefully scale down when its capacity shrinks.

In addition to the two aspects of security and scalability, we will focus in our future work on improving the current optimization approach used in CloudFinder. In particular, we will work on better determining the contribution of each of the factors discussed in Section 4.3 in the total execution time of a big data workload.

