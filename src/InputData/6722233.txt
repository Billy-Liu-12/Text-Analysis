MapReduce based Method for Big Data Semantic

Abstract?Big data analysis is very hot in cloud computing environments. How to automatically map heterogeneous data with the same semantics is one of the key problems in big data analysis. A big data clustering method based on the MapReduce framework is proposed in this paper. Big data are decomposed into many data chunks for parallel clustering, which is imple- mented by Ant Colony. Data elements are moved and clustered by ants according to the presented criterion. The proposed method is compared with the MapReduce framework based k- means clustering algorithm on a great amount of practical data.

Experimental results show that the proposal is much effective for big data clustering.

Index Terms?cloud computing; big data; MapReduce; Ant colony; k-means

I. INTRODUCTION  Big data clustering analysis is a hot topic at present, which  is of great importance in cloud computing applications. In  recent years, online data grows explosively according to IDC  Digital Universe Study Research Report, e.g., the volume of  data created or copied 2011 is 1.8ZB while it becomes 2.8ZB  in 2012. This number doubles every two years, in 2020 this  number is expected to be 40ZB. As an important research  field of data mining, text mining mainly solves problems  of document clustering with specific features and automatic  mapping of heterogeneous data with same semantic. In tradi-  tional data environment, many achievements have been got on  heterogeneous text semantic clustering: Stanford University?s  SKC project based on Ontology[1], which aims to solve the  interoperability issues of semantic heterogeneous data in infor-  mation system. The LSI (Latent Semantic Indexing) method  of Bellcore et al.[2][3] is mainly used for rapid association of  feature keywords with non-structured documents. Based on se-  mantic grid, the heterogeneous data mapping method proposed  by Mario and Domenico[4] realized the semantic association  and the constraint relationship description of structured data.

In big data environment[5], since the quantity of data increase  sharply and the data structure is more and more complex, if  existing data clustering algorithms are used to analyse the same  semantic heterogeneous data clustering in the condition of  such mass heterogeneous data, the time complexity and space  complexity will be too high. Therefore, issues like parallel  data mining algorithms, Web information mining and retrieval,  media analysis and retrieval, and natural language processing  have become hot issues of the ?big data? research presently.

For big data clustering analysis in cloud computing[6] dynamic  environment, based on the MapReduce[7] Parallel Framework,  Wang et al.[8] and Li et al.[9] put forward a parallel k-means  method for data analysis and text mining in online state, Das  et al.[10] proposed a minHash method for text mining in  online state, Liu et al. developed ANN method[11] for image  clustering.

At present, the main method for big data analysis is k-  means, whose advantage is high efficiency, however, the  number of the clustering centers must be preset and the  algorithm is quite sensitive to exceptions and the sequence  of input. Because the number of clustering centers remains  unchanged during the clustering once they are set, while the  number of clustering centers in different data block is difficult  to keep consistent, the accuracy of this method for data analysis  in big data state is not high. Based on the MapReduce Parallel  Framework, this paper proposed an Ant Colony Clustering  algorithm MBSC (Method for Big Data Semantic Clustering)  using Semantics for big heterogeneous data. This algorithm can  adaptively generate the number of clustering centers and imple-  ment automatic clustering mapping of big heterogeneous data  in same semantics by key phases parallelization. This paper  makes a comparison between MBSC and k-means clustering  algorithm in big data environment, whose experimental results  show that MBSC algorithm has better clustering performance  in such environment.



II. SEMANTIC CLUSTERING  The characteristics of data analysis in big data environment  are multiple sources, complex structure, and execution stan-  dards that is not unified, etc. Most metadata with the same  semantic which is analyzed and applied has different definition  or description forms in the source systems. For example, staff  ID is named variously in different management systems (such  as RYID, SFZH, and SFHM, etc.). The premise of data analysis  is to realize the automatic clustering mapping of heterogeneous  metadata. Metedata can be represented by several attribute  dimensions like data definitions, constraint conditions and  characteristic words, and different characteristics of attribute  dimensions can be described by characteristic values of specific  data. For non-structured text data, multiple keywords can be   DOI    DOI    DOI 10.1109/SMC.2013.480     established by frequency of used words, which can also be  used as the attribute dimension to describe the text. While for  structured data, it can be characterized by the attribute dimen-  sions like item names, type definitions, value ranges, record  characteristic values, etc. Although the heterogeneous data  with same semantic may have different definitions in one or  more attribute dimensions, but they usually have more similar  or even same definitions on other ones. Automatic clustering  mapping of heterogeneous data with same semantic can be  realized by the clustering analysis of attribute dimensions.

To illustrate the processes of clustering based on semantic,  structured data items are taken as an example, of which the  basic idea is as the following. Assume that there are N data  items from different data sources, each data item Bk has q  data records, and each data record is described by the attribute  dimension. The data records of item Bk can be denoted as  Bk = ?q i=1Xk,i, of which Xk,i = (Xk,i,1, Xk,i,2, ? ? ? , Xk,i,n)  (n is the number of attribute dimensions). The data records of  all data items are described by the same attribute dimensions,  so the total data records can be described as A = ?N k=1Bk. Let  A be the collection of data records, in which one is selected  randomly as the center, then calculate the comprehensive sim-  ilarity (swarm similarity follows defined) between the center  and the other records within its neighborhood radius area. If the  comprehensive similarity is greater than a given threshold ?1,  the center is regarded as in the same class with its surrounding  records; otherwise, if the comprehensive similarity is less than  the threshold ?1, they are regarded as in different classes, and  a next record is chosen according to certain strategy. After  several times of iterations, data records are clustered. Grouping  all data records in a class by data items, the clustering accuracy  rate (CAR) can be defined as the ratio of the number of records  of a data item and the number of its all records (e.g., if i records  of Bk are clustered into class C, the corresponding CAR is i q  ? 100] ). If the CAR is greater than a given threshold ?2, the data item is classified to class C. Semantic clustering of  heterogeneous data items is realized in this way.



III. METHOD FOR BIG DATA SEMANTIC CLUSTERING  Data clustering categories data into different clusters, whose  classes and the number of clusters are closely dependent on  related data. The accuracy of k-means clustering algorithm is  low due to its fixed number of clusters. Moreover, computing  time becomes a bottleneck of clustering algorithms in big data  environment because they are usually time-consuming. This  paper proposes an Ant Colony Clustering algorithm MBSC  (Method for Big Data Semantic Clustering) for heterogeneous  data using data semantics based on MapReduce, which is the  distributed parallel computing model of Google. The funda-  mental idea of the algorithm is to divide the data records to be  clustered into several data chunks firstly, then the independent  clustering computing in each data chunk and the parallel  computing among the chunks are carried out. The process of  clustering computing consists of two relatively independent  phases: Map and Reduce. Parallel computing is realized by  embedding the clustering method into multiple independent  MapReduce task models.

Since the data records to be clustered are divided into  several data chunks, each Map task is responsible for a data  chunk individually, and multiple Map tasks can realize parallel  clustering computing for all data chunks, the time of clustering  algorithm is greatly shorten. In each chunk, clustering is  processed by agent moving. Several agents in a Map task  individually complete the clustering moving of a chunk. Each  agent moves with carrying data records to be clustered, for  each step, certain strategies are used to determine whether to  drop the records (these data records belong to current class)  or keep moving. If a data record is dropped, the agent gets  a new record randomly and the current node is set as the  new start of the agent?s traversal path; otherwise, the agent  updates the length of the traversal path and the path nodes  set, then continues to move with the data record until the  record is dropped, or the traversal is complete, or the length  of the traversal path exceeds the reference value. When an  agent finishes its traversal and the record it carries has not  been dropped yet, the Reduce task compares and merges the  traversal pathes? lengths of all agents from all data chunks,  then obtaining the shortest length in current iteration, which  becomes the new reference value for clustering computing  in each Map task in the next iteration. The iterative process  continues after assigning the shortest length to each data  chunk until termination criterion is met. Since the clusters  and the number of cluster can be determined adaptively and  automatically based on real data, and the clustering center is  generated automatically in the iterative process instead of being  set in advance, this clustering algorithm is much more accurate.

A. Clustering based on Ant Colony algorithm  The clustering computing based on agents in each data  chunk is realized by Ant Colony algorithm[12] in which data  clustering is adaptively achieved by pheromone interaction  among ant individuals. The steps are listed as the following.

Put the data records to be clustered into an n-dimension space  where n is a predefined attribute dimension number. Each  record owns a random initial position and can move freely in  the n-dimension space under the impact of ant colony. During  the moving of a record, each ant carries the information of the  initial position in the space of the record, i.e. the values of at-  tribute dimensions of each data record. In each step of moving,  the swarm similarity of the record is evaluated in current local  space, which is transformed into the probability of dropping or  picking up the record by a probability transformation function.

The massive iterative moving of the ant colony finally makes  the data records with similar attributes form a cluster in the  same area.

Swarm similarity is a synthesis similarity of records in a  local environment. Assume that all the records to be clustered  are: A = {Xi|Xi = (xi1, xi2, . . . , xin)}(i is the total number of the records and n is the attribute dimension number). Let dij be the space distance between two records Xi and Xj which     is calculated by cosine distance:  dij =  ?n w=1 xiw ? xjw??n  w=1 x iw  ??n w=1 x  jw  (1)  Multi-dimension space with center Xi and radius r can be  denoted as N(Xi, r). Suppose that the swarm similarity factor is a, then the swarm similarity is calculated as:  f(Xi) = ?  Xj?N(Xi,r)  [1? dij  a ] (2)  The moving direction of a record Xi is determined by the  value of selection probability function based on the swarm  similarity. Assume that ?, ? are control parameters, and ?ij(t) is pheromone amount on the path from Xi to Xj at time t  where ?ij(0) = 1. The probability of merging Xi into the neighbor of Xj is calculated as:  pij(t) = ??ij(t)f  ? t (Xi)?  s?N(Xj ,r) ??sj(t)f  ? t (Xs)  (3)  The probability threshold is set to p0. The record is dropped  and merged when pij(t) ? p0, while if pij(t) < p0, the ant continues to hold Xi and move to the next node Xj with the  largest value pij(t).

B. MBSC parallel computing  The parallel computing among data chunks in MBSC com-  puting is realized by MapReduce model. The records (feature  words of non-structured text, data items in structured data and  so on) in data source are input by the Map phase into Map  function in the format of key/value pairs generating one or  more intermediate results and corresponding key values. An  independentMap phase is called aMap task. The Reduce phase  simplifies the intermediate results into final result key/value  pairs with one or more keys. Let the data records to be  clustered be divided into M data chunks, thus the number of  sub-colonies and Map tasks are also M . Am denotes a sub-  chunk meeting A = ?M m=1Am,m = 1, 2, ? ? ? ,M . The basic  progresses of MapReduce model are:  In a Map task, selection probability computing and path  length and nodes updating are processed in a parallel manner  during solution construction in sub-colonies. In each inde-  pendent Map task, Xmi denotes a data record in data chunk  Am. Assume that gmax is the maximum iteration times, then  ?mimj (g) is the pheromone value on path Xmi to Xmj after g generations with initial value 1. Meanwhile, suppose that d is the traversal path length with the data record it carries without  being dropped, dg is the shortest traversal path of all ants with  data records not being dropped in current iteration after the  gth generation, which is the initial comparing value for next  iteration. s is the edge set composed of the nodes traversed,  and sg is the edge set composed of the nodes corresponding  to dg . Map function takes (d, s) as the input key/value pair where d0 is ? initially. The description of Map function is as follows:  Algorithm 1: Map (d, s)  1 for g = 1, gmax ? 1 do 2 if g = 1 then 3 ?mimj (g ? 1) = 1; 4 else  5 Read pheromone value ?mimj (g ? 1); 6 Calculate swarm similarity by Equation (2);  7 Calculate selection probability by Equation (3);  8 if pmimj (t) ? p0 then 9 Xmi is dropped and merged to Xmj neighbor;  10 else if pmimj (t) < p0 and d ? dg?1 then 11 Xmi is seen as isolated point and dropped;  12 else if pmimj (t) < p0 and d < dg?1 then 13 Select the node with larger probability into s;  14 d = d+ dij ; 15 else  16 Traversal finishes, an ant?s current solution d and  solution set s are generated;  17 return  Reduce task takes the output of Map function as its  (key/value)input, compares and generates the optimal solution  and relative solution set based on the current solutions and  solution sets obtained by all ants from different data chunks  after one iteration, then updates the global information of path  pheromone providing the comparing parameter for the Map  task of next iteration. The parallel computing in Reduce task  improves the computing efficiency of current solution merging  as well as pheromone updating of every data chunk. The  algorithm of pheromone updating is:  ?mimj (g) = (1? ?) ? ?mimj (g ? 1) + ?e (4)  ? ? (0, 1] denotes the evaporating rate of pheromone. ?e is the pheromone intensity left by an ant passing by. It equals to 1  if an ant passes by, otherwise equals to 0. After each iteration,  the pheromone on the path passed by the ant with the shortest  traversal path increases, and the pheromone on other paths  decreases. The Reduce function is described as follows:  Algorithm 2: Reduce (d, s)  1 if d < dg?1 then  2 dg?1 = d,sg?1 = s; 3 Get the minimum dg?1 and its corresponding sg?1 = s,  output as global optimal solution (dg, sg) for current iteration;  4 Update shortest path pheromone ?mimj (g) according to Equation (4);  5 return  C. Algorithm Description  This paper adopts vector space model[13] which is widely  used in semantic search to describe the data attribute dimension     information. The vector representation of a data record Xmi is  Xmi = (xm,1, xm,2, . . . , xm,n), in which n is the dimension number and xmiw(w = 1, 2, . . . , n) is the normalized value on dimension w. The main procedures are as follows:  Algorithm 3: Main Procedure of MBSC  1 Initialize parameters m, antm, ?, ?, ?, a and p0, among  which antm is number of ants in task m;  2 Partition data chunks, generate sub-colonies and build  MapReduce task model;  3 Randomly disperse the records in data chunks and assign  normal initial value for each record;  4 Assign initial data record for each ant in each  sub-colony, start Map task;  5 for each Map task do  6 Take the coordinate of the initial record of each ant  as center and r is observation radius with respect to  the current coordinate. Calculate the record?s swarm  similarity around the range of r using Equation (2);  7 Calculate selection probability with pmimj (g) Equation (3);  8 if pmimj (g) > p0 then 9 The ant drops current data record and merge,  randomly assign a new record for the ant and  repeat 6 and 7;  10 else  11 Calculate the traversal length of the ant l;  12 if l < dg?1 then  13 The ant carries the record and continues to  move;  14 Repeat 6 and 7;  15 else  16 The record the ant carries is isolated and  dropped;  17 A new record is assigned to the ant and  repeat 6 and 7;  18 Execute Reduce task;  19 if the termination criterion is met then  20 Stop the algorithm and output the cluster number of  all the data and corresponding center value;  21 else  22 Calculate the optimal solution of current iteration  and update the pheromone;  23 Go to Line 5 and continue to iterate until outputting  the final result;  24 return.

In conclusion, Line 1-4 are the initialization procedure  in which the data chunks are divided to sub-colonies and  the MapReduce model is built. Line 5-22 is the clustering  procedure and Line 23-29 generates the clustering center.



IV. EXPERIMENTAL SIMULATION  The experimental data in this paper is all from the pro-  duction systems of the organization the author subordinates  to. The multi-type data is extracted from three systems and  integrated into the experimental environment. As to software  environment, Hadoop, Mahout and IK tokenizer are adopted.

Hadoop is an open-source distributed computing framework  (version 0.20.203) which is able to take advantage of clusters to  do high-speed computing. Mahout is an open-source machine  learning analysis tool (version 0.6) which provides operating  functions for matrix, vector and so on. Mahout can mine and  analyze mass data based on Hadoop. IK is an open-source  tokenizer for Chinese words (version U5) providing intelligent  words segmentation and the reverse finest granularity algo-  rithm, which has high efficiency for words segmentation. For  hardware environment, six PC servers are used, of which one is  served as management node (Master), while other five nodes  are computing nodes (Slave). Their configurations are Intel  E7530 CPU, 1.87GHZ, quad-core and 8G memory, on each of  them there are at most 15 Map tasks running at one time. In  the experiment, 5 attribute dimensions including data type, data  length and 3 characteristic values are selected. Reduce phase  adopts the single-task mode and data dividing uses averaging  method to simplify the program configuration. Meanwhile,  cache mechanism is applied to improve algorithm efficiency  which caches the data records in the range of neighbor radius  thus searching is not necessary in that range when executing  Step 5.

The experiments contain two parts. Experiment 1 is based  on MBSC clustering algorithm while experiment 2 the k-means  clustering algorithm. It is found out that number of iterations  is a key parameter of the accuracy and efficiency of clustering  algorithms. Therefore, an experiment for iteration times is  conducted in advance in which the number of ants in each  Map task is 3 and the amount of data records is 400k. Results  are obtained under the condition when gmax equals to 5, 10  and 20, respectively:  TABLE I: Accuracy of MBSC with different parameters  Task gmax = 5 gmax = 10 gmax = 20  time(h) accuracy (%) time(h) accuracy (%) time(h) accuracy (%)  25 8.39 91.121 11.54 96.342 23.41 97.565 30 7.89 93.055 9.26 96.365 20.77 96.900 35 6.37 91.128 7.92 96.311 18.49 96.766 40 5.47 91.205 6.59 96.455 17.01 96.715 45 4.74 91.422 5.74 96.773 16.46 98.312 50 3.46 91.579 5.13 96.850 15.33 96.459 55 3.13 92.111 5.03 96.835 13.58 96.345 60 3.24 91.323 5.55 96.745 13.11 96.211 65 3.17 90.735 5.59 97.023 12.75 97.013  It is obviously seen from Table I that accuracy improves  remarkably while the time consumed does not double when  gmax changes from 5 to 10. However, there is almost no  improvement of the accuracy and sometimes deterioration even  happens when gmax continues to increase to 20. At the same  time, the time consumed goes up strikingly with 2 times more  in the worst case.

Accordingly, for experiment 1, max iteration number is  set to 10, observation radius r equals to 0.267, and p0 is     TABLE II: Comparing of time (h) consuming of data semantic clustering  Task A = 50k A = 100k A = 200k A = 400k A = 800k  MBSC k-means MBSC k-means MBSC k-means MBSC k-means MBSC k-means  25 0.208 0.094 1.545 0.349 5.19 1.563 11.537 3.928 20.894 7.204 30 0.176 0.093 1.268 0.322 4.591 1.298 9.256 3.384 17.599 6.793 35 0.156 0.099 0.727 0.361 2.929 1.053 7.923 3.756 16.223 6.223 40 0.094 0.087 0.537 0.318 2.554 1.146 6.589 4.022 14.731 5.964 45 0.055 0.079 0.366 0.315 1.72 0.92 5.742 4.242 12.553 6.408 50 0.034 0.067 0.204 0.357 1.475 0.884 5.129 3.641 11.759 6.6 55 0.033 0.064 0.183 0.381 1.379 0.808 5.026 3.947 10.554 5.823 60 0.045 0.069 0.176 0.32 1.424 0.839 5.549 3.538 10.963 5.81 65 0.054 0.07 0.252 0.302 1.57 0.999 5.593 3.063 11.395 5.586  0.25; since the range of each dimension after normalization  is 1, the swarm similarity factor is set to 1. The parameter  of experiment 2 is based on that of experiment 1: two cluster  centers are predefined according to the two effective clustering  results obtained from experiment 1, i.e. K=2. Considering the  iterations in experiment 1, max iteration number is set to 20  and the convergence value equals to 0.02. Table 2 and Table  3 show the results of time consuming and clustering accuracy  when A is 50K, 100K, 200K, 400K and 800K, respectively.

Table 2 and Table 3 indicate that the accuracy apparently  increases since more time is consumed as the data amount  becomes larger with MBSC clustering algorithm. However,  with k-means algorithm, the accuracy does not show a linear  positive correlation to the data amount with time consumed  increasing and is not high on the whole.

Furthermore, with MBSC, as the number of Map tasks  increases, the increasing rate of clustering efficiency becomes  lower though the clustering efficiency itself is still rising. For  fixed data amount, when the number of Map tasks reaches  a certain value, the clustering efficiency does not increase  any more or even begin to decrease at an inflection point.

Moreover, the larger the data amount is, the higher clustering  accuracy it gets, which has nothing to do with the number  of Map tasks, indicating data amount is the most essential  influential factor of clustering accuracy. Meanwhile, for fixed  data amount, the accuracy increases with the number of Map  does until it reaches to a certain inflection point and turns to  decrease, and the smaller the data amount is, the earlier the  inflection point shows. In general, there is balance between the  number of tasks and the data amount approaching which the  effect of clustering is better. For fixed data amount, clustering  computing efficiency benefits little from the increasing of the  number of Map tasks with k-means algorithm. In addition,  the clustering accuracy falls as the Map tasks increases.

Compared with k-means clustering algorithm, even though  MBSC consumes more time, it gets much higher clustering  accuracy. The time-consuming difference between MBSC and  k-means algorithm becomes smaller while the clustering ac-  curacy difference larger as the number of Map tasks increases.

Accuracy per unit time i.e. accuracy/time is introduced in order  to well compare the effect of the two algorithms. For the data  amount of 400k, the results are shown in Fig.1 and Fig.2.

From Fig.1, it can be observed that the proposal spends  ?  ?  ?    ?  ??  ??  ??  ? ? ?? ?  ?  ?   ???? ???????  ??????  ???? !"?  Fig. 1: Time comparison of the two methods  ?  ??  ??  ?  ??  ???  ? ? ?? ?  ?  ?   ???? ???????#$$%&?$' ("?  ??????  Fig. 2: Accuracy comparison of the two methods  more time than k-means. However, as the number of tasks  increases, the computation time of the proposal becomes close  to that of k-means. Fig.2 implies that the accuracy per unit  time value of MBSC is better than k-means algorithm when  the number of Map tasks resides between 45 and 60. With  the same range of Map task number, it is just the ?balance  interval? for the data amount of 400K in Table II and Table III.

In a word, MBSC clustering algorithm is an effective method     TABLE III: Comparing of accuracy (%) of data semantic clustering  Task A = 50k A = 100k A = 200k A = 400k A = 800k  MBSC k-means MBSC k-means MBSC k-means MBSC k-means MBSC k-means  25 85.6824 79.2437 90.3657 77.1289 95.3315 73.5612 96.3424 74.3365 98.1723 68.4677 30 85.5653 78.8928 90.8776 78.1245 95.4125 74.923 96.3647 75.2298 98.2895 71.5482 35 86.3781 78.6511 91.7443 80.3667 95.4102 77.8799 96.3112 71.6721 98.2712 71.0467 40 86.9031 65.0903 91.6385 72.5805 95.3875 74.4409 96.455 72.1067 98.3513 68.3512 45 87.2134 79.1196 92.1377 73.5577 96.0845 68.0511 96.7733 68.8829 98.2931 71.4709 50 87.768 72.5701 92.4462 71.8388 96.1866 73.4481 96.8502 69.8551 98.3932 64.0578 55 87.032 70.0117 91.8997 73.1022 96.5131 68.1277 96.8346 68.5378 98.4679 65.3879 60 87.1963 67.5789 92.4221 65.3719 96.4946 67.3246 96.7452 70.4521 98.5123 68.9647 65 86.9884 69.3289 90.3678 66.1902 95.5876 69.1109 97.0231 71.0489 98.5046 68.7783  for data semantic clustering analysis in big data environment  avoiding the shortcomings of predefined clustering center,  which performs well when the Map task number and data  amount achieve a balance.



V. CONCLUSION  MapReduce is an effective framework for high-speed par-  allel computation in big data environment, and Ant Colony  clustering algorithm is an effective method for carrying out  semantic clustering of data based on attribute character vectors.

Based on MapReduce and Ant colony clustering algorithm, this  paper has realized automatic mapping and clustering based  on data semantic in big data, which is the premise of big  data analysis. At present, based on this clustering algorithm,  a set of heterogeneous data semantic clustering mapping tools  have been developed and tested in data analysis project with  hundred-million records, the result of data semantic clustering  has showed good accuracy and calculation efficiency. How-  ever, in the application process, combined with the above  experiment, two problems have been found: At first, on one  device, this is a new topic on how to get a balance among  the resource requirements of Map task, the number of ant in a  task, the data scale assigned and the hardware configuration of  devices, then making the device to provide the most efficient  cluster computing service. Secondly, considering the situation  of multiple devices and multiple tasks, Map tasks is dispatched  by the scheduling nodes, it is another topic on how to get the  schedule more effective and get task allocated more rational.

These two topics pointed out the direction of our next research.

