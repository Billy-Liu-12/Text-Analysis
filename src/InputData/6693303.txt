Sociometric methods for relevancy analysis of Long Tail Science Data

Abstract? As the push towards electronic storage, publication, curation, and discoverability of research data collected in multiple research domains has grown, so too have the massive numbers of small to medium datasets that are highly distributed and not easily discoverable ? a region of data that is sometimes referred to as the long tail of science. The rapidly increasing, sheer volume of these long tail data present one aspect of the Big Data problem: how does one more easily access, discover, use, and reuse long tail data to lead to new multidisciplinary collaborative research and scientific advancement? In this paper, we describe DataBridge, a new e-science collaboration environment that will realize the potential of long tail data by implementing algorithms and tools to more easily enable data discoverability and reuse. DataBridge will define different types of semantic bridges that link diverse datasets by applying a set of sociometric network analysis (SNA) and relevance algorithms. We will measure relevancy by examining different ways datasets can be related to each other: data to data, user to data, and method to data connections. Through analysis of metadata and ontology, by pattern analysis and feature extraction, through usage tools and models, and via human connections, DataBridge will create an environment for long tail data that is greater than the sum of its parts. In the project?s initial phase, we will test and validate the new tools with real-world data contained in the Dataverse Network, the largest social science data repository. In this short paper, we discuss the background and vision for the DataBridge project, and present an introduction to the proposed SNA algorithms and analytical tools that are relevant for discoverability of long tail science data.

Keywords- Long tail data, sociometric network analysis, data discoverability

I.  INTRODUCTION With the internet celebrating over a quarter century of  existence, electronic storage and publication of data has  become almost second nature to most scientists. In the past few years, the push towards open access and discoverability of research data has grown, as evinced by a White House Office of Science and Technology Policy memo addressing open access to data and directing federal agencies to provide funding towards initiatives that increase accessibility of publicly-funded research [1]. Contemporaneously, there has been a rapid growth in the large numbers of small datasets that are highly distributed, not well organized or curated, and thus are not easily discoverable or reusable. These datasets typically exist in total isolation from each other, are individually managed, and suffer from sparse and inconsistent provenance and metadata. Palmer et al. [2, 3] refer to these as the ?long tail of science? data ? the massive number of relatively small datasets which currently make up the largest proportion of scientific research data.

Though these datasets are small and individually easy to manage, they contain rich information that can be used and reused to maximize new scientific discoveries ? if the data are easily discoverable, accessible, and analyzable.

Discovering long tail data is hard because the data is often distributed in personal workspaces with little attempt made at data publication. Finding relevant data is made even more problematic by the difficulty in defining relevancy metrics for scientific datasets. Accessing relevant data is not easy when the data are distributed, not well documented, and in heterogeneous and possibly unique formats. These same characteristics also inhibit the analysis of data and are a major obstacle to generating new knowledge from this type of research data.

While we often use the term ?Big Data? to refer to very large datasets that pose problems in management and analysis due to their sheer size, these only represent one aspect of the Big Data problem. The rapidly-growing amounts of long tail data pose a different, yet fundamental, Big Data problem: how do we enable easier discoverability, use, and reuse of the massive number of smaller datasets that exist in almost total isolation from one another? How do we determine what datasets are relevant to others, and thus make discoverability of these relevant data easier? In this paper we discuss the DataBridge [4, 5], a collaborative  SocialCom/PASSAT/BigData/EconCom/BioMedCom 2013  DOI 10.1109/SocialCom.2013.6     tool to address some of the challenges associated with long tail data.

DataBridge is an NSF-funded collaboration between University of North Carolina at Chapel Hill, Harvard University, and North Carolina A&T State University to develop an e-Science environment that measures relationships between different datasets. In the DataBridge, similarity and relevancy of long tail data will be assessed on four general aspects: the contents of the data itself, contextual information about the data, producers and consumers of the data, and methods used to create and analyze the data. Using these relationships, we can discover and maintain profiles and clusters for datasets to help researchers seek, search, browse and identify data relevant to their work.

In the next section of this paper, we discuss the overarching vision for the design of the DataBridge. Section III of the paper further discusses the concept of sociometric network analysis (SNA) and details various methods used to measure the relevance relationships among datasets. Section IV presents related works, with a conclusion in Section V.



II. DATABRIDGE VISION DataBridge is an indexing mechanism for scientific  datasets, similar to web search engines that help find web pages of interest. Unlike web search engines that use the textual content of a web page and hyperlinks to identify its relevance to a query, the search space for scientific datasets is quite different and needs external resources such as tags, metadata, contexts, and naming conventions to identify relevancy. As discussed in the introduction, typical long tail datasets in isolation provide very sparse information content for search and discovery.

A resource discovery system for scientific datasets should provide a rich set of tools for mining information and context. To this end, the DataBridge system will analyze linkages between datasets. It will gather data, metadata, usage and other information, and apply SNA algorithms to map datasets connected by multi-dimensional relationships.

In this multi-dimensional network, sub-graphs, clusters, and cliques will be used to inform the discovery of other relevant datasets.

Even though a large number of datasets still remain only stored in personal workspaces, without formal organization and metadata, successful efforts have been made to provide centralized data repositories to properly share, manage and archive scientific datasets. These efforts include the Dataverse Network (DVN) and the Integrated Rule-Oriented Data System (iRODS). In the initial phase, DataBridge will draw upon these existing systems because they offer a rich set of real-world structured data and metadata that will help validate the algorithms and analysis. The DVN is an e- Science collaboration environment used to publish, share, reference, extract and analyze research data [6, 7, 8, 9, 32].

Together, University of North Carolina and Harvard University host DVN instances containing over 50,000 research studies with more than 700,000 data and  supplementary files in social science. We chose the DVN as a starting point because of its richness of data, but also because the DVN facilitates long term access and good data archival practices while the researcher retains control of, and recognition for, the data he or she deposits.  iRODS is a data grid middleware [10, 11, 12, 13, 14, 15] which provides many facilities for collection building, managing, querying, accessing, and preserving data in a distributed data grid framework. The iRODS system applies policy- based control when performing these functions.

DataBridge will eventually gather information from multiple data resources maintained by individuals, projects, regional or disciplinary repositories, and national collaboratives. This information will be integrated into a semantically-rich interface that will allow discoverability of relevant datasets based on relationships between data, users, methods and metadata. Internally, it will apply several relevance algorithms, described in the next section, to build a knowledge base and generate interconnected networks. The DataBridge will provide a venue for scientists to publish, discover, and access data of importance and to find others engaged in similar and pertinent research. The resulting networks and additional information gathered by DataBridge will be transferred back to permanent data repositories such as DVN, to facilitate discoverability within the repository and automatically enhanced the curation of datasets.

Given a set of criteria such as a sample dataset, DataBridge will query its knowledge base to find relevant datasets that are close to the initial dataset based on the given criteria. To do this, the architecture of DataBridge will consist of three functional units:  a data gatherer which interacts with data repositories to gather information about scientific datasets, a relevance engine which integrates information about datasets into a relevance network based on sociometric analysis, and a web-based user interface which searches for relevant datasets and gathers information through crowd sourcing and collaborative tagging.

In the following section, we describe the proposed SNA and relevancy measures to be applied in the DataBridge.



III. SOCIOMETRIC NETWORK ANALYSIS FOR LONG TAIL SCIENCE DATA  The DataBridge effort will create a semantically-rich, cross-disciplinary, sociometric network using modern information network analysis tools that build on seminal work by Jacob L. Moreno [16]. Moreno described sociometry as ?the inquiry into the evolution and organization of groups and the position of individuals within them.? Moreno pioneered the depiction of social relationships between people through sociograms ? graphs that symbolize individuals as nodes connected by links, or edges. DataBridge will be the first attempt to apply sociometry and its derived techniques to the study of scientific datasets.

A. Sociometric Network Analysis Algorithms Successful sociometric algorithms hinge upon  adequately detecting community structure, or clustering, in real systems. Though many disciplines, including sociology, biology, and computer science, often represent individuals as graphs, how to detect ?community? has not yet been satisfactorily solved, despite the huge effort [17, 18, 19] of a large interdisciplinary community of scientists over the past few years. Building upon the efforts of this previous work, we will explore a broad range of community detection methods for DataBridge.

The main elements of the problem of community detection in graphs are not defined ? indeed, there is not a current, universally accepted definition of ?community? [17, 18, 19]. For the purposes of graph clustering, we define communities as groups of nodes similar to each other.

Similarity between each pair of nodes can be computed with respect to a previously assigned reference property, whether or not the nodes are connected. Each node is assigned to the cluster of nodes most similar to it. If the graph nodes are embedded in an n-dimensional Euclidean space, the distance between a pair of nodes can be used as a measure of similarity. For the design of the Databridge, we will investigate several algorithms to quantify clustering including: relative proximity, cosine similarity, dissimilarity, random walk, and resistance distance measures.

The relative proximity between any two data points A=(a_1,a_2, ? ,a_n) and B=(b_1,b_2, ? ,b_n) can be measured by any norm L_m, such as the Euclidean distance (L_2- norm [SM1]), the Manhattan distance (L_1-norm [SM2]), or the L_?-norm [SM3]. Another popular spatial measure is the cosine similarity [SM4]. If the graph cannot be embedded in space, similarity must be inferred from the adjacency relationships between nodes. Another possibility based on the concept of structural equivalence [SM5] [32] defines the distance between nodes [20] as a dissimilarity measure: two nodes are structurally equivalent if they have the same neighbors, even if they are not adjacent themselves.

Using the dissimilarity measure, d_ij=0 if i and j are structurally equivalent. Nodes with large degree and different neighbors are considered very ?far? from each other and will have a greater dissimilarity measure. Alternatively, one could measure the overlap between the neighborhoods ?(i) and ?(j) of vertices i and j, given by the ratio between the intersection and the union of the neighborhoods [SM6].

Another measure related to structural equivalence is the Pearson correlation [SM7] between columns or rows of the adjacency matrix.  Table 1 is a summary of these measures.

An alternative measure is the number of edge- (or node-) independent paths between two nodes. Independent paths do not share any edge (node), and their number is related to the maximum flow that can be conveyed between the two nodes under the constraint that each edge can carry only one unit of flow (max-flow/min-cut theorem) [21]. The maximum flow can be calculated in a time O(m), for a graph with m edges, using techniques like the augmenting path algorithm.

Similarly, one could consider all paths running between two nodes. In this case, there is the problem that the total number  of paths is infinite, but this can be avoided if one performs a weighted sum of the number of paths. For instance, paths of length L can be weighted by the factor ?^l, with ?<1.

Another possibility, suggested by Estrada and Hatano [22, 23], is to weigh paths of length L with the inverse factorial 1/l!. In both cases, the contribution of long paths is strongly suppressed and the sum converges.

Random Walk properties provide another class of measures of node similarity that can be used in sociometric network analysis of long tail data. For example, one random walk property measures the commute time between a pair of nodes, or the average number of steps needed for a random walker, starting at either node, to reach the other node for the first time and to come back to the starting node.

The commute time and various variants have been used as a similarity measure by Saerens and coworkers [24, 25, 26, 27]: the larger the time, the farther (less similar) the nodes. The commute time [28] is closely related to another measure, the resistance distance [29]. The resistance distance expresses the effective electrical resistance between two nodes if the graph were turned into a resistor network. White and Smyth [18, 30] used instead the average first passage time, i.e. the average number of steps needed to reach for the first time the target node from the source.

B. Relevance Algorithms: In order to connect disparate datasets in a network and  discover multidimensional similarities, we will research and  Table 1. Similarity Measures (SM)  SM1 The Euclidean distance ( -norm) ?  SM2 The Manhattan distance ( -norm) ?  SM3 The -norm  SM4 Cosine similarity  Range of  is [0,?)  ?????????  ??	? ????? ?????? ???? ?????? ??	? ?????????  SM5 Dissimilarity measure in structural equivalence  ???????? ??	?  ??????????????? ?????  SM6 Overlap in neighborhood ??????????????	?  ???????????????????? ???	?????????  SM7 Pearson correlation between columns or rows  ? ???????? ??????????	? ?  ???? ???????????	?  ??     implement several types of relevance metrics. These metrics can be broadly grouped into: data to data connections, user to data connections, and method to data connections, as described below.

1) Data to Data Connections: The ability to understand and connect data across data types and research disciplines hinges on the quality of metadata available to describe these data. The current process of creating metadata is very labor- and time- intensive. Many long tail data suffer from sparse metadata because archives and repositories are forced to make a tradeoff to process many datasets with minimum description or to process fewer datasets and take time to create more detailed metadata. The DataBridge will include a service that generates metadata based upon the results of relevance engine processing. This new interface will automatically suggest topics and keywords to researchers uploading and ingesting data into repositories by pre- tagging the projected topic space with appropriate terms.

Accurate suggestions will encourage accurate crowd sourced tagging of the ingested data. This researcher guided/machine-learning metadata creation environment will have a profound impact on the future of data discovery and reuse [31].

As we build the data to data relevancy engine, we will compare different probabilistic topic models to determine which work best to cluster a corpus of short abstracts, explore effective metrics for sampling the abstracts to be projected together using these models, and methods to effectively scale these models (i.e. layer the topic space). We will further compare the effectiveness of these methods to random projections, which can scale to handle a large corpus.

Semantic similarity of datasets can be measured by applying technologies in Natural Language Processing (NLP) to infer the topic space as latent classes. Methods such as Probabilistic Latent Semantic Analysis (PLSA) can effectively account for both synonyms (words that refer to the same topic) and polysemy (words with multiple meanings) in a corpus of abstracts by modeling each document as a mixture of topics, each being a unigram model.

2) User to Data Connections: Using the DVN as a starting point for modeling user to data connections, we are able to search metadata fields including authors, producers, distributors, provenance, and geographic and time coverage of a dataset. DataBridge will extend beyond the current manual, passive search options and will allow for better understanding of and even prediction of possible future collaborations.  We will crawl published papers that use DVN data to identify datasets from past collaborations and explore collaboration patterns along features in the given datasets. Using these patterns and some of the similarity measures discussed in Section III B, we will build models of collaboration to predict data connections.

3) Methods to Data Connections: The use of particular models and methods to analyze datasets provides rich data for mining sociometric information. Usage methods and applications can be viewed as properties of the datasets and can be used to determine relevance between datasets. Since scant research has been conducted on measures of similarity between research methods and long tail data, an ontology of methods, tools, and applications needs to be defined. From this information, we plan to implement relevance algorithms that will use the method ontology to help define a relevance network.

4) Interactive Connections: Some of us (King and Crosas together with Grimmer, Stewart and members of the Harvard's IQSS software team) are working on a computer- assisted method to discover clusters (or partitions) in large corpora of unstructured text [46]. This method differs from the ones mentioned above because it allows users to interact with the clustering space until they find a result useful and tuned to their needs.



IV. RELATED WORK Currently, there are several national consortium-based  projects including DataONE (Dataone) [33], Datanet Federation Consortium (DFC) [34], the Consortium of Universities for the Advancement of Hydrologic Science (CUHASI) [35], iPlant Consortium (IPC) [36], and the Ocean Observatories Initiative (OOI) [37] that collect and provide access to disciplinary data collections. Additionally, scientific collaboration tools have been developed that help scientists build collections for their projects.  These include systems such as iRODS, the DVN, Fedora Commons (DuraSpace), and LOCKSS (lockss).

What?s lacking from these efforts is a network that connects datasets such that the whole becomes greater than the sum of its parts ? connections based on similarities beyond normal textual connections within the silos of single disciplines.  Missing from these works also is an explicit focus on the relationships among datasets.

Other researchers have begun to explore how to link datasets.  Tools such as Scival Experts, CiteSeerX and DataCite, for example, are designed around metadata schemas. SciVal Experts derives relationships between publications and experts by coauthorship and keywords, but does not address datasets per se. CiteSeerX includes some additional techniques such as automatic metadata harvesting from indexed articles and crowd sourcing of opinions about articles, but still does not address datasets. DataCite is designed to make it easier for researchers to find relevant datasets by collecting metadata, providing a search capability, and assigning persistent data identifiers to assist in citing and publication, but it lacks any of the sociometric infrastructure we intend to build in the DataBridge.

In relation to scholarly communication, the role of automatic metadata generation is being researched and tested as a method to help increase discoverability, access, and efficiency [38, 39, 40].  In today?s digital information age, many now consider datasets unique units of scholarly     communication in their own right [41, 42]. SEAD [43] is also concerned with sustainability of datasets in the long tail of science and proposes to provide a data repository for scientists to manage, share, and link their data with others.

However, the linking is done manually by users and not through automatic analysis as proposed here. The Linked Data project [44] links data through the use of a data description language, but the description is not generated by analysis as we propose. Google Scholar allows the user to search a wide variety of sources, including books and some web sites, but has neither a sociometric component nor a focus on datasets. Google does focus on the sociometry of datasets, but these datasets are limited to what are presented on HTTP servers that can be crawled and are normally unstructured text or images. Another related project is the CASRAI program [45], which is focused on developing common metadata standards to allow for linking research information to facilitate data exchange, collaboration, and interaction with funding agencies.



V. CONCLUSION This paper outlines the background and vision behind  DataBridge ? an e-Science collaboration system that enables researchers to discover relevant datasets in the long tail of science data by applying SNA algorithms and multiple dimensions of relevancy between datasets. Work is underway in developing this framework and implementing and evaluating the DataBridge system.

