1045-9219 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

Abstract?DNA sequencing has moved into the realm of Big Data due to the rapid development of high-throughput, low cost Next-Generation Sequencing (NGS) technologies. Sequential data compression solutions that once were sufficient to efficiently store and distribute this information are now falling behind. In this paper we introduce phyNGSC, a hybrid MPI-OpenMP strategy to speedup the compression of big NGS data by combining the features of both distributed and shared memory architectures. Our algorithm balances work-load among processes and threads, alleviates memory latency by exploiting locality, and accelerates I/O by reducing excessive read/write operations and inter-node message exchange. To make the algorithm scalable, we introduce a novel timestamp-based file structure that allows us to write the compressed data in a distributed and non-deterministic fashion while retaining the capability of reconstructing the dataset with its original order. Our experimental results show that phyNGSC achieved compression times for big NGS datasets that were 45% to 98% faster than NGS-specific sequential compressors with throughputs of up to 3GB/s.

Our theoretical analysis and experimental results suggest strong scalability with some datasets yielding super-linear speedups and constant efficiency. We were able to compress 1 terabyte of data in under 8 minutes compared to more than 5 hours taken by NGS-specific compression algorithms running sequentially. Compared to other parallel solutions, phyNGSC achieved up to 6x speedups while maintaining a higher compression ratio.

Index Terms?NGS, parallel, hybrid, HPC, MPI, OpenMP, Big Data, FASTQ, timestamps.

F  1 INTRODUCTION  S EQUENCING DNA has become a very fast and low costprocess [1] that is pushing Next-Generation Sequenc- ing (NGS) datasets into the realm of Big Data. Highly specialized compression techniques for NGS have been implemented to tackle the task of loosely preserving the key elements of these datasets. Such techniques reduce the enormous amount of resources required to store or transmit these files until further analysis is required.

In order to exploit the characteristics of genomic data, specialized compression solutions have been proposed.

Algorithms such as Quip [2], G-SQZ [3], DSRC [4], KungFQ [5], SeqDB [6], LW-FQZip [7], LFQC [8], LEON [9], SCALCE [10] and SOLiDzipper [11] take into account the nature of DNA sequences and use different data compres- sion techniques to achieve good ratios. But these algorithms process the data sequentially and do not fully utilize the processing powers of the newest computing resources, such as GPUs or CPU?s multi-core technologies, to speed up the compression of an increasing amount of genomic data.

Although several general-purpose parallel compression tools are available [12] [13] [14], algorithms specialized in NGS datasets are scarce. A parallel algorithm specific to NGS data, DSRC 2 [15], is a multi-threaded parallel version of DSRC with faster compression times, variable  ? Corresponding author: Fahad Saeed, Department of Electrical and Computer Engineering and Department of Computer Science, Western Michigan University, Kalamazoo MI 49008-5329 USA.

E-mail: fahad.saeed@wmich.edu  ? S. Vargas-Pe?rez belongs to the Department of Computer Science at Western Michigan University, USA.

Manuscript received Month day, year; revised Month day, year.

TABLE 1: Comparing compression time of genomic data for sequential algorithms versus a parallel algorithm.

Dataset Size Sequential Parallel  NGS-Specialized General Purpose NGS-Specialized DSRC Quip GZip DSRC 2  10 MB 0.20secs. 0.35secs. 0.87secs 0.38secs.

100 MB 2.08 secs. 2.42secs. 11.51secs. 0.47 secs.

1 GB 26.36secs. 27.31secs. 91.10secs. 1.99secs.

10 GB 5.56mins. 4.51mins. 23mins. 30.04secs.

100 GB 35.20mins. 42.12mins. 2hrs. 5.39mins.

1 TB 5.66hrs. DNF DNF 1.1hrs.

throughput, and comparable compression ratios to existing solutions. Table 1 compares three algorithms running se- quentially versus DSRC 2?s multithreading parallel strategy.

One can see the difference between specialized sequential compression algorithms (DSRC and QUIP) and the general purpose GZip compressor, which also runs serial code, versus DSRC 2 specialized parallel methods.

In this paper we present phyNGSC, a hybrid strategy between MPI and OpenMP to accelerate the compression of big NGS datasets by combining the best features of distributed and shared memory architectures to balance the load of work among processes, to alleviate mem- ory latency by exploiting locality, and to accelerate I/O by reducing excessive read/write operations and inter- node message exchange. Our algorithm introduces a novel timestamp-based approach which allows concurrent writ- ing of compressed data in a non-deterministic order and thereby allows us to exploit a high amount of paral-    1045-9219 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPDS.2017.2692782, IEEE Transactions on Parallel and Distributed Systems   lelism. The code for this implementation is available at https://github.com/pcdslab/PHYNGSC.

In the following sections we analyze the proposed design of phyNGSC and evaluate the performance obtained. We discuss the results of running the algorithm for big NGS datasets with distinct combinations that vary in terms of number of processes and threads per process. We then compare our solution with other models that use different approaches to achieve parallel compression.

1.1 Our Contribution One of the problems for parallel applications is concurrent reading and writing [16]. The effects of concurrent memory writing have been extensively studied [17]. However, the specific issues encountered when performing I/O opera- tions for big data are relatively new in the field.

A very common way of writing to a file in parallel is to employ dedicated processes to read and write and use the rest of the processes for computations in a master/slave approach as shown in Fig. 1a. In these parallel compression schemes, the use of a shared memory buffer is required in order for the writer to fetch the data. This guarantees a writing order since only one process is accessing the output file and the data to be written is stored in the shared memory buffer in an identifiable order. However, using this read/write scheme limits the scalability of the parallel genomic compression algorithms because the process in charge of writing has to wait for all of the workers to finish processing the data before it can write to the output file.

(a) Reading and writing scheme commonly used in traditional parallel algorithms specific to compression of NGS datasets.

(b) Reading and writing method used in the proposed hybrid strategy for compression of NGS data in pa- rallel using our TOC-W method.

Fig. 1: Comparison of the method commonly used to ad- dress read and write in parallel algorithms and our pro- posed method.

In order for a parallel compression algorithm to be more scalable, the compressed form of the data should be writable as soon as its processing is finished. In other words, the compressed form of the data should be writable in a non-deterministic form (from any of the processes) while retaining the properties that would allow reconstruction of the original data. Our proposed phyNGSC hybrid parallel strategy, allows us to write data (in a shared file space) from different processes as soon as processing completes.

In order to overcome the non-determinism and guarantee an order in the way of writing, we integrated the concept of time-stamping which we call Timestamp-based Ordering for Concurrent Writing or TOC-W. Our method uses a global clock and gets a snapshot of the time of completion for the writing operation to the output file as shown in Fig. 1b. This makes the proposed parallel strategy highly scalable since it removes the bottleneck of ordered-concurrent writing in a shared file space. We discuss the methodology in detail in Section 3.

2 BACKGROUND INFORMATION NGS datasets are commonly structured using the FASTQ format [18]. As shown in Fig. 2, FASTQ is composed of multiple records stored in plain text. Each record contains four lines:  1) Title line: Starts with the ASCII character ?@? fol- lowed by a description of the record.

2) DNA line: Contains the DNA sequence read.

3) The third line is represented by the ASCII character  ?+? which indicates the end of the DNA sequence read, followed by optional repetition of the title line.

4) Quality score line: Represents the quality value of the DNA sequence read. It uses ASCII characters from ?!? to ??? (33 to 126 in decimal). Each of the ASCII characters will represent a probability that the corresponding nucleotide in the DNA line is incorrect.

Fig. 2: FASTQ file structure.

As a proof-of-concept, we will utilize some methods de- veloped for DSRC [4] to underline the compression portion of our hybrid parallel strategy, since it exhibits superior performance for sequential solutions [19].

2.1 Related Research Hybrid parallel solutions utilizing MPI and OpenMP in high performance computing architectures have been success- fully applied to various problems [20] [21] [22]. In [22] the    1045-9219 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPDS.2017.2692782, IEEE Transactions on Parallel and Distributed Systems   authors described a hybrid framework for interleaving I/O with data compression in order to improve I/O throughput and reduced big scientific data. However, none of these works are specific to genomic datasets which may require domain-specific knowledge about Omics related data.

Another parallel strategy, based on domain decompo- sition of the NGS data, was introduced by the authors in [23]. In our preliminary work, we implemented a solution that offered modest compression ratio, acceleration, and scalability for up to 100GB of genomic data. The scheme only included a distributed memory model approach and limited optimizations for I/O operations. Also, the com- pressed form of the data was written in an orderly format i.e., process pi had to wait until process pi?1 finished its writing process before it could start. However, as reported in our paper, super-linear speedups were observed and high compression throughputs were registered.

3 PROPOSED HYBRID STRATEGY FOR PARALLEL COMPRESSION For our hybrid parallel implementation we considered an SMP system where a FASTQ file of size N bytes is parti- tioned equally among p homogeneous processes, creating a working region (hereafter refereed to as wr) of size Np for each pi, ?i ? N : i from 0 to p ? 1. FASTQ records vary in length, hence pi cannot know if a complete record (containing the 4 lines explained in the previous section) is present at the start and/or at the end of its wr. For this reason we use a small overlap between them to guarantee that each pi?s wr contains complete records. As sketched in Fig. 3, each process identifies the start of the first complete record at the beginning of its wr by checking the characters until an ?@? is encountered and it is confirmed that it belongs to a title line1. pi will continue fetching records until it reaches the end of its wr. If it does not have a complete record after reading the last byte, it will continue through the overlap portion and stop when a full record is found.

The overlap is three times bigger than the maximum length that a short read can be for common FASTQ files2. This will ensure that pi will finish reading its last record at byte b since p(i+1) started reading its first complete record at byte b+ 1.

Once the wr is correctly defined for each process, they will fetch chunks of R records out of the FASTQ file and create a threaded region where t number of threads will analyze and compress the title, DNA, and quality score portion of each fetched record, as shown in lines 7 to 15 of the pseudo-code in Fig. 4. Threaded Region 1 is a ?loop worksharing?, which means that each thread th, ?h ? N : h from 0 to t? 1, will work collectively in solving the k tasks in the threaded region in a many-to-one relationship i.e., all threads will solve one task at a time, until all tasks are done.

Processes then will prepare temporary memory buffers (line 17) to store the compression data from threads.

Threaded Region 2 (lines 19-25) will work on the com- pression of the records. This is a ?sections worksharing?  1. The use of ?@? sign to identify the start of the title line is ambiguous since the character can also appear in the quality score line.

2. Illumina NextSeq Series Specifications.

Fig. 3: Partition of a FASTQ file among processes into work- ing regions of size Np bytes each. pi?s working region ends with an incomplete record and pi+1?s working region starts in the middle of another record. pi keeps reading beyond its working region until a full record is fetched. pi+1 finds the first complete record at the beginning of its working region.

region in which a thread th will be assigned the completion of one of the k tasks in a one-to-one relationship. In this type of threaded region if the number of threads is greater than the number of tasks, t ? k threads will remain idle.

Otherwise, a thread th will be assigned to an unsolved task until all tasks are complete.

The compressed records will form a subblock which will be copied to a writing buffer wb of size W (lines 27- 41). When wb is full, pi will create a block of compressed data and write it using a shared file pointer to the output NGSC (Next-Generation Sequencing Compressed) file. Im- mediately after pi is done writing, a timestamp will be taken and stored in a time of completion toc list to be used later on for the TOC-W method (lines 44-48).

When all processes are done fetching and compressing records from their wr, they will gather all the local toc lists into the root process, generally p0 (line 50). The algorithm will terminate when p0 builds the TOC-W, creates the footer, and writes it to the NGSC file (lines 52-56).

3.1 TOC-W: Timestamp-based Ordering for Concurrent Writing  Dependencies of a parallel algorithm on previous stages is detrimental to the scalability of a high performance system.

Previous attempts to formulate a parallel algorithm for big datasets have been affected by these dependencies. In order for an algorithm to be scalable for big data and/or increas- ing number of processes, it is required that the algorithm write its results as soon as they are available. However, this is not possible with trivial techniques.

The use of shared file pointers, a feature introduced in MPI-2 [24] and the capabilities of parallel file systems, guar- antee that when a process pi wants to write to the output file it will do so in a non-deterministic fashion i.e., the process that updates the shared file pointer first will write first. This will provide faster writing phases since processes do not    1045-9219 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPDS.2017.2692782, IEEE Transactions on Parallel and Distributed Systems   Require: p processes (?p ? 2), t threads per p, FASTQ file of size N Ensure: NGSC file of size M (0 < M < N ) and all p exit correctly  1: procedure PHYNGSC(p, t, FASTQ) 2: Initiate MPI environment with p processes 3: pi defines its working region wr of N/p bytes 4: while wr contains records do 5: pi fetches R records from FASTQ file 6: 7: Begin Threaded Region 1: t threads and k tasks 8: OMP Loop Worksharing 9: for all records fetched do  10: k0: ALL threads locate records? offset 11: k1: ALL threads analyze records? title 12: k2: ALL threads analyze records? DNA 13: k3: ALL threads analyze records? Quality 14: end for 15: End Threaded Region 1 16: 17: pi prepares environment for compression 18: 19: Begin Threaded Region 2: t threads and k tasks 20: OMP Sections Worksharing 21: k0: ONE thread compresses EACH record?s title 22: k1: ONE thread compresses EACH record?s DNA 23: k2: ONE thread compresses EACH record?s Quality 24: k3: ONE thread compresses subblock?s header 25: End Threaded Region 2 26: 27: pi check available space in writing buffer wb of size W 28: 29: if wb is full then 30: pi packs subblocks in wb into a block 31: pi writes wb to NGSC file 32: pi adds a timestamp to toc list 33: end if 34: 35: Begin Threaded Region 3: t threads and k tasks 36: OMP Sections Worksharing 37: k0: ONE thread copies compressed title to wb 38: k1: ONE thread copies compressed DNA to wb 39: k2: ONE thread copies compressed Quality to wb 40: k3: ONE thread copies compressed header to wb 41: End Threaded Region 3 42: end while 43: 44: if wb is not empty then 45: pi packs subblocks in wb into a block 46: pi writes wb to NGSC file 47: pi adds a timestamp to toc list 48: end if 49: 50: pi calls mpi gather( pi?s toc list, p0 gathers ) 51: 52: if pi?s rank = 0 then 53: p0 gathers toc lists 54: p0 builds TOC-W 55: p0 writes TOC-W as footer in NGSC file 56: end if 57: 58: pi calls mpi finalize() 59: end procedure  Fig. 4: Pseudo-code for phyNGSC compression using p pro- cesses and t threads per process and a FASTQ file.

have to wait for each other to call a collective write function.

Several algorithms have been evaluated in [25] using shared file pointers, showing an increase in writing performance while avoiding dependencies that the file systems have on file locks. Although one can implement the shared file pointers without the file system support as shown in [26].

However, non-determinism poses a considerable challenge at the time of decompression because the algorithm will not  know which process wrote a particular block of compressed data and figuring this out will take a significant amount of time and substantial synchronization efforts.

To solve this problem we propose TOC-W, a method based on timestamps that guarantees ordering of the data block for concurrent writing on distributed applications.

The MVAPICH2 implementation that we use in our strategy guarantees a global, synchronized time for all MPI processes executing a program. Although a clock is local to the node where the process that calls the timer resides, a boolean value ?true? in MPI_WTIME_IS_GLOBAL will imply that all nodes participating in the parallel execution of the program have the same synchronized time in their clocks. This is what we call a global clock. The synchronization of the clocks does not produce a significant computation nor commu- nication overhead since the clocks are synchronized once when MPI_Init is called, for the rest of the application?s execution time. The effects of time drift among synchronized clocks and techniques used to keep clocks in sync are studied in [27] and [28] which are taken into account by the MPI implementation use for our proposed strategy. The timestamps for the toc are obtained with an MPI call to MPI_Wtime by each pi, which returns a number of seconds (not clock ticks) that represent elapsed wall-clock time, with a precision of 1 millionth of a second3.

In Fig. 5a we can see that a NGSC file is built by each pi writing blocks of equal lengths in arbitrary order. After a pi finishes writing a block it takes a timestamp indicating the time of completion or toc of that writing operation. The toc is the ordering element of the method; it is used to create an ascending sorted list of the timestamps of all processes as in Fig. 5b. This list is then used to assign each process to the corresponding offsets of the blocks it owns within the NGSC file. This information is stored in the footer of the file.

In order to ensure correct timestamps and ordering, the following 2 conditions need to be satisfied:  1) A global clock: This will guarantee that each pi will have synchronized clocks and time will be global for all processes. The MVAPICH2 imple- mentation of MPI provides the boolean attribute MPI_WTIME_IS_GLOBAL that indicates whether time is synchronized across all processes.

2) Equal amount of data to be written: Each pi needs to write an equal amount of compressed data to avoid incorrect ordering. Consider the scenario where pi and pj request the shared file pointer by calling MPI_File_write_shared at the same time. If there are not hardware or network issues, each process will correctly take a timestamp that will indicate which wrote the data first. Network or memory congestion in distributed systems can cause incorrect ordering of the blocks of compressed data. In order to deal with this issue, we introduce a header for each block in the NGSC file marked with a unique signature identifying the process who wrote it. The combination of both timestamps and unique signature always allows correct ordering and identification of the data.

3. For processors Intel EM64T Xeon E5 2.6 GHz, 8-core.

1045-9219 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPDS.2017.2692782, IEEE Transactions on Parallel and Distributed Systems   (a) Diagram showing concurrent writing by each pi. (b) TOC-W is build using list of timestamps.

Fig. 5: Diagram showing a global, synchronized clock and equal amount of compressed data written to NGSC file for correct TOC-W in distributed applications.

Fig. 5a shows the scenario when both conditions are satisfied: A synchronized clock among processes and equal amount of data to be written. In the event that multiple processes request an update to the shared file pointer, the update operation will be serialized internally by MPI in a nondeterministic fashion, such that race conditions will be avoided [24] . The time taken by each process waiting to update the shared file pointer, although small, guarantees that at the end of the writing process there is a difference in the time of completion of each writing operation. This is illustrated with process p0 and p2 in Fig. 5a. The oper- ation of checking the block?s signature and offsets using MPI requires minimal memory or time-resources [29]. The synchronization of MPI processes and timestamps required for our proposed strategy are discussed below.

3.2 MPI+OpenMP Hybrid Scheme  For our hybrid strategy we implemented the masteronly model described in [30]. In this model, an MPI process pi is mapped to a single core in a typical multi-socket, multi- core SMP node n [31]. When an OpenMP parallel region4  is encountered, the number of threads requested will be mapped to the idle cores of node n. The process pi will become the thread master of the threads inside this threaded region, such that if t threads are requested, t ? 1 idle cores will be mapped to an OpenMP thread plus the process pi (now functioning as a thread master). The model is shown in Fig. 6.

The masteronly model allows MPI communication (send/receive, read/write, broadcast, and so on) only out- side of OpenMP parallel regions. This is advantageous for our design because we want to use OpenMP threads to ac- celerate computations only, since MPI is optimized for inter- node communication, especially for I/O. Synchronization is also reduced using this schema, thus minimum effort is required to guarantee thread safety.

4. In this paper an OpenMP parallel region is referred to as threaded region.

Fig. 6: MPI+OpenMP hybrid approach using a masteronly model.

3.3 NGSC File Structure The proposed NGSC file is composed of many blocks of equal length W and a footer which contains information to be used for decompression such as the TOC-W and the working region offsets in the FASTQ file. Each block con- tains a header with information about itself and a signature that identifies the process that wrote it. Blocks are formed by many subblocks such that the sum of bytes for all the subblocks in the particular block is equal to W . This implies that a block will have complete and incomplete subblocks.

A subblock will have a header and the compressed title, DNA and quality lines of the R records fetched from that process particular working region.

3.4 Analysis of Computation and Communication Costs Sequential compression algorithms generally have a linear asymptotic growth of O(N), where N is the number of bytes to be compressed. We will refer to N as the problem size given in bytes. In our hybrid parallel solution, each process pi will process an approximately equal amount of bytes w = N p , let?s call w the work load and f the percentage of w    1045-9219 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPDS.2017.2692782, IEEE Transactions on Parallel and Distributed Systems   that cannot be processed within a threaded region using t threads. With Amdahl?s law we define Tt as the factor in which the processing of w increases, i.e., by adding t threads per p, the amount of work required to compressed N bytes will be in the order of O( NpTt ).

3.4.1 Communication Costs As every parallel implementation incurs in some amount of communication overhead [32], we try to keep our overhead cost To small by reducing the communication rounds to only 3 phases; reading the FASTQ file, writing the compressed data to the NGSC file, and gathering timestamps for our TOC-W method (Fig. 7). We assume that the time spent by process pi reading or writing to an I/O disk d does not depend on the distance between pi and d, but on the amount of bytes to be read/written. The same assumption will be considered for inter-node communication between a process pi in node ni and pj in node nj .

3.4.1.1 Reading Phase: Let us assume l is the size in bytes of a chunk of data that a process pi will read out of its working region wr = Np . Since each pi will use an explicit offset to read non-collectively the bytes within its wr, the communication overhead will have a complexity of O(Nlp ).

3.4.1.2 Writing Phase: Processes can write concur- rently using a shared file pointer. When pi initiates a writing operation of W bytes, it requests an update to the pointer, which reserves W bytes and points to the next available byte to be used by another process. Concurrent access to the shared file pointer is serialized in a non-deterministic way, but this degree of serialization is negligible [26] compared to collective ordered access which incurs in some synchro- nization overhead. With these assumptions we calculate the overhead incurred in the writing phase by defining r as the compression ratio of the strategy given by r = NM , where M is the size of the compressed output file NGSC.

W , as mentioned above, is the size of a compressed block in bytes that every process will write as they become available.

Approximately MW blocks will be written by p processes con- currently using the shared file pointer, hence the complexity for this phase will be represented by O( NrWp ).

3.4.1.3 Timestamp Gathering Phase: The gathering of timestamps is done once per program execution in O(log(p+g)), where g is the number of timestamps collected by each pi. g will vary depending on the value of p, N and the amount of writing operations performed.

3.4.2 Time Complexity The total time complexity of phyNGSC, given by TP = Tc + To, where Tc represents the computational costs and To the overhead costs  Tc = O (  N  pTt  ) . (1)  To = O ( N  lp  ) +O  ( N  rWp  ) +O  ( log(p+ g)  ) . (2)  TP ? O ( N  p  (  Tt +   l +   rW  )) +O  ( log(p+ g)  ) . (3)  Fig. 7: Sketch of the work flow of phyNGSC.

3.4.3 Scalability  Calculating the speedup S with increasing number of pro- cesses, threads, and data size will tell us if our strategy has good scalability. We will use the definitions and as- sumptions of [33]: S = TSTP , where TS is the sequential execution time of DSRC, ?our proof-of-concept? algorithm.

Substituting TP with equation (3) we will have  S = N  N p (  Tt  + 1l +  rW ) + log(p+ g) . (4)  Let C = 1Tt + l +  rW and eliminate log(p+ g) since its  value is very small, then we can reduce equation (4) to  S = N NC p  = p  C . (5)    1045-9219 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPDS.2017.2692782, IEEE Transactions on Parallel and Distributed Systems   We know that the part 1l +  rW of C is a constant parameter, hence we can re-write equation (5) to have  S ? pTt. (6)  4 IMPLEMENTATION RESULTS To test our implementation we used SDSC Gordon5, a high performance compute and I/O cluster of the Extreme Science and Engineering Discovery Environment (XSEDE).

Gordon has 1024 compute nodes, each with two 8-core 2.6 GHz Intel EM64T Xeon E5 (Sandy Bridge) processors and 64 GB of DDR3-1333 memory. The network topology is a 4x4x4 3D torus with adjacent switches connected by three 4x QDR InfiniBand links (120 Gb/s). The cluster implements TORQUE resource manager and has Lustre file system. C++ was used to code our implementation using the OpenMP library to provide thread support and the MVAPICH2 im- plementation of MPI (version 1.9). The Intel compiler was used for compilation.

To run our hybrid model, the system maps each MPI process and each OpenMP thread to a node?s core (Fig. 6), so if 4 MPI processes are requested with 16 threads per process to be run on 4 nodes, then the system will have 1 core in each of the 4 nodes dedicated to MPI processes.

Since each node in the SDSC Gordon cluster has 16 cores, the remaining 15 cores in each node will sleep until an OpenMP parallel region is created. At this moment the MPI processes will become the master thread and join the other 15 threads (mapped to the idle cores on the same node), for a total of 16 threads, to satisfy the amount requested. To keep the nodes at a good performance it is recommended that the value of MPI processes per node ? number of threads per process do not exceed the total number of cores per node.

The NGS datasets for the experiment were obtained from the 1000 Genomes Project6 and are shown in Table 2. In this table the column ?Renamed? is used to identify the datasets with relation to their sizes (rounded to the closest power of 10). For clarity, the ?Renamed? identification will be used throughout the rest of this paper instead of the original datasets? IDs.

TABLE 2: Datasets used for the test. The fastq file SRR622457 was append three times to itself.

IDs Organism Platform Records Size(GB) Renamed  ERR005195 Homo sapiens ILLUMINA 76,167 0.0093 10MB ERR229788 Homo sapiens ILLUMINA 889,789 0.113 100MB ERR009075 Homo sapiens ILLUMINA 8,261,260 1.01 1GB ERR792488 Homo sapiens ? 90,441,151 15 10GB SRR622457 Homo sapiens ILLUMINA 478,941,257 107 100GB SRR622457 Homo sapiens ILLUMINA 1,436,823,773 1,106 1TB  4.1 Performance Evaluation We executed the experiments using each of the datasets of Table 2 as input for phyNGSC. The algorithm ran with a set of 2, 4, 8, 16, 32 and 647 processes, each mapped to a core in an individual node. Each set of processes used  5. https://portal.xsede.org/sdsc-gordon 6. http://www.1000genomes.org 7. SDSC Gordon?s policy only allows a maximum of 64 nodes to be  scheduled for use  from 1 to 8 threads. All the experiments were repeated several times and the average execution time, max physical memory, and resulting NGSC file size were collected. DSRC version 1 (sequential) and version 2 (multi-threaded) as well as MPIBZIP2 were also tested for comparison with our algorithm.

0 10 20 30 40 50 60 70  0.1     1,000  10,000  C om  pr es  si on  Ti m  e (s  )  t = 1 per process  0 10 20 30 40 50 60 70  0.1     1,000  10,000  t = 3 per process  0 10 20 30 40 50 60 70  0.1     1,000  10,000  Processes C  om pr  es si  on Ti  m e  (s )  t = 5 per process  0 10 20 30 40 50 60 70  0.1     1,000  10,000  Processes  t = 7 per process  10MB 100MB 1GB 10GB 100GB 1TB  Fig. 8: Compression time in seconds for phyNGSC with increasing number of processes and different dataset sizes using 1, 3, 5 and 7 threads per process.

As can be seen in Fig. 8, the average compression time, measured as the wallclock time elapsed executing code between MPI initialization and finalization, decreases sharply as more processes are added with different datasets.

The compression time also improved as more threads were added. These results are in accordance with our theoretical analysis, i.e., the asymptotic growth of the parallel com- pression time remained close to that represented by TP in equation (3). Overall the implementation demonstrated a better performance when the number of threads were kept between 3 and 7. Fig. 8 also shows that the compression time starts deteriorating as more processes are added to compressed datasets 10MB and 100MB. Dataset 10MB is smaller than the size of l (3.4.1.1), set to be 8 megabytes, which causes the communication overhead of the reading phase to be greater than the computations done while compressing. Our strategy was designed to take advantage of coarse-grained task and data parallelism which is very efficient when working with large amounts of data [34].

Furthermore, Table 3 shows the percentage decrease in compression time obtained by our solution. A more than 90% decrease in compression time was obtained by using 32 and 64 processes to compress all of the bigger datasets. Also a reduction in time of more than 75% was obtained when we used 4 processes to compress datasets 100MB to 1TB. These results suggest that by doubling the amount of processes and threads the compression time changed by a decreasing percentage of approximately Tp ?  pT , in accordance with our  theoretical analysis of the computation cost Tc.

Table 4 shows the fastest times obtained by phyNGSC  compressing different dataset sizes. Using 64 processes our    1045-9219 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPDS.2017.2692782, IEEE Transactions on Parallel and Distributed Systems   TABLE 3: Percentage decrease in compression time for phyNGSC compared to DSRC?s sequential compression time.

Datasets Number of Processes  2 4 8 16 32 64  10MB 53.56% 46.22% 62.73% 59.35% 21.08% 77.62% 100MB 59.13% 81.49% 86.06% 91.85% 84.90% 79.81%  1GB 73.37% 86.49% 93.70% 96.21% 97.88% 97.99% 10GB 76.73% 87.96% 94.10% 97.01% 98.24% 98.59% 100GB 68.74% 83.71% 93.77% 96.52% 97.73% 96.61%  1TB 46.70% 78.06% 88.60% 87.53% 96.94% 97.38%  TABLE 4: Total average of the best time taken by phyNGSC to compress different datasets.

Datasets DSRC DSRC 2 Number of Processes  2 4 8 16 32 64  10MB 0.20s. 0.25s. 0.09s. 0.11s. 0.07s. 0.08s. 0.16s. 0.36s.

100MB 2.08s. 0.36s. 0.85s. 0.39s. 0.29s. 0.17s. 0.31s. 0.42s.

1GB 26.36s. 1.99s. 7.02s. 3.56s. 1.66s. 1.00s. 0.56s. 0.53s.

10GB 5.56m. 30.04s. 1.29m. 40.19s. 19.69s. 9.99s. 5.88s. 4.71s.

100GB 35.37m. 5.39m. 11.06m. 5.76m. 2.20m. 1.23m. 0.80m. 0.51m.

1TB 5.66h. 55.63m. 3.02h. 1.24h. 38.69m. 42.34m. 10.37m. 8.91m.

hybrid model compressed 1 terabyte of data in 8.91 minutes, whereas the sequential DSRC took 5.66 hours.

Fig. 9 shows phyNGSC speedup ratio. It shows that super-linear speedups were obtained when compressing the datasets using up to 32 processes and 1, 3, 5, and 7 threads per process, respectively. This behavior is due to the fact that adding threads in the same node allows faster access to a global memory. In addition, the use of their accumulated cache allows the amount of data processed at a given time to fit into the new cache size, reducing the latencies incurred in memory access. This cache effect is studied in [35].

Fig. 10 presents the maximum memory consumed by our implementation. We can observe that for datasets 10GB or bigger the memory usage decreases as more processes are added. For smaller datasets, such as those of 1GB, the memory consumption remained lower than 50 Megabytes per process.

Table 5 contains the total number of timestamps collected by processes. Timestamps can also represent a writing op- eration to the NGSC file and we can see in this table that for smaller datasets, more writing operations are required when using high number of processes, since each pi will write blocks of size less than W . On the other hand, with bigger datasets processes can pack and write full blocks and reduce writing operations efficiently.

We evaluated the compression throughput of phyNGSC to have a measure of the overall compression performance.

Throughput will be defined as the amount of data being processed per second, expressed in gigabytes per second (GB/s). Table 6 shows the compression throughput results.

Among all the datasets 10GB has the maximum through- put of 3GB/s when compressed with 64 processes, which correlates with its super-linear speedup. 32 to 64 process were used to achieve the best compression throughput for datasets 100GB and 1TB of 2.21GB/s - 1.48GB/s and 1.62GB/s - 1.88GB/s, respectively.

0 10 20 30 40 50 60 70        Sp ee  du p  R at  io  T = 1  0 10 20 30 40 50 60 70      T = 3  0 10 20 30 40 50 60 70       Processes  Sp ee  du p  R at  io  T = 5  0 10 20 30 40 50 60 70       Processes  T = 7  10MB 100MB 1GB 10GB 100GB 1TB Ideal  Fig. 9: Speedup ratio S for phyNGSC with increasing number of processes and different dataset sizes using 1, 3, 5 and 7 threads per process.

20 40 60     Processes T hre  ad s  M ax  .M em  or y  (M B)  (a)  20 40 60    Processes T hre  ad s  M ax  .M em  or y  (M B)  (b)  20 40 60    1,000  Processes T hre  ad s  M ax  .M em  or y  (M B)  (c)  20 40 60     Processes T hre  ad s  M ax  .M em  or y  (G B)  (d)  Fig. 10: Maximum memory usage for phyNGSC with increas- ing number of processes and different amount of threads per process. (a) 1GB. (b) 10GB. (c) 100GB. (d) 1TB.

4.2 Comparison with Existing Parallel Compression Al- gorithms  We mentioned in Section 1 the existence of several parallel compression algorithms, such as pigz, PBZIP2, MPIBZIP2, and DSRC 2 (multi-threaded). We selected DSRC 2 and MPIBZIP2 to compare against our implementation. Fig. 11 shows the compression time for DSRC 2 using 32 threads, phyNGSC and MPIBZIP2 using 2, 4, 8, 16, 32 and 64 pro- cesses each. We can see how phyNGSC outperforms DSRC 2 after increasing the number of processes to more than 8.

Our implementation did better than MPIBZIP2 using the    1045-9219 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPDS.2017.2692782, IEEE Transactions on Parallel and Distributed Systems   TABLE 5: Total number of timestamps collected by phyNGSC when compressing datasets of different sizes and increasing number of processes.

Datasets Number of Processes  2 4 8 16 32 64  10MB 2 4 8 16 32 64  100MB 2 4 8 16 32 64  1GB 18 20 22 28 32 64  10GB 252 254 254 258 262 266  100GB 2,225 2,227 2,229 2,234 2,244 2,256  1TB 20,825 20,826 20,830 20,832 20,842 20,860  TABLE 6: Compression throughput (GB/s) of phyNGSC for different dataset sizes and increasing number of processes using 7 threads per process.

Datasets Number of Processes  2 4 8 16 32 64  10MB 0.10 0.08 0.12 0.11 0.06 0.03 100MB 0.12 0.27 0.36 0.62 0.34 0.25  1GB 0.14 0.28 0.60 0.99 1.77 1.87 10GB 0.18 0.35 0.72 1.42 2.41 3.00 100GB 0.16 0.31 0.81 1.44 2.21 1.48  1TB 0.09 0.23 0.43 0.40 1.62 1.88  same amount of processes. MPIBZIP2 did not finish the compression of 1TB using 2 processes after it exceeded a 24 hour execution time limit. Fig. 12 shows the compression ratio algorithms. Our implementation yielded compression ratios that remained within the range of 4.3 and 7.9.

5 CONCLUSION AND DISCUSSION In this paper we presented a hybrid parallel strategy that utilizes distributed-memory and shared-memory architec- tures for the compression of big NGS datasets. In order to exploit extreme forms of parallelism we introduce the concept of Timestamp-based Ordering for Concurrent Writ- ing (TOC-W) of the compressed form of the data. TOC-W presented efficient methods to write the compressed form of the data in a non-deterministic manner while keeping the amount of information required to reconstruct it back to its original state to the minimum. This led to the devel- opment of a highly scalable parallel algorithm optimized for Symmetric Multiprocessing (SMP) clusters, which ex- hibited a tremendous reduction in compression time and faster speedups for most of the datasets when compared to traditional methods of dividing the data and compressing it on individual machines. To the best of our knowledge this is the first attempt to exploit HPC architectures for non- deterministic ordered compression of NGS data.

Theoretical analysis of our hybrid strategy suggested O( 1pTt ) reduction in the compression time and this was ver- ified by our experimental results. However, we noticed that Amdahl?s law [36] limited the theoretical linear speedups after adding 64 processes. As was observed with 1TB dataset (Table 4), 32 processes yielded a compression time of ap- proximately 10 minutes and doubling the amount of pro- cesses, only produced a 20% reduction in the compression  1 2 4 8 16 32 64  0.1   C om  pr es  si on  Ti m  e (s  )  10 MB  1 2 4 8 16 32 64    100 MB  1 2 4 8 16 32 64     C om  pr es  si on  Ti m  e (s  )  1 GB  1 2 4 8 16 32 64    1,000  10 GB  1 2 4 8 16 32 64   1,000  10,000  Processes  C om  pr es  si on  Ti m  e (s  )  100 GB  1 2 4 8 16 32 64  1,000  10,000  Processes  1 TB  phyNGSC MPIBZIP2 DSRCmt DSRCseq  Fig. 11: Compression time for different NGS datasets for DSRC sequential, DSRC multi-threaded, and phyNGSC and MPIBZIP2 with 2, 4, 8, 16, and 32 processes.

10 MB 100 MB 1 GB 10 GB 100 GB 1 TB  .3  .3  .9  .4  .3 6 .4  .8  .2  .3  .3  .6  .6  .5  .9  .9  .7  .4  .7  .6  .9  .2 6 .3  .3  .3  phyNGSC MPIBZIP2 DSRCmt DSRCseq  Fig. 12: Compression Ratio (y:1) of the resulting NGSC file for DSRC sequential, DSRC multi-threaded, phyNGSC and MPIBZIP2. A higher ratio means a better compression.

1045-9219 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPDS.2017.2692782, IEEE Transactions on Parallel and Distributed Systems   time. Due to the coarse-grained design of our parallel hybrid strategy the smaller datasets (10MB and 100MB) did not benefit as much when more processes were added.

A number of research problems remain open and the techniques presented in this paper suggest new lines of inquiry that can be pursued. For the immediate future, we plan to develop a high-performance decompression strategy by utilizing the novel data structure presented in this paper.

