Efficient Relationship Pattern Mining using Multi-relational Iceberg-cubes

Abstract  Multi-relational data mining(MRDM) is concerned with data that contains heterogeneous and semantically rich re- lationships among various entity types. In this paper, we introduce multi-relational iceberg-cubes (MRI-Cubes) as a scalable approach to efficiently compute data cubes (ag- gregations) over multiple database relations and, in par- ticular, as mechanisms to compute frequent multi-relational patterns (?itemsets?). We also present a summary of per- formance results of our algorithm.

1 Introduction  Numerous analysis and data mining tasks in a wide va- riety of applications including intelligence analysis, social network analysis, business data analysis, web data mining and bioinformatics are based as much on the links among heterogeneous entities and events as the properties of indi- vidual entities. Hence, the databases in these applications contain both attribute and semantic relationship data. This data is stored in multi-relational (multi-table) form in rela- tional database systems as a set of linked tables each corre- sponding to some conceptual entity or relationship. Multi- relational data mining (MRDM) is concerned with the dis- covery of models and patterns from such databases. How- ever, applying most MRDM algorithms to realistically large databases is fraught with several challenges including scal- ability and lack of integration with database systems [6].

The two predominant approaches to MRDM are trans- formation of multi-relational data to a single table (and then applying conventional single-table algorithms) and direct mining of multi-relational databases. A transformation ap- proach that simply converts multiple tables into one joined relation (?universal relation?) has been shown by a num- ber of studies to suffer from several disadvantages includ- ing extreme horizontal and vertical data blow-up [13], loss of important semantic entity-relationship information car- ried by the join links [14], and increased data redundancy  (duplication) that may introduce statistical skew [7]. Propo- sitionalization is a more systematic transformation (albeit limited to classification tasks) that reduces the data explo- sion by summarizing relationship information using aggre- gation functions. However, propositionalization also results in exponentially large attribute-value problems especially for non-numeric data sets [13]. Moreover, propositionaliza- tion algorithms like Polka [9] and RELAGS [10] are based on inefficient data aggregation techniques (i.e. rely on mul- tiple SQL group-bys which are known to scale poorly as compared to data cube operators).

This paper deals with the MRDM task of multi-relational association rule (pattern) mining that directly operates on multi-relational databases. We propose a scalable algorithm for frequent multi-relational pattern discovery that is effi- ciently integrated with database systems.

Multi-relational Patterns. A multi-relational associa- tion rule(pattern) [5], henceforth abbreviated as MRP, is given as:  ??  ???  ??????? ??? ?  ????  ???????  ??? ???? ???  where ?? is the target table corresponding to a particular real-world entity1) and ?? is another table in the database which is joined to table ???? through a foreign key con- straint (i.e. ?? and ???? are part of a legal join path). Pat- terns over these tables are represented as conjunctions of atomic predicates which have the form ?? ? ?? where ?? is a categorical or numerical attribute and ?? is a value from a certain domain. ? and ? are the number of predicates and ? is the number of tables over which this MRP is defined.

Each MRP has an associated support and confidence. Sup- port of a pattern is the number of tuples in ?? that satisfy it while confidence is the probability of satisfying a certain subset of the pattern given the whole MRP.

Two streams of previous work has dealt with the dis- covery of association rules over multiple relations. For  1The support count is done on tuples in the target table  0-7695-2142-8/04 $ 20.00 IEEE    prolog databases and datalog queries, WARMR [5] ex- tends the Apriori algorithm to discover a limited class of MRPs. There is also an optimization of WARMR called FARMER [12]. Our approach differs from these algorithms in the following ways: (1) It runs directly on relational databases and does not require the specification of any pat- tern structure; instead we directly use the links (i.e. foreign key constraints) among tables. (2) It is based on a much more efficient Iceberg-cube computation algorithms. (3) It does not require the key of the target table to appear in all tables as in WARMR.

The work [11, 8] has considered relational association rule mining on a star-schema where the target table is the fact table and joins are of length one. Unlike this body of work, we consider association rules over tables with arbi- trary structure and depth of joins.

2 Multi-relational Iceberg-cubes  We propose a novel approach based on Iceberg-cube computation [3] that supports the critical component of MRP mining, namely frequent ?itemset? (conjunction) dis- covery. Iceberg-cubes are a type of data cube computed over attributes (dimensions) of a table where each aggre- gated value (measure) is above a given user specified thresh- old (also called iceberg condition). When the aggregation function is COUNT, Iceberg-cubes are equivalent to fre- quent conjunctions (the threshold being the minimum sup- port value). Iceberg-cube computation algorithms have two attractive features with respect to MRP mining: (1) they are based on depth-first traversal of the pattern lattice 2 which is much more efficient than breadth-first approaches employed by past work, (2) they can be implemented inside database systems with relative ease.

Below we present an algorithm called multi-relational Iceberg-cubes (MRI-Cubes) that extends an Iceberg-cube computation algorithm called BUC (Bottom-up computa- tion) to compute MRPs. The MRI-Cube algorithm (1) effi- ciently interleaves cubing (predicate set enumeration) with join path traversal while retaining the scalability of BUC by ensuring that only one partition of a single table is read to memory at a time, (2) minimizes the number of tuples involved in the join paths reachable from the target table.

The Multi-Relational Iceberg-cube Trie. Our algorithm utilizes a prefix trees (trie) referred to as multi-relational iceberg-cube trie or simply MRI-Cube trie) to integrate cub- ing with join path traversal. Populated as the cubing algo- rithm progresses, the MRI-Cube trie also stores counts and link structures of MRPs. Each node in an MRI-Cube trie  2for single table association rule mining as well, the most efficient tech- niques like Eclat and FP-growth are based on depth-first approach (see [4] for comparison).

procedure MRI-Cube (input,CloneJoinNodes) 1: // Cubing Phase 2: ?????????????? ? find next attributes to group by 3: for each attribute ????? of the ????? do 4: ?	????? ??? Partition ????? by ????? 5: ? ? ? 6: for each partition ?? do 7: if ?	????? ??? ? ??? ??? then 8: insert count to current MRI-Cube trie ?	?????	 of ????? (if  absent, create a new one and append to the current leaf node) 9: // stop recursion at join attributes  10: if ????? is a data attribute then 11: for ? ? ? ? ? ? size of ???	??????? do 12: //test for expansion pruning 13: if ???	??????? ?? ? ???s expansion with ?? was ?  then 14: break 15: else 16: expand ???	??????? ??? using ?? 17: MRI-Cube (?? , ???	??????? ) 18: ? ? ? 19: //Expansion Phase 20: repeat 21: ???????????	? pick the LM-LD node 22: ????	????	? expand ??????????? 23: ???	??????? ?? ? insert ???????????	 (corres. to  ????????????) 24: ???	??????? ?? ? traverse all leaves and add all other clone  nodes of the current join node.

25: //use join of ????? ?????? pair in ???????????	 with the table 26: MRI-Cube (????	????	, ???	??????? ) 27: until no more expansion is possible  Figure 4. The MRI-cube algorithm  corresponds to an attribute. The target table?s key serves as the root node. A node at depth ? (see figure 3 for an example) compactly stores all information pertaining to the cuboid resulting from grouping by the attributes in the path from the root to itself.

In order to interlink group-bys over attributes from mul- tiple tables in the MRI-Cube trie, we distinguish between two types of attributes: data attributes and join attributes.

Join attributes consist of foreign and primary keys of ta- bles while all the other attributes are considered to be data attributes. We make the reasonable assumption that join at- tributes store relationship information only. The MRI-Cube trie consists of two types of nodes, namely data nodes and join nodes (designated by a boxed node in figure 2), that correspond to the two types of attributes.

Preprocessing. In a preprocessing step, we determine the join graph3 rooted at the target table as well as the cub- ing order of attributes for each table using information in the database schema (catalog). Like BUC, we order the group-by attributes based on decreasing cardinality. The exception to this is the requirement that all join attributes be placed at the end of the cubing order.

The MRI-Cube Algorithm. The algorithm is shown  3Due to limited space, we only consider patterns over acyclic join paths in this paper.

0-7695-2142-8/04 $ 20.00 IEEE    A B C D a1 b1 c1 d1 a1 b3 c3 d3 a2 b2 c2 d2 a2 b3 c3 d4 a3 b2 c3 d4  table 1  table 2  table 3  table 4  table 5  C E F c2 e1 f2 c3 e2 f1  C H c1 h1 c2 h2  D J d1 j1 d3 j2  H I h2 i1 h2 i2  Figure 1. Exam- ple Tables  A  B D  B  C  D  target table key  C D  C  D  J  r1  r2  r3  e1 r4  e2  r6  e2  r7 r10  e1 r11  e2 r8  e1  r9  e2  E H  I  E H  I  e1.r1  e3  e2.r1  JJ J D  J  F  F  D  J  F  F  C  E H  I  D  J  F  F  E H  I  e3  D  J  F  F  r5  e1  e2e2 e2  e2  e3e3 e1.r2  e1.r3  e3.r1  Figure 2. An MRI-Cube trie for the example tables with table1 as the target table  a1 a2 a3  ancestor ptr  b_ptr1 b_ptr2 b_ptr3  descend. ptr  Node A  2 2 1 Value Array  Count Array Child Pairs Pointer  Figure 3. An MRI-Cube trie Node  in figure 4. It iterates between two phases, namely cubing phase and expansion phase. The inputs are a data partition (initially the whole target table) and a queue of join nodes (which is initially empty) on which the partition is to be joined as expansion.

The Cubing Phase. The cubing phase (lines 1-18 of figure 4) starts with the target table and recursively per- forms cubing on each attribute using BUC (i.e. applying ??? ???? pruning) until a join attribute is encountered or no more group-by is possible. When a join attribute is reached, group-by is performed on it like the other data nodes. But, the recursion stops and returns towards the root.

As such, join nodes delineate the boundary of the cubing phase. Every time the recursion returns to the root, all the leaves in the MRI-Cube trie will have either join nodes or a data node with the finest granularity.

The Expansion Phase. The expansion phase is launched when the cubing recursion returns to the root. Expansion extends cubing to the next table on the join path. The ex- pansion phase involves three steps:  Step 1. Pick the left-most leaf join node with the least- depth (henceforth referred to as the LM-LD node) from the MRI-Cube trie and join the set of keys in this node with the referenced table (lines 21-22 of figure 4). The join can be done using a hash based or sort merge technique. Since only foreign keys that meet the ??? ???? condition are kept in the join node, no redundant join is performed. More impor- tantly, since the LM-LD node is the least aggregated node, the result of this join is sufficient to expand all the nodes in the MRI-Cube trie corresponding to the same join attribute as the LM-LD node (see [15] for a formal proof).

Step 2. Find all other leaf join nodes (called ?clones?) that correspond to the same attribute as the LM-LD node by traversing the MRI-Cube trie horizontally backward from right-to-left. Lines 23-24 in figure 4 perform this traversal and store the clone nodes in a queue called ???? ??????	?.

Step 3. Recursively cube the joined table from step 1 while concurrently expanding the LM-LD node as well as all its clones (in the right-to-left order) with group-bys com- puted on the joined table?s data attributes. Specifically, all clone join nodes found in the leaves of the MRI-Cube trie are simultaneously grown using only one scan (loading) of a partition from the new joined table. This can be performed  by applying intersection (e.g. hash set intersection) of the keys in the join node and the corresponding key in a parti- tion. In figure 4, the ???? ??????	? queue is passed in the recursive call to MRI-Cube (line 26) so that subsequent cubing based on each partition of the joined table is applied on all clones. Finally, once the expansion of the current LM- LD node and all its clones is completed, (i.e. the referenced table is cubed on all its data attributes), we pick the next LM-LD node and repeat the above process. This continues until no more cubing or expansion is possible (i.e. until all leaf nodes are data nodes with no right-brother).

In addition to ensuring correctness, the right-to-left ex- pansion of clones allows us to prune expansion of join nodes when a partition can not result in cells that meet the ??? ??? (lines 13-14 in figure 4). As the clone nodes are traversed from right to left (i.e. from the least aggregated granule to the finer granule), we can prune the expansion of clone nodes using the Apriori property. This is because if a partition fails to produce qualifying cells when expanding a join node, then it will not produce any qualifying cells in all clones on its left (see [15] for a formal proof).

In summary, the MRI-Cube algorithm offers: (i) CPU efficiency: a data partition is scanned only once, (ii) Space efficiency: at a time, only one partition needs to reside in memory; hence if a table does not fit in memory, it can be partitioned until it does, (iii) Two types of pruning per- formed during the recursive cubing phase (??? ???? prun- ing) and the expansion phase (clone expansion pruning).

Example 1 The cubing and expansion operations de- scribed above are illustrated in the MRI-Cube trie shown in figure 2 whose construction also mirrors the MRI-Cube algorithm. The order in which the recursive cubing phase is executed is shown by the labels ?, ?, etc. The order in which the join nodes are expanded is shown by the labels 	?, ?, etc. The horizontal line across the trie shows the point where the algorithm entered into the first expansion phase.

After the first expansion phase, labels are shown only for the cubing done on the LM-LD nodes; the same processing is done simultaneously for all clones.

3 Performance Analysis  In this section, we validate the performance of the MRI- Cube algorithm. We implemented our algorithm using C++.

0-7695-2142-8/04 $ 20.00 IEEE      0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5  ru nt  im e(  se c.

)  min. supp.

FARMER MRI-Cube  Figure 5. Comparison w.r.t. min. supp.

200 300 400 500 600 700 800 9001000  ru nt  im e(  se c.

)  data size (k)  MRI-Cube FARMER  Figure 6. Comparison w.r.t. data size    2 3 4 5 6 7 8 9 10  R un  tim e(  Se co  nd s)  No. of tables involved  Join-Then-Mine MRI-Cube  Figure 7. Comparison based on number of ta- bles (avg. dim = 4, avg.

attr. cardinality=3000)    2 3 4 5 6 7 8 9 10  R un  tim e(  Se c.

)  Avg. no. of attributes  Join-Then-Mine MRI-Cube  Figure 8. Comparison based on average number of dimensions (with 10 tables, avg. attr. cardinal- ity=3000)    2 2.5 3 3.5 4 4.5 5  R un  tim e(  Se c.

)  Max. join path length  Join-Then-Mine MRI-Cube  Figure 9. Comparison based on maximum join path length (with 10 ta- bles, avg. dim = 4, avg.

attr. cardinality=3000)          0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5  R un  tim e(  Se c.

)  min. supp.

Join-Then-Mine MRI-Cube  Figure 10. Comparison based on min supp (with 10 tables, avg. dim = 4, avg. attr. cardinal- ity=3000)  The test databases were stored in DB2 and accessed using efficient table scan calls. The experiments were conducted on a Sun Ultra Enterprise 450 with 3GB of RAM.

Real dataset. This is a bibliographic dataset drawn from CiteSeer [1] and HPSearch [2]. It consisted of tables on papers, authors, their institutes, citation and co-authorship links. The paper table contained 200K tuples, the authors table contained 126,024 tuples. Figures 5 and 6 compares MRI-Cube with the state-of-the-art MRP mining system FARMER [12] which we downloaded from the author?s web site. For this comparison we used the paper table as a target table for MRI-Cube. For FARMER, we had to convert the dataset to a predicate-based representation and specify the required patterns (similar to the schema of the database) as well as other required parameters and biases.

The figures show that, as both min supp and data size in- creases, MRI-Cube achieves greater performance. In fact for data size above 500k, FARMER could not stop after an hour (see figure 6).

Synthetic dataset. This dataset was generated using the following parameters: number of records per table, number of attributes, average number of distinct values for each at- tribute, and average join cardinality for a pair of tables. Zipf distribution was used to generate attribute values. Since FARMER requires setting complex parameters and biases which make it difficult to run over complex schemas, we do not compare MRI-Cube with it on the synthetic data. In- stead we compare MRI-Cube with a naive approach (called  ?Join-then-mine?) that first performs joins and then applies single table cubing. The joined tables are found by a series of outer joins starting from the target table. Although the resulting cubes do not possess the useful structure found in MRI-Cube patterns, we believe this approach?s perfor- mance as compared to MRI-Cube shades light on the effect of MRI-Cube?s ability to avoid redundant joins.

Figure 7 and 8 show that while the run time of Join-then- mine explodes fast with increase in number of tables and average number of attributes per table, MRI-Cube achieves a much slower rate of increase. Figure 9 compares run time as the maximum join path length increases. Again MRI- Cube showed lower rate of rum-time increase. Finally, fig- ure 10 shows performance against increase in min supp. In this case, the rate of run time growth for Join-then-mine was close to that of MRI-Cube. This may be due to the common pruning technique (i.e. Iceberg cubing) used by both.

