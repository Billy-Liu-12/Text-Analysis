1551-3203 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

Abstract?A considerable amount of research has been devoted to multi-objective optimization problems (MOPs). However, few studies have aimed at multiobjective large-scale optimization problems (MOLSOPs). To address MOLSOPs, which may involve big data, this paper proposes a message passing interface (MPI)- based distributed parallel cooperative coevolutionary multi- objective evolutionary algorithm (DPCCMOEA). DPCCMOEA tackles MOLSOPs based on decomposition. First, based on a modified variable analysis method, we separate decision variables into several groups, each of which is optimized by a subpopu- lation (species). Then, the individuals in each subpopulation are further separated to several sets. DPCCMOEA is implemented with MPI distributed parallelism and a two-layer parallel struc- ture is constructed. We examine the proposed algorithm using the multi-objective test suites DTLZ and WFG. In comparison with cooperative coevolutionary generalized differential evolution 3 (CCGDE3) and multi-objective evolutionary algorithm based on decision variable analyses (MOEA/DVA), which are state-of- the-art cooperative coevolutionary multi-objective evolutionary algorithms (CCMOEAs), experimental results show that the novel algorithm has better performance in both optimization results and time consumption.

Index Terms?decomposition, variable grouping, cooperative coevolution, large-scale optimization, message passing interface (MPI), distributed parallelism.



I. INTRODUCTION  IN real world situations, many objectives have to be ful-filled [1]. For example, in the case that an online store recommends merchandise to customers, both the accuracy and diversity should be considered to satisfy multiple consumer demands [2]. Similar kinds of big data problems can be treated as multi-objective optimization problems (MOPs), and there are always conflicts among these objectives.

Many researchers have tried to solve MOPs through math- ematical models [3]. However, it is difficult to find accu- rate mathematical models. Moreover, these methods can only  This work was supported in part by the National Natural Science Foundation of China (NSFC) under Grant No. 61303001, in part by Special Program for Applied Research on Super Computation of the NSFC-Guangdong Joint Fund (the second phase), and in part by Foundation of Key Laboratory of Machine Intelligence and Advanced Computing of the Ministry of Education under Grant No. MSC-201602A. (Corresponding authors: Zhihan Lv, Bin Cao )  Bin Cao, and Jianwei Zhao are with the School of Computer Science and Engineering, Hebei University of Technology, Tianjin, 300401, China; Key Laboratory of Machine Intelligence and Advanced Computing (Sun Yat-sen University), Ministry of Education; Hebei Provincial Key Laboratory of Big Data Calculation, China. Xin Liu is with Hebei University of Technology, Tianjin, 300401, China (email: caobin@scse.hebut.edu.cn)  Zhihan Lv is with the Department of Computer Science, University College London, 66-72 Gower Street, London WC1E 6EA, United Kingdom. (email: z.lu@ucl.ac.uk)  produce a single solution in one run, and the majority of them are sensitive to the shape of the Pareto front (PF ) (e.g., concave or discontinuous). Nevertheless, multi-objective evolutionary algorithms (MOEAs) [4] are able to generate a set of possible solutions in a single run, and they can cope with the complicated shapes of the PF . In addition, MOEA is easy to implement. MOEA has been widely used in many fields, such as path planning of unmanned air vehicles (UAVs) [5], data mining [6], machine learning [7], software engineering [8], etc.

There are many kinds of MOEAs [9], [10], among which nondominated sorting genetic algorithm II (NSGA-II) [11] and multiobjective evolutionary algorithm based on decomposition (MOEA/D) [12] are well known. NSGA-II is based on Pareto dominance, while MOEA/D is based on decomposition. Based upon these two algorithms, many algorithms have been devel- oped [10], [13].

To solve MOPs, MOEAs have to conduct a large num- ber of evolutions, which is very time-consuming. When the complexity of an MOP increases, more time will be taken.

Tan et al. [14] puts forward a distributed parallel cooperative coevolutionary algorithm (DCCEA) for multiobjective opti- mization. In the work of [15], the competitive and cooperative co-evolutionary multi-objective particle swarm optimization algorithm (CCPSO) is proposed. Due to the utilization of dis- tributed parallelism, the optimization time is reduced. With the advent of big data, more and more information is generated.

When the amount of data is large, the MOPs can be called big data optimization problems, which can be also called multiobjective large-scale optimization problems (MOLSOPs).

However, in [14] and [15], the number of variables is few (no more than 30), and the effectiveness for MOLSOPs is unclear.

For large-scale optimization, only using serial algorithms and a single personal computer is very time-consuming and very likely to result in overflow, etc. At this time, distributed evolu- tionary algorithms (dEAs) [16] will be more suitable. For high- dimensional problems, the strategies of variable grouping and cooperative coevolution (CC) [17]?[19] contribute to better optimization performance. The available grouping methods include fixed grouping [18], [20], random grouping [21], [22], the Delta method [23], dynamic grouping [17], differential grouping [24], global differential grouping [25], graph-based differential grouping (gDG) [26], etc. Cooperative coevolu- tionary generalized differential evolution 3 (CCGDE3) [20] employs fixed grouping and effectively solves MOPs with up to 5000 variables. Through decision variable analyses (DVA),    1551-3203 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TII.2017.2676000, IEEE Transactions on Industrial Informatics   the multi-objective evolutionary algorithm based on decision variable analyses (MOEA/DVA) [27] decomposes an MOP into a set of simpler and low-dimensional sub-problems, and it outperforms several state-of-the-art algorithms. However, CCGDE3 and MOEA/DVA are executed in serial, which is too time-consuming.

The message passing interface (MPI) [28], [29] is a useful tool for high performance computation (HPC). MPI is suitable for coarse-grained parallelism and complex objective function- s. In this paper, we put forward a novel MPI-based distributed parallel cooperative coevolutionary multiobjective evolution- ary algorithm (DPCCMOEA) for large-scale optimization. The characteristics of the proposed algorithm are summarized as follows:  1) Based on the DVA strategy in MOEA/DVA, we propose a modified variable analysis strategy and decompose the large number of variables to several groups. Each variable group is optimized by a subpopulation (species).

These species cooperate to better optimize the MOLSOP.

2) With the help of MPI, we implement the algorithm in a distributed platform. To increase the parallelism degree and to realize the adaptive implementation to different number of CPUs, each species is further decomposed to multiple subspecies (sets), the number of which is determined by the available number of CPUs. Therefore, the computation time is greatly reduced.

The remainder of this paper is organized as follows. In Section II, we give some preliminary knowledge to better understand this paper. Section III describes the proposed algorithm: DPCCMOEA. The parallelism implementation is in Section IV. Section V is the experimental results and analyses.

Finally, we conclude this paper in Section VI.



II. PRELIMINARIES  A. Variable Analysis and Grouping  In CCGDE3, fixed grouping is utilized. The large number of variables are randomly separated into several groups, which are fixed during the evolution. In MOEA/DVA, the variables are classified as follows: ? distance variables: affecting convergence only ? position variables: affecting diversity only ? mixed variables: affecting convergence and diversity both In addition, variable dependencies are also obtained, which  can be described as follows. For a solution vector x = (x1, x2, ..., xi, ..., xj , ..., xn), where n is the number of vari- ables, if  ?1 ??2 < 0 (1)  can be true for a set of values a1, a2, b1, b2 in{ ?1 = f(x)|xi=a1,xj=b1 ? f(x)|xi=a2,xj=b1 ?2 = f(x)|xi=a1,xj=b2 ? f(x)|xi=a2,xj=b2  (2)  variables xi and xj are considered to be interacting. Then, only convergence variables are grouped according to interactions among them.

CV  Species1  CV1 CVs CVn. . . . . .

.

.

.

S1,1  S1,A1  S1,NP  Speciess  .

.

.

Ss,1  Ss,As  Ss,NP  Speciesn  .

.

.

Sn,1  Sn,An  Sn,NP  .  .  . .  .  . .

.

.

.

.

.

.

.

.

Fig. 1. CC framework.

B. CC  In large-scale optimization, there are a large number of vari- ables. Separating variables into multiple groups and optimizing them under the CC framework [18], [19] results in good performance. As illustrated in Fig. 1, variables in different groups (species) are optimized separately and combined with each other to form a complete solution vector (CV ) that can be evaluated.

Fig. 2. Algorithm structure.

1551-3203 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TII.2017.2676000, IEEE Transactions on Industrial Informatics   Algorithm 1: DPCCMOEA /* Initialization */ Set generation number G = 0, set parameters, and allocate memory; /* Grouping variables */ Analyze and group decision variables as discussed in Sections III-B; /* Construct MPI parallel structure */ According to the grouping information, construct the MPI parallel structure as discussed in Section IV-A; /* Species Initialization */ Initialize individuals of the sets in each species; while G < GMAX do  /* Information Exchange */ Exchange information among CPUs as discussed in Section IV-B; /* Population Evolution */ Evolve each sets as discussed in Section III-C; /* Population Updating */ Update the sets as discussed in Section III-C, which is similar to MOEA/D; G = G+ 1;  end /* Population Reduction */ Perform population reduction according to reference weights, and output the final population; /* Finalization */ Destroy variables and free memory;

III. DPCCMOEA  In this section, we describe the main idea of DPCCMOEA.

First, the overall structure is given. Then, the variable analysis and grouping strategy is described. Finally, the optimization of the population is detailed.

A. Algorithm Structure  To solve MOLSOPs, we propose a decomposition based al- gorithm. To fully exploit the parallelism, we integrate a variety of decomposition strategies, which are variable (population) decomposition and species decomposition. Therefore, the pop- ulation is decomposed into several species; and individuals in each species are further separated into many sets (subspecies).

The decomposition structure can be illustrated in Fig 2.

The overall process is in Algorithm 1.

B. Variable Property Analysis and Grouping  In MOEA/DVA, the classification of variables is related to its effect on the convergence and diversity of solutions. For example, see the following problem, which is partly derived from the Rosenbrock function:  { f1(x) = x1 ? cos(2?x2) + (x3 ? x24)2 + (x4 ? 12 )   f2(x) = 1? x1 + sin(2?x2) + (x3 ? x25)2 + (x5 ? 12 )  (3) s.t. xi ? [0, 1], i = 1, 2, 3, 4, 5  f (x)  -0.5 0 0.5 1 1.5 2 2.5  f 2 (x  )  -0.5   0.5   1.5  Only varying x   Only varying x  Only varying x  Only varying x  Only varying x  Fig. 3. Illustration of sampled points of the MOP formulated in Eq. 3 by varying one variable and fixing the others to 0.5.

TABLE I RESULTS OF PROPERTY ANALYSIS  DTLZ1 DTLZ2 DTLZ3 DTLZ4 DTLZ5 DTLZ6 DTLZ7  # Div. Vars. 2 2 2 2 2 2 2 MOEA/DVA 3 3 3 3 3 3 3  DPCCMOEA 3 3 3 3 3 3 3  WFG1 WFG2 WFG3 WFG4 WFG5 WFG6 WFG7 WFG8 WFG9  # Div. Vars. 4 4 4 4 4 4 4 4 4 MOEA/DVA 3 3 7 3 3 3 7 3 7  DPCCMOEA 3 3 3 3 3 3 3 3 3  By varying the variables, we can obtain Fig. 3. From DVA, we know x1 is a position variable; x3, x4 and x5 are distance variables; and x2 is a mixed variable.

The interdependence analysis in MOEA/DVA is not so precise. Detections are conducted more than one time and several sets of values a1, a2, b1, b2 are sampled. Thus, we use graph-based differential grouping (gDG) [26] and extend it to MOLSOPs. gDG is performed with respect to each objective, and the difference values (|?1 ??2|) are stored in a matrix, based on which we summarize the difference values and obtain a nDim-tuple Vd for all variables; here, nDim is the number of variables.

We suppose that SPos, SMix and SDis are the sets of position variables, mixed variables and distance variables [27], respectively. |S| denotes the cardinality of set S. For the diversity category, there are three cases: ? If 0 < |SPos| + |SMix| < nDim, we classify all  position variables and mixed variables into the diversity category;  ? Else if 0 < |SPos| < nDim, we classify all position variables into the diversity category;  ? Else, find the maximum value Vd,max in Vd, for each variable i, if Vd,i > 0.2? Vd,max, it is classified into the diversity category.

The remaining variables will be in the convergence category.

The results of property analysis of MOEA/DVA and DPCC-  MOEA for all instances (3 objectives, 1000 variables) in test suites DTLZ (Deb-Thiele-Laumanns-Zitzler) [30] and WFG (Walking-Fish-Group) [31] after 20 runs are listed in Table

I. And we can see DPCCMOEA can correctly recognize all variables in the diversity category; while for MOEA/DVA, the results are not so good, as for WFG3, WFG7 and WFG9, more    1551-3203 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TII.2017.2676000, IEEE Transactions on Industrial Informatics   variables (21.25, 999.9 and 985.35 on average, respectively, which are much more than 4) are incorrectly recognized and the recognition is a little unstable.

After the interdependence analysis, we group variables according to their properties and the interactions among them [26], [27]. First, all variables in the diversity category are allocated to a single group. As for the MOP formulated in Eq. 3, x1 and x2 will be in the same group. Variables in the convergence category will be separated to several groups according to interactions among them.

In addition, we collect all independent variables in a sepa- rate group, and if its size is too large, we split it with respect to a predefined threshold NGroupth . For other groups, as long as the size of two smallest groups is below the threshold, we combine them so that the groups will not be too small.

Thus, there will not be too many groups with few variables, which will result in the requirement of a large number of MPI processes (CPUs). Also, as long as the number of variables in the largest group is above the threshold, we split it. Therefore, each group will have a reasonable number of variables.

C. Optimization of the Population  The optimizer used is differential evolution (DE) [32], [33].

The optimization is actually conducted in the sets (Fig. 2).

The optimization process can be described as follows.

childi,j = pi,j + F ? (pa1,j ? pa2,j) if j ? Index (4)  where pi is the parent vector; i is the subscript of the newly generated vector childi, which is selected by binary tournament; j is the subscript of the decision variable; a1 and a2 denote the randomly selected solutions other than i; and Index is the set of variables to be optimized for the current set in the current generation.

Each set optimizes the diversity category and its allocated convergence group in a mixed way, and the switch between them is determined by the average improvement of individuals.

To perform the fitness assessment, we should combine the remaining variables with the generated offspring to form a complete solution. As we also store the complete variable information about the individuals in each set, newly generated vectors will be integrated with this information to obtain a complete solution.

To enhance the exploration of the solution space and im- prove diversity, two vectors are used, which can be other stored complete solutions.

traili,j =  ??????? childi,j if j ? Index pi,j if j /? Index  ? r1 ? 0.5  pb1,j if j /? Index ? r1 > 0.5  ? r2 ? 0.5  pb2,j otherwise (5)  where traili is the trail vector; r1 and r2 are uniform random numbers within the range of (0, 1); and b1 and b2 denote the randomly selected solutions other than i. Then polynomial mutation is performed on traili. After the fitness evaluation, the population updating is similar to that in MOEA/D.



IV. PARALLEL IMPLEMENTATION AND COMMUNICATION TOPOLOGY  As described in the previous section, the decomposition structure is two-fold (Fig. 2). Based on this, we build the parallel structure.

MPI is a powerful tool for distributed parallelism. By sending and receiving messages, different CPUs communicate with each other and cooperatively complete the task. The heavy computational burden of tackling MOLSOPs can be allocated to large numbers of CPUs and the operation time will be greatly reduced.

The communication among CPUs is also time-consuming, too much information exchange will cancel out the speedup from computation allocation. Therefore, we should properly design the communication topology and choose the exchanged information.

A. Parallel Implementation In our experimentation, the maximum Fitness Evaluations  (FEs) are set to 1e4?nDim, while the FEs used for interde- pendence analysis are nDim ? (nDim + 1). The percent of FEs for interdependence analysis PFEsd is about  PFEsd = nDim? (nDim+ 1)  1e4? nDim ? 100% (6)  As nDim = 1000, PFEsd ? 10%. This seems not large.

However, if we only parallelize the other part of the algorithm, the serial interdependence analysis process will be the major consumer of time. For example, if 100 CPUs are used for par- allelization, the percent of computation time interdependence analysis takes will be  PTIMEd = PFEsd  PFEsd + (1? PFEsd )/100 ? 100% (7)  If nDim = 1000, PTIMEd ? 97.56%, which greatly hinders the computation time reduction of the parallelism.

Thus, we parallelize interdependence analysis with the help of MPI.

In the following, we will describe the parallel implementa- tion in a top-down fashion according to Fig. 2.

Assuming there are NMPI CPUs available for parallelism, for the population decomposition, CPUs are allocated to each species.

NMPIi = N MPI/NSpecies, i ? 1, 2, ..., NSpecies (8)  where NMPIi is the number of CPUs available for species i.

Then, in each species, the individuals are divided into  multiple sets (subspecies).

NSi,j = NP/N MPI i , j ? 1, 2, ..., NMPIi (9)  where NSi,j is the size of set (subspecies) j of species i and NP is the population (species) size.

Through the above allocation of CPUs in a top-down manner, each CPU is in charge of a set. All CPUs can optimize the MOP in parallel, and the operation time can be greatly reduced.

1551-3203 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TII.2017.2676000, IEEE Transactions on Industrial Informatics   B. Communication Topology  CPUs exchange information according to the von Neu- mann topology, where the CPU nodes can be seen as a two-dimensional grid and each node is connected to four nodes (Fig. 4). To identify the neighbors, the step has to be computed, and according to the step, we decide to which process the current process should send information and from which process it should receive information.

step = ? NCPU (10)  ??????? Ri,a = (Ri ? step+NCPU )%NCPU Ri,b = (Ri + step)%NCPU Ri,l = (Ri ? 1 +NCPU )%NCPU Ri,r = (Ri + 1)%NCPU  (11)  where step is the step size, which is the length of each side of the two-dimensional square grid; Ri, i ? {1, 2, ..., NCPU} is the rank of MPI process i, which is the ID for CPU i; Ri,a, Ri,b, Ri,l and Ri,r are the ranks of the four neighbors: the above one, the below one, the left one and the right one.

Fig. 4. Von Neumann topology.

Information exchange among CPUs can be summarized as follows: ? Within species  As individuals refer to their neighbors for evolution and the division of species to sets will separate adjacent individuals, each set (CPU) transmit the individual in- formation to its adjacent sets (CPUs).

? Beyond species All species exchange information with each other and the von Neumann topology is used. The whole individual information is transferred, according to which the receiver species updates its individuals.

? Global We form a communication topology among all CPUs.

During the evolution, all CPUs will exchange information according to the von Neumann topology, which includes their several best individuals according to the fitness value and the most potential individual according to the utility.



V. EXPERIMENTAL STUDY  A. Parameter Setting  In the comparison of the algorithms, we contrast the proposed DPCCMOEA with two state-of-the-art cooperative coevolutionary MOEAs for tackling MOLSOPs: CCGDE3 [20] and MOEA/DVA [27].

For fair comparison, we set the population sizes of all three algorithms to NP = 120. CCGDE3 is based on fixed grouping, we simply set the number of groups to 2 and each group has NP/2 individuals. In MOEA/DVA, the number of control variable analyses and the number of interdependence analyses are set to NCA = 20 and NIA = 6, respectively.

In DPCCMOEA, we set NCA = 20 and NIA = 1; and the group size threshold is NGroupth = 111.

The number of variables in each test problem is nDim = 1000, the objective number is nObj = 3 and the maximum number of FEs is NmaxFE = 1e4? nDim.

In MOEA/DVA and DPCCMOEA, the neighborhood size and replace limit of each new solution are set to 0.1 ? NP and 0.01?NP , respectively.

For DE, in CCGDE3 and DPCCMOEA, F and CR are set to 0.5 and 1.0, respectively. SBX and polynomial mutation are used in MOEA/DVA and polynomial mutation is used in DPCCMOEA. The distribution indices are both set to 20, and the SBX probability is set to 1.0. The polynomial mutation probability is set to 1.0/nDim.

The proposed algorithm is based on the distributed MPI framework. The testing platform is the Tianhe-2 supercomput- er. There are 24 cores in each node. In the experimentation, we make use of 15 nodes; that is, we use 360 cores. Thus, there are 360 CPUs in operation.

All three algorithms in the comparison run 20 times for each test instance.

B. Multi-objective Test Suite  The multi-objective test suites used in our experiment are: DTLZ [30] and WFG [31]. For more information about the test suites, please refer to the supplementary material.

C. Performance Metric  To compare the performances of these algorithms, we adopt the inverted generational distance (IGD) metric and the hypervolume (HV) indicator [34] (denoted as IIGD and IHV , respectively), which can comprehensively assess the conver- gence and diversity of the solutions. IGD has the following definition:  IIGD(P ?,A) =  ? p?P ?  d(p,A)  |P ?| (12)  where P ? is a subset of points uniformly sampled from PF .

A, generated by the optimization algorithm, is the approxima- tion of the PF . |P ?| is the cardinality of P ?; p is an element of P ?; d(p,A) is the minimal Euclidean distance between p and the elements in A. The smaller the IIGD value, the better the performance; while for the IHV value, the opposite situation is true.

1551-3203 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TII.2017.2676000, IEEE Transactions on Industrial Informatics   D. Statistical Comparison  To systematically measure the results of the algorithms, we utilize statistical analysis. For the characteristics of the stochastic algorithms, we adopt the nonparametric test to evaluate the algorithms [35], [36].

E. Comparison with CCGDE3 and MOEA/DVA  The results of IGD and HV indicators are listed in Tables II and IV, respectively, and the corresponding average rankings of the nonparametric Friedman test are listed in Tables III and V.

TABLE II AVERAGE IGD VALUES OF THE APPROXIMATED PF FOUND BY  CCGDE3, MOEA/DVA AND DPCCMOEA  CCGDE3 MOEA/DVA DPCCMOEA  DTLZ1 2.07E+03 (1.83E+03) 2.20E-01 (1.07E-02) 1.30E+03 (1.71E+02) DTLZ2 5.19E+00 (6.14E-01) 4.87E-02 (2.07E-08) 4.80E-02 (3.10E-04) DTLZ3 4.55E+03 (4.85E+03) 8.15E-01 (3.51E-02) 2.04E+03 (4.79E+02) DTLZ4 4.25E+00 (9.00E-01) 1.26E-01 (5.89E-08) 2.77E-02 (7.38E-04) DTLZ5 5.40E+00 (7.74E-01) 3.30E-03 (1.40E-08) 1.66E-02 (8.20E-05) DTLZ6 4.60E+02 (1.68E+01) 3.43E+02 (2.06E+00) 1.51E-02 (1.25E-04) DTLZ7 2.66E+00 (9.02E-01) 1.04E-01 (4.84E-06) 7.05E-02 (1.25E-03)  WFG1 1.59E+00 (1.40E-01) 2.64E+00 (6.97E-04) 1.11E+00 (8.58E-03) WFG2 5.73E-01 (1.26E-01) 2.05E-01 (1.51E-04) 2.43E-01 (1.05E-02) WFG3 5.11E-01 (6.49E-02) 6.22E-02 (1.20E-02) 1.88E-01 (6.09E-03) WFG4 8.18E-01 (1.90E-01) 2.27E-01 (8.59E-07) 1.57E-01 (1.31E-03) WFG5 3.33E-01 (3.49E-02) 2.76E-01 (1.24E-05) 2.10E-01 (1.07E-03) WFG6 1.76E+00 (1.64E+00) 2.77E-01 (3.76E-04) 1.82E-01 (6.58E-04) WFG7 8.13E-01 (7.37E-02) 1.76E-01 (9.09E-04) 1.74E-01 (1.03E-03) WFG8 8.53E-01 (9.96E-02) 2.13E-01 (1.14E-03) 2.09E-01 (2.01E-03) WFG9 3.49E-01 (1.55E-01) 2.07E-01 (3.61E-03) 1.93E-01 (2.80E-03)  Note1: The numbers in parentheses represent the standard deviations.

Note2: The numbers in bold are the best average IGD values for the corresponding test instances.

TABLE III AVERAGE RANKING OF THE ALGORITHMS (FRIEDMAN) WITH RESPECT  TO IGD VALUES  Algorithm Ranking Final Ranking  DPCCMOEA 1.3125 1 MOEA/DVA 1.7500 2  CCGDE3 2.9375 3  From the IGD values in Table II, we can see that CCGDE3 performs the worst in all test instances, MOEA/DVA is the second best, and DPCCMOEA is the best, which is also illustrated by the average ranking of the Friedman test in Table III.

In the DTLZ test suite, DTLZ1 has a very simple structure.

However, the function g(xM ) has the following form:  g(xM ) =   [ |xM |+  ? xi?xM  (xi ? 0.5)2 ? cos(20?(xi ? 0.5)) ] (13)  similar to Rastrigin?s Function:  f(x) = D? i=1  (x2i ? 10 cos(2?xi) + 10) (14)  which has a large number of local optima. Thus, CCGDE3 and DPCCMOEA can be easily trapped in the local optima, but MOEA/DVA performs much better. In DTLZ2, trigonometric  functions are introduced, but g(xM ) = ?  xi?xM (xi ? 0.5) 2,  which is quite simple, and all algorithms perform much better and DPCCMOEA is slightly better than MOEA/DVA. Based on DTLZ2, DTLZ3 replaces g(xM ) by that in DTLZ1, so CCGDE3 and DPCCMOEA perform as bad as DTLZ1 and MOEA/DVA is still much better. DTLZ4 maps the variables nonlinearly based on DTLZ2. Compared to DTLZ2, both CCGDE3 and DPCCMOEA have better performances, but MOEA/DVA performs much worse. For DTLZ5, there is a degeneration mapping compared to DTLZ2 and the PF is a curve. Based on DTLZ5, DTLZ6 replaces g(xM ) =?  xi?xM (xi ? 0.5) 2 by g(xM ) =  ? xi?xM x  0.1 i , so a non-  linear mapping is performed to the variables. From the opti- mization results of the three algorithms, DPCCMOEA is stable for DTLZ5 and DTLZ6. In DTLZ7, the PF is disconnected, for which, DPCCMOEA performs the best. In summary, we can know CCGDE3 and DPCCMOEA are sensitive to local optima (DTLZ1 and DTLZ3), while MOEA/DVA cannot be easily trapped in inferior local optima; for non-linear transfor- mation (DTLZ4 and DTLZ6), MOEA/DVA does not perform equally well, CCGDE3 is not stable, while DPCCMOEA is the best; for the degeneration (DTLZ5) of PF , all algorithms per- form well; and for disconnections (DTLZ7) in PF , CCGDE3 and MOEA/DVA perform worse than DPCCMOEA.

For the WFG test suite, there are complex transformations on both the variables and the shape of the PF , and the differences among algorithms are not obvious. Overall, DPC- CMOEA performs best, MOEA/DVA is slightly worse and CCGDE3 is the worst.

TABLE IV AVERAGE HV VALUES OF THE APPROXIMATED PF FOUND BY  CCGDE3, MOEA/DVA AND DPCCMOEA  CCGDE3 MOEA/DVA DPCCMOEA  DTLZ1 4.73E-01 (4.16E-01) 1.00E+00 (4.92E-10) 7.13E-01 (9.64E-02) DTLZ2 9.24E-01 (1.62E-02) 9.99E-01 (2.25E-09) 1.00E+00 (2.80E-08) DTLZ3 5.70E-01 (4.07E-01) 1.00E+00 (8.91E-09) 9.48E-01 (3.00E-02) DTLZ4 8.59E-01 (9.65E-02) 9.92E-01 (1.31E-08) 9.99E-01 (3.07E-07) DTLZ5 7.68E-01 (6.41E-02) 9.89E-01 (1.61E-08) 9.89E-01 (1.21E-06) DTLZ6 7.68E-01 (1.54E-02) 8.98E-01 (1.20E-03) 1.00E+00 (4.37E-10) DTLZ7 5.47E-01 (6.29E-02) 8.21E-01 (7.25E-06) 8.29E-01 (1.64E-05)  WFG1 7.30E-01 (2.12E-02) 6.34E-01 (2.43E-04) 7.91E-01 (1.02E-03) WFG2 8.87E-01 (1.09E-02) 8.70E-01 (1.02E-04) 9.71E-01 (1.19E-03) WFG3 8.37E-01 (1.36E-02) 8.78E-01 (1.28E-02) 8.96E-01 (1.02E-03) WFG4 8.55E-01 (1.49E-02) 7.22E-01 (8.60E-07) 9.69E-01 (6.77E-04) WFG5 8.73E-01 (1.41E-02) 8.59E-01 (7.24E-06) 9.50E-01 (8.57E-04) WFG6 9.28E-01 (4.05E-02) 8.90E-01 (2.21E-04) 9.71E-01 (7.51E-05) WFG7 8.06E-01 (2.22E-02) 9.67E-01 (2.68E-04) 9.72E-01 (8.24E-05) WFG8 8.14E-01 (1.60E-02) 9.57E-01 (3.97E-04) 9.61E-01 (8.13E-04) WFG9 8.77E-01 (3.54E-02) 9.37E-01 (2.71E-03) 9.47E-01 (3.72E-03)  Note1: The numbers in parentheses represent the standard deviations.

Note2: The numbers in bold are the best average IGD values for the corresponding test instances.

TABLE V AVERAGE RANKING OF THE ALGORITHMS (FRIEDMAN) WITH RESPECT  TO HV VALUES  Algorithm Ranking Final Ranking  DPCCMOEA 1.1875 1 MOEA/DVA 2.1250 2  CCGDE3 2.6875 3  The results of the HV indicator are similar to that of IGD except WFG2 and WFG3. The reason is that, from the    1551-3203 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TII.2017.2676000, IEEE Transactions on Industrial Informatics   visualizations of WFG2 in the supplementary material, the distribution of solutions obtained by MOEA/DVA is quite worse compared to DPCCMOEA; for WFG3, the PF is a line, and solutions of MOEA/DVA are distributed on the line while there are some solutions of DPCCMOEA scattered near the line.

TABLE VI COMPUTATIONAL TIME TAKEN BY  CCGDE3, MOEA/DVA AND DPCCMOEA  CCGDE3 MOEA/DVA DPCCMOEA  DTLZ1 7.93E+03 (3.54E+01) 2.13E+03 (9.51E+00) 2.24E+02 DTLZ2 6.89E+03 (3.04E+01) 7.41E+02 (3.27E+00) 2.27E+02 DTLZ3 7.51E+03 (3.50E+01) 2.15E+03 (1.00E+01) 2.15E+02 DTLZ4 6.62E+03 (2.97E+01) 8.13E+02 (3.65E+00) 2.23E+02 DTLZ5 6.78E+03 (1.92E+01) 7.64E+02 (2.16E+00) 3.53E+02 DTLZ6 1.43E+04 (5.99E+01) 4.99E+03 (2.09E+01) 2.39E+02 DTLZ7 6.89E+03 (3.13E+01) 7.05E+02 (3.20E+00) 2.20E+02  WFG1 2.54E+04 (9.24E+01) 1.67E+04 (6.08E+01) 2.74E+02 WFG2 1.22E+04 (5.13E+01) 7.36E+03 (3.10E+01) 2.38E+02 WFG3 1.31E+04 (5.40E+01) 7.32E+03 (3.01E+01) 2.43E+02 WFG4 1.72E+04 (6.48E+01) 1.13E+04 (4.25E+01) 2.65E+02 WFG5 1.52E+04 (5.84E+01) 8.36E+03 (3.21E+01) 2.61E+02 WFG6 7.39E+05 (3.08E+02) 7.34E+05 (3.06E+02) 2.40E+03 WFG7 1.37E+04 (5.39E+01) 1.01E+04 (3.96E+01) 2.55E+02 WFG8 2.83E+05 (2.69E+02) 2.80E+05 (2.66E+02) 1.05E+03 WFG9 1.02E+06 (3.15E+02) 1.01E+06 (3.13E+02) 3.24E+03  DTLZ 5.69E+04 (3.35E+01) 1.23E+04 (7.23E+00) 1.70E+03 WFG 2.14E+06 (2.60E+02) 2.09E+06 (2.54E+02) 8.22E+03  SUM 2.20E+06 (2.21E+02) 2.10E+06 (2.12E+02) 9.92E+03  Note: the numbers in parentheses represent the speedup ratio.

For the operation time and speedup in Table VI, because DPCCMOEA is conducted in an MPI distributed environment, it takes much less time than CCGDE3 and MOEA/DVA.

The speedups of DPCCMOEA compared to CCGDE3 and MOEA/DVA are 221 and 212, respectively, which are about 61.4% and 58.9% of the ideal speedup (360). By comprehen- sively considering the time consumed in serial implementation of CCGDE3 and MOEA/DVA, we save a large amount of time.



VI. CONCLUSION AND PROSPECT  In this paper, we have proposed an MPI-based distributed parallel cooperative coevolutionary multi-objective evolution- ary algorithm for large-scale optimization, denoted DPCC- MOEA. Through a series of decompositions, DPCCMOEA decomposes complex MOLSOPs into simpler low-dimensional subproblems. Additionally, using a 2-layer MPI parallel struc- ture, we can evolve subproblems in parallel, greatly reducing operation time. We compare DPCCMOEA with CCGDE3 and MOEA/DVA in two aspects: optimization performance and operation time. Experimental results showed that the proposed algorithm not only obtains better optimization results but also significantly reduces the time consumption. For future work, we will test the effectiveness of DPCCMOEA on wireless sensor network deployment problems, etc.

