Mining Frequent Closed Itemsets from Distributed Dataset

Abstract   In this paper we address the problem of mining frequent closed itemsets in a highly distributed setting.

The extraction of distributed frequent (close) itemsets is an important task in data mining. The paper shows how frequent closed itemsets? mined  independently in each site, can be merged in order to derive globally frequent closed itemsets. Unfortunately, as distributed setting is various, it is unreasonable to adopt only one mining approach. The paper analyzes the distributed setting from three perspectives: 1.communication bandwidth; 2.site quantity; 3 the characteristic of each site dataset, and presents two mining approaches and their algorithms in their corresponding distributed setting. The experiment results indicate that our algorithms are efficient to their corresponding distributed setting.

1. Introduction   Frequent Itemset Mining (FIM) is the most important and demanding task in many data mining applications. It requires to discover all the itemsets in a given database D which have a support higher (or equal) than a given minimum frequency threshold minsup. The FIM problem has been extensively studied in the last years. The first proposed algorithm is Apriori [1] and many different approaches have been proposed such as DepthProject [2],FP-GROWTH [3]and many others. One of the main issues emerging from these studies regards the size of the collection of frequent itemsets F while decreasing of the minimum support thresholds. It makes the mining task very hard at very low support thresholds.

Frequent closed itemsets mining (FCIM) is a solution to this problem. Frequent Closed itemsets is a small subsets of frequent itemsets, but they represent the same knowledge in a more succinct way. Using closed itemsets, we implicitly benefit from data correlations which allow to strongly reduce problem  complexity. Moreover association rule extracted from closed sets have been proved to be more concise and meaningful, because all redundancies are discarded.

Currently many efficient frequent closed itemsets mining(FCIM) algorithms have been proposed, such as A-CLOSE [4],CHARM [5], CLOSET+ [6].

While many papers address the problem of FCIM or parallel/distributed FIM, there are few proposals for distributed closed itemsets mining exists. In this paper, we address the problem of mining frequent closed itemsets in a distributed dataset. In order to satisfy the mining task under the real distributed setting, we consider of two different distributed settings.  One of different distributed settings has few sites or has small bandwidth or store huge data in site dataset, the other is reverse. We have presented a distributed mining approach respectively in term of two different distributed settings.

2. Basic Conceptions  2.1 Frequent and Closed Itemsets   Given a dataset D, D={t1,t2?,tn} contains a finite set of transactions. Let I={a1,a2,?,an}be a finite set of items where each transaction t is a subset of I. Given a itemset X, let sup(X) be its support, defined as the number of transactions in D that include X, we call X is frequent itemsets if sup(X) ? minsup ? DB, where 0<minsup ? 1 is a given minimum support threshold.

To define the conception of closed itemset, we use description of reference [7], it gives two following functions f and s:  ( ) { | , } ( ) { | , }  f T i I t T i t g A t D i A i t  = ? ? ? ? = ? ? ? ?  Where T D? and A I? , function f returns the set of items which appears in all the transactions of T, while function g returns the set of transactions which supports a given itemset A.

Definition 1: An itemset X is closed if and only if  ( ) ( ( )) ( )c X f g X f g X X?= = =  2008 International Symposium on Computational Intelligence and Design  DOI 10.1109/ISCID.2008.24   2008 International Symposium on Computational Intelligence and Design  DOI 10.1109/ISCID.2008.24     Where the composite function c= f g? is called Galois operator or closure operator.

Definition 2 if an itemset X is both closed and frequent, then X is said to be frequent closed itemsets.

Lemma 1 given a dataset D, there is one and only one frequent closed itemset which include frequent itemset X and have the same support with X [8].

From Lemma 1, it is easily getting a principle to judging a itemset is closed or not. The principle is as below: Principle 1 we assume itemset X and Y are both frequent, if X Y? and sup(X) =sup(Y) then X is not frequent closed itemset.

2.2 Distributed Frequent Closed Itemsets Mining   In the distributed system, there are several data sources iD (i=1,2,?,n) and each iD  are partitioned on  one site iS (i=1,2,?,n),for the sake of discussing ,we  give a virtual dataset ,iD D= ? for any closed item X,

X.sup is the support of X in D ,and X.supi is the support of X  in iD .

Definition 3 given a closed itemset X, if . sup min supX D? ? , X is said to be globally frequent  closed itemset; if . sup min sup i  i X D? ? , X is locally  frequent closed itemset on site i  S .

Lemma 2 if X is globally frequent closed itemset, and then X must be locally frequent in at least one site  i S .

Proof. Suppose X is not locally frequent closed on  any site, then . sup min sup i  i X D< ? ,we can get  1 1    . sup min sup  . sup . sup  min sup min sup  n n i  i  i i  n i  i  n  i  i  X D  X X  D D  = =  =  =  < ?  =  ? = ?  ? ? ? ? ? ? ?  ? ?  ?  ?    So . sup min supX D< ? , X would not be a frequent closed itemset, and this is in contradiction with the hypothesis.

Lemma 3 there are n sites in the distributed system.

Li ( 1 i n? ? ) is the locally FCI on site Si( 1 i n? ? ),  '   n  i iL L== ?  then '  L  must be the supersets of the global frequent closed itemsets L .

Proof: it can be proved easily by Lemma 2.

The main task of mining frequent closed itemset  from distributed dataset is: firstly parallel mining  locally frequent closed itemsets on every site i  S .

Secondly merge the local result to derive the globally frequent closed itemsets.

2.2.1 Identities of Frequent Closed Itemsets   FCIM not only judge X?s frequence, but also judge  X?s closure. When merging each local results, two problem must be solved. The first is the associated supports could be smaller than exact ones, because some globally FCI might be not locally frequent in some site; the second is it may produce a superset of all the FCI, as for the first problem, different distribute settings adopts different solution; as for the second problem, we adopts subset examination technology to judge X?s closure, it performs as below  Step one. Sort the candidate FCI according to the length of itemsets.

Step two: get the shortest item X and compare X to the items behind , if there is a item Y and meet X Y? and X.sup=Y.sup condition. according to  principle 1, X will be pruned, then compare Y to the items behind, repeat the same process with X,to scan the candidate closed itemsets one time, it would generate one closed item in terms of Lemma 1.

Step three: repeat the above process until all the candidate itemsets are closed itemsets.

3. Distribute Frequent Closed Mining Approach and Its Algorithm  3.1 Analyse the Character of Distribute Setting   In reality, distributed settings are various. We can analyze them from three aspects: 1.site quantity; 2.the bandwidth between sites; 3.the characteristics of dataset, characteristics includes whether it is dense or sparse, transactions? quantity etc. Based on the above analysis, we consider of two different distributed settings. The first distributed setting is that there are few sites or has small bandwidth between sites or the local FCI is very large.  The second distributed setting is reverse. As for the first distributed setting, the communication load can affect the mining efficiency much compared with the communication frequency.

As for the second distributed setting, the communication frequency can affect mining efficiency much compared with the correspondence load.  In the following, we give a distributed mining approach respectively in term of these two different distributed settings .

3.2 Distributed Frequent Closed Mining Regarding the First Distributed Setting   Approach:  Firstly every site sends its locally FCI to other sites for collecting their support, after knowing its support of all sites, do globally frequent pruning on each site, after pruning every site ,it can generate the globally FCI. Secondly choose the biggest bandwidth site as a meta-learning [9]site S , other sites send their local results to S .Such merging may produce a superset of all the FCI, adopt subset examination technology to solve the problem . Finally get the globally FCI. This mining approach can generate the globally FCI on each site, and communication load between site is small.

however, communication is frequent between sites. We give the algorithm DMCI (distributed mine closed itemsets) as below  DMCI algorithm Input: distributed data sources ( 1, 2, ..., )iD i n= ;  minimum support threshold minsup Output: the globally frequent closed itemsets  (1) for i=1,2,?,n do{ (2) L(i)=Local-Mining( , min sup  i D )  (3) For j=1,2,?,n and j i?  do{ (4) Send L(i) to  j D  (5) if L(j) has been mined from j  D {  (6) ( )X L i? ?  if ( )X L j?  write down . sup jX (7) scan  j D and check L(i);  } (8) else{ (9) scan  j D and check L(i)  } }  (10)  receive(X.supi) ( ( )X L i? )  (11)  if .sup min supiX D< ?  then delete X }  // following code were performed on meta-learning site (12)  L=receive(X,X.supi) // adopt subset examination technology (13)  L=check_close(L) } (14) return L;   3.3 Distributed Frequent Closed Mining Regarding the Second Distributed Setting   Approach: Firstly choose the biggest bandwidth site as a meta-learning site S, merge every local results to S. then collect its support of the candidate globally  itemsets which is infrequent on some site, accumulate the support of the candidate FCI for all sites, do globally frequent pruning on S. Secondly judge itemsets?s closure by subset examination technology .Finally generate the globally FCI.

Modify DMCI algorithm to get the algorithm for this mining approach. It is called DMCI-ML algorithm  DMCI-ML algorithm Modify the DMCI algorithm?s pseudo codes from step 3 to step 11 (3)  receive (i,L(i)) /* on meta-learn site, collect  itemsets and their support from every site */  } (4) for i=1 to n do{ (5) C(i)={M|find infrequent itemsets for ith site from  the candidate frequent itemsets}; (6) send(i,C(i)); /* send C(i) to i site  */ (7) scan dataset Di a time, and get support of itemset  which is in C(i) (8)  receive(i,C(i)); /* on meta-learn site, collect  support of itemset which is in C(i) */  } // on meta-learn site , prune itemsets that is infrequent (9) if . sup min_ supX DB< ?  delete X ( )X L?  4. Experience and Analyze   We carried out four sets of experiment. The first set compared DMCI with DMCI-ML on a range of site values in Ethernet-100 network, which simulated the distributed setting that has high bandwidth and many sites. The second set compared DMCI with DMCI-ML on a range of support threshold values in Ethernet-100 network which simulated the distributed setting that has high bandwidth and huge translation number. The third set compared DMCI with DMCI-ML on a range of site values in NON-LAN which simulated the distributed setting that has low bandwidth and many sites. The fourth set compared DMCI with DMCI-ML on a range of support threshold values in NON-LAN which simulated the distributed setting that has low bandwidth and huge translation number. We ran our experiments on a cluster of PentiumIII 1000MHZ CPU, 256MB Memory, 20GB HD PC.(PC machine represents site) The four experiments are performed with synthetic databases produced by IBM Almaden [10], every site is stored 100k transaction data, and adopts A-CLOSE algorithm [4] to mine locally FCI.

min_sup=1%  0.5  1.5  2.5  3.5  8 10 12 14 16  site number  t h e  e x e c u t i o n  t i m e  r a d i o   Figure 1 compare DMCI with DMCI?ML on a range of site values in Ethernet-100 network when min_sup=1%.

site number=4  1.45  1.5  1.55  1.6  1.65  1.7  0.5 1 1.5 2 2.5  support threshold(%)  t h e  e x e c u t i o n  t i m e  r a d i o   Figure 2 compare DMCI with DMCI-ML on a range of support threshold values in Ethernet- 100 network when site number=4   min_sup=1%   0.2  0.4  0.6  0.8   1.2  1.4  1.6  8 10 12 14 16  site number  th e  ex e cu  ti o n  ti m e  ra d io   Figure 3 compare DMCI with DMCI-ML on a range of site values in NON-LAN when min_sup=1%   site number=4  1.1  1.2  1.3  1.4  1.5  1.6  0.5 1 1.5 2 2.5  support threshold(%)  t h e e  x e c ut  i o n t  i m e  r ad  i o   Figure 4 compare DMCI with DMCI-ML on a range of support threshold value in NON-LAN when site number=4.

The figure 1 shows the execution time radio of DMCI algorithm and DMCI -ML algorithm on a range of site values under the same support in Ethernet-100 network. Due to the same Ethernet-100 network, the communication bandwidth is very large, so the most crucial factor that affects performance of mining is the number of communication rather than communication load. For DMCI algorithm, its communication frequency complexity is O (n2), while for DMCI-ML algorithm, its communication frequency complexity is no more than O (n). The execution time radio is larger and larger with increasing of the number of site.

The figure 2 shows the execution time radio of DMCI algorithm and DMCI -ML algorithm on a range of support threshold values under the same site?s number in Ethernet-100 network, the execution time radio is slightly decline with decreasing of support threshold. The reason for decline is that the amount of patterns becomes larger and larger with decreasing of support threshold, which leads DMCI-ML algorithm dealing with much more patterns than DMCI algorithm does. Each time the communication load of the DMCI- ML algorithm is the sum of patterns from all sites, however, the communication load of DMCI algorithm are patterns of each site.

The figure 3 shows the execution time radio of DMCI algorithm and DMCI-ML algorithm on a range of site values under the same support threshold in NON-LAN, the execution time radio is trend to decline with increasing of site?s number, in NON-LAN environment, the communication bandwidth is low, so the communication load also affects performance of mining, and the communication load of DMCI algorithm is much smaller than that of DMCI-ML algorithm.

The figure 4 shows the execution time radio of DMCI algorithm and DMCI-ML algorithm on a range of support threshold values under the same site?s     number in NON-LAN, the execution time radio is decline faster than that in figure 3, the reason for decline faster is that  as the site number is invariant in figure 4, the different value of communication of two algorithms is invariant , while in figure 3 the different value of communication of two algorithms is become larger and larger with of increasing site?s number. The reason for decline is the same with figure 3  5. Conclusion   We analyze the distributed setting and have presented a mining approach and its algorithm respectively on two different kinds of distributed settings. Compare with frequent itemsets, closed frequent itemsets are more valuable to find non- redundant association rules and are helpful to make correct decision on each site. However, in the distributed setting, local FCIM is still a time consume task, moreover in order to find the globally frequent closed itemsets, it has to scan dataset several times.

Future works regards improving mining efficiency on each site and decreasing of the scan-number.

