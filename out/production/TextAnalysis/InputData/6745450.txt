Sparsity-based Representation for Categorical Data Remya Menon, Shruthi S Nair, Srindhya K, M R Kaimal

Abstract?Over the past few decades, many algorithms have been continuously evolving in the area of machine learning. This is an era of big data which is generated by different applications related to various fields like medicine, the World Wide Web, E-learning, networking etc. So, we are still in need for more efficient algorithms which are computationally cost effective and thereby producing faster results. Sparse representation of data is one giant leap toward the search for a solution for big data analysis. The focus of our paper is on algorithms for sparsity- based representation of categorical data. For this, we adopt a concept from the image and signal processing domain called dictionary learning. We have successfully implemented its sparse coding stage which gives the sparse representation of data using Orthogonal Matching Pursuit (OMP) algorithms (both Batch and Cholesky based) and its dictionary update stage using the Singular Value Decomposition (SVD). We have also used a pre- processing stage where we represent the categorical dataset using a vector space model based on the TF-IDF weighting scheme.

Our paper demonstrates how input data can be decomposed and approximated as a linear combination of minimum number of elementary columns of a dictionary which so formed will be a compact representation of data. Classification or clustering algorithms can now be easily performed based on the generated sparse coded coefficient matrix or based on the dictionary. We also give a comparison of the dictionary learning algorithm when applying different OMP algorithms. The algorithms are analysed and results are demonstrated by synthetic tests and on real data.

Index Terms?sparse representation, dictionary learning, sparse coding, cholesky decomposition, OMP, SVD

I. INTRODUCTION  M achine Learning algorithms have a very successful his-tory in data analysis. In this era of big data, these algorithms have to compete with the volume aspect both in the number of data as well as dimensionality-wise. So we need to devise new algorithms or extend the existing algorithms to suit the need of the hour. The focus of our paper is to find a solution which will ease the process of big data analysis by giving a compact representation and thereby also achieve computational cost effectiveness. Also, the data that is generated by different applications related to various fields like medicine, the World Wide Web, E-learning, networking etc are mostly categorical in nature. We focus our attention on an approach that is currently being adopted in the field of image and signal processing called the sparsity-based representation.

We extend this approach to the categorical domain where data can be both single-valued as well as multi-valued (set-valued).

The main objective of sparsity-based representation is to come  up with the most compact representation of data so that it helps in efficient memory utilization when used as input to different classification or clustering techniques.

The general notations that we use throughout the paper are up- percase letters (D, H) to indicate matrices, vectored lowercase letters (~y, ~?) to indicate column vectors and lowercase letters to indicate size of the matrices and count of the elements.

We know that in the context of a linear equation solver, if we have a set of equations in three unknowns (?1, ?2, ?3), as given in Eqn (1),  d1?1 + d2?2 + d3?3 = y1 d3?1 + d4?2 + d5?3 = y2 d2?1 + d1?2 + d6?3 = y3 (1)  we can rewrite this using matrix representation as follows:??? d1 d2 d3d3 d4 d5 d2 d1 d6  ??? ??? ?1?2 ?3  ??? = ??? y1y2 y3  ??? (2) Under a similar logic, if we have a data item, ~y ? <n, where n is the number of dimensions, we can decompose it as in Eqn (3).

~y = D~? (3)  where D is the dictionary and ~? is a sparse coefficient vector.

To further illustrate this approach, we first need to define the relevance of D and ~?. The dictionary, D ? <n?k, is an n?k matrix where each of its k columns, {~di}ki=1, indicating a basis function, can be linearly combined to generate ~y. ~? ? <k, where k is the number of dictionary columns, is the coefficient vector which represents ~y using D. As it is so called, the non- zero coefficients of ~? indicate which dictionary columns will be involved in the linear combination. If we apply a minimum constraint on the number of non-zero values (l0-norm) of ~?, we can also say that ~? indicates a sparse representation of ~y, a process which is well known as sparse coding. By setting up a minimum constraint, we are trying to achieve a reduction in the number of bases used to represent ~y. Vectors are often used to represent large amount of data which can be difficult to store or transmit. By using a sparse representation, the amount of space needed to store the vector would be reduced to a fraction of what was originally needed. Sparse representations can also be used to analyze data by showing how the column vectors in a given basis come together to regenerate the original data,  2013 IEEE Recent Advances in Intelligent Computational Systems (RAICS)     ~y. If we know D and ~? we must be able to regenerate ~y. Due to the presence of noise or outliers in the data, we can also say ~y ? D~?. Hence, our objective function can be defined as in Eqn (4).

min~??~??0 subject to ?~y ? D~??2 ? ? (4)  where ? is an error threshold that we need to recommend and ?.?2 is the l2 norm. Clustering techniques have adopted the l2 norm to measure the extend of dissimilarity between two data items. Likewise, our intention is to measure such a dissimilarity and minimize it.

The sparse coding can be derived by using any one of the pursuit algorithms like the matching pursuit [5], the focal underdetermined system solver (FOCUSS) [10], the basis pursuit [9] or the orthogonal matching pursuit(OMP) algorithm [4]. In order to achieve our objective function, dictionary convergence is necessary in addition to sparse coding. The dictionary has to be learned using the techniques like singular value decomposition [6] or the method of optimal directions (MOD) [14].

In a nutshell, the goal of our paper is to have a sparse representation of categorical data, performance comparison of dictionary learning with different sparse coding algorithms. We achieve our goal using a preprocessing phase which represents the example categorical dataset as a vector space model, dictionary learning phase which takes this input as vectors and iterates between two sub phases (the sparse coding phase and the dictionary update phase) until convergence in order to meet our objective function. In the sparse coding phase, we use the OMP and the Batch-OMP method where the orthogonal- ization is improved using the Cholesky decomposition. For the dictionary update phase, singular value decomposition (SVD) is adopted. Here, one column of the dictionary is updated in each iteration so that only one column needs to be pushed into the memory at a time, thereby reducing memory utilization.

In Section 2, we refer to the recent work done in the field of sparse-representation, dictionary design and describe different algorithms that were proposed for this task. Section 3 describes the solution approach, the adopted methodologies, algorithm used and its relation to previously proposed methods. The results of the algorithm on different datasets and performance comparison of Batch-OMP against OMP-Cholesky using a same data set are presented in Section 4. The conclusion of our work is given in Section 5.



II. RELATED WORK  Most of the related research work associated with our paper has its roots in the field of signal and image processing.

This goes way back to the late 1980s when heavy research began in the area of signal compression [7], [8], signal reconstruction [10], vector quantization [15], orthogonalization [13], linear inverse problems, machine learning algorithms for classification and clustering like the K-means, expectation- maximization (EM), support vector machines and the like. In signal and image processing, sparse representation was looked  into with the goal of denoising, compression and inpainting [6].

Dictionary learning is extensively used in the areas of informa- tion coding, classification and face recognition. Recent works include stage-wise K-SVD [1], label-consistent K-SVD [2] and discriminative K-SVD [3]. All the three work towards image classification and face recognition.

[5] gives a framework for incorporating both reconstruction and discrimination terms into dictionary learning. All the above four approaches use the K-SVD algorithm for dictionary updates. So we then focused our study to the implementation aspects of the K-SVD algorithm. For this we concentrated on [4], [6]. [4] discusses an efficient implementation of K-SVD algorithm, which both accelerates it and reduces its memory consumption. The two basic components of the implementa- tion are the replacement of the exact SVD computation with a much quicker approximation, and the use of the Batch- OMP method for performing the sparse-coding operations. The paper describes various matching pursuit algorithms like OMP, OMP-Cholesky and Batch-OMP. The paper describes how the batch-OMP algorithm can handle large set of signals.

While [4] gives more focus on the sparse coding phase of dictionary learning, [6] gives emphasis to the dictionary update phase. [6] designs the K-SVD algorithm as a generalization of K-means clustering.As these papers are successful in image classification, we extend the K-SVD to the domain of categor- ical data.

Different pursuit algorithms have been proposed in the past for sparse coding. [4] gives an elegant collection of these al- gorithms as also mentioned in [8], [11]. Matching pursuit [11] and OMP [8] are greedy algorithms that select the dictionary columns sequentially and computes the inner product with the signal to generate sparse coefficients. It uses the l0 norm. [9] describes a basis pursuit approach which replaces this l0 norm with the l1 norm. [10] is different from the previous papers by using the lp norm with p ? 1.

In MOD [14], the dictionary update is very simple. If we know the sparse coefficient for a data item, ~yi, then we can define the error as ei = ~yi ?D~xi. The overall mean square error is given by ?E?2F = ?Y ?DX?2F .

In [13], a dictionary is composed of a union of orthonormal bases, D = [D1, D2, ..., DL] where Dj , j = 1, ..., L, are orthonormal matrices. The residual matrix, Ej , is computed as Ej = Y ?  ? i6=j DiXi. If we consider ?Ej ? DjXj?2F  as the penalty term, then EjXTj = Dj . Further, EjX T j is  decomposed using Singular Value Decomposition (SVD).

Here, update is only done to the dictionary and not to the sparse coefficient, ~?. Both [13] and [6] use SVD for dictionary update but in [6], the SVD is also used to update the sparse coefficient so as to enable faster convergence.

[15] gives an outline of the vector space model, QR factoriza- tion, Singular Value Decomposition using the Gram-Schmidt     Orthonormalization method.

All the above related works indicate that while adopting these concepts, flexibility, simplicity and efficiency must be maintained.

In addition to the above mentioned desirable properties, our objective is to maintain less memory utilization.



III. SOLUTION APPROACH AND METHODOLOGY  Fig. 1: Architecture of Sparse Representation  Algorithm 1 Algorithm for Sparsity-based representation of categorical data  1: Input: Categorical dataset from a relational database as Y={??y i}, i = 1 to N .

2: Output: Sparse coefficient matrix A = { ~?i}, i = 1 to N and learned dictionary D from the dataset, Y .

3: Phase 1: Preprocessing 4: Convert the categorical dataset to a vector space model  using the TF-IDF weighting scheme.

5: Initialization Stage 6: Dictionary matrix D(0) ? <n?k is initialized with l2  normalized columns of weight matrix, Y .

7: Repeat the following two phases until convergence (min-  imum representation error, ?~y ? D~??2F 6 ?) 8: Phase 2: Sparse Coding 9: To compute the coefficient vector ~?i,  For each sample from Y , Call OMP (~y, D, threshold) or Call Batch-OMP (~y, D) so as to satisfy our objective function: min ~?i?~?i?0 subject to ?~y ? D~??22, for i = 1 to N  10: Phase 3: Dictionary Update 11: Each column k = 1, 2, ..., k in D is updated iteratively by  the method of Singular Value Decomposition (SVD).

In this section, we introduce our approach to the problem of sparse representation of categorical data and thereby the methodology used in achieving it. If we are given a sample dataset of N data items, given as Y ? <n?N , we need to decompose it as Y ? DA where D ? <n?k is the dictionary and A ? <k?N is the sparse coefficient matrix which gives the sparse representation for the dataset, Y thereby satisfying the objective function given in Eqn (4).

For attaining our goal, we divide the steps into three main  TABLE I: Sample Data  Documents Attribute 1 (Reactome ID) Attribute 2 (Interpro ID) Doc1 REACT 1497 IPR000276, IPR017452 Doc2 REACT 1698, REACT 1791 IPR000276, IPR016695  TABLE II: Sample Weight Matrix  Terms Doc 1 Doc 2 REACT 1497 0.3010 0  IPR000276 0 0 IPR017452 0.3010 0  IPR000276,IPR017452 0.3010 0 REACT 1698 0 0.3010 REACT 1791 0 0.3010  REACT 1698,REACT 1791 0 0.3010 IPR016695 0 0.3010  IPR000276,IPR016695 0 0.3010  phases as given in Fig. 1.

Phase 1 is the preprocessing phase which is used to generate  Algorithm 2 Basic-OMP 1: Function OMP (~y, D, threshold) 2: Output: Approximation vector, ~? 3: Start by setting the residual vector, ~r = ~y, index set I0 =  (), iteration number m = 1 and ~? = ~0 4: Let im = k, where k is derived from the solution of:  Argmax ??? ~dkT~rm???, where ~dk are the column vectors of  D  5: Update the set Im (indices of selected dictionary columns) with im:  Im = Im?1 ? {im}  6: Solve for the coefficients ~?Im by using the concept of orthogonal projection  ~?Im = (DI + m~y)  = ( DTImDIm  )?1 DIm~y  7: Perform Cholesky decomposition on DIm 8: Calculate the new residual using ~?Im  ~rm = ~y ?DIm ~?Im 9: Set m = m+ 1  10: If m has not reached its threshold (stopping criterion) then return to step 4.

11: End Function  a numerical dataset (where each of its columns are assumed to be vectors) given a categorical dataset as input.

In our paper, we have applied the TF-IDF approach to the domain of categorical data. Each record in a dataset is assumed to be a document. The terms are taken as the different domain values of every attribute. In this model, the term weight is calculated as in Eqn (5).

Term Weight, wi = tfi ? [log (D/dfi)] (5)  We have also considered multi-valued datasets where all possible (2n ? 1) combinations, where n is the number of     distinct elements of each attribute entry, are considered for term generation. The sample shown in Table I is an example from the real data that we have chosen for our experiments.

Table II is a sample weight matrix for the data mentioned in Table I.

Dictionary initialization is done using the normalized columns of Y . This normalized dictionary is used as the input to the first iteration of the sparse coding and the dictionary update phase. The subsequent iterations of the sparse coder uses the updated dictionary from the dictionary update phase.

Algorithm 1 gives the detailed description for the sparsity- based representation of categorical data. A detailed description of phase 2 and phase 3 is given below.

Basic OMP algorithm can be used in the sparse coding  Algorithm 3 Batch-OMP 1: Function Batch-OMP (~y, D) 2: Precomputation: ~?0 = DT~y, ?0 = ~yT~y, H = DTD, target  error,? = ?0 ? (?0/ 2) 3: Output: Sparse coefficients, ~?i, ? ~yi in Y 4: Initialization: Set I = (), L = [1], ~? = ~0, ~? = ~?0, ?0 = 0, m = 1, cnt = 1  5: while ?m?1 > ? && cnt < k/2 do 6: k1 = Argmaxk {|~?k|} 7: if m > 1 then 8: Perform Cholesky decomposition on HI,I 9: end if  10: I = (I, k1)  11: ~x = ( LLT  )?1 ~?0I  12: ~?I = ~x 13: ~? = HI~?I 14: ~? = ~?0 ? ~? 15: ?m = ~?TI ~?I 16: ?m = ?m?1 ? ?m + ?m?1 17: m = m+ 1 18: cnt = cnt+ 1 19: end while 20: End Function  phase. At each iteration, the basic OMP algorithm selects the dictionary column with the highest correlation to the current data component (residual). Once the column is selected, each data vector is orthogonally projected to the span of selected dictionary column(s)1. Then the residual is recomputed and the process repeats until all the documents in the given set are handled. In the basic OMP algorithm, the number of coefficients are fixed initially itself and the algorithm iterates upto the count of the coefficients mentioned which is given as threshold in Algorithm 2. This is also taken as the stopping criteria for the algorithm. Cholesky decomposition (Algorithm 4) can be used with the basic OMP for better approximation of the input data.

1http://faculty.uml.edu/dklain/projections.pdf  Batch-OMP is simpler compared to other pursuit algorithms like Matching Pursuit, FOCUSS and Basis Pursuit. It is a greedy algorithm that selects the dictionary atoms sequentially.

Batch-OMP, a modified version of OMP, has its advantage of reducing the work involved in sparse coding the entire set of data when the number of data vectors needed to be represented over the same dictionary increases.

The greedy Batch-OMP algorithm stores some pre-computed values that are useful for the entire set of data. Batch-OMP is specifically designed for sparse coding large set of smaller data vectors [4]. Batch-OMP improves the efficiency further, as only the projection of the residual onto the dictionary needs to be computed, not the residual itself for each iteration.

The precomputation done in Algorithm 3 includes each doc- ument?s projection onto the span of initial dictionary, ~?0, covariance of each data as initial error, ?0, gram matrix, H for avoiding matrix operation involved in each iteration of Basic OMP implementation, and target error, ?.

Step 6 in the algorithm finds the index of the highly correlated dictionary column, k with the current document component.

The steps 7 ? 9 perform Cholesky decomposition, which is needed to reduce the work involved in matrix inversion for the forward/Backward subtitution in step 11. Cholesky decomposition gives the lower Cholesky factor of H , which reduces the memory requirement. In step 10, the selected indices of dictionary are stored to I().

Algorithm 4 Dictionary learning using SVD 1: Input:Normalized dictionary, D, generated coefficient ma-  trix, A from the sparse coding phase and the weight matrix, Y .

2: Output:Updated dictionary, D and the updated coefficient matrix, A.

~?Tj ?jth row in A  3: For each column k = 1, 2, ..., k in the Dictionary, update it by the following steps  4: Find out the group of documents which use that particular dictionary column ie, find wk :  wk = { i : 1 ? i ? N, ~?Tk (i) 6= 0  } 5: Compute the overall representation error matrix, Ek by  Ek = Y ? ? i6=k  ~di~? T j  6: From the error matrix, choose only the columns corre- sponding to the indices in wk and obtain the matrix, EkR.

7: Perform SVD on EkR to get U S V T . Then the dictionary  column is updated with the first column of U matrix and the corresponding coefficient vector is updated by multiplying the first row of V T with the spectral norm of S matrix.

In Batch-OMP, unlike basic OMP-Cholesky algorithm, explicit computation of DT r in each iteration (step 4 in Algorithm 2) is reduced by computing ~? (step 13) and ~? (step 14). The current set of coefficients ~?I are computed in step 11 and     12. An error based stopping criteria is used in this algorithm as in step 5 and ? is computed in step 15 for this purpose.

Squared approximation error ?m is computed in step 16 by using the previous iteration?s error ?m?1 and by using previous and current iteration?s ? values.

We can set the target error, ?, in terms of the initial error, ?0, which is given in step 2. We limit the number of coef- ficients, ?I , used to represent ~y using step 5. The practical implementation of Batch-OMP uses a progressive Cholesky decomposition method to reduce the work involved in inverting the matrix (HI,I)  ?1. Here HI,I remains non-singular due to the orthogonalization process which ensures the selection of linearly independent atoms. The matrix HI,I is an SPD (symmetric positive-definite) matrix which is updated every iteration by simply appending a single row and column to it, and therefore its Cholesky decomposition requires only the computation of its last row.

The third phase comprises the dictionary update. The dictio- nary learning process requires a normalized initial dictionary, D and a coefficient matrix, A. Here, the coefficient matrix is obtained from the sparse coding stage which is further used in the dictionary update stage. The dictionary update process updates one column at a time, fixing all the remaining columns and finds a new column and its new coefficient values that reduce the mean squared error. The new dictionary column and the new coefficients are obtained by the process of Singular Value Decomposition. Mathematically, the theorem is represented by the equation defined in Steps (4) to (7) (Algorithm 4). When all the dictionary columns get updated, again sparse coding phase is done. This process is iteratively done until a stopping criterion is reached according to the error constraint as given in Eqn (4).

Each row of the coefficient matrix represents the correspond- ing dictionary column, ~di and each of its columns represent a document, ~yi in the weight matrix, Y . In step 4 (Algorithm 4), we consider each row of the coefficient matrix, Row ? <N , to find out the number of non-zero coefficient values. This gives the group of documents that make use of that dictionary column.

Algorithm 4 has an effective sparse coding phase and it is more effective when the dimensions of data vectors are huge and when the dictionary size is large.



IV. EXPERIMENTAL RESULTS  Experiments have been carried out using the algorithms ap- plied on both the synthetic and real data. The implementation of the dictionary learning algorithm has been carried out with both OMP-Cholesky and Batch-OMP algorithms in the sparse coding stage.

All the algorithms mentioned in this paper are implemented in Java. As the real dataset has above 10,000 terms generated from above 900 documents (records from a medical dataset as given in Table I), we are demonstrating the experimental results using a sample set of data.

Fig. 2: Preprocessed data from the sample biological dataset  Fig. 3: Generated dictionary from the sample biological dataset  To demonstrate the regeneration of data on the biological dataset, we took a sample of 7 documents and 8 terms to form the 8? 7 data matrix as per Fig. 2. The dictionary generated for this data is shown in Fig. 3. The corresponding coefficient matrix is given in Fig. 4. The regenerated data matrix with a very small reconstruction error (frobenius norm) of 0.761 is shown in Fig. 5.

For the objective of comparison of dictionary learning algorithm using OMP-Cholesky and Batch-OMP algorithms,  Fig. 4: Generated coefficient matrix from the sample biological dataset  Fig. 5: Regenerated data using the above dictionary and coefficient matrix     experiments has been done on the synthetic data with different dimensions and its performance is evaluated. The running time of both the algorithms is noted and is mentioned in Table III.

TABLE III: Performance Comparison  Dimensions T1 T2 5 4 5  10 5 6 25 7 7 50 8 9 75 8 9 100 9 11 200 13 18 300 17 24  T1 - Time taken (secs) by dictionary learning algorithm using Batch-OMP T2 - Time taken (secs) by dictionary learning algorithm using OMP-Cholesky The same experiment is performed on the biological data sample presented above in Fig. 2 and it also shows that Batch-OMP outperforms the basic OMP-Cholesky algorithm in the sparse coding stage of dictionary learning algorithm.

When the dimension increases, the Batch-OMP proves to be a better solution than the basic OMP-Cholesky algorithm for dictionary learning.

Convergence of the reconstruction error using Basic OMP- Cholesky and Batch OMP when applied on the sample bio- logical data (Fig. 2) is shown in Fig. 6. Here also we can see that the dictionary learning algorithm using batch OMP outperforms the dictionary learning algorithm using Basic OMP-Cholesky.

Fig. 6: Reconstruction Error for dictionary learning algorithm using Batch-OMP and OMP-Cholesky

V. CONCLUSION Sparse representation is widely used in applications related to signal and image processing. In this era of big data analysis, this approach indeed brings about a new outlook to big data representation. The focus of our paper is on applying this con- cept to categorical datasets which include data having multiple values for each attribute. This datasets have been preprocessed using the TF?IDF weighting scheme to generate the weight matrix which is the input to all our algorithms. We have succeeded in generating sparse representations for different datasets. Dictionary learning with the SVD approach that  includes sparse coding using both OMP-Cholesky and Batch- OMP has been carried out in the process of generating sparse representations. The dictionary learning algorithm generates an updated dictionary and sparse coefficients which can also be used for regenerating back the original datasets. We have also succeeded in showing that dictionary learning using Batch- OMP outperforms that using OMP-Cholesky.

