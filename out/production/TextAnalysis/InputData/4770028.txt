INTERNET ANOMALY DETECTION WITH WEIGHTED FUZZY MATCHING OVER FREQUENT EPISODE RULES

Abstract: Recent attacks demonstrated that network intrusions  have become a major threat to Internet. Systems are employed to detect internet anomaly play a vital role in internet security. To solve this problem, a technique called Frequent Episode Rules (FERs) base on Data Mining has been introduced into anomaly detection system (ADS). These episode rules are used to distinguish anomalous sequences of TCP, UDP, or ICMP connections from normal traffic episodes. Unfortunately, this technique is so depend on the machine learning that we may get some false alarms if the results of machine learning cannot cover all the normal traffic data. In this paper, we introduce a new approach for internet anomaly detection with weighted fuzzy matching over frequent episode rules. We use weighted fuzzy matching algorithm to match the rules, though machine learning may not cover all the normal traffic. The results show that the proposed approach can improve the detection performance of the ADS, where only pure frequent episode rule is used.

Keywords: Internet security, Anomaly detection, weighted fuzzy  matching, Frequent episode rule, Traffic data mining  1. Introduction  Internet is so deeply influence our daily life that we can't live without it. But with the sharply growth of internet-based services and severity information on networks, the number and the severity of internet-based computer attacks have distinctly increased [1]. Thus, Intrusion Detection Systems (IDSs) become a crucial part of network security [2]. Misuse detection and anomaly detection are two major intrusion detection techniques.

Misuse-based systems can detect known attacks with high success detection rate, but very powerless when face the unknown attacks. For this reason Anomaly-based systems are employed to detect the unknown intrusions which cannot be addressed by misuse detection. The critical technique of anomaly detection is to build profiles  of normal usage [7], [8], [12]. Then frequent episode rule, support by the data mining technique, is introduced to anomaly detection system. The anomaly is detected once the episode rule describing the real traffic connections cannot find any match with normal connection rules in the database [3], [8]. Thus if the database cannot covers all the normal connection rules, we may have a false alarms.

Here, we propose a new anomaly detection scheme that can avoid the false alarms above. Through elaborate examination, we can obtain a nice result of success detection when we use the weighted fuzzy matching algorithm to match the rule which cannot find in the normal rules.

We use NetAttack datasets for evaluation, which mix Internet trace data with MIT Lincoln Lab IDS attack data.

Experiment shows that our approach detects abnormal traffic with very high accuracy and low false alarm.

Paper Organization: This rest of paper is organized as follows: In Section 2, we discuss the related work. In Section 3, we introduce Episode rules and pruning techniques. In Section 4, we introduce the weighted fuzzy matching technique and it's usage of anomaly detection, we describe the experiments and evaluation in Section 5.

Finally, we summarize the paper and outline our future research plans in Section 6.

2. Related Works  Anomaly detection system (ADS) which can detect unknown attacks plays a vital role in intrusion detection.

As data mining has been introduced to the ADS, many detection approaches which base on it have been developed. In the past, data mining techniques such as hidden Markov model, based on machine learning, has been used to build automatic anomaly detection system. At the same time a technique called association rules were employed to build ADS [4], [5], [9], [10], [11]. Among this     method, many other approaches based on association rule mining have been proposed for anomaly detection. Then the frequent episode rules (FERs) was first proposed by Mannila and Toivonen [22]. With huge audit records, data mining generates many complex FERs with redundancy or repetitions [6]. Later, a framework was made up by Lee, et al that employed the FERs for anomaly detection by comparing the current activities with normal traffic profiles.

They introduced the axis and reference attributes in the JAM project to constrain the rule generation and to obtain temporal features. Subsequently, Fan et al. [8] extended Lee's work to find accurate boundaries between known and unknown attacks.

When base-support algorithm was introduced, Qin and Hwang [6] refined the rule formulation procedure with it to mine normal traffic data. Compare with Lee's algorithm, the base-support algorithm is fairer to various axis attribute values. Due to the fact that the same percentage of records is required for different axis attributes, so, higher base-support value will result in fewer FERs.

3. Episode rules and Pruning techniques  In this section, we will give a brief show about Internet Episode Rules generation and after the rules generation some redundant rules will be born, thus we must eliminate these redundant rules.

conneton E2 ES E] EIl ES E4 El E2 E:l E2 E6 E2 eentS  0 1 2 3 4 5 _5> 8 9 10 1 16i23 N4 15 16 i7 18 Fig. 1. A frequent episode rule generation by scanning a stream of  connection events using a sliding window.

3.1. Generation of Internet Episode Rules  The performance of anomaly detection system is directly affected by the Internet Episode Rules generation.

We use sequence of connection events, such as TCP, UDP, ICMP, or other representational events, to build the Internet episode which we use it to distinguish the abnormal episodes from the normal episodes.

Let E be a set of connection events and A be a set of attributes define over E. Then an attribute-value pair over A composes a set I. For example, I= {timestamp=10 sec, duration=1 sec, service=http, desthost=192.168.21 1.1 for a typical http connection event El. A frequent episode is a set of connection events exceeding the occurrence threshold in a sliding window and expression as follows:  E3 - El, E2(c, s, window) (3.1)  All connection events are sequentially ordered, that is E3, El, E2 must occur in order as listed, like in Fig. 1.

The support and confidence of the above rule are formally defined as follows:  s = Support(E3U ElU E2) 2 so  Support(E3U ElU E2) > c(  (3.2)  (3.3) Support(E3)  The support value s is defined by the percentage of occurrences of the episode out of total number of traffic records audited. And the confidence level c is the probability of the minimal occurrence of the joint episodes out of the left-hand side (LHS) episode [6]. Here, both parameters s and c are bounded from below by a minimum support threshold so and a minimum confidence level co, respectively. We use the window size to regulate the scanning period of the window.

After this we use a Base-Support data mining scheme, introduced by Lee et al. and improved by Hwang et al. in 2007 [6] [3], to eliminate the infrequent traffic patterns which will make the anomaly detection system ineffective in detecting rare network events.

3.2. Pruning of Ineffective Episode Rules  It is very obvious that many rules will be generated after long time machine learning, but some of these rules are uninteresting or ineffective. Thus we use a pruning technique to reduce the episode rules.

a. Transposition of Episode Rules Comparing the following two FERs, the set  (service=http, flag=sO) is implied by (service=smtp, flag=SF) in Rule 1. Thus, the second one can be covered by the first one. So we just need only to include Rule 1 in the normal rule set.

Rule1. (service=smtp, flag=SF)-*(service=http, flag=sO), (service=http, flag=SF),  Rule2. (service=smtp, flag=SF), (service=http, flag=sO) ->(service=http, flag=SF),  b. Elimination of Redundant Episode Rules In general, rules with shorter LHSs are more  effective than rules with longer LHSs [6] [3]. This is because shorter one is much easier to compare.

c. Reconstruction of Episode Rules Many FERs detected form the internet traffic  have some transitive patterns. Suppose we have two rules A-*B and B-*C in the rule set. Then, the longer rule A-*B,C is implied. Since we reconstruct this rule from two shorter rules, the longer rule A-*B,C   c =    becomes redundant. The reconstruction helps us split longer FERs into shorter ones [6] [3].

The rule pruning eliminates most of the ineffective  rules automatically, thus this process raise the efficiency of detection.

4. Weighted Fuzzy Matching technique  Traditionally, the anomaly is detected once the episode rule describing the real connections cannot find any match with normal connection rules in the database. But there is an obvious problem that the normal connection rules may not cover all the normal connections, thus unsuccessful match does not means we detect an abnormity. So we use weighted fuzzy matching technique to handle this problem.

For universal consideration, we define the algorithm as  follows: Let a subset of T is Tij =(ti, t+1?. .t) and i<i<j<n (T=T11n),  and a subset of S is SIn = (SI, S2, S3...Sn). If the Ti and S n include a same subset Tskt,k2, Tsk3k4, . .. Tskx,ky (1<k1<k2... kx5ky5n, we count the similar degree as:  sim(X, S) = Len(TkIkj) / Len(T)  5. Experiments And Evaluation  We test our anomaly detection scheme using the NetAttack dataset. First, we have evaluated the effect of the pruning algorithms on different window sizes. The testing results are shown in Fig.2.

Fig.2. Episode Rule Growth  (4.1)  Here, Len represents the number of attributes.

Obviously, the value of similar degree is close to 1  represents the two sets are almost accordance.

Here we ignore the weight of each attributes when we  count the similar degree, so it is not very accurate. Because not every attributes have the same weightiness. To handle this problem we add the weight value to the fuzzy matching algorithm.

We give each items t, and s, a weight value and form a  pair like <t1, wi>. So the weighted similar degree is: w_ sim(X, S) = Weight(T, k,W k) / Weight(T) (4.2)  If the weighted similar degree exceeds the threshold min_sim we give, we judge it as a normal traffic.

For example, A FER in normal database is  Rule1. (service=smtp, flag=SF)->(service=http, flag=sO),(service=http, flag=SF) and we have a rule that we attain from internet traffic is: Rule2. (service=smtp, flag=SF)-*(service=http). But we cannot find any match with normal rules. So we use weighted fuzzy matching.

We suppose the weight value of each attributes is as follows: <service,0.4>, <flag,0.2> so we can attain the weighted similar degree is 0.55. If the threshold min_sim is 0.87, so we can judge it as an abnormal traffic.

Fig.3. Pruning effect on false alarms in a week  With pruning, some ineffective rules are abandoned, and then we test the pruning effects on false alarms in a week. We find that large scan window sizes will bring more false alarms due to the background noise. Without pruning, it lead to higher false alarms, the results are shown in Fig.3.

Due to the fact that each attack may generate many anomalous rules and the rule pruning algorithm cannot remove them all, we did not find any changes in false negatives in all experiments. And we still can use the remaining anomalous rules to identify the attack.

Meanwhile, we find that when use weighted fuzzy matching, an appropriate threshold of similar degree is very important for judging the network traffic normal or abnormal. This threshold is decided by diversiform factors such as weight value of each attributes and the number of FERs in normal database. The results over our work are shown in Fig.4.

