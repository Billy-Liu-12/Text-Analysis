ACN: An Associative Classifier with Negative Rules

Abstract  Classification using association rules has added a new dimension to the ongoing research for accurate classifiers.

Experiments have shown that these classifiers are signif- icantly more accurate than decision tree classifiers. The idea behind most of the existing approaches has been the mining of positive class association rules from the training set and then selecting a subset of the mined rules for future predictions. However, in most cases, it is found that the final classifier contains some weak and inaccurate rules that were selected for covering some training instances for which no better rules were available. These rules make poor predictions of unseen test instances and only for these rules, the overall classification accuracy is drastically reduced. The idea of this paper is to eliminate these weak and inaccurate positive rules as far as possible by accurate negative rules. The generation of negative associations from datasets has been attacked from different perspectives by various authors and this has proved to be a very computationally expensive task. This paper approaches the problem of generating negative rules from a classification perspective, how to generate a sufficient number of high quality negative rules efficiently so that classification accuracy is enhanced. We extend the Apriori algorithm for this and show that our classifier ?Associative Classifier with negative rules?(ACN) is not only time-efficient but also achieves significantly better accuracy than four other state-of-the-art classification methods by experimenting on benchmark UCI datasets.

Keywords: associative classification, data mining, negative rule  1. Introduction  Classification is a very important problem that has been studied for years now. The goal of the classification  algorithms is to construct a model from a set of training data whose target class labels are known and then this model is used to classify unseen instances. Many different types of classification techniques have been proposed in literature that includes decision trees [12], na??ve-Bayesian methods [6], statistical approaches[10] etc.

Recently a new classification technique has come into existence. In this technique, at first classification rules are mined from training data using an association rule mining algorithm and then a subset of these rules are used to build a classifier. This technique which uses association rules for classification is called ?Associative Classification(AC)?. A number of associative classification algorithms have been proposed in literature but most of them use only positive association rules and differ mainly in rule mining and classifier construction from the set of mined rules.

One of the first algorithms to use association rules for classification was CBA(Classification based on Association)[11]. CBA uses the Apriori algorithm[1] in order to discover all frequent ruleitems. Then it converts any frequent ruleitem that passes the minimum confidence threshold into a rule. After that it sorts the rules based on a rule ranking criteria and selects a subset of the rules that are needed to cover the dataset. These ordered rules are later used for classification.

An AC approach that uses multiple rules for making a single prediction is CMAR (Classification based on Multiple Association Rules)[9] . Instead of using Apriori, this method adapts the FP-tree algorithm[7] to mine the class association rules and makes use of a CR-tree structure to store and retrieve the mined rules. But the major distin- guishing feature from CBA is that here the classification is performed based on a weighted ?2 analysis using multiple association rules. Some other associative classifiers based on positive association rules are discussed in [8, 18, 4]  A positive association rule is of the form X? Y where    X , Y both are a set of items and X ?  Y is ?. A negative association rule is of the form X ? Y where in addition to being a set of items, X or Y will contain at least one negated item. An interesting approach that uses both posi- tive and negative rules for classification is ARC-PAN[2]. It examines the correlation of each frequent itemset with the class label. If the correlation is positive, a positive rule is discovered. If the correlation is negative, two negative rules are discovered. The negative rules produced are of the form X?? Y or ? X? Y which the authors term as ?confined negative association rules?. Here the entire antecedent or consequent is either a conjunction of negated attributes or a conjunction of non-negated attributes. The problem with this approach is that it results in a very small set of rules which may not be adequate to provide classification for all training and test instances.

In this paper, we introduce a new method for associative classification named ?Associative Classifier with Negative Rules?(ACN) that extends the Apriori algorithm to mine a relatively large set of negative association rules and then uses both positive and negative rules to build a classifier. The set of mined negative rules and the way ACN generates them are totally different from ARC-PAN.

The benefit of our approach is that the number of negative rules generated is much larger and so in general, a lot of good (high confidence and high support) negative rules are found that can be used in place of some weak positive rules. As a result, the number of inaccurate positive rules in the final classifier is greatly reduced and classification accuracy is increased. The major bottleneck in associative classification is the computational cost for the discovery of association rules and if a classifier uses negative rules together with positive rules, this cost can increase further.

ACN tries to address this issue by mining a relatively large set of negative rules but with as low overhead as possible.

So it adopts a variant of Apriori algorithm and generates a set of negative rules from the available frequent positive itemsets. These negative rules come almost free-of-cost since their support and confidence can be readily calculated from available positive rules and no extra database scan is required.

The rest of the paper is organized as follows.In Section 2, we discuss the related work in negative association mining.

Section 3 presents ACN in details. Section 4 presents our experimental results and finally in section 5, we conclude the paper with some remarks.

2. Negative Association Mining  Efficient mining of negative association rules has recently received much attention from a large group of  researchers. Negative associations can inform us of facts like ?If a customer purchases Coke, he is not likely to pur- chase Juice.? etc. Such associations are quite natural and occur frequently in practice. However, mining transactional databases for negative associations is really challenging because typically a super-store contains thousands of items and each transaction contains only a few of them. As a result, each transaction has a large number of negated or absent items and the number of possible negative associations under the support-confidence framework turns out to be overwhelmingly large. Moreover, a large portion of the discovered rules may be uninteresting from the user?s perspective. These difficulties have forced researchers to incorporate additional rule interestingness measures[14, 16] over the support-confidence framework or incorporate domain knowledge[13, 19] to reduce the search space.

However, it is evident that important differences exist between the mining of negative associations in transac- tional databases and the mining of them in classification datasets. Firstly, in classification datasets, typically the number of attributes is not large and each attribute has a small number of possible values. So the value for an attribute in a record indicates the absence of other possible values for that attribute. This is much smaller compared to the number of absent items in a transaction in a super-store database. Secondly, the purpose of mining is on increasing classification accuracy rather than presenting the set of mined rules to the user and satisfying his interest. Thirdly, the process of negative rule generation must be as cheap as possible to avoid further increasing the complexity of the mining phase of an associative classifier.

Keeping the above challenges in mind, we designed an efficient algorithm for generation of both positive and negative rules. Using both sets of rules, we were able to obtain classification accuracy higher than other methods.

A number of negative rule mining algorithms have already been proposed in the literature[19, 16, 3, 17, 13, 15, 14].

However, to the best of our knowledge, the algorithm for negative rule mining in ACN best meets the challenges described above. This approach is novel and has not been introduced in literature before.

3 ACN  This section describes ACN in more details. First, we discuss how ACN generates rules, then we present the classifier builder for ACN and finally we discuss different pruning strategies to reduce the number of generated rules.

3.1 ACN Rule Generator  Let a relational schema contains n data attributes (A1,A2,. . . , An) and one class attribute Z. Each attribute has a set of possible values. Each record is of the form (a1,a2,. . . ,an, z) where a1,a2,. . . ,an are the values of the corresponding data attributes and z is the class label. Given a set of training instances, the task of a classifier is to learn the relation between set of attribute values to class label and later use this relation to predict class labels for test instances from the values of their data attributes only.

For mining class association rules using Apriori, each itemset is considered to be of the form (conditionset, z) which represents a rule: conditionset?z where condition- set is a set of (attribute, value) pairs and z is a class label.

The terms rule and itemset will be used interchangeably afterwards and should be understood from context. The rule has confidence equal to (ruleSupportCount / conditionSup- portCount) * 100% where, conditionSupportCount is the number of cases in the dataset D that contain conditionset and ruleSupportCount is the number of cases in the dataset D that contain conditionSet and are labeled with class z. The rule has support equal to (ruleSupportCount/|D| ) *100%, where |D| is the size of the dataset.

The idea behind the Apriori algorithm is the property that all subsets of any frequent itemset must also be fre- quent .This allows Apriori to generate the frequent itemsets in a level wise manner. In the following discussion, we assume that the notation Ck represents the candidate itemsets of length k and Lk represents the frequent itemsets of length k generated by Apriori. We also assume that the notation li[j] refers to the j?th item in li where li represents an itemset. Obviously for all k, Lk is a subset of Ck and each member of Lk has support higher than the user specified support threshold. In Apriori, there are two steps: the join step and the prune step. In the join step; a set of candidate itemsets Ck is generated by joining Lk?1 with itself. If we assume that the items in the itemset are sorted in lexicographic order, then members l1 ? z and l2 ?z of Lk?1 are joinable if (l1[1]=l2[1]) ? (l1[2]=l2[2])? (l1[3]=l2[3]) ? . . . (l1[k-2]=l2[k-2])?(l1[k-1] <l2[k-1]).

The resulting rule generated by joining l1 and l2 is l=l1[1] ? l1[2]? . . .? l1[k-1] ? l2[k-1]? z. Now comes the prune step of Apriori. It checks to see whether the all subsets of the generated candidate itemsets of length k-1 are frequent.

If not, that itemset cannot be frequent and can immediately be discarded. Otherwise, a full database scan must be made to count the support for the new itemset to decide whether it is actually frequent or not.

We call each candidate itemset of length k for which all  subsets of itemsets of length k-1 (for all k ?2) are frequent, a ?legal candidate?. For each item of this legal candidate, ACN replaces this item with the corresponding negated item, creates a new negative rule and adds it to the negative ruleSet. The generation of positive rules continues without disruption and the abundant but valuable negative rules are produced as by-products of the Apriori process.

For the trivial cases, the set of frequent 1 positive rule items are generated without any join step. Frequent 1 negative rules are generated by simply considering the negation of the single antecedent of each positive rule item.

Example: We will explain the rule generation of ACN us-  ing an example. Let us consider a case with 5 data attributes A,B,C,D,E and one class attribute Z. The domains for the four data attributes are respectively {a1,a2,a3},{b1,b2,b3,b4},{c1,c2,c3},{d1,d2,d3},{e1,e2,e3,e4}.

Z can take on the possible values of y and n.

Suppose, in the Apriori rule mining process, the set of frequent ruleitems of length 3 are found to be a1 ? b1 ? c1 ? y, a1 ? b1 ? d1?y, b1 ? c1 ? d1?y, a2 ? b2 ? c2?n, a2 ? b2 ? d4?n, b1 ? c1 ? e3? n.

Now the set of candidate ruleitems of length 4 after join step is a1 ? b1 ? c1 ? d1 ? y, a2 ? b2 ? c2 ? d4 ? n) Out of these two candidates, only the first one (a1 ? b1 ? c1 ? d1? y) possibly can be frequent since all its subsets are frequent as found from the set of frequent ruleitems of length 3. So according to our definition, it will be a legal candidate and from this ruleitem, four rules of the form ? a1 ? b1 ? c1 ?d1? y, a1 ? ? b1 ?c1 ?d1? y, a1 ?b1 ? ? c1?d1? y and a1?b1?c1? ? d1? y will be generated.

In this way, negative rules will be generated in all phases of the Apriori algorithm. These negative rules will not take part in generation of any new rule but they will compete for a place in the final classifier with the positive rules. Please note that each legal candidate ruleitem with n number of items in the antecedent will generate n new negative rules, even if the candidate ruleitem turns out to be infrequent.

L1=frequent-1-Positive-itemsets(D) N1=frequent-1-Negative-itemsets(D) for(k=2;Lk?1!=empty;k++)  PCk= legal candidates generated for level k.

for each legal candidate generated  for each item on the candidate create a new negative rule by negating that item.

add this rule to NCk.

end  end calculate support for each rule of PCk by scanning the database.

calculate support for each rule of NCk from supports of members of PCk and Lk?1.

Lk=candidates in PCk that pass support threshold.

Nk=candidates in NCk that pass support threshold.

end Return L=L1 ?L2 ? . . . ? Lk?2 ?N1 ?N2 ? . . . ?Nk?2  Figure 1. ACN Rule Generator  3.2 Classifier Builder  ACN sorts the set of positive and negative rules on the following rule ranking criteria, a rule Ri will have higher rank than rule Rj if and only if 1) conf(Ri) > conf(Rj) or 2)conf(Ri) = conf(Rj) but correlation(Ri) > correlation(Rj) or 3) conf(Ri) = conf(Rj) and correlation(Ri) = correla- tion(Rj) but sup(Ri) > sup(Rj) or 4) conf(Ri) = conf(Rj) and correlation(Ri) = correlation(Rj) and sup(Ri) = sup(Rj) but size of conditionset of Ri < size of conditionset of Rj or 5) conf(Ri) = conf(Rj) and correlation(Ri) = correlation(Rj) and sup(Ri) = sup(Rj) and size of conditionset of Ri = size of conditionset of Rj but Ri is a positive rule and Rj is a negative rule. After sorting, ACN builds a classifier based on database coverage similar to CBA except for the fact that before a negative rule is taken, ACN calculates the accuracy of the rule on the remaining uncovered dataset and if it is greater than a specific threshold, only then the rule is taken in the final classifier. There is no such restriction for positive rules.

Sort rules based on rule ranking criteria for each rule taken in order  If the rule classifies at least one remaining training example correctly  If the rule is a negative rule and accuracy on remain- ing data >threshold  Include that rule in classifier and delete those examples  end if the rule is a positive rule  Include the rule in classifier and delete those examples  end end  end If database in uncovered  select majority class from remaining examples else select majority class from entire training set.

end  Figure 2. ACN Classifier Builder  3.3 Pruning Strategies  ACN adopts several pruning strategies to cut down the number of generated rules. In the first strategy, only negative rules are pruned. Consider two rules l and m from which a negative rule n is produced. Let, l = l[1] ? l[2] ? . . .? l[k]? z and m = l[1] ? l[2] ? . . .? l[i-1] ? l[i+1] ? . . .? l[k]? z and n = l[1] ? l[2] ? . . .? ? l[i] ? . . .? l[k] ? z. If confidence of l > confidence of m , it can be proved that confidence of n < confidence of m. So according to our rule ranking criteria, m will precede n and n can be pruned because the coverage of m is a superset of coverage of n.

Secondly, ACN prunes all rules that have confidence less than the minimum confidence.

Thirdly, ACN prunes both positive and negative rules based on a correlation coefficient threshold.

3.4 Time-Efficiency of ACN:  In this section, we theoretically prove that ACN does not perform any extra dataset scan to count the support or confidence of the generated negative rules. So there is no big I/O overhead for generating the negative rules.

Theorem: ACN performs no extra dataset scan than normal Apriori Process.

Proof:  The proof is by contradiction. Suppose at some pass of Apriori, ACN generates a negative rule l1? l2 ? . . .? li?1 ? ? li? . . .? lp ? z for which the support and confidences cannot be calculated without a database scan. This rule can only be generated from a candidate ruleitem l1 ? l2 ? . . .? li?1 ? li ? . . .? lp ? z. According to the rule generation method of ACN, this ruleitem must be a legal ruleitem, i.e., all subsets of this ruleitem of length p-1 must be frequent. As a result, the ruleitem of the form l1 ? l2 ? . . .? li?1 ? li+1 . . .? lp ?z must be frequent and its support and confidences must have been calculated in the previous pass of the Apriori algorithm. For the legal candidate ruleitem l1 ? l2 ? . . .? li?1 ? li ? . . .? lp ?    z, support and confidence will be calculated via database scan at current pass. Now for the rule l1? l2? . . . li?1 ? ? li ? . . .? lp ?z we can obtain,  supp (l1 ? l2 ? . . . li?1 ? ? li ? . . .? lp?z)= supp(l1 ? l2 ? . . .?li?1 ?li+1 . . .? lp ?z) - supp(l1 ? l2? . . .? li?1 ? li ? . . .? lp ?z)  supp(l1? l2 ? . . .? li?1 ? ? li . . .?lp)= supp (l1 ? l2 ? . . .?li?1 ?li+1? . . .?lp) ? supp(l1 ?l2 ? . . .?li?1 ?li?li+1? . . .?lp) conf (l1 ? l2 ? . . . li?1 ? ? li ? . . .? lp ?z)= supp (l1 ? l2 ? . . . li?1 ? ? li ? . . .? lp ?z) / supp(l1? l2 ? . . .? li?1 ? ? li . . .?lp)  So no such negative rule will be generated by ACN for which a database scan will be needed to count the support and confidence. So ACN performs exactly the same number of dataset scans as Apriori.

Figure 3. Support and Confidence calculation of negative rule items from other positive rule items  So ACN does not need to perform any extra database scan to calculate support and confidences of the generated negative rules. For any rule, to gather the support and confidence values, we need to consult O(r) records where r is the number of records in the database. But here the support and confidence of a negative rule can be calculated in O(1) time and no I/O operation is needed. As a result, the mining phase of ACN remains time-efficient.

4 Experimental studies  In this section, we present some experimental facts re- garding ACN and also compare it with other state-of-the-art classification algorithms in terms of accuracy.

Table 1. Number of positive and negative rules generated for two data sets  Confidence Diabetes Diabetes Heart Heart + rules - rules + rules - rules  50-60% 223 571 3438 10496 60-70% 200 546 1474 4823 70-80% 163 520 1770 5201 80-90% 133 448 952 2917 90-100% 142 474 999 2900  Table 1 gives the number of positive and negative rules generated in experiments on two data sets Diabetes and Heart. From the table, it is evident that the number of generated negative rules of high confidence(90-100%)is several orders of magnitude in number than the number of generated positive rules of that confidence. In general, a rule is good if it has high confidence. So taking the negative rules into account, ACN generates more good rules than other classification methods that generate only positive rules. So the class association rule set for ACN is much larger and richer.

Table 2 gives the comparison of accuracy among ACN, C4.5, CBA and CMAR. The accuracy of ACN was obtained by 10 fold cross validation over 16 datasets from UCI ML repository[5].We have used C4.5?s shuffle utility to shuffle the datasets. Discretization of continuous attributes is done using the same method in CBA.

For ACN, the minimum confidence was set to 50%, the correlation coefficient threshold was set to 0.2 and the remaining accuracy threshold for negative rules was set to 55%. These were the best values for the parameters that we obtained experimentally.

From Table2, we see that the average accuracy of ACN is better than CBA, CMAR and C4.5. Moreover, out of the 16 datasets, ACN achieves the best accuracy on more than half (9) datasets.

Table 3 gives the comparison of accuracy among ACN and ARC-PAN. We have used only 6 datasets for compari- son here since the accuracy of ARC-PAN was available for these 6 datasets only.

Table 2. Comparison of C4.5, CBA, CMAR and ACN on accuracy  Dataset ACN CMAR CBA C4.5 diabetes 76.3 75.8 74.5 74.2  pima 75.1 75.1 72.9 75.5 tic-tac 99.7 99.2 99.6 99.4  iris 95.3 94 94.7 95.3 heart 82.2 82.2 81.9 80.8  lymph 83.1 83.1 77.8 73.5 glass 73.8 70.1 73.9 68.7 austra 85.5 86.1 84.9 84.7 led7 71.9 72.5 71.9 73.5 horse 83.7 82.6 82.1 82.6 sonar 79.8 79.4 77.5 70.2 hepati 83.2 80.5 81.8 80.6  crx 85.2 84.9 84.7 84.9 cleve 81.5 82.2 82.8 78.2 hypo 98.9 98.4 98.9 99.2 sick 97.3 97.5 97 98.5  Average 85.4 84.6 84.3 83.0  Table 3. Comparison of ACN and ARC-PAN on accuracy  Dataset ACN ARC-PAN diabetes 76.3 74.9  pima 75.1 73.1 iris 95.3 94.0  heart 82.2 83.8 led7 71.9 71.1  breast 95.3 96.2 Average 82.68 82.18  For ARC-PAN, we consider the results obtained when all rules (both positive and negative) are used for classification.

From table 3, we see that the win-loss-tie record of ACN against ARC-PAN is 4-2-0.

5 Conclusions  In this paper we proposed a novel classification algo- rithm ACN that mines both positive and negative class association rules and uses both sets for classification. We showed that the number of generated negative rules is large and so using them in place of some weak positive rules can enhance classification accuracy. Our experiments  on UCI datasets show that ACN is consistent, highly effective at classification of various kinds of databases and has better average classification accuracy compared to C4.5,CBA,CMAR and ARC-PAN.

