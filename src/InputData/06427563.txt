

Abstract? Due to prevalent use of sensors and network monitoring tools, big volumes of data or ?big data? today traverse the enterprise data processing pipelines in a streaming fashion. While some companies prefer to deploy their data processing infrastructures and services as private clouds, others completely outsource these services to public clouds. In either case, attempting to store the data first for subsequent analysis creates additional resource costs and unwanted delays in obtaining actionable information. As a result, enterprises increasingly employ data or event stream processing systems and further want to extend them with complex online analytic and mining capabilities.

In this paper, we present implementation details for doing both correlation analysis and association rule mining (ARM) over streams. Specifically, we implement Pearson-Product Moment Correlation for analytics and Apriori & FPGrowth algorithms for stream mining inside a popular event stream processing engine called Esper. As a unique contribution, we conduct experiments and present performance results of these new tools with different tumbling and sliding time-windows over two different stream types: one for moving bus trajectories and another for web logs from a music site. We find that while tumbling windows may be more preferable for performance in certain applications, sliding windows can provide additional benefits with rule mining. We hope that our findings can shed light on the design of other cloud analytics systems.

Keywords: Data streams, Complex Event Processing, Association Rule Mining, Stream mining, Correlation, Apriori, FP-growth.



I.  INTRODUCTION What is Big Data? Big Data has four dimensions, which  makes it extremely hard to manage: Volume, Variety, Velocity, and Veracity, also known as the ?4Vs of Big Data? [13]. Many organizations today including telecommunication operators, e-commerce sites, banks, municipalities, media networks, and governments are generating Terabytes of data each day. They are also attempting to insert this high ?volume? data into databases that already contain Petabytes.

The data come from various sources such as mobile devices, web logs, sensors, cameras, etc. which creates a ?variety? for the data to be processed and stored (e.g. unstructured txt, semi-structured XML, structured CSV, relational, or binary audio/video data). New distributed data processing frameworks such as Apache Hadoop [2] have been invented solely to deal with volume and variety of data within the last decade. Hadoop system is being heavily used today by cloud  computing companies such as Google, Yahoo, Facebook, and many others for large-scale data storage (HDFS), parallel processing (MapReduce) and warehousing (Hive) purposes. Hadoop is usually deployed as a private cloud service, but is also being provided as a public cloud service by several vendors today.

While volume and variety had always been a challenge for data management, its increasing ?velocity? is the new issue of the 21st century [13]. Attempting to store these data first to analyze them later creates additional costs, unwanted delays to actionable information, and loss of business opportunities. Fortunately, there are now tools to process data on-the-fly as it moves from distributed sources (e.g.

sensors) to selected destinations. Since the number of data sources and sampling frequencies is steadily increasing, processing in-flight data in-memory is still a big challenge.

This paper is only interested in doing analytics and mining over streaming text log data and does not cover processing encoded media (image and audio/video) types.

Streaming data from sensors is also prone to errors, which reduces its ?veracity? (or accuracy). The tuples can missing, broken, out-of-order, or can have wrong values.

Table 1 shows sample data from a real bus tracking application, which shows multiple such cases. The system accidentally dropped zeros from the least significant digits of longitude and latitude of bus locations (denoted as bold records), missed some tuples (at time 7:26 only 1-of-3 records exist), and possibly inserted inaccurate speed values (0, 1 km/h).

Most of these big data analytics challenges can be addressed through the use of Data Stream Management Systems (DSMS) [1,4] in the data pipelines of organizations.

Therefore, enterprises increasingly these systems and extend their basic filtering facilities with complex online analytics and mining capabilities. The resulting software tools are sometimes called Complex Event Processing (CEP) engines in the literature [6]. The benefits of using DSMS and CEP systems are at least three-fold: They can eliminate unwanted data early in the pipeline,  saving further CPU, memory, storage and energy costs.

They can turn raw data into actionable information  quickly, thus helping businesses catch profitable opportunities or avoid losses due to fraud or operational inefficiencies.

They can catch transient or emerging patterns, which never show up in an offline data mining analysis.

joy ??        TABLE I: SAMPLE DATA FROM A REAL BUS TRACKING SYSTEM.

BUSID LONGITUTE LATTITUDE SPEED DATE & TIME  00-123 28,863,169 4,105,348 42 12/8/2009 7:23  00-123 2,886,469 41,052,845 3 12/8/2009 7:23  00-123 28,866,064 41,052,856 26 12/8/2009 7:23  00-123 28,867,975 410,522 37 12/8/2009 7:24  00-123 2,886,879 4,105,189 1 12/8/2009 7:24  00-123 28,869,068 41,051,792 6 12/8/2009 7:24  00-123 28,869,884 41,051,376 16 12/8/2009 7:25  00-123 28,870,121 41,051,258 0 12/8/2009 7:25  00-123 2,887,055 41,051,044 16 12/8/2009 7:25  00-123 28,870,613 4,105,191 15 12/8/2009 7:26  00-123 28,868,597 4,105,249 46 12/8/2009 7:27  00-123 28,866,816 4,105,319 19 12/8/2009 7:27  00-123 288,657 41,053,898 20 12/8/2009 7:27   The contributions of our paper are briefly as follows: we implement and demonstrate that both statistical analysis (e.g.

stream correlation) and data mining (e.g. rule mining) can be implemented over the same system and be used for different real-time applications. We also show that for both analytics and mining tools, the semantics of time-windows (type and size) can have a great impact on both the performance and usability in different applications.

The rest of the paper is as follows. In Section 2, we give an overview of DSMS and CEP systems and describe the architecture of our data stream analytics and mining system.

In Section 3, we show an application for using statistical correlations, specifically Pearson Product Moment Correlation (PPMC), over GPS streams and provide some performance as well as usability results. In Section 4, we discuss the integration of two ARM algorithms (Apriori and FP-Growth) into Esper, give their performance comparison and describe how they can be used to support a real-time music recommendation engine. We discuss related work in Section 5. We conclude the paper in Section 6 and discuss future work.



II. SYSTEM ARCHITECTURE DSMS engines provide effective queuing, scheduling,  time and count-window support, and fast in-memory processing of high-speed, continuous, unbounded data streams [1]. They parse, optimize and execute queries written in declarative languages such as Event Processing Language (EPL) in Esper. EPL syntax and semantics are quite similar to that of Structured Query Language (SQL) in databases, but there are additional clauses such as WINDOWs to support sliding or tumbling window-based analytics over data streams. Figure 1 shows these two types of windows.

Sliding-time windows are used to buffer event tuples whose occurrence times fall within a certain time period (e.g. last 1 minute) and to replace events that are older than the time window. The window will move or ?slide? in time with a  period that is usually smaller than the size of the window and therefore the two event epochs overlap. Similarly, sliding- count windows buffer last X events. Tumbling windows on the other hand jump to the next epoch by moving as much as the window size and there is no data overlap between the two event epochs. Other types of windows include Landmark and Damped [9], where the former considers events from a past landmark time until present and the latter gives more weights to recent events. We only use Sliding windows in this paper and leave the rest as future work.

Figure 1.  Tumbling vs. Sliding Window semantics.

EPL queries can be used for continuous filtering (e.g.

SELECT x,y FROM Stream<x,y,z> WHERE ?) as well as aggregations: algebraic (COUNT, SUM, AVERAGE) or holistic (MIN, MAX). Complex aggregation functions such as TOP-K, DISTINCT, QUANTILES, and SKYLINE can also be found or implemented.

A. Stream Analytics and Mining Architecture Figure 2 shows the components and high-level  architecture of our data stream analytics system. High-speed raw data streams are first subjected to Select and Project operators inside the DSMS to get rid of tuples unwanted further in the data pipeline. The data volume can be reduced row-wise (Select) and/or column-wise (Project). Basically, the preprocessing stage of the stream mining is done at this stage. The rest of the complex analytics and mining clauses are implemented inside the CEP system [6], which is deployed either as a private or public cloud system (see part B). Data from streams can be correlated among each other or with data stored in persistent repositories such as DBMS and NoSQL systems. NoSQL (?Not-only SQL?) is a generic term for highly-scalable, distributed data storage and processing engines coupled with a declarative or procedural language for analytics on top. Examples include, Apache HBase, Cassandra, MongoDB and many others. In many private cloud deployments with high-speed event stream processing today, NoSQL systems are used in support of the event queues to extend the correlation windows.

New CEP operators developed in this paper are denoted with the butterfly sign in Figure 2: aggregate (correlation) and stream mining (Apriori & FP-Growth). We describe the integration of machine learning algorithms with Esper in Section IV.C. The statistical results and alerts are sent to Business Process Management systems and visualization tools for further actions. The feedback loop shown at the CEP system output denotes the registration of rules learned through predictive mining back into the CEP engine for faster descriptive processing. For example, an association rule such as (A&B C) can be registered as a sequence <<A,B>,C>. However, we defer this automated registration of rules into the private cloud CEP system as future work.

joy ??         Figure 2.  Data stream analytics and minin system architecture.

B. Public vs. Private Cloud Deploymentsfor Streams Most organizations retain streaming data within their  private networks as this data usually carries mission-critical importance. Therefore, they also prefer a private cloud-based (shared) data stream analytics deployment model. However, it is also possible to forward data streams to a public or community cloud analytics and mining service for pre-or- post processing, if this service is closer to the data than the companies own servers and is reliable. The definitions of private and public clouds are also quite loose: Inside (resp.

outside) of the network domain of a company or a country may be considered private (resp. public) by some organizations. With respect to performance issues, streaming data to the closest computation or pushing computation to the data when possible are cheaper alternatives than trying to move large-amounts of data to distant computation.



III. STREAM ANALYTICS - CORRELATION In this section, we describe our implementation of  Pearson Product Moment Correlation (PPMC) operator over streams and show its application to route-matching over GPS data streams. Briefly, correlation is the covariance of two variables divided by their standard deviations. The correlation value can change between [-1,+1], where +1 denotes a high-positive correlation, 0 denotes no correlation and -1 denotes high-negative correlation.

For this paper, to correlate a bus to its previously recorded route or to ensure that two vehicles move together, we implemented and used the following continuous queries on vehicle latitudes and longitudes (Fig. 3 shows longitude):   SELECT CorrelationLong FROM VehiclePairStream.

WIN:length(50).stat:correl(a.long, b.long)  SELECT CorrelationLat FROM VehiclePairStream.

WIN:length(50).stat:correl(a.lat, b.lat)   where the VehiclePairStream is constructed by joining the two streams (a, b). Figure 3 shows the time-series data for longitudes collected for two different busses on the same route. The two stream variables could also belong to the current bus under investigation and its pre-recorded route   Figure 3.  Real-time longitute information compared to the recorded or  grouped vehicle longitutes. This data is used for correlation.

obtained by averaging the longitudes of the busses that travel daily on the same route. The goal is to continuously track buses and detect anomalies in real-time such as group separation, out-of-route movements or extreme traffic delays.

Figure 4 shows the results of correlation for different sliding and tumbling window sizes (25-50-100). If the correlation C is lower than a certain threshold (e.g. C < 0.8) an alarm can be generated. The bus has either gone out of route or is not moving timely, both of which denote anomalies. Note that smaller window sizes have less data to compare, therefore when buses move even slightly different wrt. each other, the correlation value drops sharply for that period. Therefore, small windows result in high false positive rates. For larger window sizes like 50-100 this effect is compensated for, as one bus usually catches up with the other (or the same bus compensates for its transient delay over the same route at different times). To reduce the amount of processing and output produced, we could use tumbling windows which only publish results at the end of a time or count period. We found that tumbling window is basically a discrete version of the continuous sliding window and similar correlation results are published by both. Therefore, we skip tumbling window results for brevity and refer users to our previous work [3]. As shown in Figure 4b, changing the sliding window size does not affect the processing latency, since parts of the correlation formula are calculated incrementally. For tumbling windows the delay increases with window size, because all the data that is collected until the end of a time interval is processed at once. This finding is in line with the motivations for fast incremental updates (FUP) used in finding frequent itemsets over streams [7,8,9].



IV. RULE MINING OVER STREAMS This section describes implementations of Apriori and  FPGrowth algorithms over data streams and their application as a real-time music recommendation engine.

A. Association Rule Mining (ARM) Algorithms In an association rule denoted by X Y (S,C), X and Y  refer to the frequent itemsets, S to the Support and C to the Confidence for the rule. Support of an itemset is the  joy ??          (a)  Change of Correlation value with different Sliding Count-Windows.

(b)  Comparison of Sliding-Tumbling windows processing latencies.

Figure 4.  (a) Change of lat-long correlations values over time for different sliding window sizes (b) Performance comparison of sliding vs. tumbling.

percentage of records in a database that contain that itemset (either X, Y, or both). Confidence of the above rule is calculated as the percentage of records that contain X that also contain Y. The Confidence formula can also be shown as Conf(X Y) = Support(X Y)/Support(X). Apriori algorithm counts frequent itemsets, generates candidate itemsets using the minimum support value (e.g. 0.1), prunes the infrequent ones, calculates confidence on all permutations of the frequent itemsets and selects those above the given Confidence threshold. FP-Growth algorithm does a first pass over the transactions creating a frequency-sorted database of items, omits the infrequent items, and finally creates an FP-tree [5,8].

B. Why is rule mining over Streams critical?

We are at an age, where the trends do not hold for long.

Therefore, the temporal aspects of recommendations are extremely important. Unfortunately, when the streaming data volumes as well as output rules, the reaction of data analysts is to increase support and confidence values to obtain fewer rules with stronger lift. Yet, rules that present themselves with high lift values over a long time period  (e.g. a month) may have already become outdated by the end of that period. For example sales of ice-cream, cold drinks, and plastic cups will be extremely popular for the single hottest month of the year, as sand-bags and shovels are during a hurricane. There will be no sales opportunity after the trend is gone. These temporal emerging patterns occur even much faster for online sales or in stock markets where millions of transactions occur every second. Stream rule mining can extract a trend such as ?when stocks HPQ and MSFT go down by >1%, DELL follows? within a single hour or day. Assume a recent trend (i.e. a rule with a minimum support and confidence) that emerges only in a certain timeframe. Its confidence value may not be the highest globally, but its local business value may be quite high. Our suggested systems are designed to catch these rules. Other applications of stream mining include clustering and classification over streams [7].

Think of a tumbling window size that is so large that it can cover all the data that is used for offline analysis. In that case, the offline analysis and the online one with single big window would generate the same ruleset results. With CEP systems and tumbling windows, it simply takes one parameter (i.e. window size) to switch from offline to near- real time analysis. If additional benefits are expected from sliding time-window analysis, again it is an insignificant effort to switch the window type.

C. ARM Query Implementation We obtained the Java implementations of the Apriori  and FP-Growth algorithms from the Association rule library of the famous machine learning tool called Weka [12] and integrated these algorithms in Esper engine which is also Java based.  One needs to implement a custom aggregation function in Esper (AggreationSupport class) to add new operators. We implemented this interface to add the algorithms directly into Esper for stream rule mining:  QUERY 1: SELECT Apriori(parameters, table.feature1, table.feature2) FROM event.win:length(5) AS table   The parameters we used to initialize Weka?s Apriori algorithms inside Esper were as follows: '-N 10 -T 0 -C 0.9 -D 0.05 -U 1.0 -M 0.1 -S -1.0 -c -1' where N is the number of rules to output, T is   the metric type by which to rank rules (0=confidence | 1=lift | 2=leverage | 3=Conviction), C is minimum metric score (e.g. min confidence = 0.9) of a rule, U/M are the upper/lower bounds for minimum support (defaults = 1.0 and = 0.1), D is the delta by which the minimum support is decreased in each iteration (default = 0.05), S is the significance level, and c is the class index (default = last).

It is possible to assign equal values to U/M bounds (e.g.

0.3) to avoid iteration, which would be the right choice for stream processing environments. However, in that case the user has to know the domain well and set the values  joy ??        correctly to find just the right number of rules for each time- window. In dynamic streaming environments fixed manual settings could lead to either too many or too few rules to be extracted. Therefore, we preferred to leave this dynamic adjustment to be done by the Weka system. The features selected are described in part D.

QUERY2: SELECT FPgrowth(parameter, table.feature1, table.feat2) FROM event.win:length(5) AS table  The parameters we used to initialize Weka?s FP-Growth algorithms inside Esper were as follows: '-P 2 -I -5 -N 10 -T 0 -C 0.9 -D 0.05 -U 1.0 -M 0.7' where P is the attribute index for binary attributes in normal dense instances (default index 2 for sparse instances is used), I is the maximum number of items to include in large items sets (and rules) (default = -1, i.e. no limit.), N is required number of rules output, T is the metric type by which to rank rules (0=confidence), C is minimum metric score (e.g. min confidence = 0.9) of a rule, D is the delta by which the minimum support is decreased in each iteration (default = 0.05), U/M are the upper/lower bounds for minimum support (defaults = 1.0 and =0.1).

D. LastFM Dataset and Preprocessing LastFM data contains information about ~1000 people  (lastfm-1K dataset) [10] listening to songs in the LastFM databases. There are approximately 75,000 unique artists, a few hundred thousand unique songs, and millions of transactions in this ~3GB dataset. Briefly, the fields include <user-id, timestamp, artist-mbid, artist-name, song-mbid, song-title>. In the preprocessing phase, we first cleaned the records with missing artist information and removed the time-song fields which did not contribute to the rule extraction. This process was done offline and our future work includes doing online preprocessing. We used count- based sliding windows. Finally, we had two features of the dataset (<user-id, artist-mbid>). Since the Apriori algorithm uses high amounts of memory, we further trimmed the data to include users that listened to more than 100 songs and songs that have been listened more than 3000 times overall. This resulted in 967 unique users listening to 1105 unique artists.

E. Performance Results The offline analysis results given in Table 2 show that  FP-growth generates "Top10 rules" results 75x-613x faster than the Apriori algorithm over the data stream. This is in line with most previous work [9], since FP-growth avoids the iterative candidate generations calculated by Apriori.

TABLE II: RESULTS OF OFFLINE ANALYSIS.

[i:967 a:1105] [i:1105 a:967] Apriori 61.403s 226.502s FPGrowth 0.811s 0.369s (i: instances, a: attributes)  The online analysis was done over smaller 10x10 user- artist sliding count windows. Figure 5 illustrates Weka?s dynamic changes of Support (minSupport, minConfidence) and the corresponding number of rules generated in each interval for the LastFM dataset. The X-axis for sliding count window graphs increment by 1 every 1 event count, whereas the tumbling windows will increment by 1 at every slide that moves the window by 10 events. Therefore, a 10-to-1 mapping between the two graphs is required (e.g. 88 8 or 9) to obtain the same time region. We see that Weka dynamic support (U/M) update operation is working properly to generate TOP10 rulesets for each period. We also find that the tumbling windows practically generate a Cumulative Distribution Function (CDF) or an ?aggregated? ruleset for the rules found by sliding windows. Yet, the most important learning from these results is that there exists transient rulesets in sliding window analysis, which are missed by the tumbling window online analysis due to the aggregation. The rulesets were computed around 300-500ms for each interval as shown in Figure 7 for both window types. We skip results with larger windows sizes for brevity.



V. RELATED WORK Our paper describes different ways of summarizing row  data by extracting actionable information from it through statistical correlations and rule mining. Other data-based techniques for reducing raw data size without losing the patterns carried inside include sampling, load shedding, sketching, synopsis and aggregations [7]. Aggregations over streams can include basic counting and summations as well as more complex mean, variance and correlation calculations. An overview of these techniques can be found in [7]. StatStream [15] is among the first in literature to discuss scalable stream correlations by using Discrete Fourier Transform (DFT) coupled with approximation techniques. In this paper, we applied correlation techniques to spatio-temporal data shown that correlations may not guarantee spatial locality [3].



VI. CONCLUSIONS AND FUTURE WORK In this paper, we presented implementation details for  doing both correlation analysis and rule mining over streams.

We analyzed different sliding window types and sizes with moving object and web log datasets. In the future, we plan to (1) test damped window (or time-fading) models, (2) automatically register extracted rules into CEP, (3) preprocess streaming data online, and (4) use Java set or multi-set classes to join the rulesets (Union, Intersection, and Difference) found by Apriori and FP-Growth.

