

Abstract? Classification using association is a recent data mining approach that integrates association rule discovery and  classification. A modified version of the Multi-class  Classification based on Association Rule (MCAR) is proposed  in this paper. The proposed classifier, known as Modified  Multi-class Classification based on Association Rule, MMCAR,  employs a new rule production function which resulted only  relevant rules are used for prediction. Experiments on UCI  data sets using different classification learning algorithms  (C4.5, RIPPER, MCAR) is performed in order to evaluate the  effectiveness of MMCAR. Results show that the MMCAR  produced higher accuracy compared to C4.5 and RIPPER. In  addition, the average number of rules generated by MMCAR  is less than the one produced by MCAR.

Keywords-multi-class classificationt; association rule; rule  mining;data mining; rule pruning

I.  INTRODUCTION  The availability of high performance computers, data  collection tools and the huge memory capacities made  gathering and saving huge quantities of data possible. For  example, the number of sales transactions executed during  one year in a large retail supermarket such as Carrefour is  numerous and the amount of data on the World Wide Web  (WWW) is extremely massive as well. This vast growth of  stored databases provides an opportunity for new automated  intelligent data analysis methods that summarizes  information from these databases. The process of generating  this useful knowledge is accomplished using data mining  techniques. In [1], the authors defined data mining as one of  the primary steps in Knowledge Discovery from Databases  (KDD), which finds and generate useful hidden knowledge  from large databases.

Classification using Association (CuA) also known  as associative classification is a research field in data mining  that integrates association rule discovery and classification.

CuA utilises association rule to discover knowledge and  selects a subset of which to build the classifier [2]. The main  goal for CuA is to construct a classifier based on the  identified knowledge from labelled input. This model is  later used to predict the class attribute for a test data case  [3].

In the last few years, several CuA algorithms have  been developed such as CPAR [4], Live and Let Live (L3G)  [5], MCAR [6], CACA [7], BCAR [8], LCA [2] and others.

These research studies showed that the approach produces  better classifiers (in terms of accuracy) than traditional  classification data mining approaches like probabilistic [9],  and decision tree [10]. Nevertheless, the CuA algorithm  suffers from exponential growth of rules, meaning they  derive large numbers of rules which makes the resulting  classifiers outsized and consequently limits their use since  decision makers face difficulty in understanding and  maintaining a large set of rules.

In this paper, an algorithm named ?Modified Multi  class Classification using Association Rule? (MMCAR) is  proposed to reduce the number of rules produced by  association rule based algorithms. The proposed MMCAR is  developed based on existing approaches of CuA.

Nevertheless, MMCAR employs a new class assignment  method which resulted only relevant rules are used to  predict test cases. The training method of MMCAR scans  training data sets only once [6] and the class assignment  (prediction) method makes a group of rule prediction  instead of utilizing only a single rule.

Different data sets from the UCI repository [12] have  been utilised to evaluate the proposed learning algorithm  and compare it with other traditional data mining  classification techniques including decision tree (C4.5) [13],  greedy classification (RIPPER) [14], and MCAR [6]. The  measures used in the experiments for comparison are the  prediction accuracy and the number of rules derived.

The paper is structured as follows: The problem of CuA  and its related works are presented in Section 2. The  proposed algorithm is discussed in Section 3. Data sets and  the experiments of using different classification algorithms  are demonstrated in Section 4, and the conclusions and  further research works are given in Section 5.



II. CLASSIFICATION USING ASSOCIATION  CuA is a case of association rule discovery, in which the  rule on right hand side (consequent) is the class label, and  the rule on left hand side (antecedent) is attribute values. So  In this paper, an algorithm named ?Modified Multi class Classification using Association Rule?  (MMCAR) is proposed to reduce the number of rules produced by association rule based algorithms. The proposed MMCAR is developed based on existing  approaches of CuA. Nevertheless, MMCAR employs a new class assignment method which resulted only relevant rules are used to predict test cases. The  training method of MMCAR scans training data sets only once [6] and the class assignment (prediction) method makes a group of rule prediction instead of         for a rule R: X ???Y , (X) is a conjunction of attribute  values, and (Y) is the class attribute. The ultimate goal of  CuA is to extract the complete set of rules normally called  class association rules (CARs) from the training data set.

A. The Problem    In the definition of the CuA problem, we employ [6]. Let T  be the input training data set with k different attributes A1,  A2, ? , Ak and L is a set of class labels. A specific attribute  value for Ai is represented by ai, and the class labels of L  are represented lj.

Definition 1: An AttributeValue (Ai, ai) is combination of  between 1 and k different attributes values, e.g. < (A1, a1)>,  < (A1, a1), (A2, a2)>, (A1, a1), (A2, a2), (A3, a3)>, ? etc.

Definition 2: A class association rule (CAR) is given in the  following format: ( Ai1 , a i1 ) ? ( Ai 2 , a i 2 ) ??... ??( A1k , a i k ) ???l i  where the antecedent is a conjunction of AttributeValues and the  consequent is a class.

Definition 3: The frequency (freq) of a CAR in T is the  number of cases in T that match r?s antecedent.

Definition 4: The support count (suppcount) of a CAR is the  number of cases in T that matches r?s antecedent, and  belongs to a class li for r.

Definition 5: A CAR (r) passes the minsupp if for r,  suppcount(r)/ |T| ? minsupp, where |T| is the number of  cases in T.

Definition 6: A CAR (r) passes minconf if suppcount(r)  /freq(r) ? minconf.

B. CuA Main Steps    Figure 1 depicts the main steps used in CuA. The first  step involves the discovery of frequent item set [15]. This  requires methods that find complete set of frequent items by  separating items that are potentially frequent and determine  their frequencies in the training data set (step 1). A rule will  be produced if an item set exceeds the Minconf threshold  value. The rule will be in the form of : X ??l , where l is the  largest frequency class associated with X in the training data  set (Step 2). In step 3, a selection of an effective subset of  rules ordering is performed using various procedures while  quality of the selected subset is measured on an independent  (test) data set in step 4.

Figure 1: Main Steps in CuA (adopted from Thabtah, 2007)    To explain the CuA discovery of rules and classifier  development, consider data shown in Table 1. The table  consists of two attributes, A1 (a1, b1, c1) and A2 (a2, b2,  c2) , and two class labels (l1, l2). Assume Minsupp = 30%  and Minconf = 80%, the frequent one, and two items for  data depicted in Table 1 is shown in Table 2, along with the  associated support and confidence values. Frequent items  (in bold) in Table 2 denotes those that pass the confidence  and support thresholds, which later are converted into rules.

Finally the classifier is constructed using a subset of these  rules.

Table 1: Training Data Set  RowNo Attribute1 Atribute2 class  1 a1 a2 l1  2 a1 a2 l2  3 a1 b2 l1  4 a1 b2 l2  5 b1 b2 l2  6 b1 a2 l1  7 a1 b2 l1  8 a1 a2 l1  9 c1 c2 l2  10 a1 a2 l1    The support threshold is the key to success in CuA.

However, for certain applications, rules with large    Table 2 : Potential Classifier For Data in Table 2                   confidence value are ignored because they do not have  enough support. Traditional CuA algorithms such as CBA  [16] and MCAR [17] use one support threshold to control  the number of rules derived and may be unable to capture  high confidence rules that have low support. In order to  explore a large search space and capture as many high  confidence rules as possible, such algorithms commonly  tend to set a very low support threshold, which may lead to  problems such as generating low statistically support rules  and a large number of candidate rules which require high  computational time and storage.

In response to these issues, one approach that suspends  the support and uses only the confidence threshold to rule  generation has been proposed in [18]. This confidence-  based approach aims to extract all rules in a data set that  pass the Minconf threshold. Other approaches are to extend  some of the existing algorithms such as CBA and CMAR.

These extensions resulted in a new approach called multiple  supports [3,5] that consider class distribution frequencies in  a data set and assigns a different support threshold to each  class. This assignment is done by distributing the global  support threshold to each class corresponding to its number  of frequencies in the training data set, and thus, considers  the generation of rules for class labels with low frequencies  in the training data.

An approach called MCAR [6] that employs vertical  mining was proposed in which during the first training data  scan, frequent items of size one are determined, and their  appearances in the training data (rowIds) are stored inside  an array in a vertical format. Any item that fails to pass the  support threshold is discarded. The rowIds hold useful  information that can be used laterally during the training  step in order to compute the support threshold by  intersecting the rowIds of any disjoint items of the same size.

It should be noted that the proposed algorithm of MMCAR  also utilises vertical mining in discovering and generating  the rules. Section 3 describes the proposed algorithm based  on an example.



III. MMCAR ALGORITHM    MMCAR goes through three main phases: Training,  Construction of classifier, and Forecasting of new cases as  shown in Figure 1. During the first phase, it scans the input  data set to find frequent items in the form <AttributeValue,  class> of size 1. These items are called one-item. Then the  algorithm repeatedly joins them to produce frequent two-  items, and so forth. It should be noted that any item that  appear in the input data set less then the MinSupp threshold  will be discarded.

Once all frequent items of all sizes are discovered, the  algorithm will check their confidence values, if confidence  value larger than the MinConf threshold becomes a CAR.

Otherwise, the item gets deleted. The next step is to sort the  rules according to certain measures and choose a subset of  the complete set of CARs to form the classifier. Details on  the proposed MMCAR are given in the next subsections.

Figure 2: MMCAR Algorithm    A. CARs Discovery and Production    MMCAR uses an intersection method based on what  so called Tid-list to compute the support and confidence  values of item values. The Tid-list of an item represents the  number of rows in the training data set in which an item has  occurred. Thus, by intersecting the Tid-lists of two disjoint  items, the resulting set denotes number of rows in which the  new resulting item has appeared in the training data set, and  the cardinality of the resulting set represents the new item  support value. Such method of computing support of all  items without scanning (going through) the training data set  for several times is represented as in Figures 2 and 3.

The proposed algorithm goes over the training data set only  once to count the frequencies of one-items, from which it  discovers those that passes the MinSupp. During the scan,  frequent one-items are determined, and their appearances in  the input data (Tid-lists) are stored inside a data structure in  a vertical format. Also, any items that didn?t pass the  MinSupp are removed. Then, the Tid-lists of the frequent  one-item are used to produce the candidate two-item by  simply intersecting the Tid-lists of any two disjoint one-  items. Consider for instance, the frequent attribute values  (size 1) (<a1>, l1) and (<a2>, l1) that are shown in Table 2  can be utilised to produce the frequent item (size 2) (<a1,         a2>, l1) by intersecting their Tid-lists, i.e. (1,3,7,8,10) and  (1,6,8,10) within the training data set (Table 1). The result  of the above intersection is the set (1,8,10) which its  cardinality equals 3, denotes the support value of the new  attribute value (<a1, a2>, l1). Now, since this attribute value  support is larger than or equal the MinSupp threshold, i.e.

30%, this 2-item will become frequent.

The above described training approach is called vertical  mining and has been used successfully in association rule  discovery, i.e. [19], and in classification, i.e. [6]. This  approach transforms the training data set into items table  that contains the locations (Tid-lists) of each item in the  training data set, and employs simple intersections among  these locations to discover frequent values and produce the  rules. Once all items of all sizes are discovered, then  MMCAR checks their confidence values and generate those  which pass the MinConf threshold as classification  association rule(s).

Figure 3: MMCAR Rule Production Function

I. Building the Classifier   One primary limitation of CuA approach in data mining is  the exponential growth of rule [20][21] , and thus a primary  motivation of this work is to cut down the number of  generated CARs. Prior to pruning redundant rules and to  build the classifier, rules must be sorted in order to prioritize  quality rule to be chosen as part of the classifier. In this  work, we have sorted the rules according to the following  guidelines:    1) The rule with higher confidence is placed in a higher rank  2)If the confidence values of two or more rules are the same,  then the rule with higher support gets a higher rank  3)If the confidence and the support values of two or more  rules are the same, the rule with less number of attribute  values in the antecedent gets a higher rank  4) If all above criteria are similar for two or more rules then  the rule which was produced first gets a higher rank    For each sorted rule (CAR), MMCAR applies it on the  training data set; the rule gets inserted into the classifier if it  covers at least one case regardless of the rule class similarity  to that of the training case. Now, once a rule gets inserted  into the classifier, all training cases associated with it are  discarded. In situations where a rule fails to cover any  training case then it will be removed. The same process is  repeated until no more cases remains in the training data set  or all CARs are tested.

In predicting test data case, the prediction method of  MMCAR divides all rules which match the test case into  groups; one for each class label and calculate the average  confidence and support values. Lastly, it assigns the test  case with the class of the group with largest average  confidence. In cases where there are two or more groups  with similar average, the prediction method considers then  the largest average support group. Unlike other current CuA  methods like MCAR [6] and CBA [16] which employ only  the highest confidence rule for predicting the test case, our  algorithm makes the prediction decision based on multiple  rules [20][21] Finally, in cases when no rules in the  classifier are applicable to the test case, the default class  (majority class in the training dataset) will be assigned to  that case.



IV. EXPERIMENTS RESULS  In this section, different classification algorithms are  compared with MMCAR with reference to classification  accuracy and number of rules produced by the classifier. A  total of six UCI data sets [12] have been used in the  experiments (refer to Table 3), and the algorithms included  in the comparison are the RIPPER [14], C4.5 [13] and  MCAR [6]. The reason behind selecting these algorithms is  the different training strategy they use in discovering the  rules. For example, C4.5 employs divide and conquer while  RIPPER utilises heuristic based strategy. On the other hand,  MCAR is a CuA algorithm.

Table 3: UCI Data Sets                            We employed cross validation which divides training data  set into (n+1) folds arbitrary and rules get learned from n  folds in each iteration and evaluated on the remaining hold  out fold. The process is repeated n+1 times and the results  are averaged and produced. In the experiments, we have set  the number of folds in cross validation to 10 as employed in  existing studies, i.e. [6].

Moreover, the main parameters of MMCAR and MCAR  which are the MinSupp and MinConf were set to 3% and  50% respectively in the experiments. This is since 3% of  support usually balances between the number of rules  discovered and processing time. All of the experiments were  performed on Pentium IV machine with 2.0 GB RAM and  2.6 GH processor. We have implemented MMCAR and  MCAR using Java, and the results of RIPPER and C4.5  were derived from WEKA [21], an open source machine  learning tool.

A. Results and Analysis    Table 4 contains the results of the classification accuracy  and Table 5 contains the number of rules , produced by  MCAR, and MMCAR. The results in the Table 4 show  consistency in the classification accuracies of both MMCAR  and MCAR and on average the CuA based algorithms  (MCAR and MMCAR) outperformed both decision tree and  rule induction classification approaches. Particularly,  MMCAR achieved higher accuracy than the C4.5 and  RIPPER with the average of 1.64% and 0.928% respectively.

MCAR algorithm achieved slightly better prediction  accuracy than the proposed algorithm with an average of  0.615%.

Table 4: Prediction Accuracy                                    Analysis on the number of rules were then performed  on results derived by MCAR and MMCAR. Data depicted  in Table 5 indicates that the proposed algorithm (i.e  MMCAR) produces less number of rules than MCAR.

MMCAR has derived on average 1% less number of rules  than MCAR on the employed data sets. This indicates that  classifier developed using MMCAR algorithm reduces the  size of the outputting classifiers which is also based on CuA  algorithm (i.e MCAR). Though, the accuracy of these  classifiers is slightly less than those of MCAR, it is believed  that ending up with a moderate size classifier can be  tolerated with possibilities of occurring error. This is  because end-users can easily control and maintain the rules  produced.

Table 5: Total Number of Rules

