2006 International Symposium on Evolving Fuzzy Systems, September, 2006

Abstract- The purpose of this paper is to propose a new easily implementable approach to a linguistic summarization of trends that may occur in temporal data, to be more specific - time series. To characterize the trends in time series, we use three parameters: dynamics of change, duration and variability, and apply to them the fuzzy linguistic summaries of data (databases) in the sense of Yager (cf. Yager [13], Kacprzyk and Yager [7] and Kacprzyk, Yager and Zadrozny [8]) which in the form of natural language-like sentences subsume the very essence of a set of data. A genetic algorithm is used to generate the linguistic summaries sought.



I. INTRODUCTION By providing tools and techniques to model and process  imprecise data, information, relations, etc., fuzzy logic based approaches make it possible to model and describe real life, human centric systems in a more human-consistent and concise way. A basic understanding of a system might be obtained via an analysis of its numerical characteristic features measured over a time span. In the simplest case such a characteristic feature may be treated as a real-valued function of time, i.e., a time-series data. Here we are interested in such a description of time series data that accounts for a proper understanding of the main trends regarding the system's behavior. There are many techniques providing such a de- scription, including those deeply rooted in statistics. However, usually they lack human-consistency that in many cases is what is primarily needed. In order to overcome this difficulty and supplement known strict methods we propose to employ linguistic summaries in the sense of Yager (cf. Yager [13], Kacprzyk and Yager [7] and Kacprzyk, Yager and Zadrozny [8]).

Linguistic summaries of data are meant as a concise, human-consistent description of a data set. In this approach a data set, e.g., the content of a database, is summarized using a natural language like expression such as "Most employees are young". Semantics of a particular linguistic terms involved, such as "most" and "young", and of the whole summary is determined using Zadeh's calculus of linguistically quantified propositions [14].

In order to describe time-series data by linguistic summaries first we need to pre-process the data. An analyzed period of time is partitioned into intervals in such a way that in each  Janusz Kacprzyk and Anna Wilbik are with Systems Research Institute, Polish Academy of Sciences ul. Newelska 6, 01-447 Warsaw, Poland Email: {kacprzyk,wilbik } @ibspan.waw.pl  Slawomir Zadrozny is with Systems Research Institute, Polish Academy of Sciences ul. Newelska 6, 01-447 Warsaw, Poland and Warsaw Infor- mation Technology (WIT) ul. Newelska 6, 01-447 Warsaw, Poland Email: zadrozn @ibspan.waw.pl   interval a certain trend might be observed in data. In each such an interval the time-series is approximated by a line segment.

Then each interval is characterized by a number of numerical attributes concerning the corresponding line segment such as its slope, length, etc. Finally, linguistic summaries of the intervals described as above, meant to represent trends in time- series data, are derived.

There is no generally adopted method for the generation of linguistic summaries. Kacprzyk and Zadrozny [9] proposed the use offuzzy association rules. George and Srikanth [3] as well as Kacprzyk and Strykowski [5] employed genetic algorithms to derive the summaries. In what follows we will employ the latter approaches and adapt them to a specific setting of time- series data.



II. CHARACTERIZATION OF TIME SERIES In our approach time series data {(Xi, Yi) } are approximated  by a piece-wise linear function f such that for a given E > 0, there holds  Vi: f(xi)- y < (1) There exist many algorithms that find such approximations.

Our starting point is the Sklansky and Gonzalez algorithm [12] that seems to be a good choice due to its simplicity and efficiency. We modified it in the following way. The algorithm constructs the intersection of cones starting from a point pi of the time series and including the circle of radius E around the subsequent data points Pi+j, j 1, 2,. . ., until this intersection becomes empty. If for Pi+k the intersection is empty, then the points pi, Pi+?,,.., Pi+1k- are approximated by a straight line segment, and to approximate the remaining points we construct a new cone starting at Pi+k-1. Figure 1 presents the idea of the algorithm. The family of possible solutions, i.e., straight line segments, to approximate points P1 and P2, is indicated with a dark gray area.

To make it more intuitively appealing we will now present the algorithm as a pseudocode. First, denote by:  . p_0 - the current starting point,  . p_1 - the last point successfully checked, i.e. for which the intersection of the cones starting at p_0 is non-empty,  . p_2 - the next point to be checked * Alpha_01 - a pair of angles (yi/31), meant as an  interval that defines the current cone, as shown in Figure 1 (indicated by light gray and dark gray area),  * Alpha 02 - a pair of angles defining the cone con- structed to check p_2 (i.e., the cone starting at point p_0 and inscribing the circle of radius E around the point p_2  7 (cf. (-'2, 2) in Figure 1))    - / /32 2 x  Fig. 1. An illustration of the algorithm [12] for an uniform -approximation  * function read-point ( ) fetches the next data point, * function find () finds a pair of angles defining the cone  starting at point p-0 and inscribing the circle of radius Earound of the point p 2.

The pseudocode of the procedure that extracts the trends is depicted in Figure 2.

read point(p_0); read-point (p_1); do)  p_2 = p_1; Alpha_02 = Alpha_01 = do  find(); Alpha_02;  Alpha_01 = Alpha_01 n Alpha_02;  pl=p_2; read-point(p_2); Alpha_02 = find();  while(Alpha_01 n Alpha_02 + 0);  save_found_trendo; p_O = pi1; p1 p2;  Fig. 2. Pseudocode of the procedure for extracting trends  The bounding values of Alpha_02 (Y2, 32), computed by function find(), are the slopes of the two lines such that:  . they are tangent to the circle of radius E around point p_2  . they start at point p_0  Let AX = XO-?2 and Ay = Yo -Y2. Then the angles Y2, can be expressed by the formulas:  'Y2 = arctg (( (Ax)2-) 2  32 = arctg (AX (AY) + 1 (AX)2arcug ~~(AX)2 2 (AY)2)  Then, as an approximation of points po, .P.,p1 we assume either a single straight line segment chosen as, e.g., a bisector, or one that minimizes the distance (e.g. assumed as the sum of squared errors, SSE) from the approximated points, or the whole family of possible solutions, i.e. the segments of rays of the cone.

This method is fast as it requires only a single pass through the data.

We characterize the trends, meant as "behavior" of a  sequence of the straight line segments of the uniform ?- approximation described above, using the following three features:  . dynamics of change,  . duration, and * variability.

In what follows we will briefly discuss these factors.

A. Dynamics of change Under the term dynamics of change we understand the  speed of changes. It can be described by the slope of a line representing the trend, (cf. any angle rq, r C(e , ) in Figure 1).

Thus, to quantify the dynamics of change we may use the interval of possible angles r C (- 90; 90) or their trigonometric transformation.

However it might be impractical to use such a scale  directly while describing trends. Therefore we may use a fuzzy granulation in order to meet the users' needs and task specificity. The user may construct a scale of linguistic terms corresponding to various slopes (steepnesses) of a trend line as, e.g.:  . quickly decreasing,  . decreasing,  . slowly decreasing,  . constant,  . slowly increasing,  . increasing, and  . quickly increasing.

Clearly, there may be more or less linguistic terms assumed  but, as it is well known, the so-called Miller's magic number 7 ? 2 is a good choice as it has a strong psychological motivation.

Figure 3 illustrates the lines corresponding to the particular linguistic terms.

In fact, each term represents a fuzzy granule of directions.

In [1], [2] there are presented many methods of constructing such a fuzzy granulation. The user may define a membership function of a particular linguistic term depending on his or her needs.

{    f(t) quickly increasin increasing  / / ~~slowly-  increasing  constant t  decreasing  qtuickly \ decreasing decreasing\  Fig. 3. A visual representation of angle granules defining dynamics of change  We map a single value r1 (or the whole interval of the angles corresponding to the gray area in Figure 1) characterizing the dynamics of change of a trend identified using the algorithm shown in Figure 2, into a fuzzy set best matching a given angle. Then we will say that a given trend is, e.g., "decreasing to a degree 0.8", if Ildecreasing(T1) = 0.8, where Idecreasing is the membership function of a fuzzy set representing the linguistic term "decreasing" that is a best match for the angle r1 characterizing the trend under consideration.

B. Duration Duration describes the length of a single trend. Again we  will treat it as a linguistic variable. An example of its linguistic labels is "long" defined as a fuzzy set whose membership function might be assumed as in Figure 4, where OX is the axis of time measured with units that are used in the time series data under consideration.

1'  I_-- t  Fig. 4. Example of membership function describing the term "long" concerning the trend duration  duration depend on the perspective assumed by the user. He or she, analyzing the data, may adopt this or another time horizon implied by his or her needs. The analysis may be part of a policy, strategic or tactical planning, and thus, may require a global or local look, respectively.

C. Variability Variability refers to how much "spread out" (in the sense of  values taken on) a group of data is. There are five frequently used statistical measures of variability:  * the range (maximum - minimum). Although this range is computationally the easiest measure of variability, it is not widely used as it is only based on two extreme data points; This make it very vulnerable to outliers and therefore may not adequately describe a real variability;  * the interquartile range (IQR) calculated as the third quartile minus the first quartile that may be interpreted as representing the middle 50% of the data. It is resistant to outliers and is computationally as easy as the range;  * the variance is calculated as 1/n Ei(xi- x)2, where x is the mean value;  * the standard deviation - a square root of the variance.

Both the variance and the standard deviation are affected by extreme values; and  * the mean absolute deviation (MAD), calculated as 1 /nEi xi- x . While it has a natural intuitive definition as the "mean deviation from the mean", the introduction of the absolute value makes analytical calculations using this statistics much more complicated.

We propose to measure the variability of a trend as a distance of data points covered by this trend from a linear uniform E-approximation that represents a given trend. For this purpose we propose to employ a distance between a point and a family of possible solutions, indicated as a dark gray cone in Figure 1. Equation (1) assures that the distance is definitely smaller than E. We may use this information for the normalization. The normalized distance equals 0 if the point lays in the dark gray area. In the opposite case it is equal to the distance to the nearest point belonging to the cone, divided by E.

Alternatively, we may bisect the cone and then compute the distance between the point and this ray.

Again the measure of variability is treated as a linguistic variable whose values are linguistic terms (labels) modeled by fuzzy sets defined by the user.



III. LINGUISTIC SUMMARIES AND THEIR APPLICATION TO TREND SUMMARIZATION  A linguistic summary, as presented in [10], [11] is meant as a natural language-like sentence that subsumes the very essence of a set of data. This set is assumed to be numeric and is usually large, not comprehensible in its original form by the human being. In Yager's approach (cf. Yager [13], Kacprzyk and Yager [7], and Kacprzyk, Yager and Zadrozny [8]) the following context for mining linguistic summaries is assumed:  * Y = {Yi, y},y is a set of objects (records) in a The actual definitions of linguistic terms describing the dgodatabase, e.g., the set of workers;    . A = .A... Am} is a set of attributes characterizing objects from Y, e.g., salary, age, etc. in a database of workers, and Aj(yi) denotes a value of attribute Aj for object yi.

A linguistic summary of a data set D consists of: * a summarizer P, i.e. an attribute together with a linguistic  value (fuzzy predicate) defined on the domain of attribute Aj (e.g. "low salary" for attribute "salary");  . a quantity in agreement Q, i.e. a linguistic quantifier (e.g.

most);  . truth degree (validity) T of the summary (meant as a proposition with fuzzy linguistic quantifiers) that is calculated using Zadeh's calculus linguistically quanti- fied propositions, i.e. a number from the interval [0,1] assessing the truth degree of the summary; usually, only summaries with a high value of T are interesting;  . optionally, a qualifier R, i.e. i.e. another attribute Ak together with a linguistic value (fuzzy predicate) defined on the domain of attribute Ak determining a (fuzzy subset) of Y (e.g. "young" for attribute "age").

In what follows we will often for brevity identify sum- marizers and qualifiers with linguistic terms they contain. In particular we will refer to membership functions ,Up or PR of the summarizer or qualifier as to be meant as membership functions of respective linguistic terms.

Thus, a linguistic summary may be exemplified by  T(most of employees earn low salary) = 0.7 (2)  A richer form of a linguistic summary may include a qualifier (e.g. young) as in, e.g.,  T(most of young employees earn low salary) = 0.9 (3)  Thus, basically, the core of a linguistic summary is a linguistically quantified proposition in the sense of Zadeh [14].

A linguistically quantified proposition corresponding to (2) may be written as  Qy's are P (4)  and the one corresponding to (3) may be written as  QRy's are P (5)  Then, the component of a linguistic summary, T, i.e., its truth degree, directly corresponds to the truth degree of (4) or (5). This may be calculated by using either the original Zadeh's calculus of linguistically quantified propositions (cf.

[14]), or via other interpretations of linguistic quantifier based aggregations.

In order to characterize the summaries of trends we will refer to Zadeh's concept of a protoform (cf. Zadeh [15], [16]).

Basically, a protoform is defined as a more or less abstract prototype (template) of a linguistically quantified proposition.

Then, summaries mentioned above might be represented by protoforms of the following form:  We may consider a short form of summaries:  Q trends are P (6)  like, e.g.,  Most of trends have a large variability  . We may also consider an extended form of the summary represented by the following protoform:  QR trends are P (7)  and exemplified by Most of slowly decreasing trends have a large vari- ability  Using Zadeh's [14] fuzzy logic based calculus of linguisti- cally quantified propositions, a (proportional, nondecreasing) linguistic quantifier Q is assumed to be a fuzzy set in the interval [0,1] as, e.g.

/Q(x) { 2x  for x>0.8 0.6 for 0.3 < x < 0.8  for x<0.3 (8)  The truth degrees (from [0,1]) of (6) and (7) are calculated, respectively, as  T(s) = T(Qy's are P) = pQ [ E UP (Yi) (9)  T(s) = T(QRy's are P) = HQ E=1 (PR(yi) A tip(yi))2 (10)  Ei=l1 R (Yi ) Both the fuzzy predicates P and R are assumed above to  be of a rather simplified, atomic form referring to just one attribute. They can be extended to cover more sophisticated summaries involving some confluence of various attribute values as, e.g, "slowly decreasing and short".



IV. A GENETIC ALGORITHM  The basic idea of a genetic algorithm (GA) was originally proposed by Holland [4]. GA is inspired by mechanism of natural selection in which stronger individuals are more likely to win in a competitive environment.

Genetic algorithms are probabilistic (non-deterministic) al- gorithms in which populations of individuals are generated, each individual being a potential solution of the problem under consideration. The individuals are characterized by a set of parameters. These parameters are regarded as genes of a chromosome and can be structured by a string of values in a binary form.

A genetic algorithm tries to find a best solution. The quality  of a solution is evaluated using a so called fitness function.

Thus, the value of this function for a given individual, known as a fitness value, reflects the degree of "goodness" of the individual.

A new population of individuals is generated using genetic  operators, notably the mutation and crossover. The mutation operator changes randomly alleles in every chromosome.

The crossover operator, from two individuals, called parents, forms a new solution, an offspring, that has some genes from each of its parents. The selection operator indicates individuals who will create a new population.

A    Genetic algorithms involve a continuous adaptation of the population to the environment. Unfortunately, there is no guar- antee that the process of adaptation will terminate providing a most desirable individual. However, under some conditions, it is possible to prove that the probability of success increases and tends asymptotically to 1.

We apply genetic algorithms to derive linguistic summaries  in the following way. Summaries are defined by their parame- ters, i.e., a quantifier, a summarizer and sometimes a qualifier, and are represented as chromosomes. We will discuss the details on an example.

Let us consider the generation of summaries concerning the stock quotations on the Warsaw Stock Exchange, where shares of almost 140 companies are quoted and traded. We use the binary coding, however the use of the integer coding is equally possible. A chromosome representing an individual (a linguistic summary) may have the form:  Q R y p  where Q is a coded value of a label of a quantifier, R stands for a qualifier (attribute and a label of its linguistic value).

The absence of the qualifier is encoded in a special way. y is an encoded name of share or group of shares, e.g. high tech companies or banks. P is a summarizer, encoded similarly to a qualifier.

If we assume a simplistic situation that we characterize the extracted trends using three features only as discussed in Section II: dynamics of change, duration and variability, and each factor, as well as the quantifier is described by seven labels, then there are about 2200 possible summaries. In our examples there are approximately 400000 possible summaries.

The linguistic summaries may be generated for a data set  concerning longer or shorter time horizon. If various time horizons are to be considered during the generation then the number of possible summaries grows rapidly.

In the basic form of our approach we employed here the fitness function (9) in a simpler form without a qualifier:  T(s) = T(Qy's are P) = UQ [ ZPI(i)Y or (10) in a extended form with a qualifier:  T(s) = T(QRy's are P) =  linguistic labels: low variability, moderate variability and high variability. There are 7 labels describing the quantifier: very few, few, below half, around half, above half, most and all.

Then a solution is coded with 13 bits as:  DDD DD DDD DD DDD Q R labelR P labelp  For example 0011001011011 denotes a summary: 'few long trends have a high variability".

In this basic version we propose to use the one point crossover operator. Other crossover operators like the two point or uniform crossovers are also possible. First, a crossover point on the parent organism string is selected. All data beyond that point in the chromosome string is swapped between the two parents yielding two offsprings as depicted in Figure 5.

parents: DDDDDDDDDF DDDD EEEEEEEE Eu..

offsprings: DDDDDDDDDFlUME EEEEEEEEE1 DDD  Fig. 5. Example of the one point crossover  Additionally the offspring is randomly mutated. For each gene a random number is generated. If its value is smaller than a certain probability of mutation, then the value of this gene is changed from 0 to 1 or from 1 to 0.

Afterwards the chromosome is checked, if it represents a proper summary, and if needed, it is corrected.

For this simple example we have obtained a few summaries such as:  * most trends are very short with the truth degree 1.0 * few trends are slowly decreasing with the truth degree  0.8609 * over half trends with a high variability are constant with  the truth degree 1.0 * most very short trends have a low variability with truth  value 0.7467

V. CONCLUDING REMARKS [Z=l(i(yi) A iP(,,V)l We proposed a simple, yet efficient method for temporal  n iPR (v ) data summarization. We identify trends in time-series dataL L 1iI'R(Yij zusing an effective and efficient modification of the algorithm where s is a summary. proposed in [12]. Then, having trends spotted, we propose We are interested in getting all summaries having the truth to characterize them with three basic attributes concerning  degree high enough, greater than a certain context-dependent their: dynamics, variability and duration, as proposed in [6].

threshold value defined by the user. Thus all such summaries Finally, the data set obtained is described by a set of linguistic are recorded at the end of each genetic algorithm iteration. summaries in the sense of Yager. To overcome numerical  Let us consider the following toy example in which we difficulties, associated with a need to maximize a highly non- only summarise the quotation of one investment fund. We linear function we use a genetic algorithm.

assume the granulation of dynamics of change as depicted in The results of some preliminary attempts to linguistically Figure 3. The trends may be characterized as very short, short, summarize trends of share quotations at the Warsaw Stock average, long or very long. Variability is described by three1 Exchange seem to be promising.

The next step will consist in using a more sophisticated form of the fitness function. We will take into account the idea of so called constraint and constituent descriptors, proposed by George and Srikanth [3]. A further research will also focus on the search for other possible ways/attributes to characterize the trends.

