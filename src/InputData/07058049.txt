Itemset Approximation Using Constrained Binary Matrix Factorization

Abstract?We address in this paper the problem of efficiently finding a few number of representative frequent itemsets in transaction matrices. To do so, we propose to rely on matrix decomposition techniques, and more precisely on Constrained Binary Matrix Factorization (CBMF) which decomposes a given binary matrix into the product of two lower dimen- sional binary matrices, called factors. We first show, under binary constraints, that one can interpret the first factor as a transaction matrix operating on packets of items, whereas the second factor indicates which item belongs to which packet.

We then formally prove that one can directly mine the CBMF factors in order to find (approximate) itemsets of a given size and support in the original transaction matrix. Then through a detailed experimental study, we show that the frequent itemsets produced by our method represent a significant portion of the set of all frequent itemsets according to existing metrics, while being up to several orders of magnitude less numerous.



I. INTRODUCTION  Frequent pattern mining is a major domain of pattern mining. Its goal is to automatically extract frequently occur- ring patterns in large datasets. The fundamental problem of frequent pattern mining is to mine frequent itemsets [1], i.e.

sets of items frequently occurring together in transactional databases. This problem originated from the analysis of commercial databases, and was later extended to more complex patterns and a broad range of domains (see [2] for example).

Frequent pattern mining algorithms have to explore a huge combinatorial space of patterns. This leads to two main problems: first, mining algorithms usually need a large amount of time to output the results. Second, the set of frequent patterns is often huge: millions of frequent patterns are commonplace. This issue makes the data analysis task much more difficult.

There has been a large body of research on condensed representations of frequent patterns such as closed frequent patterns [3], [4], that can usually output one order of mag- nitude less patterns without loosing information. However, one order of magnitude reduction is often not enough when dealing with millions of patterns, hence a current trend of research focuses on discovering a small set of patterns that would represent most of the information contained in the set of all frequent patterns, and that would be small enough to be manageable by the analyst [5], [6].

Our contribution is to introduce a new method for com- puting a reduced set of representative frequent patterns, that is both computationally efficient and produces very few high quality frequent patterns. Our method is based on Constrained Binary Matrix Factorization (CBMF) [7], [8] which has been widely used in data mining applications recently. This technique reduces data dimensionality while keeping most of its information. Exploiting this tool, we theoretically study the characteristics of a matrix decom- position method adapted to frequent itemset mining. We then prove that extracting frequent itemsets from the factors of the decomposition guarantees to find existing frequent itemsets with an approximation support threshold and size.

Additionally, we show through a detailed experimental study that the frequent itemsets output by our method represent a significant portion of the set of all frequent itemsets according to existing metrics [9], while being up to nine orders of magnitude less numerous.

The outline of this paper is as follows: Section II describes the main notions and Section III reviews the state of the art.

The core of the paper is composed of the theoretical analysis in Section IV. Section V explains the implementation and Section VI reports experimental results. Lastly, Section VII concludes the paper and discusses some future research directions opened by this work.



II. PRELIMINARIES  A. Frequent itemset mining  Let I be a set of items. An itemset is a subset of I . A transaction database (TDB) is a set of transactions D where each transaction t 2 D is an itemset of I . We will denote by m the size of a transaction database, i.e. its number of transactions. For an itemset P ? I and a transaction database D, the support of P in D, denoted support(P,D), is the number of transactions of D including P :  support(P,D) = |{t | t 2 D ^ P ? t}|  The frequency of P is frequency(P,D) = support(P,D) m  .

P is frequent if its support is greater than a given minimum support threshold minsup: frequency(P,D) ? minsup.

B. Constrained Binary Matrix Factorization The general goal of matrix decomposition is to factorize a  matrix into the product of two or more smaller matrices. The Constrained Binary Matrix Factorization (CBMF) problem, as a special case of matrix decomposition, is described as follows: given X 2 {0, 1}M?N and K 2 N, K << min(M,N): >>>>><  >>>>>:  argmin W,H  ||X?W ?H|| p  (p = 1 or p = 2)  subject to: (i) W 2 {0, 1}M?K ,H 2 {0, 1}K?N  (ii) W ?H 2 {0, 1}M?N  where ||.|| p  denotes either the L1-norm (p = 1) or the L2- norm (p = 2). Constraint (i) guarantees that the factors W and H are binary and constraint (ii) that the matrix they yield (W ?H) is also binary.



III. STATE OF THE ART We briefly review the major studies in the recent domain  of finding an informative subset of the frequent patterns. In order to output a small set of frequent patterns, some studies explicitly define a targeted number of patterns, and criteria that have to be verified by the set of patterns. Guns et al.

[6], for example, present an exhaustive approach, where the criteria to be satisfied are expressed through constraints such as coverage or area. Other approaches, such as [10], make use of redundancy-based heuristics to determine the set of patterns.

Other studies rely on compression, either over all the frequent patterns [11], or over the dataset [5]. In the first case, frequent itemsets that best approximate the set of all frequent itemsets are computed. In the second case, the authors exploit the Minimal Description Length principle to compute which set of patterns best compresses the database.

All of the previously cited studies consider the set of all frequent patterns in order to compute the reduced subset of patterns. The approach in [5] is a two pass approach, that first has to compute all the frequent patterns and then select a subset of them, which is computationally inefficient. On the other hand, the approach in [6] computes in one pass a subset of frequent patterns that optimize a given criterion. However, to do so, it has to explore a complex search space over the sets of patterns, which is also computationally expensive.

Our approach, however, is different from the previous studies in that we rely on main components of the data.

One should note that the proposed approach is also different form studies like [12] which use the Boolean decomposition and closed itemsets to reduce the dimension. Contrary to other pattern set discovery methods, by moving the burden of reducing the number of patterns from the pattern search space to the input matrix itself, one can design mining algorithms that are computationally efficient and that yield good results.



IV. THEORETICAL ANALYSIS  We first define two kinds of decompositions, strong de- composition and approximately strong decomposition, that are well adapted for frequent itemset mining. Then, we show the relation between the frequent itemsets that can be discovered in the matrix decomposition factors and those of the original matrix.

We first define the matrix notations used in this paper. For a given matrix X  m?n, Xij denotes the element of the matrix located in row i and column j. X  i? denotes the summation of row i (i.e. X  i? = P  n  k=1 Xik) and X ?j denotes the  summation of the jth column of X (i.e. X?j = P  m  k=1 Xkj).

The row vector taken from column-wise summation of X is denoted by X?. Similarly, the column vector obtained from row-wise summation of X is shown by X?. The ith row of X is denoted by X  i  and the jth column of X is denoted by Xj . The following example, illustrates the notations that we use in this study.

Example 1: Consider the following matrix, X . In this matrix, X3,4 = 0, X1? = 4, X?4 = 1, X?, X?, X2 and X5 is shown below:  X =   @ 1 1 1 0 1 0 0 1 1 1 1 0 0 0 1 1 1 0 0 0 0   A  X? = ? 3 3 3 1 1 0 0  ? X? =   @   A  X2 = ? 1 1 1 1 0 0 0  ? X5 =   @   A  We now intuitively explain how itemsets can be found in the factors of a matrix decomposition. Consider a de- composition of an input matrix X with k latent factors: X  m?n ?Wm?k?Hk?n. Each of the k columns of W can be mapped to a set of columns of X (through the product with H). As columns of X correspond to items, columns of W correspond to packet of items. On the other hand, in H , one observes the relation between each packet of items of W and each original item of X . Using this setting, one can easily reconstruct itemsets of X . To formally define this setting, we start by defining the notion of valid transaction matrix, that will be the input of our method.

Definition 1: (Valid transaction matrix) A matrix X m?n  is called a valid transaction matrix if it is binary and there is no rows or columns with all elements equal to zero. More formally, X  m?n is a valid transaction matrix if: (1) 8i, j, 1 ? i ? m, 1 ? j ? n, X  ij  2 {0, 1} (2) 8i, j, 1 ? i ? m, 1 ? j ? n, X  i? > 0, X?j > 0 Now we consider different binary decompositions of a valid transaction matrix:    A. Strong decomposition In this case, the decomposition is exact: Definition 2: (Strong decomposition) A matrix X  m?n is strongly decomposable if: (1) it is a valid transaction matrix (2) 9k, 1 ? k ? n such that X  m?n = Wm?k ?Hk?n (3) W 2 {0, 1}, H 2 {0, 1}, W ?H 2 {0, 1} Condition (3) states that the factors W and H as well as their product (W ? H) are binary. The notion of strong decomposition furthermore implies that one can find factors that are all relevant, in the following sense.

Property 1: If X can be decomposed strongly, then there is a strong decomposition of the form X  m?n = Wm?k ? H  k?n such that there is no columns in W and no rows in H with all elements equal to zero. More formally, for all t, 1 ? t ? k, W ?t > 0 and H  t? > 0.

Proof: If X is strongly decomposed and W (resp.

H) contains one or more fully-zero columns (resp. rows), then one can remove the zero columns of W and their corresponding rows in H (resp. zero rows of H and their corresponding columns in W ), and still obtain, through the product of the simplified matrices, a strong decomposition of X with k0 latent factors such that k0 < k.

Using the above definitions, we are now ready to provide the following theorem which describes a first link between the itemsets mined from W and H and itemsets in X .

Theorem 1: Let X m?n = Wm?k ? Hk?n be a strong  decomposition of X . If a packet of items with support value f is found in W (i.e. 9p, 1 ? p ? k, W ?p = f ), then there is at least one itemset with support value of at least f in X .

Proof: Suppose that packet p, 1 ? p ? k, has support value f in W , i.e  P m  i=1 Wip = f . Because of the definition of strong decomposition and of Prop. 1, there exists at least one q, 1 ? q ? n, such that H  pq  = 1. Then, for all i, 1 ? i ? m, we have:  X iq  = P  k  j=1 WijHjq ?WipHpq  And thus: P  m  i=1 Xiq ? P  m  i=1 WipHpq = Hpq P  m  i=1 Wip  So finally: P  m  i=1 Xiq ? f  which establishes the theorem.

In simpler terms, Theorem 1 shows that, in a strong decom- position, if there exists a frequent packet of items in W with support value of f , then there is a corresponding frequent itemset with support value of at least f in X .

An important consequence of Theorem 1 is regarding the size of the captured itemset. In this theorem, for a p such that W ?p = f , if only one q is found such that H  pq  = 1, then there is an itemset of size one (a singleton), which is not very useful. However, it is easy to prove that if several such q?s are found, then an itemset, and not a singleton, in X is captured. This is expressed in the following corollary.

Corollary 1: If X m?n = Wm?k ? Hk?n is a strong  decomposition of X , and there exists a packet of items p, 1 ? p ? k of size l in H (i.e. H  p? = l) with support value f in W (i.e. W ?p = f ), then there exists an itemset of size at least l and support value of at least f in X .

Corollary 1 can be easily derived from Theorem 1. This result is very important since it shows that one is able to mine frequent itemsets of any size using the factors of the decomposition. As one can see, once a transaction matrix has been strongly decomposed into latent, binary factors, we can efficiently obtain some frequent itemsets from the decomposition without reloading the data into memory. In practice, however, a strong decomposition of a transaction matrix may not exist, and we now turn to a more realistic decomposition and reformulate the materials provided so far.

B. Approximately strong decomposition A direct extension of strong decompositions is to no  longer assume that the decomposition is perfect, i.e. has no error. This can be simply done by adding an error term in the reconstruction of X from the latent factors.

Definition 3: (Approximately strong decomposition) A matrix X  m?n can be decomposed approximately strongly if: (1) it is a valid transaction matrix (2) 9k, 1 ? k ? n such that X  m?n = Wm?k ? Hk?n + ? m?n  (3) W 2 {0, 1}m?k, H 2 {0, 1}k?n and W ? H 2 {0, 1}m?n Note that having values of X , W , H and W ?H in {0, 1} implies ? 2 {?1, 0, 1}m?n. It should be also noted that Prop. 1 holds for the approximate decomposition as well.

Based on definition 3, now we can rewrite Theorem 1.

Theorem 2: Let X m?n = Wm?k ?Hk?n + ?m?n be an  approximately strong decomposition of X , and let ? max  be the maximum of absolute values of ??. If a frequent packet of items with support value of f is found in W (i.e. 9p, 1 ? p ? k, W ?p = f ), then there is at least one itemset with support value of at least f ? ?  max  in X .

Proof: Suppose that packet p, 1 ? p ? k, has support  value of f in W , i.e P  m  i=1 Wip = f . Because of the definition of approximately strong decomposition and of Prop. 1, there exists at least one q, 1 ? q ? n, such that H  pq  = 1. Then, for all i, 1 ? i ? m, we have:  X iq  = P  k  j=1 WijHjq + ?iq ?WipHpq + ?iq  And thus: P  m  i=1 Xiq ? P  m  i=1 WipHpq + P  m  i=1 ?iq = H  pq  P m  i=1 Wip + P  m  i=1 ?iq = f + P  m  i=1 ?iq  So finally: P  m  i=1 Xiq ? f ? ?max  which establishes the theorem.

Considering itemsets of a given size with approximately strong decompositions is however slightly more difficult than    with strong decompositions. The following theorem extends the corollary of Theorem 1 (Corollary 1) to the case of approximately strong decompositions.

Theorem 3: Let X m?n = Wm?k ? Hk?n + ?m?n  be an approximately strong decomposition of X , and ?(a, b) = (W a ? ?b) where ? represents dot product (a 2 {1, ? ? ? , k}, b 2 {1, ? ? ? , n}). Also let ?(p) = min  1?j?n H  pj  ? ?(p, j), 8p, 1 ? p ? k. Now, if there exists a packet of items p, 1 ? p ? k, of size l (e.g. {q1, q2, ? ? ? , ql}) in H (i.e.

H  pq1 = Hpq2 = ? ? ? = Hpql = 1 and Hp? = l) with support value of f in W (i.e. W ?p = f ), then there exists an itemset of size at most l and with support value of at least f +?(p) in X . Furthermore, if ?(p) = 0, then {q1, q2, ? ? ? , ql} is an itemset of size l in X with support value of f .

Proof: First note that ?(a, b) corresponds to the re- construction error for item b in packet a, and that ?(p) corresponds to the minimum reconstruction error of all items belonging to packet p. Without loss of generality, we assume that a frequent itemset of size l and support f occurs in the first f rows of W and first l columns of H (see the following illustration).

W =   BBBBBBBB@  1 ? ? ? k 1 1 ...

f 1 ... 0 ...

...

m 0   CCCCCCCCA  H =   @  1 ? ? ? l l + 1 ? ? ? n 1 1 ? ? ? 1 0 ? ? ? 0 ...

k   A  We thus have:  1st item : P  f  i=1 Xi1 = P  f  i=1 Wi1H11 + P  f  i=1 ?i1 ...

lth item : P  f  i=1 Xil = P  f  i=1 Wi1H1l + P  f  i=1 ?il  which can be rewritten:  1st item : P  f  i=1 Xi1 = P  m  i  0=1 Wi01H11 + P  m  i  0=1 Wi01?i01 ...

lth item : P  f  i=1 Xil = P  m  i=1 Wi1H1l + P  m  i0=1 Wi01?i0l  Furthermore, according to the definition of ?(p) for any packet p, we have:  1st item : P  f  i=1 Xi1 = f + ?(1, 1) ? f + ?(1) ...

lth item : P  f  i=1 Xil = f + ?(1, l) ? f + ?(1)  which shows that there is an itemset included in {i1, ? ? ? , il} with support of at least f + ?(1). Furthermore, if ?(1) = 0, then {i1, ? ? ? , il} is an itemset of size l and support value f in X .

Considering Theorem 3, one can easily mine approxi- mate itemsets using an approximately strong decomposition.

Algorithm 1 shows this procedure (note that, according to Theorem 3, when ?(p) = 0, one is able to identify an itemset in X with its exact support and size).

Algorithm 1 Min-supp Min-size Approximate Itemset Min- ing Input: W , H , vector ?, min-supp f , min-size l Output: S : a set of itemsets with length of at most l and  support of at least f 1: S  ; 2: for all rows r in H do 3: if H  r? ? l then 4: if W ?r + ?(r) ? f then 5: P  items corresponding available in r 6: S  S [ {P} 7: end if 8: end if 9: end for

V. IMPLEMENTATION  The previous development shows how one can mine a transaction matrix from its decomposition. Based on that, having a valid transaction matrix, one can easily apply the decomposition (to obtain the factors, i.e. W and H) and start mining the itemsets. According to the input of Algorithm 1, we need a decomposition method to obtain high quality factors. There are generally two approaches to solve the CBMF problem described in Section II (one should note that we are not considering the Boolean matrix decomposition [13] in this paper since all our theoretical analysis is based on classical matrix multiplication). One of these methods has been described in [7] and is called Proximus. In this approach, a decomposition problem with k latent factors is iteratively solved via simpler problems with k = 1. The solution to the original problem is then the aggregation of all k = 1 decompositions.

Another more recent approach to solve the CBMF prob- lem is presented in [8] and relies on k-means (we refer to this approach as clustering-based approach). The difference between the clustering-based approach and Proximus is that in the former, the problem of decomposing into k factors is solved directly (starting with k factors from the beginning), while the latter solves the problem indirectly (solves a k = 1 problem at a time).

In the experimental section, we use the clustering-based approach [8] since it performs better decomposition com- paring to the Proximus. One should note that both of these methods obtain binary reconstruction through enforcing the orthogonality on one of the factors, which is a sufficient condition to have a binary reconstruction.



VI. EXPERIMENTS  In this section we evaluate the proposed method (called DIM: Decomposition Itemset Miner) based on the quality of produced itemsets and the mining efficiency through a set of experiments on different datasets. The qualitative experiments compare the set of itemsets output by DIM with the set of all closed frequent itemsets output by LCM algorithm [4]. The efficiency experiments compare the mining time of the LCM. For LCM, we use its authors C implementation, while DIM is implemented in Matlab, which has very efficient matrix multiplication and decom- position implementations. The experiments were conduced on an computer with an Intel Xeon E-2630 with 6 cores @ 2.30 Ghz with 32 GB of RAM. No multiprocessing or multithreading has been applied on DIM.

The datasets used in the experiments are publicly available datasets from the FIMI repository 1. Following the method- ology presented in [14], we choose pumsb, accidents and T40I10D100k as representatives of structured dense, dense and sparse datasets respectively. We have chosen one dataset from each category for space reasons; however, one should note that the results are similar for other datasets of each category.

A. Efficiency evaluation  Contrary to other frequent itemset mining techniques, the complexity of our approach does not depend on the minimum support threshold, i.e. once a decomposition is ob- tained, the mining process is quite fast and straightforward.

This experiment may be considered unfair, as LCM aims at finding all closed frequent itemsets, while our approach only computes a small set of representative frequent itemsets.

Our approach might thus be compared with approaches computing a representative subset of the frequent itemsets.

However, as discussed in the related work section (section III), most existing approaches to identify a representative subset of the frequent itemsets are two-pass approaches, that first compute all (closed) frequent itemsets and then compute a subset of these frequent itemsets. The existing one-pass approach [6] is more general than ours and does an exhaustive search on a huge search space. Its time complexity is thus much higher than ours. Therefor, their running time is higher than the one of LCM alone, and comparing our running time to the one of LCM is indeed unfair, but to our disadvantage.

1http://fimi.ua.ac.be/data/ (last visited 01/07/2014)  Figure 1 present the mining time with a varying minimum support for both LCM and DIM. As expected, DIM running time is constant. Note that, for each support value, we reported the decomposition time (clustering-based decompo- sition time) plus itemset mining time (Algorithm 1). In prac- tice, the decomposition has only to be done once, and then it can be exploited by Algorithm 1 for any support value. It can be observed that, for high support values, LCM is faster than DIM. However, for lower support values, LCM suffers from the combinatorial explosion of the number of results, and needs much more computation time than DIM. This point is more crucial in structured dense datasets, like pumsb, where even with high support values, itemset mining task is very difficult. In dense datasets, like accidents, LCM becomes very slow for support values smaller than 10%. In sparse datasets, like T40I10D100k, it is easy to find the itemsets with high support values. However, as these datasets contain quite huge itemsets with small support values, LCM spends a long time to find closed itemsets.

B. Qualitative evaluation  DIM mines very few itemsets and the efficiency ex- periments have shown that for low support values this approach could output a solution faster than state-of-the- art algorithms. It thus remains to evaluate the quality of the itemsets found w.r.t. the complete set of closed frequent itemsets.

For this, we rely on metrics presented in [9], designed to evaluate the quality of approximate frequent itemset mining algorithms. In these metrics, itemsets output by LCM are called base-itemsets, and itemsets outputs by DIM are called found-itemsets.

Recoverability: This metric measures how well a col- lection of found-itemsets can cover base-itemsets. Conse- quently, this metric is similar to recall. For a base-itemset B  i  , recoverability is calculated as follows: among all found- itemsets F  j  we look for the one which has the maximum number of items in common with B  i  , namely F i max  . The recoverability of B  i  is defined as:  recoverability(B i  ) = |B  i  \ F i max  | |B  i  | (1)  The total recoverability is a weighted average (bigger base-itemsets contribute more than smaller ones) of the recoverability of all base-itemsets.

Precision: As we can see, having one single large found- itemset (possibly including all items) results in a recover- ability of 1 for all base-itemsets. Therefore, we need another metric to penalize such cases. Spuriousness is a metric to evaluate the quality of found-itemsets. For a found-itemset F i  , spuriousness is defined as follows: among all base- itemsets, we find the one which has the maximum number items in common with F  i  , namely Bi max  . Then spuriousness          30 35 40 45 50  T i m e ( s )  min-supp(%)  pumsb  LCM DIM          5 10 15 20  T i m e ( s )  min-supp(%)  accidents  LCM DIM            0.1 0.3 0.5 0.7 1  T i m e ( s )  min-supp(%)  T40I10D100k  LCM DIM  Figure 1: LCM and DIM time comparison varying the min-supp for pumsb, accidents and T40I10D100k  of F i  is defined as follows:  spuriousness(F i  ) = |F  i  ? F i  \Bi max  | |F  i  |  In this case also the total spuriousness is computed by a weighted average (bigger found-itemsets contribute more than smaller ones) of the spuriousness of each found-itemset.

Precision of an itemset is then defined as 1?spuriousness.

A summary of the results is presented in Table I. Number of (closed) frequent itemsets for both LCM and DIM as well as the precision and recoverability of DIM itemsets are shown in this table. The minimum size for the itemsets is set to 2 since singletons are not of interest. The number of latent factors (k) is set to 30 since larger values of k results in uninteresting itemsets (i.e. itemsets with large error value).

For the sake of readability, high numbers are suffixed by M (millions) or B (billions). Cells with a star (*) report an estimate: LCM outputs too many items, resulting in output file of hundreds of Gigabytes and hard disk saturation. In these cases only a uniform random sample of 5 million itemsets was used to compute the metrics.

Dataset min-supp #(LCM) #(DIM) Prec. Recov.

Pumsb  40% 44.7M 7 71% 72% 20% 7.49B 10 34%* 76%* 10% ?10B 15 27%* 94%* 5% ?10B 17 25%* 94%* 1% ?10B 20 22%* 94%*  accidents 10% 9.97M 9 75% 67% 5% 64.8M 13 69% 68% 1% 1.62B 15 40%* 53%*  T40I10D100k  1% 64481 1 88% 2.8% 0.5 % 1.27M 7 84% 9.5% 0.3 % 3.56M 8 87% 10% 0.1% 18.4M 9 87% 18%  Table I: Comparing LCM closed itemsets and DIM itemsets  Two points in this table are of particular interest. First, whereas the number of closed frequent itemsets can get over billions of itemsets (in some cases it is event hard to count them, see pumsb dataset for example), the number of frequent itemsets output by DIM is always very low.

Thus, the frequent patterns output by DIM can be quickly examined by an analyst, which is not the case for all closed frequent itemsets produced by LCM.

Second, despite being so few, DIM frequent itemsets exhibit significant recoverability values. This fact becomes more important when the precision is also high, like accidents with minimum support of 10% and pumsb with minimum support of 40%. In these cases, not only we are able to recover a large amount of base-itemsets, but also we can guarantee that these itemsets are not long, uninteresting ones (according to the precision value). In other words, using DIM itemsets, an analyst can have a general, precise view of the entire data without being overwhelmed by millions or billions itemsets. For instance, in case of accidents with minimum support of 10%, we can look at 9 itemsets produced by DIM (instead of looking at 9.97M LCM itemsets) and recover 67% of all closed itemsets.This is an important contribution: it experimentally shows that by looking at DIM patterns, an analyst will have a significant glimpse of the knowledge that can be extracted from the dataset, in a very short amount of human analysis time.

The case for sparse datasets like T40I10D100k is different. For this dataset DIM extracts few itemsets with very high precision values but a relatively low recoverability (18% for a support of 0.1%). This result is in line with the different nature of T40I10D100k compared to Pumsb and accidents: T40I10D100k is a synthetic, sparse dataset and it does not exhibit the same strong ?pattern structure? as the two other datasets. This can be confirmed with LCM results: there are few patterns comparatively to the other datasets, and they appear at very low support values. In such a case, our method based on matrix decomposition is not the most adapted to get a representative view of the complete set of itemsets of LCM, as matrix decomposition techniques are likely to remove small groups of ones from the matrix for the sake of approximation. This results in losing some information that appears in the complete output of LCM. On the other hand, the itemsets captured by DIM are captured with an extremely high precision; for instance, for a support    of 0.1% the analyst will have to analyze only 9 itemsets, and get a good idea of the content of about one fifth of the complete number of itemsets.

Since DIM provides few itemsets, one may wonder if the amount of information provided by these itemsets can be also obtained by taking the top p (here top is considered w.r.t. support value) itemsets produced by LCM, where p is equal to the number of itemsets produced by DIM. Table II gives recoverability for the top p itemsets of LCM for each dataset (precision is by definition 100% in this case).

For instance, in the accidents dataset with minimum support of 10%, DIM outputs 9 itemsets, so we take the top 9 itemsets from the results of LCM, and compute their recoverability, which is 13%. The recoverability of the top p itemsets of LCM is more than 4 times lower than those of DIM in real datasets. In the synthetic dataset, however, the improvement is marginal.

The table, in general, confirms that DIM itemsets convey more information than traditional top p itemsets techniques.

Dataset p min-supp Recov. (LCM) Recov. (DIM) pumsb 7 40% 15% 72% accidents 9 10% 13% 67% T40I10D100k 9 0.1% 12% 18%  Table II: Comparing top-p LCM closed itemsets with DIM itemsets

VII. CONCLUSION AND FUTURE WORK  In this study, we have examined the problem of frequent itemset mining through decomposition of the input matrix.

Using theoretical analysis, we have shown that matrix de- composition can help us mining frequent itemsets.

Our experiments have shown that although for high sup- port values our approach is less time efficient than state- of-the-art algorithms, for low support values our approach is much faster than those algorithms. This approach is also highly scalable with respect to other algorithms according to the fact that the classical itemset mining approaches become very slow since they need to explore a huge combinatorial space as the number of items increases.

The proposed method only finds a handful of itemsets, but the experiments show that these itemsets convey a significant portion of the information of all frequent itemsets. We advocate that contrary to classical methods, the mining time of our method is well invested: what is the utility of a very fast algorithm that outputs millions of patterns that an analyst will take hours to make some sense of? With our method, the computer does most of the work, and the analyst is presented with concise, reliable and manageable information.

This work opens many future research directions such as studying the nature of the itemsets not recovered by our method or characterizing the set of frequent itemsets  obtained from latent factors with respect to the sets of frequent itemsets found by methods such as [5]: do such methods also re-discover the latent factors?

