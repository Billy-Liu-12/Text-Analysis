Enhancing Web Access Using Data Mining Techniques

Abstract  In this paper we study data mining techniques as tools for reducing the time needed to access web pages in the environment of a corporation where users connect to the Internet through a proxy server. We add a data mining server to the traditional web architecture. This server computes, using sequential patterns, the pages likely to be requested by the users, taking into account their web access history. Then, the server loads these pages into the proxy?s cache in order to have them available when they are actually asked for. We describe our implemen- tation and the results obtained, showing that the average access time to a web page can be dramatically reduced using this technique. Further, we also implemented our proposal using simple association rules, showing that se- quential patterns result in a smaller and more accurate set of rules, as a consequence of taking into account the or- der of the requests. Finally, we discuss the differences with other prefetching proposals.

1 Introduction  An important goal for Internet service providers is to increase web surfing speed. This requirement can be satisfied either with higher bandwidths(at the expense of higher costs), or implementing technological solu- tions, like intelligent algorithms for web page access and caching. In this work we study the use of data mining [5] techniques (association rules and sequential patterns, al- though the model supports any other technique) in order to predict the web pages likely to be accessed by an In- ternet user, in the environment of a corporation where all the users access the Internet through a proxy server. We developed a system that loads those pages in the proxy server?s cache while users may be performing another task. Thus, when users actually ask for a certain page, they will have it readily available. We also present the re- sults of a series of experiments, showing that using this  approach the average response time can be reduced by a factor up to six. We address in detail problems over- looked in similar proposals, like the requests that can- not be considered for analysis, or the need for data pre- processing. A distinctive characteristic of our proposal is the use of sequential patterns for prefetching, instead of plain association rules. With this approach, we take into account the order in which pages are visited, reduc- ing storage needs and bandwidth usage. In order to vali- date this hypothesis we also developed a rule engine that uses plain association rules, and compare the results.

Let us consider the following situation: Mary logs on to Internet every day for checking the news.

She usually visits the web sites of two newspapers: The Globe & Mail (www.theglobeandmail.com), and the Toronto Star(www.thestar.com). She checks the finance section of the first newspaper (www.globeinvestor.com), and then the weather forecast (www.theglobeandmail.com/weather). If she has enough time, she visits the Toronto Star?s web site. However, generally she only has time for visiting the Globe & Mail?s web site. In this setting, using simple association rules would yield the rules visits(Mary,globeandmail) ? visits(Mary,thestar) ? visits(Mary, thestar) ? visits(Mary,globeandmail) (we omit the intermediate pages that would also appear). Thus, even if Mary will not be able to visit the Toronto Star?s home page, the system preloads these pages in the cache, with the conse- quence of storage and bandwidth misuse. On the other hand, using sequential patterns will yield a sequence: visits(Mary,globeandmail), visits(Mary,investor), visits(Mary, weather). These rules will better account for the real activity of Mary on the Internet, and the system, in this case, will not preload pages that are not likely to be requested. The generated rules are used by the Page loader for caching pages in advance. Figure 1 shows the general system architecture. The rule generator gets  Proceedings of the 14th International Workshop on Database and Expert Systems Applications (DEXA?03)    Web Log Proxy Server  Rule Generator  Page Loader  URLs to preload  Session Information Requested URLs  Rules to use  Actions  Generated rules  Internet  GET<obj>  Cache replacement algorithm  Cache  Association Rules Sequential Patterns  or  Rules  Figure 1. Data mining proxy architecture  data from the proxy?s log and generates the association rules or sequential patterns which are stored in the knowledge base. This base is read by the page loader, which stores the rules in the cache of the proxy server, using the proxy?s replacement algorithm. The remainder of the paper is organized as follows. In Section 2 we comment previous work in the field. In Section 3 we describe the system?s architecture and give details of the implementation. In Section 4 we present the tests performed. We conclude in Section 5.

2 Background  Data mining [5] refers to the extraction of knowl- edge from large databases. We are interested in us- ing data mining algorithms over data stored in a proxy server?s log. In particular we will adapt the well-known Apriori[1] and AprioriAll [2] algorithms, developed by Agrawal et al, for discovering Association Rules and Se- quential Patterns, respectively. Our work fits into the so-called Web Usage Mining, defined as the process of using data mining techniques on data requested by the users [7]. Many works approached the subject of web caching, from different points of view. Schechter et al [9] introduced the use of path profiles for predicting http requests. However, they limit to study the algo- rithm?s behavior, with synthetic data, and neither present a complete system using the algorithm, nor consider real world limitations. Fan et al [4] present a tech- nique called proxy-initiatedprefetching, and a prediction algorithm based on the Prediction-by-Partial-Matching data compressor. Their implementation (unlike our pro- posal) requires modifying the browsers at the client side.

Nanopoulos et al [8] also introduce prefetching schemas.

They use association rules for predicting user access  sending hints to the clients from the server. A work by Lan et al [6] presents a strategy similar to the one adopted in our work, although their study is limited to measure hit ratios and traffic increase, without any further consider- ation of the requirements of an implementation in a real- world environment, and they do not account for order of the visits to web pages.

Web logs and proxy servers. We will assume an envi- ronment in which a user is accessing the Internet from an Intranet through a proxy server. A proxy server could be seen as a mediator between a Local Area Network (LAN), and the Internet such that only one IP address is needed for connecting any number of workstations to the Internet. An important feature of a proxy server is its cache?s capacity. In general, the higher this cache, the better the performance. The data mining algorithm which produces the rules used to predict future user be- havior, takes as input data reflecting previous user activ- ity. These data can be obtained mainly from two alterna- tive sources: (a) data stored in the local cache; (b) data stored in the proxy server log, containing all the objects accessed by the client machines in the Intranet. Although option (a) allows using the Application Program Inter- faces (APIs) in a Windows environment, it requires in- stalling an application in every client machine. On the other hand, option (b) only requires installing an appli- cation on the server, but it does not allow using the sys- tem APIs. We have chosen alternative (b) for our project, and developed an interface allowing interaction between the proxy server and the data mining algorithm. Web us- age mining, although promising for enhancing web us- age performance, has some drawbacks: there is no safe way of knowing when a session begins and ends; there is no access to the information residing in a client ma- chine?s cache; information may be incorrect if a page is relocated or a server renamed or eliminated.

3 System Architecture  Figure 2 depicts the detailed system architecture. At a high level of abstraction we can see the data mining server as a standard proxy server to which two modules are added: one for discovering the sequential patterns or association rules from the user?s navigation history, and the other for loading the proxy server?s cache with the pages likely to be requested by the user, obtained from the rule base. The transaction pre-processor fil- ters out records in the log of the proxy server (see be- low for details). It was written in Transact-SQL (a lan- guage for writing Stored Procedures in MS SQL Server).

The data mining algorithm takes as input data in the filtered log and generates rules of the form (URL1) ? (URL2), or rules including the user?s identification  Proceedings of the 14th International Workshop on Database and Expert Systems Applications (DEXA?03)    Data Mining Algorithm  Pre-processed Transaction Log  Proxy Server Log Cache  Page Loader  Internet  Intranet Transaction Pre-processor  Server Proxy  Sequential Patterns or Association Rules  Rules  Figure 2. Detailed System Architecture  if the configuration of the proxy server allows it (v.g.

(userA,URL1) ? (userA,URL2)). If the proxy server is configured to accept anonymous connections, only IP addresses are supported. The page loader loads web pages into the proxy server?s cache. It was written in Java and implements the connection between the data mining and proxy servers. The Page Loader can interact with any cache replacing scheme.

The system works as follows: users request web pages through the LAN(Intranet). The users requests are stored in the proxy server?s log, implemented as a rela- tional database. The transaction pre-processor filters out the records in the log which will not be useful for fur- ther processing; thus, a new transaction log is generated, holding the records which will be used as input for the data mining algorithm. The algorithm is run periodically and the sequential patterns are found; as we already ex- plained, URLs are the items of the transactions used by the algorithms. The Page loader keeps the cache contents updated, loading the pages from the URLs appearing on the discovered sequences. When a user requests a page satisfying a rule in the rule base (i.e., a URL belonging to a sequence or matching the left side of some rule, if as- sociation rules are used) the proxy server does not need to go to the Internet for the page, because it will be in the proxy?s cache.

Data Pre-processing. Not every tuple in the web log will be useful for being processed by the data mining algorithm. The reasons for filtering out these useless tuples are two-fold: speeding up the data mining algo- rithm, and preventing the generation of erroneous infor- mation. For instance, dynamically generated pages(like ASP or PHP pages) are not considered because they can- not be stored in the cache. Thus, taking their addresses into account will negatively affect performance. Further,  we will only consider user requests successfully com- pleted, carried out using the HTTP protocol. The sys- tem only considers tuples in the web log which represent a successfully retrieved object. Incomplete tuples (tu- ples with missing information) are discarded. Requests posed by automatic agents do not represent user?s be- havior because they are triggered automatically. Thus, considering them may generate wrong rules. The field ClientAgent in the log tuple allows detecting these kinds of requests (the client program identifier must contain the keyword Mozilla). Finally, the system will only consider objects retrieved via the HTTP protocol.

Data Mining Process. We implemented the AprioriAll algorithm [2] as an SQL Server Stored Procedure. De- tails of this algorithm can be found in the references. In order to measure the effect of taking into account the order in which pages are visited, we also implemented the Apriori algorithm [1]. The parameters support and confidence are defined in the usual way. A sequence is an ordered set of itemsets ? S?? S?? ? ? ? ? Sn ? ? Each item is an URL address. The support of a sequential pat- tern is the number of sequences containing the pattern.

The AprioriAll algorithm applies the well-known Apri- ori property to sequences: if a pattern of size k is not fre- quent(i.e., its support is less than the minimum support), then, no subset of this pattern can be frequent.

Communication between the proxy and mining servers. As we were only interested in supporting ob- jects in the WWW, and not every service available(v.g.

news, mail, video streaming), we only needed a proxy server supporting the HTTP protocol. Thus, we worked with Microsoft?s Proxy Server 2.0 [3]. MS Proxy Server 2.0 supports two kinds of web log formats: text files and database tables (SQL Server or MSAccess). We chose the latter, avoiding parsing the strings in the text file when processing the web log. A typical log record has twenty-two fields, from which the following ones were used in our implementation: (a) ClientIP: stores the IP address of the client making the request; (b) ClientUser- Name: holds the name of the user (in the Windows NT network) making the request; (c) DestHost: the name of the remote site accessed; (d) Protocol: indicates if the object was accessed via HTTP or FTP; (e) Uri: the com- plete URL of the accessed object.

We provide a way for establishing a bidirectional con- nection between the proxy and mining servers, such that the latter can access the data generated by the proxy server, and the pages likely to be visited by the user can be loaded in advance in the proxy?s cache in order to be available when they are requested.

Proceedings of the 14th International Workshop on Database and Expert Systems Applications (DEXA?03)    4 Experiments  The tests we performed on the system aimed at: (a) determining how support affects the results of the pro- cess, in order to define the best values for this system parameter; (b) discovering user trends and preferences, and their impact over the system; (c) evaluating how the addition of the data mining server influences the sys- tem?s performance. They were conducted on a computer with an AMD K6-2 processor, with a processor speed of 500Mhz, hosting both applications, client and server.

The installed applications on the server side were Mi- crosoft Windows NT 4.0 Server (Service Packs 4, 5 y 6), Microsoft SQL Server 2000, and Microsoft Proxy Server 2.0; on the client side we had an Internet Explorer 5.0 browser. Two other computers hold browser clients.

The data set was collected from the log of a proxy server in a school in Argentina. Data cover a month of activ- ity, and the database holds 60.538 records, from which we obtained 109 transactions. We also performed tests with synthetic data, with the following parameters: 100 transactions, up to 25 different URLs per transaction, and 25 different URLs in the data set, with an average of 15 URLs per transaction. The transactions occurred be- tween 9 A.M and 6 P.M. When tests started, the server?s cache was already loaded.

We performed six kinds of tests: Test #1 measures how support affects the number of generated rules. This test was run 3 times, with support values of 1%, 2% and 3%. Test #2 measures how support affects the time needed for generating the rules. The tests were run in a way analogous to test #1. In test #3 the goal pursued was finding out how many times in a day the same site is ac- cessed by a user. In test #4, we measure the number of accesses to the Internet as a function of the time of the day. The results allow defining which are the best times in the day for running the rule generator (i.e., the mo- ments in which activity is low). Test #5 categorizes the requests made to the proxy server. In order to measure the system?s applicability, we needed to test the num- ber of requests valid for analysis (i.e., the ones which will be the input to the data mining algorithm). Test #6 measures the system?s performance. The test compares page access time with the data mining server activated, against the access time obtained with the standard archi- tecture. Client latency with prediction was measured in the following way: first, information on past requests was stored in the proxy server?s log, and filtered. Then, the data mining algorithm was executed every twenty- five minutes, generating the rules. Every five minutes the page loader module scanned the rule base, in order to store the pages in the cache. After this stage, we reg- istered the web page access times.

Support Sequential patterns Association rules 1% 214(13) 173 2% 32(2) 98 3% 3(1) 6  Figure 3. Comparing sequential patterns and association rules  Discussion of Results. We ran tests #1 and #2 with sequential patterns and simple association rules (for the latter, using only synthetic data). Figure 3 shows the re- sults in terms of the number of rules that were generated.

We can see that as support increases, the number of gen- erated rules decreases. Between parentheses we show the results using real data for sequential patterns. We also measured the time needed for generating the sequential patterns and association rules, not only their number. In this case, simple association rules take less time to com- pute than sequential patterns except for rules with low support (1%). The latter occurs because the number of iterations of the AprioriAll algorithm was lower than in of association rules (the paths were of length less than 3).

Regarding the characteristics of the data sets, our tests showed that 62% of the sites are accessed only once in a day, and eight percent of the sites are accessed more than five times in a day. Thus, users in this site make requests through search engines in order to satisfy one- time needs. For synthetic data, 80% of the pages are ac- cessed more than five times. If we compare with real data, and perform a correlation with the number of gen- erated rules, it is clear that this is the reason for hav- ing much more rules when using synthetic data. Thus, the largest the number of repeated accesses to the same pages, the largest the number of rules that are generated (and the largest the benefit of sequential patterns over simple association rules). Finally, an important aspect affecting the system?s performance is the way users ac- cess the Internet, because low demand periods are suit- able for rule updating. As it was expected, the demand peaks between 11:00 a.m. and 12:00 a.m., and between 3:00 p.m. and 6:00 p.m.

Results for Test #5 show that only 53 percent of the requests are suitable for being included in the transac- tion set over which data mining techniques can be ap- plied. However, we must consider that within these 53 percent, there is another 40 percent of dynamically gen- erated pages, which cannot be stored in the cache.

Figure 5 shows that on the average, the access time is reduced in a factor of six, from 600 msec to 100 msec, proving the effectiveness of the page prediction process.

Measuring average access time instead of hit ratios has  Proceedings of the 14th International Workshop on Database and Expert Systems Applications (DEXA?03)    Figure 4. Distribution of request results  Figure 5. Performance  the advantage of considering the real impact over the users of the issues mentioned above. Moreover, given that 85 percent of the requested sites are local (i.e., sites within Argentina), the improvement can be higher when the percent of requested foreign sites increases.

5 Conclusion and Future Work  In this paper we have presented a system in which we added a data mining proxy to the traditional web architecture. This new server, using data mining tech- niques, predicts(based on previous accesses, stored in the proxy?s log) the pages likely to be requested by users in an Intranet located ?behind? a proxy server, and loads these pages into the proxy?s cache. We performed sev- eral tests on the prototype, with promising results which we discussed. We compared the use of sequential pat- terns against simple association rules, concluding that the latter reduce the number of generated rules, saving storage space and bandwidth usage. Our solution does  not prevent the use of other proposals for cache manage- ment which exploit semantic information and addresses in detail problems not covered in other works, concern- ing user activity and data pre-processing, which have im- pact over the final system performance.

