QoS-aware Packet Chunking Schemes for M2M Cloud Services

Abstract?With the emergence of machine-to-machine (M2M) communications, huge numbers of devices have been connected and massive amounts of traffic are exchanged. However, since M2M applications typically generate packets of very small size, the number of incoming packets is becoming larger than the maximum number of packets that a router can handle per second, and network throughput is decreased. That is, network resources cannot be used efficiently. Therefore, the M2M cloud network operator makes chunks of these small packets, but a waiting time for chunking is inevitable, and thus application requirements may not be satisfied. This paper proposes new packet chunking schemes to both meet the QoS application requirement and improve the router?s achievable throughput.

The proposed schemes need multiple buffers and then classify the arrival packets to one buffer based on their acceptable waiting time. After that, the chunk packet is made based on an attribute of each packet to meet the application requirements. Through simulation experiments, we showed that the proposed scheme attains excellent performance even in cases of overloaded and imbalanced traffic.



I. INTRODUCTION  Machine-to-machine (M2M) applications are currently emerging and some reports estimate that as many as 50 billion M2M devices will be connected by 2020 [1]. In M2M communication systems, a huge number of connected devices are embedded in observation systems, manufacturing systems, automobiles, and so on, and they exchange enormous amounts of data on wireless and wired networks [2],[3]. These heterogeneous data, known as ?big data?, are collected, stored and analyzed to explore new approaches that could solve a wide spectrum of issues.

The Internet needs to efficiently handle these emerging data traffic as well as the ever-increasing mobile traffic generated from smart phones, etc. The Internet is mainly composed of two types of networks: (1) access networks to which the end user first connects and (2) core networks connecting access networks. The core networks are very high-speed networks conveying a huge number of packets of relatively large size generated from file transfer and video streaming services. The achievable throughput of routers is usually measured in bit/s, and at the same time, the maximum number of packets a router can handle is limited, so that another measure, packets per second (PPS), should be considered as well. Because M2M applications are considered to generate packets of very small size, routers cannot reach their achievable throughput in terms of bit/s [4]; for example, when routers can transfer packets at  10 Gb/s and handle 5M PPS, where all packets are 40 bytes in length, 5M packets transferred at 40 bytes/s becomes only 1.6 Gb/s. However, it should be noted that routers can handle packets with diverse requirements, such as the acceptable delay time.

Therefore, the future Internet needs an effective way of accommodating huge amounts of diverse packets efficiently and meeting their QoS requirements. In this paper, we focus on some schemes of chunking packets in the router placed at the edge of core networks and propose some effective chunking schemes and evaluate their performance. The routers bundle some number of packets stored in the buffer together into large chunk packets, e.g., 9000 bytes maximum, when some acceptable waiting time expires.

This paper is organized as follows. Section 2 describes related work, and the network model is described in Section 3. We propose new packet chunking schemes in Section 4.

Section 5 describes the simulation model, and Section 6 discusses the simulation results. Finally, we conclude this paper in Section 7.



II. RELATED WORK  So far, numerous existing studies have focused on data aggregation technology. For example, several papers including [5], [6] have proposed layer 2 frame aggregation schemes to decrease the signaling overhead over WLANs and thereby improve not only the efficiency of data transmission but also the throughput. Reference [7] applied a data aggregation technology to improve the throughput performance over multi- hop wireless networks. Furthermore, [8] focused on sensor networks and proposed a data aggregation scheme to reduce energy consumption by reducing the quantity of transmitted data in a sensor network.

Active queue management technologies have been studied and some of them have already been implemented in real products. Recently, [9] conducted a wide survey and classified these technologies from a bird?s-eye view. Although many queue control technologies have been proposed to achieve different objectives (fairness, delay reduction, etc.), none of the existing studies focuses on the limitation of the forwarding capacity of routers.

To the best of our knowledge, there is little literature, which has focused on the issue of the router?s forwarding capacity and proposed a data aggregation method to solve this issue   DOI 10.1109/WAINA.2014.36    DOI 10.1109/WAINA.2014.36     Fig. 1. Target network model  while simultaneously improving the throughput performance and guaranteeing the delay constraints.



III. TARGET NETWORK MODEL  A. Types and role of nodes  Here, we focus on the core network. Currently, the core network consists of core routers and edge routers. Core routers provide high-speed communications in the core network and edge routers connect the access network and the core network.

In this paper, we discuss the function of ?packet chunking/de-chunking? added to the edge routers for achieving high throughput in the core network while meeting the delay requirements of packets. As shown in Fig. 1, the ingress edge routers chunk small packets, while the egress edge routers de-chunk the received chunk packets and send them to the cloud servers. Moreover, we prepare a monitoring server to detect abnormal events in the core network and to support the message exchange of network information (e.g., throughput, RTT, etc.) between the ingress and the egress edges. The roles of the edge routers are as follows.

? Ingress edge router: Identifies incoming packets and grasps their QoS applica- tion requirement. Then, the router makes chunk packets based on their QoS-related information.

? Egress edge router: Unpacks the received chunk packets and then forwards the unpacked packets to the cloud servers (e.g., data centers) or the destination device.

B. Maximum waiting time for chunking  We suppose that application types of incoming traffic on an ingress edge router can be identified in some way and their maximum acceptable waiting time (Q) can be obtained based on the identified information.

Then, let us assume the time for identifying the application types is X , the maximum waiting time for chunking is W , and the network delay is D. From this, as shown in Fig. 2, one-way delay is calculated as follows,  One? wayDelay = X +W +D.

Fig. 2. One-way delay between an ingress router and an egress router.

The application?s maximum acceptable waiting time Q should be larger than the ?one-way delay? to guarantee the application?s QoS requirement. Therefore, the maximum amount of acceptable waiting time for chunking Wmax is Wmax = Q? (X +D).



IV. PACKET CHUNKING SCHEME  The purpose of the packet chunking schemes is to make the best use of the core router?s forwarding capacity while meeting the application?s QoS requirement for packets. We propose a scheme for the ingress edge router, which buffers incoming packets and packs them into chunk packets of large size.

Large chunk packets reduce the number of packets for- warded to the core network and thus avoid exceeding the maximum number of packets per second (PPS) that the router can handle. Then, the core routers can provide their maximum achievable throughput in terms of b/s. Thus, their maximum size should be the maximum transmission unit (MTU) of the core routers to prevent both packet fragmentations and drops at the intermediate routers in the core network. As a result, the MTU size of a jumbo frame (9000 bytes) is the maximum size of a chunk packet.

Under these assumptions, the straightforward scheme is as follows: an ingress edge router continues to buffer incoming packets until the queue length reaches the MTU size. The router then makes a chunk packet and transmits it. However, this scheme has the following two problems.

? Problem 1 When the traffic load is lower, the buffer waiting time for chunking increases, and then the waiting time could exceed Wmax, defined previously.

? Problem 2 When the traffic is heavily loaded, the queue length can increase, and so the waiting time can also exceed Wmax.

We define the ?residual waiting time? as ?(maximum wait- ing time (Wmax)) - (the time that packets have already stayed at the buffer)?. We propose several chunking schemes based on the residual waiting time.

A. Scheme 1  Scheme 1 makes chunks of packets by considering both the MTU size and every packet?s residual waiting time by the following methods.

Fig. 3. Problem in scheme 1  ? Enqueue method ? We prepare one buffer that stores packets in order of  arrival (FIFO) and one timer for packet chunking.

? The chunking timer is updated to the minimum value  of the residual waiting time of all buffered packets.

? Dequeue method  ? If either of the following two conditions is satisfied, a chunk packet is made of the buffered packets.

1) Chunking timer is expired.

2) Queue length exceeds the MTU size.

This scheme can solve problem 1 (i.e., buffering time exceeds Wmax). However, this scheme cannot solve problem 2. For example, as shown in Fig. 3, if a packet with a relatively short residual waiting time (pkt A) is stored behind other packets with relatively long residual waiting time, scheme 1 cannot transmit a chunk including pkt A, even when pkt A?s residual waiting time is reduced to zero. To solve problem 2, some type of priority mechanism for managing packets is needed.

B. Scheme 1?  Scheme 1? modifies the enqueue method, which prioritizes incoming packets based on their residual waiting time to solve problem 2.

? Enqueue method ? Whenever packets are received at the ingress edge  router, this method arranges the incoming packets in ascending order of their residual waiting time.

? The chunking timer is updated to the residual waiting time of the head-of-line (HOL) packet.

? Dequeue method ? Same as the dequeue method in scheme 1  Scheme 1? can make a chunk in ascending order of each packet?s residual waiting time. However, sorting procedures are inevitably required for every packet enqueue procedure.

As a result, its computational load increases exponentially with the increase in queue length [10].

C. Scheme 2  In scheme 1?, incoming packets are stored in a single buffer and sorted based on their residual waiting time, and thereby transmit chunk packets under consideration of their priorities.

Scheme 2 further improves the enqueue management by eliminating the load of the packet sorting process. Instead, the ingress edge router prepares multiple buffers (waiting buffers),  Fig. 4. Queue management in scheme 2  and classifies the arrival packets into one of these buffers based on both the packet?s residual waiting time and the queue?s chunking timer.

In scheme 2, we prepare three waiting buffers and assign three timers with different time for chunking, as shown in Fig. 4. Specifically, we assume that in each buffer, the max- imum waiting time for chunking is predefined as 50 ?s, 100 ?s, and 200 ?s. Thus, this system cannot guarantee a delay requirement of less than 50 ?s. Although the timer value greatly depends on both the category and the QoS application requirements, determining the timer value is still a subject of future work.

Then, the enqueue process classifies the incoming packets by the following rules. Note that packet classification should be executed on the safe side (i.e., the packet?s residual waiting time is always greater than or equal to the queue?s chunking timer) to guarantee the QoS application requirement.

? If the residual waiting time is more than or equal to 200 ?s, the 200 ?s buffer is selected.

? If the residual waiting time is less than 200 ?s (but more than or equal to 100 ?s), the 100 ?s buffer is selected.

? If the residual waiting time is less than 100 ?s and more than or equal to 50 ?s, the 50 ?s buffer is selected.

The details of the queue management procedures for scheme 2 are described below.

? Enqueue method ? Multiple buffers with different chunking timers are  prepared in advance. Then, each arrival packet is classified and stored in one of the buffers based on both the packet?s residual waiting time and the buffer?s chunking timer.

The chunking timer is initiated whenever the queue length is increased to one from zero, i.e., when a packet newly arrives at the empty buffer.

? Dequeue method ? Same as in scheme 1 and scheme 1? When the chunking timer of a buffer expires, the HOL  packet and its subsequent packets queued in the same buffer are the first chosen for a new chunk packet. When the chunk packet is filled with all of those packets and reaches the MTU size, chunking is finished. Otherwise, as many as possible packets from another buffer with     Fig. 5. Queue management in scheme 3  a smaller chunking timer are added. The process of chunking packets is triggered by queue length as well.

When the queue length of some buffer exceeds the MTU size, a chunk packet is composed of the incoming packets of the buffer.

In this way, since scheme 2 prepares multiple buffers based on the FIFO order, the enqueue procedure is lightweight.

However, the following points should be considered.

1) Increase in waiting time.

Packet transmission from each buffer is in sequential order. Therefore, the waiting time for chunking is dras- tically increased due to the simultaneous expiration of multiple timers.

2) Complexity of selecting appropriate packets for chunk- ing.

Scheme 2 selects packets for chunking in a rather simple way, even though many ways are possible for selecting the best packets from the multiple buffers in order to optimize some performance measure. These other ways, however, are very difficult to implement.

3) Complexity of multiple timer management.

Since multiple timers have a potential to expire simulta- neously, this scheme needs to not only manage multiple timers but also to prioritize these buffers.

D. Scheme 3  In scheme 2, incoming packets are stored in different multiple buffers based on their delay constraints and thereby reduce the computational load of the enqueue procedure and improve the router?s achievable throughput.

Scheme 3 further improves the way of chunking packets by checking HOL packets in all multiple buffers. In this method, an additional buffer, called the chunking buffer, is placed after the multiple buffers, as shown in Fig. 5.

Various policies are possible to select a packet among the HOL ones in the multiple buffers for the chunk- ing buffer. Here, we examine the following three policies: delay-constraint-first policy (DCF), throughput-performance- first policy (TPF), and chunking-efficiency-first policy (CEF).

In addition, we introduce some attribute value attached to each of the packets to implement the policies. Defining an attribute value that achieves high performance while meeting the delay constraints is a major concern. The attribute value, ?, is defined in each of the following policies:  Fig. 6. Simulation topology  ? Delay-constraint-first policy (DCF) ? ? = 1(residual waiting time)  ? Throughput-performance-first policy (TPF) ? ? = 1(residual waiting time) ? (packet size)(MTU size)  ? Chunking-efficiency-first policy (CEF) ? ? = 1(residual waiting time) ? 1(packet size)  Detailed queue management procedures based on the at- tribute value are described below.

? Enqueue method ? Same as in scheme 2  ? Dequeue method ? The method calculates an attribute value of the  HOL packets of each buffer, and the packet with the maximum value is moved to the buffer for the dequeue procedure.

? When the chunking buffer receives new packets, the chunking timer is updated to the minimum residual waiting time of all packets in the chunking buffer.

? The chunk packet is transmitted only when either of the following two conditions is satisfied.

1) Chunking timer is expired.

2) Queue length exceeds the MTU size.



V. SIMULATION ENVIRONMENT  In this section, we describe the simulation models (see Fig.

6) and the traffic model employed to evaluate the effectiveness of our proposed schemes. The NS-3 (Network Simulator-3) is used for the performance evaluation [11].

A. Simulation model  In our simulations, the network topology consists of three nodes: node-0 as the sender/ingress edge router, node-1 as the core router, and node-2 as the egress edge router/receiver. Note that we assume the packet handling capability of each router is 5M PPS by referring to the value of the ?HP MSR3000 Router Series? [12]. We set the link bandwidth of the core network to 10 Gbps.

The ingress edge router?s hardware component is shown in Fig. 7. This ingress edge router has 126 input lines and 1 output line. Then, each of the three different types of traffic arrives independently from 42 input lines.

Fig. 7. Ingress edge router?s hardware component  TABLE I TRAFFIC MODEL  App. type Pkt. size Max. waiting time (Wmax) Sensing 40 bytes 50 ?s  Real time 200 bytes 100 ?s File transfer 1500 bytes 200 ?s  B. Traffic model Since the end-to-end one-way delay in the core network is  known to be on microsecond order [13], we set the different maximum waiting time on microsecond order for each appli- cation. The traffic parameters are listed in Table I.

The incoming traffic model on the ingress edge router is illustrated in Fig. 7. The ratio of incoming traffic to the link bandwidth of the output line is defined as ?, and the ratio of each of the three traffic lines is defined as ?1, ?2 and ?3. That is, these parameters always satisfy the following condition,  ? = ?1 + ?2 + ?3.

We examine the effectiveness of the proposed schemes, when the values of ? and ?i(i = 1, 2, 3) are varied. In our simulations, as the packet length is increased, the waiting time is also increased, so that the performance of DCF will be similar to that of CEF. We investigate the performances of DCF and TPF only.

We employ the following three traffic patterns, which start the communication at 1 s and stop at 2 s.

(a) Case for basic traffic (homogeneous traffic model) (? = 0.9, ?1 = ?2 = ?3 = 0.3)  Packets from each class arrive at the same rate. The total arrival rate is set to 8.97 Gbps.

(b) Case for overloaded traffic (? = 0.8 ? 1.2 ? 0.8, ?1 = ?2 = ?3)  At the beginning of communication, ? is 0.8, and is then increased up to 1.2 at 1.3 s. After that, at 1.5 s, ? is decreased to 0.8. Note that ?1, ?2, and ?3 are always the same.

(c) Case for imbalanced traffic a) Small (sensing) packets are dominant (? =  0.9, ?1 = 0.88, ?2 = 0.01, ?3 = 0.01) During the simulation experiment, ? is fixed to  0.9, but small (sensing) packets are dominant.

b) Large (file transfer) packets are dominant (? =  0.9, ?1 = 0.01, ?2 = 0.01, ?3 = 0.88)  TABLE II OUTPUT TRAFFIC FROM THE INGRESS EDGE ROUTER  Scheme PPS Output data rate [Gbps] Basic scheme without chunking 4511420 3.52  Scheme 1 129000 8.97 Scheme 2 124581 8.97  Scheme 3 (DCF) 132191 8.97 Scheme 3 (TPF) 126099 8.97  TABLE III RATIO OF PACKETS VIOLATING THE REQUIREMENT (BASIC TRAFFIC  PATTERN, UNIFORM DISTRIBUTION)  Scheme Sensing [%] Real time [%] File transfer [%] Scheme 1 0 0 0 Scheme 2 0 0 0  Scheme 3 (DCF) 0 0 0 Scheme 3 (TPF) 0 0 0  During the simulation experiment, ? is fixed to 0.9, but large (file transfer) packets are dominant.

C. Performance measures  We evaluate the performance of the proposed schemes with respect to the following performance measures.

1) The number of output packets: Under the basic pattern (uniform distribution), we eval- uate the number of output packets at the ingress edge router.

2) Output data rate: Under the basic pattern (uniform distribution), we eval- uate the output data rate at the ingress edge router.

3) Ratio of packets violating the delay constraint at the ingress edge router: Under all traffic patterns, we evaluate the ratio.



VI. SIMULATION RESULTS In this section, first we show the effect of packet chunking  in the basic traffic case (a). Then, the performance of the chunking schemes is examined in the overloaded traffic case (b) and the imbalanced traffic case (c).

A. Fundamental performance in the basic traffic case  Table II shows the number of packets that the ingress edge router actually forwarded (PPS) and the output data rate at/from the ingress edge router. For reference, the table includes the performance of the usual queue management based on FIFO service without chunking (i.e., ?basic scheme without chunking?). From this table, we see that packet chunking schemes can successfully reduce the number of PPS to approximately 1/35 of that of the scheme without chunking.

As a result, without chunking, the throughput of more than 3.52 Gbps cannot be achieved, because the number of arriving packets, 8.97 Gbps, is more than the maximum number of packets that the ingress edge router can handle.

TABLE IV RATIO OF PACKETS VIOLATING THE REQUIREMENT (DURING A PERIOD OF  OVERLOADED TRAFFIC)  Scheme Sensing [%] Real time [%] File transfer [%] Scheme 1 99.88 99.76 99.55 Scheme 2 20.42 38.33 99.84  Scheme 3 (DCF) 0 0 99.79 Scheme 3 (TPF) 99.9 0 0  Fig. 8. W=50 ?s, the ratio of packets violating the requirement  Fig. 9. W=100 ?s, the ratio of packets violating the requirement  On the other hand, packet chunking schemes can improve the output data rate up to 8.97 Gbps, which is the same as the incoming traffic rate. As a result, packet chunking allows the ingress edge router to utilize the physical bandwidth as much as possible. It should be noted that no packets exceed the maximum waiting time in our proposed schemes (see Table III).

B. Performance in the overloaded traffic case  We examine the case where packets arrive at the ingress edge router at a rate larger than the transmission rate of the output link for some duration. Namely, we study how the chunking scheme works when the buffer accommodates a large number of packets. The performance measure of concern here is the ratio of packets whose waiting time exceeds their acceptable time, which is shown in Table IV and Figs. 8- 10.

From these table and figures, we can see that, in scheme 1, almost all packets of any class transmitted during the overloaded period (i.e., ? > 1) are forced to wait for a duration  Fig. 10. W=200 ?s, the ratio of packets violating the requirement  TABLE V RATIO OF PACKETS VIOLATING THE REQUIREMENT (SENSING TRAFFIC IS  DOMINANT)  Scheme Sensing [%] Real time [%] File transfer [%] Scheme 1 0 0 0 Scheme 2 0 15.62 43.59  Scheme 3 (DCF) 0 0 0 Scheme 3 (TPF) 0 0 0  larger than their acceptable waiting time. That is, scheme 1 cannot handle incoming packets and meet their constraints on the waiting time when the buffer is filled with a large number of packets.

In contrast, although scheme 2 totally decreases the ratio of packets violating the requirement, 20.42% of sensing traf- fic, 38.33% of real-time traffic, and approximately 100% of file transfer traffic still cannot meet the QoS requirements.

Specifically, we can say that scheme 2 provides better delay performance, especially for sensing and real-time traffic than does scheme 1. This is because the value of the timer for strict delay-constraint applications is set to a small value, and thereby allows priority control to work for sensing and real- time applications.

Next, we focus on the performance of scheme 3. Scheme 3 with DCF satisfies the QoS requirement for sensing and real- time applications, whereas it cannot guarantee the requirement for file transfer. The attribute value, ?, of packets with strict delay constraints is likely to be large, and thus these packets are preferentially chunked. On the other hand, scheme 3 with TPF guarantees the QoS requirement from real-time communication and file transfer applications, but not that from sensing ones completely. In such a scheme, ? of large packets tends to be large, and thereby packets are transmitted preferentially.

From these results, we can say that scheme 2 and scheme 3 can reduce the number of packets violating the delay con- straints. In particular, scheme 3 with DCF provides excellent performance.

C. Performance in the imbalanced traffic case     Fig. 11. The ratio of packets violating the requirement (scheme 2)  TABLE VI RATIO OF PACKETS VIOLATING THE REQUIREMENT (FILE TRANSFER  TRAFFIC IS DOMINANT)  Scheme Sensing [%] Real time [%] File transfer [%] Scheme 1 0 0 0 Scheme 2 11.03 0.03 0  Scheme 3 (DCF) 2.21 0.54 3.74 Scheme 3 (TPF) 32.22 0 0  Fig. 12. W=50 ?s, the ratio of packets violating the requirement  1) Sensing traffic is dominant (? = 0.9, ?1 = 0.88, ?2 = 0.01, ?3 = 0.01): Table V and Fig. 11 show the ratio of packets violating the requirement, when the sensing traffic is dominant. In Table V, the ratio of proposed schemes is almost zero in all schemes (except for scheme 2). In scheme 2, on average, 15.62% of the real-time traffic and 43.59% of the file transfer traffic do not meet the delay time constraint. The rea- son is explained as follows. When the chunking timer of some buffer expires, the HOL packet and its subsequent packets of the same buffer are the first chosen for the new chunk packet.

The chunking timer of the buffer for sensing applications is the smallest and the related packets are dominant, and so the timer of sensing applications frequently expires and their packets are very likely to be in chunk packets. Therefore, packets of other buffers are forced to wait for a rather long time.

On the other hand, Table V indicates that scheme 3 with DCF and scheme 3 with TPF can achieve excellent perfor- mance, since all the packets meet the constraint.

Fig. 13. W=100 ?s, the ratio of packets violating the requirement  Fig. 14. W=200 ?s, the ratio of packets violating the requirement  2) File transfer traffic is dominant (? = 0.9, ?1 = 0.01, ?2 = 0.01, ?3 = 0.88): Table VI and Figs. 12- 14 show the ratio of packets violating the requirement when the file transfer traffic is dominant. In scheme 2, we can see that 11.03% of the sensing packets cannot meet the requirement.

Since the incoming packets of file transfer are dominant, the queue length of the buffer for file transfer applications frequently exceeds the MTU size, so that all of their packets can be successfully transmitted within the delay constraint.

In contrast, scheme 3 with DCF can effectively reduce the ratio, but 2.21% of the sensing traffic, 0.54% of the real- time traffic, and 3.74% of the file transfer traffic cannot yet meet the delay constraint. In scheme 3 with TPF, 32.22% of the sensing traffic cannot meet the QoS requirement. This is because the larger packets are preferentially moved into the chunk buffer due to the definition of ?, and thus the smaller packets (especially sensing packets) more frequently experience time expiration.

From these results, we conclude that scheme 3 with DCF achieves excellent performance when file transfer traffic is dominant.



VII. CONCLUSION AND FUTURE WORK  This paper focuses on the emerging M2M communications in which numerous devices typically generate packets of quite small size. In such cases, the number of arrival packets is beyond the router?s forwarding capability and thereby causes inefficient utilization of network resources. In this paper, we proposed new packet chunking schemes (DCF, TPF, CEF)     that can meet the QoS application requirement while improv- ing the maximum achievable throughput. Through simulation experiments, we found that scheme 3 with DCF guarantees the QoS requirement of all kinds of applications in the case of (i) overloaded traffic and (ii) imbalanced traffic. In future work, we plan to evaluate the performance of the proposed schemes under more practical environments (e.g., coexistence of multiple flows).

