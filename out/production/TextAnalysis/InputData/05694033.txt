Approximation of Frequentness Probability of Itemsets in Uncertain Data

Abstract?Mining frequent itemsets from transactional datasets is a well known problem with good algorithmic so- lutions. Most of these algorithms assume that the input data is free from errors. Real data, however, is often affected by noise.

Such noise can be represented by uncertain datasets in which each item has an existence probability. Recently, Bernecker et al. (2009) proposed the frequentness probability; i.e., the probability that a given itemset is frequent, to select itemsets in an uncertain database. A dynamic programming approach to evaluate this measure was given as well. We argue, however, that for the setting of Bernecker et al. (2009), that assumes independence between the items, already well-known statistical tools exist. We show how the frequentness probability can be approximated extremely accurately using a form of the central limit theorem. We experimentally evaluated our approximation and compared it to the dynamic programming approach. The evaluation shows that our approximation method is extremely accurate even for very small databases while at the same time it has much lower memory overhead and computation time.



I. INTRODUCTION  In frequent itemset mining, the considered transaction dataset is typically represented as a binary matrix M where each line represents a transaction and every column corre- sponds to an item. An element Mij represents the presence or the absence of the item j in transaction i by the value 1 or 0 respectively as in Table I (left). This is the basic traditional model, where we are certain that an item is present or absent in a transaction. For this type of data many algorithms have been proposed for mining frequent itemsets; i.e., sets of columns of M that have all ones in at least a given number of transactions (see e.g. [6] for an overview on frequent itemset mining).

In several applications, however, an item is not present or  absent in a transaction, but rather the probability of it being in the transaction is given. This is the case for data collected from experimental measurements susceptible to noise. For example, in satellite picture data the presence of an object or feature can be expressed more faithfully by a probability score when it is obtained by subjective human interpretation or an image segmentation tool.

Such data is called uncertain data and Table I (right)  presents a popular type of uncertain database. This example dataset consists of 4 transactions and 3 items. For every transaction, a score between 0 and 1 is given to reflect the probability that the item is present in the transaction.

TID a b c t1 1 1 0 t2 0 0 1 t3 1 1 0 t4 1 1 0  TID a b c t1 0.9 0.8 0.2 t2 0.1 0.1 0.9 t3 0.6 0.8 0.3 t4 0.9 0.9 0.2  Table I CERTAIN DATASET (LEFT) AND UNCERTAIN DATASET (RIGHT)  For example, the existence probability of 0.9 associated to item a in the first transaction represents that there is a 90% probability that a is present in transaction t1 and 10% probability that it is absent. Table I (left) actually represents an instantiation of the uncertain dataset depicted in Table I (right). Such instantiations are called possible worlds. There are 2|T |?|I| possible worlds, where |T | is the total number of transactions and |I| the total number of items in the dataset. Under the assumption that presence and absence of the different items is statistical independent, the probability of a possible world is obtained by simply multiplying the individual item probabilities. We will call this model the independent uncertain database model. For example, the probability of the world in Table I (left) is 0.9 ? 0.8 ? 0.8 ? 0.9 ? . . . ? 0.8 = 0.0914. A far less probable world is obtained if we take the complement of Table I (left); i.e., we switch the ones to zeroes and the zeroes to ones. The probability of this world is 1.92?10?10.

The probabilities of all possible worlds sum up to 1.

Mining frequent patterns from this kind of datasets  is more difficult than mining from traditional transaction datasets. After all, computing the support of an itemset now has to take the existence probabilities of the items into consideration. To provide information about the frequency of an itemset, two approaches exist. One is based on the expected support as introduced by Chui et al. [5]. For every itemset, its expected support is computed and those for which it exceeds a minimum threshold are reported as frequent. The second one is called frequentness probability and it was introduced in [2]. For a particular itemset, it takes into consideration the probability distribution of the support and it gives its probability of being frequent at a given minimum support threshold. The existing methods have the drawback of being computationally costly and being exposed to rounding errors when dealing with low probability values.

DOI 10.1109/ICDM.2010.42     For example, the dynamic programming approach proposed in [2] to compute the frequentness probability requires O(minsup|D|) computation steps, where minsup is the minimal support threshold and |D| the size of the dataset.

This paper is similar in spirit as [3]. In [3] we showed  that for the independent uncertain database model a simple sampling technique gives very good results when computing the expected frequency of an itemset. In this paper we show that also for estimating the itemset frequency distribution there are good statistical tools available. This time, however, we do not rely on sampling and Hoeffding?s inequality to bound the error on the mean, but instead we use a Normal approximation. Concretely, we show that the frequentness probability of itemsets can be approximated by a normal distribution function using a weak version of the Central Limit Theorem. Even for small datasets this approximation turns out to be surprisingly accurate. In contrast to the exact computation, the approximation scales linearly in the number of transactions and is independent of any minimal support or minimal frequentness thresholds.

We would like to stress that the main core of this paper  is about how the frequentness probability computation for a single itemset can be optimized using standard statistical techniques. As such, the conclusions of this paper extend to every data mining method that is based on frequentness properties of itemsets. The main message of this paper there- fore is: In the probabilistic database model that assumes independence between items in a transaction database, stan- dard statistical tools such as the Central Limit Theorem can and should be used to characterize the frequency distribution of the itemsets.



II. FREQUENTNESS PROBABILITY In this section we formally define the notion of the un-  certain database, frequency distribution, expected frequency, and frequentness probability. The definitions given here are equivalent to those in [5], [2].

Definition 1 (Uncertain database): Let T be a set of  transaction identifiers, and I a set of items.

An uncertain database D over T ? I is a total function  from T ? I to the interval [0, 1].

A possible world W of D is a subset of T ? I . The  probability of the possible world W given the uncertain database D, denoted PD(W ), is defined as  PD(W ) :=  ? ? ?  (t,i)?W  D(t, i)  ? ? ? ? ?  (t,i)?(T?I)\W  (1?D(t, i)) ? ?  Let WD denote the set of all possible worlds of D.

The support of an itemset X ? I in a possible world W  is defined as the number of transactions in W that contain X ; i.e.: sup(X,W ) := |{t ? T | ?x ? X : (t, x) ?W}|.

It is easy to see that PD describes a probability distribution over the possible worlds of the uncertain dataset. Computing  the frequency of an itemset in uncertain datasets is based on this distribution. Among all possible worlds, we don?t now which one is the true world; rather, PD assigns the probability of being the true world. Given this, the expected support of an itemset X ? I is:  expSup(X) := ?  W?WD  PD(W )sup(X,W ) ,  which is equivalent to the definition in Chui et al. [5]:? t?T  ? x?X  PD(t, x) .

That is, because all item occurrences are independent, we can rewrite the expression with the possible worlds into one that only involves for every transaction in T , the probability that it contains the itemset X .

The distribution of the support of an itemset X ? I can  now be computed as follows:  PD(sup(X) = i) = ?  W?WD sup(X,W )=i  PD(W ) ,  which is equivalent to [2]:  ? S?T  |S|=i  (? t?S  PD(X ? t) )?? ?  t?T\S  (1? PD(X ? t)) ? ? ,  where PD(X ? t) denotes ?  x?X PD(t, x). The probability of X having a support of i can thus be computed by summing over all subsets S ? T of size i, the probability that the transactions in S contain X , and all others do not.

A. The Frequentness Probability Problem For given minimal support minsup, the frequentness  probability of itemset X [2] is now defined as:  PD(sup(X) ? minsup) , which equals  ? S?T  |S|?i  (? t?S  PD(X ? t) )? ? ?  t?T\S  (1? PD(X ? t)) ? ? ,  In [2], the following Probabilistic Frequent Itemset Min- ing (PFIM) problem was proposed: given an uncertain database D, a minimal support threshold minsup and a minimal frequentness probability ? , find all itemsets X with a frequentness probability of at least ? ; i.e., PD(sup(X) ? minsup) ? ? .

For solving the PFIM problem, in [2] a dynamic pro-  gramming approach is proposed based on the following observation: let T = {t1, . . . , tn}, and let Dj denote the restriction of the uncertain database D to its first j     transactions t1, . . . , tj . Following the notations of [2], we denote PDj (sup(X) ? i) by P?i,j . Then,  P?i,j = PD(X ? tj) ? P?i?1,j?1 + (1? PD(X ? tj)) ? P?i,j?1 .

By incremental computation of the values P?i,j , the frequentness probability PD(sup(X) ? minsup) = P?minsup,|T | can be computed in O(minsup|T |). Different optimizations are possible, such as 0 ? 1 optimization and early pruning for itemsets not meeting the frequentness probability. For details we refer to the paper [2]. The main take-away message we want to stress here is that the algorithm of [2] is exact, computes the frequentness probability for one itemset X , and has complexity O(minsup|T |). For computing all itemsets that satisfy the frequentness probability threshold, the monotonicity of frequentness is used: the frequentness probability of an itemset is at most as high as the frequentness probability of its subsets. This follows trivially from the fact that in every possible world where an itemset is frequent, also all its subsets are frequent.



III. APRIORIAPPROX: A METHOD FOR APPROXIMATING THE FREQUENTNESS PROBABILITY  In this section we propose an alternative method for approximating the frequentness probability, by approximat- ing the frequency distribution of an itemset with a normal distribution using a weak version of the Central Limit Theorem. Every transaction can be considered as a single coin toss; either the transaction contains the itemset X , or it doesn?t. The final support of the itemset X is the sum of the outcomes of these coin tosses. We can now use a weak form of the central limit theorem, known as the Lyapunov?s Central Limit Theorem: Let X1, X2, . . . be an infinite sequence of stochastic variables, and let, for all numbers N , s2N denote  ?n k=1 ?  k (?2k denotes the variance  of variable Xk). The theorem states that if for some ? > 0 the following two conditions hold: 1) E[|Xk|2+?] is finite for all k, and 2) limN?? 1  s 2+? N  ?N i=1E[|Xi ? ?i|2+?] = 0  then the Central Limit Theorem still holds; i.e.,  Zn =  ?n i=1(Xi ? ?i) sn  converges in distribution to a standard normal random vari- able as n goes to infinity.

In our case, Xk is a stochastic variable denoting if trans-  action tk contains itemset X . Xk follows a Bernoulli distri- bution; PD(Xk = 0) = 1?  ? x?X D(tk, x), and PD(Xk =  1) = ?  x?X D(tk, x). Therefore, ? k = PD(Xk = 1)(1 ?  PD(Xk = 1)), and s2N = ?n  k=1 PD(Xk = 1)(1?PD(Xk = 1)). The support can be expressed by the probabilistic variable S =  ?|T | i=1Xi, and the expected support equals  E[S] = expSup(X ).

Algorithm 1 Apriori Framework Require: D,?,minfreqprob Ensure: F (D,?, freqprob) 1: C1 := {i|i ? I} 2: k := 1 3: while Ck is not empty do 4: for all transactions t in D do 5: for Candidates in Ck do 6: if X ? t then 7: update(X) 8: end if 9: end for 10: end for 11: Fk := {X ? Ck|X.freqprob ? minfreqprob} 12: Ck+1 := generateCandidates(Fk) 13: k ++ 14: end while  We will show that the two conditions hold for ? = 1.

Clearly, for all k, E[|Xk|3] ? 1, as Xk is either 0 or 1,  and it also holds that:  lim N??   s2+?N  N? i=1  E[|Xi ? ?i|2+?] = 0  Therefore, ?N  i=1(Xi??i)  sN converges to a normal distribution  for increasing N . For N = |T |, we get:?N i=1(Xi ? ?i) sN  = S ? expSup(X)?  s2N  Hence, for sufficiently large databases T , S?expSup(X)? s2  N  converges in probability to the standard normal distribution, and thus:  P (S ? minsupp)  = ?  ( minsupp ? 0.5? expSup(X)?  s2N  )  A. Computing All Itemsets Computing the frequentness probability can be done in a  systematic manner thanks to its anti-monotonic property. For enumerating the candidates we use an Apriori-like breadth- first strategy depicted in the Algorithm 1. This approach consists in iteratively generating, counting and pruning the candidates. The differences from the traditional Apriori are in the counting step (line 7) and in pruning criterium (line 11). For the counting step, instead of incrementing the support, we have to update two variables: one for the expected support and the second one for  s2n =  n? k=1  PD(Xk = 1)(1? PD(Xk = 1))     # trans. Avg.

length  # items density  KOSARAK 990980 8.1 119 0.02% CONNECT4 67557 43.0 129 33,3% T40I10D100K 100000 39.6 942 4.2% T25I15D320K 320000 26.28 994 0.02%  Figure 1. Datasets Summary  The approximation of frequentness probability will be com- puted after the dataset is scanned using the cumulative distribution function ?:  freqprob(X) = ?  ( minsupp ? 0.5? expSup(X)  sn  ) The pruning step relies on the frequentness probability counted as above. The pruning criterium is based on the condition that the frequentness probability is lower than the minimal frequentness threshold.



IV. EXPERIMENTS The experiments were conducted on a GNU/Linux  machine with a 2.1GHz CPU Dual Core and 3.5 Gb of main memory. We use the datasets from [1]. Con- nect4 and Kosarak are available on the FIMI repository1.

T40I10D100K and T25I15D320K were generated using the IBM synthetic data generator2. A brief description of these datasets is available in 1. The original datasets were transformed by Aggarwal et al. [1] into uncertain datasets by assigning to every item in every transaction existential probabilities according to the normal distribution N(?, ?2), where ? and ? were randomly and independently generated with values ranging between [0.87, 0.99] and [1/21, 1/12] respectively.

For producing the results, we implemented the AprioriAp-  prox method described in Section III. In order to evaluate our method in terms of execution time and accuracy of the results, we have implemented the PFIM method based on the Probabilistic Frequent Itemset Mining approach proposed in [2] and discussed in Section II-A. The same Apriori frame- work has been used with the difference that, for every item- set, a vector containing the non-zero existence probabilities of the itemset was stored. This is the 0-1 optimization since, for large sparse datasets, it is important not to carry vectors of the size of the dataset consisting mainly of null values.

Based on this vector, the exact frequentness probability is computed according to the dynamic programming scheme.

We consider this implementation as base-line method for validating the accuracy of our approximation.

In Figure 2 we compare the execution times of the two  approaches. The X-axis represents the varying minsup, in  1http://fimi.cs.helsinki.fi/ 2http://www.almaden.ibm.com/cs/projects/iis/hdb/Projects/data mining/  datasets/syndata.html       1e+06  1e+07  0.01 0.1 1 10 100  nu m  be r o  f p at  te rn  s  minsup (%)  Connect4 T25I15D320K T40I10D100K  Kosarak  Figure 3. Number of patterns for varying support  percentage of the dataset size, and the Y-axis the execution time. The minimum frequentness probability is set to 0.8.

We also used, for the Kosarak and datasets the early pruning optimization which speeds up the base-line method as it can be seen in Figure 2(b) and 2(c), but is still few orders of magnitude slower than AprioriApprox. As expected, in terms of execution time, AprioriApprox always outperforms PFIM.

The reason is that our method does not overload the memory by constructing the vectors as PFIM. Moreover, PFIM requires O(minsup|D|) computation steps, where |D| is the size of the dataset and minsup is the minimum support. So, for higher minsup, more computations are needed, which is not the case of AprioriApprox. For the cases where PFIM overloaded the memory, we used a technique to avoid this, by splitting the set of candidates into batches such that by one traversal of the dataset only the candidates in the active batch were counted and then pruned, without overloading the memory. However, executions exceeding 2 hours were stopped.

Figure 3 shows the number of patterns for different values  of minsup. Both X and Y axes are on log scale. We report the number of itemsets obtained with the approximation and, for the cases where the FPIM gave results, we compare the error introduced and the number of False Positives and False negatives which are depicted in Table II.

The error in approximation is depicted in Figures 5 and 6.

The average error and the maximum error for every collec- tion of patterns are represented. These errors were calculated only for the runs where PFIM succeeded since this gives the exact value of the frequentness probability. Almost exactly the same collections of patterns were generated, the average error in support being less than 0.0005 and never exceeding 0.0035. Very few false positives and false negatives were generated as can be seen in Table II. This was the case only for itemsets for which the frequentness probability was very close to the minimum probability threshold.

The next experiment investigates the impact of the number  of tuples in the dataset on the accuracy of the approximation.

We took the Kosarak dataset and we randomly sampled           20 25 30 35 40 45 50 55 60 65  tim e  (s )  minsup (%)  AprioriApprox  (a) Connect4          0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5  tim e  (s )  minsup (%)  PFIM PFIM+P  AprioriApprox  (b) T40I10D100K         0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5  tim e  (s )  minsup (%)  PFIM PFIM+P  AprioriApprox  (c) Kosarak  Figure 2. Execution Time  minsup T40I10D100K 0.01% FP -  FN - TP -  0.02% FP 47 FN 4 TP 276438  0.04% FP 1 FN 1 TP 45155  0.06% FP 0 FN 0 TP 16072  1% FP 0 FN 0 TP 4238  3% FP 0 FN 0 TP 410  5% FP 0 FN 0 TP 216  minsup Kosarak 0.03% FP -  FN - TP -  0.05% FP 10 FN 4 TP 116973  0.1% FP 0 FN 0 TP 16672  0.2% FP 0 FN 0 TP 3535  0.3% FP 0 FN 0 TP 1626  0.5% FP - FN - TP -  0.8% FP - FN - TP -  Table II CONFUSION MATRICES  it in order to obtain datasets with different number of transactions. From each of this newly generated datasets we extract the itemsets that satisfy the frequentness probability threshold set to 0.8 as in the previous experiments, given a minimum support of 0.2%. The results are presented in Table

IV. As it can be seen, the approximation remains accurate, even for a very low number of transactions.



V. RELATED WORK Recent work on frequent itemset mining in uncertain data  focuses on methods for efficiently computing the expected support. This methods inherit the breadth-first and depth- first approaches from traditional frequent itemset mining and adapt the data structures to the probabilistic model.

U-Apriori [5] represents a baseline algorithm for mining frequent itemsets from uncertain datasets. Because of its generate and test strategy, level by level, the method does not scale well. UCP-Apriori [4] is based on the decremental   0.005  0.01  0.015  0.02  0.025  10 100 1000 10000 100000 1e+06 E  rr or  transactions in dataset  MaxError MeanError  Figure 4. Max and mean approximation error for varying dataset size   2e-05  4e-05  6e-05  8e-05  0.0001  0.00012  0.00014  0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5  m ea  n es  tim at  io n  er ro  r  minsup (%)  T25I15D320K T40I10D100K  Kosarak  Figure 5. Mean approximation error for varying support   0.0005  0.001  0.0015  0.002  0.0025  0.003  0.0035  0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5  m ax  e st  im at  io n  er ro  r  minsup (%)  T25I15D320K T40I10D100K  Kosarak  Figure 6. Max approximation error for varying support     #sample tuples False Positives False Negatives True Positives Kosarak 990980 0 0 3535 Kosarak0.1 98869 0 0 3459 Kosarak0.01 9896 0 0 3223 Kosarak0.001 948 4 0 420 Kosarak0.0001 95 5 0 82 Kosarak0.00001 11 0 0 4  Table III THE IMPACT OF THE DATASET SIZE ON THE APPROXIMATION FOR KOSARAK  pruning technique which consists in maintaining an upper bound of the support and decrementing it while scanning the dataset. The itemset is pruned as soon as its most optimistic value falls below the threshold. This approach represents the state of the art for mining frequent patterns from uncertain data with a generate-and-prune strategy. UF- growth [8] extends the FP-Growth algorithm [7]. Aggarwal et al. [1] extend several existing classical frequent itemset mining algorithms for deterministic data sets, and compare their relative performance in terms of efficiency and memory usage. The UH-mine algorithm, proposed in their paper, provides the best trade-offs. The algorithm is based on the pattern growth paradigm. The main difference with UF- growth is the data structure used which is an hyperlinked array. The limitations of these existing methods are the ones inherited from the original methods. The size of the data for the level-wise generate-and-test techniques affects their scalability and the pattern-growth techniques require a lot of memory for accommodating the dataset in the data struc- tures, such as the FP-tree, especially when the transactions do not share many items. In the case of uncertain data, not only the items have to be shared for a better compression but also the existence probabilities, which is often not the case.

Another approach for the independent uncertain database model is proposed in [3]. It is shown here that a simple sampling technique gives very good results when computing the expected frequency of an itemset.



VI. CONCLUSION The focus of this work is approximating the frequentness  probability of itemsets in uncertain datasets based on a special form of the Central Limit Theorem. Using this technique, the frequentness probability can be computed with minimal computational efforts and the results are highly accurate, actually invalidating the use of complicated algorithms for computing the exact solution. Similarly as in the paper [3], again we show that in the independent model one can easily apply simple statistical tools to reach excellent results. Basically, one could argue that for those independent cases statisticians solved the problems already a long time ago. A logical next step is now to see if these results can be extended to more complicated probability models. Clearly the statistical analysis of such cases will become much more challenging.

ACKNOWLEDGEMENTS We are grateful to Charu C. Aggarwal, Yan Li, Jianyong  Wang and Jing Wang for making available the datasets.

