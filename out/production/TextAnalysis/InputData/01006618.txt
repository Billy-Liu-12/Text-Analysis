Associations and Rules in Data Mining: a Linkage Analysis

Abstract - We discuss a problem of synthesis and analysis of granular rules emerging in data mining. Two descriptors of the rules (that is relevance and consistency) being viewed individually and en block are introduced. The relevance of the rules is quantified in terms of the data being covered by the antecedents and conclusions standing there. While this index describes each rule individually, the consistency of the rule deals with the quality of the rule viewed vis-a-vis other rules. It expresses how much the rule ?interacts? with others in the sense that its conclusion is distorted by the conclusion parts coming from other rules. We show how the rules are formed by means of fuzzy clustering and their quality can be evaluated in terms of the above indexes. Global characteristics of a set of rules are also discussed and? related to the number of information granules being constructed in the data space.



I. INTRODUCTION  Rule-based systems are granular and highly modular models [ 1][4][5] [6][7][8] [9][11][ 121 [ 13][ 141. The granularity of rules becomes fully reflected in the form of the antecedents and conclusions and is quantified in the language of sets, fuzzy sets, rough sets, probability, to name the main formal vehicles. The evident transparency of these systems is their genuine asset. Several design issues remain still open and tend to become even more profound as we move toward developing larger systems (a) The origin of the rules and their eventual quantification (e.g., confidence or relevance of relationships where the confidence measure is expressed in relation to experimental data)  proceeding with further detailed construction and refinement of the rule-based architecture.

More descriptively, one can consider the outcome of the link analysis to arise as a web of connections between the granules. As fundamentally we distinguish between input and output variable(s), there are two essential facets of the analysis that deal with the following aspects (a) expressing strength between the granules in the input and output space (relevance of the rule), and (b) completing a consistency (crosstalk) analysis in which we quantify an interaction between the given link and the links that are invoked owing to the interaction (overlap) between the information granules. In the language of the calculus of rule-based systems, the notion of relevance is linked with the notion of strength of the rule whereas the second aspect of consistency is concerned with an interaction between the information granules.

The material is organized into 6 sections. First, in Section 2 we discuss clustering as a basic means of information granulation and pose the problem of constructing rules.

Section 3 contains a discussion of two descriptors of the rules, namely rule relevance and rule consistency.

Experimental studies are covered in Section 4 while conclusions are included in Section 5.

11. CLUSTERING AS A VEHICLE OF INFORMATION GRANULATION  Quite often, the Fuzzy C-Means (FCM) algorithm arises as a basic vehicle of data granulation. As the method is well- known in the literature, cf. [2], we will not discuss it here but  (b) The dimensionality problem becomes of concern when the number of variables in the rules increases (then the number of rules tend to explode at the exponential rate)  Fuzzy clustering gives rise to fuzzy sets and fuzzy relations being thus constructed in a fully algorithmic fashion. They are examples of information granules regarded as a basic building canvass of numerous fuzzy models. For given collections of such information granules, studying and quantifying relationships between them leads to the emergence of the rule-based models. The preliminary development step of this nature is referred to as a link analysis - a phase in which we reveal and quantify dependencies between information granules before  rather clarify the notation and cast the method in the setting of the problem at hand.

The input and output data spaces are denoted by X and Y, respectively. The numeric data set under discussion assumes a form of input - output pairs (x(k), y(k)), k=1,2,. . .,N where xk E R? and Y k  E R?. The results of clustering carried out for x and Y separately are discrete fuzzy sets (more precisely, fuzzy relations) defined over these data sets. In general, we end up with ?c[l]? fuzzy relations in x, say A,, A2, ... A 4 1  and ?c[2]? fuzzy relations in Y, namely B1, B2,  0-7803-7280-8/02/$10.00 02002 IEEE 867    ..., B cp~.  Technically, they are organized in a form of partition matrices so we have  TAl 1 TB1 1  Our ultimate goal is a formation of a thorough and constructive description of a web of directed links between Ai and Bj. In a nutshell, such task gives rise to a collection of rules, see Figure 1  - ifAithenB,  The techniques of fuzzy clustering, no matter what type of objective function we employ, share several main features whose understanding is of importance in the framework of this problem. Clusters are direction-free (relational) constructs: in the clustering process there are no provisions as to a possible direction between the variables.

Figure 1. Rule-based system as a web of directed links between information granules (shadowed regions) formed in the space of  antecedents and conclusions through fuzzy clustering  This implies that the resulting granules do not accommodate any request that relates to the mapping itself and are not formed in a way they reflect a directionality component. In contrast, any mapping (function) is a directional construct. If we establish a link between Ai and B,, its nature needs to be inspected with respect to the directionality of the rule obtained in this manner. More formally, if there is no directionality component (or it is ignored), we are essentially assessing properties of a Cartesian product of the two information granules, namely A, x Bj .Fuzzy clusters  (information granules) are sound building blocks of rule- based systems as they provide an answer to the two challenges outlined at the beginning of the previous section.

First, they reflect the structure of the data so we anticipate that any cluster comes with enough experimental evidence behind it (otherwise it would not have been formed in the first place). Second, as we are concerned with fuzzy relations rather than fuzzy sets, the dimensionality problem does not arise. For instance if c[l]=c[2]=c then we have (potentially) ?c? meaningful rules in spite of the potentially high dimensionality of the input and output space. Otherwise, as it happens quite often when dealing with the individual fuzzy  sets, we end up with a combinatorial explosion of the collection of the rules produced at this level.

In what follows, by proceeding with information granules formed by clustering we synthesize a collection of rules and analyze their properties. The crux of this development is as follows. With each Ai i=1,2, ..., c[l] we associate a single granule in the conclusion space, that is we form a mapping  Ai 3 Bj (1)  (the way of determining the associations will be discussed later on). Following this procedure, we build a collection of rules, R, where its cardinality is equal to c[l]. In a concise way, we can summarize the rules as a list of pairs of indexes of the respective information granules  where j k  is in the range of integers from 1 to c[2].

The characteristics of this form of assignment between the information granules are quantified in the next Section.

111. CHARACTERISTICS OF THE RULES  There are two main descriptors of the rules. The first one reflects the experimental evidence behind the rule (association). The second captures the relationships between the rules discussed as a collection of entities.

Relevance  Let us view a certain rule as a Cartesian product) meaning that we do not consider the ?direction? of the rule but look at it as an entity linking two information granules defined in the two different spaces (input-output). The rule (1) comes with the (experimental) relevance equal to  N  rel(Ai X Bj) = Ai (X k)tB, (yk ) k = l  (2) where ?t? is a t-norm viewed here as a model of an and logical connective. If rel(.) attains higher values, we say that the rule comes with more experimental relevance (in other words, it is more justifiable from the experimental point of view). For fixed ?i?, we order all associations (rules) Ai 3B1, Ai + Bz.. .. Ai 3 B c [ 2 ~  according to the associated relevance level. The highest value of the relevance identifies a rule of the form Ai + B ,(Q.

In light of the above realization (properties of t-norms), we come up with a straightforward monotonicity property, namely: If Ai A?i and B, B?, then rel(Ai XB,) 5 rel(A?i XB?,). Intuitively, note that if we increase the size of the information granules, this change contributes to the increasing level of relevance of the particular rule as in this  0-7803-7280-8/02/$10.00 (92002 IEEE 868    way we tend to ?cover? more data and thus elevate an experimental evidence of this rule.

AI andA2 different  The relevance defined above exhibits a close analogy to the notion of rule support encountered in data mining that is articulated in the language of probability theory and reads in the form  conflicting rules exhibit rules are different  similar conclusion  Support(A9B) =Prob(A X B)  Consistency  The associations we have constructed so far were totally isolated. We have not expressed and quantified possible interactions between the rules. Nevertheless, this interaction does exist owing to the nature of the overlapping fuzzy sets (relations). Considering two rules, we note that their antecedents and conclusions overlap to a certain degree. The overlap at the condition end could be very different than the one encountered at the conclusion end. The differences between the overlap levels shed light on an important issue of consistency of the rules. In turn, this leads to a detection of conflicting rules (where the term of conflict itself is rather continuous than binary, so we talk about a degree of conflict, or equivalently, a degree of consistency).

The problem of conflicting rules is well known in the literature and has a long path in the research in rule - based systems, especially those in the realm of fuzzy controllers, cf [IO]. Bearing in mind the origin of the control knowledge, this effect was attributed mainly to some deficiencies of knowledge acquisition when working with a human expert.

As in this study, we are concerned with an automated vehicle of information granulation, the effect of conflict is a result of incompatibility of information granules in the spaces of conditions and conclusions. Before we move on with a detailed quantification of this effect, let us concentrate on Table 1. It summarizes four different scenarios of interaction occurring between two rules AI+ B1 and A2 3 B2.

I BI and B2 similar I BI and B2 different A, and A? similar I rules are redundant I rules are  We develop an index that captures the effect of consistency of two rules. In its construction, we follow the observations coming from Table 1. The consistency measure is developed in two steps by (a) expressing a measure of consistency of the rules for a single data point (x(k), y(k)) and (b) constructing a global performance measure over all data (X, v)  Note that the term of similarity is invoked at the level of the information granules rather than original data, so in essence we are looking at Al(x(k)) and Az(x(k)) along with Bl(y(k)) and B2(y(k)). To express a degree of similarity, we use the formula that is deeply rooted in the language of logic and set theory: we say that two sets are equal if the first is included in the another and vice versa. The continuous version of this statement being realized in the framework of membership grades of the respective fuzzy sets reads as  A, (x(k)) = A2 (x(k)) = (A, ( W )  + A2 (x(k))t t(A 2 (x(kN + A I (x(k))  (3) (we observe that the implication in logic corresponds to the inclusion operation in set theory). The implication is implemented in the form of the residuation operation implied by a certain t-norm, namely  a + b = sup{c E [0,1] I atc I b}, a, b E [0,1] The consistency is low only if AI and A2 are similar and BI and B2 are different. This naturally leads to the following expression as a measure of consistency  Al(X(k)) = A2(x(k)) + (B,(Y(k)) = B,(Y(k)) (4)  To gain a better insight into the character of this expression, we plot it for the residuation generated by the product operation, refer to Figure 2.

Figure 2. Plot of the residuation c + d induced by the product operation, (?c? and ?d? denote a level of similarity in the space of  antecedents and conclusions).

0-7803-7280-8/02/$10.00 02002 IEEE 869    The above expression concerns a single data point. Naturally, a sum over the entire data is a legitimate global measure of consistency of the two rules (rule-1 and rule-2),  reasons; we discuss this selection in more detail later on).

The relevance of the rules are shown in Figure 3.

I  2 A,(x(k)) = A,(x(k)) +.

cons(l,2) = N k = l  Likewise we may like to express a consistency of a given rule versus all other rules in the ruleset R. This leads to the expression  Cons(i, R) = cons(i, j) j=l j#i  (6 ) where ?i? and ?3?? are indexes of the rules in R.



IV. EXPERIMENTS  In this section, we are concerned with a synthesis and analysis of fizzy rules for one selected data sets available on the WWW (ftp://flp.ics.uci.edu/pub/machine-leaming- databases/), namely fuel consumption. In both cases, we use the FCM method set up in the same way across all experiments. The fizzification factor (m) standing in the standard objective function is equal to 2,  where c = c[l] or c[2]. The dissimilarity between the pattems is expressed in terms of a weighted Euclidean distance. A partition matrix is initialized randomly. Once the information granules (clusters) have been generated, the analysis of the rules is completed in terms of their relevance and consistency. Furthermore we carry out a global analysis as to the number of rules and quantify them as an overall collection and derive characteristics related to their suitability in the description of data. As to the implementation details of t-norms, we use a product operation. The implication is also induced by the same t-norm.

The dataset under consideration comes from the StatLib library that is maintained at Carnegie Mellon University. The data concerns a city-cycle fuel consumption that is expressed in miles per gallon and has to be predicted on a basis of 7 attributes (number of cylinders, displacement, horsepower, weight, acceleration, model year, and origin). All but the fuel consumption are treated as inputs. City-cycle fuel consumption is an output variable (conclusion).

We choose the number of clusters (information granules) to be equal to c[l] = c[2] = 7 (this is done for illustrative  output  Fig\ -. = input  Jre ?3. Relevance of the rules  In each row of the matrix, Figure 3, we have at least one fairly dominant association. By selecting the dominant links, we end up with the following seven rules: 1 3 5 2 3 7 3 + 3 4 + 5  5 3 6  6 + 4  7 3 4  wherethe above is a schematic summary of the links (associations) between the information granules in the input and output space. Noticeably all rules are quite similar in terms of their relevance. One may anticipate this to be a result of using clusters as generic building blocks of the rules that comes with a similar experimental evidence behind the fizzy relations).The consistency levels of these rules are as follows  Again, these levels of consistency are fairly similar across all the rules with a single exception (rule 3 3  3 is the most consistent with the corresponding value of the consistency level of 3.45).

Now we investigate a situation where there is a significantly different number of the information granules in the input and output spaces. More specifically, we analyze (a) c[1] =3 and c[2]=10 and (b) c[l] =10 and c[2]=3. The results are visualized in Figure 4 and Tables 2 and 3.

Figure 4. Relevance of the rules for c[l]=3 and c[2]=10  0-7803-7280-8/02/$10.00 02002 IEEE 870  ftp://flp.ics.uci.edu/pub/machine-leaming   I 1  rule I +  6 2 +  7  relevance consistency 28.63 0.76 23.85 1.02  3 + 8  I 33.98 Table 2. Rules (associations) formed by the information granules for  c[l]=3 and c[2]=10  1.20  rule 1+ 1 2 3 3 3 + 1  relevance consistency 39.32 3.26 18.98 2.23 33.16 3.43  7 + 2  I 26.36 I 3.54 8 + 3  18.20 2.25  4 + 3 5 + 1 6 + 3  ~~ ~  9 + 1  I 22.02 I 2.30 10 + 3  22.23 2.3 1  18.62 2.27 23.54 2.35 19.16 2.25  Table 3. Rules (associations) formed by the information granules for c[l]=3 and c(2]=10  The most striking is a fact of increasing consistency of the rules with the increasing number of the information granules in the condition space. The consistency goes down significantly when the values of c[2] get lower. The change in c[21 from 10 to 3 decreases the level of consistency by a factor of 2.



V. CONCLUSIONS  The study focused on the synthesis of information granules (fuzzy sets and fuzzy relations) and generation of rules (associations) composed of them. Such associations are characterized by two indexes. The first one is about the relevance of the rule and expresses how much experimental evidence is behind the association. The second one is about a directional aspect of the construct and describes much a given rule interacts with all others and produces a crosstalk (relational) effect. We also looked into an interesting numeric quantification of the rules with respect to the size of the vocabulary of the information granules both at the antecedent and conclusion part of the rules. It was revealed that there are some cutoff values of these granules beyond which the quality of the rules drops off significantly.

The issue of the numeric quality of the rules has not been discussed at all (namely, a problem of expressing the quality of the rules vis-&vis the original experimental numeric data).

In other words, we have not studied the features of the transformation of the inference results (coming from the rules) into numeric representations. This phase is definitely related with the clustering mechanism itself, the number of clusters in the space of conclusion and a way of aggregation of the conclusions.

The above analysis imposes a minimal level of structural dependencies between the rules. The rules developed here are the direct product of data summarization as we use fizzy clustering to reveal and capture the structure of the data. It should be stressed that the cluster-based rules help avoid combinatorial explosion in cases of high dimensional spaces.

The language of relations (rather than fuzzy sets) becomes instrumental in this setting.

