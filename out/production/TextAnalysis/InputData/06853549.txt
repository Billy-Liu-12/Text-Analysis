ONLINE DICTIONARY LEARNING FROM BIG DATA USING ACCELERATED STOCHASTIC APPROXIMATION ALGORITHMS

ABSTRACT  Applications involving large-scale dictionary learning tasks moti-  vate well online optimization algorithms for generally non-convex  and non-smooth problems. In this big data context, the present  paper develops an online learning framework by jointly leveraging  the stochastic approximation paradigm with first-order acceleration  schemes. The generally non-convex objective evaluated online at  the resultant iterates enjoys quadratic rate of convergence. The gen-  erality of the novel approach is demonstrated in two online learning  applications: (i) Online linear regression using the total least-squares  approach; and, (ii) a semi-supervised dictionary learning approach  to network-wide link load tracking and imputation of real data with  missing entries. In both cases, numerical tests highlight the potential  of the proposed online framework for big data network analytics.

1. INTRODUCTION  As pervasive sensors collect and record massive amounts of high-  dimensional data from communication and social networks, and  storage along with processing capacities of computers grow, new  analytical tools are necessary to comb through these ?big data?  sets to distill out subsets of interest. Further, as many data sources  continuously generate data in real time, analytics must often be  performed in real time, without a chance to revisit past entries.

Given an M ? 1 real data vector yk ? RM , indexed by k ? N (the set of non-negative integers), dictionary learning (DL) has  emerged as a prominent tool for modeling yk as a product of  an unknown over-complete dictionary D := [d1, . . . ,dQ] ? R  M?Q, Q ? M , times an unknown sparse coefficient vector sk ? RQ [1?3]. If D were known, basis pursuit would yield sk [4]; but with D unknown, one can use multiple vectors in  YK := [y0,y1, . . . ,yK?1] to solve  min (SK ,D)?R  Q?K?D   ?YK ?DSK?2F + ?s?SK?1 (1)  where SK := [s0, . . . , sK?1], D := {D ? RM?Q | ?dq? ? 1, q ? {1, . . . , Q}}, ?s ? R>0, and ???F, ???1 denote the Frobenius and ?1-norms, respectively. The unit-norm constraint on the columns of D is incorporated to cope with the inherent scale ambiguity of the  bilinear fit, and also ensure that the solution of (1) remains bounded.

If M is excessively large, solvers of (1) remain tractable ei- ther after splitting YK in multiple row sub-blocks, and running DL  per sub-block on parallel processors; or, by simply down-sampling  Work in this paper was supported by the NSF Eager Grant 1343860.

Moreover, it has been co-financed by the European Union (European Social Fund - ESF) and Greek national funds through the Operational Program ?Ed- ucation and Lifelong Learning? of the National Strategic Reference Frame- work (NSRF) - Research Funding Program: Thalis - UoA - Secure Wireless Nonlinear Communications at the Physical Layer.

(a.k.a. sketching) the rows of YK . Thanks to the sparsity of SK , DL  remains tractable also when the collected data vectors have missing  entries for reasons such as privacy, storage limitations or due to the  high cost of data gathering [5]. But even when M is of manage- able size, the streaming nature of data presents major challenges to  solving (1) as K grows.

Although the cost in (1) is non-convex, owing to the bilinear  form DSK , it is convex in each of its arguments, SK or D, if the  other one is held fixed. Block coordinate descent methods (BCDMs)  have recently gained popularity in the big data optimization area  [2, 3, 6?17], largely because they exploit structure of the objective  functions, they have low memory requirements, and also incur low  cost per iteration. BCDMs optimize (exactly or inexactly) the ob-  jective function over one (block) variable at a time, while holding  all other fixed. However, the computationally expensive and time-  consuming quest for (almost) exact minimizers per BCDM iteration  can be prohibitive in the big data context, where the streaming nature  and sheer dimensionality of data dictate stringent policies on com-  putational power and CPU time. Notwithstanding the importance of  accuracy in estimation, the sequential nature and non-stationarity of  data places time-adaptivity attributes of algorithms at a central po-  sition; online solutions should ?monitor? their batch counterparts,  without ?over-fitting? them, since that would degrade their agility to  track time-varying and non-stationary big-data processes.

Projected proximal stochastic (sub)gradient methods are attrac-  tive low-complexity online alternatives to BCDMs mainly for op-  timizing convex objective functions [18?22]. Unfortunately, such  first-order solutions tend to exhibit slow convergence even for con-  vex problems, due to their perplexed means of choosing step-size  parameters, which become increasingly complicated as the objec-  tive functions become more complex. On the other hand, acceler-  ated variations for convex problems enjoy provable quadratic conver-  gence rate of the objective function values, meaning they are optimal  among first-order methods [23?28]. However, convergence claims  for non-convex objective functions are largely uncharted territories.

The present work introduces a scalable, online, and low-  complexity iterative learning approach for a class of non-convex  optimization tasks, going beyond and subsuming (1). Using only  first-order information of the underlying objective function, the  convergence rate is proved to be quadratic. The novel algorithm  employs the method of [26] as a starting point, developed originally  for convex loss functions and equipped with flexibility in parameter  selection. Subsequently, it adapts stochastic approximation (SA)  [29] tools to ensure generalization to out-of-sample data. The ana-  lytical results are tested on two instances of broad practical interest:  Online, robust, linear regression based on the total least-squares  (TLS) criterion; and the online semi-supervised DL approach put  forth in [5] for network-wide link load tracking and imputation.

2. PROBLEM STATEMENT  Although the proposed approach and theoretical claims apply to  any finite number of block variables, exposition here will focus for  brevity and simplicity on only two blocks, namely (x(1), x(2)) ? H1 ? H2, where H1,H2 are any finite-dimensional linear vector spaces, with inner products ??, ??  H1 and ??, ??  H2 , respectively.

With reference to (1), consider the following sequence of  loss functions (Fk : H1 ? H2 ? R ? {+?})k?N, defined as Fk(x  (1), x(2)) := fk(x (1), x(2)) + g1(x  (1)) + g2(x (2)), where  fk : H1 ? H2 ? R is non-convex, while gi ? ?0(Hi), where ?0(Hi) stands for all (proper lower semicontinuous) convex func- tions [30] defined on Hi with values in R?{+?}, i ? {1, 2}. Since {g1, g2} can be non-differentiable, Fk is generally non-smooth. The features of the DL problem in (1) appear in fk. Indeed, ?k func- tion fk is assumed convex with respect to (w.r.t.) each one of its arguments, whenever the other one is fixed. Finally, fk is assumed Lipschitz continuously differentiable in x(1) and x(2), with (local) Lipschitz constants Lfk|x(1)(x  (2)) and Lfk|x(2)(x (1)), respectively.

To facilitate convergence analysis on the premises of stochas-  tic approximation (SA) [29], the following stationarity assumption  on (Fk)k?N is also adopted. (Generalizations are possible but go beyond the scope of this paper.)  Assumption 1. There exists a function F : H1?H2 ? R?{+?} such that (s.t.) the following stationarity holds true:  E?|(x(1),x(2)){Fk(x(1), x(2), ?)} = F (x(1), x(2)), ?k ? N where ? denotes a trial within the ensemble of trials ? of a prob- ability space [31] that characterizes randomness in Fk originating from the observed data. Moreover, E?|(x(1),x(2)) denotes expectation  w.r.t. ?, conditioned on (x(1), x(2)) held fixed, since the arguments of Fk are in general vector- or matrix-valued random variables (r.vs.) x(i) : ?? Hi. Indeed, this is true on the premises of Alg. 1, where (x  (1) k , x  (2) k ) at time k generally depend on the observed data for time  instants {0, 1, . . . , k ? 1}.

For concreteness, examples of principal practical interest follow.

Example 1 (Total least-squares). A special case of (1) with M = 1 is the linear regression model yk = u  ? ?ks? + vk, k ? N, where  (u?k, s?) ? RQ ? RQ, with the regressor vector u?k assumed random; ?? denotes transposition; s? is assumed sparse, and vk stands for noise. Observed data are (yk,uk)k?N ? R ? RQ, where uk is a noisy version of the true u?k. Here, H1 = H2 = R  Q,  with ??, ?? RQ  being the dot-vector product. Following the total least-  squares (TLS) criterion and the resultant errors-in-variables (EIV)  modeling approach [32], the following sequence of functions is con-  sidered; ?k ? N: Fk(s, e) :=    [ yk ? (uk + e)?s  ]2 ? ?? ?  fk(s,e)  +?s?s?1? ?? ? g1(s)  + ?e ?e?2? ?? ?  g2(e)  , (2)  where e ? RQ models EIV, and ?s, ?e ? R>0. Notice that a Lip- schitz constant of ?sfk is Lfk|s(e) = ?(uk + e)(uk + e)?? ? ?(uk + e)(uk + e)??F = ?uk + e?2, where ?A? denotes the spectral norm of a matrix A, and fk|s the restriction of fk on the s-domain. On the other hand, a Lipschitz constant of ?efk is Lfk|e(s) = ?ss?? ? ?ss??F = ?s?2.

It is not hard to verify that  E?|(s,e){Fk(s, e, ?)} =   s ? ee ? s+ s? E?|(s,e){uk}e?s  +  s ? E?|(s,e){uku?k }s+ E?|(s,e){yk}e?s  + E?|(s,e){yku?k }s+ 1 E?|(s,e){y2k}.

Assuming that all expected values remain invariant w.r.t. k, then As- sumption 1 holds if F (s, e) := E?|(s,e){Fk(s, e, ?)}.

Example 2 (Semi-supervised dictionary learning [5]). Consider an  undirected weighted graph G(V, E), where V denotes the vertex set, with cardinality P ? N?, and E is the edge set. Connectiv- ity and edge strengths of G are described by the adjacency matrix  W ? RP?P , where [W ]ij > 0 if nodes ni and nj are connected, while [W ]ij = 0 otherwise. At every k ? N, a variable ?kp ? R, which describes a network-wide dynamical process of interest, cor-  responds to a node np. All node variables are collected in a sin- gle vector ?k := [?k1, . . . , ?kP ]  ? ? RP . A sparse representa- tion of the process over G models ?k as a linear combination of  ?few? atoms in a dictionary D ? RP?Q, Q ? P : ?k = Dsk, where sk ? RQ is sparse. Further, only a portion of ?k is observed per time slot k. Let now Mk ? RM?P , M < P , denote a bi- nary measurement matrix, with each row of Mk corresponding to  the canonical basis vector for RP , selecting the measured compo-  nents of yk ? RM . In other words, the observed data per slot k are yk = Mk?k+vk, where vk denotes noise. To enable imputation of missing entries of ?k in yk, the topology of G will be utilized. The  spatial correlation of the process is captured by the Laplacian matrix  L := diag(W1P ) ? W , where 1P ? RP is the all-ones vec- tor. In this setting, H1 = R  Q, with ??, ?? H1  denoting the dot-vector  product, and H2 = R P?Q, with ?D1,D2?H2 = trace(D  ? 1 D2),  ?(D1,D2) ? H22 .

Given a ?forgetting factor? ? ? (0, 1] to gradually diminish the  effect of past data (and thus account for non-stationarity), define  Fk(s,D) :=  fk(s,D)? ?? ?  2?k  k? ?=0  ?k???y? ?M?Ds?2 + ?L s ? D ? LDs  + ?s?s?1? ?? ? g1(s)  + ?D(D)? ?? ? g2(D)  (3)  where ?k := ?k  ?=0 ? k??, and ?D stands for the indicator function  of D, i.e., ?D(D) = 0 if D ? D, and ?D(D) = +? if D /? D.

The term including the known L quantifies a priori information on  the topology of G, and promotes ?smooth? solutions over strongly  connected nodes of G [5]. This term is also instrumental in accom-  modating missing entries in (?k)k?N.

It can be easily verified that  E?|(s,D){Fk(s,D, ?)}  =  2?k  k? ?=0  ?k?? [ s ? D ? E?|(s,D){M?? M?}Ds  ?2E?|(s,D){y?? M?}Ds+ E?|(s,D){?y??2} ]  + ?L  s ? D ? LDs+ ?s?s?1 + ?D(D). (4)  As before, if all expected values are invariant w.r.t. k, then Assump- tion 1 holds true with F (s,D) := E?|(s,D){Fk(s,D, ?)}.

Notice that a Lipschitz constant of ?sfk is Lfk|s(D) = ?D?AkD? ? ?D?AkD?F ? ?D?2F?Ak?F, where Ak := ??1k  ?k ?=0 ?  k??M?? M? + ?LL; whereas a Lipschitz constant of ?Dfk is Lfk|D (s) = ?s?2?Ak?F.

3. ALGORITHM  This section introduces Alg. 1, which is built on a few basic notions  outlined next.

1 Choose any initial points (x (i) 0 , y  (i) 1 , ?  (i) 1 ), i ? {1, 2};  2 for k = 1 to +? do 3 for i = 1 to 2 do  4 if minx(i) fk(x (i) | x(?i)k+i?2) + gi(x(i)) is feasible then  5 x (i) k ? arg minx(i) fk(x(i) | x(?i)k+i?2) + gi(x(i));  6 else  /* Acceleration for i-th block of coordinates: */ /* Initialization: */  7 (x0, y1, ?0) := (x (i) k?1, y  (i) k , ?  (i) k );  /* Perform Ri cycles of (5): */ 8 for r = 1 to Ri do 9 ?r := (xr?1, yr, ?r);  10 (xr, yr+1, ?r+1)? Accel ( fk(? | x(?i)k+i?2), gi,?r  ) ;  11 end  /* Update i-th block of coordinates: */  12 (x (i) k , y  (i) k+1, ?  (i) k+1) := (xRi , yRi+1, ?Ri);  13 end  14 end  15 end  Algorithm 1: A Gauss-Seidel-inspired acceleration scheme.

Given (?, ?) ? ?0(H)? R>0, consider the proximal mapping Prox??(x) := arg min  ??H ?(?) +   2? ?x? ??2, ?x ? H.

Clearly, when ? equals the indicator function ?D of a closed convex set D, then ?? ? R>0, Prox?? boils down to the (metric) projection PD onto D [cf. (1)].

Given functions (?, ?) ? ?0(H)2, where ?? is Lipschitz con- tinuous with constant L? ? R>0, ? := ? + ?, and the parameters ?r := (xr?1, yr, ?r) ? H? H? R>0, define the core block of acceleration in Alg. 1 (cf. [26]):  Accel(?, ?,?r)  =  ?????????????????????????? ?????????????????????????  ?r ? [??, 1] ?r ?  [ ??,min  { ??,   L?  }] , ?r+1 ? ?r  Lr ? [L?, L?] ?r ?  [ 1??1? ?rLr?r  Lr , 1 +  ? 1? ?rLr?r Lr  ] zr :=Prox?r?  ( yr ? ?r??(yr)  ) xr ? argmin  x  { ?(x) | x ? {xr?1 + ?r(zr ? xr?1), xr?1}  }  tr+1 :=  ? 4t2r + ?  1?  r+1 + ?1?r+1  , t1 := ?1  yr+1 := tr  tr+1 yr +  ( 1? ?1  tr+1  ) xr ? tr ? ?1  tr+1 xr?1  ? tr tr+1  ?r?r ?r  (yr ? zr).

(5)  Parameters (??, ??, ??) ? R3>0 are user-defined, while L? ? R>0 stands for an upper bound on all Lipschitz constants used in this pa-  per. The acceleration operator (5) is applied to Alg. 1 in a successive  or Gauss-Seidel fashion. First, it is applied to x(1) for R1 ? N? cycles, and then to x(2) for R2 ? N? cycles.

A few comments regarding Alg. 1 are now in order. Vector x(?i)  denotes all variables in (x(1), x(2)) other than x(i), i ? {1, 2}.

Line 4 in Alg. 1 allows for exact computations of the minimizer,  whenever closed-form solutions are available (cf. Sec. 4.1); or,  whenever time and CPU resources are available for finding a highly  accurate estimate of the minimizer. The computational complex-  ity of Alg. 1 on Examples 1 and 2, including computations of  Lipschitz constants and function evaluations, are in the order of  O[(R1 +R2)Q] and O[(R1 +R2)(Q+ P )P ] per k, respectively.

Theorem 1. Assuming that all the employed r.vs. have finite first-  and second-order moments, Alg. 1 enjoys the following properties.

1) (E{Fk(x(1)k , x(2)k )})k?N is non-increasing; ?k ? N, E{Fk(x(1)k+1, x(2)k+1, ?)} ? E{Fk(x(1)k , x(2)k , ?)}  where expectation is taken both w.r.t. ? and (x (1) ? , x  (2) ? )  k+1 ?=0.

2) If F is bounded from below, and (x (1) k , x  (2) k )k?N is uniformly  bounded over ? (the ensemble of trials), then the convergence rate of the iterates is quadratic, i.e., ?(k0, F?, C) ? N? R ? R>0 s.t.??E{Fk(x(1)k , x(2)k , ?)} ? F??? ? C(1 + k)2 , ?k ? k0.

3) If g1, g2 are coercive, meaning limk?? |gi(?k)| = +? when- ever limk????k?Hi = +?, i ? {1, 2}, if F is bounded from below, and if (x  (1) k , x  (2) k )k?N is uniformly bounded over ?, then ?k  there exist subgradients [30] (h (1) k , h  (2) k ) ? ?x(1)Fk(z(1)k , x(2)k?1, ?)?  ?x(2)Fk(x (1) k , z  (2) k , ?) s.t.

lim k??  E {?h(1)k ?2 + ?h(2)k ?2} = 0.

Coercivity is a quite general property; any ?? -norm, with ? ? [1,+?), as well as the indicator function ?D associated with any set D are coercive.

4) If g1, g2 are coercive, if F is bounded from below, and if  (x (1) k , x  (2) k )k?N is uniformly bounded over ?, then the previous  F? satisfies  F? ? lim sup k??  min x(i)  E { Fk(x  (i), ? | x(?i)k ) } , i ? {1, 2},  provided also that the set of all minimizers of E { Fk(?, ? | x(?i)k )  } is  uniformly bounded over k ? N.

Due to lack of space, proofs of the previous properties will be  presented elsewhere. It is worth stressing however that convergence  analysis in [24?26] pertains only to the sequence of objective func-  tion iterates, and not to the sequence of primal variables.

4. NUMERICAL TESTS  Although on smaller dimensions than those involved with big data  applications, preliminary tests of the novel approach were performed  using both synthetic and real data.

4.1. Synthetic data  To test Alg. 1 on Example 1, both regressors (u?k)k?N and noise (vk)k?N were generated as zero-mean, i.i.d. mutually independent Gaussian processes with variances 1 and 10?2, respectively. The nonzero entries were placed randomly (following a uniform distri-  bution) in the sparse s?, with values generated independently also  by a zero-mean, unit-variance Gaussian process. Alg. 1 was tested  in two scenarios, namely a low-dimensional one corresponding to  (Q, ?s??0) = (100, 10), and a relatively high-dimensional one with (Q, ?s??0) = (750, 75), tagged ?low-dim? and ?high-dim? in Figs. 1a and 1b, respectively. The regressors (uk)k?N were formed by adding to (u?k)k?N i.i.d., zero-mean Gaussian noise with variance 10?2. The following additional parameters were used in Alg. 1: ?0 := 10  ?6, ?k = 1, ?k ?= 0, ?0 := 10?6, ?s := 10?6, and ?e := 10  2 was selected to prevent large values of ?e?.

Alg. 1 was tested against the standard subgradient descent  (SGD) method with constant step size 10?3, and the block coordi- nate descent (BCD) scheme having the cost in (2) per BCD iteration  of s maximally separated in scalar-valued blocks. Given the co-  ordinates {sj}j ?=i of s, minimization of (2) w.r.t. si amounts to the following scalar-valued optimization task: minsi?R 0.5[yk ??  j ?=i(ukj + ej)sj ? (uki + ei)si]2 + ?s|si|, which can be solved in closed form using the scalar-valued soft-thresholding  operator. In all methods, the exact minimization step over e is  straightforward: Given s, the minimizer of (2) w.r.t. e is e? = (yk ? u?k s)(ss? + ?eIQ)?1s, where the required inverse is per- formed using the matrix inversion lemma: (ss? + ?eIQ)  ?1 = ??1e [IQ ? ss?(?e + ?s?2)?1]. It is worth noticing here that R1 = 1 for the inner loop of Alg. 1 in lines 8?11.

Figs. 1a and 1b illustrate the performance of all methods tested.

Fig. 1a depicts the error fit 0.5[yk ? (uk + e?k)?s?k]2 across time, where (s?k, e?k) denote estimates per slot k. Fig. 1b shows the nor- malized deviation Q?1?s?k?s?? versus k on the support of s?. The smooth curves of Figs. 1a and 1b were obtained after averaging 100 realizations. Although BCD exhibits fast convergence of the error  function iterates, it does not identify correctly s?. The behavior of  SGD confirms the known fact that (sub)gradient techniques are gen-  erally slow convergent. However, SGD shows the best performance  as a system identification module. The proposed accelerated method  outperforms both SGD and BCD in fitting the data accurately, and is  only inferior to SGD in identifying s?.

4.2. Real data  Along the lines of Example 2, Alg. 1 was validated also on esti-  mating and tracking network-wide link loads taken from the Inter-  net2 measurement archive [33]. The Internet2 network consists of  P = 54 links and 9 nodes. Using the network topology and routing information, network-wide link loads (?k)k?N ? RP become avail- able (in Gbps). Per time slot k, only M = 30 of the ?k components, chosen randomly via Mk ? RM?P , are observed in yk ? RM .

The cardinality of the time-varying dictionaries is set constant to  Q = 80, ?k. To cope with pronounced temporal variations of the Internet2 link loads, the forgetting factor ? in Example 2 was set equal to 0.5. Initial values for both (s,D) were randomly drawn from the feasibility regions seen in Example 2. The parameters used  in this realization of Alg. 1 were selected as follows: ?1 = 10 ?3,  ?0 = 10 ?6, ?s = 10  ?3, and ?L = 10 ?1.

Fig. 1c depicts estimated values of both observed (dots) and  missing (crosses) link loads, for a randomly chosen link of the net-  work. The normalized squared estimation error between the true ?k and the inferred ??k, namely ??k ? ??k?2??k??2, is also plotted in Fig. 1c versus time k. Alg. 1 was compared with the state-of-the-art scheme in [5] that relies on the alternating direction method of mul-  tipliers (ADMM), see e.g., [34], to minimize a cost closely related to  (3) w.r.t. s, and uses BCD iterations requiring matrix inversions to  optimize (3) w.r.t. D. On the other hand, the number of inner loops  in Alg. 1 w.r.t. s were set to R1 = 1, while in order to retain the same overall estimation accuracy as [5], R2 = 10 was used for the inner loops w.r.t. D. It is worth noticing here that ADMM in [5]  requires multiple iterations to achieve a prescribed estimation accu-  racy, and that no matrix inversion was incorporated in the realization  of Alg. 1. Both Alg. 1 and [5] perform comparably in the simulated  tests, but only those of Alg. 1 are shown here for clarity.

?20  ?15  ?10  ?5    Time index  D at  a fi  t      Accel. (low?dim) SGD (low?dim) BCD (low?dim) Accel. (high?dim) SGD (high?dim) BCD (high?dim)  (a) Fitness to data vs. time.

?3  ?2  ?1   Time index  N or  m . d  ev ia  tio n      Accel. (low?dim) SGD (low?dim) BCD (low?dim) Accel. (high?dim) SGD (high?dim) BCD (high?dim)  (b) Normalized deviation from s? on the support of s?.

2500 2600 2700 2800 2900 3000  0.5   1.5  Link #50  L in  k lo  ad  Time      True (observed) Estimated      True (missing) Estimated  2500 2600 2700 2800 2900 3000  ?4  ?2   E st  im at  io n  er ro  r  Time  (c) Link load tracking and imputation, as well as normalized squared es- timation error.

Fig. 1. Numerical results for the synthetic [(a), (b)] and real data  [(c)] of Secs. 4.1 and 4.2, respectively.

5. REFERENCES  [1] I. To?ic? and P. Frossard, ?Dictionary learning,? IEEE Signal Process.

Magaz., vol. 28, no. 2, pp. 27?38, Mar. 2011.

[2] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, ?Online learning for ma- trix factorization and sparse coding,? J. Machine Learning Research, vol. 11, pp. 19?60, Mar. 2010.

[3] J. Mairal, F. Bach, and J. Ponce, ?Task-driven dictionary learning,? IEEE Trans. Pattern Analysis Machine Intelligence, vol. 34, no. 4, pp.

791?804, Apr. 2012.

[4] D. L. Donoho, ?Compressed sensing,? IEEE Trans. Inform. Theory, vol. 52, pp. 1289?1306, 2006.

[5] P. A. Forero, K. Rajawat, and G. B. Giannakis, ?Semi-supervised dic- tionary learning for network-wide link load prediction,? in Proc. CIP, Baiona: Spain, May 2012.

[6] L. Grippo and M. Sciandrone, ?On the convergence of the block non- linear Gauss-Seidel method under convex constraints,? Operations Re- seach Letters, vol. 26, no. 3, pp. 127?136, 2000.

[7] P. Tseng, ?Convergence of block coordinate decent method for nondif- ferentiable minimization,? J. of Optimization Theory and Applications, vol. 109, pp. 475?494, June 2001.

[8] P. Tseng and S. Yun, ?A coordinate gradient descent method for nons- mooth separable minimization,? Math. Program., Ser. B, vol. 117, pp.

387?423, 2009.

[9] S. J. Wright, ?Accelerated block-coordinate relaxation for regularized optimization,? SIAM J. Optim., vol. 22, no. 1, pp. 159?186, 2012.

[10] Y. Nesterov, ?Efficiency of coordinate descent methods on huge-scale optimization problems,? SIAM J. Optim., vol. 22, no. 2, pp. 341?362, 2012.

[11] Y. Xu and W. Yin, ?A block coordinate descent method for regularized multiconvex optimization with applications to nonnegative tensor fac- torization and completion,? SIAM J. Imaging, vol. 6, no. 3, pp. 1758? 1789, 2013.

[12] P. Richt?rik and M. Tak?c?, ?Iteration complexity of randomized block- coordinate descent methods for minimizing a composite function,? Math. Program., Ser. A, pp. 1?38, Dec. 2012.

[13] S. J. Wright, R. D. Nowak, and M. A. T. Figueiredo, ?Sparse recon- struction by separable approximation,? IEEE Trans. Signal Process., vol. 57, no. 7, pp. 2479?2493, July 2009.

[14] M. Razaviyayn, M. Hong, and Z.-Q. Luo, ?A unified convergence anal- ysis of block successive minimization methods for nonsmooth opti- mization,? SIAM J. Optimization, vol. 23, no. 2, pp. 1126?1153, 2013.

[15] S. Shalev-Schwartz and T. Zhang, ?Stochastic dual coordinate ascent methods for regularized loss minimization,? J. Machine Learning Re- search, vol. 14, pp. 567?599, 2013.

[16] Q. Tao, K. Kong, D. Chu, and G. Wu, ?Stochastic coordinate descent methods for regularized smooth and nonsmooth losses,? in Lecture Notes in Computer Science, ser. Machine Learning and Knowledge Discovery in Databases, P. A. Flach, T. de Bie, and N. Cristianini, Eds., vol. 7523. Springer, 2012, pp. 537?552.

[17] M. Mardani, G. Mateos, and G. B. Giannakis, ?Decentralized sparsity- regularized rank minimization: Algorithms and applications,? IEEE Trans. Signal Process., vol. 61, no. 11, pp. 5374?5388, Nov. 2013.

[18] B. Recht and C. Re, ?Parallel stochastic gra- dient algorithms for large-scale matrix completion,? 2011, submitted for publication. [Online]. Available: http://pages.cs.wisc.edu/ brecht/papers/11.Rec.Re.IPGM.pdf  [19] S. Shalev-Shwartz, ?Online learning and online convex optimization,? Foundations and Trends in Machine Learning, vol. 4, no. 2, pp. 107? 194, 2012.

[20] J. Duchi and Y. Singer, ?Efficient online and batch learning using for- ward backward splitting,? J. Machine Learning Research, vol. 10, pp.

2899?2934, Dec. 2009.

[21] J. Duchi, E. Hazan, and Y. Singer, ?Adaptive subgradient methods for online learning and stochastic optimization,? J. Machine Learning Re- search, vol. 12, pp. 2121?2159, 2011.

[22] A. Agarwal, S. Negahban, and M. J. Wainwright, ?Stochastic optimization and sparse statistical recovery: An optimal algorithm for high dimensions,? 2012, submitted for publication. [Online]. Available: arXiv:1207.4421v1  [23] Y. Nesterov, ?A method for solving the convex programming problem with convergence rate O(1/k2),? Dokl. Akad. Nauk SSSR, vol. 269, pp.

543?547, 1983, in Russian.

[24] A. Beck and M. Teboulle, ?A fast iterative shrinkage-thresholding al- gorithm for linear inverse problems,? SIAM J. Imaging Sciences, vol. 2, no. 1, pp. 183?202, 2009.

[25] ??, ?Fast gradient-based algorithms for constrained total variation image denoising and deblurring problems,? IEEE Trans. Image Pro- cessing, vol. 18, pp. 2419?2439, 2009.

[26] M. Yamagishi and I. Yamada, ?Over-relaxation of the fast iterative shrinkage-thresholding algorithm with variable stepsize,? Inverse Prob- lems, vol. 27, no. 10, 2011.

[27] M. Yamagishi, M. Yukawa, and I. Yamada, ?Acceleration of adap- tive proximal forward-backward splitting method and its application to sparse system identification,? in Proc. IEEE ICASSP, Prague: Czech Republic, May 22?27 2011, pp. 4296?4299.

[28] C. Hu, J. T. Kwok, and W. Pan, ?Accelerated gradient methods for stochastic optimization and online learning,? in Proc. of Advances in Neural Information Processing Systems (NIPS), 2009.

[29] J. C. Spall, Introduction to Stochastic Search and Optimization: Esti- mation, Simulation, and Control. Wiley, 2003.

[30] H. H. Bauschke and P. L. Combettes, Convex Analysis and Monotone Operator Theory in Hilbert Spaces. New York: Springer, 2011.

[31] J. Neveu, Mathematical Foundations of the Calculus of Probability.

San Francisco: Holden-Day, 1965.

[32] S. Van Huffel and P. Lemmerling, Total Least Squares and Errors-in- Variables Modeling. Springer, 2002.

[33] [Online]. Available: http://www.internet2.edu/observatory/  [34] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, ?Distributed op- timization and statistical learning via the alternating direction method of multipliers,? Foundations and Trends in Machine Learning, vol. 3, no. 1, pp. 1?122, 2011.

