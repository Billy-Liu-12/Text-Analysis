OntOAIr: a method to construct lightweight ontologies from document collections

Abstract?Ontologies are key components in the development of the Semantic Web, as they provide a shared understanding of a domain. However, acquiring the knowledge to construct ontolo- gies is a costly task that requires much time and many resources.

This work describes OntOAIr, a semi-automatic construction method of lightweight ontologies from document collections. The OntOAIr method uses simplified representations of documents, an adaptation of the Frequent Itemset-based Hierarchical Clus- tering algorithm (FIHC), and ontological engineering techniques.

We present experimental results and we describe applications that use the constructed ontologies

I. INTRODUCTION  Ontologies are key components in the development of the Semantic Web, as they provide a shared understanding of a domain. In the field of digital libraries, ontologies support tasks such as the description of resources, integration of information, interoperability, search and browsing. However, acquiring the knowledge to construct ontologies is a costly task that requires much time and many resources. This task can be supported by ontology learning methods, which are often based on machine learning techniques, natural language processing or clustering algorithms.

The literature distinguishes lightweight from heavyweight ontologies according to the degree of formality involved in their encoding [1], [2]. The scope of our work is limited to lightweight ontologies. They range from an enumeration of terms to a graph or taxonomy of concepts with well-defined relationships among them, which provide a representation of an information space.

This work presents the OntOAIr method (Ontologies from Open Archives Initiative Repositories to Support Information Retrieval), a semi-automatic method for the construction of lightweight ontologies called ontologies of records. In the development process of these ontologies, the participation of domain experts is minimum.

We use collections provided by the Open Archives Initiative (OAI) as a testbed. OAI is an organization that promotes standards of interoperability in digital libraries [3]. With respect to our work, the following characteristics of OAI are noteworthy:  ? Each collection has its own policies and administration  ? Collections updates are not reported ? OAI has hundreds of members. New members are incor-  porated frequently ? Collections are built independently as a response to the  needs of particular communities  Protocol for Metadata Harvesting (OAI-PMH) enables ex- ternal online access to OAI collections. However, this protocol does not offer content retrieval mechanisms. We propose the use of ontologies as a data model able to provide an unambiguous and shared terminology, and as the basis to support indexing and retrieval of OAI collections.

The remainder of the document is organized as follows.

Section II describes the related work. Section III presents the OntOAIr method for constructing ontologies. Then Section IV describes the evaluation of OntOAIr method. Next Section V proposes a keyword-based information retrieval model and an ontology-based exploration model as applications of the OntOAIr method. Finally, Section VI includes conclusions and suggests future directions of our work.



II. RELATED WORK  Ontologies are often used to describe documents when the domain has been previously identified. In other scenarios, document clustering and information retrieval techniques are applied. At the time of this writing, a completely automatic construction of ontologies has not been achieved. This would require that machines should be able to acquire, manage and represent knowledge that should be accepted and shared by the users of the constructed ontology. However, there are several contributions in this direction.

The DOME project [4] use some documents as prototypes to form document classes. It relies on ontologies to enable semantic interoperability between a distributed query engine and OAI collections aiming at user-transparent query solving via retrieving and integrating information. A shared ontology is maintained for all the available OAI collections. In the context of OAI there cannot be prototype records at all.

Although it is very likely that similar records are spread over several collections.

OntoLearn [5] is an ontology learning method that applies a hierarchical algorithm to a set of documents from dedicated   DOI 10.1109/ENC.2008.35     web sites and document warehouses. OntoLearn uses the output of the algorithm and WordNet1 to construct domain ontologies. This method proposes a semantic interpretation of terms, in such a way that the constructed ontologies describe concepts with terms composed by two or more words. From our point of view, the main drawback of OntoLearn is that it requires the existence of previously classified documents.

Ljubie? at al. [6] proposes a method to semi-automatically construct ontologies from a set of documents. Ontologies are used to structure information about competencies of compa- nies. Unlike [6], the method proposed in [7] uses the k-means algorithm. Here, ontologies are used to group companies ac- cording to their domain of expertise. The experimental domain is taken from Yahoo! business data. The main disadvantage of this method is the stopping criterion of the k-means algorithm, because setting this parameter in real-life scenarios is a hard task.

Karouri and Aufaure [8] propose a method based on the incremental use of k-means algorithm to construct ontolo- gies from HTML documents. Their method is called Con- textual Ontological Concept Extraction (abbreviated COCE) and based on an unsupervised and hierarchical algorithm that takes into account the implicit semantic structure of HTML labels to extract keywords. HTML documents belong to the tourism domain. Authors report a high precision (almost 87%), however, as in [6], the disadvantage is the determination of appropriate values for the input parameters of the clustering algorithm.

Diederich and Balke [9] propose the semantic grow-bag approach to create light-weight topic categorization systems from free-text documents. The approach uses the keywords provided by authors of digital objects to compute a new co- occurrence metric, finds relations between keywords based on the new co-occurrence metric and constructs graphs that represent the neighborhood of the keywords. The approach is computed off-line and re-run periodically to update the graphs according to the added author keywords. The documents in OAI are not free-text, they are semi-structured documents that expose metadata.



III. THE ONTOAIR METHOD  The ontOAIr method consists of four main tasks: harvesting, representation, clustering and formalization. The harvesting task obtains the documents from the collections. The repre- sentation task constructs a vectorial representation for each harvested document. The clustering task applies an exclusive hierarchical algorithm to the vectors in order to produce a tree of clusters. The formalization task transforms the tree of clusters into an lightweight ontology.

A. Harvesting  Harvesting uses the request verbs of OAI-PMH protocol [3] to obtain documents from collections; the Identify and  1WordNet a lexical database for the English language. Cognitive Science Laboratory, Princeton University. http : //wordnet.princeton.edu/ June 28th 2008  TABLE I EXAMPLE OF A FEATURE VECTOR  Identifier oai:thesisUDLAP:98 URL http : //ict.udlap.mx : 9090/Tales Keyword TF IDF digital 1 10.397 libraries 3 9.133 reference 1 6.089 services 1 7.783  ListRecords verbs are often used for this purpose. This is a repetitive, time-consuming task because collections typically listed hundreds or thousands of documents that need to be transmitted throught the network. Harvesting offers interesting challenges because it depends on external factors such as network overhead and availability of collections.

B. Representation  Once documents from collections have been harvested, sim- plified representations of documents are produced. Unqualified Dublin Core (DC) is the default metadata format for OAI-PMH Version 2.0.

In our work, content information of documents is extracted from the elements dc:title and dc:description.

They contain the title and the description of a document, respectively. dc:title and dc:description elements have just one value.

DC elements contain free text. Before processing, any case sensitivity in the data is removed. Then, feature vectors are generated. A feature vector is a simplified representation of a document formed by keywords (all terms other than stop- words extracted from DC elements) and weights (numeric values that represent the relevance of keywords). The name is adopted from [10].

Our work adds two new elements to a feature vector: an identifier to discriminate between documents and a URL with the location of its collection. Both elements uniquely identify a document in OAI; they do not add content information.

Table I shows a typical feature vector. The weights reflect that some words are better at discriminating between records than others. A TF -IDF factor (term frequency - inverse document frequency) is used as the weight of keywords [11]. TFij represents the number of occurrences of term i in document j. The IDF for the term i (IDFi) is defined as follows:  IDFi = log (  N  ndi  ) (1)  where N is the number of documents of a collection, and ndi the number of documents of the collection that have the term i.

Feature vectors allow users to have a compact view of a collection. They can be stored in text files or in a database to     be processed afterwards.

C. Clustering: enhancements of FIHC  The clustering consists in choosing an exclusive hierarchical algorithm to obtain groups of similar feature vectors. We first suggested the use of the Frequent Itemset-based Hierarchical Clustering (abbreviated as FIHC) in [12].

FIHC is an agglomerative clustering algorithm proposed by [10]. The selection of this algorithm is based on the following hypothesis: ?if a group of documents refers to the same topic, the documents would share a set of terms?. We believe that the terms of cluster labels can be used to construct a vocabulary to describe the main topics of a document collection.

In FIHC algorithm, the sets of shared terms are called frequent itemsets. The generation of frequent itemsets is a prerequisite of this algorithm. The a priori algorithm has achieved high accuracy [13] to extract frequent itemsets (about 84%). The a priori algorithm requires a minimum support to produce the frequency for each itemset.

FIHC uses feature vectors and the generated frequent item- sets to produce a hierarchical structure of non-overlapping clusters. This requires two mandatory input parameters called global support (the percentage of documents in a collection that contains a frequent itemset), and cluster support (the per- centage of documents in a cluster that contains a frequent item- set). In FIHC algorithm (as in k-means or another clustering algorithm), the values of input parameters are experimentally determined by the user. An optional parameter can be added to force this algorithm to produce a fixed number of clusters.

FIHC defines global frequent itemsets as frequent itemsets that appear in more than one user-specified fraction of the document set, and global frequent items as items that belong to some global frequent itemset. Furthermore, the algorithm states the meaning of an item x considered cluster frequent in a cluster Ci if x is contained in some minimum fraction of documents in Ci.

The FIHC algorithm follows the next steps to form clusters (a detailed description can be found in [10]:  1) An initial cluster is constructed for each global frequent itemset. The label of initial clusters is taken from the terms in the global frequent itemset  2) Feature vectors are assigned to initial clusters if they contain the terms of the cluster labels. At this step, a feature vector might belong to several clusters  3) Feature vectors are reassigned to the ?best? initial clus- ters according to the following score function:  Score(Ci ? docj) = (?  x  n(x) ? cSp(x) )  ? (?  x? n(x?) ? gSp(x?)  )  where x is a global frequent item in the docj feature vector that is also cluster frequent in Ci, x? is a global  frequent item in docj that is not cluster frequent in Ci, n(x) is the weighted frequency of x in the docj feature vector, and n?(x) is the weighted frequency of x? in the docj feature vector. In the score function, cluster support and global support parameters are abbreviated as cSp and gSp, respectively.

4) Recompute the cluster frequent items for each cluster 5) Prune the tree of clusters. This process requires of the  merge of similar clusters. Formally, the similarity of Cj to Ci is defined as  Sim(Ci ? Cj) = Score(Ci ? doc(Cj))? x n(x) +  ? x? n(x?)  + 1 (2)  where Ci and Cj are two clusters, doc(Cj) stands for combining all the documents in the subtree Cj into a single document; x represents a global frequent item in doc(Cj) that is also cluster frequent in Ci; x? represents a global frequent item in doc(Cj) that is not cluster frequent in Ci; n(x) is the weighted frequency of x in the feature vector of doc(Cj); n(x?) is the weighted frequency of x? in the feature vector of doc(Cj).

The inter-cluster similarity between Ci and Cj is defined as the geometric mean of the Sim(Ci ? Cj) and Sim(Cj ? Ci):  InterSim(Ci ? Cj) = [Sim(Ci ? Cj)]  ? [Sim(Cj ? Ci)]  6) Child pruning. Scanning the tree in the bottom-up order, for each non-leaf node in a level greater or equal than the second, the inter-cluster similarity is computed between the node and each of its children, and prune the child cluster if inter-cluster similarity is above a threshold.

7) Sibling pruning. Apply the inter-cluster similarity to the clusters at level 1 until the user-specified number of clusters is reached.

We implemented FIHC algorithm as proposed by [10], however, we experimentally determined that pruning the tree of clusters does not achieve a better clustering. Pruning the tree corresponds to the steps 5, 6 and 7.

Child pruning is addressed to reduce the width of the tree of clusters until a user-specified number of clusters is reached.

From our point of view, this process does not contribute to improve the cluster accuracy since the number of main clusters in document collections is unknown.

Sibling pruning is addressed to reduce the depth of the tree of clusters considering only the non-leaf nodes at level two or greater. Since the F and entropy measures take the nodes at level one into account, under these measures, clustering accuracy would not be affected by the elimination of this process.

Therefore, we propose an adaptation of FIHC algorithm where the child pruning and the sibling pruning are removed from the construction of the tree of clusters.

TABLE II EXCERPT OF AN OWL ONTOLOGY OF RECORDS  <?xml version=1.0?>  <rdf:RDF xmlns:owl=...>  <or:algorithm> <model:dataPropertyclustersupport>30  </model:dataPropertyclustersupport> <model:dataPropertyglobalsupport>25  </model:dataPropertyglobalsupport> <model:dataPropertyname> FIHC  </model:dataPropertyname> </or:algorithm>  <or:cluster> <or:label> Calculus </or:label> <or:level> 1 </or:level> <model:hasrecord rdf:resource=??#record8??/> <model:hasrecord rdf:resource=??#record9??/>  </or:cluster>  <or:record rdf:ID=??record8??> <or:title> Calculus I </or:title> <or:subject> Calculus </or:subject> <or:description> </or:description> <or:identifier> oai:libroUDLAP:12087  </or:identifier> </or:record>  <or:record rdf:ID=??record9??> <or:title> Diferential Calculus </or:title> <or:subject> Diferential Calculus </or:subject> <or:description> </or:description> <or:identifier> oai:libroUDLAP:18043  </or:identifier> </or:record>  </rdf:RDF>  D. Formalization  The formalization task refers to the representation of the tree of clusters in a machine-accessible language. We have explored the use of XML, RDF and OWL languages. We have suggested the use of these languages in [14] and [15].

As a way of illustration, Table II shows the use of OWL language to represent ontologies of records. The code includes the parameters of FIHC algorithm and the definition of a cluster with two records.



IV. EVALUATION OF ONTOAIR  The harvesting task depends on external factors such as net- work overhead and availability of collections. Thus, assuming that a set of documents have been harvested, experiments were conducted in order to test the feasibility of OntOAIr method.

This section describes the experiments as well as their results.

The evaluation is focused on representative situations that are plausible in actual information retrieval contexts.

A. Clustering evaluation  Clustering evaluation is aimed at quantifying the goodness of a clustering algorithm, which may be judged differently depending on which measure one uses [16]. This section presents the comparison of our adaptation of FIHC algorithm  with other widely used clustering algorithms: UPGMA [17], [18] and bisecting k-means [17], [18], [16].

In the comparison, the vector space model with TF*IDF- weighting to represent the texts and the cosine measure for calculating similarity between text and clusters are used.

DocCluster is used as the implementation of FIHC algorithm, while Cluto [19] produces the result of UPGMA and bisecting k-means.

1) Clustering measures: The evaluation method uses two clustering measures: F measure and entropy. F measure esti- mates the accuracy of the produced clustering solutions taking reference collections into account. It is assumed that the documents of reference collections belong to a single class or topic called natural class. Entropy provides a measure of goodness for non-nested clusters or for the clusters at one given level of a hierarchical clustering.

By using the F measure, each cluster is treated as the result of a query and each natural class as the relevant set of documents for the query. F measure is oriented towards measuring the effectiveness of clustering [16].

Formally, the recall, precision and F-measure (F) for natural class Ki and cluster Cj are computed as follows:  Recall(Ki, Cj) = nij |Ki| (3)  Precision(Ki, Cj) = nij |Ci| (4)  F (Ki, Cj) = 2 ?Recall(Ki, Cj) ? Precision(Ki, Cj) Recall(Ki, Cj) + Precision(Ki, Cj)  (5)  where nij is the number of members of natural class Ki in cluster Cj . Intuitively, F (Ki, Cj) measures the quality of cluster Cj to describe the natural class Ki. If F (Ki, Cj) is used in a hierarchical structure, all the documents in the subtree of Cj are considered as the documents in Cj .

The quality of a clustering result C using the weighted sum of the maximum F-measures for all natural classes defines the overall F-measure of C, denoted F(C):  F(C) = ?  Ki?K  |Ki| |D| maxCj?CF (Ki, Cj) (6)  where K denotes all natural classes; C all clusters at all levels; |Ki| the number of documents in a natural class Ki; and |D| the total number of documents in the data set. The range of F(C) is [0,1]. A larger F(C) value indicates a higher accuracy of clustering.

Entropy is the second clustering measure used in this work.

It provides a measure of ?goodness? for un-nested clusters or for the clusters at one level of a hierarchical clustering [16].

If CS is a clustering solution, for each cluster the class distribution of the data is required. Then, the entropy of each cluster j can be computed as follows:  Ej = ? ?  i  pij log(pij) (7)     Data Number of Number of Class Number of Set documents classes size words  Classic4 7094 4 1033-3203 12009 Hitech 2301 6 116-603 13170 Re0 1504 13 11-608 2886 Reuters 8649 65 1-3725 16641 Wap 1560 20 5-341 8460  TABLE III SUMMARY DESCRIPTION OF DATA SETS  where pij is the probability that a member of cluster j belongs to class i, and log(pij) is the logarithm base 10 of pij .

The total entropy for a set of clusters (ECS) is the sum of the entropies of each cluster weighted by the size of each cluster. It is computed as follows:  ECS = m?  j=1  nj ? Ej n  (8)  where nj is the size of the cluster j, m is the number of clusters and n the total numbers of data. In general, a smaller ECS value indicates a higher accuracy of clustering.

2) Data sets: Our work uses the data sets proposed by [10] and [20] as test collections, which are often used in document clustering research. Table III shows a summary description of these data sets, which are heterogeneous in terms of document size, cluster size, number of classes and documents distribution.

Classic4 is formed by the CACM, CISI, CRAN and MED abstracts 2. Hitech and Wap are from the San Jose Mercury newspaper articles 3 and the Yahoo! subject hierarchy web pages4, respectively. Reuters and Re0 are extracted from newspaper articles [21]. For Reuters, only the articles that are uniquely assigned to a natural class are used. In all of the data sets, stop words have been removed.

3) Results: Table IV shows the overall F measure values with different user specified number of clusters, where a dash indicates that the algorithm is not scalable to run (the algorithm consumes all the main memory before producing a clustering solution). For our implementation of FIHC algorithm, a cluster support of 25% and a global support of 5% are used. A minimal support of 3% in the a priori algorithm is used. The values of these parameters were experimentally determined using the Tales collection as reference. In all the algorithms, the documents in the subtree of Cj at the first level are considered as the documents in Cj .

The main disadvantage of UPGMA is that it is not scalable for large data sets. Bisecting k-means and FIHC are scalable.

However, k-means presents two drawbacks. First it requires the introduction of the number of clusters; in real life scenarios, this number is unknown. Second, this algorithm does not  2ftp://ftp.cs.cornell.edu/pub/smart/ 3Text REtrival Conference TIPSTER. 1999. < http : //trec.nist.gov >.

November 1st 2007.

4Yahoo!. http : //www.yahoo.com. November 1st 2007.

Data Set Number of FIHC UPGMA Bisecting (Number of classes) clusters k-means  15 0.50 - 0.41 Classic4 (4) 30 0.51 - 0.45  60 0.49 - 0.29 15 0.40 0.37 0.43  Hitech (6) 30 0.39 0.51 0.31 60 0.39 0.49 0.24 15 0.43 0.53 0.37  Re0 (13) 30 0.41 0.47 0.37 60 0.38 0.38 0.30 15 0.59 - 0.44  Reuters (65) 30 0.58 - 0.37 60 0.60 - 0.33 15 0.54 0.59 0.55  Wap (20) 30 0.53 0.58 0.46 60 0.52 0.57 0.39  TABLE IV F MEASURE COMPARISON  Data Set Number of FIHC UPGMA Bisecting clusters k-means  Classic4 4 1.45 1.59 1.37 Hitech 16 1.62 1.87 1.65 Re0 13 1.83 1.98 1.71 Reuters 65 2.01 2.03 1.92 Wap 20 1.66 1.48 1.77  TABLE V ENTROPY COMPARISON  propose cluster labels. As a result, the search of relevant documents would require the exploration of all the clusters.

According to Table IV, FIHC is robust enough to produce consistently high quality clusters in many cases. Thus, the selection of the FIHC algorithm as a key component to con- struct ontologies of records considers the following aspects: (1) the results presented in [10] and [20], (2) the overall F- measure values of Table IV; and (3) the characteristics of the constructed tree of clusters, specially that each k-level cluster is described by a k-term label.

Intuitively, each class in FIHC has a ?core? vocabulary that acts as a simple disambiguation mechanism. For instance, if a cluster label is formed by the terms radio, television and internet. Then, users could deduce that the word radio in the documents of this clusters refers to a communication media instead of a chemical element or a mathematical measure.

However, these core vocabularies may overlap.

Table V shows the total entropy. The number of desired clusters was introduced as another input parameter.

According to Table V, bisecting k-means and FIHC are the best algorithms as they have similar behavior with respect to entropy, whereas UPGMA does poorly. Thus, we conclude that the partitions provided by FIHC are useful. They could be used as reasonable aid for classification in OAI contexts and might even provide new insights into what collections are about.

TABLE VI SUMMARY DESCRIPTION OF TALES DATA PROVIDER  Total of documents: 1287 Documents with title: 1287 Documents with title and abstract: 783 Number of terms in titles: 6022 Different terms in titles: 2809 Number of terms in titles and abstracts: 109451 Different terms in titles and abstracts: 16230  An experiment to estimate how well cluster labels describe the documents also has been carried out. It consisted of comparing the keywords of feature vectors with the cluster labels (cluster frequent itemsets). The Tales collection 5 has been used for this experiment.

For the experiment, Tales offered over one thousand metadata documents that describe undergraduate and graduate digital theses. Documents are organized into sets, one for each academic department. Currently, there are 29 sets. Table VI shows the main characteristics of this collection.

About 44% of the frequent items in the documents with title and abstract coincide with cluster labels, and about 38% for documents without abstracts. The same values of cluster support, global support and minimal support of Section IV-A3 were used.

B. Complexity analysis  In the OntOAIr method, clustering consists of two main tasks: (1) the generation of frequent itemset; and (2) our implementation of the FIHC algorithm. A native (but not scalable) solution for the first task, is to scan the collection of documents once and keep a counter for each itemset.

The runtime is (O|L|), where |L| represents the length of k- itemsets. Other solution is to scan the collection of documents ?L? times, looking for 1?, 2?, ..., k? itemsets.

Our work uses the a priori algorithm to generate the frequent itemsets. The most expensive task of this algorithm is the generation of candidates. It requires the k-1 frequent itemsets to generate k-itemsets candidates. The complexity depends on the sorting order of the items at the top level. The most favorable execution time is achieved if the first k-items are ordered by increasing frequency [22].

The runtime of the a priori implementations vary according to the value of the minimum support, the length of maximum frequent itemsets and the data structures used. As the mini- mum support decreases, the execution time increases as the total number of candidates increases.

The a priori algorithm is tractable if the database can reside in memory. Implementations like the AprioriHybrid algorithm [23], Apriori-C algorithm [24] or Frequent Parent - Growth algorithm (FP-Growth) [25] are linear with the number of documents.

5Available at: http : //ict.udlap.mx : 9090/Tales/Oaitesis?verb = Identify. June 16th 2008.

The second task of OntOAIr method, the implementation of FIHC algorithm, involves three steps: identification of global frequent itemsets, construction of disjoint initial clusters and construction of the tree of clusters [10]. Its runtime increases as the minimum support to generate the frequent itemset decreases.

The identification of global frequent itemsets has the same runtime than the generation of frequent itemsets. To construct disjoint initial clusters, the feature vectors are scanned twice, once to construct initial clusters and once to make the initial clusters disjoint. This step is not more expensive than the generation of frequent itemsets.

The construction of the tree of clusters first removes all empty clusters with a maximal cluster label. The remain- ing number of clusters is much smaller than the number of documents, thus this step is linear with respect to the numbers of remaining clusters. Again, the construction of the tree of clusters is no more expensive than the generation of frequent itemsets. Therefore, the complexity analysis of OntOAIr method shows that it is linear with respect to the number of documents.

C. Ontology evaluation  This section describes two approaches to evaluate the ontologies of records. The first approach, called technical evaluation, is focused on the structure of the ontologies. It is carried out by the developers. The second approach, called task-based evaluation, is addressed to determine the usefulness of the ontologies to retrieve documents from multiple OAI- compliant collections. The task-based evaluation is also carried out by developers, although it takes the point of view of users into account.

1) Technical evaluation: The technical evaluation is based on two complementary methods proposed in the frameworks of Methontology [26] and OntoClean [27].

According to Methontology, the technical evaluation con- sists of two phases: the evaluation of concepts definitions and the evaluation of the taxonomy. The first phase uses the elements of a generic document hierarchy as the frame of reference [15], and the criteria of consistency, completeness and conciseness.

For consistency, the inexistence of contradictory definitions and the consistency of each definition were checked. The inclusion of all the definitions of the frame of reference in the ontology was verified for completeness, while useless and redundant definitions were removed for conciseness.

The second phase of Methontology verifies the absence of the following errors in the taxonomy of an ontology of records:  ? A class defined as a specialization or generalization of itself  ? A concept defined as a subclass of a concept to which it does not really belongs  ? Disjoint knowledge omission ? Exhaustive knowledge omission ? More than one definition of any of the hierarchical  relations     Class Metaproperties Cluster +R+I-O+U Description +R-I-O+U Label +R+I+O+U Object +R+I+O+U Ontology of records +R+I+O+U Record +R+I+O+U Subject +R-I-O+U Title +R-I-O+U  TABLE VII CLASSES AND METAPROPERTIES OF AN ONTOLOGY OF RECORDS  ? Identical formal definition of classes The evaluation based on Methontology did not suggest  changes in the OWL representation of the ontology of records.

In contrast, the use of OntoClean method caused the redefin- ition of relations between classes.

The OntoClean method proposes a set of activities to remove incorrect ?subclass of?? relations from a taxonomy based on two main tasks: (1) the allocation of meta properties to the classes, and (2) the application of a set of rules to validate the allocation [27]. The meta properties are briefly described as follows:  1) Rigidity: A property is rigid if and only if it is necessarily essential to all its instances  2) Identity: A property carries an identity criterion if and only if all its instances can be identified by means of a sameness relation  3) Unity: A property carries unity if there is a common unifying relation such as all the instances of the property are whole  Table VII summarizes the assignation of meta properties.

The values ?+R? and ?+U? indicate that the classes have rigidity and unity, respectively. The value ?+I? means that the class carries an identity criteria. Otherwise, the value ?-I? is used. The value ?+O? means that the class supplies an identity criteria if and only if such a criterion is not inherited by any subsuming class or property. Otherwise, the value ?-O? is used.

Figure 1 shows the result of restructuring the ontology of records after the evaluation of Ontoclean. As in previous cases, XML, RDFS and OWL are used to implement the restructured ontology.

2) Task-based evaluation: In absence of a commonly agreed-upon schema for analyzing the properties of an ontol- ogy, a common way to proceed is to evaluate it within some existing application. This section describes the competency of the ontologies of records to support information retrieval.

The test bed is the OntoSIR system. Hereafter, the OWL implementation of the restructured ontologies is used.

Althought the Dublin Core elements dc:title, dc:description and dc:subject are commonly used to obtain keywords [28], [29] and [4], in our work, keywords from the dc:subject element are excluded from the experiments. The reason for this is that the OntOAIr method  Fig. 1. Result of restructuring the ontology of records after OntoClean method  produces disjoint classes, while the OAI-PMH protocol does not restrict the number of subjects.

The OntOAIr method is compared against the Vector Space Model (VSM) [11]. Precision and recall measure effectiveness.

The implementation of the VSM that is used in the experi- ments is provided by the Apache Lucene search engine [30].

At the time of this writing, OAI does not suggest a test collection, thus we choose the CACM collection6 because it can be regarded as an OAI-compliant collection. This is a collection of titles and abstracts from the CACM magazine.

CACM collection consists of 3204 documents; each record includes information about the author, title, abstract, (manually assigned) keywords and information about cites.

The CACM collection includes queries formed by experts and their relevance judgments. In our work, keywords are extracted from the titles and the abstracts. The same indexing process applied to process the collection is used to process the queries. In the experiments, all the query keywords were given a weight of 1. Table VIII shows the results of the comparison between the OntOAIr method and VSM. It includes averaged values of precision to standard values of recall.

Table VIII shows an improvement for every standard level of recall (average percent 17.8%). Precision is interpolated to standard levels of recall and averaged on the number of queries  6Test collections.

http : //www.dcs.gla.ac.uk/idom/irresources/testcollections/.

June 16th 2008.

TABLE VIII RECALL AND PRECISION RESULTS  Recall VSM precision OntOAIr precision 0.1 0.53 0.61 0.2 0.41 0.51 0.3 0.32 0.36 0.4 0.25 0.28 0.5 0.20 0.25 0.6 0.15 0.19 0.7 0.10 0.13 0.8 0.07 0.09 0.9 0.03 0.04 1.0 0.01 0.02  Average: 0.21 0.25  Fig. 2. Recall/precision curve  (15).

In order to produce the recall-precision curve of Figure 2,  recall and precision were normalized taking the best and the worst case into account [11]. Table IX shows the normalized recall for 15 queries; normalized precision for the same queries is shown in Table X.

The experiments show that the OntOAIr method is stable and feasible to support information retrieval. It improves slightly the retrieval effectiveness in comparison with VSM, especially at the top documents retrieved. However, more experimentation and statistical analysis are needed in order to generalize this argument.

The experiments were conducted on a WinXP system run- ning on 2.8GHz Pentium (R) processor with 1GB of memory.

In order to reduce inaccuracy, only necessary applications are loaded during testing.

Another goal of the task-based evaluation is to assess the utility of OntOAIr method. A constituent of this goal is the assessment of the adequacy of the knowledge represented regarding the impact on effectiveness.

The construction of heavyweight ontologies would need  TABLE IX NORMALIZED RECALL FOR 15 QUERIES OF CACM COLLECTION  Query identifier VSM recall OntOAIr recall 2 0.92 0.91 5 0.87 0.87 8 0.81 0.81  13 0.91 0.90 16 0.83 0.83 25 0.89 0.89 27 0.90 0.89 35 0.93 0.92 37 0.84 0.85 39 0.73 0.73 48 0.94 0.93 49 0.89 0.91 53 0.92 0.91 56 0.74 0.74 59 0.88 0.89  Average: 0.86 0.87  TABLE X NORMALIZED PRECISION FOR 15 QUERIES OF CACM COLLECTION  Query identifier VSM precision OntOAIr precision 2 0.71 0.72 5 0.63 0.63 8 0.75 0.76 13 0.68 0.68 16 0.73 0.74 25 0.80 0.79 27 0.77 0.78 35 0.85 0.85 37 0.57 0.56 39 0.59 0.60 48 0.84 0.83 49 0.75 0.76 53 0.47 0.47 56 0.86 0.85 59 0.65 0.65  Average: 0.71 0.71  very advanced semi-automatic knowledge extraction tech- niques that are not available in the current state of the art.

However, ontology learning methods such as OntOAIr address this goal.

Therefore, ontologies of records can be regarded as interme- diate representations to communicate the knowledge acquired by a clustering algorithm to domain experts. The XML, RDF and OWL implementations would allow experts to elaborate the following types of queries [31]:  ? Selection queries: They retrieve parts of the data based on its content, structure or position  ? Extraction queries: They extract substructures, and can be considered as a special form of a selection query  ? Reduction queries: They specify what parts of the data must to be included in the answer  ? Restructuring queries: They allow data restructuring, possibly into different formats or serialisations  ? Aggregation queries: They add several data items into one new data item  The queries should be easy to design in order to facilitate the task for domain experts, who often lack necessary technical skills. However, the use of ontology query languages requires     one to submit a textual query, a description logic (DL) sentence or SQL-like query and the use of a reasoner to get an appropriate answers. The following list contains potentially useful queries:  ? subClass or superClass of a given class type ? subProperty or superProperty of a given property type ? what type of class (all or direct superclasses) a given  instance is ? whether two given instances or two types are the same  or different ? what is the value of a specified property of a instance ? all the instances of a class ? all the properties of a given instance The above queries can improve the access to large, hierar-  chically structured document collections in order to support such as knowledge-repository exploration or cross-repository exploration.



V. APPLICATIONS  Retrieval models involve indexing processes, representation for documents and queries, matching operations between them and criteria of similarity to extract results. This section de- scribes two models that exploit ontologies of records to search on collections. The first is keyword-based, while the second uses the semantic features of ontologies for exploration.

A. Keyword-based retrieval model  Web search engines generally are keyword-based search.

They choose documents according to Boolean combinations of term matching conditions and similarity measures [32] . This section describes a keyword-based model that retrieves clusters of records from XML ontologies. Here, records are treated as documents, queries as specifications of clusters of documents and ontologies as the structures that store documents of collections.

Ontologies of records can be regarded as trees that cluster metadata documents, so that each k-level cluster is described by a k-term label. For each cluster, each of its documents contains the k terms of its label.

The Boolean model, the VSM model or latent semantic in- dexing can be adapted to search on ontologies. However, they would produce lists of non-related documents. Thus, we have developed a model that searches for relevant clusters through matching operations between queries and cluster labels. XML ontologies are used for this purpose, since the model only requires the labels of each cluster.

In the proposed model, natural language queries are con- verted to keyword-based queries. Query keywords are ex- tracted after a stopword elimination process. The stopword list used is formed by intersecting the stopword lists of CACM and Time collections 7. Keywords are case-insensitive.

7The collection of Communitcations of the ACM (CACM) and the Time collection are available at http : //ir.dcs.gla.ac.uk/resources/testcollections June 16th 2008.

Algorithm V-A describes the search process on the ontology of a collection. It comprises two main stages: selection of potential clusters and application of a similarity function. This algorithm uses the following definitions:  ? Length of a query: The number of query keywords ? Potential cluster: A cluster for which a similarity measure  between query keywords and the cluster label is equal or greater than a threshold  ? Potential collection: The set of potential clusters ? Similar record: An element of a potential cluster Also, Algorithm V-A uses the following conventions: ? A query q of m terms is represented as a tuple q  (q1, q2, ...qm) where m is the length of q ? Query terms are connected with AND operators ? An ontology of records is briefly represented as a tuple  O(t,d,c,r) where t is the number of nodes at the first level (it corresponds with the number of frequent 1-itemsets), d is the number of levels, c is the number of clusters and r is the total number of records. The root cluster has level 0.

? C(l) indicates that l is the label of a cluster C ? S(q,l) denotes a similarity function between a query q and  a cluster label l.

? card(X) represents the cardinality of a set X, that is, the  number of elements of X Algorithm V-A  Algorithm V-A.

input: q(q1, q2, ..., qm):query of length m, O(t,d,c,r):ontology of records output: P: potential collection begin 1. determine the set C of c potential clusters such as ?qi, qi ? c(l),  0 ? i ? m, 2. for each c ? C do 3. add the label l to the list of labels Lc 4. for each label l ? Lc do 5. apply S(q,l) 6. order Lc in a descendent way according to S(Q,l) 7. form a sublist Lr by selecting the first n labels from Lc, where n is the maximum number of potential clusters 8. for each label lr ? Lr 9. retrieve the similar records of each cluster c(lr) to form P 10. if card(C)<n, P = P ? P?, where P? is the set of relevant documents according to the Boolean model applied to q and the root cluster of O(t,d,c,r) end.

Function 9 measures the similarity between the label l of the cluster c and a query q(q1, q2, ..., qm).

S(q, l) = ? qi  w(qi), 0 ? i ? m (9)     where qi represents a query keyword and w(qi) is the average of the weights of qi in the feature vectors of cluster c.

Algorithm V-A makes retrieval faster because it applies the similarity function S(q,l) only to cluster labels instead of all records of an ontology. Intuitively, labels with more terms represent more specific topics than labels of ancestor clusters.

A retrieving task using multiple collections requires the management of multiple ontologies. For this purpose, the first five steps of Algorithm V-A are implemented per collection in such a way that Lc contains the labels of all potential clusters. Steps 6, 7, 8 and 9 are implemented without any modification. Step 10 requires to generate the union of root clusters of the participating ontologies. This task eliminates duplicate documents.

The representation of the potential collection P in the web interface shows a tree of clusters that enables the retrieval of groups of similar documents. The tree has links that allow users to access metadata documents.

B. Ontology-based exploration model  This section describes an exploration model that uses the semantic features of ontologies of records. At this point, it is assumed that an ontology of records of a collection has been constructed and mapped in XML, RDF and OWL languages.

The input to the ontology-based exploration model is a structured query which is generated from a form-based inter- face where the user selects an implementation of the ontology and enters queries about the path of the elements of the ontology such as classes, relationships or property values. The query is executed against the chosen implementation to obtain a result, which is formed by instance tuples that satisfy the query. The result is encoded in XML to form a response.

XML ontologies can be queried with XML query languages or parsers. We use XPath [33] to allow users to address parts of ontologies through the hierarchical structure of their elements.

In this implementation, ontologies of records are regarded as sets of nodes and attributes.

RDF and OWL implementations have enough expressive power to represent the semantics of the real world information in machine-accessible ways. The former has formal syntax, formal semantics and XML Schema datatypes, while the latter is the standard recommended by the W3C for Semantic Web.

At the time of this writing, there is no standardization of query languages for RDF and OWL. We have used Jena8 , a tool with some degree of support for OWL and RDF to search on these mappings, where queries express conditions involving domain ontology instances, document properties, or classification values.

The keyword-based retrieval model and the ontology-based exploration model are similarity-based retrieval tasks.

8Jena - A Semantic Web Framework for Java.

?http://jena.sourceforge.net/?. June 16th 2008.



VI. CONCLUSIONS  This work presented OntOAIr, an ontology learning method to construct lightweight ontologies from multiple document collections. The method is based on four main tasks: harvest- ing, representation, clustering and formalization. The harvest- ing task obtains the documents from the collections. The rep- resentation task constructs a vectorial representation for each harvested document. The clustering task applies an exclusive hierarchical algorithm to the vectors in order to produce a tree of clusters. The implementation task transforms the tree of clusters into a lightweight ontology. The construction of ontologies requires minimal human intervention.

We used the FIHC algorithm proposed by [10] as the exclusive hierarchical clustering algorithm, for which and we proposed an adaptation. Experimental evaluations have shown that the FIHC algorithm is as at least as good as UPGMA algorithm and bisecting k-means algorithm.

The ontologies constructed by the OntOAIr method con- stitute a data model able to provide an unambiguous and shared terminology that human and software agents can both understand and use. Ontologies are also an information re- trieval model on their own because they support indexing and retrieval.

The indexing process is based on two tasks: (1) the se- lection of informative elements of each record, and (2) the organization of the records in data structures that enable the search. The retrieval process has been implemented through the keyword-based retrieval model and the ontology-based exploration model. These models assume that a record that does not match any term in the query is not relevant.

An algorithm to establish similarity between queries and groups of records has been developed for the keyword-based retrieval model. We have conducted some small scale exper- imentation and, as a result, we have demonstrated that the effectiveness of the keyword-based retrieval model is similar to that of the vector spaces model. Further experimentation and larger document sets are needed to test and improve our method.

The OntOAIr method can be used to support manual construction of ontologies, to cluster the responses of search engines, or as a basis to support reasoning in Semantic Web contexts. As future work, we propose three tasks for the maintenance of these ontologies: 1) the inclusion of a set of records in a previously constructed ontology without compromising the accuracy of the clustering or the effectiveness of the retrieval, 2) the management of versioning, which should help to keep track of their evolution and 3) the incorporation of inference mechanisms that search through the ontologies and deduce results in an organized manner in order to complete missing information. Inference can be used for implicit query expansion or to automatically maintain the consistency.

