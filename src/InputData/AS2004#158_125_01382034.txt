<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Proceedings

Abstract: Concept lattice represents knowledge with the relationships between the intension and the extension o l concepts, and the relationship between the generalization and the specialization of concepts, thus it is properly applied to the description of association de mining in database. The Quantitative Extended Concept Lattice (QECL) evolve from concept lattice by introducing equivalent relstionships to its intension and qnantity to its extension, Based on it, we can mine asmiation rules, compared witb well-known Apriori algorithm, without calculating frequent itemsets, w i l y  obtain intemting association d e s ,  in the meantime a lot of redundant rules are reduced, thus the efficiency and veracity of mining de are improved.

Keywords: Assaeiation rules; Apriori; eoncept lattice; frequent itemsets 1. Introduction Association rules"' mining is an important branch of data mining, which describes potential relationships among data items (attribute, variant) in databases, the well-known Apriori algorithm'21proposed by R. Agrawal et. a1 in 1993 is an influential algorithm for mining frequent itemsets for Boolean association rules, usually most algorithms are impioved on basis of the algorithm, but there exist some problems, for example, much more time complexity, more redundant rules and so on.

However, association d e  mining based on concept lattice has some advantages: Firstly, concept lattice represents knowledge with the relationship between the intension and the extension of concepts, and the relationship between the generalization and the specializations of concepts, thus it is properly applied to the description of association rules mining in databases.

Secondly, based on QECL, we can mine association d e s , compared with well-known Apriori algorithm, without calculating frequent Itemsets. Finally we easily obtain interesting association rules, in the meantime a great number of redundant rules ate reduced. For users, most of importance, they can freely mine interesting association rules by themselves, therefore the efficiency and veracity of mining rules are improved.

2. Association d e Association rule mining can be stated as follows: Let I=[&amp;, i2, ... in ] be a set of items. Let D, the task-relevant data, be a set of transactions, where each transaction T is a set of items such that TzI. The quantities of items bought are not considered. Each tmnsaction is assigned an identifier, called TID. Let A be a set of items,.A transaction T is said to contain A if and only if Afl .  An association rule is an implication of the form A a B ,  where A d ,  B d , and AnB=0. The rule A 3 B  holds in the transaction set D with support s, where s is the percentage of transactions in D that contain AuB (i.e., both A and B). This is taken to be the probability, P(AuB). The rule A a B  has confidence c  in the transaction set D if c is the percentage of transactions in D containing A that also contain B. This is taken to be the conditional probability, P(BIA). That is, Support (A*B)=P(AwB)=s, Confidence (A*B) =P(BIA) =Support (A=+B)/Support (A)=c.

Mining association rules is to mine strong association    Mining association rules is to mine strong association rules that satisfy the user-specified both minimum support threshold and confidence threshold.

3. Apriori algorithm The algorithm has become one of the most popular data mining methods, which was proposed by Agrawal et al.

The aim of the association rule mining is that how to find out those interesting relations among the amibutes in the 0-7603-8403-2/04/$20.00 @2W4 IEEE 26-29 August 2004 large databases. Many methods have been proposed to reduce the number of scans and candidates by hashing, sampling, pattitioning, dynamic itemsets counting, etc. but the essential of them have not changed.

Apron algorithm is an influential algorithm for mining frequent itemsets for Boolean association rules, employs an iterative approach known as a Level-wise search, where k-itemsets are used to explore (k+l)-itemsets. First the set of frequent 1-itemsets is found. This set is denoted Ll. LI is used to find b, the set of frequent 2-itemsets, which is used to find LJ, and so on, until no more frequent k-itemsets can be found. The finding of each Lk requires one full scan of the database. Once the frequent itemsets from transactions in a database D have been found, it is straightforward to generate strong association rules from them, where strong association rules satisfy both minimum support and minimum confidence. However, there exist some problems.

1) The algorithm approach requires multiple scans of the database to generate all the candidates, which becomes a bottleneck. 2) There are a great number of redundant association rules that make it difficult for users to mine all interesting association rules, and workload of mining association rules increases rapidly.

4. Concept lattice A context is defined as a triple C= (0, D, R)  , where 0 is a set of objects, D is a set of attributes, and R is the binary relationships between 0 and D, according to the inclusion relationships between extensions, there is a unique ordered set that descrihes the structure of inherent lattice, which defines natural groupings and relationship descriptions between the objects and their attributes in the context. This structure is also known as Concept (galois) lattice 13?.

Definition 4.1: Each pair such as (A, B) derived from the context is called a concept, where AE P(O), BE P(D), and P(O), P(D) are the power sets of 0 and D respectively. A and B create a connection in. terms of the following properties: A?=( me DlVgc A,gRm],B?=( gc OlVge B, ? g b j .  Kinere K=B, B?=A, then A is called the extension of concept, and B is called the intension of concept.

Definition 4.2: In the context C=(O, D, R), for a given concept (A, B), if Bi CA? and Bi?=A, then Bi is called an equivalent feature grouping of A?, In particular, A? is also an equivalent feature group of itself, then Qu(A)={Bil Bi CA? and B,?=A} can be defined.

Definition 4.3: A pair (A,B) derived from context C is called a basic concept, where AEP(O), BEP(D) and  BeEqu(A). If two concepts CI= (A,B,) ,CZ= (A,Bz) are equivalent two basic concepts, BI,  Bz are equivalent grouping, then we also call that BI, B2 are equivalent basic intensions, therefore, C=(A, Equ(A)=(B, UB,)) is obtained, the rest may be deduced by analogy, When there are k basic concepts CI, CZ. . .CL, we can deduce C= (A, Equ(A)) =(A, (BI UBzUB,, ... UBt)). whichiscalled aextendedconcept, or simply called concept, and B, (i=1,2, ... k) is called equivalent intension.

Definition 4.4: In a ECL, partial order relation ?&lt; ? between concept Cg=(AI.BJ and Cz=(A2,Bz) is defined as    concept Cg=(AI.BJ and Cz=(A2,Bz) is defined as C I &lt; C z o B z ~ B ~ ,  then CI is the sub-concept of Cz (the son of the father CZ), and CZ is the sup-concept of Cdthe father of the son Cl), that is, the relationship between CZ and CI is the relationship father and son, If CI&lt;Cz, there isn?t a concept C=(A,B) such that satisfy CI&lt;C&lt;C2. CI&lt;Cz is called an direct-sub-concept -direct-sup-concept-relation between C1 and Czr the Hasse diagram of concept lattice is generated according to the partial order relationship.

If concepts Cl=(AI,BI) and C2=(AZ,Bz) in a ECL satisfy AIcAz in the context C, there is a sub-concept-sup-concept-relation between C, and CZ.

denoted as CI&lt;Cz Concept sets and their dmt-sub-concept- direct-sup-concept relationships in the context consist of a complete concept is called Extended Concept Lattice ??,5,61.

Definition 4 . 5  The Generalized Super-concept of concept C, denoted as superfC), can be defined as follows: 1. The direct-supcoucept of C is generalized Super(C); C&lt;CI* CI E super(C). 2. Generalized Super(C) includes of generalized Super(C) of the direct-sup-concept of C.

namely, (C&lt;Cl)h(C2E super (CI))*{C~ E super(C)).

Definition 4 . 6  As for concept C=(A,B), C?=(IAI,B) is called Quantitative Extended Concept L4561 of C, C is real concept of C?, where IAl is the cardinal of extension A.

We mark basic quantitative extended concept comprised of intension B by C p ) ,  and extensions of those are N by BasC(N). The equivalent basic concepts C1=(A,B1), CZ=(A,BZ) are corresponding to equivalent basic quantitative extended concepts Cl?= (IAI,BI), Cz?=(IAI,Bz) respectively, Lattice derived from quantitative extended concepts is cd!ed Quantitative Extended Concept Lattice (simply called QECL). Since we have ignored the specific extension information, realization of some problems solutions will be changed.

Theorem 4.1: Concept C1=(Bl) and concept C2=(Bz) whose intensions can not intersect each other is equivalent in a BasC(N) if and only if C(BluBz)c BasC(N).

Theorem 4.2: If concept Cl=(Nl,Bl), concept CZ=(NZ.BZ) in a BasC(N) satisfy BILBZ, then CI a is equivalent CZ.

Theorem 4.3: In a BasC(N), Cl=(NIBI)oC~=(NZ,B2) if and only if 3 B l , ~ B I . 3 B ~ 1 ~  BI, satisfying C(BI,)OC(BZJ Theorem 4.4: CI=(NlrBI) and Cz(N2.B2) in a BasC(N), 26-29 August 2004 satisfying BIaB2, and NI fNz, then C2e super(CI).

Above the proofs of theorems, we can prove them by the defmitions and theorems of extended concept lattice, here are abridged.

5. Mining association d e s  in quantitative extended concept lattice As for concept C=(IAI, B=(BI.B2. ..., BL)) in a QECL, we can intrcduce Support(B)=IAVM of intension B to concept C, M is the sum of transactions in D, IAl is the  extension cardinal of concept C.

Definition 5.1: If Support (B) of concept C in a QECL is big than the support threshold, C is called an frequent concept (usually called frequent itemsets), where each intension B,(i=l ... k) of B is a basic frequent items, the redundant intension of B is called redundant frequent items.

We can acquire basic frequent items from frequent concepts in a QECL, obviously, each grouping basic intension in frequent concepts is basic frequent itemsets.

Since redundant intension is obtained from basic intension, therefore, we can also obtain basic frequent itemsets f? the corresponding to basic intensions of frequent concepts.

Theorem 5.1: Frequent items is corresponding to the intension of ECL each other.

Theorem 5.2 If C=(IAI,B=(BI,Bz, ..., Bk]) is a concept in a QECL, the support of B, (kl, ..., k) is IAVM, where IAl is    QECL, the support of B, (kl, ..., k) is IAVM, where IAl is the extension cardinal of C, M is the sum of transactions in D. If B is frequent itemsets whose support is bigger than the support threshold, B must be a group of equivalent intension (basic or redundant intension) of the concept.

Base on the theorems mentioned above, we can mine association rules on a given QECL.

Theorem 5.3: 1. If concepts CI=(NI,BI). C2=(Nz,B2) satisfy C2csup(C1), we can gain association rule: Bz BI-B2, its confidence is NI / N2. otherwise, B,-B2 =$ Bz, its confidence is 100%.

2. If concepts Cl=(N,,Bl), Cz=(N2,B2) do not satisfy C2~sup(CI). and there exists nonempty common maximum-sub-concept C=(N,B), namely, OC?=(W,B?), which makes CE sup(C?)ACI~sup(C?)AC2~ sup(@) true, there are association rules between BI and B2: B e B 2 ,  B2 BI, their confidences are N/N,, N/N2 respectively.

6. The generation algorithm of quantitative extended concept lattice From above the discussion in Section 4, we can obtain the algorithm to generate quantitative extended concept lattice, which should be inserted initial quantitative extended concepts (corresponding to initial concepts) one by one, then adjust it according to Theorem 3.1-3.4. The algorithm is written as following: The algorithm Insertion (QECL,C): insert a quantitative extended concept into QECL.

(1) A quantitative extended concept C=(N,B) is inserted in the QECL, if there is equivalent concept of C in the QECL such that 3Cli(Cli oC), we merge the intension of C into the intension of Cli  then finish the operation of insertion, that is,, Intension(Cli):=Intension (Cli)uIntension(C); Else (2)on the assumption that CO is an immediate sup-concept of C, fmtly insert C as a sub-concept of CO into the QECL, finish the operation in tum as follows: I)  Each Cli (Cli&lt;Co,Cli&lt;C) connects C as an immediate-sup-concept of Cli,at the same time, remove the connections between Cli and Co. 2) Insert the concept Ck=(Nk,Ir) that is generated when C intersects concepts in the QECL into the QECL, by the way, the judgment of the relationships of equivalence and father-son in the algorithm can be implemented according to Theorem 4.1 -4.4.

Attribute set A=(al,a2 ,... ,a,,) in the context (0,D.R).

partition generated by amibute a, can be expressed by (Oil/vil, Oi2/viz, ... , Oi&amp;M), where Oil/vij is a object set Oij of attribute a, whose value is vij, therefore, we can gain a array of initial quantitative extended concepts QC(ai)=((Ni~w=vi~), (Ni~,a,=vd,.. (Ni,,a,=vid) by attribute a,.

The generation algorithm of the QECL  Create-In-Attr(QECL) is below: I)  Initialization: To generate QECL with the complete concept (0, ( )) and the null concept ((),all) as only two concepts. 2) for I = l  to n do, for Ci EC(a,) do, Insertion(QECL, CJ.

7. Algorithm analysis Compared with Apriori algorithm, the fmding frequent itemsets algorithm base on QECL has some advantages: 1. On time complexity: Apriori algorithm employs an iterative approach known as a level-wise search, where k-itemsets are used explore (k+l)-itemsets. Its time complexity is O(2? ), In the worst case, since confiied conditions of the thresholds of support and confidence are added to mine association rules with Apriori algorithm, in practice, Its time complexity will be improved. However, time complexity of the QECL generation algorithm is exponential in n, In practice, the QECL has typically O(n2) or even O(n) rather than O(2 ?), resulting in a typical running time of o($) L7.81.

2. On storage complexity: There isn?t obvious difference, they need large storage.

they need large storage.

3. On association rules minimg: The Hasse diagram of QECL can show all frequent itemsets and their supports on itself while Apriori algorithm can not; on the one hand, when amihutes, objects, the conditions of association rule mining have been changed, we have nothing to do hut repeat if we use Apriori algorithm to mine association rules, However, if amihutes and objects have not changed, when the mining association rules conditions have changed, QECL will not he reconstructed again, we can mine association rules until they satisfy the user?s specified conditions; on the other hand, QECL can present association rules concise and distinct, while Apriori algorithm?s presentation is complicated and abstract.

8. A comwrison example of association rules mining with QECL and Aproiri-algorithm From the definitions and theorems mentioned above, we TID can mine association rules on a QECL, taking a typical no0 Example an lysis 6.1 on as page market 232 basket in 1 no0 literature L9?, There are 9 transactions and 5 Items in ALLELectronics transaction database D, that&gt;: M=9, we use data in Table 1 to generate QECL according to the definitions and theorems in Sections above, draw the corresponding Hasse diagram of QECL from the mm ALLELectronics transactions database D. as shown in Fig. 1.

By Apriori algorithm, association rules mining will he carried out as foUows: There are 9 transactions and 5 Items in ALLELectronics transaction database D, that is M=9, 26-29 August 2004 The Hasse diagram of QECL corresponding to the context in Table 1.

and we set the Minimum support threshold determined is 1/9, scan D for count of each candidate, frequent Itemsets CI: (III.IIzI~ l I ~ l , l b l . ~ 1 ~ l ,  whose the support of frequent  itemsets is 6/9,7/9,6/9, 2/9, 2/9, respectively, remove these frequent itemsets whose the support is lower than the minimum support threshold 18, we can easily obtain frequent itemsets LI: (I1].(I~Jr(I3],{b), (IS), generate C?

candidates from LI, frequent itemsets CZ: (IIIz), {1113), IIibI, IIihI, IIzhI. ~ ~ z b l ~ 1 ~ z ~ ~ l , l ~ ~ b l , l ~ ~ ~ s l ~ l ~ s l ~  scan D for count of each candidate, calculating thee support of thses frequent itemsets is 4/9, 4/9,1/9,2/9, 40 ,  2B, 2/9, 0, 1/9, 0, respectively, remove these frequent itemsets whose the support is lower than 1/9, we can easily get frequent itemsets Lz: [ 1 1 1 ~ 1 , ~ 1 ~ 1 ~ 1 , ~ 1 ~ 1 ~  ),I 1 .~1~b1,  [IzIsI,generate C3 candidates from b, scan D for count of each candidate C3: (111213). [IIIzI~),  calculating the support of C1 is 29, 2/9,respectively, then frequent itemsets L3 can be easily gained, then association rules mining has been finished.

However, in Fig.1, the support of frequent itemsets is not divided by M=9 in order to be convenient for explanation, we can easily obtain frequent itemsets, for example. such as the support of frequent itemsets [Il),[12]and[13] is 6/9, 7/9, 6/9 respectively, the support of frequent itemsets (III~),[I~I~).(I~I~).(I~~.bj is 4/9,4/9,4/9, 29,  respectively, the support of frequent itemsets ( I I I ~ I ~ ) , ( I ~ I ~ I ~ , I ~ ) ,  ( I12b] is 2/9, 2/9, 1/9, respectively etc.L9? Once the frequent itemsets from transactions in a database D have been found, we can generate association    database D have been found, we can generate association rules from them, for example, we can select some attribute at random, association rule 11 A 12qIs, the support of frequent itemsets (IJZJ and (II121s] is 4/9,2/9, respectively, then the confidence of association rule II AIz=&amp; is 2/4, as A Iz. whose confidence is U2, U2, U6, 217, 2/2, respectively.

Similarly, we can generate association rules on the Hasse diagram in Fig. ],for example, association rule II A the support of frequent itemsets (1112) and [I~IzIs, Is) is 4/9, U9 respectively, then the confidence of association rule I I A I z ~ I S  is 214, as such IzAIS*I~, II*IzAIs. 1 2 3 1 1 AI,, whose confidence is 2 2 ,  2/6, 2/7 respectively, However the association rule I z A I s ~ I l ,  Since concept (121s) does not exist, there is an common maximum-sub-concept (II121s, Is), whose support is 2, we can obtain the association rule IzAIs*Il,whose confidence is U2. The results are identified with Apriori in generating association rules.

Finally, from the same result of association rules Such IIAIs*Iz, IzAIs*II, I,*IzAIs, I z d i  AIS, Is I1 26-29 August 2004 mining base on QECL and with Apriori algorithm, we can reach a conclusion that association rules mining base on QECL has more advantages, the approach presents association rules visual, vivid and concise, improves mining association rules efficiency and avoids the blindness of mining association rules, Above all, user can interactively mine useful association rules on Hasse diagram of concept lattice according to his subjective interests [lo.l?l.

9. Conclusions In the paper, we discuss the algorithm of mining association rules base on QECL, Since frequent itemsets and their supports are shown on the Hasse diagram of QECL, it is more convenient for us to mine association rules, improve mining association rules pertinence, reduce searching space of the algorithm. Thus it is ohviously superior to Apnori algorithm, which need to scan databases  for a number of times, and sometimes repeat from scratch when the thresholds of support and confidence of association rules have been changed.

Acknowledgements The paper is supported the National Natural Science Foundation of P. R. China under Grant No.60273044 and the Natural Science Foundation of Anhui Province of China under Grant No. 01042201.

