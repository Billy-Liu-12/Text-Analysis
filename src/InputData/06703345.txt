Improved multi-level association rule in mining  algorithm based on a multidimensional data cube

Abstract?OLAP is a widely used data warehouse practical technology. It not only can be appropriate integrated detailed data, multidimensional view complete data and interactive queries, but also can be used in multi-dimensional analysis, providing analytical modeling tools to generate aggregated data, multidimensional data storage engine for trend analysis and statistical analysis. However, due to too many problems with relying on user input hypothesis, user preconceived problems and limitations severely limit the range of assumptions, which will affect the final conclusion. Compared to OLAP, DM Data mining is based on the various data sources, its analytical process is automatic, users do not need to make clear the problem requires when simply use the tool to dig out from the hidden data collection or potential data model  to make predictive analysis. So it is more conducive to find out the unknown but potentially useful information.

OLAP and data mining technology are both strengths, but also have shortcomings. If they can combine the advantages of organic development based on OLAP data cubes and data warehouse technology, a new data mining technology will better suit the actual needs. In order to achieve enhanced when combined with OLAP efficiency and flexibility purposes, this paper combines technology and association rule in mining algorithm together, and conduct an appropriate improvements cube at the same time.

Keywords?Apriori algorithmt; data cube; Apriori_cube; hash technology;items sets; frequent verb set;

I.  TRADITIONAL METHOD  A.  Apriori algorithm Apriori algorithm, also known as Breadth First or Level  Wise algorithm which is put forward by Rakesh Agrawal and Rmakrishnan rikant in 1994, It is almost the core content except for AIS algorithms and algorithms beyond SETM frequent sets discovery algorithm.

Rakesh Agrawal and others devised a basic algorithm and proposed mining association rules is an important way-set of ideas based on the two-phase frequency method, the association rule mining algorithm broken down into two sub- problems:  (1)To calculate all the satisfied minimum support minsup frequent sets obtained in D.

(2)The use of frequent sets generated to meet the minimum confidence of all association rules.

The first sub-question is the key algorithm, Apriori algorithm is based on the frequency set theory recursive method to solve this problem. Apriori algorithm use candidate set to find frequent set. Looking for frequent sets are found in the core part of the rules. Because in solving the frequent item sets L, the number of different sets of items is likely to reach 2m (m is the number of items I) And database transactions may also many. Therefore, it is almost impossible if a different set of items for the database are scanned for data supporting.

B.  Apriori_cube Algorithms Apriori_cube algorithm is based on improved Apriori  algorithm. Generation algorithm is based on the data cube, in which corresponding predicate set support count has been recorded in each box. Thus as long as the count-by-value data cube sweep seek their support, if its support is greater than the minimum support, the grid corresponding set of predicates (these items are from a different dimension to the dimension members) is a requirement for frequent predicate sets.

According to the algorithm process can be seen, if the data cube is poor, only a few item set is frequent, especially when the number of the item set increase, frequent item sets are very few. The Apriori_cube algorithm by using Apriori properties of candidate sets of clip, greatly reduce the length of the candidate set, measure the level of reducing the time and improve the algorithm performance. Of course if the data cube is very tight, the connection and cut time spent will be very long. In this case, Apriori_cube algorithm is difficult to show its efficiency, especially when a data cube is very tight. Take a long time for connection and pruning often can't significantly reduce the size of the candidate set. So it is obvious that using this way will do more harm than good.



II. PROBLEMS OF TRADITIONAL ALGORITHMS There are several problems in Apriori_cube based on the  above analysis of algorithms. These problems not only affect     the efficiency of the algorithm, but also affected the outcome of the correct rate:  (1)Since the warehouse data cube dimension is hierarchical, the rules dimension hierarchies must be judged in order to tap the user?s interest. Otherwise, when the user needs are very abstract, there may be an association rule mining have a lot of borings, contrary, sometimes the user?s needs are very specific, there may be less mining association rules.

(2) Apriori rules generate a lot of candidate item sets. These frequent item sets cost great store and count. Therefore, the improvement in the other direction is set to reduce the number of candidates. This can reduce the number of frequent item sets, improve efficiency of the algorithm.

(3) In practical applications, the basic association rule generation algorithm is too complicated to generate greater redundancy association rules. Here is a simple example: The rule A => B is the redundant rule of A  C => B. How to speed up the process of association rules generated rule generation and removing redundant rules, is a significant improvement of the algorithm to another component.



III. THE IMPROVED METHOD OF THE ALGORITHM (1)The use of concept hierarchy tree in the mining process  to dynamically adjust dimension hierarchy, in the mining process is to determine the level of dimension hierarchies, drilling operations using adjusted data cube dimension hierarchies, adjusted to continue digging. Overly abstract user requirements, such as dimension hierarchy is too high, you should use the drill operation reduces the dimension hierarchy.

Conversely, overly detailed user requirements, such as dimension hierarchy is too low, you should use drilling operation improves dimension hierarchy. Mining tasks identified in accordance with the user data cube dimensions and dimension level to generate the data cube.

If the user determines the m-dimensional, and hope that a multidimensional association rules mined can include the m- dimensional, to include only (m - 1) or less d association rules is not very concerned about. To the performance requirements to the mining algorithm of association rules, In the process of looking for frequent verb set, Can find frequently m-verb set.

According to the nature of the Apriori: frequent m-verb set, each subset m-m-verb set should be frequently. In any one dimension there are frequent m - verb set, when a dimension does not exist frequent m - verb set, frequent n - verb set can't dig it out. So users need mining multidimensional association rules. This paper will find the dimension after frequent m - verb set, also to judge it. If a particular d does not exist on the frequent m - verb set that d level is too low, Should use drilling operation improve d level; If a particular d on each m - verb set all frequent m - verb set that d level is too high, drilling operation should be used to reduce the dimension hierarchies.

(2) Use hash technology to optimize the mining algorithm.

By analyzing the Apriori algorithm, in a loop, the search to each transaction database after calculating the support of candidate item sets, can produce frequent item sets, the time space of the process cost price is very big. So the key to  improve efficiency is to generate a smaller set of candidate projects. In traditional Apriori algorithm of candidate item sets Ck+1 by Lk Lk generated. At this time of the Dg+1 is very big.

Uses hash technology, by removing unnecessary candidate item sets to reduce the number of Dg+1, decreases in the time space overhead and algorithm efficiency is improved.

When g is small, such as when g=2, the number of candidate item sets Dg is very large. Produced by Mg Mg, Dg quantity change of the amplitude is also very big. When using hash technology, the improvement is very great. However, the number of candidate item sets in the late algorithm Dg has decreased, change is very gentle, hash technology is need to use. If you use the hash technical efficiency reduce instead. So this article will hash technology is applied to the process of M1 generation M2, using the properties of hash to reduce the candidate 2-verb set. Here a brief introduction of basic content of hash technology.

Hash method is on the table between storage location and the key to establish a certain corresponding functional relationship. Each key correspond to a single storage location in the structure: Address=Hash(Rec.key).

In the search, the first function for the table of key code is calculated, the function values as a storage location, in the structure by the comparing table items of this position. The search success when the key is equal. In the table items are kept in accordance with the same function calculation storage location, and stored at this location. This method is the hash method. The hash method used in the conversion function is called a hash function. Hash function is a compressed image function.

Key collection is much bigger than hash address collection.

Therefore after the calculation of hash function, the different key is mapped to the same hash address, this leads to conflict.

For any kind of hash function also cannot avoid conflict, so choose a good solution to the conflict is very important. When should we construct the hash function due to the following requirements:  a) Hash function should be simple, in a relatively short period of time to calculate the results.

b) Hash function domain must include all of the need to store the key, if the hash table is allowed m address, its domain must be between 0 to m-1.

c) The address of the hash function calculated should be evenly distributed throughout the address space: If a key is randomly selected the key set a key code, hash function at the same probability should be able to take each value of 0 to the m-1.

In order to reduce conflict, hash table must be modified. If a hash table named HT has m address, changed m to barrels a day. The bucket, and hash address should be one-to-one correspondence, The bucket number i (0 i<m) is the hash address of i. Each barrel can deposit S tables. The tables of key are synonymous. If the two different key use hash table item-function to calculate the same hash address, creates a conflict, they can be placed in different position in the same bucket. Only when all S table item location within paragraphs with table and then join table overflow is produced in the     bucket. Usually the size of S is set smaller, so mostly use a sequential search in barrels. If set in the hash bucket addressing comes 0 to m-1 while joined a table item R2, we use its key as R2. By the Hash function Hash (R2.Key) calculation, we can obtain its storage barrels number j.

(3) Generation algorithm based on the rules of association.

Generation algorithm based on the disadvantage of traditional association rule is proposed in this paper based on the rule of association rule generation algorithm. The algorithm uses the nature of the frequent item sets through the depth of recursion method of generating frequent subset to accelerate the process of generating association rules at the same time reduce the redundancy of association rule generation.

Association rules are established or not according to the rules of confidence and minimum confidence level of the comparison results. For each verb set l frequently, find out the loophole all set, for each of these corners set s, if sup (l) and sup(s) ratio is greater than the minimum confidence level (s), which can be formed following a rule: s=> (l-s). There will be a subset of frequent item sets as rules (premise), before the remainder as rules after a (conclusion).

Against the rules of the former and later parts, there are two different improved methods. But the generation algorithm based on association rule is efficiency better than before which is based on association rule. A brief introduction of the principle algorithms are as follows:  Theorem 1: in frequent verb set l, the rule s=>(l-s) is true, if there is a subset of q(s q , the rules of q=>(l-q) must be established.

As theorem 1 is known for frequent verb set l, q=> (l-q) is the redundant rules of s=> (l-s). In the generated rules, in order to avoid this kind of redundant rules, we should calculate before a contains less predicate rules, if set up is no longer considered include the regulation of the predicate; if  it does not continue to set up calculation. Former algorithm based on the rule of is improved according to theorem 1.

Theorem 2: in frequent verb set l, the rules s=> (1-s) is true, if any verb frequently set n exists, s n l, the rule s=> (n-s) must be established.

As theorem 2 is known for frequent verb set n, l and frequent subset rules s=>(n-s) is the redundant rules of s=> (l- s). In the generated rules, in order to avoid this kind of redundant rules, generate candidate set of data items that can be used to produce strong rules, that is for a set of frequent items l ,After generate into a data item of the set of rules, and then after the use of the strong rule and the candidate item sets generation process gen_candidate(). That can be generated in l after all the pieces for the two strong rules of the candidate set of data items, Then, in turn, the recursive call themselves repeatedly, until the generated frequent rules of concentration of data items is so strong. A improved algorithm based on the rules after is according to the theorem 2.



IV. THE IMPLEMENTATION AND ANALYSIS OF IMPROVED ALGORITHM  (1)The implementation of  the improved  algorithm.

Implementation process is divided into two parts, the first part is the legislative body scan data to find frequent verb set Lk, the second part is on the basis of the verb set frequently generate strong association rules.

For the first part, the search for frequent verb set Lk, is divided into three steps:  Step 1: generate frequent 1-verb set, determine and adjust the dimension hierarchies, and generate candidate 2-verb set hash (H2), according to the minimum support to calculate the minimum support count s barrels.

Step 2: on the basis of the hash table H2, the candidate set to make use of the minimum support count S filtration, removal of those who do not meet the minimum support count of barrels, generate candidate verb set C2, and call the frequent predicate generation function generated L2.

Step 3: next in L: recycling Apriori algorithm on the basis of the candidate data generation method to generate candidate itemsets verb set, and generate the verb set frequently, finally get the Lk.

(2) The analysis of the improved algorithm a) In the improved algorithm, increased the size of the  support time is, the more the time used in the drilling operation. Because the size of the support time is a linear trend, the time of scanning database is:  , |Ck|=Cn k |L1,j|  |L1,j| is the number of frequent 1 ? item sets of j, tk on behalf of the drilling operation time, namely after scanning each time and increases the drilling operation.

b) The improved efficiency of mining algorithm was optimized by using hash is very obvious when g is equal to 2.

When D2 is generated from M M1 ,the number change very much, use hash technology has the obvious effect. Through the experiment, when M1 is large enough and take an appropriate support, uses in the original algorithm efficiency multiplied by a factor less than 1 mining to improve system performance.

Calculation formula is as follows: a[jj][j]=a[jj][j]+eomputel.eomputenum(back_li, xk) J stands for candidate item g in the ordinal number of  candidate for concentration, jj is multidimensional ordinal.

Back_li candidate item sets can hold every set of g, because of using the hash optimization method, which reduces the number of candidate item, eventually reduce the time consuming of the algorithm. But uses hash technology itself need more scanning the database again, So looking for frequent 2-verb set higher than the original algorithm of the time, but the effect of shrinking the size of its predicate candidate set is very obvious, makes the whole algorithm the execution time of the fall.

In actual operation, to more than about 80000 transaction data, the basic algorithm of Apriori algorithm consumes about machine time is about 6 minutes, After the hash algorithm used, the machine time is almost about 4.5 minutes, the reason is: hash technology reduce the two the number of candidate sets.

c) Generation algorithm based on association rule do process of generate strong association rules of redundant rules for processing, greatly reduce the number of the production rules, and based on the association rule generation algorithm is  better than the original algorithm of computing time, reduces the redundant rules to improve the efficiency of calculate.

CONCLUSION In order to overcome the existing multi-dimensional  multi-level association rule mining algorithm based on data cube, in this paper, the algorithm was improved, and obtained the certain effect.

But due to the limitation of time the work of the paper and there are many improvements and further research, basically has the following several aspects:  1) This article by multidimensional association rules mining based on multidimensional data cubes are built and stored in a data warehouse. But in practice, if the dimension is large, build data cube are often vary large, we need to compute the relationship between a large number of dimensions or details, this kind of data can?t be materialization in advance. So it is necessary to calculate the data cube online. In this paper, the cube in the calculation is using existing algorithms, not according to the specific circumstances of the cube calculation in-depth study.

(2) In the experiments, due to time relationship, this paper is based on data of legislative body multi-dimensional multi-level association rules algorithm to carry on the research and exploration, and failed to involve other mining algorithm. In fact, there's a lot of mining algorithm based on data cube, each mining algorithm has its own advantages and disadvantages, how to choose according to actual data mining algorithm is also need further study.

(3) In this paper, the OLAP and DM technology integration also belongs to the loose integration, and not fully realize the seamless integration of OLAP and DM technology. At the same time, on the premise of reasonable further optimization algorithm, the mining results add 3D graphic display function also takes time to complete the things in the future.

