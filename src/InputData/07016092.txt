A Bidirectional Process Algorithm For Mining  Probabilistic Frequent Itemsets

Abstract?Nowadays, frequent itemset mining is the major task in association rule mining. With the observation that the support plays an important role in  mining frequent itemsets, in this paper, we review the previous efficient algorithms and study the effect of different order of support in the performance of frequent itemsets mining algorithms and propose our improved schedule. In our new algorithm, items are sorted in descending order according to the frequencies in transaction cache while itemsets use ascending order of support during support count.

Compared with other algorithms, the results of experiments show that the new algorithm gains better performance on on well- known benchmark data sets.

Keywords?frequent itemset; support count;  Eclat algorithm

I.  INTRODUCTION Nowadays, frequent itemset mining is the major task in  association rule mining. In the past twenty years, great efforts have been dedicated to this field and researchers have made tremendous achievements in developing efficient and scalable algorithms for mining frequent itemsets in precise and certain databases.  All these algorithms can be categorized as variants of one of the three different base algorithms, Apriori[1], FP- growth[2] and Eclat[3].

However, in the emerging applications, we may face numerous research problems in the database on consideration.

These studies can be broadly divided into two categories: researches on  functionality and on performance.

As to researches on functionality, the core question considered is what kind of frequent itemsets should be found.

For example, probabilistic frequent itemsets should be found in uncertain databases, and approximate frequent itemsets are welcome in databases with partly missing items, sequential frequent patterns are available in time series databases.

Therefore, different applications represent different frameworks and thus focus on different problems.

As to studies on performance, the core problem considered is how to find the frequent itemsets as efficiently as possible.

with a generate-and-test schedule, Apriori-based algorithms look at small sets firstly and recognize large candidate frequent sets if all its subsets are frequent, thus eliminate most large infrequent itemsets. Various improvements to the basic Apriori  algorithm concentrate on reducing the number of scans over the database and reducing the size of the candidate k-itemsets.

The FP-growth algorithm compresses the frequent items into a frequent pattern tree and then divides the compressed database into a set of conditional datasets; each associated with one frequent pattern fragment, and mines each dataset separately.

The improvements of basic FP-tree concentrate on excess overhead of constructing many conditional FP-Trees during mining, because its weakness reduces the performance of FP- Growth algorithm as the patterns get longer and/or the support level gets lower. Eclat algorithm is an acronym of equivalence class clustering and bottom up lattice traversal. In Eclat, frequent items are generated by intersecting the tid-lists of all distinct pairs of atoms and checking the cardinality of resulting tid-list. A recursive procedure call is made with those itemsets which are found to be frequent at the current level. This process is repeated until all frequent itemsets have been enumerated without much pruning in Apriori or FP-growth algorithm. Obviously, the run time increases exponentially with a larger number of transactions which drops off the performance of Eclat algorithm eventually.

Based on previous work[4][5][6], in this paper, we reviewed the previous research of frequent itemsets mining in different database communities, both achievements and challenges. Then we propose our improvements on traditional Eclat algorithm and present a probabilistic frequent itemset mining algorithm with Eclat framework in databases of uncertain transactions, based on the possible world model.

Finally, we analyze its performances and test it on well-known benchmark data sets.

The rest of the paper is organized as follows. In next section, we define relevant terms and review several principles. In Section II, we describe the main idea of our new strategy. In Section IV, the improved algorithm on Eclat is presented and described. We then experimentally validate our approach on real and synthetic datasets in Section V and finally present some conclusions in section VI.



II. CONCEPTIONS Items and Support Count Let I={i1,i2,...,imax} be the set of  all items. Any subset X?P(I) of I is called an itemset. Let  * Corresponding author: Hong Wang Supported by Natural Science Foundation of China  (61373149).

DOI 10.1109/BWCCA.2014.122     ? ? P(I) be a multiset of itemsets, called transaction database, and its elements T={t1,t2,...,tn}?  ? is called transactions. Each transaction ti contains a subset of items chosen from I. An important property of itemsets is the support count, which refers to the number of transactions that contain a particular itemset. The support count Sup(X), for an itemset X can be expressed mathematically as follows[7]:  s u p ( X ) | { | , } |i i it X t t T= ? ?         (1)  Support determines how often a rule is applicable to a given data set and we say an itemset is frequent if its support count is equal to or bigger than the given integer called minimum support threshold (min_sup).

Expected Support Frequent Itemset Given an uncertain transaction database UDB which includes N transactions. An itemset X is expected support-based frequent itemset if and only if exsup(X) ? N?min_ sup, where min_sup is a minimum expected support ratio specified by user and the expected support of  itemset X is defined as:  i 1 ( )  N  iexsup(X) p X =  =? (2)  Probabilistic Frequent Itemset Given an uncertain transaction database UDB which includes N transactions, and a minimum support ratio min_sup. An itemset X is a probabilistic frequent itemset if and only if X?s frequent probability is larger than the probabilistic frequent threshold, namely[8]:  Pr(X)=Pr{sup(X) N?min_sup}>min_prob           (3) Here, min_prob is a minimum probabilistic support  threshold specified by user.

Apriori property All nonempty subsets of a frequent  itemset must also be frequent. As a corollary, we get that all supersets of an infrequent itemset are infrequent[9].



III. THE IMPROVEMENT ON ECLAT-BASED ALGORITHMS  A vertical tid-list of database format was created by Zaki in Eclat algorithm[3]. All frequent itemsets can be enumerated via simple tid-list intersection. Furthermore, a lattice-theoretic approach and prefix-based technique are introduced to decompose the original search space (lattic) into small sub- lattice. Finally, the bottom-up search strategy is applied to enumerate the frequent itemsets within each sub-lattice corresponds to a reverse lexicographic order.

A. The problem of the Eclat algorithm In Eclat algorithm, there is not clear boundary between the  process of support count and candidate generation. Some possible drawbacks can be seen during the implementation:  ? In Eclat algorithm, the computation is done by intersection of the tid sets of the corresponding k- itemsets. The tid sets can be quite long, which takes substantial memory space as well as computation time for intersecting the long sets, thus stress the available  main memory with increasing of length in frequent itemsets.

? In Eclat algorithm, the classes are solved  from bottom to top, corresponding to a reverse lexicographic order.

Here, the information on support count is available and may play an important role in pruning, but Eclat algorithm does not make full use of the information in sorting and reducing the useless operation which will be deleted in the support-based pruning.

? Eclat algorithm does not take full advantage of Apriori property in reducing the number of candidate itemsets explored during frequent itemset generation. Therefore, the number of candidate itemsets generated in Eclat algorithm is much greater than that in Apriori algorithm. The situation gets worse as the data set containing many dense and long patterns.

B. A property on support count Conjecture: A subset with high support has more  probability to become a part of the long frequent itemset; While a subset with low support count has less probability to become a part of the long frequent itemset.

Proof:  Let X?P(I) be the itemset which is composed of some items in I. The items in X can be sorted in different order: X={xmax,xmax-1,...,x1} means items in X sorted in descending order of support, while X={x1,x2,...,xmax}(max>=2) means items in X sorted in ascending order of support.

For n=1: We consider sup({x1})?sup({x2})?sup({xmax- 1})...?sup({xmax}) and X is a frequent itemset. Based on the Apriori or anti-monotone principle, we get the comparison on support as follows:  sup({xmax}?{xmax-1})?min(sup{xmax},sup{xmax-1})? sup({xmax-1})  sup({x1}?{x2})?min(sup{x1},sup{x2})?sup({x1})  Given sup({x1}) ? min_sup ? sup({xmax-1}), item xmax-1 is a frequent item while item x1 is  infrequent and should be pruned.

For sup({x1}) sup({xmax-1}), it is likely that sup({xmax-1}) has more chance to satisfy the inequity of sup({xmax-1}) min_sup.

That is to say, item xmax-1 has more chance to be a frequent item and used to generate the longer candidate itemsets. Thus with 1-item-extension of X, prefix xmax-1 has more chance of becoming a part of the long frequent itemset.

To verify the inductive step, we assume the truth of n=k. X is a frequent (k-1)-itemset and y is a frequent item, and the items in X can be sorted in different order: X={xk-1,xk-2,...,x1} means items in X sorted in descending order of support, while X={x1,x2,...,xk-1} means items in X sorted in ascending order of support. we get the k-itemset with 1-item-extension of X: Y1 {xk} ? X={xk} ? {xk-1,xk-2,...,x1},Y2 {y1} ? X {y1} ? {x1,x2,...,xk-1}, then we have the inequities of sup({y1}) ? sup({x1}) ? sup({xk-1}) ? sup({xk}) and sup({y1} ? X ? sup({xk} ? X).

This assumption leads us to the following: with sup({x1}) ?     sup({x2}) ? ... ? sup({xk-1}) ? sup({xk}) ? sup({xk+1}), we consider the (k+1)-itemset with 1-item-extension of X: Z1= {xk+1} ? {xk,xk-1,xk-2,...,x1} {xk+1} ? Y1 Z2= {z1} ? {y1,x1,x2,...,xk-1} {z1} ? Y2, where sup({z1}) ? sup({y1}) ? ...? sup({xk}) ? sup({xk+1}). Based on the Apriori or anti- monotone principle, we get the comparison on support as follows:  sup(({xk+1} ? Y1)?({xk} ? Y1)) ? min(sup({xk+1} ? Y1), sup({xk} ? Y1)) ? sup({xk} ? Y1) ? sup({xk} ? X) sup(({z1} ? Y2)?({y1} ? Y2)) ? min(sup({z1} ? Y2)), sup({y1} ? Y2)) ? sup({z1} ? Y2) ? sup({y1} ? Y2) ? sup({y1} ? X)  with the assumption of n=k, we have the inequity of sup({y1} ? X ? sup({xk} ? X). Now given the min_sup which satisfies sup({y1} ? X) ? min_sup ? sup({xk} ? X),  we can draw the conclusion that itemset {xk} ? X has the chance to be a frequent itemset while {y1} ? X has not. Because sup({y1} ? X) ? sup({xk} ? X), it is likely that sup({xk} ? X) has more chance to satisfy the inequity of sup({xk} ? X) ? min_sup. That is to say, as a frequent item, xk+1 has more chance to be used to generate the longer candidate itemsets.

Thus with 1-item-extension of Y, prefix xk+1 has more chance of becoming a part of the long frequent itemset.

Consequently, we now know from the Principle of Mathematical Induction that the conjecture holds for 1-item- extension of frequent itemset.

With the similar proof, we can prove the same conclusion on generation of frequent k-itemset with the intersection of any two (k-1) subsets. Therefore, we get a property on support:  Property: A subset with high support has more probability to become a part of the long frequent itemset; While a subset with low support count has less probability to become a part of the long frequent itemset.

C. support counting During support counting, in order to decide which  candidates are contained in the given transactions, a part of the equivalence lattices have to be traversed. In Eclat, it uses prefix-based equivalence relation along with bottom-up search. To reduce the expected number of unnecessarily visited atoms(items), first we have to check if the transaction contains the least frequent item since it is likely to provide the strongest filtering among the items of the candidate, i.e., this is the item that is contained in the least amount of transaction.

Next, the second least frequent item is advised to check, then the third, and so on. We expect the least amount of redundant checks and thus the best run-time, so we choose the order of items corresponds to the ascending order according to the supports.

Considering the influence of different orders on generation of frequent itemsets, based on the support property, we know that frequent itemsets generation prefers ascending order of support to reduce candidate itemsets or intersecting operations. Therefore we use ascending order of support  instead of lexicographic order in traditional Eclat during support count. And the simulation proved its effectiveness in section V.

Our improvement benefits traditional Elcat algorithm at least in two ways: The first, based on the Apriori or anti- monotone principle, our improvement finds those infrequent itemsets as early as possible and prunes or deletes them since infrequent items will not be part of any frequent set.

Furthermore, with the ascending order of support in atoms of the sets, frequent subsets with low support are checked firstly, then we can cease the intersecting operation at the infrequent point and the rest redundant intersections are avoid, because the resulting itemset can not be frequent since part of the intersection is testified incompetent.

D. Transaction caching I/O and string to integer parsing costs are reduced if the  transactions are stored in the main memory instead of disk.

This technique is used in Eclat as well as other algorithms. In the implementation of traditional Eclat, the lexicographic order on item is usually used. However, based on the support property and the Apriori property, we learn that a subset with high support count has more probability to become a part of the long frequent itemset. Thus the intersecting operation on atoms with high support may be more than that on the atoms with low support. That is to say, atoms with high support may be visited more frequently than the atoms with low support in transaction cache. Therefore, we suppose that transaction cache requires descending order according to the frequencies in order to be storage efficient. In this way can we get the less amount of comparison and the better run-time.

In summary, descending order is good for the compactness and efficient store, while ascending order results in a fewer redundant in support counting. Therefore, transaction caching uses descending order according to the frequencies while ascending order of support is used during support count in our improvement. These two requirements can be satisfied at the same time. The items are recoded according to ascending order of supports, but the items are stored descending in tid- list after being inserted into the cache. Since the efficient support count requires the items of the transaction to be stored ascending, we simply reverse each transaction when it is retrieved from the cache.



IV. THE BI-ECLAT ALGORITHM In this section, we describe the Bi-Eclat algorithm which is  an improved algorithm based on traditional Eclat firstly, then provide an example to illuminate the main idea of Bi-Eclat.

A. Bi-Eclat algorithm There are three steps in the algorithm as follows:  1) Represent the database in vertical format and sort the tid-list of atoms in ascending order of items: After a transformation from horizontal to vertical on-the-fly, we scan the tid-list of items successively and prune the item?s with count of its transactions less than min_sup. After the     computation of frequentness probability and comparison with the min_prob, we get the frequent items sorted with bidirectional process strategy.

2) Prune items and generate candidate probabilistic frequent itemsets: At first, we take advantage of the ascending order of items to discard infrequent items as well as reduce redundant intersection. Then an itemset is generated as a union of subsets of the sets of atoms, and its frequentness probability is computed with the Formula (3). In fact, by intersecting the itemsets with the same transaction in their tuples, we get all candidate k-itemsets efficiently thanks to the descending order of transaction counting.

3) Mine probabilistic frequent itemsets in candidate database Dk+1: With the Anti-monotonicity property in uncertain environment, some infrequent itemsets can be identified quickly. In addition, other pruning strategies are often used, such as pruning based on length of the tidlist and Chernoff Bound. After all k-frequent itemsets are found, a k- projected database Dk is generated, in which tidlists of k- frequent itemsets consist of transaction identification ti and itemset?s frequentness probability px(ti). In this way, our improved Eclat-based algorithm recursively mines all approximate probabilistic frequent itemsets in the uncertain database of D.

The main pseudo code for Eclat-based PIM algorithm is as follows:  Algorithm: Eclat-based PIM Input: vertical tid-list database with bidirectional process  strategy Output: frequent itemsets Begin For all atoms Xi ?  S do Ti=? For all atoms Xj ?  S, with cnt(Xj)>cnt(Xi) do //prune itemsets with cnt(X)<minsup(X)  R=Xi ? Xj; tid-list(R)=tid-list(Xi)? tid-list(Xj); DPEclat(X); If Px[i,j]? min_prob  S=S ? {R}; Ti=Ti ? {R}; End If  End For While Ti ? ? do Eclat-based PIM(Ti)  End For End In the process of Eclat-based mining, the candidate  generation and counting phases perform in a single step. This is done because the PIM algorithm offers natural pruning of irrelevant transactions as a result of an intersection. Another feature of the mining operation is the utilization of the independence of classes, where each frequent item is a class that contains a set of frequent k-itemsets. All of these gain good performance for the Eclat-based mining algorithm. Therefore, the computation of the new approach achieves at most O(|D|*min_sup) time  and O(|D|) space.

Fig.1.  Tid-list for Items  B. an example of Bi-Eclat The Bi-Eclat algorithm is illustrated by the following  example.

Table 1. Attribute Uncertainty Model of Data Sets  IID  Transaction List B T1(0.5) T3(0.2) E T5(0.3) T2(0.25) T4(0.2) A T2(0.7) T1(0.6) T5(0.5) T3(0.3) C T3(0.8) T4(0.7) T5(0.7)T1(0.3) D T2(0.8) T1(0.5)T3(0.4) T4(0.3)   Considering the influence of different orders on generation  of frequent itemsets, based on the Anti-monotonicity property proved above, we learn that frequent itemsets generation prefers ascending order of itemset counting to reduce candidate itemsets or intersecting operations in computation.

For example, we combine AB and AE to obtain the frequent itemset ABE.

Pr({ABE})=Pr({AB}?{AE}?{BE}) =Pr({T1,T3}?{T2,T5}?? )  At this time, if alphabetical order is applied we have to run the calculation on itemset {AB} and {AE} before we realize that itemset {BE} is not frequent, thus redundant steps on intersection and comparison are exerted which has no contribution to the generation of super-itemsets. Therefore we use ascending order instead of alphabetical or lexicographic order in Eclat during frequent itemsets generation. In this way, frequent subsets with low itemset counting are checked firstly, then we can cease the intersecting operation at the infrequent point and the rest redundant intersections are avoid according to Anti-monotonicity property. Moreover, pruning technology based on itemset counting is also useful to raise the efficiency of the mining process.

Furthermore, based on definition and the recursive relationship in equation (3) , we calculate the frequent probability Pr?i,j(X) by summing up the required frequent probability with Pr?i,j(X) and Pr?i,j(X) iteration until i reaches min_sup. So the probability with higher value contribute more to the results than the lower one. For instance, in the calculation of Pr(AC) in section 2, Pr{sup(AC)=2}=0.14484 makes the largest part of contributions to the results of Pr(AC), while Pr{sup(AC)=3}=0.01512 makes little contributions.

That is to say, the contributions of the frequent probability to the result is proportional to the values of them. Therefore, it is necessary to sort the transactions of the same tuple in descending order according to their frequent probabilistic, thus obtain the result of Pr?i,j(X) as efficiently as possible since we cease the computation as soon as the value of Pr?i,j(X) reaches min_sup. In this way we can get the fewer amount of computations and then the better running time.



V. EXPERIMENTS We evaluate our Bi-Eclat algorithm by applying it on the  public databases downloaded from the FIM repository[10].

Seven different minium support values were used for each database. Results from different databases sometimes reveal similar performance on the algorithms, so only the most typical ones are shown in the figure.

The tests were runed on a PC with a 2.5 GHz Intel(R) core(TM) i5-2520M processor and 4 Gbytes of RAM. The operating system was Window 7 and ubuntu Linux 12.04.2.

Based on the present Eclat algorithm[11],we designed the improved Bi-Eclat algorithm and obtained the results, then described them with mapping software.

TABLE 1.SOME CHARACTERS OF THE DATABASE IN OUR EXPERIMENTS  Database Items Avg.lenth Transaction  Mushroom 119 23 8,124  Chess 75 37 3196  Connect 129 43 67,557  Accidents  468 33.8 340,183  Retail 16469 10.3 88,126  T40I10D100k 943 40.61 100,000  T10I4D100k 871 11.1 100,000    A. Experiments on Dense Database  At first, we used 3 different orders to test the effect of ordering: support count order in Bi-Eclat algorithm, reverse support count order in Bi-Eclat algorithm and the lexicographic order in traditional Eclat algorithm. The results in dense databases such as Mushroom are shown in Fig. 2.

With little difference in memory occupation, experiment results show that running time is greatly affected by the ordering used. Experiments with Eclat algorithm with different orders meet our expectations: support count order in Bi-Eclat algorithm leads to the smallest running time, while reverse support count order in Bi-Eclat algorithm leads to the highest. This agrees with the heuristic: descending order is  good for the compactness and efficient store; ascending order results in fewer redundant steps in support counting.

Fig 2. Support Comparison on   Database of  Mushroom  B. Experiments on Moderate Database We also performed several experiments on a moderate  database of accidents. As can be seen from the figure (Fig.3), Bi-Eclat algorithm gains much advantage in running time, though it is slighter than that in the databases of mushroom and connects. The same as before, reverse Bi-Eclat algorithm showed the worst performance in the three algorithms.

Fig 3.  Support Comparison on   Database of  Accident   Some possible reasons can be drawn from the experiments.

We use the ascending order in frequent itemsets generation, and the items at the beginning are frequent items with lower supports than the items following. This means that the most selective items are checked at the beginning during support count and the support count is terminated as early as possible.

Otherwise, when descending order is selected during support count, many items with higher support are visited and further search is needed in finding frequent itemsets. Therefore the search will not be terminated as immediately as in the case of the ascending order and the redundant steps will be generated.

C. Experiments on Sparse Database      Fig.4. Support Comparison on   Database of T10I4D100K  Some other experimental results we got on sparse database. As we can see in Fig.4, on the dataset of T10I4D100K, Bi-Eclat algorithm loses the competition with reverse Bi-Eclat algorithm at one or two point of support count.

It may indicate that the Bi-Eclat algorithm is not suit for special sparse database, which is the future work we concentrate on.



VI. CONCLUSION AND FUTURE STUDY In this paper, with the observation that support count play  an important role in frequent item mining, we proposed an improved algorithm based on the traditional Eclat algorithm.

The new Bi-Eclat algorithm sorts in descending order according to the frequencies in transaction cache while use ascending order of support during support count. Compared with Eclat algorithm, the results of experiments show that the improved algorithm gains better performance on several databases given.

In this paper, we have assumed that transaction database and tid-list will fit in main memory. But the assumption does not play in massive datasets and online mining of frequent itemsets which will stress the available main memory. To extend this algorithm to massive datasets, we plan to make further research on the algorithm, then utilize and develop the previous studies into emerging applications.

REFERENCE [1] R. Agrawal and R. Srikant. Fast algorithms for mining association rules.

Bases(VLDB?94). pp.487?499.September 12-15, 1994.

[2] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without candidate generation. Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data, ACM Press, pp. 1?12, 2000.

on Knowledge and Data Engineering, Vol.3, pp.372?390, 2000.

[4] Zhou Hai-yan An algorithm of frequent item sets mining based on transformation of the frequent item linked Journal of Chinese Computer Systems, Vol.7, pp.1254-1257, 2008  [5] Tong Y., Chen L., Cheng Y., and Yu P. S, Mining Frequent Itemsets over Uncertain Databases. In: VLDB?12, Istanbul, Turkey, Vol. 5, No.

11, pp. 1650?1661, August 27th ,2012  [6] Xiong Zhong-yang, Chen Pei-en, Zhuang Yu-fang. Improvement of ECLAT algorithm for association rules based on hash Boolean matrix.

Application Research of Computers, Vol.4, pp.1323-1325, 2010  [7] Pang-Ning Tan, Michael Steinbach, Vipin Kumar. Introduction to Data Mining. Pearson Education Asia Ltd. pp. 201-223, 2011.

[8] Thomas Bernecker, Hans-peter Kriegel, Matthias Renz, Probabilistic Frequent Itemset Mining in Uncertain Databases, In: KDD?09, Paris, France, June 28-July 1, 2009.

[9] Jiawei Han, Micheline Kamber. Data Mining Concepte and Techniques, Second Edition. Elsevier(Singapore)Pte Ltd. pp. 146-160, 2011.

[10]  http://fimi.ua.ac.be/, 2012.

[11] http://www-users.cs.umn.edu/~huix/link/fim.htm, 2012.

