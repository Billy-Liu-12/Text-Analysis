Analyzing the Interestingness of Association Rules from the Temporal Dimension

Abstract Rule discovery is one of the central tasks of data mining.

Existing research has produced many algorithms for the purpose. These algorithms, however, often generate too many rules. In the past few years, rule interestingness techniques were proposed to help the user find interesting rules. These techniques typically employ the dataset as a whole to mine rules, and then filter and/or rank the discovered rules in various ways. In this paper, we argue that this is insufficient. These techniques are unable to answer a question that is of critical importance to the application of rules, i.e., can the rules be trusted? In practice, the users are always concerned with the question. They want to know whether the rules indeed represent some true and stable (or reliable) underlying relationships in the domain. If a rule is not stable, does it  show any systematic pattern such as a trend? Before any rule can be used, these questions must be answered. This paper proposes a technique to use statistical methods to analyze rules from the temporal dimension to answer these questions. Experimental results show that the proposed technique is very effective.

6. Introduction The objective of data mining is to find interestinghseful knowledge for the user. Rules are an important form of knowledge. Existing research has produced many algorithms for rule mining. These techniques, however, often generate too many rules, and most of the rules are of no use to the user [22, 23, 151. In the past few years, a number of rule interestingness techniques were proposed to deal with the problem [12, 13, 14, 15, 21, 22, 23,4, 16, 24, 25, 281. They typically use the whole dataset to mine rules and then filter andlor rank the discovered rules in various ways to help the user identify interestinghseful ones. These techniques, however, have a major shortcoming. They are unable to answer a question that is crucial to the application of rules, i.e., can the rules be trusted or will the rules hold in the future? A rule has no use if it does not hold in the future or is not predictive.

In this paper, we argue that an important aspect of the rules has not been studied, i.e., the temporal aspect.

Analyzing rules from the temporal dimension is important  for real-life applications. In the past few years, we used a number of interestingness techniques to help our users find interesting rules [13, 14, 151. Although these techniques helped a great deal, our users often still felt uncomfortable. The reason is that the users are concerned with the reliability of the rules. They want to know whether the rules will hold in the future, i.e., whether the rules indeed represent some stable (or reliable) underlying relationships in the domain. If a rule is not stable, does it show any systematic trend? The inability of the existing approaches to answer these questions lies with the fact that they do not study the dynamic behavior of rules. In this research, we analyze the interestingness of rules from the temporal dimension to solve the problem. This approach makes sense as most datasets are collected over time and represent the behaviors of the application domains in different time periods.

In this research, we focus on analyzing association rules. Association rule mining is commonly stated as follows [3]: Let I = { i,, . . ., in] be a set of items, and D be a set of data tuples (records). Each data tuple consists of a subset of items in I .  An association rule is an implication of the form X + Y,  where X c I ,  Y c I ,  and X n Y = 0.

The rule X + Y holds in D with confidence c if c% of data tuples in D that support X also support Y.  The rule has support s in D i f s% of data tuples in D contains X U Y.  Given a set of data tuples D (the database), the problem of mining association rules is to discover all rules that have support and confidence greater than or equal to the user-specified minimum support (called minsup) and minimum confidence (called minconj).

This paper presents a set of statistical methods to analyze the behavior of rules over time. The basic idea is as follows: The dataset is first partitioned into a few blocks or sub-datasets corresponding to the time periods (e.g., years, months or weeks) in which they were collected. The granularity of the time period is application dependent. We then mine rules from each block. Since the amount of data available is usually large, the partitioning does not lose significance of the rules discovered. After all the rules are found, we analyze the supports and  0-7695-1 119-8/01 $17.00 0 2001 IEEE 377    confidences of the rules in these time periods, which allow us to answer the above questions and to find various types of important rules, e.g., 0 Stable rules: These rules do not change a great deal  over time. Stable rules are more reliable and can be trusted. That is, they can be safely used in real-world performance tasks.

Trend rules: These rules indicate some underlying  systematic trends. If the trend rules are known, the user can take necessary actions to exploit desirable trends, and to reverse/delay undesirable trends.

Our experiments and practical applications show that the proposed technique is very effective. Testing on unseen future data confirms that the stable rules and the trend rules identified by our method are indeed reliable. Our experiments also indicate that although the number of rules generated from the data can be large, the number of stable rules and those rules that show some trends is actually quite small. It is possible to manually inspect them to identify those truly useful/actionable ones.

2. Related Work Existing research on rule interestingness focuses on a few directions, namely, template matching, unexpected rule identification, rule summarization and organization. We discuss them below in turn.

[ 121 proposes a template-based approach for finding interesting rules. This approach first asks the user to specify what rules he/she wants. The system then finds those matching rules.

[15, 23, 211 propose to used user expectations or beliefs to help himher find unexpected rules. In this approach, the user first input his/her existing knowledge about the domain, and the system then finds those conforming and unexpected rules.

[ 141 presents a technique to summarize the discovered association rules using a small subset of the rules. [13] presents a technique to organize the discovered rules in such a way that they can be easily browsed by the user.

[4, 25, 18, 101 report a number of methods for ranking the discovered rules using some statistical interestingness measures.

All these techniques are different from our work as none of them analyzes rules over time. Thus, they are unable to answer the rule stability or reliability question.

[2] proposes to mine and to monitor rules in different time periods. The discovered rules from different time periods are collected into a rule base. Ups and downs in support or confidence over time (called history) are represented and defined using shape operators. The user can then query the rule base by specifying some history specifications. This is related to our work. However, there is a major difference. We analyze rules generated over  different time periods using statistical tests rather than simply matching user specifications. By using statistical tests, we can formally analyze the significance of ups and downs of support and confidence. User specifications lack statistical foundation. One will not know whether the changes are significant and worth investigating.

[6] presents a framework for measuring changes in two models. The difference between the two models (e.g., two sets of itemsets, one generated from dataset D1 and one generated from dataset D2) is quantified as the amount of work (e.g., difference in supports) required to transform one model into the other. In a related work, [5] finds the support differences of association rules mined from two datasets and uses the differences to detect emerging patterns. These are different from our work as we analyze rules over a number of time periods, while [6, 51 only compare the supportskonfidences of rules from two periods. Our method uses statistical methods to identify significant changes, trends and stable rules. Both [6, 51 do not perform these tasks.

3. Basic Steps of the Proposed Approach This section gives an overview of the proposed approach, which consists of 3 steps: 1 .  Partitioning the dataset: The original dataset D is  first partitioned vertically into a number of blocks (or sub-datasets) DI, D2, ..., D, according to the time periods, T I ,  T2, ..., T,, in which they were collected, e.g., years or months.

2. Mining rules from sub-datasets: W e  then mine rules from each sub-dataset D,. Let the set of rules mined from D, be RI.  The set R of rules that will be analyzed in the third step below is defined as follows:  This means that if a rule appears in any rule set RI it is considered a potentially interesting rule. The reason that we consider all such rules will be clear later.

Clearly, a rule r in R may appear in RI but not in RI (i # j )  because r may not satisfy minsup and/or minconf in DJ. For our analysis later, we need the supports and confidences of every rule in R in all time periods (or all 0,). Thus, the missing support and confidence information in certain time periods for each rule needs to be obtained. This can be done quite easily. We can first mine rules from each sub-dataset D, to produce R .  W e  then scan the data D once to obtain all the missing supports and confidences.

Note that in rule mining, pruning is also performed to remove those insignificant rules. We use the technique given in [14].

Analyzing rules over time: After all the necessary  information about supports and confidences of each rule r in R in all time periods is obtained, we analyze  R = { r I r E (RI  U R2 . . .U R,) )  3.

the rules to give the user different types of interesting rules. A number of rankings of rules according to different statistical measures are performed to enable the user to view those most interesting rules first.

In the next section, we discuss step 3, which is the focus of this paper. Step 1 and 2 will not be discussed further as they are fairly straightforward.

4. Analyzing Rules Over Time We now present a number of statistical tests to analyze the discovered rules in R from different perspectives. We assume that the confidence value (denoted by coni) and the support value (denoted by sup;) of each rule in R in each time period T; has been obtained by a mining algorithm. Our analysis aims to identify:  semi-stable rules stable rules rules that exhibit trends.

Note that the statistical tests presented here are by no means the only tests that can be performed on rules. In fact, many other tests may also be used for various purposes. We also point out that the kinds of datasets, and the problems, dealt with here are different from time series data. In the latter, a given population is tracked over time. For our case, although the behavior of rules over time is of interest, each time period typically corresponds to a different population.

4.1 Semi-stable rules Intuitively, a rule r in R is a semi-stable rule if none of its confidences (or supports) in the time periods, TI, T2, ..., Tq, is statistically below minconf (or minsup). If some observed confidence conJ (or support sup,) of the rule is less than minconf (or minsup), it may be due to chance.

Semi-stable rules are in contrast with stable rules which have more stringent conditions (see the next sub-section).

Definition (semi-stable confidence rules): Let minsup and minconf be the minimum support and the minimum confidence, confD and supD be the actual support and the actual confidence of a rule r obtained from the whole dataset D, coni be the actual confidence of the rule in the time period T,, and a be a specified significance level. The rule r is a semi-stable confidence rule over the time periods, TI, T2, ..., Tq, if the following two conditions are met: 1. supD 2 minsup and confD 2 minconf.

2 .  For each time period (or sub-dataset), we fail to  reject the following null hypothesis at the significance level (y:  Ho: conJ; 2 minconf Note that the first condition is to ensure that the rule r satisfies the user specified minsup and minconf thresholds  in the whole dataset. To test the hypothesis in the second condition, we can use the statistical test on a single proportion [26, 171.

Test statistic  Consider testing the following null hypothesis (Ho) against the alternative hypothesis ( H I ) :  Ho: P 2 Po H 1 : p c p o  where p o  is the given proportion. The test statistic z (with standard normal distribution) for the sample proportion, b, is:  where n is the sample size. If z is less than its critical value at the significance level a, we _reject the null hypothesis. In our case, po  is minconf a d  is coni.

Example 1: Assume in a mining session we set minconf to 50%. There is a time period Ti in which the confidence conJ of a rule, A + B, is 45%. There are 300 data tuples that satisfy the condition A, and out of these 300 tuples 135 tuples also satisfy the consequent B (i.e., coni =135/300 = 45%). The question is whether conh is statistically below 50%.

We solve the problem as follows: From the problem description, we obtain the population size n = 300, = 45% and po = 50%. We use a = 5%. The null and alternative hypotheses are:  Ho: p 2 0.50 Hi:  p < 0.50  The critical value for z is - 1.65 at a = 5%, which can be obtained from statistical tables. Let us compute z,  fi  - P O  = 0.45 -0.5 z = -  = -1.73 0.0288675  The observed value of the test statistic z = -1.73, which is less than the critical value of z = -1.65.

Therefore, we reject Ho, and accept the alternative hypothesis HI. In other words, the difference between the sample proportion and the hypothesized value of the population proportion is large and this difference is unlikely to have occurred due to chance alone.

We now define semi-stable support rules.

Definition (semi-stable support rules): Let minsup and  minconf be the minimum support and the minimum confidence, confD and SUPD be the actual support and the actual confidence of a rule r obtained from the whole dataset D, sup; be the actual support of the rule in the time period T,, and a be a specified significance level. The rule r is a semi-stable support rule over the     time periods, TI, T2, ..., Tq, if the following two conditions are met.

1 .  supD 2 minsup and confD 2 minconf.

2. For each time period (or sub-dataset), we fail to  reject the following null hypothesis at the significance level a:  Ho: sup; 2 minsup Here, again we can use the statistical test on a single proportion to test the null hypothesis.

Computation complexity: Let the number of rules in R be IRI, and the number of time periods be q. The complexity of semi-stability test is O(qlRI). Since q is normally very small, and IRI is very large (in thousands or more), the computation is thus linear in IRI.

4.2 Stable rules A semi-stable rule only requires that its confidences (or supports) over time are not statistically below minconf (or minsup). However, the confidences (or supports) of the rule may vary a great deal. Hence, the behavior can be unpredictable. In practice, the user often wants rules that are stable over time, i.e., with predictable behaviors. We call such rules stable rules. Intuitively, a stable rule is a semi-stable rule and its confidences (or supports) over time do not vary a great deal, i.e., they are homogeneous.

Since both confidence and support are population proportions, we need a statistical test for population proportions. The Chi-square test [26, 171 is a popular choice for testing homogeneity of multiple proportions.

Definition (stable confidence rules): Let minsup and  minconf be the minimum support and minimum confidence, con$ be the actual confidence of a rule in the time period T,, and a be a specified significance level. The rule r in R is a stable confidence rule over the time periods, T I ,  T2, ..., Tq, if the following two conditions are met: 1. r is a semi-stable confidence rule.

2. W e  fail to reject the following hypothesis at the  significance level a: Ho: con5 = con& = . . .= confq  Test of homogeneity: A test of homogeneity involves testing the null hypothesis (Ho) that the proportions, p I , p2 ,  . . ., Pk, in two or more different populations are the same against the alternative hypothesis ( H I )  that these proportions are not the same. That is,  Ho: pi = p2 = . . .= pk HI: the population proportions are not all equal.

We assume that the data consists of independent random samples of size nl ,  n2, ..., nk from k populations. The data is arranged in a 2xk contingency table (Figure 1). The numbers XI, x2, . . ., xk, nl-xl, n2-x2, . . ., nk-xk listed inside  Successes Failures  I I Samde I 1 2 ... k  x1 X2 ... xk nl-xl nl-xz ... nl-xk  3 80    TI T2 T3 satisfy A A B 450 (454.5) 400 (334.2) 420 (481.3)  satisfy A A 7B 230 (225.5) 100 (165.8) 300 (238.7)  test statistic 2 is 62.7 I .  If we use the significance level of 5%, the critical value for 2 is 5.99 with 2 degree of freedom (df = (2-1)(3-1) = 2). The observed 2 value is much larger than the critical value. Thus, we reject the null hypothesis, and conclude that the confidences of the rule over the three time periods are significantly different.

The above example tests the homogeneity of confidences of a rule over time. We can also use the same method to test the supports of a rule.

Definition (stable support rules): Let minsup and minconf  be the minimum support and minimum confidence, supi be the actual support of a rule r in R in the time period T,, and a be a specified significance level. The rule r is a stable support rule over the time periods, TI, T2, ..., T,, if the following two conditions are met: 1.

2.

r is a semi-stable support rule.

We fail to reject the following hypothesis at the significance level a: Ho:  SUP^ =  SUP^ = . . .= supy  After a set of stable (confidence or support) rules are found, ranking can be performed according to the mean confidence or the mean support of each rule over time.

This ranking allows the user to see rules that have higher average confidences or supports first.

Computation complexity (without considering the final ranking): Since the number of cells in a contingency table is 2q, the computation complexity of stability test is O(qlRI). Since q is small and IRI is very large, the computation is linear in IRI.

Col. Total  4.3 Rules that exhibit trends In many applications, users are interested in knowing whether changes in support or confidence of a rule over time are random or there is an underlying trend. A statistical test called the run test [ 111 is often used to test whether a sequence of data is from a random process or exhibits trend. Below, we give a brief description of the run test in our application context.

Assume we have a rule that has the following confidences for TI, . . ., T6.

' One of the main assumptions of the 2 test is that the expected frequencies are not be too small [8]. The rule of thumb is that the 2 test is appropriate only when no expected frequency is less than 5. This seldom happens in our case as we normally deal with large datasets. When an expected frequency is below 5 , extensions of the Fisher's exact test may be used, which, however, is quite cumbersome and not popular [ 8 ] .  In our application, we find that for stability test the user is willing to give a range such that if the confidences or supports are within the range, the rule is considered stable.

Time confidence change TI 70%  75 % 1 } Run 1 T2 T3 79% T4 82% + 7-5 78% - 3 Run2 T6 81% + 3 R u n 3  The direction of change in successive observations is shown by plus and minus signs. A run is a succession of plus or minus signs, surrounded by the opposite sign. If we read from top to bottom, we would say that we have a run of three +s, followed by a run of one -, and followed by a run of one +. In our example, there are 3 runs: one run of length three and two runs of length one.

The above sequence may suggest a trend toward increasing values of confidence. That is, these values of confidence may not reasonably be looked upon as being the items of a random sample.

The exact run test for trends is based on the probability density function (p.d.f) of W, the number of runs in a combined ordered sample consisting of two types of items, X and Y,  say. Let m be the number of X items, and n be the number of Y items. Then as shown in [ 1 I],  the p.d.f of W is given by: when W is odd:  when W is even:  Pr( w = 2 k ) = ( - 1 ][ " - I]/[ " ,' ) k - 1  k - 1  where 2k and 2k+l are elements that belong to the space of W, and k is a positive integer. When k = 0 (i.e., W = l), we need special treatment. There are two cases: 1 If m = 0 o r n  = 0, Pr(W= 1) = 1. That is, if m=O or n  = 0, one run is the only possibility. This case clearly indicates a trend.

If m # 0 and n # 0, Pr(W = 1) = 0. That is, if m # 0  and n # 0, it is not possible to be one run.

We are interested in testing: Ho: sequence generated by a random process H I :  sequence exhibits trend  The critical region of this test is of the form W I v. v is determined by computing a = Pr(W I v; Ho), where a is the significance level (normally 5% or 10%). Let WO denote the observed number of runs (e.g., in our above     example, WO = 3). If WO is greater than v, we fail to reject the null hypothesis. In the computation, we do not need to find the value of v. We can simply do the following: We compute P = Pr( W = 1) + Pr( W = 2) + . . .+ Pr( W = WO). If P I a, we reject Ho, and accept the alternative hypothesis HI: sequence exhibits trend.

When m and n are small, it is easy to compute the probability. When they are large, the distribution of W can be approximated by a normal distribution [ 111 with mean  mn m + n  p = 2 -  + 1  (P - 1)(P - 2) and variance cr? =  W e  can then use the standard normal approximation to test our hypothesis:  m + n - 1  w - P  I=-  The run test above finds those rules that exhibit trends.

However, it does not tell the types of trends. W e  rank the rules according to the number of +s in each of them to show whether a rule is more likely to have an upward or a downward trend. Thus, the rules on the top are more likely to show upward trends, while the rules at the bottom are more likely to show downward trends.

Computation complexity (without considering the final ranking): Since ?m (or n)  choose k? and finding the numbers m and n can be done in linear time (in q), the complexity of the trend analysis is O(qWoIRI). Since both q and WO (WO S q)  are small, the whole computation is linear in IRI. When q is large (more than 20 [ l  l]), we can use the normal approximation. Then, the complexity becomes O(qlRI) as z can be computed in O( 1).

5. Evaluation We now report our experiment results. The objective is to show the effectiveness and efficiency of the proposed technique. We used 4 real life datasets, one from a vehicle insurance company, one from an education institution, and two from another education institution. We did not use public domain datasets such as those in UCI machine learning repository [ 191 because most of these datasets are not time or sequence related. For those datasets that have sequence, we do not understand them and thus do not know how to partition them meaningfully.

a. Finding different types of rules Table 1 (next page) shows the datasets and the results of rule generation and statistical tests. Column 1 gives the name of each dataset. Column 2 gives the number of tuples in each dataset. Column 3 shows the number of partitions of each dataset. In our case, each partition contains one year of data. Column 4 gives the number of  rules generated from all partitions or blocks, i.e., the size of R .  Column 5 gives the number of rules generated from the original dataset D. Note that the minsup and minconf values were suggested by our users.

From column 4 and 5, we can see that the number of rules generated from each dataset is very large. Column 6 gives the number of rules left after pruning from all partitions or blocks. Column 7 gives the number of rules after pruning for the whole dataset D (we used the pruning method in [14]). From column 6 and 7, we observe that the number of rules after pruning is substantially smaller for each data. Column 8 gives the execution time (in seconds) for rule generation. Column 9 gives the execution time for statistical tests (running on Pentium I1 350 with 128MB RAM). We see that statistical tests can be done very efficiently.

The following two tables (Table 2 and 3) show the statistical test results *. W e  used the significance level of 5% (a commonly used level [26, 171) in all our statistical tests. In column 2 of both Table 2 and Table 3, we reproduce the number of rules to be analyzed for each dataset, which is shown in column 6 of Table 1. Table 2 gives the results of the analysis on rule confidence. W e observe that although the number of discovered rules from each dataset is large, the number of stable confidence rules (column 3) and the number of rules that exhibit trends (column 5 )  are actually quite small. Column 4 gives the number of semi-stable rules (which does not include the stable rules). We observe that the education domains are quite stable as most of the rules are either stable or semi-stable. The insurance domain is, however, much more volatile. A large majority of rules are unstable. Rule support analysis given in Table 3 shows similar results. Since the number of stable rules and trend rules are not that large, manual analysis is possible.

5.2 Accuracy tests The main aim of our technique is to find rules that can be trusted and used in the future. Thus, we need to see whether the stable rules identified will still be stable and trend rules will still maintain their trends in the future. For these experiments, we make use of unseen test sets. Since we partition each dataset into q blocks, we use the first q- 1 blocks to find stable rules, semi-stable rules and trend rules. We then test them on the unseen qth block.

For comparison, we use mean squared error and mean absolute error to evaluate stable rules, semi-stable rules and the rest of the rules (called unstable rules). We will use the mean (expected) confidence (or support) of each  In Chi-square test, it is assumed that the populations are independent. For the 3 education datasets, this is hue as the data from different years represent different student populations. In the insurance case, the dataset records those insurers who had accidents and made claims. It is also reasonable to assume that the data From different years are independent.

Table 1 : Application datasets and the generated rules   CONF  2 3 4 5  all rules stable stable trend semi-  lnsur  Edu2-2  1 2 3 4 semi-  SUP all rules stable stable  rule over the q-1 time periods as the predicted confidence (or support) of the rule in the new (qth block) data. We want to see the error produced by each type of rules on the test data. Mean squared error (MSE) and mean absolute error (MAE) are commonly used in statistics to evaluate model accuracy. They are defined as follows:   trend  c I Y ,  - l? I MAE = c (Y, - f )  S MSE = S  where Y; is the confidence (or support) of a rule in the new data, and 2, is the estimated confidence (or support) of the rule, which is the mean value of the confidences (or supports) of the rule in the q-1 blocks. S is the total number of rules tested.

Table 4 and 5 show the accuracy results on confidence and support respectively. We discuss Table 4 first.

Column 2 of Table 4 gives the percentage of stable confidence rules that remain stable after considering the qth block (the final year) of data. That is, our statistical tests now consider one more year. In the case of insurance data, 96.4% of stable rules remain stable. 2.4% of the  lnsur  Edu2-2  stable rules become semi-stable. For the other three datasets, the percentages of stable rules remaining stable are slightly smaller, but are still very large. We believe the reason is that these datasets are smaller and thus tend to have bigger variations over time. However, the remaining rules are almost all semi-stable. Column 4 shows that almost all semi-stable confidence rules are still semi- stable after the qth block is considered (note that the semi- stable rules here do not include stable rules). Column 5 shows that most trend rules still exhibit trends. The proportions are slightly less than those for stable rules and semi-stable rules because the number (q) of time periods in our experiments is small and thus the trend test is less stable. Column 6, 7 and 8 show the mean squared errors for each dataset when we use the mean confidences of stable rules, semi-stable rules and unstable rules to predict the confidences of these rules on the qth data block respectively. Column 9, 10, and 1 1  give the respective mean absolute errors for each dataset. We can see that stable rules commit least error. The error of unstable rules can be very large. For example, in the case of Edul, for  Table 4: Accuracy results on confidence analysis  Table 5: Accuracy results on support analysis     unstable confidence rules, the error in confidence on the qth block is 16.8% on average, while the error in confidence for stable confidence rules is only 3.5%. The errors with the insurance data are smaller because this dataset is large and thus the variance is small. Note that the MSE and MAE values are not computed for trend rules, as it is not reasonable to use the mean confidence as the predicted confidence in the next time period due to the trend. Our trend test is mainly to give the user an indication that trends exist rather than predict the confidence of the rule in the future, for which we need a larger sample (large q) and further statistical analysis.

Table 5 gives the corresponding results on the analysis of supports. From column 2 and 3, we see that almost all stable support rules are still stable. A large majority of semi-stable support rules are also still semi-stable (column 4). Column 5 shows that most trend rules still exhibit trends. From column 6-11, we observe that the stable support rules commit least error. The results from both Table 4 and 5 confirm that stable rules and trend rules identified by our technique are indeed reliable, and can be trusted in the future.

6. Conclusions Association mining is an important data mining task. Its application is, however, hampered by the fact that it often generates a large number of rules. In this paper, we argue that the large number of rules is only part of the problem.

Lack of systematic analysis procedures to help the user understand the behavior of rules is also a major issue.

Without understanding the behavior of a rule over time, the rule cannot be used in practice. This paper presented a number of statistical tests to analyze the behavior of rules.

Using these tests, we are able to identify stable rules and trend rules, and at the same time remove those unreliable (unstable) rules. Experiment results showed that the proposed technique is very effective and efficient.

