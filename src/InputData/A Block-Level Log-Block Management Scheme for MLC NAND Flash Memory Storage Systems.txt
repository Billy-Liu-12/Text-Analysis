0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

Abstract?NAND flash memory is the major storage media for both mobile storage cards and enterprise Solid-State Drives (SSDs).

Log-block-based FTL (Flash Translation Layer) schemes have been widely used to manage NAND flash memory storage systems in industry. In log-block-based FTLs, a few physical blocks called log blocks are used to hold all page updates from a large amount of data blocks. Frequent page updates in log blocks introduce big overhead so log blocks become the system bottleneck.

To address this problem, this paper presents BLog, a block-level log-block management scheme for MLC NAND flash memory storage system. In BLog, with block-level management, the update pages of a data block can be collected together and put into the same log block as much as possible; therefore, we can effectively reduce the associativities of log blocks so as to reduce the garbage collection overhead. We also propose a novel partial merge operation strategy called reduced-order merge by which we can effectively postpone the garbage collection of log blocks so as to maximally utilize valid pages and reduce unnecessary erase operations in log blocks.

Based on BLog, we design an FTL called BLogFTL for MLC (Multi-Level Cell) NAND flash. We conduct a set of experiments on a real hardware platform. Both representative FTL schemes and the proposed BLogFTL have been implemented in the hardware evaluation board. The experimental results show that our scheme can effectively reduce the garbage collection operations and reduce the system response time compared to the previous log-block-based FTLs for MLC NAND flash.

Index Terms?NAND flash memory, flash translation layer, log block, garbage collection, response time.

?  1 INTRODUCTION  NAND flash memory is widely adopted in various storage systems from USB drives, digital camera memory cards, to SSDs (Solid State Drives) due to its non-volatility, low power consumption, high density and good shock resistance [1].

However, NAND flash has some constraints that make chal- lenges in its management. First, it suffers from out-of-place updates, i.e., an update (re-write) to existing data on a given physical location (known as a page that is the basic unit for read and write operations) should be preceded by an erase operation on a larger region (known as a block that is the ba- sic unit for erase operations). Second, a block becomes worn- out if its erase count reaches a limited number. For example, one block in SAMSUNG K9F1G08U0C SLC (Single-Level Cell) NAND flash has 100K erase counts, while the one in SAMSUNG K9G4G08U0A MLC (Multi-Level Cell) NAND flash has only 5K erase counts. To conceal these unfavorable characteristics and make NAND flash work like an ordinary block device, an intermediate software module called flash translation layer (FTL) is employed to serve inputted I/O requests and manage NAND flash correspondingly [2], [3], [4], [5]. Among various FTLs, log-block-based FTL schemes  ? Y. Guan and G. Wang are with Beijing Advanced Innovation Center for Imaging Technology and the College of Computer and Information Management, Capital Normal University, Beijing, China. Y. Wang is with the College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China. Renhai Chen is with Tianjin Key Laboratory of Cognitive Computing and Application, School of Computer Science and Technology, Tianjin University, Tianjin, China. C. Ma and Z. Shao are with Embedded Systems and CPS Laboratory, Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong.

? The corresponding author: Y. Wang (Email: yiwang@szu.edu.cn).

have been widely used in industry. This paper addresses the log-block management that is one of the most important problems in log-block-based FTLs.

In log-block-based FTLs [6], [7], [8], [9], the physical space of NAND flash is divided into two parts: data blocks and log blocks. Data blocks are a large amount of physical blocks that are used to store the first written data, and managed with the block-level mapping. Log blocks are a few physical blocks that are used to hold updated data from all data blocks, and managed with the page-level mapping.

The mixture of block-level and page-level mappings can constitute a trade-off between the size of the mapping table and the performance [10].

In this paper, we propose a block-level log-block man- agement scheme called BLog (Block-level Log-Block Man- agement). In BLog, we make the free space of all log blocks be freely used by all data blocks, and maintain a block- level mapping between a data block and the log blocks that contain its updated pages. Based on this mapping, the update pages of a data block can be collected together and put into the same log block as much as possible; therefore, we can effectively reduce the garbage collection overhead by downgrading the associativity of a log block, avoiding unnecessary merge operations, and decreasing valid-page copies in merge operations. Furthermore, we propose a novel partial merge operation called reduced-order merge by which we can do partial merge while simultaneously reduc- ing the associativities of log blocks. Based on reduced-order merge, we propose two garbage collection policies consid- ering the space and associativity of log blocks, respectively.

Through these policies, the garbage collection of log blocks can be effectively postponed, and an erase operation to a log block occurs only when there are no free pages in log    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2679180, IEEE Transactions on Computers   blocks. In this way, we can maximally utilize valid pages and reduce unnecessary erase operations in log blocks.

Our log-block management scheme can be applied to manage both SLC and MLC NAND flash. This paper focus on managing MLC NAND flash as it is the mainstream NAND flash product in the market. Based on BLog, we design an FTL called BLogFTL for MLC NAND flash.

BLogFTL is built upon a hybrid address mapping mecha- nism: At the block level, a logical block is mapped to a data block or log block; at the page-level mapping, we propose an efficient fine-granularity page-level management method, in which a page mapping table of a logical block (stored in the spare areas of NAND flash) can be efficiently retrieved through a space-efficient page-mapping structure called Page-Mapping-Table-Directory in RAM. With this mecha- nism, pages can be written sequentially into a block and a page only needs to be written once, so the new constraints of MLC NAND flash can be satisfied. At the same time, via Page-Mapping-Table-Directory, we can directly access the page-level mapping table of a logical block so as to improve the system response time.

We conducted experiments on a real embedded platfor- m. The hardware platform employs an ARM 11 processor core (Samsung S3C6410) with ARMv6 architecture. Both the proposed BLogFTL and several representative FTLs at different levels of mapping granularity including the page- level FTL [11], DFTL [12], Superblock FTL [9], and GFTL [13] have been implemented on the evaluation platform. The experimental results show that our scheme can effectively minimize valid-page copies and reduce the total number of erase operations so as to improve the system performance compared with DFTL, GFTL and Superblock FTL.

The remainder of this paper is organized as follows.

Section 2 introduces the background and the motivation.

The log-block management scheme is presented in Section 3.

BLogFTL, our FTL technique, is introduced in Section 4. The performance analysis of BLogFTL is presented in Section 5.

Section 6 presents the experimental results. The conclusion and future work are presented in Section 7.

2 BACKGROUND  2.1 NAND Flash  A NAND flash memory chip consists of many blocks, and each block is divided into a fixed number of pages. Block is the basic unit for erase operations while page is the basic unit for read/write operations. A page contains a data area and a spare area also known as Out Of Band (OOB) area.

An OOB is mainly used to store the Error Correction Code (ECC) of the data of a page and other ad-hoc information.

Table 1 shows the parameters of a 4GB MLC NAND flash from Samsung that has been used in our experiments.

From Table 1, we can see that while we can efficiently read pages or OOBs (60?s for one page; 20?s for one OOB), page write and block erase operations are more time consuming (800?s for one page write; 1500?s for one block erase). There are two major methods to manage NAND flash memory storage systems: Flash translation layer (FTL) [15], [16] or flash-based file system [17]. This paper focuses on the FTL-based design. Therefore, when designing FTLs, if  TABLE 1 Samsung 4GB MLC NAND Flash (K9G4G08U0A [14]).

Page size 2KB OOB size 64B Block size 256KB(data) + 8KB(OOBs) Pages per Block 128  Page read time (Trp) 60?s OOB read time (Trs) 20?s Page write time (Tw) 800?s Block erase time (Te) 1500?s  Endurance 5000 times ECC coding 4 bits/512B  we can reduce unnecessary write and erase operations, the system performance can be effectively improved.

Most NAND flash memory chips support partial-page programming. It can facilitate to store smaller amounts of data on the large size of NAND flash page. Each NAND flash memory page can be partitioned into multiple PC- sized subpages [18]. For SLC flash memory, it supports the programming of a page for multiple times. For example, a NAND flash page contains four subpages, and it can be programmed four times before an erase operation is required [18]. For MLC flash memory, only one partial-page programming per page is allowed between every two erase operations.

MLC technology further increases the capacity of NAND flash memory by storing multiple bits of data per cell. How- ever, two new constraints are introduced in MLC NAND flash: (1) Pages within a block must be programmed in a consecutive manner [19], [20]; (2) Only one partial-page program operation is allowed in one page [21]. These new constraints pose challenges for designing effective FTLs in MLC NAND flash memory storage systems. Considering the sequential-page-write constraint in MLC NAND flash, when a page is written into a block, it can only be written to an empty page whose block offset is bigger than that of an existing page written before. For many existing FTL schemes [22], [23], block offsets are used to determine the location of a page in a block in write operations so pages are written in a block in a random manner. The one-partial- page-programming constraint also makes some previous FTL schemes inapplicable. In these FTL schemes, when a page is updated, a flag bit in the OOB of this physical page is set as ?invalid?; later read/write/erase operations are dependent on this flag bit to proceed correspondingly. How- ever, separately setting a flag bit after a page is written is no longer supported with the one-partial-page-programming constraint in MLC NAND flash. These influences should be considered in FTL designs for MLC NAND Flash.

2.2 Log-block-based FTL Schemes  Frequent page updates in log blocks introduce big over- head so log blocks become the system bottleneck. Various methods have been proposed to manage log blocks. In BAST [6], a log block can only be used by one data block.

As there are only very few log blocks, they will be used up very quickly; then merge operations will be triggered, which causes big overhead with extra valid-page copies and erase operations. To solve this problem, in FAST [7],    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2679180, IEEE Transactions on Computers   a fully-associative approach is proposed, in which all pages in a log blocks can be used by any data blocks. This can effectively postpone merge operations caused by log blocks.

However, one log block can be associated with many data blocks. When log blocks are full and a merge operation is triggered, it will incur a lot of valid-page copies and erase operations.

KAST [8] solves this problem by limiting the associativ- ity of each log block, in which a log block is enforced to be used by at most K data blocks, so as to guarantee the worst-case log block merge time. Furthermore, log blocks are divided into two groups, sequential and random write log blocks. Sequential-write log block is used to store the sequential updated data, while random-write log block is used to store the updated data for random write operations.

Compared with FAST, KAST can effectively improve the worst-case merge time. However, in KAST, merge opera- tions may be triggered earlier when log blocks are not full.

Moreover, its sequential-write requirement is very strict, which can cause extra merge operations. In Superblock FTL [9], log blocks are managed with an approach similar to KAST but without sequential-write log blocks. Specially, a superblock that is N adjacent logical blocks can use up to M log blocks. As N logical blocks in a superblock can fully utilize up to N + M physical blocks, the merge cost can be reduced. However, hot superblocks may quickly use up log blocks allocated so as to trigger merge operations even when there are still free pages in log blocks allocated to cold superblocks.

2.3 The Implementation of Superblock FTL  In this section, we introduce the implementation of Su- perblock FTL that is one representative log-block-based FTL and can be applied to manage MLC NAND flash.

In Superblock FTL, N adjacent logical blocks consist of a superblock, and a superblock can contain up to N + M physical blocks, where M refers to the additional physical blocks that are assigned to N physical blocks. As a result, N logical blocks can use N + M physical blocks. A page in a superblock can be mapped to any location in up to N +M physical blocks, which is achieved by maintaining a page mapping table for each logical block of a superblock.

To avoid using a large amount of RAM space, this page mapping table is stored into the spare area of NAND flash memory and retrieved through a three-level structure con- sidering the limited space of a spare area. In Superblock FTL, pages within a data block are written sequentially; therefore, it is suitable for MLC NAND flash. An example of Superblock FTL is shown in Figure 1.

In the example, there are nine data blocks and four log blocks, and each block has four pages. In the figure, the number inside a page in a block denotes the corresponding logical page number whose data is stored in the page. In Su- perblock FTL, one superblock contains three logical blocks (N = 3). As shown in Fig. 1, there are three superblocks, and each one contains three data blocks. For the illustration purpose, suppose some pages have been written into data blocks, and then a set of page update requests are coming.

To serve these page update requests, log blocks are allocat- ed. Finally, two log blocks (Blocks 9 and 10) are assigned to  ???? ???? ???? ????  ??? ??? ???  ??? ??? ???  ???? ???? ????  ???? ???? ????  ??? ??? ???  ??? ??? ???  ???? ???? ????  ???? ???? ????  ??? ??? ???  ??? ??? ???  ??? ??? ??? ???  ??? ??? ???  ??? ??? ???  ???? ???? ????  ???? ???? ????  ??? ??? ???  ??? ??? ???  ??? ??? ??? ???  Free page Invalid page Valid page Num LPN  Page update requests (Logical Page Numbers): 0, 4, 8, 1, 2, 0, 12, 27, 5, 9  Superblock 2 Superblock 3Superblock 1     12 27   0 21 3 4 6 7  9 10 11 12  5 8      16 20 24  28 32   Block  BlockData  Log  Fig. 1. Illustration of Superblock FTL.

Superblock 1, and one is assigned to Superblocks 2 and 3, respectively. At this moment, a merge operation is triggered in Superblock 1 as there is no free space in the log blocks assigned to it. Although there are still free pages in other log blocks like Blocks 11 and 12, they have been assigned to other superblocks and cannot be used by Superblock 1.

If we can utilize all free pages in log blocks, this kind of unnecessary merge operations can be avoided.

To do the merge operation in Superblock 1, suppose Block 9 is picked up as the victim block (both Blocks 9 and 10 are associated with three data blocks, i.e., Blocks 0, 1, 2, but Block 9 has less valid pages). As it contains valid pages in three data blocks, we have to do the full merge. This means we need to copy valid pages from Blocks 0, 1, 2, and 9 to three new data blocks, and then erase Blocks 0, 1, 2, and 9. A lot of valid-page copies and erase operations are caused, because the page updates of the three data blocks in Superblock 1 are put into the two log blocks allocated one by one following the order in the update requests. Compared to other log-block-based FTLs, Superblock FTL has improved the associativity (i.e., the number of data blocks to share one log block) by limiting the associativity of a log block to N .

However, we can reduce the associativity of the victim block by exploring the block-level information of page updates (which page updates belong to which data blocks). For example, by grouping all updates of a data block into the same log block, we can put Pages 0, 1, 2, 0 into Block 9 (as the updates of the data block 0) and Pages 4, 8, 5, 9 into Block 10; in this way, the associativity of Block 9 (the victim block) becomes one, and only Blocks 0 and 9 will be involved in the merge. In our scheme, the block-level information of page updates is utilized to reduce the associativity of a victim log block.

2.4 Motivation  From the above example, we can see that full merge oper- ations should be avoided when log blocks are not full, as they introduce a lot of unnecessary valid-page copies and erase operations. Furthermore, if we can group all updates of a data block into the same log block, we can effectively reduce the associativities of log blocks so as to reduce the merge cost. Based on the above observations, we design our block-level log-block management scheme next.

0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2679180, IEEE Transactions on Computers   3 BLOCK-LEVEL LOG-BLOCK MANAGEMENT 3.1 System Architecture  BLog uses two tables to manage log blocks. The tables are Data-block to Log-block mapping Table (DLT) and Log-block Management Table (LMT). An example is shown in Fig. 2.

??? ??? ???  ??? ??? ???  ??? ??? ???  ??? ??? ???  Num LPN  Valid pageInvalid pageFree page  PBN (Physical Block Number)  LBN ( Logical Block Number)  0 21     BlockData 3 4 5   16 20  6 7 8   28 32  BlockLog  0Page update request (Logical Page Numbers):   9 10 12  (a)  B( Log      lock?level         ?block Management)BLog  The Log?block Management Table (LMT)  OffsetPBN LBN (l=2)   (Data blocks)(Log block)   9 1   The Data?block to Log?block mapping Table (DLT)  LBN PBN (u=2) (Log blocks)  0 9  (Data block)  (b)   Fig. 2. An example to illustrate how DLT and LMT are updated when one page update request (Page 0) is handled.

DLT is used to record the log blocks allocated to a data block. In the DLT, each row has the following items: LBN (the logical block number of a data block), and PBN (a set of log blocks allocated to this logical block, and the maximum number is u). u is used to control the maximum associativity of a data block (the maximum number of log blocks that can be allocated to a data block). Initially, the table is empty, and then it is updated based on the log-block management scheme.

LMT is used to record data blocks mapped to a log block.

Each log block has one row in LMT. Each row contains a log block (the physical block number of the log block is recorded in Column PBN), its first free page (recorded in Column Offset), and all data blocks mapped to it (the logical block numbers of the data blocks are recorded in Columns LBN).

l is used to control the maximum associativity of a log block (the maximum number of data blocks that can be mapped to a log block).

There should be a page-level mapping table so for a logical page, we can find its corresponding physical page in log blocks. When our scheme is applied to manage SLC NAND flash, we can have a page-level mapping table stored in RAM similar to other log-block FTLs like BAST, KAST and FAST. For MLC NAND flash, as discussed in Section 4, this is handled by a page-mapping structure stored in the spare areas of NAND flash in our FTL. The details will be given in Section 4.

3.2 Log-block Space Allocation  Compared with other log-block-based FTLs, one of the most important differences in BLog is its log-block space allocation scheme. Our basic idea is to collect the updated pages of a data block together and put them into the same log block as much as possible. Log-block space allocation may trigger merge operations. In this section, we discuss the  case in which no merge operation is triggered, and the case to trigger merge operations will be discussed in Section 3.3.

The case without triggering merge operations can be further divided into two situations based on whether or not a new log block is required to be allocated. Let us first consider the situation in which a new log block is required. This is the situation when either no any log block is allocated to a data block, or an allocated log block is full. In this situation, we will apply our log-block space allocation policy to allocate a new log block. The policy is to find a log block in LMT whose associativity is the smallest among all log blocks with the maximum free pages. In this way, a data block can fully utilize such an allocated log block that has the maximum free pages among all log blocks. Based on this policy, if there is such a log block in LMT, this log block and its first free page will be allocated to the requested data block. Correspondingly, in DLT, the logical block number of the data block and the physical block number of the allocated log block will be recorded; in LMT, Offset will be increased by one (to point to the next free page) in this log block. An example is shown in Fig. 2, in which the page update for the logical page number 0 is put into the first physical page in log block 9 (the first log block (PBN is 9) in LMT) where u = 2 in DLT and l = 3 in LMT.

The situation in which a new log block is not required occurs when a log block has been allocated to a data block and this log block has free space. In this situation, the logical block number obtained from the page update request will be used to check Column ?LBN? in DLT, and then the log block allocated to this data block can be obtained from the corresponding row in DLT. In this row, there may be more than one log block recorded. However, only the last one is used as all the previous ones are full. Based on the last log block in DLT, we can find its corresponding row in LMT where we can find this log block and its first available page. The first free page of the log block will be used to serve the page update and offset of this log block in LMT will be increased by one. Note that for a data block, based on DLT, we will put all its updates into its last log block until the log block is full. In this way, we can group all the updates of a data block and put them into one log block as much as possible. Fig. 3 shows an example where several page update requests are processed and DLT and LMT are updated correspondingly.

3.3 Garbage Collection  In this section, we discuss how to handle the case when merge operations are triggered. A merge operation will be triggered if one of the following three situations occurs:  Situation 1: All spaces of u log blocks allocated to a data block have been used up (u is the maximum number of log block that can be allocated to a data block). This situation is handled by reduced-order merge.

Situation 2: There is free space in log blocks but either the associativity of a log block has reached to l (l is the maximum number of data blocks that can be mapped to a log block) or a log block is full. This situation is handled by Associativity-GC (Associativity-related Garbage Col- lection).

0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2679180, IEEE Transactions on Computers   ???? ???? ???? ????  ??? ??? ???  ??? ??? ???  ???? ???? ???? ????  ???? ???? ???? ????  ??? ??? ??? ???  ???? ???? ???? ????  ??? ??? ???  ??? ??? ???  ???? ???? ????  ???? ???? ????????  ???? ???? ????  ???? ???? ???? ????  ??? ??? ???  ??? ??? ???  9 10 12  4 8      Num LPN  Valid pageInvalid pageFree page   0 21     BlockData 3 4 5   16 20  6 7 8   28 32  BlockLog  (a)  (b)  B( Log      lock?level         ?block Management)BLog  The Log?block Management Table (LMT)  OffsetPBN LBN LBN (l=2)  PBN (Physical Block Number)  LBN ( Logical Block Number)     (Log block)PBN (u=2) (Log blocks)   Page update requests (Logical Page Numbers): 0, 4, 8, 1, 2, 0, 12, 27, 5, 9  (Data block) (Data blocks)   The Data?block to Log?block mapping Table(DLT)  Fig. 3. Process the same page update requests shown in Fig. 1 with BLog.

Situation 3: No free space in log blocks. This situation is handled by Space-GC (Space-related Garbage Collection).

In Situation 1, log blocks may contain free pages; how- ever, they cannot be allocated to a data block as the max- imum associativity of the data block has been achieved.

We propose a new partial merge operation called reduced- order merge to handle this situation. Given the logical block number, LBN, of the corresponding data block, a reduced- order merge operation has two actions:  Partial merge. From its corresponding row in DLT, if a log block contains all valid pages of the data block, then it is swapped to be the data block, and the original data block is erased and used as the corresponding log block; otherwise, if we cannot swap any log blocks associated with this LBN in DLT, all valid pages of this LBN are copied to a new data block, and the old data block is erased.

Reduce associativity. Based on the corresponding row related to the LBN in DLT, we remove the LBN from each associated log block in LMT so as to reduce its associativity, and finally remove the row from DLT. We do this because all valid pages from both the data and log blocks will be moved to the new data block; therefore, there is no need to record any previous log blocks associated with this data block.

Reduced-order merge can move valid pages to a new data block and simultaneously reduce the associativity of log blocks. Note that in a reduced-order merge, it will not erase any log block. Therefore, we can maximally utilize valid pages and reduce unnecessary erase operations in log blocks.

In Situation 2, either a log block is full or its asso- ciativity has achieved its maximum value. Therefore, our Associativity-GC policy targets at maximally reducing the associativities. In Associativity-GC, first, a log block with the maximum free pages in LMT is selected; then for each LBN associated with this log block in LMT, its corresponding row in DLT is checked and the row with the maximum number of log blocks in DLT is selected. Next, a reduced-order merge operation is performed on the selected row (the data block) in DLT.

As the associativity becomes the major problem in Situation  Index 2Index 1 Index n  0 /  1  FLAG DLT_POSITION OFFSET  Log?block Mangment  Page_Mapping_Table_DirectoryOffsetPBN  Mapping Table (LDMT) The Logical?block to Data?block The Data?block to Log?block mapping Table (DLT)  (u)PBN (Log blocks)  L1  (Data block) LBN  LBN  L1 LOG 1  L1 K  (l)LBN (Log block)  PBN Offset  The Log?block Management Table (LMT)  (Data blocks)  P  Data blocks Log blocks  NAND Flash memory  Fig. 4. The system overview of BLogFTL and the format of a Page- Mapping-Table-Directory index.

2, this scheme can guarantee that at least a log block with the maximum free pages will be available after the garbage collection (by first selecting the log block with the maxi- mum free pages), and at the same time it can maximally reduce the associativities of log blocks (by selecting the data block associated with the maximum number of log blocks). As a reduced-order merge will not erase any log block, in Situation 2, there is only one erase operation for the corresponding data block.

In Situation 3, there is no free space in log blocks, so our Space-GC policy aims to reclaim space while introducing the minimum overhead. In Space-GC, a log block with the min- imum associativity in LMT is selected as the victim block. Once the victim log block is selected, the corresponding LBNs (all the data blocks mapped to this log block) in LMT can be obtained. For each LBN related to this log block in LMT, a reduced-order merge is performed. Finally, the log block is erased, and its offset in LMT is updated. In this situation, we have to erase a log block to reclaim space so all data blocks associated with it need to be erased as well. This is why we pick up the log block with the minimum associativity.

Our Space-GC is similar to full merge in other log-block- based FTLs but it will introduce the minimum overhead with its victim log-block selection policy. Note that Space- GC is only triggered when all log blocks are full, while in other log-block-based FTLs, full merge can be triggered very early. Furthermore, by postponing the garbage collection of log blocks, when Space-GC is triggered, a victim log block selected may only contain invalid pages so we do not need to erase any data block.

4 BLOGFTL FOR MLC NAND FLASH 4.1 System Overview  As shown in Fig. 4, in BLogFTL, one more table called LDMT (the Logical-block to Data-block Mapping Table) is added in additional to the two tables in BLog. Like other log-block-based FTLs, LDMT contains the mapping between logical blocks and physical blocks. Besides that, we add new information so we can efficiently retrieve the page-level mapping information of each logical block.

In LDMT, each logical block has one corresponding row, and each row contains three items, namely, PBN, Offset, and Page-Mapping-Table-Directory. PBN is used to record the    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2679180, IEEE Transactions on Computers   data block (the physical block number) allocated to a logical block; Offset is used to record the current free page in the data block; Page-Mapping-Table-Directory is used to record the indexes of the page-level mapping table of a logical block. To satisfy the new constraints of MLC NAND flash, in BLogFTL, pages are written in a physical block sequentially.

Therefore, for each logical block, we need to record its page- level mapping information so we can find the physical page allocated to a logical page. To achieve this, we maintain a page-level mapping table for each logical block. In order to save RAM space, this table is stored in the OOB areas in NAND flash. To efficiently retrieve this table, we store its address information in Page-Mapping-Table-Directory in RAM.

The page-level mapping table of a logical block may be too big to be stored in one OOB. Therefore, the table may be divided into several parts and each part is stored in one OOB. Correspondingly, Page-Mapping-Table-Directory may contain several indexes, and each index refers to one OOB area in a page. To find an OOB, we need to know its physical block number and page offset. However, as we can find the data block and log blocks allocated to a logical block in LDMT and DLT, in Page-Mapping-Table-Directory, we do not directly store physical block numbers, and instead we store its relative location information in LDMT and DLT so as to save RAM space.

The format of a Page-Mapping-Table-Directory item is shown in Fig. 4. For each OOB that stores one part of the page-level mapping table of a logical block, it has one index in a Page-Mapping-Table-Directory. Each index contains FLAG, DLT POSITION, and OFFSET. Here, FLAG is used to represent whether the OOB is in the data block or log blocks, DLT POSITION is used to represent the position of the log block in DLT; OFFSET is used to represent the page offset. Based on FLAG, if one OOB is in the data block, we can find the physical block number in LDMT; otherwise, DLT POSITION is used to find the physical block number of the log block in DLT. Then using OFFSET, we can get the physical address of an OOB. Note the maximum associativity of a logical block in DLT should be less than ten in practice (we have achieved very good results with 2 in our experiments). Therefore, the size of an index should be less than 2 bytes. For example, let u = 8 in DLT and each block contains 128 pages, then the size of an index is 11 bits (FLAG needs one bit, OFFSET needs seven bits for 128 pages per block, and DLT POSITION needs three bits for the maxim associativity as 8).

In the page-mapping table of a logical block, we need to record physical page numbers that are mapped to logical pages. The same format in a Page-Mapping-Table-Directory index is used to represent a physical page number in a page-mapping table. Using this format, each physical page number can be represented by less than 2 bytes. Therefore, we can use very few OOB areas to store one page-level mapping table. For example, considering the MLC NAND flash shown in Table 1, with u = 8 in DLT (up to 8 log blocks can be allocated to one data block), we can use three OOBs to store one page mapping table.

In this flash memory, each block contains 128 pages and the size of an OOB is 64 bytes in which 2 bytes are for ECC coding (4 bits/512B). So a page mapping table contains  Algorithm 4.1 Write Operations in BLogFTL  Input: LPN (logical page number).

Output: Issue an write operation to a free physical page.

1: Obtain LBN (logical block number) and page offset.

2: Use LBN as the index to look up the table LDMT.

3: if Data Block PBN does not exist then 4: Allocate a new free physical block and its first free page from  data block list. Go to step 23.

5: end if 6: Obtain corresponding PMTD index in LDMT table.

7: if PMTD index does not exist then 8: Allocate the next available free page in Data Block from LDMT  Table. Go to step 23.

9: end if  10: Parse PMTD index to obtain corresponding physical page PMDPPN (Page-Mapping-Table-Directory Physical Page Num- ber). Read OOB of PMDPPN and check PMTD index PTPPN (Page-Table Physical Page Number)from the corresponding map- ping slot.

11: if PTPPN does not exist then 12: Allocate the next available free page in Data Block from LDMT  Table. Go to step 23.

13: end if 14: Check TailBlock by looking up the table LMT.

15: if TailBlock has free available pages then 16: Allocate a free page. Update LMT table. Go to step 23.

17: end if 18: if the table DLT achieves the maximum associativity then 19: Perform reduce-order merge.

20: else 21: Allocate a log block by applying log-space allocation policy and  the first available free page by looking up LMT table. Update LMT table and DLT Table.

22: end if 23: Update the table LDMT and write the content to the free physical  page.

128 mapping items, and each mapping item contains one Page-Mapping-Table-Directory index that can be used to locate the corresponding physical page. With u = 8 in DLT, the size of a Page-Mapping-Table-Directory index is 11 bits (FLAG-1; OFFSET-7; DLT POSITION-3); thus, each OOB can contain up to 45 page-level mapping items ([64 (OOB size) - 2 (ECC)]*8/11), and three OOBs are big enough for one page mapping table (128/45). As a result, each Page-Mapping-Table-Directory contains three indexes, and the size of one Page-Mapping-Table-Directory is less than 5 bytes (3*11=33 bits). Therefore, our method introduces small space overhead but achieves efficient address mapping.

4.2 The Write and Read Operations  Algorithm 4.1 presents how write operations work in BLogFTL. Given a write request to a logical page number, we can obtain LBN and the corresponding block offset (BO).

With LBN and BO, BLogFTL will search the page mapping table directory (PMTD) index in LDMT which stores the most updated version of page mapping subtable. If the request is not an update operation, we sequentially write data into the allocated physical page and update the LDMT.

Otherwise, we need to apply a log block space allocation policy to serve the update operation. When the log block space allocation policy is applied, the associativity-GC and space-GC operations may be triggered. When serving a write request, if the last allocated log block is full, reduce- order merge GC will be triggered. Each request will have the corresponding mapping item updated and written to the OOB area of a new page allocated by LDMT or LMT.

0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2679180, IEEE Transactions on Computers   Write(A,35) L4 #11 1 0/0/0(88)  LDMT in RAM  A PMT_088  Write(B,36) L4 #11 2 0/0/0(88)  0/0/1(89)  B PMT_1 0/0/189  Write(C,34) L4 #11 3 0/0/2(90) 0/0/1(89) C PMT_090  Write(D,34) L4 #11 3 1/0/0(136) 0/0/1(89) D PMT_0136  Write(E,34) L4 #11 3 1/0/1(137) 0/0/1(89) E PMT_0137  Write(F,33) L4 #11 4 0/0/3(91) 0/0/1(89) F PMT_091  Write(G,37) L4 #11 5 0/0/3(91) 0/0/4(92) G PMT_192  Write(H,35) L4 #11 5 1/0/2(138) 0/0/4(92) H PMT_0138  Write(I,34) L4 #11 5 1/0/3(139) 0/0/4(92) I PMT_0139  Write(J,39) L4 #11 6 1/0/3(139) 0/0/5(93) J PMT_193  Write(K,37) L4 #11 6 1/0/3(139) 1/0/4(140) K PMT_1140  DLT in RAM(u=2)  Up-to-date LDMT in RAM  Read(36) (1)36/8=4 (2)36%8=4  (3)  B89  (4) (5)  LBN PBN BO INDEX0 INDEX1 PPN Data Spare Area (OOB)  Data Storing in Flash Memory  LDMT: Logical-block to Data-block Mapping Table     LMT: Log-block Management Table      DLT: Data-block to Log-block Mapping Table  LBN: Logical Block Number                                          PPN:  Physical Page Number                   Step(1) - (5): Address Translation  LMT in RAM(l=2)  0/0/0  0/0/2 0/0/0  1/0/0 0/0/0  1/0/1 0/0/0  0/0/3 1/0/1 0/0/0  0/0/1 0/0/4  0/0/3 1/0/1 1/0/2  0/0/3 1/0/3 1/0/2  0/0/1 0/0/4 0/0/5  0/0/1 1/0/4 0/0/5  PBN BO LBN0LBN PBN0 PBN1 LBN1  L4 #11 6 1/0/3(139) 1/0/4(140)  0/0/1(89)  L4 #17 #17 1 L4  L4 #17 #17 2 L4  L4 #17 #17 3 L4  L4 #17 #17 4 L4  L4 #17 #17 5 L4  K PMT_1 0/0/1 1/0/4 0/0/5  Fig. 5. Illustration of Address Translation in BLogFTL.

Algorithm 4.2 illustrates the read operations in BLogFTL.

Given an LPN, BLogFTL first translates the LPN to LBN and page offset. By looking up the table LDMT, we first retrieve the corresponding PMTD index, and then parse the index by checking the LDMT table or DLT and LMT tables to get the required physical page. Reading out the page mapping table from the OOB area, we can obtain another PMTD index by checking the mapping item, and get the target physical page. Finally, by reading the target physical page, we can get the required data.

4.3 Data Address Translation in BLogFTL  This section presents the address translation in BLogFTL.

We use an example to illustrate write and read processes in BLogFTL. Fig. 5 shows the example. In this example, we configure u=2 (the maximum associativity of DLT is 2) and l=2 (the maximum associativity of LMT is 2). We assume that each physical block consists of 8 pages. For an LBN, the first coming write operation will be stored in the data block, and the update operation of the LBN will be sequentially appended to the allocated log blocks.

BLogFTL uses three mapping tables to maintain the address translation between a logical address in file system  Algorithm 4.2 Read Operations in BLogFTL  Input: LPN (logical page number) Output: Issue a read operation to a free physical page.

1: Obtain LBN (logical block number) and page offset.

2: Use LBN as the index to look up the table LDMT.

3: Obtain corresponding PMTD index from LDMT Table.

4: if PMTD index does not exist then 5: Return Error.

6: end if 7: Read the OOB of PMDPPN and check corresponding mapping  slot to get PMTD index.

8: if PMTD index does not exist then 9: Return Error.

10: end if 11: Parse obtained PMTD index to get physical page PTPPN by  looking up either LDMT or (DLT and LMT ).

12: Issue the read operation with the page PTPPN .

and a physical address in flash memory. The first mapping table is LDMT. Given a write request, the logical block num- ber can be obtained by dividing the logical page number with the number of pages in a block. For the write request Write(A, 35), the LBN is L4 (35/8=4). The table LDMT maintains the LBN to PBN address mapping, and a physical block #11 is allocated to LBN L4. The mapping entry BO indicates the next available offset in the physical block. For example, the BO of the write request Write(A, 35) is 1 and its PBN is #11. Then the next available physical page number (PPN) is 89 (8*11+1=89). The PPN 89 can be used for the coming write request.

The mapping entries INDEX0 and INDEX1 are used to track the most current versions of two OOBs. That is, the two PPNs that store the most recently accessed page-mapping-table-directory indexes. The entry INDEX has three flags. The first flag uses 0 to represent a data block and 1 to represent the log block. The second flag tracks the location of the log block in the table DLT, and it works when the first flag is equal to 1. The third flag denotes the offset.

For example, the INDEX0 for the write request Write(A, 35) is 0/0/0. It points to a data block and the page offset is 0. That is PPN 88 (#11*8+0=88). Similarly, 0/0/1 represents PPN 89 and 0/0/2 represents PPN 90.

The flags stored in the spare area (OOB) of a physical page depend on the offset. We assume that each block contains eight pages. Then the page offset ranges from 0 to 7. The offset (or remainder) 0-3 is stored in INDEX0, and the offset 4-7 is stored in INDEX1. For the write request Write(A, 35), the offset is 3 (35%8=3). Therefore, the flags 0/0/0 are stored in offset 3 (the last partition in OOB of PPN 88).

The update request will be allocated to a log block. Two mapping tables DLT and LMT are used to record the log block. The table DLT maintains the LPN to PBN address mappings, and the table LMT maintains the PBN to LBN address mappings. For an update request Write(D, 34), a log block (#17) is assigned to logical block L4. The entry INDEX 1/0/0 denotes the log block (#17) with offset 0.

Therefore, the data will be stored in PPN 136 (#17*8+0=136).

0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2679180, IEEE Transactions on Computers   TABLE 2 The timing analysis of different FTL schemes.

Schemes Best case Worst case PFTL Tr/Tw Np ? (Tr + Tw) + Te DFTL Tr/Tw Tr +Np ? (Tr + Tw) + Te SFTL (2? To + Tr/Tw) N ? {(Tr + Tw)?Np + Te}+ Te  BlogFTL (To + Tr/Tw) l ? {(Tr + Tw)?Np + Te}+ Te  For the division of 34 by 8, the remainder is 2. Therefore, the index 1/0/0 is stored in the offset 2 (the third partition in OOB) of physical page PPN 136. The mapping entry BO in the table LMT will be updated to 1, which represents that the next available page is PPN 137.

For the read operation, BLogFTL first obtain the logical block number and the remainder. For example, the read request Read(36) corresponds to LBN L4 (36/8=4) and remainder 4 (36%8=4). Since the INDEX1 accesses the map- ping indexes for remainders 4-7, BLogFTL can find the phys- ical page (PPN 140) that stores the up-to-date mappings. By checking the content in the first OOB partition of PPN 140, the physical page (PPN 89) that stores the real data can be obtained. The content in data area of PPN89 is B, which is the requested data. Then the whole read request is served.

5 PERFORMANCE ANALYSIS In this section, we analyze the performance of PFTL [11], DFTL [12], Superblock FTL [9], GFTL [13], and the proposed BLogFTL in terms of timing analysis. We also discuss space overhead and metadata management overhead of BLogFTL.

5.1 Timing Analysis  Table 2 discusses the timing analysis of different FTL schemes. The terms Tr , Tw, To, and Te represent the time cost to read a page, write a page, read one OOB, and erase a block, respectively. Np represents the number of pages in a block. The worst case scenario of PFTL is when serving a write request, the garbage collection is involved. DFTL loads the needed portion of page mapping table into RAM on demand and consumes less RAM space than PFTL. The best case occurs when a request is cache hit in RAM while the worst case happens when the RAM table is full and eviction is needed. During eviction, merge operations are triggered which also downgrade overall system performance. Since SFTL stores first level page mapping in RAM and the remaining two level mappings in spare area of each page, every time a request is served, two different OOBs are accessed. Once log blocks assigned to a Superblock are full, merge operations are triggered. For BLogFTL, the worst case occurs when the three types of garbage collection including reduced-order merge GC, associativity GC and space GC are triggered.

5.2 Space Overhead Analysis  DFTL divides the overall flash space into two types: da- ta blocks and translation blocks. While data blocks serve write requests, translation blocks are reserved to store the remaining page mapping tables beyond those reside in RAM. In terms of SFTL, flash memory are classified into  data blocks and log blocks (update blocks). The log blocks are reserved for update requests of superblock. BLogFTL focuses on the address translation of the FTL design. It could be combined with wear-leveling schemes to further improve its performance. Similar to SFTL, BLogFTL also needs to configure the total number of log blocks that should be reserved. Specially, BLogFTL has two parameters u and l to restrict both the number of log block for a physical block and the number of physical block associated with a log block.

The aim of this scheme is to use up every free pages of log blocks and postpone garbage collection process. Therefore, the log blocks can achieve very good space utilization ratio.

Wear-leveling techniques can help to identify hot or cold data to let all physical blocks get the similar block erase counts.

5.3 Metadata Management Analysis  In terms of PFTL, since the mapping is at the page-level, it may consume a large amount of space. Suppose there are Nblk free blocks and each block contains Np pages; 4 bytes are sufficient to represent one particular logical page number. Therefore the total RAM space needed for PFTL is Nblk? Np? 4 bytes. Assume there are 1024 blocks and 128 pages per block, then the RAM space is 512KB.

For BLogFTL, there are totally 3 tables (LDMT, DLT, LMT) which need to be stored in RAM. Assume that we have Ndata data blocks and Nlog log blocks. As section 4.1 mentioned, in LDMT, each logical block has one corre- sponding row, and each row contains three items, namely, PBN, Offset, and Page-Mapping-Table-Directory. Then 16 bits (2bytes) are sufficient enough to indicate a PBN. Since there are 128 pages per block, 7 bits are needed for Offset, which are smaller than 1 byte. Each Page-Mapping-Table- Directory index contains 3 indexes, and the size of one Page- Mapping-Table-Directory (3*11 bits) is less than 5 bytes.

Therefore, each row of LDMT needs (2+1+5)=8 bytes. Since we assign each LBN with a data block, the total number of bytes consumed by LDMT is no more than Ndata ? 8 bytes.

For DLT, each logical block has one corresponding row, and each row contains 2 main components, a data block and u log blocks. To represent a particular data block number or log block number, less than 2 bytes are needed. Therefore, we need less than (2+2u) bytes for one row. The total bytes needed by DLT are no more than Ndata ? (2 + 2u) bytes.

For Nlog log blocks, there are Nlog rows in LMT. Each row contains 3 main components, PBN, Offset and l asso- ciated with data blocks. Similarly, for each row, it needs no more than (2+1+l ? 2)=(3+2l) bytes. The total RAM space needed for LMT is Nlog ? (3 + 2l) bytes. Therefore, the RAM usage of BLogFTL is 10 + 2u ? Ndata + (3 + 2l) ? Nlog bytes. Assume that there are 1024 data blocks and 512 log blocks, u=2 and l=4, then the total RAM space needed is no more than 20 KB. As DFTL only caches a portion of the whole page mapping table in RAM space, the RAM space needed is configurable. In order to have fair comparison, in the experiments, the size of the cached mapping table of DFTL is configured as the same as that of BLogFTL.

SFTL applies a hybrid three-level address translation and makes use of the spare area of each page to store the page tables. Therefore, the RAM space needed for SFTL is    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2679180, IEEE Transactions on Computers   relatively small. Only the first level mapping called Page- Global-Directory (PGD) is stored in main memory. Assume each row of the table consumes 4 bytes, then the total RAM space needed is 4?Nblk. If Nblk is 1024, then no more than 4KB RAM will be consumed.

6 EVALUATION To evaluate the effectiveness of the proposed BLogFTL, we conduct a series of experiments on a real hardware platform and present the results with analysis. We compare our proposed BLogFTL scheme with PFTL [11], DFTL [12], Superblock FTL [9], and GFTL [13]. Both BLogFTL and other previous FTL schemes have been implemented on a real hardware evaluation platform. The major performance metric to be evaluated is the average system response time, i.e., the average system response time for all requests. In this section, we first introduce the experimental environment and benchmarks. Then we present the experimental results and discussion.

6.1 Experimental Setup  We used an embedded developing board to conduct ex- periments. The board employs an ARM 11 processor core (Samsung S3C6410) with ARMv6 architecture. The ARM processor core is running at 532 MHz, and it consists of a 16 KB instruction cache and a 16 KB data cache. The platform has a core board with an 4 GB NAND flash memory with 2KB capacity per page and 256 MB SDRAM, and a mother board with RJ45 interface. A pin connector is used to connect the core board and the mother board.

We conduct experiments using benchmarks Bonnie [24], Postmark [25] and Tiobench [26] to evaluate the proposed approach. These benchmarks are running on the evaluation board as programs. As shown in Table 3, we configure the size of the data set of Bonnie and Postmark as 150 MB and 200 MB, respectively. In terms of the Tiobench benchmark, we issue 4 threads and 6 threads to proceed around 240 MB, respectively. We also performed file operations on the eval- uation board and these benchmarks are named as SD Card and NFS (Network File System), respectively. Specifically, for SD Card Large File and NFS Large File, we copy a single 200 MB large file twice from an SD Card or through the Network File System into the flash partition. For SD Card Small File and NFS Small File, we copy multiple small files with the total capacity of 200 MB twice from the SD Card or through the Network File System. Workloads CAMRESHM- SA01 and CAMRESISAA02 are I/O trace files from the SNIA IOTTA trace repository [27]. The detailed characteristics of these workloads are summarized in Table 3.

Figure 6 illustrates the framework of our experimen- tal platform. We implemented PFTL [11], DFTL [12], Su- perblock FTL [9], and GFTL [13] as block device drivers in Linux 3.0.8 on this embedded platform. The corresponding device files are created under the directory /dev. Then with the help of file system formatting tools such as mkfs.ext2, the file system information is written in the NAND flash memory. By mounting these device files to a specified file directory, the NAND flash memory can be operated through normal file operations, such as file creation, file reading, etc  Samsung 8Gb  SLC NAND Flash Memory  Flash Translation Layer  (Block Device Driver)  F il  e S  y st  em  Virtual File System  Ext2  Buffer Cache  Memory Technology Device Layer (MTD)  NFS /PROC  R esu  lts Benchmarks  (Bonnie, Postmark,  Tiobench)  Applications  (NFS, MPlayer,  SD Card)  System Call  L o  w er  D ev  ic e  D ri  v er  User Space  Kernel Space  System Call  Fig. 6. The framework of experimental platform.

[28]. In order to have a fair comparison, each time before the benchmark starts, the whole flash partition is erased.

Our experimental results are generated in the FTL layer.

Since user could not access kernel space data directly, the /proc file system is used for the user space and kernel space communication by means of creating a specified file under the /proc file directory. Experimental results are obtained through the /proc file system. We use the Linux kernel function do gettimeofday to measure the system response time.

To explore the effect on the system performance when the page updates arrive in a non-sequential way, we use a benchmark called Iometer which allows us to configure portion of randomness to conduct the experiment. Assume we have t page update requests, then benchmark 20% randomness indicates that 20% of the t requests arrive in a random manner while the remaining 80% of M page update requests arrive in a sequential manner. Similarly, the 40%, 60%, 80%, 100% randomness show different randomness configuration of Iometer. The capacity of flash memory is configured to be 277MB and the total amount of data written in is 512MB in this experiment.

TABLE 3 The characteristics of the workloads.

Benchmarks Numbers Ave.Resp. Avg.Req.

of Request Time (ms) Size (KB)  Bonnie 150MB 108,555 4,906 1.41 Bonnie 200MB 148,287 5,736 1.39  Postmark 150MB 49,003 4,638 3.13 Postmark 200MB 70,100 4,653 2.92 Tiotech 4thread 67,744 5,063 3.62 Tiotech 6thread 69,593 6,030 3.53  SD Card Large File 39,551 4,634 5.18 SD Card Small File 38,389 4,620 5.33  NFS Large File 41,845 5,245 4.89 NFS Small File 40,344 5,236 5.08  CAMRESHMSA01 297,890 6,759 3.52 CAMRESISAA02 409,600 6,950 3.56    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2679180, IEEE Transactions on Computers   TABLE 4 Performance comparison in BLogFTL with u = 1 and u = 2.

Workloads total 512 log blocks l=4, u=1 l=4, u=2  Bonnie 150MB 5,504 4,909 Bonnie 200MB 6,259 5,599  Postmark 150MB 5,231 4,611 Postmark 200MB 4,957 4,588 Tiobech 4thread 5,260 4,584 Tiobech 6thread 6,060 5,422  SD Card Large File 4,635 4,635 SD Card Small File 4,621 4,621  NFS Large File 5,246 5,246 NFS Small File 5,236 5,236  TABLE 5 The average system response time from BLogFTL with various  numbers of log blocks.

Workloads Number of log blocks (u = 2 and l = 4)  64 128 256 512 1024  Bonnie 150MB 6,194 5,900 5,765 4,909 4,979 Bonnie 200MB 6,277 6,410 6,267 5,599 5,589  Postmark 150MB 6,039 6,331 5,207 4,611 4,586 Postmark 200MB 6,457 6,328 5,207 4,588 4,627 Tiobech 4thread 8,655 8,243 7,483 4,584 4,581 Tiobech 6thread 9,127 8,853 8,181 5,422 4,596  SD Card Large File 4,630 4,630 4,630 4,635 4,632 SD Card Small File 4,608 4,610 4,608 4,621 4,621  NFS Large File 5,240 5,239 5,239 5,246 5,246 NFS Small File 5,231 5,229 5,235 5,236 5,230  6.2 Results and Discussion  In this section, we present the experimental results with analysis. For BLogFTL, the average response time (us) is influenced by u, l and the number of log blocks. Therefore, for BLogFTL, we first conduct experiments to show the results with various numbers of log blocks and with various u and l, respectively. Finally, we use one configuration to compare various FTLs. The total line of code developed for BLogFTL is 1818.

6.2.1 BLogFTL with u = 1 and u = 2  We compare the average response time while setting u = 1 and u = 2, and the results are shown in Table 4. The overall system performance of BLogFTL when u = 2 outperforms that when u = 1 among almost all the benchmarks. u = 1 indicates that the maximum log block for one LBN is one.

Once the log block allocated to a particular LBN is full, reduced order merge garbage collection will be triggered.

With smaller u as 1, more merge operation may be needed, therefore, the overall performance is affected.

6.2.2 BLogFTL with different numbers of log blocks  In this section, we show the results for how the number of log blocks influences the performance of BLogFTL. We fix u = 2 and l = 4 and let the number of log blocks vary from 64 to 1024. Table 5 presents the experimental results of the average system response time with different numbers of log blocks.

The average system response time is mainly affected by the number of valid page copy operations and the number of block erase counts. For standard benchmarks Bonnie,  Postmark, and Tiobench, it can be observed that the average system response time is decreased as the number of log blocks increases. The log blocks can postpone the write op- erations to data blocks, which reduces the valid page copy operations and the block erase counts. We can also observe that the average system response time for workloads SD Card and NFS are not sensitive to the number of log blocks.

That is because, for read-dominant workloads, very few garbage collection operations are triggered. For workloads with limited numbers of updates, a small number of log blocks (e.g., 64) can adequately serve the coming requests.

In this case, increasing the number of log blocks may not contribute to the reduction of the system response time.

6.2.3 BLogFTL with different configurations of u and l The proposed BLogFTL has two parameters u and l. Param- eter u is used to determine the maximum number of log blocks that can be allocated to a data block. Parameter l is used to control the maximum associativity of a log block.

That is, at most l data blocks can be mapped to a log block.

We conduct a set of experiments to show the impact of parameters u and l on the system response time in BLogFTL.

For each workload, we set the number of log blocks to be 512, and then apply different configurations of u and l.

Table 6 shows the experimental results. For workloads SD Card and NFS, different configurations of u and l do not affect the average system response time. These results further illustrate that the read-dominant workloads may not influence the configurations of associativities of log blocks.

For standard benchmarks Bonnie, Postmark, and Tiobench, the average system response time shows different trends for u and l. It can be observed that Parameter u has no big impact in the system performance. The general trend is that the increasing of u normally leads to longer system response time. This is because log blocks have been full when most data blocks do not achieve their maximum associativity. On the other hand, Parameter l has more impact on the system performance. When l is increased from 2 to 4, the average response time decreases. However, when it is increased from 4 to 6, the average system response time increases. This is because when a log block is allowed to be utilized by too many data blocks, it may involve more erase operations in Space-GC.

In this set of experiments, we also test two write- intensive workloads (CAMRESHMSA01 and CAMRE- SISAA02) from the SNIA IOTTA trace repository. For work- loads CAMRESHMSA01 and CAMRESISAA02, the write request ratios are 84% and 88%, respectively. The flash mem- ory capacity is configured as 500 MB, which is the largest capacity that the evaluation board can support. The data size for workloads CAMRESHMSA01 and CAMRESISAA02 is 1 GB. When l is fixed (l = 4 in the experiments) and u is increased, a log block can serve more data blocks. that more log blocks could be served for the requests. Therefore, for write-intensive workloads, the average system response time is improved due to better utilization of log blocks.

When u is fixed (u = 2 in the experiments), the case for l = 4 outperforms other cases. For the case where l = 2, as one log block can only serve two data blocks, it may not be fully utilized before garbage collection occurs. On the other hand, when l is equal to 6, more data blocks get involved    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2679180, IEEE Transactions on Computers   TABLE 6 The average system response time (?s) from BLogFTL with different u and l .

Workloads (l=4, the number of log blocks is 512)  l=4, u=2 l=4, u=3 l=4, u=4 l=4, u=5 l=4, u=6  Bonnie 150MB 4,909 5,022 4,893 4,908 4,906 Bonnie 200MB 5,599 5,776 5,636 5,628 5,736  Postmark 150MB 4,611 4,640 4,636 4,655 4,640 Postmark 200MB 4,588 4,671 4,645 4,641 4,653 Tiobech 4thread 4,584 4,915 4,903 4,819 5,064 Tiobech 6thread 5,422 5,973 6,017 5,894 6,030  SD Card Large File 4,635 4,635 4,635 4,635 4,635 SD Card Small File 4,621 4,621 4,621 4,621 4,621  NFS Large File 5,246 5,246 5,246 5,246 5,246 NFS Small File 5,236 5,236 5,236 5,236 5,236  CAMRESHMSA01 6,759 6,581 6,145 5,231 4,913 CAMRESISAA02 6,950 6,429 6,069 5,500 5,065  Workloads (u=2, the number of log blocks is 512)  l=2, u=2 l=3, u=2 l=4, u=2 l=5, u=2 l=6, u=2  Bonnie 150MB 5,036 5,013 4,909 4,985 4,984 Bonnie 200MB 5,772 5,684 5,599 5,823 5,738  Postmark 150MB 4,647 4,631 4,611 4,642 4,640 Postmark 200MB 4,647 4,650 4,588 4,639 4,638 Tiobech 4thread 6,090 5,743 4,584 4,619 4,627 Tiobech 6thread 6,336 6,286 5,422 5,610 5,004  SD Card Large File 4,635 4,635 4,635 4,635 4,635 SD Card Small File 4,621 4,621 4,621 4,621 4,621  NFS Large File 5,246 5,246 5,246 5,246 5,246 NFS Small File 5,236 5,236 5,236 5,236 5,236  CAMRESHMSA01 7,184 6,828 6,759 7,090 7,505 CAMRESISAA02 7,871 7,217 6,950 7,651 8,319  Bonnie-1 50MB Postmark  -150MBTiotech-4 thread  SDCard C opy - Lar  geFile NFS - La  rgeFile        Va lid  P ag  es C  op y  PFTL DFTL GFTL SFTL BlogFTL  (a) Bonnie-2  00MB Postmark  -200MBTiotech-6 thread  SDCard C opy - Sm  allFile NFS - Sm  allFile        Va lid  P ag  es C  op y  PFTL DFTL GFTL SFTL BlogFTL  (b)  Fig. 7. The valid pages copy of different FTLs over ten workloads.

in the garbage collection operation. Therefore, the case for l = 4 achieves the best system response time.

6.2.4 Comparison with different FTLs  In this section, we compare the proposed BLogFTL with several FTLs that can support MLC NAND flash. We set u = 2 and l = 4 with 512 log blocks in BLogFTL.

This configuration is selected because we want to make a fair comparison with Superblock FTL. In Superblock FTL, N = 4 and M = 2 with 512 log blocks, which is the major configuration used in the evaluation in [9]. No cache is applied for both the schemes. For DFTL, 20 KB RAM space is used as the cache so the total RAM spaces used by DFTL and BLogFTL are similar. For GFTL, the number of log blocks is 512 as well. During this experiment, in order to impose sufficient pressure on garbage collection for the benchmarks, we configure the overall flash memory to be 300MB.

Figure 7, Figure 8 and Figure 9 present the experimental results for garbage collection overhead and average system response time (us) for each FTL scheme. It can be observed that BLogFTL can greatly reduce valid pages copy and erase  Bonnie-1 50MB Postmark  -150MBTiotech-4 thread  SDCard C opy - Lar  geFile NFS - La  rgeFile        Er as  e C  ou nt  s  PFTL DFTL GFTL SFTL BlogFTL  (a) Bonnie-2  00MB Postmark  -200MBTiotech-6 thread  SDCard C opy - Sm  allFile NFS - Sm  allFile        Er as  e C  ou nt  s PFTL DFTL GFTL SFTL BlogFTL  (b)  Fig. 8. The erase counts of different FTLs over ten workloads.

Bonnie-1 50MB Postmark  -150MBTiotech-4 thread  SDCard C opy - Lar  geFile NFS - La  rgeFile             Av g  R es  po ns  e Ti  m e  (u s)  PFTL DFTL  GFTL SFTL BlogFTL  (a) Bonnie-2  00MB Postmark  -200MBTiotech-6 thread  SDCard C opy - Sm  allFile NFS - Sm  allFile             Av g  R es  po ns  e Ti  m e  (u s)  PFTL DFTL  GFTL SFTL BlogFTL  (b)  Fig. 9. The average system response time of different FTLs over ten workloads.

operations so as to improve the average system response time in contrast to DFTL, GFTL and SFTL. The improvement of valid pages copy over superblock FTL is up to 98.84% for benchmark named Tiobench-4 thread. For erase counts, as Figure 8 shown, in most cases, BLogFTL has the second best result and the maximum improvement over SFTL is 87.48% for the NFS Large File benchmark. In terms of overall average system response time, the improvement over SFTL is up to 18.97% for benchmark named Postmark 200MB.

Comparing to page-level FTL schemes (i.e., PFTL and DFTL), our BLogFTL experiences longer average response time than PFTL and shorter average response time than    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2679180, IEEE Transactions on Computers   TABLE 8 The wasted free pages for data blocks upon the garbage collection with different u and l .

Workloads (l=4, the number of log blocks is 512)  l=4, u=2 l=4, u=3 l=4, u=4 l=4, u=5 l=4, u=6  Bonnie 150MB 5,600 4,618 4,199 4,108 4,011 Bonnie 200MB 34,538 21,420 16,613 16,628 15,810  Postmark 150MB 1,184 933 918 862 857 Postmark 200MB 1,912 1,384 1,148 1,053 1,009 Tiobech 4thread 1,024 792 630 603 587 Tiobech 6thread 3,840 3,473 3,087 3,066 2,987  CAMRESHMSA01 146,304 110,683 102,605 94,506 89,217 CAMRESISAA02 194,695 151,949 136,308 117,329 108,023  Workloads (u=2, the number of log blocks is 512)  l=2, u=2 l=3, u=2 l=4, u=2 l=5, u=2 l=6, u=2  Bonnie 150MB 5,952 5,664 5,600 5,744 5,809 Bonnie 200MB 36,509 35,169 34,538 36,628 37,810  Postmark 150MB 1,368 1,352 1,184 1,320 1,557 Postmark 200MB 2,752 2,504 1,912 1,953 1,889 Tiobech 4thread 2,368 1,725 1,024 1,500 1,587 Tiobech 6thread 4,244 3,941 3,840 3,893 3,987  CAMRESHMSA01 151,477 146,304 145,878 169,222 196,747 CAMRESISAA02 224,014 203,850 194,695 238,432 277,789  TABLE 7 Performance Comparison of SFTL and BLogFTL with different  percentages of randomness.

Workloads Metrics FTL Schemes improve  SFTL BLogFTL ment  20%rand Valid-page copies 40615 17939 55.8% Erase counts 1295 991 23.5% response time (?s) 5,969 5,414 9.3%  40%rand Valid-page copies 44289 25567 42.3% Erase counts 1360 1113 18.2% response time (?s) 6,730 5,775 14.2%  60%rand Valid-page copies 47701 44572 6.6% Erase counts 1515 1475 2.6% response time (?s) 7,139 6,683 6.4%  80%rand Valid-page copies 51465 44767 13.0% Erase counts 1525 1513 0.8% response time (?s) 7,569 6,728 11.1%  100%rand Valid-page copies 54565 46578 14.6% Erase counts 1575 1549 1.7% response time (?s) 7,988 7,140 10.6%  DFTL. Page-level FTL schemes should get better perfor- mance at normally conditions; however, it is hardly used in low-end embedded systems because it consumes too much RAM space to maintain the address mapping table.

Demand-based FTL schemes have shown a promising solu- tion to reduce the RAM cost. These schemes can store the majority of address mapping tables in flash memory and only store the demanded mapping items in RAM. Although demand-based FTL schemes can solve the critical issue in RAM cost for page-level address mappings, they still suffer from long access latency to track the mapping items in flash.

Besides, when there are garbage collection, more and more data blocks or translation blocks will be merge in order to release free spaces for incoming requests which worsen the performance of DFTL. Our BLogFTL can be extended to demand-based FTL schemes to further reduce the RAM cost.

These results show that the proposed BLogFTL can capture the properties of log blocks and reduce unnecessary garbage collection operations.

6.2.5 BLogFTL with different percentages of randomness  BLogFTL can handle both sequential and random access- es. In fact, the log blocks are effectively utilized due to allocating multiple physical blocks to one log block when handling random accesses. In order to demonstrate this, we conduct a set of experiments using Iometer. Based on the experimental results shown in Table 7, it can be observed that by increasing the percentages of random accesses, the access latencies of both BLogFTL and SFTL become worse.

This is because a physical block may cater multiple logical blocks with random logical addresses, which will trigger more erase operations and valid page copy operations dur- ing each garbage collection. From the experimental results, it can be seen that our technique can still get better access latency compared to SFTL.

6.2.6 Wasted free space for different configurations of u and l  As discussed in Section 3.3, there are only three situations that a merge operation will be triggered. The first situation is handled by reduced-order merge, and it will not erase log blocks. For the second situation, either the log block is full, or it will be handled by associativity-GC, which will be further handled by reduced-order merge. Since reduced- order merge will not erase the log block, the second situation also does not waste free pages. For the third situation, there is no free space in log blocks. This case will trigger the garbage collection of log blocks. However, the log block in this situation does not contain free pages. Therefore, our BLogFTL will not waste any free pages under all three situations. To prove our observations, we also conduct experimental results to quantitatively evaluate the number of free pages in the log blocks upon the garbage collection operations. As expected, our experimental results show that no free pages are wasted. Furthermore, we also evaluate the number of free pages in the data blocks when the garbage collection operations occur. The experimental results are shown in Table 8. The results with different configurations    0018-9340 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2017.2679180, IEEE Transactions on Computers   of u and l still follow the trends that are found in the average response time in Table 6.

7 CONCLUSION AND FUTURE WORK  In this paper, we proposed a block-level log-block man- agement scheme called BLog by which the update pages of a data block can be collected together and put into the same log block as much as possible. We also proposed a novel partial merge operation called reduced-order merge by which we can effectively postpone the garbage collection of log blocks. Based on BLog, we designed an FTL called BLogFTL for MLC NAND flash. We conducted experi- ments, and the results show that BLogFTL outperforms the previous log-block-based FTLs for MLC NAND flash.

There are several new directions for future work. Partial page programming can help improve the lifetime of NAND flash. We will study how to extend BlogFTL by incorporat- ing this in the future. How to extend BLog to other log- block-based FTLs should be an interesting problem. The concept of reusing a data block that has may free pages as a log block has the benefit of postponing the garbage collection operation and improving the average system re- sponse time [29]. We plan to combine BLog with physical block reuse strategy to further improve the performance. We also plan to extend BLog to other emerging technologies, e.g., phase-change memory (PCM) [30], [31], storage class memory [32], spin-transfer torque RAM (STT-RAM) [33], memristors [34], and 3D memory hierarchy [35], [36] to further reduce the system response time and enhance the endurance and power consumption.

8 ACKNOWLEDGMENTS  The work described in this paper is partially supported by the grants from National Natural Science Foundation of China (Projects 61502309 and 61373049), the Research Grants Council of the Hong Kong Special Administrative Region, China (GRF 152138/14E and GRF 15222315/15E), the Hong Kong Polytechnic University (4-BCBB), Guangdong Natu- ral Science Foundation (2014A030313553, 2013B090500055, 2014A030310269 and 2016A030313045), Shenzhen Science and Technology Foundation (JCYJ20150529164656096 and JCYJ20150525092941059), and Natural Science Foundation of SZU (701-000360055905, 701-00037134, and 827-000073).

This version is a revised version. A preliminary version of this work appears in the Proceedings of 14th ACM SIG- PLAN/SIGBED Conference on Languages, Compilers and Tools for Embedded Systems (LCTES) [37].

