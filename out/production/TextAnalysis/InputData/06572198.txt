Rules Maps for Scheduling Algorithm Knowledge Martin Dubois

Abstract?The increasing possibilities of the multicore chip and system on a chip have brought task scheduling to the forefront of efficient system design. In previous work, we described a method to predict the effectiveness of a scheduling algorithm for a given application. It was based on association rules between the attribute values of a set of sample applications and one or more performance metrics for the scheduling algorithm. We then used the approach to devise a methodology to compare scheduling algorithms two at a time, using rule confidence differences. In this paper, we describe a visualization approach that allows comparing an arbitrary number of algorithms, by showing the relevant data in different perspectives thanks to a rules map. Three examples illustrate the effectiveness of the method.

Index Terms?scheduling, data mining, knowledge, list heuristics, directed acyclic graph  1. INTRODUCTION Users consistently demand better performance for embedded, mobile and cloud systems, in addition to low power consumption and small chip area. The typical approach to system design in multiprocessor systems is currently to partition applications into interacting tasks that are executed on one or more processing units, and tools are needed to help the designers allocate and schedule them on those units.

Task-scheduling has received wide attention in the past [1]?[6]. The typical work starts with a set of user applications and runtime architecture, and optimizes the execution process according to one or more constraints.

The usual means to do so starts by representing an application with a directed acyclic graph (DAG) where each node stands for a processor and each arc is a communication link between nodes. Then, several methods exist to schedule node execution [1][4][6], most of them focusing on optimizing a limited number of parameters such as execution speed, number of processors and communication contention. However, there are various ways to compute the cost of a node or arc [6], and the different scheduling approaches can lead to distinct performances for the same DAG.

Since each of the many scheduling methods that have been reported in the literature has its own parameters and can have numerous sub-algorithms (defined as variations with different properties and strengths), choosing the right one for a given application is not obvious and various performance metrics must be computed to compare algorithms and make a decision. Currently, the comparison is based on statistical measures, with global conclusions that apply on average and may not be appropriate at the local individual application level. A more focused approach should start with distinct application information and match it against the performance of each scheduling algorithm in order to determine appropriateness for the application. This paper presents a method to exhibit such  knowledge using a colored map, leading to a novel perspective on choosing the right one for the user?s application. It operates by providing a visual overview of algorithm performance versus application attribute values, making algorithm comparisons easy to make. This is accomplished by finding association rules [7] between the application attributes and individual algorithms performance metrics, and then visualizing the obtained information as colors in a map.

The paper is organized as follows: in section 2, we provide background about the scheduling problem; in section 3, we present our mapping methodology to extract knowledge about scheduling algorithms. Section 4 presents experimental results that show the potential of the rules mapping technique we used. The last section discusses the proposed methodology before conclusion.

2. BACKGROUND Given a set of N tasks comprising an application, and the allocation and scheduling of each task on an execution unit in a given deployment architecture, we start by creating a high-level abstraction of the problem with a DAG.



I. Task-Scheduling Problem A DAG is formally defined as a couple G = <V, E>,  where V represents a set of nodes and E a set of forward- oriented edges between the nodes. Each node is indivisible (atomic) and can be duplicated and implemented in software or hardware. Moreover, each node and each edge have performance constraints imposed upon them by the designer. The DAG can be implemented with a homogeneous or heterogeneous architecture, distributed or not, and the architecture must usually satisfy constraints such as number of processors, power consumption and speed [6]. The problem is to allocate and schedule the different nodes and their interactions to meet the imposed constraints. Since the scheduling problem is NP-Complete, except for a few basic cases, the standards methods for addressing it use estimated values for the node computation and communication costs, and focus on specific architectures to reduce complexity [5].

At least four classes of algorithms have been proposed in the literature to solve the scheduling problem: list, clustering, duplicated and random. The list methods include the performance-effective task scheduling (PETS), heterogeneous earliest finish time (HEFT) and critical path on a processor (CPOP) algorithms [4][7]. They are based on timing levels and prioritized nodes. Each node has a priority, and the algorithm schedules nodes starting with the one having the highest priority. In the clustering class, we find the clustering and scheduling system (CASS) method [10] which clusters nodes into groups, tapping the advantage of using more processors to reduce schedule length. In the duplicated class methods, the critical path fast-duplicated (CPFD) algorithm [11] allows node          cloning; the resulting node multiplicity copies methods for reducing communication delays. Finally, the random methods mostly use genetic or simulated annealing [12][13]; they can be very fast and are simple to implement.

All the previous techniques make architectural assumptions about the system under consideration, with maximum efficiency for specific architectures. Thus, the solutions they offer depend greatly on which graph and performance estimators are used, and on which deployment architecture is targeted.



II. Performance Metrics and Estimators The existing performance metrics for algorithm  comparison belong to two groups. The first one evaluates the scheduling algorithm itself. The normalized schedule length (NSL), the scheduling length ratio (SLR) and the speedup ratio [14][4] are examples of such metrics. NSL defines the system length as the longer time between the first and last tasks; SLR represents the ratio between the length and the critical path, defined as the longest path in a DAG, including all arcs costs and computing node costs.

The speedup ratio (SR) for a given application is the execution time ratio between a sequential execution and parallel execution.

The second group of performance metrics serves to compare algorithms. The average percentage degradation (APD) and the number of best solutions (NB) [6] are typical examples. APD evaluates how bad an algorithm will perform with respect to another, and NB is the number of times that an algorithm will perform better than another.

Both groups of performance metrics are effective for evaluating algorithm performance in general, but they offer no guaranty of applicability to a specific DAG. This is a weakness as useful local application information exists which can be easily extraction from the each instance of the DAG profile. For instance, it may include the DAG?s width, depth and communication-to-computation cost ratio (CCR), defined as the complete communication cost of each arc over the total computation cost of each node. This information provides insight that may also help profile a scheduling algorithm in terms of strengths and weaknesses with respect to the application under study. In particular, patterning can be used in this area as described below.



III. Association Rule Mining  We first define a few concepts as used in this work:  Association rule: Implication x ? y, where x and y are disjoint subsets of a pattern p, defined as a binary sequence where each bit position denotes the presence or absence of a specific application attribute or scheduling algorithm performance metric value, or range  thereof. In other words, using the bits in p, the association rule reveals the co-occurrence of application attribute and performance metric values that led to the pattern.

Rule confidence: Given a set of different patterns P = {pi}i=1,2?,M,  the confidence level of an association rule with respect to P is the relative measure of how often the consequent of the rule is true when the antecedent is true:  )( )()(  xSupport yxSupportyxConfidence ?=?                        (1)  Support(x) is the relative frequency of x within P. Thus,  confidence(x ? y) can be viewed as the estimate of the conditional probability of y given x. There exist frequent and rare association rules [16], depending on the level of minimal support.

We proposed in [20] to use association-rule learning to find the strengths and weaknesses of an algorithm with respect to the attribute values of an application. The algorithm operated by first encoding the relevant input information in a binary format. For example, consider a DAG profile defined by the tuple (N, C, ?, ?, D), where N is the number of nodes, C the communication-to- computation cost ratio (CCR), ? the parallelism ratio1, ? the range of computation of each node1 and D the DAG density. Then a given DAG profile instance and metric value of interest can be easily turned into an equivalent binary representation using bit position encoding.  For example, for E = (20, 0.3, 0.8, 0.8, 0.7) and M=2.5, we could obtain the sequence 100 100 010 010 0010 001. This sequence can be labelled N1 N2 N3 C1 C2 C3 ?1 ?2 ?3 ?1 ?2 ?3 D1 D2 D3 D4 M1 M2 M3 for easy referral to the individual bits and corresponding attribute or metric values they represent, and association rules can pair performance levels with specific attribute values after a training stage.

The training is done with a set of data obtained by applying one (or more) scheduling algorithm to different applications.  More details are provided in [20]. In the end, given a binary outcome M that denotes the presence or absence of a desired performance and a set of scheduling algorithms, we derive a set of rules for each one with respect to M. Then, each rule?s premise will be a set of attribute values in a DAG profile instance (application characteristic) and each conclusion is the corresponding value of M. For example, a scheduling algorithm A might have the rule (?2, ?3, C1)? M, meaning that the triple of values (?2, ?3, C1) for DAG profile attributes ?, ? and C leads to a positive outcome for M, while a scheduling algorithm B might have no such rule, or have the same or opposite consequent. Then, one could infer from this information how relevant the presence of triple (?2, ?3, C1) is to algorithm A or B with respect to M.

We also showed in [20] how association rules learning can be used to compare scheduling algorithms with respect to a given outcome by using ? rules. We defined a ? rule by the difference in confidence levels of two scheduling algorithms for the same rule. For example, suppose that algorithm A has 80% confidence for rule R and algorithm B has 70% confidence for the same rule. Thus, the ? rule of R has ?10% confidence, meaning that algorithm A is preferable to algorithm B when rule R applies in an application as it has 10% higher confidence.

However, the methodology described in [20] could only compare scheduling algorithms two at a time. In this paper, we propose an approach that does so for an arbitrarily large number of scheduling algorithms. We also present a new approach about rule mining.

3. PROPOSED METHODOLOGY  Given a DAG, we would like to have a performance picture when a list of scheduling algorithms is applied to it. To do  1 The parallelism ratio is defined as ? N/DAG depth; the range of  computation of a node is defined as (Cmax -Cmin)/2Cm, and where Cmax, Cmin are the maximal, and minimal computation costs of the node, and Cm is the average computation cost of all nodes, respectively.

this, we propose a visual approach inspired from heat maps where colors replace the entries of a matrix. Given LA a list of algorithms, we define a rules map for it as a scalar matrix RM = [Ri,j]1?i?m,1<j<n  where each element represents a relevant information about a scheduling algorithm and an association rule, typically a confidence level. Then, the rules map is converted to a heat map by mapping its elements into color zone gradients via a mapping vector that we call GC. In this section, we show four different ways to exploit this visualization technique: rules,   ? rules, sub-algorithms and metrics maps.

The first step to build RM is the creation of two lists of association rules. The first one is the algorithm association rules (AAR) list which contains the association rules for a particular algorithm A, referred to as A.AAR. There is one such list for each scheduling algorithms. The second list is the association rule (AR) list, or common list, which is the union of all A.AAR lists. The pseudo code to create the two lists is as follows:  1) For each scheduling algorithm A, create it association rule set A.AAR for a performance metric of interest as indicated in [20].

2) Starting with  an empty list AR, for each algorithm A in LA and each rule eR in A.AAR: a) Add eR to AR if not already an element of AR.

For example, given four scheduling algorithms A, B, C and D with the following rules set:  ? A. AAR ={N1, N1C2, N1C3} ? B. AAR = {N1, N1C2, N3C4} ? C. AAR = {N1, N1C2} ? D. AAR ={N1}  we have AR = {N1, N1C2, N1C3, N3C4}.

Normally, the AAR lists are generated automatically in  step 1) above, but we can also create a custom list called CAR if desired, allowing us to start with a specific list of association rules, say CAR = {N1, N1C2}.

With the AAR (or CAR) and AR lists defined, there are many ways to create and visualize rules maps. The most obvious one uses rule confidence as the information to display.  The following pseudo-code shows how to build it with AR or CAR:

I. Common or Custom Rules Map This method involves five steps:  1. Choose rules from AR or CAR to define a list LR; 2. For each rule in LR that is not in any AAR list2, apply  equation (1) to determine its confidence level for the performance metric of interest and update its description record.

3. Label the rules map such that the antecedent of each rule in LR corresponds to a column of RM and each algorithm name corresponds to a line.

4. Set each element RMij to the corresponding rule confidence.

5. Display each RMij using the corresponding gradient color defined in vector GC.

The gradient color normally corresponds to a range  of values for the rule confidence level. This is useful for showing the level of sureness in terms of patterns and anti- patterns (This subject will be the topic of another article by  2 This can happen with a custom list or a scheduling algorithm that does  not share rules with already studied ones.

the authors). The idea is to easily identify spots in RM that indicate when a scheduling algorithm can be applied confidently or not. For example, the good pattern zone will follow a green color gradient to display the high confidence values, while the anti-pattern zone will follow a red color gradient.



II. ? Rules Map Rule We can also use a rules map with ? rules to have a  better understanding of algorithm performance with respect to a reference A1.   The method is similar to the previous one; exception for the use of ? rule confidence for the elements of RM is step 4. This allows easy identification of algorithms that are better or worse that a reference, for each of set of rules. For example, we can display a ? rule greater than 15% as green and a ? rule lower than ?15% as red.

Then, being in the green zone means that it is better to stay with the reference algorithm; else, we can improve the situation by using the red zone?s data to merge algorithms or create a rules class algorithm as shown in [21].



III. Sub-algorithm Rules Map We define a sub-algorithm as the same technique  implemented in a different way. A scheduling method such as HEFT has more than ten declinations. Creating and visualizing the corresponding rules map can show the impact of using the different algorithmic variations. The process is the same as in sub-section I, but sub-algorithm names will be used instead of different algorithms. Each row will then represent a different implementation approach.



IV. Impact of the Estimators It is well known that the choice of metric and how they  are computed has an impact on determining the performance of a scheduling algorithm. To visualize this situation, we use a list of performance estimators to replace algorithm names. Given an algorithm of interest, this will show the impact of each metric on the confidence levels of different rules. The rules list can be from AR or CAR and a column with the same color indicate that the algorithm of interest is consistent with respect to different estimators.

4. EXPERIMENTAL RESULTS We used the approach in [20] to generate data to display with our heat map visualization technique. Fig. 1 illustrates the comparison of three scheduling algorithms, CPOP, PETS and HEFT using a heat map. The x-axis represents different attribute values from the signatures of a group of DAGs and the y-axis identifies the algorithms (it could also have been used for performance metric values). The left part of Fig. 1 gives the rule confidence to color mapping. A red zone indicates a rule with low confidence, while a green zone indicates a rule with high confidence.

Fig. 2 illustrates the comparison of a set of sub- algorithms for HEFT. Finally, Fig. 3 illustrates the impact of using different estimators in a scheduling algorithm. In this figure, HEFT, HEFTB, HEFTW and HEFTME are the default, best, worst and median costs used by HEFT. As each figure shows, the use of a heat map makes it easy to assess the performance of an algorithm (or the effectiveness of a performance metric) given the presence of one or more attribute values in the DAG signature.

Figure 1. Example of a rules map for algorithm comparison   Figure 2. Example of a rules map for sub-algorithm comparison   Figure 3. Example of rules map for parameter effect comparison  Notice that the displayed association rules may be so at different granularities of information as the x?axis values can represent single or multiple attribute values. Moreover, the association rules can be used to investigate groups of applications. For instance, the methodology can use only two important attributes, N and CCR, and the designer can separate a set of applications into n groups that are represented by as many columns in the rules map. This constitutes an effective way to represent the performance data instead of using a text format.

5. DISCUSSION AND CONCLUSION We have shown a technique to visualize rules map for scheduling algorithm comparisons and provided examples of use. The methodology is applicable to either a single algorithm or a group, and among it features is the easy identification of problem spots in the rules map. The y?axis in the heat map can represent either algorithms or metrics to evaluate their performances, individually or in groups.

As a result, the interpretation of the heat map depends on the kind of data we use in the rules map.

The use of heat map visualization increases the  designer?s knowledge about different scheduling algorithms, and the various combinations of x and y axis variables are nearly endless. In conclusion, the use of association rules learning to derive rules maps and the visualization method described in this paper gives a new perspective to view scheduling algorithms and it can be a welcome complement to traditional benchmarks.

