978-1-4244-5998-8/10/$26.00 2010 IEEE

AbstractThis paper presents a new classification and search  method of 3D object features views.

This method is an application of algorithms:  Charm  for an object views classification purpose  Algorithm for extracting association rules in order to  extract the characteristic view.

We use the geometric descriptor of Zernike Moments to index 2D  views of 3D object. The proposed method relies on a Bayesian  probabilistic approach for search queries. The resulting outcome  is presented by a collection of 120 3D models of the Princeton-  based benchmark and then compared to those obtained from  conventional methods.

(Abstract)  Keywords-component; Model 3D, indexation 3D, characteristic  views, Zernike Moment, CHARM, association rules.



I. INTRODUCTION  The rise of new technologies and communication networks  that emerged from the internet made it easier to create, use and  distribute three-dimensional models. The development of  modeling tools, 3D scanners, 3D graphic accelerated hardware,  Web3D and so on is enabling access to three-dimensional  materials of high quality.

In recent years, many systems have been proposed for  efficient information retrieval from digital collections of  images and videos. However, the solutions proposed so far to  support retrieval of such data are not always effective in  application contexts where the information is intrinsically  three-dimensional. The 2D/3D shape retrieval methods are    based on the computation of 2D images rendered from multiple  viewpoints, considering that two models are similar when they  look similar from all viewing angles. The 3D model is then  indirectly represented by various 2D-shape descriptors  associated with 2D views so that the 3D-shape matching is  transformed into similarity measuring between 2D images.

Silhouettes and depth-buffer images are the most widespread  projections. A silhouette is a binary image whereas a depth  buffer image contains the information of the distance between  the object and the viewing plane in its pixels.

In [1], the silhouettes are encoded by their Zernike  moments and Fourier descriptors. The dissimilarity between  two 3D models is defined as the minimal dissimilarity of the  silhouettes over all rotations and all pairs of vertices on the  corresponding dodecahedrons. Filali Ansary et al [2] introduce  a novel probabilistic Bayesian using a characteristic views  selection. Curvature scale space (CSS) descriptor is used in [3].

Many of the 2D/3D approaches based on depth images [4, 5, 6]  use the two-dimensional discrete Fourier transform (2D-DFT)  as a 2D-shape signature. They differ mainly in the similarity  estimation technique used and in the number of views retained.

Chaouch et al [5] have introduced a technique to enhance  2D/3D shape descriptors. To take into account the dispersion of  information in the views, they associate to each view a  relevance index.

In this paper, we propose a data mining model of 3D object  indexing based on 2D views. The goal of this model is to  provide a method for optimal selection of 2D characteristic  views from a 3D object, and a probabilistic Bayesian method  (introduced by T. Filali Ansary and all [9] [10][11])   for 3D  object indexing from these views. The data mining model  based on Algorithm CHARM[2] for construction .The data  mining model is totally independent from the 2D view  descriptor used, but the 2D view descriptors should provide  some properties. The data mining model has been tested with  Zernike Moment 2D descriptors [16] .

Section 2 describes the different methods of 3D object  indexation by views. Section 3 details our approach for  characteristic views selection and search 3D object indexing  and explains the principle of association rules and its  algorithm. Finally, the results obtained from a collection of 3D  models are presented for Zernike Moments 2D view descriptor    showing the excellent performances of our Data mining Model.



II. METHODS OF 3D OBJECT INDEXATION  A. Indexation 2D/3D  The methods that use the 3D model views came from  psychophysical results which imply that the human visual  system represents 3D objects as a group of 2D views rather  than a three-dimensional model. The objective of these  methods is to search three-dimensional models that are the  closest to the 2D image.

This process can be divided into two phases: an indexation  phase and a search phase. In the indexation phase, for each and  every three-dimentional model in the base, we determine the  characteristic views and their associated vectors using a  method of 2D form analysis : : CSS[3], ART[4], Zernike[5][6]  etc. Then comes the search phase where the requested image  goes through a process similar to views in the base, in which a  descriptor is computed and compared to the descriptors of the  base.

The two main issues are the choice of the finest indexation  method of 2D views and the choice of the optimum number of  views to be taken into account and which views are to be used.

The selection of the form descriptor of 2D views will not  modify the main concept, but it will change the performances  and efficiency according to the chosen method.

In our study, Zernike Moments were chosen to describe  characteristic views.

B. Search of association rules  Introduced by Agrawal and al.[7], the searching method of  association rules has been proposed to allow us to analyze the  supermarket sells to extract rules  such as  : when a customer  buys bread and butter, he also buys milk 9 times over 10. As  Hbrail suggests [8], even though this association rule concept  was developed for marketing purposes, it could also be  implemented in another field of research like the research of  frequent co-occurrences, pairs or values if the data structure is  in accordance with this research.



III. PROPOSED METHOD  A. Properties of the algorithm of selection of characteristic  views  Now that we have each initial view represented by a vector  of Zernike Moments, we wish to reduce this set of vectors to  the minimum by removing the redundancies that could have    been created in a row of BDT (impossible to see 2 equal views  in the same row of a BDT).

We use the 2 phases of the extraction algorithm of  association rules (described above) between our BDT views in  order to classify the set of views (the number of classes is  unknown) depending on the choice of the threshold distance  and to extract the characteristic view of each class.

According to CHARM[2], the outcome will be a certain  number of groups. Each group will contain a set of minimal  reduced views depending on the minimum support threshold.

These views are the most frequent ones.

We use the confidence value between the views of the same  class to extract the characteristic views of the 3D Object (one  view per class only). The characteristic view of a class is the  one that has the minimal confidence value in relation to the sets  of views of its class.

B. Algorithm of extraction of association rules  CHARM Algorithm  CharmEtend  Input : [P],FC  Output: FC modified  for every  Ai x e(Ai) ? [P] do  [Pi] =0 and X= Ai  ? for  every Aj x e(Aj) ? [P] & Aj?Ai  ? X=X?Ai  ? Y=e(Ai) ? e(Aj)  ? Charm-Properties([P], [Pi])  ? if [Pi] ?0 then  CharmEtend([Pi],FC)  Delete([Pi])  FC=FC?X  ? End if  ? End for  ? End for  return FC  charm properties :  ? For every itemset X, support(X)=support((X))  ? The rule X -p?Y is equivalent to the rule (X) -q?(Y)  and p=q  ? We have X1 x e(X1) and X2 x e(X2) and X1 ? X2  ? if e(x1)=e(x2) then e(X1 ?x2) =  e(x1)?e(x2)=e(X1)=e(x2) => we replace each    occurrence of X1 by X2  ? if e(x1) ?e(x2) then e(X1?x2) =  e(x1)?e(x2)=e(X1)?e(x2) => we replace each  occurrence of X1 by X1 ?x2 because X1 appears  in every object where x2 appears  ? if e(x1) ?e(x2) then e(X1  ?x2)=e(x1?x2)=e(X2)?e(x1) => we replace  each occurrence of X2 par X1 ?x2 because X2  appears in every object where x1 appears  ? if e(x1) ? e(x2) then e(X1?x2)=  e(x1?x2)?e(X2)?e(x1) => we remove nothing.

X1 and X2 lead to a closed 1.

C. Generation of association rules  The association rules that are used here are not limited to the  rules which consequences are composed of one single item. To  generate the rules, we take into account all non empty sub-sets  of f for each frequent itemset of f. And, for each of these sub-  sets h, we return a rule in the form of h => (f-h) if the ratio  support  (f) /support (h) equals at least a minimal confidence  threshold  minconf. We evaluate all the sub-sets of f to  generate the rules which consequences are the itemsets of size  greater than one.



IV. EXPERIMENTS AND RESULTS  In order to measure the performance of our algorithms, we  used non-complex objects the Princeton benchmark base that is  classified into 20 classes depending on their visual similarity.

The 3D Objects of figure 1 shows examples of 3D objects of  different classes.

In our current implementation, Zernike moments are  extracted from views from second order. To compare two  Zernike moments, the distance Minkowski order 1 is used.

Figure 1.  Examples of 3D Objects of different classes.

The choice of a minimum support and a minimum  confidence threshold is justified by the number of experiences.

For a minsup <50%, we will have enough characteristic  views, with unsignificant views among them.

For a minsup >60%, we will have a small number of  characteristic views which represent a limited number of initial  views of the 3D object.

In order to get characteristic views that represents best their  class, it is better to choose a minsup that belongs to the interval  [48% , 60%], as well as a minimum confidence threshold    greater than 70% .

A. Steps of the experiment  Example 1: extraction of characteristic views :  Based on a minimum support of 50% and a minimum  confidence threshold of 80%, we tested our method on a 3D  object of the class Helicopter. The obtained outcomes are  shown in figure 2:  Figure 2. Characteristic views of the object  m1312.off  Example 2: Research of objects that are similar to the 2D  requested view  Using a minimum support of 50% and a minimum  confidence threshold of 80%, the research of similarity  according to our method and the use of probability on a 2D  request example produces the outcomes displayed in figure3.

Figure 3. 3D objects that are similar to the requested 2D view of class  airplane  B. Measurement of performance indices  For a more accurate analysis of performance, a study in  terms of curve (Recall/Precision) will be performed. This can  be denoted by the following fomula (Osada)[15] :        We apply our algorithm  CHARM and Extraction of  association rules between views and the Kmeans algorithm  over 10 requested views of the same object of the class airplane  of the Benchmark base. These views are randomly selected by  a program. ie from a 342 views of the object "fighter", Our  search algorithm selects a random 10 views queries. Figure 4  displays results comparing Kmeans (in red) and the suggested  algorithm in blue.

Figure 4. Curves of Recall/ Precision with values of 10 requested views  To measure the performance of our algorithm, we applied  the two indices of performance mentioned above.

Figure 4 displays the Recall/Precision curve computed using  our algorithm and the Kmeans curve. In this figure, we notice  that the accuracy score of the first 35 K (first third) is close to  100% for our suggested method. This means that most of the  objects that are similar to the requested object are placed at the  top of the search list. And, as of the K greater than 35 (second  thirds) in the search list, the Recall score goes near 100%,  which signifies that the rest of the objets (belonging to the    same class) that are similar to the requested object are  ranked/placed* as two thirds in the search list. These indices  are in accordance with the visual results displayed in Figure 3.

On the other hand, the Kmeans algorithm gives a result  between 70 and 33% for the first 35K (first third). This implies  that the first third of the search list contains a combination of  objects: those belonging to the same class as the requested  object and others belonging to other classes. Moreover, the K  that is greater than 35 corresponds to a score less than 85%,  which suggests that the rest of the objects belonging to the  same class that are similar to the requested object are far from  the top of list search.



V. CONCLUSION  In this article, we introduced the concept of 3D object  indexation, in particular the indexation from the views of these  3D objects. First, we introduced an algorithm that is  independent of the 2D used descriptor in order to extract the  characteristic views of a 3D object. The outcomes of this  method are very satisfying since this latter reduces the 3D  object size (instead of using 342 initial views, the system  automatically reduces this number depending on the threshold  of the distance  to a smaller number-). The 3D object will then  be characterized by a small number of views called  characteristic views. Next, we used the probabilistic bayesian  view (translated by T.F Ansary, J.P. Vandeborred and M.

Daoudi[5]) for the indexation of these 3D models. The  displayed results highlight the good performances of this  method compared to some classical methods of classification.

This method produces great results when the object size is  big enough, with more than 340 views per object.

These tests are performed based on Princeton Shape  benchmark.

