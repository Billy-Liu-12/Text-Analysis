Proceedings o f  1993 International Joint Conference on Neural Networks

Abstract Recurrent associative neural networks with a new model of connection weights, which we call an asymmetric Hebbian rule, are discussed. By using this model, we extend associative functions of neural networks. Networks with the model has associative functions such as conditional association, robustness to  unmemorized patterns and cooperative association are discussed. We also propose a model of multiple association modes realized by using the properties of the same model. Multiple levels of hidden memories can be embedded and they are activated or deactivated by controlling threshold values.

I Introduction The memory fmctions of our brain, which we can feel every day, are surprisingly subtle and complicated.

Associative neural networks [I] are important mod- els to  investigate these biological memory systems, however, we think that only a small part of associa- tive functions of biological memory systems has been implemented with neural network models.

Some of the main characteristics of recurrent as- sociative neural networks we focus in this article are (1) the dynamical properties (2) distributed repre- sentation of information. The motivations of our report in this article are the following. Concerning (l), we think that it is not easy for those models which have fixed point attractors as memorized pat- tern to have dynamical properties of biological mem- ory systems and concerning (2), cooperations of neu- rons have been investigated and applied in most of reported researches but coooperations between mem- orized patterns, which can be resources of associative functions, do not seem to be discussed enough.

We propose a new model of connection weights of recurrent neural networks. One of its properties, cooperative association, is applied to  new associa- tive functions such as conditional association or pat- tern sequence association are realized. The networks with this model also have multiple association modes.

With their properties multiple levels of hidden mem- ory configurations are embedded and their activation and inhibition are controlled, which configure wan- dering associative dynamics between embedded at- tractors.

2 Model In this section, we first describe a requirement of con- nection weights which is the basic assumption for the construction of our model. Then we derive, from it, an asymmetric Hebbian rule and discuss its proper-  ties especially new functions such as embedding pat- terns of various activities and cooperative associa- tion.

2.1 An asymmetric Hebbian rule The rule [2] is roughly described in table 1. The synaptic modulation occurs only when post-synaptic neuron fires. Whether the weights are increased or descreased depends on coming signal from post- synaptic neurons and the magnitudes of modulation are generally different between increase and decrease.

Table 1: Rough description of our Hebbian rule  This model, which we call an asymmetric Hebbian rule, is derived from the requirement that output sig- nals of neurons do not depend, in average, on activi- ties of network states. If activities of network states, which is one dimensional quantity, is dominant to network dynamics, distributed representation of in- formation is no longer necessary. This is the reason we have this requirement.

Mathematical description of it is that  j  holds for all a where w = (wij) is connection weights, s = ( s j  E (0, l))T is network state, a(.) := & E .  sj is activity of state s, N is number of neurons, ( 3 is average, K is a constant and 6 is a small fluctuation.

Concerning the K in eq.( l), the following calculation proves that it is equals to 0.

where E, is summation over patterns whose activity is a. Because this average is required to be constant     over various activities of s and because E, -$- does not depend on j but on the activity of s, it holds that cj wij  N 0, so that K = 0. It is also easily verified that the requirement of eq.(l) is equivalent to  (aN)  (3)  We apply this requirement to a general form of connection weights of recurrent neural network. The general form is  where p k  = ( p t , p , " ,  . . - , p & )  is a pattern to embed and cy, ,fi and y are constants. One of the sufficient conditions for a, ,fi and 'y to satisfy eq.(3) is the fol- lowing.

(5) cya(pfi) + p = 0 { Y = o  With new constants T = -$ = & and R = -p, eq. (4) becomes   When network dynamics is defined as s ( t  + 1) = O( ws - h ) where O ( x )  is Heaviside step function which is 1 if x 2 0 and 0 otherwise and where h is threshold, it  is verified that ( p k )  are fixed points of this dynamics[2]. In general, pattern transition from p to  q is embedded by adding the following to w , ~ .

(7)  2.2 Characteristics of the asymmetric Hebbian rule  The asymmetric Hebbian rule realizes embedding patterns and retrieving them which is one of the ba- sic properties of the associative memory[2]. Further- more, there are new properties which can be applied to  extend associative functions of the networks. First,  when pattern s has small correlations to  all the em- bedded patterns, it  holds that  c w i j s j  N 0. (8) j  This means that weighted sum of the receiving sig- nals of all the neurons are very small. In other words, the dynamics of networks with this rule are robust to unmemorized patterns.

Second, a state whose activity is large, in general, cannot be stable. When all the neuron fires which is the extreme case of large activity, the weighted sum of receiving signals become E, wij. Because of the requirement of eq.(3), this value is very small and the neurons do not fire at the next time. The actual up- per limit of the activity depends on conditions such as threshold value, which is principal, or memory con- figurations or normalization constants.

Third, when p and q are memorized as fixed points of the dynamics, p U q is also a fixed point where U represents boolean OR operation. This was verified by calculating cj wtj ( p j  U q j )  and numerical simulation. There are some applications of this inter- esting property of superposed patterns. For example, suppose fixed point p and transition p U q -+ r are embedded. In this case, the transition from q to T occurs only when p is activated because embedded transition is not p ---t q but from p U q. This mecha- nism realized a conditional association.

Figure 1 shows a behavior of network dynamics discussed above. In this network, five transition paths of patterns were embedded, among which there were three transitions that started with 2U8,4U9 and 6U10 respectively. (See caption for detail.) All of these elementary patterns were included in the other two paths. The graph shows that the paths which starts with the superpositions are activated when these su- perpositions were generated but not activated when only a part of the superpositions are activated. These associative properties are cooperations of embedded patterns, There are more possible applications such as association of sequences of patterns [2]. These facts means that not only memorized patterns but also their combinations are resources of information processing in these networks.

3 Multiple association modes  For the study of neural network and biological mem- ory, the dynamical properties of networks are one of the most important subjects. In human memory, for example, it seems that very complicated associations are executed. Starting with a word or a scene or a  melody, we can recall various related things, which is very different from memory systems in digital com- puters.

For models of such wandering associative behavior of memory systems, we propose multiple association modes of neural networks. A mode of associative dy- namics is a qualitative state of networks and accord-     a b C   $2    0 10 20 3 0  40 5 0 time  Figure 1: Network dynamics of an asymmetric Hebbian network. Horizontal axis is time and vertical axis represents overlaps between network state and embedded patterns. Embedded memories were five paths of ( 1 , . - . , 6 ) ,  ( 7 , s . .  , ll), (2U8,12,13), (4U9,14,15,16) and (6U10, - - ,20). The A represent activations by an external forces. The latter three paths (upper in the graph) were activated only when starting superposed pattern appeared.

ing to the modes, networks perform different asso- ciative functions. For example, Hopfield model [l] of associative networks has a single mode which realized retrieval of embedded fixed points.

T~here are reported models which have properties of multiple association modes, among which chaotic dynamics and its intermittency seem to be especially important [3, 41. Because of the asymmetric connec- tion weights and the non-equilibrium dynamics, net- works with the asymmetric Hebbian rule have some chaotic properties. In this discussion, however, we only mention its existence and leave its application open. We concentrate on models of hidden memory configurations.

Embedding multiple levels of hidden memory con- figurations by using some properties of the asymmet- ric Hebbian rule devices a model of multiple associa- tion modes. Suppose two pattern transitions  P + Q 4 P )  + 4 a >  (9) are embedded where ~ ( p )  is a pattern generated from p and some of its elements whose value is 1 are flipped to  0. We call it a nipped pattern. The problem is whether association ~ ( p )  --i q occurs or not. If memorized patterns p and q are random patterns and flipped elements of their nipped patterns are ran- domly chosen, i t  holds that  Wij.j(P) = a ( J 4 4 )  WijPj (10) j 3  because of the linearity of the summation and ran- domness of tu, p and U. This implies that appearence  and hiding, in other words activation and inhibition, of ~ ( p )  -+ q can be controlled by threshold values of neural dynamics. Suppose wijpj is normalized so that its value is nearly equals to 0 or 1 (This is when R = 1 - a(p) ) .  -Then u ( p )  -+ q is, roughly speaking, activated if the threshold values of all the neurons are less than a ( p )  and it is deactivated when they are big- ger. Because the threshold of activation/deactivation depends on the activities of nipped patterns, There are, in principle, multiple levels of association modes.

To sum up, only a part of embedded memory config- urations, such as pattern transitions, whose level is higher than corresponding threshold value of neural dynamics is active so that some qualitative properties of associative networks can be controlled by threshold values.

For a model of wandering memory association, we apply the multiple association modes to  hidden inter-memory paths. Figure 2 shows dynamics of a recurrent network. In this network the following 5 ?top-level? cycles and 1 inter-cycle hidden cycle are embedded  Note that the hidden cycle goes through all the other cycles via one of their nipped patterns. The plotted line on the bottom represents history of activity of the network and others, from the second up to top, shows overlaps between network states and memo- rized patterns, pll, p12, - - - ,  pZ1, - - - ,  p55, v(pl?),     d- - 7 I I I I I I I  50 100 1 5 0  2 0 0  2 5 0  300 3 5 0   time  Figure 2: Realizing multiple association modes and inter-memory hidden paths. In the network, 5 cycles and 1 inter-cycle hidden cycle were embedded. When network state went into a cyclic pattern, the threshold values were changed over the border of the association modes. Stating with pattern cycle which is plotted on the bottom, network states went through all other cycles.

v(p21) ,  a ,  v(p51) respectedly. In this simulation, threshold value was changed over the threshold of the association modes when the network states became a cyclic pattern. One of the threshold value was bigger than the threshold of activation/deactivation of the hidden cycle and the other is smaller. We can find the times when threshold was changed by the history of activity.

The figure shows that hidden cycle, which is inter- cycle, was activated when threshold was small and  was deactivated when it became larger value. For example, (p5") which appeared from time 65 to 123 jumped to some of other cycles at time 124. Because the hidden cycle go through all the "top-level" cycles, the network state, starting with a pattern in the cy- cle (p'" I m = l,---, 5 ), went through all the other cycles. But in general, jumping of network states between memorized cycles or fix points occurs only when there are paths of hidden memory.

4 Concluding remarks Acknowledgements w e  discussed the necessity of novel dynamical Proper- ties Of associative neural networks. For their model, we Proposed a new mAel  of connection weights of recurrent neural networks. This, which we call an Japan.

