SSDT A Scalable Subspace-Splitting Classifier for Biased Data

Abstract  Decision trees are one of the most extensively used data mining models. Recently, a number of eficient, scalable algorithms for constructing decision trees on large disk- resident dataset have been introduced. In this papel; we study the problem of learning scalable decision trees from datasets with biased class distribution. Our objective is to build decision trees that are more concise and more inter- pretable while maintaining the scalability of the model. To achieve this, our approach searches for subspace clusters of data cases of the biased class to enable multivariate split- tings based on weighted distances to such clusters. In order to build concise and interpretable models, other approaches including multivariate decision trees and association rules, often introduce scalability and perjiormance issues. The SSDT algorithm we present achieves the objective without loss in eficiency, scalability, and accuracy.

1 Introduction  Decision trees are one of the most extensively used data mining models. Decision tree induction is a greedy algo- rithm that partitions the data in a top-down, divide-and- conquer manner. Decision trees are especially attractive in mining large datasets because i) the decision tree model is easier to interpret [5] and, ii) the induction process is more efficient compared to other methods [ 12,8]. Recently, a number of efficient, scalable algorithms for constructing univariate decision trees from large disk-resident data have been introduced [ 12, 8,7, 141.

Our work focuses on the scenario where the training data has a highly imbalanced class distribution, i.e., we assume there are only 2 class labels, positive and negative, and the positive (target) class accounts for a small fraction (say be- tween 0.1% and 5%) of the entire dataset. This situation arises frequently in the data mining environment, such as in fraud detection, network intrusion detection, and etc. In this paper, we first discuss several limitations of the deci-  Philip S. Yu IBM T. J.  Watson Research Center  Yorktown Heights, NY 10598 psyu @us.ibm.com  sion tree induction process under this scenario. Then, we introduce a new technique, SSDT, which aims at overcom- ing these problems while preserving the efficiency of the decision tree algorithms.

Representational limitations of univariate decision trees The decision tree induction process has several deficiencies.

Consider the training set in Figure l(a), where P l a y ?  is the class label. The C4.5 decision tree is shown in Figure l(b), which has only one node. Obviously, it misses the pattern of (hot, high) that has a strong support of No. The difficulty is largely due to univariate tests at each node and it is also compounded by the use of categorical attributes.

Applying multivariate decision tree algorithms, such as OC 1 [ 111 and LMDT [6], to such datasets may reveal pat- terns univariate decision tree can not discover. However, it is computationally expensive to gauge the value of a linear combination of many variables per node, and is practically infeasible to use on large disk-resident datasets.

Biased data distribution Training sets of certain data mining tasks have a very biased data distribution, inasmuch as the target class accounts for an extremely tiny fraction (say 0.5%) of the data. Most learning algorithms, such as decision trees, will generate a trivial model that always predicts the majority class and reports an accuracy rate of 99.5% [ 161. However, data cases of the biased class often carries more significant meanings and are often the primary targets of mining.

Decision trees have been shown unsuitable for such tasks [ 101. In Figure l(c), we show the distribution of a syn- thetic dataset in a two-dimensional space, where the dark re- gions represent the data cases of the biased class. Data cases of the majority class are either clustered at different spots or simply distributed randomly and are not shown explicitly. A representative univariate decision tree learned from such a dataset without pruning is shown in Figure l(d). The deci- sion tree induction process keeps on partitioning the space either horizontally or vertically at each node, and the re- sulted decision tree often has a very large size. Furthermore,  542 0-7695-1119-8/01 $17.00 0 2001 IEEE  mailto:us.ibm.com mailto:us.ibm.com   Temp cool mild  I mild I normal I yes I  Humid Play?

high yes high yes  I hot I drv I yes I I hot I normal I yes I  hot high very hot high  (a) Training Set (b) C4.5 decision tree for (a) (c) 2-dimensional biased dataset (d) decision tree for (c)  Figure 1. Limitations of Decision Tree Classifiers  it is easy to see that the size of the tree will grow with the dimensionality of the data, as in a higher dimensional space more tests are required to locate the subspace region where the data cases cluster.

A decision tree that assigns an unknown instance a def- inite class label (?positive? or ?negative?) or a probability based merely on the data distribution in the leaf nodes usu- ally has a low predictive accuracy for instances of the biased target class. Furthermore, the model can hardly discrimi- nate among data cases of the majority class on the basis of their closeness to the target class. For instance, the two ?X?s in Figure l(c) are in the same leaf node. Hence, they are assigned a same probability by the model, although one of them is much closer to the biased class. This causes prob- lems in applications such as target marketing: the market- ing department wants to send out promotions to 20% of the people in their database while only 1 % of the people in the database are recorded buyers. The rest 19% has to be se- lected based on their closeness to the buyer class.

2 Our Approach: An Overview  It is clear from the examples in Figure 1 that univariate splitting conditions often results in undetected patterns or decision trees of formidable sizes. To build concise mod- els, it is essential that more than one variables are taken into consideration in splitting the data. However, scalability re- quirements forbids us considering all possible combinations of these variables as OC1 [ 1 I] and LMDT [6] do.

We improve the decision tree induction process by pro- viding an additional splitting criterion which results in more concise and more interpretable decision trees. The mo- tivation is that, in datasets with biased class distribution, while the negative instances are distributed ?everywhere?, instances of the target class tend to form clusters in some subspaces. These clusters are the foundation of accurate predictions of unknown instances. The proposed approach,  SSDT, uses an efficient multivariate search algorithm to lo- cate subspace clusters so as to enable (multivariate) splitting based on weighted distances to the centroid of the clusters.

While it is computationally prohibitive to search for all the clusters, it is more feasible to search for clusters formed by points in the biased target class, since they only account for a small fraction of the data. Our algorithm detects candi- date clusters from lower dimensions to higher dimensions, and prunes away all the candidates as soon as we find that partitions based on these clusters can not offer a better pu- rity than the univariate splits.

Related Works Much work has focused on how to tackle the deficiencies of the decision tree discussed in the pre- vious section. Among them, multivariate decision trees overcome a representational limitation of univariate deci- sion trees [ 13, 41. However, performing a multivariate par- tition often leads to a much larger consumption of compu- tation time and memory, which may be prohibitive for large datasets.

The target selection problem also attracts lots of atten- tion [9, 101. Closeness estimations of an unknown instance to a certain class can be solved by clustering and nearest neighbor algorithms. A naive approach searches for clus- ters of the biased data and scores an unknown sample by its distance to the closest cluster. Association rule mining [3] is also used to solve the target selection problem. A po- tential problem with association rules is the combinatorial explosion of ?frequent itemsets? [9, lo], which can be pro- hibitive for large datasets with biased data distribution even after sophisticated pruning.

Contributions of this Paper We use novel splitting cri- teria in building decision trees. We discover data clustering in correlated dimensions and partition the data by distance functions defined in the corresponding subspaces. With our multivariate splitting condition, decision trees can be built     smaller, and more interpretable. However, unlike other mul- tivariate decision tree algorithms that are usually prohibitive for large datasets or high dimensions, our approach is effi- cient and scalable.

3 Definitions  Let S denote the training set belonging to a node of a decision tree. Each point in S has n attributes in addition to the special attribute: class label. Let C = (21, ..., z k }  be a cluster of points where each xi has the same class label.

The centroid of C is the algebraic mean of points in C.

To measure the closeness of a point to a clyster, we use weighted Euclidean distance function, Dist(d,$, 5) = d w ,  where z i s  a point, $the centroid of a cluster, and w? the weight vector. The Euclidean distance is indeed defined in a subspace that is specified by a set of dimensions whose weights are non-zero.

The Euclidean distance only works for numerical at- tributes. For ordinal attributes, we can map their values to the range of [0,1]. For categorical attributes, the hetero- geneous Euclidean metric [ 151 defines the similarity of two values by their relative frequency of occurrences in the same class. However, for attributes with many values and a biased dataset, certain values may never occur in the training set.

In our algorithm, we use a distance matrix A4 supplied by the user, such that the value M ( i , j )  E [0,1] denotes the distance between categorical value i and j .

We use the gini index,  j=1  wherepj is the relative frequency of class j in S, to measure the ?goodness? of all the potential splits. If S is partitioned into two subsets S1 and S2, the index of the partitioned data gini(S) can be obtained by:  where nl and 122 are the number of points of S1 and Sa, respectively.

4 Scalable Decision Tree Classifiers  A decision tree classifier recursively partitions the train- ing set until each partition consists entirely, or almost en- tirely, of records from one class.

The SPRINT algorithm has been proposed to build de- cision trees for large datasets [ 121. The splitting criterion used by SPRINT is based on the value of a single attribute (univariate). For a continuous attribute, it has the form of  A 5 C where A is an attribute and C is a value in the do- main of attribute A.

SPRINT avoids costly sorting at each node by pre- sorting continuous attributes only once, at the beginning of the algorithm. Values of each continuous attribute are main- tained by a sorted list. Each entry in the list contains i) a value of the attribute, ii) its record id (rid),  and iii) the class label of the record. The node is split on the attribute which yields the least value of the gin2 index (Gbest). Based on the sorted list of the splitting attribute, a hash table is con- structed to map each record (rid) to one of the subnodes which the record belongs to after the split. Entries in other attribute lists are moved to the attribute list of the subnodes after consulting the hash table as to which subnode this en- try belongs to. The sorted order is maintained as the entries are moved in pre-sorted order.

5 The SSDT Algorithm  The core of SSDT lies in detecting subspace clusters of positive points. However, finding all such clusters is both time consuming and unnecessary. We are only interested in clusters that can offer a better split than univariate parti- tions. In Section 5.1, we prove an important property which enables us to narrow down our search to those clusters that have the potential. The actual clustering algorithm is in- troduced in Section 5.2, where we use an Apriori-like al- gorithm to find subspace clusters from lower dimensions spaces to higher dimensional spaces. In Section 5.3, we compute the exact gini index for cluster-based partitioning by scanning a small proportion of the data, thus keeping the overhead of the multivariate partitioning to a minimum.

SSDT is based on the framework of SPRINT, where pre- sorted attribute lists are maintained at each node. We con- sider only two class labels, positive and negative, and the target class (positive) is biased, usually account- ing for only a small fraction of the data. We normalize the values on each dimension to the range of [0, I].

The SSDT approach is outlined in Algorithm 1. To par- tition a dataset, we first compute the gini index on each of its attributes. While we scan through the pre-sorted at- tribute list, we also derive I-dimensional clusters of the biased data for each dimension (described in detail later).

Next, we locate subspace clusters of the positive data cases.

The minimal gini index produced by the univariate splits, Gbest, is passed in as a parameter to ClusterDetect() so that clusters can not possibly deliver a g in i  index smaller than Gbest are pruned as early as possible. We then com- pute the gini index of splits based on the distance to each subspace cluster. The process, DistanceEntropy(), de- scribed in Algorithm 2,  does not require globally reorder- ing the data according to the distance. If the minimal gini index is achieved by some subspace cluster, we partition     Algorithm 1 SSDT(Dataset: S) 1: 2: 3: 4:  5: 6: 7:  a: 9:  10:  11: 12:  13: 14: 15: 16: 17:  S, t points of the biased class in S; for each attribute k do  scan the sorted attribute list of k and compute: - Ik : the gini index on attribute k; - L k  : clusters of points in S, on attribute k;  end for Gbest t min Ik; c t ClusterDetect(Gb,,t, s,, L) ; for each cluster c E C do  I: t DistanceEntropy(c, S ) ; end for if mixi If < Gbest then  CEC split S into two subsets S I ,  S2 based on the distance;  else split S into subsets on the attribute with Gbest;  end if call SSDT(Si) on each subset Si if Si does not satisfy the termination condition;  k  the dataset based on the diGance to such a cluster. More spec$cally, given a point d, instead of using! univariate test di 5 U, we use a test in the form of Dist(d,j?, w') 5 U, where @is the centroid of the cluster and .w' is a weight vec- tor of all the dimensions. As in the SPRINT algorithm, the partition process also keeps the sorted order of the attribute lists, so that no reordering is required. The partition stops when a node is composed entirely of negative points (100%) or almost entirely' of positive points.

5.1 Minimal Support of Subspace Clusters  To find subspace clusters of points (of the biased class), we need to find: i) the centroid p'of the cluster, and ii) the weight .w' which defines the subspace (Gi = 0 means dimension i is irrelevant) of the cluster. Several subspace clustering algorithms have been introduced in the literature.

The CLIQUE algorithm [2] reports connected dense units in subspaces but the centroids of clusters are not detected.

Another method, called PROCLUS [l], uses a hill climb- ing method to successively improve a set of centroids, and derives a set of dimensions for each cluster. This algorithm however, requires that the number of clusters k to be found is pre-known. Both methods are time consuming since they aim at discovering all the subspace clusters.

Given a found cluster, Algorithm 1 partitions a dataset S into 2 datasets,, S1 and S2, such that SI contains points close to the centroid of the cluster. Instead of checking all  I We assign a higher weight to each positive point to balance the biased distribution. In our algorithm, we stop if more than 90% of the points in the node is positive.

the subspace clusters, we are only interested in those that can result in partitions with a gini index lower than Gbest, the minimal gini index we get by partitioning the data on single attributes.

Proposition 1 tells us how to narrow our search on qual- ified clusters. We define the support of a cluster as P ' l P , where P' is the number of (positive) points in the cluster, and P is the total number of positive points in S.  We prove the following proposition:  Proposition 1. If the gini index of a cluster-based parti- tion of S is lower than Gbest, then the cluster must have  where q is the per- a support greater than 2:-2;- G b e s t , centage of the positive points in &.

Proot Let N be the total number of points in S. Let P be the total number of positive points in S. Thus, q = PIN.

Assume S is partitioned into S1 and S2, and S1 contains the points in the cluster. According to Formula 2, we have:  2 - 2  2-Gbest  N - P I - N ' g in i (S)  = - + "gini(S1)  + N gini (S2)  where P' and N' are the number of positive points and negative points in SI respectively. Given N >> P', it can be shown that the lowest gini (S)  is achieved if SI contains only positive points, that is, N' = 0 and gini(S1) = 0.

That the partition produces a gini index lower than Gbest means:  Expanding gini(S2) using Equation 1 ,  we get:  -2NP' - 2P2 + 2 P P ' +  2 N P N ( N  - PI) Gbest  Substituting PIN with q,  we get:  The minsup given by Formula 3 is a lower bound, be- cause the cluster we find usually does not contain only pos-  0 itive points (i.e., N' > 0).

5.2 Cluster Detection  To find subspace clusters of points in the biased class, we use an iterative approach that is very similar to the apri- ori algorithm [3] for finding frequent itemsets. A clus- ter whose support is lower than minsup in  k-dimensional space can not have support larger than minsup in (k + 1)- dimensional space. We first find clusters in 1 -dimensional spaces, then we combine them to form candidate clusters     in 2-dimensional spaces. We count the number of points in each cluster and eliminate those candidate clusters whose support does not satisfy the constraint in Proposition 1.

Then we combine clusters in the 2-dimensional spaces to form candidates in the 3-dimensional spaces, and so on, un- til no more qualified clusters can be found.

Figure 5.2(a) shows an example where points of the bi- ased class form two clusters in a 3-dimensional space. We use a simple approach to detect 1-dimensional clusters. In Figure 3, the range of each attribute is divided into 10 bins and we keep the counts of points that fall in each bin. This is done when we scan through attribute lists to evaluate splits on single attributes, so there is minimal extra cost intro- duced. The horizontal line in Figure 3 indicates the average density and we regard each continuous region above the av- erage density line as one cluster. Thus, we detect one cluster around .1 with radius .1 on attribute X, two clusters around .3 and .7 respectively both with radius . l  on attribute Y, and one cluster around .2 with radius .l on attribute Z.

in Figure 5.2(b). Each centroid is represented by values  and cj are centers of clusters on dimension i and j re- spectively, and ri and rj are their radius. Values on the other dimensions are unknown. Next, we make one scat ttrough all the points in the biased :lass: for all points d , d E C = { d r i  2 14 - ciI,rj 2 ldj - c j l } ,  we compute the mean ck = czEc &/lcl, and the radius rk on each of the dimension I C .

on two dimensions only, (..., ci/Ti, .... Cj/r j ,  ...), where ci  Algorithm 2 ClusterDetect(Gini1ndex: Gbest, Biased Data: S,, CenterRadius Lists: L )  I ~ n i n s u p  t derived by Gbest (Formula 3);  {Step 1. j n d  clusters in 2-dimensional space} 3 for each centerlradius c , / r ,  in L ,  do 4 for each centerlradius c , / r j  in L,, j > i do 6 .  end for  2 1 t 2 ; C ,  t 0;  add (..., c z / r z ,  .... c 3 / r 3 ,  ...) to Cl;  I: end for  8: while Cl # 0 do 9:  {Step 2. jind clusters from lower to higher dimensions} [ X l Y l Z ]  scan the points in S, and increase the count of cluster c E Cl for each point that belongs to c;  c is less than minsup;  cl + combining clusters in dimension I -  1;  .1/.1 .7/.1  1 - .3/.1 .2/.1 IO. eliminate cluster c from Cl if the number of points in I 0 0  x - .7/.1 .2/.1  11: l t l + l ; 12.

13 end while  14 return top-k leaf clusters;  (a) Points of biased class (b) Potential centroids form two clusters of clusters  {Step 3. return the clusters} Figure 2. Cluster detection  After clusters whose support is less than the value given by Formula 3 are pruned, we explore clusters in higher di- mensional spaces and repeat this process until no more clus- ters can be found. Finally, ClusterDetect() returns the found clusters. We are only interested in leaf clusters, which 0 1 2 3 4 5 6 7 8 9  0 1  2 3 4 5 6 7 8  9 are clusters that do not contain other clusters. Among all the (a) Density of Points on X (b) Density of Points on Y  & leaf clusters, we return the top K clusters in the higher di- mensions, where K is a user-specified parameter.

In order to compute the weighted Euclidean distance be- tween a point and a cluster, we need to find out the weight on each dimension. For a non-clustered dimension i, we set Gi = 0; otherwise, we set Gi = l/r:, where ri is the radius of the points' distribution on dimension i. Thus, the distance is normalized to reflect the span of the points on each dimension.

5.3 Split by Distance  . . . .

~~~~~~~~ . . . . . .  . . . .

. . . . . .  . . . . . . . .  . . . . . . . .  . . . .  . . . . . .  . . . . . .  , . . , : : : average - -  - - --._ .& - - : : : : : : : : : :  ' density  0 1 2 3 4 5 6 7 8 9  (c) Density of Points on Z  Figure 3. Histograms of positive points on each attribute  Assuming all the 1-dimensional clusters shown in Fig- ure 3 has support larger than minsup, we then form a list of potential centroids in the 2-dimensional subspace as shown  For each cluster returned by CEusterDetect(), we de- rived a distance function Dist(&$, G ) .  The next step is to     find the value w so that the split by the test Did(&$,  G)  5 w offers the minimum gini index.

A straight-forward approach is to reorder all the points by their distances to the center 6, and compute the gini in- dex by scanning the ordered points. This is costly for large datasets. Another approach is to discretize the distance into intervals and for each interval we keep the counts of posi- tivehegative cases whose distance to $are in that interval.

One shortcoming of this approach is the loss of accuracy due to discretization.

Our approach, outlined in Algorithm 3, avoids reorder- ing all the data and any loss of accuracy. This is achieved by making the followin$ two observations: i) if point d is close to centroid $, then di, the coordinate on the i-th dimension, must also be close to $i; and ii) the best splitting position should be close to the boundary of the cluster.

Let N be the number of dimensions with non-zero weights (clustered dimensions). Let D be the set of points t!at are within an initial radius T = 6 to 5 For any point d E D, the inequality 2iji(di - 6)' 5 T' must hold for each clustered dimension i. With the ordered attribute lists, it is easy to find D', points that satisfy the inequality on all the N clustered dimensions. Obviously D' 2 D ,  for D' can contain points whose distance to p'is up to ~fl. After sorting D' by distance, we compute the gini index up to radius T ,  and we keep the points in D' - D and discard D .

We then increase the radius T by 6 and repeat the process.

However, we do not have to consider all the points. We are computing the gini index based on the distance to the cluster centroid we found. Thus, we expect a good gini index near the boundaries of the cluster. According to the weighting scheme discussed in the previous subsection, Gi is set to for each clustered dimension i ,  where ~i is the span of the points on that dimension. Thus, we have Gi(di -&)' 5 1 for any point p'that is inside the cluster. In additjon to these points, we consider all points that satisfy G,(di - 6)' 5 2 on each dimension i .  Thus, the maximum  5.4 SSDT Examples  Let us review the two problems in Section 1. Un- like the C4.5 decision tree, which fails to detect pattern (hot, high) and builds a trivial decision tree in Fig- ure l(b) , the SSDT algorithm accurately captures the pat- tern and constructs a compact decision tree in Figure 4(a).

The second problem is introduced by datasets with biased class distribution. The decision tree model shown in Fig- ure 1 (d) used 11 tests to classify a 2-dimensional dataset shown in Figure l(c). SSDT, shown in Figure 4(b), uses only 4 tests. Apparently, such differences tend to be more significant if the dataset has more than 2 dimensions.

Algorithm 3 distance-entropy(Dataset: S, Centroid: $, Weight: 5)  1: T t 6 ; 2: N t # of dimensions with non-zero weights; 3: repeat 4: 5:  6:  7: 8: end for 9:  10:  for each relevant _dimension i do find instances d that satisfies ( r  - 6)' 5 t&(& - 6)' < T' using the ordered attribute lists; dlcount t, dlcount + 1; {check ifd satisfies all the inequalities} add d t o  the ordered set D' if dlcount = N;  compute gini-index for splits by distance up to r ; remove in tree D' the branch that represents data cases within distance T top',  11: T t T + 6 ; 12: until T > &W; 13: return I' and v ;  n  D(hot,high)<=O  (a) SSDT of Figure l(a) (b) SSDT of Figure l(c)  Figure 4. SSDT for datasets in Section 1  6 Evaluations  We evaluate the SSDT algorithm in various aspects. We study the size of the decision tree generated by the algo- rithm, the influence of the biased class distribution, the ac- curacy of predictions, as well as the efficiency and scalabil- ity issues. The tests were performed on a 700-MHz Pentium I11 machine with 256M of memory, running Linux.

Synthetic Data Generation We generate synthetic data in d-dimensional spaces with two class labels, positive and negative. Points have coordinates in the range of [0,1] and positive points account for p = 1% - 10% of the total data.

To generate clustered points in subspaces, we use a method similar to [ 11. The difference is that the number of positive points is controlled by the biased class ratio. Our method takes 4 parameters: n ,  the number of clusters; I C ,  a Poisson parameter that determines the number of relevant dimensions in each cluster; p, the percentage (biased ratio) of positive points, and N, the total number of points.

First we determine the subspace for each cluster. For a given cluster i, the number of relevant dimensions, Si, is     picked from a Poisson distribution with mean k. However, an additional restriction, 2 5 Si 5 d, must be observed.

We generate centroid ~ 7 i  for each cluster i. We simply generate a uniformly distributed point in the d-dimensional space. We then decide the spread (radius) of the cluster on each dimension. We set ?ij = 0.5 for irrelevant dimension j .  For a clustered dimension, we fix a spread parameter s and choose the radius Gj E [0, s] uniformly at random. For our data generation, we use 3 values for s = .l, .2, .5.

We generate positive points in each cluster i in two dif- ferent ways: i) points are distributed uniformly in the re- gion; ii) for each dimension j ,  coordinates of points on the dimension follow a normal distribution with mean cj and variance ej. We determine the size of each positive cluster by Ni = pN&, where wi is the volume of cluster i defined as vi = n,d=1(2 * ri j ) .

Finally, we generate (1 - p ) N  negative points. The negative points either i) uniformly distribute at random in the entire space, or ii) form clusters in subspaces and are generated by the method described above with parameters k = 0.8d and s = 0.5. If a negative point is generated inside one of the positive clusters, it is discarded with a probability B. For our data generation, we choose 6 = 0.5.

.=l  -,-,0.3 /,-,-,-,-,0.66,-,-  Experiments: Tree Size and Scalability We generate 6 clusters (Table 1) of positive points out of a training set of total lOOK records and 10 attributes. The total number of positive cases account for 2% of the training set. The av- erage radius of the cluster on each clustered dimension is 0.05, and the negative points are uniformly distributed at random. The split at the root of the decision tree, for exam- ple, uses the distance function defined for Cluster 6. Total 5 clusters are used at different nodes for splitting, resulting in a tree of 37 leaf nodes before pruning, while the SPRINT algorithm uses 7 1 leaf nodes before pruning.

121 1  L I  Table 1.5 clusters are detected by SSDT. Un- clustered dimensions are denoted by ?-?.

We compare the size of the decision tree (in terms of number of leaf nodes) generated by SPRINT and SSDT.

The datasets we use have 10 attributes, and the 5 clusters of biased points account for 1%, 2% and 5% of the total data. Figure 6(a) indicates that trees built by SSDT are sig- nificantly smaller, and the sizes of the trees generally do not increase as the training sets become larger.

Next, we vary the number of clusters in the training sets and show the results in Figure 6(b). The datasets are gen- erated with the same class ratio: 2%. The size of the tree increases significantly as there are more positive clusters in the dataset. The trees generated by SSDT are much smaller.

Figure 6(a) shows the scalability of SSDT as the size of the dataset increases from 0.1 to 2.5 million. The dataset has 10 attributes, 8 clusters with an average dimensional- ity of 4, and a biased class ration of 1%. The execution time increases linearly with the size of the dataset, since SSDT is able to detect the clusters and the resulted decision tree has similar heights, which means the number of passes through the database does not change. Figure 6(b) shows the scalability of SSDT when the average dimensionality of the positive clusters is increased from 2 to 12. The dataset used in the test has 1 million records, 8 clusters, 1% posi- tive ratio, and a total of 20 attributes. It indicates that cluster dimensionality has little impact on the performance.

We study the impact of the number of positive clusters on the scalability. In Figure 6(c), we increase clusters from 4 to 20. The dataset has 1 million records, among which 1% are positive. There are 10 attributes and the clusters have an average of 5 dimensions. Since the number of positive data cases is kept unchanged during the test, each cluster contains fewer records as more clusters are used. The curve is steeper than in the previous cases because more scans of the dataset have to be performed. In Figure 6(d), using the dataset of the same size, dimensionality, and 8 positive clusters, we found the performance is stable.

We compare the performance of SSDT with SPRINT in Figure 6(c). The datasets have 10 attributes, 8 positive clusters, and a class ratio of 1%. In this case, SSDT has an advantage over SPRINT because SSDT trees are much smaller. As we increase the class ratio and the number of clusters, SPRINT becomes faster than SSDT. Indeed, SPRINT is 20% faster than SSDT when there are 20 clusters with a 15% class ratio, which means SSDT works best with biased class distributions. The association rule algorithm for mining datasets with biased class distribution does not scale well. Overall, SSDT is an efficient and scalable algo- rithm, despite the multivariate search it performs.

7 Conclusion  We presented a novel decision tree algorithm. The key idea is to take advantage of the subspace clusters formed by      2w  I80   I40  -$ 120 5 1W 2 60 -  Bo    I 7W  Bw   4w H 3w 2w  1W  I  0 2MM) WXa w 8woo lwwo 4 8 8 10 12 I 4  16 0 0 5  I 1 5  2 2 5 lofnmdr # of FasdlY. dYI1.R #of rronlr (NI rnllrnS)  (a) # of leaf nodes v. training set size (b) # of leaf nodes v. # of clusters (c) Execution Time  Figure 5. Experiments and Comparisons  m i .  . . . . .

2 ' 8 8 1 0 1 1 D d w " t a ( . i @ l - p s t  lol&(lombnr) d n m m y l d - a  t d h . f n o l  Wmpns  (a) # of records (b) dimensionality (c) # of clusters (d) class distribution  Figure 6. Scalability  the data in the biased class. Once these subspace clusters are efficiently detected, a compact and accurate decision tree can be constructed by splitting a node based on the distance to such clusters. Our multivariate decision tree algorithm has proven to be scalable and efficient. Indeed, it has better performance over SPRINT for very skewed distributions.

