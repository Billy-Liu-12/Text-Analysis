Moving Big Data to The Cloud Linquan Zhang?, Chuan Wu?, Zongpeng Li?, Chuanxiong Guo?, Minghua Chen? and Francis C.M. Lau?

Abstract?Cloud computing, rapidly emerging as a new com- putation paradigm, provides agile and scalable resource access in a utility-like fashion, especially for the processing of big data.

An important open issue here is how to efficiently move the data, from different geographical locations over time, into a cloud for effective processing. The de facto approach of hard drive shipping is not flexible, nor secure. This work studies timely, cost-minimizing upload of massive, dynamically-generated, geo- dispersed data into the cloud, for processing using a MapReduce- like framework. Targeting at a cloud encompassing disparate data centers, we model a cost-minimizing data migration prob- lem, and propose two online algorithms, for optimizing at any given time the choice of the data center for data aggregation and processing, as well as the routes for transmitting data there. The first is an online lazy migration (OLM) algorithm achieving a competitive ratio of as low as 2.55, under typical system settings. The second is a randomized fixed horizon control (RFHC) algorithm achieving a competitive ratio of 1+ 1  l+1 ? ?  with a lookahead window of l, where ? and ? are system parameters of similar magnitude.



I. INTRODUCTION  The cloud computing paradigm enables rapid on-demand provisioning of server resources (CPU, storage, bandwidth) to users, with minimal management efforts. Recent cloud platforms, as exemplified by Amazon EC2 and S3, Microsoft Azure, Google App Engine, Rackspace, etc., organize a shared pool of servers from multiple data centers, and serve their users using virtualization technologies.

The elastic and on-demand nature of resource provisioning makes a cloud platform attractive for the execution of various applications, especially computation-intensive ones [1]. More and more data-intensive (big data) applications, e.g., Facebook, Twitter, and big data analytics applications, such as the Human Genome Project [2], are relying on the clouds for processing and analyzing their petabyte-scale data sets, using a computing framework such as MapReduce and Hadoop [3].

An important issue however has largely been left out in this respect: How does one move the massive amounts of data into a cloud, in the very first place? The current practice is to copy the data into large hard drives for physical transportation to the data center [4], or even to move entire machines [5]. Such physical transportation incurs undesirable delay and possible service downtime, while outputs of the data analysis are often needed to be presented to users in the most timely fashion [5]. It is also less secure, given that the hard drives are prone  The research was supported in part by Hong Kong RGC General Re- search Fund (717812, 411209, 411010, and 411011), a China 973 Program (2012CB315904), an Area of Excellence Grant (AoE/E-02/08), and two gift grants from Microsoft and Cisco.

to infection of malicious programs and damages from road accidents. A safer and more flexible data migration strategy is in need, to minimize any potential service downtime.

The challenge escalates when we consider that data are dynamically and continuously produced, from different ge- ographical locations, e.g., astronomical data from disparate observatories [6], user data from different Facebook front-end servers. For dynamically-generated data, an efficient online algorithm is desired, for timely guiding the transfer of data into the cloud over time; for geo-dispersed data sets, we wish to select the best data center to aggregate all data onto (e.g., Amazon Elastic MapReduce launches all processing nodes in the same EC2 Availability Zone [7]), given that a MapReduce- like framework is most efficient when data to be processed are all in one place, and not across data centers due to the enormous overhead of inter-data center data moving in the stage of shuffle and reduce [8].

As the first dedicated effort in the cloud computing lit- erature, this work studies timely, cost-minimizing migration of massive amounts of dynamically-generated, geo-dispersed data into the cloud, for processing using a MapReduce-like framework. Targeting a typical cloud platform that encom- passes disparate data centers of different resource charges, we carefully model the cost-minimizing data migration problem, and propose efficient online algorithms, which optimize the routes of data into the cloud and the choice of the data center for data aggregation and processing, at any give time. Our detailed contributions are as follows: ? We analyze the detailed cost composition and identify the performance bottleneck for moving data into the cloud, and formulate an offline optimal data migration problem. The optimization computes optimal data routing and aggregation strategies at any given time, and minimizes the overall system cost and data transfer delay, over a long run of the system.

? Two online algorithms are proposed to practically guide data migration over time: an online lazy migration (OLM) algorithm and a randomized fixed horizon control (RFHC) algorithm. Theoretical analyses show that the OLM algorithm achieves a worst-case competitive ratio of 2.55, without the need of any future information and regardless of the system scale, under the typical settings in real-world scenarios. The RFHC algorithm achieves a competitive ratio of 1+ 1l+1  ? ? that  approaches 1 as the lookahead window l grows. Here ? and ? are system dependent parameters of similar magnitude.

? We conduct extensive experiments to evaluate the perfor- mance of our online algorithms, using real-world meteorolog-   2013 Proceedings IEEE INFOCOM     ical data generation traces. The online algorithms can achieve close-to-offline-optimum performance in most cases examined, revealing that the theoretical worst-case competitive ratios are pessimistic, and only correspond to rare scenarios in practice.

The remainder of this paper is organized as follows. We discuss related work in Sec. II, and present the system model and the offline optimal data migration problem in Sec. III. We design two online algorithms and analyze their competitive ratios in Sec. IV. Evaluation results are presented in Sec. V.

Sec. VI concludes the paper.



II. RELATED WORK  The recent years have witnessed significant interest in mi- grating different applications onto the cloud platform. Hajjat et al. [9] develop an optimization model for migrating enterprise IT applications onto a hybrid cloud. Wu et al. [10] advocate deploying social media applications into clouds, for leveraging the rich resources and pay-as-you-go pricing. These projects focus on workflow migration and application performance optimization, by carefully deciding the modules to be moved to the cloud and the data caching/replication strategies in the cloud. The very rudimentary question of how to move large volumes of application data into the cloud however is not explored.

Few existing work discussed such transfer of large amounts of data to the cloud. Cho et al. [11] design Pandora, a cost- aware planning system for data transfer to the cloud provider, via both the Internet and courier services. Different from our study, they focus on static scenarios with a fixed amount of bulk data to transfer, rather than dynamically generated data; in addition, a single cloud site is considered, while our study considers multiple data centers.

A number of online algorithms have been proposed to address different cloud computing and data center issues.

For online algorithms without future information, Lin et al. [12] investigate energy-aware dynamic server provisioning, by proposing a Lazy Capacity Provisioning algorithm with a 3-competitive ratio. Assuming lookahead into the future, Lu and Chen [13] study the dynamic provisioning problem in data centers, design future-aware algorithms based on the classic ski-rental online algorithm. Lin et al. [14] investigate load balancing among geographically-distributed data centers with a receding horizon control (RHC) algorithm, and show that the competitive ratio can be reduced substantially by leveraging the predicted future information.



III. THE DATA MIGRATION PROBLEM  A. System Model  Consider a cloud with K data centers distributed in a set of regions K (K = |K|). A user (e.g., a global astronomical telescopes application) continuously produces large volumes of data at a set D of geographic locations (e.g., dispersed telescope sites). The user connects to the data centers from different data generation locations via VPNs, with G VPN gateways (G) at the user side and K VPN gateways each collocated with a data center (Fig. 1). A private network of  Legend  Data Location 1  Data Location 2  DC 1  DC 2  DC 3  DC 4  Gateways at the user side  Gateways at the data center side Intranet links in cloud  Internet links Intranet links at the user side  GW2'  GW1'GW1  GW2  GW3  GW4  GW3'  GW4'  Fig. 1. An illustration of the cloud system.

the user inter-connects all the data generation locations and the VPN gateways at the user side. Such a model reflects typical connection approaches between users and public clouds (e.g., AWS Direct Connect [15]), where dedicated, private network connections are established between a user?s premise and the cloud, for enhanced security and reliability, and guaranteed inter-connection bandwidth.

While intra-cloud links and links in the private network are usually over provisioned, the bandwidth Ugi on a VPN link (g, i) from user side gateway g to data center i is limited, and constitutes the bottleneck in the system.

B. Cost-minimizing Data Migration: Problem Formulation  We consider a time-slotted system with slot length ? . Fd(t) bytes of data are produced at location d in slot t. ldg is the latency between data location d ? D and gateway g ? G, pgi is the delay along VPN link (g, i), and ?ik is the latency between data centers i and k. These delays are dictated by the respective geographic distances.

A cloud user faces the problem of deciding (i) via which VPN connections to upload the data into the cloud, and (ii) to which data center should they be aggregated, for processing by a MapReduce-like framework, such that the monetary charges incurred, as well as the latency for the data to reach the aggregation point, are minimized.

Decision variables. (1) Data routing variable xd,g,i,k(t) de- notes the portion of data Fd(t) produced at location d in t, to be uploaded through VPN connection (g, i) and then migrated to data center k for processing. xd,g,i,k(t) > 0 indicates that the data routing path d ? g ? i ? k is employed, and xd,g,i,k = 0 otherwise. Let ?x = (xd,g,i,k(t))?d,g,i,k, the set of feasible data routing variables are:  X = { ?x(t) |  ? g?G,i?K,k?K  xd,g,i,k(t) = 1 and xd,g,i,k ? [0, 1],  ?d ? D, ?g ? G, ?i ? K, ?k ? K } . (1)  Here ?  g,i,k xd,g,i,k(t) = 1 ensures that all data produced from location d are uploaded into the cloud in t.

(2) Binary variable yk(t) indicates whether data center k is target of data aggregation in time slot t (yk(t) = 1) or not (yk(t) = 0). At any given time, exactly one data center is chosen. Let ?y(t) = (yk(t))?k?K, the set of possible data aggregation variables are:  Y = { ?y(t) |  ? k?K  yk(t) = 1 and yk(t) ? {0, 1}, ?k ? K } . (2)  2013 Proceedings IEEE INFOCOM     Costs. The costs incurred in time slot t include the following components.

(1) The overall bandwidth cost for uploading data via the VPN connections, where  ? d?D,k?K Fd(t)xd,g,i,k(t) is the amount  uploaded via VPN connection (g, i), and fgi is the unit charge for uploading one byte of data via (g, i):  CBW (?x(t)) ? ?  g?G,i?K (fgi  ? d?D,k?K  Fd(t)xd,g,i,k(t)). (3)  (2) Storage and computation costs are important factors to consider in choosing the data aggregation point. In a large- scale online application, processing and analyzing in t may involve data produced not only in t, but also from the past, in the form of raw data or intermediate processing results [16].

Without loss of generality, let the amount of current and his- tory data to process in t be F(t) = ?t?=1(??  ? d?D Fd(?)),  where ?  d?D Fd(?) is the total amount of data produced in time slot ? from different data generation locations, and weight ?? ? [0, 1] is smaller for older times ? and ?t = 1 for the current time t. Assume all the other historical data, except those in F(t), are removed from the data centers where they were processed. Let ?k(F(t)) be a non-decreasing, convex cost function for storage and computation in data center k in t. The aggregate storage and computing cost in t is:  CDC(?y(t)) ? ? k?K  yk(t)?k(F(t)). (4)  (3) The best data center for data aggregation could differ in t than in t? 1, due to temporal and spatial variations in data generation. Historical data needed for processing together with the new data in t, at the amount of  ?t?1 ?=1(??  ? d?D Fd(?)),  should be moved from the former data center to the current.

Let ?ik(z) be the non-decreasing, convex migration cost to move z bytes of data from data center i to date center k, satisfying triangle inequality: ?ik(z) + ?kj(z) ? ?ij(z). The migration cost between time slot t? 1 and time slot t is:  CtMG(?y(t), ?y(t? 1)) ? ? i?K  ? k?K  ([yi(t? 1)? yi(t)]+  [yk(t)? yk(t? 1)]+?ik( t?1? ?=1  ?? ? d?D  Fd(?))).

(5)  Here [a? b]+ = max{a? b, 0}.

(4) We use a routing cost to model delays along the selected routing paths:  CRT (?x(t)) ? ?  d,g,i,k  Lxd,g,i,k(t)Fd(t)(ldg + pgi + ?ik), (6)  where xd,g,i,k(t)Fd(t)(ldg + pgi + ?ik) is the product of data volume and delay along the routing path d? g ? i? k. L is the routing cost weight converting xd,g,i,k(t)Fd(t)(ldg+pgi+ ?ik) into a monetary cost, reflecting how latency-sensitive the user is. In this work, L is a constant provided by the user a priori. The latency ldg + pgi + ?ik is fixed in each time slot, but can change over time.

In summary, the overall cost incurred in t in the system is: C(?x(t), ?y(t)) =CBW (?x(t)) + CDC(?y(t))+  CtMG(?y(t), ?y(t? 1)) + CRT (?x(t)).

(7)  The offline optimization problem. The optimization problem of minimizing the overall cost of data upload and processing over a time interval [1, T ], can be formulated as:  minimize T?  t=1  C(?x(t), ?y(t)) (8)  subject to: ?t = 1, . . . , T , (8a) ?x(t) ? X , (8b)  ? d?K,k?K Fd(t)xd,g,i,k(t)/? ? Ugi, ?i ? K, ?g ? G,  (8c) xd,g,i,k(t) ? yk(t), ?d ? D, ?g ? G, ?i ? K, ?k ? K, (8d) ?y(t) ? Y,  Constraint (8b) states that the total amount of data routed via (g, i) into the cloud in each time slot should not exceed the upload capacity of (g, i). (8c) ensures that a routing path d?g?i?k is used (xd,g,i,k(t) > 0), only if data center k is the point of data aggregation in t (yk(t) = 1).



IV. TWO ONLINE ALGORITHMS  We next design two online algorithms for guiding data routing and aggregation over time. The first algorithm relies only on the current and historical information, and the second further exploits predicted information from the future.

A. The OLM Algorithm  The offline optimization problem in (8) can be divided into T one-shot optimization problems at each time t:  minimize C(?x(t), ?y(t)) subject to: (8a)(8b)(8c)(8d). (9)  A naive online algorithm that solves (9) in each time slot can be far from optimal, migrating data back and forth prematurely. We design a more judicious online solution by exploring the inter-slot dependencies for data center selection.

We divide the overall cost C(?x(t), ?y(t)) into: (i) migration cost CtMG(?y(t), ?y(t ? 1)) defined in (5), related to decisions in t?1; and (ii) non-migration cost that relies only on current information at t: Ct?MG(?x(t), ?y(t)) = CBW (?x(t))+CDC(?y(t))+CRT (?x(t)). (10)  We design a lazy migration algorithm that postpones data center switching indicated by the one-shot optimum, until the cumulative non-migration cost (in Ct?MG(?x(t), ?y(t))) signifi- cantly exceeds the potential data migration cost.

As shown in Alg. 1, we solve the one-shot optimization in (9) at t = 1, and obtain the optimal data center in- dicted by ?y(1), the optimal routes ?x(1). Let t? be the time of the data center switch. In each following time slot t, we compute the overall non-migration cost in [t?, t ? 1],?t?1  ?=t? C??MG(?x(?), ?y(?)).The algorithm checks whether this  cost is at least ?2 times the migration cost C t?MG(?y(t?), ?y(t??1)).

If so, it solves the one-shot optimization to derive ?x(t) and ?y(t) without considering the migration cost, i.e., by minimizing Ct?MG(?x(t), ?y(t)) subject to (8a) ? (8d) and an additional constraint, that the potential migration cost, CtMG(?y(t), ?y(t ? 1)), is no larger than ?1 times the non- migration cost Ct?MG(?x(t), ?y(t)) at time t. If a change of migration data center is indicated (?y(t) ?= ?y(t ? 1)), the  2013 Proceedings IEEE INFOCOM     Algorithm 1 The Online Lazy Migration (OLM) Algorithm 1: t = 1; 2: t? = 1; //Time slot when the last change of aggregation data center  happens 3: Compute data routing decision ?x(1) and aggregation decision ?y(1) by  minimizing C(?x(1), ?y(1)) subject to (8a)? (8d); 4: Compute C1MG(?y(1), ?y(0)) and C  ?MG(?x(1), ?y(1));  5: while t ? T do 6: if C t?MG(?y(t?), ?y(t?? 1)) ? 1?2  ?t?1 ?=t?  C??MG(?x(?), ?y(?)) then 7: Derive ?x(t) and ?y(t) by minimizing Ct?MG(?x(t), ?y(t)) in  (10) subject to (8a) ? (8d) and constraint CtMG(?y(t), ?y(t ? 1)) ? ?1Ct?MG(?x(t), ?y(t));  8: if ?y(t) ?= ?y(t? 1) then 9: Use the new aggregation data center indicated by ?y(t);  10: t? = t; 11: if t? < t then //not to use new aggregation data center 12: ?y(t) = ?y(t? 1), compute data routing decision ?x(t) by solving  (9) if not derived; 13: t = t+ 1;  algorithm accepts the new aggregation decision, and migrates data accordingly. Otherwise, the aggregation point remains unchanged, and only data routing paths are computed.

In Alg. 1, ?2 and ?1 reflect the ?laziness? and ?aggressive- ness? of the algorithm: a larger ?2 prolongs the inter-switch interval of the aggregation data center, while a larger ?1 invites more frequent switches. We next analyze the competitive ratio of the OLM algorithm, i.e., the ratio of the worst-case total cost incurred by the OLM algorithm in [1, T ], over that of the offline optimal algorithm.

Lemma 1. The overall migration cost in [1, t] is at most max{?1,1/?2} times the overall non-migration cost in this period, i.e.,  ?t ?=1 C  ? MG(?y(?), ?y(? ? 1)) ?  max{?1, 1/?2} ?t  ?=1 C ? ?MG(?x(?), ?y(?)).

Lemma 2. The overall non-migration cost in [1, t] is at most ? times the total offline-optimal cost, i.e.,?t  ?=1 C ? ?MG(?x(?), ?y(?)) ? ?  ?t ?=1 C(?x  ?(?), ?y?(?)), where   = max ??[1,T ]  max?y(?)?Y,?x(?):(8a)?(8c) C??MG(?x(?), ?y(?)) min?y(?)?Y,?x(?):(8a)?(8c) C??MG(?x(?), ?y(?))  is the maximum ratio of the largest over the smallest possible non-migration cost incurred in a time slot, with different data upload and aggregation decisions.

Theorem 1. The OLM Algorithm is ?(1 + max{?1, 1/?2})- competitive.

Detailed proofs of the lemmas and the theorem are in our technical report [17]. The value of ? depends more on data generation patterns over time, and less on system scale. Under a typical value ? = 1.7 from our experiments, setting ?1 = 0.5 and ?2 = 2 leads to a competitive ratio of 2.55.

B. The Randomized Fixed Horizon Control (RFHC) Algorithm  In practical applications, near-term future data generation patterns can often be estimated from history, e.g., using a time series forecasting model [18]. We next design an algorithm that exploits such future information.

Time        Fig. 2. An illustration of different FHC algorithms with l = 2.

We divide time into equal-size frames of l + 1 time slots each (l ? 0). In the first time slot t of each frame, suppose we can predict all future information on data generation for the next l time slots: Fd(t), Fd(t + 1), ..., Fd(t + l), ?d ? D. We solve the following cost minimization problem over [t, t + l] ? given ?y(t? 1), to derive ?x(?) and ?y(?), ?? = t, . . . , t+ l:  minimize t+l?  ?=t  C(?x(?), ?y(?)), (11)  subject to: constraints (8a)?(8d), for ? = t, . . . , t+ l.

The method is essentially a fixed horizon control (FHC) algorithm, adapted from receding horizon control in the dy- namic resource allocation literature [14]. Allowing the first time frame (of l+ 1 slots) to start from different initial times p ? [1, l + 1], we have l + 1 versions of the FHC algorithm (Fig. 2). In particular, for FHC(p) starting from slot p, (11) is solved at t = p, p + l + 1, p + 2(l + 1), . . ., for routing and aggregation decisions in the following l + 1 time slots.

For each FHC(p), an adversary can tailor an input with a surge of data produced at the beginning of each time frame.

A high migration cost is likely to occur at each frame start, since the surge was not considered by the decision making in the previous frame. A randomized algorithm defeats such adversaries by randomizing the starting times of the frames.

Alg. 2 shows our Randomized Fixed Horizon Control (RFHC) algorithm. It first uniformly randomly chooses p ? [1, l+1] as the start of the first time frame of l+1 slots, i.e., it randomly picks one specific algorithm FHC(p) from the l+1 finite horizon control algorithms: at t = 1, it solves (11) to decide the optimal data routing and aggregation strategies in the period of t = 1 to p?1 (p ?= 1); then at t = p, p+ l+1, p + 2(l + 1), . . ., it solves (11) for optimal strategies in the following l + 1 time slots, respectively.

Algorithm 2 The RFHC Algorithm 1: ?y(0) = 0; 2: p = rand(1, l + 1); //A random integer within [1,l+1] 3: if p ?= 1 then 4: Derive ?x(1) ? ? ? ?x(p? 1) and ?y(1) ? ? ? ?y(p? 1) by solving (11) over  the time window [1, p? 1]; 5: t = p; 6: while t ? T do 7: if (t? p) mod (l + 1) = 0 then 8: Derive ?x(t), ? ? ? , ?x(t+ l) and ?y(t), ? ? ? , ?y(t+ l) by solving (11)  over the time frame [t, t+ l]; 9: t = t+ 1;  Lemma 3. The overall cost incurred by FHC(p) is upper- bounded by the offline-optimal cost plus the migration costs  2013 Proceedings IEEE INFOCOM     TABLE I PERFORMANCE COMPARISON AMONG THE ALGORITHMS: SPOT INSTANCE  PRICING, P = 0.25, L = 0.01  Simple OLM RFHC(0) RFHC(1) Offline Overall cost ($) 24709 16870 16676 16327 15944 Ratio 1.55 1.06 1.05 1.02 1  to move data from the aggregation data center computed by FHC(p) to the optimal one, at the end of the time frames.

That is, letting ?x(p) and ?y(p) be the solution derived by the FHC(p) algorithm and ?p,t = {?|? = p + k(l + 1), k = 0, 1, . . . , ? t?pl+1 	}, we have for any t ? [1, T ],  t? ?=1  C(?xp(?), ?yp(?)) ? t?  ?=1  C(?x?(?), ?y?(?))  + ?  ???p,t C?MG(?y  ?(? ? 1), ?y(p)(? ? 1)).

Theorem 2. The RFHC algorithm is (1+ 1l+1 ? ? )-  competitive. Here l is the number of lookahead steps, ? = supt?[1,T ],?y1(t),?y2(t)?Y  CtMG(?y 1(t),?y2(t))  ?t?1 ?=1(??  ? d?D Fd(?))  is the maximum migration cost per unit data, and ? = inft?[1,T ],?x(t),?y(t):(8a)?(8d)  C(?x(t),?y(t)) ?t?1  ?=1(?? ?  d?D Fd(?)) is  the minimum total cost per unit data per time slot.

The proofs of the lemma and theorem above can be found in our technical report [17]. Theorem 2 reveals that the more future steps predicted (the larger l is), the closer the RFHC algorithm can approach the offline optimum. Values of ? and ? are related to system input including prices and delays, and are less involved with the data generation patterns and the number of data centers. Under the practical settings used in our experiments, ?? ? 0.69. In this case, even with l = 1, the competitive ratio is already as low as 1.34.



V. PERFORMANCE EVALUATION Due to space limitations, detailed experiment set up and  more empirical results are provided in our technical report [17]. We have implemented and compared our offline and online algorithms, as well as a Pandora-like simple algorithm that fixes the choice of the data center for aggregation to be the one in Hong Kong.

We investigate dynamical VM prices (time-varying data processing costs) following the Spot Instance prices from Amazon EC2 during Apr. 2012 to Jul. 2012. From Tab. I, we see that due to lack of future price information, the OLM algorithm performs slightly worse than the RFHC algorithm with lookahead window l = 1.

We also evaluate the RFHC algorithm with different looka- head window sizes, with dynamic prices. Tab. II shows that, with the increase of the lookahead window, the performance is approaching the offline optimum. Just a 1- or 2-step ?peek? into the future drives the performance very close to the offline optimum.



VI. CONCLUDING REMARKS This paper designs efficient algorithms for timely, cost-  minimizing migration of enormous amounts of dynamically-  TABLE II PERFORMANCE OF RFHC ALGORITHMS WITH DIFFERENT LOOKAHEAD  WINDOW SIZES: SPOT INSTANCE PRICING, P = 0.25, L = 0.01  RFHC(0) RFHC(1) RFHC(2) RFHC(3) Offline Overall cost ($) 16676 16327 16105 16138 15944 Ratio 1.05 1.02 1.01 1.01 1  generated, geo-dispersed data into the cloud, for processing using a MapReduce-like framework. Two novel online algo- rithms are designed to practically guide data migration in an online fashion, based on solid theoretical analysis. The OLM algorithm achieves a worst-case competitive ratio of as low as 2.55 under typical real-world settings, without the need of any future information; the RFHC algorithm provides a decreas- ing competitive ratio with increasing size of the lookahead window. Our extensive experiments reveal the close-to-offline- optimum performance of both algorithms, by comparing them with a simple algorithm and the optimal offline algorithm, under real-world meteorological data generation patterns.

