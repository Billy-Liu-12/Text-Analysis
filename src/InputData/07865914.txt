SPECIAL SECTION ON HEALTHCARE BIG DATA

ABSTRACT Clinical practice calls for reliable diagnosis and optimized treatment. However, human errors in health care remain a severe issue even in industrialized countries. The application of clinical decision support systems (CDSS) casts light on this problem. However, given the great improvement in CDSS over the past several years, challenges to their wide-scale application are still present, including: 1) decision making of CDSS is complicated by the complexity of the data regarding human physiology and pathology, which could render the whole process more time-consuming by loading big data related to patients; and 2) information incompatibility among different health information systems (HIS) makes CDSS an information island, i.e., additional input work on patient information might be required, which would further increase the burden on clinicians. One popular strategy is the integration of CDSS in HIS to directly read electronic health records (EHRs) for analysis. However, gathering data from EHRs could constitute another problem, because EHR document standards are not unified. In addition, HIS could use different default clinical terminologies to define input data, which could cause additional misinterpretation. Several proposals have been published thus far to allow CDSS access to EHRs via the redefinition of data terminologies according to the standards used by the recipients of the data flow, but they mostly aim at specific versions of CDSS guidelines. This paper views these problems in a different way. Compared with conventional approaches, we suggest more fundamental changes; specifically, uniform and updatable clinical terminology and document syntax should be used by EHRs, HIS, and their integrated CDSS. Facilitated data exchange will increase the overall data loading efficacy, enabling CDSS to read more information for analysis at a given time.

Furthermore, a proposed CDSS should be based on self-learning, which dynamically updates a knowledge model according to the data-stream-based upcoming data set. The experiment results show that our system increases the accuracy of the diagnosis and treatment strategy designs.

INDEX TERMS Big data, case-based reasoning, clinical diagnosis, decision tree, data streammining, disease detection, electronic health record, medical record, semantic integration.



I. INTRODUCTION Given the recent dramatic progress that global endeavors have made in health care, it is surprising that human errors remain the leading cause of death even in developed coun- tries such as the US [1]. Among all of the contributing factors, errors related to medication are the most common category in medical practices [2]. A large number of adverse  drug effects are reported annually [3], [4], with statistics revealing that approximately 50% are preventable [3]. Sub- groups of this category of error include incorrect prescrip- tion, drug dose and administration. In addition, incorrect diagnosis is another typical human error, which causes fun- damentally wrong medical decisions that lead to serious consequences [5]. To reduce the risk of human error as  VOLUME 5, 2017 2169-3536 2017 IEEE. Translations and content mining are permitted for academic research only.

Personal use is also permitted, but republication/redistribution requires IEEE permission.

See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

S. Yang et al.: Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine  well as the workload of the medical staff, the application of medical software has long been suggested as a possible tool [6]?[8], [58]. According to [2], different strategies of software design are used to solve the two aforementioned problems. To prevent medication errors, the application should be designed as an automated database with historical and current medical records of a patient, as well as other key information, including all prescription and personal allergic reaction documents to prevent any inappropriate prescrip- tions and provide warnings on them. In addition, computer- assisted diagnosis software is used to increase the accuracy of the diagnosis and decrease the time that is needed for decision making. A few previous research studies have shown the feasibility of computer-assisted clinical information access and practice in terms of significantly reduced incidence of medical errors or improved accuracy of diagnosing mul- tiple diseases [9]?[11]. Currently, one popular strategy in the implementation of automated clinical practices is the integration of clinical decision support systems (CDSS) and health information systems (HIS) to directly read electronic health records (EHR) for analysis [12], [13]. This method greatly reduces the cost of training and the time required for entering massive amounts of patient-related data during each visit [14], [15], and it dynamically reconstructs the diagnosis model according to the real-time fluctuations of the patient?s condition [16].

However, several remaining problems still challenge the prospect of CDSS applications:  (1) Rapid growth of data. With the enormous number of patients who are exposed to all types of tests, the size of the medical information database is increasing rapidly inmillions of multi-dimensional records (e.g., physical indicators of the human body collected by smart clothing [59], [60], [61]) every day, which makes classical data mining methodologies no longer sufficient [17]. Therefore, the low efficacy of data inquiry and mining becomes an emerging problem for future data analysis [18].

(2)Cross-context interoperability problem. Historically, multiple clinical document standards were selected by differ- ent versions of EHRs. Because no perfectly unified vocabu- lary and data format across all versions of EHRs have been established, misinterpretation of health information by het- erogeneous EHRs could occur; similarly, different versions of HIS could use various default clinical terminologies to define input health data [16]. As a result, a series of information islands could be left behind, failing to be merged with other data sources for more systemic analyses.

(3) Demand for personalization and profession. As the name says, CDSS is still regarded as a ??clinical decision?? support system, with little effort being made toward improve- ment of the patient?s experience [62]. Currently, it is very common for patients to search doctors or hospitals [57] according to their preference (e.g., distance from home) or symptoms (e.g., which hospital specializes in cardiovascular diseases), but too much information could be retrieved [19].

Unfortunately, at present, such personalized and professional  demands cannot be completely satisfied; however, with the enormous amount of patient care data from different hospitals in EHRs, CDSS could be designed to analyze all of the historically hospitalized patient care data and make recom- mendations. This arrangement will help the patients to make optimal decisions according to their personalized demands.

To address these challenges, this article proposes a per- sonalized and professional clinical diagnosis and treatment system (CDTS) that combines machine learning algorithms with an inference engine. Specifically, this article makes the following contributions:  ? Devise a voted ensemble classification algorithm that is suitable for data stream mining to meet the big data computational demand;  ? Implement clinical tabular document syntax (DocLang) to integrate heterogeneous clinical information for accu- rate disease diagnosis and treatment. This approach has two advantages: (1) maintaining semantic consistency among heterogeneous contexts; and (2) facilitating auto- matic document understanding and processing across different contexts, because DocLang is not only a docu- ment representation language but also a rule language;  ? Support personalized and professional demands based on the patients? and doctors? features. Patients are able to receive suggestions (e.g., disease diagnosis or treat- ment suggestion) without going to hospitals according to their preference (e.g., choosing a clinician in another country).

The remainder of this article is organized as follows.

Section II presents related studies on decision tree and case- based reasoning. In Section III, we introduce the frame- work of the clinical diagnosis and treatment system (CDTS).

Section IV presents a clinical tabular document model (CTDM) that is used for clinical document representation in CDTS. Section V discusses a new decision-tree-based data stream mining algorithm to provide rules for the semantic inference algorithm proposed in Section VI. Section VII shows the performance of our proposal via experiment results and analysis. Finally, we conclude this article in Section VIII.



II. RELATED WORK A. DECISION TREE The theoretical foundation of disease diagnosis in this paper is a decision tree-based classification algorithm. A decision tree (DT) is a typical classification algorithm. Let DT be a function of examples. The class label for an example e is obtained by passing the example from the root down to a leaf, to be tested on a different attribute at each non-leaf node, and then, following the branch that corresponds to the attribute?s value in e [20]. It is also feasible to improve traditional DT models by integrating complex tests on internal nodes and complex classification rules on leaf nodes [20].

Classical decision tree learners (e.g., ID3, C4.5 [21], and CART [22]) require all of the training datasets to be loaded into memory. When the distributions of the datasets change,  3530 VOLUME 5, 2017    S. Yang et al.: Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine  these learners must reconstruct new tree models rather than updating old ones. In particular, these classical algorithms are not applicable to data stream mining, where poten- tially there is no upper bound on the number of instances that arrive sequentially [20]. In 2000, Domingos and Hulten [23] developed the Hoeffding tree algorithm, which is an incremental decision tree induction methodology that enables learning from successive data streams. In 2001, Oza and Russel designed the Ensemble Hoeffding Tree [24], an online bagging method that integrates some very fast deci- sion tree classifiers. Bifet, Holmes, Pfahringer, Kirkby and Gavalda [25] developed the Adaptive Size Hoeffding Tree approach, which was derived in 2009 and was a very fast decision tree that additionally sets a maximum number (? ) of split nodes: if the number of splitting nodes is larger than ? , the algorithm will delete some nodes to minimize the tree size. In addition, it can handle concept-drift data streams.

B. CASE-BASED REASONING The theoretical foundation of disease treatment in this paper is case-based reasoning. Ocampo et al. in [26], conclude that case-based reasoning (CBR) is an approach in which people?s current problem solution is referred from their past experience. According to [27], a case is a contextualized piece of knowledge that represents a certain experience, and it contains previous lessons and context in which that lesson can be applied. It can also be defined as a complete description of a problem, with its respective solution and also an assessment of the solution?s efficiency [28]. Thus, the CBR strategy is simple: when confronting a new problem, it first reviews past cases to identify the most similar one(s), and then, it adopts corresponding solution(s) instead of building a complex and explicit model.

Technically, CBR is commonly described as an iterative procedure that can be divided into four steps [26]: Step 1 (Case Recovery):This step includes three tasks [26]:  (1) identify the characteristics that describe a new problem; (2) locate the relevant cases; and (3) choose the best candi- date(s) among the relevant cases. Two of the most currently used techniques are the recovery of the closest neighbor and inductive recovery [31], [32].

Step 2 (Solution Suggestion): Usually, when a case is  recovered, an analysis is conducted to determine its similar- ity with the current problem. This step identifies the differ- ences between the current case and the recovered cases and, afterward, applies constraints (e.g., formulas, rules) to those differences for determination of the final solution [26].

In general, there are two types of solution suggestions: (i) structural adaptation, which applies rules and formulas directly to the stored solutions, and (ii) derived suggestion, which generates new solutions by reutilizing the original rules and formulas for the recovered solution [27].

Step 3 (Solution Revision and Confirmation): After the  solution is suggested, it is necessary to evaluate the fitness of the solution to a new case. If a suggested solution to the new problem is not suitable, then it shall be updated, and the  algorithm would learn from mistakes. Basically, two points are undertaken: (i) the applicability of the solution to the new case is determined by experts, and (ii) if there is a change in the solution, then the case will be updated before saving it [26].

Step 4 (Retention/Remember): This step incorporates the  useful information in a new solution into a knowledge base [26]. It involves two key points: (i) select the specific information to be retained from the new case, and (ii) inte- grate it into the knowledge base [29], [30].

FIGURE 1. Framework of the CDTS.



III. PROPOSED SOLUTION A. FRAMEWORK OF THE CLINICAL DIAGNOSIS AND TREATMENT SYSTEM (CDTS) To facilitate the disease detection and treatment, this paper proposes a new clinical diagnosis and treatment system (CDTS), which is conceptually represented in Fig. 1.

In CDTS, a new clinical tabular document model is provided as a standard for clinical document representation. After a patient chooses preferred doctors or hospitals, the clinical documents of the patient will be transferred to them through the network. The critical component from the perspective of the doctor or hospital is a semantic inference mechanism that consists of two stages: knowledge extraction and reasoning.

Its objectives are as follows: (1) reading the input clinical document and transforming it into an instance for machine learning algorithms (e.g., clustering, classification) as well as a set of input facts and rules that are compatible with the infer- ence engine; (2) collecting expert experience of professional physicians as expert rules and recording them through a Rule Editor; and (3) making semantic inferences and obtaining the results of the inference rules and transforming them into the output clinical document. In the following sections, we explain each of these components in detail.



IV. CLINICAL TABULAR DOCUMENT MODEL Each patient medical record corresponds to a medical exam- ination on a given date [33]. A formal definition of a medical record is given in Definition 4.1.

Definition 4.1 (Medical Record): Assume that? = {e1, e2, . . . , ek} is the set of examinations and  VOLUME 5, 2017 3531    S. Yang et al.: Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine  9 = {p1, p2, . . . , pn} is the set of patients; a medical record r models a list of examinations E = {e1, ex , . . . , ey} ?  ? that are performed on a patient pj ? 9 on a given date.

In the treatment space, each patient is represented by a vec- tor, which represents the patient?s treatment history. A vector element c = (pi,Ei, ti) corresponds to a treatment ti to patient pi under an examination Ei. A formal definition of a patient?s treatment history is as follows: Definition 4.2 (Patient Treatment History): Assume that? = {e1, e2, . . . , ek} is the set of examinations, 9 =  {p1, p2, . . . , pn} is the set of patients and 0 = {t1, t2, . . . , tm} is a collection of treatment strategies. Each treatment history T ? 0 on patient pi ? 9 is represented by a vector V with the number N(pi) of elements. Each element V  j pi  of vector V reports a treatment tj for patient pi under the examination ofEj = {e1, e2, . . . , ex} ?  ? . Thus, T = V pi =  [V 1pi ,V pi . . . ,V  x pi ].

Medical information is exchanged among different health information systems (HIS) in terms of a clinical document, or electronic health record (EHR), which is the combination of a medical record and patient treatment history. In this paper, a clinical document should be comprised of at least the following parts: (1) condition statement is the description of the patient?s symptoms as clinical features; (2) solution is the disease diagnosis that is given by the doctor (or diagnosis sys- tem); (3) assessment indicates the accuracy of the diagnosis; and (4) treatment represents the treatment procedure.

To implement the semantic interoperability of clinical doc- uments that are transferred among heterogeneous HIS, this paper designs a novel tabularized clinical document model, called the Clinical Tabular Document Model (CTDM). It is clear that the information model of a clinical document in the source context should be fully transformed to that of a target context without a semantic loss [34], [35]. In this paper, a semantic loss refers to missing or alternation of the term meaning and semantic relation when transferring documents across different contexts. For storage and commu- nication, an information model must be expressed in terms of a logical data model (i.e., logical structure, e.g., relational, object-oriented or tree-based, in other words, implement- independent) and physical data model (e.g., HTML, XML, PHP or JSON, which is tool specific) [36]. A logical data model decides the organization of the elements for the data storage, while a physical data model focuses on the actual data transfer manner, storage format and presentation style. A logical data model and physical data model con- struct a syntactic document representation. For unambiguity at the semantic level of an overall information model, a conceptual model is introduced to implement the semantic document representation. The CTDM in this section, as a union of the syntactic document representation and seman- tic document representation, describes all of the character- istics of the clinical document exchanged among the HIS.

Based on the CTDM, clinical documents (also as semantic documents [37]) are transferred from the document writer  FIGURE 2. Syntactic and semantic document representation.

(Party A) to the document reader (Party B), and they are faithfully understood by the latter. The following sections show the syntactic representation and semantic representation of the CTDM.

A. SYNTACTIC CLINICAL DOCUMENT REPRESENTATION Syntactic document representation (SDR) is a grammatical relationship among the terms in a document on how they are organized in a grammatical pattern [37]. This strategy can be applied to clinical documents. In this paper, we take the Vector Tree [38] (a tree structure) for the role of a logical data model. This choice is made because each clinical tabular doc- ument element (e.g., an empty placeholder or a single term) can be uniquely identified and hierarchically positioned, if its logical structure is a tree-based representation. A vector tree strengthens this point by assigning each tabular document element with a vector that represents its position from the root to its location in the tree, as shown in Fig. 3(4). For a physical data model, SDR contains a matrix tree [38] for presentation and a newly designed Clinical Tabular Docu- ment Language (DocLang, XML-based) for implementation.

This arrangement is made because automatic analysis of an arbitrary document with a complex layout is an extremely difficult task. However, a critical component in facilitating the understanding of a document layout is its representation in memory [39]. Different research studies have represented document layouts in various ways [37], [40]. To achieve consistency of a layout, a clinical document is presented as a nested table, in other words, a nested matrix in mathematics.

Nested matrixes construct a semantic document in another tree structure called amatrix tree. Thus, the vector tree, matrix tree and DocLang together constitute the syntactic document representation in this paper, as shown in Fig. 2. A vector tree can be alternatively constructed as a table by building its mapping to a matrix tree that shows positional relationships of cells in a table or embedded tables. This mapping is created by associating the position of each node in a vector tree with the position of each cell in a matrix tree, in such a way that any node in a vector tree has a unique corresponding position in a table. In a vector tree, the position is identified as the term identifier (TID), and in a matrix tree, it is identified by a cell identifier (CID) of a table.

3532 VOLUME 5, 2017    S. Yang et al.: Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine  FIGURE 3. Procedures of constructing a clinical tabular document.

To facilitate clinical document exchange, DocLang inte- grates the vector tree for the logical structure and the matrix tree for presentations. Thus, DocLang guarantees syntactic consistency of the exchanged clinical documents in terms of a logical data model and physical data model among heterogeneous contexts. One advantage of DocLang is that clinical document designers do not need to consider the bottom implementation when designing a clinical document template. What layout they design in the front end will be automatically transformed to DocLang-based files at the back-end. When creating a table, the users first construct its basic table form, which is shown in Fig. 3(1). Second, we embed a basic sub-table form in each cell to create a complicated table if needed, and we iterate this step until reaching a desirable table format, as shown in Fig. 3(2).

Because there are semantic relations between cells, embed- ded sub-table forms have semantic relations with one another.

Because each type of basic sub-table forms maps to certain sub-tree structures (i.e., logical structure), semantic relations are also established among sub-trees (as shown in Fig. 3(3)), which constructs a large tree that represents the entire logical structure of the clinical tabular document (Fig. 3(4)).

At this stage, we use XML to implement DocLang. Fig. 4 shows the XML schema of DocLang, which is designed as follows: (1) There is only one category of element <s> (or <sign>) with groups of predefined denoters (i.e., attributes) as basic grammatical elements. An instance of the XML schema is a reusable document template that includes only abstract elements<s>without reification. (2) An empty element (or a placeholder) is reified by the attribute ?term? using a set of atomic terms that refer to concepts in a commonly known dictionary (e.g., CONEX [37], [41]) by attribute ?ref?. (3) The attribute ?anno? is a concept definition of one element, whose value is also a set of atomic terms in the dictionary. (4) The attribute ?r? denotes the position of an element in matrix tree (or layout) structure by matrix position [38], while ?rt? denotes which element(s) the current element  FIGURE 4. XML schema of DocLang.

has a relation with. (5) The attribute ?tid? clarifies the position of an element in a vector tree (or logical) structure using a vector [38]. (6) The attribute ?obj? specifies whether a vector tree can be mapped into a table, list or cell. (7) The attribute ?doct? indicates the scenario for certain document templates to be applied, or the behavior of the template, e.g., an inquiry or an offer. (8) An element has its semantic relation defined by the attribute ?st?, and its cell value type is defined by another attribute, ?t?. (9) The attributes ?s?, ?pt? and ?mc? are designed for claiming the style format, position of the value in a cell and whether the cell is merged, respectively. (10) The attribute ?ass? specifies an association in which the document elements can form a minimum semantic unit that is identified by the attribute ?gid? (group identifier). (11) Specifically, when associated document elements form a mathematical relation or a logical relation, the formula expression should be defined with the attributes ?fl? or ?lg?, respectively. (12) The attribute ?state? represents the status of a checkbox, while ?order? defines a superiority for preference. TheXML schema is well-formed and valid under XMLSpy 2013 sp1.

B. SEMANTIC CLINICAL DOCUMENT REPRESENTATION Semantic document representation is a conceptual relation- ship among the terms in a document on how they are orga- nized in a semantic pattern [37]. Similarly, this strategy can  VOLUME 5, 2017 3533    S. Yang et al.: Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine  also be applied to clinical documents. In previous sections, information models for representing the semantics of a doc- ument require a conceptual model [35]. However, knowl- edge manifests itself in various relations among concepts or entities [42], [43]. Thus, the conceptual model in this paper includes the CONEX dictionary [37], [41] as a common dictionary and semantic relation types for illustrating rela- tionships among terms. It guarantees that all of the clinical tabular documents that are created, communicated and used are semantically consistent and inferable without ambigu- ity by two semantic chains: the term chain and semantic relation chain. The term chain ensures semantic consistency at the vocabulary level, which is defined as the ??reified concept1 (riid1)? local concept1 (liid1)?mapping concept (liid1, ciid)? common concept (ciid)? mapping conceptb (ciid, liid2) ? local concept2 (liid2) ? reified concept2 (riid2)??. The notation ? represents a mapping procedure.

The term chain has already been discussed in previous papers [37], [44]. A semantic relation chain guarantees semantic consistency at the data type level with ??one seman- tic relation type (for Party A)? one type of sub-tree struc- ture (logical structure)? one type-of attribute-value pair in DocLang? the same semantic relation type (for Party B)??.

This subsection exemplifies eight types of semantic rela- tions that were imported from information science [45] for semantic representation of clinical documents. They are part- of relation, reference relation, calculation relation, parallel relation, progressive relation, sequence relation, instance relation and choice relation. Each type of semantic relation is logically represented by an abstract sub-tree structure that corresponds to one type-of attribute-value pair in DocLang (XML-based). Each type of sub-tree structure represents an independent semantic unit, which can bemapped onto several types of sub-table forms for presentation. The links between the semantic units also adopt different semantic relations.

All the semantic units, with the links in between, constitute the entire semantics of the clinical tabular document. Another expression for semantics is rules. In this way, a clinical tabular document as a semantic unit can be represented by a set of rules (i.e., a rulebase). The rule expression of the semantics will be discussed in Section VI. The presentational structure is a tabularized result of the logical data structure. It repre- sents one display layout type of its corresponding physical data model. Because we use a tabular form to present docu- ments and the display layout varies, one logical data structure corresponds to one or many tabular formats for visualization.

As a semantic relation?s logical structure is represented by an abstract sub-tree structure, all of the sub-tree structures of the semantic relations in a table construct a logical structure of the whole tabular document. Indeed, the logical structure of a tabular document is not necessarily a tree; it can also be a network in certain cases. In other words, it behaves like a net overall by allowing associations among any nodes in the tree.

The purpose of each type of semantic relation is dis- cussed as follows. (1) Sequence relation defines a processing order (e.g., inference in an inference engine) among objects  (e.g., cells) in a clinical tabular document. For any objects O1 and O2 that form a sequence relation in a clinical tabular document, if and only if O1 is processed before O2, then we determine that there is a sequence relation between them.

The sequence relation indicates a dependency in the process of semantic interpretation of a clinical tabular document. Its abstract representation in DocLang documents is: seq =: s1, s2, . . . , sn where the notation =: means that the elements si(i = 1, . . . , n) at the right side satisfy the semantic relation at the left side. (2) Part-of relation defines a composition relation among the objects in a clinical tabular document.

This relation expresses the formation of an object that consists of simpler components, for example, a book that has many chapters that can be further divided into sections and subsec- tions. Its abstract representation is: partOf =: s1(s2, . . . , sn).

(3) Instance relation defines that one object is an instance of another. Instance objects automatically inherit semantic relations from their instantiated objects. Its abstract repre- sentation is ins =:si(vi). (4) Reference relation expresses that a fact is complete if and only if the referred objects are regarded as further explanation of the reference objects.

Its abstract representation is: ref =: s1@(s2, . . . , sn). (5) Calculation relation defines a mathematical formula or a logical operation for objects. If needed, an arbitrary num- ber of cells in a table could generate a calculation relation (cal). Its abstract representation is the following: cal =: Mathx(si|Mathy(sa, sb), sj|Mathz(sp, sq)?). (6) Choice rela- tion defines the availability status of selected items. Check- boxes are often used in clinical tabular documents to indicate a choice relation among items. Its abstract representation is: choice =: {sj|sj| . . . |sn}. (7) Progressive relation defines hierarchical priorities among objects in ascending (ASC) or descending (DESC) order, while (8)Parallel relation defines a same priority among objects. Their respective abstract rep- resentations are: prog =: s1 << s2 << . . . << sn and para =: s1, s2, . . . , sn.



V. KNOWLEDGE EXTRACTION FOR RULES The source of the rules in a knowledge base for inference can be divided into two categories: (1) decision tree mod- els generated by classification algorithms; and (2) clinical experts (e.g., doctors) through a rule editor. This section mainly discusses the techniques of knowledge extraction for rules.

A. MACHINE LEARNING ALGORITHMS For rules that are generated by classification algorithms, this paper proposes a voted ensemble multi-classification model that simultaneously integrates multiple classifier algorithms and votes for the best performance as the classification results. In this paper, we use state-of-art decision-tree-based classification algorithms that create rules in the form of a decision list. Each rule is followed by statistical output (e.g., accuracy rate) that is convenient for users (e.g., doctors) to consider.

To fulfill the reliability of classification, decision trees  should be updated continuously. Instead of re-constructing a  3534 VOLUME 5, 2017    S. Yang et al.: Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine  classification model each time when new instances arrive, the classification algorithms support incremental learning and, thus, reduce the cost in terms of the time and error rate. This section first introduces popular machine learning algorithms, including Hoeffding Tree, Hoeffding Adaptive Tree, Hoeffd- ing Option Tree and iOVFDT, and then, it presents a new classification algorithm to perform the classification task.

1) HOEFFDING TREE Hoeffding tree (HT) is an incremental learning algo- rithm [46]. It constructs a decision tree by using data streams as training datasets, provided that the statistical distribution of the instances always stays the same [23], [46]. The Hoeffding tree does not store any instance in the main memory, and thus, it requires only the space that is proportional to the size of the tree and the associated sufficient statistics. It updates the tree model whenever a new piece of data arrives [23].

HT guarantees that an optimal splitting attribute can be acquired based on a partial dataset rather than on a complete dataset. An obvious merit of HT is that its output is asymp- totically nearly identical to that of a non-incremental learner using infinite many instances. This idea is mathematically supported by the Hoeffding bound (HB) [46], which selects attributes as test nodes by using a training dataset that is as small as possible, which produces the same result as the result that would be chosen using an infinite dataset with high confidence (e.g., the confidence inWeka is 1-1.0E-7). In fact, HB [47] states that with a probability of 1 ??, a random variable in the range Rwill not differ from the estimatedmean after n observations by more than  HB = ? (1/2n)R2 ln (1/?) (1)  where R is the class distribution, and n is the number of instances that fall into a leaf. Note that the Hoeffding bound is a monotonically decreasing function of instance number n.

Assume that Xi is the attribute that has the highest evaluation value Ei (e.g., the information gain or Gini Index), and Xj is the attribute that has the second highest evaluation value Ej.

WhenEi?Ej > HB, Xi is chosen as the best splitting attribute at the current node with a confidence of (1? ?).

2) HOEFFDING ADAPTIVE TREE AND HOEFFDING OPTION TREE The Hoeffding Adaptive Tree (HAT) uses the ADWIN algo- rithm [20] to monitor the performance of the branches during the period of tree construction. In HAT, when the accuracies of the old branches are lower than those of the new branches, they are replaced. This approach places an adaptive window of instances at each node, which raises an alert whenever a change in the attribute-class statistics is detected at the node [48]. The essence of the ADWIN algorithm is the following: whenever two ??large enough?? sub-windows of a sliding window W display ??distinct enough?? averages, it is confident that their corresponding expected values are differ- ent, and the older portion of the window must be discarded  [20]. The Hoeffding bound is eventually updated as follows:  m =  1/|W0| + 1/ |W1| (2)  HB = ? (1/2m) ? ln(4 |W | /?) (3)  where m is the harmonic mean of the length of subwin- dows |W0| and |W1|, and ? is a confidence bound that indicates how confident we want to be in the algorithm?s output.

HoeffdingOption Trees (HOT) are normal Hoeffding trees, which contain option nodes that apply different tests [49].

An instance travels down multiple paths of the decision tree and arrives at multiple leaves [50]. HOT is capable of simulta- neously representing multiple trees in a single structure [25].

It introduces another parameter, ?  ?  , for deciding when to add another split option beneath a node that has already been split [50]. A new option can be added if the best unused attribute looks better than the current best existing option according to the G? criterion and a Hoeffding bound with confidence ?  ?  [50]. ? ?  can be expressed in terms of a multi- plication factor ?, which specifies a fraction of the original Hoeffding bound:  ? ?  = e? 2 ln ? (4)  HB = ? (1/2n) ? R2 ? ln (1/?? ) (5)  3) iOVFDT iOVFDT [51] optimizes the process of tree construction via functional tree leaf and incremental optimization to obtain a tradeoff between the accuracy and tree size. iOVFDT utilizes a weighted Na?ve Bayes classifier to reduce the effect of having an imbalanced class distribution, where the classifier on the leaf can further enhance the prediction accuracy via the embedded classifier. It chooses the class that has the maxi- mum possibility computed by the weighted Na?ve Bayes [51] as the predictive class in a leaf:  pijk = ?ijk P(xij|yk ) ? P(yk )  P(xij) where ?ijk =  nijk?K k=1 nijk  (6)  where xij is the jth value of attribute Xi, and yk is the kth  class value. Here, nijk is the sufficient statistic that reflects the number of instances that have attribute Xi equal to xij and class value equal to yk.

4) VOTED ENSEMBLE MULTI-CLASSIFICATION ALGORITHM Ensemble methods merge several models, whose individual predictions are combined in a certain manner (e.g., averag- ing). The output of this method is a final prediction that has better accuracy and convenience to scale and parallelize compared with single classifier methods [25]. Among the current ensemble methods, component classifiers are merged into an ensemble that has a fixed size, and models are built from relatively small subsets of the data. Once the ensemble is full, new classifiers are added only if they satisfy some quality  VOLUME 5, 2017 3535    S. Yang et al.: Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine  Algorithm 1 Voted Ensemble Multi-Classification Algorithm Input: data stream, different classifiers (e.g., Hoeffding Tree) Output: a decision tree Set classifier model T = null; while more data points are available read |Xi| instances between samples of the learning per-  formance; // |Xi| is the sample frequency for each classifier Cj in ensemble E if (Ti?1?T 6=null) //Ti?1 is a temporary classifier model  in the last iteration.

update temporary classifier model Ti?1 using Cj on  Xi and acquire Ti; //update not re-construct model elseif (temporary model Ti?1 = null)  build a new model Ti using Cj on Xi; if (Quality(Cj) > Quality(Cj?1)) //modify votes Ti = Tij; //Tij is created by the classifier Cj with the  best quality (e.g., accuracy or Kappa statistic in E) end_for save temporary classifier model Ti; end_while  criterion (e.g., accuracy), which is based on their estimated ability to improve the ensemble?s performance [52]. Due to the fixed size of the ensemble, one of the existing classifiers must be removed. Then, a tree is iteratively built upon upcom- ing data. Performance estimations are conducted by testing the new tree (and the existing ensemble) on the next chunk of data points [52].

In our voted ensemble multi-classification (VEMC) algo- rithm, as shown in Algorithm 1, decision trees are constructed by different classifiers that are designed for data stream mining. Initially, each classifier receives equally weighted votes. With upcoming instances, based on the error rate or prediction accuracy, the weighted vote for each classifier is adjusted dynamically based on the computing quality of the corresponding classifier. The basic idea of the VEMC algorithm is based on two assumptions: (1) the classification quality of an ensemble classifier is better than an ad hoc classifier, and (2) with successive upcoming instances, the performance of an ensemble classifier improves.

The two assumptions above can be demonstrated by a simple case. We use HT algorithms with different numbers of instances that a leaf should observe (i.e., grace period) between split attempts as different classifiers. Assume that HT1 and HT2 with 200 and 100 grace periods, respectively, are applied on two cancer datasets [53] with 34200 and 342000 instances each. From Table 1, in the first dataset (34200 instances), HT2 is better than HT1 in terms of accu- racy and stability, but its performance becomes worse in the second dataset (342000 instances). Thus, it is advisable to use an ensemble classifier because the performance of different classifiers could be changed with different datasets.

TABLE 1. A simple case.

Moreover, the classifier quality is positively correlated with the number of instances.

B. EXPERT EXPERIENCE In addition to the rules that are generated from the decision tree, it is also important to model the knowledge that medi- cal experts use for clinical diagnosis [54]. This approach is important because of some specific cases, such that cases in which several diseases that are inferred by the rules in a decision list could have probabilities that are not significantly different from one another (i.e., p ? 0.05). Medical experts are encouraged to enter new rules or edit existing rules to provide a solid decision. Rules that are directly provided by medical experts are given different priorities based on what stage in the diagnosis and treatment protocol the newly created/modified rule pertains to. This paper designs a rule editor to facilitate medical experts to enter rules.



VI. SEMANTIC INFERENCE The whole procedure of semantic inference on clinical doc- uments in the inference engine can be segmented into two stages. The first stage focuses on disease detection, which relies on knowledge that is learned from clinical document content and the rules transformed from decision trees as inputs to infer the type of disease. The second stage makes an inference on disease treatment, using the diagnosis output of the first stage and then comparing between the clinical document and historical medical records with similar symp- toms. To implement these two stages, a semantic inference algorithm with defeasible logic as an inference strategy is proposed in this section.

In the following sections, we first discuss the inference strategy that is used in our semantic inference scheme, and then, we discuss how to translate different types of semantic relations in our clinical tabular document into rules. Finally, the semantic inference algorithm that is used in the inference engine is given.

A. INFERENCE STRATEGY 1) DEFEASIBLE LOGIC In the semantic inference scheme, defeasible logic is applied for rule reasoning, which handles both strict and defeasible rules, especially the priority (e.g., a progressive relation in a document). It is a simple rule-based approach to reasoning with incomplete and inconsistent information.

3536 VOLUME 5, 2017    S. Yang et al.: Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine  TABLE 2. Mapping strategy between SR and RS (partial).

Based on different structures of rules, the Tabdoc rule syn- tax [55] is used to define the rules as different technical com- ponents, such as (i) normal rule: each normal rule has a head and a body. The head part proceeds to the keyword THEN, whereas the body part follows the keyword IF and precedes the THEN part. (ii) fact: a fact is a special rule that has no body part. When defining a fact, only the head part is given.

(iii) semantic relation: different semantic relations stand for different functional relationships between the constituents in a document. Before performing semantic inference on a clinical tabular document, its semantic relations should be represented by different types of rules (e.g., facts, queries or normal rules). This mapping from semantic relations to rules is explicitly discussed in Table 2. (iv) function: functions used in the head of a Tabdoc rule are called responders, and they usually implement functionalities such as assignment or query. Functions used in the body of a Tabdoc rule are called testers, and they judgewhether conditions are true or false. (v) queries and answers: queries are special rules without heads.

When defining queries, only the body part of a Tabdoc rule is given. The answer to a query is oftenmodeled as a set of facts.

(vi) processes: a process is a conditional sequence of activities (or operations) and is heterogeneous in different parties [10].

Different parties are likely to design heterogeneous processes that might be applicable and adaptable for specific contexts.

Activities can be divided into three categories: private, com- munity and public. Private activities are internally defined operations of a party and cannot be understood by the outside world. Community activities are mutually understandable within a group of parties. Public activities are public oper- ations that are understandable by all parties. In this paper, a clinical document template corresponds to an activity that is defined in the design phase of the document template.

By utilizing the ConexNet concept representation [56], users can collaboratively create new concepts in vocabularies for activity definition when creating processes. For example, a process could be similar to the following: IF activity1(doc1) THEN activity2(doc2), where an activity contains a clinical document as its real parameter.

Having received a clinical tabular document (CTD), it is required to process the document and extract a series of components as the input to an inference engine. Table 2 partially summarizes the mapping strategy between each type of semantic relation (SR) in CTD and the rules used in the inference engine.

2) FUNCTIONALITY OF RULES In an inference engine, the priority in the execution order of the rules is determined by their corresponding functionalities.

In this paper, rules have the following functionalities: (i) basic conditions (Rc) transformed from a newmedical record; (ii) decision rules (Rdr) to generate the temporary result set TRS; (iii) expert rules (Repr ) for the temporary result set update and decision; (iv) rules for a temporary treatment strategy (Rts); and (v) rules for a treatment strategy (Remr ) update. The priority of rules with different functionalities is defined as {Rc > Rdr > Repr > Rts > Remr}.

B. SEMANTIC INFERENCE ALGORITHM (SIA) Based on the inference strategy in Section 6.1, this section describes a Semantic Inference Algorithm (SIA) that is used in the inference engine in the clinical diagnosis and treat- ment system (CDTS). The goal of the algorithm is to imple- ment a diagnosis and determine a treatment strategy with an input clinical tabular document and the corresponding rules (e.g., decision rules in a decision list).

1) PRECONDITION OF SIA SIA has preconditions as follows.

Rc : a set of rules about patient basic conditions (e.g.,  patient symptoms) that are transformed from a clinical tabular document (as shown in Table 2) in a logical format (e.g., facts, queries).

DS (Data source): historical medical records of patients, in  the form of .arff or.csv.

Rdr : decision rules transformed from the decision tree gen-  erated by the voted ensemble multi-classification algorithm.

Repr : expert rules created by medical experts for decision  making.

Rts/Remr : a set of rules to query/update a treatment  strategy.

VOLUME 5, 2017 3537    S. Yang et al.: Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine  Algorithm 2 SIA 1 BEGIN  2 Pcd = Transform(clinical document) THEN /?  transform received clinical document into a medical record in a compatible form with DS ?/  3 ??? Cluster(DS 2 Pcd); /? cluster a new medical record into a group with similar historical medical records and form a new dataset ?/  4 TRS???Apply(r:Rc2 r:Rdr); /? Apply decision rules (Rdr) on basic conditions (Rc) to get temporary result set (TRS) ?/  5 IF |TRS| = 0 6 THEN{ send error message;} 7 ELSE {  8 IF |TRS| = 1 /? |TRS| = 1 means the number of final result is only one. ?/  9 THEN { 1TRS??? TRS; } 10 ELSE{ 11 IF (Repr!=NULL) 12 1TRS??? Apply(TRS 2 r: Repr); /?Apply  expert rules (Repr ) on TRS to update |TRS|;?/} }  13 FINAL { FRS??? Query(2 Rts); /?Apply treatment strategy rules Rtson based on 1TRS and get disease treatment set (FRS); ?/  14 IF (Remr!=NULL) 15 1FRS??? FRS 2 r:Remr; /?Use emergent  rules Remr to update the treatment strategy in FRS; ?/} 16 END  2) POST-CONDITION OF SIA The post-condition of SIA is the inferred results (e.g., disease diagnosis and treatment strategy).

3) SIA ALGORITHM Based on the inference algorithm SIA, as shown in Algorithm 2, the whole inference procedure is mainly divided into three steps, as follows.

Step 1 (line 1-9): This step first uses a clustering technique  to form a groupwhere themedical record of a new patient is clustered into the group together with similar historical med- ical records in terms of patient symptoms. This paper takes the COBWEB method as the clustering algorithm, which is based on a category utility (CU) function that measures the clustering quality. The definition of CU is  CU (C1,C2, . . . ,Ck)  =  ? l Pr[Cl]  ? i ?  j (Pr[ai = vij|Cl] ? Pr [ai = vij]2)  k (7)  where C1,C2, . . . ,Ck are the k clusters; the outer clusters summation is over these clusters; the next inner one sums over the attributes; and ai is the ith attribute, and it takes on the values vi1, vi2, . . . ., which are addressed by the sum over j.

TABLE 3. Groups of manifestations in lung cancer (lc).

Note that the probabilities are obtained by summing over all of the instances.

Then, the decision rules (Rdr) are created by our clas- sification algorithm (see Section 5.1.4) and are applied to basic conditions (Rc) to infer possible diagnoses, generating a temporary result set TRS (e.g., TRS = {diabetes mellitus, breast mass}). If the number of inferred diseases in TRS is zero, then it sends an error message. If the number of components in TRS is one, then it takes the TRS as the final diagnosis (1TRS).

Step 2 (Line 10-12): This step is to exclude the unlikely  inferences by using expert rules (Repr), namely, deducing the number of components in TRS and finally outputting the updated result set (1TRS) as the diagnosis. This step can be ignored if Repr is empty.

Step 3 (Line 13-16): The purpose of this step is to find  suitable previous patient treatment records for a new patient.

A query is executed on historical patient records  about the same disease in 1TRS by using the rules Rts, and then, it stores the treatment strategy of the best matching record in the disease treatment set (FRS). If there is a set of emergent rules (Remr) that are designed by clinicians, it then uses Remr to dynamically update FRS and thereby output the new treatment strategy 1FRS. In this step, we must compute the similarity degree among the different medical records.

Specifically, the similarity between the medical records is evaluated based on the symptoms. Table 3 shows some of the symptoms (i.e., manifestations) of lung cancer. Due to the complexity of the pathology, the symptoms of each patient can be regarded as a combination of specific manifestations.

Mathematically, the calculation of a similarity degree between the symptoms in two medical records is essen- tially the comparison of two arrays. We can use a matrix to represent the comparison result. For simplicity, we assume that different symptoms are independent of one another. Thus, when all of the symptoms are represented by symbols such as ?a, b, . . . , z, aa, bb, . . .? and sorted in alphabetical order, only  3538 VOLUME 5, 2017    S. Yang et al.: Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine  TABLE 4. Comparison of symptoms between two medical records.

the principal diagonal elements in the matrix are likely to be non-zero. Table 4 shows an example of a comparison between two medical records.

The similarity computation (Sim or S) between two medi- cal records is based on the similarity measurement (Satt ) for each pair of attributes (att ?Attributes). The following for- mula shows one example of a similarity calculation between two medical records.

Sim (src, trg) = n  ? att?Attributes  (Wa?Satt (src, trg)) (8)  where src (source) is a historical medical record, and trg (target) is a new patient record. Sim(src, trg) represents the similarity degree for the symptoms in the two records (src and trg). Here, ?n? is the number of attributes (i.e., symptoms) in these two records. Satt represents the similarity degree for a certain attribute in the two records.Wa is the weight of attribute a, which represents the correlation degree between attribute a and the class attribute (i.e., the disease type).

Wa = GainRatio = InfoGain(a)?  a?Attributes InfoGain(a) (9)  The higher the value of Wa, the more correlated that attribute a is with the class. Thus, it is necessary to consider the weight of the attribute when judging the similarity degree of twomedical records. In a decision tree, an attribute that has a high correlation degree with the class attribute is close to the root node. InfoGain (a) is the information gain of attribute a.

InfoGain (a) = I (S1, S2, . . . , Sm)? E (a) (10)  where I (S1, S2, . . . , Sm) is the information amount that is required to split a dataset.

I (S1, S2, . . . , Sm) = ? ?m  i=1 pi log2 pi (11)  where Si is the number of samples in classes Ci (i= 1, 2, . . . , m), and pi means the probability of the instances in S = {S1, S1, . . . , Sm} that belong to class Ci. E (a) is the infor- mation amount for splitting a sub-dataset by attribute a.

E (a)= ?v  j=1  (s1j + s2j + . . .+smj) s  ?I ( s1j, s2j, . . . , smj  ) (12)  where sij is the set of instances whose class values are Ci in the subset of {sj|a = aj, j ? 1, 2, . . . , v,sj ? S}.

I ( s1j, s2j, . . . , smj  ) means the average information amount  that is required to identify the class labels for all of the instances in sj.

I ( s1j, s2j, . . . , smj  ) = ?  ?m i=1  pij log2 (pij) (13)  where pij is the probability that the instances in sj belong to the class value Ci. The value of Satt depends on the data type of attribute att. If attribute att is a numeric attribute, then  Satt (x, y) = min{xatt , yatt }  Ba (14)  where Ba is the breadth of the range of attribute att, and xatt , yatt , Ba are all mapped into the interval [0, 1]. This approach is applicable because the range of every numerical attribute in this application is bounded. If attribute att is a nominal attribute, then  Satt (x, y) = { 1, iff x = y 0, iff x 6= y  (15)  where x, y ? {true/false, yes/no, and so on}. Table 4 can be abstracted as amatrix called a similaritymeasurement matrix, as shown below.

? =  a b c d e  ?????? Sa 0 0 0 0 0 Sb 0 0 0 0 0 Sc 0 0 0 0 0 Sd 0 0 0 0 0 Se  ?????? (16) Similarity measurement matrix ? is a diagonal matrix,  while the range of the value of the diagonal elements is in [0, 1] and the values of the non-diagonal elements are zero.

1 means that the values of the two medical records regarding the same attribute are equal, while 0 means that they are unequal. We take the historical medical record whose matrix ? has themaximum rank as the best matching case for the new medical record in the treatment inference phase. The rank is the number of non-zero rows in the reduced row echelon form of the matrix. The rank could vary, because different medical records could have different values on the same symptoms.

Specifically, if any one value out of two medical records regarding the same symptom (e.g., attribute a) is null, then the diagonal element becomes 0. The diagonal element becomes 1 if the attribute a is nominal and the two values on a are the same. If attribute a is numerical, then a probability will be given to describe the similarity between the records.



VII. EXPERIMENTS Our aim is the evaluation of CDTS for disease detection and treatment suggestionswithmachine learning algorithms (e.g., clustering and classification) and an inference engine.

A. EXPERIMENTAL DATA AND EVALUATION MATRICES 1) DATA SOURCE The dataset has been crawled from the website of the Cancer Data Access System (CDAS, https://biometry.nci.nih.gov/ cdas/), which records data from the NLST (National Lung Screening Trial), PLCO (Prostate, Lung, Colorectal, and  VOLUME 5, 2017 3539    S. Yang et al.: Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine  FIGURE 5. Snapshot for GUI-based clinical tabular document template designer.

Ovarian Cancer Screening Trial), and IDATA (Interactive Diet and Activity Tracking in AARP study) cancer stud- ies. It includes 342 outpatient records with 31 attributes by integrating several datasets, such as Lung Cancer, Spiral CT Comparison Read Abnormalities and Diagnostic Procedures, through the patient?s ID as a key. The attributes of each medical record include the lung cancer grade, cancer position, treatment type, and abnormality type.

2) EVALUATION MATRICES This paper uses the accuracy and kappa statistic as the eval- uation matrices that are most commonly used in machine learning algorithm evaluation. The accuracy (Acc) is defined as the number of true positives (TP) over the number of true positives plus the number of false positives (FP), as defined in formula (17).

Acc = Tp  Tp + Fp (17)  The kappa statistic (?) measures the agreement of a predic- tionwith the true class, where 1.0means complete agreement, as defined in formula (18).

? = po ? pe 1? pe  (18)  where po is the relative observed agreement among the raters, and pe is the hypothetical probability of a chance agreement.

B. EXPERIMENTAL PROCEDURE The experimental procedure includes the following steps:  Step 1 (Data Preprocessing): This step focuses on two types of data transformation. One type is to transform his- torical medical records (HMR) to historical patient instances in .arff or .csv format. The other type is to transform a new DocLang-based clinical tabular document into a patient instance in .arff or .csv format, where the language terms that are present in the template become attributes whose corre- sponding values are textual terms that are input from users (e.g., clinicians). Because the clinical document is based on the clinical tabular document model (CTDM), which guaran- tees semantic consistency through the term chain and seman- tic relation chain, the transformation of clinical documents among heterogeneous contexts does not cause semantic loss.

Fig. 5 shows the GUI of the clinical tabular document template designer (CTDTD). CTDTD generates customized clinical document templates that enable clinicians to design a variety of clinical tabular documents according to their particular needs. For example, a template that allows patients to enter their symptoms about lung cancer can be made, in such a way that new medical records can be created for online consultation. In this case, a reified clinical document corresponds to a record in a dataset (e.g., a row in an .arff or .csv file). Standard windows data entry elements (e.g., check boxes, radio buttons, input boxes and combo boxes) can also be inserted into the templates. To further facilitate the data input, a semantic input method (SIM) [37] is also integrated in CTDTD. Any term that is input through SIM is referred from the CONEX dictionary [37], [41] by selecting the exact meaning from a drop-down list. Each concept in the CONEX dictionary has a unique identifier (iid) that points to the same meaning regardless of the context. The information to  3540 VOLUME 5, 2017    S. Yang et al.: Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine  TABLE 5. Examples of patient records in dataset .

exchange is the logical combination (i.e., the logical structure in the CTDM model) of iid(s) of different concepts.

CTDTD consists of several frames. The frame of the Sub- table form lists the most-often used basic sub-table forms.

The Template Preview frame is an operation platform for designing the layout structure of the clinical tabular docu- ments. The users drag the required sub-table forms to the console in the Template Preview frame or manually create the sub-table forms by merging or splitting the table cells. Each type of sub-table form has to be mapped to at least a certain type of semantic relation managed by the attribute ?st? in the Property List frame, which is identified by a group identifier GID. The semantic relations of manually created sub-table forms should be set in the Property List frame by the users themselves. In addition, clicking on any cell in the Template Preview frame will pop up the Property List frame that shows all of its denoters. All of the sub-table forms in the Template Preview frame construct a complete clinical tabular document template. The Logical Structure frame shows the logical data model in a tree-based pattern.

Step 2 (Clustering):Compare the new patient instance with  historical ones to acquire a set of the same or similar instances and form a group. Two assumptions are defined for simplicity of discussion. The first is that patients with the same or similar attributes (i.e., physical examination parameters) and corre- sponding values are very likely to have the same disease. The second is that different hospitals have the same examination protocol on symptoms of the same disease. The purpose of these two assumptions is to ensure that all of the patients with the same class of diseases are clustered into the same group as much as possible. The group that the new patient instance falls into is taken as a new dataset.

Step 3 (Classification):Different diseases can share similar  symptoms (e.g., fever, white blood cell counting increase, and so on). Thus, patients who have the same or similar symptoms could have different diseases. According to the new dataset  (i.e., the group) acquired in step 2, a decision tree is constructed via running the voted ensemble multi- classification algorithm on .

For example, if the new patient being tested has lung  cancer, the new dataset  as input for classification includes 342 outpatient records of the data source (see Section VII- A1). Each record in contains 31 attributes with the attribute ??lc_grade?? (see Table 3) as a class. The class attribute has five categories which indicate the severity of lung cancer.

Table 5 exemplifies five historical patient records in the dataset .

TABLE 6. Decision tree vs. decision rules.

Step 4 (Rule Set Creation): Extract rules from the decision tree and form a decision list to be used as the rule base for inference. Then, the decision list will be the input of the inference engine. Table 6 shows an iOVFDT-generated decision tree and its decision rules.

Step 5 (Disease Detection): Predict the type of disease for  the new patient instance by using the rules in the decision list, and then, store the inferred possible diseases in TRS.

Fig. 6 shows the GUI of the disease detector to cluster similar cases and infer possible diseases. The top frame contains a detailed description of the recovered cases, while the left- bottom frame contains inferred diseases with probability val- ues followed.

Once similar instances are recovered by clicking the clus- tering button at the bottom right, users (e.g., doctors) can select the most similar ones. As shown on the top frame in Fig. 6, each instance is followed by a multiple choice box.

When the clustering button is clicked, multiple choice boxes of all instances are selected because the system by default considers all cluttering-generated instances to be similar to the new one. After that, the classification button is clicked to show the probabilities of inferred diseases. Then, the doctor is encouraged to judge whether the inferred diagnosis is correct or not. Initially, the check box followed by ?Primary disease detection? is selected. When the doctor considers that any disease in ?Other disease detection? has high probability, the  VOLUME 5, 2017 3541    S. Yang et al.: Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine  FIGURE 6. Snapshot for GUI-based disease detector.

corresponding check box can be selected. If diagnosis by the doctor does not appear in the inferred results, it can be manually typed using the ?Expert decision? input box with a probability. Once disease detection output is revised by the doctor, the new instance including the diagnoses will be stored to a TRS.

Step 6 (Disease Revision): Doctors need to make a def-  inite clinical decision by designing the negative as failure in defeasible logic (i.e., negative rules) through a rule editor (see Fig. 7) to avoid impossible cases in TRS. For example, if score on one symptom is higher (or lower) than certain value, it is unlikely to be certain disease. Repeat this procedure until only one disease is left and we obtain updated temporary result set 1TRS.

Step 7 (Treatment Strategy Reasoning): Different diseases  have different treatment protocols. In addition, even for the same disease, because the physical conditions of a patient are ever-changing during the treatment period, the treatment strategies could vary frequently and cannot be determined dynamically by most traditional CDSS. Thus, we must define more general rules. In the following section, we first propose an assumption before giving two general rules as an example for treatment strategy reasoning.

Assumption: different patients have different treatment  strategies. A treatment strategy can be divided into different phases, as shown below.

FIGURE 7. Rule editor.

Rule 1: If the symptoms of a patient at the current stage of treatment match those of a historical medical record during the same period, then the patient takes the corresponding treatment from that old record as the strategy in the current phase.

Rule 2: If no historical medical records match the new  medical record in terms of symptoms at the current treatment phase, then, according to the symptoms of the new patient in all previous treatment phases ? , we collect a group  with records that share similar symptoms with ? in previous phases. The next step is to compare the symptoms of a new patient at the current stage with phases of each record in   3542 VOLUME 5, 2017    S. Yang et al.: Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine  TABLE 7. Comparison of different classifiers used in CDTS.

and determine the one whose symptoms are the closest to those of the new patient at this stage, and its corresponding treatment strategy is hereby applied to the patient at this stage.

It is of note that the treatment phase numbers of the old and new medical records might not be the same in this case.

Afterward, the treatment phase number for the new patient is adjusted to the historical medical record?s next treatment phase number. Repeat Rule1 until the patient fully recovers.

For example, a dataset  will be established when the symptoms of a new patient in the third phase do not match the historical records at the same time point. Then, we go through all of the records again, and if any two phases of one historical record share similar symptomswith the first two phases of the new patient, it will be kept in . Given that any historically successful treatment ? hasm treatment phases on average, the comparison procedure will be conducted C2m = m(m? 1)/2 times at most. If there are n cases, then the time complexity is O(n?m2). This method is used to capture extended similar historical cases. After the establishment of, all of the phases of the cases in are compared with the third phase of the new patient treatment on the symptoms to find the most similar one with the maximum rank at the similarity measurement matrix ? , which determines the treatment strategy for the new patient at the third phase.

C. RESULTS AND ANALYSIS To enlarge the training dataset, we randomly resample the dataset 1000 times without replacement by using the filter Resample in Weka. In Table 7, iOVFDT constructs the small- est tree model compared with the other three classifiers in terms of the nodes, leaves and depth. HT and HAT also perform well in the construction of a nice tree model with 97 and 116 nodes, 50 and 58 leaves, and 21 and 23 depths, respectively. HOT has the largest tree size, with more nodes and leaves, which implies training dataset overfitting.

As shown in Figs. 8 and 9, Hoeffding-based tree classifiers have better performance compared to iOVFDT in terms of the classification correctness (i.e., accuracy) and kappa statis- tic (i.e., stability). With continuous incoming instances, the accuracy and stability of Hoeffding-based tree classifiers rise with some fluctuation, while the performance of iOVFDT is kept at a horizontal level. Similar to HAT, the voted ensemble  FIGURE 8. Comparison of classification correctness.

FIGURE 9. Comparison of the Kappa statistic.

FIGURE 10. Comparison of the evaluation time of the CPU.

multi-classification algorithm (VEMCA) also performs well in terms of accuracy and stability and outperforms HATwhen the number of instances is larger than 340000.

Figs. 10 and 11 compare the evaluation time of the CPU and the memory cost of the five classification algorithms.

iOVFDT shows the fastest speed and the smallest memory cost among the five classification algorithms. HT gives bet- ter performance than HAT and HOT with 6.93 seconds in CPU occupation and a 0.91-Mb memory cost. VEMCA?s evaluation time and memory are 20.73 seconds and 1.62 Mb, respectively.

VOLUME 5, 2017 3543    S. Yang et al.: Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms With an Inference Engine  FIGURE 11. Comparison of the memory cost.



VIII. CONCLUSIONS Modern techniques such as CDSS and HIS have substan- tially facilitated clinical diagnosis and treatment. However, some of the emergent problems remain to be considered.

The first problem is challenging the traditional data analy- sis techniques due to rapidly increasing amounts of multi- dimensional clinical data. The second is the difficulty of clinical information integration. Clinical information from different contexts are heterogeneous in terms of structure and semantics. For the benefit of patients and long-term clinical development, it is very promising to integrate clinical infor- mation (e.g., outpatients? medical records) from different clinicians or hospitals to assist in clinical decision-making.

The third problem is the call for personalized medicine and professional medical treatment, because different doctors or hospitals are good at diagnosing and treating different dis- eases, and the patients have the freedom to choose among them.

To solve the three problems above, this paper proposes a clinical diagnosis and treatment system (CDTS) that assists patients in choosing clinicians or hospitals according to their requirements (e.g., distance from home to hospital). The foundation for disease detection in CDTS consists of deci- sion trees that are created by our voted ensemble multi- classification algorithm that enables decision-tree-based data stream mining. Furthermore, to integrate clinical documents and heterogeneous health information systems, a new clin- ical tabular document model is proposed, which represents clinical documents in tabular format and maintains consis- tent vocabulary terms and semantic relations among differ- ent contexts through the term chain and semantic relation chain. Additionally, a novel semantic inference algorithm is designed for disease detection and treatment suggestion, based on the rules that are generated by decision trees and similarity computations among medical records. The contri- bution of this paper can be summarized as follows: ? A unified clinical tabular document model (CTDM)  implemented by a new clinical tabular document lan- guage (DocLang, XML based) facilitates the interoper- ability among different clinical decision support systems (CDSS).

? A new voted ensemble multi-classification algorithm is proposed, in terms of running multiple decision tree- based classification algorithms simultaneously on the same data stream and voting for the best output. The results are associated with statistical information on the accuracy of classification, which provides proof of opti- mization.

? A novel clinical diagnosis and treatment system with a newly designed semantic inference algorithm supports clinicians in the decision making process as well as saving time and expense for the patients.

For future work, it is necessary to expand DocLang to become a more comprehensive markup language by import- ing more semantic relation types for more complex clinical documents. These studies are in progress and are expected to present valuable results later.

