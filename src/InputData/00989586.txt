A Fast Algorithm to Cluster High Dimensional Basket Data *

Abstract  Clustering is a data mining problem that has received sig- nificant attention by the database community. Data set size, dimensionality and sparsity have been ident@ed as aspects that make clustering more dificult. This work introduces a fast algorithm to cluster large binary data sets where data points have high dimensionality and most of their coordi- nates are Zero. This i s  the case with basket data transactions containing items, that can be represented as sparse binary vectors with very high dimensionality. An experimental sec- tion shows performance, advantages and limitations of the proposed approach.

1 Introduction  Clustering algorithms identify those regions that are more densely populated than others in multidimensional data [8, 131. In general clustering algorithms partition the data set into several groups such that points in the same group are close to each other and points across groups are far from each other. It has been shown that high dimensionality [lo], data sparsity [ 11 and noise [2] make clustering a harder problem.

In this work we focus on the problem of efficiently clus- tering binary data sets that are sparse and have very high dimensionality. This is precisely the case with basket data transactions, where transactions contain combinations of a few items out of thousands of items. Our approach can be used as an alternative data mining technique to association rule discovery [3].

1.1 Overview  We introduce a fast clustering algorithm for sparse high dimensional binary data (basket data) based on the well- known Expectation-Maximization (EM) clustering algo- rithm [7, 17,6, 131. The EM algorithmis a general statistical method of maximum likelihood estimation [7, 14, 171. In  ?This work wassupported by grant LM 06726from the NationalLibrary of Medicine  particular it can be used to perform clustering. In our case we will use it to fit a mixture of Normal distributions to a sparse binary data set.

Our algorithm is designed to efficiently handle large prob- lem sizes as typically encountered in modern database sys- tems and it is guaranteed to produce high quality solutions as it will be shown by our experiments.

The proposed clustering algorithm builds a statistical model so that the user can understand transactions at a high level. Items are mapped to binary dimensions and transac- tions are thus mapped to binary data points. The basic idea is to group similar transactions. Clusters of transactions can have different interpretations. Each cluster can tell us what the typical transaction looks like; this is precisely the mean or average of transactions per cluster. Each cluster describes which items commonly appear together in each transaction.

Since cluster centroids are averages of binary numbers the mean of a certain dimension can be interpreted as a proba- bility or a percentage. If transactions are not well clustered in certain dimensions this can be explained by the deviation they have from the mean. The user will be able to compare several cluster models by looking at a quantity measuring their quality.

1.2 Contributions and paper outline  This is a summary of our contributions. We introduce a novel algorithm to cluster very high dimensional and sparse binary data sets. The proposed solution does not require complex data structures to store patterns or model param- eters, but only matrices that in general can fit in memory.

From a quality point of view the algorithm computes highly accurate clusters. From a performance point of view the algorithm is fast, having linear time complexity in data set size, in transaction size and in the desired number of clusters.

The rest of this paper is organized as follows. Section 2 provides definitions and statistical background. Section 3 contains the algorithm to cluster high dimensional and sparse binary data sets. Section 4 contains a brief experimental evaluation. Section 5 discusses related work. The paper concludes with section 6.

0-7695-1 119-8/01 $17.00 0 2001 IEEE 633    weiehts  Matrix N M  Table 1. Output matrices  size contents k X 1 d x k M3 = c2, t , ,V t ,  E D3  2 Definitions and statistical background  This section provides formal definitions that will be used throughout this work. First, basic statistical background on EM and the mixture problem are described. Second, addi- tional definitions relating transactions and multidimensional binary vectors are introduced.

The multivariate normal density function for a d- dimensional vector 2 = [ X I ,  2 2 ,  . . . , 2 d I t  is:  where p is called the mean vector and I; is called the covariance matrix; p is a d-dimensional vector and C is a d x d matrix. Our algorithm uses diagonal covariance matrices.

The input to EM are n, d-dimensional points and k ,  the desired number of clusters. These n points are modeled as a mixture of normal distributions as defined above. This mixture has 3 parameters, namely, the means, the covari- ances and the weights. Data set size, i.e. number of points, is n. The desired number of clusters is IC. Dimensionality is d. The parameters computed by the EM algorithm are stored in the matrices described in table 1. In the statistical literature all parameters are used as a single set called 0, i.e.

0 = {C, R, W } .  To refer to one column of C or R we use the j subscript (i.e. Cj, Rj).

Since it is our intention to cluster basket data we will combine our previous definitions with additional defini- tions commonly used for association rules [3, 41. Let D = { T I ,  T2, . . . , T,} be a set of n transactions contain- ing items, and let Z be a set of d items, Z = { i l ,  i 2 .  . . i d } , where each item will be identified by its index, that is, an integer in { 1 , 2 ,  . . . , d}. Let DI  , D2, . . . , Dk be k subsets of D (i.e. Dj D , j  = 1 . . . k ) .  s.t. DjnDj j  = 0, j # j' (i.e.

they are a partition of D induced by clusters). Each sub- set Dj represents one cluster. We use Ti to avoid confusion with ti that will be used as a binary vector: Ti will be a set of integers and ti will be a binary vector. Items will be mapped to binary dimensions. For each item i l ,  i2, . . . , i d  there will be a corresponding dimension bl . Each transaction T, will be given as a set of integers (items), T = {il , i 2 ,  . . . , i ~ } , where i l  E { 1 , . . . , d} and i (without subscript) denotes the  number of transaction; i E { 1 , 2 ,  . . . , n}. Then the nota- tion ti is used, meaning a binary vector, where each entry corresponds to one dimension (item). Then ( t i ) l  = 1 for 1 = i l ,  i 2 ,  . . . , i~ and ( t i ) l  = 0 otherwise. Each transaction becomes a sparse binary vector having d entries, but only K of them different from zero. So D in this case can be considered a huge and sparse d x n matrix. Each item il will be an integer, i l  E { 1 , 2 ,  . . . , d }  to index matrices to refer to one dimension. Mathematically transactions will be points in [0, I Id  space, but for the algorithm they will be sets of integers.

3 A clustering algorithm for binary data sets with very high dimensionality  3.1 Improvements  We propose several improvements and changes on EM to deal with very high dimensionality, sparsity, null co- variances, large data set size and slow convergence. Such improvements include suitable initialization for high dimen- sional data, sufficient statistics, covariance matrix regular- ization techniques, sparse distance computation and learning steps.

Initialization is based on the global statistics of the data set: the global mean and the global covariance. They are computed in a one-time pass over the data set and are avail- able thereafter. Seed centroids are initialized based on the global mean and standard deviation of the data. Sufficient statistics [ 12, 111 (table 2) are used to summarize infor- mation about clusters; this reduces I/O time by avoiding repeated passes over the data and by allowing to make pa- rameter estimation periodically as transactions are being read. The E step is executed for every transaction and the M step is executed a fixed number of times making con- vergence to the solution fast. The algorithm uses sparse distance computation and sparse matrix additions to make the E step faster. It uses regularization techniques [ 151 to deal with zero covariances, common with sparse data and specially with basket data. The algorithm requires two scans over the data per run. The main input parameter is only the desired number of clusters.

Input. Ti, T2,. . . , T, and k.

Output 0 = {C,  R, W }  and L ( 0 ) a t ( d k ) - I , L  t 50 FOR3 = I T O k D O  I* Initialize */  C, t p k UI' diag[u], RI t I, W, t I l k A, = S(a,C,,R,) = C;Ry1C3 M,  t C,, NI t 1  ENDFOR FOR scan = I TO 2 DO  L ( 0 )  = 0 F O R % =  1 T O n D O  t ,  t vect[T*] FOR? = 1 TO k D O  /* E step *I  st, t J(t* ,C, ,R, ) .

~ 1 3  t ( ( 2 ~ ) d l R ~ 1 ) - " ' ~ ~ p ( - J ~ ~ / 2 )  ENDFOR Let m be s t p , ,  2 p,,Vj E I . . . k M ,  t M ,  + t , ,  N, t nTm + 1 IF( z mod ( n / L )  = 0 ) THEN L ( 0 )  t U@) + W J t J )  /* M step *I FOR? = I T O k D O  Cl t Al,/N, RI t M,/N, - MliWJ/Nj -k I  A, t C:R;'C; ~ V J  4- x v j  / 1 ivJ  ENDFOR ENDIF  ENDFOR I* Reset sufficient statistics */ IF scan= I THEN M, t C, , N,  t I ENDIF  ENDFOR  Figure 1. Clustering algorithm for sparse high dimensional binary data  3.2 Algorithm to cluster sparse binary data sets with very high dimensionality  The pseudo-code of our clustering algorithm is in figure 1. This is a high-level description. The input is a set of transactions D = { T I ,  T2, . . . , T,, } and IC, the desired num- ber of clusters. The output is 0 = {C, R, W } ,  describing the mixture model, L ( 0 )  measuring model quality and a partitioning of D into k subsets. The constant cy is used to seed C based on d and I C .  The global statistics p and 2 are computed in a one-time scan and are available thereafter.

Standard deviations are computed as CTII = 6. The E step is executed for every transaction (n  times). S,j is efficiently computed using A?. The M step is periodically executed every n / L  transactions ( L  times). L is typically a num- ber between 10 and 100. The update formulas for C,  R,  Mi are based on sufficient statistics [ 121 M ,  N ,  shown in table 2, and regularization techniques [ 151. AI, N are the mul- tidimensional version of the univariate sufficient statistics shown in [ 121 when points are binary; due to lack of space we do not explain how to derive their formulas. Sufficient statistics are reset at the end of the first scan. The goal of the first scan is to get accurate cluster centroids C; and accurate  Figure 2. Quality of results  Figure 3. Performance  covariances Rj. The goal of the second scan is to tune 0 and recompute L ( 0 ) .  Dimensions (items) are ranked within each cluster by their value in Cj to make output easier to understand.

4 Experimental evaluation  This section includes experimental evaluation of our al- gorithm. All experiments were performed on a Sun Machine running at 600 MHz with 256 Mb of memory. This machine had several Gb of available disk space. Our algorithms were implemented in the C++ language and compiled with the GNU C++ compiler.

Our algorithm was evaluated with large transaction test files created by the well-known IBM synthetic data generator [4]. Test files are named after the parameters with which they were created. The standard way [4] is to use T (average transaction size), I (pattern length) and D (for us n )  to label files since those are the most common parameters to change.

The algorithm parameters were set as follows. L = 50 and  In this paragraph we explain quality of results. The left graph in figure 2 shows how our algorithm converges on the 1st scan. The 2nd scan just tunes the solution without decreasing L ( 0 ) .  The right graph in figure 2 shows how model accuracy increases as IC increases; the behavior is clearly asymptotic.

In this paragraph we describe performance with large data sets. Note that d = 1000 is a very high dimensionality.

The left graph in figure 3 shows running as time as we vary n for several typical transaction files; the algorithm scales  cy = l / (dk ) .

linearly. The right graph i n  figure 3 shows the impact of average transaction size ( T )  on performance; the algorithm is linear. Times varying k are also linear; this graph is not shown.

5 Related work  There has been so much work on both clustering and as- sociation rule mining that i t  is impossible to compare our approach with everybody else?s. To the best of our knowl- edge there is no previous work on clustering high dimen- sional and sparse large binary data sets using EM. We do not know work where there are experiments with 1,000 or more dimensions [9, 10, 1, 2, 61. Also, we believe that the idea of building a statistical model based on clustering for basket data has not been explored before. The only work that has analyzed how to cluster basket data transactions is [ 161; their approach goes in the oppositedirection since they mine associations and from them clusters are generated. We are not the first to propose a scalable and faster version of EM for data mining applications. One important work that also studied how to construct a faster and Scalable EM algorithm (SEM) is [6]. This work extended previous work on scaling K-means [ 5 ] .  The authors present an algorithm, also based on sufficient statistics [ 121, that makes compression in two phases for dense and quasi-dense regions. The authors use it to build several models concurrently. SEM is significantly different from ours. It is designed for low dimensional con- tinuous numerical data without zero covariance problems, and then it is not suitable for very high dimensional sparse binary data. It does not incorporate sparse distance compu- tation, regularization techniques. Initialization is done by sampling and it keeps sufficient statistics on many subsets of the data, many more than k .  Also, it uses an iterative K-means algorithm [ 141 to cluster data points in memory and then it does not make a fixed number of computations.

One advantage over ours it that it only requires one scan over the data, but it makes heavier CPU use and it requires careful buffer size tuning.

6 Conclusions  This paper presented a new clustering algorithm. The proposed algorithm is designed to work with large binary data sets having very high dimensionality. The algorithm only requires two scans over the data to cluster transactions and construct a statistical model. Each cluster is a summary of a group of similar transactions and thus represents one significant pattern discovered in the data. Experimental evaluation showed transactions can be clustered with high accuracy. Model quality mainly depends on IC, the desired number of clusters. The algorithm makes its best effort to get a high quality model given data characteristics. Performance  is linear and it is mainly affected by TI ,  k and transaction size, and minimally by dimensionality since data sets are sparse.

The algorithm is restricted to problem sizes whose model can fit in main memory.

