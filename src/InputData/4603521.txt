New Algorithm of Maximum Frequent Itemsets for Mining Multiple-level  Association Rules

Abstract  Discovering maximum frequent itemsets is a key issue in data mining applications. Most of the previous studies adopt an Apriori-like candidate itemsets generation-and-test approach, however, candidate itemsets generation is costly. In this study, we propose a new algorithm named ML_Pincer for discovering maximum frequent itemsets in multiple-level association rules. ML_Pincer algorithm combines the top-down and the bottom-up directions progressive deepening searching ideas, moreover, it uses two-way pruning tactic: the information which gathered in one direction can prune more candidate itemsets during the search in the other direction. It decreases candidate itemsets greatly and avoids making multiple passes over database, consequently, it reduces CPU time and I/O time remarkably. Experiments prove that ML_Pincer algorithm is more efficient than PMAM algorithm, especially when some maximum frequent itemsets are long.

1. Introduction   Association rules is an important KDD research issues [1, 2, 3] first presented by Agrawal, which task is to discover the hidden association relationship between the different itemsets in database. It?s the most important step and technique for finding the frequent itemsets in association rules. Current association rules include: Apriori, AprioriTid, AprioriHybird, Partition, DHP, ML_T2L1 [4], DMFI [5] etc, they are most improved based on Apriori algorithm. The algorithms based on Apriori principle we talked above are very time consuming on making passes over transaction database and generating new candidate itemsets in the processing of discovering the frequent itemsets, which is two aspects of disadvantages about Apriori-like algorithms, especially  on generating new candidates. Consequently, reducing the number of generated candidate itemsets is an efficient method to improve the efficiency of Apriori- like algorithm. Because all frequent itemsets are considered implicitly in the maximum frequent itemsets, we can convert the issue of discovering the frequent itemsets to the issue of discovering the maximum frequent sets.

With deepen research in mining association rules and there is hierarchy and taxonomies in most concepts in real life, users maybe not discover meaning patterns by only researching detail data in transaction databases, but they may find meaning patterns in higher level of concepts hierarchy. Furthermore, though the patterns which obtained from some hierarchy can be used for one user but nothing for another, consequently, association rules should provide efficient methods for mining multiple hierarchy, then the idea of data mining in multiple-level association rules is presented.

Current algorithms about multiple-level association rules include: ML_T1LA, ML_TML1, ML_T2LA etc, they are mostly based on Apriori algorithm which have two main aspects of disadvantages about scanning database repeatedly and candidate itemsets generation- and-test. Once there are many taxonomy items in concept hierarchy and the frequent itemsets are long, the disadvantages are more evident. In this paper, we develop a new algorithm named ML_Pincer algorithm, it reduces search times on discovering every level?s frequent itemsets in multiple-level association rules by utilizing the characteristic of Pincer-Search algorithm which combines the up-down and the bottom-up directions progressive deepening searching ideas when discovering maximum frequent itemsets, it prunes the maximum frequent candidate itemsets by using infrequent itemsets and avoid scanning database repeatedly, consequently, it increases searching efficiency. Experiments prove that ML_Pincer is more preponderant than PMAM. We will introduce some basic concepts in next chapter.

The 3rd Intetnational Conference on Innovative Computing Information and Control (ICICIC'08)     2. Basic concepts  2.1 Frequent itemsets and maximum frequent itemsets   Let I={i1,i2,?,im} be a set of m distinct items. A  transaction T is defined as any subset of items in I. A transaction database D is a set of transactions like T. A transaction T is said to support an itemset X I if it contains all items of X. The fraction of the transaction in D that support X is called the support of X, denoted as support(X). An itemset is frequent if its support is above some user defined minimum support threshold.

Otherwise, it is infrequent.

If all supersets of frequent itemsets F are infrequent itemsets, then F is denoted as a maximum frequent itemsets, all sets of like F are called maximum frequent sets (MFS), which stores all maximum frequent itemsets discovered.

Maximum frequent candidate set [5, 6, 7] (or MFCS for short) which is the smallest itemset, its sets of subset includes all the current frequent itemsets known, but it does not include any infrequent itemsets.

Obviously MFCS is a superset of MFS at any time of algorithm running. When algorithm terminates, MFCS and MFS are equal. We can possibly discover some maximum frequent itemsets by applying MFCS in early passes. Earlier discovery of maximum frequent itemsets can reduce the number of candidate itemsets which would be generated, therefore it can decrease CPU and I/O time. If the maximum frequent itemsets discovered are long, the algorithm?s performance is more excellent. The initialization situation of MFCS only includes one itemset, which includes all items in transaction database.

Obviously, all frequent itemsets are subset of maximum frequent set, therefore, the issue of discovering frequent itemsets can be converted to the issue of discovering maximum frequent itemsets.

2.2 Concepts of multiple-level association rules   The hierarchical tree [4] is a hierarchical  relationship tree which is from common concepts to concrete concepts as shown in figure 1. The data mining of multiple-level association rules discover association rules from every concept hierarchy. Firstly, we encode the hierarchical tree, the method is as follows: we encode root node of every hierarchical tree as a sequence digits, then we encode every tree?s node by every level, the code of sub node is encoded which combines code of the father node and the sequence  digits according to the sub node?s position in sub tree, the encoded hierarchical tree is shown in Figure 2.

Then all items in transaction database would be replaced by its corresponding code in the encoded hierarchical tree, and form the hierarchy information encoded transaction table T[1], as shown in Table 1.

Computer     Table 1. T[1]  TID Transaction 001 111, 121, 211, 221 002 111, 211, 222, 323 003 112, 122, 221, 421 004 111, 121, 421 005 111,122,211,221,413 006 211, 323, 524, 413 007 323, 524, 713   3. Tactic in mining maximum frequent itemsets  Property 1 If an itemset is infrequent, all it superset must be infrequent.

Property 2 If an itemset is frequent, all it subset must be frequent.

Property 3 If X={x1,x2,?,xm-1,xm} is an infrequent itemset, and Xi={xj|xj X,j i} maybe frequent itemset.

The bottom-up search algorithm only uses the property 1, which discovers the frequent-1 itemsets L1,then discover frequent-2 itemsets by using L1, repetitively until can?t find the frequent itemsets. The   --------    111 112 141 142  Figure 2. The encoded hierarchical tree  12 13  PC  --------  Laptop  IBM HP Legend Haier  Model Mainboard  Figure 1. The hierarchical tree  The 3rd Intetnational Conference on Innovative Computing Information and Control (ICICIC'08)    up-down search algorithm only uses the property 2, which begins from the only itemsets(it includes all items in L1), it reduces some candidate itemsets in every pass. If k-itemsets is infrequent in pass k, then check all (k-1)-itemsets in next pass; If k-itemsets is frequent, all its subsets are frequent itemsets according property 2. Consequently, it can discover all maximum frequent itemsets. The bottom-up search tactic is suitable for the situation when maximum frequent itemsets are short, however, the up-down search tactic is suitable for the situation when maximum frequent itemsets are relatively long.

By combining both search tactic we will be able to make full use of the information gathered in one direction to prune more candidate itemsets during the search in the other direction, namely two-way search tactic: it processes one step of the bottom-up search and multiple steps of the up-down search in every pass, if it discovers a frequent itemset in the processing of the up-down search, then this frequent itemset can prune some itemsets that involves any subsets of the frequent itemset in frequent candidate itemsets which discovered in the processing of the bottom-up search.

The itemset removed from frequent candidate itemsets would not be considered any longer, which must be frequent itemsets according property 2. Similarly, if it discovers one infrequent itemsets in the processing of the bottom-up search, consequently, the algorithm will use it to update MFCS according property 1, the subset of MFCS must not contain this infrequent itemsets.

The ?two-way approaching? method can use sufficiently two characters to accelerate the discovery of maximum frequent set.

4. ML_Pincer algorithm  Input: T[1], a hierarchy information encoded transaction database, and the minimum support threshold(or minsup) which defined by user; Output: all maximum frequent itemsets in MFS which satisfy minsup defined by user.

Begin  For(lv=1;lv MAX_LEVEL;lv++)  // MAX_LEVEL is the maximum level in multiple-level.

k=1, MFS= , MFSlv represents maximum frequent sets in the first level and MFSlv= , L1(L1 is frequent 1- itemsets), infrequent itemsets Sk=1-itemsets L1, the maximum frequent candidate sets MFCSk={i1,i2,?,im| ij L1,1 j m}  while(MFCSk ) for( g MFCSk)  If (SupportDB(g)<minsup)  Generate g(if g is m-itemsets) all (m-1)- itemsets g , MFCSk+1=MFCS k+1 g Else MFSlv=MFSlv g, g Lk, if g g, remove g  from Lk \\ update candidate itemsets generated in processing of bottom-up search  for( g Sk, g MFCSk ) If(g g) remove g  from MFCSk    \\  update MFCSk  Lk generates Ck+1 and C k+1 generates Lk+1 S k+1=S k (C k+1 L k+1) end while MFS=MFS MFSlv Return MFS The following example shows how to figure out  MFS by using ML_Pincer, given a transaction database DB denoted as T[1], let the minimum support threshold be 4 (i.e., minsup=4) and the maximum level be 3 in the hierarchical tree. First, it mines the top- most level?s maximum frequent itemsets, MFCS1= {{{1**},{2**},{4**}}}, though MFCS1 involves only one frequent itemset {{1**},{2**},{4**}}, which support is 0 and less than 4, then geneates 3 itemsets {1**,2**}, {1**,4**},{2**,4**} to replace original itemset {{1**},{2**},{4**}}, so MFCS2= {{1**,2**},{1**,4**},{2**,4**}}, and support of {1**,2**} in MFCS2 equals 4, therefore adds itemset {1**,2**} to MFS1, simultaneously itemset {1**,2**} prunes the frequent itemsets in candidate itemsets which generated in processing of bottom-up search, in other words it removes all itemsets {1**}, {2**}, {1**,2**} from candidate itemsets. At the same time support of itemsets {1**,4**},{2**,4**} in MFCS2 which calculated is all less than 4, then it analyzes the infrequent itemsets {1**,4**},{2**,4**} according method stated above; Besides the up-down search, the infrequent which discovered in the processing of the bottom-up search would update itemsets in MFCS, eventually, MFSlv which is at the top-most level is {{1** 2**},{4**}}. Similarly, we can calculate paralleled and obtain MFS2v at the second level and MFS3v at the third level according the method stated above, the process repeats at even lower concept levels until no maximum frequent itemsets can be found, at last we combined MFSlv, MFS2v, MFS3v into a complete MFS, as a result we carry out discovering maximum frequent itemsets in multiple-level association rules.

5. Performance Study   To study the performance of the proposed algorithm, ML_Pincer algorithm and PMAM  The 3rd Intetnational Conference on Innovative Computing Information and Control (ICICIC'08)    algorithm are implemented and tested on the computer with 512M main memory, Pentium 4-2.4GHz CPU and Windows XP operation system. We employed the test database similar to that described in [5], the transaction database includes 8124 pieces of transactions, it records 23 kinds of attribute of mushroom, we extracted 6000 pieces of transactions to test in sample database at randomized. The transaction database is converted into an corresponding encoded transaction table, according to the information about the generalized items in the item hierarchy table, denoted like T[1]. The maximal level of the concept hierarchy in the items table is set to 3. As shown in Figure 3, it indicates the difference execution time between the ML_Pincer algorithm and the PMAM algorithm under the difference minimum support, the lower the minimum support (i.e., the candidate itemsets is much more), the longer CPU execution time runs, the ML_Pincer algorithm performs to be shorter execution time than the ML_Pincer algorithm in the same support. As shown in Figure 4, it indicates the different number of itemsets which would be counted between the ML_Pincer algorithm and the PMAM algorithm in the processing of algorithm execution, the ML_Pincer algorithm counts less itemsets under the same support.

Figure 3. Execution time    Itemsets Number  minsup(%)   Figure 4. Itemsets number  PMAM  6. Conclusion   We present a new algorithm in mining maximum frequent itemsets based on multiple-level association rules, the ML_Pincer algorithm combines efficiently the bottom-up and the up-down search tactic, furthermore, it prunes and drops dimensions for maximum frequent candidate itemsets by using infrequent itemsets information. It reduces candidate itemsets which would be generated and passes of scanning database, in such case, it decreases I/O time and CPU time greatly. Experiments indicate that the ML_Pincer algorithm is faster and more efficient than other algorithm in mining multiple-level association rules.

