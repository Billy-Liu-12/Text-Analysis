Towards Multi-way Join Evaluating with Indexing Partition Support in Map-Reduce

Abstract?In the era of ?big data?, the emergence and increasing adoptions of the related enabling technologies make it possible for Map-Reduce to accommodate DSS (Decision Support Systems) load, which is commonly targeted for high-performance Data Warehouse analyses in the context of RDBMS. However, the non-predetermined mapping of the Map-Reduce tasks to the physical machines makes it difficult to utilize the pre-partitioned and indexing techniques of DBMS to improve the data locality.

In this paper, towards multi-way join evaluating OLAP (OnLine Analysis Processing) workloads, we introduce table partitioning by reference to Map-Reduce. For avoiding the dispersion of the initial tuples that belong to the same segment keys, we present a detailed description of the data organization model that partitions the dominated tables by cascade reference constraints. In order to push multiple joins on these clustered partitions down to the map task, we design a one-pass multi-way join algorithm along with its optimization implementations for the major Map-Reduce stages. We conduct an empirically study with TPCH benchmark on different scales of clusters, and experimentally verify the high efficiency of the proposed optimization model.

Keywords?Cascade reference, Relational Partition, join index, Map-Reduce, DBMS

I. INTRODUCTION  We are witnessing a proliferation of ?big data? over which the timely and cost-effective analytics is increasingly consid- ered as the key ingredient for success in many businesses, scientific and engineering organizations, and government en- deavors. It is widely accepted that, besides the availability and flexibility of the computing infrastructure, execution of complex queries with high efficiency and high performance is critically desirable for big data analytical applications. In recent years, there has been increasing interest in using Map- Reduce as a fundamental platform for the data-intensive analy- sis tasks, especially when massive computation faces a shared- nothing MPP architecture built upon cheap and commodity hardware. Due to the extreme simplicity, Map-Reduce as well as its open-source implementation Hadoop cater to the application scenarios that require low-cost hardware, fault- tolerance, scalability, and elasticity. However, despite the parallel scheduling, the inherent share-nothing infrastructure of Map-Reduce poses a fundamental barrier to the intra- operator pipeline. This blocking execution strategy introduces a potential bottleneck, especially for the ad-hoc OLAP query that entails combining structural datasets with multiple join operations.

For providing more abstract query facility than using the primitive Hadoop API, the high-level programming languages plus the related optimization techniques were recently pro- posed. These emergence and increasing adoptions of enabling technologies, along with the gradually convergence of rela- tional Data Warehouses and Map-Reduce, make it possible for Map-Reduce to accommodate the OLAP workload that is commonly in the context of DBMS. However, despite the parallel scheduling, the inherent share-nothing infrastructure poses a fundamental barrier to the intra-operator pipeline. It implies that the preceding operators, generally implemented with a series of Map-Reduce jobs, must successively write their outputs to disk for sake of the huge volume. This blocking execution strategy introduces a potential bottleneck, especially for the ad-hoc OLAP query that entails combining structural datasets with multiple join operations. Moreover, the non-predetermined mapping of the Map-Reduce tasks to the physical machines makes it difficult to utilize the pre- partitioned and indexing techniques of DBMS to improve the data locality.

In this paper, by placing the cascade referenced partitions into different groups, we present a one-pass multi-way join strategy founded on the proposed hybrid organization model.

? We detail the data organization model directed by cascade reference constraints, where the dominated join within a Map-Reduce analytical workload accepts the derived cascade partitions as its task inputs.

? We design a one-pass multi-way join algorithm and present the optimization implementations. This prac- tical algorithm targets pruning the cardinality of join input locally, either before assigning the repartitions or while performing the second cascaded join operator.

? For validating the proposed method, we conduct an empirically study with TPCH benchmark on different scales of clusters, and experimentally verify the high locality of the proposed organization model.

The rest of this paper is organized as follows. We separately discuss the related work and our research preliminaries in Section II. In Section III, we present our partitioning strategy and the data organization. The multi-way join algorithms as well as the experimental evaluations are given in Section IV and V respectively. Finally, Section VI summarizes our research.

DOI 10.1109/.50    DOI 10.1109/.50    DOI 10.1109/.50    DOI 10.1109/ICPADS.2013.51

II. RELATED WORKS  On the observation that many data-intensive tasks have the same execution model, Map-Reduce Framework separately generates partial results in each Map task, of which the outputs are further merged by Reduce tasks for the purpose of aggregating. Recently, query optimization issues for enormous datasets, especially when the analyses are on multiple struc- tural tables [1], [8], poses a particular challenge in devising a Warehouse in the shared-noting environments.

For avoiding the redundant IO and communication cost derived from these blocking factors, previous efforts have tried to deploy Map-Reduce (MR) framework to perform the compositional jobs with on-pass processing. Besides a detailed implementation, the original multi-join Map-Reduce optimiza- tion utilizes the hash based one-pass processing method for incrementally performing the reduce function without sort, which are further optimized with a memory resident key selection strategy [14]. Parallel Secondo [2] builds on the optimizing research to bring together ideas from MR and database systems, and presents a shared-nothing multi-database framework. As an essential operation for many data analysis tasks in MR framework, join and its variants appear in a dozen of recent researches. By exploiting additional input statistics for the join attributes, Okcan and Riedewarld study the problem of how to generate the optimal mappings of grouping tuples to the reducers, among which the join loads are balanced with two proposed mapping algorithms [15] . As for multiple join operation, Afrati et al. [3] shares the redundant buckets among the reduce tasks, such that a valid join result can be extracted on at least one reduce node. This replicas based shuffling strategy, particularly in the context of equal- join, provides a novel attempt to execute Chain Joins in one- pass. Both of the theta join and the multi-way join are jointly considered in [17], under a staged cost model for the purpose of optimal join path generation in one-pass MR processing.

Different from work flow adjustment, the data layout researches tend to guarantee data colocation by directly import- ing partitioning-related technologies to a predefined warehouse schema. The current popular Column-wise Storage enhances OLAP efficiency of the traditional DBMS [5], and is brought into the MR framework in [7] to improve the data locality, resulting in a significant speedup in comparison to Hive.

Similar to the file organization of PAX in DBMS, RCFile [10] provides a hybrid HDFS storage implementation that has been published in the recent version of Hive, as well as the relevant optimization techniques [13]. The work of [18] demonstrates the measurement for the different types of partitioning schemes, from the perspective of the repartition operator in SCOPE. Similarly, Cheetah [4] also provides a succinct language to query the virtual view that is defined on top of the star or snow-flake schema, and further integrates the dimensional attributes into the fact table at the cost of denormalization. Under similar consideration for join, Clydes- dale presents a ?full-mirror? strategy for each of the dimension tables. Combined with the columnar storage CIF under the star schema, the research of [12] gives a series of experiments in the SSBM, and show that Clydesdale is on average 38x faster than Hive.

We notice that the advanced query optimization technolo- gies [11], covering DBMS and distributed computing, have  emerged as the potential motivating factor for large scale MR analysis. It is well known that Tablespace of RDBMS, gener- ally binding partitions with the individual disks, provides an efficient approach for either pruning the reference partitioned tables or joining the partition-wise datasets. Coupled with the filesystem?s enhancement techniques [16], DBMS commonly packs the associated tuples into the fixed-length blocks of the relation groups, of which the schemas satisfy referential constraint from one table to another. For simplifying the maintenance of the horizontal partitions correlated with foreign key links, Oracle provides an automated way to cascade partitioning operations performed on parent to children [6].

Meanwhile, as one of the most crucial enhancement methods in DBMS, index researches recently play a central role in filtering over a decentralized environment, especially for the equality and range queries in the P2P context. Nevertheless, the non- predetermined block placement of Cloud paradigm make it difficult to maintain a global index for the dispersed items [7].



III. MULTI-WAY JOIN WITH PARTITION  This section presents our data organization strategy for multi-way join, analyzes the bottleneck of Hive multiple join execution, and highlights the data layout strategy along with referenced partitions.

A. Join Breakdown in Map-Reduce  We firstly give an illustrative example introduced from TPCH Q5 on an 8 nodes cluster, of which the MR platform employs an open source warehouse system Hive running over Hadoop. This query is used to list the revenue volume done through local suppliers, and can be formalized with the fol- lowing join sequence regardless of the other operations, where a combinable star join associates Customer(c custkey, o nationkey) with both Order(o custkey) and Supplier(s nationkey) in bidirectional connection.

(((((( C ?? O  ) ?? L  ) ?? S  ) ?? C  ) ?? N  ) ?? R  ) (1)  By placing the joining relations in different orders, Q5 can be executed with HQL (Hive Query Language), as described as  ((((( L ?? O  ) ?? S  ) ?? C  ) ?? N  ) ?? R  ) (2)  ((((( N ?? R  ) ?? S  ) ?? L  ) ?? O  ) ?? C  ) (3)  For a 20G TPCH datasets, the two HQL transformations result in different processing plans and thus take on variant execution time, as shown in Fig.1. In the two time series revealing different thread numbers and durations, four stages in each of the seven Hadoop jobs (five joins followed by Groupby and Aggregation) are partially interleaved to successively perform each job. It is obvious that plan1 is much slower than plan2, mainly because of the time-consuming jobs  ( L ?? O  ) ?? S  and (( L ?? O  ) ?? S  ) ?? C. In general, we observe an average  80% execution time is consumed by LOC relevant joins, and a further difference of plan1?s shuffle from plan2 indicates that the job execution time dependents on shuffle to a large extent.

As can be seen in job2 of plan1, the shuffle?s tasks decreases due to the improper partition settings, and thus depresses the reduces into no more than two tasks. This leads to very poor reduce efficiency even though the merge workload is quite negligible.

0 100 200 300 400 500 600 700 800         Time (second)  N um  be r  of T  as ks  High?cost join sequence of TPCH Q5  0 100 200 300 400 500 600 700 800        Low?cost join sequence of TPCH Q5  Time (second)  N um  be r  of ta  sk s  Map Shuffle Sort Reduce  Map Shuffle Sort Reduce  Fig. 1: Execution time of TPCH Q5 in two execution plans.

Due to the limited network bandwidth, processing sequen- tial MR jobs relies on the operator?s selectivity. In plan1, the intermediate datasets transferred from the first join to the third is significant larger than that of plan2, which can incur much more shuffle load as well as the sort-merge computational complexity. Due to the fact that reduce-side sort and merge can be performed only when shuffle is completed, this brings up quite longer wait time while depressing the degree of merge parallelism. On the contrary, the two joins  ( N ?? R  ) ?? S in  plan2 reduces the subsequent workload by 8%, as comparison to plan1. As for the main workloads of the LOC join, this reduction significantly decreases the execution time, of which the job takes one of LOC as its join participant.

Another main efficiency factor regards the computational complex derived from the LOC?s large volume. The 80% execution time of each plan, as observed above, stems from employing sort-merge to join each of the three tables with their counterparts. This cost, however, can be dramatically reduced by introducing the indexing join and pre-partition based colo- cation techniques. As distributed index significantly depends on the dispatching coverage, a local index is more suitable for this cascade reference join. However, more complex problems may sometimes arises in the case that two or more referenced groups are connected by the same fact table. This organization model, also called star-schema, is extremely common in the traditional relational database paradigm. Although the size of the tables can be significantly reduced, it is far from optimal due to the costly multiple cascade join operations induced by foreign key references, particularly in the share-nothing storage environment.

B. Partitioning by Reference  Based on the referential constraint along with a cascade reference path, the partitioning by reference method divides a series of tables into a set of non-overlapping piece groups.

In each group, the child piece can be seen as a cascaded horizontal partition that is induced by the referenced partition keys of each generation in the reference path. In MR Hadoop  schedule model, this mapping strategy is quite preferable for reducing communication among either tasks or jobs, especially with a pertinent colocation design. However, since there exists multiple reference paths in the schema (i.e., the TPCH fact Lineitem has at least three referenced table), it is crucial to choose one path to produce cascade partitions.

Except the small dimension tables, Nation, Region and Supplier of TPCH, there are two possible choices to partition the cascade tables, i.e. L?PS?P (Lineitem? PartSupp? Part) and LOC(Lineitem ? Order ? Customer). Since referenced tables of the both paths have the same cardinali- ties, it is difficult to determine which of the two choices is more appropriate to conduct the cascade partitions. In this circumstance, from the perspective of total query throughput, the frequency of addressing referenced tables can be used for establishing the cascade reference path. In TPCH benchmark, 60% of the queries refer to the LOC (compared with 45% of L?PS?P ). In addition to the more attributes, the base tables Orders and Customer are more preferable to partition the fact Lineitem, as shown in the following cascade mapping.

In Fig. 2, two one-to-many mappings in turn are devoted to divide the three tables into a set of separate partition groups.

Within each group, any tuple of Orders and Lineitem can be retrospect to the same Customer partition. It is natural that  Fig. 2: Partition groups cascaded as COL.

all the foreign keys of Orders in PG1, with the same hash value i1 (h(o ck) = i1) as that of Customer (h(c ck) = i1), apply their induced primary partition to the same hash function H(o ok) = i11, i12, ..., i1n, and hence partition Lineitem on its foreign key l ok. This staged strategy guarantees that the fact table can be evenly partitioned if the hash functions are selected equally likely. It means that, we can achieve load balance among the local cascade joins, by packing each group onto one data block for the performance of one-pass multi-way join.

C. Data Placement for Multi-way Join  As for data organization in the distributed computing framework Hadoop, robustness and aggregation are the two most important factors regarding the efficient performance of job sequence. Based on HDFS, we propose a hybrid organiza- tion model for the join involving base tables, and emphasize the data placement and split implementation for the above partition groups without changing the Hadoop framework at all. A one-pass multi-way execution strategy is finally given to join dimensional table, with respect to the join selectivity of the non-fact tables, to the cascade partitions. This one- pass strategy focuses on tightening cascade join as well as the filter of small dimensions in the Map tasks of a single job, by introducing both the indexing join and the broadcast join.

1) Hybrid organization mode: In terms of the cardinalities, the proposed data organization model differentiates the based tables into three categories, i.e., Cascade tables, Attached tables, and Small dimensions. For avoiding high frequent cascade joins and shuffling massive intermediated results, fact table together with their major referenced tables (also called Cascade tables) are cascade partitioned and grouped into a set of Huge blocks. On the contrary, the full copies of Small dimensions are broadcasted onto all the datanodes, such that each Huge block handled by one Map can join all the Small dimensions in one-pass. Finally, as the same referenced target as Cascade tables, Attached tables are treated as normal HDFS blocks due to their subordinate performance impact.

Meanwhile, for enhancing the join selectivity, each Attached table can be filtered in a MR job and hence treated as same as Small dimensions in a pre-join phase, with which the Map task build a hash table for the dominated join operation. As shown in Fig.3, all three categories of data are either dispersed over  ????  ?  ?  ?       ??  ?    ??  ?  ?  ?  ?????????? ?????????? ?????????? ??????????  ??????????????  ????????????????  ????!????????? ? ? ? ? ? ? 	 	 	 ? ? ? ? ? ? 	 	 	 ? ? ? ? ? ?  "?#?????  $???%?&??? ?????'????(  $???%?&??? ?????'????(  )(? *?????!(???  )??? *?????!(???  ++(?+??? ,(?-#??.

??(?  ++(?+??? ,(?-#??.

??(?  ++(?+??? ,(?-#??.

??(?  ++(?+??? ,(?-#??.

??(?  ++(?+??? ,(?-#??.

??(?  ++(?+??? ,(?-#??.

??(?  ++(?+??? ,(?-#??.

??(?  /000  1?##?(  "??-??(  $???%?&??? ?????'????(  $???%?&??? ?????'????(  )(? *?????!(???  )??? *?????!(???  $???%?&??? ?????'????(  $???%?&??? ?????'????(  )(? *?????!(???  )??? *?????!(???  $???%?&??? ?????'????(  $???%?&??? ?????'????(  )(? *?????!(???  )??? *?????!(???  Fig. 3: Dominated join based on hybrid layout.

the HDFS or placed on each local disk. It should be pointed out that Hadoop provides a distributed cache mechanism to broadcast data onto each datanode. In this setting, Map tasks may join the cascade partitions with the hashed Small dimensions during the index based dominated join. Moreover, for efficiently joining the Cascade tables, the partition groups as well as the join index are separately packed into the Huge block (the rectangle container labeled with Cascade tables in Fig.3), of which the block size is remarkable larger than normal blocks. As opposed to these two special layouts, Attached tables are distributed over the datanodes in HDFS, following the block schedule protocols of Hadoop.

2) Huge block for indexing partition: Recall from Subsec- tion III.A, to extract the corresponding items among Cascade tables, most of the time is spent on running the blocked job sequence, and thus degree of parallelism is limited by the bottleneck derived from the massive shuffle workloads.

By adopting grouped cascade partitions storage, Huge block strives to support one-pass multi-way join with a flexible Split design, while keeping the scan operation efficient. Consider the cascade LOC in TPCH, Fig. 4 illustrates Huge block layout with both the global and local views. By placing the non- overlapping groups onto each physical block with a linked block structure, Huge blocks locally adopt both the data and join indexes of LOC, affording cascade join without referenc- ing each other. As shown in the first Huge block of Fig.4, the header (Meta data with the red item) is mainly composed of five entries that in turn keeps the pointer lists of the cascade tables (Lineitem, Orders and Customer) and the two join index (mapping of o ok to c ck, and vice versa). This implies that cascade LOC join can be locally accomplished by a flexible one-pass three-way join, in which random correlation between Orders and Customer is exploited with the OC(CO) join index, and thus supports the pipelined hash join with Lineitem (on the condition o ok = l ok) during the relevant scan and filter.

In Fig.4, the actual storage of Huge block is divided into a set of equal-size (data) blocks, of which the pointers are maintained in the five secondary lists (following the Meta data in Fig. 4). This cascade two-level storage model, hence, ex- hibits an autonomous yet uniform organization perspective for both the base tables and the correlative indexes. Different from the conventional iterative bucket join and multi-phase blocked joins, this indexing partition organization grants cascade join the divide-and-conquer ability, especially when the cascaded many-to-one mappings exist in the dominated join relations.

?????( $???%?? 2?????  ?-+??????? ?-+??????? ?-+??????? ?-+???????  1???????? ???????? 3(??(?  ?-?????( 3??*????????% ?3?*????????%  Fig. 4: Illustration of Huge block for the LOC tables of TPCH.

Local join index. For accelerating the inner-partition join, Huge block builds the local join indexes to optimize the costly reference joins. It is very suitable for joining Cascade tables with Attached tables or Small dimensions, on which the high- selectivity operations are generally used to filter out most of the irrelevant data. This join pattern is very common in TPCH queries, and hence implies that the related query can treat the Cascade tables as a whole. In this context, the derived high- selectivity cascade join can be efficiently achieved within each Huge block, by choosing a proper join sequence to carry out the pipeline join with the join indexes. Also notice that the two join indexes can be built during the loading procedure of Orders, as if the whole primary key of Customer?s partition was kept in the memory.

Block size determination. From the above discussion, it is obvious that query efficiency in Huge block model dedicatedly depends on the size selection of both Huge block or the linked block. The pre-join of each Map in Fig.3, just before each of the dominated cascade join, is used to extract the global intermediate results filtered from Small dimensions or Attached tables. Small Huge block can make too much pre-joins, and thus increases the redundant computation. On the other hand, if we choose an excessively large size, the memory requirements of both the join indexes and intermediate results cannot be satisfied. For each compute node with m parallel Map tasks, suppose the memory size is M , the largest cardinality of the r referenced tables is constrained with  n = ( M ? ca  ??Ra ??+ cs  ??Rs ??)/(2mr), (4)  and thus Huge block size can be calculated by  Bs = nfF |PF |+ ?r  i=1 (nfi |Pi|) + 2nr. (5)  Where ca ??Ra  ?? and cs ??Rs  ?? denote the size of intermediate results that are filtered apart from Attached tables and Small dimensions. As well as the relevant table width |P?|, the cardinality ratios of all the cascade referenced tables (fi) and the fact (fF ) to the largest referenced one (fi = 1), ultimately result in the physical block layout. Conversely, for a given size selection Bs and the schema structures, the available memory of the join indexes are mainly depended on the largest of the i referenced tables.

Scan efficiency consideration. In this Huge block model, partition scan is another essential factor regarding the operation efficiency. Since relational queries usually operate the attribute sets of Cascade tables, it is important to offer optimization to the physical blocks. As shown in Fig.4, a array of points are kept in the meta lists, so that operators can access the each partition individually. For providing high space utilization as well as fine convergence, the physical block size should refer to the smallest Cascade partition, and can be determined with respect to Huge block size as  bs = ?nfr |Pr|? (6) Although the smallest partitions are different among Huge blocks, this difference is often negligible due to the stable many-to-one mapping of cascade references as well as the large block size. In our LOC partitioning experiments, among all the Huge blocks, the size difference of the Customer partitions is less than 1%.

3) One-pass multi-way join strategy: In general, all the intermediate results and the base tables with extremely low cardinality are treated as the hash join counterparts, then we can discover a transformation of the related multiple join operations to a pipelined multi-way join. However, this one- pass join strategy need to be combined with others MR jobs, of which the large cardinalities led to significantly low join selectivity and thus breaks the memory requirement of pipelined Cascade join. In this context, we consist that the optimal plan is a sequence of MR jobs, of which the dominated join can be treated as a pipelined join of Cascade tables to the results of a series of broadcast join.

Broadcast join selection model. In order to grant autonomy to the Huge block model, broadcast join exists in either Small dimensions or the high-selectivity Attached tables. It is means that each Map, running over a Huge block, must access the global image of the filtered intermediate results. For example, in this model, Q5 can be transformed into the following union form of the multi-way join.

? i=1,...,P  ( R ?? N ?? S  ) ?? LOCi (7)  Where LOCi denotes the cascade join running on each par- tition group, and correctness of the transformation would be guaranteed due to the fact that each local group LOCi can join the global image of R ?? N ?? S. This condition can be further extended to the Attached tables, by choosing the high-selectivity results obtained from a relatively short-running MR job, and broadcasting them onto the whole datanodes.

For supporting the memory resident pipelined join, as ex- pressed in equation (4), the total size of these global images ca  ??Ra ??+ cs  ??Rs ?? may not be too large to be cached together  with join indexes. An pertinent arrangement is to shrink the join indexes by appropriately decreasing Bs, and determine the broadcasting join targets in terms of the residual memory.

Join sequence optimization. In our research, the basic principle of join sequence optimization is to take the smallest correlative result as the initial input, with one of Cascade tables joining on it, and thus apply pipeline join to the other hashed intermediate results. Before this procedure, a partitioning scan within each Huge block is firstly triggered, once attributes of the corresponding Cascade table acts as a relational operand. Based on equation (7), the global process- ing of TPCH Q5 can be transformed to a set of local pipelined join executions, of which the query flow is compressed by cut off the MR join jobs for the cascade reference tables. In this query, each LOC partition in Huge blocks is associated with either a external cascade join or a straightforward selection, which results in different input cardinalities for the cascade join. In this context, it is natural that changing join order can result in very different execution cost.

Whatever we choose the join plan, it is indispensable to scan all the three partitions, and thus the execution cost mainly depends on the join complexity. Suppose each Huge block is 750M, then the input cardinality of the cascade join can be computed. Therefore, it is better to firstly hash both Customer and Orders, then scan Lineitem and pipelined join it with the hashed intermediate results S,O,C in the order of L ?? S ?? O ?? C. In contrast, if we alter the plan as O ?? C ?? S ?? L or O ?? C ??  ( L ?? S  ) , the enormous filtered results from     Lineitem can be hardly hashed in memory. As for the join indexes, it should be mentioned that the scan of Customer or Orders can be eliminated at the cost of a certain volume of memory, as long as there is no filter operation apply to these tables, such as Q7, Q9 and Q10.



IV. MULTI-WAY JOIN ALGORITHM  By separating the Cascade tables involved relational oper- ators from the step-by-step MR job sequence, the proposed algorithm emphasizes multi-way processing the dominated join in one-pass. Other than the traditional repartitioning join strategy, this join algorithm pushes the filter operations of Cascade tables down to the Hadoop Split, by providing the optimization implementations for the three main stages. Notice that the Reducer implementation in Section IV.A, derived from a MR job operating on the Attached tables, is mainly to broadcast the small results onto each node. By employing the above Huge block model, Section IV.B and IV.C give the core procedures of filter and pipeline join respectively. For simplicity, we provide the core processing flow in the following discussions, and do not differentiate between the algorithms and interface implementations.

A. Broadcast of Small Attached results  For providing a full image of Small Attached table that is filtered by the high-selectivity operator(s), broadcast is used to preprocess the outputs of the previous MR jobs, and make each Mapper of the cascade join job access the intermediate results. In this context, DistributedCache in Hadoop gives us an applicative approach.

It should be emphasized that the selectivity of these jobs must be significant high, due to the fact that broadcasting the results can run out the precious network bandwidth, and rapidly slow down the execution of cascade join tasks. The cardinality of the largest Small dimension involved in a query can be seen as a referential parameter. Still, it is nuisance if the huge Attached tables join with Cascade tables after the formed has been joined with others. Suppose this cascade join has a quite low-selectivity, the large shuffle volume can block the job sequence, no matter how large the selectivity of the attached join is. Reducer of the pre-join jobs, as discussed in Section III.C, filters the Attached tables and passes the results to the cascade join. For shrinking job sequence and decreases the jobs dominated join, each pre-join?s Reducer uploads their output onto the DistributedCache as same as Small dimensions.

B. Inner-split Filter  In Hadoop, InputFormat is a basic data organization inter- face, and is generally extended by the partitioning research that targets the fine-gained block layout as well as optimal access efficiency. The standard interfaces of InputFormat, couple with OuputFormat, are the actual I/O stream that have to shoulder the entire burden of transmitting the intermediate results among the task sequence. By binding Huge table with the split, Procedure 1 gives the configure input part for the implementation version HugeBlockInputformat of Inputformat.

The filter condition in Section III.C is expressed as the tetrad ?tblID, colID, predicate,Obj?, where colID denotes the se- lection column of Cascade table with the table ID number  tblID. Line 3-4 separately append the filter condition and the projection column sets to the parameter lists, and help eliminate the information on both the attribute columns and the selection row.

Procedure 1 HugeBlockInputformat Configuration 1) List?PredictCnd? pc = new List?PredictCnd?(); 2) List?SelectCols? sc = new List?SelectCols?(); 3) pc.add(new PredictCnd(tblID, colID, predicate, Obj)); 4) sc.add(new SelectCols(tblID, colID)); 5) ConfigUtils.setPredict(conf, pc); 6) ConfigUtils.setColumn(conf, sc);  On the other hand, each output tuple with the arranged attributes is emitted by the RecordReader class, with the dedicated design for its next() method.

Procedure 2 HugeBlockInputFormat.RecordReader.next() Input: Configurature conf, FileSplit split  1) List?PredictCnd? pc=ConfigUtils.getPredict(conf); 2) List?SelectCols? sc = ConfigUtils.getColumn(conf); 3) init indexes and destTbls of Cascade partitions; 4) foreach destTbl in non-fact tables do 5) if (destTbl in pc.tblID) or (destTbl in sc.tblID) 6) foreach tuple in destTbl do 7) temp ? attributes that is not in sc.tblID 8) if (temp satisfies IN of pc.predicate) 9) emit(destTbl.refKey, temp hash join pc.Obj)  10) else if (temp satisfies Range of pc.predicate) 11) emit(destTbl.refKey, temp) 12) done 13) else if (destTbl is not fact) 14) emit(join index of destTbl.primarykey) 15) done 16) foreach tuple in the fact table do 17) emit(tuple.fgnkey, tuple after attr and predict filter) 18) done  As shown in Procedure 2, the method in turn emits each tuple of the Cascade tables from parent to child, until all the satisfied ?key, value? pairs of the fact table are extracted out and passed to the Mapper consequently. Recall that each Cascade table is organized with a series of physical blocks, of which the pointers are kept in the metadata lists (Line 4 omits this step for simplicity). Firstly, Procedure 3 scans all the non- fact tables, if each of them is filtered by a Range selection predicate (line 10), or joins with a intermediate results on the reference foreign key (line 8). If neither filter nor join is associated with a non-fact table, the ?key, value? pairs implied by the persistent join index are thus emitted to the Mapper (line 14). Once all these non-fact tables are completely processed, each tuple of the fact table is emitted similarly with that of the referenced table. Therefore, Mapper can perform pipelined join on account of the ordering on the input tuples. This one- by-one emission strategy makes it possible to firstly build the hash tables, and carry out pipelined join during scanning the fact table.

C. Map-side Pipelined Join  Under the assumption that Huge block size is determined by equation (5), the pipelined join can be easy to be realized in corresponding with the above Map inputs. Procedure 3     Procedure 3 Pipelined join in Mapper Input: join indexes or ?key, value? from parent to child  1) foreach pair of the referenced tables do 2) build hash tables for the ?key, value? 3) done 4) keep join indexes in memory 5) firstidex ? Smallest hash or index with fact join key 6) foreach tuple in the fact table do 7) foreach hash or indexes do 8) join with the key of the hash or index 9) tuple ? the attributes of the join result  10) done 11) output(tuple.gourpbyAttributes, tuple) 12) done  proposes the core pipelined join in Mapper, of which the hash tables or the join indexes is firstly built to support the stream based pipelined join (as shown in line 5).

In the above discussion, we omit other standard implemen- tations due to the fact that the dominated workloads mainly concentrate on Mapper, so that Reducer as well as the relevant customizable interfaces can be directly treated as a groupby operation followed by aggregation and projection.



V. EXPERIMENTAL EVALUATION  We use the TPCH benchmark to evaluate our research and focus on three main aspects. In this setting, two representative experiments entail verifying the scalability as well as the multiple join efficiency of our method, at different scales of both the data volume and the node number. We compare the efficiency experimental results with that of Hive, and measure the scalability factor by altering the Huge block size at different cluster scales.

A. Experiment settings  Cluster Setup. The experiments were evaluated on a 34- node HP ProLiant DL145 G2. Each server consisted of 2 dual- core AMD Opteron 64bits 1.8GHz processors, 4GB RAM, 32GB SCSI disks, and interconnected with 1GB Ethernet.

The OS is SUSE Linux (Kernel version 2.6.32.19), JRE 1.7, Hadoop 1.0.3. All experiments were repeated at least 3 times, and we report the average measurements for the experiments.

Experimental Workload. As discussed in 4.3, for veri- fying the speed-up of the pipelined join strategy established on Huge block, we choose the LOC involved TPCH queries (Q3, Q5, Q7, Q8, Q9, Q10) as our test cases. Among them, Q9 and Q8 also reference the Attached tables Part or PartSupp, and will obtain more attention in the following experiments.

