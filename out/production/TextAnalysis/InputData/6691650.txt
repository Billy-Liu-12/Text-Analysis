Accelerating Semantic Graph Databases on Commodity Clusters

Abstract?We are developing a full software system for accelerating semantic graph databases on commodity cluster that scales to hundreds of nodes while maintaining constant query throughput. Our framework comprises a SPARQL to C++ compiler, a library of parallel graph methods and a custom multithreaded runtime layer, which provides a Par- titioned Global Address Space (PGAS) programming model with fork/join parallelism and automatic load balancing over a commodity clusters. We present preliminary results for the compiler and for the runtime.



I. INTRODUCTION  Data coming from large networks such as the World  Wide Web, geographical systems, transportation, telephones,  social and biological networks have largely exceeded the  petabytes, and are exponentially growing in size [1]. Graph  databases appear well suited to organize the data of these  emerging fields. Graph databases exploit graph structures,  with nodes, edges and properties to represent and store  data. They provide index-free adjacency, meaning that every  element contains a direct pointer to its adjacent element.

Thus, no index lookups are necessary. Data manipulation  is expressed by graph-oriented operations and type con-  structors. Graph databases naturally map to the new emerg-  ing network-like datasets, which expose a large number  of interconnections among their elements. In contrast to  relational databases, they can manage ad hoc and changing  data with evolving schemas, they can represent and explore  more easily the relationships among the data, and they can  naturally scale to large datasets, because they do not require  space and time expensive join operations.

Semantic graph databases are specific instances of graph  databases, usually built by exploiting the Resource Descrip-  tion Framework (RDF). RDF is based upon the idea to make  statements about resources in the form of ?triples?, struc-  tured as subject-predicate-object expression. These state-  ments naturally represents a labeled, directed multigraph.

Languages such as SPARQL can perform queries on graph  databases through graph matching operations. There are sev-  eral commercial and open source semantic graph databases  engines. They include purpose-built databases for the storage  and retrieval of triples (triplestores) [2], triplestores mapped  on top of relational databases engines (SQL-based) [3], triple  stores that try to leverage Map-Reduce infrastructures [4],  or even custom systems such as the Cray?s Urika [5], a  multinode system with the multithreaded Cray ThreadStorm  2 processors, running custom graph search software as a  backend to the Apache JENA Java framework. However,  even purpose-built databases generally convert the queries  to intermediate representations in relational algebra, thus  resorting to relational join and select operation. With large  datasets these operations can, in turn, significantly increase  space and time complexity with respect to graph pattern  matching operations, because their execution requires inter-  mediate data structures (tables). Furthermore, the majority of  triplestores is also based on Java, which may add unneces-  sary overheads, does not guarantee the level of performance  required in High Performance Computing environments and  constraint the scalability in terms of dataset sizes.

Graphs have large amounts of parallelism: a system can  potentially spawn a thread for each vertex or edge. High  performance clusters, which provides unprecedented levels  of parallelism through multicore processors, may thus appear  a suitable platform for these workloads. However, graphs  usually are described through inherently irregular pointer- or  linked list-based data structures. To obtain high performance,  these data structures must be traversed in memory, but they  generate fine-grained (often single word) and unpredictable  accesses to memory and network. This makes ineffective  the complex cache hierarchies of modern processors, which  rely on temporal and spatial locality to reduce the memory  access latencies, and the high performance networks of the  latest clusters, which are optimized for large, batched data  transfers.

In this work-in-progress paper we introduce the full  software system for accelerating semantic graph databases  on commodity cluster that we currently are developing.

At the top of the stack, a compiler converts SPARQL  queries to parallel graph matching operations expressed in  C++ code. The C++ code invokes a custom graph database  engine, which provides the data storage and the application  programming interfaces to explore the graph with common  graph traversal operators. The parallel graph matching op-      Figure 1. The architecture of our software system for semantic graph databases  erations and the graph database engine build upon a custom  runtime. Graph exploration primitives involve large amounts  of parallelism, usually expressed in well defined nested loops  on vertices or on edges. The custom runtime provides a  shared memory abstraction on top of a distributed memory  cluster, a fork/join control model, lightweight software mul-  tithreading, dynamic load balancing and data aggregation to  enhance the performance of graph crawling algorithms. We  provide some preliminary results of the software system,  separately showing: the performance of the generated query  code from the SP2Bench [6] on a 48-core shared memory  multiprocessor (SMP) system, compared to a commercial  database such as Virtuoso, and the strong and weak scaling  on up to 128 cluster nodes of our custom runtime with a  typical graph exploration kernel, breadth-first search. We are  currently porting the graph database engine and the graph  exploration primitives to the custom runtime, therefore we  validate the compiler on a SMP with an OpenMP version  of the libraries.



II. SOFTWARE SYSTEM  Figure 1 shows an overview of the architecture, our  software system for semantic graph databases. As in classic  systems, an analyst performs a query on the database to  obtain a result, which may be a single value, a set of values  or a set of tuples. The query are expressed through SPARQL.

A compiler converts the SPARQL query in parallel graph  pattern matching operations, expressed in C++ code. The  Semantic Graph Library (SGLib) manages the execution of  the query and the graph database itself. SGLib generates the  graph database by ingesting RDF expressions stored in N3  format. The ingestion process generates the graph database  itself and the related dictionary, which may be dynamically  updated. Our system stores the dictionary by exploiting a  parallel hashmap, which maps strings to integers. Our engine  uses this dictionary data structure and uses the same parallel  hashmap technique to provide parallel set and multi set  data structures for storing intermediate and final results. The  database engine exposes the C++ parallel graph exploration  API that the compiler can exploit to perform the queries. The  parallel hasher, set, multiset and graph API are mapped on  GMT, a custom multithreaded runtime that enables scaling  of irregular applications (such as graph exploration) on  multi-node commodity clusters.

A. Compiler  Our database system supports an interface that allows  writing SPARQL queries, and saves them in XML format.

The XML representation allows our code generator to be  independent from the original query language, facilitating  extension to other languages. The XML representation better  exposes the query structure, easing the construction of the  Internal Representation (IR) adopted in the framework. The  SPARQL compiler then parses the XML representation and  builds an intermediate representation (IR) which semanti-  cally maps the original query to a graph pattern. In the  simplest case, the representation obtained is a Basic Graph  Pattern or a Basic Group Pattern. These simple patterns can  be matched directly through standard structural approaches,  such as Ullmann?s algorithm [7]. However, SPARQL offers  additional features, such as optional matching directives  and filters that complicate the definition and the generation  of graph patterns. Our compiler supports these features  by partitioning the structural patterns in mandatory and  optional components, depending on their nesting level in  the SPARQL description. The partitioning induces a hier-  archy, which is then exploited for the code generation: for  example, anticipating mandatory components allows pruning  infeasible mappings of optional components.

The IR generation stage translates the query in one or  more graph patterns to be matched. The compiler matches  each pattern through a distinct search strategy. Filtering  constraints, if any, are evaluated exploiting the results of  the previous search. If the query is composed of multiple  disjoint patterns, the compiler defines a priority order among  them. A simple heuristic establishes the priority ordering.

It assigns higher priority to patterns having fewer target  nodes, i.e. nodes included in the result tuples. This order  may reduce the complexity of intermediate results needed to  perform implicit joins, since target variables must be stored  in addition to nodes involved in constraints. The generation  process explores each pattern graph, incrementally building  the output code. In the presence of optionals, nodes are  traversed according to their hierarchy level.

The automatically generated pattern matching routine  implement a breadth-first search (BFS) over the data graph,  mapping variables to data (vertices or edges). The generated  code invokes the graph exploration API. The APIs includes  look up operations, to check labels of edges and vertices, and  parallel constructs that load neighbor lists and explore their  vertices or edges, or that explore in edges or out edges of a  vertex. They are equivalent to for loops that execute on the     edges or on the vertices, but they allow abstracting the entire  procedure as a search executed through graph exploration  primitives. The compiler generates the tasks that the parallel  constructs execute for each vertex or edge, and the data that  are passed to the tasks. Tasks can contain nested parallel  constructs. The use of a graph API also allows hiding the  underlying multithreaded runtime: we only need to adapt the  graph API, and not the compiler, to fit other architectures  with different parallel programming models.

B. SGLib and GMT  SGLib, the graph exploration API, and the data structures  are mapped on top of our custom Global Memory and  Threading (GMT) runtime. The parallel tasks that perform  the search present fine-grained parallelism, and potentially  run on millions of vertices or edges. On the other hand, the  graph database structure may be very complex, extremely  large and difficult to partition. GMT provides a global ad-  dress space across multiple cluster nodes, so that there is no  need for SGLib to partition the data structures. This allows  exploiting memories of multiple cluster nodes to manage,  in-memory, very large graph databases. However, besides  enabling the management of larger databases, we designed  our system to also scale performance as new nodes are  added. Our approach is in opposition to solutions that exploit  a single node but integrate fast mass storage devices, such as  Solid State Disks, to swap data to and from the RAM. The  graph exploration primitives and SGLib employ put and get  operations which allow writing to and reading from the glob-  ally shared memory. Because graph databases may be very  complex, and often describe a large number of relationships,  accesses to their elements are mostly unpredictable. Thus, on  a distributed memory system employing a shared memory  abstraction, a large number of data requests may access the  network. In such a case, maximizing performance means  maximizing network bandwidth utilization. However, data  accesses to graph data structures usually are fine-grained.

GMT allows tolerating the latency for fine grained data  accesses through multithreading. Through GMT, each node  hosts thousands of lightweight tasks that allow tolerating  latencies to access data residing on other network nodes.

GMT also supports data aggregation: fine grained memory  operations directed towards the same node are aggregated  together in a single network packet, reducing overheads and  optimizing network utilization. SGLib and the graph API  exploit GMT?s parallel for constructs to extract parallelism.

These constructs provide a simple fork/join control model:  GMT can spawn a lightweight task for every loop iteration,  all over the cluster, even for nested loops. This is in  opposition to typical message passing programming models  for distributed memory clusters, which typically use a Single  Program Multiple Data (SPMD) control model.

Figure 2 presents an overview of GMT?s architecture.

SGLib and the Graph exploration primitives exploit GMT  Figure 2. Architecture of the GMT runtime  by using its low level Application Programming Interface  (API). GMT?s API includes global array allocation and  freeing primitives, non-blocking (with the associated wait)  and blocking put and get commands to save and retrieve  data from the shared arrays, atomic addition and compare-  and-swap operation, and, finally, a parallel for construct  (gmt parFor), which allows generating tasks from loop  iterations. Each node in the cluster executes an instance of  GMT, which includes three different types of specialized  threads, as follows:  ? Workers: execute the application code, partitioned in  tasks. There are multiple workers per node.

? Helpers: manage global address space and synchroniza-  tion. There are multiple helpers per node.

? Communication server: network entry point, manages  incoming and outgoing communication at the node  level with MPI. GMT uses one communication server  per node.

GMT instances communicate through commands. The  commands include data accesses, synchronization and thread  management operations. They may also have data attached.

Workers only generate commands, while helpers receive  and generate commands.The communication server transfers  commands from one node to the other through MPI. When  a task executes a data access or a synchronization operation  on a remote memory location, the worker generates the cor-  responding command and switches its execution context to  another task in its own private queue. This allows tolerating     Table I Comparison with Virtuoso and related speedups  1M 5M 10M  Query VIRT 1T 48T MSU VIRT 1T 48T MSU VIRT 1T 48T MSU  Q1 0.446 0.0035 0.00023 1866.1 9.496 0.0115 0.00049 19183.8 15.583 0.0184 0.0008 17707.9  Q2 4.02 0.689 0.101 39.746 27.316 5.127 0.324 84.279 61.35 13.134 0.725 84.551  Q3a 0.806 0.115 0.023 33.932 2.834 0.450 0.0903 31.367 4.75 0.966 0.153 30.871  Q3b 0.007 0.094 0.0041 1.687 0.023 0.417 0.0163 1.407 0.05 0.824 0.02915 1.715  Q3c 0.009 0.092 0.0032 2.747 0.022 0.411 0.0123 1.788 0.029 0.814 0.0220 1.313  Q4 117.23 15.495 1.231 95.227 838.063 100.827 8.174 102.519 1787.212 287.545 22.650 78.904  Q5a 2.622 0.492 0.0801 32.717 21.024 1.785 0.334 62.810 41.95 4.526 0.646 64.890  Q5b 3.909 0.835 0.0610 63.982 17.803 3.876 0.244 72.668 47.686 10.704 0.624 76.310  Q6 12.744 0.869 0.182 69.871 69.112 3.847 0.746 92.623 130.1 10.636 1.727 75.326  Q7 1.09 0.816 0.162 6.723 9.07 2.5011 0.322 28.147 23.239 5.396 0.535 43.401  Q8 0.114 1.914 0.0649 1.756 0.109 12.407 0.421 0.258 0.112 61.284 1.531 0.0731  Q9 1.303 3.318 0.722 1.803 7.04 19.850 1.595 4.413 13.523 82.232 3.723 3.631  Q10 0.005 0.00016 0.00008 60.975 0.006 0.00017 0.00008 74.074 0.007 0.00043 0.00012 54.687  Total 144.305 24.736 2.636 54.726 1001.918 151.515 12.283 81.567 2125.591 478.084 32.371 65.662  network latency. To reduce overheads for fine-grained net-  work transactions, GMT aggregates the commands directed  towards a node before sending them out. GMT supports two  levels of aggregation. The first one happens at the level  of the specialized threads. Before sending out commands,  workers and helpers accumulate them in different arrays  depending on their destination node. When an array is full  in terms of number of commands or in byte equivalent size  (commands may have data attached), or after a predefined  time interval (to guarantee a higher bound for the maximum  latency of a memory operation), the thread moves the data  in the shared aggregation queues for the whole node. There  are shared aggregation queues for each one of the possible  destination nodes. When one of the specialized threads finds  that the aggregation queue it is writing to is full (because  of the number of commands, or the equivalent byte size),  or the predefined time interval for aggregation has passed,  it prepares the data and copies them in one of the buffers  that the communication server uses to send out data.



III. PRELIMINARY RESULTS  This section presents, separately, preliminary results for  the compiler and for the custom runtime.

A. Compiler  We evaluated the compiler with the SP2Bench SPARQL  performance benchmark [6], which includes both a data  generator and a set of benchmark queries. We translated  each SP2B query into data parallel C code, exploiting SGLib  and the graph exploration API. To validate the compiler,  we use a version of SGLib and of the graph exploration  API that exploits OpenMP pragmas instead of GMT, and  runs the queries on a single node shared memory system.

We execute the queries on a quad-socket system with four  AMD Opteron 6176SE (?Magny Cours?) processors. Each  processor includes 12 cores (for a total of 48 cores), 12 MB  of cache L3 and runs at 2.3 GHz. The system has 256 GB of  DDR3 memory at 1333 MHz. We conducted the experiments  generating RDF data sets of 1, 5 and 10 millions tuples.

Table I shows the execution times of the queries for  sequential execution (columns 1T - 1 Thread) and for  parallel execution (48T - 48 Threads), and compares them  to OpenLink Virtuoso [8] (column VIRT). We also report  the Maximum Speed-up (MSU), corresponding to the 48T  runs. Query Q1, which runs more than 17000 times faster  for both the 5M and 10M data sets, provides the highest  speed up. This highlights that using graph pattern matching  operations, rather than relational algebra and tables, reduces  the complexity of intermediate results and has a huge impact  on the execution time. In only one case (query Q8) Virtuoso  provides an answer faster than the generated code. The  reason is the presence of a filter constraint, composed of  four different expressions that can be separately evaluated.

B. GMT  The graph pattern matching code generated by our com-  piler substantially performs graph exploration, with only  the addition of vertex and edge lookups. For this reason,  we evaluate our multithreaded runtime by executing a full  breadth first exploration on a cluster. We used Pacific  Northwest National Laboratory?s Olympus supercomputer,  listed in the TOP500 [9]. Olympus is a cluster of 604  nodes interconnected through a QDR Infiniband switch with  648 ports (4GB/s). Each Olympus? node features two AMD  Opteron 6272 processors (codename ?Interlagos?) at 2.1 Ghz  and 64 GB of DDR3 memory clocked at 1600 Mhz. Each  socket hosts 8 processor modules (two integer cores, one  floating point core per module) on two different dies, for  a total of 32 integer cores per node. We run GMT with 15  workers, 15 helpers and one communication server per node,  and use from 2 to 128 nodes.

Figure 3 shows the weak scaling of a simple queue-based  BFS (one memory access per edge) on GMT, measured in  million of traversed edges per second (MTeps). The graphs     Figure 3. Million traversed edges per second for the GMT implementation of the BFS (weak scaling)  Figure 4. Million traversed edges per second for the BFS implementation on GMT, UPC, Cray XMT, OpenMP (strong scaling). The X-axis represents nodes for GMT, XMT and UPC and cores for OpenMP  used for these experiments are randomly generated, with 1  million of vertices for every node added. Each vertex has  at most 4000 edges connecting to random vertices in the  graph. Therefore, the larger graph used with 128 nodes has  128 million vertices and 258 billion edges, for a memory  footprint of ? 2 TBytes. GMT?s performance scales almost linearly while the size of the of the graph increases. Figure 4  shows the strong scaling of BFS on GMT, compared to  the equivalent queue-based implementations for UPC, the  Cray XMT and OpenMP. The UPC implementation runs  on Olympus, while the OpenMP implementation runs on  the same 48-cores system employed for evaluating the  compiler. The Cray XMT is, instead, the 128-node Cougar  XMT machine located at PNNL [10]. Given the memory  limitations of the other systems, for strong scaling exper-  iments we used a random graph composed of 10 million  vertices and 2.5 billion edges. We see that BFS on GMT  outperforms the other implementations. However, because  the graph is relatively small, and GMT needs 2 million tasks  to fully utilize a system with 128 nodes and 15 workers, its  performance starts to decrease above 64 nodes.



IV. CONCLUSIONS  In this paper we presented the work-in-progress towards  the development of a scalable, in-memory SPARQL graph  engine that scales to hundreds of nodes while maintain-  ing constant query throughput. We discussed the overall  infrastructure, describing the various components and their  functionality. This paper mainly discusses the compiler  that converts SPARQL queries to parallel graph matching  operations expressed in C++, and the custom multithreaded  runtime that enables the database engine to scale to hundreds  of nodes of a commodity cluster. Finally, we presented  preliminary results for these two components.

