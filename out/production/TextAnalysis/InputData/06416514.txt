A Genetic Programming Free-Parameter Algorithm for Mining Association Rules

Abstract?This paper presents a free-parameter grammar- guided genetic programming algorithm for mining association rules. This algorithm uses a contex-free grammar to represent individuals, encoding the solutions in a tree-shape conformant to the grammar, so they are more expressive and flexible. The algorithm here presented has the advantages of using evolution- ary algorithms for mining association rules, and it also solves the problem of tuning the huge number of parameters required by these algorithms. The main feature of this algorithm is the small number of parameters required, providing the possibility of discovering association rules in an easy way for non-expert users.

We compare our approach to existing evolutionary and exhaustive search algorithms, obtaining important results and overcoming the drawbacks of both exhaustive search and evolutionary algorithms. The experimental stage reveals that this approach discovers frequent and reliable rules without a parameter tuning.

Keywords-Genetic Programming; Association Rules; Free- Parameters; Data Mining;

I. INTRODUCTION  Association rule mining (ARM), which is considered as an important area of data mining, has received more and more attention since it was defined by Agrawal et al [1]. ARM was conceived as an unsupervised learning task, having a descriptive nature and searching for strong relationships among items that are apparently hidden in large datasets.

An association rule could be defined as an implication  of the form IF antecedent THEN consequent, both the antecedent and consequent being disjoint sets, i.e., they have no items in common. The meaning of an association rule is that if all the items in the antecedent exist in a transaction, then it is highly probable that all the items in the consequent are also in the transaction [2].

First approaches for mining association rules are based on  an exhaustive search methodology, trying to obtain frequent and reliable association rules, i.e., those accurate rules that appears in a high percentage in the dataset. In the exhaustive search algorithms, the process of mining association rules is divided in two steps, firstly mining frequent patterns, and then extracting as many reliable association rules as possible.

In these first proposals, the mining process is hindered due to these two steps, which require a large amount of computational time and large amounts of memory.

With the growing interest in the storage of information, more and more amounts of memory are required, so the mining process is hampered. Furthermore, this growing interest in the storage of information is giving rise to the use of real-world datasets containing numerical values.

In ARM, the use of numerical datasets is not a trivial issue, increasing the search space since numerical domains typically contain many distinct values. In such situations, exhaustive search ARM algorithms cannot directly deal with numerical domains as they become hardly maintainable.

Evolutionary algorithms (EA) were proposed to overcome  the high computational time and the memory requirements in the ARM field. The use of evolutionary methodologies also allows of mining numeric association rules, defining interval rules and solving the problem of increasing the search space. First evolutionary algorithms for mining as- sociaiton rules were based on genetic algorithms [3], facing up to the exhaustive search approaches computational and memory requirements. Recently, this unsupervised learn- ing task was studied by using a grammar-guided genetic programming (G3P) approach [4]. This algorithm, called G3PARM [5], was presented as the first G3P proposal for mining association rules, providing excellent results and overcoming those drawbacks of current ARM algorithms in terms of execution time, the mining of numerical domains, and solution complexity. The main feature of using G3P is that the solutions are represented in a tree form having variable-length hierarchical structures, where the size, shape and structural complexity are not constrained a priori.

G3PARM was originally described as a fully configurable  algorithm, where a number of input parameters were re- quired, e.g., support and confidence thresholds, crossover and mutation probabilities, the number of generations, the population size, the number of rules to be mined, etc. The same occurs when using any of the existing evolutionary algorithms in this field, which are highly recommended in many situations, especially when they are used by qualified data miners. Nevertheless, the possibility of mining asso- ciation rules using a high-performance algorithm without the need of specifying a large number of parameters is a requirement for non-expert users.

This paper proposes a G3P approach for mining as-  sociation rules without requiring as many parameters as     G = (?N , ?T , P , S) with:  S = Rule ?N = {Rule, Antecedent, Consequent, Comparison } ?T = {?AND?, ?=?, ?IN?, ?Attribute categorical value?, ?Attribute numerical value? } P = {Rule = Antecedent, Consequent ;  Antecedent = Comparison ; Consequent = Comparison ; Comparison = ?AND?, Comparison, Comparison ; Comparison = ?=?, ?Attribute categorical value? ; Comparison = ?IN?, ?Attribute numerical value?, ?Attribute numerical value? ;  Figure 1. Context-free grammar expressed in extended BNF notation  G3PARM and other evolutionary algorithms do. This free- parameter algorithm makes the mining process easier for non-expert users, not requiring an optimal configuration of the parameters to carry out the mining. Furthermore, since this approach is based on G3P, it provides all the advantages of G3PARM [5].

This paper is structured as follows: the most relevant  related work is presented in Section II; Section III describes the model proposed as well as its main characteristics; Section IV describes the experiments, including the datasets used, and discusses the results obtained; finally, in Section V, some concluding remarks are outlined.



II. RELATED WORK  In ARM, most of the existing exhaustive search proposals are based on the Apriori algorithm [1]. Apriori achieves good performance by reducing the number of candidate patterns by using the anti-monotone property, which establishes that if a length-k itemset is not frequent in a dataset, none of its length-(k+1) super-itemsets can be frequent. Nevertheless, this algorithm is not appropriated in datasets with a large number of frequent patterns caused by quite low minimum frequency thresholds.

In order to overcome some of the drawbacks of Apriori,  Han et al. [6] proposed the FP-Growth algorithm, which stores information about patterns in an extended prefix-tree structure. The frequent-pattern mining only need to work on the tree instead of the whole dataset. However, this algorithm still suffer with the growth of the number of transactions and the use of a very low minimum support threshold, causing a huge number of association rules.

Recently, a novel exhaustive search algorithm, called  Predictive-Apriori, was proposed by Scheffer [7]. In this algorithm the author proposes to maximize the expected accuracy that the association rule will have for future data.

This fast algorithm finds the n best rules that maximize the accuracy, not requiring any threshold to determine whether a pattern is frequent or not.

In addition to the described problems of the exhaustive  search algorithms, these algorithms only works on datasets with categorical values. However, more and more data in real-world applications usually consist of numerical values,  so exhaustive search algorithms cannot be used directly in the extraction of association rules. A well-known solution to deal with numerical values is the discretization of the dataset, i.e., the division of their domains into intervals followed by applying categorical values to the intervals.

Nevertheless, the use of a previous discretization step is not exempt from problems, requiring to choose the correct number of intervals.

For the sake of overcoming the exhaustive search prob-  lems, i.e., the large amount of memory required, the huge computational time, and the fact of dealing with numerical attributes, evolutionary algorithms were proposed for mining association rules such as QuantMiner [8] and ARMGA [3].

The first algorithm for mining association rules by using  grammar-guided genetic programming was introduced by Luna et al. [5]. This algorithm, called G3PARM, makes use of G3P to define expressive and understandable individu- als. These individuals are defined by using a context-free grammar that establishes the syntax constraints for each one, allowing both categorical and quantitative attributes to be defined and obtaining feasible solutions without requiring large amounts of memory.



III. G3P FREE-PARAMETER ALGORITHM  In this section, the proposed algorithm is described in depth. The main peculiarity of this algorithm is that it com- bines the strength of optimizing by means of evolutionary algorithms, the ability of representing rules in an expressive and flexible way thanks to G3P, and finally, it does not require a parameter tuning as evolutionary algorithms do.

A. Encoding Criterion  The algorithm presented in this paper represents the individuals by a genotype and a phenotype. The former is defined by means of a tree structure, having different shapes and sizes, conformant to a context-free grammar G (see Figure 1). The latter allows of representing the meaning of the tree structure, i.e., the phenotype represents a rule having an antecedent and a consequent.

A context-free grammar could be formally defined as a  four-tuple (?N , ?T , P , S), ?N being the non-terminal symbol alphabet, ?T denoting the terminal symbol alphabet,     P standing for the set of production rules, S for the start symbol, and ?N and ?T being disjoint sets, i.e., ?N ? ?T = ?. Any production rule follows the format ? ? ? where ? ? ?N , and ? ? {?T ? ?N}?. Beginning from the start symbol S, each individual is represented in a derivation syntax-tree as a sentence conformant to the grammar. To obtain individuals, a series of production rules is applied from the set P . This process begins from the start symbol Rule, which always has a child node representing the antecedent and the consequent of the rule.

Once a grammar is defined either to describe valid ex-  pressions or to impose restrictions to the search space, it is necessary to validate this grammar. Thus, con- sidering the grammar defined in this problem and de- picted in Figure 1, the following language is obtained: L(grammar) = { (AND Condition)n Condition ? (ANDCondition)mCondition : n ? 0m ? 0}. Therefore, the grammar is well-defined and structured since any rule having at least one condition in the antecedent and conse- quent is obtained. Notice that the antecedent and consequent are disjoint sets, i.e., they have no items in common. Using this grammar it is possible to mine any association rule containing either numerical or nominal features. Numerical attributes are used by applying the operator IN, and ran- domly selecting two feasible values.

B. The G3P Algorithm  The EA proposed follows a generational schema, as depicted in Figure 2. It starts by generating an initial set of individuals. The size of this population depends on the number of rules to be mined. These initial individuals are originated conformant to the grammar and they have a pre- fixed length, depending on the type of rules desired by the data miner. The algorithm brings the possibility of searching for rules having a different number of conditions, indicating the minimum and maximum number of conditions available in each association rule.

The goal of this algorithm is to return the best n associa-  tion rules discovered along the evolutionary process. To do so, a pool of individuals with a predefined size of n is used, this pool working as an elitist population to maintain the best n rules throughout the generations. In each generation, this elitist population is updated with the best individuals, i.e., the individuals are ranked according to their fitness function values, and the best n individuals are kept for new generations.

For the sake of generating new individuals in each gener-  ation of the evolutionary process two genetic operators are applied, which are properly described in subsequent sec- tions. These two genetic operators (crossover and mutation) are used based on certain probabilities. Most evolutionary algorithms require fixed values, so the optimal probability values are determined by the data miner based on the dataset used. A major feature of the G3P free-parameter  Figure 2. The flowchart for the proposed algorithm  algorithm presented in this paper is its ability to update the genetic operator probabilities, not requiring any previous study of the parameters to obtain the optimal results. In the initial generation, both operators have initial values, and in subsequent generations, these probabilities are updated according to the average fitness value obtained in the elite population aforementioned.

The algorithm proposed continues its iterative process  without requiring a maximum number of generations as many evolutionary algorithms do. In the algorithm described in this paper, the evolutionary process is carried out if the elite population improves with the pass of the generations, measuring this improvement by using the average fitness function values of the n best rules. Finally, if the rules mined do not improve in 20 generations, then the algorithm finishes and the best rules discovered in the evolutionary process are given to the data miner.

In order to optimize those numerical intervals desired  by the data miner, the algorithm brings the possibility of carrying out a post-processing step. In this final step, a subset     of rules mined by the algorithm could be selected, and the intervals could be optimized.

C. Genetic Operators  To obtain new individuals in every generation of the evolutionary process, the proposal described in this paper uses two genetic operators: crossover and mutation.

Crossover: This genetic operator swaps a randomly selected condition in one parent for another randomly se- lected condition in the other parent. Therefore, the goal of this genetic operator is to obtain new individuals having the genotype of the parents. For a better understanding, the pseudocode of this genetic operator is shown in Algorithm 1.

Algorithm 1 Crossover operator Require: parents Ensure: offsprings 1: offsprings ? ? 2: for all individuals in parents do 3: ind1, ind2 ? getIndividuals(parents) 4: if random() < crossoverProbability then 5: cond1 ? getRandomCondition(ind1 ) 6: cond2 ? getRandomCondition(ind2 ) 7: newInd1 ? exchange(ind1,cond1, cond2) 8: offsprings ? offsprings ? newInd1 9: newInd2 ? exchange(ind2,cond1, cond2) 10: offsprings ? offsprings ? newInd2 11: end if 12: end for 13: return offsprings  Mutation: The main goal of this genetic operator is to maintain the diversity in the population. In such a way, a randomly chosen condition in an individual is mutated to obtain a new one. Algorithm 2 shows the pseudocode of this genetic operator.

Algorithm 2 Mutation operator Require: parents Ensure: offsprings 1: offsprings ? ? 2: for all individuals in parents do 3: ind ? getIndividual(parents) 4: if random() < mutationProbability then 5: cond ? getRandomCondition(ind) 6: newCond ? newCondition() 7: newInd ? exchange(ind,cond, newCond) 8: offsprings ? offsprings ? newInd 9: end if 10: end for 11: return offsprings  A major feature of this G3P algorithm is its ability to update both genetic operator probabilities. In the first generation both probabilities have the initial value of 0.5.

In subsequent generations, these probabilities are updated depending on if a higher diversity or a higher convergence is required. The algorithm calculates the average fitness value  obtained from the pool, i.e., the average fitness of the elite population. Based on this average value, it increases the population diversity by exploring the search space or it re- duces the diversity by exploiting the knowledge represented within the population. If the average fitness value obtained from the algorithm is improving along the generations, then the exploration should gradually change into exploitation by increasing the crossover probability and decreasing the mutation probability. On the contrary, if the fitness value obtained is not improving along the generations, then a higher exploration is required by decreasing the crossover probability and increasing the mutation probability.

It is interesting to note that in the earliest generations,  both probabilities increase and decrease while the optimal average fitness value is not found. Then, the exploration begins to be more important than the exploitation since it is more difficult to improve the solutions, and therefore, the optimal values are obtained. After 20 generations where the average fitness function value is not improved, the algorithm finishes returning the elitist population having the best n rules discovered.

D. Evaluate Individuals  The main issue in any evolutionary model is the process of evaluating each individual or solution, allowing to assign a fitness function to each individual in order to determine how promising certain individual is, i.e., how close a given solution is to achieving the aim.

Different researchers have described some objective mea-  sures for evaluating association rules [9]. Two of the most important and widely used measures in this field are support and confidence. The former is defined as the proportion of the number of transactions including the antecedent and consequent in a dataset. The latter is defined as the proportion of the number of transactions that include the antecedent and consequent among all the transactions that comprise the antecedent. In this paper, we also propose the use of a third measure to bring more effective information to the data mining task. This third measure is lift, which serves to calculate how many times more often the antecedent and consequent are related in a dataset than would be expected if they were statistically independent. The lift measure is calculated as the confidence of the rule divided by the consequent support.

In opposition to Apriori-based algorithms, the EA de-  scribed in this paper does not require two phases to mine rules. In this algorithm, each rule is evaluated according to a fitness function, which could be established by the data miner among the three measures aforementioned. In such a way, it is possible to choose the way the algorithm searches for rules, establishing an order of priority. In this paper, we determine that the first measure is support, the second one is the confidence measure and the last one is the lift measure.

Therefore, a rule is more promising than another one if its     support value is greater. If both rules have the same support value, then the confidence is used to determine which one is better, and so on.

E. Optimizing Intervals  Once the algorithm has finished, it is possible to run a post-processing algorithm whose aim is the optimization of those intervals desired by the data miner. The minimum and maximum bound of the intervals are established by the data miner, so the aim of this final step is to run a local search model to optimize the rule within the interval defined by the expert. This post-processing algorithm follows the well-known hill climbing optimization technique, which is an iterative algorithm that starts with an arbitrary solution to a problem and it attempts to find a better solution by incrementally changing a single element of the solution.

If the change produces a better solution, an incremental change is made to the new solution, repeating until no further improvements can be found.



IV. EXPERIMENTAL STUDY  In this section, a complete analysis of the effectiveness of this proposal compared to other existing proposals for mining association rules is carried out. JCLEC [10], a Java library for the evolutionary computation, was used in the proposal presented in this paper.

In the experimentation stage and in order to analyse the  performance of our proposal, a series of executions were performed using different algorithms and different datasets.

The results obtained by each evolutionary algorithm are the average results obtained running each one five times using different seeds each time. G3PARM was originally compared with other exhaustive search (Apriori and FP- Growth) and evolutionary algorithms (ARMGA and Quant- Miner) in [5], providing excellent results. At a significance level of p = 0.01, G3PARM was significantly different from exhaustive search algorithms in support and confidencemea- sures. Focusing on evolutionary algorithms and the support measure, at a significance level of p = 0.05, G3PARM was significantly different from the other algorithms. Thus, for reasons of brevity, only the G3PARM evolutionary algo- rithm and the Predictive Apriori algorithms are used in this experimental stage. The datasets used in this experimental stage are: computer hardware dataset (Cpu) having 209 instances and 9 numerical attributes, diabetes dataset (Diab) having 768 instances and 9 numerical attributes, and finally glass dataset (Glass) having 214 instances and 10 numerical attributes.

Focusing on the optimal parameters of the evolutionary  algorithms, the best results for G3PARM are those given by the authors, i.e., a population size of 50 individuals, 100 generations, 70% crossover probability, 14% mutation probability, a maximum derivation number of 24, an external population of size 20, a 90% external confidence threshold  and a 70% external support threshold. As for the Predictive Apriori algorithm and the proposed algorithm, notice that they do not need any parameter, only the number of rules to be mined is required. Finally, and in order to carry out a fair comparison, the notEqual operator is not bear in mind neither by the G3PARM algorithm nor by the algorithm proposed in this paper, since the Predictive Apriori algorithm does not use this operator, which allows of mining rules having a higher support as described in [5].

One of the problem of the G3PARM algorithm is that it  requires support and confidence thresholds, so a previous knowledge of the data distribution is required. Sometimes, the fact of using high thresholds gives rise to a small set of rules discovered. Table I shows the number of rules discovered by the algorithms using the configuration param- eters previously described; where Cpu?N , Diab?N and Glass?N represent the datasets discretized in N intervals.

Table I NUMBER OF RULES DISCOVERED BY THE ALGORITHMS  Dataset Predictive-Apriori G3PARM Proposal  Cpu-5 20 9.6 20 Cpu-10 20 1.0 20 Cpu-15 20 0.0 20 Diab-5 20 0.0 20 Diab-10 20 0.0 20 Diab-15 20 0.0 20 Glass-5 20 3.0 20 Glass-10 20 0.0 20 Glass-15 20 0.0 20  In order to carry out a fair comparison, the support and confidence thresholds of the G3PARM algorithm are fixed to the minimum, since this algorithm is not able to mine 20 rules having the thresholds aforementioned. In this comparison, a series of statistical tests were carried out, using the Friedman test to compare the results obtained and to be able to precisely analyze whether there are significant differences among the three algorithms. If the Friedman test rejects the null-hypothesis indicating that there are signicant differences, then a Bonferroni-Dunn test is performed to reveal these differences. The results obtained in this study are shown in Table II. It should be noted that the original datasets, i.e., the datasets without any previous discretization step, cannot be used with the Predictive Apriori since it does not deal with numerical values, so the symbol ?-? is included to show it.

The Friedman average ranking statistics for the support  measure distributed according to FF is 42.243, which does not belong to the critical interval [0,(FF )0.01,2,22 = 5.719].

Thus, we reject the null-hypothesis that all algorithms per- form equally well for this measure. In order to analyze whether there are significant differences among them, the Bonferroni-Dunn test is used, 0.800 being the critical dif- ference value for p = 0.1; 0.915 for p = 0.05; and 1.146 for p = 0.01, so there exist significant differences between     Table II RESULTS OBTAINED BY THE ALGORITHMS  Support Confidence Lift Dataset Predictive-Apriori G3PARM Proposal Predictive-Apriori G3PARM Proposal Predictive-Apriori G3PARM Proposal  Cpu - 0.975 0.839 - 1.000 0.919 - 1.010 1.017 Cpu-5 0.315 0.719 0.757 0.998 0.953 0.971 1.067 1.079 1.088 Cpu-10 0.398 0.475 0.517 0.992 0.834 0.929 1.325 1.166 1.216 Cpu-15 0.304 0.355 0.497 0.996 0.778 0.846 1.303 1.213 1.368 Diab - 0.754 0.942 - 0.999 0.979 - 1.004 1.006 Diab-5 0.078 0.370 0.437 0.996 0.803 0.827 1.344 1.163 1.127 Diab-10 0.070 0.221 0.218 0.998 0.612 0.759 1.641 1.113 1.239 Diab-15 0.040 0.188 0.161 0.998 0.581 0.643 1.667 1.103 1.219 Glass - 0.897 0.955 - 0.999 0.989 - 1.008 1.009 Glass-5 0.445 0.513 0.646 0.998 0.966 0.928 1.071 1.076 1.035 Glass-10 0.223 0.341 0.341 0.997 0.793 0.779 1.207 1.127 1.140 Glass-15 0.233 0.246 0.292 0.998 0.679 0.780 1.122 1.305 1.202 Ranking 3.000 1.708 1.291 1.499 2.333 2.166 1.999 2.250 1.750  the Predictive Apriori algorithm and the proposal presented in this paper at a significance level of p = 0.01, the new proposal being statistically better.

As for the other measures, the FF value is 2.655 and  0.733 for the confidence and lift measures, respectively.

All of these values belong to the critical interval [0,(FF )0.01,2,22 = 5.719], so it is not possible to assert that there are significant differences among the algorithms, but the new approach behaves better for the lift measure. As for the confidence measure, this new approach behaves better than the G3PARM algorithm.



V. CONCLUSION  In this paper, a G3P proposal for mining association rules without requiring any parameter tuning was presented. In this proposal, each solution is represented as a derivation syntax-tree conformant to a context-free grammar. This representation of the individuals provides expressiveness, flexibility, and the ability to restrict the search space over any domain.

The experimental study reveals that the new proposal  behaves statistically better than the other algorithms for the support measure. Focusing on the confidence and lift measure, it is not possible to assert that there are significant differences among the three algorithms, but the new proposal obtains the best ranking for the lift measure and also very reliable association rules, obtaining rules having a confidence value higher than 0.9 in most cases.

