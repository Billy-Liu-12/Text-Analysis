Data Stream Warehousing Lukasz Golab 1 and Theodore Johnson 2

Abstract?Data stream warehousing is a data management technology designed to simultaneously handle big-data and fast- data. Conceptually, a data stream warehouse can be thought of as a data warehouse system that is updated in nearly- real time rather than during downtimes, or as a data stream management system that stores a very long history. In this tutorial, we 1) motivate the need for data stream warehouse systems using real-life examples drawn from our experiences in network and data center monitoring, 2) describe several possible system architectures for data stream warehousing, 3) discuss various issues in query languages, performance optimizations and data stream quality, and 4) conclude with a discussion of open problems.



I. INTRODUCTION  Data volumes and velocities have been increasing with the emergence of new data sources, data collecting mechanisms and data-intensive applications, such as sensor and RFID data feeds, Internet traffic analysis, data center and infrastructure monitoring, intelligent transportation systems and power grids, financial transactions, social network updates, user behaviour logs, and results of large-scale scientific experiments [28]. The high-volume and multitudinous data streams that are generated and collected provide tremendous opportunities for deep data analysis and mining. By correlating multiple data sources, and mining recent as well as historical data, analysts can develop insights ranging from sales opportunities to predictors of imminent network failures.

Large entities generally develop big-data analysis systems (using conventional database management systems or Hadoop / MapReduce-based approaches) for archival data mining and Data Stream Management Systems (DSMSs) [21] for light- weight processing of very fast data streams. While it is possible to build separate systems for either real-time or long-term data analysis, it is not clear where to divide the systems.

Furthermore, historical data provides a context for interpreting new data [4], [11], [39]. As a result, data stream warehouses have recently been proposed to bridge the gap between big- data and fast-data management. Conceptually, a data stream warehouse is a database management system that is (nearly-) continuously loaded rather than being updated during down- times, or similarly, a data stream management system that stores a very long history.

Building an efficient and effective data stream warehouse system is challenging because it requires solving traditional data warehousing issues, such as storing and querying very large amounts of historical data, and data stream processing  issues, such as handling ordered data, best-effort consistency, and nearly-real time response. The purpose of this tutorial is to review recent research and open problems in data stream warehousing, including motivating applications, system archi- tectures and performance optimizations.



II. TUTORIAL OUTLINE  A. Motivating Applications  We start with motivating examples of applications that combine (nearly) real-time analytics with longer- term data histories. We draw on our experiences in network and data center monitoring [4], [17], [18], and especially the AT&T Darkstar project [25] and its network alerting and troubleshoot- ing tools such as Argus [39] and Mercury [33]. Darkstar ingests billions of records per day from hundreds of data sources to provide real-time alerting and troubleshooting tools to network operations, as well as serving as a primary platform for data- driven networking research. Alerting applications typically detect anomalies by comparing current data to prior trends; e.g., identifying peaking network loads during otherwise low- usage times. Then, to understand the problems raised by alerting applications, users need to do troubleshooting, which requires access to the data on which the alert was based and to related prior and historical data.

Further motivating examples will be drawn from web data analytics and online advertising [2], [7], social media monitoring [30], algorithmic trading [11], industrial process optimization [27], [28], and smart power grid analytics [6], [34].

B. System Architectures  Next, we describe and classify system architectures for data stream warehouses. Beyond big-data and fast-data manage- ment, a data stream warehouse should support streaming appli- cations (e.g., alerting) and data warehouse functionalities (ma- terialized views, deep analytics, complex workflows, etc.). Two special considerations which distinguish stream warehousing from conventional stream processing are workflow scheduling and management (e.g., [35]), and handling of late and out-of- order data (e.g., [8], [26]). We start with an overview of issues and techniques common to stream warehouses.

In our review of related systems, we have discerned three types of architectures:     1) Starting with a database management system and extending it with the ability to load newly arrived data in nearly-real time and continuously evaluate queries.

Examples include DataDepot [17], [18], DejaVu [11], [12] and Truviso [26]. We also discuss an interesting related system ? DBToaster [1] ? designed for streaming ingest into a data warehouse but which starts with a novel architecture.

2) Starting with a data stream engine and adding persis- tent data storage (e.g., Moirae [4]).

3) Starting with a distributed analytics framework such as Hadoop/MapReduce and adding continuous data processing (e.g., HOP [9], Muppet [30], and Nova [35]).

We will explain the pros and cons of each approach. For instance, systems that start with a data stream engine are typically designed for insert-only stream warehousing.

C. Query Languages and Semantics  We then discuss query languages used in data stream ware- houses, including ?streaming? SQL [23] with sliding window constructs [13], event-oriented languages (e.g, DejaVu [11]), sequence languages (e.g., AQuery [32] and Egil sequencing extensions [18]), and workflow languages (e.g., Nova [35]).

In terms of query semantics, problems may arise with interpreting the results of queries that access ?stable? historical data along with new data that arrive continuously and possibly out-of-order. We will discuss recent work on consistency in data stream warehouses, which can provide temporal data consistency and completeness guarantees to users [16].

D. Performance Optimizations  In this section, we provide an in-depth technical discussion of various performance optimizations for handling big data and fast data in a stream warehouse.

? Fast ETL: A critical features of stream data ware- houses is the ability to keep up with continuously arriving data. This requires efficient extract-transform- load (ETL) processes. We will discuss efficient algo- rithms for streaming ETL tasks such as joining in- coming data streams with disk-resident lookup tables [36].

? Data layout: Handling real-time and historical data requires novel data layout techniques. A common trick is to horizontally partition the data according to a timestamp attribute, in which case the arrival of new data triggers the creation of a new partition and corresponding indices, while older partitions are not affected. We will explain various optimizations of this approach, including data aging via variable- size partitions [17], merging out-of-order data into existing partitions [26], and transforming historical data from a write-optimized (e.g. row-oriented) to a read-optimized (e.g., column-oriented) format [38].

? Materialized view maintenance: In addition to fast ETL, efficient maintenance of materialized views is critical ? recomputing views from scratch whenever  new data arrives is not feasible. Furthermore, late data arrivals are common and their effects must be propagated to existing data partitions. We will dis- cuss incremental view maintenance through partition rebuilding [17], [24] and through revision partitions which contain the late-arriving data [26].

? Streaming dimension tables: In a data stream ware- house, fact tables typically collect data from insert- only ?event streams? that report the state of various entities at a particular time. Dimension tables, which represent properties that entities have, also change over time, but they may incur arbitrary insertions, deletions and modifications. In order to reconstruct what has happened in the past, we need to store previous versions of the data in dimension tables. We will discuss the design and optimization of these types of temporal dimension tables.

? Concurrency control: While traditional data ware- houses are updated during downtimes (e.g., every night), stream data warehouses are updated continu- ously. Multi-version concurrency control techniques are required to ensure that queries are not blocked by updates, but can be implemented very inexpensively [17], [37].

? Update scheduling: Near-real time scheduling tech- niques may be used to schedule data imports and materialized view updates in a way that maximizes data freshness. We present bounded-tardiness schedul- ing from the real-time systems community [31] and research targeted towards minimizing data tardiness in data warehouses [19], [22], [29]. We present additional issues from our experiences, e.g., graceful handling of overload conditions.

? Distributed stream warehousing: One way to im- prove scalability and reliability of a stream warehouse system is to distribute it among multiple machines.

However, if the data are distributed, then joins may be inefficient, especially if the tables than need to be joined reside on different machines. We will discuss recent techniques for co-locating related data items to improve the performance of distributed queries [10], [14], [15].

E. Data Quality in Stream Warehouses  Stream warehouses ingest data feeds generated from a wide variety of unsynchronized external sources, some of which may be machine-generated and may contain new kinds of errors due to the temporal nature and inherent imprecision of the sources (e.g., sensors), or due to hardware and software problems at the sources or at the data collecting mechanism. In general, we may have to deal with incorrect, missing, duplicate and delayed data.

A popular method for data quality monitoring is to define the expected data semantics using integrity constraints (e.g., Conditional Functional Dependencies); deviations from the ex- pected semantics may indicate data quality problems. We will discuss novel integrity constraints for expressing the semantics of streaming data feeds [20] and constraints specifying data completeness [16].

We will also present data stream cleaning techniques that are applicable to stream warehouses; one example is to dis- cover systematic correlations between different types of data quality problems across multiple streams [5].

F. Open Problems  We conclude the tutorial with a discussion of open prob- lems in data stream warehousing, including:  ? Exploiting modern hardware for stream warehousing, including main-memory warehousing [38] and SSDs [3].

? Hybrid system architectures and cross-system opti- mizations for stream warehouse systems composed of a combination of data stream engines, database management systems and MapReduce.

? Stream warehousing on the cloud.

? Adding data mining and machine learning functional- ities to stream data warehousing.

? Stream data profiling.

? Complexity management and administration of a large multi-source stream warehouse.



III. AUTHOR BIOGRAPHIES  Lukasz Golab is an Assistant Professor at the University of Waterloo. Before joining Waterloo, he spent five years at AT&T Labs - Research as a Senior Member of Research Staff.

He obtained a B.Sc. from the University of Toronto and a Ph.D. from the University of Waterloo, winning the Alumni Gold Medal for top graduate. Lukasz?s research interests are in big data management and analytics, data stream processing, data cleaning, and data management for sustainability. Lukasz has co-authored over 30 refereed articles, including a book on Data Stream Management, and three U.S. patents in data stream processing and data stream warehousing.

Theodore Johnson is a Lead Member of Research Staff at AT&T Labs - Research and an AT&T Fellow. Previously, he was an Associate Professor at the University of Florida.

Theodore received a B.Sc. from the Johns Hopkins University, and a Ph.D. from New York University. His research interests include streaming data, data warehousing, and data quality. He has written three applications in widespread use within AT&T: GS (a data stream management system), Data Depot (a very large scale data warehousing system), and Bellman (a data quality browser for large and complex databases). He has co- authored two books, Distributed Systems and Algorithms, and Exploratory Data Mining and Data Cleaning, and more than 80 refereed papers.

