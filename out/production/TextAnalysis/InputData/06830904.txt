Towards Knowledge Discovery in Big Data

Abstract?Analytics-as-a-Service (AaaS) has become indispensable because it affords stakeholders to discover knowledge in Big Data. Previously, data stored in data warehouses follow some schema and standardization which leads to efficient data mining. However, the Big Data epoch has witnessed the rise of structured, semi-structured, and unstructured data; a trend that motivated enterprises to employ the NoSQL data storages to accommodate the high-dimensional data. Unfortunately, the existing data mining techniques which are designed for schema-oriented storages are non-applicable to the unstructured data style. Thus, the AaaS though still in its infancy, is gaining widespread attention for its ability to provide novel ways and opportunities to mine the heterogeneous data. In this paper, we discuss our AaaS tool that performs terms and topics extraction and organization from unstructured data sources such as NoSQL databases, textual contents (e.g., websites), and structured sources (e.g. SQL). The tool is built on methodologies such as tagging, filtering, association maps, and adaptable dictionary. The evaluation of the tool shows high accuracy in the mining process.

Keywords?Tagging; Filtering; Terms; Topics; Association Rules; Dictionary; Big Data; Unstructured Data Mining; Analytics as a Service

I. INTRODUCTION There is high spectrum of data being generated and  collected today at an unprecedented scale; this paradigm is described as ?Big Data? [1]. The big data field which is receiving significant attention from diverse stakeholders in the academia, industry, and policy makers is being studied from three perspectives: big transaction data, big interaction data, and big data processing [2]. In order to simplify the meaning of big data and what it entails, we define the area within the concept of the 5V model which is illustrated in Fig 1. Our model is an extension on the previously defined 3V model by the IBM researchers in [3].

Volume (the era of size): The amount of data being generated across enterprises and corporations is growing at an exponential rate. The social media forum, Twitter, alone generates 12 Terabytes of data daily [4], with similar quantum of data or higher being generated from other forums. Thus, the consideration of data size is shifting from Terabytes to Zettabytes.

Variety (the era of unstructured data): The data is coming from multiple sources and comes in heterogeneous formats such as multimedia, text, blogs, emails, sensors, etc.

Furthermore, the data has no schema which is shifting the focus from structured and semi-structured data storage to entirely unstructured data.

BIG DATA  Variety  Unstructured Data  Velocity  Streaming Data  Volume  Data in Zettabytes  Veracity  Noise Elimination  Value  Cost  Fig. 1. The 5V model that currently defines Big Data.

Velocity (the era of streaming data): The rate at which the data is being generated is very fast which has shifted our focus from data sets (batch) to streaming data.

Value (the era of cost associated with data): While the data is being generated, collected, and analyzed from different quarters, it is important to state that today?s data has some costs. The data itself can be a ?commodity? that can be sold to third parties for revenue. Moreover, understanding the cost or value of the data can aid in budget decision making at estimating the storage cost of the data. For instance, is it worth it to spend 6 million US dollars on infrastructure cost to keep data that is worth 1 million dollars?

Veracity (the era of data pollution that needs cleansing): There is the need to check the accuracy of the data by eliminating the noise through methodologies such as data pedigree and sanitization. This is to ensure data quality so that  2014 IEEE 8th International Symposium on Service Oriented System Engineering  DOI 10.1109/SOSE.2014.25     decisions that are made from the collected data are accurate and effective.

In this paper, the focus is on the concept of ?variety? which is the result of unstructured data; and how we can ensure ?veracity? in such environments. Since the ever-growing data comes in multiple formats and there is no standardization from policy makers on data generation, the challenge that we face is how to effectively mine the data. This challenge is prevalent because existing data mining techniques have been designed for schema-oriented and structured data storages. Analytics-as- a-Service (AaaS) [5] is therefore becoming a popular discipline which involves the studies of unstructured data mining. In the introductory work in [32], we proposed and designed a unique AaaS tool that aids enterprise consumers to perform topics and terms mining from unstructured data silos.

Specifically, we aim at NoSQL storages where little has been done on developing terms mining tools. Our proposed tool is also applicable to other textual contents such as tag-based files (e.g., HTML, XML, etc.) and non-tag based documents (e.g., PDF). This paper details the design of the proposed AaaS framework and showcase a series of pilot tests to validate our design decisions. The results show high accuracy for the term and topic extraction process. Also, we have made a significant progress in reducing the duration for mining the terms.

The remaining sections of the paper are organized as follows. Section II expounds on unstructured data and in Section III, the progress made at mining textual contents, the emergence of Analytics-as-a-Service (AaaS), and our research motivation. Section IV introduces our proposed AaaS tool from the architectural design and implementation perspectives. The evaluation of the tool is carried out in Section V and the paper concludes in Section VI.



II. UNSTRUCTURED DATA The heterogeneity in data (i.e., ?Variety?) arises because  today?s data economy cuts across several enterprises. Though it may appear that the data is in silos (e.g., social media information vs. corporate data), there are new opportunities that can be created when all of these data can be analysed.

These opportunities are beneficial for end-users, enterprise consumers, services providers, and prosumers. For instance, online social analytics tools can aid the security agencies to determine criminal communities while advertising agencies can re-align products and services towards more demand- driven audience based on social media analytics.

However, the potential opportunities that we seek and desire is hampered by the nature of the data, which is unstructured. The data is unstructured means:  a) The data is represented in varying formats (documents, multimedia, emails, blogs, websites, textual contents, etc.)  b) It is schema-less because there is no standardization or constraints placed on content generation and storage.

c) It is from diverse sources (e.g., social media, data providers such as Salesforce.com,  mobile apps, sensors, etc.)  d) It requires multiple APIs to aggregate data from the multiple sources; and the APIs are not standardized nor have common Service Level Agreements (SLAs).

The above highlighted descriptions aided the data integration community to push for the adoption of NoSQL databases [6]. Thus, enterprises are embracing NoSQL storages since they support schema-less, semi-structured, and structured data storage. Moreover, some accommodate textual and file attachments (e.g., CouchDB). Also, there are some of the NoSQL storages that stores only files such as DropBox, MEGA, Amazon S3, and so on. The NoSQL platforms that support actual data (e.g., MongoDB, Google Bigdata, etc) allow the users to store data in formats such as JSON, XML, or other textual formats.

More recently, NoSQL storages have also adopted graphing methodologies (e.g., Google Knowledge Graph [7]). In essence, graph-based NoSQL databases can be employed to build data storages in a way that can aid in simple queries as well as establishing relationship between the data without writing relational database queries. Relational databases rely on keys and join operations which makes searching tedious in data silos. Further, most of the previously deployed NoSQL databases rely on MapReduce functions which cannot be shared but are specific to each NoSQL database. However, graph databases allow direct access to the data source of interest without any join or MapReduce operation. Moreover, graph databases are transactional and are optimized for cross- record connections. At the moment, some NoSQL graph databases as listed in [8] are Neo4J, Infinite Graph, InfoGrid, HyperGraphDB, GraphBase, and so on.

Despite the efforts being made to deploy NoSQL databases, there are still challenges. It is important to note that while NoSQL storages have aided the accommodation of unstructured data, the analytics of the data is an entirely different issue.

The problem is that, these NoSQL databases are products of different vendors so their query styles vary. Moreover, in order to aggregate the data from all of these sources, a mashup service has to be deployed which presents the researcher with two problems. Firstly, multiple APIs has to be studied and secondly, the returned data from the individual data silos are in different structures [6]. Further, the querying of the individual data sources can be time consuming and computationally demanding.

These problems will be explained in detail when we discuss the motivation of our work. In the next section, we highlight the field of Analytics-as-a-Service and how this has become important in the era of big data.



III. UNSTRUCTURED DATA ANALYTICS  A. Analytics-as-a-Service (AaaS) IBM Research has identified AaaS as an area that can offer  business value [5]. This is because AaaS is able to aid in the transformation of unstructured data into business creation ventures. In this regard, IBM pushed for the creation of an AaaS platform that can aid end-users and companies to submit     their data (in either structured or unstructured format) for analytics purposes. This platform as depicted in Fig. 2 is meant to reduce the financial burden on companies from maintaining in-house data analysts; the same view is shared by EMC [9].

As an on-going work, IBM has identified the following hurdles that must be overcome in order for the AaaS to materialize: Definitions of Service Level Agreements, Quality of Service monitoring methodologies, pricing, unstructured data manageability, and business processes (models).

Fig. 2. The Analytics-as-a-Service provisioning as proposed by IBM [5]  Another major enterprise player in the AaaS platform services deployment is SAS [10]. The AaaS service provided by SAS is based on predictive analytics which enables customers to quickly access solutions to their business problems.

Sun et al. [11] propose an AaaS framework as an extension to a Software-as-a-Service (SaaS) that enables enterprises to access data analytics as services. While focusing on multi- tenancy, the authors propose an SLA customization methodology which supports multiple analytics capability demands of tenants. Their architecture details how the virtual servers are constructed to accept the data input from the users.

The servers that perform the actual analytics are referred to as scoring servers.

Deepak et al. [12] also detail the architectural design of their proposed AaaS platform which is cloud-hosted. Users are facilitated to upload files and data over a web interface which is the means of interacting with the system. Uploaded files are pushed to the analytics engines which comprise SPSS, R, SAS, Cluto, and WEKA. This type of AaaS platform requires technologies from other providers as well and the analyzed result is stored in data systems such as oracle, DB2, and file systems.

B. Open Issues The ongoing works on AaaS are mainly driven by cloud  services providers who have higher financial muscle to construct such facilities. Firstly, that means the users have to pay for their data to be analyzed. Secondly, this also means companies have to submit their data to the AaaS platform providers for the latter to do the analysis, which can lead to  privacy issues especially when the two companies are product competitors.

Hence, as first step, we aim at offering an open source AaaS tool that can be adopted by companies to do their terms and topics extraction from their unstructured data sources. The idea is that, the proposed framework can be downloaded and either hosted internally on private servers or on a dedicated cloud platform (e.g., Amazon EC2) for the data extraction tasks.

However, the ability to deploy an AaaS tool requires deeper knowledge in textual mining methodologies. Thus, after reviewing over 400 works from both academic and industry research publications on methodologies and tools on textual mining, the next section gives a brief overview of the area. On the whole, most of the methodologies are based on Natural Language Processing (NLP) which is inherited from Artificial Intelligence, Linguistics, and Ontological Semantics [13-15]  C. Unstructured Data Mining Without data mining, we cannot deduce any meaning out of  the high-dimensional data at our disposal. As already posited, existing data mining techniques which are well-advanced have been designed to work with schema-oriented databases which are structured [16]. However, these existing techniques are no longer relevant to the modern challenges of information extraction especially at the application level. To enhance the data mining process, scientist in both the academia and industry are beginning to explore the best methodologies that can aid in the unstructured data mining process; especially in the context of textual data mining. As a result, we have witnessed some methodologies such as: Information Retrieval algorithms based on templates [17], Association Rules [18-19], Topic tracking and topic maps [20], Term crawling (terms) [21], Document Clustering [22], Document Summarization [23], Helmholtz Search Principle [24], Re-usable Dictionaries [25], and so on.

In Table I, the summary of the methodologies is presented.

Since our focus is on terms and topics extraction, these are discussed further for the reader?s better understanding.

1) Terms Extraction Terms extraction unlike topics publishing focuses on  establishing a network of associative relationships between terms [20]. A typical example is the presence of concurrent terms in a World Wide Web (WWW) document. The main benefit of term mining is that, it optimizes the search space vector thereby reducing processing time [26-27]. Term crawling for example aims at indicating the relevance of information gathered in an unstructured document by showing the interdependencies between the terms.

The work by Feldman et al. [27] extended on the Knowledge Discovery in Databases (KDD) by shifting from topic tracking and word tagging to term identification in unstructured documents. The authors push for term normalization based on lemmas with corresponding part-of- speech tags such as stock/N, market/N, annual/Adj, interest/N, and rate/N where N is a noun and Adj is an adjective.

TABLE I. SUMMARIZED METHODOLOGIES OF UNSTRUCTURED DATA MINING [33]  Information Extraction  Association Rules Topics Terms  Document Clustering  Document Summarization  Re-Usable Dictionaries  Goal Data Retrieval, KDD Process  Trend discovery in  text, document or file  Topic Recommendation  Establish associative  relationships between terms  Organize documents into sub-groups with closer identity  Noise Filtering in Large  Documents  For Knowledge Collaboration and Sharing  Data Representation  Long or short text, semantics and ontologies,  keywords  Keywords, Long or short  sentences  Keywords, Lexical chaining Keywords  Data, Documents  Terms, Topics, Documents  Words, Sentences  Natural Language Processing  Feature extraction  Keyword extraction Lexical chaining  Lemmas, Parts-of- Speech  Feature extraction  Lexical chaining  Feature extraction  Output Documents  (Structured or semi-structured)  Visualization, Summary  report Topic linkages Visualization, crawler depth Visualization  Visualization, Summery  report Summary report  Techniques  Community data mining,  Tagging, Normalization  of data, Tokenizing,  Entity detection  Ontologies, Semantics, Linguistics  Publish/ Subscribe,  Topics- Associations- Occurrences  Linguistic Preprocessing,  Term Generation  K-means clustering,  Hierarchical clustering  Document structure analysis,  Document classification  Annotations, Tagging  Search Space Document, Files, Database Document,  Files, Database Document, Files,  Databases Topics vector  space Documents, Databases  Databases, Sets of documents  Databases, Sets of documents  Evaluation metrics  Similarity and relatedness of Documents,  sentences and keywords  Similarity, Mapping, Co- occurrence of  features, correlations  Associations, Similarity, Sensitivity, Specificity  Frequency of terms,  Frequency variations  Sensitivity, Specificity, Balanced Iterative  Reducing and Clustering (BIRCH)  Transition and preceding  matrix, Audit trail  Word extensibility  Challenges and Future Directions  Lack of research on  Data Pedigree  Applies varying approaches to different data sources. E.g., relational, spatial, and transactional data stores  Identification of Topics of interest  may be challenging  Community based so  cross-domain adoption is challenging  Can be resource intensive  therefore needs research on  parallelization  Needs research on Data Pedigree  Lack of research on dictionary adaptation to new words  A proposal for term extraction module is put across which is primarily responsible for the labeling of extracted terms from a particular textual document. Overall, the term extraction module performs Linguistic Preprocessing, Term Generation, and Term Filtering. What is also important to highlight in the work of Feldman et al. [27] is that, the authors provide a score test based on relevance in the document using the following techniques: Deviation-Based Approach?calculate the frequency of a term in the document based on the relative standard deviation; Statistical Significance Approach?to test the significance of the relative frequency variation of a term; and Information Retrieval Approach?using the maximal term frequency ? inverse document frequency (tf-idf) to assign a score for information retrieval. The work further explores the option of building a taxonomy constructor based on terms. The use of taxonomy constructors facilitates the search for specified terms which have association rules in an optimal time rather than searching through an entire document.

There are other ways of using terms such as the conversion of terms into a vector space and calculating the cosine distance between the search terms [28]. The idea is to understand the size of the cosine distance, the frequency of the search term, and the document correspondent.

One of the areas that terms are dominant to a software process is Online Analytic Processing (OLAP) applications [21]. Furthermore, based on term occurrences as central root, a star-like architecture can be established using the following child dependencies: time, location, term, document, and category [29]. In a star-like architecture, the idea is to determine the occurrences of a specified term based on the listed dependencies.

Also, as the computing landscape is venturing into the adaptation of methodologies from related and non-related fields, term extraction can be a very optimal approach to perform some of the time consuming tasks. For example, there     is enough room to explore terms applicability to traceability, readability of bugs, maintainability, and so on. As an extension on Table I, terms extraction can be challenging due to linguistic issues. For instance, similar words in different languages can mean different things and even similar written words in the same language can mean different things. This problem is also present in topic mining scenarios.

2) Topics Topics tracking is generally employed to recommend  subjects of interest to a user. Most online subscription systems (e.g., hotel booking, flight, news articles, etc.) are based on topics methodologies where keywords are extracted from users? subscription to form a basis for the users? interests [20, 30]. Based on these key words, a mechanism of grouping the related keywords (called Lexical Chaining) can be employed to extract subsequent published messages [20].

Topic Maps (ISO/IEC 13250:2003) on the other hand focuses on the representation and knowledge interchangeability in a repository [20]. Topic maps deal with the following elements (commonly referred to as TAO) as representations: topics?refers to the entities being referred to which are mainly names, events, application component and modules etc.; associations?graphical links between the topics of interest; occurrences?refers to the relevant linkages of the information to topics. Abramowicz et al. [20] researched on the automation of topic map creation and outline four procedures which can be adopted such as: subject recognition, information extraction and preparing, RDF modeling, and mapping RDF model into a topic map. The basic idea is that the information extraction is initialized after the topic of interest is identified.

Information Extraction at this point from the unstructured data source can be done by exploring other techniques such Natural Language Processing and Shallow Text Mining. RDF models are triples of the format subject (e.g., resources), property (e.g.

property type of the subject), and value (e.g., URL). RDF models label the corresponding objects to topics in a map and their occurrences and associations. However, mapping RDF model into a topic map has various processing procedures to follow which include One-time Processing, Repeated Processing, and Continuous Processing [20].

D. The Way Forward From the findings in the background works, the next  section discusses our designed Analytics-as-a-Service (AaaS) tool. The functional/research needs that separate our tool from others are to build an Analytics-as-a-Service (AaaS) tool that:  ? Is easily configurable to extract topics and terms from NoSQL storages. As of the time of this work, we could not find any tool that does this except for the Hadoop platform.

? Can extract topics and terms from structured data sources such as SQL.

? Can extract topics and terms from textual and semi- structured data sources (e.g., HTML, RDF).

? Can be elastic enough to perform topics and terms extraction from multiple data sources of different schema at the same time.

? Organize the output for informative visualization.

The second and third points are not entirely new features to our work. There are existing tools that mine topics from structured data sources (e.g., SQL) and most of the initial works on textual mining focus on HTML (e.g., Web crawlers).

However, building a modern AaaS tool should not only focus on unstructured data sources but should also accommodate structured data sources. More importantly, the tool should support any data source and style. And this is what we sought to achieve.



IV. THE ARCHITECTURAL DESIGN OF THE AAAS The architectural design as graphically illustrated in Fig. 3  encompasses four major actors: the human users, the front-end, the data sources, and the AaaS framework. The human user interacts with the system through the Input Layer. The input layer is the interface (currently designed in HTML5) that enhances interactivity between the user and the AaaS framework. Though it is designed in HTML5, it can be customized in any UI design when adopted as open source. The input layer which is part of the front-end component can be installed on any device (devices with either heavy or light processing capability) since it only affords the user the flexibility of interacting with the system.

On the input layer, the user is facilitated to perform two initial tasks: Search Criteria selection and Data Source specification. The search criteria tell the system whether the user wants to perform the topics and terms mining based on certain specifications. A typical specification can be the extraction of topics which are related to some particular keywords; where related can mean keywords which are antonyms or synonyms to the specified topic. Users can also define their own specifications, which will be explained later.

Further, the user has to specify the data sources from where the data can be mined. A URL or relative path to the database(s) can be provided in this case.

Once the user?s initial tasks (i.e., inputs through selection and specification) are completed, the information is parsed to the Request Parser, which is an interface of the AaaS. As suggested in earlier sections, the AaaS framework is considered as a back-end system and can be deployed on an organization?s internal server or on a cloud platform. However, we strongly recommend a powerful platform for hosting because the AaaS is responsible for the high computation and processing of the data mining tasks. The entire implementation of the AaaS is in Erlang [31] which is a programming environment that supports concurrency and inter-process communication. Since the AaaS framework acts as an application server, the Request Parser is the network component that receives the information from the input layer and routes the request to the other AaaS components.

Currently, the request parser supports both WebSocket and HTTP connections.

Textual Content (e.g., PDF)  Artifact Extraction Definition  Topics  Terms  Search Algorithm  NoSQL  Serializer  Clustering Data (JSON)  Search Criteria  Visualization  Topics Clustering  Terms Clustering  Thesaurus Dictionary  Semantic Engine  Topic Parser  R eq ue st P ar se r  Search Criteria  Search Criteria  Association Rules  Topics Mapping  HTML/RDF  Data Sources  SQL  API Interface Definitions  Extracted Artifacts  TaggingFiltering  O ut pu tL ay er  Web Browser  Mobile (e.g., Notebooks, Tablets)  Dashboard  REST API SOAP API  MAPREDUCESQL QUERY  TAG DEFINITIONS  TEXT PARSER  User  In pu tL ay er  Display  Fig. 3. The architectural design of our Analytics-as-a-Service (AaaS) tool.

The requests are first routed to the Artifact Extraction Definition component. The responsibilities of this component include the validation of the input information from the user, interpretation of the requests, and the request categorization.

The user input can be invalidated for instance if the specified data source does not exist or file format is currently not supported or the specified path is wrong. Though currently not implemented, we want to perform other tasks in the future such as document clustering, file structure organization, etc. In that case, the artifact extraction definition component will be responsible for the categorization of the input request based on the data or document extraction needs of the user. When the request is validated, the artifact extraction definition component then interprets the request as topics extraction or terms extraction. The interpreted request is then sent to the Semantic Engine layer.

The goal of the semantic engine layer is to pass the request through series of reviews that will improve the quality of the data mining result. In the current version of the AaaS, we treat Topics as keywords that the user wants to extract from the data source. Typical cases of topics extraction can be looking for artifacts such as ?Contract?, ?Economy?, ?Sports?, etc. from a data source. The semantic engine will not have much to do in this case but to pass the topics for extraction at the next stage.

However, Terms extraction involves a lot more because terms consider a specified keyword plus possible dependency keywords for interpretation. For example, when the artifact  ?Contract? is specified, it can also mean ?Agreement? therefore the mining process will have to consider the latter keywords as well. This latter requirement convinced us to design the terms extraction as an extension on the topics. The consideration of the latter keyword can be specified by the user as part of the specification process on the input layer or left for the AaaS layer to decide and later the user can refine the result.

In order to clearly explain the workings of the semantic engine to the reader we provide Table II as an example that we shall use to discuss the remaining sub-components.

Assuming the keyword ?Contract? is the specified term, we necessarily need to understand what the word mean from the user?s perspective. In the English Language, ?Contract? can be synonymous to ?Agreement? and the other words in Table II or can be synonymous to ?Afflict? and the other words in similar line. The first problem that arises here is the presence of semantic issues where the mining process can potentially return terms involving ?Acquire? when the user actually wants ?Agreement?. Even if we assume that the extracted terms should just return everything on ?Contract?, the outcome will be a result with a lot of False Negatives. This will eventually affect the accuracy and specificity of the result as we shall see later in the evaluation section.

So, when an artifact is validated and sent to the semantic engine, the latter considers that artifact as a Topic. This topic is then forwarded to the Topic Parser where the actual meaning     of the artifact is defined in order to generate an enhanced result. The topic parser is also important for the minimization of the other tasks ahead such as data cleansing. The topic parser has two components which are the Dictionary and the Thesaurus. These two components are storages which are built in DETS (a disk storage facility in Erlang) to keep a tabular structure as Table II.

TABLE II. SAMPLE ARTIFACT  Artifact Synonyms Antonyms  Contract  Arrangement Disagreement  Agreement Misunderstanding  Compact Break-off  Settlement Fired  Pact  Contract  Acquire Give  Afflict  Indispose  Develop  Disorder  The dictionary contains predefined artifact in English, their corresponding synonyms, and antonyms. However, another concern at this point is how to make the dictionary adaptive.

To achieve the adaptability of the dictionary, we build a separate lexical component which is the thesaurus.

Adaptability in our case means that the topics and terms process should be extended or expanded to other domains outside the English Language. For instance, in the medical domain there are jargons that are not in the English dictionary or there are medical specific lexicons. To make our AaaS framework as generic as possible, the thesaurus only need to be populated with the domain specific jargons such as the case of the medical domain.

Since the dictionary and thesaurus are both tabular, we need to establish relationship between the artifacts. This requires a network (or graph) of inter-connected words because, for each word, we need to know all other artifacts that are dependencies either as synonyms or antonyms. Here is where we define a component called Topics Mapping where words are linked to each other based on the relationship that exists between the words. The topics mapping component is a script running on a Bloom Filtering methodology. Considering the vast amount of data that can exist in the lexicon (such as the thesaurus), it will be time consuming to go through the entire table in search for the synonyms and antonyms. Another way to imagine this scenario is: an artifact will have a synonym and the same synonym can later be an artifact which means the previous artifact becomes a synonym. For example, an artifact can be ?Settlement? as shown in Table II which means that artifact will have dependency such as ?Contract?. So, going through the thesaurus based on sequential indexing will  generate a complexity of O(n2). Thus, the Bloom Filtering methodology is adapted to rather build pointers between the words. In that case, for every word, we can determine the dependency keywords through referencing.

After the determination of all dependencies through topics mapping, an Association Rule is established where the user request is matched against the topics mapping. For example, the following Association Rules can be defined (there are lot more Association Rules that can be defined based on the corporate/enterprise domain):  ? Extract all terms based on the antonyms  ? Extract all terms based on synonyms  ? Extract terms based on a specific synonym only  These rules are contained in a JSON script internally; for example, in the case of the third rule, the JSON will be as shown below:  { ?Keyword?: ?Contract?,  ?Synonym?: ?Agreement?  }  The establishment of the Association Rule prepares the semantic engine for the Terms extraction. The rule-based JSON file is then passed to the Search Algorithm layer. The search algorithm determines which methodology to apply when searching through the data sources. As an ongoing work and discussions with some of our research partners there are proposals for the following algorithms which are adapted from [33]:  ? Linear/Random Search algorithm: This means the data sources should be searched sequentially but after the implementation, we found the methodology is slow and causes a lot of waiting queues.

? Parallel/Concurrency Search algorithm: This works by determining the number of data sources and applying the data extraction to all the data sources simultaneously. This aided us to reduce the waiting queue.

? Pessimistic and Optimistic Search algorithms: Though we have explored these options in other ongoing works, we have not implemented it in the current work.

It will be considered later. Optimistic search aids the system to specify only directories which should be searched while pessimistic search allows the specification of directories that should not be searched.

? As discussions are advanced, other search algorithms can/will be considered.

The search algorithm specification activates the API Interface Definition layer. Here is where the queries are generated, constructed, and issued for the specific data sources.

For instance, SQL query cannot be issued for a NoSQL specified database nor a Map/Reduce query for SQL database.

This means for every supported data store, the query style that is specific to that data source must be issued. In the current implementation, the following Application Programming     Interfaces (APIs) are defined and fully functional. This is the part that is crucial to the developers because the understanding of the APIs is what will allow the customization of the queries for the specific mining task.

REST API: The REpresentational State Transfer (REST) API is defined for NoSQL databases that support clear semantic queries. These databases support HTTP operations such as GET (to read data), POST (to create records), PUT (to update existing records), and so on. Also, there are publicly available NoSQL service providers that expose their services end-point as REST. Examples are Amazon S3, etc.

SOAP API: The SOAP API support SOA-based data stores over Web services calls. The returned result for the SOAP API query is in XML format. The SOAP requests are all sent using the HTTP POST method. Amazon Services and most existing legacy services are running on the SOAP protocol.

MAPREDUCE: The Map/Reduce API is employed to make the REST-based query much simplified on the AaaS. For example, Map/Reduce involves filtering and sorting for NoSQL systems such as Hadoop, CouchDB, etc. and this leads to processing workload. So, instead of delegating the Map/Reduce functionality to the data store to process, we allow the processing to be done by the AaaS. Thus, when the request reaches the data store, the only workload is the processing of the request (i.e., the workload regarding the request construction is offloaded to the AaaS).

SQL QUERY: This is where queries are constructed for the structured and schema oriented databases such as the SQL. In the Erlang environment, the connection to the SQL is over ODBC as shown below:  odbc:start().             //to start the ODBC  {ok, Ref} = odbc:connect("DSN=sql-bigserver; UID = bigdata; PWD=conf", []).          // to connect to the database  TAG DEFINITIONS: This API is defined for tag-based data sources (i.e., file databases) such as XML, HTML, and RDF.

These styles of databases are described as semi-structured because though the content is textual, the texts are defined within specific tags. Thus, the text mining can be done by defining tags.

TEXT PARSER: This API is designed for purely text-based files with no structure. Examples are PDF files, .DOC, etc. The Text Parser API considers the specified artifact (i.e., Topic or Term) and does scanning through the file in order to read the occurrences of those artifacts.

The completion of the query construction opens the connection to the specified database on the Data Sources layer.

This layer is not necessarily on the same environment as the AaaS framework. The data source can be on remote servers within the same organization, outside repositories, or on the same server as the AaaS. For every request that is issued by the specific API interface to the data source, a response is expected which is passed to the Extracted Artifacts layer.

The extracted artifact layer is where the Knowledge Discovery in Database (KDD) process starts. The goal here is to determine existential evidence of topic and terms  relationship through the KDD process. The data which is returned from the data source through the API interface is passed to the Serializer component. There are cases where the returned data is from multiple API interfaces especially when the data mining process involves multiple data sources. Thus, the Serializer is the first component that receives the data and performs the following macro activities:  ? Normalization: reducing the redundancy of the data and establishing relationship between the artifacts.

? Grouping: Organizing similar artifacts into categories, and  ? Integration: Merging the artifacts into a format that gives single dimensional overview of the search result.

After the Serializer is done, Tagging is performed the data.

Tagging involves determining the sources of each data, occurrences of specific artifacts, frequency of artifact occurrences etc. for statistical purposes. Once the tagging process is complete, the data is passed to the Filtering component. Here is where the topics and terms are organized based on relevance. For instance, we can determine relevance based on the frequency of occurrences of terms from the tagging process; and only report results based on certain thresholds such as top 100 terms and so on. The filtered data is then organized based on Clustering where the data is modelled in JSON format for easy passing. Depending on the data mining need as specified by the user, the JSON data is parsed as Topic Clustering or Terms Clustering to the Visualization layer. For instance, Fig. 4 gives a tag cloud visualization of terms extraction related to ?Psychiatry? using our AaaS tool from over 3 million HTML-based sources on the Web from North America. The visualization reports 42 most relevant keywords that form part of the term ?Psychiatry?. We measured relevance based on the rate of recurrence of those keywords from the various sites. Currently we are aiming at integrating our visualization with the InfoVis framework.

Results from the visualization layer are sent to the Output layer which is primarily the platform of choice that the user prefers to view the result. Currently, results can be viewed as a browser content, on tablet and notebooks, and as dashboard.

In the next section, we discuss some of our preliminary results after evaluating the AaaS tool.



V. EVALUATION The AaaS framework is evaluated based on the quality of  term mining from the various data sources. The accuracy of the terms being extracted is measured in the NoSQL, Web, and SQL environments. We deployed our framework on our Window-based system with the following specifications: Windows 7 System 32, 4.12 3.98 GHz, 16GB RAM, 1TB HDD.

We considered a training data set of 30 million entries which is spread across 30 different NoSQL databases (CouchDB) with file attachments which are deployed on an Amazon EC2 instance with the following specifications: Processor: Intel Xeon, CPU E5410 @ 2.33GHz, 2.41GHz, RAM: 1.70GB, System 32-bit operating system. The 30 million medical related records are divided into 30 groups where 1 million records are     stored in each CouchDB database.  This dataset is a validated data which is used in the experiments in [34]. Then, we crawled some medical information websites such as healthcare providers, facilities, locations, specializations, etc. from approximately 5000 websites. This information forms the basis of our training datasets in this experiment. It is important to state that the information collected is just for test purposes of our tool and not for any other reason.

By focusing on the various data sources, we perform the terms extraction based on the proposed mining algorithms such as Linear Search, Parallel Search, Pessimistic Search, and Optimistic Search. In Table III, we report the evaluation from  the NoSQL databases. The Pessimistic Search and Optimistic Search are dependent on the user who is performing the experiments because the choice of selection is subjective. For instance, the choice of a user?s data source not to be searched may be different from the choice of another user. From the result, the Pessimistic Search and the Optimistic Search shows high false negative because of the user selection. In cases where the user thinks there can be terms, that is not the case.

This also applies to the cases where the user thinks terms may not be found somewhere. The advantage of the two methodologies however is the ability to perform quick search when the user has an idea about the location of the specified term.

TABLE III. ANALYZING THE EXTRACTED TERMS FROM THE NOSQL  Linear Search Parallel Search Pessimistic Search Optimistic Search  True Positive 94.80% 94.27% 87.21% 88.07% False Positive 2.60% 2.81% 2.34% 3.20% True Negative 100.00% 100.00% 97.28% 96.56% False Negative 0.34% 0.77% 41.31% 23.32%  Precision 97.33% 97.11% 97.39% 96.49% Recall (Sensitivity) 99.64% 99.19% 67.86% 79.06%  Specificity 97.47% 97.27% 97.65% 96.79% Accuracy 98.51% 98.19% 80.87% 87.44% F1 Score 98.47% 98.14% 79.98% 86.91%  TABLE IV. ANALYZING THE EXTRACTED TERMS FROM THE WEB  Linear Search Parallel Search Pessimistic Search Optimistic Search  True Positive 91.61% 96.33% 75.55% 74.09% False Positive 5.23% 8.34% 6.85% 5.89% True Negative 95.85% 97.09% 99.44% 99.03% False Negative 4.79% 5.23% 24.86% 28.45%  Precision 94.60% 92.03% 91.69% 92.64% Recall (Sensitivity) 95.03% 94.85% 75.24% 72.25%  Specificity 94.83% 92.09% 93.56% 94.39% Accuracy 94.93% 93.44% 84.66% 83.45% F1 Score 94.81% 93.42% 82.65% 81.19%  TABLE V. ANALYZING THE EXTRACTED TERMS FROM THE SQL  Linear Search Parallel Search Pessimistic Search Optimistic Search  True Positive 97.33% 97.81% 93.74% 94.55% False Positive 1.06% 1.01% 0.40% 0.20% True Negative 99.97% 99.91% 99.93% 99.90% False Negative 0.01% 0.01% 0.11% 0.11%  Precision 98.92% 98.98% 99.58% 99.79% Recall (Sensitivity) 99.99% 99.99% 99.88% 99.88%  Specificity 98.95% 99.00% 99.60% 99.80% Accuracy 99.46% 99.49% 99.74% 99.84% F1 Score 99.45% 99.48% 99.73% 99.84%     The Linear Search and Parallel Search however shows consistent result since the focus is on the entire specified data source. The false negative values are small and the true positive index is also high. An observation however is that, the false positive and false native results in the Linear Search also re-occur in the Parallel Search. This can be attributed to the fact that the Linear Search is the underlying methodology on which the Parallel Search is running.

In Table IV, the result from crawling some selected websites are shown. This websites are for health care providers. Using similar types of terms from the NoSQL, we perform similar searches on the Web. The accuracy index dropped because there were higher false positive and false values. This is more likely due to the fact that the web is wide with similar written terms but with different meanings. Since not all the keywords from the web are captured in the Dictionary/Thesaurus, most of the specified terms were mislabeled.

In Table V, we report the results from the search in the SQL database. The data here is a fraction of the NoSQL version. The results in the structured data source show very high accuracy in comparison to the rest of the data sources.

This is not because the proposed framework works better with structured data source but, because the data itself is very small in comparison to the rest. Further, we know where to find most of the terms we were looking for so narrowed our database choice in the case of the Pessimistic and Optimistic Search methodologies.



VI. CONCLUSION With the present direction of corporate transactions, it is  evidently clear that ?Big Data? has come to stay. This is because most traditional business transactions which use to be paper-based are all being digitized. Besides, user generated content across different spectrum of the enterprise landscape is increasing in volume at an exponential rate. While Big Data has its tremendous advantages, the fact that the data is heterogeneous (i.e., variety) poses new challenges.

Previously, the data mining and knowledge discovery in database (KDD) procedures were designed for schema-oriented databases. However, the data today is multifaceted and has no schema. Though the NoSQL databases have been proposed to accommodate the data, not many tools are available that performs data mining and analytics from such storages.

Our work details an Analytics-as-a-Service (AaaS) tool that performs terms mining and analytics from multiple data sources. Currently, the following data sources are supported: NoSQL (e.g., document style storages such as CouchDB), SQL, Tag/Flat file storage (HTML, XML, RTF), and Textual Documents (e.g., PDF). Furthermore, we conducted experiments using the various styles of storage such as NoSQL, Web, and SQL. The pilot testing shows an encouraging result especially from the NoSQL domain. Our future extension aims at incorporating learning and adaptability features. This will further facilitate the ability to do recommendations for users based on their term and topic mining requirements.

