Non-Almost-Derivable Frequent Itemsets Mining

Abstract  The number of frequent itemsets is often too large to  handle, so it is very necessary to work out a condensed  representation of the collection of all frequent itemsets.

In this paper, we propose a new condensed representation  called frequent non-almost-derivable itemsets. This repre-  sentation is a subset of the original collection of fre-  quent itemsets. For any removed itemset X(which is  called an frequent almost-derivable itemset), we can  derive a lower and an upper bound of its support from  this representation, and the lower bound and the upper  bound  is close enough(can be controlled by a user-  defined parameter). We also propose an Apriori-like  algorithm, which can extract all frequent non-  derivable itemsets. Extensive empirical results on real  datasets show the compactness and good approxima-  tion of this representation.

1. Introduction  The frequent itemsets mining plays an essential  role in many important data mining task, e.g., associa-  tion rules mining [1]. However, the number of frequent  itemsets is often huge, which greatly affects the effi-  ciency and effectiveness of frequent itemsets mining.

Since the collection of frequent itemsets often  show redundancy, one solution to this problem is  based on the following idea: Instead of mining all fre-  quent patterns, we only extract a particular compact  subset of the frequent itemsets. The size of this subset  is often far smaller, but it keeps. This kind of subset  can be called a condensed representation of the set of  frequent itemsets. By mining some kind of condensed  representation, we can greatly reduce the number of  itemsets to be mined, stored and analyzed, while al-  lowing derivation and support determination of all  frequent itemsets without accessing the original data-  base [2,3,7].

In this paper, we introduce a new condensed rep-  resentation. We defined a structure called almost-  derivable itemsets. For any almost-derivable itemset,  we can derive a lower bound and an upper bound of its  support by the support of its subsets. We show the  collection of all frequent non-almost-derivable  itemsets can be taken as a condensed representation of  the collection of all frequent itemsets. We verify our  approach in two real-life datasets. The experiments  results show the compactness and good approximation  of this representation.

The organization of the paper is as follows. In the  next section, we introduced some preliminary knowl-  edge,   especially the way how support is deduced. In  section 3, we propose a structure and a condensed rep-  resentation. In section 4 we give an algorithm extract  this condensed representation. Section 5 discusses the  empirical results and the related works are discussed in  section 6 briefly.

2. Basic notions and support deducing  Let I = {x1, ..., xn} be a set of items. A transac-  tion T is a non-empty subset of I. A transaction data-  base D is a set of transactions. An itemset X is a set of  items. Itemset X is called an m-itemset iff the number  of items in X is m. We say itemset X is contained in a  transaction T iff  X T. The support of an itemset X in  database D, denoted by su(D,X), is the number of  transactions in D that contain X. Given a user-defined  minimum support threshold min_sup>0, an itemset X  is called frequent iff su(D,X) min_sup. The frequent  itemset mining problem is to find the full collection of  frequent itemsets, denoted by Freq(D,min_sup)(Freq  in short when D and min_sup is clear from the context).

Next we?ll show how to compute the lower and  upper bounds of the support of an itemset by the sup-  port of its subsets using the well-known inclusion-  exclusion principle [12]. This method is also illustrated  by Calder in [8].

Definition 1 (Generalized itemset) A generalized itemset WVG is a conjunction of items and nega-  tions of items. A transaction contains a general itemset  WVG means all the items of V appeared in the  transaction and none of the items of W appeared in the     transaction. The support of a generalized itemset G,  denoted by su(G), is the number of transactions con-  tains the generalized itemset. We say that a general  itemset WVG is based on itemset X iff  WVX . We call WVG  an odd generalized  itemset iff |W| is odd; otherwise it is called an even  generalized itemset.

For a general itemset WVG  based on X,  )()1()()1(  )()1()(  |||\|  |\|  XsuYsu  YsuGsu  w  XYV  VY  XYV  VY  So  )()1()()1()( ||1|\| GsuYsuXsu w  XYV  YX .

So, we have  oddis)()1()(  evenis)()1()(  1|\|  1|\|  GYsuXsu  GYsuXsu  XYV  YX  XYV  YX  So the term  XYV  YX  X YsuWVGS )()1()( 1|\|  is a lower (|W| is even) bound or an upper (|W| is odd)  bound of su(X), depending on the parity of |W|.

Definition 2.

dd}ois|||)(max{  }evenis|||)(max{  WWVGSUB  WWVGSBL  XX  XX  As is shown in [7], the bound is tight, i.e., for every  smaller interval ],[],[ '' XX UBLBublb , there doesn?t  exist such a database such that the support of each sub-  set of X is satisfied, but ],[)( '' ublbXsu .

3. Frequent non-almost-derivable itemsets as a condensed representation  Given an itemsets X, as is illustrated in section 2,  we can use the support of all its subset to compute a  lower bound LBX and an upper bound UBX of its sup-  port. If UBX -LBX , where  is a user-defined error-  tolerance threshold, this means we can get enough  good approximation of its support. So we don?t need to  mine and store the support information in our collec-  tion because this information can be inferred from the  collection.

Definition 3 (almost-derivable itemset, non-almost- derivable itemset) Given a user-defined error-tolerance threshold 0, itemset X is called an almost-derivable  itemset iff UBX -LBX ; otherwise it is called a non-  almost-derivable itemset.

The collection of non-almost-derivable itemsets  with error-tolerance threshold  is denoted by NAD( ),  in brief NAD if the  is clear in the context. Similarly,  the collection of almost-derivable itemsets is denoted  by AD( ), in brief AD. Clearly, NAD AD= .

Lemma 1 Let X be an almost-derivable itemset, there must exist an even generalized itemset G1 and an odd  generalized itemset G2 based on X, su(G1)+su(G2) ,  and vice verse.

Proof: There exist an even generalized itemset G1 and an odd generalized itemset G2 based on X, LBX =su(X)  - su(G1), UBX =su(X) + su(G2). So UBX -LBX = su(G1)  + su(G2). So X AD( )  UBX -LBX su(G1)+  su(G2) .

Theorem 1 (Anti-Monotonicity) Let X be an itemset, if )(ADX , then Y X, Y AD( ) .

Proof: According to lemma 1, there exist an even gen- eralized itemset G1 and an odd generalized itemset G2 based on X, UBX -LBX = su(G1) + su(G2) . Y X,  Let G1?= G1  (Y \ X), G2?= G2  (Y \ X) . G1? and G2?  are both based on Y. UBY ?LBY su(G1?) + su(G2?)  su(G1) + su(G2) . For G1? is an even generalized  itemset and G2? is odd, so Y AD( ) .

Corollary 1 If X NAD, Y X, Y NAD.

This corollary is immediate from the theorem 1.

Definition 4 (frequent almost-derivable itemset, fre- quent non-almost-derivable itemset) Itemset X is called a frequent almost-derivable itemset iff X is al-  most-derivable and also frequent. Similarly, X is called  a frequent non-almost-derivable itemset iff X is non-  almost-derivable and also frequent.

Given a error-tolerance threshold 0 and a mini-  mal support threshold min_sup, the collection of all  frequent non-almost-derivable itemsets is denoted by  FNAD( ,min_sup), in brief FNAD if  and min_sup is clear from the context; similarly, the collection of all  frequent almost-derivable itemsets is denoted by  FAD( ,minsup), in brief FAD.  Clearly, FNAD = NAD Freq, FAD = NAD Freq, FNAD FAD = Freq.

Corollary 2 If X FNAD, Y X, Y FNAD. If X FAD, Z X, Z FAD.

This corollary tells the anti-monotonicity property  of frequent non-almost-derivable itemset, which is  very important for the mining algorithm.

Theorem 2 Let X be an itemset. If UBX - su(X) /2 or su(X) - LBX /2, then Y X, Y AD( ).

The proof is omitted for the limit of space.

Now we show the collection of frequent non-  almost-derivable itemsets can be taken as a condensed  representation, which can be used to answer frequency  queries of any itemset approximately.

The support of the any itemset X can be inferred  from FNAD as follows:  1. If X FNAD, then return its support directly.

2. If X is not in FNAD but all its subsets do,  then compute XLB and XUB . If UBX -LBX>  or UBX<min_sup, then X must be infrequent;  otherwise return its bounds.

3. If some of its subsets are not in FNAD,  compute the support of these subsets recur-  sively first. If some of them are found out  infrequent, then X is also infrequent; other-  wise compute the support bounds of X. No-  tice in this case, the supports of some of its  subsets are given in bound interval. Either  the lower or upper bound is used, based on  different sign of it in the inequalities. The  bounds of X computed by this way is de-  noted by lbX and ubX. If ubX<min_sup, then  X is infrequent, otherwise return its bounds.

Case 3 may lead to two side-effects. One is ap-  proximate error accumulation, i.e., it is possible for  some long itemsets, its bound interval are larger than .

In section 5, experiments show the effect of error ac-  cumulation is very acceptable. Another side-effect is  that the supports of some ?borderline? infrequent item-  sets may fail to be determined infrequent. But this can  not be a big issue because the error-tolerance threshold  is usually much less than the minimum support thresh-  old. Further, in a somehow extreme condition, even we  care enough to known exactly whether it is frequent or  not, an extra database scan can always be made to  check these borderline itemsets.

4. Discovering frequent non-almost- derivable itemsets  In this section, we give an Apriori-based levelwise  algorithm [1,2] finding out the collection of frequent  almost-derivable itemsets.

Since frequent non-almost-derivable itemsets has  the anti-monotonic characterize, some prune technique  similar with the well-known prune technique in Apri-  ori can be employed during the mining process, which  means if X can be removed from the collection, then all  of its supersets can be removed as well.

The process of this algorithm is very similar with  Apriori. The main difference is that there is an addi-  tional check step in FNADMINER. After the candi- dates are generated, the lower and the upper support  bounds of them are computed and checked. For a can-  didate X, if its upper bound is below the minimal sup-  port threshold, then it cannot be frequent, so it is  pruned. If UBX -LBX , then X is an almost-derivable,  so it is also pruned.

Algorithm 1 FNADMINER  Input: Transaction database D, minimal support threshold , maximal error bound  Output FNAD(D, , )  ;  ;1:  };)(,{:  and  andcompute  ;:  ;to1levelofcandidatesgenerate  };)(|(,{:  ;1:};1{:;:       FNAD  ii  XSuXCC  -LBUBUB  UBLB  CCX  C  CCi  XsuCXXsuXFNADFNAD  C  iitemsetsCFNAD  ii  XXX  XX  i  i  i  i  i  output  while;end  for;end  if;end  thenif  ;  doallfor  dowhile  Theorem 3 (Correctness) Algorithm FNAD- MINER find out the collection of all frequent non- almost-derivable itemsets correctly.

6. Experiments  Our experiments are conducted on two real-life data-  sets, Mushroom and Chess. They are both obtained  from the machine learning repository of UC Irvine1.

The dataset Mushroom contains 8124 transactions,  each transaction contains 23 items, and 120 items in  total. The dataset Chess contains 3196 transactions,  each contains 37 items, and 75 items in total.

6.1 Collection size  Table 1 shows the size of original collection of fre-  quent itemsets and the collection of frequent non-  almost-derivable itemset at different error bound on  the   dataset Mushroom; table 2 are results on the data-  set Chess. It can be observed that the size of this repre-  sentation is far smaller than the original collection.

Besides the size of collection, the maximal itemset  length is also reduced substantially. The detail infor-  mation of the max itemset length is omitted from this  paper because of the limit of space.

1 http://www.ics.uci.edu/~mlearn/MLRepository.html     minsup 15% 20% 25% 30% 35%  |Freq| 98575 53583 5545 2735 1189  =0 2153 1319 661 421 302  =10 2034 1242 627 401 285  =20 1932 1190 605 390 277  =30 1899 1166 602 389 276  =40 1855 1139 598 388 275  =50 1745 1090 580 379 270  Table 1: Collection size on Mushroom dataset  Figure 1    Maximal absolute error on Mushroom  Figure 3    Average relative error on Mushroom  Figure 5   Error w.r.t. itemset length on Mushroom ( min_sup=35%, =20)  minsup 85% 80% 75% 70% 65%  |Freq| 2669 8227 19951 48969 111778  =0 446 684 1042 1527 2251  =10 296 462 699 1016 1424  =20 233 381 562 826 1171  =30 180 304 454 688 1000  =40 176 292 438 664 966  =50 161 177 411 627 916  Table 2: Collection size on Chess dataset  Figure 2 Maximal absolute errors on Chess  Figure 4    Average relative error on Chess  Figure 6 Error w.r.t. itemset length on Chess ( min_sup=70%, =50)     6.2 Approximation error  The absolute error is defined as the interval width be-  tween the lower and upper support bounds of a fre-  quent almost-derivable itemset. The relative error is  the absolute error divided by its support. The average  relative error is the total relative error divided by the  number of frequent almost-derivable itemsets.

Figure 1 and figure 2 show the maximal absolute  error under different support thresholds and different  error-tolerance thresholds. From these two figures, we  can see the maximal absolute error is usually only sev-  eral times of the error-tolerance threshold . Since the  error-tolerance threshold is often very small, the  maximal error is in fact very controllable.

Figure 3 and Figure 4 show average relative error.

Figure 3 shows the relative error on Mushroom is often  below 0.4%. The results on Chess are a little bigger,  because there are 3196 transactions in Chess, fewer  than 8124 in Mushroom, and therefore the same error  threshold is in fact relative larger for Chess.

Figure 5 and figure 6 show the error distribution  with respect to the length of itemsets. For the Mush-  room dataset, when min_sup=35% and =20, as can be  seen from figure 5, with the itemset length grows from  2 to 4(there are no frequent almost-derivable itemsets  with length less than 2), the error grows accordingly  because of error accumulation. But when itemset  length grows continually, the error drops instead. This  is because the longer an itemset be, there are more  generalized itemsets based on it, thus possibly it can  get tighter bound of its support with more inequalities.

This fact also prevents the error from accumulating too  much for long itemsets.

7. Related work  The concept of closed set was firstly proposed by  Parquier in [4]. A closed set is an itemset such that its  support does not equal the support of any of its super-  sets. For any non-closed set, the support can be in-  ferred by the maximal support of its closed subsets.

Free-sets [5], introduced by Boulicaut and Bykowski, is  an approximate representation. An itemset X is said to  be a -free-sets iff there doesn?t exist its subset Y, the  support of Y is close with the support of X within a  user-defined threshold . Disjunction-free sets [6], also  proposed by Bykowski, is an itemset which have no  disjunction rule based on it. Non-derivable itemsets [9]  can be seen as a special case of our non-almost-  derivable itemset when error-tolerance threshold is 0.

Generators [11] are itemsets whose support does not  equal the support of any of its subsets. Other con-  densed representations have also been introduced by  researchers, such as disjunction-free generators [110]  introduced by Kryszkiewicz; Bd and Bm by Pei[7].

8. Conclusion  In this paper, we introduce two structures called  almost-derivable itemset and non-almost-derivable  itemset. We show the collection of frequent non-  almost-derivable can be server as a condensed repre-  sentation of the collection of all frequent itemsets. We  test our approach on two real-time datasets and the  experiments show: (1) The size of our representation is  very small; (2) The representation can give a very  good approximation of the original collection.

