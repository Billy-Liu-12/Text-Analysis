Scalability of OATl

Abstract  Mining user access patterns from clickstream data has attracted much attention from the research community.

However, the scalability testing of corresponding mining algorithms has been virtually ignored. Memory requirements of these algorithms may be quite large due to the fact that in-memory data structures whose size depends on the number and length of patterns is often assumed. Due to the importance of the scalability of algorithms to the usefulness of the Web Usage Mining (WUM) techniques, we propose two new sampling techniques, continuous and random, which can be applied to static sized test datasets to examine WUM algorithm  scalability. We illustrate the usefulness of these scalability approaches by performing scalability tests using the  Online Adaptive Traversal (OAT) pattern mining algorithm. These experiments show that indeed the OAT  algorithm adjusts to the amount of memory and time requirements grow at a linear rate.

1. Introduction  Mining Web access patterns in clickstream data has recently attracted much research interest. Research in this area includes association rule mining [ 1], sequence mining [3][9], and more recently tree mining [ 1 1] [ 12].

In previous research we have developed the OAT pattern mining algorithm for clickstream data that determines Maximal Frequent Sequences (MFS) [ 10] [9].

By applying the OAT algorithm to a web log, it can incrementally examine and mine the continuously arriving sessions by storing the sub(sequences) in a suffix tree. A unique feature of OAT is that the suffix tree can be compressed by using local pruning or cumulative pruning based on the availability of main memory. In this work we examine the scalability of OAT. Scalability is crucial  1 This material is based upon work supported by the National Science Foundation under Grant No. nS-020S741.

Y ongqiao Xiao  Dept. of Math & Computer Science Georgia College and State University  Milledgeville, Georgia 31061  yxiao@gcsu.edu  to the usefulness of WUM techniques, however, many algorithms assume the use of memory resident data structures. This obvious conflict should be addressed by algorithms which scale and adapt to the available main memory [10]. Although this research targets the OAT algorithm, the techniques used to perform the scalability experiments are applicable to other algorithms as well.

We propose two new sampling techniques, continuous and random, which facilitate the testing of scalability with a fixed size Web log. Experiments show the usefulness of these scalability approaches.

In the next section we provide a brief survey of previous related work including OAT. We then examine our proposed scalability techniques that are based on sampling and discuss some of the problems associated with performing WUM scalability experiments in section 3. In Section 4, we discuss the results of the OAT scalability experiments. The paper is concluded with an overview of future work.

2. Overview of OAT and Related Work  The OAT algorithm was created in response to the need for an algorithm that would not be hamstrung by lack of memory [ 10]. Many previous algorithms would not adapt to the amount of memory that was available and the user would need to either have enough memory available for the algorithm or would need to insure that the algorithm would not use more memory then was allocated to it [2]. The OAT algorithm gets around the memory limitation by adjusting its need of resources based upon the memory it finds available. OA T uses a Suffix Tree to store and determine Maximal Frequent Sequences. Maximal Frequent Sequences should not be confused with Maximal Forward References. Maximal Forward References possess the same idea, except they do not include backward browsing [7]. The primary advantage of the OAT algorithm is that it processes    within the limitations of the memory by compressing the Suffix Tree when it runs out of available memory. After compression, the algorithm continues reading sessions and adding patterns to the suffix tree [10][9].

The OAT algorithm reads the Web log sequentially and views the log as being divided into contiguous sections (or partitions). The important thing to know about compression is that it happens only at the end of each partition. Thus the number of partitions equals the number of compressions. During compression, sequences that are not Maximal Frequent Sequences are removed from the Suffix Tree. Generally, the more compression we perform, the smaller the tree will be. It is important to point out, however, that the OAT algorithm still finds all MFSs even when the tree is compressed.

153-16 150 146 -17 152155 -IS  PatiltiOll 1  152155155155152 156 152 157158147 ISS 15'7 152 -19 144 -20 159160161 162 163 164165 -21  Paliltion 2  15'7 -22 152151 157150146 153 155 166 167156 -23 PatiltiOll 3  Figure 1. Local vs. Global Compression  There are two types of compression: Local and Cumulative. Local Compression takes only the last partition processed and determines which patterns are Maximal Frequent Sequences based only on the last partition processed. For example, if there was a sessionized file that looked like Figure 1 and we divided it into partitions (as shown by the dotted lines). Local Compression would only consider Partition 1 during the first compression, only Partition 2 during the second compression, and so on. Cumulative Compression considers all the patterns in the tree from all the previous partitions as well as the partition just processed. It then compresses the tree using patterns found using all the data read in so far. So using our above example: during the first compression Cumulative Compression looks at Partition I, then during the second compression it looks at the combination of Partitions I and 2, then during the third compression it looks at the combination of Partitions I, 2 and 3, and so on. Regardless of the type of compression used, the parts of the tree that get removed are only the infrequent sequences.

The OAT algorithm is influenced by work done by Chen, Park, and Yu [3] [4], especially in the use of the term Maximal Frequent Sequences. Chen, Park, and Yu created two algorithms to handle the mining of Large Reference Sequences: Full-Scan and Selective-Scan.

Large Reference Sequences are paths taken by users that  appear "a sufficient number of times in the database" [4] (the database being the web log). Maximal Frequent Sequences resemble Large Reference Sequences in most cases, except that Maximal Frequent Sequences contain backward references. Backward references are times when a user visits a page they have already visited.

Before the Full-Scan and Selective-Scan algorithms are run, however, an algorithm (called MF) is used to convert raw log data into traversal subsequences. Both the Full? Scan algorithm and the Selective-Scan algorithm use hashing and pruning to reduce the size of the data they need to handle. The Selective-Scan algorithm has the advantage over Full-Scan because it reduces the number of scans of the data. However, Selective-Scan needs to be able to store its discoveries in memory [4]. This is where the OAT algorithm surpasses it, in being able to work within the memory constraints that are placed upon it.

The field of Path Traversal can also be used, not just to determine the most frequently used paths through a web site, but to determine where a user intended to go [8].

This is accomplished by analyzing the web log for backward references and then comparing the path taken to the page where the user ended. In addition, the more closely tied to OAT algorithm is the idea of using this web log data to determine user profiles [5]. The similarity between paths taken by different users through the web site is computed by clustering algorithms. In this way a web site can deliver more personalized content to its users.

3. Scalability Testing via Sampling  Given the huge size of a Web log, especially Web logs of popular websites, scalability of WUM algorithms is a key issue, as the amount of memory resources needed to apply the algorithms is large and may exceed the available main memory. However, very little previous research in this area can be found from available literature.

To date most scalability testing involves fixed size Web logs. Certainly sets of larger Web logs can be examined, but the performance of a WUM algorithm depends not only on the size of the Web log, but its contents. For example two logs with the same number of entries (clicks) could represent completely different numbers of sessions.

A major key to scalability testing is to obtain a large amount of data. Our approach is to perform tests by obtaining samples from a fixed size input dataset. In this manner very small and very large (even larger than the given dataset) datasets can be obtained. Time and space requirements can then be measured by running multiple experiments on the generated datasets. However, looking at the time or space complexity is not sufficient. If the assumption is made that the amount of memory available is large enough to hold the needed in-memory data    structure, then the ability of an algorithm to run successfully with a smaller fixed memory size must also be examined.

To illustrate the problems that exist with scalability testing, we describe the performance of the initial sets of experiments. An original dataset covering accesses to the SMU Engineering School Web pages during the time period from February to June 2000, or approximately 150 day and nearly 1.3 gigabytes, was selected. To test scalability we generated larger datasets by simply concatenating the file with itself. Thus we obtained datasets that were n times larger than the original one where n was provided as input. The problem with this approach was two fold:  1. Smaller datasets were not obtained.

2. The larger datasets were exact duplicates of the  original ones. This means that no new (different) patterns would be found. The memory demands to thus find patterns would not grow at all. This is not what would happen as Web logs grow.

