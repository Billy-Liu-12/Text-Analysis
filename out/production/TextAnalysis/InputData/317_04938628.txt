

Abstract?In this paper, a distributed Expectation Maximization (EM) algorithm is proposed for estimating parameters of a Gaussian mixture model in a peer-to-peer network. This algorithm is used for density estimation and clustering of data distributed over nodes of a network.

Scalability and fault tolerance are two important advantages of this method. In the E-step of this algorithm, each node calculates local sufficient statistics using its local observations.

A peer-to-peer algorithm is then used to diffuse local sufficient statistics to neighboring nodes and estimate global sufficient statistics in each node. In the M-step, each node updates parameters of the Gaussian mixture model using the estimated global sufficient statistics. The proposed method is then used for environmental monitoring and also distributed target classification. Simulation results approve promising performance of this algorithm.



I. INTRODUCTION DVANCES in network technology, like peer-to-peer networks on the internet or sensor networks, have  highlighted the need for efficient ways to deal with large amounts of data that are distributed over a set of nodes.

Examples are financial data reported on the internet, weather data observed by a set of sensors, etc. In particular, in many data mining applications we are interested in extracting parameters of a global model from experimental data through learning. Estimating a probability distribution or clustering the data, without transferring all the data to a central unit are some of examples [1].

Distributed Data Mining (DDM) has recently emerged as an extremely important area of research [2]. The objective, here, is to perform data mining tasks (such as association rule mining, clustering, classification) on a distributed database, that is, a database distributed across several sites (nodes) connected by a network. Research in this field aims at mining information from such databases while minimizing the amount of communication between nodes. For example, an algorithm for distributed association rule mining in peer- to-peer systems has been presented in [3]. Techniques for performing non-parametric density estimation over homogeneously distributed data have been studied and experimentally evaluated in [4]. A collective approach has also been presented in [5] for learning a Bayesian network from distributed heterogeneous data.

B. Safarinejadian is with the Electrical Engineering Department, Amirkabir University of Technology, Tehran, Iran (e-mail: bsafari@aut.ac.ir).

M. B. Menhaj is with the Electrical Engineering Department, Amirkabir University of Technology, Tehran, Iran (e-mail: tmenhaj@ieee.com).

M. Karrari is with the Electrical Engineering Department, Amirkabir University of Technology, Tehran, Iran (e-mail: karrari@aut.ac.ir).

The EM (Expectation Maximization) algorithm [6-11], is an important method of density estimation in which some of the variables are assumed to be missing or unobservable. A good reference on EM can be found in [12]. A distributed EM algorithm has been developed in [13] for density estimation in sensor networks assuming that the measurements are statistically modeled by a mixture of Gaussians. This algorithm requires that the global summary quantities be transmitted through the entire network. It will slow down the algorithm when the network becomes complex. Furthermore, since the algorithm needs to access all nodes in each updating step, its fault tolerancy is poor. In other words, in the case that one node fails, the algorithm will be stopped.

Assume that the data set distributed over the nodes of a network can be modeled by a finite mixture model. Here, we propose a distributed EM algorithm to estimate the parameters of this mixture in a peer-to-peer network. In the standard EM algorithm for the Gaussian mixtures, the local sufficient statistics can be calculated only using local data in each node in the E-step, while the global sufficient statistics is required in the M-step. Our proposed distributed EM algorithm calculates the local sufficient statistics in the E- step as usual first. Then, it estimates the global sufficient statistics based on a peer-to-peer distributed averaging approach. Finally, the parameters are updated in the M-step using the estimated global sufficient statistics. Since the peer-to-peer averaging approach is a local algorithm, i.e.

each node only needs to communicate with its neighbors and gradually gains global information, this distributed algorithm is scalable. The equations of parameter estimation in this algorithm are not related to the number of sensor nodes.

Thus, it is also robust. Failures of any nodes do not affect the algorithm performance given the network is still connected.

Finally, the estimated parameters can be accessed from any nodes in the network. The proposed method can also be used as a general distributed data mining algorithm for density estimation and clustering of the data distributed over the nodes of a network.

The rest of the paper is organized as follows. Section II introduces the EM algorithm for Gaussian mixture learning.

Section III proposes the distributed EM algorithm based on the peer-to-peer averaging approach. Section IV is devoted to show performance of the proposed algorithm based on simulated data sets. Finally, section 5 concludes the paper.



II. EM ALGORITHM FOR GAUSSIAN MIXTURES Consider a network of M nodes and a d-dimensional  A Fault Tolerant Peer-to-Peer Distributed EM Algorithm Behrooz Safarinejadian, Mohammad B. Menhaj, and Mehdi Karrari  A          random vector 1 , , Td  m m mY Y? ?= ? ?Y ?  which corresponds to  node m. Each data (measurement) 1, , ,, , Td  m i m i m iy y? ?= ? ??y  of the node m is a realization of the random vector mY .

Assume that distribution of the measurements is represented by a Gaussian mixture model as:  ( ) ( ), , ,  | | , J  m i m j m i j j j  p ? =  =?y ? yN ? ?  (1)  where ( )| ,?N ?y  denotes the evaluation of a Gaussian density with mean ?  and covariance ?  at the point y .

{ }, 1 J  m j j ?  = are the mixture probabilities at node m,  { },j j j=? ? ?  is the set of parameters defining the jth component and J  is the number of mixture components.

The mixture probabilities { },m j?  may be different at various nodes while the parameters j? are the same throughout the network. The set of data points of the mth  node is represented by { }, 1 mN  m m i i= =Y y  in which  mN  is  number of measurements at node m. It is assumed that measurements of each node are independent and identically distributed.

The data at each node are assumed to be statistically independent in this paper, but this assumption can be relaxed. If the data are (spatially or temporally) correlated, then the simple "independent" likelihood model can still be employed by interpreting it as a pseudolikelihood [14].

Under mild conditions the maximum pseudolikelihood estimates tend to the true maximum likelihood estimates as the number of data tends to infinity.

Consider a set of missing variables  { },m i= zZ corresponding to { },m i= yY . Each  , , ,, , J  m i m i m iz z? ?= ? ??z  is a binary vector indicating by  which component the data  ,m iy  is produced.  We would say  ,m iy  is produced by the jth component of the mixture if for all r j? , , 0  r m iz =  and , 1  j m iz = . The pair  ( ), , ,,m i m i m i=x y z  is regarded as the complete data and we write { },=X Y Z  in which { },m i= xX .

Assume that all nodes observations are sent to a central unit where a standard EM algorithm is used to estimate the parameter set ? . The log-likelihood for the observed data is defined as  ( ) ( ), 1 1  log | mNM  m i m i  p = =  = ??L ? y ?  ( ), 1 1  log | mNM  m i m i  p = =  =?? y ?  ( ), , 1 1 1  log | , mNM J  m j m i j j m i j  ? = = =  ? ? = ? ?  ? ? ?? ? yN ? ?  (2)  Define { }  Jt t j j=  =? ? , the set of parameters at the t-th iteration of the EM algorithm. In the standard EM algorithm, given observation y and current parameter set t? , the conditional expectation of joint distribution ( ), |p y z ? is defined as  ( ) ( ); log , | | ,t tzQ E p? ?= ? ?y z? ? ? ?Y EM (Expectation Maximization) is an iterative algorithm  to obtain the maximum likelihood estimate of the finite mixture parameters.  At the E-step of the EM algorithm, the Q function is calculated and at the M-step, the parameter vector ?  is estimated such that the Q function is maximized.

Since  ( ) ( ) ( ) ( )  , , ,  , ,  , | | , | ,  | ,  m i j j m i m i  m j m i  p p p  p  ?  ? ?  = ?  = ?  y z z y y  y  ? ?   the Q function can be written as  ( ) ( ) ( )  ( )  ( )  , , 1 1 1  , , 1 1 1  ,  ; log , | | ,  log | ,  | ,  m  m  NJ M t t  m i j j m i j m i  NJ M  m j m i j m i  j j  t j m i  Q p p  p  p  ? ?  = = =  = = =  =  = ?? ?? ?  ?  ???  ???  ? ? ? ?  ?  y z z y  y  z y    In the E-step, the conditional expectation ( ); tQ ? ?  is calculated using  ( )1, , ,| ,t tm i j j m iw p+ = ?z y ( )  ( ) , , , ,  , , , ,1  | ,  | ,  t t t m j m i m j m j  J t t t m n m i m n m nn  ?  ? =  = ?  y  y  N  N  ?  ?  ?  ? (3)  and in the M-step, the parameter set is updated by maximizing  ( )1 arg max ;t tQ ?  + =? ? ?   (4) Based on Equations (3) and (4), the Gaussian mixture parameters are obtained using  1 1 , , ,   1 mNt t m j m i j  im  w N  ? + + =  = ?   (5)         , , ,  1 1 1  , ,  1 1  m  m  NM t m i j m i  t m i j NM  t m i j  m i  w  w  +  + = =  +  = =  = ??  ??  y ?   (6)  ( )( )1 1 1, , , , 1 1 1  , ,  1 1  m  m  NM Tt t t m i j m i j m i j  t m i j NM  t m i j  m i  w  w  + + +  + = =  +  = =  ? ? = ??  ??  y y? ? ?  (7)  Equations (5)-(7) can also be written in a compact form as  ,  1 1  ,   M t m j  t m j M  t m m j  m N w  +  + =  +  =  = ?  ?  a ?   (8)  ( ) ,  1 1 11  ,   M t m j Tt t tm  j j jM t  m m j m  N w  +  + + +=  +  =  = ? ?  ?  b ? ??  (9)  in which the local sufficient statistics are defined by  , , ,  mN t t m j m i j  i  w w =  =?   (10)  , , , ,  mN t t m j m i j m i  i  w =  =?a y   (11)  , , , , ,  mN t t T m j m i j m i m i  i  w =  =?b y y   (12) The global sufficient statistics are also defined as  ,  M t t j m m j  m w N w  =  =?   (13)  1 1 ,   M t t j m j  m  + +  =  =?a a   (14)  1 1 ,   M t t j m j  m  + +  =  =?b b   (15) Using the global sufficient statistics, the estimated parameters can also be calculated using    t jt  j t jw  + +  += a  ?   (16)  ( )  1 1 1  t Tjt t t  j j jt jw  + + + +  += ? b  ? ??   (17)

III. PEER-TO-PEER DISTRIBUTED EM ALGORITHMS Here, a distributed EM algorithm is developed in which  an approximate EM algorithm is performed locally and in a peer-to-peer network. At this algorithm, peers perform the  computations in parallel and its scalability is very good.

In the standard EM algorithm, it can be found that the  local summary quantities can be calculated locally, while the global summary quantities are proportional to the averages of the local summary quantities from all nodes in (13)-(15).

This view can be made more clear by defining average of the local summary quantities as follows  ,  1 Mt t j m m j  m w N w  M = = ?   (18)  ,  1 Mt t j m j  mM = = ?a a   (19)  ,  1 Mt t j m j  mM = = ?b b   (20)  Using these average quantities, the Gaussian mixture parameters can also be calculated according to  t jt  j t jw  = a  ?   (21)  ( ) t  Tjt t t j j jt  jw = ? b  ? ??   (22)  t jt  j m  w N  ? =   (23)  where mN  is the average number of data points at each node. Due to the average expressions in (18)-(20), using a peer-to-peer average computation algorithm, the global summary quantities can be estimated through information diffusion over the network. Each node exchanges the local summary quantities with its neighbors and estimates the global summary quantities based on neighbor?s local summary quantities.

Here, we propose an iterative method based on message exchange between directly connected peers to perform EM algorithm approximately and locally. The proposed distributed EM algorithm works as following. First, the algorithm is initiated with a set of randomly chosen initial conditions distributed over all peers. Afterwards, at each iteration t , each peer runs a two step process. In the first step, m-th peer mP  calculates its local sufficient statistics according to (10)-(12). Then mP  stores these local sufficient statistics and waits for answering queries from its neighbors.

In the second step, peer mP  sends a poll message, consisting its id and current iteration number to its immediate members and waits for their responses. Each response message from a neighboring peer contains the locally updated sufficient statistics of that peer for t-th iteration. Once all immediate neighbors of mP  have responded or cease to be neighbors,  mP  updates its jth sufficient statistics by taking an average (or a weighted average) of all the sufficient statistics         received from other peers and also its own. If number of data points are not the same at different peers, a weighted average may also be used. In this case, weights will be number of data points at each node. After the averages , ,t t tj j jw a b  are calculated, means and covariances are estimated using (21)- (23). The above process is repeated until convergence is reached.

Convergence criterion may be the maximum change in parameter values or likelihood variations in two succeeding iterations. Since the log-likelihood L  is sum of local terms, it can also be computed using the same protocol. For example, local log-likelihood may initially be calculated at each node using  ( ) ( ), , , , 1 1  log | , mN J  t t t t m m j m i m j m j  i j ?  = =  ? ? = ? ?  ? ? ? ? yL ? ?N ?  Using the same distributed approach, likelihood average of the peer m and its neighboring nodes can be computed.

Whenever the likelihood variations become less than a convergence threshold, the algorithm will terminate.

Note that other than executing the above steps, each peer should also respond to the poling messages received from its neighbors. Suppose peer mP  receives a poling message from peer nP  during its iteration t . The message corresponds to iteration t?  at peer nP . If t? t?  then mP sends its local sufficient statistics and counts from iteration t? . Otherwise, this poll message is placed in a queue. mP  will check this queue at every iteration and respond to any message it can. Any peer, mP , can enter a terminated state at the end of iteration t  if (for example) its likelihood value stops changing significantly. Whenever all peers enter the terminated state, there will be no communications which shows that the algorithm has terminated.

A. Algorithm B At the algorithm introduced above, E-step of the EM  algorithm is performed in a distributed manner but the M- step is done locally at each node. Another approach is possible in which the E-step is performed locally but the M- step based on a peer-to-peer distributed averaging method.

Combination of these two algorithms is also possible.

Here, the second approach is introduced briefly. At this second method, after each peer calculated its local sufficient statistics, it updates its mean and covariances using  , ,  t m jt  m j m  w N  ? =   (24)  , ,  ,  t m jt  m j t m j  a w  ? =   (25)  , ,  ,  t m jt  m j t m j  b w  ? =   (26)  Then mP  stores these local estimates and asks for the estimated parameters of neighboring nodes. Once all immediate neighbors of mP  have responded, mP  updates the jth component parameters by the following relations  , ,' , 1  m j n j m j  nN ? ?  ? + +  = +  (27)  , , , ,' ,  , ,  m j m j n j n j m j  m j n j  ? ? ? ? ?  ? ? + +  = + +  (28)  , , , ,' ,  , ,  m j m j n j n j m j  m j n j  ? ? ? ? ? + ? +  ? = + +  ? ?  (29)  where nN  is the number of neighboring nodes. Finally, covariance of each components is calculated using  ( )' ' ' ', , , , Tm j m j m j m j? ?? = ? ? .

This algorithm requires each node to know in advance  number of Gaussian mixtures. Further work can be done to estimate this number as well as the component parameters using a distributed EM algorithm.

Since peer-to-peer networks may consist large number of nodes, scalability in an important feature of peer-to-peer distributed algorithms. For example, in wireless sensor networks, the energy consumed to transmit a bit is much greater than that of required for a single local computation.

Therefore scalability should be studied in these algorithms.

Assume that the nodes are distributed over a squared area with nodes on a uniform grid. Denoting bN  as the number of bytes communicated between two nodes per time step, it can be found that the amount of communications in bytes for the centralized method in which all nodes send their data to the center of the network becomes  ( ) ( )3 21 2 2 bM M N O M+ + + =? . The worst case in this method is that the centralized unit is not in the center of the network but at the end of the area. The communication in bytes for such a case is  ( ) ( )21 2 1 bM M N O M? + ? + + =? . Once the centralized unit receives all data, it can run the standard EM algorithm.

The proposed distributed EM executes the communication and computations iteratively. The communication cost is related to the number of loops, i.e., the accuracy of the estimated results. Denoting T  as the number of loops, the communication in bytes for our proposed method is  ( )a bN MN T O M= , where aN  is the average of number of neighbors. Therefore, unlike the centralized method our         proposed algorithm is scalable.



IV. SIMULATION RESULTS This section is devoted to show the performance of the proposed distributed peer-to-peer EM algorithm. Two different cases have been considered as given below.

A. Case 1: Environmental Modeling Here, a 2D simulated data set is used to evaluate performance of the proposed peer-to-peer distributed EM algorithm. A network with 100 nodes ( 100M = ) is considered in which each node has 200 data observations ( 200mN = ). The observations are generated from three Gaussian components ( 3J = ) distributed in Fig. 1. In the first 40 nodes, 70% of observations come from the first Gaussian component and other 30% of observations evenly come from the other two Gaussian components. In the next 30 nodes, 60% of observations come from the second component and in the last 30 nodes 60% of observations come from the third component. Other observations of these nodes come evenly from the other two Gaussian components.

For the purpose of comparison, we performed two different tests. First, the standard EM algorithm is executed in each of nodes employing EM algorithm individually using only local data resulting obviously in different estimates.

The estimated mean values of each component in all 100 nodes are shown in Fig. 2. For the sake of simplicity, this figure only shows the second element of the mean vector.

In the second test, the proposed peer-to-peer EM is executed over the nodes of this network. Second elements of the estimated mean vectors in 100 nodes are shown in Fig. 3.

It can be seen that the estimated mean values in all nodes are very close to their true values (the true values are 2, 0 and - 2). The true and estimated parameters of the components using peer-to-peer EM are shown in Tables I and II, respectively. As seen, good estimates of the true values have been obtained. The values offered in these tables are the mean value of the estimated parameters at all nodes of the network. Here, the convergence threshold is assumed to be  0.1? = .

TABLE I TRUE MEAN AND COVARIANCE MATRICES FOR THE 2D DATA SET.

Component Mean vector Covariance matrix 1 [ ]0, 2?  2 0  0 0.2 ? ? ? ?? ?    2 [ ]0, 0  2 0 0 0.2 ? ? ? ?? ?    3 [ ]0, 2  2 0 0 0.2 ? ? ? ?? ?            TABLE II FITTED MEAN AND COVARIANCE MATRICES USING THE PEER-TO-PEER  DISTRIBUTED EM ALGORITHM.

Component Mean vector Covariance matrix 1 [ ]0.0041, 2.0007?  2.0171 0.0040  0.0040 0.1992 ? ? ? ?? ?    2 [ ]0.0151, 0.0017? ?  2.0137 0.0147 0.0147 0.1977  ? ? ? ? ? ?? ?    3 [ ]0.0091,1.9938?  2.0231 0.0131 0.0131 0.2172  ? ? ? ? ? ?? ?       Fig. 1. Data distribution.

0 20 40 60 80 100 -2.5  -2  -1.5  -1  -0.5   0.5   1.5   2.5  Nodes  T h  e es  tim a  te d  m ea  n v  a lu  e s   Fig. 2. Three estimated mean values using standard EM algorithm in the network with 100 nodes.

0 20 40 60 80 100 -2.5  -2  -1.5  -1  -0.5   0.5   1.5   2.5  Nodes  T h  e es  tim a  te d  m ea  n v  a lu  e s   Fig. 3. Three estimated mean values using peer-to-peer EM in the network with 100 nodes.

B. Case 2: Distributed Target Classification With the increasing use of camera surveillance in public  areas, the need for automated surveillance solutions is rising.

A particular problem is camera surveillance in wide areas, such as airports, shopping centers, etc. Such areas typically cannot be fully observed by a single camera, and surveillance of such places relies on a network of sparsely distributed cameras.

In this particular setting the problem of tracking persons across all cameras is difficult. Someone is first observed by one camera, then he/she is outside of sight of any camera, and later on he reappears at another camera. We would like to know whether the two observed persons are in fact the same individual.

In this part, we employ the proposed distributed EM algorithm in a distributed system for visual target recognition. In this system, each camera is a stand alone tracking unit, which stores its own observations and exchanges only limited data with other cameras. The local observations in combination with the exchanged data allow each camera to learn its own local model.

Similar to other approaches [15], we use appearance cues such as average color, or length to find the correspondence between observations and persons. Since the same person will appear differently each time that she/he is observed, we model the observations as a stochastic process. We assume that the observations are samples drawn from a Gaussian distribution with person specific parameters, which are constant over time. In a system where J persons are monitored, observations of all persons are generated by a Gaussian Mixture Model (GMM) with J components. Here, we consider the learning of the parameters of the GMM with the proposed algorithm. Given the learned GMM we assign the most likely person to each observation.

We have performed a series of tests with the presented peer-to-peer distributed EM. The performance of the algorithm will be compared with that of a standard EM implementation. We evaluate the algorithms on artificially generated data.

A set of 1000 observations from 5 persons, which are distributed over several cameras is used as a standard set.

Every observation iy  consists of a 3-dimensional appearance vector. The observations are randomly distributed over the cameras according to a uniform pdf. The difficulty of the generated data is measured by the c- separation and eccentricity values [16]. An increasing difficult recognition problem is indicated by increasing eccentricity or decreasing c-separation values. The dataset has a c-separation of 1 and an eccentricity of 3.

Several datasets are generated to investigate the performance of the algorithm by variations of the number of cameras, and the distribution of the observations over the cameras.

Evaluation The evaluation criteria should reflect two aspects of proper clustering. It is described that (i) all observations within a single reconstructed cluster belong to a single person, and (ii) all observations of a single person are grouped together in a single reconstructed cluster. These criteria are analogues to the precision and recall criteria often used in Information Retrieval settings. In order to evaluate both systems on one parameter we use the F1- measure defined in (32).

Because the considered clustering problem is unsupervised, the true and proposed clusters are arbitrary ordered. Therefore, we define the precision (30) and recall (31) for the proposed cluster over the best match with a real cluster. Importantly precision and recall have to be considered jointly, because it is trivial to gain recall of 1.

This is done by clustering all observations into one cluster.

However, this will result in a very low precision. The F1 measure (32) is the harmonic mean of precision and recall and will penalize for such cases.

?max1 ?  J i s i  s s  C C Pr  J C= = ?  ? (30)   ?max1 J s s i  i i  C C Rc  J C= = ?  ? (31)  21 Pr RcF Pr Rc ? ?=  + (32)  In Fig. 4 the results of the distributed EM is shown with different number of cameras. The observations are distributed over the cameras according to a uniform distribution. The performance is compared with the standard EM. The number of observations was fixed at 1000. The figure shows that the performance of the distributed EM is independent of the number of cameras. As it is seen, performances of distributed and standard EM algorithms are the same. Note that using the standard EM algorithm, the entire observations must be sent to a central processing unit, while using our proposed algorithm, only the vector of local sufficient statistics is transferred between neighboring nodes.

Furthermore, there is no need for a central processing unit in the proposed distributed algorithm.

5 10 15 20 25 30 0.88  0.9  0.92  0.94  0.96  0.98   1.02  Number of cameras  F  M e  a su  re      distributed EM standard EM   Fig. 4 The performance of distributed peer-to-peer EM and standard EM algorithms with different number of cameras.



V. CONCLUSION In this paper, a peer-to-peer distributed EM algorithm was  proposed for density estimation and clustering of data distributed over the nodes of a network. In the E-step of this algorithm, each node calculates local sufficient statistics using its local observations. A peer-to-peer algorithm is then used to diffuse local sufficient statistics to neighboring nodes and estimate global sufficient statistics in each node.

In the M-step, each node updates parameters of the Gaussian mixture model using the estimated global sufficient statistics. A second approach was also introduced in which the E-step is done locally while the M-step is performed based on a distributed averaging method. Good scalability and fault tolerance are two important advantages of these algorithms.

