Research on Cloud Computing Based Data Processing for Financial Terminal  Equipment

Abstract? In financial system, distributed terminal equipments generate huge mass of data that containing much valuable information. The integrated analysis of the distributed data combining with traditional information will reveal the potential business value and provide a brand new view. Based on the infrastructure of cloud computing service platform, propose a practical scheme of financial data processing which composing of data acquisition, processing, storage and analysis. Present the key technologies of data loading, migration, MapReduce processing, remote analysis and flashback, and describe the execution of these technologies within cloud computing framework.

Keywords-Big Data Processing; Database; Financial Terminal; Cloud Computing

I.  INTRODUCTION Nowadays enormous various financial data streams are  generated constantly from distributed financial terminal equipments. Analysis of these real time quotes of financial information would create new source of economic value, inspect the abnormal client behavior and help to affirm the market trend as soon as possible. But the vast rushed data have made it impossible to perform these operations in a stand-alone mode. Several financial studies [1] have demonstrated that based on the cloud computing platform on-line it is possible to perform computing and forecasts, using data and image information collected from embedded terminal equipments as well as traditional client?s data.

In financial area, all kinds of terminal equipments always generate huge mass of data, for example, the serial number code of banknotes which is scanned through the currency detector and identified by its image processing module. This digital output information has four key characters [2]: big data size, high generation rate, various data type and potential value, and it is belongs to the category of Big Data.

The data was simply ignored or abandoned in the past because of the shortage of storage and processing capability.

But now, in order to get real business value form the big data, enterprises need to catch and recognize the information with the aid of image formation and processing technology, then extract, transmit and transform the data to perform analysis combining with other traditional information. To apply and promote the utilization of these dispersive data, the IT infrastructure of distributed data processing must be  modified to fit the requirement of rapid mass data processing.

Aiming at the aforementioned application in financial area, this paper propose a general structure of financial terminal data processing system which based on cloud computing platform with oracle database server. The requirement of the infrastructure involves data acquirement, data organization and analysis, and it would be able to handle the large-data processing in distributed environment with predictable lower delay. So the basic procedure performed on the cloud platform and database includes data transmission, data loading, data migration, retrieval and analysis through remote connection with processing program.



II. CLOUD-COMPUTING-BASED SYSTEM ARCHITECTURE The basic target of the financial data processing system is  to gathering the real time quotes of big data from distributed terminal equipments, so that to analysis the data set in-depth in conjunction with traditional business data. So the hardware infrastructure is based on large-scale cluster server.

The cluster consist of tens of powerful servers, each CPU core of these rack-mount servers is connected by gigabit Ethernet. The data would be distributed on the network of the large-scale cluster, and each node in the network only store and handle one part of the data. The huge mass of data was processed in parallel, every node perform operation with its corresponding data, output their intermediate value, and then these values were merged together to form the final result.

The cluster server is connected with an Oracle database cloud server through InfiniBand, thus source data loading and result transmission could be done with very high efficiency. Once the data was loaded and the computing result of the cluster was stored into the database, the system clients could perform advanced analysis and make decision using software tools which located in the business cloud server. The compact integration of the cluster, database server and business server provides powerful capability of big data processing, and this constituted the basic structure of Cloud Service Platform of the system.

In the system, vast terminal financial information stream such as the cash number that identified by distributed currency detectors will be transmitted through Internet to the Cloud Service Platform, and be loaded into Oracle Database   DOI 10.1109/INCoS.2013.110     11g. In order to extract specific useful information form the data, custom program which following the architecture of MapReduce will be executed on distributed nodes of the cluster server in parallel. The merged result will be loaded into DBMS of the database server. Cloud service providers must guarantee that data are processed automatically and held securely [3][4]; hence physical and logical backup of the data in database will be stored as separate datafile or Oracle Data Pump file, which could be migrated facilely between different schemas or databases. The remote connection between business cloud server and database server will be established in advance, and the analysis modules on the business server could implement remote access at a tremendous speed, so that to provide abundant information to the decision-makers. The architecture of the system is as shown in figure 1.

Figure 1.  Architecture of Cloud-Computing-based system.

Based on this infrastructure, the system could integrate the big data gathered from financial terminal equipments and traditional business data to perform combinative analysis, and provide a brand new view over the old questions. For example, financial analyst could made tracks for specific cash flow using the data of serial number of the banknotes and the geographical information about where the number was gathered.



III. PROCESSING PROCEDURE ON THE CLOUD PLATFORM The source data of the cloud-computing-based system is  mainly from widespread financial terminal equipments, especially the serial number of every banknote which identified by improved currency detectors. The DSP of the detector processes the original image information of banknotes, by comparison with the number font?s character;  it transforms the image to digital number, and stores the data in files with extension name like ?txt? or ?csv? and so on.

The data could be generated instantaneously and transmitted over networks to the database server in a very short time.

Data loading could be performed automatically by Oracle Loader for Hadoop since the data has been prepared as a receivable format of Oracle. The control file of the loader indicates the location of original data and formulates the structure of information storage, once it is executed, the data in database server could be accessed by ordinary users rapidly and permanently.

The mass data that loaded in the database server should be distributed to the computing nodes of the cluster server and be processed by the cluster. In order to process the mass data in parallel, each node on the networks of cluster only keeps one part of the data, and executes corresponding operation separately, and then the intermediate output will be collected to form the final result after merge processing. This is the main principle of MapReduce programming mode [5], and hence the capability of the cluster could be mostly exerted. In financial system paradigm, when the serial number data of banknotes are loaded into database server, our purpose is to find out if there exist some banknotes that with the same number, and to calculate the frequency of occurrence of the same number. Firstly the mass data would be divided into a number of blocks, and assigned to every computing node of the cluster. So each node executes comparison process with its own data according to the program which defined in Map function. The function will count the frequency of occurrence of each number in that data block, and map the result to a temporary intermediate memory space. The mapping output of every computing node would be in forms of number-counts pairs, so a typical output of one single node may as shown in figure 2.

Figure 2.  Typical intermediate output of the computing node.

All of the intermediate counting result will be grouped together. For each number, the intermediate processing program will emit its entire number-counts pair to one relevant Reduce function. The function is running on every node, its purpose is to accumulate the counts of every number. This accumulation is also be performed in all the other nodes at the same time, the intermediate number- counts pair of the same number will be calculated by the same function, and the final statistical result will soon be obtained after this Reduce stage. The processing procedure of MapReduce on cluster server is as shown in figure 3.

Both of the final statistical result and the original loaded data will be back up in the oracle database server, to make sure the data security and consistency in case of one or more of the nodes collapse[6][7], and to enable data migration between different databases and between different users of the same database. The backup will be stored using technology of Oracle Data Pump, which presented after     Oracle 10g. The user could export all the data of particular schemas, or data in special Tablespace, or even the data of full database by using instruction expdp with proper authority, and back up the data as binary dumpfile which stored on disk of the server. The procedure of data export could also be done by execute parameter file which indicates precisely the storage directory, dumpfile, data object and logfile. To complete data migration between users of the database, clients need to execute the instruction of impdp with proper parameter configuration of Remap Schema; a typical Data Pump Import instruction is as follow: C:\>impdp Scott/tiger DIRECTORY=dumpdir DUMPFILE=backup.dmp LOGFILE=backup.log REMAP_SCHEMA=scott:oe JOB_NAME=imp_oe_schema  By taking similar steps, data of full database could be exported and transferred to another database server using the logical backup when necessary.

Figure 3.  Process of MapReduce programs on cluster server.



IV. DATA ANALYSIS AND RETRIEVAL In terms of financial application, in order to get real  business value, the final result of the cluster server should be analyzed in-depth combining with the other traditional data.

Because of the huge volume of data and the real-time performance requirement in financial system, we propose that the analysis operation to be done on special business  cloud server. The financial analysis application program such as Matlab and Statistics Analysis System are running on the business server, the Oracle database server and the business server will be connected through remote ODBC connection to perform high-speed data access, thus the final computing result stored in database server could be easily used by the software.

Different from local connection, when establishing remote connection, the TNS Service Name of driver configuration should be the same as Net Service Name which configured in Oracle Net Configuration Assistant. The definition of the connection includes data source name, database username and password. All the DML operation of database is depending on the combination of function ?exec?, ?fetch? and ?Data?. By executing the function of ?exec?, analysis software implements SQL database operation command, and acquires the data by function ?fetch?. Based on the configuration and operation, the application program analysis all the data in form of integrated data set, and find out a new interpretation for the traditional issue or make decision according to its analysis model.

The analysis result of application program will be finally submitted to the database server and stored together with all the other data generated during the whole processing procedure. The analysis procedure may involve much DML operation and hence including some misoperation inevitably, the data that has been changed or deleted due to misoperation of database must be retrieved, and the analysis result itself may be reformed due to the update of data or algorithm. All the data retrieval and recovery in our system is achieved by the implementation of database flashback. The two main methods usually used are flashback table and flashback database. Once be granted the authority by system administrator, clients could deal with most of the misoperation on single table using method of flashback table.

When misoperation occurred on several users of the system simultaneously and much of the data need to be retrieved, we will recovery the state of the database to a point of time in the past by flashback database. It is done mainly by the following steps: startup the database in mount mode; set the archivelog on; alter database flashback on; select current system change number; and then the administrator could flashback database to the current system number whenever the state of database has been changed.



V. CONCLUSION The assemble application of large-scale cluster server,  database and business server constituted a cloud computing service platform, and provided the main technology needed in acquisition, processing, storage and analysis of big data of financial system, thus provided abundant information for business analysts to maximize the potential value of the data and to make scientific decision. Based on the infrastructure of the cloud service platform, the data that identified and gathered from distributed terminal equipments will go through the process of data loading, migration, MapReduce processing, remote analysis and flashback, and then the     result of the data processing could be used by various end clients of financial system.

