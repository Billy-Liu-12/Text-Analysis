A PATTERN BASED FRAMEWORK FOR PRIVACY PRESERVATION

ABSTRACT  Data mining extracts novel and useful knowledge from large repositories of data and has become an effective analysis and decision  means in corporation. The sharing of data for data mining can bring a lot of advantages for research and business collaboration. The  misuse of these techniques may lead to the disclosure of sensitive information. However, large repositories of data contain private  data and sensitive rules that must be protected before published. Motivated by the multiple conflicting requirements of data sharing,  privacy preserving and knowledge discovery, and privacy preserving data mining has become a research hotspot in data mining and  database security fields. Researchers have recently made efforts at hiding sensitive association rules. This paper presents a novel  based approach that strategically modifies a few transactions in the database. It modifies support or confidence values for hiding  sensitive rules without producing many side effects. Nevertheless, undesired side effects such as nonsensitive rules falsely hidden and  spurious rules falsely generated, may be produced in the rule hiding process.

Index Terms-Data mining, Association Rule, Mining methods, Rule hiding, Association Rule mining, Privacy Preservation  1. INTRODUCTION  Data mining is defined as the process of finding useful  patterns from data which is already existed in the  database. Like knowledge discovery in artificial  intelligence also called as machine learning, or statistical  analysis, data mining attempts to discover rules and  patterns from data. A variety of data mining problems  have been studied to help people get an insight into the  huge amount of data. Here in this paper a framework for  providing data security and avoiding side effects has been  proposed. Association rule mining is used to provide  privacy automatically or indirectly. Although a business  might use a set of programs that it knows and trusts, and  might train its administrators and Help Desk personnel to  support those programs, administrators lose control as  soon as users begin running unknown code. The  increasing role of the Internet in business puts your  network at a greater risk than ever before. Such user?  installed programs can conflict with other installed  programs, change critical configuration data, or introduce  a virus. Users often cannot make safe or informed choices  about what software to run because viruses intentionally  conceal the malicious purpose of the program. Also, the  problems that are associated with running unknown code  can increase support costs substantially because they lead  to more system maintenance, more help desk time, and  lost user productivity.

Data mining extracts knowledge to support a variety of  domains marketing, weather forecasting, medical  diagnostics, and national security, but it is still a challenge  to mine certain kinds of data without violating the data      privacy and confidentiality. How to mine patients private  data, for example is an ongoing problem in healthcare  applications. As data mining becomes more pervasive,  such concerns are increasing. Online data collection  systems are an example of new applications that threaten  individual privacy. Already companies are sharing data  mining models to obtain a richer set of data about mutual  customers and their buying habits. The computing  community must address data mining privacy before data  mining techniques become wide-spread and the threat to  private information spirals out of control. The sticking  point is how to protect privacy while preserving the  usefulness of data mining results. Much research is under  way to address obstacles, but practical privacy-preserving  data-mining systems are largely in the research and  prototyping stages.

2. Privacy Violations in Data Mining  Understanding privacy in data mining requires  understanding how privacy can be violated and the  possible means for preserving privacy violation. In  general, one major factor contributes to privacy violation  in data mining is data misuse.

User's privacy can be violated in different ways and with  different intentions. Although data mining can be  extremely valuable in many applications for example  business, medical analysis, and etc, it can also, in the  absence of adequate safeguards, violate informational  privacy. Privacy can be violated if personal data are used  for other purposes subsequent to the original transaction  between an individual and an organization when the  information was collected.

In individual privacy generally when people talk of  privacy," keep information about me from being available  to others". However, their real concern is that their  information not be misused. The fear is that once  information is released, it will be impossible to prevent  misuse. Utilizing this distinction ensuring that a data  mining project won't enable misuse of personal  information opens opportunities that "complete privacy"  would prevent. To do, this project need technical and  social solutions that ensure data will not be released. The  primary goal of data privacy is the protection personality  identifiable information.

3. Existing schemes for privacy  3.1 Modifications through Audit-Trailing  The central role of audit trails, [11] or (more properly)  logs, in security monitoring needs little description, for it  is too well known for any to doubt it. Auditing, or the  analysis of logs, is a central part of security not only in  computer system security but also in analyzing financial  and other non-technical systems. As part of this process, it  is often necessary to reconcile logs from different sources.

Consider for example intrusion detection over a network.

In this scenario, an intrusion detection system (IDS)  monitors several hosts on a network, and from their logs it  determines which actions are attempts to violate security  (misuse detection) or which actions are not expected  (anomaly detection). As some attacks involve the  exploitation of concurrent commands, the log records may  involve more than one user, process, and system. Further,  should the system security officer decide to trace the  connection back through other systems, he must be able to  correlate the logs of the many different heterogeneous  systems through whom the attacker may have come.

Synchronization of time among hosts, a method for  correlation of host-specific information, and a standard  logging format. Such a format has several benefits. First,  it makes analysis of the logs by a central engine simpler,  because that engine need not know the types of systems  generating the logs. Secondly, it enables logs generated  for very different purposes to be reconciled. Suppose a  credit card transaction is made over the Internet.

3.2 Three Tier-Architecture Approach  Privacy-preserving data mining usually has mUltiple steps  that translate to a three-tire architecture [10] .  At the  bottom tier are the data providers, the data owners, which  are often physically distributed. The data providers submit  their private data to the data warehouse server. This  server, which constitutes the middle tier, supports online  analytical data processing to facilitate data mining by  translating raw data from the data providers into  aggregate data that the data mining servers can more  quickly process.

The data warehouse server stores the data collected in  disciplined physical structures, such as a      multidimensional data cube, and aggregate and  precomputes the data in various forms, such as sum,  average, max and min. In an online survey system, for  example, the survey respondents would be data providers  who submit their data to the survey analyzer's data  warehouse server, an aggregated data point might be the  average ago of all survey respondents. The aggregated  data is more efficient to process than raw data from the  providers. At the top tier are the data mining servers,  which perform the actual data mining. In a privacy?  preserving data mining systems, these servers do not have  free access to all data in the data warehouse. In a hospital  system, the accounting department can mine patients'  financial data, for example, but cannot access patients'  medical records.

Central to this strategy are three protocols that govern  privacy disclosure among entities. First data collection  protocol protects privacy during data transmission from  data providers to the data warehouse server. Second  inference control protocol manages privacy protection  between the data warehouse server and data mining  servers. Third information sharing protocol controls  information shared among the data mining servers in  different systems.

3.3 Sliding Window Algorithm  Transforming a database into a new one that conceals  some strategic patterns (restrictive association rules)  while preserving the general patterns and trends from the  original database [13] . The procedure of transforming an  original database into a sanitized one is called the  sanitization process and it was initially introduced in. The  sanitization process acts on the data to remove or hide a  group of restrictive association rules that contain sensitive  knowledge. To do so, a small number of transactions that  contain the restrictive rules have to be modified by  deleting one or more items from them or even adding  noise to the data by turning some items from 0 to 1 in  some transactions. An efficient one-scan algorithm, called  Sliding Window Algorithm (SW A) that improves the  balance between privacy and knowledge discovery in  association rule mining. This algorithm requires only one  pass over a transactional database regardless of the  database size and the number of restrictive association  rules that must be protected.

3.4 The Framework for Privacy Preservation through  Itemset Mining  One crucial aspect of privacy [12] preserving frequent  item set mining is the fact that the mining process deals  with a tradeoff, privacy and accuracy, which are typically  contradictory, and improving one usually incurs a cost in  the other. One alternative to address this particular  problem is to look for a balance between hiding restrictive  patterns and disclosing non- restrictive ones. That the user  propose a new framework for enforcing privacy in mining  frequent itemsets, combine, in a single framework,  techniques for anciently hiding restrictive patterns, a  transaction retrieval engine relying on an inverted file and  boolean queries, and a set of algorithms to sanitize a  database. In addition, performance measures for mining  frequent itemsets that quantify the fraction of mining  patterns which are preserved after sanitizing a database.

As given in figure framework encompasses a  transactional database (modeled into a document  database), an inverted file, a set of sanitizing algorithms  used for hiding restrictive patterns from the database, and  a transaction retrieval engine for fast retrieval of  transactions. The inverted file and the transaction retrieval  engine in this section has been described.

Inverted File Transaction  Retrieval Engine  ?  I'- --- Transaction  Database Set of Sanitizing  Algorithm  Fig.1 Privacy Preservation through itemset mining  4. System Framework for Heuristic Rule Finding  Figure 2 shows the framework of proposed system approach that consists of six components. Initially, the original database is converted into the transaction table,      where each distinct item is encoded as a unique prime number.

This conversion helps the efficiency and scalability. The sensitive rule table and the nonsensitive rule table are built to record the rule information. The transaction-rule index is also constructed using the concept of inverted lists to correlate the tables for efficient retrieval. The main challenge of rule hiding is how to select the items and transactions to modify. To represent each class of modifications as a template and then select templates in a one-by-one fashion has been done. The templates are kept in the template table and the selected templates are put into the action table. The six components are updated each time a template is selected. When all the sensitive rules are hidden or the template table is empty, the templates  Original  Database  Modified  Database  Mining  Sensitive  Non-Sensitive  Rule  Template  Table  Transaction Table l14.r--_---' Fig 2. System Framework for Privacy  Preservation  in the action table are applied to modify the original database. If some sensitive rules are not hidden, the user can release as it is, release nothing, or relax the constraint to hide more sensitive rules. In the proposed system framework that the section 4.1 is concerned of an approach for rule hiding through classification and modification. And the section 4.2 is proposed for reducing the side effects.

4.1 Special Approach for Rule Hiding  4.1.1 Classification of Modifications  the rule hiding process can be viewed as a series of modifications. class of modifications can be expressed in a logical form if we regard each transaction as a bit vector. For example, deleting items in Y from the transactions containing X U Y can be expressed as: X /\ Y => X /\: Y, where an itemset f{xl; x2 . . .  xn}is expressed in the form of x 1 /\ x2 . ...  xn.

Definition: Transaction containment. A transaction t is contained in a template _ if both the following conditions  hold, where the set of all transactions contained by 7" is  denoted as T-r: I. PP" ?t and "J'E E P-rl F'1.. t.

2. 'vJ1-r E t if -r = Del; MI-r ? t if 01" = In, ,  Definition: Sensitive rule coverage. A sensitive rule r :  X -Y is covered by a template T" if Sup XUY or Conf r will  b d d d?fy 11,- . . b e ecrease as we mo 1 _ m transactIOn t y  OT "it E T' . The set of all the sensitive rules covered by r is denoted as R" and the number of rules in is  T(I 'R. D called the coverage of ? '1" ? ?  The coverage of a template is the fifth component (CV) in its template  representation. From the above, a template r correlates a  f ? ? I ''ll'-  ? h  f . T. set 0 sensItIve ru es . ? WIt a set 0 transactIOns T  in such a way that modifying the transactions in T" can help to hide the sensitive rules in  ' R..,- .

To reduce the number of modified entries, we estimate  the minimal number of transactions in .,.. that should be  modified to hide all the sensitive rules in S'R,. as follows:  Definition: Difference of a template. The difference of a  template -;- is defined as the maximum of MCr, .,..V'" E . R"f"I if.o.,.. is Del; or the  . P rV7' E : 'R'r' ?f.o.,..? I Th d?f'" maxImum . I IS ns. e 1 terence is the sixth component (DF) of a template. Considering 1"3 as an example, by Definition 3. 4, only one sensitive rule (a - b) is covered by it. Since 0 is Del and MI is the  consequent b, by Defmition 3. 6 the DF of 1"3 is equal to  MCa-b,1"3, which is the minimum of MSCa-b and      MCCCa--+b by Observing the logical fonns ofT 5 and" 7.

We find that the modification in the fonn of "'(11\ ..,h l\ ..,rl :::::} a A b A .., rl  h th .

_ as e same Impact on hiding ab --+ d because it inserts items a and b to the transactions that do not contain a, b, and d. We call it the auxiliary template. To keep the number of modified entries small, we generate the auxiliary templates only when the templates currently generated are not sufficient to hide all the sensitive rules. Given a number of templates, we adopt a heuristic method to select the one with the lowest DF as the pivot. If several templates have the lowest DF, we select the one with the largest CV.

Moreover, if several templates have the largest CV, we randomly select one of them because they have the same impacts on the number of hidden sensitive rules and the number of modified entries.

4.2 Avoidance of Side Effects  Definition: Transaction group. For a template"', the  transactions in T '(can be divided into groups such that the transactions in a group are identical. A transaction  group 1i is represented as < l1r j JI. 1r > 2 where 111: N'  denotes the set of distinct items in 11" and 11: keeps the  total number of transactions in11'.

Definition: Sensitive rule conflict. Given a template "'  , we say that a transaction group but it also increases the support of ab and makes it difficult to hide a --+ b in the next run. Therefore, among the transaction groups, we filter out those who conflict with sensitive rules. This operation is executed when a template is generated. After that, for each transaction group, we estimate the maximal number of transactions that can be modified without producing side effects. A rule that can be affected by the insertion or deletion of an item must have that item in its precedent or consequent.

Therefore, we use MI 1"" and 11"" to identify all the nonsensitive rules and spurious rules that can be affected by modifying the transactions in 11" . Finally, two upper bounds on the number of transactions in 1I"to be modified are computed. They give the same upper bound on the number of transactions to be modified and the same number of affected nonsensitive rules. As a candidate 1i is selected, we further check the spurious rules that can be affected by modifying the transactions in 1r and compute another upper bound. Since there can be a huge number of spurious rules, we use the association graph proposed in to reduce the search space.

5. CONCLUSION AND FUTURE WORK  In this paper, a novel approach that modifies the database to hide sensitive rules with limited side effects. To classify all the valid modifications such that every class of modifications is related with the sensitive rules, nonsensitive rules, and spurious rules that can be affected after the modifications. With the methods proposed in this paper, the transactions in an order can be modified.

So that both the numbers of hidden sensitive rules and modified entries are considered. In most cases, all the sensitive rules are hidden without false rules generated. In addition, it is observed that the common items and the overlapping degrees among sensitive rules have a great impact on the performance of rule hiding. Efficient mechanisms are required to speed up the rule hiding process for large databases. Another issue is the fast recognition of sensitive rules that cannot be hidden according to the user-specified constraint. An ideal goal is to build a system that can aid the database administrator to find the sensitive rules for hiding. The other issue is to remove the threshold assumption.

REFRENCES  [1] R. Agraval and R. Srikant, "Fast Algorithms for mining Association Rules,"Proc. Con. Very Large Data Bases, pp. 487-499, 1994.

[2] R. Agraval and R. Srikant, "Privacy-Preserving Data Mining," Proc. ACM Conf. Management of data, pp.l4- 19,2000.

[3] R. Agraval and T. Imielinski, and A. Swami, "Mining Association Rules between sets of Items in Large Databases," Proc. ACM Conf. Management of data, pp.

207-216,1993.

[4] C. Clifton and D. Marks, "Security and Privacy Implications of Data Mining," Proc. ACM Workshop Research Issues in Data Mining and Knowledge Discovery, pp. 389- 398, 1999.

[5] S.R.M. Oliveira and O.R. Zame, "Privacy Preserving Frequent Item set Mining," Proc. IEEE ICDM Workshop Privacy, Security, and Data Mining, pp. 43-54, 2002.

[6] SJ. Rizvi and lR. Haritsa, "Maintaining Data Privacy in Association Rule Mining," Proc. Conf. Very Large Data Bases, pp. 682-693, 2002.

[7] Y. Saygin, V.S. Verykios, and A.K. Elmagarmid, "Privacy Preserving Association Rule Mining," Proc.

IEEE Workshop Research Issues in Data Eng., 2002.

[8] V.S. Verykios, A.K. Elmagarmid, E. Bertino, Y.

Saygin, and E.Dasseni, "Association Rule Hiding," IEEE Trans. Knowledge and Data Eng., vol. 1 6, no. 4, pp. 434- 4 47, 2004.

[9] S.J. Yen and A.L.P. Chen, "A Graph-Based Approach for Discovering Various Types of Association Rules," IEEE Trans. Knowledge and Data Eng., vol. 13, no. 5, 2001.

[10] Nan Zhang and Wei Zhao, "Privacy Preserving Data Mining Systems", in IEEE Computer Society, 2007.

[11] Matt Bishop, "A Standard Audit-Trail Format" in  [12] Stanley R.M Oliveira and Osmar R. Zaine, "Privacy Preservation Frequent Itemset Mining", in IEEE Computer Society, 2002.

