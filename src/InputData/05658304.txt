A Double Search Mining Algorithm in Frequent Neighboring Class Set

Abstract-This paper addresses the existing problems that present frequent neighboring class set mining algorithms is  inefficient to extract long frequent neighboring class set in  spatial data mining, and introduces a double search mining  algorithm in frequent neighboring class set, which is suitable  for mining any frequent neighboring class set in large spatial  data through down-top search strategy and top-down search strategy. Firstly, the algorithm turns neighboring class set of  right instance into digit to create database of neighboring class  set, and then generates candidate frequent neighboring class  set via double search strategy, namely, one is that it gains  (k+1)-neighboring class set as candidate frequent items by  computing (k+1)-superset of k-frequent neighboring class set,  the other is that it gains I-neighboring class set as candidate  frequent item by computing I-subset of (/+1)-non frequent neighboring class set. The mining algorithm computes support  of candidate frequent neighboring class set by AND operation.

The algorithm improves mining efficiency through these  methods. The result of experiment indicates that the algorithm  is faster and more efficient than present algorithms when  mining frequent neighboring class set in large spatial data.

Keywords- spatial data mining; neighboring class set; double search strategy; AND operation  1. INTRODUCTION  As we all know, geographic Information Databases is an important form of spatial database in spatial data mining, mmmg spatial association rules from Geographic Information Databases is one important part of spatial data mining and knowledge discovery (SDMKD), which is also known as spatial co-location pattern as in [1]. Spatial co? location pattern are some implicit rules expressing construct and association of spatial objects in Geographic Information Databases, and also expressing hierarchy and correlation of different subsets of spatial association or spatial data in Geographic Information Databases as in [2]. Nowadays, in research of spatial data mining, there are mainly three kinds methods of mining spatial association rules as in [3], such as, layer covered based on clustering as in [3], mining approach based on spatial transaction as in [2, 4, 5, and 6] and mining approach based on non-spatial transaction as in [3]. The first two methods may be also used to mining frequent neighboring class set, The spatial association as in [4, 5, and 6] is quite single, because they only express spatial association among these objects which are all close to objective. However, neighboring class set expresses another    spatial association among these objects which are close to each other. MFNCS as in [2] uses the similar method of Apriori to search frequent neighboring class set, which gains some right instance of (k+ 1 )-neighboring class set only through connecting right instance of k-neighboring class set according to down-top strategy, and so this algorithm is not suitable for mining long frequent neighboring class set according to character of Apriori. Hence, this paper introduces a double search mining algorithm in frequent neighboring class sets, denoted by DSMA, which is suitable for any frequent neighboring class set according to double search strategy.



II. PRELIMINARY KNOWLEDGE  As we all know these basic knowledge as in [2], preliminary knowledge of neighboring class set is expressed as follows: every entity in spatial scope forms a spatial data set, which is described as this data structure, denoted by <Entity Identify, Class IdentifY and Spatial Location>. Here, identifY of different class in spatial data set is expressed as Class IdentifY; IdentifY of different entity in the same class is expressed as Object IdentifY; Location coordinate of entity is expressed as Spatial Location. We regard an entity as an instance of corresponding class, and so spatial data set is made up of these instances of spatial Class IdentifY. Sets of Class IdentifY are thought as a class set, denoted by T = {T], T2 ... Tm}, means there are m different classes.

A. Definition and Property  Definition 1 Neighboring Class Set, it is a subset of spatial class set in spatial data set, which is expressed as {Tn]' Tn2 ... Tnd (nk"'S m), denoted by NCS.

Let I = {in!> in2, . ... ink} be an instance of neighboring class set denoted by NCS = {T nl' T n2' ... , T nk}, here, inj is an instance ofTnj GEl, 2 ... k).

Example, let {V, W, Y, Z} be a NCS, and I = {V3, Ws, Y4, Z2} is an instance of NCS.

Definition 2 Neighboring Class Set Length, its value is equal to the sum of class set existing in neighboring class set.

If the length of NCS is equal to k, it is denoted by k-NCS.

Example, let {U, W, X, Z} be a NCS, and its length is 4.

Definition 3 Right Instance of Neighboring Class Set, let  I = {in!' in2, .... ind be an instance of NCS, if V ip and iq (ip, iq E I), and distance (ip, iq) "'S d, and then we think I be an right instance of NCS. Here, d is the minimal distance used    by deciding two spatial entItIes are close to each other, Euclidean distance is expressed as distance (ip, iq).

Definition 4 Neighboring Class Set Support, it is equal to the sum of right instance of neighboring class set, which is denoted by support (NCS).

Definition 5 Frequent Neighboring Class Set, its support is not less than the minimal support given by user.

Property 1 Let k-NCS is not frequent neighboring class sets, and (k+l)-NCS is not also frequent neighboring class set. Here, k-NCS c (k+l)-NCS.

Property 2 Let (k+ 1 )-NCS is frequent neighboring class set, and k-NCS is also frequent neighboring class set. Here, k-NCS c (k+ 1)-NCS.

B. Problem Description  As above definition, property and these representations as in [2], we describe frequent neighboring class set mining as follows:  Input: (1) Class set is denoted by T = {T], T2 . . .  Tm}, instance set is denoted by I = {in p in2, ... , ind, each ik (ik E I) is expressed as above defined data structure.

(2) Minimal distance is denoted by d.

(3) Minimal support is denoted by s.

Output: Frequent neighboring class set.



III. THE DOUBLE SEARCH MINING ALGORITHM IN FREQUENT NEIGHBORING CLASS SET  A. Forming Mining Database  In order to form mining database of neighboring class set, we need find corresponding NCS of every right instance, and tum NCS into a digit. The course of turning used by the algorithm is described as follows:  Firstly, we defme the order of class as T = {T], T2 . . .  T k . ? .  T m}, and so each class existing in  right instance has order denoted by OJ. And then we will gain a digit by computing this value, denoted by i: 2 'r' , here L is equal to  j?l  Neighboring Class Set Length, and namely, it is the sum of class existing in this neighboring class set.

According to this way, the algorithm will gain a digit by scanning every right instance and this digit also denote this corresponding NCS of right instance. Finally, we will gain a database as many digits.

Now we save these digits by this data structure expressed as follows:  Structure neighboring class set { Int Digit; II saving digit expressed as NCS of right  instance Int Count; I I saving the sum of same digit expressed as  NCS} NCS Example, here class set is denoted by T = {V, W, X, Y,  Z}, the first three right instances are denoted by 11 = {V2' W4, Y3, Zd, h = {V6, Ys, Z7}, 13 = {V3, Ws, Y4, Z2}. Now we use the above introductive way to form mining database, this course is expressed as follows:   NCS of 11 is denoted by {V, W, Y, Z}, and the order sequence is denoted by {I, 2, 4, 5}, and then digit = i1?1) + i2.1) + i4.1) + 2(5.1) =27, NCS[0).Digit=27, NCS [O).Count=l.

NCS of 12 is denoted by {V, Y, Z}, and the order sequence is denoted by {l, 4, 5}, and then digit = 2(1.1) + 2(4.1) + is.1) =25, NCS [I). Digit =25, NCS [1).Count=l.

NCS of h is denoted by {V, W, Y, Z}, and the order sequence is denoted by {I, 2, 4, 5}, and then digit = i1.1) + i2.1) + 2(4.1) + 2(5.1) =27, because NCS [0] has already saved this information, and only NCS[O).Count=NCS[O).Count +1=2, ...... .

B. The method of generating candidate frequent NCS  The algorithm generates candidate frequent neighboring class set according to down-top search strategy and top? down search strategy. Namely, one is that it gains (k+l)? neighboring class set as candidate frequent items by computing (k+l)-superset of k-frequent neighboring class set, the other is that it gains I-neighboring class set as candidate frequent item by computing I-subset of (l+l)-non frequent neighboring class set. The search strategy used by the algorithm is similar to B _ ARDSM as in [7].

Suppose class set is denoted by T = {V, W, X, Y, Z}, the course of generating candidate is expressed as follows:  Step 1: computing all I-candidate.

Frequent itemset is denoted by F = {i?1, 23.1, 24.\ 2S?1},  non-frequent itemset is denoted by NF = {22.1}.

Step2: according to result of stepl, the algorithm gains 4-  candidate.

4-FNCS1 = {i?l+23.1+24.1+2s.1}, its support is not less than minimal support.

4-NFNCS1 = {22.1+23.1+ 24.1+2s.1}, it contains item of NF. 4-NFNCS2 = {21.1+22.1+24.1+2s.1}, it contains item of NF. 4-NFNCS3 = {21.1+22.1+23.1+2s.1}, it contains item of NF. 4-NFNCS4 = {i?l+22.1+23.1+24.1}, it contains item of NF. Step3: computing 2-candidate according to F of step 1.

2-candidate is denoted by {i?1+23.1 21.1+24.1 21-1+25.1  23.1+24.1, 23.1+25.1, 24.1+2s.1}. Since thes? itemsets 'are subset of 4-FNCS], and they are not candidate to compute support.

Step4: computing 3-candidate according to NFNCS of step2.

3-candidatel = {23.1+ 24.1+2s.1} (subset of 4-FNCS1). 3-candidate2 = {22.1+ 24.1+2 S.1} (non frequent itemsets).

3-candidate3 = {22.1+ 23.1+25.1} (non frequent itemsets). 3-candidate4 = {22.1+ 23.1+24.1} (non frequent itemsets). 3-candidates = {21.1+ 24.1+2s.1} (subset of 4-FNCS1) 3-candidate6 = {21-1+ 22.1+25.1} (non frequent itemsets).

3-candidate7 = {21-1+ 22.1+24.1} (non frequent itemsets).

3-candidates = {i?1+ 23.1+2s.1} (subset of 4-FNCS1). 3-candidate7 = {21.1+ 22.1+23.1} (non frequent itemsets).

3-candidates = {i?1+ 23.1+24.1} (subset of 4-FNCS1). Step5: Input 4-FNCS1.

C. The method of computing support  Accordingly to chapter A and logical AND operation, we may gain this property as follows:  Property Let p and q are two digits, and let them express NCS of two right instances, let Tp be a NCS denoted by p, let Tq be a NCS denoted by q, then Tp ? Tq ?:> p and q = p.

Logical AND operation is used to compute support according to this property. The method is described as follows:  Suppose class set is denoted by as T = {U, V, X, Y, Z}, there are six neighboring class sets as follows:  NCS1 = {V, X, Y, Z}; NCS2 = {V, Y, Z}; NCS3 = {U, V, X, Y, Z}; NCS4 = {V, X, Y}; NCS5 = {Y, Z}; NCS6 = {X, Y, Z}; These digits expressed as neighboring class set of right  instance are denoted by {30, 26, 31,14,24 and 28}. Suppose a candidate is 24 which is denoted by C-NCS = {Y, Z}, and then (30 and 24) =24, (26 and 24) =24, (31 and 24) =24, (14 and 24) * 24, (24 and 24) =24, (28 and 24) =24. Because of this, we write 5 to support (C-NCS).

D. The process of mining frequent neighboring class set  Input: (1) Spatial class set is expressed as T = {T), T 2 ... T m}.

(2) Instance set is expressed as I = {i), i2 . ?  .in}.

(3) The minimal distance is expressed as d.

(4) The minimal support is expressed as s.

Output: Frequent neighboring class set.

Stepl: Firstly, the algorithm computes all these right  instances denoted by I' from instance set in spatial database as I according to the minimal distance as d.

Step2: And then, forming mining database as NCS after scanning once right instance set as I' via the method in chapter A of III.

Step3: Using double search strategy to generate candidate frequent NCS according to this chapter B of III. Namely, one is that it gains (k+ 1 )-neighboring class set as candidate frequent items by computing (k+1)-superset of k-frequent neighboring class set, the other is that it gains I-neighboring class set as candidate frequent item by computing I-subset of (1+ I)-non frequent neighboring class set. In this course, if candidate is frequent, it will be written in FNCS and delete all its subsets in FNCS.

Step4: Output FNCS according to chapter A of IlI.



IV. THE ANALYSIS AND COMPARING OF CAPABILITY  Nowadays, there are very little documents of research frequent neighboring class set. Here we compare DSMA introduced by this paper with MFNCS as [2] as follows:  A. The Analysis of Capability  MFNCS: this algorithm uses idea of Apriori to find frequent neighboring class set, which is made of three stages, firstly, computing all the frequent l-NCS, secondly, generating all the 2-NCS by range query, and generating all the k-NCS (k>2) by iteration. The algorithm has some repeated computing and superfluous candidate frequent neighboring class set, which gains some right instance of (k+ 1 )-neighboring class set only through connecting right instance of k-neighboring class set. As this algorithm adopts down-top strategy to generate candidate frequent   neighboring class set, it is only suitable for mining short frequent neighboring class set.

DSMA: this algorithm adopts double search strategy to generate candidate frequent neighboring class set, which is made of four stages, firstly, computing all the frequent 1- NCS, secondly, computing the 1 sl m-NCS which contains all classes, and generating k-NCS (k=m-l), thirdly, generating all the I-NCS (1=2) via frequent I-NCS. Finally, generating all (k-l)-NCS (k>3) and (I+1)-NCS (1)1) by iteration. The algorithm is suitable for mining any frequent neighboring class set for double search strategy.

B. The comparing of experimental result  Now we use experiment to prove above analyses and comparison. Two mining algorithms are used to generate frequent neighboring class set from 12267 right instances, whose class sets are denoted by digit from 3 to 8191, neighboring class set does not include any simple class, namely, it has two classes at least, the number of spatial class set is denoted by m=13, the number of right instance included by these neighboring class set observe the discipline which is expressed as follows:  NCS of digit denoted by 8191 has one right instance.

NCS of digit denoted by 8190 has two right instances.

NCS of digit denoted by 8189 has one right instance.

NCS of digit denoted by 8188 has two right instances.

Our experimental circumstances are expressed as follow: Intel(R) Celeron(R) M CPU 420 @ 1.60 GHz, 512MB, language of the procedure is Visual C# 2005.NET, OS is Windows XP Professional.

The experimental result of two algorithms is expressed as Fig.l and 2, where support is absolute. The runtime of two algorithms is expressed as Fig.3 as support and length of neighboring class set change.

"b .. l .. t? S ... pport:  ?N: : ?? ?:' :? K?t:t :::?::???t::?t:: ] If. !ITt ... NTC={K,r.t..III ." M, .. 1i'l'C={C,a:,l,.MJ No 41l ... NTCziG,I(.l,.IIJ If" 9It ... ?lll'C"{I.r.l,.MJ 110 SIt ... wrc:-tr,K..L.MI Ifo 1011?.?Jl'PC-(II.?L..1 , .

'r.?"ent It .. 1 715 25.7  lfo.TI?lIl'?ICC )ir--lfo&It_ :lll'C"i1i:-r.-LIII No. 211 .. :N1G-1I. K. 1..1111 If ?. TIt_:Wl'C-jD. r. 1.. MI Ifo. 311_:!IJ'Ct{H.!:. 1. III If ?. 8It_:MlC=IC, Ii(. L. MJ No.41l ... :NT?IG,I(,L..} h. QIt_:ftPC:{I,IC,I..,M} No. SIt .... NYc:: {,., It, L.M} II ?. IOlt ?? : JIl'C={A, k. L, III  Figure 1. The experimental result  III Th. ' .'HI< of ... ,.n " . " ' r;;]1'QJ?  Ab.olul" S .... 'orl:  L .... , :J C J  rlf-;(ll_ : ?P5 IJ., G, K-;-I, 1, 1:, L,III-If ?. 6.Ih.:Il'C={Aj ::.???:??:?::?:??:?:?t:: :: ??::?:g:1!i Mo. Ht_:NlC-1A, D, M. I, 1,1:. LR' Mo.9Ih.:IIl'C>1 Ro. 511_:II1C=(A, C, K, I, 1, x, L,R, Ro.IOll_:II1C=(  ,? .

Figure 2. The experimental result     ! 40000 ? 30000 ::E ]' 20000 ? 10000 '"  o ???--?--?----?--?--?==?  1000(3) 500(4) 200(5) 100(6) 50(7) 20(8) 10(9) 5(10)  S'l'Port(Length)  Figure 3. The comparing of runtime

V. CONCLUSION  This paper introduces a double search mining algorithm in frequent neighboring class sets, which is suitable for any frequent neighboring class set according to double search strategy. The result of experiment indicates that the algorithm is faster and more efficient than present algorithms when mining frequent neighboring class set in large spatial data.

