Effective Interpretation of Bucket Testing Results through Big Data Analytics

Abstract?Bucket testing is a common practice in the internet industry, where new features and services are tested by exposing them to a randomly selected small subset of users.

However, in this simple version of bucket testing, since a very small fraction of the total users are selected through uniform independent sampling of the population, the samples chosen, at times, do not adequately serve as a reasonable statistical proxy for the total population. This may lead to erroneous interpretation of the bucket testing results, particularly for on- line sites having large audiences with varying demographics and preferences. In this work, we present a novel algorithmic framework that addresses this challenge and provides an efficient and more accurate interpretation of the bucket testing results by analyzing the big audience data and factoring in the nature of the overall population in terms of the different user attributes. We demonstrate the effectiveness of our algorithm through the data obtained from real experiments conducted on Yahoo's bucket testing platform.

Keywords-big data; bucket testing; classification; sampling; user attributes

I.  INTRODUCTION Bucket testing, also known as split testing or A/B testing,  is frequently used by on-line sites with large audiences to gauge the impact of a new feature on a small subset of its users, before releasing it to the entire user population [1,2].

Usually bucket testing operates by randomly selecting two mutually exclusive small samples of equal size from the total user population. The users belonging to one sample (test bucket) are exposed to the new feature while the users falling in the other sample (control bucket) continue to experience the existing one. The effect of the new and existing feature on the exposed groups is then evaluated and compared against each other to determine if the user engagement has indeed increased with the new feature or not. The test bucket should comprise of a very small fraction of the overall population, so that in worse cases only a small number of users are adversely impacted. Likewise, Yahoo?s own bucket testing platform [3] allows site administrators to run experiments for a sufficient period of time to test their new services and site optimizations on a smaller audience before releasing them on a wider scale. Usually, each of the test bucket and control bucket comprises of 5% of the total population.

Generally, in internet industries, different user attributes like demographic information, frequency of usage, user preferences, etc. together constitute the profile of the overall user population. Usually the nature and profile of the end user population vary across the different on-line sites.

Ideally, the profile of the user population in the test and control buckets of bucket testing should be consistent with the actual overall population. The following section briefly describes the motivation behind this work.



II. MOTIVATION In bucket testing owing to random sampling, the profiles  of the test and control buckets often differ to that of the actual user population; thereby leading to misinterpretation of bucket testing results. We demonstrate this point through the data obtained from an experiment conducted on Yahoo?s bucket testing platform with logged-in users.

In the experiment, users in the test bucket (5% of the population) would see popular modules appearing constantly in the right rail of the page as they navigate through the article pages. In contrast, users in the control bucket were shown empty right rail. The experiment was conducted to evaluate if this change can really increase the click-through rate (CTR) of the Yahoo article pages. The particulars of the experiment are not disclosed in this paper as part of standard business practices. Table I compares the actual user distribution with the distributions in the control and test buckets during the experiment based on the users? frequency of usage.

TABLE I.  USERS PARTITIONED BY FREQUENCY OF USAGE  Usage User Distribution In  Actual population Control bucket Test bucket  Category I 55.98% 73.73% 67.56%  Category II 14.64% 16.44% 18.28%  Category III 24.94% 9.65% 13.2%  Category IV 4.44% 0.18% 0.96%    As shown in table I, the user distributions vary greatly for some sections of the population. The new feature experimented can either positively or negatively impact one or more sections of the population. Since the affected sections can be undermined or overestimated in the test and control buckets, the overall split testing results should be adjusted before interpreting them. Interestingly, there are occasions where experiments, not showing improvements in user engagements, were abandoned, which could otherwise have been adopted, if they were properly interpreted. Our proposed approach presented in the next section resolves this issue.

2013 IEEE International Congress on Big Data  DOI 10.1109/BigData.Congress.2013.79

III. APPROACH In this section, we first present the concept of well  behaved user attributes before discussing the detailed approach. We define a user attribute as well behaved if the user distribution based on it tends to remain constant over a considerable period of time. Analysis of the big user data shows that demographic attributes like age, gender, country, and usage are examples of well behaved user attributes. It is interesting to note that these attributes behave well independently as well as together with other well behaved attributes. Thus, the user distributions, based on more than one well behaved attribute, also show negligible variance over time. The distribution of users across a number of Yahoo pages based on different combinations of well behaved user attributes was examined for five successive weeks. Interestingly, irrespective of changes in the total traffic, the percentage of total users for each combination of the well behaved attributes remained nearly same. The results for some of the combinations of the attributes age, gender and usage are reported in fig. 1. For each combination of the well behaved attributes, we can calculate its corresponding percentage of total users from the overall data and consider them as their corresponding weights. Now, the user engagement metric M for each bucket (test and control) will be calculated as,  ?= i wM i * (mi / ui)  where, wi represents the previously computed weight, mi and ui represent the user engagement metric and number of users respectively computed from the current bucket corresponding to the ith combination of the well behaved attributes. Now, we can compare the user engagement metric of control bucket and test bucket to reach the inference.

While calculating the weights, we can also consider historical data along with the actual data of the bucket testing period to remove minor aberrations, if any.

Figure 1.  User distribution across certain Yahoo pages for certain combinations of age, gender and usage over five successive weeks.



IV. EXPERIMENTAL RESULTS In this section, we compare the results of the experiment  cited in section I. In the experiment, 5% of the total users belonged to test and control buckets. The enhancement was included in a number of Yahoo pages and released to all the users, after it was verified from the bucket testing results that the change significantly increased the CTR and user engagement. The increase in CTR percentage, as computed from the experimental data through our algorithm is very close to the actual CTR improvement percentage seen after the production release. The result is summarized in table II for two of the Yahoo sites that have gone live with this enhancement.

TABLE II.  COMPARING CTR IMPROVEMENT PERCENTAGES  CTR Percentage Increase  Direct bucket testing results  Derived from our algorithm  Evaluated from real data after  release  5.97% 5.37% 5.18%  3.34% 2.76% 2.8%   These encouraging results show that our big audience  data driven approach for interpreting bucket testing results is practical and effective. Our algorithm has more effectively interpreted the bucket testing results and predicted the actual scenario. We are currently attempting to include other attributes like user interests and preferences to broaden our approach.

