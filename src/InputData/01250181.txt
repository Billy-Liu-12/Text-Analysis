Parallel Mining of Maximal Frequent Itemsets from Databases

Abstract In this paper, we propose a parallel algorithm for  mining maximal frequent itemsets from databases. A frequent itemset is maximal if none of its supersets is frequent. The new parallel algorithm is named Par- allel Max-Miner (PMM), and it is a parallel version of the sequential Max-Miner algorithm [3]. Most of existing mining algorithms discover the frequent k- itemsets on the kth pass over the databases, and then generate the candidate (k + 1)-itemsets for the next pass. Compared to those level-wise algorithms, PMM looks ahead at each pass and prunes more candidate itemsets by checking the frequences of their supersets.

We implemented PMM on a cluster of workstations, and evaluated its performance for various cases. PMM demonstrated better performance than other sequen- tial and parallel algorithms, and its performance is quite scalable, even when there are large maximal fre- quent itemsets (i.e., long patterns) in databases.

Key words: Parallel data mining, maximal frequent itemsets, association rules, scalability.

1 Introduction Let I = {i1, i2, . . . , im} be a set of items. Let D be  a set of transactions, where each transaction T con- tains a set of items. An association rule is an impli- cation of the form X ? Y , where X ? I, Y ? I, and X ? Y = ?. The association rule X ? Y holds in the database D with confidence c if c% of transactions in D that contain X also contain Y . The association rule X ? Y has support s if s% of transactions in D contain X ? Y . Mining association rules is to find all associ- ation rules that have support and confidence greater than or equal to the user-specified minimum support (called minsup) and minimum confidence (called min- conf), respectively [1]. The first step in the discovery of association rules is to find each set of items (called itemset) that have co-occurrence rate above the mini-  ?This research was supported in part by Ohio Board of Re- gents, LexisNexis, NCR, and Wright Brothers Institute (WBI).

mum support. An itemset with at least the minimum support is called a frequent itemset. The size of an itemset represents the number of items contained in the itemset, and an itemset containing k items will be called a k-itemset. The second step of forming the association rules from the frequent itemsets is straight- forward as described in [1]: For every frequent itemset f , find all non-empty subsets of f . For every such sub- set a, generate a rule of the form a ? (f ? a) if the ratio of support(f) to support(a) is at least minconf.

In mining association rules, the most time- consuming job is finding all frequent itemsets from a large database with respect to a given minimum sup- port. Many sequential and parallel algorithms have been proposed to solve this problem. The most com- mon sequential algorithms are Apriori [1] and its varia- tions. Apriori-like algorithms employ a strict bottom- up, breadth-first search and enumerate every frequent itemset [3]. They require multiple passes over the database. In the first pass, the occurrences of individ- ual items are counted and frequent 1-itemsets are de- termined. Then the frequent 1-items are used to gen- erate the potentially frequent 2-itemsets, called can- didate 2-itemsets. In the second pass, we count the occurrences of the candidate 2-itemsets, so that we can determine the frequent 2-itemsets. Frequent 2- itemsets are used to generate the candidate 3-itemsets, and so on. This process is repeated until there is no new candidate itemset generated.

In Apriori, if Fk?1 denotes the set of frequent (k ? 1)-itemsets discovered in the (k ? 1)th pass over the database, then the set of candidate k-itemsets for the next pass, denoted by Ck, is obtained by the natural join of Fk?1 and Fk?1 on the first k ? 2 items. For example, if F2 includes {1, 2} and {1, 3}, then {1, 2, 3} is a candidate 3-itemset. However, a candidate k-itemset is pruned if any of its (k ? 1)-subsets is not frequent. Thus, for {1, 2, 3} to remain as a candidate 3-itemset, {2, 3} also should be a frequent 2-itemset.

This subset-infrequency based pruning step prevents many candidate k-itemsets from being counted in each      pass k.

Due to the large size of the database to be mined,  parallel data mining is a very promising direction, and a few parallel algorithms were proposed for mining as- sociation rules. The most well-known one is Count Distribution (CD) algorithm [2], which is a parallel version of the Apriori algorithm. The database is partitioned and distributed over multiple processing nodes initially. At each pass k, every node collects local counts of the same set of candidate k-itemsets.

Then, these counts are merged between processing nodes, so that each node can identify the frequent k- itemsets and generate the candidate (k + 1)-itemsets for the next pass, as in Apriori. To merge the lo- cal support counts of the candidate itemsets, synchro- nization between nodes is required at every pass, and maintaining the same set of candidate itemsets in all the nodes is redundant.

In Apriori-like algorithms, if there is a frequent itemset with length l, then they will generate and count its 2l subsets. This exponential complexity makes Apriori-like algorithms just suit for mining small frequent itemsets (i.e., short patterns) [3]. To address this problem, algorithms that can efficiently mine the maximal frequent itemsets (MFI) were pro- posed. The basic idea is that if we find a large frequent itemset (i.e, long pattern) early, we can avoid count- ing all its subsets because they are all frequent. So, these algorithms always look ahead and try to find large frequent itemsets early. Once we find all the maximal frequent itemsets, all frequent itemsets can be obtained from them.

In this paper, we propose a new parallel algorithm, named Parallel Max-Miner (PMM), for mining max- imal frequent itemsets. PMM is a parallel version of the sequential Max-Miner algorithm [3]. However, it requires multiple passes over the database, like the Count Distribution algorithm. In each pass, all the nodes have the same set of candidate groups to be counted; and after each pass, the count information is exchanged between nodes, so that every node holds the same global count information and then generates the same candidate groups for the next pass. PMM uses this bottom-up search, but it looks ahead at each pass and prunes more candidate itemsets by check- ing the frequences of their supersets. Moreover, PMM does not require the transfer of transactions between nodes.

2 Max-Miner Algorithm Unlike Apriori-like algorithms, the Max-Miner al-  gorithm [3] extracts only the maximal frequent item- sets, from which all the frequent itemsets can be ob-  tained. Max-Miner always attempts to look ahead in order to identify large frequent itemsets early, so that all subsets of these discovered frequent itemsets can be pruned from the search space. This method is called superset-frequency based pruning. Max-miner used the set-enumeration tree to represent the search space. How to construct the set enumeration tree is illustrated in Figure 1 for four items: 1, 2, 3, and 4.

The set-enumeration tree lists all combinations of the four items from level 0 to level 4.

Level   0  Level   1  Level   2  Level   3  Level   4  ?????????? ???????????????{}  1 2 ??????????? ???????????   3,41,2 ????????????? ?????????? ?  1,41,3 2,3 2,4  2,3,41,3,41,2,3 1,2,4  1,2,3,4  ????????????? ?????????? ? ??????????????? ?????????? ?  ?????????? ???????	?	???  Figure 1: Search space of Max-Miner  Each node in the tree is called a candidate group, and a candidate group g consists of two components which are actually two itemsets. The first itemset is called the head of the group and denoted by h(g). The second itemset is called the tail of the group and de- noted by t(g). t(g) is an ordered set and contains all the items not in h(g) but can potentially appear in any subnode derived from node g. For example, the node (i.e. candidate group) g1 enumerating item 1, which is the leftmost node at level 1, has two components: h(g1) = {1} and t(g1) = {2, 3, 4}.

The main procedure of Max-Miner can be explained as follows. From the root of the tree at level 0, we count the support of 1-itemsets. Only the 1-itemsets which are frequent can be enumerated at level 1. In this example, 4 nodes are generated at level 1 if 1, 2, 3, and 4 are all frequent 1-itemsets. For the node g1, which corresponds to item 1 at level 1, we need to count the support of {h(g1)  ? t(g1)} = {1, 2, 3, 4}. If  the support of {h(g1) ?  t(g1)} is equal to or greater than minsup, then we do not need to expand the tree from the node g1 anymore. Similarly for the node g2, which corresponds to item 2 at level 1, the support of {h(g2)  ? t(g2)} = {2, 3, 4} is counted. If  {h(g2) ?  t(g2)} is frequent, then the subnodes of node g2 are not generated at level 2.

At any node g, if {h(g)? t(g)} is not frequent, for each item i in t(g), we check if {h(g)? i} is frequent.

If {h(g)? i} is frequent, a corresponding subnode is generated. The head of the subnode is {h(g)? i} and its tail contains all the item j, such that {h(g)? j} is frequent and j comes after i in t(g). For example, if {1, 2}, {1, 3} and {1, 4} are frequent when {1, 2, 3, 4} is not frequent, then we generate three subnodes of node g1. They are enumerated at level 2 as: [h(g?) = {1, 2}, t(g?) = {3, 4}], [h(g??) = {1, 3}, t(g??) = {4}], and [h(g???) = {1, 4}, t(g???) = {}]. This process continues level by level until the whole tree is completed.

Here, we can notice that, for a candidate group node g, if an item appears last in the tail of g in order- ing, it will appear in most offsprings of the node g [3].

For example, item 4 is the last item in the tail of the root node in Figure 1, and appears in either the head or tail of every node at level 1 and below. On the other hand, item 2 appears in a much smaller number of nodes. Thus, to discover the long patterns (maximal frequent itemsets) early, we better order the subnodes of each node in ascending order of their support. At level 1 of the set-enumeration tree, Max-Miner orders the frequent 1-itemsets in ascending order of their sup- port. And at lower levels, it orders the tail items of each node g in ascending order of support({h(g)? i}), for i ? t(g). In the above example, if support({1, 4}) ? support({1, 2}) ? support({1, 3}) when we gener- ate the subnodes of node g1, then the items are re- ordered in its tail t(g1) as {4, 2, 3}; and the corre- sponding subnodes are generated in that order as: [h(g?) = {1, 4}, t(g?) = {2, 3}], [h(g??) = {1, 2}, t(g??) = {3}], and [h(g???) = {1, 3}, t(g???) = {}]. This heuristic reordering strategy increases the effectiveness of the superset-frequency based pruning.

In [3], they also proposed how to estimate a lower- bound of the support of each itemset using the sup- port of some of its subsets. By discovering the max- imal frequent itemsets early, Max-Miner can reduce the number of passes over the database. As a result, it can reduce the total execution time considerably, compared to other level-wise algorithms.

3 Parallel Max-Miner (PMM) Algo- rithm  We present the PMM algorithm in this section. It is implemented on a shared-nothing multiprocessor sys- tem where each node has private memory and a pri- vate disk. All nodes communicate with each other by passing messages through a communication net- work. The database is evenly divided into N parti- tions {D0, D1, D2, . . . , DN?1}, one for each of the N nodes {P 0, P 1, P 2, . . . , PN?1} involved in the parallel mining; i.e., each node has the same number of trans- actions allocated.

PMM requires multiple passes over the database.

For each pass k, all the nodes have exactly the same set of candidate groups, which is denoted by Ck, and each node counts the occurrencies of the candidate groups in the local database, independently. At the end of each pass, all nodes exchange the count infor- mation so that they can generate the same set of can- didate groups Ck+1 for the next pass. After the first pass, each node determines which items are frequent, and in the second pass, each node counts the occurren- cies of all the pairs of frequent 1-itemsets using a two- dimensional data array to maintain the counts of the candidate 2-itemsets appearing in the local database.

Here, we do not count the occurrences of the large itemset {h(g)? t(g)} of each candidate group g be- cause those large itemsets turn out to be infrequent most of the time at this stage, as reported in [3].

After exchanging and merging of the local counts of the candidate 2-itemsets, every node can identify the same set of frequent 2-itemsets. Then, each node generates the same set of candidate groups for the fol- lowing third pass. The procedure of generating the candidate groups for the kth pass, k ? 3, is exactly same as that of sequential Max-Miner algorithm. For example, if {1, 2}, {1, 3}, and {1, 4} are frequent 2- itemsets, then we generate candidate groups whose heads are {1, 2, 3}, {1, 2, 4}, and {1, 3, 4}, respectively.

In general, each candidate group generated for the kth pass has k items in its head.

This process of counting candidate groups, ex- changing and merging count information, and gener- ating candidate groups for the next pass is repeated until there is no more candidate group for the next pass. PMM algorithm is simple and efficient, and it has lower communication overhead because it does not require the transferring of transactions between nodes during the processing. Like Count Distribution, PMM requires the synchronization between nodes to exchange the count information after each pass. How- ever, the number of passes required in PMM is smaller than that of Count Distribution because the superset- frequency based pruning is performed in each pass by looking ahead to find maximal frequent itemsets.

3.1 Cube-based Communication between Processors  To perform the communication between nodes effi- ciently, we impose a logical binary n-cube structure on the processing nodes. Then the nodes can exchange and merge the local count information through in- creasing dimensional links between them [4]. In the n-cube, there are 2n nodes, and each node has n-bit binary address. Also, each node has n neighbor nodes      which are directly linked to that node through differ- ent dimensional links. For example, there are 8 nodes in a 3-cube structure, and node (000)2 is directly con- nected to (001)2, (010)2 and (100)2 through a 1st- dimensional link, a 2nd-dimensional link, and a 3rd- dimensional link, respectively. Thus, in the n-cube, all the nodes can exchange and merge their local counts in n steps, through each of the n different dimensional links.

3.2 Pseudo-code of PMM As we assume a homogeneous distributed comput-  ing environment where all the nodes are the same, we just give the pseudo-code of the PMM algorithm run- ning on a node P i.

/* Pass 1 */  P i counts the occurrencies of items in Di;  n = log2 N ; /* N nodes are used for mining */  for (j = 1; j ? n; j + +) P i exchange and merge the local counts of items  with a neighbor node through a j-dimensional link;  P i determines F1; /* F1 is the set of frequent 1-itemsets */  /* Pass 2 */  P i generates C2 by pairing the members of F1;  P i counts the occurrences of the C2 members in D i;  for (j = 1; j ? n; j + +) P i exchange and merge the local counts of C2 members  with a neighbor node through a j-dimensional link;  P i determines F2 /* F2 is the set of frequent 2-itemsets */  P i generates C3 /* all nodes have same C3 */  /* C3 includes all candidate groups generated based on F2 for pass 3 */  /* Pass k, for k ? 3 */ /* Ck includes all candidate groups generated based on Fk?1 for pass k */ while (Ck ?= ?) {  P i scans Di to count the candidate groups in Ck;  /* counting all {head ? i}, for i ? tail, and {head ? tail} for each candidate group in Ck */  for (j = 1; j ? n; j + +) P i exchange and merge the local counts of Ck members  with a neighbor node through a j-dimensional link;  P i identifies frequent itemsets;  P i inserts frequent itemsets into GM , and keeps only maximal frequent itemsets in GM ;  P i generates Ck+1 using Fk; /*superset-frequency based pruning is applied */  k + +;  } /* GM is the set of all maximal frequent itemsets */  4 Performance Evaluation Our test platform is an 8-node Linux cluster system  where nodes are connected by a Fast Ethernet switch.

Each node has a 800 Mhz Pentium processor, 512 MB memory, and a 40 GB disk drive. The processes are communicating using the MPI (Message Passing In- terface).

The databases used in our experiments are syn- thetic sales transaction databases generated as in [1].

All parameters used for generating databases are de- scribed in Table 1. For all databases, c = 0.5, m = 0.5, v = 0.1, |L| = 2000 and NI = 1000. Table 2 lists all databases used in our performance evaluation exper- iments. The size of each database is about 360 MB.

When running the parallel algorithm on a database, we need to partition it into local databases. To bal- ance the size of the local databases, each transaction is randomly allocated to a node.

In order to compare the performance of PMM and Count Distribution, we also implemented Count Dis- tribution on the same platform.

Table 1: Synthetic database parameters |D| Number of transactions in the database |T | Average size of the transactions |I| Average size of the maximal potentially frequent itemsets |L| Number of maximal potentially frequent itemsets NI Number of items c Correlation level m Mean of the corruption level v Variance of the corruption level  Table 2: Databases Name |T | |I| |D| T10 I02 D7852K 10 2 7852K T20 I04 D4288K 20 4 4288K T25 I06 D3504K 25 6 3504K T30 I08 D2954K 30 8 2954K T40 I10 D2256K 40 10 2256K  4.1 Improvement of PMM over Count Distribution  We ran both PMM and Count Distribution on dif- ferent synthetic databases with different minsup val- ues. Table 3 shows the speedup of PMM over Count Distribution (CD). If we define TCD and TPMM as the execution times of CD and PMM, respectively, then the speedup of PMM over CD is TCD/TPMM . In Table 3, the speedup of PMM is shown for different databases listed in the first column and for different values of minsup listed in the first row. In these ex- periments, all 8 nodes in our cluster system were used.

Table 3: Speedup of PMM over CD (8-node case) Database 1% 0.75% 0.5% 0.25% 0.15% 0.1% T10 I02 D7852K 1.01 1.02 1.01 1.07 1.08 1.08 T20 I04 D4288K 1.04 1.02 1.04 1.30 1.64 1.66 T25 I06 D3504K 1.06 1.07 1.45 2.46 2.98 4.19 T30 I08 D2954K 1.05 1.97 2.79 5.82 16.80 21.34 T40 I10 D2256K 1.40 1.70 2.83 9.38 20.10 29.76  When minsup is high, PMM is comparable to or a little bit slower than Count Distribution. We also ran Apriori and Max-Miner for these cases, and found that Max-Miner doesn?t show much improvement over Apriori, either. That is because the high minsup lim- its the number of frequent itemsets and the length of those frequent itemsets. Thus, the effect of look-ahead technique used by Max-Miner is not clearly shown, and naturally PMM has the same result.

As minsup decreases, PMM begins to show more and more improvement in our tests. As shown in Table 3, when the value of parameter |I| of the database is large, such as 8 or 10, even if the minsup is as high as 0.5%, PMM is faster than Count Distribution with a speedup above 2.5. It is because a large |I| value results in larger frequent itemsets (i.e., long patterns), which benefits PMM. If minsup is less than 0.25%, PMM outperforms Count Distribution considerably.

4.2 Synchronization Requirement of PMM and Count Distribution  We compared the number of synchronizations needed between processing nodes in PMM and Count Distribution. Table 4 shows the comparison results.

Here, we define SPMM and SCD as the number of synchronizations needed in PMM and CD, respec- tively. The first row of the table lists various val- ues of minsup and the first column lists the names of databases. The values in each entry of the table represents SPMM : SCD.

Table 4: Comparison of synchronization requirement Database 1% 0.75% 0.5% 0.25% 0.15% 0.1% T10 I02 D7852K 2:3 3:4 5:5 5:9 6:9 5:9 T20 I04 D4288K 4:6 6:6 8:9 10:10 10:11 9:11 T25 I06 D3504K 7:8 7:8 11:11 11:15 10:15 12:16 T30 I08 D2954K 6:6 12:12 13:15 13:16 17:18 17:18 T40 I10 D2256K 11:12 9:13 14:15 16:17 17:18 18:19  As we can see, in most cases, PMM requires less number of synchronizations than Count Distribution, but the difference is small. Our experiments showed that MaxMiner does not reduce the number of passes over the database much, compared to Apriori. As a parallel version of MaxMiner, PMM does not reduce the number of passes much, either.

4.3 Communication Requirement of PMM and Count Distribution  Like Count Distribution, all nodes running PMM have the same set of candidates in each pass. So each node sends and receives the same amount of count in- formation for the candidates. The difference between two algorithms is the meaning of candidates, which determines how much count information must be ex- changed between processing nodes during the mining.

In Count Distribution, its candidates are the poten- tial frequent itemsets generated, as in Apriori. On the other hand, in PMM, candidates are the candi- date groups defined exactly as in Max-Miner, where each candidate group contains a head (itemset) and a tail (itemset). For each candidate group, we can say there are two kinds of candidates: potential fre- quent itemsets, each of which contains the head and one of the tail items, and a potential maximal fre- quent itemset which is the union of the head and the tail. Thus, to exchange the count information for each candidate group, processing nodes need to ex- change multiple integers for the two kinds of candi- dates. It may sound that we need more communica- tion between processing nodes in PMM. However, in reality, PMM?s look-ahead technique can reduce the communication requirement considerably, because if one potential maximal frequent itemset is found fre- quent early, then many candidates can be removed due to the superset-frequency based pruning. So, com- pared with Count Distribution, which does not use the superset-frequency based pruning, the total amount of count information exchanged in PMM is smaller in most cases.

We implemented two versions of Count Distribu- tion: one is using the n-cube communication, and the other is using the all-to-all communication. We com- pared the average amount of data each node communi- cates with others when we executed PMM and Count Distribution on the T30 I08 D2954K database with various values of minsup, and the results are shown in Figure 2.

Compared with Count Distribution using the all- to-all communication scheme, PMM shows a big im- provement in communication for all cases shown in Figure 2. When both algorithms use the same n-cube communication scheme, the communication overhead of PMM is still a little lower than that of Count Dis- tribution.

4.4 Sensitivity Analysis of PMM In this section, we evaluate the characteristics of  the PMM algorithm in terms of speedup and sizeup.

All tests were performed with a minsup of 0.25%.

1 0.75 0.5 0.4 0.3 0.25 0.2 0.15 0.1 Minimum Support (%)  D a ta  C o m  m u n ic  a tio  n (  M B  ) PMM  Count Distribution(n-Cube)  Count Distribution(all-to-all)  Figure 2: Comparison of communication requirement  4.4.1 Speedup  We evaluated the speedup of PMM as the number of nodes increases while the database size remains the same. For the databases listed in Table 2, we kept the same database size of 360 MB, but the database was partitioned into 2, 4, and 8 parts when the number of nodes were 2, 4, and 8, respectively.

Figure 3 shows the execution time of PMM on the 2-node, 4-node, and 8-node systems. To demonstrate the speedup, we also ran the sequential Max-Miner for each database on a single node. As the number of nodes is doubled, the execution time of PMM de- creases by about 40% to 50%. In PMM, all the nodes generate and count the same set of candidates in each pass, no matter how many nodes are used. Thus, PMM shows an almost linear speedup.

0 2 4 6 8 10  Number of Nodes  E xe  cu tio  n T  im e  ( se  c)  T10_I02_D7852K T20_I04_D4288K T25_I06_D3504K T30_I08_D2954K T40_I10_D2256K  Figure 3: Speedup of PMM  4.4.2 Sizeup  For the sizeup test, we fixed the system to the 8-node configuration, and distributed each database listed in Table 2 to the 8 nodes. Then, we increased the local  database size at each node from 45 MB to 215 MB by duplicating the initial database partition allocated to the node. Thus, the data distribution characteristics remain the same as the local database size is increased.

The results shown in Figure 4 indicates that PMM has a very good sizeup property. Since increasing the size of local database doesn?t affect the local mining result of PMM at each node, the total execution time increased just due to more disk I/O and computation cost which scaled almost linearly with sizeup.

0 45 90 135 180 225 270 Amount of Data per Node (MB)  E xe  cu tio  n T  im e  ( se  c)  T10_I02_D7852K T20_I04_D4288K T25_I06_D3504K T30_I08_D2954K T40_I10_D2256K  Figure 4: Sizeup of PMM  5 Conclusions In this paper, we proposed a parallel maximal fre-  quent itemset mining algorithm, named Parallel Max- Miner (PMM), for shared-nothing multiprocessor sys- tems. PMM is based on the sequential Max-Miner al- gorithm and avoids enumerating all potential frequent itemsets. So, PMM has much lower computation cost than Count Distribution which is a parallel version the Apriori algorithm. A cube-based communication scheme is employed by PMM for efficient communica- tion between processing nodes. PMM also has very good speedup and sizeup properties, as evidenced by the corresponding test results.

