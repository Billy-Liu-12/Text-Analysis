A Calculation Mechanism for Similarity Measure with Clustering an Unbalanced Hierarchical Terminology Structure

Abstract  The effective retrieval of reverent information often is quite useful to the user, for example, to query the respectful knowledge or information, especially for on-line e-leaner. The most common method is to make use of synonym and antonym from a dictionary with the most frequent terms. However, sometimes we are focusing on a pair of or a set of associated keywords offered by user, instead of same meaning.

Generally, we would probably adopt the association rule to solve the problem. Nonetheless, the keywords or terms sets extracted from huge queries often contain sparse information composed of a wide range of keywords, with each term set only containing a few terms. These data render basket analysis with extremely low item support, lift the term to a higher level of concept hierarchy may get enough support, but missing the detailed information.

Although a similarity measure represented by counting the depth of the least common ancestor normalized by the depth of the concept tree lifts the limitation of binary equality, it produces counter intuitive results when the concept hierarchy is unbalanced since two terms in deeper subtrees are very likely to have a higher similarity than two terms in shallower subtrees. The research proposes to calculate the distance between two terms by counting the edge traversal needed yet from user s viewpoint to link them in order to solve the issues. The method is straight forward yet achieves better outcome with information query when concept hierarchy is unbalanced.

Keywords: Similarity Measure, Hierarchy, Clustering, Data Mining  1. Introduction In view of the fact that Internet and database  technology grown-up and vigorous application, large amount of data collection through the automation or the semi-automation of software and hardware tools already became various enterprises' daily routine work in the world of the globalization and electronic commerce. Knowledge discovery in database (KDD) may analyze from the massive information and  discover the significant relations or the associations so as to forecast the possible events occurring in the future. Using this technique leads not only the change of user's behavior but also the business model change.

The availability of inexpensive and quality information technology has in many organizations resulted in an abundance of data. In many web applications such as e-learning and e-commerce, personal recommendation system had developed their model as content based filtering, collaborative filtering and hybrid approaches [4]. However, the limitation of the sparse data and cold start obstacle the knowledge discovery. The effective retrieval of reverent information often is quite useful to the user, for example, to query the respectful knowledge or information, especially for on-line e-leaner. The most common method is to make use of synonym and antonym from a dictionary with the most frequent terms. However, sometimes we are focusing on a pair of or a set of associated keywords offered by user, instead of same meaning. Generally, we would probably adopt the association rule to solve the problem. According to the previous research, the query data can be applied with various popular data mining algorithms, such as association rule [1] [5] [15], keywords/document segmentation (clustering) or classification [2]. All of these algorithms assume that query data are a composition of terms and some repeatedly appear in a significant portion of query data so that similar terms or subsets of terminology can be discovered, clustered, and classified.

However, the assumption may not be valid with sparse datasets. The keywords or terms sets extracted from huge queries often contain sparse information composed of a wide range of keywords, with each term set only containing a few terms. These data render association rule with extremely low item support, lift the term to a higher level of concept hierarchy may get enough support, but missing the detailed information!

To alleviate the issue, several researchers have proposed to take into consideration the concept of hierarchy to increase the counts of items. Attribute Oriented Induction [7] is proposed to reduce the number of items by replacing low level items in concept hierarchy with higher level items to increase  0-7695-2934-8/07 $25.00  ? 2007    the number of occurrences. Multiple-Level Association rule [5] is proposed to count the occurrence of items at a higher level with the summation of item counts at a lower level to increase the counts of items at higher levels. Although some patterns or rules of higher level items can be derived with these approaches, a detailed level of knowledge is discarded.

The limitation imposed by binary equality relationships between terms in query data, [12] is proposed to change the relationships to a similarity represented by counting the depth of the least common ancestor normalized by the depth of the concept tree. Although this approach lifts the limitation of binary equality, it produces counter intuitive results when the concept hierarchy is unbalanced since two items in deeper subtrees are very likely to have higher similarity then two items in the shallower subtrees even though the two pairs have to climb the same number of nodes to reach to the least common ancestors. The research proposes to calculate the distances between two items by counting the edge traversal needed from user s viewpoint to link the two items to solve the issues.

The method is straight forward yet reaches better outcomes with information retrieval data when concept hierarchy is unbalanced.

The rest of the paper is organized as follows: section 2 presents the related similarity and distance measures, section 3 proposes the model and methodology, section 4 compares the proposed method against traditional similarity computation models by showing several experiment results, and section 5 summarizes the contribution and future work of the research.

2. Related Works  The similarity of instances can be measured by distance, the higher the similarity, the less the distance. There are a variety of ways to measure the inter-object similarity where it plays an important role [11] [12] [13]. The notion of similarity is used in many contexts to identify objects having common  Among them, the most dominant are correlation measures and distance measures. The correlation measure of similarity does not look at the magnitude but instead at the pattern of the values, while the distance measures focus on the magnitude of values which may have different patterns across attributes. There are several types of distance measures where Euclidean distance and Manhattan distance are the most popular mechanisms to calculate distance between items.

In other cases, the distances are calculated based on itemsets. The itemsets are modeled as vectors in an n-dimensional space. The Vector Model  is a popular model in the information retrieval domain [8]. One advantage of this model is that we can now weigh the components of the vectors, by using schemes such as TF-IDF [14]. The Cosine- Similarity Measure (CSM) defines the similarity between two vectors to be the cosine of the angle between them. This measurement is very popular for query-document and document-document similarity in text retrieval [9] [14].

Some other studies use the Pearson Correlation Coefficient as a similarity measure; GroupLens[4] pertains a vector model a particular item. A collaborative filtering system looks for people sharing common interests [3]. In other cases, the similarity between two itemsets is often determined by set intersection: the more the items have in common, the similar the itemsets are.

Jaccard s Coefficient is one of them [6] [9] [10] [11].

Few existing similarity calculations take into  consideration concept hierarchy. In this research, a bottom-up distance calculation is proposed and various similarity estimations between sets of leaves are compared, and evaluated for different applications.

3. The Proposed Model and the Methodology  3.1 Unbalanced Hierarchy A terminology hierarchy is a tree structure where each internal node represents a summary category of terms. The nodes beneath an internal node represent either sub-categories of the category denoted by the internal node or terms classified into the denoted category. Terms are marked as leaf nodes, and categories are denoted by internal nodes. Except for the root node, each internal node should represent a category. The root node is just a dummy node to link all categories and the nodes below into a tree. Figure 1 shows an example of a terminology hierarchy. The terms (leaves) can be at different levels. In the figure, internal nodes and leaf nodes are marked with strings prefixed with capital letters C and E, respectively.

Readers can find that E1 and E2 are beneath the same category of C1111. In the figure, E1 and E2 are terms and C1111 is a category.

Even though most researches computing distances with the auxiliary of hierarchies assume domain hierarchy is balanced [5] [12], we would like to point out that in many applications unbalanced hierarchies are more natural. For example, an emerging term- EC, emphasizing an abundant variety of deep branches and only a limited selection of Chemistry may have a terminology hierarchy that classifies a few layers of categories.

On the other hand, if a balanced hierarchy is  imposed on the hierarchy, the designers have to face  0-7695-2934-8/07 $25.00  ? 2007    the unpleasant dilemma of either shrinking the category hierarchy under EC or meaninglessly enlarge the hierarchy beneath Chemistry. Shrinking the EC category hierarchy may obscure important information signals and enlarging the Chemistry hierarchy may render the discovered Chemistry information meaningless with few numbers of terms in each category.

Figure 1: Part of terminology hierarchy with 6 levels in a query set  3.2 Computing Distances on Unbalanced Hierarchy To cluster query terms or keywords into groups, a distance calculation mechanism based on unbalanced hierarchies is proposed. With a hierarchy, distance can be computed in top-down or bottom-up manners.

Defining the corresponding distance for the terms (leaf nodes) in the concept hierarchy helps us to define the distance between query sets which we will apply to clustering.

Definition 1: The depth of a node, d(n), is defined as the number of edges needed to be traversed from the root to the node.

For example, d(root) = 0, and d(E1) =5  Definition 2: The Lowest Common Ancestor of two nodes, lca(ni,nj), is defined as the node that is a common ancestor of ni and nj and has the greatest depth among all the common ancestor nodes of n1 and n2. d(n))arg(max)n,lca(n  21 nandnofancestorisn21  For example, lca(E1,E3)=C111, lca(X1,C2)=root.

Definition 3: Minimum Traversal Length of two nodes mtl(ni,nj)= the number of the edges on the shortest path from node ni to nj .

For example, mtl(X4,X5)=2, mtl(E1,E7)=7.

Definition 4: The top-down distance between 2 leaf nodes  .otherwise,1  ., root), nif lca(n )d(n)d(n ), nmtl(n  ), ntdd(n ji  ji  ji  ji  For example, tdd(E1, E2)=2/(5+5)=0.2, tdd(C2,C3)=2/(4+4)=0.25, tdd(E1, E1)=0, and tdd(C1, E2)=1. On the other hand, from a users perspective, they usually don t know how deep the terms are in the hierarchy level, but terms with same concept seem quite similar to them, i.e., same distance, for example, tdd(E1, E2) should be almost equivalent to tdd(C2,C3). We use the branches at level 1 as a different sub-tree. In figure 1, we have 3 sub-trees.

Definition 5: ni is said to be in a subtree rooted by nj if there exist a path originating from ni to nj The sub-tree depth of a node ni,  d(n)max)( nofdescendentais ininstd The level difference of a node ni is defined as ld(ni) = max n is a node in the tree std(n) std(ni)  For example, std(C1)= std(C2) = std(C3)=4, std(E1)= std(E2) = std(E4)=5, ld(C1)=1, and ld(E1)=0.

Definition 6: The bottom-up distance between 2 nodes  otherwise1  , )()(  root), nif lca(n )ld(n)d(n)ld(n)d(n  ), nmtl(n ), nbud(n  ji jjii  ji  ji  For example, bud(C2,C3) = 2/(5+5)=0.2, bud(C1,C2)=3/(4+5)=0.33.

Now we may proceed to define the similarity of objects (query sets). The distance between two query sets is the average of the pair wise minimal distance from the 2 terms. This idea is derived from Jaccard s Coefficient measure with introduced hierarchy, and it is very intuitive and more general.

Definition 7: Let Qa , Qb be two queries. Qa={i1, i2, .im}, Qb ={j1, j2, .jn}, where ii, jj are terms.

The distance between Qa and Qb measured with top-down terminology hierarchy traversal is defined as dt(Qa,Qb)  .

minmin  21 21  mn  )),ji),...,tdd(,j),tdd(i,j(tdd(i)),ji),...,tdd(,j),tdd(i,j(tdd(i ), Qdt(Q  n  j jjj  m  i n  ba  miii  The distance between Ta and Tb measured with bottom-up terminology hierarchy traversal is defined as bdt(Qa,Qb).

.

minmin  21 21  mn  )),ji),...,bud(,j),bud(i,j(bud(i)),ji),...,bud(,j),bud(i,j(bud(i ), Qbdt(Q  n  j jjj  m  i n  ba  miii  For example, Q1 =< E4,E8>, Q2 =< E5,E9>, Q3 =< E4, E5,E7>, Q5 =< E1,C2>, Q6 =< E2,C3>,then  0-7695-2934-8/07 $25.00  ? 2007    bdt(Q1,Q2)=0.225, bdt(Q1,Q3)=0.183, bdt(Q5,Q6)=0.2, and dt(Q5,Q6)=0.225.

Table 1 is a sample query database with Figure 1  as the corresponding concept hierarchy. The top- down and bottom up distances of term D1, D3, D4, C1, C2 , X1, and X6 are shown in Table 2. The distance matrix of Q5, Q6, Q7, Q8, Q9, and Q15 are listed in Table 3.

Table 1. A Sample Query Database.

Query# Terms Query# Terms Q1 E4, E8 Q14 C3, X5 Q2 E5, E9 Q15 X5, X6 Q3 E4, E5, E7 Q16 X3, X6 Q4 E4, E6 Q17 E1 Q5 E1, C2 Q18 X1 Q6 E2, C3 Q19 E1, E3 Q7 E2, C1, C2 Q20 E8, E10 Q8 E3, C5 Q21 E9, E11 Q9 C2, C3 Q22 E12, E13 Q10 C1, C2 Q23 C4, C6 Q11 C1, C3, C3 Q24 C5, C7 Q12 C4, X5 Q25 C8, C9 Q13 C5, X6  Table2. Distance Matrix of Terms with top-down and bottom up methods  Term E1 E3 E4 C1 C2 X1 X6tdd tdd tdd tdd tdd tdd tdd E1 0 0.4 0.56 1 1 1 1 E3 0.4 0 0.56 1 1 1 1 E4 0.56 0.56 0 1 1 1 1 C1 1 1 1 0 0.43 1 1 C2 1 1 1 0.43 0 1 1 X1 1 1 1 1 1 0 0.67 X6 1 1 1 1 1 0.67 0  Term D1 D3 D4 C1 C2 X1 X6bud bud bud bud bud bud bud D1 0 0.4 0.56 1 1 1 1 D3 0.4 0 0.56 1 1 1 1 D4 0.56 0.56 0 1 1 1 1 C1 1 1 1 0 0.33 1 1 C2 1 1 1 0.33 0 1 1 X1 1 1 1 1 1 0 0.67 X6 1 1 1 1 1 0.67 0  Table3. Distance Matrix of Queries with top- down and bottom up methods  Query# Q05 Q06 Q07 Q08 Q09 Q15 dt dt dt dt dt dt  Q05 0 0.26 0.17 0.58 0.31 1 Q06 0.23 0 0.19 0.58 0.31 1 Q07 0.17 0.19 0 0.60 0.34 1 Q08 0.58 0.58 0.60 0 0.81 1 Q09 0.31 0.31 0.34 0.81 0 1 Q15 1 1 1 1 1 0  Query# Q05 Q06 Q07 Q08 Q09 Q15 bdt bdt bdt bdt bdt bdt  Q05 0 0.2 0.15 0.5 0.3 1 Q06 0.2 0 0.15 0.5 0.3 1 Q07 0.15 0.15 0 0.50 0.31 1 Q08 0.5 0.5 0.50 0 0.7 1 Q09 0.3 0.3 0.31 0.7 0 1 Q15 1 1 1 1 1 0  3.3 Algorithm of Computing Query Distance with an Unbalanced Hierarchy After defining the distance of the terms in the  concept hierarchy, we then apply the distance between queries to clustering and a brief example demonstration, this section describes the algorithm of computing query distance with hierarchy.

Algorithm : Clustering Queries with an Unbalanced Terminology Structure  Input: 2. UTermH - an unbalanced terminology hierarchy 3. QDB query database where each query  contains terms in UTermH 4. MaxQDist - threshold of the maximum distance  between 2 queries 5. MaxMergeDist threshold of the maximum  distance while merging cluster Output: 1. ClQ - Clusters of Queries, 2. QCC - Query count within each cluster, CAD -  average distance within each cluster, MIntraDist - average intra-distance for all clusters (whole model), and MInterDist - average inter-distance of all clusters (whole model)  Method:  1. For each pair of terms ti, tj UTermH, compute the bud(ti, tj)  2. Initialize the QueryCand = , Initialize the  ClQ = 3. For each pair of queries qa:<t1, t2 tm>,  qb:<t1, t2 tn> QDB, compute dt(qa, qb), if dt(qa, qb) MaxTranDist, insert dt(qa,  qb) to QueryCand 4. Sort dt(qa, qb) in QueryCand ascending 5. For the smallest dt(qa, qb) MaxMergeDist  if set of ClQ = or (qa and qb any ClQ)  then create a new cluster ClQi, and insert qa, qb to this  ClQi  if (qa ClQi and qb any ClQ) then insert qb to ClQi  if (qa any ClQ and qb ClQi) then insert qa to ClQi  if (qa ClQi and qb ClQj) then merge ClQi ,ClQj  if (qa ClQi and qb ClQi) then continue  Compute QCC, CAD, MIntraDist and MIterDist, Maximun QCC with CAD  Step++ Next smallest dt(qa, qb)  Figure 2: Algorithm: Clustering Queries with an Unbalanced Hierarchy.

0-7695-2934-8/07 $25.00  ? 2007    4. Experimental Results  To verify the usefulness of the proposed distance measurement, the measurement is applied to a query set derived from a simulated dataset, which has over 30,000 terms organized into an unbalanced hierarchy and a sparse query set. The experiment shows that with traditional distance measuring tools, neither association rules nor clustering can render reasonable knowledge, whereas, with the proposed methods, reasonable clustering can be grouped.

4.1 Data Description The dataset contains over 30,000 terms organized  into a five-level terminology hierarchy with 1 root, 12 level-1 categories, 144 level-2 categories, 602 level-3 categories and level-4 as terms. On average, each level-3 category has 62 terms. The dataset contains 4,041 queries. Being interested in the association of the terms, we kept only queries that contained 2 to 5 terms. The top two most popular terms are shown in 164 queries (4.06%) and 141 queries (3.5%), respectively. When applying Apriori [1][15] to the dataset, we found that no patterns can be formed with a support threshold at 0.3%.

When trying to cluster the terms with AHC (Agglomerative Hierarchical Clustering) with single link distance calculation. The number of queries and query terms that remained was 4,041 and 11,243, respectively.

4.2 Comparisons of the different distance measures A good clustering method should produce high  quality clusters with high intra-cluster similarity (low intra-cluster distance) and low inter-cluster similarity (high inter-cluster distance).

In this experiment, we compared 4 similarity measures including JC, balanced hierarchy, top-down, and bottom-up methods with an unbalanced hierarchy.

Single link AHC is used to perform the clustering where two clusters with the nearest distances among the grouped queries are merged. The distances are normalized to between 0 and 1.

The quality of the clustering results is measured by the number of queries in the largest cluster, and average intra and average inter distances. The first indicator shows the effectiveness of the proposed distance calculation in clustering queries. Fig. 3 shows that regardless of increased category levels, the bottom-up distance method yields the largest clusters whereas the Jaccard method yields the tiniest clusters and the top-down distance method s result is between the two. In the experiment, increased category level is achieved by inserting a dummy node between each of the top five hundred terms and the  corresponding concept. Figure 4 shows that bottom- up method has better intra distances than the top down methods while also having more queries in the clusters. Figure 5 shows that given the threshold of intra cluster distances, the clusters grouped by the bottom up method have larger clusters compared with the other two methods.

Figure 3: Query counts of the maximal clusters.

Figure 4: Intra-cluster distance with BU and TD.

Figure 5: Query counts of the largest clusters with category level increased by 1.

0-7695-2934-8/07 $25.00  ? 2007    Figure 6: Query counts of the largest clusters with category level increased by 5.

Figure 7: Intra distance of each merge step with category level increased by 3.

Figure 8: Intra distance of each merge step with category level increased by 10  The advantage is more significant when the category level of concept hierarchy is increased by five as shown in Figure 6. The bottom-up method even beats the other methods from the beginning of the algorithm execution. From Figure 7 and 8, the reader can find that the bottom up method has a smaller intra distance at every step of the clustering merge and the benefit is more significant when the concept hierarchy is skewed. With all the data, readers can find that the bottom up method yields the best clustering result while the traditional Jaccard method yields the worst clustering result, and the performance of the top-down method lies between.

The more significant the difference the more skewed the concept hierarchy is.

5. Conclusions and Future Works This study presents a novel clustering scheme  based on a similarity (distance) measure of queries in an Unbalanced Hierarchical Terminology Structure.

We also provide an experimental comparison of our measures against traditional similarity measures, and evaluate how well our measures match human intuition. Clustering via this similarity measure offers valuable information for the end users.

In summary, the main contributions of this research are as follows.

1. We introduce concise similarity (distance) measures that can exploit unbalanced hierarchical domain structure, leading to similarity scores that are more intuitive than the ones generated by traditional similarity measures.

2. We analyze the differences between various measures, compare them empirically, and show that most of them are very different from measures that  loit the domain hierarchy.

The study applies the unbalanced concept hierarchy to clusters only. The line of research can be extended to discovering other knowledge, such as the top clusters with frequent similar terminology sub-tree could be used as recommend system while applying keyword query instead of only synonym and antonym.

Therefore, the research presents a possibility for many other related researches.

6. References [1] Agrawal, R. Srikant, Fast Algorithms for Mining Association Rules , Proc. of the 20th Int'l Conference on Very Large Databases, Santiago, Chile (1994) [2] Ball, G.H. and Hall, D.J. ISODATA: a novel technique for data analysis and pattern classification , Standford Res. Inst., Menlo Park, CA (1965) [3] Golsberg, D., Nichols, D., Oki, B. M., Terry, D., Using collaborative filtering to weave an information tapestry , Commun. ACM 35, 12, (1992) 61 70 [4] Herlocker et al., An Algorithmic Framework for Performing Collaborative Filtering , Proceedings of the 1999 Conference on Research and Development in Information Retrieval (1999) [5] J. Han, and Y. Fu., Mining multiple-level association and Data Engineering , 11(5), (1999) 798-805 [6] Jiawei Han and Micheline Kamber, Data Mining: Concepts and Techniques, Morgan Kaufmann (2000) [7] Jiawei Han, Yandong Cai, Nick Cercone, Knowledge Discovery in Databases: An Attribute-Oriented Approach , VLDB (1992) 547-559 [8] McGill, M. J., Introduction to Modern Information Retrieval, McGraw-Hill 1983 [9] Mehmed Kantardzic, Data Mining: concepts, models, methods, and algorithms, John Wiley (2002) [10] Pang-Ning Tan, Michael Steinbach, and Vipin Kumar, Introduction to Data Mining, Pearson International Edition (2005) [11] Pang-Ning Tan, Vipin Kumar, and Jaideep Srivastava, Selecting the right objective measure for association analysis . Information Systems 29, (2004) 293 313 [12] Prasanna Ganesan , Hector Garcia-Molina , Jennifer Widom, Exploiting hierarchical domain structure to compute similarity , ACM Transactions on Information Systems, Vol. 21 Issue. 1 (2003/1/1) [13] S. Chen, J. Han, and P.S. Yu., Data Mining: An Overview from a Database Perspective . IEEE Transactions on Knowledge and Data Engineering, 8(6): (1996) 866-883 [14] Salton, G., Buckley, C., Term-weighting approaches in automatic text retrieval , Inf. Process.

