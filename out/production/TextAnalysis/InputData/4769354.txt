A New Approach of Dynamic Encoded Bitmap Indexing

Abstract-In this paper we have proposed a new approach of  Encoded Bitmap Indexing [3] that makes the Encoded  Bitmap Indexing Technique well defined for most of the  selection queries by using association rule of data mining.

Furthermore, this approach makes Encoded Bitmap  Indexing (EBI) up to date after specific duration. Actually,  this thesis incorporates an idea of using dynamic n-items  pattern selection for Encoded Bitmap Indexing technique  lookup table to be well defined. We have used 2-items  pattern selection of query predicate, 3-items pattern  selection of query predicate up to n-items pattern selection  of query predicate to select the most frequent pattern for the  lookup table dynamically, consisting of database table  attribute distinct values of character string and numbers  (integer). The main feature of our work is that we have used  association rule of data mining to determine the most  frequent n-items pattern. It is the first time we select the n-  items most frequent pattern dynamically for each attribute  of the table which involved in indexing the table data.

Keywords: Encoded Bitmap Indexing, Data Mining, Lookup  Table, Query Predicate Pattern, Most Frequent N-items Pattern.



I. Introduction  After the evaluation and development of Database  System, indexing [9] is one of the important issues in the  computer science for finding the required data from the  database system efficiently. As the advancement and  increasing Database application areas, the need of  advance indexing technique is increasing day by day.

Again recently another type of database system evolves  i.e. the data warehouse system [4][5], which requires  more advances indexing technique. That?s why more and  more research is going for finding and development of the  advance indexing techniques.

Therefore, the Encoded Bitmap Indexing [3] technique  has evolved that speed up query processing up to 20% to  40% are typical, and saves the indexing space at a  logarithmic scale. Encoded Bitmap Indexing uses lookup  table to store the encoded value of the keys and the  bitmap vectors that stores the actual indexes of the  database table.

In our proposed indexing technique, we first scan and  read the query history file for the attribute of the  dimension table of which lookup table to be remapped to  select the actual pattern (which is most frequent)  dynamically and have used this pattern to remap the  lookup table. We have used association rule of data  mining for selecting the n-items pattern. We have  evaluated the support and confidence for 2-items ruleset,  3-items ruleset up to n-items ruleset to find the actual  pattern. When the confidence of any n-item ruleset is  equal to or greater than the minimum threshold value then  we have taken that n-item ruleset in the candidate ruleset.

In this way, we find the most frequent n-item ruleset and  thus find the actual pattern for the lookup table key values  arrangement. Then this pattern is used to arrange the  lookup table.



II. Literature Review A.  Encoded Bitmap Indexing  Given a table },{ 1 ntt LLLL? , where tj is a tuple of  )1{ nj L=?  , let A be an attribute of ? , denoted by  ? .A, and the domain of A be },{ 1 maa LL . Then, a  encoded bitmap index BA on ? . A, is a set of bitmap  vectors },,{ 01 BBk LL? , a one-to-one mapping (M A : A  -> })1,0},1,0{|{ 01 ?=>< ?? kibbb ik LL       ) and a set of retrieval Boolean functions ( },,{ 1 ana ff LL ),  where ? ?mk 2log= . The bitmap vectors are defined as follows: iB?  (i=0?k-1), tj (j=1?n) => Bi [j] = 1 if  M A (? .A) [i] = 1, else Bi[j] = 0, where Bi[j] denotes the j-  th bit of Bi and M A (? .A) [i] the i-th bit (from LSB to  MSB) M A (? .A). In addition, },,{     maa L1???  , the  retrieval function for ? , ?  f is a k-variable min-term  (i.e., a fundamental conjunction) xk-1,?x0, where xi = Bi,  if M A (? .A) [i] = 1, otherwise xi = Bi (i=0,?k-1). Let us  suppose an attribute STATE (Figure 1), with 3 possible  values: {NY, MA, CA}. In the simple bitmap case, we  would need 3 vectors. However, in the encoded bitmap  index case, ? ?3log 2  = 2 bitmap vectors would suffice.

Figure 1.  Encoded Bitmap indexing of typical STATE  attribute  NY ?  MA ?  CA  MA  NY  1 0 0  0 1 0  0 0 1  0 1 0  1 0 0  0 0  0 1  1 0  0 1  0 0  NY 00  MA 01  CA 10  Table?     BNY BMA BCA   B1     B0        Mapping Table  ICECE 2008, 20-22 December 2008, Dhaka, Bangladesh  978-1-4244-2015-5/08/$25.00 (c)2008 IEEE 974    A retrieval function must be defined. Say A is a value  encoded as 01 ,, bbk LL? . The retrieval function Af  is  defined as  Af  = 01 xxk LL?  Where xi is Bi , if Bi = 1, and the negation of Bi , call it Bi?? otherwise. In our example, CAf = B1B0?. We will represent the OR of two retrieval functions as Af  + Bf  ,  and the AND, as  Af Bf .

B. Data Mining  Data mining [7][8] is a process that uses a variety of data  analysis tools to discover patterns and relationships in  data that may be used to make valid predictions. Data  mining consists of finding interesting patterns in large  datasets in order to guide decisions about future activities.

The first and simplest analytical step in data mining is to  describe the data ? summarize its statistical attributes  (such as means and standard deviations), visually review  it using charts and graphs, and look for potentially  meaningful links among variables (such as values that  often occur together).

C. Association Rule  Association rule mining searches for interesting  relationship among items in a given data set. Suppose we  want to learn about the relationship between item pen  and ink in a file. This information that pen tends to stay  closer to ink in a file is represented in association rule  below:                                    {pen}  ? {ink}  More generally an association rule has the form LHS ? RHS, where both LHS and RHS are the sets of items. The  interpretation of such rule is that if every item in LHS is  occurred in a string, then it is likely that the items in RHS  are occurred as well.

There are two important measures of association rule:  Support: The support for a set of items is the percentage  of occurrences that contain all desired items together. For  example, consider the rule {pen} => {ink}. The support  of this rule is the frequency of the itemset {pen, ink} or  pen ink.

Confidence: The confidence for a rule LHS => RHS is  the percentage of such occurrences that also contain all  items in RHS. More precisely, the confidence of a rule is  an indication of the strength of the rule. As an example  (Table 1), consider again the rule {pen} => {ink}. The  confidence of this rule is  sup(penUink) / sup(pen).

Rules that satisfy both minimum threshold value of  support (min_sup) and minimum threshold value of  confidence (min_conf) are called strong.

Table 1 Transaction Table  Transaction Item  111 pen, ink, milk, juice  112 pen, ink, milk  113 pen, milk  114 pen, ink, juice    Consider the rule {pen}  => {ink}.

The support of this rule  = No. of Occurrences of {pen, ink} / Total Transaction  = 3 / 4 = 0.75 x 100 = 75 %  The confidence of this rule  = Sup(pen, ink) / Sup(pen) = 0.75 / (4/4) = 0.75 / 1 = 0.75  x 100 = 75 %.



III. Existing Methods of Bitmap Encoding   Bitmap Indexing [1][2] have many variations and they are  evolved for different purpose. The most recent variations  on Bitmap Indexing are the Encoded Bitmap Indexing [3].

It has evolved for overcoming the problem of other  encoding techniques, such as, Range Encoding, Equality  Encoding etc. The main objective of the [EBI] is to  overcome the problems of low efficiency and space  consumption which affect bitmapped indices when the  attributes being indexed have high cardinality domains.

Since the existing technique [3] does not support most of  the complex and iterative queries [6] efficiently of the  data warehouse, the new approach needs to overcome this  problem. Again this technique is static and thus it is not  well suited for the data warehouse environment.

Our proposed dynamic Bitmap Indexing technique [EBI]  overcomes the problems of the existing Bitmap Indexing  technique.



IV. Proposed Encoded Bitmap Indexing  Here we have discussed our proposed method for well-  defined encoding scheme of Encoded Bitmap Indexing in  detail. The application of association rule in selecting n-  items pattern in specific order is also shown.

A. Proposed Procedure for Finding Well Defined Encoding Scheme  1. Generate the 2-items rules for the dimension  attribute distinct value (i.e. cardinality of the  attribute), which involve in indexing fact table.

2. Read the query history source file for the  dimension table attribute which involve in  indexing fact table transaction by transaction and  count the frequency for each rules and find  support and confidence for each rules.

3. Compare the confidence of each rule against  minimum confidence threshold value (i.e. 50%)  to select 2-items candidate pattern.

4. Again generate 3-items rules for each of the 2-  items candidate pattern starting with a highest  confidence valued candidate pattern.

5. Rescan the query history source file for the 3-  items rules and count the frequency for each rule  and find support and confidence for each rule.

6. Compare the confidence of each rule against  minimum confidence threshold value (i.e. 50%)  to select 3-items candidate pattern.

7. Continue the same process for 4-items candidate  pattern, up to n-items (i.e. up to attribute  cardinality value) candidate pattern.

8. Select the highest confidence valued n-items  pattern.

9. Remapped the mapping table of the Encoded  Bitmap Index for this attribute with this n-items  pattern in the way that the pattern order exists.

B. Flow Chart/Proposed Model of Encoded  Bitmap Indexing The proposed model of Encoded Bitmap Indexing  technique shows the overall procedure for finding well-  defined encoding scheme. The flow chart of the proposed  technique is in Figure 2.

Figure 2. Flow Chart of Proposed EBI   C. Association Rule in Selecting n-items Pattern Let consider that the cardinality of the state attribute of  the customer table is 5. And state attribute contains the  distinct values {NY, MA, CA, FA, LA}. At first the  support and confidence for the 2-items rules set R1 are  evaluated (figure 3). We have selected the minimum  confidence threshold for 2-items candidate pattern set C1 is 50%. The frequent 2-items pattern set C1 is found by  comparing the confidence of the candidates with the  minimum confidence threshold value. Then the 3-  Start  Generates 2-items  Ruleset Mapping Table  Evaluate  Confidence Query History  Confidence > =  Threshold  confidence  Candidate Ruleset  Generates 3 to  n-items Ruleset  Ruleset == n  n-items Candidate  Ruleset  Confidence =  Max Confidence  Well Defined  Pattern  For each rules  Yes  No  For each candidate rules  Yes  No  Output  No  Scans  Scans       Keys Encoding  NY 000  MA 001  CA 010  LA 011  FA 100  STATE = 5 (Cardinality)  Mapping Table Rules (2-items) Confidence(%)  NY => MA 0.95  NY => CA 0.20  NY => FA 0.60  NY => LA 0.55  MA => CA 0.85  MA => FA 0.40  MA => LA 0.45  CA => FA 0.30  CA => LA 0.25  FA => LA 0.48  Confidence  evaluation  for 2-items  rules  R1  Rules (2-items) Confidence(%)  NY => MA 0.95  MA => CA 0.85  NY => FA 0.60  NY => LA 0.55  C1  Comparing with minimum  confidence threshold (50%) Rules (3-items) Confidence(%)  NY ^ MA => CA 0.90  NY  ?MA => FA 0.40  NY ^ MA => LA 0.60  MA  ?CA => NY 0.42  MA ^ CA => FA 0.85  MA  ?CA => LA 0.48  NY  ?FA => MA 0.35  NY  ?FA => CA 0.28  NY  ?FA => LA 0.47  NY  ?LA => MA 0.48  NY  ?LA => CA 0.38  NY  ?LA => FA 0.24  Generating 3-items  rules from C1  R2  Rules (3-items) Confidence(%)  NY ^ MA => CA 0.90  MA ^ CA => FA 0.85  NY ^ MA => LA 0.60  C2  Comparing with minimum  confidence threshold (50%)  Rules (4-items) Confidence(%)  NY ^ MA ^ CA => FA 0.95  NY  ?MA  ?CA => LA 0.40  NY  ?MA  ?LA => CA 0.45  NY  ?MA  ?LA => FA 0.42  MA ^ CA ^ FA => LA 0.75  MA  ?CA  ?FA => NY 0.48  R3  Generating 4-items  rules from C2  Rules (4-items) Confidence(%)  NY ^ MA ^ CA => FA 0.90  MA ^ CA ^ FA => LA 0.75  Comparing with minimum  confidence threshold (50%)C3  Rules (5-items) Confidence(%)  NY ^ MA ^ CA ^ FA => LA 0.78  MA ^ CA ^ FA ^ LA => NY 0.65  R4  Generating 5-items  rules from C3  Rules (5-items) Confidence(%)  NY ^ MA ^ CA ^ FA => LA 0.78  MA ^ CA ^ FA ^ LA => NY 0.65  C4 Comparing with minimum  confidence threshold (50%)  The Pattern  NY ^ MA ^ CA ^ FA => LAHighest confidence  valued n-items pattern     Figure 3. Selection of n-items pattern (up to 5-items) by using proposed algorithm  items rules are generated from the 2-items candidate set  C1 and the support and confidence for the 3-items rules set  R2 are evaluated. Again the frequent 3-items candidate  patterns set C2 is found form R2 by evaluating their  confidence against minimum confidence threshold value.

Then the 4-items rules are generated from the 3-items  candidate set C2 and the support and confidence for the 4-  items rules set R3 are evaluated. Again the frequent 4-  Items candidate pattern set C3 is found form R3 by     evaluating their confidence against minimum confidence  threshold value. We have decided to continue this process  to find n-items pattern. And then we will select the  highest confidence valued n-items pattern. For 2-items  pattern we have selected the minimum threshold for  confidence as 50(%) and 50(%) as well for the minimum  threshold value for 3-items pattern. The 50(%) minimum  threshold confidence value is selected for each n-items  pattern. In each level of items pattern the pattern which  exceeds this minimum confidence threshold value is  selected as the candidate this level pattern from which  next level patterns are generated and again compared and  generated up to n-level pattern. Thus the highest  confidence valued n-items pattern is selected from the n-  level patterns.

For the example we have the following 5-items pattern  that is very frequent which is obtain through association  rule of mining of items pattern. The pattern ?    NY ^ MA ^ CA ^ FA => LA        NY ? MA ? CA ?  FA ? LA    This pattern will then mapped into the lookup table in the  order that is extracted in the order through mining of  items pattern.



IV. Comparative Study Here we have compared between the existing Encoded  Bitmap Indexing and our proposed Encoded Bitmap  Indexing technique. The comparison is done through the  experiment result of both the technique. In the  comparison process we have used some useful criteria  such as ? query processing time for the sample selection  predicate, scan time and found set percentage. Hence, we  introduce the cost model to measure the performance of  these two indexing techniques ?    The response time of a query = Access time of the table  level + Access time of the index level + Complexity of the  algorithm.

A. Evaluation of Both Techniques with Example For the evaluation of both the techniques we use the  sample selection predicate and mapping table as an  example. The cost model analysis for this sample  selection predicate is given below ?  If the following query were posed,    SELECT  *  FROM  Customer c  WHERE  c.state IN {NY, MA, LA, VA} OR  c.state IN  {MA,VA,FA,NC};  As an example, let us consider the following  encodings:         Mapping Table1 (P.EBI)  Mapping Table2 (E.EBI)                        Figure 4. Two different mapping for the same attribute    Here, we would need only one bitmap vectors for  answering it, if we have mapping table number1 i.e.

B1?.On the other hand, six bitmap vectors are required according to the second mapping table i.e. B2?B1? + B2?B0 + B1?B0.  For the first mapping table we need no logical operation also but the second one needs 3 AND and 2 OR  logical operation.

Now, evaluate the response time of both the technique by  using the following cost model formula:  CostModel(sc) = 1/8 * ((|sc| * att) + (|sc| * ati ) + |sc| *  itime(ebm))  Where sc = selective conditions  |sc| = the total number of found sets in selective  conditions  m = total number of tuples in the relation  att = access time per tuple  itime = instruction time of an algorithm  ati = access time per index  ebm = Encoded Bimap Index algorithm  For the, above selection predicate  Let |sc| = found set * total number of tuples = 10% *  20000 = 2000.

att = 0.1  ati = 0.01  itime(ebm) = 1  The response time of a query using mapping table1  = 1/8 * (|sc| * (att + ati + itime(ebm)))  = 1/8 * (2000 * (0.1 + 0.01 + 1))  = 277.5 sec.

The response time of a query using mapping table2  = 1/8 * (|sc| * (att + ati + itime(ebm)))  = 1/8 * (2000 * (0.1 + 0.01*6 + 1.4))  = 390 sec.



V. Conclusion  In our proposed method towards dynamic n-items pattern  selection for the remapping of lookup table of the specific  attribute we have tried our best to reduce scan time  complexity of query history file using normal approach.

Though scan time raises significantly it may be noted that  still the query processing time remains reasonable for  most of the ad hoc queries.

