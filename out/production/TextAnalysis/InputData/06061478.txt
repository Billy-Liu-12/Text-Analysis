Detecting Recurring Themes in Personal Media  Collections

Abstract? The goal of this work is to automatically detect frequently occurring groups of media in a user?s collection that have a unifying theme. These groups provide a narrative structure that ties in images that are temporally far apart and cannot be browsed easily. The media in the collection is analyzed by a variety of algorithms to generate metadata of different types.

The media and associated metadata are represented as a transactional database, and frequent itemset mining is employed to detect frequently occurring groups of images that share several metadata in common. It is expected that a user?s primary picture-taking interests (e.g., baby, garden, school sports, etc.), will appear as groups based on some combination of underlying metadata. A confidence and interest measure relevant to the consumer domain is used to determine the quality of the frequent itemsets and create a list of the top "themes" within the collection. We also detect annually recurring groups in multi- year collections, as these capture common themes such as birthdays and holidays. Because the detected recurring groups are strictly data-driven (with no a priori assumptions about a user?s collection), they are customized to the type of content in specific user?s collections. Experiments with large user collections show the usefulness of our approach.

Keywords-data mining; media collection; metadata; image understanding; clustering

I.  INTRODUCTION   With the ubiquitous presence of digital cameras and camera phones, people capture large numbers of images and videos to mark both events that are important to them, as well as day-to- day occurrences that chronicle their lives. This data capture is performed for two important reasons ? for sharing with friends and family, and to re-live the events at a later time. Both sharing and re-living are enhanced when the media is presented in the form of a story or chronicle. According to noted philosopher N?el Carroll [1], a chronicle connects at least two events that have a unified subject and show temporal ordering. In a consumer media collection, examples of this would be pictures of an individual from birth to ten years of age, multiple summer vacations to the same place over years, and pictures of a hobby or sport that are captured occasionally.

Attention has focused on the use of digital collections for  storytelling [2,3]. User studies in these works show the value of storytelling. In these systems, the narratives (stories) are created by the users, and the system provides a supportive user interface. This is also true of other works that provide human- centric interfaces [4,5] that are more intuitive to use. However, given the lack of available time experienced by most users, the time and effort required to create such narratives makes it unlikely that users will be able to use these features regularly.

Typical browsing tools provide a temporal view of a media  collection, with some that support browsing by tags or faces of recognized people (e.g. Picasa, iPhoto). Since a typical user has already accumulated many years? worth of digital images, finding images that fit a specific narrative thread by browsing through temporally distant media is time-consuming. In a system that supports tags, a user could find interesting groups by specifying a set of tags, but many patterns in a collection are based on a complex set of features, and a few high-level tags can generate only a limited variety of groups. In addition, the number of images in such groups can be too many or too few to be useful. An automated system could potentially be used to create stories of a pre-defined format (e.g., pictures of one person over time, or images at the same GPS location), but it is not possible to create stories that are customized to a user?s interests without an attempt to understand the specific user?s media collection.  For example, a user is a gardening enthusiast who primarily captures photographs of his own flowers, so a system that creates a location-based story would detect the same location in most of the user?s images.

There has been work that attempts to understand images ?  through object detection [6], tagging based on similar images on the Web [7], and through the use of content-based features [8]. Collections have also been analyzed to determine ?events? within the collection [9], based on temporal and content-based features. Some steps have been taken to determine the type of event depicted in an image or video [10,11]. The largest portion of consumer images is dominated by the subject of people, who can be tagged through the use of commercially available packages that provide face detection and recognition capability. Captured media may also include GPS information that identifies location of capture. To date, research on the use   DOI 10.1109/ICSC.2011.70     of these types of metadata has focused on providing better ways to search [12,13] and organize [14] collections.

In this paper, we show that this rich set of metadata in a  user?s collection can be mined to determine a wealth of information that can be used to generate narratives that are specific to individual users. Groups of media can be detected that have a unifying theme across a wide temporal distance, satisfying the definition of a chronicle. It is expected that a user?s primary picture-taking interests (e.g., baby, garden, school sports, etc.), will appear as groups based on some combination of underlying metadata. Because the detected groups are strictly data-driven (with no a priori assumptions about a user?s collection), they are customized to the type of content in specific users? collections. Each detected group represents a theme in the collection, and can be presented as a narrative to the user. These automatically generated groups require no work by the user. If the user does generate tags (such as names of people present) in the process of organizing their collection, the tags are used by our system in addition to the automatically generated metadata.

Some of the recurring themes are calendar-based?events that typically occur around the same date every year, e.g., birthdays, anniversaries, and some holidays. However, the exact date is often not followed year-to-year. These include holidays that occur on a certain day of the week during a specified week and month (e.g., Labor Day and Mother?s Day in the US), plus those that are computed from the phase of the moon or sun (such as many Asian religious and cultural festivals). In addition, birthdays and anniversaries could be celebrated during the nearest weekend rather than on the exact date.

Attempting to identify some of these events using a generic calendar of important dates can detect a limited number of events, but no user-specific special dates (e.g., birthdays). Also, this approach assumes that all users celebrate the same holidays in a specific region, when in reality a diverse population requires use of separate calendars for each group of people. In addition to different calendars for different cultures, the user?s location contributes local events to a calendar, e.g., Lilac Festival in Rochester,NY,  balloon festival in Albuquerque, NM. We include both media groups that are calendar-related and thematically similar but unrelated to the calendar in our effort to detect meaningful chronicles in a collection.



II. OUR APPROACH   The main observation exploited in this work is that related media that share a common theme will also share some common metadata. If a variety of metadata types is included for every media in the collection, recurring themes will generate a signature of metadata groups that are common to a significant number of media within the collection.

Fig. 1 gives an overview of the main steps in our method for detecting recurring themes. The first step is to process the media collection to generate many different types of metadata associated with each media in the collection. Low-level, mid- level and high-level image features are computed using image  understanding algorithms.  A detailed description of the metadata generated is available in section 3.

There has been extensive work in data mining to derive useful rules from transactions history of people (purchases, online activities, social network interactions etc). A transaction typically contains a transaction identifier and a set of items that belong to the transaction. This is also called ?market basket?-style data, from its roots in the listing of contents of a supermarket shopping cart of individual shoppers. A transactions database contains a large set of transactions. Standard data mining techniques [15] provide tools for extracting frequently occurring groups of items (itemsets) in transactions databases. The Apriori algorithm [16] produces strong association rules that satisfy both a minimum support and minimum confidence constraint. The Eclat algorithm [17] produces frequent itemsets in the database through a variant of depth-first search. The main aim of these methods is to discover interesting relationships between items, e.g., a person who buys milk will likely buy bread. These strong associations or rules are then used to guide decisions regarding product placement, advertising, scheduling, etc.

Association rules derived through frequent itemset mining  have also been used for creating classifiers [18,19] by detecting rules that are of the form ?features ?class?. The combination of features (which are the items) detected through frequent itemset mining provides a more discriminating classifier than single features alone. However, these methods require a priori knowledge of classes being detected and a large set labeled with those class labels.

In our work, association rules are not meaningful because  we have no a priori classes identified and do not have any preference for which features need to be on the right hand side of a rule. Instead, we are interested in the frequent itemsets themselves. We convert the metadata database to the format of a transactions database and use the Eclat algorithm to detect frequent itemsets (groups of features that co-occur frequently).

It is typical in most cases to generate a very large number of frequent itemsets that need to be filtered further by using additional interest measures. Some of the frequent itemsets generated may be spurious, due to the inaccuracies of the metadata generation algorithms and variability within images of the same theme. In our final step, we filter the frequent itemsets by a confidence score that incorporates the temporal structure of the image database, and use an interest measure relevant to our domain to produce a ranking of the top frequent itemsets in a collection. These media groups are then considered to represent different themes present in the media collection.

To detect calendar-based themes, we first need a more stable indication of a notable happening than the raw image counts (number of images captured per day), which are highly variable for the same event that is celebrated in different years.

For this purpose, we use the concept of an event [9,19] that consumers often use to describe their picture-taking occasions.

Because the size of the event (number of pictures) can vary     widely, using events as the unit that is being tracked makes our approach robust to this variation. The events are represented in a 2D space where proximity indicates similarity in calendar location, and clustering is performed in this space.

Section 4 provides details of the above steps. The results are then verified using a number of large user collections, the details of which are provided in section 5. Conclusions are presented section 6.

Figure 1. An overview of the steps in detecting interesting patterns in an image collection.



III. METADATA GENERATION The choice of features used to describe the media in a collection needs to be diverse for our method to work, since we do not know a priori what kind of recurring themes may be present in a given collection, and what associated features would correlate well with those themes.  We have chosen to incorporate many different types of features, including low-, mid- and high-level features. Table 1 provides a list of the features used.

Features need to be expressed as a finite number of levels so that they can fit in as items in a transactions database e.g. for number of people in an image, the choices could be OnePerson, GroupOf2 and 3OrMore. In many cases, we can express the features as binary variables ? either they are present or absent.

A. Event Information The collection is segmented into events based on temporal  information and visual similarity [9]. The event detection algorithm we use attempts to match user's subjective perceptions of specific occurrences that correspond to events.

Briefly summarized, a histogram of time differences between adjacent images or videos is clustered into two classes: time differences that correspond to event boundaries, and those that do not. Color block-based visual similarity is used to refine the event boundaries. The event information is not used as part of the features that describe an image, but is incorporated in the filtering process following the extraction of frequent itemsets from the collection, and to detect calendar-based groups.

B. Semantic Features There has been extensive work towards determining the  semantic content of images, resulting in the availability of a number of high-level image feature detectors [8,20]. These include scene classifiers, materials detectors and object detectors. Scene classifiers output a probability score of the image being of the given scene type. Some of these scene classifiers use information from the EXIF header (such as flash fired or focal length), in addition to pixel-based information.

Materials detectors provide a probability map that show the probability of each pixel in the image being of the given material type (e.g., sky, grass, etc.). A threshold is used to determine whether the overall image is tagged with the presence of the material.

Some of the specific scene classifiers (sunset, flower image, text image) were developed using bag-of-words features that combine low-level color and texture features. One challenge in creating scene classifiers is to develop a large and diverse training set that captures all the variations that are encountered in the unconstrained consumer picture domain.  Even when a good training set is created, it is often not possible to train a specific classifier because of the large variability in the training set and the lack of features that are invariant to the variability.

C. Low-level Features We include colors present in an image as a low-level feature that captures similarity of appearance between two images.  Typically, color is represented by a color histogram of the image. To achieve a fixed number of levels, we translate the native RGB color space of an image into color names derived from the ISCC-NBS color naming system, with the color names collapsed to the main color categories as described in [21]. This limits the number of choices to thirteen named colors. To ensure that the detected colors match the perceived color regions in an image, we determine spatially coherent color regions in an image before translating the colors from coherent regions into the color name space.

D. Features Based on Camera Metadata The capture date and time is an important piece of metadata provided by the camera. The capture date and time is used to determine the time of year in terms of seasons (summer, winter, fall or spring), the time of day (morning, afternoon,     evening, night) and time of week (weekday, weekend). In addition, capture conditions are described in terms of the scene lighting ? backlight or light and dark (high contrast scene) and focal distance (distant or close-up scene). Some collections have GPS-tagged images. These can be translated to the nearest town or natural park name using a GNIS lookup database. Location information may also be provided by the user as location tags. When present, the place names are used as features.

E. Face-based Features Because pictures of people form an important segment of  consumer collections, it is important to capture as many aspects of faces in an image as possible. Face detection [22] provides the location of faces and the size of the bounding box around each detected face. From this data, we compute a three-level feature for the number of people in an image and a three-level feature that describes the size of faces in an image (small, medium, or large). By analyzing the facial features of the detected faces, we categorize the faces into male or female, and assign an age category.

For some collections, the identity of the persons in the images is also available when the user has tagged the images either manually or by using a face recognition and clustering interface as provided by some software packages (e.g. Picasa, iPhoto). When present, these people names are used as features.

The automatically generated demographic information is not used when people names have been tagged in images.

TABLE I.  FEATURES INCLUDED IN METADATA DATABASE  Feature Category Examples  Scene classification  NatureLandscape, CityBuildings, Beach, Indoor/outdoor, Sunset  Materials classification  Sky, snow, water, fall foliage, green foliage, text  Content Colors  Camera metadata  Focal distance, backlight, exposure  Face/demogra phics  Number of faces, size of faces, age, gender  Time Time of day, time of year, time of week  Specific tags People names, geographic location

IV. MINING THE METADATA DATABASE We pose the problem of finding recurring themes given the metadata database corresponding to a user?s media collection as follows:  Let ? be the set of all possible symbolic feature descriptions in the metadata database. Any subset F ? ? is an itemset. Let each image i in the collection correspond to a transaction ? containing an itemset of variable length consisting of the feature descriptions associated with the image in the metadata database. Then ? = <?1 , ?, ?n> is a transaction database consisting of a sequence of itemsets corresponding to each    Figure 2. A section of the metadata database converted to transactions showing transaction ID (image filename) and associated items (image features).

image i in the collection. Fig. 2 shows a snapshot of a section of the transaction database for a collection. The first column of each row is the transaction ID, which in our case is the image filename. This is followed by the feature descriptors corresponding to the image.

For any itemset, F, the cover of F is defined as:  cover(F) = { ? ? ? | F ? ? }  and the support of F is the size of the cover:  support (F) = | cover(F) |  Frequent itemsets ? are defined for a given minimum support, minsup as:  ? = { F | support(F) ?  minsup }  The minimum support is specified in the interval [0,1] as a fraction of the total number of transactions in the database. A frequent itemset is maximal if there are no proper supersets of F in ? that are also frequent.

We use the implementation of the Eclat algorithm to mine frequent itemsets provided in [23] through the R package arules [24]. We generate maximal frequent itemsets using a minsup value of 0.05, and consider frequent itemsets with 3 or more items only. The minsup threshold controls the minimum number of images in an itemset for it to be considered further .

A. Filtering Detected Frequent Itemsets  Most features used in this work are automatically generated by algorithms that are inaccurate, producing both false positives and false negatives. These may result in false positives and false negatives in the frequent itemsets generated from the inaccurate metadata database. There has been some work in handling noise in the database entries [25]. In the consumer domain, temporal closeness of captured images indicates that they are more likely to be related to each other.

In particular, if they fall within the same event as defined in [9], they are likely to be similar in appearance as well as other features.  We determine the confidence level of each identified frequent itemset based on whether there is supporting evidence from related images from the same event. Considering the supporting transactions (images) for a given frequent itemset, if there are multiple images from the same event, the itemset is assigned a higher confidence value. For a given itemset, confidence score is computed as:  conf (F) = support (F) / ? Number of images in events included in cover(F)     A threshold is determined experimentally to filter out spurious itemsets. Typically, there is significant variation in images within an event (e.g. some may depict the people present, while others may show the venue), but there are usually multiple similar images captured if the subject is of interest to the cameraman, and these share features in common.

B. Ranking Frequent Itemsets   The frequent itemsets that remain after thresholding based on confidence score can be further ranked by criteria appropriate to the application for which they are being used. A number of interest measures defined in data mining literature have already been implemented in [24]. However, these are generic in nature, and do not necessarily capture the characteristics of this domain.

Note that narratives or chronicles are more interesting when the themes cover a large time span. However, because images from the same event are likely to share many features in common, some of the frequent itemsets may well be images from the large single events in the collection. The confidence value assigned does not distinguish between itemsets with support from a single event or multiple events. To make this distinction, we use an interest measure that computes the longest time difference between images within a single frequent itemset, and the frequent itemsets are ranked by decreasing order of time span. This gives preference to themes that span longer time periods, and are therefore more suitable as a narrative.

C. Detecting Calendar-based Recurring Groups Events are represented in a two-dimensional space that has  the calendar day on the x-axis and the year on the y-axis, as shown in Fig. 3. Representing events in this 2D-space places potentially recurring events in spatial proximity with each other, making it possible to use spatial clustering. We use density-based clustering [15] because it is useful for discovering clusters of arbitrary shapes, as expected in this space. The algorithm proceeds by identifying core points that contain a minimum number of points in its predefined neighborhood. The algorithm then iteratively collects directly density-reachable objects from the core objects, until no new points can be added. An overview of our temporal clustering method is shown in Fig. 4. There has been work in identifying periodic patterns from data [26, 27]. However, these methods work with 10-100 thousand data points. Even a five-year collection would be inadequate for detecting patterns, as an annual pattern appears five or fewer times during this period. A visualization-based solution for detecting periodicities such as [28] would need to be adapted to the image domain and would involve extensive user participation.

In our formulation, the neighborhood is defined as (x ? 2, y ? 2) for detecting events closely tied to the calendar date.

Core objects are identified that have greater than a minimum threshold of points (5 points in this work) in their neighborhood. A score is computed for each cluster so that clusters can be ranked, and the ranking can be used to select a limited number of recurring groups by choosing the top groups.

Figure 4. Overview of calendar-based recurring group detection.

In this 2D space, elliptical clusters that have vertical major axes are more likely to be recurring events than ellipses along the horizontal (date) axis. Therefore, the score is computed as the ratio of the spread of the cluster in the y direction to its spread in the x direction; a higher score implies a more desirable cluster for being a calendar-based recurring event.

The temporal recurring groups that are detected are passed through a filter, to remove spurious groups. The filter checks if the images (transactions) within a recurring group belong to a frequent itemset computed using the metadata database. If the fraction of images that belong to the same frequent itemset are higher than a threshold, the recurring group is finalized. If the total number of images in a recurring group is less than the support threshold required for a frequent itemset, a different filter is used. In this case, the Jaccard similarity [29] is computed between the images in the group, and a threshold is applied to the overall similarity value. The Jaccard similarity is computed as the number of items that occur in common between two transactions, divided by the total number of items in the two transactions.



V. EXPERIMENTS WITH PERSONAL COLLECTIONS Multi-year personal image collections were gathered from  15 individuals. The dataset consists of approximately 15,000 images with over 1000 automatically detected events (using [9]). The collection owners were asked to label the events with a descriptive label so that experimental results could be validated without further involvement from the individuals.

Two additional databases were collected from users who have people tags and geographic locations available in their collection. This is a richer metadata set that can potentially yield more interesting results. More such databases are being collected for further experiments. Fig. 5 shows the frequent items in one such collection, which shows that place and people names are among the frequently occurring metadata.

Fig. 6 shows a visualization of 10 users? top 5 frequent itemsets by enumerating the items (or features) in them. Lighter colors indicate higher number of instances of the feature. Of the 20 features used in the x-axis of the table, the first 4 pertain to the time of day, the next two indicate indoor or outdoor, number 7 and 8 denote weekday or weekend, 9 to 12 are the seasons of the year, 13 to 15 are scene classifiers, 16 to 18 indicate the number of persons in a image, and the last two features indicate the age demographic of people present in the images.  This view shows that each user has a different feature signature  that  indicates  differences in  the  make  up of  the       Figure 5. Frequency plot of items in a user?s collection, that includes people and location tags.

collection. So our system is able to produce groupings that incorporate the specific characteristics of a user?s collection.

Table II shows the top itemsets detected in some of the user collections, along with the corresponding ground truth (user labels). The average number of itemsets after filtering was 51, covering over 80% of the images in the collection, with images appearing in multiple top itemsets. Many of the itemsets correspond to easily determined themes, e.g., users 1,5 (same location), users 4,6,8 (same activity) and users 7,9 (related locations indicating similar activity). The itemsets listed for users 2 ,3 appear to lack a common theme in terms of the event labels provided. Based on the correlation with ground truth event labels, the average precision of the system is 64%.

However, a first-party user study needs to be conducted to evaluate the user reaction to the frequent itemsets that are detected by the system but have no support in the user-provided event labels. Some of these coincidental itemsets could be interesting to the user - they may depict people in common, or have some other common thread unrelated to the event label.

The recall does not have much meaning for this system, as the number of potential narratives in a collection is vast and cannot be enumerated.

Fig. 8 shows some visual examples to illustrate the types of groups detected by our system. The first two rows indicate images grouped by the use of scene classifier features augmented by other features. The errors made by the ?FallFoliage? detector are filtered out by the use of the additional features in the itemset capturing the time of year, the lack of detected faces and the constraint of being outdoor images. The second row produces a coherent set showing water scenes, again by combining the features of water, nature, no faces and outdoors. The third and fourth rows are interesting in that pictures of the same person are grouped together without knowing their identity. In the first case, because the person was a frequent subject of indoor images showing her various activities, and in the second case because the person wore a  spring/fall jacket with a distinctive color. Even though these underlying features may not directly provide a meaningful reason for grouping these images, the overall semantic link is obvious to the user viewing the groups. The fifth row gives an example of a grouping that lacks a semantic connection, mainly because all the features defining the group are too generic.

For the temporal recurring groups, the ground truth is more specific, as each group recurring across years has a specific meaning to the user. The average precision of our approach here is 72%. Recurring groups were deemed to be detected correctly when the majority of the events in the group were from the same recurring theme.  Fig. 7 shows an example from a user collection, with the user-provided labels shown next to the groups. Note that some of the groups (e.g. summer soccer leagues) are not strictly calendar date-based, but are still detected because they occurred around the same week of the year for multiple years. Other examples of events that are loosely tied to the calendar but are still detected using our method includes regular vacations (e.g., during school breaks), parties/gatherings , and sporting events.

Figure 6. The top features in each user?s frequent itemsets.

TABLE II.  EXAMPLES OF FREQUENT ITEMSETS DETECTED IN USER COLLECTIONS  (WITH NO PEOPLE OR PLACE INFORMATION)  User Top itemset  Ground Truth  1 Teen,3OrMore,Indoors,Night, Weekday, Winter  Caf? Nov 2004, Caf? Feb 2005,..

2 Nature,Outdoors,Sky,Weekend NASCAR, Can.

Lake, Florida II,..

3 Adult, OnePerson,Indoors New Years 2004, Therapy Pets,  Halloween 2005,..

4 Adult,3OrMore,Outdoors,Summer, Weekend  Surprise Party July 2004, Beach Party  July 2005,..

5 Buildings,Morning,Spring,Weekday Italy Part 1 Jan 2003, Italy Part 2,  Jan 2004..

6 Adult,Afternoon,3OrMore,Indoors,  Weekday  Bowling 2005 ? through the year in  different events     7 Indoors,Morning,Summer,Weekday Hotel in Lake District, Trafalgar  Sq, Paris,..

8 Teen,3OrMore,GreenFoliage,Outdoors,  Summer  Futures Regionals, Empire State  Games, NFT Circle Play,..

9 Morning,Nature,Outdoors,Summer,  Weekend  Webster Park, Turning Pt. Park, Driving Park,..



VI. CONCLUSIONS AND FUTURE WORK We have proposed a framework for mining interesting  chronicles or narratives from multi-year consumer media collections, including themes related to the calendar date. The media in the collection is analyzed by a variety of algorithms to generate metadata of different types. We use metadata based on camera capture information, low-level image processing, semantic image understanding, facial processing and event clustering to gather a large set of diverse metadata. The media and associated metadata are represented as a transaction database, and frequent itemset mining is employed to detect frequently occurring groups of images that share several meta-                         Figure 7. An example of temporal recurring groups detected.

data in common. A confidence and interest measure relevant to the consumer domain is used to determine the quality of the frequent itemsets, and create a list of the top "themes" within the collection. Since a variety of metadata types are included for every media in the collection, we are able to capture a broad variety of recurring themes without a priori knowledge of the themes in a given collection. Because the system also discovers frequent patterns that the user may be unaware of, the system seems to display ?intelligent? behavior by providing unexpected groupings that could not have been extracted by merely searching.

We also detect calendar-related recurring themes in multi- year collections. We define collections in terms of events, and represent them in an appropriate two-dimensional temporal  space. We use density-based clustering followed by filtering to reduce the number of false matches in a group. Our approach detects personal special dates such as birthdays and anniversaries, seasonal activities and holidays celebrated. Since both the calendar-based groups and feature-based thematic groups detected are strictly data-driven (with no a priori assumptions about a user?s collection), they are customized to the type of content in specific user?s collections.

Further work in this area includes a user study to evaluate user reaction to these automatically generated narratives from their collection. Some of the user studies will focus on using the output of our method for slideshow-based reliving of a collection. We also plan to gather more collections with people and location metadata. Another direction of exploration is in improving the performance of scene, material and object classifiers by incorporating contextual information from seemingly unrelated metadata, as suggested by the frequent itemsets detected by our system. Also, further improvement of the precision of detecting recurrent themes can be obtained by incorporating the discrimination power of different features into the confidence score to avoid making groups based on generic features.

