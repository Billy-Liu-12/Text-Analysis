Mining Association Rules Based on Apriori Algorithm and Application

ABSTRACT: In the data mining research,mining association rules is an important topic.Apriori algorithm submitted by Agrawal and R.Srikant in 1994 is the most effective algorithm.

Aimed at two problems of discovering frequent itemsets in a large database and mining association rules from frequent itemsets , I make some research on mining frequent itemsets algorithm based on apriori algorithm and mining association rules algorithm based on improved measure system.Mining association rules algorithm based on support,confidence and  interestingness is improved,aiming at creating interestingness useless rules and losing useful rules. Useless rules are cancelled,creating more reasonable association rules including negative items.The above method is used to mine association rules to the 2002 student score list of computer specialized field in Inner Mongolia university of science and technology.

KEYWORDS:apriori algorithm;recognizable matrix; association rules mining; application

I.  INTRODUCTION Apriori algorithm[1] [2]submitted by Agrawal and  R.Srikant in 1994 is the most effective algorithm of mining association rules.Two problems of discovering frequent itemsets in a large database and mining association rules from frequent itemsets need to be solved. Technique based hash[3],partition[4],sampling[5] is putted forward by J.S.Park, A.Savasere,H.Toivonen,Jiawei Han[6].They mainly research on problem of decreasing I/O operation and reducing amount of candidate sets[7]. In this paper, I make some research on improved algorithm based on recognizable matrix and improved algorithm for mining association rules. The above schema is used to mine the students? data table.



II. SCHEMA OF MINING ASSOCIATION RULES Information System (S) is a structure {U,I,F} where U is  a object set { X1,X2, ?XP },I is a property set ?I1,I2,?Im? ,fi  is  a injection on U and Ii,Vi is Value field of Ii,written as fi:U ? Ii ? Vi,i=1,2,?,m,F is a injection on  U and I,V=?  m  i iV  1=  is Value field  of  I,written as F:U? I ? V. If for  a= Ii ?I,we have F(Xi,a)=fi (Xi,Ii)?V.

In the Information System S={U,I,F},if  i? :Vi ? {0,1}? i(x)=1 x? vi1  ? i(x)=0 x? vi2 ,vi1  ? vi 2 =Vi  ? :V ? {0,1},x ? Vi ? (x)= ? i(x),matrix Dpxm is obtained from? and V.Matrix Dpxm is called recognizable matrix of I.

In the Information System S={U,I,F},if U={ T1,T2, ?TP } and I=?I1,I2,?Im?,Dp?m=?D1?D2??Dm?.If a=Ij , Va=Vi ? V and Dj=? j(Vj),j=1,2, ?,m.

Dj=  ? ? ? ? ?  ?  ?  ? ? ? ? ?  ?  ?  d  d d  pj  j  j  ...

is called recognizable vector of Ij.

support-count?Ij?=? =  p  i ijd     Dp?m=(D1 D2 ? Dm)=  ? ? ? ? ?  ?  ?  ? ? ? ? ?  ?  ?  ddd  ddd ddd  pmpp  m  m  ...

...

...

...

2- itemsets?Ii?Ij?written as Rij,  Dij= Di?Dj=  ? ? ? ? ?  ?  ?  ? ? ? ? ?  ?  ?  d  d d  pj  j  j  ...

?  ? ? ? ? ?  ?  ?  ? ? ? ? ?  ?  ?  d  d d  pj  j  j  ...

=  ? ? ? ? ?  ?  ?  ? ? ? ? ?  ?  ?  d  d d  pj  j  j  ...

is called recognizable vector of Rij ,dkij= dki?dkj  support-count?Rij?=? =  p  k kijd     k- itemsets?I1,I2,I3,?Ik?written as R12?K D12?K=D1?D2???Dk= (D1?D2???Dk-1 )?Dk is  called recognizable vector of R12?K.

The set consisting of suffix of each item in itemset is  called suffix set of itemsets. The set consisting of suffix of each item in infrequent itemset is called infrequent itemset suffix set L, .L??w?=?y ?y?L,? y ? w? is called lower approximate set of suffix w.

In the transactional databases D, itemset I=?i1,i2, ?in? is called positive word set. Inverse of element in positive word set is called negative word.If A(? I) is a positive word set and B is a mixed set of positive word and negative word, implication pattern A ? B is called negative association rules, also called association rules of D.[7]  A?B is association rules of D. Probability of including A and B in transaction of D is called support of A? B.

Probability of including B in transaction of including A is  2009 International Forum on Computer Science-Technology and Applications  DOI 10.1109/IFCSTA.2009.41     called confidence of A?B. Probability of including B in transaction of D is called expectedconfidence of A?B.

B)nfidence(Aexpectedco B)(Aconfidence ?  ?=i )sup()sup(  }),sup({ BA  BA ?  =  is called interestingness of A?B.

If I=?i1,i2, ?in?is positive word set and S( ? I) is a  itemset, SN=?ij?;ij? ?S or i j??S,1?j?m?is called a inverse itemset of S.[8]

III. DISCOVERING FREQUENT ITEMSETS L IN INFORMATION SYSTEM S  Improved algorithm based on recognizable matrix: Step1 Input information system S and minsupport Step2 Recognizable matrix D is obtained from S Step3 L1 is obtained from D and let k be 2 Step4 Set R of k- itemsets is obtained from k-1- itemsets  and suffix set W of R is also obtained.

Step5 Computing lower approximate set of each suffix wi  in W. If there exist suffix wi?W such that L ? ?wi???,let  W be W-{wi} and L ? be L??{wi}.

Step6 Obtaining recognizable vector of itemsets in W and computing Support-count (Rwi).If Support-count (Rwi) < minsupport? D , let W be W-{wi} and L? be L??{wi}.

Step7 There are two cases: Case1 If W??,we obtain frequent k-itemsets from W  and let k be k+1.Goto step4.

Case2 If W=?,we go to step 8 Step8 Output frequent itemsets ?  k kLL =

IV. MINING NEGATIVE ASSOCIATION RULES FROM FREQUENT ITEMSETS L  There exist two cases to mining association rules from frequent itemsets. Case1: we obtain association rules such that support<minsupport and confidence< minconfidence.

Case2: If interestingness<1,we obtain negative association rules such that support<minsupport, confidence< minconfidence and interestingness< mininterestingness.

Algorithm based on improved measure system: For each frequent k-itemset lk?Lk,k?2 {H1={i|i? lk};genrules?lk,H1??} ?  k kRR =  genrules?lk,Hm? {m<=k-2  { Hm+1=Apriori-gen?Hm?;hm+1?Hm+1 {Conf=sup? lk? /sup ? lk- hm+1? ;Ec= sup?  hm+1?;Int=Conf/Ec; If Int>min-int conf?min-conf;  RK=RK?{lk- hm+1 ?  hm+1  With  sup,conf,int} If Int<1;  For each negative itemset hm+1 ?of hm+1  { S=sup?lk- hm+1?hm+1 ? ?;C=sup?lk- hm+1?hm+1  ?  ?/ sup?lk- hm+1?;E=sup?hm+1 ? ?;I=C/E  If S?min-sup C?min-conf I?min-int; RK=RK?{lk- hm+1? hm+1?With S,C,I} }  } genrules?lk,Hm+1?;  } } If B only include one item, we have simple algorithm  because 1 is between int(lk- i ?  i) and int(lk- i? i?).

For each frequent k-itemset lk?Lk,k?2 { H1={i|i? lk};i?H1  {Conf=sup ? lk ? /sup ? lk-i ? ;Ec= sup ? i ?;Int=Conf/Ec;  If Int>=min-int conf?min-conf; RK=RK?{lk- i? i With sup?conf?int}  else S=sup?lk- i?i  ? ?;C=sup?lk- i?i  ? ?/ sup?lk- i?  ;E=sup?i??;I=C/E If S?min-sup C?min-conf I?min-int;  RK=RK?{ lk- i? i? With S,C,I} }  }

V. MINING ASSOCIATION RULES FROM THE STUDENTS? DATA TABLE  A. Interrelated data A transactional database is 2002-student score list of  computer specialized field in NeiMongol university of science and technology where a student?s score is a record, score of a course is student?s property. The sum of record is sum of students (98), the sum of property is sum of courses (9).

B. Data preprocess Data transform:1) Course coding: Let names of courses  be i1?i2??, i9 ;2) Score separate: Let score be rate.

Input: Student score list Output: Recognizable matrix  C. Discovering frequent 1-itemsets L1 Input: Recognizable matrix and minsupport 0.25 Output: L1={ i1,i3,i4,i5,i6,i7,i8}  D. Discovering frequent item sets L Input:L1 Output: ?   1=  = k  kLL  L1={ i1,i3,i4,i5,i6,i7,i8},L2= {i14,i16,i17,i34, i45,i46,i47,i48,i56,i67},L3={ i146,i147,i167,i456,i467},L4={ i1467}  E. Mining association rules set R Input:L,minconfidence 0.75 and mininterestingness 1.6 Output: R1: i1?  i46 (s=0.3061,c=0.7895,i=1.6118) R2: i7?  i14 (s=0.2959,c=0.8788,i=2.6097) R3: i1?  i47 (s=0.2959,c=0.7632,i=2.3372) R4: i7?  i16 (s=0.2653,c=0.7879,i=2.4129)     R5: i7?  i46 (s=0.2959,c=0.8788,i=1.7942) R6: i67?  i14 (s=0.2653,c=0.8966,i=2.6625) R7: i47?  i16 (s=0.2653,c=0.8125,i=2.4883) R8: i17?  i46 (s=0.2653,c=0.8966,i=1.8305) R9: i16?  i47 (s=0.2653,c=0.8125,i=2.4883) R10: i14?  i67 (s=0.2653,c=0.7879,i=2.6625) R11: i7?  i146 (s=0.2653,c=0.7879,i=2.5737)  F. Explaining rules R1: score of Analog Electronics Technique is more than  75? score of Assembly Language is more than 75,score of operating system is more than 75 (minsupport =0.3061,minconfidence=0.7895,mininterestingness=1.618).

G. Application R1:A.Minsupport that scores of Analog Electronics  Technique, Assembly Language, operating system are fine is 25%,it illustrate that strong influence exist between the three courses and the order is rationale. B.In practice teaching, the teachers can predict scores of other courses from scores of some courses and speak or act with a well-defined objective in mind. To be good for students? mature, the teachers carry out different train pattern to inhomogenous students.

