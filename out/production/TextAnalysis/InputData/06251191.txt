A Case Study on the Application of Instance

Abstract?When considering data sets characterized by a large number of instances, the computational time required to apply Genetic Algorithms for generating Fuzzy Rule-Based Classifiers increases considerably, mainly due to the fitness evaluation.

Another important problem associated to these kinds of data sets is an undesired increase of the obtained model complexity.

These two problems can be addressed by using Instance Selection techniques, which aim to obtain a representative subset of input data with a lower size with respect to the original set, while maintaining or even improving the classification accuracy for new input data.

The aim of this study is to analyze a wide range of Instance Selection techniques together with a Genetic Fuzzy Rule-Based Classification system, namely the Fuzzy Association Rule-Based Classification Model for High-Dimensional Problems, in order to discover which reduction method or family of methods outperforms the others.

The results on 36 different Instance Selection methods show that a particular family of them is very promising, since the complexity of the obtained models is significantly decreased while accuracy is maintained, in comparison with the original model without Instance Selection. This provides a first experimental framework for further developments on this type of methods, which seems to be very appropriate for Instance Selection on classification problems addressed by applying evolutionary learning of linguistic Fuzzy Rule-Based Classifiers as a kind of pre-processing that can help to significantly decrease the complexity of the obtained models.



I. INTRODUCTION  A classification problem consists of a set P of objects to be  classified and a set L of labels or classes to be assigned to  them. The aim is to assign one label to each object so that the  input-output relationship is consistent with the set of observed  data describing the problem.

Classification problems can be addressed by using different  approaches. An interesting possibility is using Fuzzy Rule-  Based Classifiers (FRBCs), which perform the classification  Supported by the Spanish CICYT Project TIN2011-28488, the Andalusian Excellence Regional Project TIC-6858 and the European Project 238819 under FP7-PEOPLE-ITN-2008 Marie Curie Action (Networks for Initial Training).

task by means of fuzzy rules usually associated to linguistic  terms. The automatic generation of this kind of Fuzzy Rule-  Based Systems (FRBSs) from data samples can be considered  as an optimization or search process that can be managed by  using Evolutionary Algorithms (EAs), and in particular Ge-  netic Algorithms (GAs) [1], [2]. GAs have been so extensively  and successfully applied to learn FRBSs in recent years, that  the term Genetic Fuzzy Systems (GFSs) has been introduced  to denote them. [3], [4], [5].

The optimization process aims to find the appropriate FRBS  structure and parameters by coding them, or a part, as chromo-  somes that evolve in order to find the optimum with respect to  a fitness function (typically an error associated to the system).

One of the major drawbacks in learning linguistic FRBSs  when considering datasets with a large number of instances  is the final complexity of the obtained models, since learning  techniques usually take into account all the instances and try  to cover all of them. Further, the computational time required  by the fitness evaluation could be a problem in the particular  case of using GAs. However, we would like to point out this  latter is not the main aim of our contribution, since scalability  issues can be addressed by considering appropriate scalable  GFSs [6], [7].

Many different approaches have been used to address these  problems. For example, data reduction techniques aim to  obtain a representative subset of input data, which has a  lower size than the original one, but can provide a similar or  even higher classification accuracy for new input data. Data  reduction processes can be performed in several manners, e.g.

with Instance Selection (IS) techniques [8], [9], which can  be divided into two types depending on the aim pursued by  the reduction: if the set of selected instances will be used  as the reference data to instance-based classification, then the  approach is called Prototype Selection (PS) [10]. On the other  hand, if the subset will be used as input or training set of any  data mining algorithm for building a model, then the approach  U.S. Government work not protected by U.S. copyright  WCCI 2012 IEEE World Congress on Computational Intelligence June, 10-15, 2012 - Brisbane, Australia FUZZ IEEE      TABLE I INSTANCE SELECTION METHODS USED IN THE CURRENT STUDY.

Complete name Abbr. name  Condensed Nearest Neighbor CNN  Reduced Nearest Neighbor RNN  Edited Nearest Neighbor ENN  Selective Nearest Neighbor SNN  All-KNN AllKNN  Tomek Condensed Nearest Neighbor TCNN  Mutual Neighborhood Value MNV  MultiEdit MultiEdit  Instance Based 3 IB3  Random Mutation Hill Climbing RMHC  Minimal Consistent Set MCS  Explore Explore  Model Class Selection ModelCS  Variable Similarity Metric VSM  Relative Neighborhood Graph Editing RNG  Generational Genetic Algorithm GGA  Modified Edited Nearest Neighbor MENN  Decremental Reduction Optimization Procedure 3 DROP3  Iterative Case Filtering ICF  Modified Condensed Nearest Neighbor MCNN  Intelligent Genetic Algorithm IGA  Prototype Selection using Relative Certainty Gain PSRCG  Reconsistent Reconsistent  C-Pruner CPruner  Steady-State Genetic Algorithm SSGA  Population Based Incremental Learning PBIL  CHC Evolutionary Algorithm CHC  Patterns by Ordered Projections POP  Nearest Centroid Neighbor Edition NCNEdit  Edited Normalized Radial Basis Function ENRBF  Edited Nearest Neighbor Estimating ENNTh  Modified Selective Subset MSS  Generalized Condensed Nearest Neighbor GCNN  Fast Condensed Nearest Neighbor 1 FCNN  Steady-State Memetic Algorithm SSMA  Hit Miss Network Edition Iterative HMNEI  1) discovering the association rules inherent in a database; 2)  selecting a small set of relevant association rules to construct  a classifier.

Association rules are expressions of the type A ? B, where A and B are sets of items and A ? B = ?. This implies that, if all the items in A exist in a transaction then all the  items in B with a high probability are also in the transaction,  and A and B have no common item. Fuzzy association rules  [18] are a generalization of association rules and extend the  types of relationships that may be represented, facilitate the  interpretation of rules in linguistic terms and avoid unnatural  boundaries in the partitioning of attribute domains [19].

The FARC-HD method aims to learn FRBCs that include  a compact set of fuzzy association rules and present high  classification accuracy, with a low computational cost. The  method is based on three stages which will be briefly described  in the following subsections.

A. Stage 1: Fuzzy Association Rule Extraction  In the first stage, a search tree is used to list all the possible  fuzzy itemsets of a class. The root or level 0 of a search  tree is an empty set. All attributes are assumed to have an  order, first setting the fuzzy items of the first variable, then  the fuzzy items of the second variable, and so on. The one-  itemsets corresponding to the attributes are listed in the first  level of the search tree according to their order. The children  of an one-item node for an attribute A are the two-itemsets  that include the one-itemset of attribute A and an one-itemset  for another attribute after attribute A in the order, and so on.

No repeated itemset appears in the tree.

Each itemset is evaluated with respect to a minimum support  and a minimum confidence: an itemset with a support higher  than the minimum support is a frequent itemset and an itemset  with a confidence higher than the confidence?s threshold has  reached the quality level demanded by the user. Therefore,  if the support of an n-itemset in a node A is lower than  the minimum support, it does not need to be extended more.

Likewise, if a candidate item set generates a classification rule  with confidence higher than the minimum confidence, it is  again unnecessary to extend it further.

Once all frequent fuzzy itemsets have been obtained, the  candidate fuzzy association rules are generated by setting the  frequent fuzzy itemsets in the antecedent of the rules and  the corresponding class in the consequent, and this process is  repeated for each class. The number of frequent fuzzy itemsets  extracted depends directly on the minimum support, which is  defined for each class using the distributions of the classes  over the data set. This stage generates a large number of rules,  which can hardly handled by human users. To simplify the  understanding of the model, the depth of the trees, and so the  length of the fuzzy rules, is limited to a fixed value.

B. Stage 2: Candidate Rule Prescreening  Even though the order of the associations is limited in the  association rule extraction, the number of rules generated can  be very large. In order to decrease the computational cost  of the genetic post processing stage, the use of subgroup  discovery based on an improved weighted relative accuracy  measure (wWRAcc?) is considered to preselect the most  interesting rules by means of a pattern weighting scheme [20].

This scheme treats the patterns in such a way that covered  positive patterns are not deleted when the current best rule is  selected. Instead, each time a rule is selected, the algorithm  stores a count i for each pattern of how many times (with  how many of the selected rules) the pattern has been covered.

Weights of positive patterns covered by the selected rule  decrease according to the formula w(ej , i) =  i+1 . In the first  iteration the same weight w(ej , 0) = 1 is assigned to all target class patterns, while in the following iterations the contribu-  tions of patterns are inversely proportional to their coverage  by previously selected rules. In this way the patterns already  covered by one or more selected rules decrease their weights  while uncovered target class patterns whose weights have not  been decreased will have a greater chance of being covered  in the following iterations. Covered patterns are completely  eliminated when they have been covered more than kt times.

Thus, in each iteration of the process the rules are ordered  according to a rule evaluation criteria from the best to the    worst. The best rule is selected, covered patterns are re-  weighted, and the procedure repeats these steps until one of  the stopping criteria is achieved: either all patterns have been  covered more than kt times, or there are no more rules in the  Rule Base (RB). This process is repeated for each class.

C. Stage 3: Rule Selection and Lateral Tuning  A GA is used to perform a rule selection process and  a tuning process, which are applied to the RB generated  by the previous stage. The rules are described by means  of the linguistic 2-tuple representation [21], which allows  the lateral displacement of the labels by adopting only one  parameter. This implies a simplification of the tuning search  space that eases the derivation of optimal models. In this way,  membership functions achieve a better covering degree while  maintaining the original shapes. Thus, accuracy is increased  without a significant loss in the interpretability of the fuzzy  labels.

In this approach, the tuning is applied to the level of lin-  guistic partitions (global approach), so that the pair (Xi, label)  takes the same ? displacement value in all the rules where  it is considered. In the following, the main characteristics  of the genetic approach are presented: a) genetic model, b)  codification and initial gene pool, c) chromosome evaluation,  d) crossover operator and e) restarting approach.

a) CHC Genetic Model: the approach proposed in [22]  considers the use of a specific GA, namely the CHC algorithm  [23]. This genetic model employs a mechanism of selection  of populations so as to perform an adequate global search. P  parents and their corresponding offspring compete to select  the best P individuals which will take part in the next popu-  lation. The CHC approach makes use of an incest prevention  mechanism and a restarting process to encourage diversity in  the population, instead of the well known mutation operator.

b) Codification and Initial Gene Pool: to combine the  rule selection with the global lateral tuning, a double coding  scheme for both rule selection (CS) and lateral tuning (CT )  is used:  ? For the CS part, each chromosome is a binary vector that  determines when a rule is selected or not.

? For the CT part, a real coding is considered. This part is  the joint of the ? parameters of each fuzzy partition.

Finally, the whole chromosome C is coded in the following  way: C = CSCT . To make use of the available information, all the candidate rules are included in the population as an  initial solution.

c) Chromosome Evaluation: to evaluate a determined  chromosome penalizing a large number of rules, the classifica-  tion rate is computed and the following function is maximized:  Fitness(C) = #Hits  N ? ? ?  NRinitial  NRinitial ?NR+ 1.0 , (1)  where #Hits is the number of patterns correctly classified, NRinitial is the number of candidate rules, NR is the number  of selected rules and ? is a weighting percentage given by the  system expert that determines the trade-off between accuracy  and complexity. If there is at least one class without selected  rules or if there are no covered patterns the fitness value of  a chromosome will be penalized with the number of classes  without selected rules and the number of uncovered patterns.

d) Crossover Operator: the crossover operator depends  on the chromosome part where it is applied:  ? for the CT part the Parent Centric BLX (PCBLX) oper-  ator is considered (an operator based on BLX-?). This  operator is based on the concept of neighborhood, which  allows the offspring genes to be around the genes of one  parent or around a wide zone determined by both parent  genes.

? In the CS part, the half uniform crossover scheme (HUX)  is used. The HUX crossover exactly interchanges the mid  of the alleles that are different in the parents (the genes  to be crossed are randomly selected from among those  that are different in the parents). This operator ensures  the maximum distance of the offspring to their parents  (exploration). In this case, four offspring are generated by  combining the two from the part CT with the two from  the part CS . The two best offspring obtained in this way  are considered as the two corresponding descendants.

e) Restarting Approach: to get away from local optima,  this algorithm uses a restart approach. In this case, the best  chromosome is maintained and the remaining are generated  at random. The restart procedure is applied when a threshold  value L is below zero, which means that all the individuals  coexisting in the population are very similar.



IV. EXPERIMENTAL RESULTS AND ANALYSIS  Several experiments have been carried out in this work  to evaluate the performance of FARC-HD by applying the  different IS techniques with respect to the original one. In this  section, first, we describe the experimental framework used in  this study and then we summarize and analyze the obtained  results.

A. Experimental Set Up  As previously stated, the aim of this study is to apply IS  methods together with the FARC-HD algorithm, to discover  which reduction methods perform better. To this end, 20 real-  world data sets have been chosen, with different number of  instances, attributes and classes. Table II summarizes the main  characteristics of the 20 data sets and shows the web link  to the Knowledge Extraction based on Evolutionary Learning  (KEEL)-data set repository [24] from which they can be  downloaded. In the data sets, possible instances with missing  values have been removed, since the FARC-HD algorithm was  not designed to handle them.

To develop the different experiments, we consider a tenfold  cross-validation model, i.e., we randomly split the data set into  ten folds, each containing 10% of the patterns of the data set,  and use nine folds for training and one for testing1. For each  1The corresponding data partitions (tenfold) for these data sets are available at the KEEL-data set repository [24]: http://sci2s.ugr.es/keel/datasets.php    TABLE II SUMMARY DESCRIPTION FOR CLASSIFICATION DATA SETS.

DATA SET #EX #ATTS #NUM #NOM #CL  australian 690 14 8 6 2 automobile 205 25 15 10 6 balance 625 4 4 0 3 bupa 345 6 6 0 2 cleveland 303 13 13 0 5 contraceptive 1473 9 9 0 3 crx 690 15 6 9 2 ecoli 336 7 7 0 8 german 1000 20 7 13 2 glass 214 9 9 0 7 haberman 306 3 3 0 2 heart 270 13 13 0 2 hepatitis 155 19 19 0 2 iris 150 4 4 0 3 newthyroid 215 5 5 0 3 pima 768 8 8 0 2 tae 151 5 5 0 3 vehicle 846 18 18 0 4 wine 178 13 13 0 3 wisconsin 699 9 9 0 2 http://sci2s.ugr.es/keel/datasets.php  of the ten partitions, we executed three trials of the algorithms.

In this way, for each fold the data sets are divided into training  (TR) and test set (TS) and the IS methods have been applied  to each training set in order to obtain the reduced training  set (S). The S has been used by the FARC-HD algorithm  to obtain compact and accurate FRBCs, then the classifiers?  performances have been computed by applying the TR and the  TS.

In this way, two different comparisons can be performed:  a) Comparison among classification results obtained by  training the classifiers with the original and the S. This  assesses the quality of the reduction, i.e. allows us to  understand if the reduced set leads to more compact  classifiers with the same or even improved classification  results.

b) Comparison among different IS techniques, when used  with the FARC-HD algorithm. The aim of this study is  to highlight the IS methods that better perform when  considering the classification accuracy.

B. Results and Analysis  The results obtained by applying 36 IS methods on the  20 data sets are summarized in Table III. The RED column  denotes the reduction rate achieved by the IS methods with  respect to the TR; the TRA ACC and the TST ACC columns  denote the percentage of correctly classified patterns obtained  in the TR and the TS, respectively; the #RULES column iden-  tifies the average number of rules obtained in the FRBC; the  #COND column represents the average number of antecedent  conditions in each rule; finally, the TIME column reports the  average time elapsed in seconds to complete a run of an IS  method. In the case of FARC-HD, the time required is set to  0, since no IS method is run before. For each measure, the  methods are ordered from the best to the worst.

In order to assess whether significant differences exist  among the results, we adopt statistical analysis [13], [14] and  in particular non-parametric tests, according to the recommen-  dations made in [12], where a set of simple, safe and robust  non-parametric tests for statistical comparisons of classifiers  has been introduced.

For NxN multiple comparison Friedman?s test [25], Iman  and Davenport?s test [26] and Holm?s method [27] have been  used. A wider description of these tests, together with software  for their use, can be also found on the web site available at:  http://sci2s.ugr.es/sicidm/.

In order to perform a first study on the accuracy rates  among the results obtained by applying the IS methods, the  Friedman and Iman-Davenport tests have been carried out,  considering the average results obtained in test (tst Acc). Table  IV shows the Friedman and Iman-Davenport statistics, and  it relates them to the corresponding critical values for each  distribution by using a significance level, i.e., ? = 0.05. The  p-value obtained is also reported for each test.

TABLE IV RESULTS OF THE FRIEDMAN AND IMAN-DAVENPORT TESTS (? = 0.05).

Friedman Test  Statistic (X 2 F  ) p value  380.296 0  Iman-Davenport Test  Statistic (FF ) p value  21.270 0  The statistics of Friedman and Iman-Davenport tests show  that they are clearly greater than their associated critical  values, therefore there are significant differences among the  observed results with a level of significance ? ? 0.05. Table V shows the rankings (which are computed by the use of a  Friedman test) of the different methods that are considered in  this study.

TABLE V AVERAGE RANKINGS OF THE METHODS (FRIEDMAN).

Algorithm Ranking  CHC 31,525 MCNN 31,400 SNN 30,750 Explore 28,800 SSMA 28,400 Cpruner 27,075 VSM 27,025 RNN 26,950 PBIL 26,925 SSGA 26,750 DROP3 25,525 IGA 25,075 GGA 24,525  Algorithm Ranking  IB3 22,650 RMHC 21,650 MNV 21,525 ICF 21,100 ENRBF 20,625 MCS 18,800 FCNN 17,650 GCNN 17,200 Reconsistent 15,700 CNN 15,300 PSRCG 15,200 TCNN 14,475 MENN 14,025  Algorithm Ranking  HMNEI 13,950 AllKNN 13,100 Multiedit 12,775 ENNTh 12,425 MSS 9,350 NCNEdit 8,650 ENN 8,600 ModelCS 8,175 RNG 8,150 POP 6,400 FARC-HD 4.800  We now apply Holm?s test to compare the best ranking  method (FARC-HD without IS) with the remaining methods.

Table VI shows these results. In this table, the methods are  ordered with respect to the z-value. Holm?s test rejects the  null-hypothesis in 24 cases, therefore these methods are not  statistically equivalent to the control method. In the remaining  comparisons, the null-hypothesis is not rejected. This group  contains methods that achieve good results with respect to the  accuracy of the test.

TABLE III AVERAGE RESULTS OBTAINED BY APPLYING IS METHODS OVER DATA SETS.

RED TRA ACC TST ACC #RULES #COND TIME  Explore 0.980 FARC-HD 0.873 FARC-HD 0.785 Cpruner 2.530 Cpruner 1.190 FARC-HD 0.000 CHC 0.980 POP 0.868 POP 0.778 CHC 3.390 Explore 1.390 POP 0.100 SSMA 0.960 MSS 0.858 ModelCS 0.774 Explore 3.460 ENRBF 1.450 ModelCS 0.270 PBIL 0.950 ModelCS 0.853 MSS 0.768 SSMA 4.830 CHC 1.510 IB3 0.380 GGA 0.950 NCNEdit 0.830 NCNEdit 0.768 PBIL 6.240 MENN 1.720 Multiedit 0.410 SSGA 0.940 CNN 0.828 RNG 0.768 RNN 6.300 SSMA 1.730 MENN 0.470 RNN 0.920 RNG 0.823 ENN 0.762 GGA 6.370 Multiedit 1.770 ENNTh 0.490 Cpruner 0.920 TCNN 0.823 AllKNN 0.751 SSGA 6.670 ENNTh 1.770 ENN 0.500 IGA 0.920 Reconsistent 0.823 Multiedit 0.750 ENRBF 7.430 RNN 1.800 MSS 0.660 MCNN 0.900 FCNN 0.822 ENNTh 0.749 IGA 8.200 GGA 1.820 ENRBF 0.770 RMHC 0.900 PSRCG 0.820 HMNEI 0.747 RMHC 8.320 PBIL 1.820 FCNN 0.810 DROP3 0.820 ENN 0.817 TCNN 0.745 MENN 8.630 RMHC 1.850 ICF 0.950 ICF 0.750 MNV 0.812 CNN 0.741 MCNN 9.200 AllKNN 1.880 PSRCG 0.970 SNN 0.720 MCS 0.807 MENN 0.740 ENNTh 9.280 SSGA 1.900 VSM 0.990 IB3 0.700 HMNEI 0.807 Reconsistent 0.740 Multiedit 11.130 IGA 1.900 MCNN 1.030 FCNN 0.610 AllKNN 0.799 FCNN 0.735 AllKNN 12.070 ENN 1.950 AllKNN 1.210 MNV 0.610 IB3 0.796 PSRCG 0.734 DROP3 12.330 RNG 1.960 Cpruner 1.260 TCNN 0.600 GCNN 0.789 MCS 0.729 ICF 12.440 NCNEdit 1.990 NCNEdit 1.470 VSM 0.580 Multiedit 0.779 MNV 0.720 ENN 14.430 ICF 2.010 MCS 1.690 Reconsistent 0.580 ENNTh 0.776 GCNN 0.717 RNG 14.820 HMNEI 2.020 HMNEI 1.760 CNN 0.570 MENN 0.768 IB3 0.716 NCNEdit 15.070 ModelCS 2.100 SNN 2.120 MCS 0.570 ICF 0.763 RMHC 0.710 HMNEI 15.480 FARC-HD 2.110 DROP3 4.010 HMNEI 0.520 VSM 0.759 ICF 0.706 IB3 19.290 POP 2.130 CNN 6.420 PSRCG 0.510 DROP3 0.739 ENRBF 0.692 SNN 20.510 DROP3 2.170 Explore 6.550 GCNN 0.500 RMHC 0.736 GGA 0.686 ModelCS 21.160 MCNN 2.210 TCNN 6.650 MENN 0.470 ENRBF 0.713 DROP3 0.684 GCNN 21.240 SNN 2.220 RNG 7.890 ENNTh 0.460 SSGA 0.706 IGA 0.679 MNV 21.550 MSS 2.230 MNV 9.790 MSS 0.460 RNN 0.702 SSGA 0.677 TCNN 21.680 IB3 2.240 Reconsistent 11.960 AllKNN 0.350 IGA 0.701 RNN 0.675 CNN 21.790 Reconsistent 2.250 RNN 37.410 Multiedit 0.330 GGA 0.698 PBIL 0.673 FCNN 21.890 GCNN 2.260 SSMA 78.290 ENRBF 0.320 PBIL 0.694 VSM 0.655 VSM 21.890 FCNN 2.280 GCNN 84.890 ENN 0.250 SNN 0.685 Explore 0.647 MCS 22.070 VSM 2.290 RMHC 104.530 RNG 0.250 SSMA 0.669 Cpruner 0.644 Reconsistent 22.220 CNN 2.290 CHC 106.820 NCNEdit 0.230 MCNN 0.668 SSMA 0.643 PSRCG 22.980 MCS 2.300 SSGA 197.560 ModelCS 0.140 Explore 0.663 CHC 0.632 MSS 23.530 MNV 2.300 PBIL 207.820 POP 0.100 Cpruner 0.653 MCNN 0.623 POP 23.600 PSRCG 2.310 GGA 253.760 FARC-HD 0.000 CHC 0.642 SNN 0.618 FARC-HD 24.480 TCNN 2.310 IGA 347.960  Since the statistical tests provide more reliable results on  a reduced number of methods, we applied them again con-  sidering only the not-rejected methods. This time, both the  accuracy on test and the complexity (number of rules) have  been considered. Table VII shows the Friedman and Iman-  Davenport statistics, by using a significance level, i.e., ? =  0.05.

The statistics of Friedman and Iman-Davenport tests show  that they are greater than their associated critical values,  therefore there are significant differences among the observed  TABLE VI HOLM TABLE FOR THE DIFFERENT METHODS WITH ? = 0.05.

i Method z p ?/i Hypotesis 36 CHC 7.81 0 0.0014 Rejected ... ... ... ... ... ...

13 PSRCG 3.04 0.0024 0.0038 Rejected 12 TCNN 2.83 0.0047 0.0042 Not rejected 11 MENN 2.70 0.0070 0.0045 Not rejected 10 HMNEI 2.67 0.0075 0.0050 Not rejected 9 AllKNN 2.42 0.0153 0.0056 Not rejected 8 Multiedit 2.33 0.0198 0.0063 Not rejected 7 ENNTh 2.23 0.0259 0.0071 Not rejected 6 MSS 1.33 0.1838 0.0083 Not rejected 5 NCNEdit 1.12 0.2607 0.0100 Not rejected 4 ENN 1.11 0.2669 0.0125 Not rejected 3 ModelCS 0.99 0.3241 0.0167 Not rejected 2 RNG 0.98 0.3277 0.0250 Not rejected 1 POP 0.47 0.6402 0.0500 Not rejected  results. For both measures (accuracy on test and complexity)  the rankings have been computed again, by using the Friedman  test, and results are shown in Table VIII. As expected, the  accuracy achieved on test is nearly proportional to the number  of rules of the obtained fuzzy model. Concerning the accuracy,  the selected control method is FARC-HD without any IS  method previously applied, while regarding the complexity,  the control method is MENN.

Finally, the Holm?s test is applied again, in order to under-  stand if there are some methods that decrease the complexity  while maintaining the accuracy. Table IX presents these results  TABLE VII RESULTS OF THE FRIEDMAN AND IMAN-DAVENPORT TESTS (? = 0.05).

Friedman Test - TST ACC  Statistic (X 2 F  ) p value  48.190 <0.0001 Iman-Davenport Test - TST ACC Statistic (FF ) p value  4.773 <0.0001  Friedman Test - #RULES  Statistic (X 2 F  ) p value  163.503 0  Iman-Davenport Test - #RULES Statistic (FF ) p value  40.610 0    TABLE VIII AVERAGE RANKINGS OF THE METHODS (FRIEDMAN).

TST ACC Algorithm Ranking  TCNN 9.275 HMNEI 9.025 MENN 8.675 AllKNN 8.300 ENNTh 8.225 Multiedit 7.975 MSS 7.175 RNG 6.325 NCNEdit 6.050 ModelCS 6.000 ENN 5.625 POP 4.375 FARC-HD 3.975  #RULES Algorithm Ranking  POP 11.60 FARC-HD 11.35 MSS 10.25 ModelCS 9.78 TCNN 8.70 RNG 6.93 NCNEdit 6.90 ENN 6.68 HMNEI 5.80 Multiedit 4.40 AllKNN 4.30 ENNTh 2.58 MENN 1.75  for accuracy on test with respect to the control algorithm  (FARC-HD without IS), while Table X presents the results for  complexity of the NxN comparisons for all the methods with  respect to FARC-HD without IS. In this way, we can analyze  adequately how using or not IS affects the results of FARC-  HD with respect to accuracy and complexity (interpretability)  of the obtained models.

Considering the accuracy, Holm?s test rejects the null-  hypothesis in 6 cases, therefore the remaining methods can  not be assumed to be statistically different from FARC-HD  without IS. On the other hand their accuracies on test are  very similar. By observing these methods (those not rejected),  we can point out that they belong to the same family in  the majority: four methods are included into the family of  edition methods, while the other two belong to the family of  condensation methods.

Considering the complexity, only in four cases the null-  hypothesis is not rejected by the Holm?s test, therefore most  of these approaches are statistically better than FARC-HD in  terms of the complexity of the models. In fact, the average  number of rules in those methods for which the null-hypothesis  is rejected is in the range R = [8.50, 15.10]. Then they manage to reduce the complexity of the fuzzy models in comparison  with models obtained by applying FARC-HD without any IS  method (24.48 average rules) about 50%. This is a sound conclusion from the point of view of the interpretability  TABLE IX HOLM TABLE FOR THE DIFFERENT METHODS WITH ? = 0.05 (TST ACC).

i Method z p ?/i Hypotesis 12 TCNN 4.303589 0.00002 0.00417 Rejected 11 HMNEI 4.10059 0.00004 0.00455 Rejected 10 MENN 3.81639 0.00014 0.00500 Rejected 9 AllKNN 3.511891 0.00045 0.00556 Rejected 8 ENNTh 3.450991 0.00056 0.00625 Rejected 7 Multiedit 3.247992 0.00116 0.00714 Rejected 6 MSS 2.598393 0.00937 0.00833 Not rejected 5 RNG 1.908195 0.05637 0.01000 Not rejected 4 NCNEdit 1.684896 0.09201 0.01250 Not rejected 3 ModelCS 1.644296 0.10012 0.01667 Not rejected 2 ENN 1.339797 0.18031 0.02500 Not rejected 1 POP 0.324799 0.74533 0.05000 Not rejected  TABLE X HOLM TABLE FOR THE DIFFERENT METHODS WITH ? = 0.05 (#RULES).

i Comparison z p ?/i Hypotesis 77 FARC-HD vs. MENN 7.79 6.431E-15 6.493E-4 Rejected 75 FARC-HD vs. ENNTh 7.12 1.038E-12 6.666E-4 Rejected 68 FARC-HD vs. AllKNN 5.72 1.036E-8 7.352E-4 Rejected 67 FARC-HD vs. Multiedit 5.64 1.667E-8 7.462E-4 Rejected 61 FARC-HD vs. HMNEI 4.50 6.587E-6 8.196E-4 Rejected 53 FARC-HD vs. ENN 3.79 1.469E-4 9.433E-4 Rejected 51 FARC-HD vs. NCNEdit 3.61 3.022E-4 9.803E-4 Rejected 49 FARC-HD vs. RNG 3.59 3.267E-4 0.0010 Rejected 32 FARC-HD vs. TCNN 2.15 0.0314 0.0015 Not Rejected 17 FARC-HD vs. ModelCS 1.27 0.2009 0.0029 Not Rejected 11 FARC-HD vs. MSS 0.89 0.3717 0.0045 Not Rejected 4 FARC-HD vs. POP 0.20 0.8391 0.0125 Not Rejected  (simplicity) of the obtained linguistic models, since this is one  of the main aims of linguistic modeling techniques.

Considering those of these methods (rejecting the null-  hypothesis in complexity) which also were not rejected in  accuracy, again in this case, all the methods belong to the  same family, i.e. edition methods, and in this case to the  same subfamily, i.e. decremental methods. They are, MENN,  ENNTh, Multiedit, ENN, NCNEdit and RNG. For example,  ENN is only losing about 2% in test accuracy but obtaining  almost 50% of rule reduction (14.43 rules on average) with less than 2 conditions per rule.



V. CONCLUSION  In this paper we have performed a study to analyze the per-  formance of a wide range of IS methods when combined with  an evolutionary-based FRBC, i.e., the FARC-HD classifier.

The methods have been applied before FARC-HD execution  and their results have been compared with the results obtained  by FARC-HD without any IS method.

To this aim, 20 small-size data sets have been used and  the results show that there is a family of IS methods that  outperform the others, both on accuracy and complexity. These  methods are MENN, ENNTh, Multiedit, ENN, NCNEdit and  RNG, being all of them edition methods performing decremen-  tal edition. For these methods reduction in complexity, which  improves interpretability of the obtained linguistic models, is  about 50% on average while they are maintaining almost the  same test accuracy.

The aim of the paper is to provide an experimental frame-  work for further developments on those types of methods pro-  viding promising results. In this way, we can finally conclude  that decremental edition seems to be very appropriate for IS  on classification problems addressed by applying GFSs to the  derivation of linguistic FRBCs. Further studies will explore the  convenience of the application of IS techniques when dealing  with large data sets.

