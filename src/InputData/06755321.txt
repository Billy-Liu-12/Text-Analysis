Reduction of Association Rules for Big Data Sets   in Socially-Aware Computing

Abstract ? Reduction of the number of association rules in data mining is a very important issue in the field of socially- aware computing in which big data need to be manipulated.

The existing schemes based on the frequency of occurrences are not effective for relatively large size dataset. In this paper we propose the wTabular-algorithm that assigns a weight to each rule for the removal of unimportant rules and employs the Quine-Mccluskey method for rule reduction. Computer simulation reveals that the proposed scheme significantly improves support, credibility, rule reduction rate, and processing time compared to the representative existing schemes such as Apriori and FP-growth algorithm.

Keywords - Big data mining; Association rule reduction; Socially aware computing; Quine-Mccluskey method; wTabular- algorithm.



I. INTRODUCTION In socially-aware computing the service is provided  which is deemed to be most suitable for the user?s current context. Here the rules extracted from the datasets of a number of users are utilized. Recently, socially-aware computing receives a great deal of attentions, and various application domains including the health-care system and intelligent vehicle system have started to take advantage of its paradigm. In the implementation of socially-aware computing, accurate and fast extraction of valuable rules from big data is a crucial task, along with efficient handling of the context of the user for providing the best service.

In practice the socially-aware computing system usually needs to satisfy the real-time criteria. Therefore, it is important to minimize the amount of manipulated data by applying a scheme which can reduce the number of association rules, and as a result decrease the data processing time. The rule reduction scheme guaranteeing data integrity and high temporal efficiency is a critical factor  for the deployment of the paradigm of socially-aware computing in real environment.

The data to which the rule reduction scheme is applied typically have binary attribute. Various algorithms mining the association rule with the data of binary attribute have been suggested, including the direct hashing and pruning [11], Partition [12], and dynamic item set counting scheme [6]. The commonly adopted approaches in these schemes for enhancing the performance of mining are reducing the number of scans of the database and decreasing the number of candidate data sets. In addition, the methods of mining the generalized association rule using the taxonomy of the attribute or user constraints were proposed. The techniques which use histogram and sample, and mine negative association rule by finding unrelated items have also been suggested. The basic approach employed for the analysis of the association in the target data is to find the item whose frequency of occurrences in the transaction is larger than the lower bound. Since the data structure used for the technique is simple and the result is deterministic, the frequency-based approach has been widely adopted. However, as the number of items increases, the computation time rapidly increases and it becomes difficult to extract the information on the rule for the data of low frequency of occurrences.

In order to deal with the drawbacks of the existing approaches for mining the association rule in big data, we propose wTabular-algorithm which assigns a weight to each item by expressing the datasets in binary numerals.

Also, the Quine-Mccluskey method [21] is used for simplifying the logical functions into an algorithm. The proposed approach can reduce unnecessary rules and improve the efficiency of the mining of association rules.

Computer simulation shows that the proposed scheme allows higher performance than two representative rule reduction schemes such as Apriori-algorithm [4] and FP-   DOI 10.1109/CSE.2013.140    DOI 10.1109/CSE.2013.140     growth algorithm [1], in terms of the usefulness and validity of the association rules extracted by the proposed scheme, rule reduction rate and processing time. The proposed scheme is thus preferred in the field of socially- aware computing where timely processing of big data is essential.

The remainder of the paper is organized as follows. In Section  the existing algorithms related to association rule reduction are explained. Section  presents the proposed wTabluar-algorithm with an example, and Section compares its performance with the existing methods using computer simulation. Lastly, Section  presents the conclusion and future study.



II. THE RELATED WORK Association rule is outcome of a process of mining the  correlation between the transactions with a database after a large number of transactions are accumulated, where the regularities between the items having some association are found through a statistical method. For example, it indicates the association such as ?if an incident happens, the other incident also happens.?  By gathering a number of transactions, the association of an item set with other item set can be extracted, which indicates the similarity or pattern existing between them.

The basic property of association rule is as follows. For a set of transactions expressed as I, if non-empty set X and Y satisfy X I and Y I, it indicates that when incident X happens, incident Y also happens.

The grade of association of a rule is measured by ?Support? and ?Credibility?. With two given data sets, A and B, support, S, decides how often a rule can be applied to the given dataset, and credibility, C, decides how frequently the items belonging to B appear in A. These measures are defined by Eq. (1) and (2), respectively.

( ): ( ) A BSupport S X Y  N ?? = ?            (1)     ( ): ( )  ( ) A BCredibility C X Y  A ?  ? ? = ?        (2)   Here ???? denotes the number of items contained in A.

?Support? is the measure evaluating the usefulness of the association rule. A rule of very low ?Support? can be judged as occurring occasionally, and thus it is hard to say that the rule is a useful one. ?Credibility? measures the validity of an inference made by a rule, and it can be said that the higher the ?Credibility? of the given rule A?B is, the higher the possibility for B to exist in the transaction which includes A is. Therefore, ?Credibility? defines the conditional probability of B with given A.

Finding the association rules from the given database requires two steps of processing: finding frequent items and generating the association rules.

Step 1: Finding frequent items  The maximum size of frequent items is the size of the power set of the item set. If the number of items which the users are interested in increases, it thus increases geometrically. The database is scanned and the Support of each item is calculated to find the frequent items.

Step 2: Generating association rule  Assume that b is the set of frequent items. First, power set of b excluding the empty set is found. After that, regarding each subset, a, if  ????	?  ??? ??? min Credibility, the  rule is generated in the form of a (b ? a). Here min Credibility is a preset value, usually 20%.

We next introduce two algorithms broadly used for mining association rule, Apriori-algorithm and FP-growth algorithm.

A. Apriori-algorithm The Apriori-algorithm is applied to the candidate items.

The frequency of each candidate item is calculated first, and then frequent items are decided based on the minimum support of the data set [4]. Below is the procedure of Apriori-algorithm.

Apriori-algorithm  ?= {large 1- itemsets}; For(k=2; ???=0; k++) do begin ??= Apriori-gen(???); for all transactions t  D do begin ??= subset(??? ??? for all candidates c ?? do  c.count++; end ??= { c  ?? | c.count  min sup};  end Answer = ? ??? ;   First, C1 is generated which contains candidate frequent  items. Then, the database is scanned for each candidate frequent item belonging to C1. In other words, the sup value is calculated for each element. Next, the items whose support value satisfies the requirement on the ?Credibility? are selected and put into a set denoted as L1. These steps are then applied to each subset of two elements, and the process is repeated as the number of elements in each subset is incremented by one. The process stops when no more element satisfying the requirement exists. Figure 1 shows an     example of handling large itemsets with the Apriori- algorithm.

Figure 1. An example of handling large itemsets with the Apriori-algorithm [22].

The Apriori-algorithm generates the candidate sets  consisting of the items whose support value is greater than the minimum credibility, and thus reduces the search space.

However, as the number of items satisfying the minimum sup increases, a large number of candidate sets requiring repeated scan of the database are generated. For generating the candidate set, the power set of the set of all the items is produced as candidate set and the frequency of each element of the candidate set is measured through the scan of database. This wastes time and space for producing non- existent items as the candidate set within the transaction data.

B. FP-growth algorithm The FP-growth algorithm [1] which will also be  compared with the scheme proposed in this paper compresses the database having frequent items into FP- tree, generates conditional-pattern-tree which is the mined tree associated with each item using a divide-and-conquer scheme, and mines the association rule. Figure 2 shows an example of FP-tree.

Figure 2. An example of  FP-tree [19].

FP-growth algorithm  FPGROWTH(R,P,minsup): If IsPath (R) then  foreach Xi   R do X ? P  Xi Supp(X) ? mini X{sup(i)} print X, sup(X)  else I ? {i | i  R, sup(i) ? minsup} foreach i  I in increasing order of sup(i) do  X ? P  {i} sup(X) ? sup(i) print X, sup(X) RX  ? ? foreach path  PathToRoot(i) do  pathsup(i) ? support of i in path path Insert p into FP-tree RX with support pathsup(i)  if RX ? ? then FPGROWTH (RX,X,minsup)   Given an FP-tree, R, the subsequent FP-trees are built  for each frequent item in R in the increasing order of support. In [20], all the occurrences of the frequent items in the tree are found, and for each occurrence, the corresponding path to the root is determined. The support value of the item on each such path is recorded, and the path is inserted into the newly built tree, RX, where X is the itemset obtained by extending the pre?x P with the items.

While inserting the path, the weight of each node in RX is incremented by the path support of item-i, pathsup(i). The resulting FP-tree is a projection of X that comprises the current pre?x extended with item-i. Then, the FPGROWTH function is called recursively with RX and X as the parameters. A basic case occurs when the input FP- tree is a single path. Single-path FP-trees are handled by enumerating all itemsets that are subsets of the path, with the support value of each such itemset given as the least frequent item in it [20].

Since the FP-growth algorithm does not generate candidate sets and share frequent items, it wastes less space than the Apriori-algorithm. It also excels it in the aspect of speed because it scans the database only twice.

However, the FP-growth algorithm requires the tree structure to be compressed for fast mining of association rule, causing the addition of new data difficult.

C. Quine-Mccluskey method The Quine?McCluskey algorithm is used for the  minimization of boolean function. It is functionally identical to Karnaugh mapping, but the tabular form is efficient to be used as computer algorithm. It also allows a deterministic way for checking if the minimal form of Boolean function     has been reached. It is sometimes referred to as the tabulation method. The algorithm consists of two steps:   1. All prime implicants of the function are found.

2. From the prime implicant chart, the essential prime  implicants of the function are found.

For example, assume to minimize the function of Eq. (3).

( , , , ) (4,8,9,10,11,12,14,15)f A B C D =?     (3)   One can easily form the canonical expression of sum of products of Eq. (4), simply by summing the minterms.

, , ,A B C Df A BC D AB C D AB C D AB CD AB CD ABC D ABCD ABCD  ? ? ? ? ? ? ? ? ? ?= + + + ? ? ? ?+ + + +  (4)   Eq. (4) is not minimal. In order to minimize the  function, all minterms that evaluate to one are first placed in the table of minterm. Then the minterms are combined with other minterms. If two terms vary by only a single digit, that digit is replaced by a dash indicating that the digit does not matter.

The process is continued until none of the terms can be combined any more, and this point the table of essential prime implicant is constructed.

Table 1. The essential prime implicants for Eq. (3).

A B C D  m(4,12)* -100  - 1 0 0  m(8,9,10,11)* 10--  1 0 - -  m(8,10,12,14)* 1--0  1 - - 0  m(10,11,14,15)* 1-1-  1 - 1 -   As shown in Table 1, four essential prime implicants have been extracted for Eq. (3). The third prime implicant can be 'covered' by the second and first, and thus neither is essential. In this example, one can combine the essential implicants with one of the two non-essential ones to yield either Eq. (5) or (6)   , , ,A B C Df BC D AB AC? ? ?= + +                (5) , , ,A B C Df BC D AD AC? ? ?= + +                (6)   Both the equations are functionally equivalent to the  original Eq. (4) [21]. Part C in Section 3 explains how this method is applied to the dataset.



III. THE PROPOSED SCHEME In this section a new method of mining is presented  which efficiently reduces the number of association rules  from big data while allowing high credibility and support.

Finding a rule with high credibility for the data of a large- scale database is very important in various fields including socially-aware computing where efficient big data processing is the main issue. To this end, the wTabular- algorithm is proposed which expresses the set of transaction data in binary numerals and combines the Quine-Mccluskey method with the weight-assignment method, and as a result increases the efficiency of mining the association rule.

A. The Basic Operation The proposed wTabular-algorithm allows efficient  mining of data by expressing the data set in binary numerals and combining the weights with the Quine- Mccluskey method to simplify the mining process. It consists of two phases: three steps of preprocessing and four steps of post-processing. The three preprocessing steps are for removing overlapped or unnecessary data and converting the data into a format suitable to the analysis as follows.

Step 1. The rules of the antecedent portion are aligned in  the number of 1?s, smaller first.

Step 2. Among the rules of the database, the rule of the  antecedent portion whose value of the consequent portion is 1 is found.

Step 3. The weights of the rules found in Step 2 are calculated, and those whose weights are smaller than a preset level are removed.

In order to reduce the number of rules generated after  the preprocessing above, the post-processing of four steps is proceeded by using the Quine-Mccluskey method. An example of the operation of the proposed scheme is presented in Subsection C.

Step 4. The data generated through the preprocessing are  compared with each other, and then reduced.

Step 5. The terms are reduced by comparing the terms  whose number of 1 is i and i+1, respectively. First, i=1. Here, the combined values are expressed as ?don?t care? bit (-). For example, for ? having one 1?s, 0100, and for ??  having two 1?s, 0101, {?? ??}=010-.

Step 6. The results of the first reduction are further reduced by applying Step 4 and Step 5 repeatedly. If no more reduction is possible, it stops.

Step 7. The overlapped rules are removed.

In order to evaluate the association of the rules obtained,  ?Support? and ?Credibility? are measured. Figure 3 shows the flow chart of the entire process of the proposed scheme.

B. Assignment of Weights to Association Rule Table 2 below is the dataset randomly generated to  explain the proposed scheme. S1-S4 represent sensor, and     the data obtained from the sensors are converted into binary data according to the given rule. F expresses the occurrence of the event in binary data (1: occurrence, 0: no occurrence) decided based on the S values.

Figure 3. The flow of the proposed reduction algorithm for association rule.

Table 2. The input dataset.

? ?? ?? ?? F 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1   Table 3 shows the data achieved by mining the data of  Table 2 via the pre-processing of the proposed scheme. In other words, a data set is formed using the rows whose F value is 1 in Table 2.

Table 3. The data whose value of pre-processing is 1.

? ?? ?? ?? F ? 0 0 0 1 1 ?? 0 1 0 1 1 ?? 1 1 0 0 1 ?? 0 1 1 0 1 ?? 0 0 1 1 1 ?? 1 0 1 1 1 ?? 0 1 1 1 1   Table 4 shows the prior probability, PPij, obtained for  the data of Table 3. PPij denotes the prior probability of Di for Sj. The prior probability is the percentage of the value of the sensor. For example, there are seven rows in Table 3, and every row has 0 value. As a result, the percentage of ?0? value for S1 is 5/7 0.7, and 0.3 for ?1? value. Similarly, that of ?0? value for S2 is 3/7 0.4.

Table 4. The prior probability of the data of Table 3.

? ?? ?? ?? F ? 0.7 0.4 0.4 0.7 1 ?? 0.7 0.6 0.4 0.7 1 ?? 0.3 0.6 0.4 0.3 1 ?? 0.7 0.6 0.6 0.3 1 ?? 0.7 0.4 0.6 0.7 1 ?? 0.3 0.4 0.6 0.7 1 ?? 0.7 0.6 0.6 0.7 1   The posterior weights are assigned to the data of Table  4, which is based on the Bayesian theory. The posterior weight of Di, Wi, is obtained by Eq. (7).

n  ij j  i  PP W  n == ?  (7)   In Eq. (7), n denotes the number of sensors,  and thus Wi  is average prior probability of Di, Table 5 shows the weights obtained by Eq. (7). Here ?? is removed since its weight is lower than the minimum weight of 0.5 to guarantee desired ?Credibility? and ?Support? with the reduction of association rule.

Table 5. The weights of the data of Table 4.

? ?? ?? ?? W ? 0.7 0.4 0.4 0.7 0.55 ?? 0.7 0.6 0.4 0.7 0.6 ?? 0.3 0.6 0.4 0.3 0.4 ?? 0.7 0.6 0.6 0.3 0.55 ?? 0.7 0.4 0.6 0.7 0.6 ?? 0.3 0.4 0.6 0.7 0.5 ?? 0.7 0.6 0.6 0.7 0.65     C. Reduction of  the Rules The rules are reduced by using the proposed wTabular-  algorithm. Table 6 shows the rules after removing the rules of lower weight than the minimum weight.

Table 6. The rules after the reduction.

? ?? ?? ?? F ? 0 0 0 1 1 ?? 0 1 0 1 1 ?? 0 1 1 0 1 ?? 0 0 1 1 1 ?? 1 0 1 1 1 ?? 0 1 1 1 1   The first reduction of Step 4 of the post-processing is  applied when f = 1 for the data of Table 6. Table 7 shows the results of the first reduction.

Table 7. The results of the first reduction.

? ?? ?? ?? F ?,?? 0 - 0 1 1 ?,?? 0 0 - 1 1 ??,?? 0 1 - 1 1 ??,?? 0 1 1 - 1 ??,?? - 0 1 1 1 ??,?? 0 - 1 1 1   After the first reduction, Step 5 and 6 of the post-  processing are applied repeatedly until no reduction is possible. Table 8 shows the result of the second reduction, and Table 9 shows the final association rule achieved.

Table 8. The result of the second reduction.

? ?? ?? ?? F ?,??,??,?? 0 - - 1 1 ?,??,??,?? 0 - - 1 1   Table 9. The final association rule.

? ?? ?? ?? F 0 - - 1 1    D. Evaluation of the Association Rule According to the formula defined by Eq. (1) and (2), in  case of the rule, { ?? } ? f, of Table 8, ?Support? and ?Credibility? are 100%. The support and credibility of {??}?f satisfying the minimum support of 20% from Table 1 are calculated by using the Apriori-algorithm, and they are about 38% and 87%, respectively. The reduction rate of Apriori-algorithm and wTabular-algorithm are about 69% and 92%, respectively.

As can be seen from the example above, the proposed wTabular-algorithm can be superior than the Apriori-  algorithm not only in terms of ?Support? and ?Credibility?, but also in the reduction rate of the rule.



IV. PERFORMANCE EVALUATION In this section the performance of the proposed  wTabular-algorithm is evaluated using computer simulation.

The system used for the performance evaluation has Intel 2.8Ghz core 2 Quad CPU, 8GB RAM, 32bit Window 7, and the simulation code is written in C++ Language with Visual studio 2010.

The Apriori-algorithm and FP-growth algorithm broadly used for the mining of association rule are also evaluated along with the proposed wTabular-algorithm on credibility, support, rule reduction rate, and data processing speed as the size of the data is varied.

The database used for the evaluation is randomly built whit the sensed data from human body, having the attributes of seven preprocessing and two post-processing. Each data item is expressed in binary such that the value is 1 if it is higher than the threshold, and 0 if not. For example, if the weight is normal, then it is 0. Otherwise, it is 1. The evaluation was made as the size of the dataset is gradually increased from 100,000 to 1,000,000.

Figure 4 and 5 show the results of the comparisons of Support and Credibility as the size of data varies in selecting the rule of {??}? ? among the results of reduction with the given algorithm.

Figure 4. Comparison of ?Support? of the schemes.

Observe from the figures that the proposed wTabular scheme allows higher ?Support? and ?Credibility? than the other two algorithms.

Also, as can be seen from Figure 6, the proposed scheme shows the highest rule reduction rate among the three. It can thus said to be suitable for socially-aware computing in which accurate service needs to be provided through fast comparison of minimum data with high credibility.

1 3 5 7 10 (X100000)  Su pp  or t (  % )  Number of data  Apriori FP-growth wTabular     Figure 5. Comparison of ?Credibility? of the schemes.

Figure 6. Comparison of the rule reduction rates.

Figure 7 shows the comparison of the rule mining times for different size data. When the size of data is relatively small as below 500,000, the proposed scheme takes slightly longer than the FP-growth algorithm, but it is faster for larger size data. It is thus deemed to be preferred for the applications handling big data.

Figure 7. Comparison of the data processing times.



V. CONCLUSION AND FUTURE WORK Reduction of association rules for the given database is a  very important issue in the field of socially-aware computing that needs to manipulate large-scale data. A number of studies have been reported regarding the taxonomy of attribute and negative association rule in order to efficiently reduce the data and find the association rule.

This paper has presented the wTabular-algorithm which reduces the number of association rules by removing the data dissatisfying the minimum weight and employing the Quine-Mccluskey reduction method. Evaluation of the proposed scheme through computer simulation reveals that ?Support?, ?Credibility?, rule reduction rate and the data processing time are improved compared to the existing schemes.

As future study, the proposed approach will be extended for real time processing of streaming data. Note that sensors collect unformatted streaming data in real time. Therefore, there is a need for the study on the methodology of association rule reduction to directly handle unformatted data without converting into binary data. Moreover, further research is needed to improve the performance of association rule reduction by constructing the association between the sensors in advance through semantic sensor web. The proposed approach will also be evaluated against a variety of domains and different-size data set.

ACKNOWLEDGEMENT   This research was supported in part by Korea Association of Industry, Academy and Research Institute(C0017380), DAPA and ADD(UD10070MD), Basic Science Research Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Education, Science and Technology(2012R1A1A2040257) and MSIP(Ministry of Science, ICT & Future Planning), Korea in the ICT R&D Program 2013(1391105003). Corresponding author: Hee Yong Youn.

