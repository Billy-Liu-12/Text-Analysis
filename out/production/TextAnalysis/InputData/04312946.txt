A Binary Decision Diagram to discover low threshold support frequent itemsets

Abstract? Discovering association rules that identify relation- ships among sets of items is an important problem in data mining. Finding frequent itemsets is computationally the most expensive step in association rule discovery and therefore it has grasped significant research focus [1]. Discovery of frequently occurring subsets of items, called itemsets, is the core of many data mining methods. Most of the previous studies adopt Apriori- like algorithms, whom iteratively generate candidate itemsets and check their occurrence frequencies in the database. These approaches suffer from serious costs of repeated passes over the analyzed database. In this paper, we propose a new BDD-based (Binary Decision Diagram) data structure called TREESUPBDD.

The TREESUPBDD extends the idea claimed by the authors of FP-TREE [9] and ITL-Tree [5] structures, aiming to improve storage compression and to allow frequent pattern mining with- out an ?explicit? candidate itemset generation step. To address this problem, we propose a novel method, called TREESUPBDD- MINE, for reducing database activity of frequent itemset dis- covery algorithms. The idea of TREESUPBDD-MINE consists in using a Binary Decision Diagram and a tree for representing both database and frequent itemsets. The proposed method requires one scan over the source database : to create the associated tree and BDD and check discovered itemset supports. The originality of our work stands on the fact that the proposed algorithm extracts the frequent itemsets directly from the TREESUPBDD.

Carried out experiments showed very encouraged results. Its performance improvements have been shown in a series of our experiments. We extend the binary decision diagram structure to store transaction groups and propose a new method to discover frequents itemsets. To study the trade-offs in the new representation of transactions in binary decision diagram, we compare the performance of our algorithm with the fastest Apriori [2] implementation algorithm and the latest extension of FP-Growth [15]. We have tested all the algorithms using different benchmark datasets. The performance study shows that the new algorithm significantly reduces the processing time for mining frequent itemsets from dense datasets that contain relatively long patterns and for low threshold. We discuss the performance results in detail and also the strengths and limitations of our algorithm.

Keywords: data mining, association rules, frequent item sets, binary decision diagram

I. INTRODUCTION  The discovery of association rules is an important problem in data mining. It is a two-step process beginning by finding the frequent itemsets and then generating association rules from the output of the first step. Most of the research attention  was paid on efficient methods of finding frequent itemsets because it is computationally the most expensive step [16]. In this paper, we browse the possibility to use a new data structure and a more efficient algorithm for mining frequent itemsets from datasets. The improvement is achieved by sweeping the database just once and by reducing item traversals within transactions. We present performance comparisons of our algorithm vs the fastest Apriori implementation and the latest developed FP-growth algorithm. These results show that our algorithm largely outperforms both Apriori and FP-growth on several benchmark datasets with short and long frequent patterns and also for low support thresholds. The rest of this paper is organized as follows. In section (2) we briefly recall basic definition of association rules. Section (3) introduces our approach which is based on the adaptation of binary decision diagram to discover frequent itemsets. Experimental results are shown in section (4). Section (5) concludes and skittles future avenues of following work.



II. BACKGROUND  Frequent itemset mining came from efforts to discover useful patterns in customers? transaction databases [1]. A customers? transactions database is a sequence of transactions (T = {t1, t2, ..., tn}), where each transaction (ti ? T ) is an itemset. An itemset with k elements is called a k-itemset. The support of an itemset X in T, denoted as support(X), is the number of those transactions that contain X. An itemset is frequent if its support is greater or equal than a given support threshold, originally denoted by minsup. The frequent itemset mining problem is to find all frequent itemsets in a given transactions database.

Finding frequent itemsets is the most fundamental and essential problem in finding association rules, from which as- sociation rules can be directly generated [10]. The search space is exponential in the number of database attributes. Hence, the problem of I/O constituted a bottleneck of large approaches [16]. Due to its important, finding frequent itemsets attracts important attention in numerous research communities, a lot of algorithms have been proposed in the different mining literature algorithm [6], [7]. Many tendencies for representing  18th International Workshop on Database and Expert Systems Applications  DOI 10.1109/DEXA.2007.150     and handling transactions databases may be presented out as follow :  ? Horizontal format: Used in the Apriori algorithm, which traverses the search space in a pure breadth-first manner and finds the support information by explicitly generating and counting each node. This format is considered as the classical representation. The dataset is seen as a succession of transactions, each one identified by an identifier tid.

? Vertical format : It is used by Eclat [16] and Partition [11]. It consists in representing the dataset vertically by giving to each item its tidset, i.e. the set of transactions containing this item.

? Bitvectors : Bitvectors consist in representing data as bitvectors compressed using a strategy called Run Length Encoding.

? FP-Tree : This data structure is an extended prefix-tree structure for storing compressed and crucial information about frequent patterns. This data structure has been used in the FP-growth algorithm for mining frequent patterns [8]. It uses a prefix-tree structure to mine fre- quent itemsets without generating candidates and scans the database only twice. Song and al [14] proposes Trans- action Mapping algorithm using lexicographic prefix tree to generate candidate itemsets. This algorithm employs the vertical representation of a database. Transaction ids of each itemset are mapped and compressed to continuous transaction intervals in a different space thus reduc- ing the number of intersections. When the compression coefficient becomes smaller than the average number of comparisons for intervals intersection, the algorithm switches to transaction id intersection. This transactions tree is similar to FP-Tree except that there is no header table or node link. The transactions tree is a compact representation of all the transactions in the database. Each node in the tree has an identifier corresponding to an item and a counter keeping the number of transactions that contain this item in this path. Gopalan and Sucahyo [5] propose the ITL-Mine algorithm using Item-Trans Link structure, that combines the vertical and horizontal data layouts. Item-Trans Link is constituted by an ItemTable (contains all individually frequent items) and TransLink (represents each transaction of the database that contains items of ItemTable). ITL-Mine reduces the cost by scan- ning the database only once, by significantly reducing the horizontal traversals of the transactions in memory and keeping the links between transactions in memory unchanged during the mining process. After that Gopalan et al propose another algorithm calling TreeITL-Mine [4].

This algorithm uses a modified prefix tree to compress the transactions before storing them in Item-Trans Link.

Transactions that contain the same set of frequent items (or 1-frequent itemsets) are grouped together using the prefix tree. Each group of transactions is then replaced by a single transaction and a count of the number of  transactions in the group.

A. Discussion  In the context of mining frequent patterns in transaction databases or many other kinds of databases, an important number of studies rely on Apriori-like ?test-and-generate? approach [1], [13]. However, this approach suffers from a very expensive candidate set generation step, especially with long patterns or under low user-requirements. This drawback is reinforced with tediously repeated disk-stored database scans.

To avoid the approach bottleneck, other studies like FP-Tree structure [8] proposed to adopt an advanced data structure, where the database is compressed in order to achieve pattern mining.

The idea behind the use of compact data structure FP-Tree is that when multiple transactions share an identical frequent itemset, they can be merged into one with a registered number of occurrences.

In addition to the costly sorting step, the proposed FP- Tree structure is unfortunately not suited for an interactive mining process, in which a user may be interested in varying the support value. In this case, the FP-Tree should be rebuilt since its construction is support dependent. structure, called CATS, in which a single item is An intuitive idea stands on the fact that we introduce a support independent, more compact structure called TREESUPPBDD, in which each node is composed by an itemset and index a SupBDD (Support Binary Decision Diagram) in order to compute the associate support.



III. BINARY DECISION DIAGRAM AND MINING PROCESS  Several works have addressed the problem of efficiently mining frequent itemsets. One of the key problems is to find an appropriate data structure for representing the had led data during this process into main memory like transactions data base and frequent itemsets. In the work of Salleb [12], the author uses an extension of BDD having multi-terminal nodes, called Algebraic Decision Diagrams (ADD) as a data structure for representing and loading transactional datasets.

A. Binary Decision Diagram  A Binary Decision Diagram (BDD) is a graph-based repre- sentation of boolean functions. Functions are represented by a directed acyclic graph. BDD is a directed acyclic graph with 2 terminal nodes 1 and 0. Each non-terminal node has an index to identify a variable of the boolean function, and has two outgoing edges ; the dashed one means that the variable is fixed to 0, whereas the other one means that the variable is fixed to 1. A BDD represents a disjunctive normal form of a boolean function : each path from the root of a BDD to a leave indexed by number 1 gives a conjunction of literals (where a literal is either a variable or the negation of a variable) that is true for that boolean function. Given a boolean function, it is possible to represent it by a canonical graph, using the following rules.

Fig. 1. Associated binary tree to the function (a), Apply of the sharing of all the subgraphs (b) and BDD (c).

1) Choose an order on variables : x1 ? x2 ? ... ? xn ; variables appear in this order in all the paths of the graph and no variable appears more than once in a path.

2) Eliminate all the redundant nodes whose two edges point to the same node.

3) Share all equivalent subgraphs.

Operations (AND, OR, ...) on BDDs have been defined in [3].

A function graph can be reduced in size without changing the denoted function by eliminating redundant vertices and duplicate subgraphs.

We illustrate in Figure 1(c) the corresponding BDD of the boolean function f(x,y,z)=(?x? y ??z)? (?y ? z)? (x? y), with the order of variables (x ? y ? z). In Figure 1(a), we present the associated function with a binary tree. Figure 1(b) presents the sharing of all the subgraphs.

B. From Databases to Support Binary Decision Diagram : TREESUPBDD  In this paper we propose a new data structure, called TREESUPBDD (Tree Support Binary Decision Diagram) aim- ing to cope with the revealed drawbacks by representing data using a tree and a modified BDD. Any node of this tree represents a frequent itemset and have an index to identify ?implicitly? the associated transactions identifiers. This in- dex is a SupBDD (Support Binary Decision Diagram). The SupBDD represents the transactions identifiers (TIDs) for the corresponding itemset. That is, suppose the data for a given problem are encoded as bit vectors of length n. Then any subset of the vectors can be represented by a boolean function over variables yielding 1 when the vector corresponding to the variable assignment is in the set. In our case, any transaction can be encoded with b1b2...blog2(m) bits, where each bit bk is in {0, 1} and m is the total number of transactions. The SupBDD of an itemset I is a reduced BDD. Support Binary Decision Diagram (SupBDD) is a BDD with only one terminal node. The bit vectors themselves can have many elements equal to zero. In fact, our particular encoding to reduce BDD was chosen for this purpose. This motivates choosing a representation that exploits both forms of sparseness. Reduced  BDDs are much like BDDs, except that we use a new reduction rule. That is, a node 0 can be omitted with the corresponding edges. For sparse sets (vector with many 0), this condition frequently arises, and hence many edges eliminations are possible. Formally, given a boolean function, it is possible to represent it by a canonical graph, adding the following rule for the three rules already presented.

? Delete rule: Eliminate the zero terminal node and the associated edges.

Hence, with this structure TREESUPBDD, we represent either the transactions identifiers and also frequent itemsets.

Furthermore, we can compute the support without accessing database. A large TREESUPBDD can be constructed by scan- ning the database where each different itemset is mapped into different locations in the tree, then the entries of the tree index is a BDD that gives ?implicitly? the actual count (i.e support) of each itemset in the database. The support of any itemset I can be computed using the following lemma 1.

Lemma 1: support(I) = ?R  i=1 2 Nbsauti  Where ? R is all paths from the root to the terminal node 1.

? Nbsauti is the cardinality of the pruning nodes.

Table I represents a transaction database. Having six trans-  Tid items T1 A, C, T, W T2 C, D, W T3 A, C, T, W T4 A, C T5 A, C, T T6 C, D, T, W  TABLE I  A TRANSACTION DATABASE D  actions, the transaction identifiers can be decoded with 3 bits (?log2(6 + 1)?=3). The corresponding truth table of transactions identifiers is presented in table II. The itemset  ID x y z T1 0 0 1 T2 0 1 0 T3 0 1 1 T4 1 0 0 T5 1 0 1 T6 1 1 0  TABLE II  A TRUTH TABLE  A, is frequent relatively the threshold 3, since it appears 4 times in D Support(A) = 21 + 21 (the first 21 : corresponds to the omitted variable y int the first path and the second one corresponds to the omitted variable z in the second path).

For the itemset T , the support(T ) = 21 + 20 + 20 = 4, 20 corresponds to zero omitted variable in the path. The associated SupBDD is given by figure 2 left. We show the complete TREESUPBDD in Figure 2 right.

A T W  x  z   y  x  z   y  z  x  z   y y  z  C  x  z   y  T  x  z   y  C x  z   y y  z  T  x  z   y  T W  x  z   y  z  x  z   y y  z  W x   y  zz  W x   y  zz  Fig. 2. Left : First level of TREESUPBDD structure for the transaction database given by table I ; Right : Complete TREESUPBDD.

X : an itemset.

T : a transaction from the database.

Li : a list of itemsets.

TID : a transaction identifier.

X.nocc: number of occurrence of X.

XLf : child list of X.

SA : Result of AND operation between tow SupBDDs.

cardA : #SA.

IF : The set of frequent itemsets.

kIF : frequent k-itemset .

TABLE III  NOTATIONS USED IN ALL ALGORITHMS  We introduce here the algorithm TREESUPBDD-MINE aiming to compute frequent itemsets. Notations used by dif- ferent algorithms are summarized in table III.



IV. IMPLEMENTATION AND EXPERIMENTAL RESULTS  We developed a prototype, called TREESUPBDD-MINE to discover frequent itemsets. It has been developed in C language and is based on the use of an adapted BDDs to represent frequent itemsets and to compute supports. Our implementation relies on the buddy library : BuDDy - A Binary Decision Diagram Package. This free library is able to manage BDDs with a number of nodes up to 228, i.e., more than 250 million nodes. The aim of the experiments is twofold : first, to test whether that data structure is suitable for loading transaction database in memory ; second, to study the performance of the new algorithm for real databases and for low thresholds. Experiments have been carried out on a PC Pentium 4, 2.66 GHz processor with 1 GO of main memory, running under Linux Mandrake 9.2. We have experimented TREESUPBDD-MINE on some benchmark datasets : MUSH- ROOM, CHESS and CONNECT(1) and compare it with Apriori and FP-growth algorithms implementation. The description of  1Available at: http://fimi.cs.helsinki.fi/data  Algorithm: TREESUPBDD-MINE1 Input: Database (D), minimum support (minsup) Output: The complete set of frequent itemsets.

Begin2  /* Step 1: reading and extracting the items */3 Foreach (Transaction T ? D ) do4  Foreach (X ? T ) do5 Add items(X , Li);6  End For;7  End For;8 /* Step 2: remove infrequent itemsets */9 Remove item(Li,minsup);10 /* Step 3: TREESUPBDD construction */11 IF=Create Tree(Li, minsup);12 return IF13  End14 Algorithm 1: TREESUPBDD-MINE  Algorithm : Create Tree1 Input: List of frequent 1-itemsets Li Output: The complete set of frequent itemsets.

Begin2  X = head of liste of frequent itemsets ;3 Foreach (X ?= ?) do4  Xsucc=X.next;5 XLf = ?;6 kIF=kIF ? X;7 IF= IF ? kIF ;8 Foreach (Xsucc ?= ?) do9  SA=intersection(X.SupBDD,10 Xsucc.SupBDD); cardA=calcul support(SA);11 If (cardA ? minsup) then12  insert(Xsucc, XLf );13  End If;14 Xsucc=Xsucc.next;15  End For;16 Create Tree(list of children of X);17 Eliminate the last element of kIF ;18 X=X.next;19  End For;20 return IF21  End22 Algorithm 2: Create Tree     these databases is given by the table IV. In fact, in the data  DATABASES NUMBER OF NUMBER AVG. LENGTH TRANSACTIONS OF ITEMS OF TRAN.

CHESS 3196 75 37 MUSHROOM 8124 119 23 CONNECT 67557 129 43  TABLE IV  CHARACTERISTICS OF DATABASES.

base Mushroom, maximal itemsets are long which is quite the average length of a transaction. In addition, MUSHROOM, CONNECT and CHESS are known to be dense (with long maximal itemsets). We present performance comparisons of our algorithm against the fastest Apriori implementation and the latest developed FP-growth algorithm. For dense datasets, statistics are illustrated by Figure 3 recapitulate performances for MUSHROOM, CONNECT and CHESS respectively. We notice that the execution time is polynomial dependent of the number of items. Also, execution times obtained for the MUSHROOM base are lower than those obtained for the CHESS. This difference can be simply explained by the density, highlighted by the corresponding number of nodes, of the considered bases.

In the sequel, we are also interested in the study of another aspect putting the focus on the possibility to manipulate less threshold support. We remarked that our algorithm can generate frequent itemsets with low thresholds compared with FP-growth and Apriori. However, this possibility tends to stagnate reaching a given density of transactions database.

Our experiments showed that a better performance is ob- tained when using TREESUPBDD-MINE. As shown in Figure 3 our algorithm is very efficient for low thresholds values.

0  1  2  3  4  5  6  7  8  9  10  R un  tim e  (s )  minsup (%)  Mushroom  TreeSupBDD-Mine Apriori  FP-Growth           10  20  30  40  50  60  70  80  R un  tim e  (s )  minsup (%)  Connect  TreeSupBDD-Mine Apriori  FP-Growth           0  10  20  30  40  50  60  70  R un  tim e  (s )  minsup (%)  Chess  TreeSupBDD-Mine Apriori  FP-Growth  Fig. 3. Sequential Runtime versus threshold support for Mushroom, Connect and Chess databases

V. CONCLUSION In this paper, we have studied the issue of mining frequent  BDD from databases. We have presented a new efficient  algorithm that mines frequent itemsets. The algorithm is built based on a canonical form called TREESUPBDD. The experiments showed that the proposed algorithm performs in polynomial way instead of the exponential growth shown by other algorithms. Experiments led on real data show very promising results compared to the literature results. Then, we will propose a parallel version of this algorithm under cluster of PC?s. The idea is to construct Tree-SupBDD in parallel. Indeed, the construction method leads to a natural parallelization, in the sense where each processor of a parallel architecture can construct locally its ordered structure. Once the local structures are constructed, a master processor can merge them to derive a global Tree-SupBDD. Detailed results of such parallelization are under study.

