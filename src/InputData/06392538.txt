An Incremental Semi Rule-based Learning Model for Cybersecurity in Cyberinfrastructures

Abstract?Classification algorithms are popular data mining methods that are widely used in intrusion detection systems.

However, they tend to show good performance on off-line data and their accuracy drop when they are deployed to realtime intrusion detection systems that receive data streams constantly.

In other words, the problem with a conventional classifier model, when applied to realtime detection systems, is that it is likely to perform poorly against new data that is different from the patterns against which the model was trained. This work presents an adaptive classification framework for a realtime intrusion detection system that changes its behavior according to the patterns occurred in the new received data. The performance of the system is constantly monitored and whenever it drops under a certain level of accuracy, the classifier model is retrained to learn new patterns and eliminate the obsolete ones from its knowledge.

Further, the framework uses a semi rule-based approach to classification in order to make the model more understandable for human experts and facilitate the user interference in the learning process.



I. INTRODUCTION  While the development of cyberinfrastructure that provides online services and hosts applications continues to increase in a wide variety of disciplines such as healthcare, homeland security, energy, telecommunications, environment, transporta- tion, and manufacturing, cyber threats still remain a ma- jor concern in threatening the integrity, confidentiality and availability of resources and services provided by the cyber systems. Vulnerabilities in common security components of cyber systems are inevitable due to design and program- ming errors. Therefore cyberinfrastructure can be constantly attacked through existing vulnerabilities [1].

Cybersecurity researchers aim to design and develop various cyber defense systems to secure information management systems against intentional and potentially malicious threats.

Security techniques for cybersecurity problems are categorized to proactive and reactive solutions. Proactive approaches re- quire user authentications, information protecting, a software capable of avoiding programming errors. In contrast, reactive security solutions detect intrusions based on the information from log files and network flow, so that similar attacks can be prevented in the future. Considering the characteristics of reactive cybersecurity solutions, data mining methodologies have the capabilities to address the challenges of cybersecurity in cyberinfrastructure.

Data mining is the process of extracting buried or previously unknown pieces of information from large databases [5]. It concentrates on the automated discovery of facts, patterns and relationships to be used for knowledge-driven decisions. Data mining methods have been widely applied to many application domains such as finance, engineering, and biology. Due to the availability of large amounts of data in cyberinfrastructure and the number of cybercrime attacks attempting to gain access to the data, different data mining methodologies have been designed and successfully deployed to secure the cyberinfras- tructure. One area of cybersecurity that benefits a lot from data mining methods is the intrusion detection task.

In cyberinfrastructure an intrusion is defined as any set of actions that attempts to compromise the integrity, confidential- ity or availability of a resource [2]. Denning [3] summarizes the generic model of an intrusion detection mechanism by means of system input and the specific intrusions to be monitored. An intrusion detection system (IDS) monitors and restricts user access (behavior) to the computer system by ap- plying certain rules. The rules are based on expert knowledge extracted from skilled administrators who construct attack scenarios and apply them to find system exploits. The system identifies all intrusions by users and takes or recommends necessary action to stop an attack on the system. Classification algorithms are popular data mining methods that are widely used in intrusion detection systems.

The intrusion detection problem is dynamic in nature. In other words, there is always the possibility that new attacks happen and the system can not detect them simply because it has not learned patterns occurred in unknown attacks during the training phase. Therefore the system does not have the required knowledge to identify the new attacks. However, cyberinfrastructure provides a huge amount of data stream- ing continuously and dynamically. These data can be used for incremental learning. Constructing incremental prediction models are challenging whereas ?time? adds important infor- mation for the understanding and learning of anomalies. New hybrid data mining methodologies, algorithms, and tools are required for incremental modeling to extract new patterns from raw data that identify normal and anomaly behaviors.

This paper presents an adaptive cyber defense system using several data mining methodologies. In particular, an incremental learning approach to building a classifier model   Cyber Technology in Automation, Control and Intelligent Systems  May 27-31, 2012, Bangkok, Thailand    is proposed for a realtime intrusion detection system. The proposed solution is based on a semi rule-based classifier algorithm that integrates class association rules and a well- known machine learning technique, support vector machines (SVM). The reason behind using class association rules is to improve the interpretability and understandability of the learning and prediction tasks for domain experts. Further, pure machine learning techniques work based on mathematical and statistical algorithms, and use domain independent biases to extract the rules. Therefore, they are not able to discover all the interesting rules and limit the user interference in the learning process. Further, to make the classifier adaptive, the proposed algorithm utilizes a clustering approach.



II. BACKGROUND Data mining employs algorithms and techniques from statis-  tics, machine learning, artificial intelligence, databases and data warehousing, etc [5]. Classification, clustering and asso- ciation rule mining are among the most common data mining tasks.

A. Classification  Data objects stored in a database are identified by their attributes, and the main idea of classification [5] is to explore through data objects (training set) to find a set of rules which determine the class of each object according to its attributes.

Theses rules are later used to build a classifier to predict the class or missing attribute value of unseen objects whose class might not be known. Classification consists of two steps. In the first step (the learning process), the aim is to build a model that describes the characteristics of data classes and attributes of the training set by generating a set of rules. In the second step (the testing process), the model is run over a test set to predict the classes of objects or data whose classes are not provided. There exist many classification algorithms for supervised learning. In this study, we have used support vector machine algorithm to build the classifier model.

Support Vector Machines: Support vector machines (SVM) are known as a powerful technique for classification prob- lems [6]. The goal of SVM is to construct a separating hyperplane that is maximally distant from different classes of the training data (Optimal Separating Hyperplane). The formal definition of linear SVM can be described as follows [6]. Let the training data of two separable classes with n samples be represented by (?x1 , ?y1) , (?x2 , ?y2) , . . . , (?xn , ?yn) , i = 1, 2, . . . , n where ?x ? RN is an N dimensional space (feature vector) and yi ? {?1,+1} is the class label, maximize:  L (?) =  n?  i=1  ?i ? 1  n?  i=1  n?  j=1  ?i?jyiyj ( ?xTi ? ?xj  ) (1)  , under the constraints ?n  i=1 ?iyi = 0 and ?i ? 0, i = 1, 2, . . . , n. The optimal separating hyperplane can be found from the solutions ?is to this maximization problem. A soft- margin algorithm as an extension of the basic algorithm is available, when the instances are not linearly separable.

B. Clustering  Among all data mining techniques, clustering a.k.a unsupervised learning, is one of the most useful tasks for discovering groups and identifying interesting patterns in the underlying data. Clustering algorithms group a dataset into categories where there are similar proximity among data points that belong to a given category according to a distance metric. Distance measures are usually used in clustering tasks to quantify the closeness of a particular object to its neighbors. There exist many clustering algorithms for unsupervised learning. In this work, we have used DBScan clustering algorithm to identify the new types of attacks.

DBScan:Density based algorithms typically regard clusters as dense regions of objects in the data space that are separated by regions of low density. In this paper, the DBSCAN (Density Based Spatial Clustering of Applications with Noise) algo- rithm [7] was chosen. The key idea in DBSCAN is that for each point in a cluster, the neighborhood of a given radius has to contain at least a minimum number of points. The main ad- vantage of DBSCAN is that it can handle noise (outliers) and discover clusters of arbitrary shape. The DBSCAN algorithm is based on the concepts of density reachability and density- connectivity. In the DBSCAN terminology, a cluster is defined as the set of objects in a data set that are density-connected to a particular core object. Any object that is not part of a cluster is known as noise. The DBSCAN algorithm starts with an arbitrary point p to be a core object, and retrieves all points reachable from it. All such nodes are therefore part of the same cluster, together with the arbitrary starting point. If DBSCAN discovers p is not a core object, then p is considered to be noise and DBSCAN chooses the next unassigned object. Once every object is assigned to a cluster or noise, the algorithm stops.



III. THE PROPOSED MODEL Our proposed online adaptive cyber defense framework  includes the following components: data preprocessing, rule mining algorithm, learning algorithm, prediction model, and a component that maintains the high accuracy of the classifier by using an incremental learning approach. This framework utilizes different methods and algorithms corresponding to the aforementioned components. In the data preprocessing step, useful features relevant to the pattern analysis are first selected and then the collected data is prepared in an appropriate format for rule mining task. Once features are selected and data is preprocessed, the rule mining algorithm extracts strong class association rules from the training set. The extracted rules are first weighted based on a scoring metric strategy. Then, the training data is transformed from the input space to a new rule- based space in which space features represent the extracted rules and the transformed data, presented in a set of rule- based feature vectors, shows the distribution validity of rules over the training set. Eventually, rule-based feature vectors are given to the SVM algorithm and a classifier model is built. The performance of the classifier is constantly monitored and the classifier is re-built if its quality drops under certain threshold.

Get initial dataset, minimum accuracy  and within-class (WC) variation thresholds  Build SVM classifier  Select effective features  Discretize dataset  Extract class association  rules  Map dataset to the rule  space  Calculate WC  variation of ck  Get and discretize new instance  Predict class ck of new instance by SVM  classifier  Update the support and confidence of the existing rules  Update the rule-based feature vectors  Cluster instances of Unlabelled set  Add labeled instances to the training set  Extract class association rules  Is there any new data instance?

Pre-Processing  Incremental Learning  Monitoring the performance of the classifier  Does the quality of ck drop more  than the threshold?

Is time to update  the model ?

Updating the Classifier  Add the new rules and eliminate the obsolete rules to/from rule space  Yes  No  No (classified)  No Yes  Yes (misclassified)  Does it improve the quality of any existing  class ci?

Add the data instance to the training set with label ci and update the support and  confidence of the existing rules  Add the data instance to the unlabeled set  No  Yes  Fig. 1. Activity diagram of the proposed framework  Figure 1 shows a high level activity diagram of our system.

In the rest of this section, the different components of the proposed model will be described.

A. Data Preprocessing  In a dataset used for learning patterns, a feature is consid- ered to be useful, if it can discriminate patterns from different classes. In order to improve the performance of our semi-rule- based classifier, a simple filtering step is applied before gen- erating class association rules. This filtering procedure works as follows. Given a dataset and a threshold t, feature values are distributed to different sets each of which corresponds to an existing class. Then, the mean value of each set as well as the standard deviation of the mean values are calculated. If the standard deviation is below the given threshold, that feature is filtered out and does not survive to the next step.

One problem of (class) association rule mining algorithms is that they require that objects are described using categorical attributes, i.e., the rule mining component of our classifier can not directly deal with numeric attributes. As quantitative information is available in IDS, this is a problem of practical importance. This issue is usually addressed by applying a data discretization technique. Discretization is the process of mapping the range of possible values associated with a continuous attribute into a number of intervals each denoted by a unique integer label; and converting all the values associated with this attribute to the corresponding integer labels. In our study, discretization of continuous attributes is conducted using a method developed by LUCS-KDD group1.

1http://www.csc.liv.ac.uk/?frans/KDD/Software/LUCS-KDD-DN/lucs-kdd DN.html  B. Semi-Rule-based Prediction Model  Mining Class Association Rules: In the proposed framework, a class association rule which is simply an ?if-then? rule can represent an intrusion signature. The set of intrusion signatures extracted by the rule mining algorithm from the intrusion and normal data is learned by the classifier for future intrusion prediction. Given a transactional dataset D consisting of transactions T = {t1, t2, . . . , tn}, a set of items I = {i1, i2, . . . , ik} consisting of all items in D, which is also the range of possible values for the transactions, a set of class labels C = {c1, c2, . . . , cn}, support and confidence thresholds; the objective of a class association rule mining algorithm is to generate the set of rules, in the form (X ? c), where X ? I and c ? C, that have a support and confidence greater than given thresholds. Support of a class association rule (X ? c) in D is defined as the fraction of transactions that contains X ? c to the total number of transactions. The support of a rule indicates the statistical importance of the correlation between X and c in dataset D. Confidence of a class association rule (X ? c) in D is defined as the fraction of transactions that contains X ? c to the total number of transactions that contain X . It can also be interpreted as a measure to identify how strong a class association rule is.

The method we have used for class association rule mining follows the CBA rule generator approach, called CBA-RG [4].

In CBA-RG, an apriori association rule mining algorithm is adapted to find desired class association rules. Please refer to [4] for further information about the CBA-RG algorithm.

Once the rules are extracted, they can be manipulated (add/delete/update) by a human expert. For instance the expert can eliminate some of the rules which might not make sense in reality or can add to the rule set some interesting     rules that are not generated by the algorithm.

Weighting Rules: The significance of a class association rule is determined by its discrimination ability over the training set when used in the learning process. We argue that a rule is a significant discriminator if it is strong and covers more patterns (transactions) in the training set. Therefore, we weight class association rules based on their support and confidence. Confidence and support values indicate the strength and statistical importance of the rules, respectively. We use the following scoring metric: W  ( rCi  ) = rCi .conf ? rCi .sup/dCi , where rCi denotes  a class association rule r from the set of extracted class association rules whose antecedent is class Ci, and dCi denotes the distribution of class Ci in the training set, i.e., number of training data items whose class labels are Ci.

Mapping Data to the Rule Space: A feature vector has the form f1 = v1 ? f2 = v2 ? f3 = v3 ? . . . ? fn = vn, where fi is a feature (attribute) and vi is its value. A feature vector is used to describe the key properties of the training set to be learned by the classification algorithm. However, in order to make the learning process more understandable for domain experts, we transform the input space to a new rule space. In the rule space, features represent the extracted class association rules. Corresponding to each data point in the input space, we construct a new rule-based feature vector.

If a rule is valid for a data point (i.e., the rule is covered by the data point), the value of the feature representing that rule in the feature vector is set to the rule?s weight.

Otherwise, the value is set to 0. Feature vectors constructed in the new rule space describe the distribution validity of high discriminative class association rules over the training set.

Building Classifier Model: There exist many classification algorithms for supervised learning. In our framework, we take advantage of the SVM power in the context of computational complexity to use as many rules as possible in order to improve the accuracy of the classifier model. Our proposed system integrates in the process of building a classifier model more correlation knowledge extracted from the training set and represented in the form of rule-based feature vectors.

We argue that rule-based feature vectors provide to the SVM learner algorithm more complete and interesting information about every single pattern from the training set. The task of building a classifier model in our approach is to apply the SVM algorithm to build a classifier model. The input to the algorithm is a set of rule-based feature vectors. Feature vectors are used by the algorithm to train a classifier model. We use the cross validation technique to evaluate the accuracy of our classifier model.

C. Incremental Learning of New Attacks  As mentioned earlier, in most intrusion detection systems, data arrive in a timely manner and a realtime decision is required to indicate whether the data belongs to an attack class  or not. The prediction component of the proposed framework uses an incremental learning approach to adapt itself to the new patterns observed in the live data. The realtime characteristic of a system may influence the performance of an IDS and may raise the need for rebuilding the classifier because of two reasons: 1) some of the patterns based on which the classifier was built earlier become obsolete. In other words, these patterns are not observed any more in the new received data. The obsolete patterns are probably related to some old intrusion attacks that do not target the system anymore, 2) the system observes new patterns in the received data about which it has no prior knowledge because they were not available while the classifier was trained. The new patterns will negatively affect the performance of the classifier. They may reveal specification of new potential attacks for which the system needs to be re-trained. The incremental learning component of the system has been demonstrated in the activity diagram shown in Figure 1. The proposed algorithm monitors the performance of the classifier constantly. Whenever the performance of the classifier model drops under a certain level of accuracy specified by an expert, the current classifier will be rebuilt.

Monitoring the Performance of the Classifier: The component that constantly monitors the performance of the classifier is executed in the back-end of the system and works as follows. For each data instance from the test set (new received data), the classifier model predicts its class.

Then the algorithm, running in the back-end, measures the quality of the class to which the new instance has been assigned. If the quality improves or the quality drop is above the minimum quality threshold, it is counted as a correctly classified instance and will be neglected in the future training, i.e., it will not be added to the training set. If there is a considerable drop in the quality of that class, the prediction is considered as a misclassification prediction by the system.

Then the algorithm determines if the quality of any existing class improves by adding this misclassified data instance.

If there exists such a class then the misclassified instance will be added to the training set as a member of that class though it has been misclassified by the current classifier. At this stage, the support and confidence of the existing rules are re-calculated as well according to the label assigned to the test instance. If the test instance does not improve the quality of any class, it is added to the set of unlabelled instances for future learning. When the number of instances in the unlabelled set increases, it shows the classifier?s lack of knowledge about the incoming data. Accordingly, the classifier overall accuracy drops and therefore it is time to rebuild the classifier model. The accuracy of a classifier at any point of time is the ratio of correctly classified instances observed by the classifier. In order to measure the quality of a class the within-class variation is considered. Suppose that rk represents the center of the class Ck and is defined as rk = 1nk  ? x?Ck x , where nk is the number of points in  Ck. The measure of within-class variation is simply the sum       Initial training set After receiving 10 new instances After updating the training set  . ....

. . ...

c1  c2 c3  ....

. . ...

c1  c2 c3  ....

.x xx  x  x  c1 c1  c1 c1  c2  c2  c3  c3 c3 c3  ....

. . ...

c1  c2 c3  ....

.x xx  x  x  c4  c5  ..

x  Initial data points New received data points (existing pattern) New received data points (new pattern)  c1 : Satan c2 : Smurf c3 : Guess-passwd c4 : Spy c5 : Rootkit  Initial intrusion types  Suspicious intrusion types to be labeled by experts  Fig. 2. Unsupervised learning of new attacks  of squares of distances from each point to the center of the class it belongs to: wc(Ck) =  ? x(i)?Ck d(x, rk)  2 , where d(x, rk) is the distance between a data point within Ck and its center rk.

Updating the Classifier: During the updating stage, the new patterns need to be extracted and integrated to the knowledge carried by the classifier. In order to learn the new patterns, first we have to identify the new classes of intrusions among the unlabelled data. Unlabelled data represent classes of attacks for which we do not have any prior knowledge. Therefore an unsupervised learning algorithm is required to detect the classes of new attacks. We utilize the DBScan clustering algorithm at this stage to identify the new attacks. The new classes are then labelled by a domain expert and will be used in the process of updating the classifier. In other words, all instances of unlabelled set along with their assigned classes will be added to our initial training set. Figure 2 shows an example of unsupervised learning of new attack classes. As can be seen, out of ten new received instances, five of them have been wrongly classified by our classifier and since they do not improve the quality of any existing class, they are assigned to the unlabelled set. An unsupervised learning algorithm like DBScan can identify two groups for these five data instances.

The new classes are suspicious intrusion types.

Once the new classes are identified and labelled, the class association rule mining algorithm is executed to extract the strong class association rules corresponding to each new class.

The extracted rules are then added to the rule space as new features that represent the new attacks. Further, if the support or confidence of any rule has dropped under the minimum support and confidence thresholds, that rule will be eliminated from the rule space because it has become obsolete and it is not a significant rule to distinguish between different classes any more. Now that the set of features in the rule space has been refined, and new training instances are available, the set of rule-based feature vector is updated as well. Finally, given the new set of feature vectors, the SVM algorithm is executed and a new classifier is built. The new classifier inherits the useful portion of knowledge carried by the previous classifier. In addition, it has been enriched by the recently learned patterns  about the new classes of attacks.



IV. EXPERIMENTAL ANALYSIS  To evaluate the performance of the proposed system, we conducted experiments using the publicly available KDD Cup?99 intrusion detection dataset [13]. This dataset has different distributions of attacks for the test dataset and the training dataset. Only 22 out of 39 attack subclasses in the test data are present in the training data. The different distributions between the training dataset and the test dataset, and the new types of attacks in the test dataset reflect the dynamic change of the nature of attacks common in real-life systems. This dataset has 41 attributes and 96675 data instances. We used around 65% of this data (64675) as the initial training set and then we simulated the realtime scenario by feeding the rest of data instances (32000) one by one in a time sequence to the system.

To build the initial classifier model, we first preprocessed the data. 11 features were eliminated as they were identified not to be significant discriminators over the training dataset.

Then the numeric attributes were discretized. Once the training set was preprocessed, the rule mining algorithm was utilized to extract all the class association rules. The minimum support and confidence values were set to 20% and 80%, respectively.

After extracting the rules, the training set was transformed to the rule space and SVM algorithm was applied to generate the initial classifier model. Figure 3 shows the accuracy trends of the initial classifier as well as other classifier models that were built to adapt the classifier to the new patterns observed in the received data. In this chart, the x-axis shows the time sequence in which the test data was fed to the system one by one and the y-axis demonstrates the accuracy of the classifiers.

The minimum acceptable level of accuracy was set to 50%.

That is whenever the overall accuracy of the classifier, since the last update, falls under 50%, it is time to rebuild the classifier. It is worth mentioning that first, we selected the test instances from classes that also existed in the training set and then slightly moved to the test instances that did not appear in the training set. As can be seen in Figures 3, the initial classifier (Model0) shows high accuracy at the beginning. Once we started the prediction experiment with the     test instances, the system automatically monitors the accuracy.

The Model0 accuracy was acceptable until after the 2226th  instance was fed to the system. At this time, the accuracy dropped under the threshold (50%). Therefore, the existing model was automatically updated and Model1 was created.

In the next iteration the accuracy of Model1 was above the threshold until the 9678th instance was received and thus Model2 was created at this point. The accuracy of this model was acceptable until the 20992th instance was observed and accordingly Model3 was created. The accuracy of the Model3 was acceptable until the system received the last test instance from the test set and thus no more model was created.

0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9                              Model0 Model1 Model2 Model3      Fig. 3. The accuracy trend of the adaptive classifier model  Figure 3 also compares the accuracy trends of the current classifier with all the previous classifiers. That is, it shows how the previous classifiers would perform if the system did not update the classifier model to adapt it to the new patterns observed in the new received data. As can be seen, the proposed framework monitors the accuracy of the classifier and attempts to keep the accuracy level always above the minimum threshold. For instance, if the initial model (model0) was to be used to predict the class label of all the test instances, the overall accuracy of the system would be around 25%.

However, now the overall accuracy of the system by adapting different classifiers at different time points as required, keep the accuracy level around 75%. this result shows the effec- tiveness of the proposed framework for a realtime intrusion detection system.



V. RELATED WORK  Many misuse-detection techniques frequently utilize some form of rule-based analysis. Rules describe the correlation between attribute conditions and class labels. When applied to misuse detection, the rules become descriptive scenarios for network attacks. The use of comprehensive rules is critical in the application of expert systems for intrusion detection.

Han et al. in [8] proposed to use a decision tree to combine multiple host-based detection models for achieving a lower false positive rate and higher detection accuracy. Luo et al.

investigated the fuzzy rule-based anomaly detection using real world data and simulated data set [9]. However, the models are essentially static and lack adaptability, leading the performance  to degrade when new attacks target the system. In [10], the authors designed an approach to fusing multiple classifiers for intrusion detection, and each classifier ensemble was trained with distinct feature patterns. In [11], authors proposed a statistical framework for characterizing the general behaviors of anomaly detectors. The framework served as a theoretical basis to develop an adaptive middleware to broaden anomaly detection coverage, suppress false alerts, in an adaptive man- ner. Yu et al. in [12] proposed to improve the detection model dynamically during deployment. In their model, the detection performance is fed back into the detection model, and the model is adaptively tuned.



VI. CONCLUSION In this paper, we presented a general framework for develop-  ing an adaptive intrusion detection system for Cyberinfrastruc- tures. The classifier component of the framework uses a rule- based approach in that the discovered patterns are represented in a simple ?if-then? form. This representation is not only more understandable for human experts but also facilitates the user interference in the learning process. That is, a domain expert can manipulate (delete/update/add) the extracted class association rules that are used for building the classifier model.

Further, the system constantly monitors the performance of the classifier and updates the classifier according to the new patterns observed in the received data. This grantees that the proposed framework adapts its classifier to detect new types of attacks for which the classifier was not trained before.

