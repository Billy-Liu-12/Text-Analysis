Mining Significant Associations in Large Scale Text Corpora

Abstract  Mining large-scale text corpora is an essential srep in ex- tracting the key themes in a corpus. We motivate a quanti- tative measure for  significant associations thmugh the dis- rnburions of pairs and triplets of co-occurring words. We consider the algorithmic pmblem of eflciently enumerat- ing such significant associations and present pruning algo- rithms f o r  thesepmblems, with theoretical a s  well as empir- ical analyses. Our algorithmsmake use of two novel mining methods: ( I )  matrix mining, and (2 )  shortened documents.

Wepresentevidence fmmadiversesetofdocumentsthatour measure does in fact elicit interesting co-occurrences.

1 Overview  In this paper we ( I )  motivate and formulate a fundamen- tal problem in text mining; (2)  use empirical results on the statistical distributions of term associations to derive con- crete measures of ??interesting associations?; (3) develop fast algorithmsfor mining such text associations using new prun- ing methods; (4) analyze these algorithms, invoking the dis- tributions we observe empirically; and ( 5 )  study the perfor- mance of these algorithms experimentally.

Motivation: A major goal of text analysis is to extract, group, and organize the concepts that recur in the corpus.

Mining significant associations from the corpus is a key step in this process. In the automatic classification of text docu- ments each document is a vector in a high-dimensional?fea- ture space?, with each axis (feature) representing a term in the lexicon. Which terms from the lexicon should he used as features in such classifiers? This ?feature selection? problem is the focus of substantial research. The use of significant as- sociations as features can improve the quality of automatic text classification [ I Q  Clustering significant terms and as- sociations (as opposed to allterms) is shown [8, 141 to yield clusters that are purer in the concepts they yield.

*?Illhis work was conductedwhile the aulhor was visiting Verity Inc.

University of Toronto tsap @cs.toronto.edu  Text as a domain: Large-scale text corpora are intrinsically different from structured databases. First, it is known [15, 221 that terms in text have skewed distributions How can we exploit these distributional phenomena? Second, as shown by our experiments, co-occurrences of terms themselves have interesting distributions; how can one exploit these to mine the associations quickly? Third, many statistically sig- nificant text associations are intrinsically uninteresting, he- cause they mirror well-known syntactic rules (e.g., the fre- quent co-occurrence of the words ?OF? and ?the?); one of our contributions is to distill relatively significant associations.

2 Background and contributions  2.1 Related previous work  Database mining: Mining association rules in databases was studied by Agrawal et al. [ I ,  21. These papers intro- duced the supportlconfidenceframework as well as the apr i - ori pruning paradigm that is the basis of many subsequent mining algorithms. Since then it has been applied to a num- ber of different settings, such as mining of sequential pat- terns and events. Brin, Motwani and Silverstein [61 gen- eralize the a priori framework by establishing and exploit- ing closure properties for the x2 statistic. We show in Sec- tion 3.2 that the ,yz test d w s  not work well for our domain.

Brin et al. [5] extend the basic association paradigm in two ways: they provide performance improvements based on a new method of enumerating large itemsets and additionally propose the notion of implication rules as an alternative to association rules, introducing the notion of convicrion. Ba- yardo et al. [4] and Webb [20] propose branch and bound algorithms for searching the space of possible associations.

Their algorithms apply pruning rules that do not rely solely on support (as in the case of a priori algorithms). Cohen et al. [7] propose an algorithm for fast mining of associations with high confidence without support pruning. In the case of text data, their algorithm favors pairs of low support. Fur- thermore, it is not clear how to extend it to associations of more than two terms.

a-16~5-1154-4/02 $ii.ao Q 2002 IEEE 402  http://verity.com mailto:cs.toronto.edu   Extending database mining: Ahonen et al. [3] build on the paradigm of episode mining (see [ 161 and references therein) to define a text sequence mining problem. Where we de- velop a new measure that directly mines semantically useful associations, their approach is to first use a ?generic? episode mining algorithm (from [161) then post-filter to  eliminate uninteresting associations. They do not report any perfor- mancdscaling figures (their reponed experiments are on 14 documents), which is an area we emphasize. Their work is insoired bv the similar work of Lent et al. 1131. Feldman er  techniques to achieve scalable mining. To this end we propose two new techniques: (i) matrix mining (Sec- tion 4.2) and (ii) shortened documents (Section 4.3).

3. We analyze the pruning resulting from these tech- niques. A novel aspect of this analysis: to our knowl- edge, it is the first time that the Zipfian distribution of terms and pairs is used in the analysis of mining algo- rithms. We combine these pruning techniques into two algorithms (Section 4 and Theorem I). . .

al. describe the KDT system [IO, 121 and Document Ex- plorer 11 11. meir approach, however, requires prior label.

ine (throueh some combination of manual and automated  4. We give results of experiments on three test corpora for the pruning achieved in practice. These results suggest  L .  - that the pruning is more efficient than OUI (conserva- tive) analytical prediction and that our methods should scale well to larger corpora (Section 4.4).

methods) using keywords from a given ontology, and can- not directly he used on general text. DuMouchel and Predi- gibon (91 propose a statistically motivated metric, and ap- ply empirical Bayes methodology for mining associations in text. Their work has similar motivation to ours. The authors do not report on efficiency and scalability issues.

Statistical natural language processing: The problem of finding associations between words (often referred to as cul- locations) has been studied extensively in the field of Statis- tical Natural Language Precessing (SNLP) [17]. We briefly review some of this literature here, hut expand in Section 3.1 on why these measures fail to address our needs.

Frequency is often used as a measure of interestingness, together with a part-of-speech filter to discard syntactic col- locations like ?of the?. Another standard practice is to ap- ply some statistical test that, given a pair of words, evalu- ates the null hypothesis that this pair is generated by picking two words independently at random. The interestingness of the pair is measured by the deviation from the null hypoth- esis. The t test and the x2 test are statistical tests frequently used in SNLP. There is a qualitative difference between col- locations and the associations that we are interested in. Col- locations include patterns of words that tend to appear to- gether (e.g. phrasal verbs - ?make up?, or common expres- sions like ?strong tea?), while we are mostly interested in as- sociations that convey some latent concept (e.g. ?chapters indigo?- this pertains to the recent acquisition of Chapters, then Canada?s largest bookstore, by the Indigo corporation).

2.2 Main contributions and guided tour  1. We develop a notion of semantic as opposed to syntac- tic text associations, together with a statistical measure that mines such associations (Section 3.3). We point ont that simple statistical frequency measures such as the x 2  test and mutual information (as well as variants) will not suffice (Section 3.2).

2. Our measure for associations lacks the monotonicity and closure properties exploited by prior work in as- sociation mining. We therefore require novel pruning  We report results on three test corpora taken from news agencies: the CBC corpus, the CNh? corpus and the Renters corpus. More statistics on the corpora are given in Sec- tion 4.4.

3 Statistical basis for associations  In this section we develop our measure for significant as- sociations. We begin (Section 3.1) by discussing qualita- tively the desiderata for significant text associations. Next, we give a detailed study of pair Dccurrences in our test cor- pora (Section 3.2). Finally, we bring these ideas together in Section 3.3 to present our new measure for interesting asso- ciations.

3.1 Desiderata for significant text associations  We first experimented with naive support measures such as document pair frequency, sentence pair frequency and the product of the individual sentence term frequencies. We omit the detailed results here due tospace constraints. As ex- pected, the highest ranking associations are mostly syntactic ones, such as (ofshe) and (in,the), conveying little informa- tion about the dominant concepts. Furthermore, it is clear that the document level is t ~ )  granular to mine useful asso- ciations - two terms could co-occur in many documents for template (rather than semantic) reasons; for example, asso- ciations such as (business, weather), and (corporate, enter- tainment) in the CBC corpus.

We also experimented with well known measures from SNLP such as the x2 test and mutual information as well as the cunvicrion measure, a variation of the well known con- fidence measure defined in 161. We modified the measure slightly so that it is symmetric. Table 1 shows the top asso- ciations for the CNN corpus for these measures. The num- ber next to each pair indicates the number of sentences in      XJ afghani libyan :z antillian escudo :z algerian angolan :z allowances child-care :I alanis monssette :I arterial vascular :z americanas marisa :I balboa rouble :z bolivian lesotho :2 birr nicaraguana :2  conviction afghalu libyan :z antillian escudo :2 algerian angalan :z allowanceschild-care :I alanis morissae :1 arterial VaSEUlar :z amerieanas marisa : I balbaa rouble :z bolivian lesotho :2 birr niearaguan :2  mUtual information allowances child-care :I alanis morissette :I americanas marisa :I charming long-stem : I cane stalks :I hk$l16.50hk$53.50 :I ill.,-based pyrex :I bastmit grmn :I barbed inventive :I I6Okpnr teliar :I  call market :I1061 lalest news :I 1740 a of :23362  Table 1. Top associations from the CNN corpus under different measures.

which this pairappears. Althoughthese measures avoid syn- tactic associations, they emphasize on pairs of words with very low sentence frequency. If two words t and q appear only a few times but they always appear in the same sen- tence, then the pair { t ,  q }  scores highly for all of these mea- sures, since it deviates significantly from the independence assumption. This is especially true for the mutual informa- tion measure 1171. We also experimented with a weighted version of the mutual information measure [17], where we weight the mutual information of a pair by the sentence fre- quency of the pair. However, in this case the weight of the sentence pair frequency dominates the measure. As aresult, the highly ranked associations are syntactic ones.

It appears that any statistical test that compares against the independence hypothesis (such as the xz test, the t test, or mutual information) falls prey of the same problem: it fa- vors associations of low support. One might try to address this problem by applying a pruning step before computing the various measures: eliminate all pairs that have sentence pair frequency below a predefined threshold. However, this approach just masks the problem. The support threshold di- rectly determines the pairs that will be ranked higher.

3.2 Statistics of term and pair occurrences  We made three measurements for each of ow corpora: the distributions of corpus term frequencies (the fraction of all words in the corpus that are term t ) .  sentence term frequen- cies (fraction of sentences containing term t )  and document termfrequencies (fraction of documents containing term t ) .

We also computed the distribution of the sentence pair  fre- quencies (fraction of sentences that contain a pair of terms).

We observed that the Zipfian distribution essentially holds, not only for corpus frequencies but also for document and Sentence frequencies, as well as for sentence pair frequen- cies. Figure 1 presents the sentence term frequencies and the sentence pair frequencies for the CNN corpus. We use these observations for the analysis of the pruning algorithms in Section 4. The plots for the other test corpora are essen- tially the same as those for CNN.

(a) Sentence Term Frequencies (b) Sentence Pair Frequencies  Figure 1. Statistics for the CNN corpus  3 3  Thenewmeasure  Intuitively we seek pairs of terms that CO-occur frequently in sentences, while eliminating pairs resulting from very fre- quent terms. This bears a strong analogy to the concept of weighting term frequencies by inverse document frequency (idf) in text indexing.

Notation: Given a corpus of documents C ,  let Nd denote the number of documents in C, let N ,  denote the number of sentences in G and let Nr denote the the number of distinct terms in C. For a set of terms T = I t l ,  t 2 ,  . . . , t k } .  for L 2 1,1etnd(t l , t2 . .  .,tr)denotethenumberofdocumentsinC thatcontainalltermsinTandletn,(tl,tz, . . . ,  t k )  denote the number of sentences in C that contain all terms in T. We define the document frequency of T as d f ( t l , t 2 ,  . . . , ta )  = n d ( t l , t 2 . .  . , t k ) / N d .  and the sentence frequency of the set T as sf(tl,tz,. . . , t k )  = n,(tl,t2,. . . , t k ) / N 8 .  If k = 2, we will sometimes use dpf and spf to denote the doc- ument and sentence pair frequencies. For a single term t , we define the inverse document frequency o f t ,  idf(t) = fOg(.vd/nd(t)) and theinverse sentence frequency isf(t) = log(N,/n,(t)). In typical applications the base of the loga- rithm is immaterial since it is the relative values of the idf that matter. The particular formula for idf owes its intuitive justification to the underlyingzipf distributionon terms; the reader is referred to 117,211 fordetails.

Based ou the preceding observations, the following idea     I  the to in the and the a the call market latest news a to a of  deutschetelekom click hen honnkone of the  dpf x id/ x idf danmarkespaal espaol svenska danmark svenska espaol Wvelcemer danmark travelcenter svenska travelcenter espaol norge danmark norge norge svenska norge travelcenter  mph trains allegheny lukens allegheny teledyne newell rubbermaid hummer winblad hauspie lernout kthlehem lukens globalstar loral  Table 2. Top associations for variants of our measure for the CNN corpus.

suggests itself: weight the frequency of a pair by the (prod- uct of the) idf?s of the constituent terms. The generalization beyond pairs to k-tuples is obvious. We state below the for- mal definition of our new measure for arhiuary k.

Definition 1 For r e m  t l ,  t2, . . . , t k ,  the measure for rhe associarion{tl,tz, . . . ,  t r }  is  Variants of the measure: We experimented with several variants of our measure and settled on using idf rather than isf, and spf rather than dpf .  Table 2 gives a brief summary from the CNN corpus to give the reader a qualitative idea.

Replacing idf with isf introduces more syntactical associa- tions. This is due to the fact that the sentence frequency of words like ?the? and ?of? is lower than their document fre- quency, so the impact of the isf as a dampening factor is re- duced. This allows the sentence frequency to take over. A similar phenomenon occurs when we replace spf with dpf .

The impact of dpf is  too strong, causing uninteresting asso- ciations to appear. We also experimented with l og ( sp f ) ,  an idea that we plan to investigate further in the future.

Figure 2 shows two plots of our new measure. The first is a scatter plot of our measure (which weights the spf ?S by idf?s) versus the underlying spf values?. The line y = z is shown for reference. We also indicate the horizontal line at threshold 0.002 for our measure; points below this line are the ones that ?succeed?. Several intuitive phenomena are captured here. (1) Many frequent sentence pairs are attenu- ated (moved upwards in the plot) under our measure, so they fail to exceed the threshold line. (2) The pairs that do suc- ceed are ?middling? under the raw pair frequency. The plot on the right shows the distributionof our measure, in a log- log plot, suggesting that it in itself is roughly Zipfian; this requires further investigation. If this is indeed the case then we can apply the theoretical analysis of Section 4.1 to the case of higher order associations.

?The axes are scaled and labeled negative logarithmically, so that the largest values are IO the bollom left and the smallest to the top and right.

Non-monotonicity: A major obstacle in our new measure: weighting by idf can increase the weight of a pair with low sentence pair frequency. Thus, our new measure does not enjoy the monoronicirypmperry of the support measure ex- ploited by the apriori algorithms. Let I he some measure of interestingness that assigns a value I (T)  to every possible set of of terms T. We say that I is monotone if the follow- ing holds: if T? T, then I(T?) > I (T) .  This property al- lows for pruning, since if for some T? T, I(T?) 5 8, then I (T)  5 8. That is, all interesting sets must he the union of interesting subsets. Our measure does not enjoy this prop- erty. For some pair of terms { t l ,  t 2 } ,  it may be the case that mx(t l , lz)  > 8, whileml(tl)  5 8,orml( t2)  5 8 .

Formal pmblem statement: Given a corpus and a thresh- old 8, find (fork = 2,3,  . . .) all k-tuples for which our mea- sure exceeds 8.

4 Fast extraction of associations  We now present two novel techniques for efficiently min- ing associations deemed significant by our measure: mar& mining and shortened documenrs. Following this, we an- alyze the efficiencies yielded by these techniques and give experiments corroborating the analysis. We first describe how to find all pairs of terms {z, y )  such that the measure m(z,y) = s p f ( z , y ) i d f ( z ) i d f ( y )  exceeds a prescribed threshold 8. We also show how our techniques generalize for arbitrary k-tuples.

4.1 Pruning  Although our measure is not monotone we can still ex- plore some monotonicity properties to apply pruning. We observe that  m k , ~ )  = v f ( z , y ) : d f ( z ) i d f ( y )  5 s f ( z ) i d f ( z ) W y ) .

(1)  Let q(z) = s f ( z ) i d f ( z )  and f ( y )  = i d f ( y ) .  The value off ( y )  cannot exceed log N d .  Therefore, m(x,  y )  5     q(x)f(y) 5 q(x) logNd.  Thus, we can safely eliminate any term x for which q(x) 5 l?/ log . v d .  We observe experimen- tally that this results in eliminating a large number of terms thatappearinjustafew sentences. Wewillrefer tothisprun- ing step as low end pruning since it eliminates terms of low frequency.

Equation 1 implies that if m ( z ,  y) > 8 ,  then q(x)f(y) > 8. Therefore, we can safely eliminate all terms y such that f (y) 5 l?/ max, q(z ) .  We refer to this pruning step as high endpruning since it eliminates terms of high frequency. Al- though this step eliminates only a small number of terms, it eliminates a large portion of the text.

We now invoke additional information from our studies of sentence term frequency distributionsin Section 3.2 to es- timate the number of terms that survive low end pruning.

Theomm 1 Low end pruning under a power law disrri- burian far  r e m  frequencies eliminares all bur O(log2 N d ) rems.

Pruuf: The sf values are distributed as a power law: the ith-largest frequency is proportional to l/P. If ti denotes the ith most frequent term, sf(ti) = A/?? for a constant A .  Since no idf value exceeds log N d .  we have q(i;) z sf(t;)idf(ti) 5 A l o g N d / i e .  If q ( t i )  > l?/log.Vd, then l? < Alog? Nd / id .  Therefore, i < (A/l?)?Ia 1og2/m Nd.

Let c = (A/ l?) ? /Y  and f i  = Z/a. If B = c l o g  Nd, then only O( E) terms can generate candidate pairs. Since a t 1,  Pruning extends naturally to k-tuples. A k-tuple can he thought as a pair consisting of a single term and a (k - 1)- tuple. Sincemk(t1, .  . . , t k )  5 mk-,(tl, .. . , t k - l ) i d f ( & ) .

we can safely prune all (k - 1)-tuples such that mk-l(t1,. . . , tk-1) 5 l? f log N d .  Proceeding recur- sively we can compute the pruning threshold for i-tuples and apply pruning in a bottom up fashion (terms, pairs. and so on). We define B; = l?/ lo&-? Nd to be the threshold for a-tuples for all 1 5 i 5 k.

o(B) = o ( l O g z  Nd).  m  4.2 Matrixmining  Given the terms that survive pruning we now want to minimize the number of pairs for which we compute the spf(x,y) value. Let N{ denote the number of (distinct) terms that survive pruning. The key observation is best visu- alized in terms of the matrix depicted in Figure 3(left). It has N; rows and N: columns, one for each term. The columns of the matrix are arranged left-to-right in non-increasing order of the values q(x) and the rows bottom-upin non-increasing order of the values f (I). Let pi denote the ath largest value of q(z) and denote the j th largest value of f(x). Imagine that matrix cell (i, j )  is filled with the product qih (we do not actually compute all of these values).

Figure 3. Matrix mining  The next crucial observation: by Equation 1 the pair ( i ,  j ) is eliminated from further consideration if the entry in cell (i, j) is less than B. This elimination can he done especially efficiently by noting a particular structure in the matrix: en- tries are non-increasing along each row and up each column.

This means that once we have found an entry that is be- low the threshold l?, we can immediately eliminate all entries above and toitsright,and notbothercomputingthoseentries ( Figure 3). We have such a %upper-right? rectangle in each column, giving rise to a frontier (the curved line in the left     MATRIX-WAM(B)  ( I )  Collect Term Statistics (2) T c Apply pruning: n e IT1 (3) X t SORT by sj x idj in decreasingorder (4) Y t sort T by idJ in decreasing order (5)Fory=Y[O]toY[n] (6) Forx=X[O]toX[n] (7) (8) (9) Compute ~PJ(=,YI (10) (11) (12) (I3)reIurnA  if x has not been considered already i f s j (x )  x idJ(x) x idf(y) > B  if spj(z,y) x idj(z)  x idf(y) > B Add {x, y} to answer set A  else discard all lerrns right of x: brea  Figure 4. The MATRIX-WAM algorithm  figure) between the eliminated pairs and those remaining in contention. For cells remaining in contention, we proceed to the task of computing their spf values, computing m(z, y).

and comparing with 8. Applying Theorem 1 we observe that there are at most O(log4Nd) candidate pairs. In practice our algorithm computes the spf values for only a fraction of the (4) candidate pairs. Figure 3 (right) illustrates the frontier line for the CNN corpus.

We now introduce the first Word Associations Mining (WAM) algorithm. The MATRIX-WAM algorithm shown in Figure 4.2 implements matrix mining. The first step makes apass overthecorpusand coilectsterm statistics. The pruning step performs both high and low end pruning, as de- scribed in Section 4.1. For each term we store an occur- rence list keeping all sentences the term appears in. For a pair {z, y) we can compute the spf (z, y) by going through the occurrence lists of the two terms. Lines (8)-(12) check the column frontier and determine the pairs to be stored.

For higher order associations, the algorithm performs multiple matrix mining passes. In the ith pass, one axis of the matrix holds the idf values as before, and the other axis the mi-l values of the (i - 1)-tuples that survived the pre- vious pass. We use threshold 8; for the ith pass  43 Shortened documents  While matrix mining reduces the computation signifi- cantly, there are still many pairs for which we compute the spf value. Furthermore, for most of these pairs the spf value is actually zero, so we end up examining many more pairs than the ones that actually appear in the corpus. We invoke a different approach, similar to the AprioriTID algorithm de- scribed by Agrawal and Srikant [2]. Let H1 denote the set of terms that survive the pruning steps described in Section 4.1 - we call these the interesting terms. Given HI we make a second pass over the corpus, keeping a counter for each pair of interesting terms that appear together in a sentence.

Collect rem SIaliStiCS.

Hi t Rune Terms: CI + Corpus Fori = 2 10 k  For each sentence J in Ci-1 I .  = ( i -  l ) -Nple~ in~tha lare inH;_~ SI = i-tuples generated by joining I ,  with iuelf Add luples in s' to H i if 3' # 0 Add 3' to C;  H ;  t apply pruning on H i .

Figure 5. The SHORT-WAM algorithm  That is, we replaced each document by a shortened docu- ment consisting only of the terms deemed interesting.

The shortened documents algorithm extends naturally for higher order associations (Figure 4.3). The algorithm per- forms multiple passes overthe data. The input to the ith pass is a corpus C;L~ that consists of sentences that are sets of (i- 1)-tuplesand ahash tahleHj-1 thatstoresall interesting ( i -  1)-tuples. An i-tuplel isinterestingifmi(t) > 8;. Dur- ing the ith pass the algorithmgenerates candidate i-tuples by joining interesting (i - 1)-tuples that appear together in a sentence. The join operation between (i - 1)-tuples is per- formed as in the case of the a priori algorithms [21. The can- didates are stored in a hash Hi and each sentence is replaced by the candidates it generates. At the end of the pass, the al- gorithm outputs a corpus Ci that consists of sentences that are collections of i-tuples. Furthermore, we apply low end pruning to the hash table H j  using threshold 8i. At the end of the pass Hi contains the interesting i-tuples.

Figure 6. Pruned Terms for CNN corpus  4.4 Empirical study of WAM algorithms  We ran ourtwo algorithmsonour threecorpora, applying both high and low end pruning. Figure 6 shows a plot of how the thresholds are applied. The terms that survive pruning correspond to the area between the two lines in the plot. The top line in the figure was determined by high end pruning,     I I CBC I CNN I Reuters c a r p  statistics  I distinctterms 16.5K 44.7K 37.1K 2 C O r p u S t e m s  471K 3.6M 1.3M 3 distinct sp's I.2M 5M 3.7M 4 corpussp's 3.9M 28.8M 16.3M  M o g  Statistics i 5 threshold I 0.002 0.001 I 0.015  I O  ~~~-~~ 8 I collected 2.798 3.006 2,699  Figure 7. Pruning for Reuters and CNN corDus I 9 I naive  computed rpf's 19.lM (80%) 47M (70%) 9 2M (57%) zero spJ 22.5M 60.6M 13.6M  SHORT-WAM Statistics (wlo high prUniag) I2 prunedcorpuslerms 45K(IO%) O.ZM(54a) O.IM(78) 13 gensp's 14 distinct ~ p 3  963K(77%) 3 6M(72%) 2.IM (57%) SHORT-WAM Statistics (with high pruning) I5 Dmnedcarnustermr 134K(29%) I 1.2M(32%) I O.IM(71)  5M (91s) 266M (921 I4'IM (86s) portance of high end pruning we implemented two versions of SHORT-WAM, one that applies high end pruning and one that does not. In the table, lines 12 and 15 show the percent- age of the comus terms that are pruned, with and without  while the bottom line was determined by low end pruning.

Table 3 shows the statistics for the two algorithms when  mining for pairs for all three corpora. In the table s p  stands for sentence pair and corpus sp's is the total number of sen- tence pairs in the corpus. We count the appearance of a term in a sentence only once. In all cases we selected the thresh- old so that around 3,000 associations are collected (line 8).

Pruning eliminates at least 58% of the terms and as much as 8490 for the Reuters corpus (line 6). Most terms are pruned from the low end of the distribution: high end pruning re- moves just 20 terms for the CBC corpus, 57 for the CNN corpus and none for the Reuters corpus (line 7). The above observations indicate that our theoretical estimates for prun- ing may be too conservative. To study how pruning varies with corpus size we performed the following experiment.

We sub-sampled the CNN and Reuters corpora, creating syn- theticcollections withsizes N d  = 2s ,~9 ,2 '0 ,21' ,212 ,213 .

For each run, we selected the threshold so that the percentage of pairs above the threshold (over all distinct pairs in the cor- pus) is approximately the same for all runs. The results are shown in Figure 7. The x axis is the log of the corpus size, while the y axis is the fraction of terms that were pruned.

Matrix mining improves the performance significantly: compared to the naive algorithm that computes the spf val- ues for all (7) pairs of the terms that survive pruning (line 9). the MATRIX-WAM algorithm computes only a fraction of these (maximum 80%, minimum 57%, line 10). Note however that most of the spf's are actually zero (line 1 I ) .

The SHORT-WAM algorithm considers only (a fraction of) pairs that actually appear in the corpus. To study the im-   - high end pruning. Obviously, high end pruning is respon- sible for most of the removed corpus. For the CNN corpus, the 57 terms removed due to high end pruning cause 28% of the corpus to be removed.

The decrease is even more impressive when we consider the pairs generated by SHORT-WAh4 (lines 13, 16). For the CNN corpus, the algorithm generates only S6% of all possi- ble corpus sp's (ratio of lines 4 and 16). This decrease be- comes more important when we mine higher order tuples, since the generated pairs will be given as input to the next iteration. Again high end pruning is responsible for most of the pruning of the corpus sp's. Finally, our algorithm gener- ates at most 72% of all possible disrinct sentence pairs (line 17). These pairs are stored in the hash table and they reside in main memory while performing the data pass: i t  is impor- tant to keep their number low. Note that ApriorinD gen- erates all painvise combinations of the terms that survived pruning (line 9).

threshold pruned terms  gensp ' s  2.4M(60%) 16.3M(56% 14.1M(86%) distinct 9p.s  898K (72%) 3.3M (67%) 2.IM (57%)  Table 4. MATRIX-WAM for triples  ./"We also implemented the algorithms for higher order tu- ples. Table 4 shows the statistics for MATRIX-WAM. for triples. Clearly we still obtain significant pruning. Further- more, the volume of sentence pairs generated is not large, keeping the computation in control.

We implemented SHORT-WAM for k-tuples, for arbitrar- ily large k. In Figure 8 we plot, as a function of the iteration numberi, thesizeofthecorpusC, (figureontheleft),as well     as the number of candidate tuples and the number of these tuples that survived each pruning phase (figure on the right).

The thresholdis set to0.07 and we mine 8,335 5-tuples. Al- though the sizes initially grow significantly, they fall fast at subsequent iterations. This is consistent with the observa- tions in [2]. - , , , , , , ,  -e -  Figure 8. Statistics for SHORT-WAM  4.5 Sample associations  At http://www.cs.toronto.edu/^.tsap~ex~inin~~ere is a full list of the associations. Table 5 shows a sample of as- sociations from all three corpora that attracted our interest.

.

deutsche telekom. hong kong. chevron fexaco, department justice, mci waddcom. aol warner, france telecom, greenspan tar. oats quaker, c h a p ten indigo, nestle putina. oil opec, books indigo, leaf maple, states united, germany west. arahia Saudi, gas oil, exxon juzy, capriati hingis  chaleau empress frontmac, indigo reisman schwmz, del monte sun-rype, cirque du soleil, bribery economics scandal, fuel spills lanker, escapes hijack yemen, SI hall mcguire, baker james secretary, chancellor lawson nigel. community ec eumpean. arahia o p e  Saudi. chief executive off- cer, child famering jesse, ncaa seth tournament. eurabond issuing priced.

falun gong self-immolation, doughnuts kreme krispy, laser lasik vision, leaf maple schneider  M p l s  Table 5. Sample associations  5 Conclusions  In this paper, we introduced a new measure of interest- ingness for mining word associations in text, and we pro- posed new algorithms for pruning and mining under this (non-monotone) measure. We provided theoretical and em- pirical analyses of the algorithms. The experimental evalua- tion demonstrates that our measure produces interesting as- sociations, and our algorithms perform well in practice. We are currently investigating applications of our pruning tech- niques to other non-monotone cases. Furthermore, we are interested in examining if the analysis in Section 4.1 can he applied IO other settings.

