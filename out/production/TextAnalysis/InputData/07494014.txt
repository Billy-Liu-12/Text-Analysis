COMPARISON AND IMPROVEMENT OF ASSOCIATION RULE MINING  ALGORITHM

Abstract: In recent years, the data mining technology has been  developed rapidly. New efficient algorithms are emerging.

Association data mining plays an important role in data mining, and the frequent item sets are the highest and the most costly. This paper is based on the association rules data mining technology. The advantages and disadvantages of Apriori algorithm and FP-growth algorithm are deeply analyzed in the association rules, and a new algorithm is proposed, finally, the performance of the algorithm is compared with the experimental results. lt provides a reference for the extension and improvement of the algorithm of association rule mining.

Keywords: Association rules; Apriori; FP-growth  1. Introduction  The concept of data mining is was frrstly raised at the ACM conference in the United States in 1995. Data mining is also called KDD, It refers to mining a large amount of data from the database to reveal the implicit, unknown, potentially useful information of the non-trivial process [1] [2]. It has a high practical value in the field of database research.

Association rule mining is an important branch of data mining research [3]. Association Rule Mining is used to find a link that is hidden in the interest of large data sets.

In this paper, the two algorithms are discussed.

2. Association rule theory  2.1 Basic definition   Assume 1= {il, 12, i3, ... In} is a collection of items, and data set D tasks related is a collection of transaction databases, and each trans action T is a collection of a number of items to make T~I. Each trans action has a representation, called TID. The transaction T contains a collection of items A if and only if A~T. An association rule is a form of A=>B logical implication, among them AcI, Bel and AnB=(I) [5]. The evaluation criteria of association rules are mainly support and confidence.

Support represents frequent degree that rules are used in the collection oftransactions. Support(A---->B)=P( A UB).

Confidence determines the predictability of the rules.

Confidence(A=>B)=P(BIA)=Support( A U B )/Support(A).

2.2 Association rule mining process  The mining process of association rules mainly contains two stages [6]: in the frrst phase, we must firstly find out all the items whose support is greater than minimum support from the transaction database, that is, to find all frequent item sets; the second stage is to generate the expected association rules from these frequent item sets.

3. Apriori algorithm  3.1 The basic idea  Apriori algorithm is a kind of typical breadth first search combined with direct counting algorithm, using the iterative strategy of layer by layer search. Firstly it will generate candidate set and filter the candidate set, then produce frequent sets of the layer [7].

3.2 Examples of Apriori algorithm  Take the supermarket retail industry as an example, mining the association rules through the analysis of the sales records. Known trans action database D, as shown in table I, the database has 7 transactions, with minimum support of2.

Table I Transaction database D TID Items Tl beef chicken milk T2 beefcheese T3 cheese boots T4 beef chicken cheese T5 beef chicken clothes  cheese milk T6 chicken clothes milk T7 chicken milk clothes  Step I: scan transactlOn database D, count the number of each item and get candidate I sets CI, with the minimum support for comparison, so that get the frequent I sets LI, such as Fig.I;  Cl LI Ttcm sets count (bcer) Ttcm sets count  f--+"i~"ii~~\~'Fi'\c,,"ll}'--------\--'i-----1 .. f-7'i~""h~C+r{C::Cll};-------1r--;------j {chccsc} {milk} {boots} {chccsc} {clOlhcs} {clOlhcs}  Fig.1 Step I  Step 2: generate candidate 2 sets C2 by frequent I sets LI, and then scan transaction database 0, and count the number of each item in C2 , with the minimum support for comparison, then get the frequent 2 sets L2, such as Fig.2;  C2 C2 Hem sets ltemsets counts (beef chickenJ [beef chieken) ,'bccf1l1ilkj {bccfmilk ', ,'bcer chccsc;' {bccfchccsc"  I--7I~""h~C7rl~C=,I~I=:I~~-!f~I; ';-, ----1 ~ {bccfclolhcs} {chickCll milk} .; chickcn chccsc;' {chickCll chccsc;' .; chickcn clothes;' {chickCll cloth es;' {milk chccsc} {milk chccsc} {milk clOlhcs} {milk clOlhcs} {chccsc clOlhcs} (chccsc clothes;'  L2 ltem sets counts [beef chieken) [beefmilkJ [beef cheeseJ l chieken milk) l chieken cheeseJ l chieken clothesJ [milk clothesJ  Fig.2 Step 2  Step 3: generate candidate 3 sets C3 by frequent 2 sets L2 and pruning, then scan transaction database 0, count the number of each item in C3, with the minimum support for comparison, then get the frequent 3 sets L3, such as Fig.3;   C3 C3 HC1l1 sets HC1l1 sets count  ~~~==~~~2~~~~~~~~~~!~~e~J-------\~~~~=:~~~~~~~~:~:~:~~~'J---r~----1 (ducken milk dothes J (ducken milk dothesJ  L3 Itcm sets count  {beef chicken milk: {bccf chickcn chccsc}  {chickcn milk clothes}  Fig.3 Step 3  Step 4: generate candidate 4 sets C4 by frequent 3 sets L3 and pruning, candidate 4 sets C4 is empty, such as Fig.4;  C4 ltemsets  [beef ducken nulk cheeseJ [beef ducken nulk dothesJ  Fig.4 Step 4  3.3 Advantages and disadvantages  3.3.1 Advantages  Apriori algorithm generates the candidate item sets by using Apriori, which greatly compress the candidate item sets and the size of the frequent item sets, and obtain good performance.

3.3.2 Disadvantages  I) The need for multiple scan database, system 1/0 load is quite large. The time of each scanning will be very long, resulting in a relatively low efficiency of Apriori algorithm.

2) It maybe produce huge candidate item sets. In the worst case, it produces the considerable proved to be non-frequent candidate item sets and the cost of counting is quite high, especially when the candidate set is relatively long, time and space is achallenge.

4. FP-growth algorithm  4.1 The basic idea  Aiming at the bottIeneck problem of Apriori algorithm, a method called FP-growth is proposed by Wang. In 2000[8].The proposed algorithm only scans the database for 2 times. FP-growth algorithm is a depth first search algorithm combined with direct counting, using recursive strategy of pattern growth, it need not generate candidate sets, instead, the trans action database is compressed into a tree structure that stores only the frequent items. All the frequent item sets are obtained by recursively mining FP-tree [4] [9].

4.2 Examples of FP-growth algorithm  This instance uses the transaction database D of table 1, with minimum support of2.

Step 1: firstly scan the database, each element is sorted descending by frequency, and delete the element whose frequency is less than the minimum support, and get frequent 1 set, as shown in table 2;  Table 2 F Item sets count {chicken} 5 {beef} 4 {milk} 4 {cheese} 4 {clothes} 3  Step 2: secondly scan database, for each transaction, resort according to the order of F 1, and delete elements that not belong to the frequent 1 sets of F I, such as table 3;  Table 3 Transaction database D TID Items Tl chicken beef milk T2 beefcheese T3 cheese T4 chicken beef cheese T5 chicken beef milk cheese clothes T6 chicken milk clothes T7 chicken milk c10thes  Step 3: msert each transactIOn record from the second step into FP-Tree, fmished FP-Tree is shown in Fig.5. The suffix mode is empty at first. The frequent item table after  Sorting is [pIP]' P is the first frequent item, and P is the remaining frequent items. Call insert tree (P] [pi, T) recursively.

Step 4: find frequent items from the FP-Tree, traverse the header in each ("cheese: 4" as an example): 1) Find all the "cheese" node from the FP-Tree, traverse  its ancestors, and get conditional mode, the suffix mode at this time is: (cheese: 1);  2) build the FP-tree on the basis ofthe conditional model, as shown in. Fig.6:  Fig.5 FP-Tree   chickt n  milk  Fig.6 Conditional FP-tree  Repeat the above operation, and mme frequent patterns, such as table 4:  Table 4 Frequent pattern table item frequent patterns c10thes {milk clothes:5} {clothes:5}  {chicken milk clothes:5} {chicken clothes:5} {chicken cheese:4} {cheese:4}  cheese {chicken beef cheese:4} {beef cheese:4}  milk {beefmilk:4} {milk:4} {chicken beefmilk:4} {chicken milk:4}  beef {chicken beef:4} {beef:4} chicken { chicken:5}  4.3 Advantages and disadvantages  4.3.1 Advantages  I) the tree - FP algorithm uses a compressed storage tree structure to access trans action records, only for the two scan, the scan time consumption is less than Apriori algorithm;  2) FP - tree algorithm does not generate candidate sets completely, and does not count the candidate item sets, so the time performance is better than the Apriori algorithm;  4.3.2 Disadvantages  I) FP-tree algorithm in mmmg frequent patterns inevitably need to create additional data structures, which will consume a lot oftime and space;  2) For the FP - tree algorithm, the performance of the algorithm will be affected if the condition tree is very rich (in the worst case).

3) The FP - tree algorithm can only be used to excavate the Boolean association rules of a single dimension.

5. Aigorithm improvement  Based on the -growth FP algorithm, the main idea is to inherit the advantages of FP-growth algorithm that need not    the generation of candidate sets, firstly count the total number of frequent I sets marked as n, then scan the database n times, each time a database subset of every item in Cl is obtained. Then, the -growth FP algorithm is used to constrain the frequent item sets in the database subsets, and obtain the frequent item sets containing the frequent item sets .In the end, merger these frequent items and get all the frequent item sets.

6. Conclusion  In order to verify the efficiency of the algorithm, Eclipse is used to implement the above algorithms. Test data contains a number of transactions 4627, and each transaction contains 217 attributes. Support was respectively 0.1, 0.2, 0.3, 0.4 and 0.5. The test results are shown in Fig.7.

Obviously, when the minimum support degree is smalI, the running speed of FP-growth algorithm is much faster than the Apriori algorithm. When the minimum support degree is large, the running speed of FP-growth algorithm is faster than the new algorithm. But the new algorithm runs faster than the FP-growth algorithm when minimum support is small to a certain degree.

The reasons for this phenomenon are as follows: the time cost of FP-growth algorithm depends on the construction and mining frequent item sets, but the time cost of the new algorithm depends on the generation of the database subsets, the trees bui Iding of the database subsets and the mining of frequent item sets. With the decrease of the minimum support degree, the total number of the total number of FP-growth algorithm increases, the memory overhead and time overhead ofthe FP-growth algorithm are becoming more and larger, so the digging speed is slower and slower. At this time, the time cost of the new algorithm in database subset generation is increasing, but in terms of building the database sub sets and constrained frequent item sets mining, due to the relatively small memory overhead, time overhead is relatively small, the overall mining speed is faster than FP-growth. The bigger database, the more obvious advantages.

"-  '" '" -  "'- I'P growth  I - Apriori  ~ \c~wn) growtll  ======= ==- Fig.7 Results   AcknowIedgments  The research work was supported by 2012 National Science and technology support program, 20 12BAH87F03.

