Using Category-Based Adherence to Cluster Market-Basket Data

Abstract  I n  this paper; we devise an eficienl algorithm for clus- tering market-baker data. Differentfrom those of the tra- d l ional  data, the features of market-basket dala are known to be of high dimensionalip, sparsip, and with massive out- liers. Withour explicitly considering the presence of the tm- onomy, most prior eforts on clustering market-basket data can be viewed as dealing with items in the leaf level of the taxonomy tree. Clustering transactions across dfferent lev- els of the taxonomy is of great importance for marketing strategies as well as for the result representation of the clus- tering techniques for market-basket data. I n  view of the features of market-basket data, we devise in this paper a novel measurement, called the category-based adherence, and utilize this measurement to perfom the clustering. The distance of an item to a given cluster is defined as the num- ber of links between this item and its nearest large node in the taxonomy wee where a large node is an item (ie., leajj or a category (i.e., internal) node whose occurrence count exceeds a given threshold. The category-based adherence of a transaction to a cluster is then defined as the aver- age distance of rhe items in this transaction to that clus- te,: With this category-based adherence measurement. we develop an eficient clustering algorithm, called algorithm CBA (standing for Caregoy-Based Adherence), for market- basket data wirh the objective fa minimize the category- based adherence. A validation model based on Information Gain (IC) is also devised fa  assess the qual ip  of clustering for marker-basket data. As validated by both real and syn- thetic datasets, it is shown by our experimental results, with the taxonomy information, algorithm CBA devised in this paper significantly outperform rheprior works in both the execution e@iency and the clustering qualify for market- basket data.

0-7695-1154-4102 $17.00 0 2002 IEEE 546  1 Introduction  Data clustering is an important technique for exploratory data analysis [16]. Explicitly, data clustering is a well- known capability studied in information retrieval [6], data mining [7], machine learning [9], and statistical pattern recognition [IS]. In essence, clustering is meant to divide a set of data items into some proper groups in such a way that items in the Same group are as similar to one another as possible. Most clustering techniques utilize a painvise similarity for measuring the distance of two data points.

Recently, there has been a growing emphasis on clustering very large datasets to discover useful patterns and/or corre- lations among attributes [3][4][10][23]. Note that clustering is an application dependent issue and certain applications may call for their own specific requirements.

Market-basket data (also called transaction data) has been well studied in mining association rules for discover- ing the set of frequently purchased items [5][13][18]. Dif- ferent from the traditional data, the features of market- basket data are known to be of high dimensionality, spar- sity, and with massive outliers. ROCK is an agglomer- ative hierarchical clustering algorithm by treating market- basket data as categorical data and using the links between the data points to cluster categorical data [I I]. The au- thors in [I71 proposed an EM-based algorithm by using the maximum likelihood estimation method for clustering transaction data OPOSSUM is a graph-partitioning ap- proach based on a similarity matrix to cluster transaction data 1191. The work in [20] proposed a K-Mean-based al- gorithm by using large items as the similarity measurement to divide the transactions into clusters such that transactions with similar large items are grouped into the same clusters.

OAK in [21] combined hierarchical and parlitional cluster- ing techniques. STC in [22] utilized a fixed small to large item ratio to perform the clustering of market-basket data.

In market-basket data, the taxonomy of items defines the generalization relationships for the concepts in different ab-    ...

Figure 1. An example taxonomy tree for books.

straction levels. Item taxonomy (i.e., is-a hierarchy) is well addressed with respect to its impact to mining association rules of market-basket database [ 13][18] and can be repre- sented as a tree, called taxonomy tree.

In view of the features of market-basket data, we de- vise in this paper a novel measurement, called the category based adherence, and utilize this measurement to perform the clustering. The distance of an item to a given cluster is defined as the number of links between this item and its nearest large node in the taxonomy tree. In the tax- onomy tree, the leaf nodes are called the item nodes and the internal nodes are called the category nodes. For the example shown in Figure I ,  "War and Peace" is an item node and "Novel" is a category node. As formally defined in Section 2,  a large item (cafegory) is basically an item (category) with its Occurrence count in transactions exceed- ing a given threshold. If an item (or category) is large, its colresponding node in the taxonomy tree is called a large node. For the example shown in Figure I ,  nodes marked gray are assumed to large nodes. The category-based ad- herence of a transaction to a cluster is then defined as the average distance ofthe items in this transaction to that clus- ter. With this category-based adherence measurement, we develop an efficient clustering algorithm, called algorithm CBA (standing for CalegoiyBased Adherence), for market- basket data. Explicitly, CBA employs the category-based adherence as the similarity measurement between transac- tions and clusters, and allocates each transaction to the clus- ter with the minimum category-based adherence. To the best of our knowledge, without explicitly considering the presence of the taxonomy, previous efforts on clustering market-basket data. [ I  1][17][19][20][21] unavoidably re- stricted themselves to deal with the items in the leaf level (also called item level) of the taxonomy tree. However, clustering transactions across different levels of the taxon- omy is of great importance for marketing strategies as well as for the result representation of the clustering techniques  for market-basket data Note that in the real market-basket data, there are volume oftransactions containing only single items, and many items are purchased in6equently. Hence, without considering the taxonomy tree, one may inappropri- ately treat a transaction (such as the one containing "parallel compiler" in Figure 1 )  as an outlier. However, as indicated in Figure 1, purchasing "parallel compiler" is in fact instru- mental for the category node "computer science" to become a large node. In contrast, by employing category-based ad- herence measurement for clustering, many transactions will not be mistakenly treated as outliers if we take categorical relationships of items in the taxonomy tree into considera- tion, thus leading to better marketing strategies. The details ofCBA will be described in Section 3. A validation model based on Information Gain ( IG)  is also devised in this pa- per for clustering market-basket data As validated by real and synthetic datasets, it is shown by our experimental re- sults, with the taxonomy information, algorithm CBA de- vised in this paper significantly outperforms the prior works [14][20] in both the execution efficiency and the clustering quality for market-basket data.

This paper is organized as follows. Preliminaries are given in Section 2. In Section 3, algorithm CBA is devised for clustering market-basket data. Experimental studies are conducted in Section 4. This paper concludes with Section 5.

2 Preliminary  TheproblandescnptionwillbepaentedinSection2.1.

In Section 2.2, we descriiea new wlidation~rnoddvEGval- idation model, for the assessment to the quality of different clustering algorithms.

2.1 Problem Description  In this paper, the market-basket data is represented by a set of transactions. A database of transactions is denoted by D = { t l , t z ,  ..., t , , } ,  where each transaction t ,  is rep- resented by a set of items {il, iz, ..., ih}. A example data- base for clustering market-basket data is described in Table 1 where there are twelve transactions, each of which has a transaction identification (abbreviated as TID) and a set of purchased items. For example, transaction ID 40 has items hand item z. A clustering U =< C,, CZ, _.., C, > is a par- tition of transactions into k clusters, where Cj is a cluster consisting of a set of transactions.

Items in the transactions can he generalized to multiple concept level of the taxonomy. An example taxonomy tree is shown in Figure 2. In the taxonomy tree, the leaf nodes are called the item nodes and the internal nodes are called the category nodes. The root node in the highest level is a virtual concept of the generalization of all categories. In     TID I O  TID 70 Items k , m , n  Items g x x  this taxonomy structure, item g is-a category B, cdtegoly B is-a category A, and item h is-a category B, etc. In this paper, we use the measurement of the occurrence count to determine which items or categories are major features of each cluster.

Definition 1: The count of an item ik in a cluster Cj, denoted by Count(ik, Cj), is defined as the number of transactions in cluster Cj that contain this item ik. An item ik in a cluster C; is called a large ifem if Count(&, C;) exceeds a predetermined threshold.

Definition 2: The count of a category ck in a cluster Cj, denoted by Count(ck,Cj), is defined as the number of transactions containing items under this category ck in cluster Cj .  A category Ck in a cluster Cj is called a large category if Count(ck, Cj) exceeds a predetermined threshold.

20 30 40 50 60  80 90 100 l l 0  I20 y g , k , n  m , n  y ,z  g , h , n  m,y y , z  h , z  &X,Y g , n  Note that one transaction may include more than one item from the same category, in which case the count contributed by this transaction to that category is still one.

In this paper, the minimum support percentage S, is a given parameter for determining the large nodes of the taxonomy tree in the cluster. For a cluster C,, the minimum support count S,(C,) is defined as follows.

Definition 3: count S,(C,) is defined as:  For cluster C,, the minimum support  SJC,) = s, * ICll.

where IC,l denotesthenumberoftransactions in cluster C,.

Consider the example database in Table 1 as an initial cluster CO with the corresponding taxonomy tree recording the counts ofthe itemdcategories shown in Figure 2. Then, Cmnt(g,Co) = 5 and Count(E,Co) = 7. With S, = 50%, we have S,(Co) = 6. In this example, all categories are large but all items are not.

2.2 Information G a i n  Validation Model  To evaluate the quality of clustering results, some exper- imental models were proposed [8][12]. In general, square  1 2 2 1  1 2  5  Figure 2. An illustrative taxonomy example w h o s e  transactions are shown in Table 2 (S,  = 0.5).

error criterion is widely employed in evaluating the effi- ciency of numerical data clustering algorithms [SI. Note that the nature feature of numeric data is quantitative (e.g., weight or length), whereas that of categorical data is quali- tative (e.g., color or gender) [16]. Thus, validation schemes using the concept of variance are thus not applicable to as- sessing the clustering result of categorical data. To rem- edy this problem, some real data with good classified labels, e.g., mushroom data, congressional votes data. soybean dis- ease [2] and Reuters news collection [I], were taken as the experimental data for categorical clustering algorithms [ I  1][20][21]. In view ofthe feature of market-basket data, we propose in this paper a validation model based on In- formation Gain (E) to assess the qualities of the clustering results.

The definitions required for deriving the information gain of a clustering result are given below.

Definition 4 The entropy of an attribute J,, in the database D is defined as:  where ID1 is the number of transactions in the database D and 1 J:l denotes the number of the transactions whose attribute J ,  is classified as the value J: in the database D.

Definition 5 The entropy of an attribute J ,  in a cluster Cj is defined as:  where IC,l is the number of transactions in cluster C,, and I J:,=, 1 denotes the number of the transactions whose     attribute J. is classified as the value J: in C,  Definition 6 Let a clustering U contain C,, C,, ..., C, clusters. Thus, the entropy of an attribute J ,  in the clustering U is defined as:  Definition 7: The information gain obtained by separating J. into the clusters of the clustering U is defined as:  Gain(J,,U) = I (J , ,D)  - E(J,,U),  Definition 8: The information gain of the clustering U is defined as:  IG(U) = Gain(J., U ) . L E I  where I is the data set of the total items purchased in the whole market-basket data records.

A complete numerical example on the use of these defi- nitions will be given in Section 3.3. For clustering market- basket data, the larger an IC value, the better the clus- tering quality is. In market-basket data, with the taxon- omy tree structure, there are three kinds of IG values, i.e., IGit,,(U), IGCei(U), and IGtoi,t(U), for representing the quality of a clustering result. Specifically, ICite,,,(U) is the information gain obtained on items and IG,.i(U) is the information gain obtained on categories. IGta,,(U) is the total information gain, i.e., ICtotol (U)  = IG,te,,,(U) + TGcot(U).  In general, market-basket data set is typically represented by a 2-dimensional table, in which each entry is either 1 or 0 to denote purchased or non-purchased items, respectively. In IG validation model, we treat each item in market-basket data as an attribute J, with two classified la- bel, I or 0.

3 Design of Algorithm CBA  The similarity measurement of CBA, called category- based adherence, will bedescribed in Section 3.1. The pro- cedure of CBA is devised in Section 3.2 and an illustrative example is given in Section 3.3.

3.1 Similarity Measurement:  Category-Based Adherence  The similarity measurement employed by algorithm CBA, called category-based adherence, is defined as follows. In the taxonomy tree, the nearest large node of an  item ir- is itself if ik is large and is its nearest large ancestor node otherwise. Then, the distance ofan item to a cluster is defined below.

Definition 9 (Distance of an item to a cluster): For an item ih of a transaction, the distance of ir to a given cluster C,, denoted by d(ik,C,), is defined as the number oflinksbetweenir, andthenearestlargenodesofik. lfak is a large node in cluster C j ,  then d(ik,Cj) = 0. Otherwise, the nearest large node is the category node which is the lowest generalized concept level node among all large ancestors of item ik. Note that if an item or category node is identified as large node, all its high level category nodes will also be large nodes.

Definition 10 (Adherence of a transaction to a cluster): For a transaction t = { i l , i 2 ,  ..., ip} ,  the adherence o f t  to a given cluster Cj, denoted by H ( t , C j ) ,  is defined as the average distance ofthe items in t to Cj and shown below.

where d(ik, C,) is the distance of 28 in cluster C,.

3.2 Procedure of Algorithm CBA  The overall procedure of algorithm CBA is outlined as follows.

Procedure ofAlgorithm CBA Step 1. Randomly select k transactions as the seed transac- tions of the k clusters From the database D.

Step 2. Read each transaction sequentially and allocates it to the cluster with the minimum category-based adherence.

For each moved transaction, the counts of items and their anceston are increased by one.

Step 3. Repeat Step 2 until no transaction is moved between clusters.

Step 4. Output the taxonomy tree for each cluster as the visual representation of the clustering result.

In Step I ,  algorithm CBA randomly selects k transac- tions as the seed transactions ofthe k clusters from the data- base D .  For each cluster, the items of the seed transaction are counted once in the taxonomy tree. In each cluster, the items and their ancestors are all large in the very beginning because their count is one (which means 100% in the only seed transaction), larger than the minimum suppolt thresh- old. For each cluster, these large nodes represent the bot sale topics in this cluster. In Step 2, algorithm CBA r e d s  each transaction sequentially and allocates it to the cluster with the minimum category-based adherence. AAer one transac- tion is insetted into a cluster C,, the counts of the items and     Figure 3. In Step 1, algorithm CBA randomly chooses the seed transaction for each clus- ter.

their ancestors are increased by one in the corresponding nodes in the taxonomy tree of C,, In addition, the minimum support count of C, is updated. In Step 3, algorithm CBA repeats Step 2 until no transaction is moved between clus- ters. In Step 4, algorithm CBA outputs the taxonomy tree of the final clustering result for each cluster, where the items, categories, and their corresponding counts are presented.

3.3 An Illustrative Example  For the example database D shown in Table I ,  we set k = 2 and S, = 50%. In Step 1, algorithm CBA randomly chooses TID 10 and TID 20 as the seed transaction of the cluster Cl and CZ, respectively. Then, for cluster C1 shown in Figure 3(a), nodes marked gray are the purchased items of TID 10 and the corresponding categories in the taxon- omy tree, and they are identified as large nodes. Similarly, for cluster CI, shown in Figure 3(b), nodes marked gray are large. In Figure 3, the count of each node is illustrated nearby. For example, Count(g, C,) is 1 and Count(g, Cz) is 0. In Step 2, algorithm CBA first allocates TID 30 to clus- ter CZ because H(30, CZ) = a(l + 0) = f (i.e., the link number of item y to categoly F plus the link number of item ztocategolyE)issmallerthanH(30,C1) = $(1+1) = 1.

Similarly, TlDs 40,50,60,90, and 120 are allocated to clus-  Figure 4. In Step 2, algorithm CBA reads each transaction sequentially and allocates it to the cluster with the minimum category-based adherence.

ter C1 which is shown in Figure 4(a). TlDs 30,70, 80, 100, and 1 I O  are allocated to cluster Cz which is shown in Fig- ure 4(b). Then, algorithm CBA derives S,(C,) = 3 and S,(Cz) = 3 hy S, * IC11 = 0.5 * 6 = 3 and S, * IC21 = 0.5+6 = 3, respectively. Because Count(g, Cl) > S,(C,), category A is identified as a large node in cluster C1 and marked gray. In Step 3, algorithm CBA proceeds to itera- tion 2. In iteration 2, two transactions, TID 50 and TID 70 are moved. TID 50 is moved from cluster CI to cluster CZ because H(50,C1) = f ( O  + 2 + 2) = $ > H(50,Cz)  = $(2 + 1 + 0) = 1, and TID 70 is moved from cluster Cz to cluster Cl due the H(70, C,) = f ( l  + 1 + 0) = f < H(70,Cz)  = +(2+0+l)  = 1. Then, algorithmCBA iden- tifies the large nodes again. In iteration 3, only one trans- action TID 100 is moved from cluster Cz to cluster C,. In iteration 4, there is no movement and thus algorithm CBA proceeds to Step 4. The final feature trees for the clustes are shown in Figure 5 .

Note that a transaction at item level may not be similar to any cluster. For example, TID 10 {g, x) and TID 40 ( h ,  z )  have no common items, hut item g and item h have common categoly B and item x and item z have common categoly E. Thus, TID I O  is similar to TID 40 in the high level concept. By taking category-based adherence mea-     Figure 5. In Step 4, algorithm CBA generates the clustering U1.

surement, many transactions may not be taken as outliers if we take categorical relationships of items into considera- tion. In addition, transactions at the item level may have the same similarities in different clusters. However, by sum- marizing the similarities of every item across their categoy levels, algorithm CBA allocates each transaction to a proper cluster. For example, TID 50 has three items: g, x, and y.

Item y is large in cluster C1 and item y is large in cluster C2. Thus, TID 50 has the same similarities in both Cl and CZ. However, item z is a category F which is a large node in Cz. Thus, TID 50 is allocated to C,.

To provide more insight into the quality of CBA, we calculate the IG values of the clustering U1 shown in Figure 5. Note that for an item ik, Ilea,lp are the two classified labels of item ir for represent- ing purchased and non-purchased values. For item g, the information gain Gain(g,Ul) = I ( g , D )  - E(g,U1) = (-$logz+ - :1oy21) - [ z ( - $ l o g 2 $  -  Sim- ilarly, IG values of other items are Gain(h,Ul) = 0.15, G a i n ( k , U ~ )  = 0.48, Gain(rn,Ul) = 0.31, Gain(n, UI) = 0.48, Gain(z, UI) = 0, Gain(y, Ul) = 0.98, and GaZn(z,U,) = 0.39. Hence, IGirem(U1) = C Gain(J,,U1) = 2.89, where I is the set of items  (9 ,  h, I C ,  rn, n, z, y, 2). Similarly, Gain(B,Ul) =  $ h g z $ )  + A(-dlogzr I - a g~oyz5)]  hZ -12 - 0.10.

J.CI  0.33, Gain(R,Ul) = 0.2, Gain(A,Ul)  = 0.41, Gain(F,U,) = 0.65, Gain(E,U1) = 0.48, and IGcat(Ul) = Gain(J,, U I )  = 2.07, where C is the  set of categories {A, B, E, F ,  RI. Then, IGfot.l(U1) = J.CC  IGitem(Ui) +IG,t(Ui) = 4.96.

4 Experimental Results  To assess the efficiency of CBA, we conducted experi- ments to compare CBA with a traditional hierarchical clus- tering algorithm, called CL (standing for Complete Link) [I41 and another algorithm proposed in [ZO] (for the con- venience, the algorithm is named as Basic in this paper).

By extending both previous approaches with taxonomy con- sideration in market-basket data we also implement algo- rithm CLT (standing for Completed Link with Taronomy) and algorithm BasicT (standing for Basic with Taxonomy) for comparison purposes. The details of data generation are described in Section 4.1. The experimental results are shown in Section 4.2  4.1 Data Generation  We take tyrz reat macket-hket data, from a r v  book- sfore company for performance study. In this real data set, there are ID1 = lOOK transactions and N' = 21807 items.

Note that in this real data, there are volume of transactions containing only single items, and many items are purchased infrequently. In addition, the number of the taxonomy lwel in this real data set is 3. In addition, to provide more in- sight into this study, we use a well-known market-basket synthetic data generated by the IBM Quest Synthetic Data Generation Code (51, as the synthetic data for performance evaluation. This code will generate volumes oftransaction data over a large range of data characteristics. These trans- actions mimic the transactions in the real world retailing environment. This generation code also assumes that peo- ple will tend to buy sets of items together, and each such set is potentially a maximal large itemset. The average size of the transactions is denoted by ITl. The average size ofthe maximal potentially large itemsets is denoted by 111. The number of maximal potential large itemsets is denoted by jL1. The number of items in database is denoted by N I .

The number of roots is denoted by N R  and the number of the taxonomy level is denoted by NL.

4.2 Performance Study  We conduct two experiments in this section for perfor- mance study and the clustering quality is evaluated by the IG values. For algorithms CBA, Basic, and BasicT, the minimum support percentage S, is set to 0.5%. Recall that     ..........................................................

+CBA --.CL ... ...CLT ..".. BuL -..- k T  Figure 6. Execution time in logarithm for CBA, CL, CLT, Basic, and BasicT when the data- base size ID1 varies.

there ;ue three kinds of IG values, i.e., IGit,,, IGCat, and IGtOt,,, for evaluating the quality of the clustering result. IGit,, is the information gain obtained on items and IGCat is the information gain obtained on categories.

1Gtot.l = IGicem + 1G.z-t.

4.2.1 Experiment One: When the database size JDJ varies  In this experiment, the scalability of CBA is evaluated by both the real data. By varying the real database size JDI from 5K to ZOK, it is shown in Figure 6 that CBA signifi- cantly outperforms other algorithms in execution efficiency.

Note that the logarithmic scale with base 10 i s  used in the y-axis of Figure 6 since the execution time of CBA is sig- nificantly shorter than those of other algorithms and the ex- ecution times of CBA increase linearly as the database size increases, indicating the good scale-up feature of algorithm CBA.

4.2.2 Experiment nvo: When the number of taron- omy levels N L  varies in synthetic data  In the synthetic data experiment shown in Figure 7, we set ID] = 100K, IT] = 5,111 = 2,ILI = 2000, N' = 5000, N R  = 100, and N L  varies from 3 to 5. When the number of taxonomy levels increases, the number of internal (i.e., cat- egory) nodes also increases. Thus, the IGCmt increases so that CBA can obtain more information gain on categories than on items, indicating the advantage of CBA by employ- ing the category-based adherence as the measurement.

....... ~ ..............................................................

P 3  Y  I  I   Figure 7. The IG values when the number of taxonomy levels N L  varies.

5 Conclusion  In view of the features of market-basket data, we devised in this paper a novel measurement, called the category- based adherence, and utilize this measurement to perform the clustering. With this category-based adherence mea- surement, we developed algorithm CBA for market-basket data with the objective to minimize the category-based ad- herence. A validation model based on Information Gain (IG) was also devised in this paper t o , a s s e s  the quality of clustering for market-basket data. As validated by both real and synthetic datasets, it was shown by our experimental results, with the taxonomy information, algorithm CBA de- vised in this paper significantly outperforms the prior works in both the execution efficiency and the clustering quality for market-basket data.

Acknowledgement  The authors are supported in part by the National Science Council, Project No. NSC 91-2213-E-002-034 and NSC 91-2213-E-002-045, Taiwan, Republic of China.

