An Algorithm for In-Core Frequent ltemset Mining on  Streaming Data

ABSTRACT Frequent itemset mining is a core data mining operation and has been extensively studied over the last decade. This paper takes a new approach for this problem and makes two ma- jor contributions. First, we present a one pass algorithm for frequent itemset mining, which has deterministic bounds on the accuracy, and does not require any out-of-core summary structure. Second, because our one pass algorithm does not produce any false negatives, it can be easily extended to a two pass accurate algorithm. Our two pass algorithm is very memory efficient, and allows mining of datasets with large number of distinct items and/or very low support levels.

Our detailed experimental evaluation on synthetic and real datasets shows the following. First, our one pass algorithm is very accurate in practice. Second, our algorithm requires sig- nificantly lower memory than Manku and Motwani's one pass algorithm and the multi-pass Apriori algorithm. Our two pass algorithm outperforms Apriori and FP-tree when the number of distinct items is large and/or support levels are very low.

In other cases, it is quite competitive, with possible exception of cases where the average length of frequent itemsets is quite high.

1. INTRODUCTION Frequent itemset mining is a core data mining operation  and has been extensively studied over the last decade [I, 9, 10, 11, 21, 221. Algorithms for frequent itemset mining form the basis for algorithms for a number of other mining problems, including association mining, correlations mining, and mining sequential and emerging patterns [ll].

Algorithms for frequent itemset mining have typically been developed for datasets stored in persistent storage and involve two or more passes over the dataset. Recently, there has been much interest in data arriving in the form of continuous and infinite data streams. In a streaming environment, a mining algorithm must take only a single pass over the data [3]. Such algorithms can only guarantee an approximate result.

In this paper, we present a new approach for frequent item- set mining. Our work has two main contributions:  In-core Mining in Streaming Environment: We present a single pass algorithm for frequent itemset mining in a stream- ing environment. Our algorithm has provable deterministic bounds on accuracy. Unlike the only other existing work in this area that we are familiar with [15], our algorithm does not require any out-of-core summary structure. We believe that this is a very desirable property, since stream mining algo- rithms may need to be executed in small and mobile devices, which do not have attached disks for storing an out-of-core  summary structure.

Memory  Efficient Accura te  Mining: A key limitation of the existing work on frequent itemset mining has been the high memory requirements when the number of distinct items is large and/or the support level desired is quite low. Our single pass algorithm has a property that it does not produce false negatives, i.e., all frequent itemsets with desired sup- port level are reported. The false positives reported by our algorithm can be easily removed through a second pass on the dataset. Our two pass algorithm provides high memory efficiency, while not compromising accuracy in any way.

Our work derives from the recent work by Karp et al. on determining frequent items (or 1-itemsets) [14]. They present a two pass algorithm for this purpose, which requires only (I /@) memory, where 19 is the desired support or frequency level. Their first pass computes a superset of frequent items, and the second pass eliminates any false positives. Our work addresses three major challenges in applying their ideas for frequent itemset mining in a streaming environment. First, we have developed a method for finding frequent k-itemsets, while still keeping the memory requirements limited. Second, we have developed a way to have a bound on the superset computed after the first pass. Third, we have developed a new data structure and a number of other implementation optimizations to support efficient execution.

We have carried out a detailed evaluation using both syn- thetic and real datasets. Our results can be summarized as follows. First, our one pass algorithm is very accurate in prac- tice. Second, our algorithm is very memory efficient. For ex- ample, using the TlO.14.NlOK dataset and a support level of 1%, we can consistently handle 4 million to 20 million trans- actions with less than 2.5 MB main memory. In comparison, Manku and Motwani's algorithm [15] requires an out-of-core data-structures on top of a 44 MB buffer to process 1 million transactions. Finally, the algorithm can handle large number of distinct items and small support levels using a reasonable amount of memory. For example, a dataset with 100,000 dis- tinct items and a support level of 0.05% could be handled with less than 200 MB main memory, a factor of 5 improvement over Apriori.

2. BASIC IDEAS This section describes the basic ideas that lead to our new  algorithm. Initially, we discuss a new approach for finding frequent items from Karp et al. [14]. We then discuss the challenges in extending this idea to frequent itemset mining, and finally outline our ideas for addressing these issues.

2.1 Finding Frequent Items     Our work is derived from the recent work by Karp, Pa- padimitriou and Shenker on finding frequent elements (or 1- itemset) [14]. Formally, given a sequence of length N and a threshold 0 (0 < 0 < I), the goal of their work is to de- termine the elements that occur with frequency greater than NO.

A trivial algorithm for this will involve counting the fre- quency of all distinct elements, and checking if any of them has the desired frequency. If there are n distinct elements, this will require O(n) memory.

Their approach requires only 0(1/0) memory. Their ap- proach can be viewed as a generalization of the following sim- ple algorithm for finding the majority element in a sequence.

A majority element is an element that appears more than half the time in an entire sequence. We find two distinct elements and eliminate them from the sequence. We repeat this process until only one distinct element remains in the sequence. If a majority element exists in the sequence, it will be left after this elimination. At the same time, any element remaining in the sequence is not necessarily the majority element. We can take another pass over the original sequence and check if the frequency of the remaining element is greater than N/2.

FindingFrequentItems(Sequence S, 0) global Set P ;  // Set  of Potentially P t 0; // Frequent Items foreach (s E S )  // each item in S  i f s E P H.count + +;  else P c {s) U P ; s.count = 1; if IPl L P/@1  foreach (p E P) p.count - -; if p.count = 0  p + p - { P I ; Output (P);  Figure 1: K a r p  et al. Algorithm for Frequent I t ems  The idea can be generalized to an arbitrary 0. We can pro- ceed as follows. We pick any 110 distinct elements in the se- quence and eliminate them together. This can be repeated un- til no more than 1/O distinct elements remain in the sequence.

It can be claimed that any element appearing more than N0 times will be left in the sequence. The reason is that the elimination can only be performed a t  most N/(1/0) = NO times. During each such elimination, any distinct element is removed at most once. Hence, for each distinct element, the total number of eliminations during the entire process is a t most NO. Any element appearing more than NO times will remain in the sequence. Note, however, the elements left in the sequence do not necessarily appear with frequency greater than NO. Thus, this approach will provide a superset of the elements which occur more than NO times.

Such processing can be performed to take only a single pass on the sequence, as we show in Figure I. P is the set of potentially frequent items. We maintain a count for each item in the set P. This set is initially empty. As we process a new item from a sequence, we check if it is in the set P. If yes, its count is incremented, otherwise, it is inserted with a count of 1. When the size of the set P becomes larger than [l/O), we decrement the count of each item in P, and eliminate any item whose count has now become 0. This processing  is equivalent to the eliminations we described earlier. Note that this algorithm requires only R(1/8) space. It computes a superset of frequent items. To find the precise set of frequent items, another pass can be taken on the sequence, and the frequency of all remaining elements can be counted.

2.2 Issues In Frequent Itemset Mining In this paper, we build a frequent itemset mining algorithm  using the above basic idea. There are three main challenges when we apply this idea to mining frequent itemsets, which we summarize below.

1. Dealing with Transaction Sequences: The algorithm from Karp et al. assumes that a sequence is comprised of el- ements, i.e., each transaction in the sequence only con- tains one-items. In frequent itemset mining, each trans- action has a number of items, and the length of every transaction can also be different.

2. Dealing with k-itemsets: Karp et al.'s algorithm only finds the frequent items, or 1-itemsets. In a frequent itemset mining algorithm, we need to find all k-itemsets, k 2 1, in a single pass.

Note that their algorithm can be directly extended to find i-itemsets in the case where each transaction has a fixed length, I .  This can be done by eliminating a group of (110) x (f) different i-itemsets together. This, however, requires R((1/0) x (f)) space, which becomes extremely high when 1 and i are large. Furthermore, in our problem, we have to find all i-itemsets, i 2 1, in a single pass.

3. Providing an Accuracy Bound: Karp et al.'s algorithm can provably find a superset of the frequent items. How- ever, no accuracy bound is provided for the item(set)s in the superset, which we call the potential frequent item(set)s. For example, even if an item appears just a single time, it can still possibly appear in the superset reported by the algorithm. In frequent itemset mining, we will like to improve above result, and provide a bound on the frequency of the itemsets that are reported by the algorithm.

2.3 Key Ideas We now outline how we can address the three challenges we  listed above.

Dealing wi th  k-itemsets i n  a S t r eam of Transactions: Compared with the problem of finding frequent items, the challenges in finding frequent itemsets from a transaction se- quence mainly arise due to the large number of potential fre- quent itemsets. This also results in high memory costs. As we stated previously, a direct application of the idea from Karp et al. will require R((1/0) x ( f ) )  space to find potential frequent i-itemsets, where 1 is the length of each transaction. This ap- proach is prohibitively expensive when 1 and i are large, but can be feasible when i is small, such as 2 or 3.

Recall that most of the existing work on frequent itemset mining uses the Apriovi property [I], i.e., an i-itemset can be frequent only if all subsets of this itemset are frequent. One of the drawbacks of this approach has been the large number of 2-itemsets, especially when the number of distinct items is large, and -9 is small.

Our idea is to use a hybrid approach to mine frequent item- sets from a transaction stream. We use the idea from Karp et al. to determine the potential frequent 2-itemsets. Then, we use the set of potential frequent 2-itemsets and the Apriori property to generate the potential i-itemsets, for i > 2. This     approach finds a set of potential frequent itemsets, which is guaranteed to contain all the true frequent itemsets, in a sin- gle pass of the stream.

Also, if a second pass of the data stream is allowed, we can eliminate all the false frequent itemsets from our result set.

The second pass is very easy to implement, and in the rest of our discussion, we will only focus on the first pass of our algorithm.

Bounding False Positives: In order to have a accuracy bound, we propose the following criteria for the reported po- tential frequent itemsets after the first pass. Besides report- ing all items or itemsets that occur with frequency more than NO, we want to report only the items or itemsets which ap- pear with frequency at least NO(1 - e ) ,  where 0 < e 5 1.

This criteria is similar to the one proposed by Manku and Motwani [15].

We can achieve this goal by modifying the algorithm as shown in Figure 2. In the first step, we invoke the algorithm from Karp et al. with the frequency level Be. This will report a superset of items occurring with frequency more than NO?.

We also record the number of eliminations, c, that occur in this step. Clearly, c is bounded by NO?. In the second step, we remove all items whose reported frequency is less than NO - c > NO(1 - E).

FindingFrequentItemsBounded(Sequence S, 0, E) global Set P ; P + 0; c +- 0; // Number of Elimination foreach (s E S)  if s E P s.count + +;  else P t {s) U P; s.count = 1; if IPI 2 r l i ( ~ ? ) i  c + +; // Count Eliminations foreach (p E P )  p.count - -; if p.count = 0  P t p - {PI; foreach (p E P )  if p.count 5 (NO - c) p + p -  {PI;  Output (P);  Figure 2: Improving Algori thm wi th  A n  Accuracy Bound  We have two claims about the above process: 1) it reports all items that occur with frequency more than NO, and 2) it only reports items which appear with frequency more than NO(1 - e ) .  The reason for this is as follows. Consider any element that appears with frequency NO. After the first step, it will be reported in the superset with a frequency greater than c, c 5 Nee. Therefore, it will remain in the set after the second step also. Similarly, consider any item that appears with frequency less than NO(1 - e). If this item is present in the superset reported after the first step, it will be removed during the second step since NO - c 2 NB(1 - e). This idea can be used for frequent itemset mining also.

In the next Section, we introduce our algorithm for mining frequent itemsets from streaming data based on the above two ideas.

3. ALGORITHM  In this section, we introduce our new algorithm in three steps. In Subsection 3.1, we describe an algorithm for mining frequent itemsets from a data stream, which assumes that each transaction has the same length. In Subsection 3.2, we extend this algorithm to provide an accuracy bound on the potential frequent itemsets computed after one pass. In Subsection 3.3, we further extend the algorithm to deal with transactions of variable length.

Before detailing each algorithm, we first introduce some terminology. We are mining a stream of transactions 2). Each transaction t in this stream comprises a set of items, and has the length (ti. Let the number of transactions in D be (Dl.

Each algorithm takes the support level O as one parameter.

An itemset in D to be considered frequent should occur more than times.

To store and manipulate the candidate frequent itemsets during any stage of every algorithm, a lattice C is maintained.

where, k is largest frequent itemset, and Li, 1 5 i 5 k com- prises the potential frequent i-itemsets. Note that in mining frequent itemsets, the size of the set CI ,  which is bound by the number of distinct items in the dataset, is typically not very large. Therefore, in order to simplify our discussion, we will not consider 131 in the following algorithms, and assume we can find the exact frequent 1-itemsets in the stream D.

Also, we will directly extend the idea from Karp et al. to find the potential frequent 2-itemsets.

As we stated in the previous section, we deal with all k- itemsets, k > 2, using the Apriori property. To facilitate this, we keep a buffer T in each algorithm to store the recently re- ceived transactions. The buffer will be accessed several times to find the potential frequent k-itemsets, k > 2.

1 StreamMining-Fixed(Stream D, 8 ) global Lattice C; local Buffer  7; local Transaction t; c t 0 ;  T t 0 ; f +- ltl * (ltl - 111% foreach (t E 2))  T t 7- u { t ) ; Update(& C, 2); if IC21 Z T l / O l  . f  ReducFreq(C,2); {* Deal with k - itemsets, k > 2 *) i t 2; while Ci # 0  i ++ ; foreach (t E 7 )  Update(t, L,  i); ReducFreq(C, i);  T t 0 ; Output (L);  Figure 3: StreamMining-Fixed: Algorithm Assuming Fixed Length  Transactions  3.1 Mining Frequent Itemsets from Fixed Length Transactions  The algorithm we present here mines frequent itemsets from a stream, under the assumption that each transaction has the same length Itl. The algorithm has two interleaved phases.

Update(Transaction t, Lattice C, i ) for  all i subsets s of t  if s E L, s.count + +;  else if i 5 2 Li .insert(s);  e lse  if all i - 1 subsets of s E Li-1 Li .insert(s);  ReducF'req(Lattice L, i) foreach i itemsets s E L;  s.count - --; if s.count = 0  Ci .delete(s);  F i g u r e  4: S u b r o u t i n e s  Descr ip t ion  The first phase deals with 2-itemsets, and the second phase deals with k-itemsets, k > 2. The main algorithm and the associated subroutines are shown in Figures 3 and 4, respec- tively. Note that  the two subroutines, Update and ReducFreq, are used by all the algorithms discussed in this section.

The first phase extends the Karp et al.'s algorithm t o  deal with 2-itemsets. As we stated previously, the algorithm main- tains a buffer T which stores the recently received transac- tions. Initially, the buffer is empty. When a new transaction t arrives, we put it in T .  Next, we call the Update routine to  increment counts in L2. This routine simply updates the count of 2-itemsets that  are already in L2. Other 2-itemsets that are in the transaction t are inserted in the sets Lz.

When the size of Lz is beyond the threshold, [ l / B l  f ,  where f is the number of 2-itemsets per transaction, we call the procedure ReducFreq to reduce the count of each 2-itemsets in L2, and the itemsets whose count becomes zero are deleted.

Invoking ReducFreq on Lz triggers the second phase.

The second phase of the algorithm deals with all k-itemsets, k > 2. This process is carried out level-wise, i.e, i t  proceeds from 3-itemsets t o  the largest potential frequent itemsets. For each transaction in the buffer 7, we enumerate all i-subsets.

For any i-subset that  is already in L ,  the process will be the same as for a 2-itemset, i.e, we will simply increment the count. However, an i-subset that  is not in L will be inserted in L only if all of its i - 1 subsets are in L as well. Thus, we use the Apriori property.

After updating i-itemsets in L,  we will invoke the ReducFreq routine. Thus, the itemsets whose count is only 1 will be deleted from the lattice. This procedure will continue until there are no frequent k-itemsets in L.  At the end of this, we clear the buffer, and start processing new transactions in the stream. This will restart the first phase of our algorithm t o deal with 2-itemsets.

We next discuss the correctness and the memory costs of our algorithm. Let C! be the set of frequent i-itemsets with support level 8 in D,  and L; be the set of potential frequent i-itemsets provided by this algorithm.

THEOREM 1. In using the algorithm StreamMining-Fixed on a set of transactions with a fixed length, for any k > 2, L! Lk.

LEMMA 1. In using the algorithm StreamMining-Fixed on a set of transactions with a fixed length, the size of Lz is bounded by ([1/81 + l ) ( r l ) .

The proofs for the Theorem 1 and the Lemma 1 are avail- able in a technical report [13]. Theorem 1 implies that  any  frequent k-itemset is guaranteed to be in the output of our al- gorithm. Lemma l provides an estimate of the memory costs for CZ.

3.2 Providing an Accuracy Bound  StreamMining-Bounded(Stream 'D, 8, E ) global  Lattice L; local B u f f e r  7 ; local Transaction t; L t 0 ;  T t 0 ; f + ltl * (ltl - c t 0; // Number of ReducFreq Invocations foreach (t E D)  T t T u { t ) ; Update(t, L , l ) ; Update(t, C, 2); if lL21 2 [1 /8~1  . f  ReducFreq(L, 2); c +  +; i t 2; while  Ci # 0  i + + ; foreach (t E 7 )  Update(t, L ,  i); ReducFreq(L, i);  T e 0 ; foreach s E L  if s.count 5 8lDl - c L;.delete(s);  Output (L); -  F i g u r e  5: S t reamMining-Bounded:  Algor i thm w i t h  a B o u n d  o n  Accuracy  We now extend the algorithm from the previous subsection t o  provide a bound on the accuracy of the reported results.

As described in Subsection 2.3, the bound is described by an user-defined parameter, E ,  where 0 < r 5 1. Based on this parameter, the algorithm ensures that  the frequent itemsets reported do occur more than (1 - r)81D1 times in the dataset.

The basic idea for achieving such a bound on frequent items computation was illustrated in Figure 2. We can extend this idea to  finding frequent itemsets. Our new algorithm is de- scribed in Figure 5. Note that  we still assume that each trans- action has the same length.

This algorithm provides the new bound on accuracy in two steps. In the first step, we invoke the algorithm in Figure 3 with the frequency level 86. This will report a superset of itemsets occurring with frequency more than Nee. We record the number of invocations of ReducFreq, c, in the first step.

Clearly, c is bounded by NO?. In the second step, we remove all items whose reported frequency is less than NO - c 2 NB(1 - e). This is achieved by the last foreach loop.

The new algorithm has the following property: 1) if an itemset has frequency more than 8, it will be reported. 2) if an itemset is reported as a potential frequent itemset, it must have a frequency more than 8(1 - E ) .  Theorem 2 formally states this property, and its proof is available in a technical report 1131.

THEOREM 2. In using the algorithm StreamMining-Bounded on a set of transactions with a fixed length, for any k 2 2,  ( I - c ) S L: c Ck c Lk .

Note that the number of invocations of ReducFreq, c, is usually much smaller than Nee after processing a data  stream.

Therefore, an interesting property of this approach is that  it produces a very small number of false frequent itemsets, even with relatively large F. The experiments in Section 4 also support this observation.

The following lemma claims that the memory cost of Lz is increased by a factor proportional to  1/e.

LEMMA 2. In  using the algorithm StreamMining-Bounded on a set of transactions with a fixed length, the size of L2 is bounded by ([l/Bel + I)(!').

3.3 Dealing with Variable Length Transactions  StreamMining(Stream 23, 0, e ) global  Lattice L; local B u f f e r  7; local Transaction, t ; L t 0 ;  7 t 0 ; f t 0; / /  ilverage 2 - itemset per  transaction c t 0: foreach (t E D)  7 t 7-u {t); Update(t,L, 1); Update(t, L,  2); f t TwoItemsetPerTransaction(t); if 11321 2 rl/ocl . f  ReducFreq(L, 2); c + + ; i t 2; while  Li # 0  i + +; foreach (t  E 7 )  Update(t, L,  i); ReducFreq(L, i);  7 t 0; foreach s E L  if s.count < BID1 - c Li.del:te(s);  Output(L);  TwoItemsetPerTransaction(Transaction t ) global  X; / /  Number o f 2 Itemset global  N ;  // Number of Transactions local f ; N + + ;  .p- rx /~i \  ' ~f 1 . ~ ~ 1  2 rlieci . f  N t N - yl/ee1; x t x - [ i /eel .  fi  r e tu rn  f;  F i g u r e  6: S t reamMining :  F ina l  A l g o r i t h m  In this subsection, we present our final algorithm, which improves upon the algorithm from the previous subsection by dealing with variable length transactions. The algorithm is referred t o  as StreamMining and is illustrated in Figure 6.

When each transaction has a different length, the number of 2-itemsets in each transaction also becomes different. There- fore, we cannot simply maintain f ,  the number of 2-itemsets per transaction, as a constant. Instead, we maintain f as a weighted average of the number of 2-itemsets that  each t r a n s action processed so far. This weighted average is computed by giving higher weightage t o  the recent transactions. The details are shown in the pseudo-code for the routine TwoItem- setPerTransaction.

To motivate the need for taking such a weighted average, consider the natural alternative, which will be maintaining f as the average number of 2-itemsets that each transaction seen so far has. This will not work correctly. For example, suppose there are 3 transactions, which have the length 2, 2, and 3, respectively, and I9 is 0.5. The first two transactions will have a total of two 2-itemsets, and the third one has 6 2- itemsets. We will preform an elimination when the number of different 2-itemsets is larger than or equal to  (118) x f .  When the first two transactions arrive, an elimination will happen (assuming that  the two 2-itemsets are different). When the third one arrives, the average number of 2-itemsets is less than 3, so another elimination will be performed. Unfortunately, a frequent 2-itemset that  appears in both transactions 1 and 3 will be deleted in this way.

In our approach, the number of invocations of ReducFreq, c, is less than IDl(ee), where ID/ is the number of transactions processed so far in the algorithm. Lemma 3 formalizes this, and its proof is available in a technical report [13].

LEMMA 3. c < I'DI(I9c) is an invariant in the algorithm StreamMining.

Note that  by using the Lemma 3, we can deduce that the property of the Theorem 2 still holds for mining a stream of transaction with variable transaction lengths. Formally,  THEOREM 3. In  using the algorithm StreamMining on a stream of transactions with variable lengths, for any k 2 2,  (1-c)S L; g Lk g L k  .

An interesting property of our method is that in the situ- ation where each transaction has the same length, our final algorithm, StreamMining will work in the same fashion as the algorithm previously shown in Figure 5.

Note, however, that  unlike the case with fixed length trans- actions, the size of L2 cannot be bound by a closed formula.

Also, in all the algorithms discussed in this section, the size of sets L A ,  k > 2 also cannot be bound in any way. Our algo- rithms use the Apriori property t o  reduce their sizes. In the next section, we evaluate the memory cost of our algorithm experimentally.

4. EXPERIMENTAL RESULTS In our experiments, we are interested in evaluating a num-  ber of different aspects of our algorithm.

Comparing the execution time and memory requirements of our one pass and two pass algorithm with those of Apriori and FP-tree based algorithms.

Evaluating the execution time and memory requirements of our new algorithms with increasing dataset size and decreasing support levels.

Evaluating the accuracy of our algorithm with different levels of E .

Demonstrating the ability of our algorithm to handle very large number of distinct items and very low support levels.

For comparing our algorithm against the Apriori algorithm, we used a well-known public distribution from Borgelt [4].

Earlier versions of this code have been incorporated in a com- mercial data  mining tool called Clementine. For comparisons with FP-tree based approach, the implementation we used is from Goethals [8]. All our experiments were conducted on a 933 MHz Pentium I11 machine with 512 MB main memory.

4.1 Synthetic Datasets The synthetic datasets we used were generated using a tool  from IBM 121. Datasets generated from this tool have been widely used for evaluating frequent itemset and association mining implementations.

The first dataset we used is TlOI4.NlOK. The number of distinct itemsets is 10,000, the average number of items per transaction is 10, and the average size of large itemsets is 4.

We used three different versions of our algorithm. Stream-el uses 1 as the value of E and does not provide any theoretical bound on the accuracy. Stream-e.75 uses .75 as the value of 6 to  provide a theoretical bound on the accuracy. Stream+ is the two pass implementation that  gives the accurate set of frequent itemsets and their frequency counts.

Figure 7 shows the execution times of Apriori, FP-tree and our three versions as the support threshold is varied from 0.1% to 1.0%. The number of transactions is 12 million. Because of high memory requirements, FP-tree could not be executed with support levels lower than 0.4%. This limitation of the FP-tree approach has been identified by other experimental studies also [6]. Up to the support level of 0.4%, the execu- tion times of all versions is quite similar. However, Apriori's execution time increases rapidly when the support level is less than 0.4%. As expected, Stream-el has the lowest execution time among all of our versions. The use of .75 as the value of E increases the execution time by up  to 25%. If a second pass is used, the total execution time is increased by up to 50%.

Figure 8 compares the memory requirements. Because the memory requirements of Stream+ are identical to  those of Stream-el, this version is not shown separately in our mem- ory requirements charts. The important property of our al- gorithm is that the memory requirements do not increase sig- nificantly as the support level is decreased.

Accuracy of an algorithm is defined as the fraction of re- ported frequent itemsets that  are actually frequent. Obvi- ously, the accuracy of Apriori, FP-tree and Stream+ is al- ways 100%. With 12 million transactions, Stream-el and Stream-e.76 give accuracy of 100% with thresholds at  1%, .8%, .6%, and .4%. With thresholds of .2% amd .I%, Stream-el has an accuracy of 95.8% and 97.8%, respectively. However, in both these cases, with .75 as the value of E ,  the accuracy again becomes 100%.

Figures 9 and 10 examine the execution times as the dataset is increased. The threshold is kept a t  .4% and .I%, respec- tively. Because of the high memory requirements of FP-tree, our algorithm is only compared against Apriori. When the support level is 0.1%, our algorithm is up t o  an order of mag- nitude faster. The relative difference is smaller when the sup- port level is 0.4%, but even our two pass version is faster than Apriori. Even as the dataset size is varied, our one pass algo- rithms always give an accuracy of 100% when the threshold is .4%. With the threshold a t  . l%,  the accuracy of Stream-e.75 is again 100% in all cases. The accuracy of Stream-el varies between 94.3% and 98.6%.

Figure 11 focuses on memory requirements with support levels of .4% and .I%. At the support level of .4%, Apriori's memory requirements are lower than our versions. However, with threshold at  .I%, our versions require less than half the memory. Moreover, it is important to  note that  with 10,000 distinct items and a support level of . l%,  the total memory requirements are only around 17 MB. Thus, our algorithm is well suited for mining streaming data  using a small device with only a limited memory.

We also conducted detailed evaluation with a number of other datasets, including T15.16.N10K, T25.14.N100K, and TlO.14.NlOK. The results from these datasets show the same  F i g u r e  7: E x e c u t i o n  T i m e  w i t h  C h a n g i n g  S u p p o r t Level  (T10.14.NlOK D a t a s e t )  F i g u r e  8: M e m o r y  R e q u i r e m e n t s  w i t h  C h a n g i n g  Sup- port Level  (T10.14.NlOK D a t a s e t )  F i g u r e  9: Execut ion  T i m e  w i t h  Increas ing  D a t a s e t Size (threshold=O.l%, T10.14.NlOK D a t a s e t )     trend, which is that our algorithm is significantly better both  F igure  10: Execut ion  T i m e  w i t h  Increas ing  D a t a s e t Size (threshold=0.4%, T1O.II.NlOK D a t a s e t )  F i g u r e  11: M e m o r y  R e q u i r e m e n t s  w i t h  Increas ing D a t a s e t  Size (T10.14.NlOK D a t a s e t )  F igure  12: Execut ion  T i m e  w i t h  C h a n g i n g  S u p p o r t Level (BMS-WebView-1 D a t a s e t )  in terms of execution time and memory requirements. De- tailed results are presented in a technical report [13].

4.2 Real Dataset The real dataset we use is the BMS-WebView-1 dataset  which contains several months of click-stream data  from one e-commerce website. A portion of it has been used in the KDD-Cup 2000 competition and also used by Zhang et al. [22] t o  evaluate traditional offline association mining algorithms.

The characteristics of the BMS-WebView-1 dataset are quite different from the IBM Quest synthetic datasets. The origi- nal dataset has 59,602 transactions and contains 497 distinct items. The maximum transaction size is 267, while the av- erage transaction size is just 2.5. For our experiments, we duplicated and randomized the original dataset t o  obtain 1 million transactions.

Because of the small size of the dataset and the small num- ber of distinct items, we did not expect to  outperform Apriori on this dataset. However, we have still compared the per- formance with Apriori to  show that  the algorithm can give accurate results in one pass, and can still be competitive.

In our experiments, we use e = 0.6. Further, we provide an- other parameter m t o  represent the maximal frequent itemsets we are interested in. This is because if we have some addi- tional knowledge about the length of the maximal frequent itemsets, the performance of our implementation can be im- proved. In this dataset, as the support level is 0.2%, 0.4%, 0.6%, 0.8% or 1%, the maximal frequent itemsets is 2, 3, 3, 4, and 6, respectively. For the online checking optimization we had described earlier, the threshold we define is 10, i.e, two transactions in the buffer will not have a common subset which contains more than 10 items. Since we have only less than 500 distinct items, we maintain all of the 2-itemsets as an array in the main memory.

Figure 12 compares the execution time. Stream-m* refers t o  ~ G e a r n ~ i n i n ~  with some knowledge of maximal frequent itemsets. For support level of 0.2%, we had m = 4, and for others, we had m = 3. Stream-m6 refers to  the version using m = 6 in all cases. Stream refers to  StreamMzning having no knowledge about the maximal frequent itemsets. Stream+m*, Stream+m6 and Stream+ refer to  the corresponding two pass versions.

The three versions have very similar results for accuracy.

For threshold levels between 1% and 0.4%, they achieve 100% accuracy. For the threshold of 0.2%, the accuracy is nearly 99%.

We can see that  the performance of Stream-m* is quite sim- ilar to  Apriori. For the Stream-m6 and Stream, we can see as the  additional information on maximal frequent itemsets is re- duced, the algorithm performance becomes less competitive.

For the two-pass algorithm, we can see that the second pass just adds a fairly small and constant time.

5. RELATED WORK In this section, we compare our work with related research  efforts in the areas of approximate (one-pass) and accurate frequent itemset mining.

The work closest to  our work on handling streaming data is by Manku and Motwani [15]. They have also presented a one pass algorithm that  does not allow false negatives, and has a provable bound on false positives. They achieve this through a very different approach, called lossy counting. The differences in the two approaches are in space requirements.

For finding frequent items, the approach we use takes 0(1/B) space. Their approach requires O((l/B)log(BN)) space, where     0 is the desired support level and N is the length of the stream.

Therefore, for frequent itemset mining, they require an out- of-core data structure. In comparison, we do not need any such structure. On the T10.14.NlOK dataset used in their paper as well, we see that  with 1 million transactions and a support level of 1%, their algorithm requires an out-of-core data-structures on top of even a 44 MB buffer. For datasets ranging from 4 million to  20 million transactions, our algo- rithm only requires 2.5 MB main memory based summary. In addition, we believe that  there are a number of advantages of an algorithm that  does not require an out-of-core summary structure. Mining on streaming data  may often be performed in mobile, hand-held, or sensor devices, where processors do not have attached disks. I t  is also well known that  additional disk activity increases the power requirements, and battery life is an important issue in mobile, hand-held, or sensor de- vices. Also, while their algorithm is shown t o  be currently computation-bound, the disparity between processor speeds and disk speeds continues t o  grow rapidly. Thus, we can ex- pect a clear advantage from an algorithm that  does not require frequent disk accesses.

Giannella et al. have developed a technique for dynamically updating frequent patterns on streaming data  [7]. They cre- ate a variation of FP-tree, called FP-stream, for time-sensitive mining of frequent patterns. Because this approach gives ad- ditional weightage to  recent transactions, it can efficiently an- swer time-sensitive queries, which we do not consider. How- ever, for queries involving queries on an entire data  stream, their approach is not efficient.

As our experimental results have shown, the memory re- quirements of our approach are significantly lower than those of FP-tree. However, we have not considered time-sensitive queries.

Recently, Yu and his colleagues proposed a new approach to mine frequent itemsets, which allows both false negatives and false positives [19]. Their approach is based on the Cher- noff Bound. In comparison, our algorithm finds a superset of frequent itemsets, and therefore, only allows false positives.

Further, if a second pass is allowed, our algorithm can also eliminate false positives.

Chi et a[. have proposed a method for maintaining closed frequent itemsets over a stream sliding window [ 5 ] .  Our work considers a different problem, as we mine all frequent itemsets over the entire stream.

Now, we compare our work with accurate frequent itemset mining algorithms, which require two or more passes. The classical work in this area is the Apriori algorithm [2, 11.

The basic idea in this algorithm has been extended by sev- eral others [20, 161. Our experimental comparison has shown advantages of our approach when the number of distinct item- sets is large and/or the support level desired is very low.

Several algorithms since then have required only two passes.

This includes the FP-tree based approach by Han and co- workers (111. Again, as our experimental results have shown, the memory requirements for maintaining the frequent pat- terns summary increase rapidly when the support levels are low. Other two pass algorithms for association mining include those from Savarese et  al. [17] and Toivonen [la]. In each of these cases, the two pass algorithm does not extend to a one pass algorithm with any guarantees on accuracy. Hidber has developed a technique which guarantees that  the results after the first pass do not include any false negatives, but produces a large number of false positives [12]. A detailed compari- son of frequent itemset mining algorithms has been done by Goethals and Zaki, as part of the FIMI workshop [9]. Our fo- cus has been on the cases when the number of distinct itemsets  is very large or the support levels are very low, which were not the emphasis of their evaluation.

6. CONCLUSIONS In this paper, we have developed a new approach for fre-  quent itemset mining. We have developed a new one pass al- gorithm for streaming environment, which has deterministic bounds on the accuracy. Particularly, it is the first such algo- rithm which does not require any ont-of-core memory struc- ture and is very memory efficient in practice.

Our detailed experimental evaluation has shown the follow- ing. First, our one pass algorithm is very accurate in prac- tice. Though a tighter theoretical bound on accuracy can be achieved by increasing memory requirements, i t  was not really required in practice. Second, the memory efficiency of our one and two pass algorithms allowed us to  deal with large number of distinct items and/or very low support levels. For other cases, where traditional multi-pass approaches have worked well in the past, our algorithms are still quite competitive.

One exception is datasets with the average length of an item- set is quite large. In such case, some additional knowledge of maximal frequent itemsets helps efficiency of our algorithms.

7 REFERENCES [ i ]  R H. ~ ~ n n ~ l a ,  R. s r i k a n t .  H. ~ ~ i v o n e n t ,  a n d  A.  ~ n k c r i  verkamo.

Fasf discovery of a s s o c ~ a t i o n  rules. I n  U. Fayyad and et a l ,  editors.  Adlrnnrrx i n  Knolrliedae D%8rnrlrry and Duto Minxnq, pages 307-328. AAAI Press ,  Menlo P a r k ,  CA. 1996.

[2] R .  Agrawal and R .  Srikant.  Fas t  algorithms for mining association rulcs. In P-C. I Q Q ~  m t .  conf. vpn, D ~ ~ ~ B ~ . ~ P ~  ( V L D R ' Q ~ ) ,  pnges 487-499, Santiago,Chile. September  1994.

(31 B. Bahcock. S. B a b u ,  M .  D a t a r .  R .  Motwani, and J .  Widom.  Models and Issues in D a t a  St ream Systems. In P m r r ~ d m q r  of thr 2002 ACM Symporntn r n P n n r i p l ~ r  of Dntnbna~ Sva t rn~s  (PODS 2002) ( I n r ~ t t ~ d  Paper). ACM Press, J u n e 7nn7 ----  (41 Chr is tan  Borgelt.  Apriori implementa t ion http://fuzzy.cs.Uni-Magd=burg.de/ borgelt/Software. Vcreion 4.08.

(51 Y u n  Chi .  Haixun Wang. Philip Yu, and Rnchard Munta. Moment: Mainta in ing Closed Frequent I temsets  over a St ream Sliding Window. In Intrmottonnl Cnnfrwnrr on  Dntn Mzmng (ICDM),  November 2004.

[6] Mohammad El-Haji a n d  O s m a r  R .  Zaiane. lnvcrted Matrix: Efficient Discovery of Frequent I tems in Large Datase ts  in t h e  Context  of Interactive Mining. h P m r ~ ~ d ~ n g  of the ACM S f G K D D  C o v f r w o r r  on Xnnrrlrrlqv ~ ~ r n r ~ ~ ~ ~  ,md Dntn Mlmng.  ACM Press, 2003.

[7] C .  Giannella.  Jiawei Han.  J ~ a n  Pei ,  Xlfcng Yan, and P. S. Yu. Mining Frequent Pat terns  in D a t a  St reams a t  M u l t ~ p l c  T i m e  Granularities In Proceedxnqa of  the NSF Workshop on Nezt Gcr~ernt'on Dotu Minlnq. November 2002.

Is] B a r t  Goethals.  Fp- t ree  implementation.

http://www.cs.helsinki.fi/~/g~ethals/aoftwaindx.html Version L s e t Updated  April 2003.

(9) B a r t  Gocthals and Mohammed J Zaki. Workshop Repor t  on  Workshop a n Frequent ltcmect Mining lmplcments t ions  (FIMI) .  2003.

[ lo]  E-H.  Han,  G Ksrypis ,  and V.  Kumar .  Scalable parallel d a t a m ~ n i n g  for association rules. IEEE lhneoctxonr on Dola and Knowler(qe Enqmecnnq. 12(3).

May / J u n e  2000.

[ l l ]  J .  Nan,  J .  Pei,  and Y Yin. Minlng frequent pa t terns  wi thout  candidate generatton. In Pmrcrdrnqr nf the ACM SIGMOD C,,nf?renr? on M n n n q ~ m ~ n t  of Data, 2000.

[I21 C .  Hidbcr. Online Association Rule Mining. In P.orred%ngr ($1 ACM SIGMOD C n n f ~ w n r r  on  Mnnnqrmrnt o f  Dntn. pages 145-156. ACM Press. 1999.

[13] Ruoming J i n  and Gagan Agrawal. An algorithm for in-core frequent itemsct mining on s t reaming d a t a .  Technical Repor t  OSU-CTSRC-2104-TR14. Ohlo S t a t e  University, 2004.

[14] Richard M .  Karp .  Chriatoe H. Pspadimi t r toua ,  and Scott Shanker. A Slmple Algorithm for Finding Frequent Element* in St reams and Bags. Available from http://www.cs.herkeIeyY~du/ c h r i r t o s / ~ c c b c r g . ~ a ,  2002  [IS] G .  S .  Manku and R .  Motwani. Approximate Frcqucncy Counts  Over D a t a St reams.  In P ~ o ~ ~ ~ d t n e r  of C n n f ~ w n r ~  07, V q ,  Lrtrqr Dn tnBr~a~*  ( V L D R ) ,  pages 346 - 357, 2002.

[16] J .  S. P a r k ,  M.  Chcn,  and P. S. Yu. A n  effecitive hash based algorithm far mininu association rules. In ACM SIGMOD Inti. Cnsf .  Mnnoqpment of Dntn,  May 1995.

[I?] A. Savasere, E .  Omiecinski,  and S.Navathc. An efficient algorithm for mining association rulcs in large database*. In 21th VLDD Conf . .  1995.

(181 H. Toivonen. Sampl ing large databasra  for association rules. I n  Pmr .  of the 22nd VLDM Cnnfrwnre. ,  1996.

[IS] Jeffrey Xu Yu, Zhihong Chong,  Hongjun 1.". a n d  Aoying Zhou. False positive or false negative: Mining frequent i tcmsct r  from high apecd t ransact ional  d a t a  s t reams.  In Pmresdmq. of thr 28th lntrrnntronni Confrrrnrr on V r q  Ln~qp  Dntn i3r1.w. IVLDB),  Toronto, Canada,  Aug 2004.

[20] M.  J .  Zaki. S .  Par thasara thy,  M Ogihara, a n d  W. Li. New algorithms for fast dnscovery of association rules. In .9rd Intl. ConJ. o n  Knowlegbe Ducauen, and Dnto Mtnmg.,  August 1997.

1211 Mohammed J .  Zaki. Parallel a n d  d is t r ibuted  aaaociation mining: A survey.

IEEE Conrrrmnry ,  7(4):14 - 25. 1999.

