CUBS : Multivariate Sequence Classification Using Bounded Z-score with Sampling

Abstract?Multivariate temporal sequence classification is an important and challenging task. Several attempts to address this problem exist, but none provide a full solution. In this paper we present CUBS: Classification Using Bounded Z-Score with Sampling. CUBS uses itemset mining to produce frequent subsequences, and then selects among them the statistically significant subsequences to compose a classification model. We introduce an improved itemset mining algorithm that solves the short sequence bias present in many itemset mining algorithms.

Unfortunately, the z-score normalization hinders pruning. We provide a bound on the z-score to address this issue. Calculation of the z-score normalization requires knowledge of some statistical values of the data gathered using a small sample of the database. The sampling causes a distortion in the values.

We analyze this distortion and correct it. We evaluate CUBS for accuracy and scalability on a synthetic dataset and on two real world dataset. The results demonstrate how short subsequence bias is solved in the mining, and show how our bound and sampling technique enable speedup.



I. INTRODUCTION It is important to classify multivariate temporal sequences.

These are sequences of events, each of several attributes, with a temporal ordering. The task consists of finding sequences that can be used to classify different example classes. For instance in many domains it is important to observe behaviors of several people in a group, and then use these observations to determine online who is currently active. Examples include identifying the current user on a computer for security issues [1] and identifying the current TV viewer for personalized TV services [2].

Multivariate temporal sequence classification is a chal- lenging task. There have been several attempts to address this problem, but none of the attempts present a full solution.

One solution is to apply time-series classification to each attribute independently e.g. [3]. This allows the use of one attribute at a time and uses the full time-series for this attribute. The main drawback of this solution is that there may be information in the combination of several attributes that does not show up when they are separated. A second approach is to perform feature selection that looks at several attributes at single time points, thus losing the temporal property, as in [4]. Another option is to perform a preprocessing stage that incorporates several attributes on several time points as done in [5]. This is a good solution if one knows what the interesting events are, but  for many cases, including the domains we investigated, this information is not available.

In this paper we present CUBS (Classification Using Bounded Z-Score with Sampling), an algorithm that finds patterns spanning several attributes over several time points without previous knowledge about the type of behaviors we are looking for. CUBS uses itemset mining to produce frequent subsequences, and then selects the statistically sig- nificant subsequences from among them as representatives to compose the model for classification. As is the case with previous sequential itemset mining algorithms, this process also suffers from a bias towards short subsequences [6].

SLPMiner [7] addresses this bias by finding subsequences with a support that decreases as a function of their length.

However this heuristic may not hold for all data. CUBS differs from previous work in that it explicitly corrects for the bias by normalizing the support of each candidate subsequence with a length-adjusted z-score.

CUBS? use of z-score normalized support eliminates statistical biases towards shorter subsequences. But it raises two challenges to the scalability of the approach: First, z- score normalization lacks the anti-monotonic property used in support based measures, and thus supposedly forces explicit enumeration of every subsequence in the database.

This renders useless any pruning of candidate subsequences, the basis for scalable itemset mining algorithms, such as SPADE [8]. Second, calculation of the necessary statistics for computation of the z-score (the average and standard deviation of the support, for each subsequence length) seems to require computing the support for every subsequence, again eliminating any benefit from pruning. CUBS addresses both of these challenges.

First, in order to provide a means for pruning candidate subsequences, we introduce a bound on the z-score of future sequence expansions. The z-score bound enables pruning in the mining process to provide scalability while ensuring closure. We use this bound with an enhanced SPADE-like algorithm to efficiently search for subsequences with high z-score values, without enumerating all subsequences.

Second, in order to remove the need for a pre-pruning pass over the entire database, we advocate using a sample of the database to estimate the necessary statistics for z- score calculations. Such sampling causes a distortion in the estimated values [9]. We analyzed this distortion, and present a formula for performing the correction.

DOI 10.1109/ICDMW.2010.38    DOI 10.1109/ICDMW.2010.38     We perform evaluation on two real world data sets and one synthetic dataset. We demonstrate that the z-score normalization provides higher accuracy than using support for classification of long sequences. We show how using the sampling with distortion correction for preprocessing provides results as good as using the full dataset for pre- processing. We also show how the bound on the z-score that enables pruning shortens the runtime of the frequent sequence mining substantially.



II. PRELIMINARIES AND NOTATION  We now present a formal description of CUBS. We intro- duce our terminology, adopted from frequent itemset mining [10] while demonstrating it via an example. Assume a com- puter user clicked the left mouse fast, then pressed the key ?A? a long time after the mouse action and then pressed enter.

The sequence of events would be: click, leftmouse, fast ?then? preskey,A, slowly ?then? presskey, enter and is expressed as C,L, F ? K,A, S ? K,E. Let us define each part of this sequence:  item The basic unit we use is called an item and is denoted using capital letters, as in: C (click).

itemset An itemset is composed of several items that occur at the same time, this is denoted by items separated with a comma, as in: C,L, F (click, leftmouse, fast).

sequence A sequence is composed of several itemsets with a temporal relationship, denoted by itemsets sepa- rated by ???. No two itemsets occur at the same time, as in: C,L, F ? K,A, S (click, leftmouse, fast ?then? preskey,A, slowly)  inclusion An itemset I ? is included in itemset I , denoted I ? ? I , if for every item i ? I ?, i ? I , as in: C,F ? C,L, F  subsequence A sequence s? is a subsequence of the sequence s, denoted s? ? s, if for all itemsets I ? ? s?, there exists an itemset I ? s such that I ? ? I and the temporal order between itemsets is the same as in s? and s (two itemsets in s? do not have to be directly consecutive in s), as in: C ? K,A ? C,L, F ? A,F ? K,A.

size and length The size of a sequence is the number of itemsets in a sequence, size(C,L, F ? K,A, S) = 2. The length of a sequence is the number of items in a sequence, length(C,L, F ? K,A, S) = 6.

We now proceed to describe the proposed approach. Our input is a set of users U = {u} where each user has a set of labeled sequences Xu = {xu}. We use these sequences to model the users. Given a new set of unlabeled sequences N = {n} we determine which of the users u ? U this set belongs to. During training the algorithm selects a set of subsequences Xu? = {x?u|?xu ? Xu, x?u ? xu}. The subsequences we place in Xu? are frequent and statistically significant in relation to other users, and are used to model the user u. Our model M is represented by the subsequences selected for each user M = {Xu?|u ? U}.

Given a new unlabeled set of sequences N = {n} we use the model M to classify N by generating subsequences N ? from N and comparing them to Xu? for each u ? U .

The user u for whom Xu? is closest to N ? is chosen as the classification for N . Intuitively we are comparing the sequence distributions using the samples Xu? and N ?.



III. BIAS REMOVAL IN MINING  We generate a set of frequent subsequences that will be used as candidates for building the classification model.

We base our frequent sequence selection on the SPADE algorithm [8], along with important modifications. SPADE is an efficient algorithm for mining frequent subsequences X ? = {x?} from a set of sequences X = {x}. In SPADE the mining is performed using support - defined as the proportion of sequences a subsequence appears in. The mining is performed by generating candidate subsequences, the support of each new subsequence is compared to the min- imum support. If their support is higher than the minimum support the subsequence is marked as frequent otherwise the subsequence is pruned. The details are beyond the scope of this paper and can be found in [8].

Support has a nice anti-monotonic property promising that it does not grow when a subsequence is expanded, e.g.

support(A,B) ? support(A,B,C). This anti-monotonic property provides closure, since it ensures that no subse- quences that were pruned could have developed into frequent subsequences, and that all frequent subsequences will be found. SPADE?s efficiency is based on this property.

Unfortunately SPADE, and other support based mining methods, suffer from a bias towards shorter sequences as has been shown in [6]. This means that in the frequent subsequence mining, short subsequences are found more often than long subsequences. This occurs even though the appearance of short candidates may be random. In order to solve this bias we propose using the statistical z-score to normalize the length of sequences when searching for frequent subsequences. In our setting a subsequence is frequent if it has a z-score that is high enough. The z-score for a sequence s of length l with a support of sup(s) is specified in Eq. 1,  Zscore(s)l = sup(s)?Al  ?l (1)  where Al is the average support of sequences of length l and ?l is the standard deviation of support for subsequences with this length. The values of Al and ?l are obtained during a preprocessing stage on a sample of the database as described in Sec. IV. The z-score provides a normalization of support relative to sequence length thus removing the bias. The use of this normalization will find subsequences based on their frequency in relation to their length. This provides an even chance to subsequences of all lengths.

However, replacing the support with a z-score normaliza- tion test spoils the anti-monotonic property. Z-score is not anti monotonic like support, and we cannot determine in advance that Zscore(A,B) ? Zscore(A,B,C), therefore pruning becomes difficult. Since z-score lacks the anti- monotonic property we cannot be sure that the z-score of a subsequence with length l will not be expanded to a subsequence with a higher z-score. Therefore we cannot prune sequences based on z-score and promise we will find all frequent subsequences. This creates a problem since without pruning our search space becomes unscalable.

We propose an innovative solution that solves the scala- bility problem. Our solution is to calculate a bound on the z-score of sequences, that can be expanded from a given subsequence. This bound is used for pruning and is described in detail shortly. The algorithm makes two decisions for a subsequence: Wether a subsequence is frequent, and should we expand this subsequence, or prune it.

We define the subsequences with the highest z-scores as frequent. The number of frequent sequences kept is determined by a parameter ?best?. We save the top ?best? z- scores, and for each z-score we keep all subsequences with a z-score of this value. The algorithm in Fig. 1 presents our improved version of the frequent sequence enumeration used by SPADE. The algorithm input is a set of subsequences, that are recursively joined to create new candidates.

We now describe how we developed the bound on the z-score that is used for the pruning. Z-score was defined in Eq. 1. We know that support is anti-monotonic therefore as the sequence length grows support can only get smaller. For a given sequence length the z-score is maximal when the support is maximal (all other values are fixed). Therefore given a sequence s of length l with a support of sup(s) we know that for all sequences s? developed from it with length l? > l the maximal support is maxsup(s?) = sup(s), and the bound on the z-score is given below:  BoundZscore(s?)l? = maxsup(s?)?Al?  ?l? (2)  The real z-score may be lower than the bound, but cannot be higher. Therefore by checking the potential z-score for all s? that can be developed from s, and determining that the bound on all options is lower than all ?best? z-scores, we can safely prune s. The procedure calculates BoundZscore(s?)l? for all l? > l (while l? is smaller than the maximal sequence length). If for any length l? > l we find that BoundZscore is higher than the lowest z-score in our list we keep this sequence for development, if not then we prune it. Using the bound for pruning reduces the search space while ensuring closure promising that all frequent sub-sequences are found.



IV. DISTORTION CORRECTION IN SAMPLE As we discussed the z-score measure makes use of the  average and standard deviation of support for each subse- quence length. We must calculate these values prior to the  1: for all x is prefix in S do 2: Tx = ? 3: for all atoms Ai ? S do 4: for all atoms Aj ? S, with j ? i do 5: R = Ai  ? Aj  join Ai with Aj 6: for all r ? R do 7: Zr = Zscore(r,Al, ?l) //l =length of r 8: if Zr > Zscore(a seq s in FR) then 9: FR = FR  ? r\s //replace s with r  10: for all k = l + 1 to k = maxlen do 11: Zb[k] = boundZscore(r,Ak, ?k) 12: if Zb[k] > Zscore(a seq s in FR) then 13: if Ai appears before Aj then 14: Ti = Ti  ? r  15: else 16: Tj = Tj  ? r  17: enumerate-Frequent-Seq-Zscore(Ti) 18: Ti = ?  Figure 1. Enumerate-Frequent-Seq-Zscore(S). S is the set of sequences to mine. A set of frequent subsequences is returned.

frequent subsequence mining. The naive way to calculate these values would be to generate all possible subsequences and calculate these measures. However this is obviously irrelevant as making a full expansion completely defeats the purpose of mining with the z-score pruning.

Therefore we propose extracting a small sample of the database and calculating these values on the sample. For the sample, full expansion is feasible and generates the necessary measures. However there is a problem that arises with the sampled measures. They do not reflect the full database measures correctly. It has been shown by [9] that there is a distortion in the values of support calculated on a sample of a database relative to support calculated over a full database. Similarly the average and standard deviation of support suffer a distortion in the sampled data. This distortion affects the frequent sequence mining. We propose a method for correcting the distortion.

1) Chernoff Bounds and Hoeffding Inequalities: We would like to evaluate how far off the sampled statistics are from the real statistics. One might suggest using Chernoff bounds as in [9] or Hoeffding inequalities as in [11] for this task. In both cases one wants to show how far off sampled support is from real support for a single subsequence.

The appearance of a subsequence in each sequence in the sample is described as a random variable with a Bernoulli distribution. These random variables are independent, and the Chernoff bounds or Hoeffding inequalities can be used.

In our case we are looking at the average of support for a set of itemsets (of a specific length). The random variable is now different and involves a set of subsequences that are strongly dependent. It is possible to investigate dependant variables     0.1 0.2  0.3 0.4  0.5 0.6  0.7 2 3  4 5 6 7  8 9      sample rate leng th  di st  or tio  n  (a)  0.1 0.2  0.3 0.4  0.5 0.6  0.7 2 3  4 5 6 7  8 9      sample rate len gth  di st  or tio  n (b)  0.1 0.2  0.3 0.4  0.5 0.6  0.7 2 3  4 5 6 7  8 9      sample rate len gth  di st  or tio  n  (c)  0.1 0.2  0.3 0.4  0.5 0.6  0.7 2 3  4 5 6 7  8 9      sample rate len gth  di st  or tio  n  (d)  2 3 4 5 6 7 8 9     length  di st  or tio  n  (e)  0.05 0.15 0.25 0.35 0.5    sample rate di  st or  tio n  (f)  2 3 4 5 6 7 8 9     length  di st  or tio  n  (g)  0.05 0.15 0.25 0.35 0.5     sample rate  di st  or tio  n  (h)  Figure 2. (a) Distortion ratios of average support on sampled data. (b), (c) and (d) show regression for equations 3,4 and 5. (e) Average support distortion, length cross cut. (f) Average support distortion, sample rate cross cut. (g) Standard deviation of support distortion, length cross cut.

(h) Standard deviation of support distortion, sample rate cross cut with regression curve.

and find a bound that is similar to the Chernoff bound.

However this is a difficult task and even if the equation describing this would have been discovered, an analytical calculation of the coefficient values is almost impossible and usually simulations create an estimation. This type of analysis is out of the scope of our research and in any case is expected to be too loose to be useful.

2) Distortion Correction using Non-linear Least Squares: Since theoretical methods are not helpful in estimating the sampling distortion we provide an alternative approach. Fig.

2(a) displays the distortion ratio between sampled average support to full data average support. This is displayed for many dataset for various sequence lengths and sample rates.

The distortion obviously has an orderly structure that we want to find. We modeled the distortion using non-linear regression. We used R Project for Statistical Computing [12] in order to find a general formula for calculating the correction factor. There are two correction parameters that we are looking for, one for the average support, the other for the standard deviation of support. Correction is performed by multiplying the distorted values by the correction factor.

We first describe the average support correction. We noticed that when we set the sample size, the distortion ratio follows a function of a negative power of the length, shown in Fig. 2(e). On the other hand if we set the length  then the distortion ratio follows a logarithmic function of the sample size, shown in Fig. 2(f). Therefore the distortion of average support is dependent both on length and on sample rate and we are looking for a function f(len, smp) where len is the length of a subsequence and smp is the sample rate. In order to perform the regression we must propose functions and then perform regression to set the parameters.

We tried to build a combination of the power and logarithmic functions that we saw when looking at each variable, into a two-variable function. We tried several candidate functions of this type and the best are presented in Eq. 3, 4 and 5. The regression surfaces for these functions are graphed in Fig.2 (b), (c) and (d). Non linear least of squares regression selects the following values for the parameters for Eq. 3: a=3.56, b=0.18, c= -3.43, RSE (residual standard error)=0.39, For Eq. 4: a=5.10, b=0.75, c= -0.52, d= -5.71, RSE=0.3596, and for Eq. 5: a= 3.78, b= 0.70, c= -0.46, d=-4.12, RSE=0.3391 We select Eq. 5 as it has the lowest RSE.

a? (len? 1)b?log(smp) + c? smp (3) a? (len)b?smp+c + d? smp (4)  a? (len? 1)b?smp+c + d? smp (5) a ? (smpb) (6)  For the standard deviation of support we search for the distortion correction in a similar fashion. Standard deviation distortion is linear relative to length (Fig. 2(g)), and is a power function of sample rate (Fig. 2(h)). Therefore the only variable involved is the sample rate as shown by the line going through the boxes in Fig. 2(h) and described in Eq. 6.

The regression parameters are a=0.92, b=-0.79 with RSE=2.

These equations are now used for correcting the average and standard deviation found on a sample set, for a chosen length and sample rate. This provides the corrected values used for z-score calculation in the frequent sequence mining.



V. CUBS ALGORITHMS  We now describe the full CUBS classification Algorithm integrating the sampling and frequent mining units we discussed. Two main parts compose the algorithm. The first models the users, and the second classifies an unlabeled user.

We describe building the model in Sec. V-A, and Sec. V-C presents the algorithm for classification.

A. Building the Model  The model we build uses the input sequences Xu. We choose representative subsequences Xu? from among the input sequences Xu for each user u. The are two traits we look for in good candidate subsequences. One is that they are frequent-appear often enough to be useful, the other is that they are significant-characteristic of only one user.

The Algorithm BuildModel() in Fig. 3 describes the procedure for building the model. Where n is the number     1: for all u ? 1...n do 2: Modelu ? 0 3: for all u ? 1...n do 4: Datau ? Xu broken into seq of ?size? 5: Frequ ? GetFrequent(Datau, best) 6: for all u ? 1...n do 7: Sigu ? GetSig(Frequ, F reqv, sig)?v ?= u 8: Model = Model ? Sigu 9: return Model  Figure 3. BuildModel({X1, , , Xn}, best, sig, size). Where {X1, , , Xn} are the input data sequences, best is the number of frequent subsequences to mine, sig is the number of significant subsequences to keep and size is the input window size. The function returns a Model.

of users, and Xu the input for each user. The best, sig, size parameters are defined shortly. The input to our algorithm for each user is in the form of several temporal sequences. Each temporal sequence xu represents one session of activity, there are several sessions for each user Xu = {xu}. For example for two users u and v with two sessions each:  xu1 : A,B ? C,D ? A,D ? A,B,C xu2 : A,B,C ? C ? A,D ? B,C Xu : {xu1 , xu1}  xv1 : B,C ? A,C ? B ? A,B,C xv2 : B,C ? D ? A,B,C Xv : {xv1 , xv1}  The full sessions are too long to be processed, so we break these sessions into smaller sequences. We assume that in the full session there are many short events that reoccur and represent the user. We would like to break each session into shorter sequences that correlate to these events. However we do not know where one event ends and where another begins.

We do not even know what these events are. Therefore we use a sliding window of size ?size? and create sequences from the input session covering all possible events. Using a window of ?size? = 2 on our input Xu results in Datau.

Datau:A,B ? C,D C,D ? A,D A,D ? A,B,C A,B,C ? C C ? A,D A,D ? B,C  BuildModel next finds frequent subsequences Frequ ? Datau in the input sequences for each user.

The number of frequent subsequences found is determined by the ?best? parameter, details on how this is used appear in Sec. III. The frequent sequences for our example are: Frequ: A,C A,B,C B,C  A,B B ? A B ? C Freqv : A,B,C A,B ? C A ? C A,D ? B,C  A,D A,D ? B A,D ? C A ? B,C Notice that the subsequence A,B,C in our example  appears in both sets of frequent subsequences, therefore it may not be a good separator, unless it appears very often in one group and rarely in the other. In order to clean the frequent list from sequences like A,B,C we use a ?2 test.

The ?2 test finds the frequent subsequences that are most statistically significantly different for each user in relation  to other users as described in Sec. V-B. The parameter ?sig? describes how many significant sequences to use, and is a configurable parameter. These significant subsequences Sigu are accumulated to build the model. This procedure is performed for each user and the model consists of the union of significant subsequences for all users Model =  ? Sigu.

Modelu,v : A,D ? B,C A,D A ? C A,B ? C A,D ? B A,D ? C A ? B,C B ? A,C A,C B,C A,B B ? A B ? C  The structure of the model is a list of subsequences, for each subsequence s? we specify what the ?2(s?) grade was, and for each user u the probability that this subsequence appears p(s?|u) or does not appear p(?s?|u) for this user.

B. Significant Sequences  The mining for frequent subsequences is performed as described in Sec. III. The next part of the CUBS classifica- tion algorithm selects statistically significant subsequences from the list of frequent sequences. This is performed using a ?2 test. Calculating the ?2 test on the subsequences in order to find the significant subsequences simply grades all subsequences. It is then necessary to decide how significant we want the subsequences to be, or in other words how many to use. One option is to use a threshold and select all subsequences with a ?2 value above the threshold. The problem with this technique is that different data sets have different ?2 values and it is unclear how to set this threshold.

The value is depends on the data size, each user has different number of representing sequences. Selecting a fixed number of subsequences for each user promises a fixed size model.

We simply select sig top candidates for each user.

C. Classification  The classification unit of CUBS is described by the Classify() function in Fig. 4. At this stage we have a Model and a new input data set x composed of a single session.

We want to classify x as belonging to one of the users in the Model. We break the session x into sequences to create the dataset Data, and find all possible subsequences Data? ? Data.

x : A,D ? A ? C Data : A,D ? A A ? C Data? : A,D A ? A D ? A A ? C  Next we clean the list Data? using the Model. This stage discards all subsequences that do not appear in the Model.

The reason for this is that we cannot say anything about subsequences that do not appear in the Model so they are of no use to us. In our example we discard A ? A and D ? A. Every subsequence s? ? Model that appears in the model is given a grade determined by the distance between the subsequence from the input s? ? Data? to the same subsequence s? ? Model in the model. The function that calculates the distance between two users, a user from the     1: Data ? x broken into sequences of ?size? 2: Data? ? GetSubSeq(Data) 3: f = Clean(Data?,M ) 4: for all s? ? M do 5: for all u ? 1...n do 6: distCu,X ? distCu,X + distCu,X(s?) 7: return u with minimum(distCu,X )  Figure 4. Classify(x,Model, size). Where x is the data to classify, Model is the Model to use for classification and size is the window size to use on the data. The function returns a classification.

model Cu and an unknown user C, for a subsequence s? is described in Eq. 7.

distCu,C(s ?) =  ??? ??  (p(s?|Cu)?p(s?|C))2 p(s?|Cu) s  ? ? C  (p(?s?|Cu)?p(?s?|C))2 p(?s?|Cu) s  ? /? C (7)  We calculate the ?distance? between user Cu to user C by summing distances distCu,C(s  ?) for all subsequences s? ? Cu. The user Cu for which distCu,C is minimal is the classification. It should be noted that in our distance calculation we may have to divide by 0 when a sequence appears in the model but not in a specific user. In this case we return a large constant. In the example we are using this would result in our test being classified as u.



VI. EVALUATION  In this section we present the experiments used to evaluate CUBS. We use both synthetic data and two real data sets. We demonstrate the strength of CUBS relative to SPADE and Decision Trees. We show how our pruning enables speedup.

A. Synthetic data  For our synthetic data set we used the IBM Quest Synthetic Data Generator [13]. With QUEST we generated one base class from which we derived 5 sets of classes.

The base class is composed of items {1, 2, 3, 4, 5, 6, 7, 9} without {8}. We created several classes, each has 8 added to a different percentage (0%-100%) of the sequences. For each class there is one training and five test sets. The classification of a class is the average accuracy over these five sets. An important result in Fig. 5(a) shows how CUBS accuracy improves when using z-score relative to using SPADE. Each point is an average of 5 runs using various settings for ?best? and ?sig? parameters described in Sec.

III and V-B, and over all five sets. As sequence length grows the z-score version performs at a higher accuracy because it can handle long sequences, whereas SPADE is bias towards short sequences and performance drops as sequence length grows. This graph shows how removing bias towards short sequences improves classification accuracy for long sequences. In order to evaluate the contribution of pruning using the z-score bound we ran tests on the    2 3 4 5 6 7 8 9 10  sequence length  a c  c u  ra c  y  zscore SPADE random  (a)         0 2 4 6 8  sequence length  ru n  ti m  e (  s e  c )  bound no bound  (b)  Figure 5. Synthetic: (a) Z-score vs Support. (b) Bound Contribution to Runtime  frequent sequence mining part of our algorithm described in Sec. III. We wanted to check how pruning using the bound affects both the results and the runtime. We verified that the same set of frequent subsequences is discovered with and without the use of the bound. Despite this the difference in the runtime is substantial. Results, presented in Fig. 5(b), show that the runtime for pruning with a bound is shorter than without the bound (no pruning). The difference in runtime increases as the sequence length grows. This important result demonstrates how using the z-score bound provides scalability without loss of information.

B. Zapping  The experiments we describe in this section use real world data that we gathered on remote control usage. We collected data from 30 households. In each household members were asked to identify themselves as they begin watching TV, by pressing a designated button on the remote, and then the ?zapping sequence? is saved, in other words the buttons they pressed on the remote while they were watching. This sequence is converted into the following events: Button pressed, Time passed since last activity and Time of day. We performed classification on pairs of viewers from the same household in various settings. The amount of data for each family is limited, therefore instead of setting some data aside for testing we perform 5 fold cross validation. We used half the data for learning the sampling distortion correction and use the other half for results presented here. We ran several sets of experiments on the zapping data. The first shows how using the sampled data with the distortion correction affects results. Fig. 6(a) shows results of the classification when using statistics from full DB and statistics from a sampled DB with distortion correction. We sampled 10% of the database for the sampled set. The results obtained with the sampled data after performing distortion correction are very close to results obtained using the full dataset.

Therefore we have shown that the distortion correction can be successfully used. In order to compare CUBS to state of the art classification algorithms we chose Decision Trees (DT). Fig. 6(b) displays the accuracy for CUBS relative to DT and random classification. In order to use DT for our multivariate temporal data, it must be preprocessed, since     0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8  1 2 3 4  size  a c  c u  ra c  y  sampled full  (a)  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8  Descision Tree CUBS random  a c  c u  ra c  y  (b)  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8  1 2 3 4  size  a c  c u  ra c  y  best=200 best=100 best=50  (c)  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8  1 2 3 4  size  a c  c u  ra c  y sig =100 sig =50 sig =20 sig =10  (d)          0 5 10 15  sequence length  ru n  ti m  e (  s e  c )  bound no bound  (e)    3 6 9 12  sequence length  a c  c u  ra c  y  CUBS random  (f)  Figure 6. (a) Zapping: Effect of Using Sampled Statistics. (b) Zap- ping: Classification. (c) Zapping: ?sig?=20, various ?best?. (d) Zapping: ?best?=100, various ?sig?. (e) Zapping: Bound Contribution to Runtime. (f) UPD: Accuracy vs. Size.

DT cannot handle the temporal sequential nature of the data.

We adapted the data so that the time dimension is removed and each vector is composed of one itemset. So one sequence of n itemsets is broken into n inputs for DT. CUBS achives a 75% accuracy rate and performs slightly better than DTs with 71% accuracy.

We explored the way various parameter settings affect the accuracy rates of CUBS. For all CUBS results we sample 10% of the data at the sampling stage. In Fig. 6(c) we set ?sig? = 20 with various ?best? values. Using higher ?best? values, meaning more frequent subsequence candidates usu- ally generates better results. Fig. 6(d) complements this by setting ?best? = 100 and varying ?sig?. Another important result is evaluating the contribution of the bound on z-scores.

Fig. 6(e) displays the results. These results strengthen the results on the synthetic data and show again how the use of the bound decreases runtime, and how this becomes more important as sequence size grows.

C. UPD  UPD (User Pattern Detection), another dataset we used for evaluation, is also based on real world data. UPD logs keyboard and mouse activity of users on a computer. It can be used to provide security on a computer system by  validating that the user currently working on the computer is the one who is logged on. The data is collected throughout the whole work session and not just at login. Each activity is logged along with the time and date it occurs. The data is then converted into the following events: pressing a key, time between key presses, key-name, mouse click, mouse double click, time between mouse movements. For each session the events are saved in sequences. Our experiments are run on 15 pairs of users. The length of sessions varies between users. For each user we use 9 sessions for training and 1 for test. The results for the UPD data are described in Fig.

6(f). These results show how the improved frequent mining algorithm incorporated in the CUBS classification performs on real data. The ?best? parameter is set to 50, and ?sig? is set to 10. The best accuracy of 83% is achieved using a window ?size? of 1, correlating to a length of 3, meaning one itemset with 3 items.



VII. RELATED WORK  Multivariate time series are characterized by time series data that have several attributes. Multivariate time series classification can be done by using metafeatures to convert each series into several features as in [5]. However, this is impossible to do if the structure of the behavior or underlying patterns is unknown. Several algorithms handle multivariate sequential data by looking at a full timeseries as one unit such as Gwadera and Crestani [14] who discover significant patterns in multi-stream sequences, and Liu et al. in [3] who perform causal analysis to find how one timeseries infers another. This structure does not assist in the search for dependencies that occur among segments of the various timeseries. A different approach is taken by Silvescu et al. [15]. They substitute feature selection by mining k-grams of sequences and using them as representatives.

However k-grams are composed of k items that appear together, this algorithm does not allow for the breaking of itemsets as we do.

One of the issues we chose to address was the bias towards short sequences as shown by [6]. This bias appears because the itemset mining that creates the subsequences is based on support. Short subsequences have a higher support than long subsequences and are chosen as frequent more often, although long subsequences may be more important and less random. This issue has been addressed by Seno and Karypis in [7], where length decreasing support constraints are used in the itemset mining in order to overcome the short sequence bias. These heuristic solutions are based on the assumption that short subsequences will be interesting when they have a very high support and long subsequences may be interesting even if they have a low support. This may not always be a correct assumption. An alternative approach is taken by Yun and Legget in WSpan [16]. They introduce a weighted mining algorithm, for sequences with weighted items. Unfortunately this is of no assistance to our     domains where these weights are unknown. In contrast, we introduce a statistical normalization that allows comparison of the frequency for subsequences with different lengths in an unbiased fashion. We make no assumptions on the relation between length to support, or on the relative weights of the items in a database.

We applied CUBS to two domains. The first involves identifying the viewer watching TV. [2] uses remote zapping data to determine if someone is watching the TV, but do not perform user identification. The second domain we use is computer user identification based on mouse and keyboard activity. Some identification algorithms use either mouse [1] or keyboard [17] data. Our setting uses data from multiple sources while a user works providing a broader and more secure solution.



VIII. CONCLUSION  This paper presents the innovative CUBS algorithm.

CUBS provides a much needed framework for classification of multivariate temporal sequences. In CUBS we mine the sequential multivariate data for frequent subsequences and then select the statistically significant candidates and use them for classification. We have demonstrated the successful classification ability on both synthetic and real data. CUBS is equipped with a novel algorithm for frequent sequence mining that introduces the use of a z-score normalization on support. Replacing the commonly used support measure with this z-score normalization shows evidence of fixing the bias towards short sequences in the mining. Fixing this bias significantly improves accuracy of classification.

We presented a bound on the z-score normalization that allows pruning of the frequent sequence search space while providing closure. We demonstrated both on synthetic data and on the zapping data how this enables speedup of the mining process.

In order to calculate the z-score normalization, statistics must be collected over the database. As using the full database is irrelevant, statistics are collected from a sam- ple of the database. These sampled statistics are distorted relative to the real statistics. We presented a method that succeeds in correcting this distortion, thus enabling the use of the sampled data.

Combining the z-score normalization with the pruning, based on the z-score bound, along with our novel sampling unit provides an improved, unbias and scalable algorithm for mining frequent multivariate subsequences.

