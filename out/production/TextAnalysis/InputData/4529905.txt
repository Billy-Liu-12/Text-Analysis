A Chain of Text-mining to Extract Information in  Archaeology

Abstract? In this paper, we describe a Text-Mining system used to extract archaeological knowledge from specialized texts. Our approach relies on two basic principals. First, we use user- friendly tools enabling experts to transfer easily their knowledge.

Second, our tools include inductive algorithms to reduce the experts? workload.

Electronic Archaeology; Text-mining; Terminology; Information Extraction

I. INTRODUCTION : In this paper, we present a project that aims to extract  knowledge by information extraction process from the now huge amount of electronic literature: it is thus relative to text- mining. Completion of archaeological knowledge would help archaeologists to elucidate some questions and infer archaeological assumptions to discover new features and relations in various domains of Archaeology/Cultural Heritage.

With this intention, we will use and develop an original approach for a complete software chain of semi-automatic information extraction from texts. This chain  includes the following steps: Gathering the corpus, text standardization, Part-of-Speech (PoS) tagging, co-reference resolution, terminology extraction, classification of concepts, establishment of extraction patterns, application of association rules, and validation of extracted information.

The effective processing of technical texts requires the co- operation of field experts in order to improve the way to simulate understanding of the text content. The development of interactive visualization tools including inductive modules will make it possible to facilitate the work of the expert and limit the intervention of data processing specialists. For each step, such software is currently developed and this software will be completed in the course in this project. The validation of the text processing system thus built can be done only by the effective use of extracted information. The developed methods of extraction are the extraction of patterns and of association rules.

This project can fit (like a module) in our global project e- archaeology+. It falls under a will of cooperation between various disciplines for better apprehending the complexity of the subject in its whole. With this intention, it will bring together experts of the field (archaeologists/Museum specialists), computer engineers and Data-Mining specialists.

The main objectives of the project are: extracting information on Archaeology from e-literature and modelling knowledge from large data on archaeology (e-archaeology+).



II. SCIENTIFIC CONTENT AND METHOLOGY This work concerns the theoretical and practical challenges  carried by the automatic exploitation of an abundant scientific literature and in particular i) the extraction of information from full-length texts and ii) integration of this information in archaeological models.

The volume of published archaeological research, and therefore the underlying archaeological knowledge base, is expanding at an increasing rate. The abundance of literature motivates an intensive pursuit for effective text-mining tools.

Such tools are expected to help to uncover the information contained in large and unstructured bodies of text. Text mining is a process of data analysis that leads to the discovery of heretofore unknown information, or to answer questions the answer of which is not currently known [1].

Significant progress has been made in applying text mining to named entity recognition, text classification, terminology extraction, relationship extraction, and hypothesis generation.

Several research groups are constructing integrated flexible text mining systems to find the nuggets of information most relevant and useful for specific analysis tasks.

In order to extract knowledge from archaeology texts, we use the following methodology [2,3,4]; several related steps are used: text standardization, lexicon building, PoS tagging, terminology extraction, co-reference resolution, detection of user-defined concepts, information extraction and knowledge discovery. Our text mining tools are adapted to technical texts.

The whole chain of treatment we deal with is shown in the ?Fig. 1?.

Figure 1.  Our global chain of text processing in view of text mining

III. STANDARDIZATION Standardization is also often called text cleaning. This step  is extremely tedious and requires a strong man-machine interaction. It consists of several types of treatments highly dependent upon the specialty and the type of information to extract. A detailed description of the task of standardization is given in [5]. The goal of this task is to limit the errors in the following steps of the text processing and to prepare the ultimate step of knowledge extraction by reducing the complexity of the used vocabulary. The standardization includes the following operations:  ? Recognizing a sentence as a textual entity. This task consists in identifying the sentence elements within a text. A key element for this task is determining the role of the dot. Indeed, in addition to ending a sentence, the dot can have several roles. It can be used in an abbreviation, for example Mr. and Dr., or in a decimal number.

?  Suppression of not useful parts of a text.

? Correction of orthographic mistakes.

? Acronym acquisition and disambiguation.

? Reduction of synonymy. For example: The most frequent form is chosen between co-direction and codirection.

Standardization is obviously specific to each domain and only the expert is able to decide what is to keep, transform or delete.



IV. PART-OF-SPEECH TAGGING The next step is relative to the PoS tagging (grammatical  tagging). PoS tagging consists in associating each word with its grammatical tag, according to its morphology and context.

There are two main approaches for PoS tagging: the linguistic and the data-driven.

The linguistic approach consists of coding the necessary knowledge in a set of rules written by a linguist. Although the linguistic approach produces high quality taggers, it is a time consuming one.

In a data-driven approach, a machine learning method is applied to learn a tagging model from an annotated corpus.

Several data-driven approaches have been applied to PoS tagging. Among them, Inductive Logic  Programming [6], Transformation Based Learning [7], and decision trees learning [8]. Whatever the technology on which they are based, the data-driven taggers obtain very satisfactory results. However, the construction of these taggers requires the availability of large annotated corpus. Acquiring such a corpus is expensive and time consuming and it is often the bottleneck to build a tagger for a new domain.

Most of the available taggers are learned from general corpora. Their good performance can be explained because test corpora are of similar type to training corpora. The problem is that the accuracy of general taggers drops down dramatically when applied to specialized corpora and high quality taggers are not available in specialized domains such as biology or archeology.

The solution we propose is to adapt a general tagger to a specialized corpus [9]. We thus use Brill?s tagger as the starting point of our system. We apply the ordered list of Brill?s rules to our corpus. Then our tagging software, ETIQ, enables the expert to display the results of Brill's tagging and allows him to add specialized rules in a simple and interactive way.

The strategy we propose is described as follows:    In a preliminary stage, we supplement Brill's lexicon by a specialized lexicon, i.e., a list of specialized words, where each one is followed by its possible tags ?Tab. 1?. Then, we apply Brill's lexical rules. To help the expert to detect the tagging errors, the system ETIQ gives him the possibility of visualizing groups of words of similar morphological features, and their tags ?Fig. 2?. Reacting to the detected errors, the expert inserts adequate lexical rules to correct these errors. We present below an example of lexical rule:  Assign the ?adjective? tag to the words having suffix ?al?.

When the expert estimates that the tagging carried out during lexical stage brings no more improvement, contextual rules may be used. These rules change word tags according to the word and its context (i.e., the word form, its tag, and neighboring words and their tags). Similarly to the lexical module, the expert can, in an interactive way, look for words, according to their forms, tags, and morphological criteria. This enables to visualize the contexts and to detect the errors. The expert can thus correct these errors by inserting specialized contextual rules.

We observe that it is sometimes very difficult to write down correction rules because the expert is unable to take into account all the possible exceptions. We thus propose the following strategy [10]: Using our friendly user interface, the expert annotates examples containing the ambiguous words. He will assign to the target word of each example (or group of examples) the corresponding PoS tag. These annotated examples allow learning automatically correction rules by using the propositional rule induction algorithms PART [11] and RIPPER [12]. The obtained rules are then converted in the ETIQ format and are inserted at the end of the contextual rule list. We present below an example of an induced correction rule:  If the current word is a plural noun and the precedent word tag does not belong to any of the groups Noun and Verb, and the next word is a singular noun then the current word must be tagged VBZ (Verb, 3rd person singular present).

In order to reduce the number of examples to annotate, we can use an active learning strategy [13]. Indeed, by using active learning [14,15], the learning system selects which examples to annotate, instead of asking blindly the human to annotate the whole set of examples.

TABLE I.  EXAMPLES OF WORDS EXTRACTED FROM THE SPECIALIZED LEXICON  Word  Part-of-Speech tag ceramic  NN (Noun) Islamic JJ (Adjective) Serail NNP (Proper noun) archaeological JJ Hellenistic JJ     Figure 2.  ETIQ: Example of visualizing words and their tags.



V. TERMINOLOGY EXTRACTION Once a correct PoS tagging is obtained, we are able to  extract doublets or triplets of successive words, called collocations. The PoS tagging is an important step, since we used grammatical patterns to extract the collocations. The collocations correspond to a specific PoS-tag series: Noun- Preposition-Noun, Noun-Noun, Adjective-Noun. The relevant collocations, i.e. collocations which are instances of a concept, are called ?terms?.

We use the EXIT software [16] that iteratively extracts complex terms. The EXIT system uses an iterative approach to extract complex terms (for example, detailed studies of Islamic levels and study of Islamic period settlement) with binary or ternary terms (for example, Islamic levels and Islamic period settlement). The EXIT system is also based on statistical measures to order extracted collocations. Many statistical criteria exist to order collocations [17,18].

The reduction of errors in PoS-tagging will have a direct effect on the quality of the terminology [10]. For example, if a noun is taken for a verb, the term that contains this noun will be omitted, and conversely, if a verb is taken for a noun, a term containing this verb will be extracted in an automatic way though it is not relevant.



VI. CONCEPT RECOGNITION Once the terminology is completed, we use it to recognize  the occurrences of concept in a text, by clustering the terms into classes, each term being a linguistic instance of a concept.

We are currently building a system (ACT) [3,19] of concept recognition in texts using the terms, the words, their tags and the context of each word or term. In our system, a syntactic relation among words may be also an instance of a concept.

The concept definition is entirely in the hands of the field expert. This process is domain specific. The expert defines    what the interesting concepts are ?Tab. 2?. Then, he associates the concept instances to their appropriate concept.

TABLE II.  EXAMPLES OF FIELD CONCEPTS DEFINED BY THE EXPERT  Field concepts CONSTRUCTION  PERIOD SITE  METHOD SOIL    The next step in the process of concept categorization is the inductive phase. In this phase, the existing categorization of concepts is supplemented automatically [20]. Once the expert has provided the set of interesting concepts, and as many as possible instances of these concepts, the induction algorithm will supplement automatically this categorization by adding new instances for each concept.



VII. INFORMATION EXTRACTION After concept categorization, we are able to extract  information from the corpus. In addition to conceptual classification, the extraction module requires, as input, a PoS tagged corpus including the terminology and field entities (for example, different categories of pottery).

Information Extraction methods are used to identify relations between types of objects. The goal of these methods is to transfer knowledge from unstructured data, the literature, to a structured form that can be included in a database or knowledge base.

Our information extraction system, EXTRACT, helps experts to extract information in convivial and interactive way [21]. The expert can make requests using a combination of words, terms and/or concepts. The goal of the requests is to find relevant co-occurrences of concepts in sentences, and thus to discover relations between concepts. The requests are based on the potential instances of concepts and the entities connecting the concepts (verbs, coordinating conjunction,?etc.). To define requests the expert use a dedicated language, which facilitates the writing of expressive rules by combining conditions using the logical operators and, or, and not. To express conditions, we can use the following linguistic constraints:  ? PoS tags (for example: plural noun, adjective...etc.)  ? Groups of PoS tags. (for example, the group of nouns contains the tags: singular Nouns, plural nouns, and singular Proper nouns)  ? Morphological features based on regular expressions (For example, concept instances ending with the suffix al). The regular expressions are pre-coded, the expert can insert them easily via a dedicated interface.

? The membership of the concept instance to a particular concept. This constraint is established by using a conceptual classification ?Tab. 3?.

By using our Information Extraction tool, the expert can, in an interactive way, apply a request to look for co-occurrences of concept instances in the sentences. This enables him to visualize the request results and he can thus improve it in a simple and interactive way.

TABLE III.  EXTRACT OF CONCEPTUAL CLASSIFICATION.  EACH TERM IS ASSOCIATED TO A CONCEPT.

Terms Concepts Grand_Serail CONSTRUCTION Mamluk_monument  CONSTRUCTION Mamluk_ribat CONSTRUCTION Roman_buildings  CONSTRUCTION acropolis-like_hill CONSTRUCTION main_streets CONSTRUCTION port  CONSTRUCTION Hellinistic PERIOD Islamic_levels  PERIOD Islamic_period_settlement  PERIOD Roman_origin  PERIOD early_Abbasid_times  PERIOD Rue_Allenby  SITE Rue_Trablous  SITE Rue_Weygand  SITE Souk_Tawileh  SITE Souk_al-Jamil  SITE Souks_area SITE area in early_Abbasid_times  SITE archeological_investigations  WORK archeological_studies  WORK studies_of_Islamic_levels  WORK excavation WORK study_of_Islamic_period_settlement WORK limestone_bedrock  SOIL limestone_promontory  SITE line_of_the city_wall  SITE Pottery  OBJECT Ceramic_ware_type_series  OBJECT urban_excavation METHOD sampling_methodologies METHOD

VIII. KNOWLEDGE DISCOVERY We can also perform knowledge discovery. The definition  of the concepts makes it possible to rewrite the corpus in a more compact way by replacing all different occurrences of a concept by the name of this concept [3]. Then we search association rules [22] between concepts. The discovered knowledge will have the form conceptA  conceptB. After that, experts will validate the knowledge obtained.

Instead of seeking information with key words like it is usually done, we search for precise information (for instance, "there is a weak interaction between object A and object B").

This information is collected by finite-state machines built according to the questions of the researchers. In fact, considering the diversity of the language, to answer only one question, one needs thousands of finite-state machines, which is currently impossible to realize, whereas our tools summarize "intelligently" these thousands of automats in a very small number. The validation will be done on two levels. The first level of validation will consist in a direct expertise on the quality of extracted information which will result in an    effective use into phase of constitution of the archaeology knowledge. The second level of validation will consist in automatically integrating the information extracted by the software chain we developed, then to appraise the quality of the results so obtained.

CONCLUSION AND FUTURE WORK Knowledge and information extraction from archaeological  texts requires the application of a complete process of text mining. Our text mining process is composed of the following steps: Standardization, PoS tagging, terminology extraction, concept recognition, information extraction and knowledge discovery. The quality of each step depends strongly on the preceding steps.

The processing of specialized texts requires the co- operation of field experts and thus the use of convivial software allowing an effective work. The use of inductive methods is an important aspect in our text mining process. Indeed, for each step of the process, a specific inductive method is applied.

Within the framework of the e-archaeology+ project, we will continue to adapt and to improve the treatments carried out in each step.

For the standardization, we will develop an interactive software which would make it possible to the expert to apply standardization algorithms, to visualize their effects, and to insert easily correction rules.

For PoS tagging, the first task will be the acquisition of a specialized lexicon starting from the corpus and external resources. Then, the specialized rules will be acquired by using our tagging software ETIQ.

In our system, the concept instances may be terms or syntactic relationships. The extraction of syntactic relationships will be improved by using our PoS tagger results as input of the syntactic parser.

The process of concept categorization is a important subtask in information extraction. We will continue to improve our inductive, convivial and cooperative method to effectively include the expert in this process.

The validation of the data processing system can be done by the effective use of the information extracted from texts.

