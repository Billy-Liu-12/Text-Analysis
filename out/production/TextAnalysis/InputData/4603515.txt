Hierarchical Document Clustering Using Fuzzy Association Rule Mining

Abstract In this paper, we will present an effective  Fuzzy Frequent Itemset-Based Hierarchical Clustering (F  IHC) approach, which uses fuzzy  frequent itemsets discovered by fuzzy association rule mining to improve the clustering accuracy of FIHC (Frequent Itemset-Based Hierarchical Clustering) method. Our approach can alleviate the deficiencies of most of the traditional document clustering methods in dealing with the problems of high dimensionality, large data size, and meaningful cluster labels. We have conducted experiments to evaluate our approach on Reuters-21578 dataset. The experimental results show that our approach not only absolutely retains the merits of FIHC, but also improves the document clustering accuracy quality as compared with the FIHC method.

1. Introduction  To browse documents or organize the retrieval results smoothly, hierarchical clustering technique is proposed to cluster a collection of documents into a hierarchical tree structure.

Despite that, there still exist several challenges for hierarchical document clustering, such as high dimensionality, large volume of data, and meaningful cluster labels [2][4]. Fortunately, Fung et al. [2] proposed a novel approach, namely Frequent Itemset-based Hierarchical Clustering (FIHC), to produce a hierarchical topic tree for document clustering.

For improving the accuracy of the clustering task, it is important to find out appropriate cluster labels to cluster documents. Therefore, we propose a fuzzy association rule mining algorithm for documents, which modifies the method devised by Hong et al. [11], but it regards a document as a transaction, and those term frequency values in a document as the quantitative values (i.e., the number of purchased items in a transaction). By employing predefined membership functions, our approach calculates three fuzzy values, i.e., Low, Mid, and High regions, for each term based on its frequency to discriminate the degree of importance of each term within the document set  in the mining process. The fuzzy frequent itemsets, which are regarded as the candidate clusters, will be derived by the fuzzy association rule mining.

The objective of our effective Fuzzy Frequent Itemset-Based Hierarchical Clustering (F2IHC) approach is ameliorating the document clustering accuracy quality of FIHC. Our approach also retains FIHC?s advantages of reducing the high dimensionality of text, efficiency and scalability for large document sets, and a meaningful description of the discovered clusters organized into a hierarchical cluster tree.

Our paper is organized as follows. In Section 2, we briefly review some related works. Section 3 presents our approach in three stages.

Experimental results are presented and analyzed in Section 4. Finally, we conclude in Section 5.

2. Literature Review  Because text data are inherently unstructured and fuzzy [1], some researches [6][8][10] applied the technique of association rule mining in document management. For example, Delgoado et al. [6] think that association rule mining is the first data mining techniques employed in mining text collections. It is very interesting since many applications related to text processing involve associations and co-occurrences between texts. Feldman and Dagan [8] had presented a Knowledge Discovery in Text (KDT) system, which used the simplest information extraction to get interesting information and knowledge from unstructured text collections. Moreover, Lin et al.

[10] proposed a method, namely Mining Term Association, to acquire the semantic relations between terms when applying to documents.

These works mainly focused on analyzing the co-occurrence terms for document management.

In our work, we intend to focus on employing fuzzy association rule mining to find the association relationships between terms for clustering textual documents.

Association rule mining techniques can also be employed for document classification. Liu et  The 3rd Intetnational Conference on Innovative Computing Information and Control (ICICIC'08)    al. [3] proposed a method to integrate classification rule mining and association rule mining to build an effective classifier from frequent itemsets of a database. Then, Zaiane et al. [7] presented a modification of this approach, namely Association Rule-based Classifier By Categories (ARC-BC), for the purpose of text classification.

For document clustering, Fung et al. [2] used frequent itemsets derived from association rule mining to cluster documents. Then, a hierarchical topic tree will be constructed from the clusters. They also proved that using frequent itemsets for document clustering can reduce the dimension of a vector space effectively. Their experiments had shown that clustering with reduced dimensionality is more efficient and scalable. Therefore, this approach not only reduces dimensionality, but also offers efficient processing of high volume data, supports ease of browsing, and provides meaningful cluster labels. In the following, we will propose a method stem from FIHC approach of Fung et al. [2], together with a modification of the algorithm developed by Hong et al. [11], to find fuzzy frequent itemsets for constructing hierarchical clusters. The mined fuzzy frequent itemsets are expressed as cluster labels for their description.

3. F IHC Algorithm  There are three stages in our framework as shown in Fig. 1. We explain them as follows:  Figure 1: The framework of F2IHC  1. In the first stage, the key terms will be retrieved from the document set for removing noise, and each document is pre-processed into the designated representation for the following mining process.

2. In the second stage, a fuzzy association rule mining algorithm is employed to discover a set of highly relevant fuzzy frequent itemsets, which contains key terms to be regarded as the labels of candidate clusters.

3. In the final stage, the documents will be clustered into a hierarchical cluster tree based  on these candidate clusters. The obtained hierarchical cluster tree with meaningful cluster descriptions can offer users a more flexible ability in document management.

3.1 Stage 1: Document Pre-processing  This stage devotes to the transformation processes of documents to obtain the desired representation of documents. Due to there are thousands of words in a document set, the purpose of this stage is to reduce dimensionality for high clustering accuracy. The representation of a document can be derived by Algorithm 3.1 and explained as follows.

Algorithm 3.1:  Input: a document set D = {d1, d2,?, di,?, dn};     a pre-defined minimum frequency threshold ?; Output: The key term set of D; The representation of all documents in D; 1. Extract the term set T = {t1, t2,?, tj,?, ts} of D.

2. Remove all stop words from T.

3. Apply Word Stemming for T.

4. Count the frequency of each term tj, and remove the terms with frequencies less than ?, from T. The final result obtained will be the key term set K = {t1, t2,?, tj,?, tm} of D.

5.For each tj ? K, count its frequency in di to obtain di = {(t1, fi1), (t2, fi2),?, (tj, fij),?, (tm, fim)}.

3.2 Stage 2: Candidate Clusters Extraction  In the following, we define the membership functions, and present the fuzzy association rule mining algorithm for text.

3.2.1 The Definition of Membership Functions  Each fuzzy value r ij  w  has a corresponding membership function, denoted ( )r  ij ij w f , to  convert the key tem frequency fij into a value in [0, 1], where r can be Low, Mid, and High. For example, the sample membership functions can be shown in Fig. 2, where min(fi) is the minimum frequency of terms in the document set D, max(fi) is the maximum frequency of terms in D, and avg(fi) = ?(min(fi) + max(fi))/2?.

Figure 2: The predefined membership functions  3.2.2 The Algorithm  For the generated text transactions, we apply Algorithm 3.2 to generate the fuzzy frequent 1-itemsets based on the minimum support ?, then, use the minimum confidence ? to produce the target fuzzy k-frequent itemsets, where k > 1, and finally regards them as the candidate cluster set for output.

The 3rd Intetnational Conference on Innovative Computing Information and Control (ICICIC'08)    Algorithm 3.2:  Input: a set of documents D = {d1, d2,?, dn}, where di=  {(t1, fi1), (t2, fi2),?, (tj, fij),?, (tm, fim)};  a set of membership functions; two threshold values of  minsupp ? and minconf ?;  Output: Generate a candidate cluster set C% 1. Use the given membership functions to transform  each term frequency fij into fuzzy values in the designated three fuzzy regions.

2. Construct the fuzzy frequent itemsets from the set of documents D.

3. Find all the fuzzy frequent itemsets that have a support value above the threshold ?.

4. Hold those rules which the confidence values of the rule pair are all larger than or equal to the predefined confidence value ?.

5. Output the fuzzy frequent 1-itemsets and fuzzy frequent k-itemsets, where k > 1, derived in Step 4 to form the candidate cluster Set  1 2{ }l kC = c , c , , c , , c % % % % %K K .

3.3 Stage 3: The Cluster Tree Construction  The candidate cluster set generated by the previous steps can be viewed as a set of topics (ex. fuzzy frequent 1-itemsets) with their corresponding sub-topics (ex. fuzzy frequent k-itemsets) in the document set.

In this stage, we first construct the Document-Term Matrix (DTM) and the Term-Cluster Matrix (TCM) to form the Document-Cluster matrix (DCM) for assigning each document to a fitting cluster. Based on the assignment result, we will find the target clusters in this section, and then use these target clusters to form a hierarchical tree for the document set D. To avoid including too many clusters in the constructed cluster tree, two methods, namely child pruning and sibling merging, will be used to prune unnecessary clusters.

3.3.1 Building the Document-Cluster Matrix  To represent the degree of importance of a document di in a candidate cluster lc% , an n ? k DCM will be constructed to calculate the similarity of terms in di and lc% . A formal illustration of DCM can be found in Fig. 3.

11 12 11  2 21 22 2  1 2  DCM                                       DTM                                       TCM          k  k  n n n nk  v v vd  d v v v  d v v v  ? ? ? ? ? ? ?  K  K  M M M O M  K   21 22 2 22   1 2 1 21 2 max  1 1 max max max max  2 2  max            j  j j j j  m  j  m  mk R  R R R R  R n m  c ct t tc c c gd t  d tw w w g  d t g n k n m  ?  ? ? ? ?  ?  ? ?? ? ? ?? ?? ? ?? ?? = ? ? ?? ?? ? ?? ?? ? ?? ?? ? ? ? ?? ?? ?  % % K% % %K  L  M M M   k  c  m k?  %L  Figure 3: Document-Cluster Matrix  3.3.2 Building the Hierarchical Cluster Tree  Based on the obtained DCM, we employ  algorithm 3.3 to build the cluster tree for assigning each document to the target cluster.

Algorithm 3.3:  Input: a document set D = {d1, d2,?, di,?, dn}; the key term set K = {t1, t2,?, tj,?, tm} of D; the candidate cluster set  1 2{ }l kC = c , c , , c , , c % % % % %K K  of  D; a minimum Inter-Sim threshold ?;  Output: a cluster tree 1 2{ }l fC T =  c , c , , c , , c ) ) ) )  K K   1. Build n ? m Document-Term Matrix W.

2. Build m ? k Term-Cluster Matrix G.

3. Build n ? k Document-Cluster Matrix V  W G = ? .

4. Based on V, assign di to its target cluster ci  { }1 2 = | = { , , ..., }i i il i i ikc d v max v v v 5. Apply the Tree construction.

6. Apply child pruning until the Inter_Sim of ci with  each of its child target clusters is less than ?.

7. Apply sibling merging until the Inter_Sim of each  pair of target clusters at level 1 is less than ?.

8. Output the cluster tree CT.

4. Experiments  We have experimentally evaluated the performance of the proposed algorithm by comparing with that of the FIHC method. We make use of the FIHC 1.0 tool [5] to generate the results of FIHC. The produced results are then fetched into the same evaluation program to ensure a fair comparison.

4.1 Datasets  We used two standard datasets in our experiments, and the detailed information of these datasets is described as follow: ? Re0: Re0 is a text document dataset, derived from Reuters-21578 text categorization test collection Distribution 1.0 [1]. Re0 includes 1,504 documents belonging to 13 different classes.

? Reuters: This document set is extracted from newspaper articles [1]. There are twenty thousands of documents that are different in length, over 135 categories of documents, and each document has 200 words in average. The documents are divided into 135 topics mostly concerning business and economy. In our test, we discarded documents with multiple category labels.

Table 1 indicates the statistic information of these datasets.

Table 1: Statistics for our test datasets  Data Set  Number of Documents  Number of Classes  Class Size  Number of Terms (Nouns)  Re0 1504 13 11 - 608 2886 Reuters 8649 65 1 - 3725 16641  4.2 Performance Evaluation by F-Measures  The F-measure is often employed to evaluate the accuracy of the generated clustering results.

Hence, we define a set of document clusters  The 3rd Intetnational Conference on Innovative Computing Information and Control (ICICIC'08)    generated from our approach, denote C, and another set is natural clusters, denoted C?, which each documents is pre-defined into a single cluster. The two sets of clusters are both derived from the same document set D. We use doc(cj) to denote the number of documents in cluster cj.

Moreover, doc(c?i, cj) is used to denote the number documents both in a natural cluster c?i and a generated cluster cj. Then, the F-measure F(c?i, cj) is defined as:  ' ' '  ' '  ' ' ' '  '  2 ( , ) ( , ) ( , ) ,  ( , ) ( , )  ( , ) ( , ) where ( , )  and ( , )  ( ) ( )  i j i j  i j  i j i j  i j i j  i j i j  i j  r c  c P c  c F c  c  r c  c P c  c  doc c  c doc c  c r c  c  p c  c  doc c doc c  ? ? =  +  = =    Fung et al. [2] have measured the quality of a clustering result C using the weighted sum of such maximum F-measures for all natural classes according the class size. Let |D| denote the number of all documents in the documents set D. This measure is called the overall F-measure of C, denote  '  '  '  ( ) ( ) max { ( , )}i  i j  i  j c C  c C  doc c F C F c c  D ?? = ?  In general, the higher the F(C) values, the better the clustering solution is.

4.3 Evaluation Results  Table 2 presents the obtained F-Measure values for F2IHC and FIHC algorithms with different user-specified numbers of clusters. Our algorithm, F2IHC, outperforms FIHC in terms of accuracy, specifically on the Reuters dataset.

Table 2: Comparison of the F-Measure  Data Set  (# of natural classes)  # of  Clusters  F2IHC FIHC  5 0.361 * 0.351 15 0.354  0.390 30 0.398 * 0.391 60 0.484 * 0.388  Re0 (13)  Average 0.40 * 0.38 5 0.494 * 0.362 15 0.494 * 0.411 30 0.495 * 0.419 60 0.509 * 0.426  Reuters (65)  Average 0.498 * 0.405  5. Conclusions  Although numerous interesting document clustering methods have been extensively studied for many years, the high computation complexity and space need still make the clustering methods that calculate document-to-document similarity inefficient.

Hence, reducing the heavy computational load and increasing the accuracy quality of the unsupervised clustering of documents are important issues. In this paper, we derived a  fuzzy-based hierarchical document clustering approach, based on fuzzy association rule mining, to alleviate these problems. In the total process, we begin with the process of document pre-processing; then combine the fuzzy association data mining method in second stage; automatically generate the candidate cluster set, and merge the highly similar clusters, and finally build a hierarchical cluster tree for easy document browsing. Our experiments show that the proposed algorithm has better accuracy quality than FIHC method compared on Reuters-21578 dataset.

Acknowledgements: This research was partially supported by National  Science Council, TAIWAN, ROC, under Contract No.

NSC 96-2416-H-327-008-MY2 and No. NSC  96-2221-E-009-168-MY2.

