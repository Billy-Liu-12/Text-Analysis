Arti Deshpande

Abstract? In Association Rule Mining, minimum support threshold is used to get the association rules. Deciding this threshold is quite a difficult task and has a great influence on the number and the quality of association rules. There is no chance of neglecting minimum support threshold as the large number of association rules generated missed some interesting rules discovered. The process of decision making with these rules may lead to undesirable and unexpected results. Minimum support threshold thus played a vital role in the entire process. To remove this dependency on minimum support threshold, we have proposed a framework which contains domain knowledge method,  feature selection method and pruning technique to reduce the complexity of  coherent algorithm to discover interesting positive and negative rules for  business which are discovered based on the properties of propositional logic and thus do not require the minimum support threshold. In the initial part of the paper, we have explained the formation of coherent rules.

Later, to reduce the complexity and make it more efficient we have added the feature of domain-driven to the framework of coherent rules and this feature is demonstrated with the help of implemented example. Further we have also introduced the concept of Combined Rule Mining which further enhances the results generated.

Keywords - Association rules, Domain Driven Data Mining, Combined Mining, Frequent Patterns and Propositional Logic

I. INTRODUCTION Association rule learning is a popular and well researched method for finding interesting relations between   variables in large databases. With increase in the amount of data being collected and stored in databases, there has been increase in the demand of discovering correlation among the data with the help of association rules. It aims to identify strong rules discovered in databases using different measures of interestingness. Association rules find their key applications in the process of making decisions about marketing activities such as planning promotional schemes. A popular    example  of association rule mining is ?Market Basket Analysis? and it is illustrated with the help of a diagram given below:    Fig. 1.  Market Basket Analysis  Analysis are made on customer buying habits by discovering associations between the different items that       customers place in their shopping baskets as illustrated in Figure 1.

Retailers find assistance in developing marketing strategies by identifying which items are frequently purchased together by customers with discovery of such associations. For instance, from the analysis done of the buying trends of the customers as shown in Figure 1, organizing promotional schemes for selling milk and bread together may further encourage the sale of these items together within single visits to the store.

Traditional data mining process have problems such as less repeatability, no particular interest to business and lack of end user understandability. Thus, it lacks soft power in solving real-world   complex problems [4]. Domain-driven data mining is a new paradigm shift. It is aimed at making better business solution by providing tools for   actionable knowledge which is passed on to the business committee for direct decision making and taking  appropriate action. Thus in general terms we can describe it to be a shift from data centered hidden pattern mining to domain driven actionable knowledge discovery [3]. Further, positive and negative association rules are quite important and necessary for decision making process and for     analyzing and predicting business trends. It aims at maximizing possible benefits and minimizing the harmful impacts involved in applications like product placement and investment analysis [6]. This technique  Domain Driven Approach for Coherent Rule Mining     describes the importance of negative association rules such as X Y in decision  making because  X Y can tell us that Y (which may be a harmful factor) rarely occurs when X (which may be an bene?cial factor) occurs[6]. There are many heterogeneous data sources like relational tables, files, systems and/or geographic locations which are used in everyday business applications. However the traditional data mining algorithms are not applied directly as patterns extracted from a single normalized table or subject file are less interesting or useful than a full set of multiple patterns extracted from different datasets [7]. Association mining has a drawback of producing large collections of association rules that are difficult to understand and put into action. Thus a novel notion of combined patterns is proposed to extract useful and actionable knowledge from a large amount of learned rules. It gives an account of two kinds of redundancy in combined patterns: (1) the redundancy of  combined rules within a rule cluster, and (2) the redundancy of combined rule pairs [8].

Pruning methodology is a way of extracting the interesting rules from all the generated rules. Pruning selects only those rules which are having both the properties of static dataset ?d? as well as transactional dataset ?t" and lift >=1. In this technique [9] many of the rules get omitted as they are not showing properties of both dataset ?d? and ?t?.   Significant association rules involving items in the database are generated with the help of efficient algorithms like a priori. The algorithm incorporates buffer management and novel estimation and pruning techniques [1].



II.  ASSOCIATION RULES The process of discovering association rules is based on two main factors namely support and confidence i.e. minimum support must be supplied to start the process. Without the threshold being specified, no association rules can be discovered. This is because without the specification of the factors mentioned above, the number of rules generated will be too high and may involve low interestingness measure data which may not solve the purpose for which they are being discovered. Instead, it takes more time and resources in computing uninterested data. Further, setting up the minimum support threshold requires in-depth domain knowledge before the discovery of rules. In this manner, assumption is being made on the usage of minimum support on following factors:  A.  Accuracy of the specified threshold value It basically states that a domain expert always specifies accurate threshold value. This emphasizes the need of domain expert without whom the process will land up in standstill position. Further, it also assumes that the data above the specified threshold value is only of high interestingness measure. It blindly accepts the accuracy of the specified threshold value. Consequently, this leads to loss of association rules which may have contributed in decision making process.

B.  Single Threshold is Enough for Identifying the Knowledge  The technique of finding association rules aims to work globally on any kind of data. There can be cases where data is  quite uncertain and distributed. So a situation can occur where a single threshold is not enough for extracting the knowledge from data. This depicts the need of defining   multiple thresholds for the particular dataset.  In this manner, the assumptions made above are inappropriate and thus the rules reported lead to inaccurate and inconsistent results. In this paper, we propose a framework considering the above issues by removing the need of minimum support threshold. [5]

III. PROPOSED SYSTEM  A. Architecture of System The proposed system shown in Figure 2 above works on both transactional as well as static dataset. After applying the domain knowledge, a combined dataset gets generated which has both the properties of transactional and static or demographic data. Through domain knowledge, only interesting features get selected from generated combined data and so the data gets reduced. Feature selection method helps to select only the interesting features which further reduce the data. On the generated dataset, after applying coherent algorithm, both positive and negative rules get generated.

Proposed system suggests the pruning technique which helps to filter the uninterested rules. So the finally generated rules help to take the business decisions.

Fig. 2. Architecture of proposed system  B. Measurement of Association Rules Using other Methods of Interestingness  The coherent rules are generated based on logical implications like propositional logic, Implication and Equivalence [5].

Propositional Logic is concerned with propositions and their interrelationships. A proposition is basically a condition of the world about which we want to state something. It is not necessary that the    condition must be true always for us to comment on it. We might want to state that it is false or that it is true if some other proposition is true. The syntactic rules for the language of Propositional Logic are given below. [16] In addition, the semantic interpretation for the expressions specified by these rules is elaborated. Given these     semantics, all of the logical conclusions can be drawn from any set of propositional sentences.

? A negation consists of the negation operator ? and a simple or        compound sentence, called the     target. For example, given the    sentence p as Customer buys bread, we can form the negation of p as ?p i.e. Customer does not buy bread.

? A conjunction is a sequence of  sentences separated by occurrences of the ?^? operator and enclosed in parentheses, as shown below. The constituent sentences are called conjuncts. For example, we can form the conjunction of p which is Customer buys bread and q which is Customer buys butter as (p^q) i.e. Customer buys bread and butter.

? A disjunction is a sequence of      sentences separated by occurrences of the ?v? operator and enclosed in parentheses.

The constituent sentences are called disjuncts. For example, we can form the disjunction of p which is Customer buys bread and q which is Customer buys butter as (p v q) i.e.

Customer buys bread or butter.

? An implication consists of a pair of sentences separated by the  operator and enclosed in parentheses. The sentence to the left of the operator is called the antecedent, and the sentence to the right is called the consequent. The implication of p which is Customer buys bread and q which is Customer buys butter is (p q) i.e. If Customer buys bread then he buys butter.

? A reduction is the reverse of an implication. It consists of a pair of sentences separated by the  operator and enclosed in parentheses. In this case, the sentence to the left of the operator is called the   consequent, and the  sentence to the right is called the antecedent. The reduction of p which is Customer buys bread to q which is Customer buys butter is (p  q) i.e. If Customer buys butter then he buys bread.

? Equivalence is a combination of an implication and a reduction. For  example, we can express the equivalence of p which is Customer buys bread and q which is Customer buys butter as (p q) i.e. Customer buys bread if and only if he buys butter and vice-versa.

Table 1 . Truth Table for a Material Implication     Table 2 . Truth Table for an Equivalence  Logic: We highlight here that an implication is formed using two propositions p and q. These propositions can be either true or false for the implication?s   interpretation. From these propositions, we have four implications:  p  q,  p  ?q,  ?p  q, ?p  ?q   Each is formed using standard symbols  and ? as explained previously. For example, if butter is observed in a customer market basket, then bread is observed in a customer market basket. For this statement, rule formed will be p  q. On the other hand, for the statement if butter is not observed in a customer market basket, then bread is not observed in a customer market basket, rule formed will be ?p  ? q. This highlights the concept of Negative Association Rules which gives certainty of the result generated.

C. Generation of Coherent Rules For generation the coherent rules, following steps are involved.

Algorithm:  (a) Generation of Combined data (b) Selection of dataset through Domain knowledge (c) Support Matrix Generation (d) Coherent Rules Generation (e) Pruning for rule reduction  (a) Generation of Combined data  In combined rule mining static and dynamic datasets are involved. For example, assume there are static attributes like Gender {Male, Female} i.e. for a particular customer these values do not change throughout the dataset. On the other hand, attributes like a particular product which conveys whether a user will buy it or not is dynamic. This value is dynamic since the customer who currently is not buying it might buy it in future. So for marketing purpose we need to analyze the association of the static attributes with dynamic attributes to predict the possibility of a particular product being marketed for a particular set of people. For example if a rule states that {Bluetooth_Headphone: True, Car_Video: True  MODERATE}. This rule depicts that if an individual belongs to a MODERATE class then he will buy Bluetooth_Headphone and Car_Video. So if in case, a promotional campaign needs to be launched for people buying these products then there will be large set of people to be considered. On the contrary, taking combined rule mining example clarifies this situation as follows:{Gender: Male, Marital_status: Single, Bluetooth_Headphone: True, Car_Video: True  Moderate}. Here, this rule overcomes the drawback of above rule i.e. inspite of considering the entire set of people belonging to moderate class, it focuses on males who are not      married and are of moderate class. This reduces computational cost.

(b) Selection of dataset through Domain knowledge  While selecting the dataset for mining, if the domain knowledge is given as input to the system, then selected dataset  gets reduced. The steps for selection of the domain knowledge are given below:  p q p superset q  T T T  T F F  F T T  F F T  p q    p  q  T T T  T F F  F T F  F F T     1. List the transactional and demographic tables at run time.

2. List the characteristics/features of selected table 3. List the distinct values of each      selected characteristics 4. Select only the interested values of listed features 5. Select the class attributes.

Domain knowledge method reduces the selection of data by feature selection method to generate the   combined data. So the time complexity and space complexity for the computations get reduced.

(c) Algorithm to generate Support Matrix  The algorithm gives the number of times the items are purchased together as well as the number of times those items are not purchased together. Based on this criteria support matrix is generated.

Input: Class [ ]: Unique class names N: total number of classes in class [ ] set  Items [ ]: Power set of unique selected items M: total number of items in Items [ ] db: dataset with k records Algorithm: domain_driven_coherent (Items [], class []) Begin  For each record in Class [] // class labels {For each record in Items [] // power set { For each record in db // records { If (db[t].contains (class[j])) { If (db[t].contains (Item[i]))  ClassPresent_ItemPresent ++; Else ClassPresent_ItemAbsent ++; } Else { If (db[t].contains (Item[i])) ClassAbsent_ItemPresent ++; Else  ClassAbsent_ItemAbsent ++; } } } End; Output: Support Count Matrix   Table 3. Algorithm to generate support matrix    (d) Coherent Rules Generation  Suppose p and q are objects in the dataset and S (p) and S (q) are supports of p and q respectively. After we get the support matrix, coherent rules are generated if the following conditions are satisfied: ?  S(p, q) > S(?p, q) and  S(p, q) >    S(p, ?q) ? S(?p, ?q) > S(?p, q) and  S(?p, ?q) >  S(p, ?q)  E.g. If the support matrix formed by applying the algorithm mentioned in the section 3.2.3 is as follows:    Table 4. Support value matrix for p and q  Now, the conditions necessary for forming coherent rules are checked as follows:  ? (S (p, q) = 10) > (S (?p, q) = 9) ? (S (p, q) = 10) > (S (p, ?q) = 5) ? (S (?p, ?q) = 15) > (S (?p, q) = 9) ? (S (?p, ?q) = 15) > (S (p, ?q) = 5)  In this example, the above condition is satisfied and hence coherent rule (p q) is formed and this also implies (?p ?q).

This is called as negative association rule mining [6].

(e) Pruning Pruning selects only those rules which are having both the properties of static dataset d as well as transactional    dataset t and lift >=1.  In this step many of the rules get omitted as they are not showing properties of both dataset d and t. With reference to  Table 5 given below, rules 1, 2 and 3 are not useful to generate combined    patterns. Only the rules given in Table 7 are considered for the generation of combined coherent rules. For each generated rule, interesting measures like Support, Confidence, Lift and Irule are calculated as given below: Assume that the rule is a ^ b  T Support: The support of the rule, that is, the relative frequency of transactions that contain a ^ b and T.

support (a ^ b  T) =  support  (a+b+T) Confidence: The confidence of the rule.

confidence (a ^ b  T) = support (a+b+T) / support (a+b) Lift: The lift value of the rule is the additional interestingness measures on the rules. These       measures can then be used to either rank the rules by importance (present a sorted list to the user) or as an additional pruning      criterion.

Lift (a ^ b  T) = confidence (a ^ b  T) / support (T) Two new lifts for measuring the interestingness of combined association rules are [17] as Lifta (a ^ b  T) =Lift (a ^ b  T) /lift (b  T) Liftb (a ^ b  T) =Lift (a ^ b  T) /lift (a  T) Where, Lifta (a ^ b  T) is the lift of ?a? with ?b? as a precondition, which shows how much ?a? contributes to the rule.

Liftb (a ^ b  T) gives the contribution of ?b? in the rule.

Irule: Based on the above two new lifts, the interestingness of combined association rules is defined as Irule.  Irule indicates whether the contribution of ?a? (or ?b?) to the occurrence of T increases with a (or b) as a precondition.

Irule (a ^ b  T) = Lifta (a ^ b  T)/ lift (a  T).

Or Irule (a ^ b  T) = Liftb (a ^ b    T)/ lift (b  T).

The value of Irule falls in [0, + ]. When Irule > 1, the higher Irule is, the more interesting the rule is. Once the Irule is calculated for each rule,     arrange them in descending order, the rule with Irule > 1 and higher Irule value is more interesting. Before going for the pruning process, ensure that maximum frequent patterns are generated i.e. the ones having  S(p) S(?p)  S(q) 10 9  S(?q) 5 15     support  value at least as much as the user   specific percentage of the database.

Pruning technique encompasses of following steps:  ? Selection of rules having both properties of both static and transactional dataset. Calculate the confidence, support, lift with the help of formulae given above.

Select those rules which have confidence value greater than the desired value and lift>=1.

? Suppose there is a pattern of rule A T, then check if A ={X ^ Y} i.e. whether A is a combination of more than one item including both static and transactional   feature.



IV. EXPERIMENTAL RESULTS The proposed technique is tested on the subset of retail demo dataset available on msdn site. The sample subset of data of 139 customers for 7 different products is selected for experiment.  Customer data is classified as High, Moderate and Low customers based on their features. The aim of the experiment was to find the association of demographic features of customer, product buying pattern and the class of customers which could help to give a promotional campaign on different products based on customer class and their demographic feature. The experimental setup used SQL server 2008 and DOTnet technology for implementation purpose. As per the proposed framework, domain knowledge method and feature selection method is used to generate coherent rules.

Table 5 displays the subset of coherent rules generated after applying the algorithm given previously in section III. Using domain knowledge method, Marital_status and Gender from the demographic data is selected. From the transactional data only the interesting products are selected.  For example, if we select certain attributes like Car_Video and Bluetooth_Headphone from the entire set of attributes then rules will be generated only for those selected products and thus we can reduce the complexity by reducing the number of computations. Further, provision has also been given to select the feature of the selected attributes which further help in achieving the desired results and also reduce the number of computations to a large extent. For example, after selecting attributes like Car_Video and Bluetooth_Headphone, we can further reduce computations by providing user an option of whether one is interested in generating rules for presence of the product or absence of product. Accordingly, then the rules will be generated which help the user in making promotional events to increase the business of their products. Table 6 given below displays the results after selecting 7 attributes namely Marital_status, Gender, Bluetooth_Headphone, Car_Video, Home_theater_system, Mp4_Mp3 and Television and the  values of the above mentioned attributes that are selected. The domain driven concept is further expanded to the selection of the class labels for which rules needs to be generated. Finally only the subset of the rules with static as well as transactional features having lift>=1 are selected as shown in Table 7.

Those rules can be reduced by applying pruning technique as given in section III. After pruning and combined mining, the rules generated with the selected features are shown in Table 7.  Total 8 combined rules have been generated. This result shows that how the different class customer with different demographic characteristics changes the product pattern. In Table 6, rule 3 states {Gender: Female, Bluetooth_Headphone: False, Car_Video: False  HIGH}.

This rule states that if Class is HIGH and the individual is female then there is minimal possibility of that individual buying the products like Car_Video and Bluetooth_Headphone. So schemes related to these products need not be provided to that individual. On the contrary, that individual must be provided with the schemes related to products in which he/she is interested which in turn increases the possibility of purchasing the product, consequently increasing the sales of the product intelligently. This saves a lot of work and effort required. Such type of knowledge helps to give the promotional campaign on products with respect to the class of customer and their demographic features. The rules are readable and understandable to human to take business decision and can reduce the cost of promotion.

V  CONCLUSION This paper basically introduces the technique of finding association rules and the drawbacks of the same. Through this introduction, it highlights the need of introducing the concept of coherent rules and the benefits of the same. Coherent Rules gives both positive and negative rules without giving support threshold. Further, it enhances the concept by adding certain features like domain-driven knowledge which further embellish the quality of the result generated in terms of optimization of time and space complexity. The domain driven concept bestows the idea of selection of data features before generating the frequent patterns. Hence, the users have the freedom to select the product on which the company wants to give promotional campaign before generating the rules.

