Traceability ReARMed

Abstract  Traceability links connect artifacts in software engineer- ing models to allow tracing in a variety of use cases. Com- mon to any of these use cases is that one can easily find related artifacts by following these links. Traceability links can significantly reduce the risk and cost of change in a software development project. However, finding, creating and maintaining these links is costly in most cases. In any real-world project of significant size the creation and main- tenance of traceability links requires tool support.

In this paper, we propose a novel approach to support the automation of traceability link recovery based on Asso- ciation Rule Mining and operation-based change tracking.

Traceability link recovery is the activity of finding missing or lost traceability links. Our approach automatically gen- erates a list of candidate links based on the project history along with a score of support and confidence for every can- didate link. We transformed the data from an operation- based change tracking system to sets of frequent items, which serve as input for Association Rule Mining (ARM).

We applied our approach to data from a software develop- ment project with more than 40 developers and assessed the quality of the candidate links in interviews.

1 Motivation  Traceability is an important property of any software model and can be defined as follows: ?Software artifact traceability is the ability to describe and follow the life of an artifact (requirements, code, tests, models, reports, plans, etc.) developed during the software life cycle in both forward and backward directions (e.g., from requirements to the modules of the software architecture and the code components implementing them and vice-versa)? [12] [18].

Missing support for traceability has been recognized as a major cause for project overruns and failures [7] [16]. Due  to the iterative nature of software development, the man- ual detection and maintenance of traceability links has to be performed frequently and is costly [4]. The need for tools to support finding and maintaining traceability links is widely recognized in academia [9] [25] [21] [22]. Ex- isting approaches for recovering traceability links are based on content and rely on the hypothesis that related artifacts are also related in content. Traceability links can therefore only be found if the two artifacts that are to be linked have similar content. This is a major drawback and either results in many false positives if the content similarity threshold is low, or in many false negatives if it is high. During the case study presented in section 5, we also observed that related artifacts must not be similar in their content at all, especially if the content is sparse and the model is still under develop- ment. De Lucia et al. [18] explicitly suggest to solve the limitations of content-based approaches by complementing them with other approaches.

In this paper, we propose a novel approach to traceabil- ity link recovery called ReARM (Recovery of traceability links based on Association Rule Mining). ReARM does not rely on content similarity, but on the history of a soft- ware model, overcoming the limitations of content-based approaches [18]. Especially in models under development, where content may still be sparse, content-based similar- ity is not an appropriate indicator for related artifacts. By recovering links from the previous change and navigation behavior on the model, the candidate links are restricted to relevant parts of the model and to links that would have been of use in the past. Our approach is also applicable to unified models, which incorporate a broader range of software en- gineering artifacts than previous software models. Further, ReARM can be used in addition to existing content-based approaches, completing their results with links that are not related in content but according to their change history.

2009 33rd Annual IEEE International Computer Software and Applications Conference  DOI 10.1109/COMPSAC.2009.52     2 Related Work  For tool-supported traceability link recovery, there are two types of approaches in general, content-based and non content-based ones. Several content-based approaches make use of methods from Information Retrieval (IR) such as Latent Semantic Indexing (LSI), deriving traceability link candidates from content similarity. Non content-based approaches often use data mining techniques, such as Asso- ciation Rule Mining (ARM), to discover traceability links symbolically.

Content-based Approaches The majority of existing approaches is based on content. Several content-based ap- proaches employ basic matching techniques as done by Murphy et al. [21], where regular expressions and nam- ing conventions are leveraged to map the developer?s model to the source model, using the ?reflection model technique?.

The tool QuaTrace by Maletic et al. [25] integrates the com- mercial tools RequisitePro and Rhapsody to provide trace- able requirements using guidelines during document cre- ation and name matching. Zisman et al.[29] also use syn- tactical matching of related terms in requirements and in the source code of the system. All these approaches depend on naming conventions and adherence to guidelines.

As mentioned before, many content-based approaches are based on information retrieval methods. According to Antoniol et al. [2], the probabilistic model and the vector space model from information retrieval are both suitable for traceability recovery and show similar performance in this field. These methods require a prior stemming step, i.e. the reduction of words in a text to their corresponding stems to make comparisons easier. Settimi et al. [24] utilize the vec- tor space model to trace requirements to UML artifacts and code. In [13], Hayes et al. introduce the RETRO tool which utilizes an improved vector space model technique as well as latent semantic indexing (as an extension of the vector space model). David proposes a recommendation system, which is based on a recurrent neural network as well as on LSI, to predict likely further changes ? given a set of already changed software artifacts [6].

Cleland-Huang et al. [5] introduce three strategies for improving probabilistic retrieval algorithms, i.e. hierarchi- cal enhancement, clustering enhancement and graph prun- ing enhancement. Richardson and Green [22] use a method called surface traceability, in which small perturbations are made and the corresponding changes are observed to au- tomatically discover traceability links between the specifi- cation and parts of a program ? generated from this spec- ification using program synthesis. While this technique is interesting for generated programs, it was not developed for traceability discovery in manually coded programs.

Maletic and Marcus [19] make use of latent semantic  indexing, showing that latent semantic indexing performs better than vector space or probabilistic models and does not rely on stemming either. Maletic et al. [20] extend the latent semantic indexing approach by creating a for- mal hypertext model for link recovery and maintenance of the conformance of the links over time. Lormans and van Deursen [17] reconstruct links between requirements on the one side and design artifacts and test case specifications on the other side also using latent semantic indexing. They discuss two possible link selection strategies, which include to apply a 1-dimensional as well as a 2-dimensional filter to the term-document matrix. De Lucia et al. [18] cre- ated an artifact management system with traceability re- covery based on latent semantic indexing called ADAMS.

They conclude that in order to go beyond the limitations of information retrieval methods, a combination with syntax- based approaches would be necessary. Summing up the ex- isting approaches in the field of traceability link discovery, content-based approaches mainly use information retrieval methods based on the vector space model such as LSI.

These techniques rely on static similarity between differ- ent development artifacts (entity-centric). Sparse or miss- ing artifact contents imply a need for non content-based ap- proaches such as syntax-based analysis or association rule mining. These methods often address traceability from an activity-centric point of view, discovering traceability links from the traces of different development activities.

Non Content-based Approaches The CAESAR ap- proach developed by Gall et al. [10] attempts to measure the coupling of multiple releases of a system ? based on empiri- cal analysis. Using a product release database, they identify change patterns using change sequence analysis and change report analysis and employ these patterns to detect coupling between subsystems. Egyed and Gru?nbacher [9] utilize a graph of nodes, which can be built by observing the classes and methods used while testing a scenario. Trace analysis is then performed by iteratively manipulating this graph. Gall et al. [11] exploit CVS (Concurrent Versions System) data by quantitative analysis, change sequence analysis, and re- lation analysis to analyze software evolution and to display change behavior. CVS data is also the basis for the work of Ying et al. [27], who utilize association rule mining to determine change patterns from the change history of the CVS repository. They exploit this information to recom- mend possibly relevant artifacts to the programmer. ROSE, as presented in Zimmermann et al. [28], also makes use of association rule mining on a CVS repository to predict fu- ture changes, to reveal the coupling of entities, and to pre- vent errors occurring due to the incompleteness of changes.

Both approaches operate on the source code only.

Non-content based approaches traditionally determine change patterns based on the changes to a CVS-like repos-     itory. Change-impact mining on a unified model using a broader range of software engineering artifacts, as de- scribed in section 3.1, can help to improve these ap- proaches.

3 Prerequisites of the ReARM Approach  In this section we will introduce prerequisites for our ap- proach. The traceability links we recover are instances of a unified model that combines models from different domains such as UML models, rationale models, or schedules. In section 3.1, we describe the idea of a unified model and de- fine its terminology [26]. Since we recover traceability links from the history of the models, our approach is based on Software Configuration Management (SCM), more specifi- cally, on a operation-based versioning system (section 3.2, [15]). Finally, we introduce association rule mining as the data mining technique used to process the history data (sec- tion 3.3).

3.1 Unified Model  In software engineering, a model can describe the system under development on different levels of abstraction, at dif- ferent points in time, and from different points of view. Typ- ically, in a software development life-cycle, many models of the system under development are produced. These models range from requirements models to class models of the enti- ties involved in the system. However, models in a software development project may also build an abstraction of the software development project itself in terms of the organi- zation, resources, or schedules. Many management artifacts fall into this category, such as work breakdown structures or organizational charts. We therefore define the term system model as the entirety of models describing the system un- der development, while the project model is the entirety of models describing the development project itself.

Models define model elements and links, in the termi- nology of a unified model they represent an aggregation of model elements and model links. Model elements are nodes that have a certain number of attributes of a certain type. For example, in a UML class model, there are class nodes including but not limited to the attributes name, is- Abstract, and package. The requirements model contains requirement nodes with attributes such as stakeholder or priority. Model links define the connections that are valid among nodes. In the UML class model example, links in- clude association, aggregation, and composition links, in the requirements model links might specify a refiningRe- quirements relation. Essentially, all instances of models are graphs with instances of model elements as nodes and instances of links as edges.

Since several models can describe the same system under development, it is often useful to add additional links con- necting elements of these different models. For example, it can be essential to link a requirement from a requirement model with a use case from a use case model to express that the use case is describing the requirement in greater detail.

A unified model defines models with their model elements and links among the model elements from the same or dif- ferent models.

3.2 Operation-based Change Tracking  Commonly instances of a model are stored in a Software Configuration Management (SCM) system. Our approach is based on the history provided by an SCM system, which serves as input for the association rule mining algorithm.

For the ReARM approach, we need to analyze the changes that were performed on the granularity of model elements and links. Most existing SCM systems however provide data about the evolution of a model on the granularity of files. We therefore rely on the SCM system presented in [15] and implemented in Sysiphus [8]. This SCM system works in an operation-based way and provides change in- formation on the granularity of model elements and their attributes. Each change is called an operation and describes the attribute that was changed, the respective model ele- ment, and the old and new value of the attribute. All model elements also contain reader information, that is informa- tion about the time a user has last read the element, if at all. Since reading a model element will update its reader information, reading a model element is also recorded as a change which we will exploit in the following.

changesFrom1To2: ChangePackage  version1: Version  version2: Version linkAtoB:  ModelOperation  createRequirementA : ModelOperation  createUseCaseB: ModelOperation  Figure 1. Instance of a version graph.

The SCM system records the history of an instance of a unified model in a version graph. Figure 1 shows an in- stance of a version graph. Each instance of Version rep- resents a project state, having a predecessor and a succes- sor Version instance. Modifying version version1 creates version version2. The modifications on the model that oc- curred in version1 transforming it into version2 are repre- sented by the instance changesFrom1To2 of the association class ChangePackage. From a technical point of view, we     say applying changesFrom1To2 to version1 results in ver- sion2. The ChangePackage changesFrom1To2 is a com- posite of three ModelOperation instances. The ModelOp- eration linkAtoB is an operation that was performed on the model between version1 and version2. It describes the cre- ation of a link between two model elements A and B, where A is a requirement and B is a use case.

On the basis of this change tracking infrastructure, the PAUSE framework [14] was constructed. It provides ac- cess to project iterators returning the project state and the changes applied to the project on any day of the project life- cycle.

3.3 Association Rule Mining  Association rule mining (ARM) is a data mining tech- nique usually applied to large databases of sets of co- occurring items. The technique was first introduced by Ar- gawal, Imielinski, and Swami [1], who used it for market basket analysis. Its goal is to discover buying patterns of the form ?85% of the purchases including diapers also in- cluded beer?. Such probabilistic rules can then be used for decision support systems regarding price promotion or store layout. Figure 2 shows the functionality of ARM from a blackbox perspective, receiving itemsets as input and pro- viding discovered rules as output.

Figure 2. Blackbox view of association rule mining with generic input and output representation that is applicable to many different domains and their problems.

Recently, association rule mining has also been applied to software engineering. One contribution was made by Zimmermann and his colleagues, who mined version his- tories to guide software changes [28]. When a developer works on a programming task, relevant software artifacts such as documents, source code, multimedia files, etc. that might require corresponding changes, can be proactively recommended: ?Programmers who changed these artifacts also changed ...?.

In this paper, we also use an association rule mining tech- nique based on the apriori algorithm [23]. We define the fundamental concepts of association rule mining [3] by the following terminology.

? Items I = {i1, i2, . . . , im}: The set of all existing items each represented by a unique symbol.

? Itemset X ? I: A subset of the globally available items, e.g. a shopping cart of goods to be bought. For facilitating the mining process, the items are ordered lexicographically, (x1, x2, . . . , xn) with x1 ? x2 ? . . . ? xn. A k-itemset is an itemset containing k items x1, . . . , xk.

? Transaction T = (tid, X): Each transaction is a 2- tuple consisting of a tid, which is simply an identifier in form of a natural number, and an itemset X .

? Database D = {T1, T2, . . . , Tn}: A database is the collection of transactions, which were captured and stored in a transaction table < tid,X >.

? The cover of an itemset X is a set of identifiers, de- nominating the transactions that contain X: cover(X) := {tid|(tid, Y ) ? D, Y ? X}.

? The frequency of an itemset X in the database D is the probability of its occurrence within a transaction T ? D: freq(X) = support(X)|D| = P(X), where support(X) := |cover(X)|.

? Itemsets X ? D that satisfy a given frequency thresh- old t, freq(X) ? t ? [0, 1], are of special interest.

The concrete values of these terms are typically measured by counting the entries of a transaction database, which rep- resents the dataset to be analyzed. ARM works upon un- ordered sets and not upon ordered sequences, which is a fundamental premise and constraint of this technique. Thus order-sensitive sequences such as words of a grammar can- not be represented without loss of information.

The ARM process consists of two subsequent steps; first, all frequent itemsets in the database have to be discovered.

In the second step, the actual association rules are derived from the set of frequent itemsets: 1. Mining of Frequent Itemsets: There are  (|I| k  ) -many k-itemsets that all have to be  checked for minimum support in case of a naive approach to itemset mining. Thus the worst case costs sum up to O( ?|I|  k=1  (|I| k  ) ) = O(2|I|); 2. Association Rule Mining:  ? An association rule is an implication of the form X ? Y , X ? Y = ?. X is the rule?s body, Y is the head.

? The support of an association rule A ? X ? Y is computed based on the support of (frequent) itemsets: supp(A) = supp(X ? Y ).

? The confidence of an association rule A ? X ? Y is defined as: conf(A) = supp(X?Y )supp(X) .

The confidence can also be interpreted as condi-     tional probability of the rule?s head: P(Y |X) = P(Y,X) P(X) , P(X) 6= 0. The desired minimal confidence is  chosen by domain experts before starting the frequent itemset mining.

The association rule mining step requires a set of frequent itemsets as input, which is the output of step 1. Based on the discovered k-itemsets, which are frequent in the under- lying database, association rules A of the form A := (Y ? X ? Y ) are computed. Only rules with the desired min- imal confidence are considered. The confidence conf(A) can be calculated ? Y ? X in the following simple way: conf(A) = supp(Y ?(X\Y ))supp(Y ) =  supp(X) supp(Y ) .

Apriori Algorithm To solve the frequent itemset prob- lem in a more efficient way than checking each k-itemset X = {(x1, x2, . . . , xk)}, xi ? {i1, i2, . . . , im}, i ? {1, 2, . . . , k} regarding its support (a total of 2m ? 1 eval- uations), a dynamic programming approach is carried out.

This technique is known as the apriori algorithm for ARM, which exploits a monotony property. The monotony prop- erty states that all supersets X ? of an itemset X , X ? X ?, cannot be frequent, if X is not frequent. This property nar- rows the search space significantly and thus speeds up the computation of frequent itemsets. The constrained search space corresponds to the set of candidate itemsets, whose support has to be computed. Since the candidate generation can be additionally pruned by discarding those candidates not contained in the transaction database, the whole ARM algorithm is very fast (magnitude of several seconds) for up to several hundred thousands of itemsets, which addition- ally qualifies this technique for traceability link discovery.

4 Traceability Link Discovery Approach  In the ReARM approach, we apply association rule min- ing (ARM) to the history and therefore also to the user navigation traces of software models to recover traceability links. Our main contribution is the combination of ARM and operation-based change tracking as well as their appli- cation to recover traceability links. Also we apply ReARM to a broad set of artifacts from a unified model (cf. section 3.1). Thereby, we only rely on the history of the models not on the content of any model element, overcoming the limitations of content-based approaches [18]. Especially in models under development, where content may still be sparse, content-based similarity is not an appropriate in- dicator for related artifacts. By recovering links from the previous change and navigation behavior on the model, the candidate links are restricted to relevant parts of the model and to links that would have been of use in the past. As opposed to previous approaches, ReARM exploits the his- tory of read-activities upon model elements, which is more  fine-grained than only mining the file-based version histo- ries provided by the respective versioning system. Our hy- pothesis is that reading one or several artifacts often repre- sents the initial point for a user to change related artifacts.

The ?initial? artifacts do not necessarily have to be changed in order to cause an intra- or inter-model change, since in many cases, reading them within the same session already reminds the user of related changes to be carried out.

In the following, we describe our approach to recover traceability links, presenting a short overview of the recov- ery process and then describing the details of each step. Fig- ure 3 shows an overview over the process that consists of three major steps: session mining, read-read partitioning, and association rule mining.

As outlined in section 3.2, the history of a model instance is recorded in change packages. Our first step is to cluster the change packages into sessions per author. A session is a list of change packages from one author, where the time between two consecutive change packages remains below a given threshold, the session timeout. In a further step, certain changes are filtered from the sessions, e.g. those that relate to deleted elements and thus are useless. Each session represents a set of model operations, which is parti- tioned according to a read-read heuristic in the second step.

Finally, the association rule mining procedure results in a set of rules. Each rule is given by a set of model elements on the left hand side and a set of model elements on the right hand side of the rule. Each rule represents a candidate for a traceability link, which is annotated with a confidence and a support value.

Session Mining Association Rule Mining Read-Read Partitioning  Figure 3. Coarse-grained work flow of the ReARM trace- ability link discovery approach.

Step 1: Session Mining The history of a model in- stance consists of a list of change packages representing all changes that ever occurred on this model instance. Each change package is a list of model operations. Each change package was created by exactly one author and was com- mitted at a given point in time. We define a session as a list of model operations from one author so that the period of time in between two succeeding change packages is within a given session timeout. This way, we are able to trans- form the list of change packages into a list of sessions. If two commits from one author occur within a few minutes or even seconds, we observed that the authors are working on similar modeling tasks in most cases. In an ideal scenario, a     commit would be equal to a session and each author would commit only after and also immediately after completing one modeling task. The session timeout represents a degree of freedom in this step, thus we vary this parameter in our evaluation.

Step 2: Read-Read Partitioning In the second step, we need to filter the model operations in the sessions to avoid some unwanted results. As we are only able to find traceability links between existing elements, we filter all model operations referring to deleted elements. Addition- ally, we remove changes that refer to administrative model elements, such as project configuration data. On every read of a model element, its reader information is updated and recorded as a change. Based on the hypothesis that users trace artifacts even if they are not connected by a link, we filter the changes to reader information changes. Thereby, we model the navigation of a traceability link by frequent and sequential reader information changes on model ele- ments to be linked. As input for ARM, we use a set of tuples of model elements. Each pair of succeeding reader informa- tion changes on two model elements that are not yet linked represents such a tuple within the session timeframe. In the case a traceability link is absent, the user may take several approaches to find the correct element, therefore we also include transitive links up to a certain depth (transitivity- depth). In other words, for a fixed model element, we gen- erate n pairs with the n succeeding elements that were read ? not only with the direct successor. As result of step 2, we obtain a set of tuples of model elements that were read in sequence for each session obtained from step 1.

Step 3: Association Rule Mining In the final prepro- cessing step, the two model elements from each tuple are joined to get one itemset. The collection of all of these itemsets is fed into the ARM mechanism as a set of sets.

Before starting the mining process, the two parameters sup- port and confidence are chosen, e.g. minSupp = 0.015 and minConf = 0.7, which means that the items that take part in an association rule occur in at least 1.5% of all itemsets. The more itemsets are contained in the un- derlying database, the lower the minSupp value has to be chosen, since otherwise very few association rules would be found. Now consider the association rule A := (X ? Y ), where supp(X ? Y ) ? minSupp (frequent itemset). The value minConf is the minimal fraction of itemsets (here 70%) that also contain the elements of itemset Y , given that they already contain those of itemset X , more formally, conf(A) = supp(X?Y )supp(Y ) ? minConf .

Finally, the ARM algorithm returns a set of rules {A1, . . . , An}, each of them fulfilling the minConf con- straint. Since our input only contained tuples the cardinal- ity is two for each Ai. We only keep those rules, which  point from the first element to the second element in any tu- ple. In other words, we discard those rules, whose left hand side (body) as well as right hand side (head) are never the first or second element in a tuple respectively. This way, we obtain rules with the following meaning: Reading the model element on the left hand side often results in reading the model element on the right hand side. Each rule rep- resents one traceability link candidate. Each candidate is annotated with a support and a confidence value from ARM (cf. section 3.3). In the following evaluation, we discuss several instances of links discovered by the ARM mech- anism and present the quantitative results of the ReARM approach within an industrial case study.

5 Evaluation  We evaluated the ReARM approach within a case study on a project named DOLLI (Distributed Online Logistics and Location Infrastructure) at a major European Airport.

The objective of the DOLLI project was to improve the air- port?s existing tracking and locating capabilities and to inte- grate all available location data into a central database. Lug- gage tracking and dispatching of service personal as well as a 3D visualization of the aggregated data were developed.

More than 40 developers worked on the project for about five months. All modeling was performed in the Sysiphus CASE tool [8]. This resulted in a comprehensive project model consisting of about 15.000 model elements and a his- tory of over 53.000 versions.

Before presenting the quantitative results of our evalu- ation, we give concrete examples of link candidates gen- erated by ReARM based on the history of the DOLLI project. One of the traceability link types within the sys- tem model points from use cases to UML classes and to UML class diagrams, respectively, which are an aggrega- tion of classes. We discovered a link from an use case de- scribing the recording of object movements on the airport to a class diagram representing the entity model of track- able objects and their location data. This link allows to up- date the entity model in case the use case is changed or to trace from the use case to the classes in order to get a more detailed view on the system. While the use case contains only the short text ?The user should have the possibility to record the movement of objects for short periods of his/her absence so that he/she can plot them at a later time. (Not available for normal employees).?, the class diagram con- tains many classes (over 20), most of them only containing a class name such as Car, which is a subtype of trackable objects. For a content-based approach, it is likely to be dif- ficult to relate the use case with the diagram, since content is sparse in both entities and the only relationship between them is that all the objects in the diagram are movable.

In the unified model, an ActionItem dealing with imple-     mentation can be linked to the requirements from the sys- tem model which it partially implements. We recovered one instance of this link type. The task is about the implemen- tation of a timeout detection for an RFID reader antenna.

The functional requirement is about the detection of RFID tags. Without the suggested link, a project manager cannot trace from the requirement to the tasks that are still open and that refer to this requirement. The content of the action item is only ?Implement antenna timeout detection?, while the content of the requirement is ?Detection of RFID tag?.

For a content-based approach, there is hardly any relation detectable using only this sparse information.

For the quantitative part of our evaluation, we ran the link recovery with different parameters to optimize the results.

The best result generated by a run with a session timeout of 20 minutes, a transitivity-depth of 10, and support as well as confidence thresholds of 1.5E-3% and 30%, respectively, is shown in table 1. The table lists source and target element types and the number of links in forward and backward direction of these link candidates as well as a total count for each link type. Although we consider the links to be bidirectional, it is interesting to see in which direction they were discovered since this also implies the most frequent direction of usage. Links with only few instances were ag- gregated into classes such as System Model Element and Project Model Element (see 3.1. Unexpectedly, many of the traceability links originate from or point to elements of the Project Model. We would have expected more links within the system model, especially from requirements to use cases. By examining the input data, we figured out that there are only few navigation instances from require- ments to use cases and vice versa. The requirement and use case model are not consistent with each other. There are re- quirements that are not required for any use case; in other words, use cases are missing. By contrast, there are use cases that are not reflected in the requirements. We discov- ered the reason for this mismatch by means of the conducted interviews. While some of the teams worked mostly with use cases, other teams relied on requirements as primary representation for the customer?s requirements. Therefore there has been hardly any navigation from requirements to use cases. On the other hand, it was a prerequisite from project management to report on the completion of the sys- tem in terms of open tasks. This is probably the reason for many navigation instances from Project Model to System Model, resulting in many recovered links in this segment.

This is also reflected in the resulting model, in which almost 50% of the action items are related to a requirement, but less than 10% of the requirements are linked to use cases.

In summary, 22% of the recovered links belong to the sys- tem model, 36% point from the system model to the project model, and 42% belong to the project model. From these re- sults, we conclude that our approach is very sensitive to the  LinkSourceType LinkTargetType Fwd Bwd Total  Action Item Functional Requirement 38 17 55 Action Item System Model Element 6 0 6 Action Item Meeting 4 0 4 Action Item Rationale 21 6 27 Action Item Workpackage 15 0 15  Class Action Item 6 0 6 Class Subsystem 17 0 17 Class Scenario 1 0 1 Class UML Diagram 9 2 11  Design Goal Scenario 4 0 4 Meeting Action Item 5 0 5 Meeting Comment 4 0 4 Meeting System Model Artifact 8 1 9 Rationale Meeting 17 7 24 Rationale Rationale 20 0 20 Rationale System Model Element 5 1 6  UML Diagram Project Model Element 3 0 3 UML Diagram System Model Element 12 0 12  Use Case Class 4 0 4 Use Case UML Diagram 4 0 4  Total 203 34 237  Table 1. Classification of the discovered traceability links.

Fwd/Bwd denotes the number of traceability links in for- ward/backward direction.

actual usage of the model. We can only derive traceability links which are frequently used. This may sound like a dis- advantage, but indeed it is an advantage: we only discover traceability links, which would have been used frequently.

To assess the quality of the recovered links, we inter- viewed several project participants. The test set was ran- domly selected from all candidate links. From a total of 43 links, 5 links were rejected. For 12 links the probands could not tell whether these actually represent traceability links. The remaining 26 links were accepted. In the over- all experiment, the probands considered more than 60% of the candidates as meaningful traceability links, even when discarding the links the probands were not sure about. The problem with evaluating the overall quality of our approach is that by design the overall number of traceability links is unknown and very hard to estimate. Nevertheless, any re- covered traceability link represents value added for the re- spective development project, unless the effort to review the recommended candidate links is overwhelming. Therefore the number of recommended candidate links can be con- trolled via the mining parameters support and confidence.

Since it is likely that a higher confidence value of an associ- ation rule implies a higher applicability of the correspond- ing traceability link, we can cut down the result size by increasing the minimal confidence. Furthermore, a higher support value implies a higher applicability of the discov- ered rule within the space of all model elements.

6 Conclusion and Future Work  In this paper, we proposed a novel approach called ReARM for discovering traceability links based on asso- ciation rule mining and operation-based change tracking.

Our approach operates on a broad set of artifacts as part of a unified model including UML models and project man- agement models such as schedules. In contrast to content- based approaches, we do not only consider the model at a certain point in time, but we take its history into account, and thereby also the activity of the developers working on the model (activity-centric). This results in two main ad- vantages: First, content-based approaches such as similarity search using latent semantic indexing (LSI) do not address traceability as an activity that is carried out by developers in a software project, but only consider their static work prod- ucts such as requirement documents or use case descrip- tions. Recovered links are not rated by frequency of use and might thus of limited interest for the actual activities performed in a specific software development project. Sec- ond, since we do not rely on content, we can discover links even in situations where content is sparse or the involved ar- tifacts are not related in content, but nevertheless require a traceability link. These situations often occur when a model is still under development and not yet complete, correct, nor consistent. Our approach can be used in addition to exist- ing content-based approaches, completing their results with links that are not related in content but according to their change history.

We applied the ReARM approach to the artifacts gener- ated by an industrial project in order to show its feasibil- ity and to evaluate the link recovery results. We conclude that our approach represents a useful alternative to content- based approaches, especially in situations where these ap- proaches fail: Traceability links that do not result from sim- ilarity in content and links among artifacts with sparse con- tent. The ReARM approach is not limited to traceability link recovery and also shows a potential for other applica- tions. In future work, we will apply our technique to change impact analysis scenarios. Cost and time metrics based on the result of ReARM can be used to quantify the impact of a change, which in turn may support a go/no-go decision of the project?s change committee.

