iGroup: Weakly supervised image and video grouping

Abstract  We present a generic, efficient and iterative algorithm for interactively clustering classes of images and videos.

The approach moves away from the use of large hand la- belled training datasets, instead allowing the user to find natural groups of similar content based upon a handful of ?seed? examples. Two efficient data mining tools origi- nally developed for text analysis; min-Hash and APriori are used and extended to achieve both speed and scala- bility on large image and video datasets. Inspired by the Bag-of-Words (BoW) architecture, the idea of an image sig- nature is introduced as a simple descriptor on which near- est neighbour classification can be performed. The image signature is then dynamically expanded to identify common features amongst samples of the same class. The iterative approach uses APriori to identify common and distinctive elements of a small set of labelled true and false positive signatures. These elements are then accentuated in the sig- nature to increase similarity between examples and ?pull? positive classes together. By repeating this process, the ac- curacy of similarity increases dramatically despite only a few training examples, only 10% of the labelled groundtruth is needed, compared to other approaches. It is tested on two image datasets including the caltech101 [9] dataset and on three state-of-the-art action recognition datasets. On the YouTube [18] video dataset the accuracy increases from 72% to 97% using only 44 labelled examples from a dataset of over 1200 videos. The approach is both scalable and ef- ficient, with an iteration on the full YouTube dataset taking around 1 minute on a standard desktop machine.

1. Introduction There is a large and ever growing quantity of media  available on-line and on personal computers and therefore an increasing demand to meaningfully sort and group me- dia efficiently, without the need for large scale groundtruth labelling or meta annotations. Ideally, users would weakly supervise the learning process by picking a few images or  videos that belong to the same class or group. An automatic approach would then extract and learn rules that can gener- alise and cluster the remaining unseen media.

In order to achieve this, we aim to move away from hard supervised learning that is often proposed [19, 10, 23, 26, 12]. These approaches rely on groundtruthing large quanti- ties of training data, previously, approaches have been pro- posed to use a ?single example?, so called ?one shot learn- ing? [24, 20], where only one example is used in train- ing. However, these approaches are sensitive to the training examples ability to generalise and techniques are generally applied to simple staged datasets.

The similarity measure employed in this paper extends the min-Hash algorithm that was originally designed to identify the similarity between text in documents [3] by effi- ciently computing the distances between high dimensional representations. Similarly, the association rule data min- ing technique used (known as APriori [1]), was originally designed to identify co-occurrences from large text files.

Rather than using groundtruthed training data, that can be increasingly hard to produce as datasets get larger and more complex, we rely on an iterative process to select far fewer training examples. Using APriori, the distinctive and dis- criminative elements of these selected examples are iden- tified, and accentuated across the dataset by dynamically augmenting the representation with new compound visual words to form an image signature.

In the paper we propose and explain the idea of the im- age signatures in section 3 and extend the min-Hash algo- rithm for video similarity through the use of visual words in section 4. Furthermore, an iterative process is presented, that in section 5 uses APriori data mining to identity com- mon elements of the signatures, that are then accentuated in section 6 to further increase accuracy. Extensive results of the approach on image and video datasets are shown in section 7.

2. Related Work Within computer vision there has been a number of ap-  proaches that utilise data mining?s ability to work efficiently       and with large amounts of images or video.

Within the image domain, Quack et al [22] applied As-  sociation rule data mining to supervised object recogni- tion by mining spatially grouped SIFT descriptors. This made use of the algorithms ability to efficiently compute the distinctive spatial combinations of the SIFT features.

Chum [5] proposes a min-Hash based approach to detect rare co-occurrences of data in groups of image in a similar fashion to that of APriori data mining.

Within the temporal domain, Gilbert et al [10] argue that with many other action recognition approaches [15, 16, 8], the features used are engineered to fire sparsely, to ensure that the overall problem is tractable. However, they sug- gest that this can sacrifice recognition accuracy as it can- not be assumed that the optimum features in terms of class discrimination are obtained from this approach. In con- trast, they take an over complete set of Harris corners [13], group them spatially and temporally and mine out the op- timal feature combinations, to be used to classify the video sequences.

Chum et al [6] demonstrated the ability of min-Hash to efficiently identify near duplicate images within datasets.

This was extended further by Chum et al [7] to an effi- cient fast method to approximate the histogram intersec- tion of images to improve near duplicate image detection.

We bring many of the tools and ideas together to iteratively cluster data regardless of source or representation, based on the idea of an image signature  3. Input Data Signature An image signature is a frequency histogram of a set  of discrete symbols similar in nature to the popular Bag of Words (BoW) model [17, 25]. In fact we demonstrate how a BoW histogram can be used as the initial stage of the sig- nature in section 7. However there are two key differences from a classical BoW: Firstly the signature is increased in size to accentuate symbols and combinations of the symbols that discriminate or provide similarity between examples of classes. Secondly the signature can be based on any feature and we demonstrate signatures based on both BoW repre- sentations and other classifier responses for both image and video media  Figure 1 gives an overview of the approach in this paper.

First the feature classifiers are applied to the input sample to form the initial signature. The visual words of the signature could come from unsupervised clustering of training data, as is commonplace for BoW approaches, or as the output of a set of classifiers. We will demonstrate both approaches in the results section. This signature is then converted from a weighted histogram into the min-Hash representation to facilitate high speed similarity measures between the input samples. The min-Hash process provides a signature-wise similarity, which the user can utilise to select M signatures  Figure 1. An overview of the proposed approach  that are true positive and a single false positive for a par- ticular class. These selected signatures are then mined to identify the distinctive combinations of words. The rules of mining are then converted into new compound visual words and appended to all the signatures, which will have the ef- fect of pulling the signatures from the positive examples closer together. This process is then iterated, allowing a user to iteratively cluster data.

4. Similarity of signatures Min-Hash was originally developed for near-duplicate  detection of text [3] and was more recently adapted for the near duplicate detection of images [7]. It is a randomised hashing approach, where the computation is proportional only to the number of input samples rather than the size of the vocabulary. This means that it is ideally suited for use as the similarity measure for image signatures which can be of high and increasing dimensionality.

4.1. The min-Hash algorithm  The distance measure between two input samples is com- puted as the similarity of sets S1 and S2, which is the ratio of the number of elements in the intersection, over the union of the two sets.

sim(S1, S2) = |S1 ? S2| |S1 ? S2|  (1)  min-Hash is able to estimate sim(S1, S2) without perform- ing an exhaustive naive element by element comparison of S1 and S2. In order to estimate sim(S1, S2), N ran- dom permutations, ?, of the vocabulary, ?, are formed.

These are used to form min-Hashes of the input samples.

A min-Hash is defined as min ?(Si), i.e. the minimum element position of the ? permutation that is present in the set Si. For example, given the visual word vocabu- lary of ? = A,B,C,D,E, F and the three sets {A,B,C}, {B,C,D} and {A,E, F}, with 4 random permutations, N , the resultant min Hashes are shown in Table 1. The similar- ity, sim(S1, S2) is estimated as l(1,2)/N , where  l(1,2) = ?  (min ?(S1) = min ?(S2)) (2)     Table 1. Example of the 4 random permutations and resultant min- Hashes  Permutations min-Hash ABC BCD AEF  A B E F C D 1 1 1 B C E A F D 1 1 3 E A B A D F 2 3 1 F A E D B C 2 4 1  For the example in table 1, the estimated similarity be- tween sets {A,B,C} and {B,C,D} will be 0.5 as they share 2 min-Hashes, which is the same as the exhaustive (naive) similarity, while the sets {A,B,C}, and {A,E, F} share 1 min-Hash, with an estimated similarity of 0.25, compared to the exhaustive value of 0.2.

The false positive rate can be further reduced by grouping the min-Hash results into ?sketches?. Where K(S1) is the sketch {min ?1(S1), ...,min ?n(Sn)} con- sisting of n Hashes. The grouping process will only estimate the similarity for input sample pairs that have at least m (set as 1) identical sketches out of the h sketches (K1, ...,Kh). For example, taking Table 1, with a sketch size of n = 2, the sets {A,B,C}, {B,C,D} and {A,E, F} would be represented by the sketches ((1, 1), (2, 2)), ((1, 1), (3, 4)), ((1, 3), (1, 1)) re- spectively. The only sketch match would be {A,B,C}, {B,C,D} in the first sketch (1, 1), with no other pair con- sidered similar.

4.2. Histogram weighting approximation  The original min-Hash algorithm [6] is designed for a set of uniformly weighted symbols, therefore to be able to rep- resent the frequency of symbols, the algorithm is adapted.

For a visual vocabulary containing |X| visual words or features, e.g. X = {A,B,C}, ti is the frequency re- sponse of the feature e.g., t1 = {3, 0, 2} t2 = {2, 1, 0}.

In order to convert the frequency based image signature into a min-Hash set of uniform symbols, the symbols are duplicated based upon their frequency to produce t1 = {A1, A2, A3, C1, C2} t2 = {A1, A2, B1}.

5. Expanding signatures through co-occurring discriminatory features  With challenging datasets including recognition of real- istic video actions and the inherent noise and redundancy of any signature based upon a BoW approach, it is likely that min-Hash will produce falsely matching signatures that are unrelated. This is likely as there is often minimal inter (across) class variation, while lacking intra (within) class similarity. Therefore we propose a novel approach to ?pull?  positive sets together. Association Rule data mining will be used to identify the compound visual words that are distinc- tive and descriptive to a selected subset of the true positives (positive transaction sets) when compared to the false posi- tives (a negative transaction set). The new compound visual words are then added to all the image signatures and this, in-turn, will provide an increase in intra class similarity.

To identify the unique set elements, a version of asso- ciation rule data mining called APriori [1] is used. This has been used to great success for both image [22] and video [11] recognition, as it is well suited and computa- tionally efficient for large amounts of data. This is a brief introduction to the data mining APriori algorithm, but for a more detailed explanation see [11]. The algorithm, is de- signed to search databases and identify the set elements that co-occur most frequently within the positive sets with re- spect to the negative sets. The frequency of a set element is related to the support and confidence of an association rule.

An association rule of the form A ? B is evaluated by looking at the relative frequency of its antecedent and con- sequent parts i.e. the set elements A andB. The support for a set element is the probability that a Transaction contains the set element. For A, this is calculated as the size of the set of all the transactions T , such that T is an element of the overall database of the transactions D and A is a subset of T , normalised by the size of D. The support of the rule A? B is therefore  sup(A? B) = |{T | T ? D, (A ?B) ? T}| |D|  (3)  and measures the statistical significance of the rule. The confidence of a rule is then calculated as  conf(A? B) = sup(A ?B) sup(A)  = |{T | T ? D, (A ?B) ? T}| |{T | T ? D,A ? T}|  (4)  The support for the rule is the probability of the joint occurrence of A and B i.e. P (A,B) while confidence is the conditional probability P (B|A).

In addition to the set elements being frequent, they must also be discriminative with respect to the negative set. To achieve this, the algorithm is run on transactions from both the positive and negative sets. The transaction vectors of all examples are appended with a label, ?, that identifies if the set is a positive or negative example. The results of data mining then include rules of the form (A,B) ? ? and an estimate of P (?|A,B) is given by the confidence of the rule. As the Transaction database contains both posi- tive and negative training examples P (?|A,B) will be large only if (A,B) occurs frequently in the positive examples     but infrequently in the negative examples. If (A,B) oc- curs frequently in both positive and negative examples, then P (?|A,B) will remain small. The support threshold is the number of positive transactions over the total number of transactions, while the confidence threshold is 100, to en- sure an association rule is only found if the elements are within all the positive sets and none of the negative sets.

6. Iterative signature learning  The mined association rules are employed to adjust the min-Hash representation of the image signatures. Each rule is taken in turn and if it contains the returned mined sub- set an additional min-Hash symbol element is added. This will accentuate common and distinctive elements of the pos- itive rules from the mining and increase their overall sim- ilarity - inessence ?pull? the positive signatures together.

For example, using the image signatures from section 4.2, t1 = {A1, A2, A3, C1, C2} t2 = {A1, A2, B1}. If the as- sociation rule returned from the mining was a subset of ti, e.g. Ax where x is any number, the element (A4) would be added to set t1 and the element (A3) would be added to set t2. The process would be repeated over all the input sets, however if the set does not contain the subset (Ax), it would not be incremented. This increased weighting on the subset (A) would ?pull? together sets that contain subset (A) over time improving accuracy. In addition, the min- ing can return association rules that contain multiple sub- sets that together are descriptive and distinctive. Using the same example, if the mining returns the rule (A2, B1), it would not be appended to t1 as the set does not contain any (B) elements. However, it would be appended to t2, mak- ing t2 = {A1, A2, B1, AB1} and then treated as a single element. This means that for the min-Hash permutations to match, both sets would have to contain the symbol A2 and B1, not just a subset. This has the ability to reduce the confusion between classes further.

The adjusted min-Hash representations of the input im- age signatures can then be used to recompute similarities based on the set overlap and the mining cycle is repeated.

7. Results  To illustrate the generalisation of the approach, and to evaluate the quality of the clustering and categorisation of the approach, both image and video clustering were in- vestigated. We demonstrate iterative grouping on the im- age categorisation dataset of [21] using their histograms of colour and edges to form the initial signatures, on the cal- tech101 [9] dataset using the BoW features of [4], on the KTH dataset and the YouTube dataset using the compound corners features of [10], and on the Hollywood2 dataset us- ing the BoW features of [19]. To evaluate grouping per- formance, each item is assigned to the class of its nearest  neighbour.

7.1. Images  Designed to demonstrate that the approach is not re- stricted to a single media type and to illustrate the approach, the images in two image datasets are grouped. First the Im- age Scene dataset used in [21] is tested. It consists of 100 images spread across 4 classes of landscape: city, jungle, mountain and winter, with examples shown in Figure 2  (a) city (b) jungle  (c) mountain (d) winter  Figure 2. Image Dataset examples  In order to form the signatures for the images, an 11 bin colour histogram and a 42 bin edge orientated histogram were concatenated and histograms computed over each im- age. The image signature database had, ? = 432, with N = 1500 min-Hash permutations, and a sketch size n = 3.

For each query image, the distance to all other images in the dataset was computed using the min-Hash histogram simi- larity measure and a nearest neighbour assignment used to group all images. The class label of the resultant maximised image was compared to the query image, and a true positive occurred if both images had the same class label. This is visualised in Figure 3(a), which shows the correlation be- tween the query image and the rest of the dataset. The white dot shows the closest image in the dataset. Self similarity along the diagonal is ignored but a strong block diagonal shows good class clustering. Class boundaries are indicated as green boxes.

It can be seen that initially there is a degree of confu- sion between the classes (Figure 3(a)) and this is reflected in the 65.5 % overall accuracy. To improve the accuracy, the iterative mining process is run by selecting M true pos- itive examples from a class and 1 false positive. The APri- ori identifies the elements within the min-Hash sets that are distinctive and discriminative to the true positive examples with respect to the false positive example and all the im- age signatures are adjusted accordingly. Figure 4 shows that     (a) (b)  Figure 3. (a) Initial confusion between query image and dataset, (b) Confusion after 7 iterations using M = 1 (Green lines indicate class boundaries  Figure 4. Image Dataset Accuracy with respect to iteration level, and varying M  the accuracy increases with respect to iterations for different values of M . For example with M = 3 it takes 5 iterations to achieve an accuracy of 91%, therefore requiring a total of 20 labelled images. However with M = 1 it takes only 7 it- erations, therefore only requiring 14 labelled images for the full dataset of 100 images. The resultant confusion matrix between images in the dataset is shown in Figure 3(b).

To provide a more challenging test and demonstrate flex- ibility to features, the commonly used benchmark dataset, Caltech101 [9] was used for evaluation. The dataset con- sists of 101 object categories with between 31 to 800 im- ages per category. The semantic image signature is then iteratively adapted using 15 training examples randomly se- lected from each class. Then these image signatures were tested on another 50 unseen ?test? images from each class by performing a nearest neighbour assignment to the clos- est class and this process was repeated 10 times. The ini- tial image signatures were formed from BoW histograms of standard SIFT descriptors with the dimension reduced to 30 as employed in [4]. The average results of the 10 runs are shown in table 2 where a 40% increase in performance is seen from the baseline signature, this compares well with accuracies achieved in classification by Cai [4] .

Table 2. Accuracy of Caltech101 dataset Approach Accuracy  Cai [4] 64.9% Baseline min-Hash 21.54%  Sig min-Hash 59.7%  7.2. YouTube Video Dataset  The YouTube dataset [18] consists of eleven categories: basketball shooting, cycling, diving, golf, horse riding, jug- gling, play swings, tennis swinging, trampolining, volley- ball, and dog walking. This real life dataset contains over 1200 videos, that exhibit large variations in camera motion, object appearance and pose, object scale, viewpoint, clut- tered background and illumination conditions. Some exam- ples of the dataset are shown in Figure 5.

(a) Cycling (b) Juggling (c) Basketball  Figure 5. YouTube dataset examples  7.2.1 Feature Classifier  To form the initial image signature, a histogram is com- puted using the features by Gilbert et al [10]. These con- sist of compound corner classifiers trained on the KTH dataset [23], with results on the KTH dataset also shown in the following section. While these features might be optimal for classification of the KTH dataset, it is not the case for other datasets and it is unlikely that they are opti- mal in any sense for the YouTube dataset. However, as they were constructed to discriminate between different types of motion, they should provide some class discrimination over other datasets including motions. We will rely on the image signature to differentiate between the classes. Other than the different features used in the initial image signature, the experimental process is the same as outlined for the image datasets.

For the video domain, the size of the initial signatures now increases to 3108, due to the additional feature classi- fiers used. The results over the whole YouTube dataset are shown in Figure 6(a), for the accuracy with respect to the number of iterations, with varying sizes of M . The size of the vocabulary is shown in Figure 6(b), while the speed for each iteration is shown in Figure 6(c)  For the YouTube dataset it can be seen that the accuracy increases from a basic min-Hash baseline of 72%, to 97%     (a) YouTube Dataset Accuracy with respect to iteration level, and varying M  (b) Signature size for YouTube, with varying M  (c) YouTube Time taken per iteration for varying M  Figure 6. The YouTube dataset results  correctly clustered and takes only 36 labelled groundtruth videos in 6 iterations using M = 5. This is a small amount, with other standard approaches using a leave one out training-test partitions requiring 1121 labelled videos for training. More importantly, as the accuracy increases, so does the complexity of the vocabulary. However, the av- erage time for each iteration increases by only 1 second for M = 5. This indicates the important feature of min-Hash, that it is computationally affected only by the number of input samples or signatures and not the complexity of the actual signature.

For a comparison with more standard published ap- proaches, we adopt the commonly used Leave-One-Out Cross-Validation. More specifically, for the YouTube dataset, adopting the settings given in [18], the dataset was divided into 25 subsets, out of which 24 subsets were used  for training and the remaining subset was used for test- ing. Where for each unseen test subset, the other subsets were used to adjust the signatures using M = 5, for 4 it- erations. However as only 4 iterations were used only 24 videos needed to be groundtruthed unlike the 1121 videos used in the other approaches. The semantic clusters were it- eratively built on the 24 training subsets, and then classified by performing a nearest neighbour assignment to the closest class. Table 3 shows the results for our signature min-Hash approach compared to other recently published results on the same dataset.

Table 3. Accuracy of YouTube dataset Approach Accuracy  Cinbis [14] 75.2% Liu [18] 71.2%  Bregonzio [2] 63.1% Baseline min-Hash 56.4%  Sig min-Hash 79.7%  The results compare well to other state of the art results, especially as the feature classifiers used to form the image signatures are built on the KTH dataset. In addition, this approach needs very little actual groundtruthed data, just 44 labelled images per class, so a total of 528 labelled ex- amples in total.

7.3. KTH Dataset  The well known and popular KTH dataset [23] was also evaluated, it contains 6 different actions; boxing, hand- waving, hand-clapping, jogging, running and walking, ex- amples of each action are shown in Figure 7.

(a) boxing (b) handclapping (c) handwaving  Figure 7. Examples from the KTH dataset  There are a total of 25 people performing each of the 6 actions, 4 times; giving 599 video sequences (1 sequence is corrupt). Each video contains 4 instances of the action to- talling 2396 unique actions. We present results using train- ing and testing data split as suggested by Schu?ldt, with 8 people for training, and 8 people testing. The image signa- tures were iteratively retrained using the 8 training people.

To construct the image signatures, the same features as used for the YouTube dataset are employed. These are pro- vided by Gilbert et al [10] and consist of compound corner classifiers trained on the KTH dataset [23].

Table 4. Accuracy of KTH dataset Approach Accuracy  Schu?ldt [23] 71.71% Laptev [16] 91.8% Laptev [16] 91.8% Wang [26] 92.1%  Gilbert [10] 95.7% Baseline min-Hash 44.3%  Signature min-Hash APriori 91.2%  Table 4 shows the results, it can be seen that the initial baseline min-Hash value is very low at only 44.3% however after seven successive iterative selections, the accuracy is increased to 91.2%. The 91.2% compares very favourably with the other state-of-the-art, especially when it is noted that as M = 5, only 42 videos were required as labelled training data, instead of the normal 192 videos.

7.4. Hollywood2 Dataset  To provide additional challenge, the Hollywood2 dataset [19] is evaluated. It builds upon [16] and consists of 12 action classes; AnswerPhone, DriveCar, Eat, Fight- Person, GetOutCar, HandShake, HugPerson, Kiss, Run, Sit- Down, SitUp, StandUp with around 600,000 frames or 7 hours of video sequences split evenly between training and test datasets, of clips made from hollywood movies.

(a) AnswerPhone (b) HandShake (c) HugPerson  Figure 8. Examples from the Hollywood2 dataset [19]  We obtain HoG/HoF descriptors using the interest point detection method of [15], and construct a visual dictionary using K-Means with K = 4000 visual words and train a SVM classifier. The classifier response is used as the input for the image signature. For the Hollywood2 dataset, the clean train and test partitions proposed by Marszalek [19] were used. There are a total of 810 videos within the train- ing subset spread over the 12 action classes, with 884 test sequences. Table 5 shows the baseline and final accuracy of the iterative min-Hash APriori method.

The final accuracy of 43.2% is nearly double the original baseline of 26%. This improvement is from using 8 itera- tive selections of 5 true positive classifications and a single negative classification in the training subset. This means that a total of 48 videos are actual labelled compared to the approaches by Gilbert [10] and Wang [26] requiring all of the 810 training videos.

Table 5. Accuracy of Hollywood2 dataset Approach Accuracy  Marszalek [19] 35.5% Han [12] 42.1%  Wang [26] 47.7% Gilbert [10] 50.9%  Baseline min-Hash 26.9% Signature min-Hash APriori 43.2%  7.5. Computational costs  The approach has been designed to be efficient, allowing for near real time operation. Table 6 shows how that as the dataset size increases so does the time taken, from 1 sec- ond per iteration on the Image Scene dataset, to 40 seconds for the Hollywood2 dataset with 884 videos. However the  Table 6. Computational Time of datasets  Dataset Dataset Size Img Sig Size Iter Time Image Scene 100 53 1 sec Caltech101 5050 2150 30 sec YouTube 1200 3108 63 sec  KTH 768 1204 25 sec Hollywood2 884 4503 45 sec  approach is ideally suited to video based problems as the computational cost is mainly proportional to the size of the dataset, not the dataset complexity.

8. Conclusion While the iterative clustering procedure provides excel-  lent results across a range of datasets and features using rel- atively little training data, it is important to note that in the case of Sec 7.3 and 7.4, while the grouping requires mini- mal data, the features themselves have been learnt over the entire training data. This serves to highlight that features that are learnt for classification, may not be the best fea- tures for grouping or clustering using naive distance met- rics, but through the use of signatures, these features can be reweighed appropriately. We have presented an approach to efficiently cluster similar videos and images from large datasets. By selecting a small subset of the true positive and false positive results, we can pull additional true positive examples of the classes together by identifying the distin- guishing components of the input samples signature. The use of a signature for each input sample allows for a very flexible generic approach to the input format. In future, the implemented feature classifiers; mined compound cor- ners; HoG/HoF; edges and colour information, could be ap- pended by the response of further feature classifiers, such     as SIFT. This additional information about the input sample would then be able to increase the performance but due to the min-Hash there would be minimal additional computa- tional costs.

