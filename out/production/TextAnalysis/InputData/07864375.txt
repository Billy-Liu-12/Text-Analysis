An Adaptive Fusion Strategy for Distributed Information Estimation Over Cooperative

Abstract? In this paper, we study the problem of distributed information estimation that is closely relevant to some network- based applications, such as distributed surveillance, cooperative localization, and optimization. We consider a problem where an application area containing multiple information sources of interest is divided into a series of subregions in which only one information source exists. The information is presented as a signal variable, which has finite states associated with certain probabilities. The probability distribution of information states of all the subregions constitutes a global information picture for the whole area. Agents with limited measurement and communication ranges are assumed to monitor the area, and cooperatively create a local estimate of the global informa- tion. To efficiently approximate the actual global information using individual agents? own estimates, we propose an adaptive distributed information fusion strategy and use it to enhance the local Bayesian rule-based updating procedure. Specifically, this adaptive fusion strategy is induced by iteratively minimizing a Jensen?Shannon divergence-based objective function. A con- strained optimization model is also presented to derive minimum Jensen?Shannon divergence weights at each agent for fusing local neighbors? individual estimates. Theoretical analysis and numerical results are supplemented to show the convergence performance and effectiveness of the proposed solution.

Index Terms? Cooperative information estimation, adaptive distributed fusion, nonlinear constrained optimization, multi- agent networks, Jensen-Shannon divergence.



I. INTRODUCTION  NETWORK-TYPE systems are general in both nature(such as fish schools, ants and honeybee swarms) and engineering (such as unmanned aerial vehicles, mobile robots, and other wireless sensor networks). In these systems, infor- mation estimation and fusion over multi-agent networks is of great significance, which can support individual agents to achieve some common tasks in a distributed manner such as  Manuscript received May 15, 2015; revised December 19, 2015, May 20, 2016, and November 27, 2016; accepted February 12, 2017. Date of publication February 24, 2017; date of current version April 19, 2017. This work was supported by the National Natural Science Foundation of China under Grant 61672082. (Corresponding author: Zhengguo Sheng.)  D. Tian and J. Zhou are with the Beijing Key Laboratory for Cooperative Vehicle Infrastructure Systems and Safety Control, Beijing Advanced Inno- vation Center for Big Data and Brain Computing, School of Transportation Science and Engineering, Beihang University, Beijing 100191, China (e-mail: dtian@buaa.edu.cn; jianshanzhou@foxmail.com).

Z. Sheng is with the Department of Engineering and Design, University of Sussex, Richmond 3A09, U.K. (e-mail: z.sheng@sussex.ac.uk).

Communicated by T. Javidi, Associate Editor for Communication Networks.

Color versions of one or more of the figures in this paper are available  online at http://ieeexplore.ieee.org.

environmental monitoring, global localization, self-defending or attacking invaders, etc. However, some challenges exist to be dealt with for practically realizing highly-scalable infor- mation estimation and fusion paradigms such as limited indi- vidual detection and interaction, lack of centralized control, and dynamic and noisy nature of measurements obtained by every agent. In some application scenarios relevant to object detection or target locating, the goal of an information estimation model is to estimate the actual probability that a certain target is present in a given closed region. The probability of a target existence within a given surveillance region is usually assumed to follow the Bernoulli distribution and all of the probabilities corresponding to different regions constitute a so-called probability map [1]?[3]. The estimation of the individual probability map can be iteratively updated by following Bayesian rule. For example, in [3], to realize a distributed strategy for probability map estimation, Bayesian updating is combined with the traditional consensus proto- col, which is used for fusing different individual probability maps of the neighbors of an agent. Nevertheless, although the proposed estimation fusion strategy based on Bayesian updating is useful in the static object detection, it may fail to be applied in a more general scenario where the probability of a subregion state or an object state does not follow the Bernoulli distribution.

Additionally, many distributed solutions have been pro- posed in the context of adaptive distributed LMS (Least- Mean-Square) estimation, which include the incremental adap- tive strategies [4]?[6], the consensus based strategies [7]?[9] and the adaptive diffusion strategies [10]?[14]. Specifically, two diffusion strategies ATC (Adapt-Then-Combine) and CTA (Combine-Then-Adapt) have been proven to be powerful to realize distributed optimization and cooperative learning over networks [12], [13], [15]?[19]. In most of these stud- ies, the distributed optimization is always modeled as an unconstrained LMS estimation problem, in which the global objective function is formulated as a sum of all individual components. The global function has to be localized so that the distributed optimization procedure can be induced by adopting the steepest descent algorithm. However, the unconstrained LMS estimation solutions are not suitable in some specific application situations, where a global task should be formu- lated as a constrained distributed optimization problem. In this context, distributed solutions are required to satisfy some certain estimation constraints at each agent, which could be  See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

TIAN et al.: ADAPTIVE FUSION STRATEGY FOR DISTRIBUTED INFORMATION ESTIMATION 3077  more challenging and beyond the conventional unconstrained LMS estimation schemes. Since the information estimation over a distributed system can be modeled as a distributed optimization problem, some distributed optimization algo- rithms with consideration of certain constraints have been proposed [20], [21]. In [21], two distributed optimization cases are considered: the first case does not take into account the equality constraints and employs Lagrangian relaxation approach to devise a distributed Lagrangian primal-dual sub- gradient algorithm; the second case takes into account the equality constraints and adopts a penalty relaxation approach.

Although both of equality and inequality constraints are taken into account in the second case, it require identical local constraint sets to guarantee the Slater?s condition due to the nature of penalty relaxation approach.

In this paper, we present an ATC-type distributed frame- work, which includes a nonlinear constrained optimization model and a fusion-weight optimization model. The objective function of the optimization model is defined based on the Jensen-Shannon divergence [22], which allows the individual fusion to iteratively approach to the actual information in terms of minimizing the information-theoretic divergence measure based on the Shannon entropy. Besides, the fusion-weight optimization model is proposed to iteratively adapt the fusion weights of each agent according to estimation results collected from the other neighbors. Different from the existing work, the distributed optimization algorithm proposed in this paper does not need the assumption of identical constraint sets.

Furthermore, another essential difference between our work and [21] is the distributed framework, where we adopt a projection gradient approach and the distributed processing is based on ATC. By resorting to the probabilistic theory, the distributed information estimation is generalized as the process of learning and approaching to probability distributions over the multi-agent network. We do not assume any specific given distribution (for instance, the Bernoulli distribution adopted in [1], [3], [23], and [24]) for the probabilities of the infor- mation states. Hence, the proposed solution can be deployed for a wide range of distributed applications once the detection information of interest is appropriately represented as a certain discrete probability distribution.

The remainder of the paper is organized as follows.

In Section II, relevant preliminaries are briefly outlined for the distributed information estimation problem, which include main mathematical notations and definitions. In Section III, we introduce the non-cooperative individual information estima- tion scheme, where the Bayesian rule is adopted to incorporate the individual measurements. In Section IV, the nonlinear constrained optimization model is proposed for improving individual estimation. Section V presents the experimental results of the adaptive fusion strategy. Finally, Section VI concludes this paper.



II. PRELIMINARIES  A. Notations and Definitions  Notations: Throughput this paper, we use col{x1, . . . , xn} to represent a column vector constructed by stacking entries  x1, . . . , xn on top of each other, and diag{x1, . . . , xn} to represent a diagonal matrix with diagonal entries x1, . . . , xn .

Besides, let 1n?1 be a column vector of n dimensions all of whose entries are equal to 1, and 0n?1 be a full-zero column vector of n dimensions. The identity matrix of size n is denoted by In . Unless otherwise specified, all vectors are column vectors and denoted by boldface lowercase letters, while matrices are denoted by boldface capital letters.

Definitions: Given a multi-agent network, we use a graph G(V, E) to represent its communication topology with V = {1, 2, . . . , n} denoting a node set and E ? V?V denoting an edge set consisting of unordered pairs E = {(i, j)|i, j ? V} excluding self-loop (i, i). (i, j) ? E represents a mutual com- munication between the agents i and j , and any agent i ? V is supposed to be periodically communicating with its immediate neighbors { j | j ? V, ( j, i) ? E} through one-hop broadcasting- based communication. We assume that the communication graph is a connected graph, i.e., the communication is bi-directional and there is always a path between any two agents in the network.

B. Problem Formulation  The multi-agent network is assumed to be deployed to detect a geographical region that contains multiple information sources. The set of the whole information sources is denoted by ?, whose cardinality is m, i.e., m = |?|. Then, the entire surveillance region can be divided into a series of surveillance subregions, each of which corresponds to one information source. An agent i can only detect a part of subregions, i.e., a fraction of information sources. The set of partial information sources in i ?s detection range is defined as ?i , ?i ? ? for all i ? V and ?  i?V ?i = ?. Without loss of generality, we  also assume that the detection regions of any two different agents i1 ?= i2 (i1, i2 ? V) are not identical, i.e., ?i1 ?= ?i2 .

This implies that two general situations are considered in our study: i) some information sources can be only observed by any single agent, and ii) some others can be observed by multiple (at least two) agents simultaneously. We need to point out that in the first situation, only a single agent?s observa- tion contributes to information gain of the overall network in estimation of an information source without overlapped detection. As for any other agent who is blind in observation of this information source, the sense of a fusion strategy is reduced to the point that the agent simply needs to collect the other?s useful observation information diffused over the network and incorporates it into its own individual estimation, at the meanwhile keeping silence in order to avoid diffusing its blindness. By contrast, in the second situation, several agents with overlapped detection can contribute to information gain of the network in estimation of an information source in their common detection region through diffusing and fusing the multiple observation information with a certain fusion strategy. The sense of the fusion strategy lies in that an agent with overlapped detection region can combine several others? observation information with its own to enhance its own individual estimation, at the same time diffusing its own observation information for others? fusion. In this paper, we     provide a unified algorithmic information fusion framework to deal with both of the two considered situations.

Furthermore, we point out that the information released by the sources can be some parameters of interest such as tem- perature field, multi-target locations, or some phenomenons of interest in the corresponding subregions. We divide the detection time into discrete time intervals t ? Z?0. Consid- ering the dynamic and noisy nature of information detected by agents, we formulate the detection signal at time interval t corresponding to an information source k ? ? as a random variable vk(t), such that its time-dependent variability can be modeled by a certain discrete probability distribution. The ranges of the detection signal vk(t) are represented as various information states. Let the total number of the information states associated with the source k be Lk . The l-th information state is denoted by Sk,l =  [ ak,l , bk,l  ) for l = 1, . . . , Lk ? 1,  while Sk,Lk = [ ak,Lk , bk,Lk  ] . ak,l and bk,l are the lower and  the upper bounds of the detection signal vk(t) in the state Sk,l , which satisfy bk,l = ak,l+1 for l = 1, . . . , Lk ? 1. Each information state Sk,l is associated with a certain probability, denoted by p  ( vk(t) ? Sk,l  ) , which indicates the possibility of  the detection signal vk(t) currently ranging within Sk,l . Hence, for any k ? ?, we have ?Lkl=1 p(vk(t) ? Sk,l ) = 1. The objec- tive of the information estimation over the multi-agent network is to enable each individual agent to approach the actual probability distributions of information states of the entire sur- veillance region,  { p(vk(t) ? Sk,l )|l = 1, 2, . . . , Lk; ?k ? ?  } ,  through local measurements, estimations and interactions.

Define by vk(i, t) the detection signal of an agent i  received from the information source k ? ?i at any time interval t . Then, this agent is assumed to take multiple measurements on vk(i, t) during this time inter- val. We collect these real-time measurements into a vec- tor vk(i, t) =  { vk,s(i, t)|s = 1, 2, ..., Vi  } , where vk,s(i, t)  is the s-th sample on the information source k and Vi denotes i ?s sampling number. With the individual cumula- tive observations {vk(i, ? )|? = 0, 1, . . . , t}, the agent i can plot a discrete histogram of vk(i, t), which implies a dis- crete conditional probability distribution of the informa- tion states of k,  { pk,l(i, t) = p  ( vk(i, t) ? Sk,l |vk(t) ? Sk,l  )} ,  where ?pk,l(i, t) ? 0 and ?Lkl=1 pk,l(i, t) = 1. Indeed, this conditional probability pk,l(i, t) representing the possibility that the real-time detection signal range estimated by i is in the l-th information state of k given that the actual detection signal value exactly belongs to the same state. The conditional probability pk,l(i, t) indicates the accuracy of observed infor- mation at i . Correspondingly, the false detection probability pk,l(i, t) is  pk,l(i, t) = p(vk(i, t) ? Sk,l |vk(t) ? Sk,l? , l ? ?= l)  = ?Lk ?l? ?=l pk,l? (i, t) ?Lk ?l? ?=l 1  = 1? pk,l(i, t) Lk ? 1 (1)  We further denote the actual probability distribution of information states of any subregion k ? ? at time interval t as pk(t) = col  { p(vk(t) ? Sk,l )|l = 1, . . . , Lk  } , and then define  p(t) = col { pk(t)|k = 1, . . . , m} to collect all of the probabil- ity distributions. Similarly, for any individual i ? V , we denote  pk(i, t) = col {  p(vk(i, t) ? Sk,l |vk(t) ? Sk,l )|l = 1, . . . , Lk } ,  and p(i, t) = col { pk(i, t)|k = 1, . . . , m}. In addition, we use q to denote the total number of information states of the entire region, i.e., q =?mk=1 Lk . We note that the sizes of pk(t) and pk(i, t) are identical to Lk , while the sizes of p(t) and p(i, t) are identical to q .



III. INFORMATION ESTIMATION BASED ON INDIVIDUAL OBSERVATION  The agents can yield the real-time posterior information by combining the information accuracy of their real-time observations, pk,l(i, t), based on the well-known Bayesian rule, and have  p(vk(t) ? Sk,l |vk(i, t) ? Sk,l ) = p(vk(i, t) ? Sk,l |vk(t) ? Sk,l )p(vk(t) ? Sk,l )  p(vk(i, t) ? Sk,l |vk(t) ? Sk,l )p(vk(t) ? Sk,l )+?k,l (i, t) = pk,l(i, t)p(vk(t) ? Sk,l )  pk,l(i, t)pk,l (vk(t) ? Sk,l )+?k,l(i, t) (2)  where ?k,l(i, t) is:  ?k,l(i, t)  = p(vk(i, t) ? Sk,l |vk(t) ? Sk,l? , l ? ?= l)p(vk(t) ? Sk,l , l ? ?= l)  = Lk?  ?l? ?=l p(vk(i, t) ? Sk,l |vk(t) ? Sk,l? )p(vk(t) ? Sk,l? ) (3)  By using (1), ?ki (n, t) can be further expressed as:  ?k,l(i, t) = Lk?  ?l? ?=l pk,l(i, t)p(vk(t) ? Sk,l? )  = 1? pk,l(i, t) Lk ? 1  Lk?  ?l? ?=l p(vk(t) ? Sk,l? )  = 1? pk,l(i, t) Lk ? 1  ( 1? p(vk(t) ? Sk,l )  ) (4)  Generally, because of the existence of noises in agents? observations, the actual probability distribution of information states corresponding to any k ? ?,{  p(vk(t) ? Sk,l )|l = 1, . . . , Lk } , is unknown to these  agents. At this point, it is unpractical to directly apply the equation (2) to distributed estimation since the calculation of this formula requires the exact knowledge of the parameter p(vk(t) ? Sk,l ). To obtain the current posterior information, we adopt the recursive Bayesian updating method to combine the past posterior information and the current observation information. For simplicity, we denote i ?s real- time estimation on the posterior probability by ?k,l (i, t), i.e., ?k,l(i, t) = p(vk(t) ? Sk,l |vk(i, t) ? Sk,l ). Then, we use the previous posterior estimation, represented by ?k,l(i, t ? 1), to substitute the unknown p(vk(t) ? Sk,l ) in the right term of (2) and get  ?k,l(i, t)  = pk,l(i, t)?k,l (i, t?1) pk,l(i, t)?k,l (i, t?1)+ 1?pk,l (i,t)Lk?1  ( 1? ?k,l(i, t ? 1)  ) (5)    TIAN et al.: ADAPTIVE FUSION STRATEGY FOR DISTRIBUTED INFORMATION ESTIMATION 3079  In this paper, we generalize the term pk,l(i, t) in equation (5) such that it can reflect any distribution pattern captured by the individual observation information. The essential concept of (5) is that it allows us to effectively incorporate the past estimated information, ?k,l (i, t?1), and the real-time observed information, pk,l(i, t), into the current individual estimation, ?k,l(i, t). To analyze the convergence of the Bayesian updat- ing (5), we rearrange the equation (5) as  (  ?k,l(i, t) ? 1  )  = 1 Lk ? 1  ( 1? pk,l(i, t)  pk,l(i, t)  )(  ?k,l(i, t ? 1) ? 1 )  (6)  and further derive a closed-form expression on ?k,l (i, t):  ?k,l (i, t) = 1 1+  ( 1??k,l (i,1) ?k,l (i,1)  ) (  Lk?1 )t?1?t  ?=2 1?pk,l (i,? )  pk,l (i,? )  (7)  where t ? 2. By introducing an auxiliary parameter a(t) a(t) = 1? pk,l(i, t)  pk,l(i, t) (Lk ? 1) (8)  we can simplify (7) as  ?k,l (i, t) = 1 1+  ( 1??k,l (i,1) ?k,l (i,1)  )?t ?=2 a(? )  (9)  From (9), it can be found that the accuracy of the individual real-time estimation, ?k,l (i, t), depends on the number of the information states of the subregion k, Lk , and the accuracy of the individual observation, pk,l(i, t). Specifically, the influ- ences of Lk and pk,l(i, t) on the convergence of ?k,l (i, t) are summarized as the Lemma 1.

Lemma 1 (The Convergence of the Individual Bayesian Updating): Given the individual prior information state probability of any agent i ? V , pk,l(i, t) ? (0, 1) for l = 1, . . . , Lk , and the finite number of information states of any k ? ?, Lk ? 2, the following conclusions are held when calculating (9):  1) If 1Lk < pk,l(i, t) for ?t , limt?+? ?k,l (i, t) = 1 holds.

2) If 1Lk = pk,l(i, t) for ?t , ?k,l(i, t) does not converge.

Instead, it constantly equals to the initial posterior information estimation over all time intervals, i.e., ?k,l (i, 1) = ?k,l(i, t) for ?t .

3) If 1Lk > pk,l(i, t) for ?t , limt?+? ?k,l (i, t) = 0 holds.

Proof: The conditions required in the three cases  correspondingly result in three possible parameter a(t): 0 < a(t) < 1, a(t) = 1, and a(t) > 1 for ?t . Then, we examine the time-dependent cumulative product on a(t) under the three cases:  lim t?+?  t?  ?=2 a(? ) =  ? ??  ??  0, 0 < a(? ) < 1; 1, a(? ) = 1; +?, a(? ) > 1  (10)  Substituting the results in (10) into (9) leads to the result.

From Lemma 1, it can be seen that the individual estimation  can not be improved despite of its further measurements if the observed information accuracy stays at uniform level, i.e.,  pk,l(i, t) = 1Lk for ?t . Besides, if the quality of the individual observed information from the individual measurements is good enough, i.e., pk,l(i, t) > 1Lk , the individual estimation can converge to 1. Otherwise, it leads to failure in the estimation of the information state probability distribution.



IV. ADAPTIVE FUSION STRATEGY  A. Global Optimization Model  As discussed in Section II, some subregions may be out of the detection range of an agent i , which can be lumped in a set ? i = ?? ?i . The prior information corresponding to the subregion k ? ? ? i , pk?,l(i, t), can not be obtained from the individual measurements. From Lemma 1, it can be found that the recursive Bayesian updating scheme based on (5) can not be implemented in this situation. Therefore, a distributed cooperative solution for information sharing and fusion among local agents is needed to improve the accuracy of individual observed information. In order to propose a cooperative dis- tributed information estimation, we first develop a nonlinear constrained distributed optimization model. Since we formu- late the information of interest over the geographic region by a series of finite discrete probability distributions of information states of subregions, we can model the optimization objective based on the information theory. Specifically, the Jensen- Shannon divergence, as an information metric (also called information radius (IRad) [25]), is adopted to represent an estimation objective. It can measure the disparity between two finite random graphs and can reflect the mutual information between two related random variables [22]. Given a finite discrete probability distribution p = (p1, p2, . . . , pU )T where?U  u=1 pu = 1 and ?pu > 0, the amount of uncertainty of this distribution p, namely the entropy, can be calculated based on Shannon?s information entropy function E( p) [26]:  E( p) = ? U?  u=1  ( pu log2 pu  ) (11)  From Jensen?s inequality theorem, it shows that this Shannon information entropy (11) is a concave function of the multiple probabilities p1, p2, . . . , pU [22]. However, the equation (11) may not be applied under some discrete proba- bility distributions. For example, when one entry in p is equal to zero, i.e., pu = 0, (11) is not valid for numerical compu- tation. Hence, to overcome the drawback of the logarithmic function log(?), we do not directly adopt (11) in our following mathematical model. Instead, we establish a modified Shannon entropy by introducing a parameter ? into (11):  H ( p) = ? U?  u=1  ( (pu + ?) log2(pu + ?)  ) (12)  where ? ? (0, 1) is a positive constant but should be small enough. In (12), the range of the value of any pu is expanded to be [0, 1] rather than (0, 1).

Letting ?k = ( ?k,1, ?k,2  )T where ?k,1 and ?k,2 are weights  of the two information state probability distributions pk(t) and pk(i, t), respectively, satisfying ?k,1 + ?k,2 = 1 and     ?k,1, ?k,2 ? 0, we can define the Jensen-Shannon divergence of weights ?k between pk(t) and pk(i, t) by [22]:  J SDk ( pk(t), pk(i, t))  = H (?k,1 pk(t)+ ?k,2 pk(i, t) )  ?(?k,1 H ( pk(t))+ ?k,2 H ( pk(i, t)) )  (13)  Based on the Jensen-Shannon divergence (13), we further formulate an individual objective function for any agent i as follows:  fi ( p(t)) = ?  ?k?? J SDk ( pk(t), pk(i, t)) (14)  Subsequently, a global objective function can be defined by collecting all the individual components:  f ( p(t)) = n?  i=1 fi ( p(t)) (15)  For any k ? ?, the probabilities in pk(t) should satisfy an equality constraint,  ?Lk l=1 p(vk(t) ? Sk,l ) = 1, and inequality  constraints, ?p(vk(t) ? Sk,l ) ? 0. To provide the compact forms of these constraints, we introduce an equality constraint coefficient matrix, C = diag {1L1, 1L2, . . . , 1Lm  }T, and an inequality constraint coefficient matrix, E = Iq . It should be noted that the dimension of C is m ? q while E is indeed a q ? q identity matrix. The constraints on p(t) are defined as:  s.t .

{ E p(t) ? 0q C p(t) = 1m (16)  where the inequality constraint is element-wise.

Because the Jensen-Shannon divergence is a nonnegative  measure [22], f ( p(t)) is also a nonnegative real-value func- tion, i.e., f ( p(t)) ? 0 for any input p(t). Additionally, the smaller the Jensen-Shannon divergence f ( p(t)) is, the less the difference between the actual and the observed information distributions achieves. That is, the function f ( p(t)) can reach zero, if and only if any observed distribution pk(i, t) at the individual agent i totally matches the actual distribution pk(t). Therefore, treating the unknown p(t) as the decision variable and the prior probability distributions estimated from the observations, { p(i, t)|i ? V}, as input parameters, we can develop an optimization model where the Jensen-Shannon divergence based objective function (15) is expected to be minimized under the constraints (16):  min f ( p(t)) = n?  i=1 fi ( p(t))  s.t .

{ E p(t) ? 0q C p(t) = 1m . (17)  From (17), we see that the optimization model has linear constraints while its objective function is nonlinear. The fol- lowing Lemma 2 shows the convexity of its objective function f ( p(t)).

Lemma 2 (The Convexity of the Optimization Objective Function): Given ?k =  ( ?k,1, ?k,2  )T satisfying ?k,1 +?k,2 = 1 and ?k,1, ?k,2 ? 0 for k = 1, . . . , m, the optimization  objective f ( p(t)) in the global model (17) is a strictly convex function of p(t).

Proof: According to the Lemma 1 in the work [27], the Jensen-Shannon divergence based function is strictly convex, so that the objective function f ( p(t)) summing the Jensen- Shannon divergence functions of all the agents must be also strictly convex. Thus, Lemma 2 is proved.

B. Local Optimization Model  From the Lemma 2, it can be found that if an algorithm existing for the nonlinear optimization model can converge to a stationary point of f ( p(t)), this algorithm can converge to a global minimum of the model. In the application context of information estimation, such a global minimum indeed corresponds to the actual probability distribution of infor- mation states of the entire region. Hence, we consider that every individual agent?s goal is to approach the same actual global distribution, denoted by p?(t). As each individual has a common goal (determining the global distribution p?(t)), they are expected to share local independent observed information and perform local cooperative interaction with other neighbors.

Two essential issues arises when a distributed processing is considered: 1) how to enable each agent to adapt their individual estimation in real time according to its own and neighbors? continuous measurements; 2) how to enable a better local fusion of each agent?s and its neighbors? infor- mation to improve individual estimation performance rather than solely solving the global p?(t) on its own information.

To address these issues, we first propose a localized Jensen- Shannon based objective function depending on interactions among the neighboring agents. Then, each agent can minimize the localized objective function under the same constraints in the global model (17) through processing a gradient-projection procedure. Finally, an adaptive fusion strategy also based on Jensen-Shannon divergence is proposed to combine the local intermediate estimations of each agent and its neighbors.

Let Vi denote the immediate neighborhood of an agent i (including the agent i itself). By introducing some spatial coefficients  { x j,i  } ( j, i ? V), we present an objective function  for the agent i via a weighted sum strategy as:  gi( p(t)) = n?  j=1 x j,i f j ( p(t)) (18)  where the coefficients { x j,i  } are nonnegative and satisfy?n  i=1 x j,i = 1 and x j,i = 0 if and only if j /? Vi . Considering that each agent can only exchange its information with its immediate neighbors Vi , we define gi ( p(t)) as an optimization objective for the individual i . Thus, the local optimization model is defined as:  min gi( p(t)) = n?  j=1 x j,i f j ( p(t))  s.t .

{ E p(t) ? 0q C p(t) = 1m (19)  where the local objective function gi ( p(t)) combines the neighbors? individual cost functions. This enables the    TIAN et al.: ADAPTIVE FUSION STRATEGY FOR DISTRIBUTED INFORMATION ESTIMATION 3081  intermediate estimation information to be diffused among neighboring agents in the network. Iteratively solving the localized model (19) at every i can lead to a continuous local information diffusion over the multi-agent network, which turns out to improve the individual information esti- mation. Additionally, we remark that the nonnegative coeffi- cients  { x j,i  } can lead to  n?  i=1 gi ( p(t)) =  n?  i=1  n?  j=1 x j,i f j ( p(t))  = n?  j=1  n?  i=1 x j,i f j ( p(t))  = n?  j=1 f j ( p(t)) (20)  which shows that the sum of all individual objective func- tions {gi ( p(t))} is identical to the global objective function f ( p(t)). Since local observed information is diffused over the network and fused at individual nodes to enhance their p(i, t), these individuals can gradually arrive at a consensus on the probability distributions of information states of the whole surveillance region. That is, their p(i, t) is expected to approach the common distribution under t ? +?. Note the local objective function gi( p(t)) is constructed with local neighbors? p(i, t) and the global objective f ( p(t)) in (17) indeed collects every individual objectives {gi ( p(t))} (shown in (20)). Along with t ? +?, minimizing the global objective function in (17) is approximately equivalent to minimizing the local objective (18) at every node of the network G(V, E) in a decentralized manner.

C. Gradient Projection Solution  Each agent can solve the local optimization model (19) by the gradient projection algorithm. Let pi,t (t) be the agent?s individual estimation of the global p?(t) at the iteration t , i.e., a feasible iterator for the local model (19). Without loss of generality, at any pi,t (t), the inequality constraints Epi,t (t) ? 0q can be decomposed into two parts, one of which is called the active constraints, represented by Ei,1 pi,t (t) = 0qi,1 , another is called the nonactive constraints, represented by Ei,2 pi,t (t) > 0qi,2 . The coefficient matrices Ei,1 and Ei,2 are sub-blocks of the matrix E, i.e., E = col {Ei,1, Ei,2  } .

The full-zero vectors 0qi,1 and 0qi,2 are also sub-blocks of 0q , i.e., 0q = col  { 0qi,1 , 0qi,2  } , where the dimensions satisfy  qi,1 + qi,2 = q . We further point out that since all of the probabilities in a discrete distribution cannot be zero simultaneously (the sum of their values constantly equals to 1), all the inequality constraints can not be active simultaneously.

That is, the dimension of Ei,1 can not be q , i.e., qi,1 < q being always held. In fact, because there totally exist m discrete information state distributions, we can see qi,2 ? m, which equivalently implies qi,1 ? (q?m). With the active constraint coefficient matrix Ei,1 and the equality constraint coefficient matrix C , we can construct a new constraint matrix associated  with the iterator pi,t (t):  Mi,t (t) = [  Ei,1 C  ]  (21)  where the dimension of Mi,t (t) is ( qi,1 + m  )? q .

The basic idea behind the gradient-projection iterative  scheme is that a new iterator is generated in a feasible direction starting from the current feasible iterator. When the current feasible iterator is within the feasible region, the negative gradient direction can be employed for searching a new point; otherwise, when the current iterator is on the boundary of the feasible region, a new feasible direction is generated by projecting the negative gradient direction at the current point to the null space constituted by the active constraints. Thus, considering Mi,t (t) with full row rank, we are allowed to establish another new matrix Pi,t (t), called projection matrix  Pi,t (t) = E ? ( Mi,t (t)  )T (  Mi,t (t) ( Mi,t (t)  )T )?1  Mi,t (t) (22)  It is worth pointing out that when the matrix Mi,t (t) is empty, we can simply set Pi,t (t) = E.

Once the projection matrix Pi,t (t) is achieved at the agent i , it can apply a steepest-descent iterative method to optimize their individual objective functions gi ( p(t)) in a negative direction of projected gradient di,t (t):  di,t (t) = ?Pi,t (t)? p(t)gi ( pi,t (t))) (23) where ? p(t)gi( pi,t (t)) denotes the gradient of the individual objective function gi( p(t)) evaluated at the point pi,t (t).

Based on the equation (23), we can get a new iterator at the current point pi,t (t) by  ui,t+1(t) = pi,t (t)+ ?i di,t (t) (24) where the parameter ?i > 0 is a nonnegative step size.

According to the equation (24), the iterative procedure can be proceeded only when di,t (t) ?= 0. To solve the problem when di,t (t) = 0, we define an auxiliary vector si,t (t) as  si,t (t) = (  Mi,t (t) ( Mi,t (t)  )T )?1  Mi,t (t)? p(t)gi( pi,t (t)) (25)  Then, we divide this vector si,t (t) into two sub-blocks as follows  si,t (t) = [  si,t ,1(t) si,t ,2(t)  ]  (26)  where the row indexes of si,t ,1(t) and si,t ,2(t) correspond to those of the blocks Ei,1 and C in Mi,t (t), respectively.

The Lemma 3 is presented for proceeding the iteration when di,t (t) = 0.

Lemma 3 (Conditions on Gradient Projection Matrix for Iteration): Given that the active-constraint coefficient matrix Mi,t (t), the projection matrix Pi,t (t) and the iteration direction di,t (t) are derived from (21), (22) and (23), respectively, and di,t (t) = 0 is satisfied at the iterator pi,t (t), the following two conclusions are held:  1) If si,t ,1(t) is element-wisely nonnegative, i.e., Si,t ,1(t) ? 0, then the current point pi,t (t) is a minimizer for the optimization model (19);     2) If si,t ,1(t) has at least a negative element, denoted by sri,t ,1(t) < 0 where r is its row index, then one can remove the r -th row from Ei,1 to get a new active inequality constraint  coefficient matrix E?i,1 and to construct a new active constraint matrix M?i,t (t) by M?i,t (t) = col  { E?i,1, C  } . With the new  M?i,t (t), a new projection matrix P?i,t (t) can also be established by (22):  P?i,t (t) = E ? (  M?i,t (t) )T (  M?i,t (t) (  M?i,t (t) )T )?1 (  M?i,t (t) )  (27)  then di,t (t) can be re-calculated by:  di,t (t) = ? P?i,t (t)? p(t)gi( pi,t (t)) (28)  such that it can satisfy di,t (t) ?= 0 to proceed the iteration (24).

Proof: When si,t ,1(t) ? 0 and di,t (t) = 0, we can see  0 = (  E ? (Mi,t (t) )T (  Mi,t (t) ( Mi,t (t)  )T )?1  Mi,t (t) )  ? p(t)gi( pi,t (t))  = ? p(t)gi( pi,t (t))? ( Mi,t (t)  )T (  Mi,t (t) ( Mi,t (t)  )T )?1  Mi,t (t)? p(t)gi( pi,t (t)) = ? p(t)gi( pi,t (t))?  ( Ei,1  )T si,t ,1(t)? (C)T si,t ,2(t) (29)  The equation (29) is exactly the Kuhn-Tucker condition [28].

Thus, under the condition of 1) in Lemma 3, pi,t (t) is indeed a local minimizer of the optimization model (19). Recall that Lemma 2 states the convexity of the Jensen-Shannon based objective function. The model (19) has a convex objective function and linear constraints, which indicates that pi,t (t) is also a global optimum of this model.

Otherwise, there exists at least an element sri,t ,1(t) in si,t ,1(t) that is negative, i.e., sri,t ,1(t) < 0. Hence, we denote the corresponding r -th row of Ei,1 as Eri,1. To proceed this  proof, we turn to induce a contradiction given P?i,t (t)? p(t) gi ( pi,t (t)) = 0.

It can be found that (noting M?i,t (t) = col {  E?i,1, C }  )  ( Ei,1  )T si,t (t)+ CTsi,t (t) = (  E?i,1 )T  s?i,t ,1(t)+ sri,t ,1(t) (  Eri,1 )T + CTsi,t ,2(t)  = (  M?i,t (t) )T  si,t (t)+ sri,t ,1(t) (  Eri,1 )T  (30)  where s?i,t ,1(t) is composed of all the entries in si,t ,1(t) except the r -th one, and si,t (t) is composed of s?i,t ,1(t) and si,t ,2(t).

Substituting (30) into (29) gets  0 = ? p(t)gi( pi,t (t))? (  M?i,t (t) )T  si,t (t)? sri,t ,1(t) ( Eri,1  )T  (31)  Furthermore, P?i,t (t)? p(t)gi ( pi,t (t)) = 0 can lead to 0 = P?i,t (t)? p(t)gi ( pi,t (t))  = (  E ? (  M?i,t (t) )T (  M?i,t (t) (  M?i,t (t) )T )?1  M?i,t (t)  )  ? p(t)gi( pi,t (t))  = ? p(t)gi ( pi,t (t))? (  M?i,t (t) )T  si,t (t) (32)  Thus, subtracting (32) from (31) yields  0 = (  M?i,t (t) )T (  si,t (t)? si,t (t) ) + sri,t ,1(t)  ( Eri,1  )T (33)  The right side of (33) illustrates a linear combination of the row vectors of Mi,t (t). Since s  r i,t ,1(t) ?= 0, the row  vectors of Mi,t (t) are linearly dependent. This is incompat- ible with the fact that Mi,t (t) has full row rank. Therefore, P?i,t (t)? p(t)gi ( pi,t (t)) ?= 0, and the conclusion 2) of Lemma 3 is proven.

Indeed, Lemma 3 gives a theoretical condition that indicates when to stop iterating at a given point. Specifically, according to the proof of Lemma 3, an iterator pi,t (t) satisfying both of di,t (t) = 0 and si,t ,1(t) ? 0 is a Kuhn-Tucker point, it is also an optimum since the objective function of the model (19) is strictly convex as presented in Lemma 2.

On the other hand, since the objective function of the model (19), gi ( p(t)), collects the agent i ?s neighboring com- ponents,  { f j ( p(t))| j ? Vi  } , its gradient with respect to p(t)  also combines the gradient information of the neighbors, i.e., ? p(t)gi ( p(t)) = ? j?Vi x j,i? p(t) f j ( p(t)). Recall that in the equation (14), the real-time observation information of any neighbor j ? Vi , p( j, t), has been introduced into its Jensen-Shannon based function f j ( p(t)). The agent i ?s gradient formula ? p(t)gi ( p(t)) not only incorporates its own but also the neighbors? real-time observation information.

In this way, the iterative procedure based on (24) can adapt the individual intermediate estimation, ui,t+1(t), to local real-time observation information. Furthermore, once the computation of (24) is accomplished at each individual agent, the agent i can enhance its estimation at iteration (t + 1) by combining the intermediate estimations of all its neighbors based on a linear-weighted fusion strategy. That is, we compute i ?s next iterator pi,t+1(t) by  pi,t+1(t) = n?  j=1 y j,i u j,t+1(t) (34)  where {  y j,i | j = 1, . . . , n }  are some non-negative weights that satisfy  ?n j=1 y j,i = 1 and y j,i = 0 if and only if j /? Vi .

Essentially, the equation (34) can lead to the fusion of agents? intermediate estimation information over the network, which can further benefit the estimation performance of each individual. Combining (24) and (34) induces a distributed cooperative estimation solution as: {  ui,t+1(t)= pi,t (t)??i ?n  j=1 x j,i Pi,t (t)? p(t) f j ( pi,t (t)) pi,t+1(t) =  ?n j=1 y j,i u j,t+1(t)  (35)  for ?i ? V .

TIAN et al.: ADAPTIVE FUSION STRATEGY FOR DISTRIBUTED INFORMATION ESTIMATION 3083  The mathematical structure of the iterative formulas in (35) coincides with the ATC-type (Adapt-Then-Combine) com- putation framework [10], [11]. The first iteration equation in (35) can be treated as the individual adaptation to real- time observation information, while the second is the combi- nation of multiple agents? intermediate information. In (35), considering the constraints in the optimization model (19), it is required to guarantee that the estimations  { pi,t (t)  } at  each iteration t is feasible. The following theorem is pre- sented to guarantee the feasibility of the proposed iterative algorithm based on (35). Let b? j,t (t) = 0q j,2 ? E j,2 p j,t (t) and d? j,t(t) = E j,2d j,t(t) for any j ? Vi , and denote the l-th (l = 1, . . . , q j,2) elements of the vectors b? j,t (t) and d? j,t(t) by b? j,t,l(t) and d? j,t,l(t), respectively. We establish the following  Theorem 1 (The Feasibility of the Iterative Algorithm): Suppose p j,t (t) is a feasible point of the optimization model (19) for ? j ? Vi and ? j ? [0, ?maxj ]. Then, pi,t+1(t) obtained by (35) is also a feasible point. ?maxj is defined by  ?maxj = ? ?  ?  min  { b? j,t,l (t)  d? j,t,l (t)  ? ? ? ? d? j,t,l(t) < 0  }  , d? j,t(t) ? 0q j,2  +?, d? j,t(t) ? 0q j,2 (36)  Proof: Substituting the first equation of (35) into the second one leads to  pi,t+1(t) = n?  j=1 y j,i p j,t (t)?  n?  j=1 y j,i? j Pj,t (t)? p(t)g j ( p j,t(t))  (37)  The results of Epi,t+1(t) and C pi,t+1(t) are:  Epi,t+1(t)  = n?  j=1 y j,i Ep j,t (t)  ? n?  j=1 y j,i? j E Pj,t (t)? p(t)g j ( p j,t (t))  = n?  j=1 y j,i Ep j,t (t)  ? n?  j=1 y j,i? j  [ E j,1 E j,2  ]  Pj,t (t)? p(t)g j ( p j,t (t))  = n?  j=1 y j,i Ep j,t (t)  ? n?  j=1 y j,i  [ ? j E j,1 Pj,t (t)? p(t)g j ( p j,t (t)) ? j E j,2 Pj,t (t)? p(t)g j ( p j,t (t))  ]  (38)  C pi,t+1(t)  = n?  j=1 y j,i C p j,t (t)?  n?  j=1 y j,i? j C Pj,t (t)? p(t)g j ( p j,t (t))  (39)  Firstly, according to the definition of the projection matrix Pj,t (t), we can get  ? j M j,t (t)d j,t(t)  = ?? j M j,t (t)Pj,t (t)? p(t)g j ( p j,t (t)) = ?? j M j,t (t)(  E ? (M j,t(t) )T (  M j,t(t) ( M j,t (t)  )T )?1  M j,t (t) )  ? p(t)g j ( p j,t (t)) = ?? j  ( M j,t (t)? M j,t (t)  ( M j,t (t)  )T  ( M j,t (t)  ( M j,t (t)  )T )?1  M j,t(t) )  ? p(t)g j ( p j,t (t)) = ?? j  ( M j,t (t)? M j,t (t)  ) ? p(t)g j ( p j,t (t))  = 0q j,1+m (40) Note M j,t (t) = col{E j,1, C}. (40) is equivalent to  { ? j E j,1 Pj,t (t)? p(t)g j ( p j,t (t)) = 0q j,1 ? j C Pj,t (t)? p(t)g j ( p j,t(t)) = 0m  (41)  Substituting the second equation of (41) into (39) gets  C pi,t+1(t) = n?  j=1 y j,i C p j,t(t) (42)  Since p j,t (t) is a feasible point satisfying C p j,t (t) = 1m and?n j=1 y j,i = 1, (42) is equivalent to C pi,t+1(t) = 1m , which  indicates that the new iterator pi,t+1(t) satisfies the equality constraints of the model (17).

To investigate the equation (38), we consider two cases: 1) If d? j,t(t) is element-wisely non-negative, i.e., d? j,t(t) ?  0q j,2 , then the upper bound of the step size, ? max j , is set to+? accordingly. In this case, we can see that  ? j d? j,t(t) = ? j E j,2d j,t(t) = ?? j E j,2 Pj,t (t)? p(t)g j ( p j,t(t)) ? 0q j,2 (43)  is held for ? j ? 0.

Thus, according to (41) and (43), we have [?? j E j,1 Pj,t (t)? p(t)g j ( p j,t (t)) ?? j E j,2 Pj,t (t)? p(t)g j ( p j,t (t))  ]  ? [  0q j,1 0q j,2  ]  = 0q (44)  Note that the feasible point p j,t (t) satisfies Ep j,t (t) ? 0q .

Substituting (44) into (38) derives  Epi,t+1(t) ? n?  j=1 y j,i Ep j,t (t) ? 0q (45)  At this point, the new iterator p j,t+1(t) also satisfies the inequality constraints of the optimization model (17).

2) If there exists at least one element in d? j,t(t), i.e., d? j,t(t) ?  0q j,2 , we can divide d? j,t(t) into two sub-blocks d? j,t,1(t) and d? j,t,2(t) where the first sub-block d? j,t,1(t) contains the negative elements of d? j,t(t) and the second d? j,t,2(t) contains the nonnegative elements, i.e., d? j,t,1(t) < 0r j,1 and d? j,t,2(t) ? 0r j,2 . r j,1 and r j,2 are the dimensions of these two sub-blocks, respectively, and satisfy r j,1+r j,2 = q j,2. Thus, d? j,t(t) can be     rearranged as d? j,t(t) = col{d? j,t,1(t), d? j,t,2(t)}. In addition, we also re-express the matrix E j,2 as E j,2 = col{E j,2,1, E j,2,2} where E j,2,1 ? Rr j,1?q and E j,2,2 ? Rr j,2?q .

In the second case where the upper bound of the step size is limited by (36), it can be found that for any negative element d? j,t,1,l(t) ? d? j,t,1(t), the following inequality is held  ? j ? ?maxj = min {  b? j,t,l(t)  d? j,t,1,l(t)  }  ? b? j,t,l(t) d? j,t,1,l(t)  (46)  Additionally, noting d? j,t,1,l(t) < 0, (46) is equivalent to  ? j d? j,t,1,l(t) ? b? j,t,l(t) (47) We can further induce  ? j d? j,t,1(t) ? b? j,t,1(t) = 0r j,1 ? E j,2,1 p j,t (t) (48) where b? j,t,1(t) is a sub-block of b? j,t (t) whose row indexes correspond to those of d? j,t,1(t). Since d? j,t,2(t) is element- wisely nonnegative, we can also get  ? j d? j,t,2(t) ? 0r j,2 (49) Combining (48) and (49), we can get  ? j d? j,t(t) = ? j [  d? j,t,1(t) d? j,t,2(t)  ]  ? [  0r j,1 ? E j,2,1 p j,t (t) 0r j,2  ]  (50)  According to the definition of d? j,t(t), i.e., d? j,t(t) = E j,2d j,t(t), (50) is equivalent to  ? j E j,2d j,t(t)  = ?? j E j,2 Pj,t (t)? p(t)g j ( p j,t (t)) ? [  0r j,1 ? E j,2,1 p j,t (t) 0r j,2  ]  (51)  According to (41) and (51), the following inequality is held in this case:  [?? j E j,1 Pj,t (t)? p(t)g j ( p j,t (t)) ?? j E j,2 Pj,t (t)? p(t)g j ( p j,t (t))  ]  ? ?  ? 0q j,1  0r j,1 ? E j,2,1 p j,t(t) 0r j,2  ?  ?  (52)  Then, substituting (52) into (38), we further derive  Epi,t+1(t)  ? n?  j=1 y j,i  ?  ? E j,1  E j,2,1 E j,2,2  ?  ? p j,t (t)  + ?  ?  ?n j=1 y j,i0q j,1?n  j=1 y j,i ( 0r j,1 ? E j,2,1 p j,t (t)  )  ?n j=1 y j,i 0r j,2  ?  ?  = ?  ?  ?n j=1 y j,i E j,1 p j,t (t)?n  j=1 y j,i ( E j,2,1 p j,t (t)+ 0r j,1 ? E j,2,1 p j,t (t)  )  ?n j=1 y j,i E j,2,2 p j,t(t)  ?  ?  ? 0q (53) Therefore, in the second case, pi,t+1(t) also satisfies the inequality constraints. To sum up, we have proven Theorem 1.

Based on Theorem 1, we can enhance the adaption step in (35) by designing an appropriate step size ? j . Specifically, we consider to optimize ? j at each iteration t so as to minimize the objective function g j ( p). Once the iteration direction d j,t(t) is obtained by j ? Vi at any iteration t , we can treat the step size as a decision variable, denoted by ?, and formulate an objective function with respect to ?:  ? j (?) = g j (  p j,t (t)+ ?d j,t(t) )  (54)  Thus, an optimal searching step at t , denoted by ? j,t , can be derived as  ? j,t = argmin ?? [ 0,?maxj  ]  { ? j (?)  } (55)  where the upper bound ?maxj is given according to Theorem 1.

D. Optimization of Fusion Weights  We represent ui,t+1(t) = col {  ui,k,t+1(t)|k = 1, . . . , m }  where ui,k,t+1(t) is the intermediate estimation on the prob- ability distribution of information states of the subregion k.

From (35), it can be found that not only the step size ? j but also those fusion weights  { y j,i | j ? Vi  } have significant  influence on the performance of (35). For simplicity, let yi denote the set of fusion weights of the agent i , i.e., yi ={  y j,i | j ? Vi } , and Ui,k,t+1(t) =  { u j,k,t+1(t)| j ? Vi  } . The  Jensen-Shannon based cost function with respect to yi can be defined by  J SD (  Ui,k,t+1(t) ? ? yi )  = H ?  ? ?  j?Vi y j,i u j,k,t+1(t)  ?  ?  ? ?  ? ?  j?Vi y j,i H  ( u j,k,t+1(t)  ) ?  ? (56)  Subsequently, we can obtain optimal fusion weights by solving the following model  min Wi (yi ) = ?  k?? J SD  ( Ui,k,t+1(t)  ? ? yi )  s.t .

{? j?Vi y j,i = 1  y j,i ? 0 for ? j ? Vi (57)  In (57), yi represents the decision variable. This model takes Ui,k,t+1(t) as input parameters, which implies that once{ u j,t+1(t)| j ? Vi  } are aggregated at the agent i , the minimizer  of this model can be solely solved by the individual agent.

Thus, this model can be well integrated with the previous model (19). Besides, according to Lemma 2, the weight optimization model (57) also has a convex objective function.

It is a typical convex optimization problem which can be solved by many existing efficient numerical algorithms such as augmented Lagrangian methods and the sequential quadratic programming techniques [29]?[34].

A schematic diagram of the deployment of a multi-agent network is given in Fig.1. The implementation of the algorithm presented by (35) mainly involves four steps: in the first step,    TIAN et al.: ADAPTIVE FUSION STRATEGY FOR DISTRIBUTED INFORMATION ESTIMATION 3085  Fig. 1. The deployment of a multi-agent network.

Fig. 2. The framework of the adaptive fusion method.

any agent i ? V collects the gradient information from its immediate neighbors,  { ? p(t) f j ( pi,t (t))| j ? Vi  } , so that it can  construct a projection matrix Pi,t (t) as well as the iteration direction di,t (t); Then, the agent i can calculate an optimal searching step ?i,t according to (55), and obtains its own inter- mediate estimation ui,t+1(t) based on the adaptation equation in (35); Thirdly, i obtains an optimal fusion weights yi by solving the model (57) constructed based on its own and its neighbors? intermediate estimations; Finally, a next iterator pi,t+1(t) can be calculated based on the optimal fusion weights according to the combination equation in (35). These four steps are also performed at other agents, such that the overall network can achieve a cooperative information estimation.

To be specific, Fig.2 details the process of the overall proposed solution for cooperative information estimation over a multi-agent network, which combines the Bayesian updating formula (5) with (35), (55) and (57). Fig.2 shows that each agent i needs to broadcast its own intermediate estimation, ui,t+1(t), as well as the individual estimation, pi,t (t), to  its immediate neighbors at each iteration t . Especially, any agent?s individual estimation pi,t (t) is used by each of its local neighbors to evaluate the gradient of the Jensen-Shannon based function (14), and once the gradient ? p(t) f j ( pi,t (t)) is obtained by a neighbor j , it should be fed back from the neighbor j to the agent i .

There are two time indexes, i.e., t and t . The former denotes each discrete time interval, during which any agent i carries out multiple measurements (i.e., collecting Vi samples on the detection signal) to obtain the real-time observation informa- tion on the local subregions, and then performs the iterative algorithm presented by (35), (55) and (57). At the end of each time interval t , the agent calculates the Bayesian updating formula (5) with the individual estimation on the global region obtained from the iterative algorithm. As shown in Fig.1, t is used to index the iteration of the proposed algorithm. At the beginning of the iterative procedure of any agent i at t = 0, i.e., at t = 0 and t = 0, we initialize the individual estimation pi,t (t) by following the uniform distribution. That is, the adaptive fusion algorithm for information estimation is ini- tialized by setting  { pi,k,l,0(0) = 1/Lk|l = 1, . . . , Lk ; ?k ? ?  }  for all i , where pi,k,l,0(0) denotes the probability of the l- th information state of the subregion k estimated by the agent i at t = 0. The individual prior distribution of information states of any subregion out of the detection range is also initialized by following the uniform distribution, i.e.,  { pk?,l(i, 0) = 1/Lk? |i = 1, . . . , Lk? ; ?k ? ? ? i  } for all i .

In addition, for t ? 1, at the beginning of the algorithm cycle, t = 0, the individual estimation pi,0(t) is initialized as p(i, t), i.e., pi,0(t) ? p(i, t). For the sake of practical computation, we pre-specify the total number of the algorithm iterations, denoted by T , sufficiently large to guarantee the convergence of the algorithm. The detailed algorithm is shown in Algorithm 1.

The main steps of the adaptive fusion strategy are: Step 1: Observation: During time interval t , each agent  i ? V performs multiple measurements to collect Vi samples, {vk(i, t)|k ? ?i }, from the detection sig- nals corresponding to the subregions in its detection range. Then, the agent can calculate the observation information distribution pk(i, t) based on the cumu- lative observations {vk(i, ? )|? = 0, . . . , t; k ? ?i }.

For the subregions out of the detection range, i.e., ?k ? ? ? i , their prior distributions of information states are set to a zero vector. That is, let pk? (i, t)? 0 for all k ? ? ? i .

Step 2: Information estimation with adaptive fusion: Ini- tialize pi,t (t) for any i by setting pi,k,0(t) = pk(i, t) for the subregions in i ?s detection range, i.e., for all k ? ?i , and setting pi,k? ,0(t) = pk? (i, t?1) for those out of i ?s detection range, i.e., for all k ? ? ? i . Then, run the Algorithm 1 until t = T .

Step 3: Bayesian Updating: Let p(i, t) ? pi,T (t), do Bayesian updating based on (5) where p(i, t) is input parameter.

According to the iterative algorithm given above, we remark that since there may be some subregions that are located out of an agent?s detection range, this agent?s real-time observation     Algorithm 1 DCIE-AF (Distributed Cooperative Information Estimation with Adaptive Fusion)  Input: p(i, t), pi,0(t) and T Output: pi,T (t)  repeat Broadcast pi,t (t) to j ? Vi as well as collect{  p j,t (t)| j ? Vi } .

Evaluate fi ( p(t)) at each p j,t (t) and broadcast{ ? p(t) fi ( p j,t (t))| j ? Vi  } .

Collect { ? p(t) f j ( pi,t (t))| j ? Vi  } .

Construct Pi,t (t) and di,t (t) based on its own ? p(t) fi ( pi,t (t)) and  { ? p(t) f j ( pi,t (t))| j ? Vi  } .

Compute ?maxi according to (36) and obtain an optimal step ?i,t by solving (55).

Obtain ui,t+1(t) based on pi,t (t), di,t (t) and ?i,t according to (35).

Broadcast ui,t+1(t) to j ? Vi as well as collect{ u j,t+1(t)| j ? Vi  } .

Optimize the fusion weights yi with { u j,t+1(t)| j ? Vi  }  by solving (57).

Obtain new estimation pi,t+1(t) according to (35).

t ? t + 1.

until t = T  distributions over these subregions can be simply set to zero, implying that this agent does not contribute to the information gain in estimation of these subregions. In fact, following the underlying idea of the model (35), the agents can achieve their individual estimations over the subregions out of their detection range by iteratively collecting and fusing the diffused estimations of others that can directly detect these subregions.

E. Analysis of Convergence  In a distributed sensor environment, each agent?s compu- tation is not only influenced by its own local observation data but also by the shared information of its local neighbors.

To analyze the performance of the adaptive fusion strategy, we introduce an error vector:  ei,t (t) ? p?(t)? pi,t (t) (58) Combining (24) and (34), we derive  pi,t+1(t) = n?  j=1 y j,i p j,t (t)?  n?  j=1 y j,i? j,t Pj,t (t)? p(t)g j ( p j,t(t))  (59)  which can lead to (recalling ?n  j=1 y j,i = 1)  ei,t+1(t) = n?  j=1 y j,i e j,t(t)+  n?  j=1 y j,i? j,t Pj,t (t)? p(t)g j ( p j,t (t))  (60)  Besides, the mean value theorem [35] shows that for any con- tinuously differentiable function R(x), the following integral equation is held:  R(x + h)? R(x) = (? 1  R? (x + uh) du  )  h (61)  where x denotes a certain variable, h is a real parameter, and u is an integral variable. In order to relate the gradient term ? p(t)g j ( p j,t (t)) with the error quantity e j,t(t), we substitute h = ?e j,t(t), x = p?(t) and R ? ? p(t)g j (?) into (61) and rewrite ? p(t)g j ( p j,t (t)) as  ? p(t)g j ( p j,t (t))? ? p(t)g j ( p?(t)) = ?  (? 1  ?2p(t)g j  ( p?(t)? ue j,t(t)  ) du  )  e j,t(t) (62)  Since p?(t) represents an optimum of the objective function g j ( p(t)), we can have ? p(t)g j ( p?(t)) = 0. Thus, (62) is reduced to  ? p(t)g j ( p j,t(t))=? (? 1  ?2p(t)g j  ( p?(t)?ue j,t(t)  ) du  )  e j,t(t)  (63)  Substituting (63) into (60) yields  e j,t+1(t) = n?  j=1 y j,i  ( E ? ? j,t Pj,t (t)H j,t (t)  ) e j,t(t) (64)  where H j,t (t) is defined by  H j,t (t) ? ? 1  ?2p(t)g j  ( p?(t)? ue j,t(t)  ) du (65)  Lemma 4 (Positive Semi-Definiteness of Gradient Projec- tion Matrix): Given that Pj,t (t) obtained from (22) satisfies Pj,t (t)?2p(t)g j  ( p j,t(t)  ) ?= 0, Pj,t (t) is a non-zero symmetrical positive-semidefinite matrix and its eigenvalues only consists of 1 and 0.

Proof: According to (22), it is easy to validate( Pj,t (t)  )T = Pj,t (t) and (  Pj,t (t) )2 = Pj,t (t). Thus, for any  non-zero column vector x ?= 0, we can get  xT Pj,t (t)x= xT Pj,t (t)T Pj,t (t)x= (  Pj,t (t)x )T (Pj,t (t)x  )?0 (66)  Thus, Pj,t (t) is a positive semidefinite matrix.

Furthermore, since Pj,t (t)? Pj,t (t)2 = 0, the characteristic  polynomial equation of Pj,t (t) can be expressed as x ? x2 = (0 ? x)(1 ? x) = 0. Therefore, the eigenvalue set of Pj,t (t) is composed of 0 and 1. Additionally, it should be noted that since Pj,t (t) ?= 0 (otherwise, a contradiction would arise from Pj,t (t) = 0 when considering Pj,t (t)? p(t)g j ( p j,t (t)) ?= 0), all of the eigenvalues of Pj,t (t) cannot be equal to 0 simultaneously. Thus, Lemma 4 is proved.

On the other hand, according to Lemma 2, the Jensen- Shannon based function fi ( p(t)) is strongly convex. This implies that its Hessian matrix ?2p(t) fi ( p(t)) can be suffi- ciently bounded away from 0. To show this, we first derive the expression of ?2p(t) fi ( p(t)) as  ?2p(t) fi ( p(t)) = diag { ?2pk (t) J SDk( pk(t), pk(i, t))|k ? ?  }  (67)    TIAN et al.: ADAPTIVE FUSION STRATEGY FOR DISTRIBUTED INFORMATION ESTIMATION 3087  Recalling the definition of J SDk( pk(t), pk(i, t)), we further derive  ?2pk (t) J SDk( pk(t), pk(i, t)) = ?2pk(t)H (?k,1 pk(t)+?k,2 pk(i, t))??k,1?2pk (t)H ( pk(t))  (68)  Accordingly, ?2pk (t) J SDk( pk(t), pk(n, t)) can be expressed as  ?2pk(t) J SDk( pk(t), pk(i, t))  = diag {  ?k,1?k,2 ( pk,l(i, t)+?  )  (ln 2) ( pk,l(t)+ ?  )( ?k,1 pk,l(t)+?k,2 pk,l(i, t) + ?  )  ? ? ? ? ?  }  (69)  where l = 1, . . . , Lk . From (67) and (69), it can be found that ?2p(t) fi ( p(t)) is indeed a diagonal matrix. Thus, we can easily get the spectral radius of ?2p(t) fi ( p(t))  ? ( ?2p(t) fi ( p(t))  )  = max?k,l  { ?k,1?k,2  ( pk,l(i, t)+?  )  (ln 2) ( pk,l(t)+?  )( ?k,1 pk,l(t)+ ?k,2 pk,l(i, t)+ ?  )  }  (70)  Since the agent j ??s Hessian matrix evaluated at the agent j ?s individual estimation p j,t (t) can be represented by ?2p(t) f j ?( p j,t (t)), we can calculate the upper bound of the spectral radius of ?2p(t) f j ?( p j,t (t))  ? ( ?2p(t) f j ?( p j,t (t))  )  =max?k,l  { ?k,1?k,2  ( pk,l( j ?, t)+?  )  (ln 2) ( p j,k,l,t (t)+?  )( ?k,1 p j,k,l,t (t)+?k,2 pk,l( j ?, t)+?  )  }  ? max?k,l  { ?k,1?k,2  ( pk,l( j ?, t)+ ?  )  ?(ln 2) ( ?k,2 pk,l( j ?, t) + ?  )  }  = ? j ?, j (71)  where p j,k,l,t (t) corresponds to the probability of the l-th information state of the subregion k estimated by the agent j , and ? j ?, j represents the upper bound. The Theorem 2 is presented for stable-state performance.

Theorem 2 (The Stable-State Convergence): Given 0 <  ? j < min  {  ?maxj , 2?n  j ?=1 x j ?, j ? j ?, j  }  for any j ? V , the iterative procedure given in (35) asymptotically converges to a stable state for any initial feasible solutions  { p j,0(t)| j ? V  } , and as  t ?+?,  lim t?+?  ?e j,t(t)?2 = 0 (72)  is held for all j ? V .

Proof: According to Lemma 4, the spectral radius of  Pj,t (t) cannot be larger than 1, i.e., ?(Pj,t (t)) ? 1. This indi- cates that ?  ( ? j Pj,t (t)H j,t (t)  ) ? ? j ? (  Pj,t (t) ) ? (  H j,t (t) ) ?  ? j? (  H j,t (t) ) . From H j,t (t) in (65), we can further get  ? j? (  H j,t (t) ) = ? j ?  ?  ? n?  j ?=1 x j ?, j  ? 1  ?2p(t) f j ?  ( p?(t)  ?ue j,t(t) )  du  ?  ? (73)  Based on the mean value theorem and (71), there always exists a certain point ? j,t (t) ?  [ p?(t)? e j,t(t), p?(t)  ] that  ? 1  ?2p(t) f j ?  ( p?(t)? ue j,t(t)  ) du  = ?2p(t) f j ? ( ? j,t (t)  ) (? 1  du  )  ? ? ( ?2p(t) f j ?  ( ? j,t(t)  )) E  < ? j ?, j E (74)  Substituting (74) into (73), we can get  ? j ? ( H j,t (t)  ) < ? j  n?  j ?=1 x j ?, j? j ?, j (75)  Since ? j < 2?n j ?=1 x j ?, j ? j ?, j  , we can get ? ( ? j Pj,t (t)H j,t (t)  ) <  ? j ?n  j ?=1 x j ?, j? j ?, j ? 2. This result is equivalent to ? ?1? ? (? j Pj,t (t)H j,t (t)  )? ? < 1 (76)  which implies that the matrix (  E ? ? j Pj,t (t)H j,t (t) )  is stable, namely, its sub-multiplicative matrix norm ?E ? ? j Pj,t (t)H j,t (t)?2 satisfying  ?E ? ? j Pj,t (t)H j,t (t)?2 ? ?max + ? < 1 (77) where ?max = max j?V  { ? ( E ? ? j Pj,t (t)H j,t (t)  )} , and ? is a  positive number that is sufficiently small.

Now, we denote the global error vector by Qt (t) that  collects the error quantities from all the agents  Qt (t) ? col {?e j,t(t)?2  ? ? j ? V} (78)  Also, we denote a fusion weight matrix as Y ? ? [ y j,i  ] n?n  and lump all the step sizes into a diagonal matrix ?? ? diag  { ? j | j ? V  } . Subsequently, we derive two block matrices  by using the Kronecker product operator ?: Y = Y ? ? E (79) ? = ?? ? E (80)  where Y and ? are of nq columns and nq rows. Using these notations above, we can rewrite (64) as  Qt+1(t) = Y T ( Inq ??Zt(t)  ) Qt (t) (81)  where Zt(t) = diag {  Pj,t (t)H j,t (t)| j ? V } .

Since ?Y T ?2 = 1, we can calculate the 2-norm of both sides of (81) as  ?Qt+1(t)?2 = ?Y T (  Inq ??Zt(t) )  Qt (t)?2 ? ?Y T?2 ? ?Inq ??Zt(t)?2 ? ?Qt (t)?2 ? (?max + ?) ?Qt (t)?2 = (?max + ?)t+1 ?Q0(t)?2 (82)     Fig. 3. A region and a network?s communication topology with 6 agents.

(Red dots: agents. Red dotted lines: agents? detection ranges. Blue lines: the network?s communication topology).

Since (?max + ?) < 1, we can get limt?+? ?Qt+1(t)?2 = 0.

The result is equivalent to (72).



V. NUMERICAL EXAMPLES  To evaluate the performance of the distributed information estimation method, we set up a scenario where the entire surveillance region is divided into 16 = 4 ? 4 subregions and several agents are initially generated and distributed over this surveillance region. Each agent is considered to be able to detect partial subregions, and multiple agents are assumed to be locally connected when they are within each other?s com- munication range. An instance of a network?s communication topology with 6 agents is shown in Fig.3, where every grid denotes a subregion, and the whole grid plane of 2 dimensions represents the overall surveillance region.

We assume that each subregion is an information source which could generate a series of information of interest. And the information values follow some certain distributions. For example, the k-th subregion generates the information vk(t) at time t , and this subregion?s actual information is assumed to be a random variable following a finite discrete distribution that could be represented with different states  { Sk,l |l = 1, . . . , Lk  }  (Lk is the number of the whole states.). Each state denotes a certain range of the information variable vk(t) and is associated with a certain probability p  ( vk(t) ? Sk,l  ) . For  testing our model, we randomly generate the distribution of information states of each subregion, as shown by the dark histogram in Fig.4. These distributions are treated as the actual information distributions which each agent would like to estimate. In measurement interval, each agent could observe sampleNum samples from each subregion in its detection range and could not detect those subregions out of its range.

For instance, in Fig.3, the grids at the right side of the entire plane could not be detected by the agent located at the top left of the plane. We also assume that there are some certain white Gaussian noises existing in each agent?s observations.

The mean value of the noise corresponding to the agent i is set to 0, and the standard deviation ?i . For simulations, we randomly generate ?i for each agent by following a uniform distribution U [0, ?max], namely, ?i ? U [0, ?max].

Additionally, we denote the total number of time intervals as T , i.e., 0 ? t ? T , and the number of epochs for  performing the proposed iterative scheme (Algorithm 1) as T , i.e., 0 ? t ? T . We firstly fix ?max = 1.5, sampleNum = 100, T = 1000 and T = 50 respectively. In the Shannon entropy based function (12), the small parameter ? is set to 1? 10?6.

The probability distribution corresponding to the information state of every subregion maintained by each individual agent is initialized by following the uniform distribution. Then, we set the agent number n = 6 and have performed the simulations with 50 independent replications. Thus, we could average the results over those 50 simulations and then show the whole estimation results. we randomly select an agent (whose detection subregions are {2, 3, 4, 6, 7, 8}) and illustrate the convergence of its individual estimation corresponding to the whole subregions in Fig.4 (the gray histogram in Fig.4). It can be found that even though those subregions (the subregions {1, 5, 9, 10, 11, 12, 13, 14, 15, 16}) are out of the detection range of this selected agent, its actual information state distribution could also be well approached by this agent?s estimation based on our proposed method.

In order to quantify the performance, we define the evalu- ation metric, i.e., the averaged absolute error at t obtained by an agent i , which is denoted by AAE(i, t) and calculated as  AAE(i, t) = ?m  k=1 ?Lk  l=1 ? ? ?pk,l (i, t)? pk,l,T (i, t)  ? ? ?  q (83)  Similarly, to evaluate the whole agent network, we could also calculate the absolute error by averaging over all the agents? results. We define the averaged absolute error relevant to the agent network as AAE(t)  AAE(t) = ?n  i=1 AAE(i, t) n  (84)  Next, for evaluating the influence of different parameters on the proposed scheme, we set up several simulation situations.

In the first situation, we use ?max = 1.5, sampleNum = 100, T = 1000 and T = 50, and vary the number of agents from 6 to 14, namely, n ? {6, 8, 10, 12, 14}. In this situation, we also compare our method with the distributed information estima- tion strategy based on the traditional consensus approach [7], which adopts the same settings on ?max, sampleNum, T and T . The compared method is simulated at n = 6. The results at each point n are also averaged over 50 independent simulations. These results are shown in Fig.5. In the second situation, we fix the number of agents as n = 6 and then set different ?max to show the impact of the noise magnitude on the convergence. The consensus-based method uses the fixed setting ?max = 0.1 in this situation. The third situation simulates the proposed method at different settings on the sample number, sampleNum ? {10, 50, 200, 300}, while we fix sampleNum = 300 for the compared method. In this situation we also keep ?max = 1.5, T = 1000, T = 50 and n = 6.

From the results, it can be found that the agent number impacts the estimation performance of the overall network.

Particularly, with the same number of agents (e.g., n = 6), our proposed method is shown to outperform the compared one. Besides, from Fig.5a, we find that more agents does not always guarantee a better estimation. This figure shows that the    TIAN et al.: ADAPTIVE FUSION STRATEGY FOR DISTRIBUTED INFORMATION ESTIMATION 3089  Fig. 4. Simulated distributions of information states of 16 subregions. (a) Subregion 1. (b) Subregion 2. (c) Subregion 3. (d) Subregion 4. (e) Subregion 5.

(f) Subregion 6. (g) Subregion 7. (h) Subregion 8. (i) Subregion 9. (j) Subregion 10. (k) Subregion 11. (l) Subregion 12. (m) Subregion 13. (n) Subregion 14.

(o) Subregion 15. (p) Subregion 16.

Fig. 5. The results obtained from different situations. (a) The first situation. (b) The second situation. (c) The third situation.

averaged absolute error of the individual estimation obtained by a network consisting of 6 agents is slightly better than those obtained by the other networks which contain more than 6 agents. The main reason for a slight drop in the performance is that more agents, indicating a larger-scale network (involv- ing more decision variables in the optimization model), could slow down the convergence of the overall network. In order  to ensure that the distributed algorithm could approach closer to an optimal point of a larger-scale optimization problem, they need more interactions and more numbers of learning (iterations) to diffuse and fuse local information over the network. Recall that we have set the same total number of iterations between successive observations, i.e., T = 50, for all the networks. That is, the iterative algorithm dealing with a     larger-scale optimization problem in the case of a larger-scale network is terminated in the same total step size as in the case of a smaller-scale network, which could result in a relatively conservative estimate for the optimal point. By contrast, the optimization algorithm could approach much closer to the opti- mal point within a finite number of iterations when it handles a smaller-scale optimization problem. Actually, it is clear that there is a trade-off between the choice of the amount of the agents deployed (i.e., the scale of the multi-agent network) and the potential computing cost arising from complexity in the distributed optimization. On one hand, a minimum number of agents should be needed so as to ensure that their overall detection range could cover the whole surveillance area. More agents added could improve the reliability of the distributed network. On the other hand, the network size is also increased by more agents, leading to more complexity of the network topology, which would require much more effort to handle the corresponding distributed optimization problem.

Generally, a distributed algorithm needs more computing/ communication resources (e.g., data buffer, bandwidth) to address a larger-scale model and more iterations to guarantee its convergence. Thus, there should be an optimal number of agents for deployment, which could depend on not only the algorithm cost and performance in distributed computation, but also the amount and distribution of surveillance regions, noise level in observations and other comprehensive factors involved in a specified application context. The issue is related to the optimal coverage control, which aims at a solution that could attain a specified optimal objective, at the meanwhile determining a minimum number of agents whose coverage could cover the target area. This issue is out of scope of this paper, it can be referred to many existing works such as [36]?[38].

In the second situation, see Fig.5b, the increased noise level indicated by ?max could result in a larger averaged absolute error of the overall network. The noise in individual observa- tions could also lead to the randomness of the gradient process in the equation (62), so that the actual estimation error of the network could not fully converge to zeros. This fact could also be validated by the results in the first situation as shown in Fig.5a. That is, the larger amount of agents does not guarantee a lower gradient noise process of the overall network. Never- theless, from the results in Fig.5b, it can be found that once the noises of individual observations under a certain level, a good global information estimation of the overall network could be guaranteed regardless of the limitation of each individual agent?s detection and communication. The results from the third situation in Fig.5c shows that increasing the amount of measurements (denoted by the sample number sampleNum) at each individual observation could decrease the absolute error on average by the proposed method. It is because a larger number of samples observed by each individual agent could make the individual estimation more approximate to the actual distribution of a subregion. The result is in accordance with the law of large number [39]. Additionally, comparing the results obtained by our proposed method with those of the consensus- based method, the averaged absolute error converges to a lower level by our method. Specifically, even when our method  Fig. 6. Evaluation of the proposed method at different T and T .

is simulated at a higher noise level such as ?max = 0.5, 1 (see Fig.5b) or adopts a smaller sample number such as sampleNum = 50, 200 (see Fig.5c), it outperforms the consensus-based method as well.

Finally, to evaluate the influence of different observation amounts and iteration numbers on the performance of the multi-agent network, we simulate our method at different T ? {100, 400, 700} and T ? {50, 80, 100}. In addition, in this situation we fix sampleNum = 100, and the other parameters ?max and n are set to be ?max = 1.5 and n = 6. From the results shown in Fig.6, it can be found that more observations or more iterations tend to reduce the averaged absolute error of the whole network. The main reason is that more observa- tions, implying more measurements, could make each agent approach to the actual information state distributions of those subregions in its detection coverage, and increasing iteration number could make individual estimation much closer to the ideal solution of the optimization model (19). Appropriate settings on them could be selected or tuned based on a specified application scenario of interest.



VI. CONCLUSION  In this paper, we have proposed an adaptive distributed fusion strategy for estimating global information over a coop- erative multi-agent network. We have modeled the information of interest associated with a global region being detected by the network from a probabilistic perspective, in which probability is related to the accuracy of observed and estimated information. We apply an information-theoretic measure, i.e., the Jensen-Shannon divergence, to formulate two objective functions of any agent subject to linear equality and inequality constraints, one of which is used in a localized estimation optimization model and the other in a fusion weight optimiza- tion model. The adaptive fusion strategy allows each agent to achieve its own optimal individual estimation on the global information of the entire region through minimizing the local- ized objective functions in a distributed and online manner.

We have also analyzed the mean-square-error convergence behavior of the proposed algorithm. Finally, the experimental results have been provided to demonstrate the effectiveness of the proposed algorithm and its advantage over the conventional consensus protocol based algorithm. In our future research, we will extend the proposed method with consideration of    TIAN et al.: ADAPTIVE FUSION STRATEGY FOR DISTRIBUTED INFORMATION ESTIMATION 3091  optimal coverage control issue and potential failure of local communication links.

