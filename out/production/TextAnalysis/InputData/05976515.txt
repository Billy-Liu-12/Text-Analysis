2011 3rd Conference on Data Mining and Optimization (DMO)  28-29 June 2011, Selangor, Malaysia

Abstract?Increasing number of computer networks now a day has increased the effort of putting networks in secure with various attack risk. Intrusion Detection System (IDS) is a popular tool to secure network. Applying data mining has increased the quality of intrusion detection neither as anomaly detection or misused detection from large scale network traffic transaction. Association rules is a popular technique to produce a quality misused detection. However,  the weaknesses of association rules is the fact that it often produced with thousands rules which reduce the performance of IDS. This paper aims to show applying post-mining to reduce the number of rules and remaining the most quality rules to produce quality signature. The experiment conducted using two data set collected from KDD Cup 99.  Each data set is partitioned into 4 data sets based on type of attacks (PROB, UR2, R2L and DOS).

Each partition is mining using Apriori Algorithm, which later performing post-mining using Chi-Squared (?2) computation techniques. The quality of rules is measured based on Chi- Square value, which calculated  according  the support, confidence and lift of each association rule. The experiment results shows applying post-mining has reduced the rules up to 98% and remaining the quality rules.

Keywords- component; Association Rules; Intrusion DetectionSystem; Chi-Square; Apriori Algorithm

I.  INTRODUCTION  The used of various type of internet application has dramatically increased the network traffic, which causes increasing the number of attacks and types of attacks. Now a day, network system exposes with a high level risk of security.

The first threat for a computer network system was realized in 1988 when 23 year old Robert Morris launched the first worm, which override over 6000 PCs of the ARPANET network. On February 7th, 2000 the first DOS attacks of great volume where launched, targeting the computer systems of large companies like Yahoo!, eBay, Amazon, CNN, ZDnet and Dadet. More details on these attacks can be found at [1]. These threats or more others are likely to appear in the future. Therefore, various network protection systems have been developed to secure network such as like firewall, anti-virus, IDS, etc.

An intrusion detection system (IDS) inspects all inbound and outbound network activity and identifies suspicious patterns that may indicate a network or system attack from someone attempting to break into or compromise a system [2].

Now a day, IDS is becoming a critical component to secure the network. Due to large of security audit data as well as complex and dynamic properties of intrusion behaviors, optimizing performance of IDS becomes an important open problem that is receiving more and more attention from the research community [3].

In 1980 Anderson, introduced the concept of intrusion detection in [4], defined an intrusion attempt or a threat to be the potential possibility of a deliberate unauthorized attempt to access information, manipulate information, or render a system unreliable or unusable. Since then, several techniques for detecting intrusions have been studied.

Most of the techniques used in IDS exploit to learn rules from the behavior of intrusions and normal activities. Many machine-learning paradigms including neural networks, linear genetic programming, support vector machines, bayesian networks, multivariate adaptive regression spline, fuzzy inference systems, rough set theory, etc. have been applied to the design of IDS. Several data mining techniques, including discovering association rules, have been applied to intrusion detection (ID). These rules can then be used for misuse detection and anomaly detection [5].  Anomaly-based IDS detects activities that vary from established patterns for users and misuse-based IDS compares user's activities with the known behaviors of attackers. These approaches build detection models by applying data-mining algorithms to large data sets of audit data [30].

Association rule is a popular data mining techniques to produce quality misused detection. However, the due to the large number of network traffics and its attribute, it causes the large number rules to present the pattern. These large numbers of rules has reduced the performance of the IDS.

However, there are many un-useful rules are produced.

Therefore, this paper aims to apply post mining technique to reduce the association rules by remaining the quality pattern.

The paper is divided into three sections.  Second section discusses the related work on IDS, Apriori Algorithm, Post Mining and Chi-Squared techniques. The third presents how         this experiment is conducted and finally presents the experiment results and conclusion.



II. RELATED WORK  A. Intrusion Detection System The objective of intrusion detection software is to  determine intrusions into a computer or network via viewing various network actions or features. At this point intrusion refers to several set of actions that threatens the integrity availability or confidentiality of a network [6]. The main objective of IDS software is monitoring antagonistic operations of all kinds, whether individual (hackers and crackers) or programmatic (viruses, Trojan horses). IDS can function on a meticulous server or in a complete section of a network.

Fundamentally, there are couple approaches to intrusion detection model as described in [7]: Misuse detection model refers to detection of intrusions that follow well defined intrusion models. It is very practical in detecting known attack models. Anomaly detection model refers to detection achieved by detecting changes in the models of operation or performance of the system. It can be used to detect identified and unidentified attack [8]. In order to detect intrusion actions, a lot of machine learning (ML) algorithms have been widely used to the massive amount of multifaceted and dynamic dataset to detect identified and unidentified intrusions, such as neural network, support vector machine, genetic algorithm, fuzzy logic and data mining [9].

B. Misuse detection Misuse detection based on wide knowledge of models  associated with signature for known attacks supplied by individual authorities. This strategy is employed by the current generation of commercial intrusion detection systems. Misuse detection has low false positive rate, but cannot detect novel attacks. Some of the approaches for Misuse detection are pattern matching, data mining, and state transition analysis. Example misuse detection systems that use data mining include IDDM (Intrusion Detection using Data Mining Technique) by [10] are a real-time NIDS for misuse and anomaly detection. It applies association rules, Meta rules, and characteristic rules. It employs data mining to produce description of network data and uses this information for deviation analysis, MADAM ID (Mining Audit Data for Automated Models for Intrusion Detection), is one of the best known data mining projects in intrusion detection [11]. It is an off-line IDS to produce anomaly and misuse intrusion detection models. Association rules and frequent episodes are applied in MADAM ID to replace hand-coded intrusion patterns and profiles with the learned rules and Automated Discovery of Concise Predictive Rules for Intrusion Detection.

C. Intrusion detection with Association rules Given the intrusion detection import problem, the  international knowledge discovery and data mining group (KDD) is provided a benchmark (1999 KDD Cup Competition). There are four main categories of attacks were  classified according to the actions and goals of the attacker given in the KDD [12]. They are denial-of-service (DOS), probe, remote-to-local (R2L) and user-to-root (U2R).

Additionally, association rules mining, is measured as one of the most important tasks in knowledge discovery in databases [13] Among sets of items in transaction databases, it aims at discovering implicative propensities that can be precious information for the decision maker. The Association Rule technique proposed by Agrawal is a most popular tool.

In [28], they evolved the Apriori Algorithm for discovering association rules. The Apriori Algorithm requirements are Confidence and Support; these two values determine the degree of association that must hold. The Support shows how many times the items in the rule crop up together and it is the relation of transactions that include all the items in the antecedent and consequent to the number of whole transactions and the confidence shows the probability of both the antecedent and the consequent coming into view in the same transaction. Confidence is the relation of the rule support to the number of transactions that include the antecedent and it is the conditional probability of the consequent given the antecedent. Usually when both of the measures are high we need a third determent, in this case it is Lift to evaluate the quality of rules. Lift shows the power of a rule over the random co-occurrence of the antecedent and the consequent, given their individual support. It offers information about the development, the increase in probability of the consequent given the antecedent. Lift is defined as follows:  (Rule Support) Lift =                                                                                           (1) (Support (Antecedent) * Support (Consequent))   In recent times, association rules have been used in pattern recognition problems like classification in [14] and [15]. In [15] proposed a novel model namely class association rule (CAR) has been used to solve the classification problems. A CAR set is a subset of association rules with the specified classes as their consequences. Boolean and fuzzy CARs are used for building classifiers, correspondingly. One of the Association Rules problem, that a huge amount of rules have been discovered and a big part is useless rules set.

D. The Interestingness of Association Rules Within the last decade the KDD society has recognized  this face often referred to as interestingness as a significant and hard component of the process of KDD. For dealing with this problem, the most frequently approach is used based on the structure of interestingness measures (called measure for short). In association rules definition, [16] introduces two measures: support and confidence as we mentioned in the previous section. These are well modified to Apriori algorithm constraints, but are nonsufficient to capture the full features of the rule interestingness. The interestingness of the rules can be classified into two types: Subjective and Objective. Subjective measures and objective measures. The subjective measures rely on the goals, the knowledge, and the idea of the user. The objective measures are statistical indexes in reference Subjective measures explicitly depend         on the user's goals and his/her knowledge or attitudes. They are shared with specific supervised algorithms in order to evaluate the extracted rules with the user's expectations [16].

Objective measures are numerical indexes that only rely on the data distribution. Interestingness refers to the quantity to which a discovered pattern is of interest to the user and is driven by issues such as innovation, utility, relevance and statistical significance [17].

E. post-mining Post mining is a significant step in the process of  knowledge discovery to refine discovered patterns and learned models and present functional and relevant knowledge to end users. In the last couple of years; a surge in research on improving the algorithmic performance of the originally proposed algorithms, among which the Apriori- algorithm is mostly used have seen. Till today, several successful applications illustrating their usefulness have been reported in the literature. Some methods were presented and different algorithms were established to reduce the number of item sets by generating closed, maximal or optimal item sets, and several algorithms to reduce the number of rules, using pruning techniques or non redundant rules. However, post- processing methods can develop the discovered rules selection. Different complementary post-processing methods may be used, like pruning, summarizing, grouping, or visualization [18]. Pruning consists of removing uninteresting or unnecessary rules before a user is able to learn the rules and categorize interesting ones from them. Some pruning techniques has been proposed to reduce the huge amount of association rules such as Redundancy Removal, Pessimistic Error Rate, Closed Set, Chi-Square, subsumed rules.

Pruning can significantly decrease the discovered association?s number; this number might still be very huge to be tractable. In summarizing, this technique, rules can be clustering into some abstraction stages, every one based on the users preference. Advanced level rules give an extra general summary of the discovered knowledge even as the lesser level rules can be browsed for additional facts. There are a lot of methods have been applied in summarizing such as taxonomies and direction setting rules. The grouping phase point is to look at some other characteristics of the rules and cluster them based on different dimensions. In this phase, the researchers are highlighted the grouping semantics via means of the descriptive scope of time and financial significance, A lot of techniques have been applied in this phase such as economic assessment and time based patterns. Visual investigation of the potentially attractive association rules is to be visualized as an integrated and main part of any exploratory data mining surroundings, each of the above alterations can be supported via means of visually improved mining tools. The visualization improves the readability of a large number of rules by using modified graphical representations. Though, most of the offered post-processing methods are commonly based on statistical information in the database.

While rule significance powerfully depends on user knowledge and objectives, these methods do not promise that the significant rules will be extracted. For instance, if the user  looks for unpredicted rules, all the previously known rules should be pruned. Or, if the user needs to focus on exact schemas of rules, only this subset of rules should be selected.

Furthermore, as recommended in [19], the rule post- processing methods are supposed to be imperatively based on a strong interactivity with the user.

Pruning Technique The basic concept of pruning technique is reducing the  huge quantity of rules is the exclusions of generated rules number because they are manifestations of several type of useless phenomenon, there is frequently a lot of association rules discovered from a dataset and it is needed to remove useless rules before a user is able to learn the rules and categorize interesting ones from them. There are several researches have been done in this area such as [20], presented a computation namely the minimum amount of improvement defined as: confRl - confR2. In [21], has proposed essential rule set to give up a minimal rule subset, which compactly encodes without information loss the classification knowledge available in a classification rule set.

Chi-Square Pruning technique Chi-Squared (?2) is an analysis technique that helpful in  determining the association rules statistical significance level, [22]. Computing the chi-square statistic for the couple of variables (A, B) involves constructing two contingency tables. The experimental contingency table for (A, B) has four cells, parallel to the four possible Boolean combinations of A, B. The value in every cell is the number of explanation (samples) that competition the Boolean combination for that cell.

?2 = ? ??  ?  1, , expected  2), expected, observed(  jio ji jiji            (2)  Consider two binary valued random variables A and B.

where i-th row and the j-th column is called the i,j and  Oi,j = an observed frequency.

Ei,j = an expected (theoretical) frequency, asserted by the null hypothesis.

Chi-square analysis is a standard statistical technique that lets one to measure the degree of dependence between the variables A and B. The Chi-Squared statistic of equation 2, Satisfies the following Equation equality (at any time the expression on the right hand side RHS is well defined (Consequent)), in [22].

?2 = n (Lift-1)2 ))(( SupportConfidenceConfidenceLift  ConfidenceSupport  ??  ? (3)   The performance of Chi-Square has been used in many  domains to prune the discovered association rules. In Biomedical Knowledge, in [23] has proposed a method, Chi- Square association rule on Association Based on Classification ABC. The results show it significantly reduces irrelevant connection and saves a lot of computation time generates much fewer rules. In [24] has used Chi-Square significance level to remove the insignificant rules for a huge           amount of association rules are discovered for mixtures of transcription factor binding sites in the repeat sequences of the repeat database. The Apriori and AprioriTid have been applied to execute the associations from the mixtures of transcription factor binding sites in repeat sequences. The huge number of association rules makes it really hard for the user to recognize those interesting and functional ones. At last, the unnecessary rules are pruned and then the remaining rules are partitioned into cover and non-cover sets. For complex datasets described, [25] has introduced an association rule mining technique by both static and time- dependent attributes, and apply this technique to find associations among sleep questionnaire responses, clinical summary information, and all-night polysomnographic recordings of sleeping human subjects. They have designed an extension of the Apriori association rule mining algorithm to deal with time-varying sequences and they used Chi- Squared and P-value to get the meaningful associations among summary and polysomnographic time series variables.



III. EXPERIMENTED SETUPS  Figure 1, shows the steps conducted in this research. It consists of data collection, cleaning and pre-processing, partitioning, mining and post- mining.

Figure 1.  Steps Conducted  A. Data Collection The Experiment used KDD CUP 99 training dataset  which contained around 5000000 connection records, the 10% training dataset contained 494,021 records, which there were 97,278 normal connection records (i.e. 19.69%),  sample of the dataset in appendix A. Every record contained value of 41 different attributes that describe the different feature of the corresponding connection and the value of the connection is labeled either as an attack with one specific attack type or as a normal. In 10% dataset there are 24 different attack types present, each one falls exactly into one of the following categories: normal,  probing, dos, u2r and r2l. In this research two datasets have been collected randomly (which are 4000 and 2000) instances from the 10% training dataset. The two data sets have partitioned into four parts based on attack types (PROBE, DOS, R2L and U2R).

Excel sheet has been used to filter the datasets and partitioned.

B. Data Cleaning and Preprocessing The data cleaning and pre-processing follow the step  suggested by [29], the biased features which have abnormal values of distribution were removed more than 70% data distribution is same value. While in pre-processing performs three steps:  The first step converting the string and continues values to nominal numerical continues values. The second step is applying discretization using Disc-Chimerge algorithm. The KEEL 1.0 data mining tool is used for this purposed. The third step is applying feature selection technique based on P- value using a commercial tool known as PASW modeler 13 to select the most significant attribute.

C. Mining Process In the mining process Apriori Algorithm has been used to  discover the association rules for intrusion detection system.

The Apriori Algorithm has been applied for the all random datasets, first group was the 4000 records (PROBE1, DOS1 R2L1 and U2R1 datasets) and the second group was 2000 records (PROBE2, DOS2, R2L2 U2R2 datasets).

TANAGRA tool was used Apriori Algorithm and the parameters were; the minimum Support was between 0.25 ? 0.4 and the minimum Confidence was between 0.7 ? 0.9 as the literature show the rules with high confidence and support values are significant [26]. As well as the maximum length of rules was 4 and Lift filtering was from 1.0 and above [27].

Finally the association rules for each dataset which is discovered and contained from Antecedent, Consequent, Lift, Support and Confidence.

D. Pruning the Association Rules The Pruning technique aims to eliminate remain the  unnecessary association rules in each association rules datasets.

The pruning process has two steps to filter and prune the rules; the first step is removing the useless rules which did   TABLE I.  CONFIDENCE INTERVAL FOR EACH RULES SET OF DATASET 1  Dataset 1. Confidence Intervals  Class Before  Pruning  After  Pruning  90.0  %  95.0  %  97.5  %  99.0  %  99.

5%  99.

9%         Probe 374 18 - - - 4 7 7  Dos 1923 83 - - 30 9 18 26  R2L 257 26 12 - 2 10 1 1  U2R 757 34 5 - - 28 1 -   not contain any attack type (probe, dos, r2l and u2r) in the  discovered association rules.

The second step is using one of the functional in determining the statistical significant level of association rules; it is Chi-Squared analysis (?2). It is defined in terms of the entries of the observed and expected contingency tables as it seen in equation 2. Therefore, ?2 stand for a summed normalized square deviation of the observed values from the matching expected values. In the tables of the ?2 distribution, Statistical significance levels matching to specific ?2 values may be found. The degrees? number of freedom is 1 because we are dealing with binary valued attributes. The ?2 distributions with 1 degree of freedom have minimum ?2 values for selected stages.

After mining, this makes easy computation of Chi- Squared values in equation 3, smooth the pruning progress of the mined rule set based on statistical significance. It is computed directly from the values of Support, Confidence and Lift interest of the rules in question.

Each of rule set row is evaluated, either it is representing high significant rules.  The ?2 value of each rule has compared with standard ?2 distribution table as shown in table I below and ?2 significant level was selected.

TABLE II.  CHI-SQUARE DISTRIBUTION TABLE  Confidence Interval  90.

0%  95.

0%  97.

5%   %  99.

5%  99.

9%  Tail  Proba  bility  0.

0.

0.

0.

0.

0.1   0.0   0.0   0.

0.0   0.0   Degre  e of  freedo  m=1  0.

0.

0.

1.

1.

2.7   3.8   5.0   6.

7.8   10.

Insignificant                                       Significant

IV. EXPERIMENT RESULTS Table II and Table III shows the experiments result of the  number of rules before and after post mining process for data set 2000 and data set 4000 respectively.  The row class described the type of attacks; the second column shows the number of rules before post mining and the third column shows the significant rules set for each class. In the Confidence interval section, the rules concerted based on the percentage of each single rule. Table II below has shown the sets and numbers of rules before and after the pruning and the confidence interval for each rule sets for dataset 1.

The quality of the rules after post-mining is evaluated by looking at the relationship between Chi-Squared values with  the standard Chi-Squared distribution table. The results shows applying post-mining has reduced the rules up to 95% for PROBE data subset of dataset 1, 91% for DOS subset, 89% for R2L and 97% for U2R remaining the quality rules.

Figure 2 shows the graph of the rules before and after pruning for dataset1.

Probe Dos R2L U2R  Before Pruning  After Pruning   Figure 2.  Number of Rules Before and After Pruning for Data Set1  Table III has shown the total rules before and after pruning and the significant rules level with the confidence interval for each set of rules.

The reduction for dataset 2 was 97% for PROBE subset, 96% for DOS, 90% for R2L and 90% as well for U2R. From the results it can be seen the significant levels of the rules start from 90% confidence interval and cumulative to 99.9%.

TABLE III.  CONFIDENCE INTERVAL FOR EACH RULES SETOF DATAST 2  Dataset 2. Confidence Intervals  Class Before  Pruning  After  Pruning  90.0  %  95.0  %  97.5  %  99.0  %  99.

5%  99.

9%  Probe 378 8 - - - - - 8  Dos 2568 95 28 10 13 14 10 20  R2L 284 27 12 12 - - 2 1  U2R 430 43 1 - 13 13 4 12  Figure 3 shows the graph of the rules before and after pruning for data set 2.

Probe Dos R2L U2R  Before Pruning  After Pruning   Figure 3.  Number of Rules Before and After Pruning in for Data Set 2

V. CONCLUSION  This paper has presented the important of post mining techniques in network intrusion detection. The use of ?2 pruning techniques has reduced the number of rules of a         misused detection up to 98% with preserving the same quality of knowledge above 90% confident.  This research has shown the beneficial of applying post-mining in building a quality performance of IDS. The less number of rules the better performance of IDS.

