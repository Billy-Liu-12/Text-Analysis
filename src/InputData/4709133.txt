Extended MRI-Cube Algorithm for Mining Multi-Relational Patterns

Abstract   Association rule mining is one of the most important  and basic technique in data mining, which has been studied extensively and has a wide range of applications. Two stream of previous work has dealt with the discovery of association rules over multiple relations: prolog databases and datalog queries. The MRI Iceberg-cubes mining method introduces a new perspective. However, it does not take the cyclic join paths into account, therefore, in this paper, we will introduce an algorithm, Extended-MRI-cube, which is based on the MRI-Cube algorithm, to handle the cyclic join path situation. Experiments show it is more applicable and effective than the previous one.

Keywords: Data mining, multi-relational patterns, association rules,  bottom up computation  1. Introduction   Most of today?s structured data is stored in relational databases consisting of multiple relations.

Numerous analysis and data mining tasks in a wide variety of applications including intelligence analysis, social network analysis, business data analysis, web data mining and bioinformatics are based as much on the links among heterogeneous entities and events as the properties of individual entities. Hence, the databases in these applications contain both attribute and semantic relationship data. This data is stored in multi-relational form in relational database systems as a set of linked tables each corresponding to some conceptual entity or relationship. Multi-relational data mining[1,2] is concerned with the discovery of models and patterns from such databases. However, applying most multi-relational data mining algorithms to realistically large databases is fraught with several challenges including scalability and lack of integration with database systems [3].

The vast majority of frequent pattern discovery approaches, such as Apriori [4] and its variants [5],  assume that the data resides in a single relation and require preprocessing to integrate data from multiple relations into a single relation before they can be applied. Integrating data from multiple relations, however, can cause loss of meaning or information [6].

Another category of approaches to multi-relational frequent pattern discovery comes from the field of Inductive Logic Programming (ILP) [7,8,9]. The ILP frequent pattern discovery approaches have the ability to deal with data stored in multiple relations directly and aim at finding frequent patterns expressed in the relational formalism of first-order logic. However, with larger search spaces and more complex evaluation of a single candidate pattern, these approaches are inherently computation-wise. There is much scope for improving the efficiency and scalability of these approaches.

This paper deals with the multi-relational data mining task of multi-relational association rule mining that directly operates on multi-relational databases whether their join graphs are acyclic or not. We introduce a scalable algorithm for frequent multi- relational pattern discovery that is efficiently integrated with database systems.

2. Preliminaries  2.1. Trie tree    Figure 1. a trie for tab,to and inn   A trie, or prefix tree, is an ordered tree data  structure that is used to store an associative array where the keys are usually strings. Unlike a binary search tree, no node in the tree stores the key   DOI 10.1109/ICYCS.2008.165     associated with that node; instead, its position in the tree shows what key it is associated with. All the descendants of any one node have a common prefix of the string associated with that node, and the root is associated with the empty string. Values are normally not associated with every node, only with leaves and some inner nodes that happen to correspond to keys of interest.

2.2. Bottom Up Computation   The bottom-up computation algorithm (BUC) repeatedly sorts the database as necessary to allow convenient partitioning and counting of the combinations without large main memory requirements. BUC begins by counting the frequency of the first attribute in the input table. The algorithm then partitions the database based on the frequent values of the first attribute, so that only those tuples that contain a frequent value for the first attribute are further examined. BUC then counts combinations of values for the first two attributes and again partitions the database so only those tuples that contain frequent combinations of the first two attributes are further examined, and so on.

Firgure 2. BUC computation example  For a database with four attributes, A, B, C and D,  the processing tree of the BUC algorithm is shown in Figure 2. The algorithm examines attribute A, then partitions the database by the frequent values of A, sorting each partition by the next attribute, B, for ease of counting AB combinations. Within each partition, BUC counts combinations of attributes A and B.

Again, once the frequent combinations are found, the database is partitioned, this time on frequent combinations of AB, and is sorted by attribute C.

When no frequent combinations are found, the algorithm traverses back down the tree and ascends to the next node. For example, if there are no frequent combinations of AC, then BUC will examine combinations of AD next. In this way, BUC prunes passes over the data. BUC prunes most efficiently when the attributes are ordered from highest to lowest cardinality   2.3. Multi-relational patterns   A multi-relational association rule(pattern) [10], henceforth abbreviated as MRP, is given as:  ,  11 2, 1 ( , , ) ( , , )  p q r  i i j l li j i T A v T A v  = = = ? ? ?  Where T1 is the target table corresponding to a particular real-world entity and Tj is another table in the database which is joined to table Tj-1 through a foreign key constraint (i.e. Tj and Tj-1 are part of a legal join path). Patterns over these tables are represented as conjunctions of atomic predicates which have the form Ai = vi  where Ai  is a categorical or numerical attribute and vi is a value from a certain domain. p and r are the number of predicates and q is the number of tables over which this MRP is defined. Each MRP has an associated support and confidence. Support of a pattern is the number of tuples in T1 that satisfy it while confidence is the probability of satisfying a certain subset of the pattern given the whole MRP.

2.4. Multi-relational Iceberg-cubes   Paper[11] introduce an approach to mine MRP, it?s based on Iceberg-cube computation. An Iceberg-Cube contains only those cells of the data cube that meet an aggregate condition. It is called an Iceberg-Cube because it contains only some of the cells of the full cube, like the tip of an iceberg. The aggregate condition could be, for example, minimum support or a lower bound on average, min or max. The purpose of the Iceberg-Cube is to identify and compute only those values that will most likely be required for decision support queries. The aggregate condition specifies which cube values are more meaningful and should therefore be stored. Iceberg-cube computation algorithms have two attractive features with respect to MRP mining: (1) they are based on depth-first traversal of the pattern lattice which is much more efficient than breadth-first approaches employed by past work, (2) they can be implemented inside database systems with relative ease.

3.Extended MRI-CUBE Algorithm   We?ll still use the Multi-Relational Iceberg-cube Trie. It combines BUC cubing algorithm with join path traversal. The trie stores the counts and link structure of MRPs. Its nodes correspond to attributes respectively. We classify the nodes to two categories: data nodes and join nodes, corresponding to data attributes and join attributes. Join attributes can be foreign and primary keys of tables and all the other attributes are classified to data attributes. We think     data attributes carry all the information while join attributes just do the link job.

Table 1.Cyclic-Join-Path algorithm  1. determine the target table T 2. read database schema, draw the join graph G 3. for each vertex u?V(G) do 4.      color[u]?White 5.      Father[u]?Nil 6.      time=0 7. suppose vertex N represents T,Find-E-Cricle(N) 8. use contract network to reconstruct the join graph 9. using DFS to get all the join paths 10. extend all the paths using e-circles   The Extended MRI-Cube algorithm, henceforth  abbreviated as E-MRI-Cube algorithm, consists of two parts: the Cyclic-Join-Path algorithm and the MRI- cube algorithm. We?ll discuss them in section 3.1 and 3.2 respectively.

3.1. Cyclic-Join-Path Algorithm   The main contribution of this paper is to deal with the cyclic join paths of the MRPs. Cyclic join path can be very common in a large databases with plenty of foreign keys, therefore, the effectiveness of an MRP mining algorithm will be greatly improved if it can handle the cyclic join paths.

Table 2. sub-procedure Find-E-Circle  1. procedure Find-E-Circle(u) 2. color[u]?GRAY 3. time++ 4. d[u]?time 5. for each v?Adj[u] do 6.      if color[v]?White 7.      then Father[v]?u 8.              Find-E-Circle(v) 9.      else 10.             if node is not Nil 11.                  if d[v]<d[node] 12.                  then 13.                        node?v 14.             else 15.                        node?v 16. append current search path v?v to e-cricle-list[] 17. color[u] ?BLACK 18. f[u]?time?time+1   Suppose we have three tables T1(A,B,C,D), T2(C,E,F), T3(F,D,G), then they form a cyclic join path: T1?T2?T3?T1, the previous work using iceberg-cube does not consider this situation. It is not that this  situation is not important, conversely, it is. For acyclic join paths, we can use the MRI-cube algorithm directly for them. For cyclic join paths, we use the algorithm shown in Table 1 to find all the join paths.

The algorithm first constructs a directed graph.Step3-7 find all the e-circle paths in the graph. A cyclic join path p1 is an e-circle path p if all the nodes in p1 are in p and no more nodes can be added to p1 to form a cyclic join path.  Then, use contract network to treat all the e-circles as nodes to reconstruct the join path graph. After that, we get an acyclic directed graph and use DFS to find all the join paths and finally extend all the paths according to e-circles.

Table 3. MRI cubing phase 1. CubingAttributes?find next attribute to group by 2. for each attribute attri of the input do 3. CellCounts[]?Partition input by attri 4. j?0 5. for each partition pj do 6. if CellCounts[j]?min_supp then 7.  insert count to current MRI-Cube trie  LeafNode of attri 8. if attri is a data attribute then 9. for k=1?size of CloneJoinNodes do 10. If CloneJoinNodes[k-1]?s expansion  with pj was empty 11. then 12.                break 13. else 14. expand CloneJoinNode[k] 15. MRI-Cube(pj,CloneJoinNodes) 16. j++   After getting all the join paths, we need to assign orders to the attributes. The orders of them is sorted by the decreasing cardinality, with join attributes be placed at the end of the cubing order.

3.2. MRI-cube Algorithm   When we have all the join paths, we can use the MRI-cube algorithm to mining MRPs.  It consists two parts: BUC cubing phase and expansion phase.

Table 4. Expansion phase  1. Repeat 2.    currJoinNode?pick the LM-LD node 3. Joinedtable?expand currJoinNode 4.    CloneJoinNodes[]?insert currJoinNode 5.  CloneJoinNodes[]?traverse all leaves and add all  other clone nodes of the current join node 6.    MRI-Cube(Joinedtable,CloneJoinNodes) 7. Until no more expansion is possible     BUC cubing phase starts with the target table and recursively performs cubing on each attribute using BUC until a join attribute is encountered or no more group-by is possible. The algorithm is shown in table 3.

The expansion phase is launched when the cubing recursion returns to the root. Expansion extends BUC cubing to the next table on the join path. The algorithm is shown in Table 4.

Example 1 Given three tables Table1 (A,D,E),Table2 (D,F,G) and Table3(G,E) as follows (see figure 3). The trie tree of them is shown in figure 4. The process of the E-MRI-Cube algorithm also is shown in the figure, with the recursion order of r1,r2? and expansion order of e1,e2,etc.

Figure 3. tables for the example     Figure 4. process of the algorithm   4. Experiments   In this section, we?ll compare the E-MRI-Cube algorithm with MRI-Cube algorithm and FARMER.

The experiments are carried out using c++ on a Lenovo computer with 2GB of RAM and 2.33GHz of cpu frequency.

The comparison is done on a real dataset. It?s drawn from CiteSeer[12] and HPSearch[13]. It involves tables on authors, papers, citations and publisher links.

For FARMER, we first convert the dataset to a  predicate-based representation and specify the required patterns and other settings. For E-MRI-Cube algorithm, we choose table paper to be the target table. Figure 5 and figure 6 show the comparison between them. From the figures, we can see when min_supp and data size increase, E-MRI-Cube outperforms FARMER.

Figure 5. comparison w.r.t min. support     Figure 6. comparison w.r.t data size   The comparison between E-MRI-Cube and MRI-  Cube can be obvious. Since E-MRI-Cube handles a more general occasion, so it?s more applicable. On time complexity, the E-MRI-Cube algorithm handles the cyclic join paths, so it takes a longer time, but with the same time complexity scalability. This is because the time complexity of the two algorithms depends on the pattern mining step. On space complexity, both of them use trie to store the structure, if there are no cyclic join paths in the join graph, they can be the same, and if there exists cyclic join paths, MRI-Cube is not applicable, so there is no sense to compare them.

5. Conclusion and Future work   In this paper, we introduce a method, E-MRI-Cube, for MRI-Cube algorithm to explore cyclic join paths. It shows that the new method is more applicable and effective than the previous one. In the future, we will do more experiments on the comparison with other     famous algorithms, and we notice that when a join path is long enough that the attributes in the head and the ones in the end may have little chance to form a pattern.

So we will focus our attention on this point and do relative work to study it.

Acknowledgement   Our research is supported by the National Natural Science Foundation of China under Grant No.60673130, the Natural Science Foundation of Shandong Province of China under Grant No.Y2006G29, No.Y2007G24 and No.Y2007G38, the Young Scientist Prize Foundation of Shandong Province under Grant 2005BS01002,  the Science and Technology Plan of Shandong Province under Grant 2005GG3201088 and 2007GG10001009, and the Science and Technology Development Plan of Shandong  Province under Grant 2006GG2201052.

References  [1] S. D?zeroski, ?Multi-Relational Data Mining: An Introduction?, ACM SIGKDD Explorations Newsletter, 2003, 5(1).

[2] R. Agrawal and R. Srikant. Fast algorithms for mining association rules in large databases. In 20th International Conference on Very Large Databases (VLDB 94), 1994.

[3] P. Domingos, ?Prospects and challenges for multi- relational data mining,? SIGKDD Explorations, 5(1),2003, pp.80 ? 83.

[4] R. Agrawal and R. Srikant, ?Fast Algorithms for Mining Association Rules in Large Databases?, Proceedings of the Morgan Kaufmann Publishers, Inc., San Francisco, CA, USA, 1994, pp. 487-499.

[5] J. Hipp, U. Guntzer, and G. Nakaeizadeh, ?Algorithms for Association Rule Mining - A General Survey and  Comparison?, ACM SIGKDD Explorations, 2(1), July 2000, pp. 58-64.

[6] S. Wrobel, ?Inductive Logic Programming for Knowledge Discovery in Databases?, Relational Data Mining, S. Dzeroski and N. Lavrac, editors, Springer, Berlin, 2001, pp. 74-101.

[7] L. Dehaspe and H. Toivonen, ?Discovery of Frequent Datalog Patterns?, Data Mining and Knowledge Discovery, 3(1), 1999, pp.7-36.

[8] S. Nijssen and J.N.Kok, ?Faster Association Rules for Multiple Relations?, Proceedings of the 17th International Joint Conference on Artificial Intelligence,Morgan Kaufmann Publishers  Inc., Seattle, USA, 2001,pp. 891-896.

[9] H. Blockeel and L.D. Raedt, ?Top-Down Induction of First-Order Logical Decision Trees?, Artificial Intelligence, 101(1-2), 1998, pp. 285-297.

[10] L. Dehaspe and H. Toivonen. ?Discovery of relational association rules?, Dzeroski, S. and Lavrac, N., ed.

Relational Data Mining, Springer-Verlag,Berlin,2001,pp.

189?212.

[11] D.Y. Seid and S.Mehrotra,?Efficient relationship pattern mining using multi-relational iceberg-cubes?, Proceedings of Computer Society Press, Brighton, UK, 2004, pp. 515-518.

[12] Citeseer. http://citeseer.nj.nec.com/cs.

[13] Homepagesearch. http://hpsearch.uni-trier.de.

