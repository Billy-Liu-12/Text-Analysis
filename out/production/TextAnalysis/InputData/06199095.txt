

Abstract-- Classification is one of the important problems in Data Mining. There are various methods of classification available like Rule based method, Decision tree, Neural network, and Bayesian networks. This paper focuses on Associative rule based classifier. FOIL (First order Inductive Learner) and PRM (Predictive Rule Mining) algorithms have been analysed in this work. The proposed work is superior to reported works in terms of memory requirements by eliminating use of intermediate data structure without sacrificing classification accuracy. The proposed work is an enhancement of existing FOIL and PRM algorithms.

Index Terms--Association rules, classification, data mining, first order inductive learner (FOIL) algorithm, predictive rule mining (PRM) algorithm.



I.  INTRODUCTION lassification is a technique of data mining in which a model or classifier is constructed to predict the class or categorical labels of given data. Data classification is  two-step process. In first step classifier is created by analysing training data. This step is known as learning step of classification. Then in second step using classifier made in first step, prediction of given data tuples are done.

Problem definition: let X be an arbitrary measurable space, { }ky 1,0= , and let D be an unknown distribution on the product space X ? Y. Each element (x, y) in this space is composed of an instance x and a label vector y, which is a vector of indicators ( )kyyy .,.........1= that specifies which classes are associated with x. We assume that label vectors sampled from D are sparse, namely, that ( ) 1Pr =?? syj j for some constant s. A classifier is a function h : X ? Y, that maps an instance x to a label vector ( )xhy =?  There are different classification techniques like Decision Tree based Methods [1], Rule based methods [2], Memory   Devashree Rai is student of M.tech (Computer Technology),  Department of Electrical Engineering, National Institute of Technology, Raipur-492010, India (e-mail: rai.devashree@gmail.com).

A. S. Thoke is Professor in Department of Electrical Engineering, National Institute of Technology, Raipur-492010, India (e-mail: asthoke@yahoo.co.in).

Keshri Verma is Assistant Professor in Department of Computer Applications, National Institute of Technology, Raipur-492010, India (e- mail: keshriverma@gmail.com).

based reasoning [3], Neural networks [4], Naive Bayes and Bayesian Belief Networks [5].

Rule based classification basically means the process of using a training dataset of labelled objects from which classification rules are extracted for building a classifier. In short, we classify objects by using a collection of ?IF- THEN? type rules.

A rule is an implication of the form X->Y, where X represents the rule antecedent or condition and Y represent the rule consequent or class label. In data it is found that the items have closed relations in order to obtain the class label, in paper [6] the researcher uses this association to detect the class label of dataset.

FOIL is an inductive learning algorithm for generating Classification Association Rules (CARs) proposed by Ross Quinlan in 1993 [7]. This algorithm was further improved to produce the PRM (Predictive Rule Mining) CAR generation algorithm. PRM was than further developed by Xiaoxin Yin and Jiawei Han[8] to produce CPAR (Classification based on Predictive Association Rules) algorithm. These algorithms are studied and compared for their performance by Gupta et. Al [9].  In our proposed work we have optimized PRM and FOIL algorithm to reduce its memory complexity. Modification done in algorithms could be adopted by CPAR algorithm to improve its performance.



II.  ALGORITHM DESCRIPTION In associative classifier, FOIL [7] is a basic algorithm  which is then improved as PRM and this PRM algorithm is further improved as CPAR. Basic difference in these strategies is in rule generation process. Foil generates rules which are not redundant but to achieve this, it loses some important rules.  PRM extract these rules but with cost of some redundancy.  Some rule may be extracted more than once. To remove these rules pruning of rule is also done in PRM. CPAR also uses similar concept of PRM  to generate rule , but it can test more than one attribute at a time to judge whether this attribute can also give some useful rule or not. So more rules and less computation is needed in CPAR in comparison to the PRM algorithm.

A. Foil Algorithm FOIL (First Order Inductive Learner), proposed by Ross  Quinlan in 1993 [7], is a greedy algorithm that learns rules to distinguish positive examples from negative ones. FOIL repeatedly searches for the current best rule and removes all  Enhancement of Associative rule based FOIL and PRM Algorithms  Devashree Rai ,Student ,  A.S. Thoke,  Member, IEEE and Keshri Verma  C          the positive examples covered by the rule until all the positive examples in the data set are covered. The algorithm FOIL is presented below. For multi-class problems, FOIL is applied on each class: for each class, its examples are used as positive examples and those of other classes as negative ones. The rules for all classes are merged together to form the result rule set.

When selecting literals, Foil Gain is used to measure the information gain for adding this literal to the current rule.

Suppose there are |P| positive examples and |N| negative examples satisfying the current rule r's body.

After literal p is added to r, there are |P?| positive and |N?| negative examples satisfying the new rule's body.

Then the Foil gain of p is defined as,  }] ||||  ||{} |'||'|  |'|{*[|'|)( NP  PLog NP  PLogPpGain +  ? +  =  Input: Training set D = (P Union N).

(P and N are the sets of all positive and negative examples, respectively.)  Output: A set of rules for predicting class labels for examples.

Procedure: FOIL   1. rule set R ? ? 2. While |P| > 0 3. N?? N, P??P 4. Rule r ? empty rule 5. While |N?| > 0 and r.length < max rule length 6. Find the literal p that brings most gain 7. According to |P?| and |N?| 8. Append p to r 9. Remove from P? all examples not satisfying r 10. Remove from N? all examples not satisfying r 11. End 12. R<- R Union {r} 13. Remove from P all examples satisfying r's body 14. End 15. Return R  In line 1 rule set R is initialized to null. Loop from line 2 to line 14 generate rule set iteratively. P? and N? are used to store copies of P and N respectively. For each literal p gain is calculated and one which is having maximum gain is selected. In line 9, 10 examples not satisfying r is removed from P? and N?. When condition in line 5 becomes false, in line 12 rule set R is updated and all examples satisfying r are removed from P. this process continuous till condition in line 2 is true that is P is non-empty and at line 15 algorithm gives classification association rules[6] as output.

B. PRM Algorithm Predictive Rule Mining (PRM), an algorithm which  modifies FOIL to achieve higher accuracy and efficiency [7]. The problem with FOIL algorithm is that it generates a very small number of rules and therefore does not achieve higher accuracy. In PRM, after an example is correctly covered by a rule, instead of removing it, its weight is decreased by multiplying a factor. This weighted version of FOIL produces more rules and each positive example is usually covered more than once. PRM gain is defined as  }]{} ''  '{'*[)( WNWP  WPLog WNWP  WPLogWPpGain +  ? +  =  Where WP = totalWeight (P) WN = totalWeight (N) WP' = totalWeight (P') WN' = totalWeight (N')  Algorithm is as follows:  Input: Training set D = (P Union N). (P and N are the sets of all positive and negative examples, respectively.)  Output: A set of rules for predicting class labels for examples.

Procedure: Predictive Rule Mining 1. Set the weight of every example to 1 2. Rule set R?? 3. Total Weight ?TotalWeight(P) 4. While TotalWeight(P) > ?(totalWeight) 5. N?? N, P?? P 6. Rule r ? empty rule 7. While true 8. Find best literal p 9. Append p to r 10. For each example t in P?, N? not satisfying r's body  remove t from P? or N? 11. End 12. R ?R Union {r} 13. For each example t in P satisfying r's body 14. t.weight ? ?(t.weight) 15. End 16. Return R  This algorithm also selects literal that have maximum gain but instead of removing its weight is updated as in line 14. Rules generated from these algorithms are used to build a classifier. But before making any prediction, every rule needs to be evaluated.

C. Rules Evaluation Rules are evaluated to determine its prediction power. For  a rule cpppr n ????= ........21  we define its expected accuracy as the probability that an example satisfying r's Body belongs to class c. Laplace expected error estimate is used to estimate the accuracy of rules, which is defined as follows.

The expected accuracy of a rule is given by [12]:  kn nuracyLaplaceAcc tot  c  + += 1  Where k is the number of classes, ntot is the total number of examples satisfying the rule's body, among which nc examples belong to c, the predicted class of the rule.



III.  PROPOSED WORK FOIL and PRM algorithms discussed uses two intermediate data structures P? and N? to hold copies of P and N (Positive and Negative examples respectively) in line 3 of Foil and line 5 of PRM. These P? and N? are used for gain calculation in both the algorithms as well. P? and N? take redundant memory allocation for positive and negative examples. Maintaining such an copies of positive and negative examples increases memory space required by the algorithm.

In this paper we proposed enhanced version of FOIL and PRM algorithm in terms of memory complexity. This is          done by eliminating use of intermediate data structures P? and N? and accordingly modifying the formula for gain calculation.

The proposed Algorithm is as follows:  Input: Training set D = (P Union N).

(P and N are the sets of all positive and negative examples, respectively.)  Output: A set of rules for predicting class labels for examples.

Procedure: newFOIL   1. rule set R ? ? 2. While |P| > 0 3. Rule r ? empty rule, m= Number of examples in P ,  m?= Number of examples in N 4. While m? > 0 and r.length < max rule length 5. Find the literal p that brings most gain 6. Append p to r 7. m = m - |number of examples not satisfying r in P| 8. m? = m? - |number of examples not satisfying r in N| 9. End 10. R<- R Union {r} 11. Remove from P all examples satisfying r's body 12. End 13. Return R  In line 1 Rule set is initialized to null value. Line 2 starts a loop which ends at line 11. In line 3 variable m is used which contains number of examples in P and m? which contains number of examples in N.  Gain is calculated by the formula:  }] ||||  ||{} ~||~||  ~||{*[~||)( NP  PLog NP  PLogPpGain +  ? +  =  Where |P~|= number of examples in P satisfying p.

|N~|= number of examples in N satisfying p.

|P| = m |N|= m?   Instead of using number of examples in P? and N? this  algorithm only uses P and N. In line 5 and 6 literal having maximum gain value is selected and is appended to rule list r. Value of m is updated in line 7.This process continuous till condition in line 4 is true. In line 10 Rule list R is updated to contain rule generated .line 11 removes all examples in P satisfying rule r body. When P becomes empty algorithm produce output R which gives required classification association rules.

The proposed PRM algorithm also follows same approach for optimization. The algorithm is as follows:  Input: Training set D = (P Union N). (P and N are the sets of all positive and negative examples, respectively.)  Output: A set of rules for predicting class labels for examples.

Procedure: newPRM  1. Set the weight of every example to 1 2. Rule set R?? 3. totalWeight ?totalWeight(P) 4. While totalWeight(P) > ?(totalWeight) 5. Rule r ? empty rule 6. While true  7. Find best literal p 8. Append p to r 9. End 10. R ?R Union {r} 11. For each example t in P satisfying r's body 12. t.weight ? ?(t.weight) 13. End 14. Return R  }]{} ~~  ~{*[~)( WNWP  WPLog WNWP  WPLogWPpGain +  ? +  =  Where WP = totalWeight (P).

WN = totalWeight (N).

WP~ = totalWeight (P) satisfying p.

WN~ = totalWeight (N) satisfying p.

As in FOIL algorithm, PRM algorithm is also enhanced by reducing memory space required by the algorithm as shown above.



IV.  EXPERIMENTAL WORK The proposed algorithms are implemented in JAVA  version 1.6.0, i5 processor, and windows7 operating system.

The figures below gives the comparison between algorithms in terms of memory used (in bytes). Figures have been obtained using number of discretised/normalised data sets of UCI machine learning repository [13] available at [14]. The parameters for the individual algorithms are as follows:  ? FOIL: Maximum of 3 attributes in the antecedent of a rule.

? PRM: Minimum gain threshold = 0.7, total weight threshold = 0.05, and decay factor = 2/3.

In experimental evaluation it is found that memory required by proposed algorithm is less compared to existing algorithm without affecting accuracy of the algorithms.

TABLE 1 COMPARISON BETWEEN MEMORY REQUIRED BY FOIL AND  NEWFOIL  Data Sets FOIL newFOIL anneal.D73.N898.C6 3964248 3303752 breast.D20.N699.C2 2651048 2320816 car.D25.N1728.C4 4627424 3303696 cylBands.D124.N540.C2 5285912 4295312 flare.D39.N1389.C9 6279480 3964400 heart.D52.N303.C5 2642816 2312656 horseColic.D85.D368.C2 2973120 2643024 pima.D38.N768.C2 3306752 2646104 pimaIndians.D42.N768.C2 3636832 2646024 ticTacToe.D29.N958.C2 3303592 2973408 wine.D68.N178.C3 1982096 1651952  TABLE 2 COMPARISON BETWEEN MEMORY REQUIRED BY PRM AND  NEWPRM  Data Sets PRM newPRM anneal.D73.N898.C6 4956976 3303608 breast.D20.N699.C2 2645656 2315288 car.D25.N1728.C4 7601296 3303576 cylBands.D124.N540.C2 5286224 3964616 flare.D39.N1389.C9 6276760 3964520 heart.D52.N303.C5 2642952 1982152 horseColic.D85.D368.C2 2643160 2312704 pima.D38.N768.C2 2973352 2312536 pimaIndians.D42.N768.C2 3303656 2312568 ticTacToe.D29.N958.C2 3303536 2642872 wine.D68.N178.C3 1982176 1651712             Fig.1. Memory comparison of FOIL and newFOIL   Fig. 2. Memory comparison of PRM and newPRM   A.  Analysis In experimental studies the graph shows that the proposed  Foil algorithm (Fig.1) reduces the memory requirements by approximately 20 %, and proposed PRM algorithm ( fig.2) reduces the memory requirement approximately by 26%.

Memory is one of the important issue while designing the algorithm, thus proposed algorithm perform better than existing FOIL and PRM algorithm.



V.  CONCLUSION AND FUTURE WORK Classification is one of the important problems for real world data. Many experimental studies [8], [6], [10], [11] showed that associative classifier has high potential that constructs more predictive and accurate classification systems than traditional classification methods like decision trees [1]. There are much closed co-relations between the item set, this theory is used to determine the class label of dataset. In proposed paper we introduced enhanced FOIL and PRM algorithm which gives the same accuracy but reduces the space complexity of the algorithms. This novel concept could be extended and applied to CPAR algorithm to improve its performance in terms of memory requirement.

