Minimizing Makespan and Total Completion Time in MapReduce-like Systems

Abstract?Effectiveness of MapReduce as a big data pro- cessing framework depends on efficiencies of scale for both map and reduce phases. While most map tasks are preemptive and parallelizable, the reduce tasks typically are not easily decomposed and often become a bottleneck due to constraints of data locality and task complexity. By assuming that reduce tasks are non-parallelizable, we study offline scheduling of minimizing makespan and minimizing total completion time, respectively.

Both preemptive and non-preemptive reduce tasks are consid- ered. On makespan minimization, for preemptive version we design an algorithm and prove its optimality, for non-preemptive version we design an approximation algorithm with the worst ratio of 3  ? 1  2h where h is the number of machines. On  total complete time minimization, for non-preemptive version we devise an approximation algorithm with worst case ratio of 2? 1  h ,  and for preemptive version we devise a heuristic. We confirm that our algorithms outperform state-of-art schedulers through experiments.



I. INTRODUCTION  MapReduce [1] (MR) is a popular batch data processing framework that enables massive amounts of such data to be organized and then analyzed on a distributed cluster of nodes preserving the principles of data locality and bringing computation closer to where the data resides.

MR processing is devided into two phases: the map phase  and the reduce phase (Fig. 1). Correspondingly each job is divided into two types of tasks: map tasks that take the raw or processed data as the input and output key-value pairs, and reduce tasks that take the pairs output by the map tasks and compute the final results. In detail, the input to a job is divided into fixed-size splits, and for each split one map task is created.

The system sorts the output of map tasks, and then merges them and passes them to the reduce tasks as the input. Thus, having many splits means the processing time for the map tasks is often shorter than that of the reduce tasks. Moreover, if the splits are moderately small, the processing is better load- balanced when the splits are processed in parallel.

Though ?distributed? is one of MR?s hallmarks, this frame-  work has a centralized scheduler, i.e. master node that controls the execution and machine allocations for the tasks in arriving  ? Corresponding Author: Deying Li. Email: deyingli@ruc.edu.cn This work was supported in part by NSF grants CNS-1016320 and CCF-  0627233, and NSFC grants 61070191 and 91124001.

map  reduce  pr oc  es s  pr oc  es s  sort merge  Map Phase Reduce Phase  part  part  m ap  ta sk  reduce task  sp lit  sp  lit  sp lit   Figure 1: MR execution sequence  jobs. Besides FIFO, the classic yet simple approach, Fair Scheduler and Capacity Scheduler have been realized in MR implementation. The Fair Scheduler [2] aims to give a fair share of the cluster capacity to every user, on average the users will get similar cluster resources. It places jobs in pools and supports preemption. The Capacity Scheduler [3] uses queues like the pools using in Fair Scheduler, but jobs within each queue are scheduled using FIFO scheduling with priorities.

Beyond the pragmatic schedulers, some works viewed the  MR scheduling in theoretic ways. The foundation of all the theoretic analysis is the dependence between the map and reduce tasks, i.e. for any job, none of its reduce tasks can be started unless all of its map tasks have been finished. Chen et al [4] treated the sort and merge operations as an independent and integrated phase in such theoretical analysis. However, a sort operation depends on its previous map output, while a merge operation proceeds the reducer?s output. Thus, sort can be broadly a map component and merge can be part of reduce [5]. In this effort, we study MR as a two stage framework.

Probability based methods [6],[7],[8] and linear program-  ming based methods [4],[9] are so far the hottest techniques for MR scheduling study. The effectiveness of probability based methods depends on the heavy-tailed property of job process- ing times, and this kind of methods have been used to mitigate the starvation problem. Linear programming based methods,  IEEE INFOCOM 2014 - IEEE Conference on Computer Communications      on the other hand, are used mainly on optimizing the weighted flow time (completion time). Compared to the previous two popular methods, combinatorial optimization based methods need more study. Note in MR a job consists of a batch of tasks, hence the scheduling has become a novel generalization of the classic two stage scheduling problem. Except the bounds on flow time obtained in [10], few theoretical guarantees generated by combinatorial optimization have been presented, it is urgent to study MR scheduling from this perspective.

In our model, stronger than existing ones, we assume that  map tasks are parallelizable but reduce tasks are not, i.e. one map task can be executed on multiple machines at one time but one reduce task can only be executed on one machine at one time. Our model maintains the parallelism on job level but gives a constraint on the task level?s parallelism. Under our assumption, many existing preemptive methods are inap- plicable, since arbitrarily splitting reduce tasks to the machines often leads to some task being processed by many machines simultaneously. The performance analysis techniques for their schedulers are not suitable either. Hence it needs novel and careful consideration to solve our problem.

Our main contributions in the paper are: 1) We study the MR Scheduling problem of minimiz- ing Makespan, i.e. the maximum completion time. We give the uniform lower bound for preemptive and non- preemptive reduce tasks. For the preemptive version we design an optimal algorithm. For the non-preemptive version we design an approximation algorithm and prove that it has the worst ratio of 32? 12h where h is the number of machines.

2) We study the MR scheduling problem of minimizing total completion time of all jobs. For the non-preemptive version, we propose an approximation algorithm with worst case ratio of 2 ? 1h . For the preemptive version, we extend the classic McNaughton?s Wrap-around Rule and propose a heuristic.

3) Through simulations we confirm that on both makespan minimization and total completion time minimization, our algorithms outperform state-of-art schedulers.



II. RELATED WORK  The makespan minimization problem of two stage (or machine) scheduling was first studied by Johnson in [11].

He assumed that each machine can process one job at a time, and each job must be finished through machine one before it is passed to machine two. A simple optimal decision rule was obtained under the assumption that the setup time plus work time for each job on each machine is known. He also discussed a restrict case of three machine problem. In [12], Ali et al. proposed a two-stage scheduling problem with bicriteria: makespan and mean completion time. They proved this problem is NP-hard, and proposed heuristics to solve it.

For minimizing the makespan for a batch of jobs in MR,  several works have also been presented. [13] proposed a framerwork ARIA to meet the performance goals like deadline of Hadoop jobs. They estimated the makespan of a set of jobs  and devised the methods of allocating resources according to the makespan estimation. In [14], Luo et al. presented a hierarchical MR framework to gather resources from clusters and run jobs accoss them. The framework splits the map tasks to different clusters and balances the workload in a naive way.

They showed that the framework reduced the entire execution?s makespan experimentally. Verma et al. in their work [15] mentioned that the order in which MR jobs are processed has a significant impact on the makespan, they proposed a heuristic named BalancePools that achieves 15% ? 38% makespan improvements in their experiments.

Schedulers that aim to reduce the (weighted) total comple-  tion time in MR framework has also been studied. In [9], the authors devised OFFA, a 3-approximation algorithm for offline scheduling. The author also devised the online version algorithm that achieved 30% improvements compared to FIFO.

However, all their theoretical and experimental results were obtained by assuming that no precedences exists between tasks, which ignored a basic MR feature that the reduce tasks can only begin after the map tasks are done. In [4], based on [9] the authors added the precedence between map and reduce tasks, and they also claimed the shuffle phase should be considered independently. They devised offline approximation algorithms with guaranteed ratios. However frequent compu- tations are required for linear programming based method.

In [6], Tan et al. observed the starvation problem in  Fair Scheduler, and designed Coupling Scheduler that jointly schedules map and reduce tasks by coupling their progresses to mitigate this problem. [7] studied the capacity region and delay performance in MR, and presented a new queueing ar- chitecture. They proposed a scheduling algorithm joined by the Shortest Queue and Maxweight policies. The authors proved their algorithm is heavy-traffic optimal. In [8], based on a well- known fact that no online scheduling algorithm can achieve a constant competitive ratio on optimizing total completion time, the authors defined their criteria named efficiency ratio.

They assumed that each map task has 1 unit workload. Based on SRPT(Shortest Remaining Processing Time), the authors designed algorithms and claimed that in preemptive scenario their algorithm reaches a small efficiency ratio.

Moseley et al. in [10] considered MR scheduling based on  the classical two-stage flow shop problem, and studied the flow time minimization. When the machines are identical and each job consists of multiple tasks, they proposed an offline 12-approximation algorithm and an online (1+?)-speed O( 1?2 )- competitive algorithm. When the machines are unrelated and each job consists of one map task and one reduce task, they proposed an offline 6-approximation algorithm and an online (1 + ?)-speed O( 1?4 )-competitive algorithm.



III. SYSTEM MODEL AND PROBLEM STATEMENT  As mentioned in previous work that the map task has a very small size comparing to the reduce task, thus, we assume that the map task is fractional, which means that each job can be arbitrarily split between the machines, and parts of the same job can be processed on different machines in parallel.

IEEE INFOCOM 2014 - IEEE Conference on Computer Communications      r11 r21r21  r11 r21  Machine  Time r2  Machine  Time  (a) Parallel (b) Nonparallel  1 1  2 2  0 21 10 2 3  Figure 2: Parallel and nonparallel tasks  For the reduce task, we address both non-preemptive and preemptive version. If preemption is allowed, task may be preempted into a few pieces and these pieces are assigned to possibly different machines. Nevertheless, in reduce phase, the minimum unit for executing in parallel is a job. No matter if preemption is allowed or not, a single reduce task can not be executed in parallel, i.e. no reduce task can be executed on multiple machines at the same time. In Fig. 2, suppose r11 and r12 are two preemptive tasks of a job J1. In Fig. 2(a), a task can be processed in parallel and J1 is finished at time 2, in Fig. 2(b), no task is allowed to be processed in parallel thus J1 is done until time 3. In job level J1 is processed in parallel in both cases.

In existing works, to easily find the solution each reduce task  is assumed to be processed in parallel freely, however this is not true. Because compared to map jobs, reduce jobs are much more complex, and there are many operations that must be pro- cessed in sequence. Hence in this paper we assume in reduce phase a job is the minimum level of parallel processing, and every reduce task cannot be processed on multiple machines simultaneously. Both objectives of minimizing the makespan and the total completion time of all jobs are considered.

The problem can be formally described as follows. Given a  set of jobs J = {J1, J2, ? ? ? , Jn}, which must be processed on h identical parallel machines {?1, ..., ?h}. LetMj and Rj be the sets of map tasks and reduce tasks of Jj for any 1 ? j ? n separatively, and denote |Mj | and |Rj | the numbers of tasks in Mj and Rj respectively. Let Cj be the completion time of job Jj , our goal is to minimize the makespan, i.e., maximum completion timemax1?j?n{Cj} and the total completion time?n  j=1 Cj . For the sake of conciseness, we denote makespan minimization on P machines by P ||Cmax, and denote the total completion time minimization on P machines by P ||?Cj .

The performance of an offline approximation algorithm A  is measured by its worst-case ratio, which can be defined as the smallest number ? such that for any J , ZA(J ) ? ? ? ZOPT (J ), where ZA(J ) (or in short ZA) denotes the objective function value produced by A and ZOPT (J ) (or shortly ZOPT ) denotes the optimal objective function value.

Throughout the rest of this paper, the following notations  are used. Denote by p(S) the total size of all tasks in set S . Let rtii be the i-th largest reduce task in the union set ?1?j?nRj , where ti denotes that the task rtii belongs to Rti , the reduce task set of Jti . For an arbitrary task rk, we use |rk| to denotes its length.

? Computation Complexity: Under our model: 1) In the pre- emptive version, i.e. preemptions on reduce tasks are allowed,  makespan minimization is in P when all jobs arrive in the beginning, and we will give a polynomial time optimal algo- rithm. The computation complexity of total completion time optimization, on the other hand, is open when all jobs arrive at the start. 2) In the non-preemptive version, both makespan minimization and total completion time minimization areNP- hard. To prove the NP-hardness of makespan minimization, simply assume 2 same machines exist and only one job J1 exists, M1 = ? and R1 = {a1, ..., an} where ai is an integer and  ?n i ai is even. A scheduling with makespan  ?n i ai/2  exists if and only if {a1, ..., an} has a partition. Further, in this instance the total completion time equals to the makespan, thus total completion time minimization is also NP-hard.



IV. MRSM  In this section, we consider the MR Scheduling problem of minimizing Makespan (MRSM). Both preemptive and non- preemptive versions are considered. We first give the lower bound of optimal objective value below.

Theorem 1. No matter if preemption is allowed or not, the optimal objective value of problem MRSM is at least  LB = max{ ?n  j=1 p(Mj ?Rj) h  , max 1?i?h?1  fi},  where  fi = p(?1?j?iMtj ) +  ?i j=1 r  tj j + (h? i)rtii  h .

Proof: It is trivial that the objective value is at least?n j=1 p(Mj?Rj)  h according to the average load of h machines for all map and reduce tasks. We next show that it is at least fi for any 1 ? i ? h? 1.

For any 1 ? i ? h?1, let us consider the optimal schedules  for tasks in Ji = {rt11 , rt22 , ? ? ? , rtii } ? (?1?j?iMtj ). Denote by Z?i the makespan of optimal schedule for Ji. We first show that there exists an optimal schedule ?? in which each reduce task rtjj , 1 ? j ? i is non-preemptively processed on j-th machine during time interval Ij = [Z?i ? rtjj , Z?i ] as shown in Fig. 3. Note that the length of the interval Ij is |Ij | = rtjj .

Figure 3: An optimal schedule ?? for Ji  In fact, for any optimal schedule ? for Ji, we can transform it into ?? by the following steps.

IEEE INFOCOM 2014 - IEEE Conference on Computer Communications      1) For any rtjj , suppose that some of it is processed on k-th machine during time interval I , k ?= j. It is easy to obtain that there must not exist any part of rtjj on j- th machine during I because of non-overlapping rule of preemption: each task can not be processed on different machines at the same time. Thus, we can exchange the tasks on two machines in the same interval I . By this kind of exchange, all parts of rtjj can be moved to j-th machine.

2) For each j-th machine, 1 ? j ? i, there might be some map tasks between two different parts of rtjj . All parts of rtjj can be moved into the interval Ij via exchanging each part and its neighboring map tasks. This exchange is feasible because the reduce task rtjj is processed after finishing all map tasks on the same machine.

In the optimal schedule ?? as shown in Fig. 3, the reduce task rtjj is processed on time interval Ij on the j-th machine and all map tasks in ?1?j?iMtj must be finished before the reduce task rtii . It implies that there is no task in the interval Ii = min1?j?i Ij on any one of the last h ? i machines.

Hence, we conclude that the total size of all tasks in Ji is  p(?1?j?iMtj ) + i?  j=1  r tj j  ? hZ?i ? (h? i)|Ii| = hZ?i ? (h? i)rtii ,  that is,  Z?i ? p(?1?j?iMtj ) +  ?i j=1 r  tj j + (h? i)rtii  h = fi.

Note that Ji ? J for any 1 ? i ? h ? 1, we obtain that the optimal objective value is at least Z?i ? fi.

A. McNaughton?s wrap-around rule  Before we further introduce our algorithm, we give a sim- ple algorithm named McNaughton?s wrap-around rule [16].

Given a set of tasks {r1, r2, ..., rm} and at most h same machines, this algorithm creates an optimal schedule for makespan minimization with preemptions, which has a length of D = max{?mi |ri|/h,maxi ri}. The tasks are sorted and then placed on the machines. The rule fills machine ?i up until load D before it starts machine ?i+1. A task ri may be split into two parts, the first part is executed on the last e units of time of ?i, while the second part is executed on the first |ri|?e units of time on ?i+1, for some e. Because there are no more than h ?D units of processing, and since D?e ? |ri|?e for any e, a task is scheduled on at most one machine at any time.

Note that McNaughton?s wrap-around rule creates the  schedule machine by machine, rather than over time. Al- though machine {1, .., h} can be used for the schedule, when maxi ri >  ?m i |ri|/h, the load on the last scheduled machine  may be smaller than D, and some machine may have no task assigned on it, i.e. its load is 0.

B. An optimal preemptive algorithm  In this subsection, we consider the optimal preemptive algorithm for problem MRSM. Note that our goal is to mini- mize the maximum completion time of all jobs, one obvious ideal is to schedule all map tasks first then all reduce tasks, which can dodge the dependence between the map and reduce tasks. Thus, the problem becomes classic parallel machine scheduling problem of minimizing makespan[16]. It is easy to obtain the optimal makespans for the map and reduce tasks, respectively. However, the sum of two optimal makespans may not be the optimal makespan for MRSM. Let us consider an instance J with two jobs where M1 contains N tasks, R1 = ?, M2 = ? and R2 only contains one task with size of Nh?1 . Clearly, the makespan produced is  N h +  N h?1 if we  scheduling all map tasks first and followed by all reduce task, while the optimal makespan is Nh?1 by scheduling all map tasks on h? 1 machines and the reduce task on one machine independently.

We propose an optimal linear time algorithm named Pre-  eMPTive, and we call it PMPT for short in the following.

In MR scheduling, all map tasks of each job must be finished before processing any of its reduce tasks. Conversely however, for ease of description, in our algorithm, we schedule the reduce tasks first then its corresponding map tasks. Finally, we output the schedule in reverse-chronological order and thus it becomes a feasible solution. Our algorithm can be described in detail as follows.

Algorithm PMPT can be illustrated in Fig. 4.

Figure 4: Optimal schedule produced by PMPT (before reversing the execution sequence)  Remark. Note that there is no Mt3 in Fig. 4, which represents that reduce task rt33 and one of r  t1 1 and r  t2 2 belong  to the same job, that is, t3 = t1 or t3 = t2.

Let ZPMPT be the makespan produced by algorithm PMPT,  then we have the following theorem.

Theorem 2. ZPMPT = LB and thus Algorithm 1 is optimal.

Proof: Suppose the algorithm stops at step 3, by our algorithm, all reduce tasks are assigned to h machines by Mc- Naughton?s Wrap-around rule, which can generate a schedule such that the loads of all machines are the same, because the largest task is not greater than the average value of the total  IEEE INFOCOM 2014 - IEEE Conference on Computer Communications      size of all tasks, i.e., rt11 ? p(?1?j?nRj)h . For all map tasks, it is easy to evenly assign them on h machines. Thus, we obtain  ZPMPT = p(?1?j?nRj)  h +  p(?1?j?nMj) h  =  ?n j=1 p(Mj ?Rj)  h .

We next consider the case where our algorithm stops at step 24. Denote by ci the completion time produced by our algorithm after scheduling the map tasks in Mti , 1 ? i ? k.

We claim that  ci = max 1?j?i  fj . (1)  We use induction to prove (1). For i = 1, by step 13, it is easy to obtain that c1 = rt11 +  p(Mt1 ) h = f1. Suppose (1) holds  for i? 1. We then consider the case of i. If Mti ? Si?1, i.e., the map tasks inMti have been assigned before, then we have ci = ci?1 and thus (1) holds trivially. On the other hand, we need to schedule the map tasks Mti by step 13. If all these tasks can be assigned in the idle time slots during interval [rtii , r  t1 1 ], we still have ci = ci?1. Otherwise, we can obtain  that  ci = r ti i +  p(?1?j?iMtj ) + ?i  j=1 r tj j ? irtii  h = fi,  which implies (1) holds.

Finally, we consider the makespan of our algorithm after  scheduling all tasks by step 23. If all the tasks in ?1?j?nMj \ Sk can be processed in the idle time slots before rt11 , then we have  ZPMPT = ck.

Otherwise, it is not hard to get that  ZPMPT =  ?n j=1 p(Mj ?Rj)  h .

Hence, we conclude that ZPMPT = LB by Theorem 1 and thus the algorithm is optimal.

C. A non-preemptive algorithm  In this subsection, we consider the non-preemptive algo- rithm for problem MRSM. Based on algorithm PMPT, we obtain an algorithm named Non-PreeMPTive (NPMPT) with worst case of 32 ? 12h .

Remark Note that there is no preemption in the schedule for Jh by algorithm PMPT, then our algorithm is non-preemptive.

Theorem 3. The worst case ratio of Algorithm 2 is at most 2 ? 12h .

Proof: Let r be the last finished reduce task and its start time is s, then the makespan produced by algorithm NPMPT is s + r. If r ? Jh, that is, it is scheduled by step 2, then we obtain optimal makespan by algorithm PMPT. Thus we assume that r is not in Jh, which implies that the optimal makespan is at least rthh + r ? 2r. In addition, there is no idle  Algorithm 1 PMPT input: a list of jobs J1, J2, ..., Jn; output: the schedule.

Phase one:  1: Select the first h largest reduce tasks rt11 , r t2 2 , ? ? ? , rthh in  ?1?j?nRj .

2: if rt11 ? p(?1?j?nRj)h then 3: Schedule all reduce tasks by McNaughton?s Wrap-  around Rule, assign all map tasks evenly to h ma- chines, goto step 24.

4: else 5: Let k (1 ? k ? h?1) be the largest number such that  rtkk > p(?1?j?nRj)?  ?k j=1 r  tj j  h?k ; 6: for 1 ? i ? k do 7: Schedule the task rtii on i-th machine from time  0; 8: end for 9: Use McNaughton?s wrap-around rule to schedule all  the remainder reduce tasks, i.e. ?1?j?nRj \ {rt11 , ? ? ? , rtkk }  .

= R? on the left h? k machines;  10: end if Phase two:  11: i ? 1, Si ? Mti ; 12: while true do 13: Assign map tasks in Mti to the idle time slots in  interval [rtii , r t1 1 ], once all these idle time slots are occ-  upied, assign the remainder map tasks to the time slots as early as possible from time rt11 ;  14: do 15: i ? i+ 1; 16: while Mti ? Si?1; 17: if i > k then 18: break; 19: else 20: Si ? Si?1 ?Mti ; 21: end if 22: end while 23: Assign all remainder map tasks in ?1?j?nMj \Sk .= M?  to the idle time slots before rt11 , once all idle time slots are occupied, assign the remainder map tasks to the time slots as early as possible from time rt11 ;  24: Reverse the executed sequence of obtained schedule while keep the executed machines the same, stop.

time in the schedule by the algorithm, we can conclude that the optimal makespan is at least hs+rh = s+  r h . Hence  s+ r  max{2r, s+ r/h} ? 3h? 1 2h  .

The proof is complete.

As mentioned in previous subsection, it is easy to obtain an  algorithm which schedule all map tasks first then reduce tasks, denoted by FMFR. The map tasks can be assigned evenly to h machines. The reduce tasks can be scheduled by any algorithm  IEEE INFOCOM 2014 - IEEE Conference on Computer Communications      Algorithm 2 NPMPT input: a list of jobs J1, J2, ..., Jn; output: the schedule.

1: Select the first h largest reduce tasks rt11 , r t2 2 , ? ? ? , rthh in  ?1?j?nRj ; 2: For Jh = {rt11 , rt22 , ? ? ? , rthh } ? (?1?j?hMtj ), schedule  ri on machine ?i, and schedule ?1?j?hMtj by PMPT (Alg. 1). If there are some idle times between map and reduce tasks, then move the reduce tasks earlier such that no idle time exists;  3: For the remainder map tasks, assign them to h machines evenly before the tasks in ?1?j?hMtj ;  4: For the remainder reduce tasks, schedule them to the least load machine one by one after the first h tasks.

for problem P ||Cmax[17]. However, the worst case ratio of FMFR is at least 2? 1h . We still take the instance J mentioned in previous subsection as an example, we can obtain that the makespan produced by FMFR is Nh +  N h?1 while the optimal  makespan is Nh?1 as long as N is an sufficient large number, which follows that the worst case of FMFR is 2? 1h .



V. MRST  In this section, we consider the MR Scheduling problem of minimizing Total completion time of all jobs (MRST).

A. A non-preemptive algorithm  For the variant where the reduce tasks are not allowed to be preempted, based on Shortest Processing Time (SPT)[18] we propose an algorithm named Non-preemptive Shortest Processing Time (NSPT), with worst case ratio of 2? 1h .

Note that algorithm SPT is optimal for the single machine  scheduling problem 1||?Cj , where each job has one task and no precedence constraints. If, like the map task, each reduce task has a unit size, then we can evenly assign both map and reduce tasks to h machines, which is equivalent to the problem 1||?Cj on a h-speed machine. Hence, we can use SPT to generate an optimal schedule ?SPT for the problem 1||  ? Cj  on a h-speed machine, where each job Ji has a size of p(Mi? Ri).

Algorithm 3 NSPT input: a list of jobs J1, J2, ..., Jn; output: the schedule.

1: Sort jobs Ji in non-decreasing order of p(Mi ?Ri); 2: for ?Ji, 1 ? i ? n do 3: Assign all map tasks in Mi so that they can be  finished as early as possible; 4: Assign each reduce task in Ri to the least loaded  machine; 5: end for  Let C?i = h  ?i j=1 p(Mi ?Ri). Denote rmaxi and CNSPTi  be the largest task in Ri and the completion time of job Ji  in NSPT, i.e. Algorithm 3. Let Z?SPT denote the objective value of schedule ?SPT . Denoted by ZNSPT and ZOPT the objective values optimal schedule and the schedule produced by algorithm NSPT for problem MRST, respectively. It is easy to obtain the following results.

Lemma 1. (i) ZOPT ? Z?SPT = ?n  i=1 C ? i ; (ii) C  NSPT i ?  C?i + h?1 h r  max i for any 1 ? i ? n.

Proof: (i) Since all jobs are sorted in non-decreasing order of p(Mi ? Ri), we have the completion time of each job is C?i and thus the total completion time of schedule ?SPT is?n  i=1 C ? i . Clearly, the objective value of problem MRST is  not less than that of problem 1||?Cj on a h-speed machine, i.e., ZOPT ? Z?SPT .

(ii) For any 1 ? i ? n, let ri be the last finished reduce  task in Ri and its start time is si, then we have CNSPTi = si + ri. According to the rule of algorithm, we can conclude that there is no idle time on each machine before time si, then we have si ?  ?i j=1 p(Mi?Ri)?ri  h . Together with the fact that ri ? rmaxi , we have  CNSPTi = si + ri  ? ?i  j=1 p(Mi ?Ri)? ri h  + ri  ? C?i + h? 1 h  rmaxi .

Theorem 4. ZNSPT ? (2? 1h )ZOPT .

Proof: By Lemma 1(ii), we have  ZNSPT ? n?  i=1  CNSPTi  ? n?  i=1  C?i + h? 1 h  n? i=1  rmaxi .

It is clear that ZOPT ? ?n  i=1 r max i and ZOPT ?  ?n i=1 C  ? i  from Lemma 1(i). Hence, Theorem 4 stands.

B. A preemptive heuristic  Note that McNaughton[16] has shown that preemption cannot reduce  ? Cj for problem P ||  ? Cj , however, it is not  true in our problem. In fact, we can give a very simple instance J ? including only one job with M1 = ? and R1 = {1, 1, 1}.

Clearly the completion time is 2 if preemption is not allowed while the objective value is 3/2 if preemption is allowed.

Hence, we design a preemptive algorithm to minimize the total completion time.

Synchronic start is the situation where all the machines start synchronically. Fig. 5 plots that all machines start at time 0, an scenario of synchronic start. We present another definition step start, which means there exist at least two machines that start (become available) in different times. Fig. 6 plots an instance of step start.

Since McNaughton?s Wrap-around Rule only deals with the  Synchronic start, we design the Extented Wrap-around Rule  IEEE INFOCOM 2014 - IEEE Conference on Computer Communications      Machine  Time   0 2 4  Figure 5: Synchronic start  Machine  Time0 2 4   Figure 6: Step start  Machine  Time  t2 t1  t3 t4  t5  x  c(x)  Figure 7: c(x)  that can also deal with step start. To start with, we define a function c(x) on a given sequence of positive real numbers (t1, ..., th):  c(x) =  h? i=1  ci(x), where  ci(x) =  { 0 x ? ti x? ti x > ti  .

Theorem 5. Given h machines {?1, ?2, ..., ?h} such that ?i starts at time ti (1 ? i ? h), and R, a set of |R| reduce tasks {r1, r2, ..., r|R|}. Without loss of generality suppose t1 ? t2 ? ... ? th and |r1| ? |r2| ? ... ? |r|R||. Let  x? = argmin x  {x|c(x) ? |R|? i=1  |ri|} k? = argmax  k {k|tk < x?}  L = max{x?, max 1?i?k?  (ti + ri)} (2)  then there is a schedule that assigns R on {?1, ..., ?k?} which finishes R before time L on each machine.

We omit the proof of Theorem 5 here. The computation  of minx{x|c(x) ? ?|R|  i=1 |ri|} is not straightforward but it can be done in O(h) time, since c(x) = ix ? ?ij=1 rj if ti < x ? ti+1. L in (2) gives us an estimation of the time it needs to schedule the reduce tasks in R on the machines so that no task is processed on different machines at the same time. Note that Theorem 5 stands for both synchronic start and step start, however, L may not be optimal for step start.

In Alg. 4 the tasks are assigned on the machines from time L to each machine?s starting time. For example, check a task u with work load 1 and machine ?1 (starts at t1), we place u in the interval [L?1, L], while in McNaughtons wrap-around rule, u is placed in [t1, t1 + 1].

The idea of the algorithm for preemptive reduce tasks is also  based on SPT, yet its process is like playing jigsaw puzzles.

Hence we name it Jigsaw Shortest Processing Time, which is JSPT in short. JSPT assigns each job Ji one by one to finish them in sequence. When M1, the map tasks in job J1, whose size p(M1 ? R1) is the smallest, has been assigned, the machines can still start synchronically. However, after the assignment of R1, if max{|r1i |} >  ? i |r1i |/h, some machines  may have no task in R1 assigned on, and they can start  Algorithm 4 Extended Wrap-around Rule input: h machines {?1, ?2, ..., ?h}, ?i is available at time ti for 1 ? i ? h. A set of reduce tasks R = {r1, r2, ..., r|R|}.

output: the schedule.

1: Sort the tasks in R, obtain a new sequence of tasks (r?1, r  ? 2, ..., r  ? |R|) such that |r?1| ? |r?2| ? ... ? |r?|R||;  2: Sort the machines, obtain a new sequence of machines (??1, ?  ? 2, ..., ?  ? |R|) such that t  ? 1 ? t?2 ? ... ? t?h;  3: Obtain L according to formula (2); 4: i ? 1, j ? 1; 5: while i ? |J | do 6: if |r?i| + the load of ??j ? L? t?j then 7: place r?i on ?  ? j from the right, i.e. time  L, as late as possible; 8: else 9: assign r?i the earliest available e load of ?  ? j ;  10: j ? j + 1; 11: assign r?i the latest |r?i| ? e load of ??j ; 12: end if 13: i ? i+ 1; 14: end while  Algorithm 5 JSPT input: a list of jobs J1, J2, ..., Jn; output: the schedule.

1: Sort jobs Ji in non-decreasing order of p(Mi ?Ri); 2: for ?Ji, 1 ? i ? n do 3: Assign all map tasks in Mi so that they can be  finished as early as possible; 4: Assign the reduce tasks in Ri using Alg. 4; 5: end for  earlier for scheduling the next job J2, for example, suppose R1 is assigned on ?1 ? ?x, then M2 should be assigned on ?x+1 ? ?h. After R2 has been assigned by Alg. 4 there are two possible situations: 1) Its tasks have been placed on more than or equal to h?x machines, i.e. ?p??h where p ? x, as shown in Fig. 8(a) and Fig. 8(b). 2) Its tasks have been placed on less than x machines, i.e. ?p ? ?h where p > x, as shown in Fig. 8(c) and Fig. 8(d).

After M3 is assigned, 4 typical scenarios may appear  IEEE INFOCOM 2014 - IEEE Conference on Computer Communications      R2M2  R1  Machine  Time1  x  h  p  M3  M1  (a) FinishM3 before R2  M3  R2M2  R1  Machine  Time1  x  h  p M1  (b) FinishM3 after R2  M3 R2M2  R1  Machine  Time1  x  h  p  M1  (c) FinishM3 before R1  M3 R2M2  R1  Machine  Time1  x  h  p  M1  (d) FinishM3 after R1 Figure 8: Possible situations before scheduling R3  according to its load. In 8(c), we see that machines will be available at 3 different times for scheduling R3. When more jobs are assigned, the available times of different machines may vary more, which McNaughtons Wrap-around Rule can no longer handle. That is the reason why we design Alg. 4 and use it in our heuristic JSPT.



VI. SIMULATION  In this section, we present the simulation results of our algorithms. The efficacy of each of our algorithms is compared to several candidate approaches. To begin with we introduce the schedulers and experimental setting in our evaluation.

A. Simulation Setup  We compare 6 typical schedulers with our algorithms: ? FIFO: First in First out. Jobs are served in their orders of arrivals. To compare with our offline algorithms, the jobs arrive at the same time will be executed in random sequence.

This strategy is the basis of our comparison throughout this section.

? Capacity: The latest scheduler in Hadoop. To simulate this scheduler, we group jobs into several queues and machines into several clusters, and allocate queues to clusters. Within each cluster FIFO is used to process the jobs in a queue.

? SJF: Shortest Job First. The tasks in the job with shortest size are served first. The tasks in the same job are executed in random sequence.

? STF: Shortest Task First. Different from SJF, STF uses the individual size of a task instead of the total size of tasks in a job to decide the execution sequence.

? LJF and LTF: Longest Job First and Longest Task First.

Similar to SJF and STF respectively, except that the longest job and the longest task are severed first respectively.

We test following two types of distributions of the task?s  length: Uniform distribution: X?U(a, b), the Probability Density  Function (PDF) f(x) = 1b?a .

Exponential distribution: X?E(?), PDF f(x) =  ?e??x, x ? 0. We fix ? = 1. To generate the random variable in [a, b], we add a to the sample when each sample is first generated, and if the new sample is greater than b, it is abandoned until new qualified sample is obtained.

We simulate a data center with 50 machines, and check the  performances of algorithms on two batches of MapRuduce jobs, one contains 10 jobs and the other contains 100 jobs. To   0.3  0.6  0.9  1.2  PMPT NPMPT FIFO Capacity STF LTF  (a) 10 jobs   0.3  0.6  0.9  1.2  PMPT NPMPT FIFO Capacity STF LTF  (b) 100 jobs  Figure 9: Makespan (Uniform Distribution)   0.5   1.5  PMPT NPMPT FIFO Capacity STF LTF  (a) 10 jobs   0.5   1.5  PMPT NPMPT FIFO Capacity STF LTF  (b) 100 jobs  Figure 10: Makespan (Exponential Distribution)  generate the reduce and map tasks for each job J , we do the following: 1) Generate |M|, the number of map tasks uniform randomly from [20, 40]. Considering that several map tasks will merge into one reduce task, we set the number of reduce tasks in the same job, i.e. |R| = ?|M|/a, where a is a real number uniform distributed in [2, 4]. 2) Decide a distribution (Uniform or Exponential), and use the same distribution to generate the loads of all tasks. For a map task, generate its load randomly from [1, 10]. For a reduce task, generate its load randomly from [10, 100]. For all the simulation results, we normalize them to make the result of FIFO equal to 1.

B. Makespan  Fig. 9 and Fig. 10 plot the comparison of our algorithms and other schedulers on makepan minimization. SJF and LJF are not compared since their performances has no difference with FIFO. From the results we see that, when there are more jobs, the difference between preemptive and non-preemptive algorithms gets smaller. The reason is because more jobs leads to more tasks, and more tasks means the ratio between one task and the sum of all tasks are smaller, which means the effect of breaking tasks into factions which reduces the makespan in preemptive scenario gets weak. We also see that when the length of reduce task satisfies exponential distribution, the  IEEE INFOCOM 2014 - IEEE Conference on Computer Communications       0.5   1.5  NSPT JSPT FIFO Capacity SJF LJF  (a) 10 jobs   0.5   1.5  NSPT JSPT FIFO Capacity SJF LJF  (b) 100 jobs  Figure 11: Total completion time (Uniform Distribution)   0.5   1.5  NSPT JSPT FIFO Capacity SJF LJF  (a) 10 jobs   0.5   1.5  NSPT JSPT FIFO Capacity SJF LJF  (b) 100 jobs  Figure 12: Total completion time (Exponential Distribution)  difference between preemptive and non-preemptive algorithms are smaller, comparing the difference when the length of re- duce task satisfies uniform distribution. This happens because in exponential distribution, there are more small tasks, and since the big task can not be processed in parallel whether preemption is allowed or not, preemptions in small tasks bring limited reduction on decreasing makespan. However, our algorithms consistently perform better than other popular schedulers.

C. Total Completion Time  Fig. 11 and Fig. 12 plot the results of total completion time minimization of our algorithms and others. In this scenario STF and LTF are not compared, because these two schedulers break job into tasks and ignore the completion time of a job, which will lead to extremely bad performance on reducing total completion time. The difference between the worst and the best schedulers increases when comparing with the sit- uation of makespan. Some different trends appear. With the increment of number of jobs, the gap between our preemptive algorithm JSPT and non-preemptive algorithm grows, and the gap between the worst algorithm LJF and other algorithms also increases. Our algorithms are still the best ones.



VII. CONCLUSION AND DISCUSSION  In this paper, besides the precedence between the map and reduce phases in MR framework, we also consider the fun- damental distinguishing properties between map and reduce tasks: while most map tasks are simple and easy to parallelize, due to constraints of data locality and task complexity most reduce tasks are not easily decomposed. Hence, we assume that map tasks are preemptive and parallelizable, but none reduce task can be processed in parallel. Both preemptive and non-preemptive reduce tasks are considered, with the goals of minimizing the makespan and total completion time of a batch of MR jobs. On makespan minimization, for preemptive  version, we design PMPT algorithm and prove it is optimal, for non-preemptive version we design NPMPT algorithm and prove that its worst ratio is 32 ? 12h , where h is the number of machines. On total complete time minimization, for non- preemptive version we devise NSPT algorithm whose worst case ratio is 2 ? 1h , and for preemptive version we devise a heuristic named JSPT based on extending McNaughton?s wrap-around rule. Our algorithms? superior over state-of-art schedulers is verified through experiments.

