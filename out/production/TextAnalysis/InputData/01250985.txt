Interpretations of Association Rules by Granular Computing

Abstract  This paper presents interpretations for association rules. It first introduces Pawlak?s method, and the corresponding algorithm of finding decision rules (a kind of association rules). It then uses extended random sets to present a new algorithm of finding interesting rules. It proves that the new algorithm is faster than Pawlak?s algorithm. The extended random sets are easily to include more than one criterion for determining interesting rules.

They also provide two measures for dealing with uncertainties in association rules.

1. Introduction  Mining association rules has received much attention currently [7]. The frequency of occurrence is a well- accepted criterion for mining association rules. Apart from the frequency, the rules should reflect real world patterns [1] [4]. It is desirable to use some mathematical models to interpret association rules in order to obtain real world patterns.

The patterns hidden in data can be characterized by rough set theory [6], in which the premises of association rules (or called decision rules in [6]) are interpreted as condition granules, and the post-conditions are interpreted as decision granules. The measure of uncertainties for decision rules is based on well-established statistical models. This only reveals the objective aspect of decision rules. However, knowledge in some applications is based on ?subjective? judgments.

In this paper, we use granular computing to interpret association rules. We first introduce Pawlak?s method [6] and formally describe the corresponding algorithm for determining strengths and certainty factors of decision rules. We then present a new interpretation of association rules using extended random sets [3]. An effective algorithm of finding interesting rules is proposed using the new interpretation. We also show that an extended  random set can be interpreted as a probability function (which can provide an ?objective? interpretation) or a belief function (which can provide a ?subjective? interpretation).

2. Databases to Decision Tables  Let U be a non-empty finite set of objects (a set of records), and A be a set of attributes (or fields). We call a pair S = (U, A) an information table if there is a function for every attribute a?A such that a: U ? Va, where Va is the set of all values of a. We call Va the domain of a.

Let B be a subset of A. B determines a binary relation I(B) on U such that (x, y) ? I(B) if and only if a(x) = a(y) for every a?B, where a(x) denotes the value of attribute a for element x?U. it is easy to prove that I(B) is an equivalence relation, and the family of all equivalence classes of I(B), that is a partition determined by B, is denoted by U/I(B) or simply by U/B. The classes in U/B are referred to B-granules or B-elementary sets. The class which contains x is called B-granule induced by x, and is denoted by B(x).

A user may use some attributes of a database. We can divide the user used attributes into two groups: condition attributes and decision attributes, respectively. We call the tripe (U, C, D) a decision table of (U, A) if C?D=? and C?D?A, where (U, C, D) is a set of classes and each class is the representative of a group of records.

For example, we assume that there is an information table (relation) that includes 1000 records of vehicle accidents, where the set of attributes is A = {driver, vehicle type, weather, road, time, accident}. If the user only uses 4 attributes and let C = {weather, road}, and D = {time, accident}. C?D determines a binary relation I(C?D) on U, and U is classified into 7 equivalence classes, as shown in Table 1 (i.e. a decision table), where N is the number of records in the corresponding class.

Using Table 1, we also can get the set of condition granules, U/C = {{1,7}, {2,5}, {3,6}, {4}},  and decision granules, U/D = {{1}, {2,3,7}, {4}, {5,6}}, respectively.

In the following we let U/C = {c1, c2, c3, c4} and U/D = {d1, d2, d3, d4}.

Table 1. A decision table  Class Weather Road Time Accident N  1 Misty Icy Day Yes 80  2 Foggy Icy Night Yes 140  3 Misty Not icy Night Yes 40  4 Sunny Icy Day No 500  5 Foggy Icy Night No 20  6 Misty Not icy Night No 200  7 Misty Icy Night Yes 20  3. Pawlak?s Method  Every class in a decision table can be mapped into an association rule (or called decision rule in [6]), e.g., class 2 in Table 1 can be read as ?if the weather is foggy and road is icy then the accident occurred at night? in 140 cases.

For our convenience, we assume A = {a1,?, ak, ak+1, ?, am}, C = {a1, ?,  ak}, and D = {ak+1, ?, am},  where k>0, and m>k. Every class f determines a sequence a1(f), ?,  ak(f), ak+1(f), ?, am(f). The sequence can determine a decision rule:  a1(f), ?,  ak(f) ? ak+1(f), ?, am(f) or in short f(C )? f(D).

The strength of the decision rule f(C ) ? f(D) is defined as |C(f)?D(f)| /  |U|; and the certainty factor of the decision rule is defined as  |C(f)?D(f)| /  |C(f)|.

According to the above definitions, we can use the following algorithm to calculate strengths and certainty factors for all decision rules, where we assume Ni denotes the number of records in class i, and UN denotes the total number of records in U.

Algorithm 1.

1. let UN = 0; 2. for (i = 1 to n )  // n is the number of classes  UN = UN + Ni; 3. for (i = 1 to n)  {strength(i) = Ni/UN; CN = Ni; for (j = 1 to n)  if ((j?i) and (fj(C) == fi(C))) CN = CN + Nj;  certainty_factor(i) = Ni/CN; }.

If we assume the basic operation is the comparison between two classes (i.e., fj(C) == fi(C)), then the time complexity is (n-1) ? n = O(n2), where n is the number of  classes in the decision table. It also needs a similar algorithm to determine interesting rules for Pawlak?s method.

4. Extended Random Sets  Let U/C be the set of condition granules and U/D be the set of decision granules. To describe the relationship between condition granules and decision granules, we can rewrite the decision rules in Table 1 as follows:  c1 ? { (d1, 80/100),  (d2, 20/100) } c2 ? { (d2, 140/160), (d4, 20/160) } c3 ? { (d2 , 40/240)  (d4, 200/240) } c4 ? { (d3, 500/500) }.

These determine a mapping ? from U/C to ]1,0[)/(2 ?DU , such that  )(),(  = ??  snd icsndfst  for all ci?U/C  where ?(ci) is a set of decision-granule numeral pairs.

Now we consider the support degree for each  condition granule. The obvious way is to use the frequency in the decision table, that is,  ? =  icx xi Ncw )(       (1)  for every condition granule ci, where Nx is the number of records in class x. By normalizing, we can get a probability function P on U/C such that  ?  = CUc j  i i  j  cw  cw cP  / )(  )( )(  for all ci?U/C.

Based on the above analysis, we can use a pair (?, P)  to represent what we can obtain from an information table.

We call the pair (?, P) an extended random set.

According to the definitions in the previous section, we can obtain the following decision rules:  and,...,, |)(|,2,1, iciiiiii fstcfstcfstc ????  for a given condition granule ci, where )}(,),,{()( |)(|,|)(|,1,1, ii ciciiii snd,fst...sndfstc ??=?      (2)  We call snd)cP ,...,sndcP iciiii |)(|,1,  ()( ??? the strengths of these decision rules, respectively; and  ,..., )|(|,1, icii sndsnd ? the certainty factors, respectively.

From the above definitions, we have  ||  || , ,  i  jii ji c  fstc snd  ? = .

The above definitions about strengths and certainty factors are the same as Pawlak?s definitions.

5. Determining Interesting Rules  Given an extended random set (?, P), it can provide a new representation for decision rules. Figure 1 shows a such     example for representing the extended random set that is obtained from Table 2, where U/C is the set of condition granules, and ?(ci) is the set of conclusions of premise ci (i = 1, ?, |U/C|). Algorithm 2 shows the process of creating extended random sets, and the process of calculating of strengths and certainty factors of the decision rules.

U/C ?(ci) c1 ? (d1, 0.8) (d2, 0.2) c2 ? (d2, 0.875) (d4, 0.125) c3 ? (d2, 1/6) (d4, 5/6) c4 ? (d3, 1.0)  Figure 1. An extended random set  Algorithm 2.

1. let UN  = 0, U/C = ?; 2. for (i = 1 to n)  UN = UN + Ni; 3. for (i = 1 to n do ) // create the data structure  if (fi(C)? U/C) insert((fi(D), Ni)) to ?(fi(C)); else add(fi(C)) into U/C, and set ?(fi(C))=?;  4. for (i = 1 to |U/C|)  P(ci) = (1/UN ) ? ( snd icsndfst ?? )(),(  );  5. for (i = 1 to |U/C|)  // normalization { temp = 0;  for (j = 1 to |?(ci)|) temp = temp + sndi,j; for (j = 1 to |?(ci)|) sndi,j = sndi,j/temp; }  6. for (i = 1 to |U/C|) // calculate rule strengths for (j = 1 to |?(ci)|) { strength(ci?fsti,j) = P(ci) ? sndi,j; certainty_factor(ci?fsti,j) = sndi,j; }.

Because steps 4, 5, and 6 all traverse pairs in ?(ci) (i = 1, ?, |U/C|), and the number of pairs in all ?(ci) (i = 1, ?, |U/C|) is just n (the number of classes in the decision table), the time complexity of this algorithm is determined by step 3. In step 3, checking fi(C)? U/C takes O(|U/C|), so the time complexity of the algorithm is O(n?|U/C|), where the basic operation is still the comparison between classes. Since |U/C| n, Algorithm 2 is better than Algorithm 1 for the time complexity.

A decision rule jii fstc ,?  is an interesting rule if  )()|( ,, jiiji fstprcfstpr ?  is greater than a suitable constant.

From the definition of mapping ?, we have jiiji sndcfstpr ,, )|( = . To decide the probability on the set  of decision granules, we present the following function:  ]1,0[)/(: ?DUpr   such that  ??? ?=  )(),(),/( )()(  ii csnddCUc i sndcPdpr    (3)  We can prove that pr is a probability function on (U/D).

The algorithm of determining pr is only to traverse the data structure as showed in Figure 1.

Table 2. Probability function on decision granules Decision Granule Description pr  d1 Accident occurred at night 0.08  d2 Accident occurred in daytime 0.20  d3 Accident not occurred in daytime 0.50  d4 Accident not occurred at night 0.22  Table 2 shows the probability function on the set of decision granules. From Figure 1 and Table 2 we can obtain the probability of pr(fsti,j|ci) for every decision rule ci ? fsti,j, where fsti,j ?{d1, d2, d3, d4}. We can get 4 interesting rules (seeTable 3) if we assume that a decision rule jii fstc ,?  is an interesting rule iff  )()|( ,, jiiji fstprcfstpr ?  > 0.

Table 3. Interesting rules Rule Description  pr(fsti,j|ci) pr(fsti,j) Interesting rule  c1 ? d1 0.8 0.08 Yes c2 ? d2 0.875 0.20 Yes c3 ? d2 0.167 0.20 No c4 ? d3 1 0.5 Yes c2 ? d4 0.125 0.22 No c3 ? d4 0.833 0.22 Yes c1 ? d2 0.20 0.20 No  6. Discussions  In this section, we discuss other advantages of our approach except the time complexity.  We first discuss the weight functions for condition granules. We also introduce another uncertain measure on decision granules.

6.1 Weight Functions for Condition Granules  The extended random sets are easily to include other criteria apart from the well-accepted criterion ?frequencies? when determining association rules.

Although the frequency is a well-accepted criterion for data mining, it is not the only criterion for support degree because some condition granules with high frequencies may be meaningless. For example, when we use keywords to represent the meaning of documents, we usually consider both keywords frequency and inverse document frequency (e.g., the popular technique tf*idf in information retrieval) because some words (like ?information?) may have high frequencies in a document but they may appear in most documents in a collection (e.g., ?information? may appears in most documents in the information table collection).

In order to use the above idea, we assume there is a collection which contains many databases. Given a decision table (U, C, D) of a database, we can define a new weight function w on U/C to instead of the weight function in Eq. (1), which satisfies  w(ci)  = ( ? icx  xN ) ? log(M/ni)  for every ci ?(U/C), where M is the total number of databases, and ni is the number of databases which contain the given condition granule ci. It is easy to do so because this definition does not affect the rest calculation of decision rules.

6.2 Uncertain Measures on Decision Granules  The obvious way to measure the uncertainty of a set of decision granules is using a probability function. For a given set of decision granules X = {d1, ? , ds}, we may use  ?Xx xpr )(  to represent the probability of (d1 or ? or  ds). However, this measure is very sensitive to the frequencies of records.

To consider a relative stable measure, we consider a random set (?, P) (see [2] [5]) which is derived from the extended random set (?, P):  DUCU /2/: ??  such that  })(),({)( ii csndfstfstc ??=?  for every ci ?(U/C).

The random set (?, P) determines a Dempster-Shafer  mass function (see [2]) mP on U/D such that )})(),/(({)( XcCUccPXm iiiP =??=   (4)  for every X ?U/D.

This mass function can decide a belief function and  plausibility function (see [2]) as well. They are defined as follows:  ]1,0[2: / ?DUmbel , ]1,0[2: / ?DUmpl  and  ? =  XY Pm YmXbel )()( ,  ??? =  XY Pm YmXpl )()(     (5)  for every X ? U/D.

We can prove that  belm(X) ? ?Xx  xpr )( ? plm(X) for every X ? U/D.

Domain experts can use the interval [belm, plm] to check if their ?subjective? judgments for some descriptions are correct.  Table 4 shows the uncertainty measures for some descriptions.

Table 4. Uncertain measures  Description  Subset  Pr mP [belm, plm]  ?d1 or d2? {d1, d2} 0.28 0.1 [0.1, 0.5]  ?d3 or d4? {d3, d4} 0.72 0.0 [0.5, 0.9]  ?d2 or d3? {d2, d3} 0.70 0.0 [0.5, 1.0]  ?d1 or d4? {d1, d4} 0.30 0.0 [0.0, 0.5]  7. Conclusions  This paper uses granular computing to interpret association rules. The main contribution of this paper is that the concept of extended random sets is used to describe the relationships between condition granules and decision granules. It presents a new efficient algorithm to find interesting rules in databases. Apart from the ?frequencies?, the extended random sets are easily to include other criteria when determining association rules.

The extended random sets also provide more than one measure for dealing with uncertainties in the association rules significantly.

