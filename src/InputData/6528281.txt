Greening Data Center Networks with Flow Preemption and Energy-aware Routing

Abstract?Data centers are becoming a big consumer of the world?s electricity because of the explosive growth of their scale, which has become a serious concern. In recent years, many advanced data center network architectures have been proposed, which employ richly-connected topologies and multi- path routing to achieve high bisection bandwidth. Unfortunately, they undergo inefficient network energy usage. To address this issue, many network-wide energy conservation techniques have been proposed in the community.

In the paper, we explore a new dimension in saving data center network energy, i.e., letting flows exclusively occupy link resources when transmission and using a flow preemption algorithm to schedule them. The key insight behind this design choice is that exclusively occupying link resources usually results in higher link utilization because there is no collision among the traffic flows. By combining both flow preemption and energy- aware routing, we can save much more network power than with traditional ways.



I. INTRODUCTION  Cloud computing inaugurates a new era of ?computing as a utility?. Many well-known IT companies have built their own data centers recently and provided various distributed computation and storage services as well as cloud-based applications, such as Amazon?s EC2 [1], Microsoft?s Azure Platform [2] and Google?s GFS [3]. Data centers are becoming a big consumer of the world?s electricity because of the explosive growth of their scale. According to the statistics, 1.5% of the global electricity usage came from data centers in 2011 [4] and the percentage will dramatically increase to 8% by 2020. [5]. The skyrocketing energy consumption has caused serious economic and environmental concerns, and has even become an impediment to the sustainable expansion of cloud services.

The energy consumption in data centers includes three major parts: networking, servers and cooling. The networking part currently accounts for 10-20% of the overall power con- sumption of the data centers [6]. As energy-efficient hardware and scheduling techniques have been applied to servers and many efforts have been made to improve PUE (power usage effectiveness) of the data centers, the proportion of network power consumption out of the whole data center will even grow [7].

In recent years, a bunch of ?richly-connected? data center network architectures have been proposed, including Fat- Tree [8], BCube [9], DCell [10], etc. These architectures employ a large number of network devices and links to interconnect servers, and enjoy high bisection bandwidth and  network reliability. However, as the power consumption of the current network devices and components is not proportional to their traffic loads [11], many idle and under-utilized network devices waste a great deal of energy when traffic loads are light, which incurs inefficient energy usage in these architec- tures.

Many network-wide energy conservation schemes have been proposed recently to improve the network energy usage effi- ciency in data centers. They usually focus on energy-aware routing and topology designs, but neglect how to schedule traffic flows in a more energy-efficient manner. The traffic flows in these schemes are scheduled as soon as they arrive and share the link bandwidth by TCP based competition in the network. But we find that both the path selections and scheduling strategies of traffic flows affect the overall network energy used for transmitting the flows. Therefore, it is meaningful to study how to combine flow scheduling strategies with energy-aware routing to make them work cooperatively, so as to achieve more effective network energy conservation.

In this paper, we propose a joint scheme involving both preemptive flow scheduling and energy-aware routing. The basic idea is to preemptively schedule traffic flows and enable the traffic flows to exclusively occupy the bandwidth of links on their routing paths. We say two network flows are colliding with each other when there is at least one same link on their routing paths. With the preemptive flow scheduling, a flow with higher priority will be preferentially scheduled, and all the other flows with lower priorities and in collision with the flow will be paused until the flow finishes. The reason behind flow preemption is that it can make the links fully utilized and thus is more energy-efficient.

We conduct simulations to evaluate our scheme in the Fat-Tree topology, by comparing it with bandwidth sharing schemes and ECMP routing. The results demonstrate that our scheme can effectively save network energy in data centers.

A side effect is that we find our scheme can also reduce the average completion time of the flows.

The remainder of the paper is organized as follows. Section II discusses the background and related work. Section III describes our scheme combining both flow preemption and energy-aware routing. Section IV evaluates the scheme by simulations. Finally, we conclude the paper in Section V.



II. BACKGROUND AND RELATED WORK  A. Energy Consumption Model of Data Center Networks  In this subsection, we discuss the power consumption char- acteristics of typical modular network devices used in data centers, and present the energy consumption calculation model of data center networks.

The measurement results in [11] show that the current network devices and components are not energy proportional to their loads. The power consumption of a switch mainly relies on its hardware configurations, such as the type of chassis and the number of switch ports, regardless of the network traffic variation.

Based on the results, we use Eq. (1) to calculate the total amount of network energy used for transmitting a group of traffic flows in a data center. I and J (i) denote the set of switches in the data center and the set of ports in switch i respectively. Pi denotes the fixed power consumption in switch i, including chassis, fans, switching fabric, etc., and ti denotes switch i?s active working duration of transmitting the traffic flows. Qi,j and t?i,j denote the power and the working duration of port j in switch i respectively. Here, we assume that switches used in data center networks follow the IEEE Energy Efficient Ethernet standard [12]. That is to say, switches and their Ethernet ports can be put into the sleep mode and consume no power when they are idle.

As the advanced data center topologies, such as Fat-Tree [8], usually employ homogeneous switches and links to intercon- nect servers, we assume the fixed power P in each switch is the same and all the switch ports have the same power Q. As such, we simplify the network energy consumption model into Eq. (2).

E = ?  i?I (Pi ? ti +  ?  j?J (i) Qi,j ? t?i,j) (1)  E = P ? ?  i?I ti +Q ?  ?  i?I  ?  j?J (i) t?i,j (2)  B. Energy-saving Techniques in Data Center Networks  There are plenty of researches on network energy conser- vation techniques, which have been summarized by survey papers [13] and [14]. In this subsection, we focus on two kinds of typical energy conservation schemes used in data centers, namely device-level and network-wide energy conservation techniques.

1) Device-level techniques: The device-level energy con- servation schemes aim to improve the energy efficiency of in- dividual network devices with advanced hardware techniques.

Nedevschi et al. [15] claimed that the energy consumption of network devices could be effectively improved by applying the sleep-on-idle and the rate-adaptation technique to them.

Gupta et al. [16] studied the time to put idle and under- utilized Ethernet interfaces to the low-power mode, based on the number of arriving packets in a given period of time, and analyzed the tradeoff between energy conservation and packet loss and delay. Similarly, Gunaratne et al. [17] designed the  policies of link rate adaption by monitoring the link utilization and the queue length of output buffers to effectively save the Ethernet link energy. The authors in [18] designed shadow ports to buffer ingress packets, and used the schemes of time window prediction and power save mode to reduce the energy consumption of switch ports.

2) Network-wide techniques: The network-wide energy conservation schemes study how to reduce the gross network energy consumption from the topology design and the net- work routing perspective. Chabarek et al. [19] formulated the problem of minimizing network energy consumption as a multiple commodity network-flow problem, and investigated the energy-efficient network and protocol designs with mixed- integer program techniques. Vasic et al. [20] proposed a two-step flow scheduling scheme to balance optimal energy conservation and computational complexity. In the scheme, a group of energy-critical paths were pre-computed and a scalable traffic engineering mechanism was then used to schedule network flows online. Abts et al. [7] proposed an energy-efficient flattened butterfly topology and reconfigured the transfer rates of links periodically to make data center networks more energy proportional. Heller et al. [21] designed a network energy optimizer, namely, ElasticTree, which stud- ied how to choose a subset of network resources to transmit traffic flows under the restrictions of network performance and reliability. Furthermore, Shang et al. [22] presented a throughput-guaranteed power-aware routing algorithm, which used a pruning-based method to eliminate switches from the original topology iteratively while meeting the network throughput requirement.



III. DESIGN  In this section, we first take some motivating examples and then propose a joint scheme of flow preemption and energy- aware routing.

A. Motivating Examples  We take some motivating examples to reveal what effects different route selections and flow scheduling orders have on the network energy consumption and flow completion time.

Fig. 1 shows the examples in a partial 4-ary Fat-Tree topology, which contains 12 switches marked with ?S? and 8 servers marked with ?H?. We assume all switches in the topology have the same power h and all links have the same capacity e. There are two flows: flow1 ?H1?H5? and flow2 ?H3?H7? in the network, and their sizes are m1 and m2 respectively (m1 ? m2). In the topology, between any two inter-pod servers there are four available paths, traversing S1, S2, S3 and S4 respectively. We assume flow1 and flow2 arrive at time t=0 simultaneously, and flow1 is assigned to the path across the switch S1 and the path is marked with red circles in both Fig. 1(a) and (b). We select three flow scheduling solutions for flow2 as follows.

Solution1 (Bandwidth sharing + ECMP routing): choose the path (marked with yellow squares in Fig. 1(a)) across the    H1 H2 H3 H4 H5 H6 H7 H8  ???????????? ???????????? ????????????  ??????????? ??????????? ???????????  ???????????? ???????????? ????????????  S1 S2 S3 S4  S5 S6 S7 S8  S9 S10 S11 S12  ??????????? ??????????? ???????????  H1 H2 H3 H4 H5 H6 H7 H8  ???????????? ???????????? ????????????  ???????????? ???????????? ????????????  ???????????? ???????????? ????????????  S1 S2 S3 S4  S5 S6 S7 S8  S9 S10 S11 S12  (a) Solution1 and Solution2 (b) Solution3  Fig. 1. Route selections of three solutions in a partial 4-ary Fat-Tree topology.

??????????????????????????????????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????????????????????????????????? ???????????????????????????????????????????????????????????????????????????????????????????????????????????????  f1 time  bandwidth  ??????????????????????????????????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????????????????????????????????? ???????????????????????????????????????????????????????????????????????????????????????????????????????????????f2  ?????????????????????????????????????? ?????????????????????????????????????? ?????????????????????????????????????? ?????????????????????????????????????? ?????????????????????????????????????? ?????????????????????????????????????? ?????????????????????????????????????? ?????????????????????????????????????? ?????????????????????????????????????? ??????????????????????????????????????  f1  e  0 2m2/e (m1+m2)/e  time  bandwidth  ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ????????????????????????????????????????????????????????  f2  ??????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????? ???????????????????????????????????????????????????????????????????????????????????  f1  e  0 m2/e (m1+m2)/e  time  bandwidth  ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ???????????????????????????????????????????????????????? ????????????????????????????????????????????????????????  f2  ??????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????? ??????????????????????????????????????????????????????????????????????????????????? ???????????????????????????????????????????????????????????????????????????????????  f1  0 m2/e m1/e  e  2e  e/2  (a) Solution1 (b) Solution2 (c) Solution3  Fig. 2. Flow scheduling orders and flow completion time in three solutions.

switch S1 for flow2, and send flow1 and flow2 concurrently at time t=0;  Solution2 (Flow preemption + ECMP routing): choose the same path with Solution1 for flow2, and send the flow at time t=0 by preempting flow1. Flow1 is paused until flow2 finishes at time t = m2e ;  Solution3 (Flow preemption + Energy-aware routing): choose the path (marked with blue triangle in Fig. 1(b)) across the switch S2 for flow2, and send flow1 and flow2 concurrently at time t=0.

TABLE I FLOW COMPLETION TIME AND NETWORK ENERGY CONSUMPTION IN  THREE SOLUTIONS  Solution1 Solution2 Solution3 flow1 completion time m1+m2  e m1+m2  e m1 e  flow2 completion time 2m2 e  m2 e  m2 e  network overall energy h(5m1+9m2) e  5h(m1+m2) e  h(5m1+3m2) e  We show the scheduling processes of the three solutions in detail in Fig. 2(a), (b), (c) respectively. In Solution1, from t= 0 to 2m2e , flow1 and flow2 are colliding with each other on the bottleneck links, S1-S5 and S1-S7, and thus the transfer rate for both is e2 . When flow2 finishes, the remaining size m1?m2 of flow1 is sent at the rate e. In this solution, the two flows are transmitted by seven switches, among which the working duration of switches S1, S5, S7, S9, S11 is m1+m2e and that of switches S10 and S12 is 2m2e . Therefore, the overall network energy used to transmit flow1 and flow2 is h(5m1+9m2)e .

In Solution2, flow2 is preemptively scheduled before flow1, which enables the two flows to exclusively occupy the link bandwidth e. An important observation result is that the  network energy consumption in Solution2 is less than that in Solution1, although they employ the same number of switches.

This is because when using the preemptive flow scheduling, the utilization ratios of ports which are used for transmitting the two flows in switches S9, S10, S11 and S12 increase to 100%, and thus the energy usage efficiency of these switch ports are enormously improved. We get an inspiration that the network energy consumption can be usually reduced by preemptively scheduling traffic flows which are colliding with each other, to let them exclusively occupy the link bandwidth.

In Solution3, there is no shared link on the paths of flow1 and flow2. Both flows can exclusively use the link bandwidth when being scheduled concurrently. Therefore, they enjoy the shortest flow completion time. The network energy consump- tion in this solution is less than that in Solution2, because the switches S5 and S7 are shared by the two concurrent flows in Solution3, but there is no shared switch for the two flows scheduled orderly in Solution2.

B. A Joint Scheme of Flow Preemption and Energy-aware Routing  In this subsection, we propose a joint scheme of Flow Preemption and Energy-Aware Routing (FP&EAR) to achieve network energy conservation. The essence of our idea is to preemptively schedule traffic flows according to their priorities and to make the flows exclusively occupy the link bandwidth on their routing paths. Energy-aware routing is employed to assign the paths sharing switches sufficiently to the traffic flows which are scheduled concurrently in the network.

The reason behind the idea is that preemptively scheduling traffic flows can make the switch ports which are used to trans- mit the flows work at the full utilization ratio. For example, in the Fig. 1(a), the utilization ratio of the used ports on switches S9, S10, S11 and S12 is only 50% when using Solution1. But the ratio can be increased to 100% when using the preemptive flow scheduling in Solution2. As the current switches and their ports are not energy proportional to the traffic loads, the switch ports with higher utilization ratios consume lower energy to transmit unit data, and enjoy more efficient energy usage in data transmission. Besides, the energy-aware routing assignment to traffic flows can also improve the energy usage efficiency of the switches. Therefore, we apply the preemptive flow scheduling strategy to our scheme and make it work with energy-aware routing to save network energy.

We present the basic process of preemptively scheduling a flow f in the network as follows. If the flow has the available path which is not colliding with all the flows running in the network, it is scheduled immediately with the path; Otherwise, we check whether the flow has the path on which the priority of the flow f is higher than that of all the flows that traverse any of the links. If any, the flow f will be preemptively scheduled on the path, and the flow with lower priority and in collision with the flow f are paused until the flow f finishes.

If there is no such a path, the flow f is paused.

Our FP&EAR scheme consists of four modules: Flow Management (FM), Priority Comparison (PC), Scheduling    Scheduling  Computation  Priority  Comparison  Flow Management  ?????????  Network  Configuration  Data Center Network  topology and traffic  information  routing path and  flow status  configuration  information  topology and traffic  information  Fig. 3. Relationship among four modules in the FP&EAR scheme  Computation (SC) and Network Configuration (NC). Their relationship is shown in Fig. 3.

The role of the flow management module is to collect the information of network topology and traffic, including the connection relationships between switches and servers, the source and the destination server of the traffic flows, as well as flow sizes, etc. Also, the module maintains two flow sets, Fsend and Fwait. Fsend records the flows which are running in a network, and Fwait records the flows which have arrived but wait to be scheduled in the network. The flows in Fwait are sorted in a descending order of their priorities.

The priority comparison module is to compare the priorities of traffic flows according to some predefined metrics. In the FM module, it is used to sort the flows in Fwait and in the SC module it is for the preemptive flow scheduling. In our scheme, we take the flow size as a priority comparison metric.

A flow with a smaller size has a higher priority. Data center operators can also take other metrics, such as flow deadline, in accordance with their actual demands.

The scheduling computation module is responsible for cal- culating the routing paths and scheduling status for traffic flows. The module is executed when a new flow arrives at the network or a flow in Fsend finishes. In the former situation, the FM module collects the information of the newly arriving flow and outputs it to the SC module; in the latter, the FM module outputs every flow in Fwait in turn to the SC module, and the output flows are decided by the SC module whether to schedule in the network.

We define two types of paths for traffic flows: idle path and preemptive path. An idle path refers to the path on which all links are never occupied by the flows running in the network; a flow having a preemptive path means the priority of the flow is higher than that of all the flows traversing any link on the path.

The computation process of energy-aware routing and flow scheduling status in the SC module is shown in Alg. 1. As for an input flow f , the module first calculates multiple available routing paths in the topology G (Line 1), which is straight- forward in the ?richly-connected? data center networks by developing topological characteristics. Afterwards, the module uses the function SelectIdlePath(.) to choose an idle path for  Algorithm 1: Calculating the energy-aware routing path and scheduling status for a flow  Input: G: data center topology f : a newly arriving flow or a waiting flow in Fwait Output: {< fpath, fstate >}: routing path and state of the flow f  1 A ? GetAvailPathSet(G,f ); 2 B ? SelectIdlePath(A,Fsend); 3 if (|B| ? 1) then 4 b ? SelectLPPath(B); 5 Put the flow f into Fsend; 6 return < b, SEND>; 7 else 8 C ? SelectPreemptPath(A,Fsend); 9 if (|C| ? 1) then  10 c ? SelectLPPath(C); 11 Put the flow f into Fsend; 12 D ? PreemptedFlowSet(G,Fsend,f ,c); 13 for each flow d ? D do 14 dpath ? -1; dstate ? WAIT; 15 Move flow d from Fsend to Fwait; 16 end 17 return < c, SEND>; 18 else 19 Put the flow f into Fwait; 20 return <-1, WAIT>; 21 end 22 end  flow f from multiple available paths (Line2). If there is no such a path, it will calculate the preemptive path for flow f and then schedule it on the path (Line 8). Meanwhile, it pauses the flows with lower priority and in collision with flow f , and moves them from Fsend to Fwait (Lines 12-16). The reason behind is that the preemptive scheduling strategy makes traffic flows exclusively occupy the link bandwidth, so as to improve the utilization ratios of switch ports and achieve more efficient energy usage. If flow f has no idle or preemptive path, the module pauses the flow and puts it into Fwait (Lines 19-20).

In each step above, if there are multiple eligible paths, the module chooses the path which traverses the minimum number of idle switches (Lines 4 and 10). The purpose is to increase the utilization ratios of the switches and improve network consumption, since the flows can share switches sufficiently when being transmitted.

The network configuration module is to configure the rout- ing and flow scheduling decision in the data center network according to the computation results from the SC module.



IV. EVALUATION  In this section, we evaluate our scheme with simulations in Fat-Tree, which is a typical multi-rooted tree topology used for data center networks. We show what effects our scheme    0 1 2 3 4 5 6 7 8         Average flow arrival interval (ms)  N et  w or  k en  er gy  c on  su m  pt io  n( K  J)  BS+ECMP FP+ECMP FP+EAR  Fig. 4. Network overall energy used for transmitting traffic flows under different flow arrival intervals  0 1 2 3 4 5 6 7 8        Average flow arrival interval (ms)  A ve  ra ge  fl ow  c om  pl et  io n  tim e  (m s) BS+ECMP  FP+ECMP FP+EAR  Fig. 5. Average flow completion time under different flow arrival intervals  has on improving network energy consumption and average flow completion time.

A. Simulation Setup  In our simulations, we use the Fat-Tree topology composed of 24 pods, and set the total amount of traffic flows as 10000.

The traffic flow size is drawn from an exponential distribution around 64MB, which is the typical size of a data chunk in MapReduce computations [23]. For switch power, we take a common data center switch, i.e., Cisco Nexus 2224TP switch as an example: the fixed switch power is 48W and the power of a switch port is 2W [24]. The network energy consumption is calculated with Eq.(2) in Section II.A.

ECMP routing [25] is commonly used in ?richly-connected? data center network topologies to achieve load balance and high network performance. In our evaluations, we compare our scheme (denoted by FP+EAR) with two ECMP-based flow scheduling schemes. One is a Bandwidth Sharing plus ECMP scheduling scheme (denoted by BS+ECMP), which employs the ECMP routing and schedules traffic flows as soon as they arrive. The traffic flows running in the network share the link bandwidth by TCP based competition in the scheme. The other is a Flow Preemption plus ECMP scheduling scheme (denoted by FP+ECMP). It also employs the ECMP routing, but preemptively schedules traffic flows like the FP+EAR  scheme, aiming to let every flow exclusively occupy the link bandwidth on its routing path when scheduled.

B. Impact of Flow Arrival Interval  In this subsection, we evaluate the three flow scheduling schemes in terms of network energy consumption and average flow completion time when in different flow arrival intervals.

We randomly choose the source and the destination server to generate traffic flows, and vary the traffic flows? average arrival intervals, which is exponentially distributed to simulate different traffic loads. Fig. 4 and Fig. 5 show the simulation results.

In Fig. 4, we find that the network energy consumption increases in the three schemes when the average flow ar- rival interval varies within 0.2ms to 8ms. This is because shorter flow arrival intervals provide more opportunities for concurrent traffic flows to share switches, and thus enjoy more efficient network energy usage. One important observation result is that our scheme can significantly save network energy.

For example, when the average flow arrival interval is 3ms, our scheme can save 40% energy used in BS+ECMP and 30% energy used in FP+ECMP respectively.

Fig. 5 shows the average flow completion time of the three schemes in different flow arrival intervals. The results indicate that the average completion time of traffic flows declines with the increase of flow arrival intervals. It follows our intuition, because when the flow arrival interval is longer, traffic flows have lower collision probability, and thus can be sent faster.

In addition, we find that our scheme can effectively improve the average flow completion time, especially in short flow arrival intervals. For example, when the flow arrival interval is 0.2ms, our scheme can reduce almost 50% of the average flow completion time in the two ECMP-based schemes.

C. Impact of Traffic Flow Distribution  Next we investigate the effect of traffic flow distribution on network energy consumption and flow completion time in the three schemes. We vary the percentage of used servers to simulate different traffic flow distributions, and define the percentage as the number of servers used to originate traffic flows over the total amount of servers in the data center. The definition of ?r% used servers? is that only r% of the servers in the data center are randomly chosen to send or receive traffic flows. We set the average flow arrival interval as 2ms and show the simulation results in Fig. 6 and Fig. 7.

Fig. 6 shows that the network energy consumption decreases along with the increase of the percentage of used servers. This is because when the percentage of used servers is higher, the traffic flows can be more uniformly distributed onto data center servers and more sufficiently share switches. As expected, our scheme can reduce a large amount of network energy consumption, especially when the percentage is low. When the percentage is 10%, our scheme can save 65% of the network energy consumed in BS+ECMP. Another observation result is that the network energy consumption in BS+ECMP is quite high when the percentage of used servers is low. It    10 20 30 40 50 60 70 80 90 100           Percentage of servers used (%)  N et  w or  k en  er gy  c on  su m  pt io  n( K  J)  BS+ECMP FP+ECMP FP+EAR  Fig. 6. Network overall energy used for transmitting traffic flows under different flow distributions  10 20 30 40 50 60 70 80 90 100       Percentage of servers used (%)  A ve  ra ge  fl ow  c om  pl et  io n  tim e  (m s) BS+ECMP  FP+ECMP FP+EAR  Fig. 7. Average flow completion time under different flow distributions  reveals the fact that intensive distribution of network flows in BS+ECMP will intensify the network bandwidth competition and seriously prolong the active working duration of switches.

Fig. 7 shows the relationship between the average flow completion time and traffic flow distribution. Similarly, the average flow completion time in the three schemes decreases as the percentage of used servers increases. Also, the results reveal that our scheme can effectively improve the average flow completion time in different traffic flow distributions.



V. CONCLUSION  In this paper, we study how to save data center network energy with techniques of network routing and flow schedul- ing. We first take some motivating examples and then propose a joint scheme of flow preemption and energy-aware routing.

The essence of the scheme is to preemptively schedule traffic flows and make them exclusively occupy the link bandwidth on the routing paths. Simulation results show that our scheme can effectively save network energy and improve average flow completion time, compared with two ECMP-based scheduling schemes.

Our work discussed in the paper is still preliminary, but re- veals that the preemptive flow scheduling strategy and energy- aware routing can be combined effectively to improve data center network energy. In the future, we will further study how to apply our scheme to constraint-based traffic flows,  such as deadline-aware flows. We would also like to design a distributed energy-aware flow scheduling scheme and consider the details of its implementation and deployment in data center networks.

