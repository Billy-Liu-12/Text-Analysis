The Strategy of Mining Association Rule Based on  Cloud Computing

Abstract?Cloud computing provides cheap and efficient solutions of storing and analyzing mass data. It is very important to research the data mining strategy based on cloud computing from the theoretical view and practical view. In this paper, the strategy of mining association rules in cloud computing environment is focused on. Firstly, cloud computing, Hadoop, MapReduce programming model, Apriori algorithm and parallel association rule mining algorithm are introduced. Then, a parallel association rule mining strategy adapting to the cloud computing environment is designed. It includes data set division method, data set allocation method, improved Apriori algorithm, and the implementation procedure of the improved Apriori algorithm on MapReduce. Finally, the Hadoop platform is built and the experiment for testing performance of the strategy as well as the improved algorithm has been done. The results show that the strategy designed in this paper can archive higher efficiency when doing frequent itemset mining in cloud computing environment.

Keywords-Association Rule Mining; Cloud Computing; Parallel; Apriori; MapReduce

I.  INTRODUCTION Cloud Computing [1,2] is a new business model. It  distributes the computing tasks to the resource pool constituted of a large number of computers, so that a variety of application systems can obtain computing power, storage space and a variety of software services on demand. The novelty of the Cloud Computing is that it almost provides unlimited cheap storage and computing power. This provides a platform for the storage and mining of mass data.

Based on parallel computing conditions and parallel mining demand under the cloud computing environment, we study the data set partitioning, data set allocation, and association rule mining algorithms. We Improve Apriori algorithm in order to combine it with the MapReduce programming model of cloud computing and mine the association rules from the mass data in parallel.

Hadoop[3,4] is an open source software of Apache. It composes of the storage modules HDFS, computing modules MapReduce, HBase, Hive, Pig, ZooKeeper, Cascading and other modules. They work together to solve the problems of storing TB/PB-level data, and the efficient, reliable, scalable computing on such a large amount of data.



II. DESIGN OF PARALLEL ASSOCIATION RULE MINING STRATEGY  Association rule mining is an important research topic of data mining; its task is to find all subsets of items which frequently occur, and the relationship between them.

Association rule mining has two main steps: the establishment of frequent itemsets and the establishment of association rules.

Apriori algorithm [5] is the most classic and most widely used algorithm for mining frequent itemsets which generate Boolean association rules. The algorithm uses an iterative method called layer search to generate (k + l) itemsets from the k itemsets.

With the dramatic increase of data amount and the continuous maturity of parallel computing technology, some parallel association rules mining algorithm has been proposed [6-8]. The Count Distribution algorithm is the most direct parallelization of the Apriori algorithm. In this algorithm, the global candidate itemsets and frequent itemsets are stored in each node; in each calculation step it gets the support counts of candidate itemsets for the local data using the Apriori algorithm; at the end of each scanning, it exchanges local support counts with all other nodes to generate global support counts. The Data Distribution algorithm partitions the candidate itemsets of frequent itemsets over each node based on the partition of database; each node calculates its own support of candidate itemsets. The Candidate Distribution algorithm combines the ideas of CD algorithm and DD algorithm; it partitions the data sets and the candidate item sets reasonably at the same time, so that each node can work independently.

We partially references the ideas of the above algorithms, using parallel computing capabilities provided by cloud computing, design a strategy of parallel association rule mining for cloud computing environment. It includes data set division method, data set allocation method, improved Apriori algorithm, and the implementation procedure of the improved Apriori algorithm on MapReduce.

A. Improvement of Apriori Algorithm Apriori algorithm finds all the frequent itemsets by  scanning the database time after time, and it will consume a lot of time and memory space when scanning the database with   DOI 10.1109/BCGIn.2011.125       mass data, which will become the bottleneck of Apriori algorithm. So we improved Apriori algorithm to realize the parallelization, and the specific ideas are as follows:  (a) According to certain principles, the transaction database is horizontally divided into n data subsets which are sent to m nodes.

(b) Each node scans its data subset to generate the set of candidate itemset pC  (candidate 1-itemset to candidate k- itemset); the support count of each candidate itemset is set 1.

(c) The candidate itemset pC is divided into r different partitions, which are sent to r nodes with their support count.

(d) r nodes respectively accumulate the support count of the same itemset to produce the final practical support, and determine the set of frequent itemset pL  in the partition after comparing with the minimum support count min_sup.

(e) Merge the output of the r nodes to generate the set of global frequent itemset L .

The advantage of the improved Apriori algorithm is that it can find all the frequent itemsets by scanning the transaction database once, so the time complexity is significantly lowered.

B. Combination of the Improved Aprior algorithm and MapReduce Cloud computing uses MapReduce [9] programming  model. MapReduce is the emerging parallel programming system invented by Google. It puts parallelization, fault tolerance, data distribution, load balancing into a library, and the system sums up all operations on the data to Map stage and Reduce stage. To submit handling procedures of all operations to the MapReduce programmer only need to define Map function and Reduce function. Based on the size of the input data and the job configuration information, MapReduce system can automatically initialize the job to multiple same Map Tasks and Reduce tasks, and they respectively read different input data blocks and order the Map function and Reduce function to process.

Figure 1.  Aarchitecture of MapReduce  Fig. 1 shows the architecture of MapReduce, which mainly consists of three modules. Client submits parallel processing job written by the user to the Master node; the master node automatically decomposes user's job into Map tasks and Reduce tasks, and assigns the task to the worker; Worker requests to the Master for tasks, while distributed file system  consisting of multiple Workers stores input/output data of MapReduce.

We implement the improved Apriori algorithm based on MapReduce programming model of cloud computing environment as follow steps:  (a) Partitioning and distributing data  MapReduce library horizontally divides transaction database into n  data subsets which are sent to m  nodes executing Map tasks.

(b) Formatting the data subset  Format n  data subsets as <key1,value1>, which is <Tid,list>, Tid is transaction identifier; list is the values corresponding to the transaction in the transaction database.

(c) Executing Map task  The task of Map function is scanning each record <Tid, list> of the input data subset and generating a set of local candidate itemsets as pC ; Support count of each candidate itemset is set 1. Map function generates and outputs intermediate <key2, value2>, defined as <itemset, 1>, itemset is the candidate in pC . The following is pseudo code of the map function:  map(Tid list )  //Tid transaction identifier  //list values corresponding to the transaction  for each itemset in list:  EmitIntermediate(itemset,1);  (d) Operating Combiner function  When the transaction database is large, and the transaction records of each divided data subset are similar, Intermediate duplicate key2 values will account for a large proportion. For example, each Map function generates thousands of such records < {I1}, 1>. All these records will be sent to the specified Reduce function through the network. Undoubtedly that will consume valuable network resources, increase the delay and reduce the I/O performance. Therefore, add an optional Combiner function to the machine which implements Map tasks. Combiner function firstly combines the output of the Map function in the local and output <itemset, sup>, sup denotes the support count of itemset in data subset; then use the partition function as in (1) to divide the intermediate pairs generated by Combiner function into R different partitions, Each partition will be assigned to a specified Reduce function.

Rm  mmmmh Rkeyhash  k  j j  jk  k  mod)10(  ),,,( mod)(    ? =  ?=  = ? (1)  Parameter km  is the ordinal number of items of candidates k  in the transaction database, according to the assumption of  Master  Worker  Worker  Worker  User program  Worker  WorkerSplit0  Output0  Output1  Submit job  Intermediate result  Map   ReduceInput file  Output file  Split1  Split2       Apriori, items of transaction or itemsets are sorted by the order of the dictionary. Following is the pseudo code of combiner function:  Combiner (itemset, list (1 ))  int sup=0;  for each 1 in list:  sup+ =1;  EmitIntermediate(itemset, sup);  (e) Executing Reduce task  The nodes assigned Reduce tasks read <itemset, sup> submitted by Combiner function. As many different candidates are mapped to the same Reduce function, the key-itemsets are sorted to make the data with the same candidates aggregate together to form <itemset, list(sup)>. After traversing intermediate data, the worker nodes submit <itemset, list(sup)> to the Reduce function, and Reduce function adds up the support count of the same candidates and gets the actual support count of the candidates in the whole transaction database; then compare it with the minimum support count min_sup and determine the set of frequent itemsets in the partition, denoted by pL . The following is the pseudo-code of reduce function:  Reduce (itemset, list(sup))  // list(sup):  list of support counts  int result=0;  for each sup in list:  result+=ParseInt(sup);  Emit (itemset, result);  (f) Merging the output of the Reduce function  Merge the output of the Reduce to get the final set of frequent itemsets L . Thus, all the frequent itemsets have been found.

C. Partition and Distribution of Data Set In order to exert the advantage of cloud computing  adequately, it is needed to partition and distribute data reasonably.

The cluster system under the cloud computing environment emphasizes high scalability and capability of parallel processing. Data partition and distribution impact load balancing and communication granularity. There are two methods of data distribution:  (a)Static data distribution  The process of data distribution is set at the stage of the program design. While executing the program, it distributes the data to nodes doing calculation using the prearranged distribution method. The static data distribution can deal with the problem of load balancing effectively under the light network load and machine load environment.

(b) Dynamic data distribution  The process of data distribution is dynamic in the process of calculation. The master node doesn't distribute all the data once. It distributes the rest data when certain worker node completes the calculation and transmits the result to the master node until all the data is processed.

In this paper, we propose a method of data set distribution as follows:  The MapReduce library of Google divides the input file to N  data segments. The size of each data segment is between 16MB and 64MB. Hadoop sets the size of data segment as 64MB. We chooses 16MB(a quarter of data segment of Hadoop) as the size of data segment and use 16MB as the distribute unit to obtain better load balance.

Dataset D is divided into n  data subsets, if the number of nodes executing the Map task is m , there mn 4? .

? =  = n  i iDD   And ?=?? nDDD ?21 , in addition to the  last data subset nD , the rest are all 16MB. The number of distribution data set of node j  can be expressed as  MBDjj 16?? ??                                  (2)  j? expresses the ratio of distribution data set of node j :   W W j  j =?                                            (3)  W  expresses the overall capability weight of the node executing the Map task in the cluster system:  ? =  = m  j jWW   (4)  As integrated computing capabilities of a single-node in cluster are related to the three resource configuration: the CPU speed, available memory size and disk I/O speed, the integrated computing capability weight of node j  is defined as:  IO j  RAM j  CPU jj WWWW ++=                    (5)  In which, the CPU weight of  node j  is VVW j CPU j = ,  memory weight is MMW j RAM j = , I/O weight is  BBW j IO j = .  The ratio of these three resources of the node  and the corresponding resources of reference points in the cluster reflects the relative performance difference of heterogeneous nodes. The integrated computing capability weight of node j  is also simply expressed as  ( )jjjj IORAMCPUW ,,?=  .

Various computing resources of each node have different effects on different types of loads, so that the integrated weight is related to load type. The weighted load capacity of node is defined as:  IO jj  RAM jj  CPU jjj WWWW ?+?+?=  321 ???    (6)  In which, the parameter kj?  represents dynamic ratio of utilization of resource k (CPU, memory, I/O) of node j  by the load, and 1321 =++ jjj ??? .



III.  EXPERIMENT AND ANALYSIS In order to test the performance of the strategy designed by  us, experiment has done on the Hadoop platform. In the experiment, IBM's Quest Synthetic Data Generator [10] is chosen to generate the experimental data for association rules mining. The total number of transactions is 100000, and each transaction contains 20 items on average, the average length of frequent itemsets is 5.

In the experiment, Ubuntu + eclipse + hadoop are used as software environment, 10 PCs with not exactly the same configuration are used as hardware resources.

Fig. 2 shows the time effect of the improved algorithm and the data set distribution method.

Figure 2.  Experimental results  The curves reflect the changes of the time complexity of the improved algorithm when using and not using the data set distribution method proposed in this paper. Apparently, under the cloud computing environment, the improved algorithm has better performance on mining frequent itemsets from the mass data, and the data set distribution method can improve the efficiency of the improved algorithm.



IV. CONCLUSION In this paper, we have studied parallel association rule  mining strategy in the cloud computing environment; proposed the data set partition method, data set distribution method, and improved Apriori algorithm; implemented the improved algorithm based on MapReduce programming model on the  Hadoop platform. The results show that: under the cloud computing environment, the improved algorithm can effectively mine the frequent itemsets from mass data, and the dataset partition method and distribution method can improve the efficiency of the improved algorithm in the heterogeneous cluster environment.

