Big Data Integration

Abstract?The Big Data era is upon us: data is being generated, collected and analyzed at an unprecedented scale, and data- driven decision making is sweeping through all aspects of society.

Since the value of data explodes when it can be linked and fused with other data, addressing the big data integration (BDI) challenge is critical to realizing the promise of Big Data.

BDI differs from traditional data integration in many di- mensions: (i) the number of data sources, even for a single domain, has grown to be in the tens of thousands, (ii) many of the data sources are very dynamic, as a huge amount of newly collected data are continuously made available, (iii) the data sources are extremely heterogeneous in their structure, with considerable variety even for substantially similar entities, and (iv) the data sources are of widely differing qualities, with significant differences in the coverage, accuracy and timeliness of data provided. This seminar explores the progress that has been made by the data integration community on the topics of schema mapping, record linkage and data fusion in addressing these novel challenges faced by big data integration, and identifies a range of open problems for the community.



I. INTRODUCTION  The Big Data era is the inevitable consequence of our ability  to generate, collect and store digital data at an unprecedented  scale, and our concomitant desire to analyze and extract value  from this data in making data-driven decisions to alter all  aspects of society. Big Data comes with a lot of promises ? as  a Dilbert cartoon would have it, ?It comes from everywhere.

It knows all.?1  This data is being collected today in a large variety of  domains. Examples include Web text and documents, Web  logs, large-scale e-commerce, social networks, sensor net-  works, astronomy, genomics, medical records, surveillance,  etc.2 Since the value of data explodes when it can be linked  and fused with other data to create a unified representation,  big data integration (BDI) is critical to realizing the promise  of Big Data. For example, to understand habitat utilization and  animal behavior in reaction to external forces such as weather,  marine animal researchers need to combine animal tracking  data with bathymetric, meteorological, sea surface temperature  and animal habitat data.3  BDI differs from traditional data integration (which includes  virtual integration and materialized warehousing) in many  dimensions.

? Volume: Not only can each data source contain a huge  volume of data, but also the number of data sources,  1http://dilbert.com/strips/comic/2012-07-29/ 2http://en.wikipedia.org/wiki/Big data 3http://en.wikipedia.org/wiki/Data fusion  even for a single domain, has grown to be in the tens  of thousands.

For example, in the recent work by Dalvi et al. [6], where  they analyze the nature and distribution of structured data  on the Web, they studied many domains (e.g., restaurants,  automotive, libraries, schools, hotels) and showed that  each domain has tens of thousands of sources on the  Web. This is much higher than the number of data sources  considered in traditional data integration.

? Velocity: As a direct consequence of the rate at which  data is being collected and continuously made available,  many of the data sources are very dynamic.

For example, there are many data sources that provide  near real time, continuously changing information about  the stock market, including bid and ask prices, volume of  shares traded, etc. Providing an integrated view of stock  market data across all these data sources is beyond the  ability of traditional methods for data integration.

? Variety: Data sources (even in the same domain) are ex-  tremely heterogeneous both at the schema level regarding  how they structure their data and at the instance level  regarding how they describe the same real-world entity,  exhibiting considerable variety even for substantially sim-  ilar entities.

For schema-level variety, in the recent work by Li et  al. [32], a study of 55 sources in the stock market domain  identified 153 global attributes that are manually matched  from 333 local attributes. The number of providers for  each global attribute observes Zipf?s law, with 13.7%  of the attributes provided by at least one third of the  sources and over 86% of attributes provided by fewer  than 25% of the sources. For instance-level variety, Guo  et al. [20] showed that for business listings, the number  of distinct business names is typically twice as many as  the number of distinct businesses in a zipcode. Similarly,  an early study [34] on Google Base showed that just for  vehicle color, there are over 250 different colors provided  as values, including very specific ones such as ?polished  pewter? and ?light almond pearl metallic.?  ? Veracity: Data sources (even in the same domain) are of  widely differing qualities, with significant differences in  the coverage, accuracy and timeliness of data provided.

For example, the work by Dalvi et al. [6] showed that  with strong head aggregators such as yelp.com, collecting  homepage URLs for 70% restaurants that are mentioned  by some websites required only 10 sources; however, col-  lecting URLs for 90% restaurants required 1000 sources,     and collecting URLs for 95% restaurants required 5000  sources. Similarly, the work by Li et al. [32] showed that  even in the stock market domain, inconsistent values were  provided by different sources for over 80% of the data  items whose values should be fairly stable (such as daily  closing price). This is consistent with the belief that ?1 in  3 business leaders do not trust the information they use  to make decisions.? 4  This seminar explores the progress that has been made  by the data integration community on the topics of schema  mapping, record linkage and data fusion (discussed in more  detail below) in addressing these novel challenges faced by  BDI. We do this using illustrative examples that would be of  interest to data management researchers and practitioners. We  also identify a range of open problems for the community in  integrating a galaxy of data sources.



II. TARGET AUDIENCE  The target audience for this seminar is anyone with an  interest in understanding data integration in the Big Data envi-  ronment. In particular, this includes the attendees at database  conferences like ICDE. The assumed level of mathematical  sophistication will be that of the typical conference attendees.



III. SEMINAR OUTLINE  The importance of big data integration has led to a sub-  stantial amount of research over the past few years on the  topics of schema mapping, record linkage and data fusion to  deal with the novel challenges faced by big data integration.

Table I shows a summary of these techniques. Our seminar is  example driven, and organized as follows.

A. BDI: Motivation (10 minutes)  The seminar will start with a variety of real-world examples  illustrating the importance of big data integration, building on  recent work by Dalvi et al. [6] and Li et al. [32].

B. BDI: Schema Mapping (25 minutes)  Schema mapping in a data integration system refers to  (i) creating a mediated (global) schema, and (ii) identifying  the mappings between the mediated (global) schema and the  local schemas of the data sources to determine which (sets of)  attributes contain the same information [41], [3], [1].

Early efforts in integrating a large number of sources  involved integrating data from the Deep Web. Two types  of solutions were proposed. The first is to build mappings  between Web forms (interfaces to query the Deep Web) as a  means to answer a Web query over all Deep Web sources [5].

The second is to crawl and index the Deep Web data [34],  [35]. More recent efforts include extracting and integrating  structured data from Web tables [4], [40] and Web lists [21],  [15].

The number of sources also increases the variety of the  data. Traditional data integration systems require a significant  schema mapping effort before the system can be used, so is  4http://www-01.ibm.com/software/data/bigdata/  obviously infeasible when the heterogeneity is at the BDI  scale. The basic idea of dataspace systems is to provide  best-effort services such as simple keyword search over the  available data sources at the beginning, and gradually evolve  schema mappings and improve search quality over time [16],  [22], [7], [8], [42], [26], [17], [25], [43].

A related notion becoming popular in the Hadoop commu-  nity is ?schema on read? which, in contrast to the traditional  approach of defining the schema before loading data (i.e.,  schema on write), gives one the freedom to define the schema  after the data has been stored.5  C. BDI: Record Linkage (25 minutes)  Record linkage refers to the task of identifying records  that refer to the same logical entity across different data  sources, especially when they may or may not share a common  identifier across the data sources [24], [19], [30], [46], [14].

Record linkage has traditionally focused on linking a static  set of structured records that have the same schema. In BDI,  (i) data sources tend to be heterogeneous in their structure and  many sources (e.g., tweets, blog posts) provide unstructured  text data, and (ii) data sources are dynamic and continuously  evolving. These characteristics make record linkage particu-  larly challenging in BDI.

When there are a large number of sources and a large  volume of data, traditional record linkage approaches become  inefficient and ineffective in practice. To address the volume di-  mension, new techniques have been proposed to enable parallel  record linkage using MapReduce. These include techniques for  adaptive blocking [12], [44], [37] and techniques that balance  load among different nodes [29], [28].

When the data sources are dynamic and continuously evolv-  ing, applying record linkage from scratch for each update  becomes unaffordable. To address the velocity aspect, incre-  mental clustering techniques have been proposed to address  this problem [45], [36].

Record linkage between structured and unstructured data  sources arises, e.g., when linking shopping transactions of peo-  ple with tweets or blog posts about their shopping experience.

To address the variety aspect, techniques have been proposed  that tag and match free text to structured data [27].

Finally, in the BDI environment, information is typically  more imprecise and noisy. To address this veracity aspect, a  variety of clustering and linkage techniques that are robust to  noise or evolving values have been proposed [31], [20], [23].

D. BDI: Data Fusion (20 minutes)  Data fusion refers to resolving conflicts from different  sources and finding the truth that reflects the real world [2],  [11]. Unlike schema mapping and record linkage, data fusion  is a new field that has emerged only recently. Its motivation  is exactly the veracity of data: the Web has made it easy to  publish and spread false information across multiple sources  5http://howsoftwareisbuilt.com/2010/01/06/interview-with-amr-awadallah- cloudera-cto/     TABLE I SUMMARY OF STATE-OF-THE-ART DATA INTEGRATION TECHNIQUES MEETING CHALLENGES OF BIG DATA.

Schema mapping Record linkage Data fusion  Volume Integrating Deep Web, Adaptive blocking, Online fusion Web tables/lists MapReduce-based linkage  Velocity Incremental linkage Fusion in a dynamic world  Variety Dataspace systems Linking text to structured data Combining fusion with linkage  Veracity Value-variety tolerant linkage Truth discovery  and so it is critical to separate the wheat from the chaff for  presenting high quality data.

To address such veracity related challenges, techniques  have been proposed to find the single truth from conflicting  values [49], [32], [48], [39], [38], [18], [9], [47] and to find  multiple truths [50]. Such techniques have also been extended  to handle the volume of data (online data fusion [33]), velocity  of data (truth discovery for dynamic data [10]), and variety of  data (combining record linkage and data fusion [20]).

E. BDI: Open Problems (10 minutes)  Finally, we discuss cutting-edge open problems for big data  integration, such as integrating crowdsourcing data, integrating  data from data markets, providing an exploration tool for data  sources, and so on.



IV. BIOGRAPHIES  A. Xin Luna Dong  Xin Luna Dong is a researcher at AT&T Labs-Research.

She received her Ph.D. from University of Washington in  2007, received a Master?s Degree from Peking University  in China in 2001, and received a Bachelor?s Degree from  Nankai University in China in 1998. Her research interests  include databases, information retrieval and machine learning,  with an emphasis on data integration, data cleaning, personal  information management, and Web search. She has led the  Solomon project, whose goal is to detect copying between  structured sources and to leverage the results in various as-  pects of data integration, and the Semex personal information  management system, which got the Best Demo award (one  of top-3) in Sigmod?05. She co-chaired Sigmod/PODS PhD  Symposium?12, QDB?12, WebDB?10 and has served in the  program committees of ICDE?13, PVLDB?13, Sigmod?12,  VLDB?12, Sigmod?11, VLDB?11, PVLDB?10, WWW?10,  ICDE?10, VLDB?09, etc. She has presented tutorials on ?Data  Fusion: Resolving Data Conflicts for Integration? (with Felix  Naumann) at VLDB 2009, and on ?Detecting Clones, Copying  and Reuse on the Web? (with Divesh Srivastava) at SIGMOD  2011 and ICDE 2012.

B. Divesh Srivastava  Divesh Srivastava is the head of the Database Research  Department at AT&T Labs-Research. He received his Ph.D.

from the University of Wisconsin, Madison, and his B.Tech.

from the Indian Institute of Technology, Bombay. He is an  ACM fellow, on the board of trustees of the VLDB Endow-  ment and an associate editor of the ACM Transactions on  Database Systems. He has served as the program committee  co-chair of many conferences, including VLDB 2007. His  research interests and publications span a variety of topics  in data management. He has presented tutorials on ?Data  Stream Query Processing? (with Nick Koudas) at VLDB 2003  and ICDE 2005, on ?Record Linkage: Similarity Measures  and Algorithms? (with Nick Koudas and Sunita Sarawagi)  at VLDB 2005 and SIGMOD 2006, on ?Anonymized Data:  Generation, Models, Usage? (with Graham Cormode) at SIG-  MOD 2009 and ICDE 2010, on ?Information Theory for Data  Management? (with Suresh Venkatasubramanian) at VLDB  2009 and SIGMOD 2010, and on ?Detecting Clones, Copying  and Reuse on the Web? (with Xin Luna Dong) at SIGMOD  2011 and ICDE 2012.



V. CONCLUSIONS  This seminar reviews the state-of-the-art techniques for  data integration in addressing the new challenges raised by  Big Data, including volume and number of sources, velocity,  variety, and veracity. We discuss how close we are to meeting  these challenges and identify many open problems for future  research.

