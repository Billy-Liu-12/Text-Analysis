Extraction of Medical Knowledge from Clinical Data

Abstract In the last years, developed countries have faced the  high incidence of many diseases that are increased significantly.

The etiologies of those diseases are not clear and neither are the  reasons for the increased number of cases. Early detection  represents a very important factor in treatment and allows  reaching a high survival rate. Due to the high volume of data to  be read by physicians, the accuracy rate tends to decrease, and  automatic reading and analyzing of digital data becomes highly  desirable.  In this paper, we discuss the problem of the high  volume of medical data to be read by physicians, the accuracy  rate tendency to decrease, and automatic reading of data that  becomes highly desirable. The system is implemented and used  against the data of the National Center for Health Statistics  (NCHS) of the Centers for Disease Control and Prevention  (CDC). From the generated association rules, we have found  some interesting rules that can be used in the early prediction of  health phenomenon.

Keywords Data Mining, Medical knowledge, Association Rules,  Feature Extraction.



I. INTRODUCTION  In medical domain, data collected through patients  medical records, needs to be analyzed. Analyzing such data is  considered the most reliable method in early detection of  certain diseases. As an example, mammograms, X-rays, and  ECG are recommended in early detection of diseases such as,  breast cancer, lung diseases, and heart cases, respectively. Due  to the high volume of medical data to be read by physicians,  the accuracy rate tends to decrease, and automatic reading of  data becomes highly desirable. Data stored in terabytes of  storage needs a serious effort by individual humans for  manually examining and characterizing the data objects. Also,    in many cases, it has been proven that double reading of such  data by two physicians could lead to more accurate results, but  at high costs. That is why the computer aided diagnosis  systems are necessary to assist the medical staff to achieve  high efficiency and effectiveness. Also, there is a high  incidence of many diseases such as various types of cancer,  especially in developed countries, have been increased  significantly in the last years. The etiologies of those diseases  are not clear and neither are the reasons for the increased  number of cases. In many cases, early detection represents a  very important factor in treatment and allows reaching a high  survival rate. Data in images such as Mammograms and Brain  cat-scan are considered the most reliable methods in early  detection of many cases.

This work is meant to show the application of data mining  on the medical domain. We have surveyed most of the well  known techniques in all phases of data mining, and have  chosen the most suitable ones for our application domain. We  emphasis on three problems,    ? features selection,  ? data cleansing, and  ? data mining.

Feature selection Process is used in selecting a subset of  original features [11]. Finding an optimal feature subset is has  been shown to be NP-hard problem [12]. Four basic steps are  designed for feature selection process, which are, subset  generation, subset evaluation, stopping criterion, and result  validation [14].

Data cleansing is a preprocess step for data mining [20].

The process of data cleansing is normally computationally  expensive, hence doing it by using old techniques is not  desirable. Nowadays, faster computers allow data cleansing to  be performed in an acceptable amount of time on a large  amount of data. There are many issues in the data cleansing  area that interest researchers. These issues consist of dealing  with missing data, determining erroneous data, etc. Different  issues require different approaches.

After applying the data cleansing technique, we should use  a suitable data mining technique to discover information that  many traditional business analysis and statistical techniques    fail to deliver [6]. Additionally, the application of data mining  techniques further exploits the value of data warehouse by  converting expensive volumes of data into valuable assets for  future tactical and strategic business development.

Management information systems should provide advanced  capabilities that give the user the power to ask more  sophisticated and pertinent questions. It empowers the right  people by providing the specific information they need.



II. PROBLEM DEFINITION  The methods proposed in the literature classify the medical  data into two categories: normal and abnormal. The normal  ones are those characterizing healthy patients. The abnormal  ones are taken from patients with health problems [15].

Medical digital data are among the most difficult data to be  analysed. Normally, in the early stages of most diseases, signs  are very subtle and varied in values, making diagnosis  difficult, challenging even for specialists [19]. This is the main  reason for the development of classification systems to assist  specialists in medical institutions. Due to the significance of  an automated data categorization to help physicians, much  research in the field of medical data classification has been  done recently [10].

With all this effort, there is still no widely used method to  classify medical data. This is due to the fact that medical  domain requires high accuracy, especially when rate of false  negatives is very low [18]. In addition, another important  factor that influences the success of classification methods is  working in a team with medical specialists, which is desirable  but often not achievable. The consequences of errors in  detection or classification are costly. A false positive detection  may cause unnecessary procedures.

Statistics show that only, for most diseases, 20-30  percentages of a certain disease cases are proved positive [18].

In a false negative detection, an actual disease remains  undetected that could lead to higher costs or even to the cost  of a human life. Here is the trade-off that appears in  developing a classification system that could directly affect  human life.  All these reasons make the decisions that are  made on such data even more difficult. Different methods  have been used to classify and/or detect diseases, such as  wavelets [9], fractal theory [20], statistical methods [19] and  most of them used features extracted using processing    techniques [16]. In addition, some other methods were  presented in the literature based on fuzzy set theory [5],  Markov models [19] and neural networks [13]. Most of the  computer-aided methods proved to be powerful tools that  could assist medical staff in hospitals and lead to better results  in diagnosing a patient.

Processing data manually becomes unfeasible. A strong  need for an intelligent tool that processes these medical data,  and knowledge extraction has become necessary. Hence, the  term Knowledge Discovery in Databases appeared as a  response to a strong need in the field [7]. Knowledge  Discovery term describes a variety of techniques aim at  identifying chunks of information or knowledge within a mass  of data, and extracting these knowledge  in such a way that  they can be put to use in the area of decision support. No need  to say that these techniques search for correlations among the  data that are of interest to the user, yet unspecified by the user:  as the probability of unforeseen relationships to exist among  the data is very high, since the user can not visualize such a  huge amount of data. Data mining is the heart, or the crucial  step of knowledge discovery in database.



III. SYSTEM DESCRIPTION  In this work, a data mining technique based on association  rule mining is used to extract useful information from  patients data to be used in early detection of many diseases.

The goal is to effectively determine certain causes of health  problems via data mining. The detection problem is difficult  and has been evasive to an automatic detection approach.

Problems like early detection of breast cancer using  mammogram images, make it difficult for the automatic  detection. The images which are under study are Scan-  digitized Imagery, which were taken under various  atmospheric conditions, making the effective recognition and  detection a hard task. Moreover, due to the varying natures of  occurrences, good generalization is difficult. Many studies are  conducting research to study and estimate the areal extent of  specific diseases.

In order to detect a certain disease automatically, we have  to achieve the following   tasks [15]: Feature selection,  Feature extraction, and evaluation the performance of these  features using certain Evaluation criteria, and finally learning  and Classification using Data Mining techniques. These tasks    can be briefly described as follows:    Task 1 - Feature selection [1]  ? Study the images and identify the problem regions.

? Select features which described a patient among  many possible ones.

? Focus on some features such as ejection fraction and  blood pressure with specific definitions.

Task 2 - Feature extraction  ? Extract features automatically from data,  ? For image feature extraction, we try a range of  intensity where diseases spectrum correspond to,    Task 3 - Measuring these features by Evaluation criteria.

Task 4  Apply the suitable data mining technique to get a set  of rules or conclusion out of the input data.

Task 5 - Visualize the results.

Automated tools must be developed to help extract  meaningful information from a flood of information.

Moreover, these tools must be sophisticated enough to search  for correlations among the data unspecified by the user, as the  potential for unforeseen relationships to exist among the data  is very high [14]. A successful tool set to accomplish these  goals will locate useful nuggets of information in the  otherwise chaotic data space, and present them to the user in a  contextual format.



IV. SYSTEM IMPLEMENTATION  In the process of knowledge clinical data mining, there are  still three more steps to be performed. The main step is to  implement the selected mining Algorithm. The other two steps  are data transformation and data discretization. We have used  for our system the AprioriTid algorithm [3] which is a  variation of the Apriori algorithm [2].

A. Data Transformation  In data transformation, the data are transformed into forms  appropriate for mining. Data transformation can involve one  or more of the following:      ? Smoothing where noise values are removed from the  data. Such techniques include binning, and clustering.

? Aggregation, where summary or aggregation  operations are applied to the data.

? Generalization of the data, where low-level or  primitive (raw) data are replaced by higher-level  concepts through the use of concept hierarchies.

? Normalization, where the attribute value are scaled to  fall within a small specified range, such as -1.0 to 1.0,  or 0.0 to 1.0.

? Attribute construction (or feature construction),  where new attributes are constructed and added from  the given set of attributes in order to help improve  the accuracy and understanding of structure in high-  dimensional data [8].

Three different processes, which are considered as forms of  aggregation, could be applied on collected data. The three  processes are:  1)  Data Discretization    Data discretization techniques are used to reduce the  number of values for a given continuous attribute by dividing  the range of the attribute into intervals. Interval labels can then  be used to replace actual data values. Replacing numerous  values of a continuous attribute by a small number of interval  labels thereby reduces and simplifies the original data. This  leads to a concise, easy-to-use, knowledge-level  representation of mining results.

Discretization can be performed recursively on an attribute  to provide a hierarchical or multi resolution partitioning of the  attribute values, known as a concept hierarchy. Concept  hierarchies are useful for mining at multiple levels of  abstraction. The generation of numerical and categorical data  are discussed in following subsections.

2)  Discretization and Concept Hierarchy Generation for  Numerical Data    Concept hierarchies for numerical attributes can be  constructed automatically based on data discretization. Many    techniques based on concept hierarchies are used for  discretization, such as, binning, histogram analysis, entropy-  based discretization, and cluster analysis. In this section, we  only concentrate our discussion on the histogram analysis  method because it is suitable for our research.

The histogram analysis technique is an unsupervised  discretization technique where class information is not used.

Histograms partition the values for an attribute A, into disjoint  ranges called buckets. In an equal-width histogram, for  example, the values are partitioned into equal-sized partitions  or ranges (such as in Figure 1 for price, where each bucket has  a width of $10). The histogram analysis algorithm can be  applied recursively to each partition in order to automatically  generate a multilevel concept hierarchy.

The discretized numeric attributes, with their interval  labels, can then be treated as categorical attributes (where  each interval is considered a category).

Figure 1: An equal-width histogram for price, where values are aggregated  so that each bucket has a uniform width of $10.

3)  Concept Hierarchy Generation for Categorical Data    Categorical data are discrete data, where categorical  attributes have a finite (but possibly large) number of distinct  values, with no ordering among the values. Examples include  geographic location, job category, and item type. For  categorical data, concept hierarchies may be generated based  on the number of distinct values of the attributes defining the  hierarchy [17].

After review all the fundamental preprocessing steps, we  will choose only the steps needed to prepare our medical data.

So we will use the following steps as data preprocessing steps:    ? Data cleaning for missing and noisy data.

? Data reduction to reduce the dataset size by removing  the irrelevant and un-useful attributes from the  dataset.

? Data transformation to suitable formats for mining by  using Data descretization with histogram technique.

In the following example, the data descretization process is  demonstrated. For the first 10 records in our data set, the    descretization process will replace the Mother's weight  feature, which has a continuous domain, by three features; less  than 120, between 120 and 150, and more than 150. Each  feature has value 1 if the person agrees with the feature and 0  if the person disagrees with the feature. In Figure 2 we show  the descritization process.

Mother's weight Record Mother's  Weight  Record  Less  than   Between  120 and   More  than   1 160  1 0 0 1  2 122  2 0 1 0  3 125  3 0 1 0  4 165  4 0 0 1  5 133  5 0 0 0  6 160  6 0 0 1  7 110  7 1 0 0  8 105  8 1 0 0  9 120  9 0 1 0  10 133  10 0 1 0  Figure 2: Mothers weight Descritization Mapping  B. Algorithm Implementation  The selected AprioriTid algorithm is based on the Apriori  algorithm and uses the "apriori-gen" function to determine the  candidate itemsets before the pass begins.  The main  difference from the Apriori algorithm is that the AprioriTid  algorithm does not use the database for counting support after  the first pass.  Instead, the set <TID, {Xk}> is used for  counting.  (Each Xk is a potentially large k-itemset in the  transaction with identifier TID.)  The benefit of using this  scheme for counting support is that at each pass other than the  first pass, the scanning of the entire database is avoided.  But    the downside of this is that the set <TID, {Xk}> that would  have been generated at each pass may be huge.  Another  algorithm, called AprioriHybrid, is introduced in [3].  The  basic idea of the AprioriHybird algorithm is to run the Apriori  algorithm initially, and then switch to the AprioriTid  algorithm when the generated database (i.e. <TID, {Xk}>)  would fit in the memory.

The AprioriTid algorithm does not use the database for  counting support after the first pass.  Instead, a set containing  those transaction ids with their potentially large itemsets is  used in the next passes. At each pass other than the first pass,  the scanning of the entire database is avoided.  We have  implemented the AprioriTid algorithm by using C++, and on a  2.4 GHz machine, with 4 GB of RAM and running windows  Vista. The data set used was obtained from the National  Center for Health Statistics (NCHS) of the Centers for Disease  Control and Prevention (CDC)  (http://www.cdc.gov/nchswww/nchshome.htm). The main  system Interface is given in Figure 3.

Figure 3: System Interface

V. THE EXPERIMENTAL RESULTS  In the process of experimental results, we have tested  several values for minimum support and minimum confidence.

By checking the output results and looking for non-trivial  rules by experts, we found it is suitable to show only those  experiments with a minimum support value of 1%, and  minimum confidence equals to 70%.  In order to have a clear  view of the system findings, we choose to limit the length of  the generated frequent itemsets to 3. In Figure 3, the system  interface accepts four inputs.

? Input file: the data should be in a binary format  ? Output file: either frequent itemsets or frequent  association rules  ? Minimum Support as a percentage and  ? Minimum Confidence: as a percentage    There are two output modes, the first is frequent item sets  mode, and the other is the frequent association rule mode. In  Table 1, we give a sample of the frequent association rules    generated from our experiment results    TABLE1. THE EXPERIMENTAL RESULTS OF THE CLINICAL DATA MINING  No Feature Name Support Confidence   Marital status(divorced),  Sex(Male) ?  Natural  teeth(bad)  1.1% 81.02%   Race(Black), Household size  (5-8) ?  Marital  status(divorced)  2% 75.62%   Mother's weight (more than  150), Father's weight(more  than 200) ?  Ever received  orthodontic treatment(YES)  1.05% 72.31%   # episodes of cough in past 12  months(4-6), How many  rooms are in this home(3)  ?  Age of biological mom  when SP was born (more than  45)  1.4% 70.24%   Cigarettes per day person 1  smokes (more than 20),  Parent heart attack/angina  before 50 (YES) ?  Is  exhaust fan near this  stove(YES)  1.02% 73.43%   Age when stopped breastmilk  -days (0-40), Father's height  calculated in inches (64-67)  ?  Limited in activities by  health problem(YES)    2.72% 84%   Pet lives here -a cat(YES),  Marital status(single) ?

Doctor ever say SP had  asthma(YES) 1.04%  70.11%   Age when first fed formula  daily days(0-30), Is health in  general excellent(NO)  ? Parent high blood  pres/stroke before 50 1.91%  89.02%      Most of the results show that there is a strong dependency  between Age when babies stopped breastmilk and their health  in general. Also, there is a strong dependency between  mothers being smoking or not during pregnancy and the  mental health of her kids.  By going through most of the  generated rules, we could find that the results are consistent  with the latest medical research findings. In addition to that,  we have found that some of the rules are completely  nontrivial, such as that rule that states that there is  a  relationship between not having dogs or cats in household and  the effect on the kid  being shy when meeting friends,

VI. CONCLUSIONS  In this work, we have discussed the problem of the high  volume of medical data to be read by physicians, the accuracy  rate that tends to decrease, and automatic reading of data that  becomes highly desirable. Various association mining  techniques have been considered.  Of those techniques, we  have chosen AprioriTid algorithm as the basic association  mining technique. It is been implemented as the core of our  medical mining system. The system has been implemented  and used against the data of the National Center for Health  Statistics (NCHS) of the Centers for Disease Control and  Prevention (CDC), where data has been collected,  disseminated and analyzed on the health status of U.S.

citizens. The outcomes of collection and analyses are made  available through different data release mechanisms including    CD-ROMs (Search and Retrieval Software, Statistical Export  and Tabulation System (SETS)), mainframe computer data  files, publications, and through the national center for health  statistics (NCHS)  (http://www.cdc.gov/nchswww/nchshome.htm). We have run  our experiments on a 2.4 GHz machine, with 4 GB of RAM  and running windows Vista. In our experimental phase, we  have used a minimum support value of 1%, and minimum  confidence equals to 70%. From the generated association  rules that we have found some interesting rules that can be  used in the early prediction of health problems.

