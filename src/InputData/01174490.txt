WEB DOCUMENTS MINING

Abstract: By grouping similar Web documents into clusters, the  search space can be reduced, the search accelerated, and its precision improved. In this paper, one novel-clustering algorithm is introduced. In the proposed clustering method, topics are represented according to VSM (Vector Space Model), documents are represented according to the topics, and the relation between the documents and the topics is viewed as a transaction, one document corresponds to a transaction and one topic corresponds to an item. Using association rules mining algorithm discovers the frequent item sets, and the corresponding documents are seen as the initial clusters. The clusters are merged if the distance between them is small enough, or the cluster is divided if the connection strength between its documents is smaller than the given threshold. Experiments is conducted on real Web documents, results show the algorithm's effectivity and suitability for tackling the overlapping clusters inhered by documents.

With the rapid development of WWW, the resources and the services in Internet are all increasing explosively. In order to help users effectively use these resources and services, many powerful search engines were developed, which already brought many conveniences to users. In the meantime, the problem that these search engines didn't commendably meet user's needs was unfolded. By using the Web documents clustering techniques, the search space can be reduced, the search accelerated, and its precision improved. Hence, it attracts many attentions recently [1-61.

Web documents clustering approaches are mainly based on either probabilistic methods or distance measures.

Probabilistic-based methods [5s61 based on Bayesian probability theory, which uses probability distributions to describe the clustering results, can tackle the situations of overlapping clusters inhered by documents. But they don't perform well when the size of the feature space is much larger than the size of the sample size set or may depend on the independence of the underlying features. Distance- based methods r41 use feature vector, which can be viewed as one point in multi-dimensional space, and represents a Web document by calculating the distances among these points to cluster the corresponding Web documents. But the  0-7803-7508-4/02/$17.00 02002 IEEE  Keywords: Web document clustering; Association rule; Web mining  1 Introduction   feature vectors must be normalized, and they don't perform very well if the dimension of the space is very large.

We represent Web document using topics, viewing the relation between documents and topics as a transaction, using association rules mining algorithm '7' to discover the frequent item sets, and the corresponding documents are seen as the initial clusters. These clusters are further validated according to the intracluster similarity and the intercluster similarity. And the problems possessed by previously proposed methods were effectively solved.

2 The Structurization of Web Document  A Web document is one kind of semi-structured data, which must be structurized in order to be retrieved easily and efficiently. The structurization of a Web document is the process of extracting and describing its features, and set up the corresponding structure.

In order to structurize a Web document, we first represent all topics using VSM (Vector Space Model) and calculate the Topic Feature Vector, and construct Document Topic Vector according to document content and the founded Topic Feature Vector; then calculate the Relating Degree between a given document and all topics, and set up Document-Topic Transaction; finally, normalize the transaction in order that they can be compared.

The concepts involved in the above are defined and clarified as follows.

D e f ~ t i o n  2.1 (Topic Feature Vector) Let T be the set of all topics, for each topic T E T ,  its feature vector is  weight of ki, , which indicates the importance of keyword  number of keywords in topic the 1 of different topics may be different.

Defining topic feature vector using VSM, the keywords and corresponding weights of a given topic can be freely determined according to the specific situation, the number of keywords in different topics may be different, this    sufficiently gives attention to the specific situations of all topics, and possesses broad applicability and strong maintainability.

Definition 2.2 (Document Topic Vector) Let D be the set of documents, for each document D ~ E D ,  the vector  ~ D ~ ( T ~ )  represents the contribution of document Dj to  is the frequency of the P,!, = "U D j I J x  W , , t kth (1 d k d I> keyword ki,k of topic Ti appearing in  is the number of valid words in document Dj , IIU D ill document Dj , is the weight of the kth (1 d k b  I )  keyword ki,k in topic feature vector T i .

According to the definition, document topic vector  reflects the relation between a document and a topic.

Because the lengths of different topic feature vectors may be different, even if the lengths are identical, using the document topic vector represent a document will suffer from the high dimensionality. In order to reduce the high dimensionality, we must calculate the relating degrees between a given document and different topics.

Definition 2.3 ( Relating Degree ) Relating degree indicates how much the given document relates a topic. The relating degree Ai.j between document Dj and topic T, is defined as follows  4, = z P ; k  (3) k=l  where, 2 = 11<11 , is the length of the vector f : ,u:~ is the contribuation of document Dj to the kth (1  bkbl)  keyword in topic T, , can be calculated by formula (2) .

Relating degree represents the relating relation between one document and one topic using a data item, this reduces the dimensionality from p / lX c :=112, to llD 11 , greatly decreases the amount of data waiting for processing, and lays the foundation for increasing processing efficiency.

By now, we view documents as transactions, view topics as transaction items, and set up the document-topic transaction matrix as formula (4) as following. Actually, this matrix is really one relation table, refers to it as one matrix, only with the purpose of easy statement.

I . . .  . . . . . . . . . . . . . . .  I  I . . .  . . . . . . . . . . . .  . . . I  where, Ai,j is the relating degree between document Dj and topic T, , and can be calculated according to formula ( 3  ) ; each row vector represents one transaction; l d i d n ,  ldj d m ,  m is the number of topics, and n is the number of documents.

After setting up the transaction matrix U,,, , each row vector must be normalized, so the relating degrees can be compared. In the meantime, refer to the new matrix as Normalized Transaction Matrix, and denote it as Ulxm .

The document-topic transaction matrix normalization algorithm is presented as follows .

Algorithm 1. Set up normalization document-topic transaction matrix Input: D , The set of Web documents; T ,  The set of topic  Output: document-topic transaction matrix [A] ; feature vectors;  Function: Set up document-topic transactions 1) for (j := 1; j ~llql;j++ ) do begin 2) for really r (i := 1; i 5 1 1 ~ 1 1 ;  i++ ) do begin 3) for each k I 1 ,  computing p J  according to formula (2);  rC  4) end for 5) compute Ai, according to formula (3); 6) end for 7) for ( i: = 1; i 5 I(D11; i++ ) do begin S) for ( j :=  1; j ~ I I ~ l l ; j + + ) d o b e g i n  10) end for  This algorithm comprises of two parts. The first part is used to calculate the relating degree between document and topic, and to set up document-topic transaction matrix. If let NoKi be the number of key words in topic , the time  to normalize the row vector in document-topic transaction matrix, and to construct the normalized document-topic transaction matrix, the time complexity is ~ ~ ~ D ~ ~ x I I T I I )  .

Hence, the total time complexity of the algorithm of document-topic transaction matrix normalization  In order to describe it easily, we introduce the concept of document feature vector.

Definition 2.4 ( Document Feature Vector ) In the normalized transaction row vector, which presents the relating relation between a document and all  IT)) topics, we refer to it as document feature vector, and denotes it as  vd.

-  3 Web Documents Clustering Based on Association Rule  After the normalized transaction matrix ULxm has been set up, association rules mining algorithm can be used to discover the frequent set of topics, and the corresponding set of documents is the initial document clusters which are verified by the given verific&ion techniques finally.

3.1 Discover document clusters using association rules mining algorithm  Association rules capture the relationships among items that are presented in a transaction. Let I = { i,, i, ,..., i n }  be the set of items that comprises of different items, T be the set of transactions where each transaction is a subset of I , and C be a subset of I .  The support count of C in I is defined as the number of transactions that contain C as o(C)=(I{tIte ~ , C c _ t ) ( l  . An association rule is an expression of form x S .  a Y ,  where x E I , Y G I ,  the confidence a of the rule x S. a >y is defined as  a(X --f y) = d x  ? Y ) A X )  And the support S is defined as following 1 s ( X  + Y ) = - - a ( X u Y )  ( 5 )  The task of discovering an association rule is to find all rules of form x-%, such that S is greater than a given minimum support threshold and 01 is greater than a given minimum confidence threshold.

According to formula (5 ) ,  support S indicates the frequency of implication X + Y appearing in the set of transactions T. If the frequency of an implication is greater than the given minimum threshold, the items which compose the implication make up of the frequent set. We think, the reason of frequent set existing is that the corresponding transactions are similar to a certain extent.

In the document-topic transaction matrix, we view documents as transactions, and topics as items of a transaction. Thus, if some items (i.e. topics) appear together in some transactions (i.e. documents), these transactions (i.e.

documents) are similar, too. In other words, according to the frequent sets obtained by association rules mining algorithm, we can find out the corresponding transactions (i.e. documents), and view them as the initial document clusters.

But as the former description, formula ( 5  1 indicates the frequency of a set of items appearing in the whole set of  ll*II  transactions. If the frequency of a set of items is smaller than the given minimum support threshold, the set of items would cannot be a frequent set of items. We view documents as transactions, and topics as items of transaction, although the support of some items of transaction is smaller, the corresponding transactions are still similar. Hence, formula ( 5 )  already doesn?t fit into our current situation, it needs to be redefined.

Definition 3.1 ( Support ) Let r=Uzk, ?v?v c r, the support of is defined as following  where, lly,jl> 1 , is the number of documents in set W : r is the numberof elements whose value is nonzero in all document feature vectors.

Distinctly, support 6(p) of set W is defined as the average of the nonzero relating degree between all documents and all topics, it effectively solved the problem of the documents that can not to be put into one cluster only because the general support is smaller than the given minimum support threshold even though the topics involved are similar. In the meantime, it introduced the idea of fuzzy theory, and described the relating reationship between one document and all topics not simply using ?1? or ?0? but the degrees in range [0,1], which precisely reflected the real world.

When using association rules mining alogrithm to discover document clusters, we first construct frequent topic sets according to Agrawal?s fast algorithm for mining frenquent item sets [71 and the definition of support presented in this paper, and then scan the database to gain the corresponding document sets which are the initial document clusters. The fast algorithm for mining frenquent item sets has been introduced detailedly in literature [7], there is no need to repeat.

3.2 Verify the document clusters  The verification of the initial document clusters consists of two stages. The first is to calculate the coupling among different document clusters, and merge them if the corresponding coupling is greater than the given minimum coupling threshold. Then measure the cohesion of a document cluster, and divide it if the corresponding cohesion is smaller than the given minimum cohesion threshold.

We represent the coupling of the different document clusters as the distance between two document clusters.

Definition 3.2 ( Coupling ) In order to increase the coupling of a document cluster, and to decrease the cohesion among the different document clusters, we define the distance between document cluster p and document cluster q as following      where, np and nq are the number of documents in document cluster p and document cluster q respectively, xp and 4 are the center of gravity of document cluster p and document cluster 4 in turn.

We represent the cohesion of a document cluster as the connecting strength between a document and the document cluster it attached.

Definition 3.3 ( Cohesion ) The cohesion presents the similarity of documents in a cluster. Let C be a document cluster, document d e  C.  The connecting strength between document d and cluster C is defined as follows  - -  r ? ( d , C ) =  c ? p d . ,  k ! : : z  24, (8) According to the definition, connecting strength indicates  the contribution of a document to the cluster it attached. If the contribution of a document is smaller than the given minimum cohesion threshold, this document should be kicked out of the cluster it attached.

Algorithm 2. Document clusters verification algorithm Input: % , The set of initial Web document clusters; (b ,  Output: % , the validated Web document clusters Function: Validating the initial Web document clusters 1) for ( i  := 1; i 5 /[%11-1; i++ ) do begin 2) for ( j  := 2; j 5 (1s11; j++ ) do begin 3) DT, : = Compu-dist (C,E % , C,E % );  5) end for 6) delete c, and all C, cc from % ;  Threshold of coupling; 9, Threshold of cohesion  4) i f q ,  <@then c: := uc, ;  7) c+q& 8) end for 9) append U C: to 93 ; 10) for all clusters 11) forall documents d E C, do begin 12) if u(d, C, ) I v, then do begin 13) C i : = u d ;  14) delete d from Ci ; 15) end if 16) end for 17) end for 18) append U c: to% ;  c% do begin  I  Document clusters verification algorithm consists of two steps: merging and dividing. The merging step is a double loop, its time complexity is o@I[ x ([[%[I- 1)) , where % is the number of initial document clusters generated by association rules mining algorithm. The dividing step is a double loop, too, its time complexity is o(JqlXC~q), where NOD, is the number of documents of the ith cluster  Cif  Because l l q l ~ C ~ ~ ~  and 11s[)2 are extremely greater than (I%((, the time complexity of this algorithm is  4 Experimental Results  We compared our algorithm with search engine Yahoo and algorithm k-means. The Web documents used in the experiment were gotten by Yahoo on the Internet.

At first, we tested the precisions of these algorithms. We used Yahoo to search the Web documents with 50 different topics on the Internet, and downloaded the first 100 documents of each searching in the results and saw them as the 50 classes generated by Yahoo. After that, we kicked out the irrelevant documents manually, and regarded the results as base of the precision comparison. And then, we used the proposed algorithm and k-means to classify the documents gotten by Yahoo respectively.

Maybe the number of classes generated by different algorithms is different, so we only compared the first 40 high quality classes of each method. Figure 1 is the comparison of average precision of different algorithms.

Because the proposed algorithm permits the overlay between different classes, and the validation of classification is adopted, the average precision is the highest.

Fig. 1. Precision Comparison  Then, we tested the scalabilities of these algorithms. We also used Yahoo to search the Web documents relevant to the preceding 50 topics, but we only downloaded the first 10 documents of each topic, and increased with the increment 5 to the first 45 documents, formed 8 sets of documents with the number 500, 750, 1000, 1250, 1500, 1750, 2000 and 2250 respectively. And then, we classified the 8 sets of documents using the proposed algorithm and k-means in turn, and computed the average clustering time, the results were shown in Figure 2. From Figure 2 we can know, the average executing times of the two algorithms are all increased with the increase of documents, but the increment of the proposed algorithm is smaller. This that presented the scalability of the proposed algorithm is better than k-means. The reason is that the proposed algorithm reduced the dimensions of the      document feature vector by representing the document using topics. At the same time, we regarded the topics as items of transaction, and reduced the amount of data processing.

[6]  r71  Fig.2. Scalability Comparison  5 Conclusions  Web documents clustering can be broadly used in Internet based information retrieval and E-commerce.

Based on frequent items discovery algorithm, we presented one kind of Web documents clustering algorithm.

In the proposed algorithm, a document is seen as one transaction, and a topic is seen as one item of transaction. It is suitable for processing the high dimensional data and it posses better scalability. At the same time, the proposed algorithm is adaptive to the overlapped conditions inhered in document classes.

