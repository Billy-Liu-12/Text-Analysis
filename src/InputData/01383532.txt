A Comparative Study among Three Algorithms for Frequent  Pattern Generation

Abstract  Efficient algorithms to mine frequent patterns are crucial to many tasks in data mining. Since the Apriori algorithm was proposed in 1994, there have been several methods proposed to improve its performance.

However, most still adopt its candidate set generation? and-test approach. In addition, many methods do not generate all frequent patterns, making them inadequate to derive association rules. The Pattern Decomposition (PD) algorithm that can significantly reduce the size of the dataset on each pass makes it more efficient to mine all frequent patterns in a large dataset. This algorithm avoids the costly process of candidate set generation and saves a great amount of counting time to evaluate support with reduced datasets. In this paper, some existing frequent pattern generation algorithms are explored, their comparisons are discussed, which shows that the PD algorithm outperforms an improved version of Apriori named Direct Count of candidates & Prune transactions (DCP) by one order of magnitude and is faster than an improved FP-tree (Frequent Pattern) named as Predictive Item Pruning (PIP). Further, PD is also more scalable than the DCP and PIP.

1. Introduction  Data mining refers to extracting or "mining" knowledge from large amounts of data. It is also called a method of "knowledge presentation" where visualization and knowledge representation techniques are used to present the mined knowledge to the user.

Data Mining is a promising and flourishing frontier in database system and new database application. A fundamental component in data mining tasks is finding frequent patterns in a given dataset. Frequent patterns  are ones that occur at least a user-given number of times (minimum support) in the dataset. They allow us to perform essential tasks such as discovering association relationships among items, correlation, sequential pattern mining, and much more [3].

Algorithms proposed in [1, 5, 6, 8, 9] find all frequent sets in a dataset. The Apriori algorithm [1 ] accomplishes this by employing a bottom-up search. It generates candidate sets starting at size 2 up to the maximal frequent set size.

At each pass, it determines which candidates are frequent by counting their occurrence. Due to combinatory explosion, this leads to poor performance when frequent pattern sizes are large. To avoid this problem, some algorithms output only maximal frequent sets [2, 10, 11]. Pincer- Search [11] uses a bottom-up search along with top-down pruning. Max? Miner [2] uses a bottom-up search with a heuristic to try to identify frequent sets as early as possible. Even though performance improvements may be substantial, maximal frequent sets have limited use in terms of association rule mining. A complete set of rules cannot be extracted without support information of frequent subsets. An improved version of Apriori named Direct Count of candidates & Prune transactions (DCP) uses effective database pruning techniques [6]. It exploits an innovative method for storing candidate itemsets and counting their support.

Almost all previous algorithms use the candidate set generation-and-test approach. FP-tree based mining [5] is an exception which does not need to generate candidate set. The algorithm Predictive Item Pruning (PIP) [7] is an improved version of FP-tree. It has performance improvements over FP-tree since it prunes infrequent nodes. However, PIP-based mining uses a complicated data structure and performance gains are sensitive to the support threshold.

The PD algorithm [4] generates all frequent sets. It significantly reduces the dataset in each pass by reducing the number of transactions and their size to give better performance. Counting time is clearly less in a reduced dataset. In addition, the algorithm does not need to generate candidate sets; the reduced dataset contains only itemsets whose subsets are all frequent.

Intuitively, a transaction that contains infrequent itemsets can be decomposed to smaller itemsets since together they do not meet the minimum support threshold. The smaller itemsets after separations are often identical with others and can be combined, thus reducing the size of the dataset.

In this paper, major algorithms for frequent pattern generation - DCP, PIP and PD are discussed briefly, their comparisons are made and they show that the PD algorithm outperforms DCP by one order of magnitude and is faster than PIP. Further, PD is also more scalable than the DCP and PIP.

This paper is organized as follows: Section 2 presents the three algorithms we are comparing - DCP, PIP, PD, respectively. The comparisons among these algorithms are described in details in Section 3 with some empirical experimental results. Section 4 concludes the paper.

2. Frequent Pattern and Association Rules  Generally we ask for all association rules that have a specified mllllmum support and mllllmum confidence, and various algorithms have been developed for finding such rules efficiently. These algorithms first find Frequent Itemsets. Next, they generate candidate rules by partitioning each frequent itemset into two sets, LHS and RHS. For each candidate rule, the algorithms compute the confidence for the rule, and retain rules with confidence greater than minimum confidence. The expensive step is the identification of frequent itemsets, and this is where algorithms for finding association rules mostly differ [12].

Here we show three algorithms, which use different concepts for identifying frequent itemsets. Since 'association rule generation' is a trivial step in finding association rules, it is not covered in this discussion.

Focus is on the efficiency of frequent itemsets generation techniques.

2.1. Direct Count of candidates & Pruned transactions (DCP) Algorithm  Apriori [1] is one of the most popular Frequent Set Counting (FSC) algorithms. Although a number of other solutions have been proposed, it is still the most commonly recognized reference to evaluate FSC  algorithm performances. Apriori iteratively searches frequent itemsets: at each iteration k, Fk, the set of all the frequent itemsets of k items (k-itemsets), is identified. In order to generate Fk, a candidate set Ck of potentially frequent itemsets is first built. By construction, Ck is a superset of Fk, and thus to discover frequent k-itemsets the support of all candidate sets is computed by scanning the entire transaction database D. All the candidates with minimum support are then included in Fk, and the next iteration started. The algorithm terminates when Fk becomes empty, i.e., when no frequent set of k or more items is present in the database.

The new algorithm proposed in [6] is called DCP (Direct Count of candidates & Pruned transactions).

The algorithm significantly enhances the Apriori family of algorithms, and is aimed at solving the issues state above for frequent itemsets of limited length.

DCP exploits an innovative method for storing candidate itemsets and counting their support. The method is a generalization of the Direct Count technique used by Apriori for counting the support of unary itemsets, and allows the cost of the initial iterations of the algorithm to be reduced considerably.

Moreover, DCP adopts a simple and effective pruning of D, without using the complex hash filter used by DHP [8].

As an example of the heuristics used to prune D, consider that the items that are not present in any itemset of Fk are not useful for the subsequent steps of the algorithm, and can thus be removed from D.

Similarly, transactions with less than k items can also be removed from D, since they cannot contain any k? itemset. In DCP a pruned dataset Dk+l is thus written to the disk at each iteration k of the algorithm, and employed at the next iteration.

At each iteration k, DCP builds Ck on the basis of Fk-J. In this construction we exploit the lexicographic order of Fk-J to find pairs of (k-l)-itemsets sharing a common (k-2)-prefix. Due to this order, the various pairs occur in close positions within the vector storing Fk-J? The union of each pair is a k-itemset that becomes a candidate C E Ck only if all its subsets are included in Fk-J? Also in this case we can exploit the lexicographic order of Fk-], thus checking whether all the subsets of C are included in Fk-J in logarithmic time.

Pseudo code of DCP: The first iteration of DCP, during which the  database is scanned to find frequent items, is the same as the Apriori one. FJ is optimally built by counting all the occurrences of each item i E {I , ... ,m} in every tE D.

Pseudo code of the second iteration of DCP is given in the next page:    1: globat counter( G I , FI) 2: k<:-- 2  3: m2 +--- IFII  4: for all i E [1, m2 ] do  5: COUNTS[i] +--- 0 6: end for 7:D3 +---?  8: for all tE D do 9: t = globalyruning(t , GJ , 2)  10: if It I ? 2 then  11: for all {til ,ti2} c t 11 :0; i1 < i2 :0; I t I do  12: I',. = 1',.2 (til , ti2) 13: COUNTS[ 1',.] +--- COUNTS[ 1',.] + 1 14: end for 15: end if  16: ifltl ?3 then  17: D3+--- D3 ut  18: end if 19: end for 20: F2 = {cl, c2 E C2 I COUNTS[1',.2 (cl, c2)] ? min_sup}  2I: k+--- 3  Pseudo code of a generic iteration of DCP for k ? 3 is given below:  1: while Fk-1 01-? do  2: global_ counter( Gk-1, Fk-1)  3: C k = apriori_gen( Fk-1 )  4: if Ck =? then  5: return 6: end if 7: PREFIXk[ ] = init_candidates( k, Ck)  8: Dk+l +---?  9: for all t E Dkdo  10: t = global yruning( t, Ck-1, k )  11 : if t ? k then 12:  13:  14:  Initialize local counters Lk [ ]  POS[ ] =inityositions( t )  for all {til ,ti2} <;;; t 11 :0; i1 < i2 :0; I t I- k + 2 do 15: I',. = I',.k(til, ti2)  16: start = PREFIXk[ I',. ] 17: end= PREFIXk[I',.+I] - I 18:  count_candidates(1 t I, k, CbPOS, start, end, Lk) 19: end for  20: t = localyruning(i, Lk) 21: if 1/)2 (k+ 1) then  " 22: Dk+l +--- Dk+J U t 23: end if 24: end if 25: end for  26: Fk = {c E C k I c.COUNTS ? min_ sup}  27: k+--- k+ I  28: end while  D N M  T ti  Fk Ck C Ci  Dk  Mk  Th d h  h hull e terms use ere ave t e o transaction database number of transactions in D  owmg meamngs:  number of items appearing in the n transactions of D a generic transaction in D an item identifier appearing at position i in transaction t set of frequent k-itemsets set of candidate k-itemsets a generic candidate itemset belonging to Ck an item appearing at position i in the candidate itemset c pruned transaction database read at iteration k (DJ =D) set of the significant items appearing in the transaction of Dk  mk cardinality of Mk  2.2. Predictive Item Pruning (PIP) FP-Tree Algorithm  PIP [7] is a new concept which is a learning process of item effectiveness in the FP-tree [5] and can be pruned from the latter transactions in the tree building process. That is, PIP FP-tree algorithm prunes infrequent items which have been counted in FP-tree algorithm. Through simulation, it has been proved that PIP performs better than the FP-Tree algorithm.

To determine the effective frequencies of a node in the tree, correlation of the items is required to be known. To determine the correlation, a new data structure for each item in the database has been introduced and will be referred as item correlation node. Status of the items in the correlation node will determine a node to be frequent and also determine whether the node will be inserted in the tree or not.

Correlation Node: This node consisting of  ? Item name, ? Remaining frequency and ? Related items already inserted in the tree  Item header table of the FP-Tree algorithm is hence modified accordingly. Node of a item header table consists of item name and head of the link which    points to the fIrst node of the FP-Tree carrying the item name. So the Correlation Node (CN) will have the following information: ? Item name ? Remaining frequency ? Related items and their number of occurrences  with that item of a transaction already inserted in the tree  ? Head of the link to the fIrst node.

Initially there will be one CN per items in the 1-  frequent itemset. Initial remaining frequency value will be total frequency count value and head link will point to null.

Item Pruning: There are two conditions for pruning an item of a  transaction: Condition 1: While inserting a node in the tree check the remaining frequency. If the remaining frequency value is less than minimum support proceed for the next condition.

Condition 2: For an item in a transaction, check the occurrence of the other items in the related items fIeld of the CN. If no other items of that transaction are found in the list then the item can be pruned and will not be considered for insertion.

Algorithm (Pseudo code representation): Scan the Database for Frequency Count 1. Extract I-Frequent Itemset according to the  minimum support and sort the items in ascending order.

2. Create Correlation Node for each item in 1- Frequent Itemset.

3. Create the root of The Tree, T, and label it as null.

4. For each transaction t in the database do the  following 1. select one transaction tselected ii. For each Items in the selected transaction  Tselected do the following a. select one item Iselected according to  Frequency Count b. if Prun _ Item( tselected, Iselected ) returns false  InsertN ode(Iselected) /******* Prun Item Function *********/ bool Prun _ Item( tselected , Ise1ected){ If Iselected ->remaining frequency >minimum support  then Return false Else  For each item in the transaction tse1ected Check the list of related items  If Count_ oCrelated _Item + Iselected->remaining frequency <minimum support  Then Return true Else Return false  } /************* Insert function ****************/ InsertN ode (Ise1ected) { Check if T has a child C  Where Iselected.item-name=C.item-name then increase Count of the Node form Iselected by 1  Else{ Search appropriate Location for the Node Create A New Node As Node.item-name=Iselected.item-name  } Decrease remaining frequency for Iselected by 1 Increase count of all items of the transaction T's in Count of related Item.

}  2.3. Pattern Decomposition Algorithm  The central idea of this approach is to signifIcantly reduce the dataset size for improving performance.

This algorithm shrinks the dataset when new infrequent item sets are discovered. More specifIcally, the PD algorithm uses a bottom-up search to fInd frequent sets.

It consists a set of passes starting from pass 1 for a given transaction dataset D1? Each pass of the PD algorithm, say pass k and Db has two phases. First, frequent itemsets Lk and ?Lk are generated by counting for all k-itemsets in Dk. Second, PD-decompose algorithm is used to decompose Dk to get DB! such that DB! contain no itemset in ?Lk. The algorithm terminates at a pass z if Dz is empty.

Let us illustrate how a pattern in the dataset is decomposed on a specifIc pass, see Figure 1 (in the next page): Pass 1: Suppose we are given a pattern p = abedef: 1 E D! where a, b, e, d, e EL! andf ? L!. To decompose  p with ?L], we simply deleteffromp, leaving us with a new pattern abede : 1 in D2? Pass 2: Suppose a pattern p = abede : 1 E D2 and ae ? L2? Since ae cannot occur in a future frequent set, we decompose p = abede : 1 to a composite pattern q = abed, bede : 1 by removing a and e respectively from  p.

Pass 3: Suppose a patternp = abed, bede : 1 E D3 and aed ? L3? Since aed . abed, abed is decomposed into abc, abd, bed. Their sizes are less than 4, so they are not qualifIed for D4? Itemset bede does not contain aed, so it remains the same and is included in D4.

In /3, we remove patterns if their sizes are smaller than the required size of the next dataset. Here, patterns abc and abd with sizes of 3 cannot be in D4 and are deleted.

In 6, when a part of a given pattern has the same itemset with another pattern after decomposition, we    DJ !. abc del,' } f '. ::1 b c [,t: lH?? ?. abdh: )  !

!

4. t.1 1': t .. ?! ?:-:. ? 5, ?:7 be:  } }  /lede' f -l'::I fS Oce  {(Jb/ /ac/ 3  Ll -1,1 {?"IU/ ?

is lico JS Vee /,?1C/ 4  4 ii' {IYl/ J ?? ly J 'C ( t?.f j ? /f,/ 4 ill/ 1 J {(/"/ 3 Iii} } " {f!/ 2 ]  ? .1 !. ahed, !>cde: "  C1 b L< ? ? ? a b d: 'lit,  4: b c (J e?  L3 -1?3  j "  L.

)  !?' OCt is OCt': 3 ;'acf..f/ 1  {al)cl/ ;] ,  '1  {c.,:i:",I 'J  l. -l.

J  is OC:c Ii-..' Occ /bC":.le}2  Figure 1. Pattern decomposition example  combine them by summing their occurrence. Here, bcde is the itemset of pattern 4 and part of pattern l's itemset after decomposition, so the final pattern is bcde: 2 in D4.

Notably, the algorithm first counts for Lk and ?Lk and then decomposes patterns in each pass. It differs fundamentally from previous algorithms in that it avoids candidate set generation and reduces the dataset on each pass. Counting time is thus also reduced.

The PD-decompose Algorithm: Input: An itemset s, its infrequent k-item set ?qk Output: Itemsets, the decomposition results of s PD-decompose(itemset s, ?qk) 1: if(k=l ) 2: t = remove items in ?qk from s 3: else { 4: build ordered frequency tree r; 5: Sbs = Quick-split( r ); 6: t = mapping Sbs to itemsets; } 7: return t  The main objective of PD-decompose is to decompose an itemset s by its infrequent k itemsets. It consists mainly of two parts: 1) building the frequency tree, 2) splitting itemsets using the tree via a method called Quick-split and returning the resulting itemsets.

A frequency tree is a tree whose nodes are items. In the tree, items at each level are ordered by the frequency of their occurrence at the level. The most frequent item at each level is placed first. For example, suppose we are given a pattern p E D3 where p.IS = abcdefgh. In the third pass, we find infrequent 3-  itemsets {aef, aeg, aeh, afg, ajh, agh, abe, abf, abg, abh, ace, acf, acg, ach, ade, adf, adg, adh}. First, we build up a frequency tree, as shown in Figure 2. The first level consists of only a's. The second level consists of items e, I, g, and h, with e occurring the most at its level. The third level is constructed In similar fashion.

a e b  b G  f d  O??Et c f d  ?tb ? f} g ? h  c h d h  Figure 2. A frequency tree example  After we built a frequency tree, we then use the Quick-split technique to calculate the maximal frequent sets.

The Quick-split algorithm is given here: Input: A frequency tree r Output: An array of BitSets representing Itemsets Quick-split(Tree r) 1: if(r is leaf) return ? ;  2: for all x E r.subs do 3: subres[x] = Quick-split(x) u newBS(?x) 4: result =newBSO; 5: for all x E r.subs do 6: result = result & subres[x];    7: remove b E result, b.size:::; k 8: return result  To speed up calculation, an itemset is represented by a bitset with 0 and I for specifying the absence or presence of an item at a corresponding position respectively. The Quick-split performs a calculation on a frequency tree and returns an array of bitsets, which represent a group of decomposed itemsets. Splitting is accomplished by calculating bitset results in a bottom? up fashion in the tree. In the above example, we have 8 items a, b, . . .  , h corresponding to positions 0-7 in a 8- bit bitset. So p.JS = abcdefgh = {I III I Ill}; abcd = {ll1 10000}; bcdefgh = {OllillII}. The size of the bitset is the number of items in p.JS which is usually much smaller than the total item size in the dataset.

For the frequency tree in Figure 2, for the itemset abcdefgh and infrequent 3-itemsets {aef, aeg, aeh, afg, ajh, agh, abe, abf, abg, abh, ace, acf, acg, ach, ade, adf, adg, adh} , Quick-split returns the possible maximal frequent sets {abcd, bcdefgh}.

The PD Algorithm: Input: transaction dataset T Output: frequent patterns PD ( transaction-set T ) I : D 1 = {<t, 1>1 t E T}; k= I ; 2: while (Dk "* ? ) do begin 3: for all p E Dk do II counting 4: for all k-itemset s c;;; p.JS do 5: Sup(s IDk) += p.Occ; 6: decide Lk and ?Lk ;  Ilbuild Dk+] 7: Dk+]= PD-rebuild(Dk, Lk , ?Lk); 8: k++; 9: end 10: Answer = u Lk  As shown above, PD is the top-level function that accepts a transaction dataset as its input and returns the union of all frequent sets as the result. At the kth pass, steps 3-6 count for every k itemset of each pattern in Dk and then determine the frequent and infrequent sets, Lk and ? Lk; step 7 uses Dk , Lk and ?Lk to rebuild Dk+!.

PD stops when Dk is empty.

The PD-rebuild Algorithm: Input: Dataset Db frequent Lk, infrequent ?Lk Output: Dataset DB!

PD-rebuild (Dlo Llo ?Lk) 1: Dk+! = ? ; ht = an empty hash table;  2: for all p E Dk do begin 3: / / q k, ?q k can be taken from previous counting  qk={sls E p. JS nLk}; ?qk={tltE p. JS n ?Lk} 4: u = PD-decompose(p.JS, ?qk); 5: v ={s E ul s is k-item independent in u}  6: add <u-v, p.Occ> to Dk+1; 7: for all s E v do 8: if sin ht then ht. s. Occ += p. Occ; 9: else put <S,p.Occ> to ht; 10: end 11: Dk+] = Dk+] u {p E ht};  The PD-rebuild shown above is to determine DB!

by Dk, Lk and ?Lk' For each pattern p in Db step 3 computes its qk and ?qk; step 4 calls PD-decompose algorithm to decompose p by ?qk. Note that qk is not used here for decomposing p. In steps 5 to 9, we use pattern separation rule to separate p. In steps 7 to 9, PD-rebuild merges the patterns separated from p with their identical ones via a hash table ht. Since PD follows the pattern decomposition rule to decompose patterns and the pattern separation rule for merging identical patterns that yield same support, the answers generated by PD are correct.

3. Comparative Study  Our experiments were performed on a 600 MHz Pentium PC machine with 256 MB main memory, running on Microsoft Windows XP. All three algorithms were written in C++ language. The test dataset was generated in the same fashion as the IBM Quest project . We used dataset T25.I10.DIOOK . In the dataset, the number of items N was set to 1000. The corruption level for a seed large itemset was fixed, obtained from a normal distribution with mean 0.5 and variance 0.1. In the dataset, half of all items were corruptible. In the dataset, the average transaction size ITI and average maximal potentially frequent itemset size III are set to 25 and 10, respectively, while the number of transactions IDI in the dataset is set to lOOK.

3.1. Comparison of PD with DCP  ? Pattern Decomposition algorithm does not need to generate candidate sets which are the main improvement on DCP; the reduced dataset contains only itemsets whose subsets are all frequent.

? PD generates all frequent sets whether it is not certain in DCP. PD significantly reduces the dataset in each pass by reducing the number of transactions and their size to give better performance. So counting time is clearly less than DCP in a reduced dataset.

? PD is much more scalable than the DCP.

8000.----------------------.

6000 \   <.i \  ?OOO \ E f= \ ?OOO ?   Minimum Support(%)  ",PD ?"'? DCP  Figure 3. Performance comparison  8.-------------------,     ",PD ?"'? DCP  owa_L??_L??_L??_L??_L??_L?? o 50 100 150 200 250 Number of Transactions(k)  Figure 4. Scalability comparison  Figure 3 shows the execution times for different minimum support. We can see that PD is about 15 times faster than DCP with minimal support at 2% and about 8 times faster than DCP at 0.25%. In Figure 4, to test the scalability with the number of transactions, experiments on dataset D are used. The support threshold is set to 0.75%.

3.2. Comparison of PD with PIP  ? Predictive item pruning FP-tree algorithm has a complicated data structure in comparison to PD algorithm.

? PD works by decomposing transactions into short itemsets. Then regular patterns are combined together. Hence data set is being reduced at every pass. As a result, space efficiency is improved. On  the other hand, PIP FP-tree, although prunes infrequent items, its hit ratio is not as efficient as PD. So, obviously its space efficiency is not better than PD.

? As PD always concerns with the current data set, less time is required in comparison to PIP FP-tree algorithm.

? PD is more scalable than the PIP.

100r------------------?  " E F  ?   ?40 0::   ",PD ??? PIP  Minimum Support(%)  Figure 5. Performance comparison  500 r--------------------,   300 " E F ? ?oo 0::   ?  ? /  Number of Transactions(k)  / /  ",PD ??? PIP  / /  Figure 6. Scalability comparison   In Figure 5, both PIP and PD have good performance on D. But PIP takes substantially more time when minimum support in the range from 0.6% to 2%. When minimum support is less than 0.6%, the number of frequent patterns increased quickly and thus the execution times are comparable. In Figure 6, we compared the scalability of PD with PIP on the dataset D with minimum support = 0.75%. PD was clearly more scalable than that of PIP.

4. Conclusion  The programs were implemented in C++ very efficiently. Since really large datasets or data warehouse was not available for us, we run the programs of these three algorithms by using test datasets. So performance comparisons are not the absolute values. The results can vary on other computers. But it can be guaranteed that performance ratio of the algorithms will remain the same.

After making the comparisons with sample data, we came to the conclusion that PD algorithm performs significantly better than the other two especially with larger datasets. PD outperforms DCP and PIP regarding running time. On the other hand, since PD reduces the dataset, mining time does not necessary increase as the number of transactions increases and experiments reveals that PD has better scalability than DCP and PIP. So, PD has the ability to handle the large data mine in practical field like market basket analysis and medical report documents mining.

5. References  [1] R. Agrawal and R. Srikant, "Fast algoritlnns for mining association rules", VLDB'94, pp. 487-499.

[2] R. J. Bayardo, "Efficiently mining long patterns from databases", SIGMOD'98, pp.85-93.

[3] J. Pei, J. Han, and R. Mao, "CLOSET: An Efficient Algorithm for Mining Frequent Closed Itemsets (PDF)", Proc. 2000 ACM-SIGMOD International Workshop on Data Mining and Knowledge Discovery, Dallas, TX, May 2000.

[4] Qinghua Zou, Henry Chiu, Wesley Chu, David Johnson, "Using Pattern Decomposition( PD) Methods for Finding All Frequent Patterns in Large Datasets", Computer Science Department University of California - Los Angeles.

[5] J. Han, J. Pei, and Y. Yin, "Mining Frequent Patterns without Candidate Generation (PDF)", Proc. 2000 ACM? SIGMOD International Con! on Management of Data (SIGMOD'OOj, Dallas, TX, May 2000.

[6] S. Orlando, P. Palmerini, and R. Perego, "The DCP  algoritlnn for Frequent Set Counting", Technical Report CS- 2001-7, Dip. di Informatica, Universita di Venezia, 2001.Available at http://www.dsi.unive.itl?orlando/TR01- 7.pdf  [7] MD. Mamun-Or-Rashid, MD.Rezaul Karim, "Predictive item pruning FP-tree algoritlnn", The Dhaka University? Journal of Science, VOL. 52, NO. 1, October,2003, pp. 39- 46.

[8] Park, J. S., Chen, M.-S., and Yu, P. S, "An Effective Hash Based Algoritlnn for Mining Association Rules", Proc.

ofthe 1995 ACM-SIGMOD Con! on Management of Data, 175-186.

[9] Brin, S., Motwani, R., Ullman, J., and Tsur, S, "Dynamic Itemset Counting and Implication Rules for Market Basket Data", In Proc. of the 1997 ACM-SIGMOD Conf On Management of Data, 255-264.

[10] Zaki, M. J., Parthasarathy, S., Ogihara, M., and Li, W, "New Algoritlnns for Fast Discovery of Association Rules", In Proc. of the Third Int'l Con! on Knowledge Discovery in Databases and Data Mining, 283-286.

[11] Lin, D.-I and Kedem, Z. M., "Pincer-Search: A New Algoritlnn for Discovering the Maximum Frequent Set", In Proc. of the Sixth European Conf on Extending DatabaseTechnology, 1998.

[12] R. Ramakrishnan, Database Management Systems, University of Wisconsin, Madison, WI, USA; International Edition 1998.

