

Abstract- Mining the frequent itemsets are still one of the data mining research challenges. Frequent itemsets generation produce extremely large numbers of generated itemsets that make the algorithms inefficient. The reason is that the most traditional approaches adopt an iterative strategy to discover the itemsets, that?s require very large process.

Furthermore, the present mining algorithms cannot perform efficiently due to high and repeatedly database scan. In this paper we introduce a new binary-based Semi-Apriori technique that efficiently discovers the frequent itemsets.

Extensive experiments had been carried out using the new technique, compared to the existing Apriori algorithms, a tentative result reveal that our technique outperforms Apriori algorithm in terms of execution time.

Index  Terms?Data  mining,  Frequent  itemset,  Association Rules, Support, Confidence

I.  INTRODUCTION Data mining is an accumulation of techniques which can be used to efficiently automate discovery of previously unknown, novel, valid, valuable and understandable patterns in large- scale databases [1, 2]. The patterns should be actionable so they could be applied in enterprise organizations to support decision making. One of the attractions of data mining is that can help us to analyze a huge datasets in an acceptable time scale. Data mining can be additionally well suited for complex problems involving relatively small amounts of data but where you can find many fields or variables to investigate. However, for small, simple data analysis problems there might be simpler, cheaper and much more effective solutions.

Recently, data mining is a vital aspect for generating association rules, given a set of large number of itemsets. Association rule mining is a technique for discovering interesting relations between variables within the large databases [3]. It's considered as one of the important tasks of data mining intended toward decision making.

The aim of data mining process is to discover probably the most relevant and interesting patterns and trends from the given data  Rohiza Ahmad, Baharum B. Baharudin Department of Computer and Information Sciences Universiti Teknologi PETRONAS Bandar Seri Iskandar, 31750 Tronoh, Perak, Malaysia { Rohiza_ahmad, Baharbh}@petronas.com.my  repository [4, 5]. It becomes an important mechanism utilized for modern business to transfer data into intelligence prediction giving an informational advantage.

Association rule mining is often a popular research methods used to discover the relation between a set of items in large databases [6]. It is a helpful criteria that can be used to support the decision making. In order to find out the association rules between a collections of items, Agrawal et al. [7], introduced and developed a well-known algorithm; named Apriori, Apriori is useful to handle association rule mining based problem.

Currently association rule, knowledge discovery, and frequent pattern have become invaluable in domains such as, health care [8-10],   decision   support   system[9],   telecommunication networks [11], crime investigation [12], log file analysis [13], Intrusion detection[14], Risk management[15], etc. However, before using any data mining techniques in order to gather information?s from a huge dataset, data has to be ready to ensure the sufficient of obtained information.

Association rule can be written in expression as an implication of X ? Y, where X and Y are items of itemset I. where X? I, Y?I, and X?Y =?. The expression means that if a transactions T contains the items in X, it also tends to contain the items in Y. An illustration of such a rule might be that 60% out of the total transactions that contain milk also contains sugar; 40% of all transactions contain the two items together".

Here 60% will be known as confidence of the rule, and 40% will be known as support of the rule. Mining association rules from a set of items idea originates from the data analysis of market-basket, where will be the interest in mining association rules for describing customer?s interest in buying product items.



II. PROBLEM BACKGROUND The association rules problem normally can be classified into two sub problems. The first problem is to discover the itemsets that their occurrences go beyond a predefined minimum threshold [16]; if the itemsets pass this condition, then they will be known as large or frequent items. The second problem is to use those large frequent items to generate association rules with the constraints of minimal confidence. The two parameters use in finding the frequent itemset and association rules are support and confidence.  In this paper we consider the problem of finding the frequent itemsets because it is computationally expensive.



III. LITERATURE REVIEW From review of previous works, most of the traditional association rules mining algorithms [15-17] are found to be still suffering and have drawbacks in terms of efficiency; and scalability. Many research efforts are applicable to this paper, including a well-known Apriori and its variations. In Apriori, breadth search and bottom up approaches are applied. Apriori enumerates all frequent items; and Apriori based algorithms tend to achieve high efficiency; when the database transactions are scarce. In other words, even if there are thousands of items available in the database, only a few of them are accessed in transactions.

Apriori algorithm Apriori is a one of the first, famous, well known, and scalable algorithm for mining frequent itemsets and association rule.

Apriori in fact is progress of the two known methods, AIS and SETM.   Apriori   was   introduced   by   Agrawal   and Srikant [18], the algorithm idea is to search for large itemsets during its initial database pass and uses its result for the other large datasets during subsequent passes. Rules having a support level lower than the minimum support threshold are known as infrequent itemsets and those items which have support more than or equal to minimum support, will be known as large frequent itemsets. The algorithm was built on the concept that any subset of a large itemset is frequent if their superset is frequent [18]. The problem of Apriori algorithm is, it requires multiple database scan, and additionally generates many candidates. Bellow example illustrates the overall idea of Apriori algorithm.

Table (1) Sample content of database transactions TID ITEM USED 100 A , C , D  200 B , C , E 300 A , B , C , E 400 B , E                    Given five transactions as in table (1), each seen of transactions will omit items which fall below minimum support threshold.

as shown in fig (1) the first pass eliminates item D. The second pass combines two items and combinations that have support less than two will be omitted. This process continues until no more combination is possible and hence, the frequent itemset have been reaches.

Apriori algorithm [7], inherits the problem of multiple passes over the databases. That leads to unoptimal performance speed.

Thus, many new algorithms inspired their ideas from Apriori; they introduced many modifications and improvements. There are two main approaches used: the first is to reduce the total number of database passes, and the second is to explore different types of pruning techniques in order to make the number of candidate itemsets smaller. To date, these algorithms still can be further improved.



V. SEMI-APRIORI TECHNIQUE  In this part, we present our Semi-Apriori algorithm for mining frequent items which we believe that it outperforms Apriori algorithm. For the purpose of simplifying our discussion on the technique, in this paper we will highlight a database transaction that composed of items and products that are purchased together by customers who visited a supermarket. Earlier   Tabe1 (1) shows   sample   content   of   database transactions that is used in this study (Note: TID stands for transaction id and items used for transaction itemsets).

The transaction database contains only four transactions and five items; each transaction in the given table shows the purchase of one customer. The given items can be represented as a binary item list by giving 1 to the present item and 0 for the rest.

Table (2) shows this concept.

Figure (1) Apriori Frequent item mining steps        Table (2) Transaction items in a binary representation  TID A           B            C D E 100 1 0 1 1 0 200 0 1 1 0 1 300 1 1 1 0 1 400 0 1 0 0 1   Instead of searching the database looking for the occurrences of each item individually; we take a whole transaction and increment the frequency of each item appeared in the transaction by one in a vertical processing.

Table (3) below reveals all actual combinations that occur within the transactions given in table (2). For example, in transaction T100, only three items are non-zero which are ACD.

Thus, the combination of T100 will be A alone, C alone, D alone, AC, AD, CD, and ACD.

The Semi-Apriori algorithm for mining frequent items is divided into three stages. The first stage starts by finding the 1-itemsets L1 and pruning all items that have support less than the given minimum support threshold. This step is similar to the step used in Apriori [7] and FP-Growth algorithms [19]. The first stage output can be shown in table (4) and the algorithm in figure (2)   Table (4) First frequent1-itemset A B C E 2 3 3 3   Input: Dataset D, minimum support threshold S Output: frequent 1-items (FI1) for each binary transaction T  Increase the frequency, in freq array, for each item in T hat contains 1  end for for each itemi in freq array if freq(itemi)?minSup  add itemi to L1 end if  end for  Figure (2) First frequent (1-itemset) algorithm   In the second stage, the algorithm figure (3) applies self-joint of L1 using L1?L1, also using the support measure. The items which are below the minimum support will be pruned. The second stage output can be shown in table (5) and the algorithm in figure (3).

Table (5) Second frequent 2-itemset AC BC BE CE 2  2  3  2  Input: first frequent 1-itemset Output: frequent 2-items (FI2) for i=1 to length(L1)  for j=i+1 to length(L1) calculate sup (itemi, itemj) using eq. (1) if Sup (itemi, itemj)?minSup  add itemi to L2 end if calculate Sup (itemi, itemj) if Sup (itemi, itemj)?minSup  add itemj to L2 end if  end for end for  Figure (3) second frequent (2-itemset) algorithm  Table (3) below reveals all actual combinations that occur within the transactions given in table (2). For example, in transaction T100, only three items are non-zero which are ACD. Thus, the combination of T100 will be A alone, C alone, D alone, AC, AD, CD, and ACD.

Tables (3) actual combinations   TID   Items Combinations 100    A, C, D,   AC,   AD,   CD,   ACD 200    B, C, E ,  BC,   BE,    CE,    BCE 300    A,B,C,E,AB, AC, AE, BC, BE, CE, ABCE 400    B, E, BE    In the third stage, the frequent itemsets of size > 2 are generated.

The algorithm in this part starts by reading all transactions from the database. For each transaction, the algorithm selects the items in the transaction that appears in 2-itemsets and add them to the local candidate set CS. The algorithm then proceeds to generates all the subsets of CS. For each generated subset, the algorithm calculates its binary map. After getting the map the algorithm looks for the map in the frequency table FT. If the map is already available in FT then the algorithm updates the frequency of this map increasing it by one. If the map doesn?t exist then it?s appended to FT and its frequency is set to 1. In order to avoid recalculation of the subsets, the algorithm firstly check whether the binary map of CS appears before or not. If the map appeared before then the algorithm go directly for updating the frequency of all binary maps that corresponds to the subsets of CS. This last part of avoiding recalculations of subsets is not shown in the algorithm because of space limitations.

Once this process finished, the algorithm traces all the frequencies in the FT table to get frequent itemsets of size >2.

For each frequency that is having a value which is greater than threshold value of minSup the algorithm generates the reverse map of this frequency to get its constituent items back. These items represent the frequent mined items. The third stage output can be shown as in figure 4.

Figure (6) Execution time using Mushroom Dataset       Input: Dataset D, FI, minimum support threshold S Output: Frequent Itemsets of size >2  While not eof(DS) CS= {}  Read T from DS For All items(T) do  If itemi(T) ? L2 Add itemi(T) to CS End if  End for For each subset ss in CS Calculate m = map(ss)  If frequency table FT contains m Increase the frequency of m by 1 Else  Add m to frequency table FT Set FT(m) = 1  End for End while For each row r in FT if frequency(r) ?minSup  Get mapInv=map-1(r) Selects the items that corresponds to the 1s in mapInv from list of itemset.

end if end for    Figure (4) Generating itemsets of size > 2 algorithm  and 2 GB of RAM computer. Graph 1 and graph 2 in figure (5) and figure (6) respectively shows the comparison of the execution time for discovering the frequent itemsets using synthetic dataset T40I10D100K provided by the QUEST generator from IBM?s Almaden lab, and the real dataset, mushroom, that?s publicly available in the FIMI dataset repository. The two datasets are having different transaction size, item size, and other behaviors. The graph x axis has support level 0.04 to 0.09 for the first graph, and 0.05 to 0.3 for the second graph respectively, and y axis are carrying an execution time in milliseconds. This experiment considers the minimum support to count execution time.

Through analyzing the experiments it?s shown that when the minimum support is low the execution time increase and vice versa, in some cases a low measure support itemset is unlikely interesting from a business perspective when promoting the items that customer seldom bought together using   the   market   basket   analysis   concept.   From   this experimental study, the proposed semi-Apriori algorithm has shown better performance as compared to Apriori. For the both datasets, the execution time differs greatly between the original Apriori and semi-Apriori algorithms, when the minimum support is low. This is good since in data mining, scalability of the algorithms depends on ability to handle small minimum support.

Table (6) synthetic and real dataset's properties

VI. EXPERIMENTS AND RESULTS To compare performance of our algorithm with Apriori, experiments were conducted on Intel? corei5? CPU, 2.4 GHz,  Dataset Transactions T40I10D100K 100,000 Mushroom  8,124  Items Type AvgLength 1000 Sparse 40 119  Dense  23                Figure (5) Execution time using T40I10D100K Dataset          0.3 0.25 0.2 0.15 0.1 0.05  Ex ec  ut io  n tim  e( M  S)  Support %  Apriori Semi-Apriori    0.09 0.08 0.07 0.06 0.05 0.04  Ex ec  ut io  n tim  e( M  S)  Support %  Apriori Semi-Apriori

VII.    CONCLUSION Mining frequent pattern is a challenge in data mining. This paper introduced a new technique that efficiently discovers the frequent itemsets. The proposed technique avoids repeated database scans, also the algorithm avoids the cost of generating large number of candidate sets; hence, minimize the execution time. Extensive experiments have been carried out using the new technique, the algorithm was also compared to Apriori algorithm, the result reveal that our technique outperforms Apriori algorithm in terms of execution time to discover the frequent itemsets.

