Mining Method of Attribute Reduction Based on  Rough Fuzzy Set Theory

Abstract?In order to solve the reasoning and decision problems  when the given information is not sufficient, a mining method for  attribute reduction of classification rules based on Rough Fuzzy Set  theory is proposed, and its flow chart is also presented. The method  can mine hidden association rules in samples and make decisions by  using the reduction algorithm of characteristic attribute based on  rough set and induction algorithm of decision rules based on fuzzy  set. Finally, information system of satellite navigation system combat  effectiveness evaluation is applied with this approach, and the results  are satisfactory, which have proved that the approach is practicable  and effective.

Keywords-Rough Fuzzy Set; association rule; attribute reduction;  rule induction; effectiveness evaulation

I. INTRODUCTION  Rough set theory contact knowledge and partition from a new  perspective, which provides a new mathematical tool for dealing  with incomplete information, it has the advantage that knowledge  extraction is from data-driven completely and need not special  assumptions [1]. Fuzzy set theory contacts precision and fuzziness  from another new perspective, which provide a new mathematical  tool for dealing with uncertain information. It makes use of the  membership function to described fuzzy concept and effectively  solves the fuzzy boundary issue of the rough set [2]. Rough Fuzzy  Set theory combines rough set theory and fuzzy set theory  together, which uses rough set to deal with the rough concept, and  uses fuzzy set to propose the fuzzy concept. The combination of  two could be effective to deal with the data?s inconsistency,  inaccuracy, uncertainty and imperfect [3]. A mining method of  classification rules based on Rough-Fuzzy Set theory is provided  in this paper, and its basic steps: select data sources; use rough set  theory to make attribute reduction; make use of fuzzy set theory to  summarize the rules, and put the candidate rule set gotten in  inference engine to resolve the rule conflict and generate final  decision rule sets, and decision classification is used for it.



II. ATTRIBUTE REDUCTION BASED ON ROUGH SET THEORY  A. Correlative Concept  Information system is a four-element  group fVAUS ,,,= , where { }nxxxU ,,, 21= is a  finite object set, called as Universe; A is a finite characteristic  attribute set and DCA ?= , ?=DC ? ; C , D is condition  and decision attribute set respectively; ? Aa  aVV ?  = , aV is value  range of attribute a ; f denotesinformation function, VAUf ??: , for Ux ?? , Aa ? , there  is ( ) aVaxf ?, .

Let there is information system S , ( )xa notes the value in  attribute a , ( )xD  notes the value in attribute D of x , ijC   DOI 10.1109/CSSE.2008.1113     denotes the i th line, j th column element of discernibility matrix, then discernibility matrix are defined as follows:  ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )  ( ) ( ) ?? ? ?  ?  ? ?  ? ?  ?  = ?= ??  =  ji  jiji  jiji  ij  xDxD xDxDxaxa xDxDxaxaa  C 0 ?  ?   The conformation method of discernibility function: make  each attribute of ijC  "or", and then make all the ijC "and",  where Uji ,,1, = , denotes as P .

All the reduction of data set can be gotten by constructing the  discernibility matrix and reducing discernibility function which is  derived from discernibility matrix, after reducing discernibility  function to standard formula by the use of the absorption rule, all  of the attributes included in accumulation formula is all the  attribute reduction sets of information system[4].

B. ReductionAlgorithm Description Based on Discernibility  Matrix  Calculate and generate discernibility matrix, and regard the  core attribute as the ultimate attribute reduction set  ( ){ }UjiCCardCCored ijij ,,2,1,,1:Re ==== Find out all the attribute combinationsQ , which don?t contain the  core attribute and 0?ijC , from the discernibility matrix[5].

{ }UjiCCoreCCQ ijijij ,,2,1,,0,: ? =??== Denote all the combination of condition attributes which don?t  contain the core attribute as conjunction normal  form, ( )ijCP ??= , QCij ? .

Convert P into the disjunction normal form, and simplify it.

According to the requirement, different attribute combinations  are chosen as final attribute reduction set.



III. RULE INDUCTION BASED ON FUZZY SET THEORY  Through the characteristic reduction, the most simple rule set  of classification decision can be gotten, in which each group  reduction is a rule precondition, and they are the basis of rules  induction.

A. Correlative Concept  { }n???? ,,, 21= is used for denoting the ambiguity of n condition attributes, its value is defined according to the specific condition.

{ }{ }kxcyxD xcy ,,1: ??=== is used to denoting  the decision. Where xy denotes the value of decision attribute y , and k denotes the total number of samples.

Each condition attribute is corresponding to one fuzzy interval.

For n condition attributes, each attribute is denoted by using of  fuzzy set xf ( )nx ??1 , and then there will be? = n  i i f  kinds  of possible combinations, each kind of combination is denoted as a  fuzzy set vector. The fuzzy set vector under the condition of the  sample set support can be denote as rule condition (IF) mode of  generating a fuzzy rule.

Fuzzy set vector is obtained by the spanning process of tree. In  the spanning tree, the leaf node denote a kind of possible attribute  combination, the arc represents a condition attribute, and the  branch from root node to leaf node includes a condition of  candidate fuzzy rule.

The ambiguity of fuzzy rule identified by fuzzy set  vector >=< nP ??? ,,, 21 is defined as ( )xT P , the  formula is ( ) ( ) ( ) ( ){ }nnP xxxxT ??? ,,,min 221= ,  where nxxx ,,, 21 is the condition attribute value.

So, for each decision cy = , meet  ( ){ }cyjPP cy DjxTwwT == ??== : .

Set threshold value? , when a node?s combination ambiguity is less than? , then prune the node [6].

B. Rule Induction Method Based on Fuzzy Set  1) Spanning Process of Fuzzy Set Vector Tree. a) Data  Preparation: starting from the training sample, according to initial  state, establish the root node; b) Derivative nodes: starting from the  root node, add orderly a condition attribute from each node, and  get the node?s subsequent node; c) Calculation and pruning:  calculate combination ambiguity ? of each node, and keep the ?? > ?s node, and prune the ?? ? ?s node; d) Termination condition: when all the condition attributes are contained in the leaf  node, or when ambiguity of all the leafage nodes? former nodes is  less than or equal? , the tree?s spanning process will be end.

2)  The Diagram of Spanning Tree. The diagram of spanning  tree is shown in Figure 1, in which its parameter is  15.0,3 == ?n  2.0=? 4.0=?  6.0=?  2.0=? 5.0=?  8.0=? 1.0=? 3.0=?  5.0=?  pruning pruning  Figure 1.  The diagram of spanning tree

IV. INFERENCE ENGINE  Inference engine is actually a section of procedure, which is  used for solving the rule conflict existing in candidate rule set.

Let there are tCCC ,,, 21 conclusions totally in all the rules  of candidate rule sets, where the rules, which can get the  conclusion iC , have mRRR ,,, 21 , the definition of judgment  function: ( ) ( )?? ==  ?= m  j jj  m  j jiii ssCRF   // ?? , in  which js is the number of samples which support jR in  universeU , and j? is the rule ambiguity[7].

Let there are tCCC ,,, 21 conclusions totally in all the rules of candidate rule sets, and there are ZRRR ,,, 21 rules,  and when the rule?s front piece of iR , jR is the fuzzy set  vector P , and the rule?s back piece is respectively nm CC , , nm CC ? , the definition of judgment function:  ( ) ? ? ??  ? ?=  ??  ? ? ?  ??  ? ? ?  ?? ?  = ==  == PRF SS SS  PRF jP  Cy P  Cy  P Cy  P Cy  i nimi  nimi / 13.00 03.01  /

V. EXPERIMENT  A. Experiment Data  TABLE I. INFORMATION SYSTEM { } { }( )eDdcbaC == ,,,, U K a b c d e                                                         Take decision system of satellite navigation system combat  effectiveness evaluation as an example, its specific information  system is shown in TABLE?.

B. Experiment Result  According to the definition of discernibility matrix,  discernibility matrix can be constructed as shown in TABLE?.

Discernibility matrix ( ) ( ) ( )dcdcacaP ??????=     TABLE II. DISCERNIBILITY MATRIX ( ijC )  0 bc bcd 0 0 b 0 abcd  0 0 ac abc 0 abd abd  0 ab abcd bd 0 abcd  0 bd abcd 0 acd  0 ac 0 abcd  0 0 0 0 bcd  After simplification, reduction set { }da, , { }dca ,, can be  gotten finally[8]. Select the smallest reduction set{ }da, , and get TABLE ?. The fuzzy membership ? of condition attribute is defined as shown in TABLE ?.

TABLE III. INFORMATION SYSTEM AFTER REDUCTION  U K a d e  1,7                      TABLE IV. MEMBERSHIP DEGREE OF EACH CONDITION ATTRIBUTE  1, 7 2 3 4 5 6 8  0. 2 0. 4 0. 6 0. 4 0. 8 0. 6 0. 2  0. 5 0. 1 0. 2 0. 7 0. 8 0. 3 1  a  d   According to the fuzzy membership degree ? in table ?, Fuzzy Vector spanning tree can be constructed to derive a series of  nodes, and define 1.0=? . The candidate rule sets can be obtained, as shown in TABLE ?.

TABLE V. CANDIDATE RULE SET  Rule K Fuzzy degree  a=0, d=0?e=0 a=1, d=0?e=1 a=2, d=1?e=1 a=1, d=1?e=0 a=4, d=0?e=0 a=2, d=0?e=2 a=1, d=2?e=2        0.2  0.1  0.4  0.3 0.8  0.2 0.1  (pruning)  (pruning)   Using the inference engine, the ultimate decision rule set can  be gotten, as shown in TABLE ?.



VI. CONCLUSIONS  A mining method of classification rules based on Rough Fuzzy  Set theory is proposed in this paper, which makes use of the  characteristics of rough set and fuzzy set theory to find out the  association and law hidden in data and form the decision rules. The  experiment proves that the method is feasible and effective.

