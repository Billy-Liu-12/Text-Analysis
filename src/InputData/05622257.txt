Clustering Toward Detecting Cyber Attacks

Abstract?several anomaly methods have been proposed to cope with the recent booming of HTTP-related vulnerabilities which renders the security breaches of lots of vital HTTP- based services on the internet. This paper proposes a novel bottom-up agglomerative clustering method which not only spares the nuisance of a learning process that involves a big amount of manual sample taggings, but also presents a much stronger adaptiveness in being able to coping with variant situations and in detecting new samples.

Keywords: agglomerative clustering; intrusion detection; HTTP attacks; data minning;

I.  INTRODUCTION The Hypertext Transfer Protocol (HTTP)[3] has been  much more widely used in recent years. More applications are designed to run as a web service and deployed across the internet, which wires HTTP as its standard communication protocol. For example, news publishing, bank transaction, email processing, file transferring, etc. The thriving trend of web service and its popularity ensures a heavier dependency of people on HTTP-based web applications to conduct carry out their business.

The popularity of HTTP applications is a big motive for attackers to exploit HTTP related vulnerabilities. Latest released CVE vulnerability trends[1] points out that three typical HTTP-related attacks, namely Cross Site Scripting(XSS), SQL-injection and Remote File Inclusion(RFI) amount for more than half of the totally reported vulnerabilities. As the report also pointed out, it was mainly due to the ease of probing and exploitation of web vulnerabilities, combined with the proliferation of web service applications developed by inexperienced coders.

To detect attacks, traditional Intrusion Detection System (IDS) has since long been used as a key security instrument.

It is configured with a big number of signatures to detect known attacks. The sharp increase of the vulnerabilities instantly propels the number of IDS signatures, as the case with Snort[2], which renders the higher detecting consumption and lower network throughput. The dilemma was partly solved by anomaly detection, which builds normal behavior patterns rather than modeling every known attack.

However, performance of anomaly methods depends heavily on how complete the samples collected represent and  1 Correspondence Authors: Xiaofeng. YANG     yangxf.nj@gmail.com; Yongzhi. LI      yzli@njfu.com.cn  the accuracy of human taggings. Furthermore, one model learned from a given sample set applies only to very limitted detecting situations.

In this paper, we propose a bottom-up agglomerative clustering method which spares the nuisance of a learning process and has a strong adaptiveness for variant detection tasks.

The remainder of this paper is organized as follows: section II introduces the related works and methods with regard to both anomaly methods and clustering algorithms.

Section III defines the problems we are facing and the motive of our work, followed by the elaboration of our clustering method in section IV. In section V, we designed a test to demonstrate the strength of our method. Section VI concludes the paper.



II. RELATED WORKS Anomaly detection builds a normal behavior model, and  marks attack on behaviors which significantly deviate from the normal model. Though anomaly IDS researches have largely focused on traditional all-in-one IDSes, a few anomaly methods tailored for HTTP attacks have already been proposed.

C. Kruegel and G. Vigna[4] proposed a group of anomaly detecting models:  Attribute length: usually, parameters of a specific HTTP request are either fixed-sized tokens or short strings delivered from human inputs, therefore, the lengths of  the parameter values should not vary too much between requests associated with a certain program. With mean u and standard deviation s trained with normal sample lengths, anomaly score of new sample length l is computed by |l-u|/s.

Character distribution: a statistical way of measuring the distribution sequence of a string. Other include token finder, attribute presence or absence, attribute order and NFA.

KL. Ingham and H. Inoue [5] summarized and compared existent anomaly detection methods for HTTP, and mentioned Ngram, a strong means of distinguishing normal and abnormal requests.

These methods invariantly need to learn from a human tagged normal sample set before putting into detection, their performances heavily depend on the selection of the learning sample set.

H Debar and A Wespi[6] started using basic clustering techniques to cope with traditional IDS alerts. S Zanero and SM Savaresi[7] propose an unsupervised clustering method to improve the automation of IDS alerts processing. K   V12-243C978-1-4244-7237-6/$26.00     2010 IEEE    2009-09-28 02:26:25 mysite.com 60.190.218.135 GET /default.asp cat=1&page=9 80 ? 124.115.0.170 200 0 0  program parameters method  Fig. 1: sample log entry  Julisch[8] raised the idea that the root causes of most IDS alert, which represented by dense clusters of alerts shareing much resemblances can be localized by a clustering method.

Though these methods are designed to deal with traditional IDS alerts, they are a big inspiration to our work.



III. PROBEM DEFINITIONS Before proceeding to the next section where our  clustering method is unfolded in details, we introduce in this section the challenges in front of us, followed by some presumptions that works as the cornerstone in our research.

A. Web Service and HTTP Protocol Web services are deployed by providing a globally  accessible IP address or a domain name, they are programs that mostly listen on port 80 and answer to requests from clients, mostly launched by web browsers (Microsoft Internet Explorer, or Firefox etc.). Web service takes the client/server mode for communications, both sides must comply to HTTP protocol specifications to enable the proper working.

A typical web service application consists of a bunch of dynamic scripts suffixed with ?.asp? or ?.php? etc., which we call programs. Each of these programs provides a part of the service interfaces the application is designed for. Programs are organized in a root directory directly or indirectly in variant subdirectories under the root directory. Therefore, every program has a unique sub-path relative to the root directory which can locate the program. We call the unique identity as program name.

A request routed to the server mainly consists of an URL ( the program name that is to receive and process the request) and request parameters.

As stated by HTTP protocol, two methods are employed to send request, namely ?GET? and ?POST?. They differ in how to send the parameters, ?GET? sends the parameters by appending them to the end the URL, while ?POST? puts parameters in the deeper sections of the request packet.

Server side platform (e.g. IIS and Apache) logs every HTTP request from all clients scattered around any corner of the internet. A sample log entry is shown in Fig. 1.

Our method analyzes these server logs to detect malicious requests. Because request parameter sent with method ?POST? are not logged at all, POST requests are not considered in our research. However, our method is capable of coping with POST requests as well by adding a more sophisticated data capturing mechanism.

Any request to a program spawns a lot of additional requests to resource files that are referred by the program, these additional request will also be logged in log files, however these logs not only provide little information in our analysis but also confuse a lot, so we ignored them in our  work and focused only on logs for program requests.

B. Presumption The goal of our research is to classify a large request  samples into normal and malicious groups. Clustering carries the nature of producing one or more clusters from a large dataset.

We observed some apparent statistical characteristics with regard to normal and abnormal requests:  ? Normal requests take similar forms ? Normal requests reoccurs frequently, and takes a  predominant proportion ? Abnormal requests are take a much bizarre look than  the normal request, and they are much smaller in numbers.

We believe by picking up an appropriate dissimilarity measurement and a proper clustering strategy, clustering process is capable of gathering a dense cluster of normal requests in the center and leaving abnormal requests out, by which properly classify requests into normal and abnormal groups.



IV. CLUSTERING METHOD A bottom-up agglomerative clustering method is  unfolded as follows:  A. Definitions and Notations Attribute: An attribute attr is a name and value pair (a,v).

Parameters: parameters consist of a group attributes  which are catenated with a symbol ?&?.

Request: since the program name and its parameters are  all what we are interested in a request, we define a request as (pname,(a1,v1),(a2,v2),?(an,vn)), with pname representing the program name and (a1,v1) attributes passed in the request.

Abnormal Request: Attribute value, where is supposed to transfer normal user parameters, is always manipulated by attackers to probe internal information or execute malicious commands. We define a request abnormal if one of the attribute values is manipulated.

Sample set: a sample set X is the values with the same attribute name and program name. Any sample x belongs to X takes the form of a string.

Cluster:  cluster C={x1,x2,?,xn} is a set of samples that are supposed to be close to each other.

Stage: a stage in a clustering process S={C1,C2,?,Cn} is a set of sample clusters, where the union of all Ck is X and the intersection of any pair of (Ci,Cj) is empty.

Sample Distance: d(xi,xj) is the distance between the sample pair xi and xj.

Cluster distance: D(Ci,Cj) is the cluster distance between cluster Ci and Cj, a.k.a. dissimilarity of Ci and Cj.

Inner Cluster Distance: We measure the density of a cluster C as D(C), which is the farthest sample distance between any sample pair within cluster C.

Cluster momentum: cluster momentum N(C) is defined as the counts of samples belong to the cluster C.

V12-244    B. Clustering Framework Clustering starts from an initial stage S0, where every  single sample is counted as a cluster. With every clustering step, a cluster pair which among all has the shortest distance is clustered together and marked as a new cluster. It process goes on until the final stage where all samples are in a single cluster, thus a bottom-up hierarchy is formed with every later stage perches on top the previous one.

The algorithm is as follows: ? Step 1: initialize stage S0 by adding every single  sample into a separate cluster. S0={Ci={xi}},i=1,?N; t=0  ? Step 2: repeat Step 3 to Step 5 sequentially until all sample are in a single cluster  ? Step 3: t=t+1 ? Step 4: find cluster pair (Ci,Cj) from all possible  pairs (Cr,Cs), satisfying D(Ci,Cj) = min D(Cr,Cs) ? Step 5: Create new cluster Cq=Ci?Cj, update stage  St=(St-1-(Ci,Cj))?Cq For clarity, we call a repeat from step 3 to step 5 a stage  move. This clustering process creates a sequence of stages: S0S1?Sn. with each move to the next stage, normal samples which share a high resemblance tend to be grouped into a single cluster in an early stage. Every stage except the last one contains a group of clusters. Given a stage, we single out the cluster with the maximum number of samples as MaxC and mark as normal, while those left out as abnormal.

The distance sequence that involved in stage moves monotonously increases. However, there are occasions where more than 2 consecutive distances are the same, though the corresponding stage moves are carried out one after another, they nevertheless share equal priorities if parallelism is allowed. Therefore, we consider stages clustered with same distances as a single stage, and we simplify this by preserving the last one in the consecutive stage sequence. By doing so, we yield a shorter sequence S0S1?SM, where m?n and no distances with regard to two consecutive moves are the same.

Any stage provides a classification that whether a sample is normal or abnormal. A cutting strategy which selects the best stage from the stage sequence S0S1?SM is decided by two conditions:  ? Condition1: The momentum of maximum cluster accounts for more than half of the total samples. We define the max cluster in the first stage out of stage sequence which satisfies this criteria as MaxC0: N(MaxC0)>N(X)  ? Conditions2: Inner cluster distance of max cluster is no more than twice that of MaxC0: D(MaxC)<2D(MaxC0)  The best stage is the last one that satisfies both condition1 and condition2 out of the stage sequence.

Two deciding parts of cluster process that influences greatly on how the classifying result turns out are the definitions of the sample distance and cluster distance, both of which are elaborated in the following part of this paper.

C. Distance Definitions ? Sample distance Our samples are values with regard to a specific pair of  attribute name and program name. A single sample is a string, sample distance are then the distance of 2 strings. Several distinct natures with normal and abnormal requests are taken into consideration.

a) string length: distance between two strings with radically different lengths tends to be far away.

b) string shift: if one string is generated from another by few shifting operations, the two strings tend to be close. For instance, string ?aabbccdd? and ?daabbccd? are close.

We picked up a sophisticated string distance definition ?edit distance? as our sample distance. Given string a and b, edit distance ED(a,b) defines as the smallest number of elemental operations needed to change a to b. Elemental operations include ?insert?, ?delete?, ?replace? and ?substitute?.

Shift operation that we mentioned can be accomplished by a combination of ?insert?s and ?delete?s.

? Cluster distance Because we observed the normal samples are close to  each other, the cluster they represented must be dense, and the union of two normal clusters will maintain the density of its previous ones. So we choose the maximum sample distance between samples from both clusters as their cluster distance: D(Ci,Cj)=max d(xq,xr) where xq?Ci,xr?Cj.this ensures the monotonous increase of the distance sequence.



V. TEST AND DISCUSSIONS  A. Test Data Few attack data sets are made public, they include  DARPA data sets [9,10] by DARPA/MIT Lincoln Laboratories and KDDCup?99[11]. Firstly, they were recorded 10 years ago, and somewhat outdated, therefore are hard to represent the current attacking and defensive techniques. Secondly, they consist of few HTTP-based attacks, so are unable to test our HTTP-targeted methods.

Some researchers use more accurate but non-public data sets due to some privacy regulations, these data sets are not available either. So, we collected our own data set from web- sites under the domain of ?njust.edu.cn? to compare and evaluate the methods.

The log files were collected in the second half year of 2009, which covers a time-span of more than 5 months. Data set contains HTTP attacks such as SQL injection, Cross-site script, file inclusion (remote and local) and buffer overflow etc. These log files were processed and labeled half- manually and aided by specific programs. We prepared more than 100,000 labelled logs as our test data set.

B. Test Settings We strive to highlight the following points in designing  our test: ? To compare the performance of our method with  other typical abnormal methods in usual detection tasks.

V12-245     Fig.3-b ROC part view of test2  Fig.3-a ROC overview of test2  ? To highlight the strength of our method.

We selected anomaly methods proposed by C. Kruegel[4]  and H. Inoue [5] to conduct the comparison. They are ?length?, ?character distribution? and Ngram with n 5. These methods need to learn from a normal dataset to build normal behavior models before detection. In test1, we picked up all normal samples to feed these learning processes, and the whole dataset for detection. In test2, we randomly picked up half of the normal samples for learning, and still the entire dataset for detection. A fact we maintained deliberately in the second test is that the other half of the normal samples that are excluded from the learning process have no intersection with the learning half. That is to say, only half of the normal samples are learnt before detection, we did this to demonstrate the adaptiveness of all methods.

C. Test Result Receiver operating characteristic (ROC) curve shows a  trade off between true positive rate and false positive rate, and is employed in our work to visualize the test results.

Fig.2-b and Fig.3-b are zoomed-in plots of Fig.2-a and Fig.3-a, they amplified the differences of every curve.

In both tests, Character distribution method has poor performances. We see from Fig.2-a and Fig.2-b that Length, cluster and Ngram can all achieve high detection rate while maintaining a zero false positive rate. However, in test2, where learning dataset was not quite sufficient, Ngram?s performance dropped dramatically. In both tests, length and our method kept stably high detection rates and low false positive rates with our method slightly taking the leads.

D. Discussion Our method has demonstrated some advantages: High detection rate and low false positive rates:  achieving a more than 99% detection rate while keeps a zero  false positive rate. Low false positive rate is very important in real practice.

Strong adaptiveness: other abnormal methods like Ngram though quite effective if trained well, are highly dependent on the learning sample set, if learning sample set is not complete, detection performance could be well under expectation.



VI. CONCLUSION AND FUTURE WORK In this paper, following the observation that normal  requests are a predominant majority among all requests and they share a high resemblance, we proposed a clustering method for detecting HTTP attacks, which bears the merits of both high detection performance and strong adaptiveness.

Though a fine result did we achieve in the test, there are some simple doings like the selections of sample distance and cutting strategy left, more considerations taking the advantage of application priories are still needed to be added into the method for further improvements, which will be investigated in our future work.

