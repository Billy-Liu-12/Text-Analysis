An Improved Apriori Algorithm Based on Association Analysis

Abstract?Association Rules Mining is an important  branch of Data Mining Technology, of which  Apriori Algorithm is the most influential and classic  one. After discussing and analyzing the basic  concept of Association Rules Mining, this paper  proposes an improved algorithm based on a  combination of Data Division and Dynamic  Itemsets Counting. Analysis of the improved  algorithm proves that it can effectively improve the  performance of Data Mining.

Keywords-Association Rules Mining; Data Mining;  Apriori Algorithm; Data Division; Dynamic Itemsets  Counting

I. Introduction  Data Mining is a nontrivial[1] process, extracting potential, useful, novel, ultimately understandable knowledge from a large database or a large number of original data in data warehouse, which is the core of KDD and one of the most international front line in Database, Data warehouse and the domain of Information Decision. Association Rules Mining[2] is an important branch of Data Mining, widely used in various walks of life and by which analying and mining on the data relevance then eventually gets the information valuably in the process of decision making.



II. Profile of Association Rules Mining  Association Rules Mining is to discovery potential relation between data items, of which value can be described. The question is described as follows.

To presume, I={i1,i2,?,im}is an aggregation of all items, D is a transaction database, transaction T is a subset of items(T?I). Each T  owns its unique identification TID. A is a set consisting of items, namely itemset. T contains A, only when A?T. If A contains k projects, is called k itemset. The frequency of itemset A appearing in transaction database D, accounts for total transaction in D, which is named as support degree of itemset[2]. If support degree of itemset exceeds minimum support threshold value given by users, then is called frequent itemset, called frequent set for short in this paper.

Association Rules is as the logical implication form[3] of X?Y, within, X?I, Y?I, and X?Y=?.

If s percent of transaction in D contains X Y, the support degree of X?Y is s%. In fact, support degree is a probability value. If the support degree of itemset X is marked as support(X), the trust degree of Association Rules is support(X Y) / support(X). This is a conditional probability[3] P(Y|X), that is, support(X?Y)=P(X Y),confidence(X?Y)=P( Y|X).

Association Rule is the rule, whose support degree and confidence degree respectively meet the threshold values given by users. Discoverying Association Rules needs two steps as follows.

1) Find out all frequent sets, whose appearing frequency at last is the same as the minimum support degree predefined.

2) Strong Association Rules are produced by frequent sets, which must meet the minimum support degree and minimum confidence degree.

The most famous and influential Association Rules Mining Algorithm is Apriori[4] Algorithm put forward by R.Agrawal, etc. Its name originates from using the prior knowledge of nature of frequent itemset.

DOI 10.1109/ICNDC.2012.56

III.  Apriori Algorithm  Mining or identifying all frequent sets by adopting recursive algorithm based on two phase frequent set, is the core of the algorithm. In order to avoid computing the support degree of all itemsets (practically frequent sets account very a few part), Apriori Algorithm has introduced the concept of potential frequent itemset. The principle[5] of constituting potential frequent itemset is ?Subset of frequent itemset must be frequent itemset? or ?Superset of infrequent itemset must be infrequent?. So we only need compute the support degree of potential frequent itemset, reducing some amount of calculation.

A. Algorithm  Apriori Adopts recursive method to generate the required frequent sets.

Input values are transaction database D and minimum support threshold value min_sup; Output values are Li and frequent sets in D.

Processing flow is described as follows.

1) L1 = find_frequent_1_itemset (D)  // discover 1-itemset 2) for (k=2 Lk-1?? k++) { 3) Ck = apriori_gen (Lk-1 min_sup)  // generate candidate k-itemset according to frequent(k-1)-itemset  4) for each t D { // scan database to confirm support  degree of each candidate itemset 5) Ct = subset (Ck t)  // gain the candidate itemsets contained by t  6) for each c Ct c.count ++ 7) } 8) Lk = { c Ck | c.count > min_sup} 9) return L = kLk  // seek the sum of Lk procedure apriori_gen (Lk-1 min_sup) 1) for each l1 lk-1 2) for each l2 lk-1 3) if ( ( l1[1] = l2[1] ) ? ( l1[k-2] =  l2[k-2] ) ( l1[k-1] l2[k-1] )) { 4) c = l1 l2  //  join two itemsets together 5) if has_infrequent_itemset (c Lk-1) 6) delete c  // except candidates which can?t generate frequent itemsets  7) else Ck = Ck {c} 8) } 9) return Ck procedure has_infrequent_subset (c Lk-1) 1) for each ( k-1) subset s of c 2) if !s Lk-1 return TRUE, else return  FALSE Within, Apriori_gen is the function generating  candidate itemsets of this algorithm, and Lk-1 is its parameter, that?s to say the set of all large-scale (k-1) items. It can return one superset of all large-scale k itemsets. Subset function is to look for all candidates contained in some transaction t beginning with the root node, and finally returns collections of references we need.

B.  Basic Idea  The basic idea of Apriori Algorithm is to scan database repeatedly, and to generate k dimension frequent set using given k-1 dimension frequent set based on anti monotonicity property of itemset. The main method[6] is that firstly finds out frequent 1-itemset marked as L1, then mines L2 using L1, namely frequent 2-itemset, which continues until there no more frequent k-itemset can be found, and every time one Li is sought, the database must be scanned again.

Connection and Pruning are two main tasks in the implementation of Apriori Algorithm.

Connection operation[7] is as follows. Assume Lk={l1, l2, l3,?, ln}, of which li, lj 1? i ? n 1? j ? n i?j are two elements. li={li[1], li[2], li[3],?, li[k]}, and li[m] 1?m?k is the first m item. Two elements of Lk can be connected only when their former k-1 elements are the same, namely when (li[1]=lj[1]) (li[2]=lj[2]) (li[3]=lj[3]) .. (li[k- 1]=lj[k-1]) (li[k]=lj[k]), li?lj={li[1], li[2], li[3],?, li[k], lj[k]}, li?lj Ck+1.

Pruning operation[7] is described as follows.

Assume ck Ck, namely ck is a candidate     k-itemset, and ck-1 is a (k-1)-itemset of ck. From the property of Apriori, we know that, any infrequent (k-1)-itemset can?t be subset of frequent k-itemset. That is, candidate k-itemset ck should be deleted from the set of candidate k-itemset Ck.



IV.  Improvement of Apriori Algorithm and Analysis  Now, Apriori is faced with two challenges.

Maybe it needs scan transaction database repeatedly, increasing the I/O load of database.

Probably it generates a large number of candidate sets, rasing the cost of time and storage space. To reduce the required amount of calculation and the times of scanning database, an improved algorithm combining Data Division and Dynamic Itemsets Counting is proposed, which can improve the performance meanwhile resolves the problems.

A. Theme Idea  a) Data Division Firstly, divide the transaction database D into  n parts, which don't intersect each other. If the minimum support threshold value of D is min_sup, corresponding minimum support frequency threshold value for each part is min_sup?number_of_transaction_of_partition.

Scan database once, and mine all frequent sets of each division, called local frequent sets. With a special data structure, TID of transaction record containing these frequent sets can be kept track, so that all local frequent k-itemsets can be found by scanning database only once, k=1,2,?, then collect all local frequent sets to constitute the candidate itemsets of frequent sets in the entire database D.

Secondly, scan the whole database again, get support degree of all candidate itemsets and finally decide global frequent itemsets.

The technology includes two stage of processing upward. Size and number of each division can take putting each division size into memory as a standard. So each stage just reads  from database once, and the whole mining process only needs scan the entire database twice.

b) Dynamic Itemsets Counting This method is put forward when division  mining for database, exactly speaking, it is to add candidate itemsets at different times when scanning. Every divided data block is marked the start sign, and new candidate itemsets can be added at any starting point in the process of change, so as to decide candidate itemsets before scanning database every time; In addition, the technology is dynamic, because it needs estimate support degree of all itemsets counted to date.

This algorithm just needs scan database D twice.

B. Accomplishment of improved Algorithm  Input values are transaction database D, minimum support threshold value min_sup and dividing database into m blocks. Output values are Li and frequent sets in D.

Main method is mainly using local frequent sets to generate global frequent sets.

n = count (TID); part_n = n / m; p1= part_n; ?? pm= n- (m-1) part_n; L1= find_frequent_itemset (p1); L2= find_frequent_itemset (p2); ?? Lm= find_frequent_itemset (pm); L = apriori_gen (L1, m, min_sup); return L ; a) Method of generating candidate  k-itemset Ck according to Lk-1 Produce apriori_gen(Lk-1, min_sup) for each items l1?  Lk-1 for each items l2?  Lk-1 if((l1[1]=l2[2] ??? (l1[k-1]=l2[k-2])? ( l1[k -1]?  l2[k-2])) do begin  c=l1? l2 // join two itemsets together  if has_infrequent_itemset(c, Lk-1) delete c else Ck=Ck	 {c} end return Ck     b) Method of excepting candidate itemset of which subset is infrequent itemset  Produce has_infrequent_subset(c,Lk-1) forall (k-1) subset s of c  if s Lk-1 return true; else return false  C. Analysis  The mining of Apriori Algorithm based on division is to divide database into n parts of which scale is suitable, firstly generate a group of itemsets for each separate part, then put these itemsets into a global candidate frequent itemset.

This ?To delimit small, with small parallel? mining method can make the mining efficiency not suffer much effect, on the condition that data quantity increases gradually. What?s more, combing Dynamic Itemsets Counting to reduce unnecessary calculation, can satisfy quick performance of mining more.



V. Conclusion  Based on the study of Apriori Algorithm, this paper puts forward an improved one, which scans database only twice, greatly reduces the I/O load.

Introducing the method of Dynamic Itemsets Counting, reduces the calculation and quantity of candidate sets, and saves storage space to some extent. In short, the improved algorithm has improved efficiency and performance of Data Mining.

