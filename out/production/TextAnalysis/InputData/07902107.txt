Zero-Shot Scene Classification for High Spatial Resolution Remote Sensing Images

Abstract? Due to the rapid technological development of various sensors, a huge volume of high spatial resolution (HSR) image data can now be acquired. How to efficiently recognize the scenes from such HSR image data has become a critical task.

Conventional approaches to remote sensing scene classification only utilize information from HSR images. Therefore, they always need a large amount of labeled data and cannot recognize the images from an unseen scene class without any visual sample in the labeled data. To overcome this drawback, we propose a novel approach for recognizing images from unseen scene classes, i.e., zero-shot scene classification (ZSSC). In this approach, we first use the well-known natural language process model, word2vec, to map names of seen/unseen scene classes to semantic vectors. A semantic-directed graph is then constructed over the semantic vectors for describing the relationships between unseen classes and seen classes. To transfer knowledge from the images in seen classes to those in unseen classes, we make an initial label prediction on test images by an unsupervised domain adaptation model. With the semantic-directed graph and initial prediction, a label-propagation algorithm is then developed for ZSSC. By leveraging the visual similarity among images from the same scene class, a label refinement approach based on sparse learning is used to suppress the noise in the zero- shot classification results. Experimental results show that the proposed approach significantly outperforms the state-of-the-art approaches in ZSSC.

Index Terms? High spatial resolution (HSR) remote sensing images, scene classification, zero-shot learning.



I. INTRODUCTION  W ITH the development of modern sensor technologies,a large number of high spatial resolution (HSR) remote sensing images with abundant spatial and structural patterns  Manuscript received October 3, 2016; revised January 3, 2017 and February 25, 2017; accepted March 25, 2017. This work was supported in part by the National Natural Science Foundation of China under Grant 61573363 and Grant 61573026, in part by the 973 Program of China under Grant 2014CB340403 and Grant 2015CB352502, in part by the Fundamental Research Funds for the Central Universities and the Research Funds of Renmin University of China under Grant 15XNLQ01, and in part by the European Research Council FP7 Project SUNNY under Grant 313243.

(Corresponding author: Zhiwu Lu.)  A. Li and L. Wang are with the Key Laboratory of Machine Perception (MOE), School of Electronics Engineering and Computer Sci- ence, Peking University, Beijing 100871, China (e-mail: lax@pku.edu.cn; wanglw@cis.pku.edu.cn).

Z. Lu and J.-R. Wen are with the Beijing Key Laboratory of Big Data Management and Analysis Methods, School of Information, Renmin University of China, Beijing 100872, China (e-mail: luzhiwu@ruc.edu.cn; jrwen@ruc.edu.cn).

T. Xiang is with the School of Electronic Engineering and Computer Science, Queen Mary University of London, London E1 4NS, U.K. (e-mail: t.xiang@qmul.ac.uk).

Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.

are generated by various sensors everyday [1]?[5]. However, due to the huge volume and complex composition of remote sensing image data, it is difficult to directly access the HSR data that contain the scenes of interest. Therefore, how to efficiently recognize the scenes from HSR remote sensing images has become a challenging problem, which has drawn great interest in the remote sensing field [6]?[11].

In order to recognize and analyze the scenes from HSR remote sensing images, various scene classification approaches have been proposed in recent years. Zou et al. [12] proposed a deep-belief-network-based feature selection strat- egy to construct discriminative features for scene classifica- tion. Zhao et al. [13] provided a concentric circle-structured multiscale bag-of-visual-words (BOVW) model using multi- ple features for land-use scene classification. An unsuper- vised quaternion feature learning algorithm was proposed by Risojevic? and Babic? [14] for remote sensing image scene classification, where quaternion representation was exploited to capture interrelationships between intensity and color infor- mation. Zhong et al. [15] proposed a semantic allocation level multifeature fusion strategy based on the probabilistic topic model to effectively combine spectral and texture features for HSR remote sensing scene classification. Li et al. [16] proposed a multilayer feature learning approach to automati- cally learn simple edge features and complex corners/junctions features for satellite image scene classification. Considering the importance of global features in interpreting the semantics in HSR remote sensing imagery, Zhu et al. [17] improved the traditional BOVW model by introducing the shape-based invariant texture index as the global texture feature, and then effectively combined the local BOVW and global features for HSR imagery scene classification. In order to bridge the semantic gap between the low-level features and the high- level semantic concepts in HSR imagery scene classification, Zhao et al. [1] proposed a Dirichlet-derived multiple topic model (DMTM), and then, an efficient algorithm based on a variational expectation maximization framework was devel- oped to infer the DMTM and estimate its parameters.

Although the aforementioned approaches have shown yielding promising results in scene classification of HSR remote sensing images, they have two distinct drawbacks as follows.

1) First, these approaches need a certain number of labeled data for each scene class to train a good classifier for scene classification of HSR remote sensing images.

However, some of scene classes are rare, and collecting sufficient labeled training data for them may not be possible even if labeling cost is not a concern.

See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Fig. 1. Some class names and the corresponding examples from the Caltech-UCSD Birds 2011 data set.

2) Second, these approaches only utilize information from HSR remote sensing images for scene classification.

Moreover, they cannot recognize the images from an unseen scene class that is not included in training data.

To overcome these two drawbacks, we introduce a new idea, termed zero-shot scene classification (ZSSC), to the remote sensing scene classification field. ZSSC is well established in computer vision. However, to the best of our knowl- edge, it is still an unfamiliar paradigm in remote sensing.

For humans, it is an easy task to recognize a new scene class even if they have not seen a single instance before.

This is reasonable, because a lot of knowledge is preserved and conveyed to humans via texts and nowadays online sources [18]?[21]. Therefore, combining seen instances and some auxiliary information (e.g., texts), humans can easily recognize new scene classes. Inspired by this phenomenon, researchers have proposed a new approach, zero-shot learning, which transfers knowledge from labeled data (from seen classes) to unlabeled data (from unseen classes) based upon some auxiliary information. Traditional zero-shot learning approaches are mainly developed for the tasks of recognizing natural images, such as bird image classification, human position estimation, and indoor object recognition. In these tasks, the classes usually have strong semantic correlations.

For example, the Caltech-UCSD Birds 2011 data set [22] contains over 11 000 images from 200 types of birds and provides over 300 annotations per image. Fig. 1 provides some samples of class names and their corresponding visual examples in the Caltech-UCSD Birds 2011 data set. It can be seen that these class names are strongly semantically related, which is extremely important for recognizing images from unseen classes. However, for remote sensing scene classification, the names of typical scene classes are not so semantically related as those of the object classes in natural  image recognition, which limits the use of the traditional zero-shot learning approaches in remote sensing.

In this paper, to overcome these limitations, we propose a novel ZSSC approach for HSR remote sensing images.

Concretely, the word2vec model [23], a well-known distributed word representation approach in a natural language process, is first used to map names of scene classes (both seen and unseen) to semantic vectors. A semantic-directed graph is then constructed over the semantic vectors for describing the relationships between unseen classes and seen classes. Given that typical remote sensing scene classes have a limited amount of semantic relationships among each other, we adopt an unsupervised domain adaptation model [24] to provide an effective initial label prediction on test images. Here, this domain adaptation model can transfer knowledge from images in seen classes to those in unseen classes, and thus helps to overcome the limitations of the traditional ZSSC approaches.

Now with both the knowledge from images in seen classes and that from seen/unseen class semantic vectors residing on the same graph, a label-propagation algorithm [25] is developed to measure the distance between test images and each unseen class semantic vector for recognition of the unseen scene classes.

Note that the test images in the same class should have similar visual appearance. However, this is not directly con- sidered in the above-mentioned ZSSC approach, and thus, there may exist strong noise in the zero-shot classification results. Therefore, we develop a label refinement approach based on sparse learning to obtain better results. Specifically, inspired by the successful use of L1-optimization for noise reduction [26]?[29], we formulate the label refinement prob- lem as noise reduction over the labels of test images, where the L1-norm Laplacian regularization term is mainly used to reduce the noise in the labels. To solve the L1-norm optimization problem efficiently, we limit the solution to the space spanned by the eigenvectors of the Laplacian matrix based upon the manifold structure of the data, and solve this problem in a linear time complexity with respect to the number of test images. The framework of the proposed approach is shown in Fig. 2.

To verify the effectiveness of the proposed approach, we first conduct experiments on the UC Merced data set by randomly selecting a number of classes as seen classes and the other as unseen classes. To make the proposed approach more scalable in real-world applications, we also conduct experiments on a large HSR satellite image. Note that this HSR satellite image contains instances from seen/unseen scene classes, which are fully unlabeled in our experimental setting.

In fact, since providing manual labels is expensive and time- consuming, we only use labeled remote sensing images from the RSSCN7 data set [12] for recognizing this satellite image, where both the RSSCN7 data set and the large satellite image are collected from Google Earth. Experimental results show that the proposed approach significantly outperforms the state- of-the-art zero-shot learning approaches [30]?[32].

The major contributions of this paper are as follows.

1) This is the first work on scene classification of  HSR remote sensing images without seeing any visual    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

LI et al.: ZSSC FOR HIGH SPATIAL RESOLUTION REMOTE SENSING IMAGES 3  Fig. 2. Framework of the proposed approach to ZSSC.

example in some scene classes (i.e., ZSSC). Our novelty mainly lies in that the need of labeled remote sensing images can be effectively reduced, and the scalability of the traditional scene classification approaches can be improved for real-world applications.

2) By leveraging the visual similarity among images from the same scene class, the proposed label refinement approach based on sparse learning can suppress the noise in the zero-shot classification results.

3) The proposed approach is shown to significantly outper- form the state-of-the-art zero-shot learning approaches on both the UC Merced data set and the large HSR satellite image, which means that the proposed approach is more scalable in real-world applications.

The remainder of this paper is organized as follows.

Section II provides a brief review of related works on zero- shot learning. Section III describes the details of the proposed approach for ZSSC of HSR remote sensing images. Section IV presents the experimental results to evaluate the performance of the proposed approach. Finally, the conclusions are drawn in Section V.



II. RELATED WORKS  Recently, many algorithms have developed for zero-shot learning [30]?[36]. An attribute-based zero-shot learning approach was proposed by Lampert et al. [34] to recognize dif- ferent kinds of animals? images. To describe the relationships between seen classes and unseen classes, Mensink et al. [35] developed various metrics to leverage the co-occurrences of visual concepts in images, and then, a regression approach was proposed to learn a weight for each related class.

Antol et al. [33] proposed a visual-abstraction-based zero- shot learning approach to explore concepts related to people and their interactions with others, and achieved satisfactory results on human pose recognition. Zhang and Saligrama [32] viewed test instances as arising from seen instances, and attempted to express test instances as a mixture of  seen class proportions. To solve this problem, they pro- posed a semantic similarity embedding approach for zero- shot learning. A general zero-shot learning framework, which modeled the relationships between features, attributes, and classes as a two-linear-layers network, was proposed by Romera-Paredes and Torr [31] to recognize animals and nat- ural scenes. Considering the manifold structure of seman- tic categories, Fu et al. [30] provided a novel zero-shot learning approach by formulating a semantic manifold dis- tance among test images and unseen classes. Li et al. [36] proposed a novel zero-shot learning approach that automat- ically learned label embeddings from the input data in a semisupervised large-margin learning framework. The above- mentioned zero-shot learning approaches yielded promising results in the task of recognizing natural images. However, the semantic relationships among typical scene classes? names in remote sensing field are not so strong as those in natural image field, and thus, these approaches have limited use for ZSSC of HSR remote sensing images.



III. METHODOLOGY  In this section, we provide the details of the proposed approach for ZSSC of HSR remote sensing images. Specif- ically, the proposed approach contains two main steps: 1) a zero-shot learning approach based on label propagation is developed for recognizing the HSR remote sensing images in unseen classes and 2) a label refinement approach based on sparse learning is used to suppress the noise in the zero-shot classification results.

A. Zero-Shot Learning Based on Label Propagation  Let S = {s1, . . . , sp} denote the set of seen classes and U = {u1, . . . , uq} denote the set of unseen classes, where p and q are the total numbers of seen classes and unseen classes, respectively. These two sets of classes are disjoint, i.e., S ?U = ?. We are given a set of labeled training images Ds = {(xi , yi ) : i = 1, . . . , M}, where xi is the feature vector    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

of the i th image in the training set, yi ? S is the corresponding label, and M denotes the total number of labeled images. Let Du = {(x j , y j ) : j = 1, . . . , N} denote a set of unlabeled test images, where x j is the feature vector of the j th image in the test set, y j ? U is the corresponding unknown label, and N denotes the total number of unlabeled images. The main goal of zero-shot learning is to predict y j by learning a classifier f : X ? U , where X = {x j : j = 1, . . . , N}.

For zero-shot learning, we first need to estimate the semantic relationships between seen and unseen classes, which will be used for predicting the labels of images in unseen classes.

In this paper, we adopt the word2vec model [23], which was trained with over 4 000 000 text documents from Wikipedia, to represent each class (? S ?U ) by a semantic vector (empir- ically set as 400-D). We further construct a semantic-directed graph G = {V , E} over all the classes, where V denotes the set of nodes (i.e., classes) in the graph and E denotes the set of directed edges between classes. The details of graph construction are given as follows.

1) We first construct the edges among seen classes. For each seen class, the k-nearest-neighbors (k-NN) method is performed on the semantic vectors to find its k1 near- est neighbors among seen classes. A directed edge is constructed between this class and each of its neigh- bors (from seen classes), and its edge weight is defined by applying the Gaussian kernel (with the width = 1) to the Euclidean distance between them.

2) We further adopt the same strategy to construct the edges between seen classes and unseen classes. For each seen class, the k-NN method is performed on the semantic vectors to find its k2 nearest neighbors among unseen classes. A directed edge is constructed between this class and each of its neighbors (from unseen classes), and its edge weight is defined by applying the Gaussian kernel (with the width = 1) to the Euclidean distance between them.

3) Finally, for each unseen class, it has only one edge pointing to itself with a weight of 1.

By collecting the above-mentioned edge weights up, we can denote the weight matrix W of the semantic-directed graph G as  W = [ R1 R2 0 I  ] (1)  where R1 ? Rp?p collects the edge weights among seen classes, R2 ? Rp?q collects the edge weights between seen classes and unseen classes, and I ? Rq?q is an identity matrix.

We further define a Markov chain process over G by constructing the transition matrix T = D?1W , where D is a (p+q)? (p+q) diagonal matrix with its i th diagonal element being equal to the sum of the i th row of W . To guarantee that the Markov chain process has a unique stationary solution [25], we normalize the transition matrix T as follows:  P = ? p + q ? 1 (1p+q ? Ip+q) + (1 ? ?)T (2)  where ? is a normalization parameter (which is empirically set as ? = 0.001), and 1p+q and Ip+q are the one matrix and identity matrix of the size (p + q) ? (p + q), respectively.

Based on the normalized transition matrix P = [puv] ? R  (p+q)?(p+q), we formulate zero-shot learning as a label- propagation problem to propagate the labels from each unseen class semantic vector to a given test image and use the resultant propagation cost/probability as the distance for recognition  min Fi.

? u,v  ?(u)puv  ( Fiu? ?(u)  ? Fiv? ?(v)  )2 + ??Fi. ? Yi.?22 (3)  where F = [Fiu ]N? (p+q) and Y = [Yiu ]N? (p+q) collect the optimal and initial probabilities of the test images belonging to each category, respectively. Concretely, Fiu (or Yiu ) denotes the optimal (or initial) probability of the i th test image belonging to the uth category. Moreover, Fi. (or Yi.) denotes the i th row of F (or Y ). In addition, ?(u) is the sum of the uth row of the transition matrix P (i.e.,  ? v puv) and ? is a  positive regularization parameter.

The first term of the above-mentioned objective function  sums the weighted variation of Fi. on each edge of the directed graph G, which aims to ensure that Fi. does not change too much between semantically similar classes for the i th test image. The second term denotes an L2-norm fitting constraint, which means that Fi. should not change too much from Yi. .

To solve the above-mentioned label-propagation problem, we adopt the technique introduced in [25] and define the operator ?  ? = (?1/2 P??1/2 + ??1/2 P?1/2)/2 (4) where ? is a (p + q) ? (p + q) diagonal matrix with its uth diagonal element being equal to ?(u). According to [25], the optimal solution F? of the problem in (3) is  F? = Y (I ? ??)?1 (5) where I is an identity matrix of the size (p + q) ? (p + q) and ? = 1/(1 + ?) ? (0, 1). This solution can be obtained at a linear time cost with respect to N .

For zero-shot learning, we need to provide Y in advance.

Note that each row of Y consists of two parts: the prob- abilities of a test image belonging to seen classes and the probabilities of a test image belonging to unseen classes.

Given no labeled data in unseen classes, we directly set the probabilities belonging to unseen classes as 0. To compute the initial probabilities belonging to seen classes, we adopt an unsupervised domain adaptation model [24], which transfers the knowledge from labeled images in the seen classes to test images. This unsupervised domain adaptation model is developed based on a deep convolutional neural network, which has three main components: a deep feature extractor, a label classifier, and a domain classifier. In this paper, we fine- tune a GoogLeNet model [37] to obtain the feature extractor and the label classifier, and also train a gradient reversal layer to connect the feature extractor and the domain classifier for transfer learning. For test images, the outputs of the softmax layer in the label classifier denote the initial probabilities belonging to seen classes. By integrating the knowledge from seen classes into the proposed ZSSC approach, we can better strengthen the relationships between seen classes and unseen classes, which makes the proposed ZSSC model more effective in recognizing images from unseen classes.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

LI et al.: ZSSC FOR HIGH SPATIAL RESOLUTION REMOTE SENSING IMAGES 5  B. Label Refinement Based on Sparse Learning  Due to the limitations of word2vec in describing semantic relationships between seen and unseen scene classes, there still exists strong noise in the zero-shot classification results.

Considering that images in the same scene class should have similar visual appearance, we thus propose a label refinement approach based on sparse learning to suppress the noise in the zero-shot classification results.

Before giving problem formulation, we model all test images X as a graph G = {X, W } with its vertex set X and weight matrix W = [wi j ]N?N , where wi j denotes the similarity between image feature vectors xi and x j . In this paper, we use pretrained GoogLeNet features (extracted from the last layer of the GoogLeNet model [37] trained using 1.2-M images from ImageNet [38]) as the feature vectors of test images. Note that the weight matrix W is usually assumed to be nonnegative and symmetrical. In this paper, we define the weight matrix W by applying the Gaussian kernel (with the width = 1) to the Euclidean distances between the GoogLeNet pretrained feature vectors of any two test images. The normalized Laplacian matrix L of the graph G can be computed by  L = I ? D? 12 W D? 12 (6) where I is an N ? N identity matrix and D is an N ? N diagonal matrix with its i th diagonal element being equal to the sum of the i th row of W (i.e.,  ? j wi j ).

Based on eigenvalue decomposition, the normalized Laplacian matrix L can be decomposed into the following symmetrical form:  L = V 	V T = (	 12 V T )T (	 12 V T ) = BT B (7) where V is an N ? N orthonormal matrix with each column being an eigenvector of L and 	 is an N ? N diagonal matrix with its diagonal element 	ii being an eigenvalue of L (sorted as 0 ? 	11 ? ? ? ? ? 	N N ).

We further use the new matrix B = 	 12 V T to define an L1-norm smooth measure and refine the zero-shot classifi- cation results from the viewpoint of noise reduction over the labels predicted by the proposed zero-shot learning approach  min F?   ?F? ? F??2F + ? ?B F??1 (8)  where F?i. (the i th row of F? ? RN?q ) denotes the opti- mal probability of the i th test image belonging to unseen classes, F?i. (the i th row of F? ? RN?q ) denotes the probabil- ities of the i th test image belonging to unseen classes given by (5), and ? denotes a positive regularization parameter. The first term denotes an L2-norm fitting constraint, which means that F? should not change too much from F?. The second term denotes an L1-norm Laplacian regularization, which means that F? should not change too much between visual similar images. The good property of the second term in noise reduction has been given and proven in [39].

Note that directly solving (8) is computationally intractable, considering that only the computation of B would incur  too large time cost. Fortunately, we can use the dimension reduction technique to efficiently solve this problem. Con- cretely, we limit F? to the space spanned by a small set of eigenvectors of the normalized Laplacian matrix L. To ensure the consistence of F? , we choose m eigenvectors with the smallest eigenvalues as the base vectors. That is, F? = Vm A, where Vm stores m eigenvectors with the smallest eigenvalues and A denotes linear combination coefficients. Equation (8) can now be reformulated as follows: arg min  A   ?Vm A ? F??2F + ? ?BVm A?1  = arg min A  q? j=1    ??Vm A. j ? F?. j ??22 + ? ?	 12 V T Vm A. j?1  = arg min A  q? j=1    ??Vm A. j ? F?. j ??22 + ? m?  i=1   ii |ai j |  = arg min A  q? j=1  m? i=1   a2i j ?  ( V T.i F  ? . j  ) ai j + ?  ii |ai j | (9)  where F?. j denotes the j th column of F? and V.i denotes the i th column of Vm .

It can be seen that the L1-minimization problem in (8) has been decomposed into q ? m independent quadratic optimiza- tion subproblems  a?i j = arg minai j  a2i j ?  ( V T.i F  ? . j  ) ai j + ?  ii |ai j | (10)  which has the following explicit solution:  a?i j =  ????? ????  0, ??V T.i F?. j ?? ? ?  ii  V T.i F ? . j + ?  ii , V  T .i F  ? . j < ??  ii  V T.i F ? . j ? ?  ii , V  T .i F  ? . j > ?  ii .

(11)  In this way, (8) can be solved efficiently at a linear time cost with respect to N .

To sum up, by combining zero-shot learning and label refinement together, the full algorithm for ZSSC is outlined as in Algorithm 1.

C. Computation Complexity Analysis  Note that both the label-propagation problem in (3) and the label refinement problem in (8) can be solved efficiently at a linear time cost with respect to the total number of test images N . Hence, the proposed algorithm for ZSSC has a linear overall computational complexity, which is very important for the large scenes.



IV. EXPERIMENTAL RESULTS  In this section, we provide the performance evaluation of the proposed ZSSC approach. We first describe the three data sets used in the experiments. We further report the results of ZSSC on the benchmark UC Merced data set [40].

In addition, we also evaluate the proposed approach in the task of recognizing a large HSR satellite image.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Fig. 3. Example images from 21 aerial scenes in the UC Merced data set. (a) Agricultural. (b) Airplane. (c) Baseball diamond. (d) Beach. (e) Buildings.

(f) Chaparral. (g) Dense residential. (h) Forest. (i) Freeway. (j) Golf course. (k) Harbor. (l) Intersection. (m) Medium residential. (n) Mobile home park.

(o) Overpass. (p) Parking lot. (q) River. (r) Runway. (s) Sparse residential. (t) Storage tanks. (u) Tennis court.

Algorithm 1 Proposed ZSSC Algorithm Input: the set of labeled training images Ds  the set of test images in unseen classes X Zero-Shot Learning Based on Label Propagation: 1) Compute the initial probabilities of test images belonging to unseen classes Y with the domain adaptation model [24]; 2) Construct the semantic-directed graph based on semantic vectors extracted by the word2vec model [23]; 3) Compute the normalized transition matrix P according to Equations (1-2); 4) Find the solution F? of the label propagation problem in Equation (3) according to Equations (4-5); Label Refinement Based on Sparse Learning: 5) Construct a k-NN graph with its weight matrix W being defined over X ; 6) Compute the normalized Laplacian matrix L according to Equation (6); 7) Find the m smallest eigenvectors of the normalized Laplacian matrix L and store them in Vm ; 8) Find the solution A? of the L1-minimization problem in Equation (8) according to Equations (9-11); 9) Label each test image xi with scene class arg max j F??i j , where F?? = Vm A?.

Output: the labels of test images in unseen classes.

A. Description of the Data Sets  The first data set used for performance evaluation is the UC Merced data set [40], which is the most widely used benchmark data set for remote sensing scene classification.

This data set consists of 2100 remote sensing images from 21 scene classes: agricultural, airplane, baseball diamond, beach, buildings, chaparral, dense residential, forest, freeway,  golf course, harbor, intersection, medium density residen- tial, mobile home park, overpass, parking lot, river, runway, sparse residential, storage tanks, and tennis courts. Fig. 3 shows some example images from the 21 aerial scenes.

The images in this data set are manually extracted from large images from the USGS National Map Urban Area Imagery collection for various urban areas around the country.

The pixel resolution of this public domain imagery is 1 ft.

For each scene class, there are 100 images of the size 256 ? 256 pixels.

The second data set is the RSSCN7 data set [12], which contains 2800 remote sensing scene images collected from Google Earth. The images in this data set come from seven typical scene classes: grass, river, industrial, field, forest, resi- dential, and parking. For each scene class, there are 400 images of the size 400 ? 400 pixels. Some sample images from this data set are shown in Fig. 4.

The third data set is constructed from a large high-resolution satellite image, which is acquired from Google Earth, for the city of Sydney, Australia. The spatial resolution of this large image is about 1 m. The large satellite image for Sydney is of 9000 ? 9000 pixels, as shown in Fig. 5(a). There exist seven scene classes within this large image: airport, industrial, grass, ocean, residential, river, and runway. Fig. 5(b)?(h) shows some sample subimages of these scene classes. Specifically, the orig- inal large image is divided into 900 nonoverlapping subimages of 300 ? 300 pixels, where each subimage is supposed to only belong to a single scene class. In the following, the set of 900 subimages from the large satellite image is denoted as the Sydney data set.

B. Zero-Shot Scene Classification  1) Experimental Setup: To evaluate the effectiveness of the proposed approach, we conduct a group of experiments    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

LI et al.: ZSSC FOR HIGH SPATIAL RESOLUTION REMOTE SENSING IMAGES 7  Fig. 4. Example images from seven scene classes in the RSSCN7 data set. (a) Grass. (b) River. (c) Industrial. (d) Field. (e) Forest. (f) Residential.

(g) Parking.

Fig. 5. Whole image and example subimages from seven scene classes in the Sydney data set. (a) Whole image. (b) Airport. (c) Industrial. (d) Grass.

(e) Ocean. (f) Residential. (g) River. (h) Runway.

on the UC Merced data set by randomly selecting 16 of 21 classes as seen classes and the other five classes as unseen classes. Furthermore, we also test another three unseen/seen ratios to verify the effectiveness of the proposed approach in much weaker settings. For each unseen/seen ratio, the final classification results over unseen classes are averaged over 25 random seen/unseen splits. In this paper, we compare the proposed approach with the state-of-the-art zero-shot learning approaches [30]?[32].

2) Parameter Selection: The parameters of the proposed approach are selected in the unseen/seen ratio of 5/16. Con- cretely, we have randomly split the training set of the UC Merced data set into two halves, and thus tuned the parameters in a twofold cross-validation manner (i.e., images from eight classes are used for training, and images from the other eight classes are used for validation). For the label-propagation- based zero-shot learning method, we tune the parameters k1 and k2 at different values of ?, and the results are given in Fig. 6. We find that the label-propagation-based zero-shot learning method achieves the best result when k1 = 2, k2 = 3, and ? = 0.1. Therefore, we choose k1 = 2, k2 = 3, and  ? = 0.1 for the proposed zero-shot learning model in the following experiments.

Moreover, for the label refinement method, we tune the parameters k, m, and ? in the same way. That is, the para- meters k and m are tuned at different values of ? , and the results are given in Fig. 7. We observe that the label refinement method achieves the best result when k = 200, m = 100, and ? = 0.9. Therefore, we choose k = 200, m = 100, and ? = 0.9 for the proposed label refinement method.

3) Comparison to the State of the Art: Table I shows the comparison of the proposed ZSSC approach to the state-of-the-art zero-shot learning models [30]?[32] on the UC Merced data set. In Table I, ?zero-shot learning via label propagation (ZSL-LP)? denotes the zero-shot learn- ing method based on label propagation in Section III-A, while ?ZSSC? denotes the full ZSSC approach presented in Algorithm 1 (including label refinement). It can be seen that the proposed ZSSC approach not only significantly outper- forms the state-of-the-art zero-shot learning models in terms of the overall rate, but also yields the best results over ten scene classes with respect to the per-class rates. This observation    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Fig. 6. Results of the label-propagation-based zero-shot learning method when tuning parameters k1 and k2 at different values of ?. (a) ? = 0.1.

(b) ? = 0.3. (c) ? = 0.5. (d) ? = 0.7.

Fig. 7. Results of the label refinement method when tuning parameters k and m at different values of ? . (a) ? = 0.3. (b) ? = 0.5. (c) ? = 0.7.

(d) ? = 0.9.

can be explained as follows: 1) the models in [30]?[32] are proposed to cope with the traditional zero-shot learning tasks, where scene classes usually have strong semantic cor- relations. However, this is not the case for remote sensing scene classification; 2) the proposed ZSL-LP method not only exploits the semantic relationship between seen and unseen classes (also considered in the traditional zero-shot learning models [30]?[32]), but also uses a domain adaptation model to describe the relationships between the seen classes and images from unseen classes. This can strengthen the relationship between seen classes and unseen classes, and thus effectively overcome the limitations of the traditional zero-shot learning models in remote sensing scene classification; and 3) the label refinement method based on sparse learning can help to suppress the noise in the zero-shot learning results obtained by ZSL-LP (see ZSL-LP versus ZSSC).

TABLE I  PER-CLASS AND OVERALL CLASSIFICATION RATES (%) FOR DIFFERENT ZERO-SHOT LEARNING MODELS ON THE UC  MERCED DATA SET. FOR COMPACTNESS, WE ONLY PROVIDE STANDARD DEVIATIONS (OVER  25 RANDOM SEEN/UNSEEN SPLITS) FOR THE OVERALL RATES  To further verify the effectiveness of the proposed approach, we also choose another three unseen/seen ratios (i.e., 8/13, 11/10, and 14/7) in the experiments. We randomly split the 21 scene classes in the UC Merced data set into seen and unseen classes according to the corresponding unseen/seen ratios. Table II provides the comparison of different zero-shot learning models on the UC Merced data set with differ- ent unseen/seen ratios. The accuracies are averaged over 25 random seen/unseen splits. It can be seen that the per- formance of all the zero-shot learning models drops when the unseen/seen ratio increases, but our ZSSC approach out- performs the competing models in all cases. Note that the semantic correlation between unseen classes and seen classes becomes weaker with the increase of unseen/seen ratio. Since the model in [30] and the proposed ZSL-LP method both utilize the semantic correlation to infer the labels of test images, less semantic correlation between unseen classes and seen classes induces much more noise to the labels of test images (leading to the performance degradation). However, our label refinement method can utilize sparse learning to denoise the noisy labels, thus leading to significant improve- ment over the model in [30].

C. Large Satellite Image Recognition  1) Experimental Setup: To further evaluate the effective- ness of the proposed approach in large HSR satellite image recognition, we conduct another group of experiments as follows. Given that the RSSCN7 and Sydney data sets are both collected from Google Earth, we used the RSSCN7 data set as the training set and the Sydney data set as the test set. Under this setting, the set of seen scene classes contains grass, river, industrial, field, forest, residential, and parking, while the set of unseen scene classes contains ocean, runway,    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

LI et al.: ZSSC FOR HIGH SPATIAL RESOLUTION REMOTE SENSING IMAGES 9  Fig. 8. Classification maps of the large satellite image with respect to seven scene classes, namely, airport, industrial, grass, ocean, residential, river, and runway. (a) Classification map by the approach in [30]. (b) Classification map by the approach in [32]. (c) Classification map by the approach in [31].

(d) Classification map by the proposed ZSL-LP approach. (e) Classification map by the proposed ZSSC approach.

TABLE II  COMPARISON TO THE STATE-OF-THE-ART ZERO-SHOT LEARNING MODELS ON THE UC MERCED DATA SET WITH DIFFERENT UNSEEN/SEEN RATIOS. THE AVERAGE ACCURACIES ARE  FOLLOWED BY STANDARD DEVIATIONS (OVER 25 RANDOM SEEN/UNSEEN SPLITS)  and airport. Note that the test images from the Sydney data set not only come from unseen classes (i.e., ocean, runway, and airport) but also from seen classes (i.e., grass, river, industrial, and residential). We thus need make novelty detection to determine whether a test image comes from a seen class or not.

Concretely, for each seen class, a one-class support vector machine (SVM) classifier is trained with all the images from this class in the RSSCN7 data set, where the pretrained GoogLeNet features are exacted for the training images. If all the trained one-class SVMs decide that a test image does not belong to any of the four seen classes (i.e., grass, river, industrial, and residential), we regard this image as an image from unseen classes, and adopt the proposed ZSSC approach to predict its label; otherwise, only the domain adaptation approach [24] is used to predict its label. The parameters in the one-class SVM for novelty detection are tuned up on the images in the seen classes in the RSSCN7 data set.

2) Comparison to the State of the Art: Note that there exist two main steps in large HSR satellite image recognition, i.e., novelty detection and zero-shot learning. The experimental results show that both of the two steps are effective on the Sydney data set. Specifically, the novelty detection method achieves an accuracy of 89.2%, and the proposed ZSSC approach achieves an accuracy of 70.4%. Moreover, we made fair comparison to the state-of-the-art zero-shot learning mod- els [30]?[32], by replacing the proposed ZSSC approach with these models and keeping the other settings unchanged. The per-class and overall classification rates of different zero-shot learning models are reported in Table III. It can be seen that the proposed approach not only significantly outperforms the state-of-the-art models in terms of the overall performance, but also yields the best results on the largest unseen class  TABLE III  PER-CLASS AND OVERALL CLASSIFICATION RATES (%) OF DIFFERENT ZERO-SHOT LEARNING MODELS ON THE SYDNEY DATA SET. THE  OVERALL RATE IS COMPUTED OVER ALL THE IMAGES, AND NOT THE AVERAGE OF PER-CLASS RATES  TABLE IV  COMPARISON OF DIFFERENT ZERO-SHOT LEARNING MODELS IN TERMS OF TIME COST ON THE SYDNEY DATA SET  with respect to the per-class rate. In addition, the classification maps of all the models are also shown in Fig. 8. The proposed approach is still shown to perform the best. These observations demonstrate that the proposed approach is more scalable for real-world applications in remote sensing.

We also make comparison to the state-of-the-art zero-shot learning models on the Sydney data set in terms of time cost, which is shown in Table IV. All the experiments are conducted on a computer with 3.9-GHz CPU and 32-GB RAM. From Table IV, we can observe that the time-consuming of our approach on the large scenes is acceptable.



V. CONCLUSION  This paper proposes a novel scene classification approach for HSR remote sensing images to recognize images from unseen classes without any visual sample in the training set.

In this approach, we first use the word2vec model to map names of scene classes to semantic vectors. A semantic- directed graph is then constructed over the semantic vectors for describing the relationships between unseen classes and seen classes. With the semantic-directed graph and knowledge transferred from images in the seen classes by an unsupervised domain adaptation model, a label-propagation algorithm is developed to measure the distance between test images and each unseen class for recognition of the unseen scene classes.

To further suppress the noise in the zero-shot classification    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

results, a label refinement approach is developed based on sparse learning. Experimental results show that the proposed approach significantly outperforms the state-of-the-art zero- shot learning models in scene classification for HSR remote sensing image. This means that the proposed approach can provide an effective way for remote sensing scene classifica- tion in the shortage of labeled data. In the future work, we will make further improvements in two aspects: 1) the word2vec model is trained only with the documents on geoscience and remote sensing to obtain better semantic relationships among scene classes and 2) deep learning is used to directly formulate the ZSSC problem.

