Pattern Recognition, Informatics and Medical Engineering,  March 21-23,  2012

Abstract?Web Mining Systems make use of the redundancy of data published on the Web to automatically extract formation from existing web documents. The crawler is an important module of a web search engine. The quality of a crawler directly affects the searching quality of such web search engines. Such a web crawler may interact with millions of hosts over a period of weeks or months, and thus issues of robustness, flexibility, and manageability are of major importance. Given some URLs, the crawler should retrieve the web pages of those URLs, parse the HTML files, add new URLs into its queue and go back to the first phase of this cycle.

The crawler also can retrieve some other information from the HTML files as it is parsing them to get the new URLs.  This paper proposes a framework and algorithm, TOPCRAWL for mining. The proposed TOPCRAWL algorithm is a new crawling method which emphasis on topic relevancy and outperforms state-of-the-art approaches with respect to recall values achievable within a given period of time. This method also tries to offer the result in community format and it makes use of a new combination of ideas and techniques used to identify and exploit navigational structures of websites, such as hierarchies, lists or maps. This algorithm is simulated with web mining tool Deixto and the basic idea has been implemented using the JAVA and Results are given. Comparisons with existing focused crawling techniques reveal that the new crawling method leads to a significant increase in recall whilst maintaining precision.

Keywords? Blog, Mining, B-SIGNET, Association Rule mining, Clustering, Social network analysis.



I.  INTRODUCTION The Web?an immense and dynamic collection of pages  that includes countless hyperlinks and huge volumes of access and usage information?provides a rich and unprecedented data mining source. However, the Web also poses several challenges to effective resource and knowledge discovery:   1. Web page complexity far exceeds the complexity of  any traditional text document collection.

2. The Web constitutes a highly dynamic information  source: 3. The Web serves a broad spectrum of user  communities:  4. Only a small portion of the Web?s pages contain truly relevant or useful information:  A. Crawling and Crawler in search Engines  All search engines available on the internet need to traverse web pages and their associated links, to copy them and to index them into a web database. The programs associated with the page traversal and recovery is called crawlers. The main decisions associated with the crawlers algorithms are when to visit a site again (to see if a page has changed) and when to visit new sites that have been discovered through links. The parameters to balance are network usage and web server overload against index accuracy and  completeness.



II. CRAWLER  AND TOPICAL CRAWLER ? TO THE CORE  Before the advent of the web, traditional text collections such as bibliographic databases were provided to the indexing system directly. In contrast, there is no catalog of all accessible URLs on the web. The only way to collect URLs is to scan collected pages for hyperlinks to other pages that have not been collected yet.

This is the basic principle of crawlers. They start from a given set of URLs, progressively fetch and scan them for new URLs (out links), and then fetch these pages in turn, in an end less cycle. New URLs found thus represent potentially pending work for the crawler. The set of pending work expands quickly as the crawl proceeds, and implementers prefer to write this data to disk to relieve main memory as well as guard against data loss in the event of a crawler crash. There is no guarantee that all accessible web pages will be located in this fashion; indeed, the crawler may never halt, as pages will be added continually even as it is running.

A focused crawler is a program used for searching information related to some interested topics from the Internet[1]. The main property of focused crawling is that the crawler does not need to collect all web pages, but selects and retrieves relevant pages only. Because the crawler is only a computer program, it cannot determine how relevant a web page is. In the literature, there are many researches mentioning about the focused crawling.

Apart from out links, pages contain text; this is submitted to a text indexing system to enable information retrieval using keyword searches. The main goals of a crawler are:         1. The index should contain a large number of web objects that are interesting to the search engine's users.

2. Every object on the index should accurately represent a real object on the web  3. Generate a representation of the objects that capture the most significant aspects of the crawled object using the  minimum amount of resources.

We can model different crawlers using three coordinates that are web page attributes: quality, quantity, freshness.

The crawler maintains a list of unvisited URLs called the frontier. The list is initialized with seed URLs which may be pro- vided by a user or another program. Each crawling loop involves picking the next URL to crawl from the list, fetching the page corresponding to the URL through HTTP, parsing the retrieved page to extract the URLs and application specific information, and finally adding the unvisited URLs to the frontier. Before the URLs are added to the frontier they may be assigned a score that represents the estimated benefit of visiting the page corresponding to the URL. The crawling process may be terminated when a certain  number of pages have been crawled. If the crawler is ready to crawl another page and the frontier is empty, the situation signals a dead-end for the crawler. The crawler has no new page to fetch and hence it stops. The flow of a generic crawler has been given in figure 1.

Figure 1.  Generic Crawler sequence  The aspiration of this paper is to improve the document retrieval process by retrieving more relevant documents in shorter period of time than currently possible with existing techniques.

This new method focuses in particular on downloading only a minimal number of documents to be validated as downloading and validation are time consuming tasks. The principal idea of our method is based on the assumption that websites are organized in a way that allows data to be easily accessed by human users, i.e. almost all websites provide simple means of information access, both through navigational structures such as hierarchies, site maps or lists and through search functionalities.

Hence for simulation we have created.



III. COMMUNITY MINING Community mining in search engines allows us to find  clusters of related sites which are densely connected to each other, but only sparsely connected to the rest of the network.[2] In a similar way that the formal organization charts break down the employees into smaller substructures, community mining finds natural communities in the informal social network.

Members of a community tend to mainly communicate with the other members of that community, and less so with people in the rest of the network. The implications of the social networking scenario helps us to create a prototype model and to formulate effective parameters in the idea focused in this paper.

Figure 2.  Community consideration in TOPCRAWL  The figure 2 shows that the core which is surrounded by related fields are grouped based on the similarity of the TF-IDF scores.

The most important objects representing the concept of a whole community lie in the center and are called core objects. Affiliated taxonomy objects, which are related to the core objects, surround the core with different ranks. With this model, a community is defined as a four-tuple <CR, TA, FN, Va>. CR denotes the core object set; TA denotes the affiliated taxonomy object sets; FN is the affiliation definition function measuring two objects x and y, and will return a positive value if x is affiliated by y Va is the importance vector for TA to measure the affiliating degree for every object in TA to the core.



IV. PROPOSED MODEL The proposed model computes the relevance-score of the page with user-defined relevant formula, then predict the relevance- score of the link with shark-search, at last compute the authority-  Initialize list with Seed URL  Check for Termination  Pick URL from list  Fetch Webpage  Parse Webpage  Add URL to list  {No URL}  {Done}  {End}  {Not Done}  Core  Taxonomy 1  Taxonomy 2  Taxonomy 3        score of URLs in the queue to be crawling and determine their priority according to the comprehensive value of relevance-score and authority-score, namely first crawl relevant and quality page[3]. Similarity can be calculated as in Equation (1):  ? ? ? ?  ? ? ? ?2 2  _  1 1  1 2 ( 1, 2)  1 2i i  n i i  i weighted query yx  i i  cw q cw q sim s s  w q w q  ?  ? ?  ? ?  ?  ?  ? ?  (1)  where cwi(q1) and cwi(q2) are the weights of the i- th common keyword in the query q1 and q2 respectively, and wi(q1) and wi(q2) are the weights of the i-th keywords in the query q1 and q2 respectively. Usually, tf * idf is used for keyword weighting.

Figure 3.  TOPCRAWL consideration and model  Pseudo code for TOPCRAWL Step 1: while (queue of linkage is not null)  (amount of crawling pages <   T_Crawl) do Step 2: Get the linkage at the head of queue an downloading web page P , the linkage linked and calculate the relevant to topic T; relevance(P)  = similarity(P, T_Main) if relevance(P) < T_Link then goto step 3 Step 3: Dismiss Page P and all of linkages in this page; Stop the process Step 4: for each linkages a in the page P do score a as follow:  a. relevance(a) = similarity(a, T) b. if relevance(a) < T2 then c. dismiss a; Goto step 4.

Step 5:  if the linkage a has not been crawled then add linkage a into the queue of linkage Step 6: Stop the process.

TABLE I.  RELEVANCE TABLE  Search terms Weight Business 1.0  Organization 0.65 Solution 0.42  Consumer 0.71 Company 0.34 Executive 0.46  Management 0.27  A sample relevance table is shown in table 1. This table is constructed for the search term ?Electronic business?.



V. EXPERIMENTS Perhaps the most crucial evaluation of focused crawler is to measure the rate at which relevant pages are acquired, and how effectively irrelevant pages are filtered out from the  crawl. This harvest ratio must be high, otherwise the  focused crawler would spend a lot of time to eliminate irrelevant pages, and it may be better to use an ordinary crawler instead. It would be better to judge the relevance of the crawled pages by human inspection, even though it is subjective and inconsistent. But it is impossible for hundreds of thousands of pages our system crawled.

Therefore we have to run an automatic classifier over the collected pages.

__ _  relevant pagesHarvest ratio pages downloaded  ?                                         (2)   The TOPCRAWL is compared against existing  Crawler, a crawler based on the pagerank algorithms[6][7][8]. To evaluate the three crawlers impartially, we implement TOPCRAWL and these two existing algorithms on the same hardware and software platform. The following table shows some comparative results of reliable authority extraction offered by the TOPCRAWL algorithm versus baseline (PageRank algorithm)  TABLE II.  AUTHORITY PAGES DETECTION  Search Term : cellphone Website Found by  TOPCRAWL Actually existing  Found by Baseline  Mobicreed.com 289 302 57 www.mobiledia.com 493 507 58 www.Mobile-phones-  uk.org.uk 96 108 84  www. Reviews.cnet.com 86 89 88 www.mobilereview.com 612 626 142  Search Term : Digital camera Website Found by  TOPCRAWL Actually existing  Found by Baseline  Imaging-resource.com 217 255 46 Dcresource.com 134 150 82  Parkcameras.com 195 227 112 Steves-digicams.com 186 214 149  Dcreview.com 183 196 195     Lexicons  Web URL queue  Irrelevant web database  Parser, Extractor and Lexicon Constructor  Relevance Calculator  Results offered to  User  Webpage Repository  Topic Drift and Filter  Relevant web database  Seed set URL                                              Figure 4.  Screenshots of TOPCRAWL ? implemented in JAVA                          Figure 5.  Screenshots of Simulations to check the links with tools[4][5]  The experiment is conducted on the Windows XP??  Operating System with Intel? Pentium? 2.8 GHz CPU and 1024MB of physical memory. The TOPCRAWL is developed with JAVA [9][10] and simulations are done with utilities : web link checker, website analyzer. Along with that Web mining tool Deixto is used.

Figure 6 shows a good result about our crawler. The figure shows that our crawler outperforms the BASELINE crawler significantly. The ability that it finds relevant pages keeps quite a high level. In fact, after experimenting many times using different seed URLs, the results are almost the same, that is, at the beginning, it is the strongest, but as the time goes by and more pages are fetched, its performance decreases at a mild speed, and then stabilizes. Equation (3),(4) are the parameters used to assess the quality of the results.

? ? ? ? ? ?  Re Re Re  Re levantdocumets trieveddocuments  call levantdocumets   ?           (3)  ? ? ? ? ? ?  Re Re Pr  Re _ levantdocumets trieveddocuments  ecision trieved documets   ?          (4)  The comparison has been given as a graph in Figure 6.

0.5   1.5   2.5   Link CL-Level 1 CL-Level 2 CL-Level 3  BASELINE_Precision TOPCRAWL_Precision  BASELINE_RECALL TOPCRAWL_RECALL   Figure 6.  Comparison of the Performance of TOPCRAWWL with  BASELINE Method

VI. CONCLUSION  We introduced an algorithm for a crawler that combines search strategy based on content and search strategy based on link structure. Experiments show that it is valuable.

This crawler focus especially on the topical crawling and since a new technique of relevancy table and calculator has been used to improve the results to offer the communities. The term community here refers the group of results offered  by the system which were interrelated. We also plan to analyze the evolution of communities. Since deriving graph communities to lay down the relationship could yield greater benefits. We will account for it in the future.

