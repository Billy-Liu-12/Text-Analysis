Parallel Processing of Big Data using Power Iteration Clustering over MapReduce

Abstract-Extracting useful information from dataset measuring in gigabytes and tetrabytes is a real challenge for data miners.

Clustering algorithm have the problem of scalability while dealing with big data. The problem can be handled using parallel algorithm by executing them along with input data on high performance computer. The problem with graph based application requires much time for computation. PIC is an algorithm that is simple, fast, relatively scalable which requires the data and its associated matrix to fit in memory and this becomes infeasible for big data applications.  Scalability has been increased using p-PIC and this paper focus on exploring different parallelization strategies for minimizing and compelling communication cost. The algorithm works on with a parallel framework MapReduce. p-PIC algorithm deals with Hadoop cloud a parallel store and computing platform implementing p-PIC using Hadoop framework.

Keywords- p-PIC;  Hadoop; Fault tolerance; GBC.



I. INTRODUCTION Clustering is one of the most widely used techniques for exploratory data analysis, with applications ranging from statistics, computer science, biology etc.[18]. The goal of clustering is to determine the intrinsic grouping in a set of unlabeled data [9]. Clustering algorithm is an analytic tool to classify large data sets into correlative group. Most traditional clustering algorithms are sequential [6,7]. MapReduce is a framework for parallel and distributed processing of batch jobs [8]. The map tasks start with a data partition and the reduce task performs summation. MapReduce also requires that the operations performed at the reduce task to be both associative and commutative [11]. Parallel Iterative Clustering algorithm is computationally efficient because of Iterative matrix?vector multiplications and clustering of the one dimensional embedding of the data, a method for computing the largest eigenvector of a matrix is Power Iteration [12].



II. MAPREDUCING OVERVIEW MapReduce is a parallel programming technique derived from the functional programming concepts which is proposed by Google for large-scale data processing in a distributed computing environment. Hadoop project [13] provides a  distributed file system (HDFS).The representation of mapreduce in Figure 1 comprises a Map () procedure that performs filtering and sorting and a Reduce () procedure that performs a summary operation. The "MapReduce System" manages by marshalling the distributed servers, running various tasks in parallel, managing all communications and data transfers between the various parts of the system. This provides redundancy, fault tolerance and overall management of the whole process.

Figure 1.Representation of MapReduce concept   MapReduce's stable inputs and outputs are usually stored in a distributed file system. The transient data is usually stored on local disk and fetched remotely by the reducers. The mapping operation can be performed in parallel, when each mapping operation is independent of the other. The parallelism helps to overcome failure, when one node fails the work is assigned to other nodes as the input data is still available. The data structure (key, value) pair is used to define both map and reduce methods[ 15].

Map step: The master node divides the problems into smaller pieces and assigns them to work node[14].

Map (key1,value1) ? list(key2,value2)  The Map function is applied in parallel to every pair in the input dataset. This produces a list of pairs for each call. The Reduce function is then applied in parallel to each group.

Reduce step: The master node collects the solution from work node and combines them with some procedure[14].

2014 World Congress on Computing and Communication Technologies  DOI 10.1109/WCCCT.2014.16   2014 World Congress on Computing and Communication Technologies  DOI 10.1109/WCCCT.2014.16   2014 World Congress on Computing and Communication Technologies  DOI 10.1109/WCCCT.2014.16   2014 World Congress on Computing and Communication Technologies  DOI 10.1109/WCCCT.2014.16     Reduce (k2, list (v2)) ? list(v3)  The Architecture shown in Figure 2 allows end-users to implement any application-specific framework by which can request resources from the Resource Manager and utilize them as they see fit under familiar notions of isolation.

Figure 2.Architecture of MapReduce  This Architecture supports multiple programming paradigms, such as Map Reduce, MPI and iterative models and allows use of the appropriate framework for each application.



III. DESIGN AND IMPLEMENTATION  A. Single node setup and Multinode setup Hadoop cluster is started using Local (Standalone) Mode, Pseudo-Distributed Mode, or a Fully Distributed Mode [13].

Install the hadoop -common/hadoop-hdfs and exporting the $HADOOP_HDFS_HOME, untar hadoop MapReduce tarball and also set the environment variable as HADOOP_MAPRED_HOME to the untarred directory[13].

$ mvn clean install ?DskipTests $ Cd hardtop-map reduce-project $ man clean install assembly:assembly ?Pnative  Set $HADOOP_YARN_HOME the same as HADOOP_MAPRED_HOME.

Multi-node is set by means of the Ubuntu environment [13] which is configured by means of the hosts file. Download and configure the Hadoop by configuring master slave settings. Then, start the master slave setup and the first map reduce on multi-node setup[13] .

B. Parallel Power Iterative Clustering Spectral clustering (SC) is currently one of the most popular clustering techniques because of its advantages over conventional approaches such as K-means and hierarchical clustering. The use of computing eigenvectors, makes SC time consuming. To overcome this limitation, PIC finds only one pseudo-eigenvector, which is a linear combination of the eigenvectors .A pseudo-eigenvector is not a member of the eigenvectors but is created linearly from them. Therefore, in the pseudo Eigen vector, if two data points lie in different  clusters, their values can still be separated. The parallel algorithm for the PIC has been formulated[12].

C. General Algorithm  Step 1. Inputs are given to the master. The master broadcasts input to all the slaves.

Step 2. Each slave considered as a processor receives input from the master and process the input file dividing into chunk of data.

Step 3. Slave calculates the similarity and normalizes matrix.

Step 4. Slave sends the result to the master Step 5.Maser concatenates the result, produces the clusters.

D. Pseudocode The Pseudocode for the algorithm consists of a master and a slave process. The master process has been shown in Figure 3 and the slave process in Figure 4.

Figure 3.Pseudocode for Master process      Figure 3.Pseudocode for Master process                Figure 4.Pseudocode for Slave process   The experimental results of PIC algorithm are  shown in Figure 5 INPUTS:[3 12 6;8 6 12;6 23 11]      [14 21 23;46 5 6;7 8 9]      Figure 5.The graph for various inputs  Begin Receive indices from master; Read chunk ata receive from master; Calculate sub matrix Ai ; Calculate row sum Ri; Send Ri; Assign Wi = Di-1 Ai ; Receive Vt-1 ; Calculate Vit =  Wi vt-1;  Send Vit; End  Begin Calculate the no of splits of the data; Read starting and the ending indices; Send index to all slave; Broadcast index; For i=1 To n, no of slaves Receive Rib; R+=Ri; End V0=R/||R||1; Do Send V0 to all slave; For i=1 to n Receive Vit; End While(|?t-?t-1|==0); End

IV. PROPOSED WORK  Node Failure is a major issue that occurs during the implementation of the algorithm that has to be considered in parallel power iterative clustering algorithm.[12]. The failures overlap, and they are handled by (FCFS). The mapreduce performance under failures of single task performance happens when an interruption occurs which is iterate until condition is met [17]. In worst performance, the failed tasks will be re-executed on the same node after failure is recovered. For best performance, the failed tasks have data replicas on other healthy nodes which can be re-executed without any additional data migration [17].

A. Algorithm for p-PIC using MapReduce Parallelisation is a process of executing the task in parallel so that the scalability and performance can be addressed. This leads to node failure and fault tolerance because of working with the MPI framework . As MapReduce framework easily handles node failures this implementation is done with MapReduce[10]. The pseudocode for map and reduce is shown below for the   p-PIC algorithm.

input: set x={x1,x2,?,xn} representing data set and  similarity function s(xi,xj) let A[n,n] be the similarity matrix let D[n,n] be the diagonal matrix let N[n,n] be the normalized similarity matrix let R be the row sum of normalized matrix for i=1 to n do D[i,i]=0; for j=1 to n do A[i,j]= s(xi,xj) D[i,i]=D[i,i]+A[i,j] repeat repeat N=D-1A produce output record <R> end function   Figure 6.Pseudocode for Map function    input : constant ?  represents the normalizing constant let v be the vector let ? be the velocity v0 = R/||R||2 loop vt=?Wvt-1 ?t=|vt-vt-1| until (?t!=0) produce output record <vt> end function  Figure 7.Pseudocode for Reduce function

V. CONCLUSION This paper review about the map reduce and parallel-PIC algorithm and also describes the implementation of a parallelization of power iteration clustering a recently developed clustering algorithm. The parallelism is introduced    as an important solution that could improve the response time and scalability. The implementation of p-PIC in MapReduce will address the problem of node failure and fault tolerance which is to be addressed as a future work.

