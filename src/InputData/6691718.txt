A Novel Visual analytics Approach for Clustering Large-Scale Social Data

Abstract?Social data refers to data individuals create that is knowingly and voluntarily shared by them and is an exciting avenue into gaining insight into interpersonal behaviors and interaction. However, such data is large, heterogeneous and often incomplete, properties that make the analysis of such data extremely challenging. One common method of exploring such data is through cluster analysis, which can enable analysts to find groups of related users, behaviors and interactions. This paper presents a novel visual analysis approach for detecting clusters within large-scale social networks by utilizing a divide- analyze-recombine scheme that sequentially performs data partitioning, subset clustering and result recombination within an integrated visual interface. A case study on a microblog messaging data (with 4.8 millions users) is used to demonstrate the feasibility of this approach and comparisons are also provided to illustrate the performance benefits of this approach with respect to existing solutions.

Keywords-Divide and Recombine; Cluster Analysis; K- means; Visual Analysis;

I. INTRODUCTION  As humans have become more intrinsically connected to technology, details of their behaviors and interpersonal connections have become increasingly transparent. Activities and communications can automatically be collected during interpersonal activities such as mobile phone communica- tions, e-commerce transactions and internet activity (Tweets, Facebook posts, blog entries), and such data can be classified into two components: behavioral data and personal data.

Behavioral data can be thought of as a record of a user activity within society, for example, a mobile phone call between individuals, a video conference, and sending a tweet all provide links between a single user and other individuals. Such data tends to reflect a user?s daily patterns and behaviors [1], and we will refer to this type of data as user behavior data. Personal data can be thought of as being intrinsically tied to a single individual and would include things such as their age and gender. We will refer to this  type of data as user tag data.

Given the ubiquity of such data, methods for exploring  and analyzing social data links have become a critical research topic. By analyzing user tags and user behavior data, it is expected that insights into societal patterns can be gleaned and used to inform governmental policy and business decisions as well as influence human behavior [2].

For example, previous work [3] explores clustering users into groups to develop targeted market strategy based on demographics and behavioral patterns. However, our abil- ity to extract knowledge from these large heterogeneous data sources is still limited due to the latent, subtle and unpredictable relationships that may be hidden within the data. These problems are further compounded due to issues of data size and the non-linear computational complexity related to the data size.

One means of reducing the computational complexity of an analysis task is through the application of parallelization using distributed computing [4] and super-computing [5].

However, for analysis tasks that require frequent data transfer and interaction, parallelization is still problematic.

Another means of reducing the computational complexity would be through the use of divide-and-conquer schemes.

The divide-and-conquer algorithm works by recursively breaking down a problem into sub-problems that can then be recombined to address the original problem.

Recent work has demonstrated the effectiveness of divide- and-conquer approaches for handling large data. For exam- ple, RHIPE [6] is built upon R and has been specifically developed to provide a divide-and-conquer solution for the statistical analysis of large data. RHIPE utilizes the MapRe- duce framework, and MapReduce itself has been used in large scale data analysis (e.g., [5]). Furthermore, in the data mining community, it is believed that the divide-and- recombine scheme will be adapted to handle the scalability problems found in many complex machine learning tasks [7].

This paper presents our proposed methodology for en- hancing the visual analysis of large-scale social data uti- lizing the divide-and-recombine scheme. Our approach is specifically designed to handle a central problem for social network analysis: clustering users into groups by leveraging the user tags and user behavior information. The key idea is to sequentially perform data partitioning, subset cluster- ing and result recombination utilizing an integrated visual interface. The entire pipeline integrates a suite of visual analysis techniques to provide an effective workspace for the partitioning of large collections of instances, determining cluster parameters and merging multiple clusters. Our main contributions include: ? The identification and systematic implementation of the  divide-and-recombine scheme for clustering of social data in an integrated visual exploration process;  ? An incremental data clustering technique that enhances the conventional K-means algorithm in both the clus- tering quality and performance;  ? A novel context-aware subset-clustering analysis.



II. RELATED WORK  In this section, we will briefly review related works in each of these categories.

A. Data Clustering  Recently, the application of interactive supervised learning has become a major focus in the visual analytics community.

Pelekis et al. [8] designed a novel distance function as a similarity measurement for the analysis of movement data. Guo et al. [9] proposed an approach for steering the clustering process by building a hierarchical spatial cluster structure within the high-dimensional feature space.

Unsupervised learning algorithms can be divided into two major categories: hierarchical clustering and partitioning clustering [10]. Perhaps the most commonly used algorithm for partitioning data is the K-means algorithm [11]. Un- fortunately, such partitioning algorithms tend to be non- linear in their runtime, despite efforts to improve such techniques [12]. As the computational complexity increases non-linearly with the data size, new solutions are necessary.

There are also some pioneered work on parallel cluster- ing algorithms. For instance, Zhao et al. [13] proposed a parallel K-means clustering algoithm based on MapReduce framework. Yet, this  B. Divide-And-Conquer for Data Analysis  Divide-and-Conquer has proven to be an efficient mecha- nism for reducing the runtime of complicated procedures such as sorting, the multiplication of large numbers and discrete Fourier transforms. Recently, it has been applied to address the underlying issues of computational complex- ity associated with big data. Several fundamental statistics  problems including matrix factorization and statistical in- ference have been modified to utilize a divide-and-conquer methodology [7]. For example, the RHIPE system that is built on R utilizes an underlying Hadoop structure and has been applied to a variety of analytical domains [6]. Results from this work demonstrate the feasibility of utilizing divide- and-conquer for exploring large scale data.

C. Social Data Analysis Recently, many tools have been developed to explore  the complex spatiotemporal relationships underlying such data. Work by Field et al. [14] explored the analysis of microblog data and others like crime investigators [15] and urban planners [16] have also begun utilizing social data.

MacEachren et al. [17] developed Senseplace as a tool for exploring the message density of actual or textually inferred Twitter message locations, ScatterBlogs [18] presented a scalable system enabling analysts to work on quantitative findings within a large set of Microblog messages.However, no previous work in this area (to our knowledge) utilizes a divide-and-recombine scheme for large scale clustering of users and behaviors in social data analysis.



III. A VISUAL ANALYSIS APPROACH FOR CLUSTERING LARGE-SCALE SOCIAL DATA  The focus of this work is on large-scale (multi-million) user data sets. As input, our social data contains a set of user provided information (age, gender, etc.) that we denote as user tags and their associated behavioral information which we define as user behavior. The user behavior information provides links between users.

Within such data, there are expected to be clusters of individuals and patterns which can be analyzed for mar- keting purposes, criminal activity or various other research questions. A key challenge in clustering such data is that the application of clustering algorithms to a set of user tags requires the formulation of user tag and behavior informa- tion into a point data set where each point is associated with multiple attributes. The set of attributes form a user attribute vector. Note that the construction of the user attribute vectors varies with specific data and tasks, and details for formulating this vector for each dataset will be given in the case study. Our goal is to provide analysts with a means of clustering the user attribute vectors to find commonalities between users and behaviors. Conceptually, our approach consists of four stages (Figure 1): data division, data clustering, result recombination, and visual exploration.

In the first stage, a massive collection of user attribute vectors is generated from the social data. It is randomly subdivided into many subsets by means of a visually assisted data division process. In the second stage, a subset of the data is clustered using an improved k-means algorithm, in which a novel context-aware technique is leveraged to op- timize the clustering parameters. Subsequently, the result is     Subset 1  Subset 2  Subset N  Adaptive Data Division  Context-aware Subset Clustering  Incremental Data Clustering Result Recombination  Visual Exploration and Interactions  . .

.

. .

.

Parameters  . .

.

Outliers  Outliers  . . .1 2 k  Outliers:  Clusters:   ......

2Clusters: ......

Clusters: 1 2  ......

......

Clusters:  ......

1 2 ...... Outliersk  k  k  Social data  Pixel chart view  The entire user set  Figure 1. Conceptual overview of our approach. (a) Visual assisted adaptive data division; (b) Context-aware subset clustering; (c) Incremental recombination of clusters; (d) Visual exploration of clusters and unusual data points.

propagated into the other subsets one-by-one and clustering is then done on each subset. In the third stage, the clusters of all subsets are then merged with a hierarchical clustering process. An integrated visual interface is designed to allow analysts to explore the user tags and associated user behavior information of each cluster. Compared with existing divide- and-recombine approaches, our approach combines data mining techniques and interactive visuals, which enables the analyst to explore patterns in large-scale social data.

A. K-Means Clustering One of the key parts of our data analysis process is the uti-  lization of the K-means algorithm. The K-means algorithm is one of the most popular clustering methods [10], and is employed in our approach due to its simplicity, efficiency and robustness. For a set of points X = {xi|xi ? Rd}, K data points are randomly selected as the initial centroids of the intended K clusters. Two steps are then recursively performed until the algorithm converges: ? Step 1: Assign each data point to its closest centroid.

? Step 2: Update the centroids with the mean value of all  the data points assigned to them.

Suppose that the resultant clusters are X1,X2, ...,XK . The  quality of the clustering subject to K can be measured by the  sum of the distances between each point pair in the clusters:  D(K) = k  ? r=1  ? xi,x j?Xr  ?xi?x j? (1)  The clustering quality depends on the specification of the initial centroids [19]. Typical K-means solutions either repeat the computation of centroids several times using different initial seed points and then choose optimal result based on the quality of the clusters, or apply specific optimization which can result in long runtimes and storage overheads when handling large-sized data. Our approach to improving the performance of K-means in large-scale data is to utilize a divide-and-recombine scheme to reduce the data complexity of K-means clustering. Furthermore, in order to adapt the K-means algorithm specifically to social data, there are several key aspects that need to be addressed: ? Data Division How to determine the appropriate subset  size so that the combination of clustered results of the subsets can yield the same quality as the conventional K-means algorithm?

? Data Clustering How to select K and a distance metric for multi-dimensional user attribute vectors to achieve optimal clustering results?

? Recombination and Analysis How to evaluate the clustering quality and discover interesting users and patterns from the clustered results?

B. Adaptive Data Division In terms of clustering, one criteria for dividing a massive  dataset into smaller subsets is to preserve the statistical distribution in each subset [20], which poses a challenging problem in determining initial splits in the divide-and- recombine process. We propose to randomly subdivide the entire set into subsets with uniform sizes [10] and adaptively determine the subset size by considering both the perfor- mance and quality.

Our solution is to employ the use of a visually assisted adaptive data division scheme. For each subset, a pixel chart [21] is generated to show the statistical distribution of each attribute value of the user attribute vector. In particular, all subsets are organized into a 2D array, which is further represented with a set of 2D cells. The saturation of each cell encodes the percentage of users with the corresponding attribute value in each subset, as illustrated in Figure 2.

1 2 3  4 5 6  7 8 9  Cluster ID  0 1 Percentage  Figure 2. A pixel chart visualizes the statistical distribution of certain attribute with respect to a subset size. Here, each cell represents a subset, and encodes the statistical value of the subset.

If the pixel chart of an attribute exhibits an approximately uniform appearance, one can assume that the subset has a statistical distribution that is similar to the overall distribu- tion of a given attribute. If the subset size is appropriate for all attributes, then one can assume that it represents a reasonable statistical sample of the original dataset with respect to the specific user attribute vector. In this manner, an analyst can quickly assess the suggested subdivision and modify the current data division to try and improve the results. Surely, statistical sampling techniques would work in these situations, but may require more user interaction time. In order to enable such actions, our system provides simple user interactions in the pixel chart view: specifying the subset size, studying the distribution of a specific subset size, comparing the distributions of different subsets, and  determining an optimal subset size. Figure 3 shows a group of pixel charts with a subset size 50.

Figure 3. The pixel charts with respect to 42 user attribute values with 50 subsets.

The algorithm for adaptive data division is summarized as follows:  Algorithm 1 Visual Assisted Adaptive Data Division Initialization: set a small subset size Ns (e.g., 5K for a set with 1M points) to generate a relatively large number of subsets (e.g., 200).

while TRUE do  Generate a pixel chart for each attribute.

Visually explore the pixel charts of all attributes.

if All pixel charts show similar appearance then  Return the current subset size and subsets.

FALSE.

else Ns ++.

end if end while  C. Context-aware Subset Clustering Once the analyst is satisfied with the data subset choice,  the data subsets then enter the clustering phase. In clustering the first subset, two parameters are essential to the clustering quality: the cluster number, K; and the distance metric for the points.

Cluster Number K There have been many methods to find an optimal cluster number K [22]. In our system, K is heuristically determined by computing and plotting the sum of the distance function D(K) (Equation 1). Typically, the point where the curve becomes flat (shown in red in Figure 4) is identified as a reasonable choice for K [22].

Attribute Weights K-means partitions the points based on the distances between points in the high-dimensional     1         2        3         4         5         6        7        8         9         10      11      12       13      14  Distance        Figure 4. The relationship between the cluster number K (the x axis) and the sum of distance D(K) (the y axis).

space, and the choice of distance metric chosen will greatly influence the results. In this case, we cluster the underlying dataset based on the user attribute vectors, where the prob- lem is identical to specifying an appropriate weight for each attribute: a high weight can be used for a salient attribute, and an attribute has little influence on the clustering if its weight is low. For clarity we define the weights of a user at- tribute vector to be x= (x1,x2, ...,xd) as w=(w1,w2, ...,wd).

However, the influence of each attribute on the cluster- ing result and the relations among attributes are difficult to model and represent. Furthermore, attributes may have different data types (numerical, ordinal and categorical) and distinctive value ranges. Thus, it is desirable to discover the significantly relevant attributes and associated configurations by studying the influences of different weights on the clus- tering results, and design an enhanced weight computation scheme.

In our approach, the weight of an attribute subject to a clustering process is determined by evaluating the sensitivity of the clustered result with respect to different weights. For the jth attribute, a sequence of weighting configurations {w(k),k = 1,2,3, ...,M} is generated:  { wl(k) = 1/k, if l = j; wl(k) = (1?1/k)/(M?1), else. (2)  where M is an adjustable constant.

For each weighting configuration {w(k)}, a K-means  clustering is performed by using the weights in the distance metric. We further compute the Hausdorff Distance [23] between the centroid sets of the clustered results C(m) and C(n) associated with {w(m)} and {w(n)}:  H(m,n) = max x?C(m)  min y?C(n)  ?x? y? (3)  where x and y denote a centroid of the clustered resultsC(m) and C(n), respectively.

The set of {H(m,n),m= 1,2...,M,n= 1,2,3, ...,M} forms an M?M matrix. By normalizing and encoding each ele- ment of the matrix with a color, a sensitivity map associated with the underlying attribute is generated (Figure 5). This mapping implies the proximity among the clustered results under different weighting configurations. A low value of H(m,n) indicates that two weighting configurations yield similar clustered results, and vice versa. The overall distri- bution of the sensitivity map and its average value can be used to study the relevance of the attribute to the clustering.

A sensitivity map can be generated for each attribute. The set of the maps characterizes the influence of a set of user attributes on the variation of the clustering process and can be used to guide the weight function. The weight of each attribute is specified as to be proportional to the average value of its associated map. The user can manually adjust the sensitivity of a map by moving it vertically. The weights corresponding to the sensitivities are shown with a radial map.

0.1 1.0  0.1  1.0  weight w  ei g h t  0 1 sensitivity  Figure 5. A sensitivity map is generated for each attribute.

D. Incremental data clustering The first subset can be either randomly selected, or can  be manually specified by the users with the help of the pixel charts of subsets. After clustering the first subset, an incremental data clustering procedure is applied to cluster all other subsets. The clustering configuration (the weights, etc.) for the first subset is applied in the clustering process of     all other subsets. Algorithm 2 briefly summarizes the details.

Algorithm 2 Incremental Data Clustering for Each subset do  Set the clustered centroids of the first subset as the initial centroids.

Set the cluster number and attribute weights of the first subset as the ones for the underlying subset.

Perform the K-means clustering.

end for  The incremental data clustering scheme not only allows us to utilize the divide-cluster-recombine mechanism, but it also achieves higher performance and quality when compared with the standard K-means algorithm. Figure 6 (a) compares the sum of point-to-centroid distances of the cluster results of both our approach and the standard K-means algorithm.

In principal, the incremental data clustering scheme achieves higher accuracy because the cluster centroids in clustering each subset are determined in handling the first subset. With the standard K-means algorithm, the cluster centroids are randomly initialized when clustering each subset, leading to varied clustering results [24]. The benefit of the incremental scheme is also significant with respect to the running time.

Figure 6 (b) reports the total running time of both approaches for the same dataset. In general, our approach runs faster with different cluster numbers K, and achieves an average 10% - 100% acceleration. More importantly, the clustering of subsets are parallelizable because the subsets are inde- pendent. Concrete performance analysis will be given in the case study.

To allow for visual exploration of the clusters of each subset, our system employs the principle component analysis (PCA) algorithm to project all data points of a subset or the entire set into the 2D space. PCA is chosen because it is fast with a linear computational complexity, and can handle a large amount of points.

E. Result Recombination  After all the subsets are clustered, we utilize the standard hierarchical clustering method [10] to recombine the results of all subsets. Again, the cluster number K for clustering the entire set is set to be the same as that of the first subset.

The distance between cluster A and cluster B is defined as:  ?(A,B) = n(A)n(B) n(A)+n(B)  ?centroid(A)? centroid(B)?2 (4)  where is n(?) denotes the size of a cluster, and centroid(?) denotes the cluster centroid.

The key idea of the hierarchical clustering algorithm is to recursively combine all clusters (see Algorithm 3).

Standard  Incremental  Sum of Distances  cluster Number 1      2    3     4      5      6     7    8     9     10   11    12   13   14   15  16    17   18         Standard  Incremental  Time (s)  Cluster Number 1      2    3     4      5      6     7    8     9     10   11    12   13   14   15  16    17   18  (a)  (b)  Figure 6. Comparing the sum of point-to-centroid distances (a) and the running time (b) of our incremental data clustering scheme (in dark blue) and the standard K-means algorithm (in light blue).

Algorithm 3 Hierarchical clustering of the clusters of all subsets  Initialization: Load all clusters of all subsets.

while TRUE do  Compute the distances between each pair of all clusters of all subsets.

Combine two clusters whose distance is the minimal.

if There are only K clusters then  Output the clusters.

FALSE.

else TRUE.

end if end while

IV. CASE STUDY Our clustering algorithm and visual interface were devel-  oped using Java. A parallelization to our clustering kernel is also implemented with the multi-thread feature of Java.

We also implemented the standard K-means algorithm. The three implementations are named as DR, PDR and STD. The performance reports are made on a PC with an i7 3.40 GHZ CPU, 16 G memory, and an Nvidia 680 video card.

We have conducted experiments on a Microblog Dataset consisting of 4.8 million users. It was collected from the largest social network (http://www.weibo.com) in China, i.e.,     Sina Weibo. The total data sizes is 399 MB.

In particular, the Microblog Dataset consists of 4,838,573  users and 18 user attributes. In our experiments, six attributes are used: OnlineTime, Bi-friendNumber, FollowerNumber, FriendNumber, PostNumber and FavouritePostNumber. Ta- ble I lists the meaning of these attributes. The entire set is divided into 76 subsets, each of which contains 64,000 users.

Table I SIX ATTRIBUTES OF THE MICROBLOG DATASET  Attribute Name(Abbr.) Meaning OnlineTime The time the user spend on Microblog  BiFriendNumber the number of people with which the user follows each other  FollowerNumber the number of people who follow the user to see his/ her updates  FriendNumber the number of people the user follows  PostNumber the number of messages the user sends out to his/her followers through Microblog  FavouritePostNumber the number of microblogs the user collects  Quality and Performance Applying our approach to the dataset found that the optimal cluster number is six. Figure 7 (a) compares the clustering quality of our approach and the standard K-means algorithm. In particular, the summed values of point-to-centroid distances with respect to different cluster number K are plotted.

To compare the performance, the timings with DR, PDR, and STD with respect to different cluster number K are displayed in Figure 7 (b). The comparison indicates that the naive implementation of our approach achieves higher performance than the standard algorithm. The most time consuming part is the incremental clustering of subsets due to the I/O operations for clustering each subset. This inefficiency is addressed in our parallel implementation, which yields a stable and high performance record.

Social Data Analysis In addition to the 6 clusters, a group of outlier points are detected in the clustering process (Figure 8). By further exploring the views of user attributes, some interesting facts are discovered. In particular, the pixel charts with respect to different attributes enable us to identify two specific groups of user patterns. One group has very high attribute values, while the other has low values. Commonly they have a high value of FollowerNumber, but a low value of FriendNumber (almost near zero). In addition, their BiFriendNumbers are zero, and their registered addresses are all overseas.



V. CONCLUSIONS This paper presents an effective divide-and-recombine  approach for clustering massive data assisted with a visual      2000000 STD  DR  4             5             6            7             8             9            10          11            12 K  Sum of Distances      2000 Time (s)  4               5             6              7             8               9             10            11           12 K  PDR  DR  STD  (a)  (b)  Figure 7. Results for the Microblog dataset. (a) The clustering quality comparison of DR and STD; (b) The performance comparison of DR, PDR and STD.

analytics pipeline. The entire pipeline integrates a suite of visual analysis techniques to provide an effective workspace for the partition of a large collection of instances, the determination of clustering parameters, and the merging of multiple clusters. Experimental results verify that our approach outperforms conventional solutions in both the quality and efficiency.



VI. ACKNOWLEDGEMENT  This paper is supported by NSFC (61232012,61272302), National High Technology Research and Development Pro- gram of China (2012AA12090), Zhejiang Provincial Natu- ral Science Foundation of China (LR13F020001), Doctoral Fund of Ministry of Education of China (20120101110134).

