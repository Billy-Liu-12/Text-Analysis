

AN IMPROVED ALGORITHM FOR MINING FREQUENT WEIGHTED ITEMSETS    Abstract. Mining frequent weighted itemsets (FWIs) from weighted items transaction databases (WITDs) has taken the interest of many researchers and there have been several works related to mining FWIs in recent years. Beside, in real world applications, sparse weighted items transaction databases (SWITDs) are very popular. For example, in the super market there are many items, but in the transaction there is only a small number of items. This paper proposes an interval word segment (IWS) structure to store and process tidsets for enhancing effectiveness of mining FWIs from SWITDs. With this structure, intersection operations of tidsets between two itemsets are performed blazingly fast. Experimental results obtained on a number of spare databases show that IWS outperforms the existing methods.

Keywords. Dynamic bit vector, Frequent weighted itemset, Interval-word-segment, Multi-bit segment, Tidset.



I. INTRODUCTION Association rule mining (ARM) [1-5] plays an important  role in data mining. ARM is used to identify relationships among items in transaction databases. Therefore, to mine association rules, it is necessary to mine frequent itemsets (FIs) in the first step. Then association rules will be generated from these FIs. Thus, mining FIs is interesting and has taken much attention in recent years [7-15]. They are divided into three main groups: 1. Candidate generation methods: These methods use a level- wise approach for mining FIs which scan databases many times. First, they generate frequent 1-itemsets then used them to generate candidate 2-itemsets, and so on until no more candidates can be generated. Apriori [1] is an exemple.

2. Methods that adopt a divide-and-conquer strategy: These methods compress the database into a tree structure and mine FIs from this tree by using a divide-and-conquer strategy. FP- Growth [2] and FP-Growth* [3] are exemplar algorithms.

3. Methods that use a hybrid approach: These methods use vertical data format to compress the database and mine frequent itemsets by using a divide-and-conquer strategy.

Eclat [4], dEclat [5], Index-BitTableFI [6] and DBV-FI [7] are some examples.

The first approach has fairly large processing time because of scanning database many times. The runtime of the second approach requires much time to travel of FP-tree to build FIs.

The third approach is the Eclat algorithm [9] based on IT-tree, it needs to scan databases once for building tidsets of items.

Therefore, the runtime is sharply reduced and this is the good method. However, the main disadvantage of Eclat algorithm is the usage of much memory to store tidsets resulting in certain  difficulties in calculating the intersection of tidsets between two itemsets, particularly for a big database with the number of transactions up to millions. Some reseachers have proposed solutions to improve Eclat. Zaki & Gouda in 2005 proposed dEclat algorithm [5] which uses the Diffsets concept instead of tidset. Diffset only keeps track of differences in the tids of a candidate pattern from its generating frequent patterns. Dong et al. [6] proposed the usage of BitTable to store tidsets. Vo et al. [7] proposed the use of dynamic bit vector (DBV) which is a significant improvement for BitTable.

Diffset [5], BitTable [6] and DBV [7] perform well on dense databases such as telecommunications or census data.

However, sparse databases with a small number of items on each transaction such as market-basket data or bills of drug in the hospital, the performance of these methods decreases incredibly.

Mining FIs are typically mined from binary databases where each item in a transaction may have a different significance. However, WITDs commonly used in real-world applications, have attributes weight (profit or benefits) of each item in a transaction. For example, an order of goods in a supermarket, each goods has its profits, etc. Thus, mining FWIs and weighted association rule mining (WARM) from WITDs has high practical applications and attracted much research attentions recently [7-15].

In this paper, we propose an improved method for using DBV, called IWS, for fast mining FWIs from SWITDs. Some concepts, definitions, theorems and corollary are proposed.

Based on them, we present a novel method for storing and processing tidset of itemset for fast mining FWIs using IWS- Tree. This method shows the effective for mining FWIs from SWITDs through experiments.

The rest of this paper is organized as follows. Section 2 presents background and reviews some related works. Section 3 presents the data structure of IWS. Some concepts, definitions and the algorithm for computing the intersection between two IWSs are also presented in this Section. The usage of IWS in the mining FWIs from a weighted transaction database is presented in Section 4. Section 5 shows the results of applying IWS to some experimental databases. Section 6 gives the conclusions and suggestions for future work.



II. BACKGROUND AND RELATED WORK A. Mining FWIs from WITDs  A weighted items transaction database (D) is composed of tuples ?T, I, W?, where T = {t1, t2, ?, tm} is a set of  Nguyen Duy Ham Department of Math & Informatics  University of People?s Security Hochiminh City, Vietnam  duyham@gmail.com  Bay Vo Faculty of Information Technology  Hochiminh City University of Technology Hochiminh City, Vietnam  bayvodinh@gmail.com  Nguyen Thi Hong Minh School of Graduate Studies Ha Noi National University  Hanoi, Vietnam minhnth@gmail.com  Tzung-Pei Hong Department of Computer Science  and Information Engineering National University of Kaohsiung,  Kaohsiung, Taiwan, ROC tphong@nuk.edu.tw   DOI 10.1109/SMC.2015.451    DOI 10.1109/SMC.2015.451       transactions,  I = {i1, i2, ?, in} is a set of items,  and W = {w1, w2, ?, wn} is a set of weights  that correspond to the items in set I.

Example 1: Tables 1 (A) show a WITD. The set of I = {A, B, C, D, E} and there are a total of six transactions. The set of weights W = {0.6, 0.1, 0.3, 0.9, 0.2}, as shown in Table 1 (B).

Ramkumar et al. [10] identified two useful weighted, which are transaction weighted (tw) and weighted support (ws), respectively shown as follows:   tw(tk) = ? ?| |  (1)  where: - tw(tk) is the transaction weighted of a transaction tk; - wj is the weight of item ij; - |tk| is the number of of items appearing in transaction tk.

The weighted support (ws) of an itemset X is defined as  follow: An itemset X is a frequent weighted itemset if and only if  ws(X)   minws where minws is the given threshold which identified by users. The problem of mining FWIs from a WITDs is to find the set of all itemsets X such that X ? I and ws(X)  minws. Note that the downward closure property, that means if X ? Y then ws(X)  ws(Y), is also used this problem.

Mining FWIs from WITDs is a problem interested by many rersearches because WITDs is a form of database which have many real-world applications.

Ramkumar et al. [10] proposed two measurements of transactions weighted (tw) and weight support (ws) used in mining FWIs.

Tao et al. [12] based on two measure tw and ws proposed a framework to solve this problem. However, this approach based on the Apriori algorithm, thus need scan database many times lead to consume more time.

Vo et al.[9] proposed a structure namely WIT-tree, this is an extension of IT-Tree[4]. The based on WIT-tree, the author proposed an algorithm for fast mining FWIs from WITDs with scan database one time.

Based on WIT-tree and strategy diffset [5], Le et al. [8] proposed a the formula for fast computing ws of child item from ws of parent item on WIT-tree. Beside this approach saved memory to store tidset more than previous methods.

B. Some improvement for storing the tidset of itemset Zaki & Gouda [11] proposed an efficient method to  replace tidsets by diffset which only keeps track of differences between two tids of a candidate pattern from its generating  frequent patterns. Diffsets drastically cut down (by orders of magnitude) the size of memory required storing intermediate results. However, diffset is only effective with dense databases, but not on sparse databases. The reason is that diffset takes the offset on tidset while the number of items on transactions (the number of elements on tidset) is less than the total number of transactions in the database.

Dong et al. [6] proposed the use of BitTable to store tidsets.

Tidset of items in Table 1 by BitTable as description in Table 2.

Item A appear in transactions ID {1,3,4,5}, thus BitTable of item A is 101110002 (27 + 25 + 24 + 23 = 182 in decimal system), in which the bit position {1,3,4,5} from left to right have value 1, else have value 0. Similarly with B, C, D, E.

The calculation of intersection between two tidsets is implemented by using AND bitwise operation with two corresponding bytes on two BitTable of tidsets. This is an advantage of BitTable because bitwise operator is fast.

BitTable with 16 bytes is presented in Table 3.

Table 3. An example of BitTable  Byte index 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Byte value 0 0 0 0 6 9 0 0 0 0 0 1 5 1 0 0  In this BitTable, there are 11 bytes which its value equal 0 (0-byte). Especially on sparse databases the number of 0-byte is very large. Therefore, its potentially waste large amount of memory and lead to consume more time. This is a disadvantage of BitTable.

Table 4. DBV from BitTable in Table 4 Byte index 5 6 7 8 9 10 11 12 13 14 Byte value 6 9 0 0 0 0 0 1 5 1  DBV 5(6, 9, 0, 0, 0, 0, 0, 1, 5, 1) Vo et al. [7] introduced the concept of DBV, which is a  solution improving significantly BitTable in performing time and memory. The authors proposed to remove all 0-bytes at the start and end of each bytelist (no transaction is recorded in 0-bytes).

Using DBV structure, the bytelist of items is compacted considerably. However, DBV just only cuts 0-bytes at both the start and end of each tidset, while 0-bytes at the ?middle? of each bytelist are not removed. For example DBV in Table 4, there are 5 0-bytes. The disadvantage is that it consumes more memory for storing 0-byte leading to spending more time for determining intersection among two tidsets.



III.  REPRESENTATION OF INTERVAL-WORD SEGMENT  A. Structure of IWS In this paper, each IWS describes several segment of  Words (2 bytes) different 0-word arranging continuously on one word vector. Each segment includes two components:  (i) Start: start index of the segment;  Table 1. An example of a WITD A  B  ID Item  Item Weight 1 A, B, D, E  A 0.6 2 B, C, E  B 0.1 3 A, B, D, E  C 0.3 4 A, B, C, E  D 0.9 5 A, B, C, D, E  E 0.2 6 B, C, D  ws(X) = ? ?? ?  (2)  Table 2. Tidset represented by BitTable Item Tidset by BitTable Value in Decimal system  A 101110002 27 + 25 + 24 + 23  = 182 B 111111002 27 + 26 + 25 + 24 + 23+ 22  = 252 C 010111002 26 + 24 + 23  + 22  = 92 D 101011002 27 + 25  + 23 + 22  = 172 E 111110002 27 + 26 + 25 + 24+ 23  = 248       (ii) Word Vector Segment: A range of Words different 0- word, from the start to the end in which the start is Start, the end is Start + |Word Vector Segment|.

Example 2: An IWS improved the BitTable (Table 3) and DBV (Table 4) as shown in Table 5.

In Table 5, IWS only needs 6 bytes for storing word vector, while BitTable needs 16 bytes (Table 3), DBV needs 10 bytes (Table 4). Thus, The IWS reduced the size of memory required to store tidset of itemset  Definition 1. IWS of one itemset X is a set of segments of continuous word different from 0 and described as follows:  IWS(X) = {?s1( , , ?, )?, ?s2( , , ?, ) ?, ?, ?sk( , , ?, )?}.

where: - ei  si, ?i?{1, .., k}; -  is the value of word in the segments.

Example 3: Data shown in Table 5 is an example of IWS  with 2 segments: {?3(1545)?, ?6(1, 1281)?} Consequence 1.  if k = 1, IWS(X) is DBV(X) Definition 2. Segment ? , ? , ? is called a subset of segment ? , ? ,  ? if it satisfies three conditions:  1.

2. k + t 3. =    ,   =     , ?, = Example 4: Segment Sx = ?7(1281)? is a subset of segment  Sy = ?6(1, 1281)? with t = 1. Because, index of 1281 in Sx is 7 in word vector, this is the same index of element 1281 in Sy.

Definition 3. ? , ? , ?   IWS(X) if and only if ? , ? , ? is a subset segment of a segment in IWS(X).

Example 5: Give IWS(X) = {?3(1545)?, ?6(1, 1281) ? }.

Subset segment ?7(1281)?  IWS(X), because ?7(1281)?  is a subset segment of segment ?6(1, 1281? of IWS(X) Definition 4. The intersection among two segment Sx = ? , ? , ? and Sy = ? , ? , ? is a segment S, denote by Sz = Sx  Sy where Sz = ? , ? , ? and  = AND ,  = AND ?  = AND , with AND is bitwise operator if: ? , ? , ?  and ? , ? , ? is subset of Sx and Sy.

Example 6: Given two segments Sx = ?6(1, 1281)? and Sy =  ?5(3220, 8726, 3104) ? . The intersection segment of these segments is ?6(1, 1024)?. Because, 6(1, 1281) and 6(8726, 3104) are subset of  Sx and Sy, 1 AND 8726 = 1, and 1281 AND 3104 = 1024, with AND is an operator on bitwise.

Definition 5. The intersection between IWS(X) and IWS(Y) is denoted by: IWS(XY) = IWS(X)  IWS(Y). In which IWS(XY) include of segments, each segment is a result of intersection segment on IWS(X) and IWS(Y) follow definition 4.

Theorem 1. Given a segment Si = si( , , ? ) ? IWS(X), l is indices 1-bits of (from left to right) with si  sj  ei. l is mapped to Bit Vector by formula: where k is index 1-bits in Bit Vector of l  Proof: sj is indices word of  on Word Vector, there are (sj - 1) Words following , it means [(sj - 1)  16] bits before on Bit Vector. Thus, l is mapped on Bit Vector by calculator (sj - 1)  16 + l.

Example 7: Given an IWS(X) = {?3(1545)?}, tidset(X) is calculated as follow  Because the start of segment in IWS(X) = 3, so adding 32 ((3-1)  16 = 32) into each bit index of segment. Beside, index 1-bit of 1545 is {6, 7, 13, 16}. Hence we have:  32 + 6 = 38; 32 +7 = 39; 32 + 13 = 45; 32 + 16 = 48.

Therefore, tidset(X) = {38, 39, 45, 48}.

B. Algorithm for determining intersection of two IWS?s The mining FWIs on the vertical data format requests the  calculation of the itemsets union to create new itemset. This is equal to determining the intersection two corresponding individual IWS?s based on definition 5.

The algorithm of calculating the intersection between two IWS?s is described in Fig. 1.

Algorithm 1. Intersection Algorithm Input:      - IWS(X) include n1 segment - IWS(Y) include m1 segment Output: Z = IWS(X) IWS(Y) Method name: INTERSECT_IWS(IWS(X), IWS(Y))         INTERSECT_IWS(IWS(X), IWS(Y)) i = 1; j = 1; Z = ?; While (i  n1 and j  m1) Segment in IWS(X) and  in IWS(Y) is the first intersection segment of IWS(X) and IWS(Y) from segment ith on IWS(X) and jth on IWS(Y) S = ?; Start= max(IWS(X)[i1].start, IWS(Y)[j1].start); End= min(IWS(X)[i1].end, IWS(Y)[j1].end); for all k  [Start, End] S = S IWS(X)[k]  IWS(Y)[k]}; Remove 0_Word from S and add S into Z if (End = IWS(X)[i1].end) i = i1  1;  j = j1; else j = j1 1; i = i1; Return Z  Fig. 1. INTERSECTION_IWS algorithm The input of intersection algorithm is IWS(X) and IWS(Y),  where IWS(X) has n1 segment and IWS(Y) has m1 segment.

The output is Z which is an IWS. Line 3 determines the intersection segment on IWS(X) and IWS(Y). The start and the end of result segment are calculated from lines 5 - 6. Lines 8 and 9 are used to calculate word of result segment (S). After that, the 0-words are removed from S (S may has many segments) and to add into Z on line 10. Lines 11 to 13 are used to recalculate the new value of i and j. The last line Z is to return the result.

Table 5. IWS from BitTable in Table 3 Word index 3 6 7 Byte value 6 9 0 1 5 1 Word value 1545 1 1281 IWS {?3(1545)?, ?6(1, 1281)?}  k = (sj - 1)  16 + l (3)       Algorithm INTERSECT_IWS in Fig. 1 is illustrated through the following example.

Example 8: Let IWS(X) = {?3(1545)?, ?5(1, 1281, 1030)?, ?12(1, 1284, 1536)?} and IWS(Y) = {?6(3220, 8726, 3104)?, ?10(1242, 8721, 6527, 6?}.

The second segment of IWS(X) ? 5(1, 1281, 1030) ? ) intersects IWS(Y) in first segment (?6(3220, 8726, 3104)?) by 6th and 7th in words vector index. Thus, the result segment is ?6(1024, 6)? (because, 1281 AND 3220 = 1024, 1030 AND 8726 = 6). Next, the segment third of IWS(X) is ?12(1, 1284, 1536) ?  intersection with the second segment of IWS(Y) (?10(1242, 8721, 6527,6?) in word vector index are 12 and 13.

Therefore, the result segment is ?12(1, 4)? (because, 1 AND 6527 = 1 and 1284 AND 6 = 4).

The final, we have IWS(XY) = IWS(X)  IWS(Y) = {?6(1024, 6 )?, ?12(1, 4)?}.



IV. FAST ALGORITHM FOR MINING FWIs  A. IWS-Tree data structure A data structure, called IWS-Tree, is used to mine FWIs  from WITDs. Each node on IWS-Tree includes three components, namely X, IWS(X), and ws(X), where: - X is an itemset, - IWS(X) is the IWS of X, - ws(X) is the ws of X.

To connect nodes X and Y to create a new node X  Y, X and Y must have the same length and the prefix, with length |X| 1 items, and ws(X  Y)  minws, where minws is set by users.

B. A method for fast computing the ws from IWS structure  A raising problem for mining FWIs on WITDs is the ws calculation of each itemset according to formula (2). Thus, it is necessary to determine the tidset of the itemset. This is equal to determining the index set of 1-bits in an IWS. In order to solve this problem, we define a MAP array including 65.535 elements (Table 6), in which each element is a connected list to record index of 1-bits from left to right (the first bit has index 1) in each Word of the set {1, ?, 65535}.

Based on the index of segments in IWS, we mapped to the index set of 1-bits in the segment in bitlist.

The algorithm of calculating ws is expressed as in Fig. 2.

Algorithm 2. Compute ws Algorithm Input: IWS(X) Output: ws of itemset X Method name: COMPUTE_WS()         COMPUTE_WS(IWS(X)) tidset(X) = ?; for all i ? IWS(X) for all j?MAP[i] tidset(X) ? (i-1)  16  j; y = 0;  for all i   tidset(X) y =y + tw[i];   ws =y/sum_tw; return ws  Fig. 2. COMPUTE_WS  algorithm The input of algorithm for computing ws is the IWS(X),  the output is ws(X). To calculate ws(X), we need to build tidlist(X) from IWS(X) based on formula 3 in Theorem 1.

From tidlist(X) in previous step, we calculate ws(X) using formula 2. The first, tidlist(X) is empty (line 2). Lines 3 to 5 are to build tidlist(X) from IWS(X). Lines 6 to 8 are to calculate ws(X). The last line is to return ws(X).

C. Mining FWIs from a WITDs using IWS-Tree Based on WIT-FWI algorithm [8], we replace the method  of storing tidset by IWS and using IWS-Tree data structure.

We propose an algorithm for mining FWIs from WITDs namely IWS-FWI. The itemsets on an IWS-Tree are FWIs  which satisfy the minws threshold.

The IWS_Tree have many layers. The first layer includes  an equivalence class with 1-itemset which satisfies the minws (denoted by P). The k layer includes many equivalence classes which are k-itemsets. Each equivalence class is formed by combination of the parent node ((k-1) layer) with each other node in the same layer which is after the parent node. The itemsets in an IWS-Tree are thus FWIs that satisfy the minws.

The recursively algorithm for creating a IWS-Tree is expressed in Fig. 3.

Example 9: Consider the database D represented in Tables 1 with minws = 0.4. Table 7 shows the IWS of the 1-itemsets belonging to the database D in example 1.

Table 7. IWS of items from D in example 1 Item Tidset BitTable IWS ws  A 1, 3, 4, 5 101110002 {?1(184)?} 0.72 B 1, 2, 3, 4, 5, 6 111111002 {?1(252)?} 1.00 C 2, 4, 5, 6 010111002 {?1(92)?} 0.60 D 1, 3, 5, 6 101011002 {?1(172)?} 0.78 E 1, 2, 3, 4, 5 111110002 {?1(248?} 0.81  In this example, each IWS of items has only 1 segment.

We compute ws of all 1-itemsets. The result is in column 4  in Table 7. All items satisfy the minws threshold. Thus, [?] = {A, B, C, D, E}, add all the items ?A, B, C, D, E? into the IWS- Tree. To called recursive function IWS-Tree as fowllos:  ? With node A:  Algorithm 3. IWS-FWIs Algorithm Input: A Weighted database D and minws; Output:A IWS-Tree containing all FWIs which satisfy minws Method name: IWS_FWIs();  IWS_FWIs() [?] = ?j?I| ws(j) minws?; P = [?]; IWS_Tree (P); for all li [P] do [Pi]= ?; for all lj  [P], with j  i do X = li  lj; Y = IWS_INTERSECT(IWS(li), IWS(lj)); ws_X = COMPUTE_WS(Y);  if (ws_X  minws) [Pi] = [Pi]  {( X, Y, ws(X)}; IWS_Tree ([Pi]).

Fig. 3. IWS-FWIs algorithm  Table 6. MAP constant array Word index 1 2 ? 65535 Binary value 0000000000000001 0000000000000010 ? 1111111111111111 Map array 16 15 ? 1, 2, ?, 15, 16       Consider the two items A and B. IWS(A)   IWS(B) = {?1(184)?}  {?1(252)?}, it implies that IWS(AB) = {?1(184)?}.

Based on COMPUTE_WS algorithm, we have ws(AB) = ws(A) = 0.72  minws. Thus, AB is added into the IWS-Tree.

Next, consider items A and C. IWS(A)   IWS(C)= {?1(184)?}  {?1(92)?}, and it implies IWS(AC) = {?1(24)?}.

Because ws(AC) = 0.32  minws, AC is not added into the IWS-Tree.

Similar to node A, itemsets {AB, AD, AE, ABC, ABD, ABE, ABDE} are added to IWS-Tree as shown in Fig. 4.

Fig. 4. IWS-Tree with node A  Similar to node B, C, D and E, the final IWS-Tree is shown in Fig. 5. The results with minws = 0.4 include {A, B, C, D,E, AB, AD, AE, BC, BD, BE, DE, ABD, ABE, ADE, BDE, ABDE}.

Fig. 5. IWS-Tree included all FWIs

V. EXPERIMENTAL RESULTS In this section, we compare IWS with DBV and Diffset  based methods in terms of mining time and memory usage for five sparse databases, namely Retail, Mbs-Pos, Sales_Fact_97, Sales_Fact_97_98, and Salas_Fact_Sync. Two databases RETAIL, MBS-POS are obtained from http://fimi.cs.helsinki.fi/data/, and three databases Sales_Fact_1997, Sales_Fact_1997_1998 and Salas_Fact_Sync are from Foodmart2000 in SQL2000. Table 8 shows the characteristics of the experimental databases. All the experiments were performed on a personal computer with an Intel Haswell Core i5 1.4-Ghz CPU and 4 GB of RAM running Microsoft Windows 8.1. The algorithms were coded by C#.

Table 8.  Characteristics of databases used in experiments Database Number of items Number of transactions Remark  RETAIL 16.470 88.162 Modified BMS-POS 1.657 515.597 Modified SALES_FACT_97 1.753 20.522  SALE_FACT_97_98 1.753 54.537 SALES_FACT_SYNC 1.753 58.308    We modified the databases by adding a random value in the range of 1 to 10 for each item corresponding to its quantity in each transaction, and created one more table to store the weight values of items (in the range of 1 to 10).

A. Mining time comparison  Fig. 6. Comparison of runtime for RETAIL database  Fig. 7. Comparison of runtime for MBS-POS database  Fig. 8. Comparison of runtime for SALE_FACT_97 database  Fig. 9. Comparison of runtime for SALE_FACT_97_98 database  Fig. 10. Comparison of runtime for SALE_FACT_SYNC database             C. Mining memory usage comparison    Fig. 11. Comparison of memory usage for RETAIL database.

Fig. 12. Comparison of memory usage for MBS-POS database   Fig. 13. Comparison of memory usage for SALE_FACT_97 database.

Fig. 14. Comparison of memory usage for SALE_FACT_97_98 database.

Fig. 15. Comparison of memory usage for SALE_FACT_SYNC database.

Fig. 6-15 show that IWS is very effective on sparse databases. The performances of three approaches on these databases are quite different. For example, consider RETAIL database (Fig. 6 & 11) with minws = 0.1%, the mining time of IWS is 27.69 (s), while that of Diffset is 64.141 (s) and DBV is 135.576 (s). Besides, the memory usage of IWS is 24.002 (MB), while that of Diffset 48.479 (MB) and DBV is 160.687 (MB).

Sparse databases have a small number of items on transactions. Many 0_Word on a tidset are removed by IWS.

Thus, n1 and m1 in INTERSEC() function and k in WS()  function are small; the total runtime is thus small. IWS is effective in both memory and processing time.



VI. CONCLUSIONS AND FUTURE WORK This paper has proposed an effective approach in order to  reduce the memory usage for storing tidset information while mining FWIs from SWITDs. Firstly, we use the IWS concept to remove all ?0? Words. The IWS concept reduces the runtime and the memory usage. The experiments conducted on five databases show that our approach outperforms existing approaches in terms of memory usage and runtime. Especially, when the minws threshold is small, IWS is very effective.

In the future, we will further study on the problem of mining FWIs on WITDs with large databases and hierarchical databases. Besides, we will apply this method to mine frequent weighted closed itemsets and maximal frequent weighted itemsets.

