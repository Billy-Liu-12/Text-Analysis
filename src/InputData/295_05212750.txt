

Abstract: Recent researches pay more attention to stock tendency  prediction, which various machine learning approaches have been proposed. In this paper, we propose an algorithm to discover self-correlation of stock price in virtue of the notion of time series motifs, by viewing stock price sequences as time series. Generally, time series motif is a pattern appearing frequently in a time sequence, useful to forecast the stock temporal tendencies and prices as a reliable part in time series.

In the proposed approach, we firstly search for one part of time series motifs using ordinal comparison and k-NN clustering algorithm, and then attempt to discover the correlation between motifs and subsequences connected behind them. Experimental results demonstrate the positive contribution of time series motifs, the acceptable prediction accuracy, and priority of our algorithm.

Keywords: Stock prediction; time series motifs; self-correlation  1. Introduction  Stock prediction is a big charming trouble. More and more people devote their interest and efforts into the field.

Many trials, using neural networks [1, 2, 3], fuzzy sets [4], decision trees [5], and association rules [6], etc. are successful in each given experimental circumstance.

Because of the nonlinearity, complexity and uncertainty of stock data, no one can forecast stock price with high accuracy all the time. If we can decide when (during several days) and which (in large stock ticker set) to predict with a highly probabilistic accuracy, we get the target. In this work we focus on the short-term stock price trends prediction by considering the self-correlation among subsequences in a stock closing price sequence. Stock price has four types of data: opening, closing, highest, and lowest. We consider of only one-day return of a closing price, the same in [7], and highest and lowest prices were also used in [8].

Time series becomes an emerging role in the field of data mining, and also in stock tendency prediction. A daily stock closing price is just a data point on a time mark. We  can treat a stock price data as a set of time series. Each time series is combined of two parts. One is deputy of the inevitability and stability, and the other is deputy of the fortuity and uncertainty. The former contains characteristic information, and the latter is noises in substance. Stock price data has more noises than data in other fields such as medicine and network, etc. It is also more difficult to get reliable result in stock price analysis and mining. Finding reliable part of time series is an important task, especially in stock prediction. Time series motifs which is first mentioned in [9] is one of the formats of the reliable part.

Motif can be seen as a deputation of subsequences which appear frequently in a time series. Instances of one motif are made up of sets of similar subsequences without any overlapping each other. Time series motifs have revealed strong self-correlations between two time periods in a sequence of time series. Knowing from other- correlation, self-correlation is based on differences of time period, and the former is based on inter-action items.

In this paper, we introduce a novel method which is based on finding time series motifs to forecast some short- term stock price tendencies and values. Though there are several algorithms proposed for mining time series motifs [9, 11, 12, 16], it is still a hard work for a real application. Here we search for time series motifs by the way of building subsequences set, in which each subsequence is closed to others, and furthermore detect self-correlations by an improved k-NN clustering algorithm. We adopt the notions of tendency average accuracy and prediction efficiency to evaluate our method, and also compare it with those based on BP neural networks and association rules. The rest of this paper is organized as follows. In Section 2 we briefly discuss related work in stock temporal prediction and motifs mining. Section 3 describes our proposed method. We show how to find time series motifs, and then do some prediction.

Section 4 reviews the results of the experiments, and how to set parameters to achieve the best results. Finally, we have the concluding remark in Section5.

2. Related work  2.1 Definitions  We would like to begin with some definitions for concreteness. Concepts related time series have been defined for a long years. Many of them can be understood easily, and the notion of time series motifs is generalized in [9], so we just make some simple reviews and give mathematical expressions.

Definition1. Time Series: A time series is a set of real- valued variables ordered by time marks 1,... mt t .

1,... mp p are discrete values or vectors.

1 1{( , ),..., ( , )}m mT p t p t=                 (1)  Definition2. Subsequence: Subsequence ( , )iC t n with length n is a part of time series T with length m , m n? , and appears at it .

1 1{( , ),...( , )}i i i n i nC p t p t+ ? + ?= , 1 1i n m? + ? ? (2)  Definition3. Sliding Window: Sliding window of size l is a frame to build an orderly set of size 1m l? + , in which are isometric subsequences. It delays a time mark from one to the next.

Definition4. Similarity distance: 1 2( , )D i i , a positive value used to measure differences between objects, relies on measure methods. If D ?< , item 1i is similar with 2i . ? is the threshold of similarity measure.

Definition5. Time series motifs: Time series motifs are frequent patterns in time series T , all instances of motif M are subsequences. Each subsequence in  1~ ft C is similar  with the others, and 1~ ft  C are instances of M . M with frequency f can be revealed as:    { ,..., } ft t  M C C? , 1 ... ft t< <               (3)  Definition6. Subsequences confidence: There are x subsequences,  ~  xi i C C , with the same motif M . For each  subsequence, the following subsequence is denoted to  ~  xj j C C . If there are y subsequences being alike each  other in  ~ xj j  C C , the subsequences confidence is:  _ ys cof x  =             (4)  Definition7. Tendency average accuracy: Predicting k stock time series during time marks 1t to ft , there are k f? cases for all. If y of their tendencies can be predicted correctly, the tendency average accuracy is:  yTaa k f  = ?  (5)  Definition8. Prediction efficiency: For all cases of k stock time series during time marks 1t  to ft , predict n of them selectively, so prediction efficiency is:  npe k f  = ?  (5)  2.2 Stock prediction and time series motifs mining  In the last decade, there existed a great deal of efforts and productions using neural networks-based model [3, 17, 18, 19, 20, 21, 22] in financial mining works. Neural networks techniques have superior capability in dealing with complex data. Association rules-based [6, 10, 23] methods are recently attract people?s attention. Our method in this work is similar to those of association rules-based, but instead of finding frequent patterns, we search for time series motifs. The difference is the point of the view. We deal with the stock time series in the manner of self- correlation instead of other-correlation. Most other- correlation methods use the notion of association rules and the Apriori algorithm. The Apriori algorithm is used to mine patterns and rules in order to propose stock category association, and then the cluster algorithm (K-NN) is used to explore the stock clustering in order to mine stock category clusters. The goal is to present possible stock portfolios.

The notion of time series motifs was generalized in [9], and other methods had been proposed for finding time series motifs in [11,12]. Time series motifs are mainly used in regular-data field like cardiogram analysis and in bioinformatics, but seldom in financial works.

3. Proposed method  We propose a method to detect self-correlation between periods and algorithm to find time series motifs. Based on these techniques, we bring forward a management for exact prediction within specific time marks and tickers. We also      compare our prediction model to BP neural networks and association rules models.

3.1 Detecting self-correlation based on time series motifs  Our works begin at looking for motifs in a time series.

Here we would like to introduce the way on detecting self- correlation firstly.

Detecting self-correlation between different periods of time, the simplest behavior is using moving average which has been adopted by many stockjobbers in recent days. A smarter behavior uses the method which focuses on some familiar and notable stock price waves, and it forecasts the next shakeout according as these familiar and notable stock changes which have emerged many times before. For an instance, there often follows a sharp slump after a slow strengthening in stock price, so many veterans make short immediately when they meet a slow strengthening. The idea which we use time series motifs for detecting self- correlation of stock price is similar with that of the second behavior.

Our intention by dint of time series motifs is that we expect to get more reliable self-correlation. The cornerstone of our idea is trustiness of time series motifs. We think that time series motifs appear once and again because we believe histories are always the same.  Current subsequence must be similar with many historical subsequences.

Since time series motifs come up frequently, we can forecast the tendency by realizing them before. Can that be true? In fact, what we are demanding is not a whole time series motif, but a part or a prefix of a whole time series motif. We assume that a whole time series motif is made up of two parts: prefix and suffix. Here, prefix is the foreside of a motif, and suffix is a subsequence behind prefix. Therefore, there connects firmly a suffix after the prefix in a whole time series motif. Because one time series motif appears steadily, the appearances of suffix must conformably be steady. The stability of suffix is restricted in the scope of whole time series motifs. If the following subsequence of prefix is beyond the scope of suffix, the stability that we mentioned above is not reliable like that. Generally, we search for suffix with lesser size, so that it can make sure that the suffix will be under the control of the whole time series motif. That is the reason why we predict stock price for a temporal period.

Thinking that instances of a motif have presented themselves many times at past, and current subsequence approximates to foreside of the motif, we believe that the current subsequence goes like other instances in most probability. We must not try to search for motif as long as it can be, but we are expecting for the foreside of a motif.

Mostly, subsequence after what we expect to find is a part of the same motif. We see current subsequence as prefix of a motif and try to get all the instances of the motif, then most of similar suffixes may help us to predict.

Instances of different time series motifs might start with a same prefix, so it is possible that the subsequences of the same prefix go along with different tendency. Therefore, the size of the prefix is a critical factor. Large size of the prefix leads to un-frequent appearance and unreliable sequence. On the contrary, short prefix may be fit for a number of motifs, and it makes many possibilities of the suffix. We select the size of the prefix in the way as follows.

The size l  of a prefix is also the size of sliding window, and the fitting size of prefix is named fl . The size sl of a suffix is user-defined. If we want to predict the next 3 days? stock price, we set sl =3. First, we give ol  to experimental implementation. There usually occur a set of motifs in a time series. If the results are just like what are described in Table 1, it means a ol n+ size motif is divided into n motifs with size of ol  by sliding window. fl is now fixed on:  f o sl l n l= + ?       (5)   Table 1. A motifs list in a time series with sliding window  size ol . t  is beginning time marks of instances.

Motifs Expressions by beginning time marks  1M  { 1 ,..., it tC C }  2M  { 1 1 1,..., it tC C+ + } ? ?  nM  { 1 ,..., it n t nC C+ + }   In training set, we adopt the k-NN algorithm to deal with all suffixes, and initialize the two centroids of cluster algorithm with the most dissimilar two suffixes. After completing clustering processes, the values of centroid in greater cluster are values for prediction. A nature number cd derived from the margin between members in the two clusters is vital for our prediction strategy. Our strategy is to only care about those time series with biggish cd , it makes the prediction to be surprisingly precise though in a very narrow scope.

For practical significance, we use moving average method to deal with all the suffixes in order to do some compare.

3.2 SSR(sliding, sorting and removing) algorithm: finding time series motifs  We propose a simple but effective algorithm to find prefixes of motifs by moving sliding window step by step.

The subsequences built by moving the sliding window are potential prefix of motifs for which we are expected. There are two parts in our algorithm: searching and clustering. Our algorithm is described as follows.

In searching part, firstly, slide step by step. Secondly, sort ascending by ED. At last, remove subsequence which is not far from previous neighbor enough. In training set, we compare subsequence to the next one, and the intervals between the former and the latter must be greater than the value of ( )f sl l+ . The benchmark of the comparison is Euclidean distance(ED) measurement. A positive real- valued parameter? is given as threshold for ED similarity measure between two subsequences. We iterate the comparing processes with different beginning points respectively, and sort subsequences by ascending ED values.

If the distance between two proximate subsequences less than ( )f sl l+ , delete the latter.

In clustering part, in each set of subsequences, we initialize the two centroids of clustering with the most dissimilar two subsequences, and cluster these subsequences by k-NN algorithm. We thus obtain the majority clustering result as a motif.

At last, in testing set, if a subsequence which is similar with every instance in a motif, it belongs to the motif. We pay attention to subsequences which can be viewed as members of known motif, and then do some prediction during the next several days after the ending of those subsequences. Here, subsequences confidence is a key factor for further prediction. Larger subsequences confidence value means more suffixes are similar. We cluster all suffixes into two clusters to improve the value of subsequences confidence by reserve the cluster which contains most suffixes.

3.3 Similarity measure  Many similarity measure methods had been summarized and evaluated in [13]. Euclidean distance(ED) measure method had been proven to be effective in time series motifs detection [14, 15]. We select ED as similarity measure method in our work. Another reason for using ED is that we can adjust threshold of ED easily when we change the size of sliding window by using underside formula(6).

'  ' f f  l l  ? ?=     (6)  5% fp l? =                    (7) According with formula(6), if we want to change the  size of sliding window fl  to 'fl  , the threshold of similarity measure should be '? . ? can be calculated easily by using formula(7), in which p is average value of a time series.

4. Experimental evalution  The utilities of the proposed method lie on the performance of experiment.

4.1 The dataset  We tested our approaches with S&P500, 500 stock tickers daily closing price during a year (from 2008-03-07 to 2009-03-06). 80% of the dataset (from 2008-03-07 to 2008- 12-18) is training set, and the other (from 2008-12-19 to 2009-03-06) is testing set.

The values of the tickers being selected values must be without any absence during a span of a year. Most pre- process work is dealing with missing values in dataset.

Usually, we set the average of proximate values to missing value. If there are too much blank in a period, we set ?null? to each blank. Sliding window will skip over when meeting null.

4.2 Experimental results  Our experimental results are precious in forecasting some certain stock tickers and certain time mark. Our prediction is completely based on whether there exists an instance which belongs to one of these known motifs. We do not care about these stock tickers since we have not found any instances of any known motifs.

Greater value of subsequence confidence means more motifs are filtrated in clustering process. There are 232 motifs of 14 tickers under the conditions: fl =8, sl =4 and f =6. First, let us see if the future practical subsequence is similar with what we have predicted. There are 209 positive results in 232 motifs. It means 90% probability that our prediction target may be another member of a certain motif. If we are strict with the values of subsequence confidence (most suffixes must be very approximate), our prediction is nearly the same to the      truth(within 2% difference). , ,r p v  We calculate prediction error r with prediction value p and practical value v in the formula:   100 p v  r v  ? ? =      (8)  Table2. Average prediction errors running from variant subsequences confidence.

prediction errors with fl =8, sl =4, f =6 _s cof   Sum of  motifs Day1 Day2 Day3 Day4  50% 232 5.9% 6.3% 7.0% 7.2% 60% 100 5.2% 5.5% 5.8% 6.2%  70% 34 6.1% 6.0% 6.0% 6.0%  80% 12 2.1% 2.0% 2.8% 4.1%  90% 4 1.9% 1.6% 2.9% 3.4%   Data in Table2 tell us that average prediction error goes down with subsequences confidence going up.

Depressing the value of subsequences confidence, we can do more forecasting, and there also occurs perfect results.

Parts of them are listed in Table3. But at the last three of Table3, terrible results are given. It is difficult to predict without considering of the value of subsequences confidence. Our strategy performs by filtrating instances rigorously in order to insuring perfect accuracy. It is obviously that greater subsequences confidence values could make suffixes to be more reliable and the prediction is more precious, but instances and motifs are more exiguous.

Table3. First 3 rows are several of perfect results, and last 3 rows show some of worst results, in condition of _s cof <3.

prediction errors with fl =8,  sl =4, f =6 Ticker(Beginning time mark)  _s cof Time mark of first prediction day  First prediction  day r  ED(2009-01-22) 50%  2009-02-03 0.6% SO(2009-01-06) 50%  2009-01-16 0.4%  WPI(2009-02-05) 60%  2009-02-18 0.7%  FHN(2009-12-29) 50%  2009-01-09 12.8%  Q(2009-01-23) 60%  2009-02-04 13.5%  KG(2009-01-08) 50%  2009-01-21 14.6%   We would like to compare our prediction model with BP neural networks and association rules models. We frame BP neural networks architecture with 8 units in input layer, 8 in hidden layer and 1 in output layer. Closing price data of day1 to day8 is used as input for day9 prediction. We represent stock tendencies with symbols and then used Apriori algorithm to find frequent itemsets in association rules model.

Table4 shows comparisons among the three models.

Our model can do precise but inefficient prediction.

Association rules model is also an inefficient model. BP neural networks can predict every stock ticker at every time with acceptable prediction accuracy. BP neural networks model and our model can predict not only stock tendency but also price value. The priority of our model is the larger tendency prediction accuracy.

Table4. Compare prediction results with BP neural networks and association rules models.

prediction result comparisons Prediction model Tendency prediction  accuracy Prediction efficiency  Time series motif 83% 0.05%(selectively)  BP neural networks 72% 100%(unselectively)  association rules 75% 2%(selectively)  5. Conclusion  In this paper, we propose a method to search for time series motifs and design a reliable scheme to predict stock price. Precious experimental results inspire us greatly and      confirm the utilities of our prediction approach. The characteristic of our approach is making prediction depends on occurrence of motif. Actually, profiting from the vast body of stock data set, our method is rather superior.   With experiments it is proven that our approach is effective but sometimes the prediction efficiency is one problem, which should be improved with more sophisticated means in the future work.

