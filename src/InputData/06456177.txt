Multi-level association rule mining based on clustering partition

Abstract?According to the question of the traditional multi- level association rules mining in large data mining in low efficiency and accuracy ,based on clustering classification multi-level association rule mining is proposed. The method is combined with the concept of hierarchical concept, the data of the generalization sets processing, and uses SOFM neural network generalization into the database after the transaction, by way of introducing an internal threshold so no need to set the minimum support threshold, to generate the local frequent itemsets as global candidates itemsets to generate global frequent itemsets, thereby enhancing the efficiency of multi- level association rules and accuracy. And by simulating the case shows that the method can not only efficient mining single-layer and cross-layer association rules, but also the association rules is new ,easy to understand and meaningful.

Keywords-Self-organizing feature map network (SOFM);Clustering divided; Multi-level association rules

I. INTRODUCTION At present the multi-level association rules mining  algorithm has many researches. It includes association rules mining  of the same level and across levels. The complexity of its problem is higher than single association rules mining.

The commonly used algorithms of multi-level association rules mining are ML_T2L1 algorithm and the Cumulate algorithm Srikant R, Agrawal R and others proposed.

ML_T2L1 algorithm is suitable for mining association rules with level and Cumulate algorithm level can mining at the same level and across levels of association rules. Because of these two kinds of algorithms are made by Apriori algorithm extended, has the Apriori algorithm two big shortages: 1, producing a large number of candidate itemsets; 2, multiple scanning database. Therefore people is put forward based on the FP - tree multi-level association rules algorithm, the use of FP - tree don't generate candidate itemsets advantages, to enhance multi-level association rules mining efficiency. But this method has the limitations of FP - tree: storage FP - tree need to take up a lot of memory. It is not suitable for a large database of mining, and for each abstraction layer mining use the same minimum support degree threshold value and the value set depend on the user or the expert's subjective Settings. Generally it uses a single the value of minimum support threshold; Set too high, may miss low abstraction layer meaningful association rules; Set too low, may produce meaningless association rules appearing in the higher concept levels.

In view of the above mentioned insufficiency, this article puts forward the clustering based on the division of multi- level association rules mining, through clustering division affairs database,to make frequent items relatively concentrated data in each subset, and introducing the internal threshold way to need not set the minimum support threshold.It not only greatly reduces candidate itemsets, but also reduces the database scan times, to make the mining association rules  more value and  practical significance.



II. RELATED CONCEPT AND DEFINITION  A. Multi-level association rules  Set ? ?kiiiiI ,.....,,, 321?  to all items ki  (k = 1, 2, 3...) Set. D is a transaction database.  Each affairs composed of itemsets all have a unique identification number TID.Concept layer defines the concept of the bottom to top the more general concept mapping sequence, concept hierarchy in concept tree to represent.

Definition 1: If a item ki  (k = 1, 2, 3...)  belong to transaction T, the transaction T support item ki .The itemsets A support is:support (A) = (number of transaction in D supporingt the set) /( number of transaction D)x 100%.

Definition 2: Minimum support degree defines the lower limit of the rules in the statistical sense covering sample size of events.Minimum support degree is give. If the itemsets A support degree is greater than or equal to the minimum support degree, the itemsets is frequent itemsets.

Definition 3: Multi-level association rules is the form of  the containing type: BA? . Among them, ???? BAIBIA ? , and arbitrary items of  B  are not items? ancestors in A. The support degree is: ? 	 ? 	;ss BAupportBAupport ??? The confidence  is: ? 	 ? 	 ? 	 %100s/s ?? AupportBAupportBAconfidenct ?  Definition 4: Classification in the block affairs centralized mining frequent item sets are local frequent itemsets. The set of frequent itemsets of all the division form D global candidate itemsets.The global candidate itemsets in all affairs set D set in frequent itemsets called global frequent itemsets.

DOI 10.1109/ISDEA.2012.234     Definition 5[1-2] :In the target data centralized data item according to the support degree count in descending order, make the first k itemsets support count for support (x), then say support (x) for internal threshold value ?border_sup? namely boundary support degrees, define support degree count more than support (x) to collect a Top - k frequent itemsets.

B. SOFM Neural network SOFM neural network is Kohonen self-organizing  Feature Map (Self - Organizing Feature Map) neural network for short, proposed by the Finnish scientists Kohonen T. SOFM has the parallel processing ability, distributed storage and strong tolerance of fault, information processing and storage and self organized; Through the competitive learning and training weight coefficient, the clustering center automatically is obtained, to avoid  the subjective factors. It support the large number of clustering results, and effectively find abnormal data classes. Network training convergence speed is faster .

It is a two layer feed forward neural network [3-5], namely input layer and output layer. Each of the input layer and output layer neuron through the right of each neuron connected. Input layer neurons in the form of one dimensional array. The number of the input layer neurons is by the input sample object n characteristic value to said.The output layer neurons are general with one-dimensional or two-dimensional form arrangement,.the output layer is also called competitive layer, is responsible for the input mode "comparison", looking for law, and classification. The structure is as shown in figure 1. N is for input layer neurons quantity. M is for the number of output layer neurons, namely clustering partition number.

N  dimensions  feature  vector  Input layer neurons  Output layer neurons  Figure 1. SOFM structure model diagram

III. BASED ON THE DIVISION OF CLUSTERING MULTI- LEVEL ASSOCIATION RULES MINING DESIGN THOUGHT  Any level division for association rules mining in large database, generates more local frequent itemsets, and is easy to generate a mass of invalid of maximum frequent  candidate item sets. This kind of method is consuming much time, increasing the search space.Because of topological keep ability and self organization probability distribution characteristics of SOFM, we can use SOFM neural network algorithm for fast clustering division to make the local mining frequent itemsets efficiency increased because of the basic similar data concentration in each block. The mining frequent pattern is more close; the local frequent itemsets number called global candidate itemsets is greatly reduced,; the mining global frequent itemsets efficiency is improved.

This algorithm inputs for quantitative transaction data set D, concept hierarchy tree T, threshold K, field radius and learning rate, the network training iterative maximum frequency CMax, output frequent itemsets, specific steps shown below:  Step 1:The original affairsset is processed by data generalization[6].Structure the characteristic vector of sample data.If the original project number is n, the input layer neuron number is n. Use of SOFM network clustering algorithm divided data.It?s input is processed numerical sample data set, the output for the division of the affairs set partitioned.Steps are as follows:  (1) Network initialization:t=0;Input neurons and the  output neuron weights ? 	? ?0ijw are random  initialization.Gift random value in [0, 1] interval, and choose field radius and learning rate, define network training iterative maximum frequency T.

(2) Provide a new input vector model. The i-th numerical sample can be expressed n-dimensional feature vectorrrr .

(3) Look for winning neuron. Calculate sample data with all output layer neuron Euclidean  distance .If winning  neuron is a, thenn .

(4) Modify the adjoining weights between nerve cells  and selected neuron.

(i=1,2,??.,j=1,2,?m) (5) Provide new input vector and repeat steps 3 ~ 5  learning process; Until the end of algorithm convergence, divide each transaction set .

Assume that SOFM clustering division produce category to c, then each block affairs sets  (I = 1, 2,... c).

Step 2:Scan each items of each block affairs set jD  (j = 1, 2, 3... c). Combined with concept hierarchy tree, and by setting the threshold k and introducing internal threshold way [4],  make no set minimum support degree, digging out the number of frequent itemsets for k, which based on Top - k of frequent itemsets, and then use the minimum confidence,to  dig out the practical significance of multi- level association rules. The detailed process is as follows:     1 for(j=1;j<c;j++){  2 // Scan each items of each block affairs set j D  (j = 1, 2, 3... c).

3 Prior compute ancestors set of each of itemsets; Delete ancestors item which don't appear in the candidate 1 ? item.

4 Statistics all the candidate 1-itemsets (j=1,2,?..c) and their support.

5 Sort( ) is that the candidate 1-itemsets is in descending order and the first k itemsets support count selected is as border_sup. Delete the itemsets of which the support is less than ?border_sup? in  ,and the itemsets? support is degree is equal to or greater than the ?border_sup? are  (j=1,2,?..c) .

7 // Generate new candidate  itemsets[8] 8 Delete the itemsets containing items and its  ancestors items in candidate set .

9 Delete the ancestors item  not appearing in the  candidate itemset from the concept tree T.

10 //Scan  to count 11 Add ancestors item in primitive transaction.

12 //Get the subset of t 14 c.count++;} 16 Sort( );}//After the descending sort,  select the first k itemsets //support to update ?border_sup?  17 ;} 18 Itemsets_array= ;// Itemsets_array is global  candidate itemsets and //the collection of local frequent itemsets  19 Scan all transaction in the database D,and evaluate the actual support degree of each candidate itemsets in Itemsets_array  20 Sort(itemsets_array)//Global candidate itemsets are unified descending //sort by their actual support  21 Select the ck-th (c for block number) itemsets support count as border_sup.Delete the itemsets in itemsets_array of which support degree less than border_sup, and itemsets of which the support degree  equal to or greater than the border_sup is the global frequent itemsets in stored in frequent_itemsets  22 return  frequent_itemsets

IV. EXPERIMENTAL RESULTS AND ANALYSIS  A. Algorithm  analysis Based on clustering partition multi-level association rules  mining algorithm is  improved which bases on the Cumulate algorithm. Because Cumulate algorithm is extended from the Apriori algorithm. In the mining large database, it has two big main disadvantages: need multiple scanning database and producing a large number of candidate itemsets. In this paper the algorithm by using the SOFM network which has topological keep ability and self organization probability distribution characteristics, and advantage of input data clustering and feature extraction, suitable for the uncategorized multidimensional data set for coarse classification [3]. By coarse divided  large data sets, it makes similar data set in each block, and make local mining frequent itemsets efficiency improved, frequent pattern close, greatly reduce the number of local frequent itemsets. And in view of rare item problem of general multi-level association rules, by the introduction of internal threshold set mode ,to reduce the search space, it can mining Top - k frequent items in a relatively short time. Using the above way on the basis of data with similar advantage in each block, run improvement the Cumulate algorithm  without support setting. For the relatively large database, the method based on the Apriori multi-level association rules mining still has high efficiency, so the divide and rule strategy method  for large data set is feasible. Based on clustering partition multi- level association rules mining algorithm is one of these categories. Compared with other multi-level association rules algorithm, this algorithm can effectively reduce the number of scanning database, greatly reduce the candidate itemsets generation and improve  the efficiency of association rules mining  of the same level and across levels.

B. The analysis of experimental results Experimental hardware platform is: Intel core i3 dual-  core processor, memory: 2 GB, hard disk: 320 GB of computer, Software operating system Window XP, using Matlab 7.1of  simulation test.

Use the case sample data in the similar literature [7]  to test.The processed original affairs data contains items: kill, theft, robbery, rape, 1 month, 2 months, 3 months, August, September, October, November, December, junior high school, high school, elementary school. So the input layer neuron number is 19,and structure 19 dimensions feature vector.If  existing in the transaction, its value  is 1, otherwise 0. A time concept hierarchy number T as shown in figure 2 shows.

The second quarter  The third quarter  The first quarter  The fourth quarter  YEAR  January February March October November December  Figure 2. Time concept  tree     For different test of different size of data sets, between the algorithm and the Apriori based on the multi-level association rules mining algorithm the relationship of execution time and data set the size  as shown in figure 1 shows. The figure 3 knows the algorithm?s in this paper execution time than the multi-level association rules mining algorithm based on Apriori is less.While the data size increases from 10000 to 15000 or 20000 , the time gap is widening tendency, mainly because of clustering segments similar data, and the frequent patterns similar, which greatly reduce the candidate item sets.

Figure 3. The relationship of execution time and the data set size

V. CONCLUSIONS Based on theoretical research of the multi-level  association rules mining, Top - k frequent pattern mining and SOFM clustering and classification methods, according to the problem of that the traditional multi-level association rules mining based on the Apriori algorithm is not adaptable to a large database, this paper proposes a multi-level association rules mining method based on the clustering division. The experiment shows that the algorithm is feasible and effective. This paper is aim to introduce the method, and the more SOFM parameters and internal threshold Settings and performance relationships  of the algorithm will be also further discussed. In the future, we will also focus on parameter Settings difficult problems in the method and this method?s application in Web mining fields for further study.

