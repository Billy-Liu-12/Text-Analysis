ACCF: Associative Classification Based on Closed Frequent Itemsets1

Abstract   Recent studies in data mining have proposed a new  classification approach, called associative classific- ation, which, according to several reports, such as [6, 7, 8, 9], achieves higher classification accuracy than traditional classification approaches such as C4.5[11].

However, the approach also suffers from one major deficiency: a training data set often generates a huge set of rules. It is challenging to store, retrieve, prune and sort a large number of rules efficiently for classification, especially on dense databases.

In this study, we propose a new associative classification method, ACCF(Associative Classifica- tion Based on Closed Frequent Itemsets). The method extends an efficient closed frequent pattern mining method, Charm to mine all frequent closed itemsets (CFIs) and their tidsets, which would help to generate the Class Association Rules (CARs)[6]. And we also adopt a new way to classify an unseen case correspondingly. Our extensive experiments on 18 databases from UCI machine learning database repository[10] show that ACCF is consistent, highly effective at classification of various kinds of databases and has better average classification accuracy in comparison with CBA[6]. Moreover, our performance study shows that the method helps to solve a number of problems that exist in the current classification systems.

Keywords: Closed frequent itemsets, class association rules, associative classification.

1. Introduction   Building accurate and efficient classifiers for large databases is one of the essential tasks of data mining and machine learning. In recent years, a new approach called associative classification [6, 7, 8, 9] is proposed to integrate association rule mining [2] and classification.

It uses association rule mining algorithm, such as Apriori or FP-growth, to generate the complete set of  association rules [2]. Then it selects a small set of high quality rules and uses these rules for prediction. The experiments in [6, 7, 8, 9] show that this approach achieves higher accuracy than traditional classification approaches such as C4.5[11]. However, associative classification suffers from efficiency due to the facts that it often generates a very large number of rules in class association rule mining, i.e., CARs, and also it takes efforts to select high quality rules among them.

There are two current solutions to the large number of rules in association rule mining. The first one is to mine only the maximal frequent itemsets[5](MFIs), which are typically orders of magnitude fewer than all frequent pattern(AFIs).While mining maximal sets help understand the long patterns in dense domains, they lead to a loss of information, since subset frequency is not available, maximal sets are not suitable for generating rules. The second is to mine only the closed frequent itemsets [1,3]. Closed sets are lossless in the sense that they can capture all the information about frequent itemsets, and has smaller cardinality than the set of AFIs, especially on dense databases.

In this paper, we propose a novel approach called ACCF, which inherits the basic idea of CBA[6] in building a classifier from the CARs.

ACCF makes the following contributions: 1. We propose a new way to build the more  accurate classifier. ACCF generates a much smaller set of high-quality predictive rules form the datasets. Experimental results show that classifiers built in this way are, in general, more accurate than those produced by associative classification, such as CBA[6].

2. It helps to solve a number of important problems with the existing classification systems. When datasets contain a large number of rules, both rule generation and rule selection in CBA[6] and CMAR[7] are time consuming.

Some datasets cannot be completed in the experiment without rule limit. ACCF can generate a smaller set of rules, with higher   DOI 10.1109/FSKD.2008.396     quality and no redundancy. So, ACCF is much more time-efficient in both rule generation and prediction, and it also achieves higher accuracy than associative classification.

3. We present a new framework for associative classification based on the novel concept of closed frequent itemsets.

The remaining of the paper is arranged as follows.

Section 2 reviews the general idea of associative classification. Section 3 devotes to the generation of CARs based on closed frequent itemsets. Section 4 discusses how to build a classifier based on the generated CARs. The experimental results on classification accuracy and the performance study on efficiency are reported in Section 5, and we conclude the study in Section 6.

2. Basic Concepts and Definition   Let I be a set of items and D a dataset of transactions, where each transaction has a unique identifier (tid) and contains a set of items[5]. The set of all tids is denoted as T. A set X ? I is also called an itemset and a set Y ? T is called a tidset. An itemset with K items is called a k-itemset. For convenience, we write an itemset {A, C, W} as ACW and a tidset {245} as 245. For an itemset X, we denote its corresponding tidset as t(X), i.e., the set of all tids of transactions that contain X as a subset. For a tidset Y, we denote its corresponding itemset as i(Y), i.e., the set of items common to all the transactions with tids in Y. Note that t(X) =?x?X t(x), and i(Y) =?y?Y i(y)[3].

For example, in Figure 1,  t(ACW)=t(A)?t(C)?t(W) =1345?123456?12345 = 1345  and i(12)=i(1)?i(2)=ACTW?CDW=CW.

DISTINCT DATABASE ITEMS  Jane Austen  Agatha Christic  SirArthur Conan  Mark Twain  P.G.

Wodehouse  A C D T W   DATABASE  Transaction Items 1 ACTW 2 CDW 3 ACTW 4 ACDW 5 ACDTW 6 CDT  Figure 1.Example database.

The support of an itemset X, denoted ?(X), is the  number of transactions in which it occurs as a subset, i.e., ?(X)=|t(X)|[1]. An itemset is frequent if its support is greater than or equal to a user-specified minimum support (min_sup) value, i.e., if ?(X)?min_sup. Let  c:P(I)?P(I) be the closure operator, defined as c(X)=i(t(X)), where X?I. A frequent itemset X is called closed if and only if c(X)=X [1]. Alternatively, a frequent itemset X is closed if there exists no proper superset Y ?X with ?(X) =?(Y). For instance, in Figure 1, c(AW) =i(t(AW)) =i(1345)=ACW, Thus, AW is not closed. On the other hand, c(ACW)= i(t(ACW))= i(1345) =ACW, thus ACW is closed.

Property 1  X ? i(t(X)) and Y ? t(i(Y)).

For example, AC ? i(t(AC))=i(1345)=ACW.

Theorem 1 For any itemset X, its support is equal to the support of its closure, i.e., ?(X)= ?(cit(X))[1]. This theorem states that the AFIs are uniquely  determined by the CFIs. Furthermore, the set of frequent closed itemsets is bounded above by the set of frequent itemsets, and is typically much smaller, especially for dense datasets. For very sparse datasets, in the worst case, the two sets may be equal.

Figure 2 shows all the AFIs and their tidsets with minsup=50%?and the CFIs are stressed by using the pane. We can see that the set of CFIs is typically much smaller than the set of AFIs.

Figure 2 A comparison between AFIs and CFIs Our proposed framework assumes that the database is a relational table T over m non-class attributes A1?Am and one class attribute C. A case has the form <a1,?,am ,c> ,where ai  are values of Ai and c is a class of C. A rule, or a k-rule, has the form Ai1=ai1??Aik =aik?C=c, with each attribute occurring at most once.

By prefixing a value with its attribute, we can omit attributes and write a rule as ai1??aik?C=c. A rule x, ai?c is a Ai ?specialization of x?c if ai is a value of Ai. |T| denotes the number of cases in T, and num(x) denotes the number of cases in T that contain all the values in x. The support of rule x?c, denoted sup(x?c), is num(x,c)/|T|. The confidence of rule x?c, denoted conf(x?c), is num(x,c)/num(x). Given a mini-  ACT?135  AT?135 AW?1345  A?1345 T?1356W?12345  {}?123456  D?2456 C?123456  AC?1345  ATW?135  TW?135 DW?245  ACTW?135  CD?2456  ACW?1345  CT?1356  CW?12345  CDW?245 CTW?135     mum confidence minconf, a rule is confident if conf (x?c)? minconf [6,7,8,9].

3. Generating Rules for Classification   ACCF consists of two phases: rule generation and classification.

In the first phase, rule generation, firstly, ACCF generates all the CFIs by the algorithm of Charm[4] in the training dataset. Secondly, ACCF mines the tidsets of all the CFIs, and it is also necessary to compute the tidset of each class label, then the meet of the two tidsets can be found easily. The sup(R) and conf(R) of the CARs can be compute according to the meet of the two tidsets. Thirdly, ACCF only selects the CARs whose sup(R) and conf(R) pass the given support and confidence thresholds respectively. The set of the CARs thus consists of all the possible rules that are both frequent and accurate [6].

In the second phase, classification, we should solve such a problem below. For a given data object, how to identify the most effective rule at classifying a new case.

In this section, we develop methods to generate rules for classification. The second phase, classi- fication, will be discussed in Section 4.

To find rules for classification, ACCF first mines the training data set to find the CFIs. This is a typical frequent pattern or association rule mining task. To make mining highly scalable and efficient, ACCF adopts an efficient algorithm Charm[4] for mining the CFIs. It enumerates closed sets using a dual itemset- tidset search tree as shown in figure 3.The general idea of mining rules in ACCF is shown in the following example.

Example (Mining class-association rules) Given a training data set T as shown in Table 1.Let the support threshold is 1% and confidence threshold is 60%.

ACCF mines class-association rules as follows.

Table 1. A training data set.

Tid Items Class label 1 ACTW Y 2 CDW Y 3 ACTW Y 4 ACDW N 5 ACDTW N 6 CDT N   First, ACCF generates all the CFIs and their tidsets  of the training data set T. Figure 3 shows all the CFIs and their tidsets with minsup=1%.

Then, we should select the rules whose supports and confidences pass the given thresholds. We can compute the tidsets of the class labels at first, for  example, t(Y) =123, t(N)=456, and the meet of the two tidsets can be found easily. Furthermore, ACCF prunes some rules and only selects the rules whose supports and confidences pass the given thresholds.

Figure 3 All the CFIs and their tidsets generated by Charm We define the form of a possible rule in the ACCF  as :< rule, support, confidence>.We can generate 17 possible rules:  <ACDTW?N, 1/6, 1/1>, <ACTW?Y, 2/6, 2/3>, <ACTW?N, 1/6, 1/3>, <ACW?Y, 2/6, 2/4>, <ACW?N, 2/6, 2/4>,    <CW?Y, 3/6, 3/5>, <CW?N, 2/6, 2/5>,     <ACDW?N, 2/6, 2/2>, <CDW?Y, 1/6, 1/3>,    <CDW?N, 2/6, 2/3>, <CT?Y, 2/6, 2/4>,      <CT?N, 2/6, 2/4>, <C?Y, 3/6, 3/6>,       <C?N, 3/6, 3/6>, <CDT?N, 2/6, 2/2>,     <CD?Y, 1/6, 1/4>, <CD?N, 3/6, 3/4>.

But only 6 rules can be selected. There are: <ACDTW?N, 1/6, 1/1>, <ACTW?Y, 2/6, 2/3>, <CW?Y, 3/6, 3/5>,     <ACDW?N, 2/6, 2/2>, <CDW?N, 2/6, 2/3>,   <CDT?N, 2/6, 2/2>.

4. Building a Classifier   After a set of rules is selected for classification, as discussed in Section 3, ACCF is ready to evaluate all the possible subsets of rules on the training data and selecting the subsets with the right rule sequence that gives the least number of errors. In this section, we inherit the basic idea of CBA[6].

Definition: Given two rules, ri and rj, ri ? rj (also called ri precedes rj or ri  has a higher precedence than rj) if the following conditions hold: conf(ri)>conf(rj); or if conf(ri)=conf(rj), but sup(ri)>sup(rj);or if sup(ri)=sup(rj), but size(ri)< size (rj),i.e., ri  has fewer attribute values in its left hand side than  rj does; or if size(ri)=size(rj),but ri precedes rj in the lexicographical order of rules[9].

Let R be the set of generated rules i.e., CARs, and D the training data. The basic idea of the algorithm is to choose a set of high precedence rules in R to cover D.

CDT?56  C?123456 AWC?1345 WC?12345  {}  CT?1356 CD?2456  ACDTW?5 ACDW?45 ACTW?135 CDW?245     Our classifier is of the following format :<r1, r2, ?, rn>,where ri ?R, ra rb  if b > a. In classifying an unseen case, the first rule that satisfies the case will classify it.

For example, we have found 6 rules form Figure 3.

Now we should define a total order on the generated rules.

{<ACD?N, 2/6, 2/2>, <CDT?N, 2/6, 2/2>, <ACDTW?N, 1/6, 1/1>, <ACTW?Y, 2/6, 2/3>, <CDW?N, 2/6, 2/3>,   <CW?Y, 3/6, 3/5>}.

The algorithm for building a classifier using CARs  is similar to that in CBA[6].

There are three major differences: 1. ACCF can generate rules without redundancy  and with higher accuracy. For example, in CBA[6], the following seven CARs can be generated during the process of rule mining:  <A?N, 2/6, 2/2>, <C?N, 2/6, 2/2>, <D?N, 2/6, 2/2>, <AC?N, 2/6, 2/2>, <AD?N, 2/6, 2/2>, <CD?N, 2/6, 2/2>, <ACD?N, 2/6, 2/2>.

And only 3 of them can be selected for the classifier finally:  <A?N, 2/6, 2/2>, <C?N, 2/6, 2/2>, <D?N, 2/6, 2/2>.

There are two disadvantages: On the one hand, there are 4 redundant rules: <AC?N, 2/6, 2/2>,  <AD?N, 2/6, 2/2>, <CD?N, 2/6, 2/2>, <AC?N, 2/6, 2/2>.

It is challenging to store, retrieve, prune and sort a  large number of rules efficiently for classification; On the other hand, only the three selected rules  above cannot express all the information, which would reduce the accuracy of the classifier.

However, in ACCF, only one of the seven rules would be produced :< ACD?N, 2/6, 2/2>, and the rule can express all the information of the above other six rules by using our particular rule matching method below.

2.  Given a new data object t, CBA[6]  seeks the rule  x?c, and t contains all the values in x. However, in ACCF ,an object t is said to match the rule y?c if and only if : (1) t contains all the values in y, which is similar with CBA[6]; (2) or if there is no rule that applies to the case t, we select the first rule p in the classifier which contains one exactly same value in x at least, for example, CF is a new object, and ACD?N satisfies the case, because the meet of CF and ACD is C. However, in CBA[6], the default class label is considered to be the label of t.

Experiment results show that the classifier built in this way is more accurate.

5. Experimental Results and Performance Study   We evaluate ACCF using 18 datasets from UCI Repository[10] and conducted an extensive performance study to evaluate accuracy and efficiency of ACCF and compare it with CBA[6].

All the experiments are performed on a 2.40 GHz Celeron(R) PC with 768 MB main memory, running Microsoft Windows. The parameters of CBA are set as the following. We set support threshold to 1% and confidence threshold to 50% and disable the limit on number of rules without rule pruning option. In such setting, ACCF and CBA[6] generate all the rules necessary for classification and thus are compared in a fair base. Other parameters remain default. Also, we adopt the same method used by CBA[6] to discrete continuous attributes.

Table 2  Experiment results  Dataset  CBA ACCF CBA ACCF CBA ACCF  CARs CARs rules rules Accuracy Accuracy  austra     172 126.7 0.8623 0.88261*  auto     63 49.1 0.7656 0.815*  breast@ 2831 4437 64 52.1 0.9599 0.96957*  cleve     92 72.4 0.8167 0.82333*  crx     179 140.5 0.8449 0.86812*  diabete@ 3315 723.8 66 53 0.7587 0.7829*  glass@ 4234 437.9 33 25.4 0.7698 0.79048*  heart@ 52309 5267 53.2 51 0.8295 0.84815*  hepatitis     43 29.9 0.8576 0.84667  horse     123 90.2 0.8061 0.83611*  iris@ 72 60.7 10 6.9 0.9391 0.96667*  labor@ 5565 430 18 11.9 0.9499 0.78  led7@ 464 1274 46 65.6 0.695 0.86313*  pima@ 2977 723.8 66 51 0.7601 0.81711*  tic@ 7063 7443 32 44.8 0.9886 0.99053*  vehicle     148 65.6 0.7195 0.73095*  wave     661 491.1 0.8134 0.8352*  wine@ 38070 9471 11 7.6 0.9833 0.95882  average 11690 3027 105 79.71 0.84 0.8559  Table 2 shows the efficiency and accuracy of 18 datasets from UCI ML Repository.10-fold cross valid- ation is used for every dataset.

As can be seen from the table, ACCF outperforms on the number of CARs, the size of classifier and the accuracy.

Firstly, Column 1 lists the names of the 18 data sets.

Column 2 and 3 show the average number of CARs in each cross-validation using the approach of CBA[6] and ACCF respectively on 10(marked with a @ in Table 2) of the 18 datasets, and column 2 was implemented by its author. It proves that ACCF can reduce the number of CARs obviously. Column 4 and 5 give the size of classifier in terms of the number of rules. ACCF produces a much smaller classifier for many datasets.

And the last two columns list the accuracy of the classifier produced by CBA[6] and ACCF on average, it is clear that the average accuracy increase from 0.84 for CBA[6] to 0.8559 for ACCF. Furthermore, out of the 18 data sets, ACCF achieves the better accuracy in 15 ones(marked with a * in Table 2). In the experiment, ACCF is superior in several ways, and we can draw the conclusion that the set of frequent closed itemsets uniquely determines the exact frequency of all itemsets, yet it can be orders of magnitude smaller than the set of all frequent itemsets.

6. Conclusions   A new classification approach, called ACCF, is developed to integrate classification and association rule produced by CFIs. Closed sets are lossless in the sense that they uniquely determine the set of all frequent itemsets and their exact frequency. At the same time closed sets can themselves be orders of magnitude smaller than all frequent sets, especially on dense database. Based on our performance study, ACCF achieves high accuracy and efficiency, which can be credited to the following advantages: (1) it uses the concept of CFIs in generating the CARs, which is much more efficient than generating all candidate rules; (2) it is more efficient in terms of classification rule generation by reduction the redundancy ones; (3) it can achieve the higher accuracy than that of the classification based AFIs, i.e., CBA[6]. ACCF repress- ents a new framework based on closed itemsets that can drastically reduce the rule set, and can be presented to the user in a succinct manner.  It is interesting to further enhance the efficiency and accuracy of this approach and compare it with other well-established classification schemes.

7. References  [1] M.J. Zaki, ? Generating Non-Redundant Association Rules?, Proc. Of the Sixth ACM SIGKDD Int?l Conf, on Knowledge Discovery and Data Mining, Aug. 2000, pp.1-18.

[2] G. Grahne and J. Zhu, ? Fast algorithm for frequent itemset mining using FP-trees,? IEEE Trans. on Knowledge and Data Engineering, vol. 17, issue 10, Oct. 2005, pp.1347- 1362.

[3] M.J. Zaki and B. Phoophakdee,? MIRAGE: A Framework for Mining, Exploring, and Visualizing Minimal Association Rules?,Technical Report 03-4,Computer Science Dept., Rensselaer Polytechnic Inst., July 2003.

[4] M.J. Zaki, Ching-Jui Hsiao,? Charm: An efficient Algorithm for closed Association Rules Mining?, Technical Report, TR99-10.Computer Science Dept. Rensselaer Polytechnic Institute, 1999.

[5] Gouda K, Zaki MJ, ? Efficiently mining maximal frequent itemsets?, In: Proc. of the 1st IEEE Int'l Conf. on Data Mining. 2001,  pp. 163-170.

[6]  B. Liu, W. Hsu and Y. Ma, ? Integrating Classification and Association Rule Mining? ,Proceedings of the Fourth Mining, 1998, pp.80?86.

[7] W. Li, J. Han and J. Pei, ? CMAR: Accurate and efficient classification based on multiple class-association Rules?, In ICDM'01, San Jose, CA, Nov.2001, pp. 369-376.

[8] X. Yin and J. Han, ? CPAR: Classification Based on Predictive Association Rules?, Proceedings of the Third 208?217.

[9]  K. Wang, S. Zhou, and Y. He,  ? Growing decision tree on support-less association rules?, In KDD?00, Boston, MA, Aug. 2000.

[10] C.J. Merz, and P. Murphy, UCI repository of machine learning databases, 1996.

[ftp://ftp.ics.uci.edu/pub/machine-learning-databases/] [11] J. R. Quinlan, ? C4.5: program for machine learning?, Morgan Kaufmann,1992.

