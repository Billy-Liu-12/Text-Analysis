A Novel Intrusion Detection System Based on the 2-Dimensional Space Distribution of Average Matching Degree

Abstract: Because of the increasing reliance on the Internet and its worldwide connectivity, Intrusion Detection System (IDS) has attracted the attention of many researchers to strengthen the Internet security. In the field of IDS, anomaly detection still is not a mature technology yet compared with misuse detection. In this paper, a classification model (called Distance-based Classification Model) is improved, which considers both misuse detection and anomaly detection. The evaluation of the proposed model is carried out over NSL-KDD data sets, which consists of selected records of the complete KDD data sets.

Keywords: Intrusion Detection System, Distance-based Classification Model, KSL-KDD data sets, NSL-KDD data sets  1. INTRODUCTION  With the enormous growth of Internet usage and in- creased potential damage by intrusions launched by ma- licious or unauthorized users, network security has be- come a critical issue. It is no longer enough to prevent these intrusions by just deploying sophisticated firewalls or security policies because of the unknown weaknesses or bugs in systems and applications, which will be ex- ploited by cyber hackers. Therefore, Intrusion Detection Systems (IDS)[1] have become an essential component of Internet security to detect intrusions.

The field of IDS is currently about 30 years old, with the trend of gathering more attention in the next decade.

A lot of early work identified two major types of intru- sion detection strategies in IDS; misuse detection and anomaly detection[2]. Misuse detection, also called sig- nature detection, decides in advance what type of behav- ior is undesirable for the subject, then through the use of predefined signatures of such behavior, intrusions can be detected. On the other hand, in anomaly detection, ev- erything that is unusual for the subject (computer, user, etc.) is declared suspect, and worthy of further investi- gation. So it?s extremely important to form an accurate opinion of what the subject?s normal behavior is. There is few commercial products using anomaly detection ap- proaches, because of its predictability and low accuracy compared with misuse detection. But, cyber hackers, knowing that intrusion detection systems are installed in our network, will always try to develop and launch ?new? attacks. Therefore, anomaly detection must be well stud- ied due to its theoretical potential for addressing novel attacks.

In this paper, a classification model(called Distance- based Classification Model[3]) is improved, which com-  bines the positive features of both misuse detection and anomaly detection to achieve higher detection rate and lower false rate. In our model, rules extracted from well- known intrusion data are treated as signatures of mali- cious behavior in misuse detection, while rules from nor- mal access data are used to build the profile of what a nor- mal behavior is. GNP-based Class Association Rule Min- ing method[4][5], a evolutionary data mining method, is applied to mine rules from data. Then, the Distance- based Classification model is built based on these rules by using the concept of 2-dimensional space distribution of the average matching degree. Finally, this classifica- tion model can be used to detect not only well-known intrusions but also novel intrusions.

The rest of the paper is organized as follows: Section 2 reviews related works and background of our research.

Section 3 is an overview of our previous research. Section 4 discusses the details of the improved distance-based classification model. Finally, the simulation results and conclusions are given in section 5 and section 6.

2. RELATED WORKS AND BACKGROUND  At present, there are many traditional and classic al- gorithms of extracting rules from database (e.g., Apriori and FP-growth). However, these algorithms may suffer from large computational complexity for rule extraction when the database is dense. In this paper, GNP-based Class Association Rule Mining method is used to perform the task of rule extraction. Unlike traditional algorithms, GNP-based Class Association Rule Mining method is an evolutionary method, which can extract sufficient classi- fication rules in a short time. In this section, the outline of Genetic Network Programming(GNP) and GNP-based  SICE Annual Conference 2011 September 13-18, 2011, Waseda University, Tokyo, Japan  PR0001/11/0000-2829 ?400 ? 2011 SICE- 2829 -    Fig. 1 The Basic Structure of GNP  Class Association Rule Mining are briefly reviewed.

2.1 The Structure of GNP[6][7]  GNP has been proposed as an extension of Genetic Al- gorithm (GA) and Genetic Programming (GP). The basic genes of GNP are directed graph structures, whereas GA uses strings and GP uses trees. Usually, GNP is mainly composed of two kinds of nodes: judgment node and processing node. Judgment nodes serve as the if-then type decision making functions, which determine what the next node is. Processing nodes serve as the action functions, in which some processing can be done. A typ- ical structure of a GNP individual is displayed in Fig. 1.

Similar to other evolutionary methods, the structure of GNP are evolved by performing genetic operators ( e.g., selection, crossover and mutation).

2.2 GNP-based Class Association Rule Mining[4][5]  Let I = {A1, A2, . . . , An} be the set of attributes in the training database TrDB and each tuple ti in TrDB has a set of attributes Ti that satisfies Ti ? I . Usually, an association rule is an implication of the form X ? Y , where X ? I , Y ? I , and X ? Y = ?. The meaning of this association rule is that if the antecedent X is satisfied, then the consequent Y is also satisfied. A very large num- ber of candidate association rules can be generated when given the attribute set I . The key is to evaluate all these candidate association rules. Many techniques that mea- sure the interestingness of a rule have been developed. In GNP-based Class Association Rule Mining, the ?2 mea- surement is used to evaluate the relationship between X and Y of rule X ? Y . The rate of tuples satisfying X in the database TrDB is called the support of X , de- noted by support(X). The confidence of rule X ? Y is defined as the ratio of support(X ? Y )/support(X), denoted by confidence(X ? Y ).

In this paper, important association rules X ? Y should satisfy these following assumptions, which mea-  Fig. 2 GNP Representation of Class Association Rules  sure the interestingness of rules:  ?2 ? ?2min, support(X ? Y ) ? supmin,  confidence(X ? Y ) ? confmin, (1)  where, ?2min, supmin and confmin are the predefined thresholds.

In GNP-based Class Association Rule Mining, a class association rule can be represented by  (Am = 1) ? . . . ? (An = 1) ? (k ? C). (2)  Where Ai is an attribute of database TrDB with the value of 1 or 0 (1 means satisfied, 0 means not satisfied), k is the class label and C is the set of suffixes of classes.

Class association rule can be viewed as a special case of the association rule X ? Y with a fixed consequent.

In GNP-based Class Association Rule Mining, at- tributes and their values of rules correspond to the func- tions of judgment nodes in GNP. And the connection of judgment nodes can represent candidate class association rules. An example of the GNP representation is displayed in Fig. 2. Processing node P1 serves as the beginning of class association rules. A1 = 1 and A2 = 1 denote the functions of judgment nodes. For example, the candidate class association rules, such as (A1 = 1) ? (C ? k), (A1 = 1) ? (A2 = 1) ? (C ? k) and so on, can be represented by GNP in Fig. 2.

3. PREVIOUS RESEARCH In this section, we first introduce the concept of  the average matching degree, then review the previous Distance-based Classification Model. The reason why the mechanism of the average matching degree is devised is to avoid the overfitting problem by using sufficient rules in one class.

3.1 Definition of Matching Degree Between Data and Rules  The matching degree of data d with rule r in class k measures how well data d matches with rule r in class k  - 2830 -    by Eq.(3),  Matchk(d, r) = Nk(d, r)  Nk(r) , (3)  where, Nk(d, r) is the number of matched attributes in the antecedent part of rule r in class k with data d. Nk(r) is the number of attributes in the antecedent part of rule r in class k. The average matching degree of data d with all the rules in class k is defined by Eq.(4) based on the definition of the matching degree between a data and a rule.

mk(d) =  |Rk| ? r?Rk  Matchk(d, r), (4)  where, Rk is the set of suffixes of rules in class k. Us- ing the average matching degree between data d with the rules in class k, we can measure how well data d matches with the rules in class k.

3.2 Distance-based Classification Model[3] In the distance-based classification model, the basic  idea is to represent access data d in the space of the aver- age matching degree with the rules in class k, k ? C = {1, 2, . . . ,K}, which can be calculated using Eq.(4).

Therefore, the coordinate of data d can be denoted as (m1(d),m2(d), . . . ,mK(d)) in a K-dimensional space.

In the building phase of the distance-based classifica- tion model, all training data should be mapped into the K-dimensional average matching degree space. In the classification phase of the distance-based classification model, test data t is firstly represented as the coordinate of (m1(t),m2(t), . . . ,mK(t)), then the distance of test data t to the rules in class k, denoted by Dk(t), can be calculated.

In case of IDS described in Fig.3, test data t is firstly represented as the coordinate of (mN (t),mI(t)). Then, the distances of test data t to normal and known-intrusion class are calculated using N-closest neighboring points, denoted by DN (t) and DI(t), respectively. An abnor- mal point (mN (abn),mI(abn)) is manually set in the 2- dimensional average matching degree space to calculate the distance of test data t to anomaly-intrusion class, de- noted by DA(t). The class label of test data t is decided by the smallest distance among {DN (t), DI(t), DA(t)}.

For example, if DN (t) is the smallest, then test data t is labeled as normal class.

4. IMPROVED DISTANCE-BASED CLASSIFICATION MODEL  In the improved distance-based classification model, the building phase of the model is basically the same.

However, in the classification phase, misuse detection and anomaly detection are done in two steps.

Fig. 3 Distance-Based Classification Model  4.1 Misuse Detection  In the step of misuse detection, N-closest neighbor- ing points method is used to calculate the distances of test data t to normal and known-intrusion class, DN (t) and DI(t). It is reasonable to tell that test data t is more likely to be a normal data or a well-known intru- sion data by comparing DN (t) and DI(t). For example, if DN (t) is smaller than DI(t), then test data t is more likely to be a normal data, rather than a well-known in- trusion data. One advantage of using N-closet neighbor- ing points method is that detailed information on training data can be used to solve the overlapped problem, which occurs when mapping normal data and well-known intru- sion data into 2-dimensional matching degree space.

4.2 Anomaly Detection  In the step of anomaly detection, the centroid method is applied to recalculate the distances of test data t to normal and known-intrusion class, denoted by D?N (t) or D?I(t). The centroid point(mN (CN), mI(CN)) of nor- mal training data, called normal centroid, can be calcu- lated using Eqs.(5)?(6). The centroid point(mN (CI), mI(CI)) of well-known intrusion training data, called in- trusion centroid, can be also calculated in the same way.

Then, anomaly points are manually set by using the co- ordinates of normal centroid and intrusion centroid, i.e., (mN (CI), mI(CN)) and (mN (CN), mI(CI)). It?s rea- sonable to assume that anomaly data is like neither well- known intrusion nor normal data, or like both well-known intrusion and normal data. Finally, the distance of test  - 2831 -    data t to anomaly class, denoted by DA(t), is calculated.

mN (CN) =  ? d?DTrain(normal) mN (d)  |DTrain(normal)| , (5)  mI(CN) =  ? d?DTrain(normal) mI(d)  |DTrain(normal)| , (6)  where, DTrain(normal) is the set of suffixes of normal training data set.

In Fig.4, D?N (t) and D ? I(t) are calculated by  Eqs.(7)?(8) using normal centroid and intrusion cen- troid, meanwhile DA(t) is calculated by Eq.(9) using anomaly points. The class label of test data t is decided by the following mechanism: 1. If DN (t) ? DI(t) in the step of misuse detection, then DN (t) is recalculated as D?N (t) in the step of anomaly detection. If D?N (t) ? DA(t), then test data t is la- beled as normal class; otherwise, test data t is labeled as anomaly class.

2. If DN (t) ? DI(t) in the step of misuse detection, then DI(t) is recalculated as D?I(t) in the step of anomaly de- tection. If D?I(t) ? DA(t), then test data t is labeled as well-known intrusion class; otherwise, test data t is la- beled as anomaly class.

One advantage of the centroid method is that overall in- formation of training data is exploited to find whether test data t deviates significantly from normal class or well- known intrusion class.

D?N (t) =? (mN (t)?mN (CN))2 + (mI(t)?mI(CN))2,(7)  D?I(t) =? (mN (t)?mN (CI))2 + (mI(t)?mI(CI))2, (8)  DA(t) = min{? (mN (t)?mN (CI))2 + (mI(t)?mI(CN))2, (9)? (mN (t)?mN (CN))2 + (mI(t)?mI(CI))2 }.

5. SIMULATIONS The simulations of the improved classification model  is carried out on new data sets, NSL-KDD[8], which con- sists of selected records of the complete KDD data sets, referring to two important issues in the KDD data sets[9].

One issue is about the huge number of redundant records in KDD data sets, which causes the learning algorithms to be biased towards frequent records, and thus prevents them from learning unfrequent records. The other issue  Fig. 4 Centroid Method in Anomaly Detection  is about the low level of difficulty in the KDD train and test data set, which results in high accuracy rates, and thus hard to fairly evaluate different algorithms. To solve these issues, the data sets in NSL-KDD, KDDTrain+ and KDDTest+, are generated from original KDD data sets by removing redundant records and increasing the level of difficulty. The simulations will mainly focus on two parts: 1. To evaluate whether the improved classification model works better compared with the previous one.

2. To train the improved classification model by both KDD and NSL-KDD data sets, then to evaluate whether it is necessary to prune KDD train data set.

5.1 Simulation I The improved classification model is built upon 4,000  training records randomly selected from the KDDTrain+  data set in NSL-KDD data sets. The parameters of train- ing the improved classification model are listed in Ta- ble 1. According to Table 1, the training data set of the model has 2,000 normal access data and 2,000 known- intrusion data consisting of 1,500 with neptune type and 500 with smurf type. In the rule extracting phase, GNP- based Class Association Rule Mining extracts 1,711 nor- mal rules and 1,965 known-intrusion rules, which con- tains 1,000 neptune-type intrusion rules and 965 smurf- type intrusion rules. In the classification building phase, N is set at 30, which means only 30-closest neighboring points are used in misuse detection.

In the testing phase, two test cases, TestCase I and TestCase II, are considered to evaluate the effective- ness of the improved classification model. TestCase I consists of 20,000 data randomly selected from KDD data sets, while TestCase II uses the KDDTest+ set  - 2832 -    Table 1 Parameters of Training Improved Classification Model built upon NSLKDD  Normal Data 2,000 Intrusion Data 2,000 Normal Rules 1,711  Intrusion Rules 1,965 N 30  from NSL-KDD data sets. Compared with TestCase I, TestCase II contains no redundant record, but more anomaly-type intrusions. The classification results of these two test cases are shown in Table 2 and Table 3. Normal(T ), Known(T ) and Unknown(T ) indicate the number of normal, intrusion and anomaly class la- beled by the improved classification model, respectively.

Normal(C), Known(C), and Unknown(C) indicate the correct number of normal, intrusion and anomaly class, respectively.

Table 2 Classification Result of TestCase I  Normal(T) Known(T) Unknown(T) Total Normal(C) 2, 908 10 927 3, 845 Known(C) 0 14, 312 20 14, 332  Unknown(C) 779 4 1040 1823 Total 3, 687 14, 326 1, 987 20, 000  Table 3 Classification Result of TestCase II  Normal(T) Known(T) Unknown(T) Total Normal(C) 8, 050 69 1, 592 9, 711 Known(C) 0 5, 103 219 5, 322  Unknown(C) 2, 986 40 4, 485 7, 511 Total 11, 036 5212 6296 22, 544  From Table 2 and Table 3, DR(Detection Rate), PFR(Positive False Rate), NFR(Negative False Rate) and AR(Accuracy Rate) can be calculated as shown in the Ta- ble 4.

Table 4 DR, PFR, NFR and AR of two TestCases  DR PFR NFR AR TestCase I 91.42% 24.37% 4.82% 91.3% TestCase II 79.39% 17.10% 23.27% 78.24%  Based on the test result of TestCase I and TestCase II, the AR comparison of the improved classification model with the previous one is displayed in Fig. 5. To perform a fair comparison, the anomaly point in the previous clas- sification model is set to different coordinates. For ex- ample, ?previous model + (0.5, 0.5)? means the anomaly point in the previous model is set to (0.5, 0.5).

From the AR comparison in TestCase I, we can see that the performance of both the improved model and previous model is even when less anomaly intrusions are  Fig. 5 AR Comparison between improved model and previous one  included in the test data. However, from the AR com- parison in TestCase II, when more anomaly intrusion are included, it is easy to see that the performance of the im- proved model is much better, which means that the ability of detecting anomaly intrusions is improved.

5.2 Simulation II The improved classification model is built upon  10,000 training data records randomly selected from the KDDTrain data set in KDD data sets, called KDD- Classifer. The parameters of training KDD-Classifer are listed in Table 5. From Table 5, the training data contains 5,000 normal access data and 5,000 known-intrusion data. The known-intrusion data consists of 2,500 intru- sion with neptune-type and 2,500 intrusions with smurf- type. After rule extraction, 3,931 normal rules and 7,969 known-intrusion rules are extracted. One the other hand, the improved classification model built in Simulation I is called NSLKDD-Classifier. Considering the factor of re- dundant records in KDD data sets, the number of training data selected for KDD-Classifer is much more compared with 4,000 training data selected for NSLKDD-Classifier.

Table 5 Parameters of Training Improved Classification Model built upon KDD  Normal Data 5,000 Intrusion Data 5,000 Normal Rules 3,931  Intrusion Rules 7,969 N 30  In the testing phase, TestCase I and TestCase II are both used to evaluate the performance of KDD-Classifier and NSLKDD-Classifier. The performance comparison of these two classifiers is shown in Fig. 6 and Fig. 7.

In Fig. 6 and Fig. 7, DR, NFR and PFR of NSLKDD- Classifier and KDD-Classifier are quite close to each other in both test cases. But, in both test cases, the AR of NSLKDD-Classifier is much higher than that of KDD-Classifier. The main reason for this phenomenon is that NSLKDD-Classifier distinguishes known-intrusions  - 2833 -    Fig. 6 Performance Comparison of TestCase I  Fig. 7 Performance Comparison of TestCase II  from anomaly intrusions much better compared with KDD-Classifier.

6. CONCLUSIONS In this paper, firstly, a distance-based classification  model is improved based on the concept of the average matching degree using sufficient classification rules in each rule class, which is testified by the KDDTest+ data set from NSL-KDD data sets. Secondly, through remov- ing redundant records and increasing the level of diffi- culty, it is shown that NSL-KDD data sets do solve some problems that the original KDD data sets may not. For example, in Simulation I, it is hard to tell which classifi- cation model is best only using TestCase I. At last, it is found that the classification model built upon NSL-KDD data sets, i.e., NSLKDD-Classifier, outperforms KDD- Classifier.

