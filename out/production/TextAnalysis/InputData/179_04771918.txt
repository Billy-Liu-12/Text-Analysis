An Association Rule Algorithm Based on Quotient Space

Abstract?Finding the large item set fast is the crucial step in the association rule algorithm. In this paper we apply granular computing and quotient space to frequent item set discovering , by partition the information system to information granule and mapping granule object sets, the algorithm reduced the number of database scanning and reduced object sets required when computing support of candidate item, in this way we improved the efficient of running. For the dataset of small support with high complexity, it is more applicable. The experiment results show: the proposed method is effective.

Keywords-association rule ?  quotient space ?  granular computing

I.  INTRODUCTION Association rule is one of the most active fields in data  mining. The most influential algorithm is Apriori[1], which is proposed by Agrawal in 1994, which generates the k- candidate by combining two (k-1)-itemset that have the first k-2 attribute values in the two (k-1)-itemset the same and the last pair does not. A new k-candidate becomes a k-large itemset if every (k-1)-subset of the k-candidate is a large itemset otherwise it is removed. The algorithm need to do table scan of the whole dataset and examine the item sets multiple times, the process is very consuming. To solve the problem of multiple scanning database and too much candidate sets, a lot of algorithm have been improved, such as FP-Growth algorithm [2], DHP algorithm [3] and so on.

But when deal with mass data, construct FP-tree based on memory is not realistic, and DHP which needs to construct HASH table is consuming too.

We have proposed that apply granular computing to frequent itemsets discovering before, but the representation of information granule?s data structure is not very perfect. In this paper we not only introduce granular computing and quotient space to frequent itemsets discovering, but also we change the way of representation of information granule.

Firstly we get information granule from information system, the data structure of information granule is compressed binary string and linear list; and then we combine information granule into acquired item sets. In this way the complexity is decreased and efficiency is improved.



II. BASIC NOTIONS OF GRANULAR COMPUTING AND QUOTIENT SPACE  Granular computing [4] was first proposed by TY Lin and has become a very important tool in data mining since  then. Lin [5] proposed a bitmap based association rule algorithm using granular computing technique and introduce the bitmap technique to the data mining procedure. Wang and others [6-7] developed a granular computing model based on tolerance relation for processing incomplete information systems to acquire approximate knowledge and rules. Yao [8-9] and his partners have done a lot of research on granular computing and applied it to data mining to solve the problem of association rules.

There are two basic problems in granular computing: the representation of granule and the computing among the granule. In this paper, by partition information system to acquire information granule, we solved the first problem and by combine information granule in quotient space according to attribute and universe, we complete the second problem.

A. granular computing An information system [10] is defined  as ( ) }{ }{( ), , , ,S U At C D L Va a At Ia a At= = ? ? ? , where U is a finite nonempty set of objects, At  is a finite nonempty set of attributes, L is a language defined using attributes in At , Va is a nonempty set of values for a At? , and :Ia U Va? is an information function. For any a At? , ( )a x v Va= ?  where x U? , Va  is a set of attributes, is called ( )a x v=  or va . A concept definable in an information table is a pair ( )( ), m? ? , where ? is a description of ( )m ? in S , and ( )m ? is the set of objects satisfy ? . An example of information table is given in TABLE ?, which is a classic dataset from WEKA example.

Def.1 ( )gs Gr is the granule element set; it is a mapping function from granules to object sets. For any granule ( ( ))1,Gr f? ??= , which satisfy ( ) ( )1gs Gr f ??= .

Def.2 the granularity of granule ( ( ))1, f? ??  is defined as: ( ) ( )1G f? ??= , which is the size of a granule.

TABLE I.  AN INFORMATION TABLE(WEATHER)  Object outlook windy play O1 O2 O3 O4 O5 O6 O7 O8  sunny sunny overcast rainy rainy sunny sunny rainy  false true false false false false false true  yes yes no no no no yes yes  Second International Workshop on Knowledge Discovery and Data Mining  DOI 10.1109/WKDD.2009.133     B. the theory of quotient space Quotient space [11] use triple ( ), ,X f ? to describe a  problem space or simply a space, where X  denotes the universe, f indicates the attributes (or features) of universe X , and ? is the structure of universe X . When view the same universe X from a coarser grain-size, we have a coarse-grained universe denoted by [ ]X , and have a new problem space [ ] [ ] [ ]( ), ,X f ? . The coarser universe [ ]X  can be defined by a crisp equivalence relation R  on X . Then the original space ( ), ,X f ?  is transformed into a coarser space  [ ] [ ] [ ]( ), ,X f ?  called the quotient space of ( ), ,X f ? , where [ ]f and [ ]?  are the corresponding quotient attributes and quotient structure of X  respectively. And space [ ] [ ] [ ]( ), ,X f ? can further be transformed into a new space with much coarser resolution by some equivalence relation 2R .

There are two main properties in quotient space structure: truth-preserving property and false-preserving property. The former refers to if a proposition is true in the coarser grain- size space, then in certain condition; it is true in the synthetical quotient space to the corresponding problem.

Suppose we get information from two different coarse- grained spaces ( )1 1 1, ,X f ? and ( )2 2 2, ,X f ? , when we combine these two parts of information into a new one in the combination space ( )3 3 3, ,X f ? , the truth-preserving property help us reduce computational cost. The properties can be applied to data mining as long as we handle a problem from a fine space to a coarse one. The latter refers to if there is no solution in some area of quotient space, there is no solution in the corresponding area of its original space. When we solve a problem from a coarse-size world to a fine one hierarchically, if we find that some area of the coarse world has no solution, then when we enter the fine world the corresponding area in the world can be omitted for further consideration due to the false preserving property so that the computational cost is reduced.

From the point of granule, the supersets of frequent item sets can be regarded as coarser grain-size, and subsets of frequent item sets can be regarded as finer grain-size. Then the father-son relationship of frequent pattern can be regarded as partial ordering relation in granular space. So the father-son relationships of frequent pattern in association rules satisfy the truth-preserving property and false- preserving property of quotient space. And we can combine subsets into supersets just like we combine coarse-grained spaces into fine-grained spaces. And we can combine into fine-grained spaces from the point of universe, structure and attribute, details please refer to the literature.



III. FREQUENT ITEMSETS DISCOVERING ALGORITHM BASED ON QUOTIENT SPACE  The basic thought of frequent itemsets discovering algorithm based on quotient space (quo_Fre) is that: firstly we divide the information system S to get the basic information granule sets GrS , the method is that we partition each attribute according to equivalence relation. And GrS is a  quotient space of S . For Gr GrS? ? , it is a describe ( ),a v of S , and satisfy min upGr S? (minSup is the minimum support).

Each information granule is a binary string; and for Gr GrS? ? , we must record the corresponding granule element sets ( )gs Gr , the object sets are stored in a linear list, that is, we store the number which is 1 in the binary string in the linear list. Now we get all the required information and we can combine the basic information granule to a new one in universe and attribute in accordance with certain restrictive conditions, until all the frequent item sets is found.

So quo_Fre algorithm is completed in two steps: the acquisition of basic granule and combination of information granule.

A. The acquisition of basic granule Input: information system S {U,C D,V,f}= ? , minSup Output: information granule sets GrS  1) Grs = ? , m=|Attr|?n=|U|?i =1; //m is the number of attributes, n is the number of records  2) Partition each attribute according to equivalence relation: ( )iU IND Attr and get information granule ijg ,  ( )ij ig U IND Attr? ?  belong to the ith attribute and jth equivalence class.

3) For ijg? , ijg [i] =0? i=1, 2?n? ijgnode .data= ijg ; ijgnode .next=null;  4) For (i=1?i<=n; i++) //scan the table, initialize ijg For jkg?  ?if ji jkU g?  {   // jiU belong to jth attribute  and ith record jkg .count++; jkg [i] =1; gnode.data=i; gnode.next=null; jkgnode .next=gnode;}  5) For ijg? ?If (gjk.sup>=minSup) ijGrs Grs g= ?  After step A, we have information granule sets GrS , which is all the frequent 1-itemsets. But different with traditional 1-itemsets, these 1-itemsets have the information of granule element sets ( )gs Gr , which is stored in the form of linear list, and for any Gr , which is a binary string. For example, TABLE ?  is an information table, now we illuminate the process of acquiring information granule sets with step A.

Firstly we partition each attribute according to equivalence relation, and acquire the quotient set: U/IND (outlook)={[sunny],[overcast],[rainy]};U/IND(windy)={[fal- se],[true]};U/IND(play)={[yes],[no]}.Among them [sunny]= {O1,O2,O6,O7},[overcast]={O3},[rainy]={O4,O5,O8},[fal- se]={O1,O3,O4,O5,O6,O7},[true]={O2,O8},[no]={O3,O4, O5,O6}, [yes]={ O1,O2, O7, O8}. Now we scan the table to computing the support of each 1-itemsets and on the other hand we must record granule element sets ( )gs Gr of each granule, the form is binary string, for example, [sunny] corresponding to the binary string 11000110, [false] corresponding to 10111110, otherwise, the bit equals 1 in the binary string is put together in a linear list. The linear list for       information granule [sunny] is showed in Fig.1, and the linear list for information granule [false] is showed in Fig.2.

Figure 1.  Linear list of [sunny]  Figure 2.  Linar list of [false]  Figure 3.  Linear list of [sunny,false]  Suppose the minimum support is 3, after step A, we get the information granule sets {[sunny], [rainy], [false], [yes],[no]}, that is 1-itemsets, and record all the required information to combine k-itemsets, so the process of scanning database is only conduct in the step A.

B. Combination of  information granule Input: information granule sets GrS Output: frequent item sets FrS  1) For (k=2; 1kL ? ? ? ; k++) { 2) For each itemset 1 1kl L ??  //combine 1l and 2l  For each itemsets 2 1kl L ?? If (have the first k-2 attribute values the same and the  last pair belong to different attribute) { 1 2c l l= ?   //connecting step Initialize c;  3) For (j=1;j<= 1l .sup;j++){ //computing support of c in the universe of  1l  }  4)  If (c.sup>=minSup) add c to kL ;} 5)  kL L L= ? ;}  Let us continue to illuminate the example above with step B, now we want to combine granule [sunny] and [false], which accord with the condition that the first k-2 attribute values the same and the last pair belong to different attribute, so firstly we combine the two granule in the attribute, and get a new granule [sunny, false], then consider the combination in the universe, according to step B, the universe of new granule [sunny, false] must be included in the universe of two sub granule, so we calculate the support and granule element sets of the new granule in the universe of [sunny] orderly, which is {O1,O2,O6,O7}, that is we combine 1111 and 1011. Corresponding to new granule [sunny, false], the granule element sets is {O1, O6, O7}, and the binary string is 10000110, the linear list is showed in Fig.3.



IV. THE ANALYSIS OF ALGORITHM quo_Fre algorithm is considered from the following three  aspects: Firstly, don?t need multiple scan of database. We divide  the information system to get the information granule sets.

Each information granule record the granule element sets in the form of linear list and binary string. So in the process of  seek all the frequent item sets, we only need to scan database in preprocessing in step A.

Secondly, when computing the support of candidate item, we don?t need to scan database completely. Suppose the maximal item set is k in a dataset, the cycle number is at  least  K  i i  C num =  ?? , which num is the record number (or number of transaction) and iC is the number of i-itemset.

When we solve the problem with quo_Fre algorithm, that?s different. For example, for a connecting candidate item 3 1 2l l l= ? , we only need to consider the universe of one sub item set, so for the same problem above, the cycle  number is reduced to  K  i i  C num ? =  ? ?? , which ( )0 1?? ? .

Thirdly, when combine (k-1)-itemset to k-itemset, the  two (k-1)-itemset must satisfy certain restrictive conditions, that is the first k-2 attribute values the same and the last pair belong to different attribute, in this way ,we reduced the number of candidate item set as soon as possible.



V. EXPERIMENT AND RESULTS The tests were conducted using the computer with  Pentium4 CPU (2.41GHz), 512MB memory, And software is WEKA+eclipse in Windows XP Professional SP2.WEKA is open source software, which is developed by University of Waikato in New Zealand and a lot of algorithm about data mining is contained in it. So we compare Apriori algorithm in WEKA with quo_Fre algorithm, the dataset is five group UCI dataset with arff format, which is list in TABLE ?.

TABLE ? compare the time-consuming of the two algorithms with the same support and TABLE ? compare the time-consuming of the two algorithms with the different support to the same datasets.

From the compare results of test we can see that when the dataset is small, quo_Fre algorithm have no predominance too much, because the algorithm need preprocessing to get corelative information to combine into required k-itemset.

But when the dataset is larger, the time-consuming of quo_Fre algorithm is less; and for the same dataset, the smaller the support, the less time consumed, because required granule object reduced while support become less.

TABLE II.  FIVE GROUP UCI DATASETS WITH ARFF FORMAT  Dataset Rows Column Sobean Kropt Connect4 Adult Ticdata_categ47    TABLE III.  COMPARE RESULTS WITH THE SAME SUPPORT  Dataset Support Apriori(s) quo_Fre(s) sobean kropt Connect   0.047 2.031 3.531  0.048 0.921 1.923     TABLE IV.  COMPARE RESULTS WITH THE DIFFERENT SUPPORT  Dataset Support Apriori (s) quo_Fre(s)  adult  0.843 1.86 2.605  0.644 1.278 1.422  Ticdata_categ47  3.687 5.672 16.594  2.263 3.097 8.875

VI. CONCLUSION With the rapid development of network and database  technology, the scale of the dataset is bigger and bigger. So how to mining knowledge from mass data is an important problem. The research about granular computing provides new method and ideas to association rules mining. In this paper we introduce information granule and quotient space to the frequent item set discovering procedure, by mapping granule element sets, we reduced the number of database scanning and reduced the number of  cycling while compute support of each candidate. The experiment results show: the proposed method is effective.

