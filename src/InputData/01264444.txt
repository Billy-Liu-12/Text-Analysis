AN ADAPTIVE ALGORITHM FOR INCREMENTAL  MINING ASSOCIATION RULES

Abstract: An incremental updating technique is developed for  maintenance of the association rules discovered by database mining. There have been many studies on efficient discovery of association rules in large databases. However, it is non trivial to maintain the association rules current when the two thresholds change. In this paper, we propose an adaptive algorithm (IMAM) for incremental mining of association rules in light of threshold changes.

Keywords:  Incremental Mining Data Mining; Frequent Itemsets; Association Rules;  1 Introduction  In recent years, data mining has attracted much attention in database research. This is due to its wide applicability in many areas, including the retail industry and the finance sector. The availability of automated tools has enabled the collection of large amount of data. These large databases contain information that is potentially useful for making market strategies and financial forecasts.

Data mining is the task to find out such useful information from large databases. The information includes association rules, characteristic rules, classification rules, generalized relations, discrimination rules, etc.

Of the various data mining problems, mining of association rules is an important one. Various algorithms have been proposed [ 1,2,4,7] to discover frequent itemsets.

An influential association rule mining algorithm Apriori[2] has been developed for rule mining in large transaction database. DHP[3] algorithm is an extension of Apriori using a hashing technique. These algorithms adopted the generation-and -test approach. They iteratively generate the set of frequent itemset of length (k+l) from the set of frequent itemset of length k, and then check their support counts in the database.

Maintenance of discovered association rules by incremental updating has been studied in [5]. The problem of updating the association rules can be reduced to finding the new set of large itemsets. After that, the new association rules can be computed using the new large itemsets. A simple solution to the update problem is to re-compute the large itemsets under various thresholds. This is clearly inefficient because all the computations done initially for finding the old large itemsets are wasted and all frequent itmesets have to be re-computed again from scratch.

Therefore, the primary aim of incremental mining is to avoid or minimize fining the old large itemsets by using the intermediate data constructed during the earlier mining.

Jiming Liu and Jian Yin presented an efficient algorithm, called Posteriori [6], for computing the frequent itemsets under the varied thresholds. However, this algorithm generates a huge number of useless candidate itemsets and will be a time-consuming process in the prune step.

In this paper, we propose an adaptive algorithm for the maintenance of association rules in light of threshold changes. The general idea is organized as follow:  1. Make use of large and candidate itemsets and their counts in the database under old threshold and finds which rules continue to prevail and which rules fail with new threshold.

2. Apply Apriorixen function to generate candidate itemsets, thereby reduce some candidate itemsetrs and consume less time in the prune step.

The remaining of the paper is organized as follows.

Section 2 describes algorithm Posteriori. Section 3 details algorithm I M A M  for incremental mining of association rules. Section 5 discusses our algorithm with Posteriori.

Section 5 gives the conclusion.

0-7803-7865-2/03/$17.00 02003 IEEE  mailto:mxpt@public.cc.jl.cn    2 Algorithm Posteriori  2.1 Problem Description  Let I={ i , . . . , i } be a set of items. Let TDB be a set of transactions, where each transaction T is a set of items such that T c  I. Associated with each transaction is a unique identifier, called its TID. We say that a transaction T contains X, a set of some items in I, if X E T. An association rule is an implication of the form X > Y, where X c I ,  Y cI, and X n Y= 0.  The rule X 3  Y holds in the transaction set TDB with confidence c if c% of transactions in TDB that contain X also contains Y. The rule X 3 Y has support s in the transaction set TDB ifs% of transactions in TDB contains X U Y.

The problem of frequent mining is to find the complete set of frequent pattems in a given transaction database with respect to a given support threshold min-sup.

2.2 Algorithm Posteriori  The following notations are used in the rest of the paper. L, is the set of all k-frequent itemsets in TDB  under the support of s%, and L , is the set of all k-frequent itemsets in TDB under the support of s %. C , is the set of k-candidate itemsets in the k-th iteration of Posteriori.

Moreover, X.support represents the support counts of an itemset X in TDB.

When min-sup is changed, two cases may happen: 1. s ' > s, a frequent itemset in L may not be a frequent  2. s ' < s, an itemset X not in L may become a frequent itemset in L .

In the first case, Posteriori-A can generate the complete frequent itemsets. Now, let us concentrate on the second case mentioned above. At the first iteration, we scan C - L , by checking the condition X.support > s ' X ITDBI.

As all new frequent itemsets are found, we call them 1 1,  thus L =L U 1 ,  . All the frequent itemsets are divided into two non-intersecting sets: L and 1 . All the subsets of a frequent itemset must also be frequent, so, any frequent 1-itemset corresponds to a single item of k-itemsets must be an element of L Accordmg to this, we can obtain  three mutually intersecting subsets and we call them L [1],  itemset inL .

or 1  L, [a] and L, [3], respectively. Moreover, we can filter  orignal L , from L , [l], and call remaining itemset 1 , , then we have L , [l]=L , U 1 , , and also we have L , =L k [I] U L k f21 k [3i? k k [21 = 0 L , [2] n L , [3]= 0 ,and L , [ l ]  n L , [3]= 0 .

Therefore, we can decompose the generation of candidate sets into three parts: L k [ 13, L k [2] and L k [3], respectively.

For C , [ 13 and C , [2], this is simple; we also apply C , [ l]=Apriori_gen(L C , [2]=Apriori_gen(L k-l [2]) The key problem is how to generate C , [3]. Algorithm  Posteriori-B modified Apriorisen function into Posteriorisen function. The function works as follow:  First, in the join step, we join L i  [ 11 and L k-i [2]:  insert into C , [3] select p.item ,p.item , ... , p.item , q.item ,  the Apriorisen function in algorithm Apriori :  [ 11) - L ,  q.item, , ..., q.item,-; from L j  [ l ]  p, Lk-i 121 q Next, in the prune step, we delete all itemsets C E  C [3] such that some (k-1)-subset of c is not in L :  for all itemsets c E C , [3] do for all (k- 1)-subsets s of c do  if (s P L ') then delete c from C , [3]  However, we find the Posteriorijen function will generate a huge number of useless candidate itemsets and will be a time-consuming in the prune step.

3 Algorithm IMARA  This paper mainly concentrates on generating C , [3].

The general idea is that we also apply Apriorisen function to generate C , [3]. Therefore, we can reduce the some of candidate itemsets and consume less time in the prune step.

For k=2, we know all the subsets of a candidate itemsets in C or 1 , .

Therefore, we need not prune these subsets, because they are frequent. We adopt NIUAsen2 function to generate  [3] must be an element of L      C , [3]. The function works as follow: c: [3]=0 c; [3]=0 foral l iEL,  do  foral l jEl l  do if(i<j) then  else C :  [3]=C: [ 3 ] U  {ij}  C [3]= C i [3] U (j,i) C [3]=C i [3] U C [3]  '  For k L  3, we also apply Apriori-gen function to generate C , [3]. As we all know, the smaller the number of candidate itemsets is, the faster these algorithms would be.

For k=3, we use NIUA-gen3 function to generate C [3].

The function works as follow:  T,=apriori_genl(L, [l],L: [3])  T =aprioriJenl(L ; [ ~ I , L  , [21) T 1 =apriori-genl(Li [3],L; [3])  T =apriori-genl(L [3],L [3])  It avoids generate such candidate itemsets from L [3]  and L; [3], L, [l] and L; [3], and so on. We only check whether the combinations of different elements are frequent, thereby saving the effort of unnecessarily pruning. So, in the prune step, it fits L , [3] into memory, not L , .

For k 2 4, we apply NrtJASen function. To find L k ,a  set of candidate k-itemsets is generated by joining Lk-1 with itself. Therefore, we reduce a huge number of useless  candidate itemsets. The function works as follow: 1. The join step: insert into C , [3] select p.item ,p.item , ,..., p.item ,-, , p.item k-1 ,  q.item k-1  from k-1 L1l p, k-1 13i where p.item =q.item ,p.item =q.item , ,...,  p.item k-2 =q.item ,-,, p.item k-1 < q.item k-1 ; insert into C k [3]  select p.item ,p.item ,... , p.item k-2 9 paitem k-i ,  q.item k-1  from k-1 121 P, k-1 13i where pitem =q.item , p.item =q.item , ,...,  p.itemk-2=q.itemk-2 ,p.itemk-l <q.itemk-I;  insert into C , [3] select p.item, ,p.item, , ... , p.item,-, , p.item,-, ,  q.item k-1  from L R-l [31 P, L k-l 131 where  p.item ,-, =q.item ,-, , p.item k-l < q.item k-1 ; p.item, =q.item, ,p.itemz =q.item , ,...,  2. The prune step: for all itemsets c E C k [3] do  for all (k-1)-subsets s of c do if (s E L ;-I ) then  delete c from C k [3];  The general idea of IMARA algorithm is shown in the above analysis. We give a formal presentation of IMARA algorithm as follow:  IMARA algorithm: An efficient algorithm for re-mining of association rules updon support changes.

Input: (1) C , :the set of all candidate k-itemsets in TDB under s, where k=l, ..., r  (2) s : where s < s (3) L , :the set of all frequent k-itemsets in TDB under  s, where k=l, ..., r Output: L '  :the set of all frequent itemsets in TDB  under s Begin: 1 ={new frequent l-itemsets};  L;=L, v i , ;  c, [l] = c, -L, ; for(k=2;L # 0 ; k t t )  do begin  C k [2] = apriori-gen(L k - 1 ) ;  ck[3]=  0 ; if(k==2) then c k [3] =NIUA-gen20, ,1 );      4 Discussions  Comparing with Posteriori, the efficiency of IMARA comes from the following aspects.

(1) We need not prune these subsets from 2- candidate itemsets, because we all know all the subsets of a candidate itemsets in C [3] must be an element of L Therefore, they are frequent. However, Posteriori consume a lot of time in the prune step.

(2) While generating 3-candidate itemsets, we only check whether the combinations of different elements are frequent, thereby saving the effort of unnecessarily pruning.

In contrast, Posteriori need prune all the subsets from 3-candidate itemsets.

(3) We apply Apriori-gen function to generate C k [3], not modify the Aprioriden function into PosterioriJen function. Therefore, we avoid generating the candidate itemsets from L [3] and L: [3], L [l] and L f [3], and so on, thereby saving the effort of unnecessarily pruning.

or 1  5 Conclusions  In this paper we propose an adaptive algorithm (IMARA) for incremental mining of association rules in light of threshold changes. Since algorithm IMARA represents a new, highly efficient incremental mining method, it can be applied to other applications like the mining of Web, Text and so on.

