An Efficient Algorithm for Finding Frequent Items in a Stream

Abstract?Most of the existing algorithms for mining frequent items over data streams do not emphasis the importance of the more recent data items. We present an efficient algorithm where a fading factor ? is used for computing frequency counts exceeding a user-specified threshold over data streams. Our algorithm ?-Miner can detect ?-approximate frequent items of a data stream using O(??1) memory space and the processing time for each data item is O(1). Experimental results on several artificial data sets and real data sets show that ?-Miner performs better than ?-LC in terms with precision, memory requirement and time cost.

Keywords-data stream; data mining; frequent items; fading factor

I.  INTRODUCTION In recent years, researchers have paid more attention to  mining data streams. Mining frequent item sets from stream data is an important problem with ample applications in stream data analysis. Examples include stock tickers, bandwidth statistics for billing purposes, network traffic measurements, web-server click streams, data feeds from sensor networks, transaction analysis in stocks and telecom call records, etc.

Problems related to frequency estimate have been studied by many researchers in different fields. Algorithms for identifying frequent items and other statistics in the entire data stream have been proposed. Lossy Counting [1] by G. S. Manku et al was the first algorithm for finding frequent items from a data stream. Lossy Counting is a one- pass algorithm that provides an accuracy guarantee on the set of frequent data items and their frequencies reported.

Karp, Shenker and Papadimitriou [2], and independently, Demaine, Lopez-Ortiz and Munro [3], applied a deterministic MG algorithm by Misra and Gries [4] to detect frequent stream data, they decreased the processing time of MG algorithm to O(1) by managing all counters in a hash  table. This algorithm needs only 1/? counters for the most frequent data items in the stream.

Many algorithms for frequent item counting use random sampling. Flajolet and Martin [5] and Whang et al. [6] proposed probabilistic algorithms to estimate the number of distinct items in a large collection of data in a single pass.

Golab, DeHaan, Lopez-Ortiz and Demaine [7] gave an algorithm for the problem when the item frequencies are multinomially-distributed. Gibbons and Matias [8] presented sampling algorithms to recognize top-k queries.

H. Liu [9] et al presents an error-adaptive and time-aware maintenance algorithm for frequency counts over data stream.

In many applications, recent data in the stream is more meaningful. For instance, in an intrusion detection system (IDS), the current status of the network is usually more important than that of one day ago. The algorithms mentioned above do not discount the effect of old data. This is undesirable in solving many application problems. One way to handle such problem is using sliding window models which ignore the out of date data and only consider the recent data. Recently several data mining algorithms over sliding windows are proposed. Based on MG algorithm, Arasu and Manku [10] gave a deterministic algorithm for finding ?-approximate frequent items; it supports  ( )11 log ?? ??O  query and update time and uses ( )121 log ?? ??O  space. Golab, DeHaan, Demaine, Lopez-Ortiz and Munro [11] gave some heuristics for the problem to identify frequent items over sliding window. Lee and Ting [12] proposed an algorithm which achieves O(?-1) space. Their algorithm needs O(?-1) processing time for update and query. L. Zhang and Y Guan [13] have proposed a stream data frequency estimation algorithm over sliding windows. Their algorithm requires O(?-1) memory space and O(1) time to process each item data. Other recent works on data stream algorithms have been surveyed in [14, 15].

2009 Second International Symposium on Electronic Commerce and Security  DOI 10.1109/ISECS.2009.188     In this paper, we introduce a fading factor 0<?<1 to act on the density of data items in data stream. One advantage of this measure is that it takes into account the old data items in the history. Furthermore, the frequency density changes smoothly when more data arrive continuously without a sudden jump. We propose an efficient frequency estimation algorithm ? -Miner which improves the space and time costs. It can detect?-approximate frequent items in data stream with O(?-1) memory space and processes each data item in O(1) time. We verify the effectiveness of our approach empirically using real data sets and synthetic data sets. Compared with ?-LC, our method performs better in terms with precision, memory requirement and time cost.



II. DENSITY OF DATA ITEM In many data stream applications, recent data are more  important than the older ones. To capture this, we use a fading factor )1,0(??  in calculating the data items? support counts. Let the input data stream be X=(x1, x2, x3,?). For each data item x, its support count decreases as x ages. We call such modified support counts the density of the data item. In each time step, the density of the data item will be reduced by a fading factor ? .

Suppose t>ts, here t is the current time and ts is the latest time data item x being received. It is obvious that  sttstxDtxD ?= ?),(),( . (1)  Although the density of a data item should be constantly modified by a fading factor ? , it is unnecessary to update the density values of all data records at every time step.

Instead, it is possible to update the density of a data item only when an identical new data item is received from the data stream. The time when the last data item x was received should be recorded so that density of x can be updated when a new data x arrives. Assuming a new data item x is received at time tn, and the time when the last same data item x arrived is tl (tn > tl), the density of x can be updated as follows:  1),(),( += ? l tt  n txDtxD ln? . (2)  Lemma 1   Let X(t) be the set of all the data items that arrive from time 1 to t, we have:  a) ??  ? ?  ? ? ?=?  ? 1  1),(  )(  t  tXx txD , for any t=1,2,?..

b) ??  =? ??? 1  1),(lim )(tXxt  txD  Proof: For a given time t, ? ? )(  ),( tXx  txD  is the summation  of the densities of the t data records arriving at time steps 1, ? , t respectively. Denote the data record arriving at time t?, 1?t?? t, as a(t?),  and its density at time t as D(a(t?), t).

Therefore, the summation over all the data items is  ?=? =?  t  ttXx ttaDtxD  1')( )),'((),( . Since ?  =  t  t ttaD  1' )),'(( = 't t? ? , we  have ??  ?? ?  ? ? ?=?=?  =  ?  ? 1  1),(  1'  '  )(  tt  t  tt  tXx txD , Also, it is clear  that: ??  ? ?  = ? ?=?  ????? 1  1lim),(lim  )(  t  ttXxt txD .

Q.E.D.

Let )1,0(?S be a user specified threshold, at time t, a  data item x is a frequent one if its density ??  ?  ),( StxD .

Let )1,0(??  be a user specified error bound and ?<S. We are asked to discover the data items with density at least  ? ?  ? ?  S . The estimated densities output by our algorithm are  less than the complete densities by at most ?  ? ?1  .



III. THE ALGORITHM ? -MINER  A. The Framework of the Algorithm ? -Miner In our algorithm ?-Miner, for each data item, a  characteristic vector is defined as follows.

Definition 1 (Characteristic Vector in ?-Miner): The  characteristic vector of a data item x is C(x)= [x, D(x, ts), F(x, ts), ts], where ts is the last time when data item x is received, D(x, ts) is the relative density of data item x at time ts, F(x, ts)is the density correction value of x at time ts.

The algorithm processes the coming data from stream and updates a list L where each entry is the characteristic vector of a data item. The maximum size of L is len=?-1.

The algorithm does not multiply the densities by the fading factor ? for all the entries in L at each time step, instead the algorithm updates the density of a data item using ts only when an identical new data item is received from the data stream.

When the algorithm receives a new data item x from the stream, the algorithm modifies the list L by creating or updating a characteristic vector in L. Whenever the size of L goes beyond len, the entry with the smallest density should be deleted. We can store this density value instead of subtracting it from all the entries in L. To keep this deleted density value, the algorithm uses a record defined as follows.

Definition 2 (Record of the Deleted Density): ? -Miner saves the record of the deleted density ],[ ?? t , where ? is the deleted density in the algorithm at time ?t .

At beginning, the list L is empty or not full. There is no need to delete an entry. The initial value of record ],[ ?? t is set as [0,0]. Therefore, When a new entry is inserted into the list L, the new entry to be inserted into L is [x,1,0,t]. Later, whenever the size of L goes beyond len, the entry  ]'),','(),','(,'[ sss ttxFtxDx with the smallest density should     be deleted. ?  keeps this deleted density value and  ?t  is set equal to the current time t.

')','( sttstxD ?=? ? . (3)  The framework of the algorithm  ?-Miner is as follows:   Figure 1.  The framework of ?-Miner .

To avoid the operations of subtracting an identical density value from all of the entries, we just operate on the new created entry. Once the operation of deletion is happened, the value of  ?  would be changed and updated.

Whenever the new data item x is received, the new created entry [x,1,0,t] should be replaced by ],,1,[ tx tttt ?? ?? ?+? ?? where 1),( +?= ??ttstxD ? and ?  ??= ttstxF ?),( . This causes the relative density in each entry in L does not reflect its actual value. Suppose the entry of x at time t is  ]),,(),,(,[ sss ttxFtxDx . Then, its complete density ),( txDa  at time t should be  ss tts tt  sa txFtxDtxD ?? ?= ?? ),(),(),( . (4)  To answer a query request at any time t, only the frequent data entries ]),,(),,(,[ sss ttxFtxDx satisfying ),( txDa >  ? ?  ? ?  S   can be output.

The framework of the algorithm Query(t) is as shown in Fig. 2.

Figure 2.  The framework of the algorithm Query(t).

From the algorithm Query(t), it is obvious that all items  whose complete density exceeds ??1  S  must be output and  no item whose complete density is less than ? ?  ? ?  S  is  output.  The estimated densities of the output entries are less  than the complete densities by at most ?  ? ?1  .

B. Complexity of the Algorithm ? -Miner Since the maximum size of queue L is L=?-1, ? -Miner  also requires no more than O(?-1) space. ? -Miner does not update all the entries at each time, but uses ?  to save the value which should be added to the new created entries later. Also the algorithm does not multiply the densities by fading factor ? for all the entries in L at each time step, instead it updates the density of a data item using ts only when a data item is received from the data stream. In each step, the algorithm processes one data item by the operation of inserting a new entry, or modifying an old entry in L, or deleting an entry from L. Therefore, time complexity for the algorithm to process one data item is O(1). Since the maximum size of list L is L=?-1, it is obvious that computation time for answering each query is O(?-1).



IV. EXPERIMENTAL RESULTS In order to compare with the similar method, we revise  LC algorithm by adding ? on it. We evaluate our algorithm and compare its performance in handling the data streams against ?-LC in terms with space cost, computation time, recall and precision.

All experiments were carried out on 1.7GHz processor and 512 MB memory running Windows XP. In our experiments, we always fix ?=0.1S, ?=0.99.

A. The Synthetic Data Sets We randomly generate four data sets with Zipf  distribution. The size of each data set is 1000K. The parameters are respectively 0.4, 0.6, 0.8, 1.. We set ?=0.001.

Algorithm ? -Miner: Input:  X: the data stream; ?: density error bound; Output: L: a list of potentially frequent data items; Begin 1. t=0, len= ?/1 , 0=? , 0=?t ; 2. While not terminate do 3.     Receive a data item x from the stream; 4.     If there is the entry corresponding to x in L 5.        replace ]),,(),,(,[ sss ttxFtxDx  by ],),(,1),(,[ ttxFtxDx ss tts tt  s ?? + ?? ;  6.     Else 7.        If  |L|<len      //L is not full 8.           Insert new entry ],,1,[ tx tttt ?? ?? ?+? ?? in L; 9.        Else      // L is full 10. Delete the entry ]'),','(),','(,'[ sss ttxFtxDx  where ')','( sttstxD ?? is the smallest;  11. ')','( sttstxD ?=? ? ,  tt =? ;  12.       End if 13.    End if 14.    t=t+1; 15.End while End  Algorithm Query(t) Input: The list L=[ C(1) C(2),?, C(len)]; Output: F: The set of ?-frequent data items Begin 1. F=?; 2. For each entry [ x, D(x, ts), F(x, ts), ts] in D do  3.     if ? ?  ? ?>  ),( StxDa  then F=F ? {x}  4. End for 5. output F End           0.4 0.6 0.8 1  Zipf parameter  Me mo r y(  nu m be r o  f e nt r ie  s) ?-Miner ?-LC   Figure 3. Comparison of the memory requirement of the two algorithms  on different distribution data(1000K, ?=0.001)         0.4 0.6 0.8 1  Zipf parameter  Ti me  co s t(  ms / k)  ?-Miner ?-LC   Figure 4. Comparison of time cost of the two algorithms on different  distribution data(1000K, ?=0.001)   0.2  0.4  0.6  0.8   1 2 4 6 8  Length of stream(K)  Re c al l  ?-Miner ?-LC   Figure 5. Comparison of recall of the two algorithms on different  distribution data(1000K, ?=0.001)  0.72  0.76  0.8  0.84  0.88  0.92  0.96  1 2 4 6 8  Length of stream(K)  Pr ec i si  on  ?-Miner ?-LC   Figure 6. Comparison of precision of the two algorithms on zipf 1 (  ?=0.001)  The results of the two algorithms on different distribution data sets are shown from Fig. 3 to Fig. 6.  Since the space cost of ?-Miner is O(?-1), the memory requirement of ?-Miner is a constant when ? is fixed. But in ?-LC, its memory requirement is ( ))log(1 nO ?? ? , where n is the length of the data received from the stream. From Fig. 3, we can see our algorithm requires less memory than ?-LC.

The time costs of the two algorithms with ?=0.001 are shown in Fig. 4. It is obvious that ?-Miner is much faster than ?-LC. Fig. 5 shows the recalls of the two algorithms which are both 100%. The result empirically proves that the output of the algorithms have no false negatives. From Fig.

6, we can see that ?-Miner has higher precision than that of ?-LC at each step.

B. The Real Data Set In this section, we adopt the log file data of visiting the  1998 Word Cup official web as the real data set[16]. This log file records all of the visit requests for the word cup official web in the period of competition of 1998 Word Cup.

Each request consists of 8 attributions including visit time, IP address, the ID of the visit web and so on. We pick up all of the IDs of the visit webs as the experimental data set and find out the most frequent visit webs. We fix ?=0.0005.

1 3 5 7 9  Length of stream(100K) Me m or y (n um  be r o f e n tr i es )  ?-Miner ?-LC   Figure 7. Comparison of the memory requirement of the two algorithms  on World CUP-98 data(?=0.0005)   0.5   1.5   2.5  3.5   4.5  1 3 5 7 9  Length of stream(100K)  T im e c os  t( m s/ K )  ?-Miner ?-LC   Figure 8. Comparison of time cost of the two algorithms on World CUP-  98 data(?=0.0005)   0.2  0.4  0.6  0.8   1 2 4 6 8  Length of stream(K)  Re c al l  ?-Miner ?-LC   Figure 9. Comparison of recall of the two algorithms on World CUP-98  data(?=0.0005)  From Fig. 7 to Fig. 8, we can see that our algorithm requires less memory than ?-LC. It is obvious that ?-Miner is always faster than ?-LC.

0.80  0.84  0.88  0.92  0.96  1.00  1 2 4 6 8  Length of stream(K)  Pr ec i si o n  ?-Miner ?-LC   Figure 10. Comparison of precision of the two algorithms on World CUP-  98 data ( ?=0.0005)  Furthermore, we also show the results of the recall and the precision of the two algorithms on different sizes of World Cup 98 data sets in Fig. 9 and Fig. 10. The recalls of the two algorithms are both 100%. From the figures above, we can see that ?-Miner can efficiently discover the frequent items with high quality.



V. CONCLUSIONS In this paper, we present an efficient algorithm ?-Miner  for mining frequent items which introduces a fading factor 0<?<1 to emphasis the importance of the more recent data items. ?-Miner can detect ?-approximate frequent items of a data stream using O(?-1) memory space and the processing time for each data item is O(1). In order to compare with the traditional stream mining algorithms, such as Lossy Counting (LC) under the same conditions, we modify LC by adding the fading factor ? on it. Experimental results show our method has high precision, requires less memory and consumes less computation time than ?-LC.

