A LEARNING RULE FOR FUZZY ASSOCIATIVE MEMORIES *

Abstract  In this paper a learning rule for multiple pattern pairs in fuzzy associative memories (FAMs) with Max-Min composition units is presented, Under a certain condition, the proposed rule can efficiently encode multiple fuzzy pattern pairs in a single FAM and perfect association of these pairs can be achieved. The correctness of the proposed rule is proved and illustrative examples are given.

1 Introduction  I t  is well known that neural networks can efficiently model subjective phenomena such as perception, image and memory while fuzzy logic is another powerful tool to model phenomena associated with perception and thinking. In recent years, several models that combine neural networks and fuzzy logic, called fuzzy neural networks, have been developed and widely applied to prospective fields such as control, pattern recognition, decision making and expert systems[l - 5 1 .  The FAM introduced by Kosko, which involves Max-Min composition units, is one of the most useful and important paradigms[l, 2 1 .  Such a FAM is characterized by the subset recall when trained using the fuzzy Hebian rule. The inability to reliably store more than one pair is its enssential drawback.

Some endeavors have given brith to overcoming this drawback. Hirota and Pedrycz used a gradient descent algorithm with two-value derivatives under some constraints [SI. Authors of this paper ever suggested an iterative learning algorithm that employed a s o -  called h i l l  -climbing search strategy to conduct connection weight adjustments[?]. However, both algorithms may easily get struck at local minima of a defined index function although satisfactory performence of the trained FAM can usually be achieved, We propose here another learning rule that employs a suitable fuzzy logical-implication operation different from the fuzzy Hebbian rule used by Kosko. The proposed rule is capable of encoding multiple pattern pairs in the weight matrix of a FAM if there exists some weight matrix that properly forms the fuzzy relation between the input and output of the given pairs.

2 FAMs with Max-Min Composition Units  A FAM is designed to store p fuzzy ( unit-interval valued 1 pattern pairs (AI, B1 ) ,  . . .  , (Ap, Bp) in the weight matrix W, where the kth pair is represented by the vectors Ak =(a!,  . . . ,  ai 1 and Bk = ( b!,,,,, bi 1 respectively.

shou Id hold  The topology of a FAM i s  shown in Fig. 1.

In the FAM with Max-Min composition units, perfect association implies that eq. ( 1)  *This work is partly supported by both the National Natural Science Foundation of the 863-High Technology Programme of China  China and     where the symbol ' 0 '  denotes the Max-Min composition operation. Generally, there are two kinds of methods to find the weight matrix W. One is to define a performence index, for instance, the sum o f  squared errors  where bf and bj are the desired output and actual output of the jth adjustment is aimed at minimizing the index function[6]. The other is pair ( Ak, Bk ) into the matrix wk and then to superimpose the matri matrix W, i.e., W may be found using the following equation  output unit. We to encode the s es W k  s in a s  gh t ngle ngle  where the symbol "+' denotes some implication operation, and ' n ' denotes a generalized superimposition ( e.g., maxima, minima etc 1 ,  Kosko adopted a fuzzy Hebbian ( the m i n h " implication Rc rule pointwise to encode the kth pair ( Ak, Bk ) in the wk! then superimposed the matrices Wks in a single matrix W by the maximum operation. That is  P k= 1  wij = v WIj  where the symbols " V '  and " A '  stand for the maximum and minimum operations respectively, and w ijs are components of the W. Obviously, multiple pairs can not be reliably stored in the FAM. An approach to overcoming the problem is that a bank of FAMs is used, each of which seperately store only one pair. But this results in consuming space of storage.

3 The Proposed Learning Rule  As usual, we wish to distributively and reliably store multiple fuzzy pattern pairs (Ak, Bk) in a single FAM such that each Bk can be perfectly recalled when each corresponding A k is presented. Instead of using the minimum implication Rc, we associate here Bk with Ak usng the implication Rg, and we replace the maximum superimposition by the minimum one 8s well. In this way, we obtain the following learning rule     In pointwise notation, that is  l b f  if a! ) bf  (1 otherwise wij A P (af - bf)  k= 1  (6)  (7)  Obviously, the complexity of the proposed learning rule is equivalent to the rule used by Kosko.

As we know, there doesn't always exist the weight matrix of a FAM, which makes eq. ( 1) hold for arbitrary set of input-output pairs. In other words, these pairs have to satisfy a certain condition for the purpose of perfect association. On the other hand, even if there exists some desired weight matrix, the Kosko's rule is not guaranteed to yield the desired matrix, as shown in [ I ,  2 1 .  Based on analyzing the condition for existence of desired weight matrix, we show below that the proposed learning rule can yield the desired matrix if i t  indeed exists.

Since the m units in the output layer of the FAM are independent one another, we can consider only one output unit. For the jth output unit, we have  We define the following sets and value  A bf if G i j  # cp (kt Gij  Then the necessary and sufficient condition for existence of the desired weight vector that enables eq. (8) to hold can be expressed a8 follows[81  t.J Sij K iC I   (1 5 )    If there exists some weight vector enabling eq.

for all iE I .  By definition, af? ( bf? or wij< mi  = v  ( af? A wij ) < bf?. This is obvious ilkdSij  U Sij K. For any k t  K, a! < bf, or wij< mij < b f  for iE I mij 2 bf or ajf > bf and mij = bf for all i as k E  Si  v ( a j f ~ w i ~ )  = I V  ( ajf A wij 1 1  v { \ iE I ilkE Sij i I  (8) to hold such that some k ? ?  K but k? & Sij ( bf?. Hence we have bk? = V (aik? A wij)  iE I y a contradiction. Vice versa, assume that  all i as k E  Sij, but a! = b jk and  . Hence, if let wij = m i j ,  we have  = v  (ajf A mij)=bf. This means that there exists a weight vector enabling eq. (8) to hold.

So, there exists a weight matrix W enabling eq. (1) to ho, if and only if eq. ( 15) holds for any j. The proposed learning rule expressed by eq. (6) and ( 7 )  is in accordance with eq. (13). From above analysis, we can conclude that the proposed rule can yield the desired weight matrix enabling eq. (1) to hold i f  i t  exists.

ilkE Sij  4 Examples  Two sets of fuzzy pattern pairs (Datal and Data2 ) to be stored are shown in Tab. 1 and 2.

For Datal, we can obtain the weight matrix of the FAM using the proposed rule 0. 6 0. 5 1. 0 0. 9 1. 0 0. 5  (16) w=[ 0. 4 0. 5 0. 4 0. 4 0. 6 0. 4 1  The FAM with the above matrix can perfectly each output pattern Bk when presented with each input pattern Ak. But the K o s k o ? s  rule produces the following weight matrix  0. 9 0. 7 0. 6 0. 9 0. 7 0. 7  w=[ 0. 6 0. 6 0. 6 0. 6 0. 6 0. 6 1 With the matrix, the trained FAM can not perfectly all pairs in Datal.

For DataZ, two rules yield matrices  2 0. 8 1. 0 I .  0 1. 0 0.  4  6 0. 5 1. 0 0. 4 0. 4 0. 5 1 and  0. 4 0. 8 0. 5 0. 7 0. 6 0. 6  w=[ 0. 6 0. 5 0. 5 0. 4 0. 5 0. 5 1  (17)  (18)  (19)  I t  is easy to confirm that eq. (1) holds with eq. (181, but i t  dosen?t with the weight matrix eq. (19).

5 Conclusion  A learning rule for multiple fuzzy pattern pairs in FAMs with Max-Min composition units is presented. Under a certain condition, multiple pairs can be reliably stored in a single FAM and perfect associations can be achieved. The correctness of the proposed rule is demonstrated. Due to the simple complexity , the proposed rule may be much of use in the design of fuzzy systems where multiple inference rules need to be reliably stored.

