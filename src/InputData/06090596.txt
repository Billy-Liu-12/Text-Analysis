Is the Reduction of Dimensionality to a Small Number of Features Always Necessary in Constructing Predictive Models for Analysis of

Abstract? Gene expression and genome wide association data have provided researchers the opportunity to study many complex traits and diseases. When designing prognostic and predictive models capable of phenotypic classification in this area, significant reduction of dimensionality through stringent filtering and/or feature selection is often deemed imperative.

Here, this work challenges this presumption through both theoretical and empirical analysis. This work demonstrates that by a proper compromise between structure of the selected model and the number of features, one is able to achieve better performance even in large dimensionality. The inclusion of many genes/variants in the classification rules can help shed new light on the analysis of complex traitstraits that are typically determined by many causal variants with small effect size.



I. INTRODUCTION  DNA microarray technology, which made possible mea- suring the expression of thousands of genes simultaneously, has found many applications in biomedical research and made studying the variation of population through genetic markers possible. They have been widely used in medicine to help researchers in better understanding the etiology of diseases by discovering new biomarkers that correlate well with progression of a disease or by finding new drugs through studying the differences in gene expressions in cells exposed to different doses. In recent years, with the advent of genome wide association data many causal variants with strong evidence of association to complex disease or behaviors have been identified; yet, they perform poorly in a predictive setting [1], [2]. This is due to the fact that many complex traits, have a phenotypic response determined by interactions between numerous environmental and genetic factors and therefore, each individual disease locus has a small effect size [3]. Few studies have tried to capture the polygenic nature of complex traits such as overt stroke in sickle cell anemia [2], Coronary Artery calcification in atherosclerosis [4], and nicotine dependence [1] through  Amin Zollanvari and Marco F Ramoni are with Children?s Hospital Informatics Program at the Harvard-MIT Division of Health Sciences and Technology, Harvard Medical School, and Partners Healthcare Cen- ter for Personalized Genetic Medicine, Boston, MA. Gil Alterovitz is with Children?s Hospital Informatics Program at the Harvard-MIT Di- vision of Health Sciences and Technology, Harvard Medical School, Partners Healthcare Center for Personalized Genetic Medicine, and De- partment of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA. Nancy L Saccone is with Department of Genetics, Washington University School of Medicine, St. Louis, MO. Laura J Bierut is with Department of Psychiatry, Washington University School of Medicine St. Louis, MO. Correspon- dence to: Amin.Zollanvari@childrens.harvard.edu, Ga@alum.MIT.edu  constructing prognostic models capable of dissecting the complex web of interactions between causal variants; yet, they have all considered limited number of single nucleotide polymorphisms (SNPs) in the proposed prognostic model.

The limited number of features considered in the model have not been limited to the aforementioned genome wide association studies (GWAS) and SNP based models; many gene expression based studies have been following similar design machinery [5]?[8].

The commonly employed procedure to design the classi- fiers in these studies have been based on the presumption of reducing the number of dimensions from thousands of features to a small number of features, usually based on filtering and some statistical tests such as t-test, ANOVA or their variants. Then, the designer tries to find a best set of features in this lower dimension based on some search methods such as exhaustive search, best first search, ranker, and other methods to construct the classifier in a space of much lower dimensionality, such as two or three commonly in gene expressions based studies [5], [7], [8], and commonly less than one hundred SNPs in GWAS [1], [2], [4]. Eventually, assessing the performance of the classifier is performed by different error estimation techniques.

The rationale behind reducing the dimensionality involves small-sample situations and the well-known phenomenon, curse of dimensionality [14], [15]. Devroye, one of the pioneers in statistical pattern recognition, mentions that: ?Just recall the curse of dimensionality that we often faced: to get good error rates, the number of training samples should be exponentially large in the number of components. Also, computational and storage limitations may prohibit us from working with many components?. In today?s world, thanks to advanced technology, the latter problem, namely storage limitations, has been alleviated to some extend. However, the salient concluding point we try to make in this paper is that in order to achieve a classifier of reasonable performance, one does not need to presumably reduce the number of features for the classifier or the predictive model; a compromise between complexity of the model (for example measured by VC dimension [14]) and the number of features can result in substantially better performance. This needs to be investi- gated more in future as a continuum of the very few classical works on determining the optimal number of features in very specific scenarios [16]. Unfortunately due to the lack of proper measures of such a compromise, many researchers regardless of the scenarios that call for the exponentially   Boston, Massachusetts USA, August 30 - September 3, 2011  U.S. Government work not protected by U.S. copyright    higher number of samples than features, try to reduce the number of dimensions to a small number in a hope to find well-behaved predictors. This procedure not only eliminates many important features from the scope of further analysis, but also results in predictors with limited accuracy rate. For many biostatisticians and bioinformaticians, the presumption of dimensionality reduction can be rooted back to the dawn of microarray technology in which the technology was still merging; in addition, the number of volunteer patients for genetic profiling was limited. The data in many studies from that time, and even yet, are characterized by many features but very limited number of samples [5], [9], [10], and are commonly known as ?tall data? matrices [11]. In [11], the authors have pointed out several common ?fads? and ?fallacies? regarding the classification problem using microarray data; however, many hints and tips mentioned there are valid for analysis of tall data matrices. In recent years, with the advanced recent technology, which facilitates genetic profiling of the patients, and by accumulating the data over time, we are witnessing the emergence of many studies in which the number of features and the samples can be considered as being ?comparable? (at least after initial filtration or quality controls); yet, many classification rules pertinent to tall data matrices are still being employed intact. To cite just a few works with comparable numbers of samples and features, consider [19] where 3,713 SNPs were genotyped for 1,929 samples; [2] in which 235 SNPs were genotyped for 1,398 samples, and [4] with 2,882 SNPs and 712 samples. In this work, we focus on the scenario in which sample size and number of features are comparable, and using different synthetic and practical examples we demonstrate the efficacy of incorporating a large number of features in relatively simple classifiers, or in general, predictive or prognostic models.



II. SIGNIFICANT DIMENSIONALITY REDUCTION CAN BE AVOIDED WHERE SAMPLE SIZE IS COMPARABLE TO  NUMBER OF FEATURES  A. An Analytic Example  The scenario in which the number of features is compa- rable to the sample size can be analytically studied under a specific asymptotic assumption, namely double asymp- totics, which is commonly credited to S. Raudys and A.

Kolmogorov [17], [18] and was extended by us in [20], [21]. Intuitively, the behavior of a statistic is studied as both sample size and dimensionality (or generally the number of parameters) increase to infinity in a controlled fashion, where the ratio between sample size and dimensionality converges to a finite constant [18]. Denoting the Mahalanobis distance between classes by ?2p = (?0 ? ?1)T??1(?0 ? ?1), the number of samples in each class ni, i = 0, 1, and the number of features, p, the double asymptotic conditions can be represented as n0 ? ?, n1 ? ?, p ? ?, pn0 ? J0 < ?, pn1 ? J1 < ?, ?  p ? c in which c and Ji, i = 0, 1  are all constants. These conditions can be used to analyze different statistics of interests in situations where the number of samples is comparable to the number of features [20].

In order to make a simple example, we assume a binary classification setting where ni, i = 0, 1, are comparable to the number of features, p, and both are large e.g. ni = 2, 000 and p = 1, 000. Furthermore, we assume that the sampling distributions of both classes are multivariate normal distributions with common and known covariance matrix ? and ?2p = 4. We further assume that Fisher linear classifier, commonly known as Linear Discriminant Analysis (LDA) [22], is chosen to execute the classification task. LDA is given by:  ?(x) =  { 1 , if W (x) < 0 0 , if W (x) ? 0  , (1)  in which  W (X) = ( x? ??0 + ??1   )T ??1 (??0 ? ??1) . (2)  where ?i, i= 0, 1, are the sample estimates of class con- ditional densities. Under double asymptotic conditions, it can be shown that LDA constructed using all the features has on average (over sample space) a true error equal to ?p = ?0?  ( ? 12  ?2+J1?J0? ?2+J0+J1  ) + ?1?  ( ? 12  ?2+J0?J1? ?2+J0+J1  ) , in  which ?i?s are just prior probability of classes [17], [20], [21] . Under aforementioned double asymptotic conditions and certain other regularity conditions, the assumption of Gaussianity can be alleviated; however, this is not the focus of this paper and interested readers are referred to [18] for more information. Assuming equal prior probabilities and substituting Ji = pni =  1,000 2,000 =  2 , we observe that Fisher  linear discriminant using all the features has on average an error of 0.185, which is reasonably close to the Bayes error, denoted by ??. The Bayes error is the error of the optimal classifier that is often unknown in practice. Here ?? can be computed since we have the parameters of the distributions, and furthermore, we know that the optimal linear classifier for a multivariate normal distribution of classes has ?? = ? ( ? 12?  ) = 0.158. Closeness of the average error of linear  classifiers to the optimal linear classifier shows that, in this example, reduction of dimensionality is not a critical stage of classification rule. This point becomes even more clear by noticing the fact that reduction of dimensionality to a lower space, say p? < p, can even diminish the performance of the classifier. This is because ?2p is changed to ?  p? and  assuming that the common covariance matrix of classes is identity, then, it can be shown that ?2p? < ?  2 and ?p? > ?p.

We have depicted the average true error versus dimension for this example in Figure 1-a, where in order to have ?21,000 = 4, the means of the normal distributions need to be ?0 = ??1 = 0.031p. Clearly, the true error is a decreasing function of dimensionality confirming the fact that the best performance is achieved by considering all the features in the classifier.

B. A Simulation Example  Here, in contrast to the previous example, we consider a more realistic situation in which the covariance matrix of classes that appears in the discriminant W (X) given by (2) is estimated from the data using a regularized estimation of     sample covariance matrix. In order to simulate the situation under study in this paper (comparable sample size and dimension), we have generated 1,000 samples for each class taken from two multivariate Gaussian distributions of 900 dimensions with a common covariance matrix having 1 as diagonal and 0.2 as off diagonal elements and the means adjusted such that ?2900 = 4. Since the performance of regularized-LDA depends on regularization parameter, L, three values of L were selected to design three different LDAs in this scenario (L = 0.1, 1, 10). In order to estimate the error of these three designed LDAs, we have generated 10,000 additional samples from the aforementioned mul- tivariate distributions as a test set and found the rate of misclassification of these test samples by each LDA. On the other hand, in order to compare the performance of the three LDAs designed using the full 900 dimensions to classifiers designed on a much lower dimensionality, we have chosen 10,000 different randomly selected combinations of two-feature sets out of 900 features and designed LDA classifiers using each set (hence 10,000 LDAs designed in two dimensions), and estimated the error of each LDA on the test samples. The histogram of the error of these 10,000 LDAs is plotted in Figure 1-b. The errors of the three regularized-LDAs constructed by considering all the features and different choice of L, are shown as vertical dashed and dotted lines in Figure 1-b. As we can see the histogram of the error rates of 10,000 classifiers designed on two dimensionality spaces is on the right of the error rate of all three regularized-LDAs designed on 900 dimensions.

This clearly shows the advantage of using all the features in this scenario.

C. A Practical Situation  Nicotine dependence has a strong genetic component.

Twin studies have demonstrated the heritability of a large proportion of phenotypic variance ranging from 40-75% [22]. In order to identify novel causal genetic factors for nicotine dependence, several GWAS carried out using the case-control experimental design [19], [26]. To address the issues related to standard statistical methods such as logistic regression in high dimensionality space [24], and building a predictive model able to simultaneously capture interactions between causal loci, Ramoni et al. [1] considered 73 SNPs previously reported by Bierut et al. and Saccone et al. in [19], [26] as SNPs associated with nicotine dependence.

Ramoni et al. utilized a multivariate probabilistic model, namely Bayesian network, to predict the nicotine dependence with up to 75% accuracy, measured by the area under the receiver operator characteristic curve (AUROC) on the fitted data. The data we use here are a subset of data used in the Collaborative Genetic Study of Nicotine Dependence (COGEND) (for more information about the data the reader is referred to [25]). The data set we have considered is a cohort of 2,062 European Americans. We randomly split the data into 1,857 training and 205 data. After controlling for SNPs with high genotype call, and removing those with minimum allele frequency, MAF < 0.01, the number of  1,642 initial SNPs was reduced to 1,501 SNPs. After training the predictive Bayesian model, namely naive Bayesian net- work, we utilized the model to predict the risk of individuals? nicotine dependence in the validation data set containing 205 samples. To train each classifier, we first ranked the SNPs using Cochran-Armitage trend test of association [27]. Then, naive Bayesian network was employed to construct the model on the SNPs selected from the top of the list. The selected number of SNPs was increased until all 1,501 SNPs in the list were considered in the network. Figure 2 shows that the best possible performance, measured by AUROC, corresponds to the case where all SNPs are considered in the model with AUROC=0.772% on an independent data set. The AUROC of our model on the training data set (as considered in [1]) is 0.861%. Therefore, considering all 1,501 SNPs and a simple classifier, namely naive Bayesian network, we easily outperformed the classifier constructed in [1].



III. CONCLUSION A proper compromise between the complexity of the clas-  sifier and the number of features selected to be involved in the model is a critical step in achieving the best possible per- formance. Often researchers select a structure and regardless of how simple the structure is, employ stringent filtration or significant dimensionality reduction through feature selection methods. Here, three relatively simple classifiers have been designed to demonstrate the efficacy of considering large number of features in situations where the number of samples is comparable. This work shows the necessity of continuing very few classical works on determining the optimal number of features in very specific scenarios. Double asymptotics that was presented in this paper can be a promising analytical tool to accomplish this goal.

Fig. 2. True error of naive Bayesian network versus number of SNPs evaluated on 205 test samples.



IV. ACKNOWLEDGMENTS Lead investigators directing data collection are Laura  Bierut, Naomi Breslau, Dorothy Hatsukami, and Eric John- son. The authors thank Heidi Kromrei and Tracey Richmond for their assistance in data collection. This work was sup- ported by the NIH grants P01CA89392 (L. Bierut) from the National Cancer Institute, K02DA021237 (L. Bierut) from the National Institute on Drug Abuse, 5R21DA025168- 02 (G. Alterovitz), 1R01HG004836-01(G. Alterovitz), and 4R00LM00982603 (G. Alterovitz).

Fig. 1. a) True error vs. dimension for the analytic example: p and n are both large and comparable. Clearly, considering all the features is substantially better than reducing the dimensionality for the choice of classifier considered here. b) Comparison between error of two-feature LDA classifiers vs.

regularized-LDAs constructed by 900 features with different choice of L. The histogram on the right shows the error of 10,000 LDAs designed on randomly selected sets of two features from the original 900 features. The solid vertical line is the Bayes error. Other vertical lines are the error of regularized-LDAs for different choice of L and constructed by considering all the features: solid-dashed: L = 0.1, dotted: L = 1, and dashed line: L = 10. Figure shows the error rates of all two-feature designed LDAs are substantially larger than the error rates of regularized-LDAs constructed by 900 features.

