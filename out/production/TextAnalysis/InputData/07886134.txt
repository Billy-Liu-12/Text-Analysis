SVM-based Web Content Mining   with Leaf Classification Unit from DOM-tree

Abstract?In order to analyze a news article dataset, we first extract important information such as title, date, and paragraph of the body. At the same time, we remove unnecessary information such as image, caption, footer, advertisement, navigation and recommended- news. The problem is that the formats of news articles are changing according to time and also they vary according to news source and even section of it. So, it is important for a model to generalize when predicting unseen formats of news articles. We confirmed that a machine learning based model is better to predict new data than a rule-based model by some experiments. Also, we suggest that noise information in the body possibly can be removed because we define a classification unit as a leaf node itself. On the other hand, general machine learning based models cannot remove noise information.

Since they consider the classification unit as an intermediate node which consists of the set of leaf nodes, they cannot classify a leaf node itself.

Keywords-component; web content mining; machine mearning; feature extraction; support vector machines

I. INTRODUCTION Since World Wide Web has emerged in 1991, the  amount of web pages exponentially has grown. The size of web pages is as enormous as it is over a billion1.  Web Mining researches are considered as an important field according to the increasing the size of web. By means of the type of a target data, Web Mining can be divided into Web Usage/Log Mining, Web Content Mining, and Web Structure Mining [1].

We can extract only what we want from massive and heterogeneous web data via Web Mining. In the type of web data, there are text, image, video, audio, and numerical data and so on. Among them, text data is viewed as more important and informative than others because unlike other data types which are generated by sensors, text data is generated by human itself.

Even though the data has some subjectivity, it has not simple, but complex and meaningful information.

Within many kinds of text data in the web, the news articles are most useful resources because it?s mostly based on the truth. In the news article dataset, text data also has various types such as title, paragraph, date, advertisement, recommended-news and so on.

1 http://www.internetlivestats.com/total-number-of-websites  Since we do not need advertisement, footer, and navigation although they are text data, we need to do Web Content Mining which can extract only what we want. Our research aims to analyze news article dataset for horizon scanning. Before analyzing news article dataset we need to prune noise information because we need only title, date and paragraph of the body for finding issues which is the goal of horizon scanning.

The methods for Web Content Mining are widely divided into two methods like rule-based and machine learning based methods. Although there are pros and cons for each approach, rule-based models cannot generalize at all when predicting unseen formats of news articles. They have high performance only to pre-trained formats of news articles. Instead, machine learning based models are robust to unseen formats of news articles because they can make a decision boundary which can separate dissimilar classes. In addition machine learning methods are developing by virtue of big data and advanced techniques of machine learning. Therefore, our model is constructed by machine learning based method so as to solve the classification problem.



II. RELATED WORK When human see the news article data, they can  easily recognize important areas because they can see it visually and semantically. However, computers are difficult to do because they see it only by html file itself. If we make the computer understand well by doing Web Content Mining, we can not only classify important areas automatically but also take advantage of them so as to be used for various kinds of web application such as searching, information retrieval, text categorization, topic tracking and so on.

As mentioned in Section I, the methods for Web Content Mining can be divided into two parts like rule and machine learning based methods. Rule-based methods are largely divided into simple pattern [2], [3], [4] and complex pattern [5], [6], [7] recognition.

Simple pattern includes statistics such as frequency, ratio, and average or DOM tree structure such as tag name, tag attribute, tag content and node location.

Complex pattern is a combination of simple patterns and heuristic approaches. The merits of these rule- based methods are to get very high performance     within limited dataset, to do not need additional steps such as learning step in machine learning methods, and to have fast and less computation. However, rule- based methods need to have heavy human effort when designing features and the model easily becomes brittle when seeing unseen formats. The problem is that the formats of news articles are changing on the basis of time and they vary according to news websites and even sections in the same web site. For this reason, the variation of the formats between web sites is high. Therefore, we need to solve the adaptability issue of new formats of news articles.

Whereas the rule-based methods are not robust for new data, machine learning based methods trained by data can solve the adaptability issue if the models are not overfitted. According to the type of training data, learning methods can be divided into supervised methods [8], [9], [10], [11] and unsupervised methods [12], [13], [14]. Supervised methods are the way of training a machine learning model through an optimization process using labeled data and objective function. That is, in terms of our research after training the model with labeled data in supervised fashion, we predict which one is the title, paragraph or date when coming unseen news article data. In general, labeling the data to do the supervised method is cumbersome and needs human effort. Instead, the unsupervised method doesn?t need labeled data.

However, since it can only do grouping or clustering data, it needs additional process such as selecting threshold, adding another classifier when doing a classification problem.

In machine learning based models, selecting good features is crucial step. Features are mostly designed by human. For example, we can find patterns from data and then consider the patterns as features. Also, features are designed through unsupervised learning models because it can group the data through some patterns. In general, in order to solve the classification problem the first step is to extract features via the unsupervised learning and then the second step is to classify features in a way of the supervised learning.

Web Content Mining on news articles frequently use machine learning based method because of heterogeneous news article dataset. Most of them consider a classification unit as an intermediate node which is the set of leaf nodes [9], [10], [11]. The problem is that when the classification unit is the intermediate node, we cannot distinguish leaf node one by one itself. Actually, the leaf node consists of not only title, date, paragraph but also advertisement, related news, copyright, footer and so on. In order to classify all the things, we need to define classification unit as the leaf node itself.  So, we show that noise information in the body can be removed given the model is constructed with leaf classification unit.



III. FEATURE DESIGN Doing a machine learning pipeline, feature design  is a crucial part because the performance of model highly depends on it. In a classification problem, we can design the features based on as much of unique patterns to each class as possible. The unique pattern  means that it has less correlation with other patterns.

When the patterns are highly correlated to each other, we have to select a representative feature among them.

Then, we can avoid overfitting by making the model simple.

In general, features are designed by either human effort or unsupervised learning models. When using unsupervised learning models, it is possible that the features have only dependency on training data.

Therefore, we design the features via our assumptions derived after examining thoroughly the data. The assumptions are as follows:  DOM-tree structure ? leaf nodes labeled as a certain class tend to frequently appear in a certain region of DOM-tree structure.

Hierarchical structure ? since the current nodes highly depend on their parent nodes, we should use both current nodes and their parent nodes.

Tag name, tag attributes and tag content ? similar tag name, attributes and content frequently exist in a certain class.

Given these assumptions, we design the features. We can divide them into continuous and binary features according as their values are either continuous or discrete. We consider that there is a set, {node, leaf node} in the DOM-tree, where a node is defined by tag name and tag attributes while a leaf node is defined by tag name, tag attribute and tag content.

Note that a node can include the set of other nodes and the set of leaf nodes. In general the node is composed by the set of leaf nodes and it can also be called an intermediate node. Also, it is possible the node can be both node itself and leaf node.

A. Continuous features  The value of continuous features range [0, ] and they largely consist of frequency and ratio.

1) Word Frequency: the number of words in the tag content of leaf node.

2) Leftside Node Frequency: the number of leftside leaf nodes on the basis of the current leaf node and plus the number of their parent nodes from the leaf node to the root node. The leaf node is counted by two not one.

3) Parent Node Frequency: the number of parent nodes except for their children nodes from the current leaf node to the root node. The same meaning is depth in tree-structure.

4) Sibling Node Frequency: the number of sibling nodes of the current leaf node. .

5) Uppercase Ratio: the ratio of word to upper character in the tag content of leaf node.

6) Digit Ratio: the ratio of word to digit in the tag content of leaf node.

Since the tag content of paragraph leaf node has lots of words, feature 1) is meaningful. But, it?s not always crucial because the tag content of noise leaf     node like recommended-news also has lots of words.

So, we hope to distinguish it from paragraph leaf node via feature 4) in that the paragraph leaf node usually has lots of sibling nodes. From feature 2) and feature 3), we can get a geographic location from the DOM- tree. Noise leaf nodes such as footer, navigation, and recommended-news are mostly located in bottom of the tree whereas important leaf nodes such as title, date, and paragraph are usually located in the top or middle of the tree. Feature 5) will be useful in that noise leaf nodes such as advertisement, navigation, and recommended-news commonly have the uppercase on the first letter to each word. Since date and advertisement leaf nodes usually have high ratio of digit to word, we can distinguish them using feature 6). Note that there is not a representative feature for a certain class. After we use all kinds of features, we can simply get the amount of weights for features from data. For a certain class, an important feature exists but not a representative feature.

B. Binary features The value of binary features is either 0 or 1. The  binary value is determined by whether a certain word, character or format exists or not.

1) Is there a dot in the tag content of leaf node.

2) Is there a date format in tag content of leaf  node.

3) Is a tag content  totally wrapped by hyperlink.

4) Are there words from tag name dictionary in  tag name of leaf node.

5) Are there words from tag attribute dictionary  in tag attribute of leaf node.

6) Are there words from tag content dictionary in  tag content of leaf node.

Using feature 1) is useful because tag content of paragraph leaf node mostly has dot at the end of the tag content because mostly it consists of multiple sentences. In feature 2), we use python date library to check whether this tag content is date form or not. In contrast with important leaf nodes, noise leaf nodes such as advertisement, caption, and recommended- news are mostly wrapped by hyperlinks (feature 3)).

Since the problem is that tag content of recommended -news leaf node is very similar to tag content of paragraph leaf node, we consider each word in tag content as a feature itself.  The word features based on the dictionaries (feature 4, 5, 6)) are useful because the set of frequent words is different between the paragraph and the noise leaf nodes. For example, words such as ?subscribe?, ?copyright?, and ?popular? frequently appear in noise leaf nodes. Actually, we constructed dictionaries based on tag name, attribute, and content of noise leaf nodes.



IV. PROPOSED MODEL We try to solve the multi-class classification  problem based on title, date, paragraph and noise classes by using the supervised learning model trained by manually labeled dataset and designed features.

The selected model is kernel-based Support Vector  Machines (SVM) which is appropriate to finding non- linear decision boundary equivalent to the dataset that consists of complex patterns.

Figure 1. Model Prediction Flow  After training the model, we can see the flow of predicting new data using the trained model in Figure 1. The whole process can be divided into three parts such as preprocessing, feature extraction, and modeling. First of all, given html file we split it into the set of leaf nodes. Since the size of the set is very big, we need to prune unnecessary leaf nodes via preprocessing. By using pre-defined features, we generate a feature vector to each leaf node and considering it as input data of the model we can solve the classification problem.

A. Preprocessing Since a leaf node is defined as a classification unit,  we use only the set of leaf nodes in DOM-tree structure for classification problem. Since the number of noise leaf nodes is very large, we prune the exact noise leaf nodes, which mean that they have no chance to be important information. The exact noise leaf nodes are determined by tag name and tag attribute. For example, any node which has tag name such as <script>, <style>, <link>, <footer>, <nav>, <button>, and so on2 or tag attribute such as ?footer?, ?nav?, ?slidebar?, ?blurb?, ?promo?, ?ad? and so on, is regarded as the exact leaf node. Also, leaf node which has ?hyperlink?, ?strong?, ?em? or ?br? tag name is considered as not leaf node but tag content itself of their parent nodes because the granularity of their leaf nodes is very small (e.g., a word). The distribution of classes is excessively imbalanced at the first time. But, the degree of imbalance slightly moderates after preprocessing.

B. Feature extraction We extract features of leaf nodes by using pre-  defined feature forms and construct feature vectors each of which has n elements. In our model, t feature vector has 85 elements. The number of elements of the feature vector highly depends on dictionaries.

C. Modeling Since the formats of news articles are changing by  various factors, they have complex patterns. That is, the distribution of dataset is non-linear. In order to figure out this non-linear problem, we use kernel based SVMs as our machine learning model. The  2 http://www.w3schools.com/tags     kernel trick is to enable to define new features by transforming low dimensional features into high dimensional features. By doing so, the representation of features becomes complex and plentiful which can find the decision boundary separated well in nonlinear dataset. We use a Radial Basis Function (RBF) kernel using Euclidean distance similarity. The input of the function is the original features defined by us and the output of the function is a vector whose elements are similarities between input pairs [15]. Note that we should normalize the original features before putting it into the kernel function because the RBF kernel function measures similarity based on distance between pair [16]. If not, the large-scaled features easily dominate the small-scaled features. Therefore, we need to rescale continuous feature values having observed zero to infinite into from minus one to plus one.



V. EXPERIMENT In order to deal with various formats of news articles, we collected the dataset from 13 news sources by crawling. The number of articles from each news source is in Table I.

TABLE I. COLLECTED DATASET  News website Count News website Count ABCAU 161 FoxNews 200  ABCNews 211 Reuters 206  Aljazeera 183 Telegraph 170  Boston 163 TheGlobeandmail 171  CNN 197 USAToday 179  CSmonitor 196 WSJ 135  EuroNews 200 TOTAL 2573  In order to apply supervised learning, we manually labeled all leaf nodes with title, date, paragraph and noise. One thing is that the noise leaf nodes are applied to all leaf nodes except for title, date and paragraph. That is, various kinds of leaf nodes such as image, caption, advertisement, footer, recommended- news, copyright, navigation and so on are classified into noise. Since the distribution of the number of classes is skewed, we maintain the equal ratio of classes when splitting dataset into training and test data. The distribution of classes from the entire dataset is in Table II.

TABLE II. DISTRIBUTION OF CLASSES  Title Date Paragraph Noise  Count 2011 2285 29795 37510  Ratio 2.8% 3.1% 41.6% 52.3%  To measure the performance of the model, we conducted two kinds of tests such as inner test and outer test. In inner test we used training and test  dataset come from the same news sources. On the other hand, in outer test we use training and test dataset come from different news sources. It means that there is no format dependency between training and test dataset when doing outer test. Therefore, when it comes to heterogeneous formats of articles it is reasonable to verify generalization of the model when doing outer test rather than inner test. In addition we compared our ML model to rule-based model and external ML based model so as to confirm how reliable our model is.

A. Inner Test We used all news sources in our dataset to conduct  inner test. First we shuffled dataset based on leaf node and then did n-fold cross-validation. Specifically we did 3-fold cross-validation for rule-based model and 5-fold cross-validation for our ML based model.

TABLE III. RESULTS OF INNER TEST  Model Class Precision Recall F1 score  Rule based Model  Title 100.0 (0) 100.0 (0) 100.0 (0)  Date 100.0 (0) 100.0 (0) 100.0 (0)  Paragraph 100.0 (0) 100.0 (0) 100.0 (0)  Noise - - -  SVM based Model  (Our ML Model)  Title 94.8 (1.1) 99.8 (0.4) 97.2 (0.7)  Date 100.0 (0) 98.6 (0.4) 99.2 (0.4)  Paragraph 98.2 (0.4) 98.4 (0.4) 97.2 (0.4)  Noise 99.0 (0.0) 98.2 (0.4) 97.2 (0.4)  The value in parenthesis indicates standard deviation of n-fold cross-validation  Since the training dataset can cover all rules for the entire dataset in inner test, the performance of the rule-based model is all 100% as expected. Similarly our ML based model has reasonably high performance (Table III).

B. Outer Test Unlike inner test, in outer test the rules from  training set could not cover all of the test set because dataset is divided into training and test set on the basis of news source. We shuffled just the list of news websites before doing n-fold cross-validation. Like inner test we did 3-fold cross-validation for rule-based model and 5-fold cross-validation for our ML model.

The performance of rule-based model remarkably decreased and standard deviation remarkably increased compared to our ML model in Table 4. The reason why standard deviation was high in the rule- based model is because the performance tends to run both extremes. Concretely the percentage of zero was frequently measured in the rule-based model because the unseen formats from test set could not be covered by the rules made by training set. On the other hand, our ML model was robust for outer test which includes unseen formats of news articles.

TABLE IV. RESULTS OF OUTER TEST     Model Class Precision Recall F1 score  Rule based Model  Title 61.4 (47.9) 61.4 (47.9) 61.4 (47.9)  Date 44.8 (43.3) 44.8 (43.3) 44.8 (43.3)  Paragraph 42.3 (41.0) 50.3 (48.0) 45.8 (43.9)  Noise - - -  SVM based Model  (Our ML Model)  Title 88.0 (13.5) 85.8 (16.9) 86.2 (14.3)  Date 100.0 (0.0) 91.4 (13.1) 95.0 (7.7)  Paragraph 91.4 (8.8) 78.6 (16.7) 86.2 (11.2)  Noise 84.8 (10.4) 93.2 (6.7) 86.2 (6.4)  The value in parenthesis indicates standard deviation of n-fold cross-validation  Furthermore, we compared the external ML model with our ML model so as to show how reliable our model is. The external ML model is identical with our ML model in that it classifies not the set of leaf nodes but the leaf node itself. Even though the number of classes is different like 9 and 4, respectively, it could be reasonable comparison because both models use news article dataset. In Table V, the performance of external ML model is came from [8] and that of our ML model is based on Table IV. Since both models have different number of classes, we compared both by micro averages and macro averages which can be viewed as averaging precision, recall, and F1 score from all the classes.

TABLE V. EXTERNAL COMPARISON  Model Class Precision Recall F1 score  External ML Model  Micro- Average 96.11 96.06 96.06  Macro- Average 89.29 85.07 86.64  SVM based Model  (Our ML Model)  Micro- Average 86.8 (8.7) 86.8 (8.7) 86.8 (8.7)  Macro- Average 91.0 (5.9) 87.3 (10.2) 88.3 (8.3)  Value in parenthesis: Standard deviation for n-fold cross-validation  Macro-average is a way of weighting equally to each class when averaging regardless of the number of instances in the class. By contrast, micro-average is a way of weighting the number of instances in the class when averaging [17]. It means that when we deal with imbalanced classification problem, one class which has a few instances has benefit from macro-average whereas one class which has lots of instances has benefit from micro-average. In other words in micro- average, one class which has lots of instances easily dominates other class which has a few instances.

Given the fact that the number of instances in the class influences the performance, we can conclude that the classes which includes a few number of instances such as title and date of our ML model have high performance whereas the classes which have lots of number of instances such as none, advertisement, paragraph of the external ML model have high performance because our ML model relatively high performance in macro-average and low performance in micro-average compared to the external ML model.



VI. CONCLUSION It is not easy to do Web Content Mining on news  article dataset because the formats of news sources are changing according to time and also they vary according to news sources and even section of them.

Since the variation of formats is high, it is appropriate to use machine learning based model rather than rule- based model. Therefore, we solve the problem by using machine learning based model and we suggest that noise information in the body can be removed given the classification unit is a leaf node.

The experiments are divided into inner test and outer test depending on whether test set consists of unseen news sources compared to training set or not.

We closely compare our ML model with the rule- based model and we compare it with the external ML model. In inner test the performance of rule-based model is perfect as we expected because it has dedicated patterns in the same news sources whereas our ML model also gets over 97% on the basis of F1 score. On the other hand, the performance of rule- based model rapidly decreases in outer test whereas our ML model is robust for outer test because it generalizes well via its feature vector even though the data has unseen formats. In terms of comparison with the external ML model, our ML model has reasonable performance but micro-average is relatively low.

One of the reasons why our ML model has limits is because of the bottom-up process and deficient dataset. In the way of bottom-up process, we should classify all the leaf nodes at once. It would be a burden because the model should remember a lot of patterns. To deal with this problem, we will construct more dataset and need to do hybrid process of top- down and bottom-up. It means that we first classify intermediate nodes which are the set of leaf nodes and then we classify leaf nodes given the results of first classification.

