

Privacy preserving Data Mining Algorithms without the  use of Secure Computation or Perturbation     Alex Gurevich                                                                     Ehud Gudes Department of Computer Science                               Department of Computer Science Ben-Gurion University                                                Ben-Gurion University Beer-Sheva                                                                       Beer-Sheva Israel                                                                                Israel gurevich@cs.bgu.ac.il                                                      ehud@cs.bgu.ac.il    Abstract  In our era Knowledge  is not "just" information anymore, it is an asset. Data mining can be used to extract important knowledge from large databases.

These days, it is often the case that such databases are distributed among several organizations who would like to cooperate in order to extract global knowledge, but at the same time, privacy concerns may prevent the parties from directly sharing the data among them.

The two current main methods to perform data mining tasks without compromising privacy are: the perturbation method and the secure computation method. Many papers and published algorithms are based on those two methods. Yet, both have some disadvantages, like reduced accuracy for the first and increased overhead for the second.

In this article we offer a  new paradigm to perform privacy-preserving distributed data mining without using those methods,  we  present three algorithms for association rule mining which use this paradigm, and discuss their privacy and performance characteristics.

Keywords: Privacy, Association rules, Secure Computation, Distributed Data Mining  1.Introduction  In recent years the data mining community has faced a new problem. After it was shown how effective its tools are in revealing the knowledge locked within large databases, it is now required to develop methods that limit the power of these tools to protect the privacy of individuals. This requirement arises from popular concern about the powers of large corporations and  government agencies ? concern which has been reflected in the actions of legislative  bodies  Not surprisingly, the same corporations and government organizations which are the cause of concern are also among the main pursuers of such privacy-preserving methodologies. This is because of their pressing need to cooperate with each other on many data analytic missions (e.g., for cooperative cyber-security systems, failure analysis in integrative products, detection of multilateral fraud schemes, and more).

The first approach that was taken toward privacy protection in data mining was to perturb the input (the data) before the mining [1]. Thus, it was claimed, the original data would remain secret, while the added noise would average out in the output. This approach has the benefit of simplicity. At the same time, it takes advantage of the statistical nature of data mining and directly protects the privacy of the data. The drawback of the perturbation   approach is that it lacks a formal framework for proving how much privacy is guaranteed. Despite the existence of several models [1,5,7,9] for studying the privacy attainable through perturbation, there is no formal way to model and quantify the privacy threat from mining of perturbed data, and recently there  has been some evidence that for some data, and some kinds of noise, perturbation provides almost no privacy at all [14,17].

At the same time, a second approach  for  privacy preserving data mining was developed, using cryptographic techniques [3,10,22], most often the secure computation technique is used [6,11,23,24].

This approach  became very  popular for two reasons: first, cryptography offers a well defined model for privacy, which includes methodologies for proving and quantifying it. Second, there exists a vast toolset of cryptographic algorithms and constructs which can  10th International Database Engineering and Applications Symposium (IDEAS'06) 0-7695-2577-6/06 $20.00  ? 2006      be used for implementing privacy-preserving data mining algorithms.

While Secure computation (SC) has an advantage over perturbation in that it provides accurate results and not approximation, it is a much slower method and requires considerable computation and communication overhead for each SC step.

Naturally we would like to develop algorithms which include the advantages of the two approaches and do not include their disadvantages.

In this paper we offer a new paradigm to perform privacy-preserving distributed data mining without using the above  methods and we  present three algorithms for association rule mining which use this paradigm : One for vertically partitioned databases, one for horizontally partitioned databases and one for the general case. The main idea is in the architecture which separates the entity which computes the results and the entity which finally gets the results and know what they mean. For each algorithm we present, we analyze its privacy  and its performance overhead.

The rest of this paper is structured as follows.

Section 2 presents the background and related work and especially the work by Vaidya and Clifton[22,23], and the related  work by  Rozenberg and Gudes [12,13],  on vertically partitioned association rules mining.

Section 3 presents our proposed architecture and the three algorithms and discusses their privacy and performance properties, and Section four is the conclusions.

2. Background and Related Work  2.1 General  The centralized data mining model assumes that all the data required by any data mining algorithm is either available at or can be sent to a central site. A simple approach to data mining over multiple sources that will not share data is to run existing data mining tools at each site independently and combine the results. But, this will often fail to give globally valid results[3,19].  Issues that cause a difference between local and global results include: values for a single entity may be split across sources., the same item may be duplicated at different sites, and will be over- weighted in the results, and data at a single site is likely to be from a homogeneous population, hiding geographic, demographic or other distinctions between that population and others. Several algorithms have been proposed for distributed data mining. Cheung et al. proposed a method for horizontally partitioned data[4], and more recent work has dealt  with   privacy in this model[18]. Distributed classification was also  investigated.  A meta-learning approach has been developed that uses classifiers trained at different sites to develop a global classifier [19]. This could protect the individual entities, but it remains to be shown that the individual classifiers do not disclose private information. Recent work has addressed classification using Bayesian Networks in vertically partitioned data [14], and it discusses situations where the distribution itself is the interesting property learned.

2.2 Privacy preserving solutions.

There has been some research considering how much information can be inferred, calculated or revealed from the data made available through data mining process, and how to minimize the leakage of information.. In [1], data perturbation techniques are used to protect individual privacy for classification, by adding random values from a normal/Gaussian distribution of mean 0 to the actual data values. One problem with this approach is the existing tradeoff between the privacy and the accuracy of the results More recently, data perturbation has been applied to Boolean association rules [20]. An  interesting feature of this work is a flexible definition of privacy; e.g., the ability to correctly guess a value of `1' from the perturbed data can be considered a greater threat to privacy than correctly learning a `0'.

Another line of  work is in cooperative computation between entities that mutually distrust one another.

Secure two party computation was first investigated by Yao [24], and later generalized to multiparty computation. The seminal paper by Goldreich[11] proves existence of a secure solution for any functionality. The idea is as follows: the function F to be computed is first represented as a combinatorial circuit, and then the parties run a short protocol to securely  compute every gate in the circuit. Every participant gets corresponding shares of the input wires and the output wires for every gate. This approach, though appealing in its generality and simplicity, means that the size of the protocol depends on the size of the circuit, which depends on the size of the input. This is inefficient for large inputs, as is the case in data mining. Nevertheless, the SC approach has become very popular. Lindell and Pinkas[18] use SC protocols to achieve complete zero knowledge leakage for the ID3 classification algorithm for two parties with horizontally partitioned data. Du and Atallah[6] discuss several problems in the relationships between Data Mining and Secure Multiparty Computation. Although this shows that secure solutions exist, achieving efficient secure solutions for privacy preserving distributed data mining is still open.

10th International Database Engineering and Applications Symposium (IDEAS'06) 0-7695-2577-6/06 $20.00  ? 2006      2.3 Privacy preserving association rules mining.

Association rules have become one of the most common techniques for data mining, many algorithms were developed, and the most popular one is the Apriori algorithm [2]. Privacy-preserving association rules mining is also an active area of research.

Specifically, when data is distributed across multiple sites, and the sites have to mine it cooperatively, the privacy issue is most important. Kantarcioglu, and Clifton [16] investigated the problem of privacy- preserving distributed mining of association rules on Horizontally partitioned data. The algorithm uses three basic ideas, randomization of the first result, encryption of  results at each local site and a final step of secure computation. Vaidya and Clifton [22,23] developed algorithms, we call them VDC, for privacy preserving association rule mining in Vertically partitioned data.  In such databases, item-set in one database is represented as a Boolean membership vector, and the main technique, which uses the Apriori algorithm as its base, computes securely the scalar product of two or more membership vectors and checks whether the result is larger than the support threshold. Again, quite  complex cryptographic and randomization processes are used. Gudes and Rozenberg[12,13] (GR) showed some disadvantages of the VDC approach and presented a different method that uses also the Apriori algorithm but also uses the idea of faked transactions, and as a result saves considerable computation overhead.

Next we want to present the last two algorithms in more detail since they are very related to the work presented here In the VDC algorithm,  for the two-party situation [23],  the algorithm uses the apriori-gen function to generate all candidate itemsets. For each found itemset, the algorithm calculates the support by secure computation of the scalar product. For the n>2 parties case [22] they use the fact that a solution to finding association rules in vertically partitioned data is to have each site generate a list of the transaction identifiers that contain all the items in the item set which are present in that site. The intersection of these lists is the set of transactions that support the rule. To obtain this intersection, VDC uses commutative encryption; it has the property that the same data item encrypted with k different keys gives the same cipher text even if the order of encryption differs. To compute the list intersection?s size, each site picks a local key and uses it to encrypt all of its items. It then sends the encrypted list to the next site, which does the same thing. After all the sites have encrypted all the lists, they can intersect them because the encryption is commutative. The resulting intersection set?s size  gives the required result. None of the sites sees an unencrypted item other than its own, and decryption is never performed, so none of the sites learns anything from the other sites[22].

As mentioned above, the VDC algorithms involve considerable computation overhead, and also have the disadvantage that the relevant parties know exactly the item-set which is currently being computed. Another problem of the VDC  algorithms is  the fact that all parties know the exact support of each itemset which can compromise privacy. This fact can cause almost complete exposure of the real data as was demonstrated in [13]. That motivated Rozenberg to propose another way to solve the problem using the fake transactions[12,13,21] method. This method obscures the exact support from the parties and additionally reduces the communication between the sides. For example, let us describe the two-party algorithm in more details. The n>2 case is very similar. Both algorithms follow the basic collaborative Apriori algorithm of VDC[23]. However, before applying the algorithm there is a pre-processing step that populates the individual databases with fake transactions. After this step, each party can send the resulting database to the opposite party to calculate the frequent itemsets in the resulted database. Since the fake transactions do not change real data, but only add fake information, all items with real high support will be found and we only need to eliminate the false positive results.

In the first phase, one site is designated as a Master, which initiates the protocol and will get the results, and the other site as a Slave, which only contributes its information but will not do any global computations.

The Master is looking for all large itemsets according to its own real transactions, and then check whether the found itemsets are present in the Slave real transactions. This is done with the help of a third untrusted party (this party is not trusted with the database, but it is trusted with computations) or using secure computation. The parties change roles at the end of the phase.

The GR  algorithms also have some problems. Here are few of them: 1) They expose some real data to the parties (even though the parties don?t know which data is real and which is fake) 2)All the intermediate results are given to the party/parties which hold some database part, which increases the possibility to infer information by this party.

3) They are sensitive to the probing attack. Although in [12] there is a method called the e-approximation method which considerably reduces the risk of a probing attack. The idea is basically  to report success  10th International Database Engineering and Applications Symposium (IDEAS'06) 0-7695-2577-6/06 $20.00  ? 2006      whenever the support is in the range of [S, S-e] and not using an exact support threshold..

The problems with both the VDC and GR algorithms motivated us to propose some new, more general way, to solve the privacy-preserving association rules mining problem. The main idea is an architectural one.

We add to the N involved parties two new components called: Miner and Calculator which divide the work of the various algorithms and thus achieving privacy. The algorithms based on this idea  will be described in the next section  3. The new architecture and algorithms  The new architecture is depicted in Figure 1. It is composed of the participating databases, a Miner which decides what computation to be done, and the Calculator which computes without really knowing what itemset it computes. It is important to note, that only the Miner and the participants get the mining results while the Calculator only performs auxiliary computations, without knowing their meaning. Based on this basic architecture we develop three different models and algorithms. The first model supports vertically partitioned association rules mining, the second model supports horizontally partitioned association rules mining, and the third model is general and supports any data mining task. All three models use the following basic assumptions: 1) The  database  is distributed between N participants (vertically  or horizontally  partitioned).

2) There is a computer called "miner" which manages the data mining process and reports the results to the participants 3) The miner has no part of the database.

4) There is a computer called "calculator" which performs all the calculations.

5) The "calculator" has no part of the database.

6) There is no collaboration between all the participants from paragraph 1.

7) The model is semi ? honest.

8) There is no external knowledge present at any side.

Next we present first model which deals with vertically partitioned data.

Figure 1: the Architecture   3.1 First algorithm (for vertically partitioned DB).

The model : (see Figure 1)  The goal: Computation of the frequent itemsets in vertically partitioned database without compromising the participants?  privacy.

Privacy definition: The privacy will be compromised if it will be possible for any participant to compute some specific value of the database with high probability. By specific value we mean a value in a database cell which belongs to some specific transaction.

The algorithm Step 1: The miner sets the variable i(the size of the itemsets being checked now) to 1.

Step 2: The miner chooses a random permutation of itemsets of i elements and then iteratively select each of them.

Step 3: For each itemset from step 2 If all the subsets of current itemset are frequent (apriori principle),  the miner orders all participants to encrypt   the Transaction-ids of the   transactions using the same key.  The miner then asks every participant about  frequency of current itemset.

The participants send the encrypted numbers of all relevant transactions-ids to the "calculator". The "calculator" finds the intersection of    the encrypted transactions-ids.  The "calculator" informs the miner if the  current set is frequent.

Step 4: While   i is smaller from the number of database attributes The value of i is incremented by 1.

Miner calculator  Db1 Db2 Db3 ... Dbn  10th International Database Engineering and Applications Symposium (IDEAS'06) 0-7695-2577-6/06 $20.00  ? 2006      The algorithm returns to Step 2   Step 5: The miner sends the results to the participants.

Although this algorithm is somewhat similar to the VDC algorithm in [23], it is different in two respects: First, the calculator doesn?t know which itemset is being checked, and the  exact support value is not disclosed. Furthermore, only one level of encryption is used. (only transaction?ids are encrypted and not the itemsets).

The privacy analysis: The participants and the "miner" learn nothing but the results as in  secure computation.

The "calculator" learns the support of the itemsets but without the possibility to know which itemset is being tested.

No probing attack is possible, since the miner does not hold any part of the database!

However, since the results are known to each participant, there is a probability of exposure in case the local support is exactly the support threshold as discussed in detail in [13]. There,  the e-approximation method was suggested and it can be used here as well.

Note that the problem of inferring knowledge from the results of data mining is independent of the algorithm used and exists also in all secure computation based methods.

The computation and communication cost In VDC  two sided algorithm, for every itemset frequency computation it was  necessary to perform secure scalar product computation which takes for every tested set at least three times sending the N values between two sides. In our algorithm in the worst case, every party sends once N values to the calculator, which is much less communication overhead. In the n-party case we have the same communication as in VDC but our algorithm is simpler, since we use one level of encryption only.

Comparing to GR we save all the preprocessing stage of sending the database with faked transactions.

3.2 Second algorithm  (for horizontally partitioned DB).

The model: We use the same model but for horizontally partitioned database and  with the addition of one more "calculator". The modified architecture is depicted in Fig. 2. We also add the assumption  that the database is binary. The goal and privacy definition stay as in the first algorithm.

We are going to use the following lemma which was already proven in [16].

Lemma : If the database is horizontally partitioned and if an itemset has support > p% globally, it must have support > p% in at least one of the individual parties databases.[16]  3.2.1 The algorithm( first version without using  the lemma): The first version is not realistic since it requires exponential space, but its useful for explaining the concept:  Step 1: The "miner" sends to every participant two instructions:  - to create two databases each containing for each itemset the frequency of its occurrence.

Then each   participants   add to   each such frequency a random noise sent by the miner, one  database  with  positive noise,  and  the other with negative noise.

- to send the first database to "calculator1" , and  to  send  the  second    database   to "calculator2".

After execution of this step "calculator1" and "calculator2" both have the whole information about the number of appearances of each itemset but with noise.

Step 2: The "miner" performs apriori  algorithm  against the calculators. When the miner needs to compute support for some itemset, he sends the request to both calculators and some random number to one of them.

The first calculator, who gets the random number, adds to it the value which appears in his database about this itemset and sends the result to the second calculator. The second  adds his value and sends the result back to the miner. The miner divides the result by  (2 * (size of the database)) and gets the real support.

Privacy analysis : The parties ? learn nothing but the results.

The calculators ? learn nothing; because of the random number they don?t know the real support The miner ? learns global support of the itemsets but nothing on the local databases  Communication overhead Much less than in the first algorithm since the parties communicates only once with the calculators, although in this one time they send a large database.

Let us see a small example:  10th International Database Engineering and Applications Symposium (IDEAS'06) 0-7695-2577-6/06 $20.00  ? 2006      Suppose we have three parties :d1, d2, d3 ; the "miner" and two calculators. Let us take a look on some itemset ABC, see Figure 2.

The Miner sends the random noises: 3,2,1 to the three parties respectively.

Figure 2:  The second algorithm   Next, all the parties add the noise to the number of appearances of every set and send the results to both calculators, in this case for ABC: the first party sends 2+3 = 5 and 2-3 = -1, the second 2+2 = 4 and 2-2=0, the third 2+1=3 and 2-1=1.

Next (see Figure 3)  during the Apriori algorithm to compute support of some itemset, the Miner selects some random number R, sends R to first calculator, which adds his support about the itemset and sends R1 to the  second calculator, which  computes R2 and sends it to the Miner. The Miner computes (R2 ? R)/(2*3) and this is the support of itemset ABC.

Figure 3: Second algorithm ? computing the support   3.2.2 Second version (using the lemma).

Basically the same algorithm, except for the following  changes:  1) Initially, the miner chooses an encryption key to encrypt each item-set and send it to all participants  2) The participants, before sending their two databases to the calculators encrypt the corresponding itemsets  3) the participants compute the frequent itemsets locally and send results to the miner  The Miner does the Apriori only for itemsets  of which one of the local parties has found frequent! (i.e. using the Lemma).

For those   itemsets, it sends the calculators their encrypted for to enable them to retrieve their respective frequency.

Privacy analysis: Same as the first algorithm, except that the Miner now knows frequency of itemset at local sites  The purpose of the encryption is to prevent the calculators from knowing that a particular itemset is frequent at some local database. To prevent the Miner from knowing the local site frequency, we can add an extra Mediator site. The mediator will not know the itemset which is currently being tested, but will get the frequency counts from the various sites and just send the Miner a flag whether there is a site in which the frequency of the itemset is above the threshold. This increases the privacy but introduces additional communication overhead.

The computation and communication overhead are less than in the previous  algorithm because of the use of the Lemma, but obviously, this algorithm also requires exponential space for each of the local databases, therefore we need the third version.

3.2.3 Third version (using Apriori and the lemma).

This algorithm is a mix of the first algorithm (3.1) and the last one.

The Miner performs the Apriori algorithm, and at each step asks the participants to send the two frequency results with the noise to the calculators. Then the Miner asks for the support from the calculators as before. This algorithm has the same security properties as the previous two, however, no encryption is necessary, since as in the first algorithm, the Claculators don?t know which itemset is being computed.

The  communication overhead of this version  is higher than in the first two versions and is similar to the overhead in the first algorithm. The main advantage of-course is that the local parties don?t need to create and send large (exponential)  databases!

3.3 Third algorithm  (General DB).

The model: The same as in the first model, one miner, one calculator and N participants. Also, all the definitions stay as in the first algorithm,  except the for goal.

The goal: To perform any data mining algorithm without compromising privacy.

The algorithm  The miner     Calculator1  Calculator2  A B C 1 | 1  1 1 2 | 1  1 1 3 |0  0  1  A B C 4 | 1  1 1 5 | 1  1 1 6 |0  0  1  A B C 7 | 1  1 1 8 | 1  1 1 9 | 0  0 1  The miner Calculator1 Calculator2  10th International Database Engineering and Applications Symposium (IDEAS'06) 0-7695-2577-6/06 $20.00  ? 2006      Step 1: The "miner" tells to the participants to add N versions of  specific pseudorandom  noise to their data and to send it to the "calculator". That it, the calculator gets from each participant its database N times.(with a different noise).

Step 2: The miner sends to the calculator some "seed" to create the pseudo-random noise. This seed will create a noise which is identical to one of the N noises, but the calculator doesn?t know which one it is.

Step 3: For every created noise the calculator subtract the noise from all N databases and perform the mining, and sends the results back to the miner.

Step 4: The miner picks up the right results.

Privacy analysis : The parties ? learn nothing but the results.

The miner ? learns global data mining results but nothing on the local databases The calculator ? The Calculator has one real database out of the N databases, so he has the probability of 1/N to learn everything! Obviously we can reduce this probability by adding some more artificial partcipants, but each such participant increases the computation overhead.

Computation and Communication overhead : The computation overhead is much higher, it is essentially multiplied by N. The communication overhead is N*overhead of Algorithm 2, but this is a one-shot overhead, and the communication overhead between the Miner and the Calculator is greatly reduced (one message only!)  4. Conclusions.

We discussed the problem of privacy-preserving data mining in distributed databases. We suggested a new architecture (paradigm) which is based on using two separate entities: a Miner and a Calculator, both not having parts of the database. We presented three algorithms which are based on this paradigm, one for Vertically partitioned data, one for Horizontally partitioned data, and one for any data mining method.

In contrast to the previously known methods of Perturbation or Secure computation, our algorithms produce accurate results (better than perturbation), but usually with much less computation and communication overhead than secure computation.

In the future we plan to investigate these algorithms and evaluate their performance on real-life databases.

