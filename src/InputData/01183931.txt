User-directed Exploration of Mining Space with Multiple Attributes

Abstract  There has been a growing interesr in mining frequent itemsets in relational dara wirh mulriple arrribures. A key step in this approach is to select a ser of attributes that group data into transactions and a separate ser ofattributes rhar labels dara into i tem.  Unsupervised and unresrricred mining, however: is stymied by rhe combinatorial complex- i ty  and the quanrity ofparrerns as the number ofarrribures grows. In this paper. we focus on leveraging rhe seman- tics ofrhe underlying data for mining frequenr iremsets. For instance, there are usually taxonomies in the dara schema andfunctional dependencies among the attributes. Domain knowledge and user preferences often have the porenrial to significantly reduce the exponenrially growing mining space. These observarions morivate the design o f a  user- directed data mining framework that allows such domain knowledge to guide the miningpmcess andcontrol the min- ing strategy. We show examples of rremendow reduction in compurarion by Using domain knowledge in mining rela- rional data wirh multiple attributes.

1 Introduction  Mining for frequent itemsets has been studied exten- sively because of the potential for actionable insights. S p - ically, before mining is done, a preprocessing step uses data attributes to group records into transactions and to define the items used in mining. For example in supermarket data, the MarketBasket attribute might be used to group data into transactions and the ProductType attribute (with values such as domestic beer) to specify items. We re- fer to this as k e d  attribute mining in that mining does not change which attributes are used to determine transactions and items. Unfortunately, fixed attribute mining imposes severe limitations on the patterns that can be discovered in that the analyst must specify in advance the attributes used to designate items (e.g., Imported/Domes tic  and Product Class) and how to group them for the purposes of pattern discovery (e.g., Transaction or CustomerName + TimeOfDay). To this end, a frame- work for mining multi-attribute data, FARM[I4], proposes an approach to mining multi-attribute data in which item- izing and grouping attributes are selected in the course of the mining operation itself. While this greatly expands the range of patterns that can be discovered, it also creates an- other level of combinatorial complexity. This paper pro- poses a framework for specifying constraints on the item- izing and grouping attributes. Not only does this reduce computational complexity. it can also result in patterns of more interest to the analyst.

We have observed that fixing the attributes used to de- fine transactions and items can severely constrain the pat- terns that are discovered. To go beyond the limits of fixed attribute mining, multiple-attribute mining applies the no- tion of mining spaces to discover frequent patterns for trans- actions and items that are defined in terms of data attributes.

A transaction is a general term for a group of records. This approach does not require pre-specified taxonomies, al- though it exploits such information if it is available. Further, because of various downward closure properties. multiple- attribute mining is considerably faster than simply employ- ing apriori-like algorithms on each choice of attributes for defining transactions and items.

To illustrate the foregoing and to better motivate the problem we address, consider the domain of event man- agement of complex networks. Events are messages that are generated when special conditions arise. The relation- ship between events often provides actionable insights into the cause of current network problems as well as advanced warnings of future problem occurrences. Figure 1 illustrates a portion of event data we obtained from a production net- work at a large financial institution. We initially focus on the attributes Date, Time, Intrvl5 (five minute inter- val), EventType, HOS t from which the event originated, and Severity. "be column laheled Rec is only present to aid in making references to the data. The full data set evidenced the following patterns:  0-7695-1754-4/02 $17.00 Q 2002 IEEE 394  mailto:us.ibm.com   &ec Qate Time &MlPM !nuvl5 !nw130 EventType Host Site Source Subsource Severity Maint ( I )  8/21 2 1 2 2 3  AM 210 200 Tcpcls prtsvd haw infoprint pndaemon harmless No (2) 8/21 213:41 AM 210 2 W  InufcDwn neuvr38 ykt netagt cab-agt severe No (3) 8/21 21411 AM 210 2:W InufcDwn netsvr22 haw netagt cdl-agt severe No (4) 8/21 21437  AM 210 2:00 IntrfcDwn nelsvr5 haw netagt ibm-agt severe No (5) 8i2l 215:02 AM 215 200 InufcDwn netsew24 haw netagt ibm-agt severe No (6)  8/21 2:1609 A M  2:15 2:W CiscoLnkUp router16 haw cisco-agl cat5-agt severe Yes (7) 8/21 238:48 AM 2:35 230 NetMgrUp neNiewl6 ykt netview ibm-nev harmless NO (8) 8/21 2:48:23 AM 245 2:30 RlrLnkUp routerl6 haw cisco-agl cat5-agt harmless No (9) 8/21 3:13:12 A M  3:lO 3:W InnfcDwn neurv45 ykt netagt NI-agt severe No  Flgure 1. Distributed System Management Events  1. Host netsvr38 generated a large number of I n t r f c D w n  events on 8/21 and may indicate a prob- lem.

2. When host n e t s v r 2 2  generates an IntrfcDwn event, host r o u t e r 1 6  generates a C i scoLnkUp (failure recovery) event within the same five minute interval. Thus, an IntrfcDwn on n e t s v r 2 2  may provide a way to anticipate the failure of r o u t e r l 6 .

Moredetaileddiscussion about theexamplecan be found in [14].

While FARM discovers a more complete set of patterns, it creates challenges as well. First, analysts may be over- whelmed by dealing with the ahundanceof patterns discov- ered. Second, even though the FARM approach exploits downward closure properties to provide computational ef- ficiencies, the time required for pattern discovery can be substantial. For example, we show later in the paper that mining data with 20 attributes is equivalent to performing 3,485,735,825(0r3~~-2~~)roundsofmarket-basketstyle mining on the same data set if all different combinations of itemizing and groupings are to be explored. Thus, it is clear that such unsupervised approach is not feasible.

The foregoing motivates us to constrain the selection of grouping and itemizing attributes so as to make FARM more computationally efficient and its results more meaningful.

To this end, we develop attribute predicates that constrain the ways in which grouping and itemizing occur, and we show how these predicates can be incorporated into FARM.

Figure 1 provides examples of such predicates, especially if wealsoconsiderthe attributes AM/PM(twelve-hourperiod).

I n t r v l 3 0  (thiRy-minuteintervals), S i t e  (location ofthe host), andMain t .

deed, in Figure 1 we see that the failure of r o u t e r 1 6 at 2 : 1 6 :  09  occurs within the maintenance window for that host. Thus, if E v e n t T y p e  is an itemizing at- tribute, we should also include Maint.

3. Certain logical dependencies exist in the &la that can reduce the attribute combinations. For example, if we use Intrvl5.  we know I n t r v l 3 0  (i.e., there is a functional dependency). Similarly, if we know the Hos t , then  w e k n o w t h e s i t e .

Exploiting these relationships between attributes can re- sult in a substantial reduction in the number of patterns re- poned. Indeed, in our studies, reductions of several orders of magnitude are achieved.

1.1 Problem Statement and Scope  We have two goals in this study. The first is to design a small and comprehensible set of directives that allow users to specify constraints based on attributes intuitively. The specification language should be expressive enough to in- corporate common knowledge without operational instruc- tions. For example, users should be able to indicate relation- ships (such as functional dependencies) among attributes, in which way should these attributes he used in itemizing or grouping, and whether some attributes should be included in mining at all.

The second goal is to design an inference system that can translate the constraints to operational instructions that guide the frequent itemset mining algorithms to avoid un- necessary computation.

To realize these goals, we organize the search space - Time is in a twelve hour format, Thus, if the group- ing attributes include Time,  they should also include  I n t r v l 3  0.

into novel architecture that is conducive to attribute-based pruning. Our approach is based On the FARM frame-  sponds to a unique attribute mapping, and connects to other mining templates through a rich set of downward closure relationships. It is through these relationships that user- directed pruning of the search space takes place. In this  AM/pM, The Same argument applies IntrvlS and work WI ,  where each mining template directly come-  2. The reason for a perceived host failure may he that it is recovering after a normal maintenance operation. In-     paper. we omit issues such as candidate generation, aggre- gating functions, and instance counting. Interested readers can find the details in [141.

1.2 Related Work  Agrawal et.al. [Z, 31 identified the association rule proh- lem and developed the level-wise search algorithm. Since then, many algorithms have been proposed to make mining more efficient (e.g. [ I ,  4, 9, 1 I ]  and [SI for a review).

Mining data with multiple attributes has been recognized as an important task. Srikant et.al. [I61 and Han et.al. [8] consider multi-level association rules based on item tax- onomies and hierarchies. More recently, Grahne et.al. [6] proposed the dual mining for mining situations of a frequent pattern. Our previous work [ 141 developed a framework and algorithm for mining multi-attribute data called FARM that extends a tranditional mining task to a more general setting.

Considerable work has been done in characterizing pat- tern interestingness and constraints [15, 13, IO] and item constraints [IZ, 17, IO]. Such interestingness and con- straints are defined based on items. A framework has been developed for describing either interestingness or con- straints on items (e.g. city A and B can not be in the same pattern. Here, city A and B are two items of attribute city) and for efficient mining by pushing the constraints to the level-wise search in reducing the number of candidates gen- erated at each level. In contrast. this paper discusses how to express knowledge at the attribute level(e.g. attributes {event, type, name} can not used as itemizing attributes at the same time), which is on a higher level than the previous item-based approach. We demonstrate that such knowledge (orconstraints) about attributes (or variables) helps us dras- tically reduce the mining space. Further, we describe a lan- guage that can be used to describe common constraints and develop algorithms to construct optimal mining paths.

Our FARM work also relates to attribute-oriented mining [7]. Our focus is on frequent patterns mining and exploring the relationship among mining camps and constraints at the attribute level.

1.3 Paper Organization  We first review the FARM framework in Section 2. In Section 3, we describe the Attribute Specification Language (ASL), which enables users to specify attribute-level do- main knowledge. The section also shows how the system in- terprets the specifications in ASL and infer mela-properties from the basic semantic definition of predicates. In Sec- tion 4, we demonstrate how to express domain knowledge in ASL and how the inference system can greatly reduce the mining space. Section 5 concludes the paper.

2 The FARM Framework  We use the term mining camp to provide the context in which patterns are discovered. The context includes the length of the pattern (as in existing approaches), the group- ing attributes, and the itemizing attributes.

Delhition 1 A mining camp is a triple (n ,  G ,  S) where n is number of records in a pattern, G is a set of grouping attributes, S is a set of iremizing attributes, and G 0 S = 0, s # 0.

Next, we formalize the notion of a pattern. There are several parts to this. First, note that two records occur in the same grouping if their G attributes have the same value. Let r E D. We use the notation r G ( r )  to indicate the values of r that correspond to the attributes of G .

Definition 2 Given a mining camp (n,  G, S)  where S = (SI, . . . , S,,,}. A panern component or item is a sequence of attribute values sv = (SI.. . . ,sm) where s; E S, for 1 5 i 5 m. We call p = { S D I , .  . . , svn]  a panern for this mining camp ifeach sv, is a pattern component for S.

An instance of a pattern is a set of records whose values of grouping attributes agree and whose itemizing attributes match those in the pattern.

Delhition 3 Let p = {SUI, . . . , sun} be a pattern in min- ing camp (n,  G ,  S) and let D be a set of recods. An in- stance ofpartern p is a set of n records R = {n, .  . . , r,} such that T, E D and lis(r;) = sw; for 1 5 i 5 n, and r; and r ,  are G-equivalenr for all r; , rj E R.

Note that if G and S are fixed, then we have the tradi- tional fixed attribute data mining problem. Here, downward closure of the pattern length is used to look for those pat- terns in ( n  + 1,G, S) for which there is sufficient support in (n,  G ,  S).

FARM defines a rich set of interrelationships among dif- ferent mining camps. Consider a mining camp (n,G, S), and attribute Ai 6 S. Let p be a pattern in (n, G ,  S). Now consider(n+ l , G , S U { A , } ) .  Ifpisasub-patternofp' in this second mining camp, then every occurrence of p' in this camp is also an occurrence of p in the first camp.

The foregoing suggests that mining camps can be or- dered in a way that relates to the downward closure prop- erty.

Delinition 4 Given a mining camp C = (n,  G, S) and on attribute A; G U S then  1. ( n  + 1, G ,  S)  is the lype-1 successor of C.

2. (n, G U { A , ] ,  S)  is a fype-2 successor of 6.

Figure 2. Search Space MS(1, {TI ,  {I) for at- tribute set {T, A, B }  3. (n ,  G,  S U { A ; ) )  is a ope-3 successor o jC.

We use succ(C,, C,) 10 denote C, is a successor ojC,.

Figure 2 depicts the predecessor and successor relation- ships present among different mining camps. The root pre- cedes all other mining camps. (In this case, it is not a real camp since S = 0.)  3 Attribute Specification Language  In this section, we present the Attribute Specification Language (ASL) designed to express domain knowledge for the underlying relational data in mining. The language is small and easy to comprehend, at the same time it allows most types of domain knowledge to be specified easily. It is also a high level specification which hides the inference mechanism behind it. The ASL is essentially a set of pred- icates that specify the properties of the attributes. An at- tribute specification is a set of ground atoms (well-formed formulae without connectives).

A mining camp essentially specifies a way IO partition all attributes to three sets: G: the grouping attributes, S: the itemizing attributes. and 0 the rest. The main focus of the ASL is to specify whether a partition conforms to users' interest hence the corresponding mining camps should be mined.

3.1 Defining Predicates  The building blocks of the ASL are predicates. A defini- tion of a predicate containing the following two parts.

1. Its basic semantics; or allowed mnge.

2. Its meta-properties.

We will discuss the basic semantics in this subsection and meta-properties in Section 3.3.

In ASL. the built-in ASL predicates (predefined popular predicates) and user-defined predicates are defined in ex- actly the same way. Now we show how to define the seman- tics of a predicate. First, we define the signaturefuncrion.

Definition 5 Assume (G, S, 0) is a partition of all at- rribures, A is an arbirrary attribute set. The signaturefunc- lion sag is dejined as:  o i f A C O gs go so  $ A  C GUS, A n G  # 0 and A n  S # 0 f A  C G U 0, A n  G # 0 andA n 0 # 0 i fA  C S U 0. A n  S f 0 andA n 0 f 0  gso i f A i G # 0 , A n S # 0 a n d A n O # 0  When there is no constraint, the signature of an attribute set can be any of the 7 values. A specification is a restriction on the image of the signature function. The predicates of ASL is defined by their allowed range.

Definition 6 Let T = {g, s, o,  gs, go, so, gso). The al- lowed range of an n-arity predicate is a pmper subset of the product set T".

For example, the allowed range of a unary predicate is a proper subset of T, the allowed range of a binary predicate is a proper subset of T x T.

Built-in uniary predicates includes the following:  ignore(A) I sig(A) E { o }  means attribute set A has very little significance in analysis and should be com- pletely left out. For example, attributes with unique values (Rec in Figure l),  and other numerical attributes that are not appropriate for frequent itemset mining can be ignored.

togelher(A) = sig(A) E {g, s,o} means no subset of attribute set A can be used independently. In other words, all attributes in A together forms an atomic se- mantic unit.

0 item-only(A) I sig(A) E {s,o,so) specifies thatat- tributes in A, when used, should only be used as item- izing attributes.

0 grmp.mly(A) I sig(A) E {g,o,go} specifies that attributes in A, when used, should only be used as grouping attributes.

0 always.group(A) I sig(A) E {g}  means A should  alwaysitem(A) 3 sig(A) E {s) means A should  always be included as grouping attributes.

always be included as grouping attributes.

ignore together itemanly group-only always.group alwayeitem  g 5 0 gs go so gso . . . .  . .  . . . . .

Figure 3. Allowed ranges of built-in unary predicates  Users can choose any of the 2? - 1 (excluding the empty set) subset of T to define new predicates.

Unary predicates can be defined in a truth-table-like manner. Figure 3 shows the graphical definition of ASL built-in unary predicates. The symbol indicates the al- lowed values.

An atomic formula p ( A )  conforms to a partition (G,S,O)isdenotedas(G,S,O) + p ( A ) .

and A = { c , d ) .  then s ig(A)  = s. Looking at the s-column of Figure 3, we conclude that together(A), i ternanly(A),  andalways.itern(A) hold bur ignore(A), g r m p a l y ( A ) ,  and always.grmp(A) do nor hold.

nary predicates includes the following:  E-ple1 Assum (G,s,O) = ( I a , b ) , ( c , d } , { e , f } )  Binary predicates are defined similarly. The built-in bi-  I .  decide(A1, A2) specifies functional dependency: the value of attribute set A1 uniquely determines the value of attribute set Az .  If A I  c S and ai E S n A2. then the value of a;  can be inferred by the functional de- pendency, hence the result of a corresponding mining camp can be derived from the mining camp that has identical configuration except a ,  is  removed from S.  If A I  C_ G, then no member of Az can be in G because it would be redundant; also, no member of A2 can be in S either because when a; E S n A2.  the value of a;  is different in each transaction hence the support can not exceed 1.

2. follow(A1,Az) specifies those attributes that by themselves are insufficient to form an independent se- mantic unit, and thus must be combined with other at- tributes. For example, attribute C i t y  alone does not provide sufficient information for the location. There are six Orange countieslcities and 24 Springfieldcities in the US. To avoid this ambiguity, users can simply specify f o l l m ( { C i t y } ,  { S t a t e } ) .  With this, State canbeusedfreelybut wheneverCityisused.State must be used as well.

3 .  repel(A1,Az) means if A I  c G, then Az n G = 0 and if A I  c S, the A2 n S = 0. The typical  use of repel is when Az overshadow A I .  For ex- ample, in Figure 1, Intrvl5 is a finer division of time than Intrvl30, hence it is wise to specify repe l ( (h t rv l tO} ,  (lntrvl5)).

Binary predicates can also be defined by the truth-table- like notation as shown in Figure 4.

3.2 Infering Downward Closure Property  To find and utilize the downward closure property, a.k.a a priori property, is a focal topic of frequent itemset mining study. In short, the question is that when a mining camp violates the specified constraints and the system decides to skip it, whether all its successors should be skipped as well.

Definition 7 Let (G?, S?, 0?) be rhe parrifion of a mining camp rhnt is a successor ofa mining camp with (G, S, 0) partition. An aromic specification p( x) is downward closed ifforanypanition (G,S,O) p ( A )  ==+ (G?,S?,O?) P ( 4 .

Note the downward closeness is defined on the negative side of specifications. To infer the downward closure prop- erty. we have to start from predicting the sigature of an at- tribute set in successor mining camps.

Definition 8 Given an attribure ser A, the immediate suc- cessor function SUE : T - 2* maps the signarure of A in a mining camp to all possible signatures of A in the successor mining camps.

Recall that the only possible change in (G, S, 0) divi- sion from a mining camp to its successors is to move one element in 0 to either G or S, so we have the following.

succ(0) = ( g , s , o , g o , s o ) succ(g) = ( d succ(s) = (s}  suE(go)  = I s ,go ,gs ,gso } succ(s0) = { s , g s , s o , g s o } succ(gs) = ( g s }  succ(gs0) = {gs ,gso )  The graphical representation of the SUE function is shown in Figure 5 .  With this, we can define the transitive closure function SUCC? of the immediate successor function.

SUE*( . )  = 10, go, so, 9 .  s, gso, g s ) SucC*(g) = ( g } succ.(s) = {s}     g s o gs go so gso . . . . . . . . . .  . . . . . . .  . . . . . . .  . . . . . . .  . . . . . . .

(a) decide  g s 0 gs go so gso K 1 .  . . . . . . .  . . . . . . .  . . . . . . .  . . . . . . .  . . . . . . .

(b) follow  Figure 4. Binary predicates  g s 0 gs go so gso . .  . . . . . . . . .  . . .  . .

( C l  =pel  The lemma can be extended to predicates of any arity and hence the following main theorem of this work holds.

Theorem 1 An nary predicate p defutd os p ( A ) ( s i g ( A I ) , . . . , s i g ( A , ) )  E D c T" isdownwordclosedif  foreveryd= ( d l , . . . , d , )  E Dandanyd 'E  succ'(d1) x . . .  x succ*(d,). d' E b.

3.3 Meta-predicates  For each medicate. beside the basic semantics defined its  Figure 5. Signature Transltion allowed range, other logic rules can be defined to extend the scope of the predicate. For example, given decide(A I ,  Az) , deSde(As,A,) ,  and AS c Az, we would l i e  to de-  succ'(g0) = {gso ,  go ,g ,  g s }  duce decide(Al,A4).  In this subsection, we show this SUCC*(SO)  = { s o , g s o , s , g s }  deduction can partially done automatically, e.g. inferring  decide(A1, A3). and partially relies on users to define the propenies fotthe predicate.

Assume A = { A l , .  . .  ,An} and 1 5 k 5 n. Let sub(J, k, A ; )  be the result of replacing 2s k-th element with A;. Some Common meta-ProPenies include:  I .  transitivity T R ( p )  = p ( A 1 , A 2 )  A p(A2,A3)  -+  succ*(gs) = { g s } succ'(gs0) = {gso ,gs }  From the above definitions. now we are ready to ap- proach the main theorem of this study. To make it easier to understand, we start from unary predicates.

Lemma1 A unary predicate p defined as p ( A )  I p ( A L  -43).

s ig(A)  E D isdownwardclosedifforevery d E D, where b is fhe complemenr set of D. succ'(d) C D. 2. S y ~ e t r y S Y ( p )  = p ( A l , A z )  -+ p(A2 ,Al ) .

Or more intuitively, a unary predicate is downward closed if and only if once the signature of an attribute set falls out of its allowed range in a partition, it will remain so in the successor partition.

Example2 I. i t e m m l y  is a downward closed pmp- erq. By definifion, i t e m m l y ( A )  I s ig(A)  E {s,o,so] = D, then D = {g ,gs ,go ,gso} .  So  U{succ'(d)ld E D }  = {g ,gs ,gso ,go} }  c b 2. a lways i t em is nor a downward closed pmpeny. Bq  defrnirion, aluraysitem(A) I s ig(A)  E {s} = D, thenD = {g ,o ,gs ,go , so ,gso} .  So  U{succ*(d) ld  E D )  = { g , s , o , g s , g o , s o , g s o } }  e D.

3. Ascending Closeness ACt(p )  = p(A) -+ p(sub(X,k ,  At  U X ) )  for any attribute set X .

p(sub(A, k, A t  \ X ) )  for any attribute set X.

4. Descending Closeness D C t ( p )  = p ( A )  -+  The system can infer some of the properties. A predicate is symmetric if its uuth-table-like representation remains the same under transposition. Ascending and descending closeness can be inferred as well. We illustrate the infer- ence on unary predicates. Similar process can be applied to n-ary predicates.

Figure 6 shows when given the signature of A,  what might bethesignatureofAU{at} and A\{at}. The signa- ture of A U {at) is either the same or another value pointed by an upward link. The signature of A \ {at} is either the     Figure 6. Signature changes in inclusion and exclusion  same or another value pointed by an downward link, If the allowed range of a predicate is upward-complete, i.e. any value pointed hy a upward link from a value in the allowed range is also in the allowed range, then the predicate has the ascending closeness property. If the allowed range of a predicate is downward-complete, i.e. any value pointed by a downward link from a value in the allowed range is also in the allowed range, then the predicate has the descending closeness property.

However, transitivity can not be inferred automatically.

So such properties, e.g. TR(decide), have to he spec- ified while defining the predicate. Meta-properties are translated to fint order formulae. Then common infer- ence methods can infer the semantic closure of a set of constraints. In the example discussed in the begin- ning of this subsection, DCz(decide) can be proven au- tomatically hence decide(A,,AS) can be deduced from decide(A1, Az). With TR(decide) and decide(As,Ad), decide(A1,Al)  can be proven. We call the set of all constraints deducible from users? specification, such as decide(A1, Ad) in this case, the semantic closure of users? specification.

With all the definitions and theorems introduced so far, to decide a mining camp is now obvious. If a mining camp vi- olates any comtraint in the semantic closure of users? spec- ification, skip this mining camp. Ifthe violated predicate is downward closed, then skip all its successor as well.

4 Casestudy  In this section, we demonstrate how the problem size of the example in Section 1 can be greatly reduced. The data set in the example contains 13 attributes. If no constraint is applied, there are 1,586,131(= 313 - ZI3) possible map- pings. Exhausted mining is obvious not a feasible option.

However, manually selecting mappings is not a good option either because it is tedious and prone to miss some inter-  ignore( { Rec)) ignore ({Time}) together((EventType, Maint}) follou({Date}, ( A M I P M } ) fOllOW((/ntrv/5, rntrul3o}, {AMIPM)) grouponly( (Date ,  Intrvl5, Intrul30, AMIPM}) item.only((EventType, Source, Subsource, Severity}) decide({EventType), (Subsource}) decide((Subsource}, {Source}) decide((EuentType}, {Severity}) decide((Host}, (Site}) repel({lntrvl30}, {lntrvl5})  Figure 7. Specification System Management Events  esting mappings. With ASL, domain experts can state the constraints in Figure 7  As stated before, domain experts can specify their knowledge in a casual way without thinking tcw much about the implication. The inference system will find the semantic closure of the specification as true in- tention of the domain experts. So many different specifications may have the same intention. For ex- ample, fo l lm({In t rv /5 ,  Intrwl30}, {AMIPM}) can be broken up to two clauses or merge with follozu((Date}, (AMIPM})  and become f o l l m ( ( D a t e ,  Intrvl5, IntrvlJO}, (AMIPM}) .  With group-only({ Date, Intrul5, Intrvl30, A M I P M } ) already specified, repel((lntrvl5,  IntrulBO}) has the same effect as decide({Intrvl5, Intrvl30}).

The reduction of mining space is shown in Figure 8. The data set has 13 attributes so levels above 14 all have the Same number of mining camps. The table shows the num- bers of mining camps for the cases of exhausting all mining camps, ignoring 2 attributes, ignoring 3 attributes. and ap- plying the specification in Figure I.

With the help of domain knowledge, the maximal num- ber of mining camps of a level is reduced from 527,345 to 321. The problem becomes solvable and the resulting pat- terns have better chance to he interpreted and responded to.

The inference system can he used alone without actually mining data; it can list all the remaining mining camps in the mining space for user to specify more domain knowl- edge for further reduction.

5 Conclusion  Advances in association rule mining has come the point to deal with common relational data with multiple at- tributes. However, the enormity of attribute mappings posts a severe challenge to multi-attribute mining on both the     Figure 8. Number of Mining Camps of each level  computational complexity and usability of the results. How- ever. minimal domain knowledge of the attributes can pro- vide tremendous reduction on the problem size while not losing any interesting patterns.

We presented a multi-attribute data mining framework with the capability of utilizing domain knowledge about at- tributes to in association rule discovery. The framework includes two parts. The first part is an attribute specifica- tion language, ASL, that allows users to specify what and how attributes should be used. ASL consists a set of predi- cates with simple and comprehensible semantics. The sec- ond part is an inference system that is responsible for de- termining whether a mapping of attributes to itemizing and grouping attributes may produce any interesting results. We presented a case study on system management event min- ing. With domain knowledge, we can transform the multi- attribute mining problem from being virtually impossible to solve to a reasonable problem size; and the results are more meaningful.

