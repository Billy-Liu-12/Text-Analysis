A Performance Analysis of MapReduce Task with Large Number of Files Dataset  in Big Data Using Hadoop

Abstract?Big Data is a huge amount of data that cannot be managed by the traditional data management system. Hadoop is a technological answer to Big Data. Hadoop Distributed File System (HDFS) and MapReduce programming model is used for storage and retrieval of the big data. The Tera Bytes size file can be easily stored on the HDFS and can be analyzed with MapReduce. This paper provides introduction to Hadoop HDFS and MapReduce for storing large number of files and retrieve information from these files. In this paper we present our experimental work done on Hadoop by applying a number of files as input to the system and then analyzing the performance of the Hadoop system. We have studied the amount of bytes written and read by the system and by the MapReduce. We have analyzed the behavior of the map method and the reduce method with increasing number of files and the amount of bytes written and read by these tasks.

Keywords-HDFS; Hadoop;  MapReduce; Terasort; Teragen; Teravalidate; Name Node; Secondary Name Node; Data Node; Job Tracker; Task Tracker;

I.  INTRODUCTION In this big technological environment the amount of data generated is increasing at a very big rate. Computer Engineers at European Council for Nuclear Research (CERN) announced that the CERN Data Centre has recorded over 100 Petabytes of physics data in the last 20 years. Experiment in the Large Hadron Collider (LHC) generated about 75 Petabytes of this data in the last three years. Storing it is a challenge [1]. With millions of users, Facebook is collecting a lot of data. Everything is interesting in some context. So it is useful to take advantage of this data [2]. Storage of this amount of data is difficult but distributed storage system provides an efficient scheme for management of this data. Distributed storage system can be used for storing this huge amount of data. There is lots of a distributed storage system currently available. There can be three states of the data. Data can be in structured form, semi structured form and unstructured form. The amount of  structured data is very less as compared to the semi structured and the unstructured data. Internet Data Center (IDC) has given statistics of the amount of structured and unstructured data present scenario and the predicted growth of the structured and the semi structured data. There are 3V first purposed for big data, Velocity, Veracity, and Volume.

This volume was the size of the file. What if the number of files increased which is to be handled by the Hadoop distributed file system?

Graph 1: IDC Estimates [4]  A distributed system should have access transparency, location transparency, concurrency transparency, failure transparency, heterogeneity, scalability, replication transparency, migration transparency; it should support fine- grained distribution of data tolerance for network partitioning it should provide a Distributed file system service for handling files. The main aim of a distributed file system (DFS) is to allow users of physically distributed Computers to share data and storage resources by using a common file system. DFS consists of multiple software components that are on multiple computers, but run as a single system. The power of distributed computing can clearly be seen in some of the most ubiquitous of modern applications. So for storage of the Big Data and for better management of the big data we can use the distributed file   DOI 10.1109/CSNT.2014.124    DOI 10.1109/CSNT.2014.124     system. For finding information from this huge and distributed data parallel processing can be used for big data processing Google?s Map Reduce [3] provides an effective framework for finding information from this data. Hadoop provides a complete management tool for the management of the big date. Hadoop distributed file system for storage of the data and the MapReduce for the retrieval of the relevant information from this data. It is known that the Hadoop framework works well on large file size. We have analyzed the performance of the Hadoop with small but number of files.



II.  RELATED WORK: Big data can be handled by using the Hadoop and MapReduce. The multiple node clusters can be set up using the Hadoop and MapReduce. Different size files can be stored in this cluster [5]. A shared disk analysis of the Hadoop is done [6]. File of size terabytes is generated and stored in the Hadoop cluster and analyzed Teragen, Terasort, Teravalidate is used for generating sorting and validating the tera file respectively. Big data can also be provided as a service [7] in cloud computing (Data-as- service). With this big data analysis becomes important in a cloud computing environment. For management of data there is a traditional database management system, appliance, Hadoop, In memory, and SSD. It is tested that Hadoop provides the best service in the context of cost, scalability and unstructured data [8].



III. HADOOP HDFS AND MAPREDUCE: Hadoop comes with its default distributed file system which is Hadoop distributed file system [9]. It stores file in blocks of 64 MB. It can store files of varying size from 100MB to GB, TB. Hadoop architecture contains the Name node, data nodes, secondary name node, Task tracker and job tracker.

Name node maintained the Metadata information about the block stored in the Hadoop distributed file system. Files are stored in blocks in a distributed manner. The Secondary name node does the work of maintaining the validity of the Name Node and updating the Name Node Information time to time. Data node actually stores the data. The Job Tracker actually receives the job from the user and split it into parts.

Job Tracker then assigns these split jobs to the Task Tracker. Task Tracker runs on the Data node they fetch the data from the data node and execute the task. They continuously talk to the Job Tracker. Job Tracker coordinates the job submitted by the user. Task Tracker has fixed number of the slots for running the tasks. The Job tracker selects the Task Tracker which has the free available slots. It is useful to choose the Task Tracker on the same rack where the data is stored this is known as rack awareness. With this inter rack bandwidth can be saved.

Figure 2 shows the arrangement of the different component of Hadoop on a single node. In this arrangement all the component Name Node, Secondary Name Node, Data Node, Job Tracker, and Task Tracker are on the same  system. The User submits its job in the form of MapReduce task. The data Node and the Task Tracker are on the same system so that the best speed for the read and write can be achieved.

Figure 2 Single Node Architecture of Hadoop   Google has given the MapReduce programming model [3] which is based on the merge sort. It takes a set of input key/value pairs, and produces a set of output key/value pair.

map (k1,v1)  ?  list(k2,v2) reduce (k2,list(v2)) ?  list(v2) Map when used in the HDFS first maps all the requested file blocks in the HDFS then reduces them according to the required result.

Figure3 MapReduce data flow with a single reduce task [10]

IV. WORD COUNT WordCount is a simple MapReduce allocation. It counts the number of times a word is repeated into a text file of an input set. It imports Path, conf.*, io.*, maped.*, util.* of Hadoop. The Mapper is implemented by a map method; one     line is processed at a time. The input can be strings or a text file. It splits the input into tokens, for separation it takes space as a tokenizer and uses the class String Tokenizer.

Output is in the form of <<word>, 1>. Different maps generate these key value pairs for example.  For input file having text like: Student college workshop Student Lab  Table 1 Output of the Mapper       The Reducer for these maps is implemented by the Reduce method. It takes input from all the maps and like in merge sort it combines the output of all the map and produce output as (< word>, n) Where n is the number of times the occurrence of that word in the input. The output of the reducer will be like (<Student>, 2) (<college>, 1) (<Lab>, 1) (<workshop>, 1)

V.  EXPERIMENTAL SETUP Dell Precision T3500 system is used with, Intel(R) Xeon(R) CPU W3565 @3.20GHz 3.20GHz, 4GB RAM, 64-bit.

Platform Ubuntu 12.04 is on it the installation size was 30GB. Installation of Ubuntu is done using the Wubi installer. At Hadoop website Hadoop-1.2.1-bin.tar is available for download. Hadoop is installed on the Linux file system. Single node architecture of Hadoop is used in which all components of the Hadoop are on the same node.

Hadoop requires JDK for its working. Java OpenJdk is installed on this system using the apt-get command. The running component of the Hadoop can be checked using the jps command the output of the command is shown in Figure 4. We can access the name node at. We can access that interface from http//:localhost:50070.

Figure 4 Jps Output  For checking the performance of the MapReduce and the copy of the files from the local file system to the HDFS we have used dataset of the lyrics of the English songs. Here we concentrate on the number of files not on the size of the file.

A Name node has to record the metadata for each file separately in its file system. As the number of file increase metadata about the files stored will be increase we had analyzed the behavior of the copy from local to HDFS and the map reduce task running on these files in a single node environment. Number of files are doubled in each operation we start our experiment with 499 files then we doubled this number each time till 7984 files.

Table 2 Experimental Output    The performance of the MapReduce task on the basis of the byte written, File bytes read, Reduce input records, have been recorded Table 2 shows the output of the experiment.

Graph 2 shows the scenario of the byte written with respect to the number of files in the Hadoop distributed file system.

Number of bytes written by the Map Reduce task does not increase with the rate at which the number of files is increasing.

Graph 2:  Bytes written   The reason is that when the reduce function reduces the map output it just combines the output of map reduce like in example two time student is saved with only a key value increase by one. The number file bytes read by the MapReduce task have been recorded. Graph 3 shows the behavior of the map reduce task for reading the file bytes read with increasing number of the files.

Graph 3 File Bytes Read by Mapreduce Task   The reduced input records by the MapReduce task also have been recorded. Graph 4 shows the Reduce input record with respect to the increasing number of files. These records do not increase with an increase in the number of files. This is because with increases input only the key values are increased by one each time. That result in this type of behavior. There can be similar types of keywords in the record so only there counter in increased and the number of bytes is saved for storage of the similar types of records. It?s also speed up the process of reducing. Reduce uses the byte records efficiently with this increase number of files.

Graph 4 Reduced input record  Table 2 shows the output of the experiments with respect to the launched map tasks, file bytes read, Bytes read and Map output records. The Map output and the bytes read increase with increasing number of the files. The wordcount Map Reduce task is divided into parts. Then Map analyzes the input files line by line. It tokenizes the file content on the basis of spaces. Map analyzes all words on the basis of this token. Map read the records as proportional to the input files. Map output the records with key values. Map output is also directly proportional to the input files to the map method of the MapReduce task.

Table 3 Experimental Output    Graph 5 represents that with the increased amount of files MapReduces? Map output record also increase after, tokenizing the data on the basis of space in wordcount application.

Graph 5: Bytes Read, Map output records with increased files

VI. IMPORTANT APPLICATION AND SUGGESTIONS Data is increasing at a very high rate it is important to have a managed and efficient way of dealing with this data. Data is coming from the heterogeneous resources.  Hadoop provides an efficient mean for managing  the Big data. Our work provides an analysis that how the Hadoop can deal with the different and the  number of files. It provides the behavior of  MapReduce architecture  with increasing number of files. We suggest that use of the MapReduce and HDFS for storing this type of data is efficient. Bytes are efficiently written and read  by the MapReduce programs. It can be used for analyzing the output of the files such as, the files generated by the weather sensors which are distributed in the environment, log files of the different systems.



VII. CONCLUSION  AND FUTURE WORK We have analyzed the performance of the map reduce task with the increase number of files. We have used the word count application of the Mapreduce for this analysis. The output shows that the Bytes written do not increase in the same proportion as compared to the amount of files increase.

For our next work of study we will establish a cluster of nodes and then analyze the behavior of the Mapreduce task with a number of files.This can be used in analyzing the files generated as logs. It can also be used for analyzing the sensor's output which are the number of files generated by the different sensing devices.

