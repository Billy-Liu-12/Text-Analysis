A FAST ALGORITHM ON DISCOVERY LARGE ITEMSETS

Abstract: Discovering large itemsets is the key problem of  algorithm for data mining. In this paper, the support vector of itemsets is presented. The algorithm LIG can prognosticate the capability of a large k-itemset extending a candidate k+l-itemset by calculating support vector, so the size of candidate set has been reduced and the efficiency of algorithm has been raised. As the number of items is very large, the size of candidate set is huge. Main memory can?t load all of candidate set. For reduce IIO swap, the algorithm builds the candidate hash tree based on the estimated support of candidate itemsets. The performance of algorithm is better.

Keywords: Data mining; Large itemsets; Algorithm  1 Introduction  Association rule plays an essential role in data mining [l1. Some algorithms for mining associations have been proposed in recent years. Apriori algorithm [?I is one of the best-known. There are three kind of design pattern: Apriori-like candidate set, Lattice-based algorithm [?I, and Han?s FP-tree [41. To discovery large itemsets is the key for all of them.

The subset of large itemset must be a large itemset [?I.

In Apriori, the large itemset Lk-1 found in the (k-1)th pass are used to generate candidate itemsets c k ,  then, Lk is generated from ck. The size of ck is much larger than &?S.

Most of algorithms are derived from Apriori. The common weakness of them is that the size of candidate set is too large. When number of items is huge, Cz is the bottleneck of algorithms. When the number of items grows to lOOk or lager, the efficiency of CPU will be down to zero. The disk T/O swaps is frequent for main memory can?t load all of candidate sets. In this paper,we propose a fast algorithm for discovering large itemsets. It improves the efficiency by two ways:  1. Gave a concept of support vector, to count itemsets? support in transactions of different size, form a support vector. Reduce the size of candidate set;  2. Optimised the candidate hash tree, to decide  nodes? location in hash tree by their support. The nodes accessed frequently can be preserved in main memory.

The rest of this paper is organized as follows: Section 2 is our theory and algorithm on support vector and performance study. In section 3, we describe an optimised hash tree, and evaluate the performance of the algorithm.

Finally, we conclude the paper in section 4.

2 The algorithm based on support vector  2.1 Basic theory and concept  Let ipD(I) is the support of itemset I in the dataset D, minsupport is the threshold of minimum support, IT/ represents the size of the transaction T, that is the number of items in T, TED. Let d be a itemset, /d/ is the size of d, that is the number of items in d.

Lemma 2.1 In dataset D, the support of the itemset I ? is independent of the transactions whose size is less than  (All of proofs are omitted in the paper.) Lemma 2.2 In dataset D, Given the large k-itemset I k ,  Let D?= {d/dED A Idl2m>k}, if ipD(&J<minsupport, then for a large m-itemset I, on D, there must be  Definition 2.1 Whenplk<q, s k  is the times of itemset Z occurring in k-size transactions; when k=q, s k  is the times of itemset I occurring in the transactions whose size is not  less than k. Here, sk=pD(I). The vector (sp,sp+l,. . ., s ~ . ~ ,  sq) is named as the support vector of itemset I .

Lk is the set of large k-itemsets,  LCk={itemseteLkl 2 itemset.s,>minsupport), LCk is named as the seed k-itemsets.

Lemma 2.3 Vector (ibik+,, . . .,is) and (jbjk+l,. . .&) are the support vector of large k-itemset itemset, and itemset,  PI-  Q I,.

k = p  Definition 2.2  i=k+l  0-7803-7865-2/03/$17.00 02003 IEEE     respectively, so f: min(ipjp)2minsupport and )itemseti n itemsetjl=k-1 are the necessary condition that the itemset generated by itemseti and itemsetj is the element of c k + / .

p = k + l  2.2 Algorithm LIG  Now we give the LIG algorithm for discovering large itemsets. According to lemma 2.1, while passing the data to determine the set of large k-itemsets, the algorithm can delete the k-size transactions from the data. Lk is the set of large k-itemsets, L C d k  is the set of seed k-itemsets, the algorithm generates candidate set Ck+l fiom Lek, not from L k .  Using the support vector can reduce the size of L e k .

LIG algorithm is as follows:  (6) if @>k) then Lck=Lcku (C); (7) end (8) end LIG algorithm generates the candidate itemsets using  the seed itemsets, but not the large itemsets. Since the size of the seed set is less than the size of the large set, so the size of the candidate set is less, and LIG is faster.

Lemma 2.3 strengthens the restrict conditions for generating candidate set; this can reduce the size of the candidate set.

JOIN function: if (k>2) then  insert into Ck select p.iteml,p.itemz,. . .,p.itemk+q.itemk.I from LCk-1 P,LCk-I q,LCz s  where (p.iteml=q.iteml ,. . . ,p.itemkS2=q.itemk- ,,p.itemk-,  Cl={ candidate 1 -itemset); L1=(large 1-itemset);  <q. itemk- ,p . item,. I =s .iteml ,q. itemk- I =s .itemz)  4 and 2 min(p.r,q.r,s.r)>minsupport; i=2 else  LC1={itemsetEL1l itemset.si2minsupport); r=k  D'=D-{ 1-itemset}; insert into Ck for (k=2, initializing q;ILCk-lI>l;k++,q++) do  Ck=JOIN(LCk.,); // New candidates  select p . iteml ,p . item2 from LCk-1 &Ck-l  where (p.iteml<q.iteml) forall transactions tE D' do begin  Ct=subset(Ck,t); // Candidates contained in t  if (r>q) then I-q; 4 ; (1 0) (11)  ( 12) (13) c.sr++; (14) end (1 5) LCk=limit_gen(k,Ck); // Generating Lk and LCk (1 6) end  if ( F k )  then D'-=t;// Deleting k-size transactions  forall candidates c E Ct do  (17) L = u  kLk;  Based on lemma 2.2, when si<minsupport, m is a  up-limit size of candidate. Also we can using it to determine the seed set.

i=m  The limitsen function: (1) forall ceck do begin (2) for (p=q,sum=c.s,;sum<minsupport and p>k,p--)  sum+=c.s,; (4) if (sum2minsupport) then begin ( 5 )  do  Lk=LkU {c); // The set of large itemsets  and f: min(p.r,q.r)>minsupport; itemsets, it is necessary to take it as joining condition.

r=k  Since the seed 2-itemset is the bridge linking two k-  2.3 Deciding the length of support vector  In LIG algorithm, each itemset has a support vector.

For a k-itemset I, its support vector is (s,@k+l,. . .,sq-~,sq).

We'd better let the length of support vector as big as we want. In the dataset D of transactions, the size of the maximal transactions is len, if let the length of support vector be len, LIG will perform best. But in general, len is very large, the number of itemsets is huge, it will take large space of main memory. The length of transaction is given from a Poisson distribution[21, let mean be til, we can let the length of support vector be Iil+ATI. For example, given a database with the size of maximal transactions being 10 and average size of transactions being 5 ,  if we let the length of support vector be 7, the support vector of 1-itemset for the first pass will be (sI,s2,. . . , S ~ , S ~ ~ ) ,  the support vector of 2-itemset for the second pass will be (s2,s3, . . .,s7,sa),etc.

In the same way, we can set the low-limit length of     the support vector. For example, given a database with the size of maximal transactions being 20 and average size of transactions being 10, let the up-limit length of support vector be 13, then the support vector of 1-itemset for the first pass will be (sI,sz, . . . , S , ~ , S ~ , ~ ) .  However, since few short transactions contain this itemset, if the support vector of Z, is (0,0,0,0,0,2,5,7,3,. . .) and the support vector of Z2 is (0,0,0,0,1,6,3,4,8 ,... ), when joining these two itemsets to generate candidate itemset, we get a united support vector (0,0,0,0,0,2,3,4,3 ,...) in which the front sections are all zero. To reduce the space cost, we merge the front sections of vector, and set a low-limit length, If the average size of transactions is 14, we may set the low- limit length of support vector 171-AT2. In this situation, we define the support vector again: For a k-itemset I, its support vector is (sb sp, s,+l,. . .,sq-,,sq), when i=k, si is the times that itemset I presented in transactions whose size is equal to i; when i=p, s, is the times that itemset Ipresented in transactions whose size is bigger than k but not bigger than i; when q<Zq, s, is the times that itemset Z presented in transactions whose size is equal to i; when i=q, s, is the times that itemset I presented in transactions whose size is not less then i. In this way, the low-limit length of the support vector in the former example may be set 7, then the support vector for the first pass will be (s,,sg,, sa,. . . ,Sl ,>S2,3).

Pass 2.4 Performance Apriori algorithm 1 LIG algorithm  the size of I the size 1 the size of I the size  2.4.1 Example  Table 2-1 as an example, where minsupport is 2.

13 10 11 6 13 2 8 1 13 0 4 0    Table 2- 1 Database D ~ Tid I Items I Tid I Items I Tid I Items 100 I 2  I 150 I 4,8 I 120 I 1,3,7,9 I 170 1 1,5,7 130 1,3,5,9 180 1,3,4,5,9 140 1.5.6.9 190 3.5.7  I 220 I 1,5,7 I After first passing over the data, the algorithm gets  the set of large 1-itemset L1, the support vector of itemset (2) in LI is (l,l,O,O), according to lemma 2.2, it can not be extended, it isn't the member of the seed set LC].

Meanwhile, the algorithm deletes the transactions whose size is 1 in the data.

Using the JOIN function, the seed itemsets LC, are used to generate the candidate itemsets Cz, itemset (3 ) and (4) are the element of the seed itemsets LC,, the support vector of them are (0,0,1,3) and (0,3,0,1) respectively, so the united support of two is (O,O,O,l). this  don't meet minsupport. {3,4} isn't the member of C,. In the same way, {1,4},{1,8>,{3,8>,{4,7),(4,8),(4,93,(7,8},{7,9} and (899) aren't in C,. Then, using limit-gen function, the algorithm generates L2 and LC2, since the support vector of itemset ( 4 3 )  in L2 is (l,O,O,l), it isn't in the seed set LC2.

The seed itemsets LC2 are used to generate the candidate itemsets C3, the subset {7,9} of { 1,7,9} isn't in LC2, so {1,7,9} isn't contained C3, the same to {3,7,9} and {5,7,9}; the subset {3,7) of {1,3,7} is in LC2, but the united support of all its subset {1,3},{1,7} and {3,7} is (O,l,O), this don't meet the minimum support 2, so { 1,3,7} isn't the element. Also, {3,5,7} isn't in the potentially large 3-itemsets c3. so  c3 is  algorithm passes the data to count the support vector of each itemset in C3, while deleting the 3-size transactions in the data. Then, using l imitsen function, the algorithm generates L3 and LC3, since the support vector of itemset { 1,5,7} in L3 is (2,0,0,0), it isn't in the seed set LC3.and so no.

Using this example, we compare LIG algorithm with Apriori algorithm, the result is given in table 2-2. The table of result shows LIG algorithm reduced both the size of the data and the candidate itemsets effectively, improved'the efficiency of algorithm.

{ { 1,395 >,{ 1,3,9},{ 1,5,7},{ 1,5,9),{3,5,9 11. Then, the  I database I o f c k  I database 1 o f c k 1 I 13 I 36 I 13 I 11  2.4.2 Experimental Evaluation  In order to study the performance the algorithm mining rules in very large database, we make a lot of experiments. All the experiments are performed on a Dell- Edage24001667 server with 256 megabytes main memory, running on Microsoft Windows 2000 advanced server, SQL Server7.0 DBMS. All the programs are written in MicrosoftNisual C++ 6.0, access the database in the way of ADO.

The synthetic data sets were generated using the procedure described in [2].

2100 . g  I I200 .

900 .

600.

200 IS0  I 0 0  0.50 0.40 0.20 0.15 010  (a) N=100  So00  + Apriori  3SW  3 w o   2wo  so00 , I + Apriori Y4  2.00 1.50 1.00 O S 0  0 20  (b) N=500  n q b e r  of item  100.00 Ik Sk IOk 2Ok  (c)candidate 2-itemsets  Figure 2.2 Execution result  As the minimum support decreases, the execution times of both Apriori algorithm and LIG algorithm because of increases in the total number of candidate and large itemsets. The performance of LIG algorithm is better than Apriori Algorithm, especially when the threshold is small, the number of large itemsets increases, or the number of items is very large, LIG algorithm beats Apriori algorithm by more than an order of magnitude.

Figure 2.2a and Figure 2.2b compare the execution times for decreasing values of minimum support. The number of items in Figure 2.2a is 100, in Figure 2.2b is 500, clearly, the effect of LIG algorithm is better when the number of items is large and the threshold is small. In the actual application, the number of items is usually very large (such as the kind of commodity in the supermarket).

For different support, the performance of LIG algorithm is steady, and there isn't abnormity. Figure 2 . 2 ~  compares the size of the set of candidate 2-itemsets, the number of candidate 2-itemsets using Apriori algorithm increase exponentially with the number of items, LIG algorithm solves this problem to a certain extent, especially when the number of items is very large, the algorithm reduces U 0  cost markedly. Number of transactions of database has reduced while pass.

The experiment result shows that the number of items is more, the effect of LIG algorithm is better, and the size of the set of candidate 2-itemsets is reduced obviously.

3 Optimizing candidate hash tree  Candidate itemsets of Apriori ['I are storied in the Hash tree. Park put forward an algorithm basing on Hash tree to make the large itemsets [51. The document has been inquired into the Hash tree problem hrther. All these methods are based on the premises that the hash tree is stored completely in RAM, and not involve the problem of disc YO. By way of the experiment, we find that frequent disc I/O declined the utilization rate of CPU when number of item is increasing and the size of the candidate set is large (specially the candidate set 2) for they cannot be stored completely in the ROM. At the odious circumstances, the utilization ratio of CPU is close to zero; therefore reducing U 0  spending becoming the important way that raises the algorithm capability.

There are two ways to solve the mentioned above issue, the first is concept concluding, using the thought of multiple level association rules to reducing the number of items. Another kind of scheme is improving the space dedicating method, and reduces disc I/O's times. Therefore we decide to optimise the candidate Hash tree and the management with the memory.

We find the itemsets as for candidate in the hash tree, when the passing database to counts the supports; the itemsets with higher frequency have higher access frequencies. Based on this idea, we decide to foretell the access frequencies of itemsets in the candidate hash tree, and according to its frequency, we put the itemsets which will be accessed more frequent into memory, and put the itemsets rarely accessed into disk and switch them back to RAM when accessed. Then how to decide the frequent level of itemsets in the hash tree? As we known, every item in the candidate itemsets must belong to large 1- itemsets, so we can use the support of its first item to foretell the access frequency of itemsets in the candidate hash tree.

As for every item in the candidate set, we calculate its frequency with his the support of its first item. When building the candidate hash tree, order the itemsets in the hash tree by its support decreasingly, then put frequently accessed itemsets in RAM while the less frequently accessed itemsets into disc. For example, let's describe this procedure that building the candidate 2-itemset and computing the support.

Firstly, we built large 1-itemsets LI,  and then we order the itemset in LI according to the dimension of supports. We give every itemset a placement, and the precedence is continuously arranging, the placement of itemset i is for gi. The placement of the biggest support itemset in LI is I ,  and the placement of the second support itemset is 2,and so on. As we know, till now, our hash function use item i as parameter, named Hash(i), like i mod 2 . Now we decide to use a new hash function to let the items in the candidate hash tree distributed according to their supports, named Hash@). If the number of the items is N, for the candidate k-itemsets, it has k items altogether, and the scope is from 1 to N, and the scope of placement of all items in the large 1-set is from 1 to N. For example, we can define a hash function like this: Hash@)=@-1)lt , the size of Hash table is L,  t is the quotient of L divided by items number N.

After finishing the definition of hash function, we begin to build the hash tree of,candidate 2-set. Given any candidate 2-set c={Zi,Zj} ( Z H j  ), Zi is the first item of c, is a member of large 1-itemset L1. And the placement ofZi is Gi. The process of inserting itemset c into Hash tree is as follows: firstly apply the new hash function Hash(gi) to the root of the tree, that is to let gi decide the forestalled support of candidate set c .compare to others. Such a process puts the itemsets into hash tree ordered by its accessed frequency. When coming to the next layer, we use the original hash function, Hash(&) , and use the second item of itemset to select branch of tree. And the leaf layer is arrived at the next layer. Finally, we insert this  candidate itemset.

lOOk (a) 5k  .....

45000 .

40000 - 35000 .

30000 .

25000 .

20000 .

lS000 .

10000 ,  Figure 3.1  Then we let candidate itemsets accessed frequency be resident in memory, and others resident in disk. For example, if the size of the Hash table in root node is 9, then we can put the candidate itemsets, whose value in root node is from 1 to 6 into memory, and put the candidate itemset whose Hash value is from 7 to 9 to disk.

When passing database to counting the support of candidate itemsets, usually the itemsets in memory can be accessed frequently, and the itemsets in disk can be accessed rarely. When need to access the itemsets in disk, we swap them with the itemsets in memory whose accessed frequency is relatively lowest. This principle is obeyed by every swap. For a transaction t, the process that we search Hash tree to find all the candidate itemsets contained in it is as follow, at root node, we hash the order in L l  of every item in t to choose branch node. Reaching the internal node at next level, we go on to hash the next item itself, till arriving a leaf node, then find candidate itemsets contained in transaction t in this leaf node.

We compare the required space of optimised hush tree with traditional hush tree (given in figure 3.1). I/O times of optimised hush tree are markedly less. The relation between number of transactions and run time is      showed in figure 3.l(a). Number of transactions is from 5k to 100k.Minmum support is 0.5 and number of items is 1OOO.We show in figure 3.l(b) the run time between optimised and unoptimised for the T10.D10k.N1000 dataset. When minimum support is 0.7, size of candidate itemset 283619,almost all nodes of tree are storied in memory. When minimum support is 0.5, size of candidate itemset 474315,about half of nodes are storied in memory.Compare with traditional hush tree, the more nodes of tree are in RAM, the better is the performance of optimised hush tree.

The optimised hush tree rarely access disk because it put frequent accessed itemsets in RAM and rarely accessed in disk, which improve the performance of the algorithm. Since candidate 2-set's size is often very large and RAM is especially insufficient, the optimised hush tree performs even better in such situation.

4 Conclusions and Future Work  In this paper, we presented an algorithm LIG for discovering large itemsets, to solve the problems of huge number of candidate itemsets and frequent 110 operating.

First, we presented the concept of the support vector.

Since counting the support by the support vector, we could get more information while passing the database.

Therefore, it could predict the value of some itemsets as the potentially large itemsets, remove the itemsets whose superset is not large itemsets in time, and reduce the size of the candidate set, meanwhile reduce the scale of the database, and improve the efficiency of discovering large itemsets. LIG generated candidate itemsets from seed itemsets (but not large itemsets). Since the scale of seed set is smaller than that of corresponding large set, it reduced the size of candidate set generated. Furthermore we discuss how to decide the length of support vector.

Finally, we proposed the method of reducing disk 110 for the problem that candidate itemset can not fit in memory - the optimisation of candidate Hash tree. We decide their locations in the Hash tree based on the frequency of candidate itemsets, and the frequency of candidate itemsets is determined by the support of its first  item. Then we let the itemset that is more accessed frequency in the Hash tree resident in memory, and others resident in disk. So most work can be satisfied in memory when passing the database to count the support, and need rarely to access disk. The result of experiments showed that the optimised hash tree performed better.

The problem that remains in further to study have two, one is that the number of candidate itemsets is still large, we can use association-scope to reduce the size of candidate set further; the other one is that the frequency of candidate itemsets is predicted by the support of large 1- itemsets, though the result of experiments is well, we should study the corruption between transactions so that we could get a more accurate frequency of candidate itemsets.

