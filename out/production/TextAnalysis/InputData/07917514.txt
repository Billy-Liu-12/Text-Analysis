An Integrated Platform for Collecting Mobile Phone Data and Learning Demographic Features

Abstract?The problem of collecting, processing, and learning from high-volume mobile device data has become an active  research area in recent years. Time series data on application  usage, in particular promises to provide fine-grained information  on individual activity patterns, but currently poses collection and  analysis challenges. In this paper we demonstrate an integrated  system which can cheaply and easily collect application behavior  and survey data from mobile phones; we introduce several novel  features that assist the learning of individual level demographic  features (e.g., gender and age group). Specifically, our approach  for learning and inference for demographic features involves new  techniques: (i) decomposing the app usage from mobile phones  using spectral methods; (ii) learning spectral characteristics  associated with individuals using a training set; (iii) combining  other temporal features with learned spectral characteristics to  predict demographic features for out-of-sample individuals. The  core of our methodology is the utilization of spectral features  in cell phone app activity series, allowing both identification of  behavior patterns arising from particular types of cell phone apps  and leveraging of those patterns for demographic classification  and prediction. We demonstrate the effectiveness of our approach  with an application to real mobile app traffic data from the  United States.



I. INTRODUCTION  Inferring demographic information from mobile device users is a challenging problem; while user surveys are an option, these are intrusive, can suffer from low response rates, and as such cannot be administered to the entire user popu- lation. Because of the increasingly pervasive usage of mobile devices [1], inferring demographics from mobile activity itself offers the possibility for obtaining a much more accurate and time-resolved picture of population characteristics than would be possible using survey techniques. Such characteristics are of obvious interest for marketing or similar applications, as well as applications in areas such as public health or emergency management (where, on the one hand, targeting of messages to and inferring the sizes and populations of vulnerable popula- tions are of critical importance). In an application development context, software providers may likewise benefit from under- standing the changing composition of their user base (which may suggest avenues for improved service or support). Finally, effective inference for demographics from mobile devices would provide a powerful research tool, with the capacity to  obtain temporally and spatially resolved population data at low cost.

In pursuit of this objective, we here present a platform for (1) collecting anonymous survey and application usage data (the former from a subset of users), and (2) leveraging this data to infer demographic characteristics of a targeted user pop- ulation. This platform integrates three different components: mobile terminals which have installed SDKs to collect app usage behavior data and survey answers; local databases which temporally store the data collected by the SDK installed on the mobile phones; and backend servers, which get updated every 3 to 12 hours with the data uploaded from various local databases and host the modeling and learning process for demographics. Currently, app usage data is collected from more than a million of mobile devices globally. Survey data is collected from a portion of the devices which preinstalled our SDKs through out all the states in the US. Fig 1 briefly describes the data flows in the platform.



II. SYSTEM OVERVIEW  Our framework consists of three physical components: mo- bile device terminals; local databases; and backend servers.

This framework allows for the collection app usage data from mobile devices, pushing survey to the terminal de- vices/collecting survey answers from devices and training models to predict demographics at the server side. Future extensions can be implemented as, but not limited to: inte- grating the learned model to the device end and predicting demographics locally; providing an interface for externally extracting the prediction result (a feature with many potential marketing applications); and improving current models to be more computationally efficient to leverage the capacity for continuous collection of survey data streamed from spatially heterogeneous sources.

A. Data Collection  Data was collected by M2Catalyst from mobile devices with users? consent; currently, data collection is limited to Android devices. At the time of the study, our database consists of 47 thousand unique Android devices. App usage data is collected by the SDK installed in it. The SDK collects data by taking      Fig. 1: Different scenarios for generating an event record, from left to right: 1, when our app is installed; 2, when the app is started/ended; specially, when the app is running in foreground, the starting time stamp and ending time stamp are also recorded; if the app runs as a foreground for less than 15 seconds then this foreground session may not be detected or if the foreground session stopped for less than 15 seconds, then this event may not be detected and the SDK assumes the foreground runs continously; when the app is updated; when the app is uninstalled.

a snapshot of the phone every 15 secs and writes the results to the local file system approximately every 10 mins. For app usage on the phone, there are 7 different log events that can be recorded, as shown in Fig 2. For each event, an application ID is recorded together with event details. The application ID is an unique identifier based on package name, version code, and version name.

Since we already have the SDK installed on the phone, the distribution of surveys to the app users is easier. As a demonstration of the technology, we designed a survey con- taining 14 questions, mainly about demographics and family status, and it to participating users throughout all the states in the US. (Surveys and user interaction were performed by M2Catalyst.) Participation incentives were given to users who provided feedback on survey questions. More recently, we sent out a lightweight survey containing three questions: gender, age group, and ?whom do you think will win the presidential election?? Since the survey was very simple, participation was not incentivized. Based on the response rate we observed, we conclude that users are willing to participate in very short surveys without additional inventives. This provides an option for collecting survey data on targeted subjects in a quick and low cost way, compared with long-form questionnaires.

B. Data Storage and Extraction  Once the SDK is installed on the device, application related data is collected from the device. A record is created every time an event is detected on the device. These records of events are stored in a table created within the device. Number of records created for each device depends on the usage of the applications on that device. The data collected from these devices will be sent back to update the central filesystem.

There are cases when there is no connection to the central filesystem and the records will be accepted up to 15 days.

Fig 2 shows the overall file system.



III. DEMOGRAPHIC LEARNING AND PREDICTION  After we collect the app usage data and survey data, the demographic information, gender and age group, are learned by training machine learning models. Not like previous works, here we apply a spectral analysis to the app usage data which  1 2   Fig. 2: From left to right: mobile phone with pre-installed app; local file system which is on the mobile phone device; central file system. The pre-installed app contains the data collecting SDK which starts data collection process once it has been installed. It scans the phone for its status every 15 seconds and once an event happens, the app usage data is written into local file system which is on the device (step 1). With connection, an event data will be synchronized to the central file system instantly when it is created (step 2). After getting acknowledgement of successful receipt for the event data from the central file system (step 3), the record will be deleted. If there is no connection, event records will be kept up to 15 days and the records will be rejected.

is a direct representation for behaviors of the mobile device holder.

A. App Usage Data in Frequency Domain  Since the foreground time series is generated by human ac- tivity, its frequency domain structure encodes regular patterns of behavior associated with daily, weekly, or other activity cycles. Such patterns provide insight into the underlying characteristics of the device holder. In order to explore this periodic structure, we transform the app usage time series into frequency domain for analysis. Here we apply the fast Fourier transform (FFT), which is a simple and scalable technique for mapping discrete time series into the spectral domain. After applying the FFT to the original time series, we can obtain a decomposition of original activity series into a linear combination of periodic components. Phases and amplitudes of these components are then employed as features for subsequent analysis.



IV. IMPLEMENTATION As mentioned previously, 7 different types of events are  recorded. What we focus on here is the foreground app usage, which is a direct reflection of behaviors of the mobile phone holder. Before further analysis, the collected data we requires preprocessing; this is performed as follows.

A. Synchronizing Data  All event details for foreground app usage data are stored in UTC format. Since we employ app usage data as a behavioral indicator, whose social meaning is in the local time scale, it is necessary to synchronize system time and date with the local timezone. Besides the app data, we also collect and record timezone information whenever there is change in it within each device. When we synchronize an event for foreground app usage, we trace back to the latest timezone     change information for this device and synchronize the event log to the local time.

B. Aligning Data  We have 6152 unique mobile phone users in our current dataset and we extract one month app usage data for each of them. Because each user enters our database at a different time (based on when they installed our app), there is a need to align the app usage data across users. We set up a common starting time point pivot, which is a Monday 00:00, and a common ending time which is five weeks later from the common time point pivot at 24:00. In addition, there is no date associated with these two time point pivots (time is assessed relative to the start point for each user) and the time point pivot is useful for generating time series later.

C. Generating Time Series Data  The foreground app usage data can be represented as follow.

Let i be a unique device ID, and let T = {t1, t2, ..., tn} be the set of all time units. For each unique device ID i, we observe a corresponding time series Ai(T ) = {Ai(t1), Ai(t2), ..., Ai(tn)}. Since foreground app usage is directly generated by users, the time series of foreground app usage should have substantial periodic structure, reflecting the regular cycles of human activity generating it. This periodic structure provides a strong signal regarding the behavior pat- terns the user population. In addition, we expect that different subgroups of users will show different periodic structures, reflecting their different behavior patterns.

Currently, our data is resolved at intervals of 15 seconds in length. Based on the synchronized app usage event log data, we generate a binary time series in which ?1? indicates that at least one app is present on foreground within that 15 secs interval and otherwise ?0.? We generated one time series of this type for each of the unique device ID. Further, for each ID, we look for its earliest app usage date/time and figure out which day it is within a week. Then align this starting time to the corresponding date within the week of the pivot common starting time.

After we generate one time series for each ID based on the above strategy, every ID has a starting time within the first week of the common starting time pivot. Then we truncate the first week and keep the following time series for a period of four weeks (we extracted app usage data for five weeks before processing). This process is repeated for each unique ID.



V. RESULTS A. Description of the Dataset  1) Survey Data: The current dataset we currently use for this project include 6152 unique user IDs drawn from all US states. For each unique user, the ID is consistent for the app usage data. Right now we focus on demographic data on gender and age group. 4188 females and 1964 males completed the device-based survey. The age group definition and user distributions are shown in Table I.

age group age range number number female youngster [13,25) 1365 908  young [25,35) 1562 1122 middle age [35,50) 1631 1170  elder [50,120) 1594 988  TABLE I: Numbers for different groups in the dataset. Different ages are equally distributed in general, while more females took the survey for this dataset.

2) App Usage Data: We collected the app usage session data from all users participating in the survey. The total number of the unique apps used by these users is 13710, among which there are 2154 non-foreground/system apps.

For the rest 11556 apps, we classified these apps into 9 different categories: Communication (382), Education (312), Entertainment (1311), Game (3198), Lifestyle (1588), Refer- ence (1068), Social (297), Utility (3295) and Other (105).

There are 53 users who have no data under our 15 secs sampling rate so for the following learning and inference, we used the remaining 6099 as our pool.

Table II gives quantiles of time different groups spend on foreground apps. The unit scale here is 15 seconds. From this table, we observe that as age goes up, people tend to spend more time on their mobile phones and there is not much difference by gender on how much time individuals spend on apps.

age/gender group 0% 25% 50% 75% 100% youngster 1 1 41 1413.5 98911  young 1 1 197 2633 125044 middle age 1 15 498 3636.25 99069  elder 1 81 989 5840 106839 female 1 2 347 3443 125044 male 1 9 382 3336.5 90418  TABLE II: Quantiles of time for different groups spent on foreground apps.

The scale unit here is 15 secs. There is no substantial difference by gender for how long they spend their time on apps, while different age groups show obvious differences: older groups spend more time on apps. This difference may reflect a higher availability of free time for older users, and/or less competition from alternative activities.

The average proportion of time people spend on different categories is plotted in Fig 3. Younger users spend more time on apps in the categories ?utility? and ?entertainment,? while spending less of their mobile time on?games? compared with older users. Although some of this may reflect the repackaging of apps for younger users as ?education? (a category whose use declines with age), the counterintuitive nature of this pattern underscores the importance of employing direct measurement of demographic characteristics, rather than relying on prior assumptions.

B. Spectral Analysis of App Usage Data  We now proceed to an analysis of the app usage time series data. First we map all foreground app usage data from the time domain into the frequency domain. After obtaining spectra for all user IDs, Fig 4 shows the mean power distribution for the study population. There are 4 characteristic frequencies which have high power in the marginal distribution: daily, half-daily, 8 hourly and 6 hourly. These high power frequencies indicate     0.0  0.2  0.4  0.6  elder midage young youngster agegroups  va lue  categories  communication  education  entertainment  game  lifestyle  other  reference  social  utility  Fig. 3: Mean proportion values for different age groups spend on different categories of apps. The proportion is defined as the total time spend on one specific category of app divided by the total time spend on all foreground apps. From the plot we can figure out that younger people tend to spend more proportion of their mobile app time on categories ?entertainment,? ?utility,? and ?education,? and less time on ?games? compared with their elders.

1 ? 10+1  2 ? 10+1  3 ? 10+1  4 ? 10+1  7 2 1 0.5 0.33 0.25 Frequency (days/cycle)  PS D  (d B)  Fig. 4: Mean power spectral density for 6152 time series. Spikes are shown up at 4 typical human activity repeated patterns - daily , half daily, 8 hourly and 6 hourly. These typical high power spectral components match expected patterns of regular human behavior over a monthly time scale. Note that the amplitudes at each specific frequency are different, corresponding to differences in signal strength.

the dominant seasonal components associated with human app usage activities.

We obtain the typical amplitude and phase for the daily frequency for foreground app usage, with Fig 5 showing boxplots by different age group. Older groups tend to have a stronger diurnal signal (indicated by amplitude) with a more negative (i.e., earlier) phase than younger groups. This is compatible with general observations of differences in diurnal cycles over the life course. We also checked the amplitude and phase for differences by gender and, as before, we do not find a noteworthy gender effect.

C. Prediction for Demographic Features  Since spectral features are strong indicators of regular ac- tivity patterns associated with app usage, we select the typical amplitude and phase at daily cycle as predicting covariates.

Currently, we are working with the following human activity features as additional covariates.

For learning and predicting age groups, we choose: ? a_daily : amplitude of daily spectra; ? p_daily : phase of daily spectra; ? t_t : total time spent on mobile foreground apps;  0.0 2.5 5.0 7.5  10.0  elder midage young youngster age group  va lu  e  agegroup  elder  midage  young  youngster  ?2.0  0.0  2.0  elder midage young youngster age group  va lu  e  agegroup  elder  midage  young  youngster  Fig. 5: From top to bottom: log amplitude and phase plot from the daily cycles. Amplitude gives the signal strength, while phase indicates the relative starting time of the cycle; older groups show a consistently stronger diurnal pattern that begins earlier in the day than younger groups.

? p_uti : time proportion spent on category "utility"; ? p_ent : time proportion spent on category "entertain-  ment"; ? p_gam : time proportion spent on category "game".

For learning and predicting genders, we choose: ? a_daily : amplitude of daily spectra; ? p_daily : phase of daily spectra; ? p_com : time proportion spent on category "communica-  tion"; ? p_ent : time proportion spent on category "entertain-  ment"; ? p_gam : time proportion spent on category "game"; ? p_soc : time proportion spent on category "social".

In our work currently in progress, we perform inference on  the demographic features of age group and gender and test the results using 10-fold cross validation. We then examine the average of 10 random training/test splits. Each time, we randomly choose 5489 out of 6099 users to serve as our training set, and here we employ SVM for classification.

Preliminary results show fairly good accuracy for identifying younger versus older users, and further work is being done to explore the impact of kernel choice and feature selection on performance.

