

Unifying Decision Tree Induction and Association Based Classification Hongyan Liu', Je&y Xu Yu', Hongjun Lu3, Jim Chen'  ' "Depmen t  of Management Science and Engineering Tsinghua University, Haidian District, Beijing, 100084, China  {hyliuchcnj)@tsinghua.educn  'Department of System Engineering and Engineering Management Chinese University of Hong Kong, Hong Kon& China  yu@Se.CUhk.edU.hk  'Department of Computer Science Hong Kong VnivexsiQ of Science & Technology  Hong Kong, c h i luhj@cs.ust.hk  Keyword: ehsailicotion, dedaion tree, association rules, RDBMS  L nVraODUCllON Classificadon is a process of finding the comnan pmpcnies of data objects that belong to the s ~ m  class. It has a wide range of applications in d world. and has been extensively studied by rescarchm in various fields such as machine ICaming and statistics [17]. In the recent surge of data mining mearch, various classification techniques have bem re- examined. including decision tree induction [l3], Bayesian cIassifim [4]. neural networks 191, and etc. Among these methods, decision tree is still one of the most popular methods because of its fastertraining speed, reasonably good accuracy, and good scalability [IO, 14, 161. More recently. a new classification method based on ass&ations has betn developxi [Z, 3, 7, 8. 11, 15, IS]. The rationale of association-based classificatioo is rather intuitive. Given a wining dataset with n amibutes A I ,  A> ..., A. and a class label C, an association rule with.the class label as its consequent, alir a>,, ..., a* + cb (supl con/;), reprssents a classitkation rule, iI (AI = a],) and (A2 = olj) and _.. and (A.= om,) then class = c, (supb con/;). It is obvious that association based classifcation approaches aim at discovering all such association rules with the class label as  Mnsaluent From this viewpins association-based classifcarion is adopting an exhaustivc search shategy in finding classification rules. This is different fium decision wee classification when a decision tree is consttucted, we select an attribute to split at each intemal node based on the valw of certain goodness functions, such as information gain, OT gini ioda In other words, decision tree is collsrmcfed in a greedy manna. While b d s t i c s  based grady scroch can find near Opthnal solutions, exhaustive search should always find optimal solutions. This could be one of the reasons that association b a d  classification often leads to bigher accuracy wmpared to the decision tree method  By discovering all classification rules. association-based classification can i m p m  the classification accuracy to certain extent, and it also brings other issues. One of these issues is how to efficiently maintain and search from a large anmunt of rules. Most of existing association based classification mahods seem not addressing such issues very well and use unskudwed flat tile to managc thcse rules.

Urn, by flat-style. we mean there is no data skucture Iikc trees to mpporl efficient searching. A recent pmposal of association based decision me (ADP adapts a Ire srmelurc that organizes classification rules discovered based on their parmt-child relationship [IS]. A rule a,+c, is identified as a parent of ol,bl+cI. 'the purpose of using ADT is to make it easier to p m c  those redundant des by a pessimistic error estimation mnhod Although ADT plays an impoWnt rule in rule pruning, the ADT tree d o s  not impose the actual smcture on des. It still maintains every rule in di&rcnt tree nodes even though rules at a node have common attributes in their heads with their parent d e s  at their parent node.

'therefore the efficient manipulation the large n u m h  of rules remains an issue.

The above-mrntioned observations motivated the wxk reported in this paper: a generalized deciiion lrec (Om) that unifies the classical decision tree induction with the association-based classificatioa Gm, as the name implies, has the similar tree shllcfWT as decision Uas. I*I internal nodes dcterminc the splits on attribute values, and the leave nodes indicate the class to which a sample should classified Given an unseen sample, its class label can be detmnined by following the tree branches based on matching attribute values at the intemal nodes at each level of the tree.

H o w w ,  unlike decision trees rvhne only one amibute is selected to split at each intcmal node, an intemal node of GDT contains eneies for all attributes whose split will lead to meaningful classifration rules. In other words, just like association-based classification where all classification rules arc discovered, a GDT encodes all classification rules in a tree form. A path fmm the mot of a GDT to a leave node is equivalent to an association rule among the attniutes in the passing intemal nods and the class label at the leave node.

The benefits of using GDT can be summand . asfollowS.

.

A GDT W-b the result of sort of an exhaustive search of classification rules. It usually contains more wful classification rules found and dcscnbsd in a classical decision tree. which is expected to have highcr accuracyin comparison with decision trce method.

A common critique to any arhaustivc search method is its computational complexity. Evaluating attributes at each node to find good split is the main cost for decision tree conwuctl .on. To consrmct a GDT, wc can w the algorithm developed in mining sssaiation rules in large d a h  efficiently, cym with the support of relational database managemnt systems [SI.

On the other hand techniques developed in decision tree induction, such as treepruning, can bc applied to GDT, which solves one ofthe difficulties in association-bascd classification: debmining thc intemtingncss ofrules.

A GDT maintains classification rules as tree paths. It ov- the main difficulties in association-bascd classification of managiog large numbcr of rules. she cost of storage is r e d d  as rules with cormmn amibntes in their antecedent am shared through c o m n  sub-path in thc tree. Insmion, deletion and xarching a rule in a tree could bc implrmcnted efficientb.

f i e  remainder k t h c  papa is organized as ~OIIOWS. w o n  U brietly describes the concept of classical decision tree and some of its deficiencies. Section III gives the definition of Genaalized Decision Tra and corresponding algorithms to insert and search the tree. Our expcrimmgl d t s  arc presented in Section 1v. And finauy we conclude our work in won v.

IL DECISION AND AssoclAl lON RULES A dsision tree is a trec consisting of two rypss of nodes, namely, decision nodes (non-leafnodes) and class no& (leaf nodes), and edges. A decision node specifies a test to be carried out on a single attribute. An edge from a decision node is associated with an nttribuk value. Most existing algorithms c o n m a  decision trees in two phases, consmting the trre and pmning the tree. Givm a training dataset, a decision tree is constructed level by INCI in a top down manner. At each Iml, a greedy searching method is used to select the best splitting attributes based on certain goodness function. The attribute with the maximum goodness measure will be selected as the split attribute at a decision node. One m s t  widely used goodness measure is information gain [12,13].

Such a greedynature of the decision tree classification may cause some undesirable results. Let us consider an e x m e  example. Table I contains a simplified dataset for an insurance company where amibute, Risk is the class label.

Given such a da-t as the training dags*, a decision ba.

TI, consrmcted is as shown in Figun I .  Hen, information gain is used as the goodness measun when selscting the splitting attributes.

Note that forthe training dam shown in Table I .  because the information gain of the three auributes. Age-pup, Car- fype and Income, at the root am actually the same. mat is, any of them can be selected as the first split attribute. The decision trcc shown in Figun I is the result of selecting Age.

group as the first split attribute. If we chaose different attribute as the splitting attributes, we will have different  Table 1. Sample Trammg dafasn I Aer-group 1 Car-lype I Income I Risk I  ~ r r 1 . A D s i s i o n ~ ~ l ) f o r ~ i n T a b l o  1.

decision trecs. Figure 2 and Figure 3 sbow'altematives using Gr-rype  and Income as the first split attribute, respectively.

This example well illustrated the grady nature of the decision tree induction. Givm a dataset, a decision tree may only reflect part of the pmpenies rclatcd to classification. In other words, some classificalion rules could be neglected in a consbuctcd decision tree because of the selected splitting attributes. This can lead to some undesirable results.

Since SOM classification rules could missing fmm the decision trec constructed. For example, decision trecs TI, R, and T3 do not contain any classification information about Income, Age, and Car-type, respcdively. As such some unseen sample that can be classified using one tree may not be able to be classified if the tree used happens to be the one with necessaryclassification infomtion. For example.

given samples bung,  sport, low), or a sample with missing value (?, ?. low), we am unable to classify themusingTI. ButwithdecisiontreeT3,thcsetm samples can be classified as Risk= high.

For an unseen sample, the class label may not only depend on the attribute values, but also depend on the consmucting p m s  of the decision tree used. In other words, given the same training dataset and an uwen sample, the classification muh may be different depending on the decision tree used For example, for an unseen sample (old, sednn, low), it will be classified  . ., .I ~.

as Avg risk by decision tree TI (Figurel), as /ow risk by TZ (Figure 2). and as high risk by T3 (Figure 3).

Fl-2. Dccisionuul2 y$p$?-$ai avg low avg low  Flgmre 3. Dsision trc+ 'I3 If we appty the a~soeiation-bascd classification techniques.

and set the minimum support bcing 0% (for a dataset with small number of samples) and the minimum mnfidace to l00sb wc can obtain the following scf of classification rules.

The two fignres in the parenthesis are the confidence and support for the rule.

R,: UAge-grwp=okfthenRisk=ovg (lWh,33%) R2: If Agegroup = middle and Car-rYpe = sport, then Risk high (100%. 17%) R3: UAge-group = middle and Income = high, then Risk /ow (100%4 17%) RI: UAge-group =young and Cor-rupe = wn , then Risk high (100%. 17%) RI: UAge-group =young and Income = ovg , then Risk low (100%. 17%) R6: U clu-rype = sport and Income = ovg , then Risk ovg  (100%. 17%) R, :U Car-fype=sedan thenRisk=low (lOO%,33%) R8: UCar-rype = WJI and Income = high, then Risk = llvg  RgUIncom=IowthenRisk=high (l00%&,33%)  Comparing these set ofrules and the decision trees, we can sec that each decision me conshucted using decision tree based algorithms only covers five among the nine classification rules: TI covers (RI, R2, R3, R,, R>), R covers {RA L. Rb R,, 2%). and T3 covcxs (Rj .  RI. Rb Ra, Rd .  It is obvious that when we use this complete set of classification rules for classifying u n m  samples, we will not encounter the problem wc mentioned above.

While association base classification methods can fmd  more rules than a decision tree consmcted using an existing decision tree construction algorithm an astute  reader may realize that there could be a Iarge numher of rules given a training dataset. In this example, a six tuple training data generates nine rules. Such a large number of rules will obviously raise the issues of  computational efficiency.

(100%. 17%)  Ill. A GENauuza, DECISION TRe We have seen that both decision tree classification and association based classification have their own merits and deficiencies. In this section, we propose to generalize the classical decision tree to ovmome such deficiencies. At the same time, to address the issues raised for association-based classification.

A. GDT; 7hhe definition An examplc of genmlizcd decision tree (GDV for the sample d a w  in Table I is shown in Figure 4. Like decision trees, at each level of a GDT. the tree b d e s  out based on attribute values. The leave nodes of the tree arc class labels.

Unlike decision mcs, an intemal node contains entries for mo~c tban one attribute. In Figure 4. the mot node contains t h e  amibutcs wd the nodes in the nut level contain either one or two entries. There arc branches for each amibute.

We d&e a genrralizsd decision tree as follows:  Definition 3.1: A G a d  Decision Tree (GDV is an ordnsd tree coasisdng of two types of nodes: attribute node (non-leafnode) and class node (leaf-node). An amibute node contains a xt of attributes. An edge fium an amiute node to its child is associated with a pair of (attributmms attribute value). A path fmm the mot to a leaf node rrpresenrs a classification rule whose consequent is the label of the leaf node. The mot node of the tree must be an attribute node.

The tree shown in Figure 4 is a GDT foUowing the defdtion'. If we comparr Figure 4 with the set of rules dismvard by an association-based classification algorithm, we can sa that the GDT in faa eneodes all classifications in a tree form, and the nine rules correspmd to the nine paths in the tne.

-re 4. A GDT far the sample damet in Tabb I  Flprrs.AnomcrGDTfarmcsampkdatasaio Like decision trees, a number of diffmnt gendized decision trees can be conshuctd Figurc 5 depicts another  I The meaning of figurrs in the parenthesis will be explained later    GDT construned h m  the, sample datasct in Table I .  Note that, trees in Figure 4 and S look difkrat, as the ordcfing of amibutes in which the two tree were constructed is different.

However, the clsssification rules represented by the paths in the tree are the samc. This is an important propmy of the gennalized decision ha.

B. Constructing a GDT Since a GDT encodes all classification des, a stnightforward appmach to consbuct a GDT is to starl from mining association rules with consequent being a class label.

We will not discuss the details here since mining of such rules have bem well srudied In panicular. with the recent development in relational darabasc managmmt systems, association-based classification can discover all classification rules more efficiently than building decision k e a  [SI. A GDT can be consmctcd by inserting the discovered association rules into the tree. The major funeton we are going to discuss here is U) check whder  a rule is redundant acwrding to the delinition of  w i d o n  3.2: A d e s  R j  : % ..., +q (q cong). matches another rule, R; (ail. a, ..., c ?s (sup. wnf4,i/forevrryg(p=I.2 ,.... k)ofnJeR, % = %. r r u k  R j  marchw Rondq = c+ we say that n J e R  is rowredby% or& c o r n  R+  FM example, rule sedan + low matches but does not cover young, s& + high, while d e  low +high matches and covers rule young, low + high.

Dqidtion3.3: Givsnnvonr ler .~:  ail. au. .... % +q (supy confJ and R : ail. a, .... 4 +,i (supr umi4.w say &is redundant y 1. all the samples covered by % are covered by &,  2. all the samples matched by Rj are matched by Ri, and either the cod, > confj, or cod, = con$ but sup, > supp  or  Forexample, inTable I, Givens& +low,young, sedan +IOW b e M m  redundant  Lunma 3.1: Given ~ r )  rules, R, : ail. a, .._. % + cj (supI cod4 and R, : 31, .... a, + ci (supb cod$.

rule R coven R, then rule 5 is redundant.

When a classification rule is to be inscrtcd into a G M  trcc.

function IsRedwuimt, listed below, is called to check whether the rule is redudant, and damninc the i n d o n point.

Funelion IsRedunak~hf~ d e ,  T GDT) 1. begin 2.

3.

4.

5. rrtluoNuLL; 6.

7. end  T r a v w  T in depth first o&, Check if thm is a rule, r?, in T, covers r,  or matches I with higher cod. or sup. value; if such a rule,r?, exists then  else mtnm the end node position;  Function IsRdunahnt s tam from the mot and traverses the tree in a depth first o h .  Redundant rules arc discarded For a non-redundant rule, it r e m s  the insrrtion point, which is the end node of the last s h a d  commm prefix with an existing rule in the ordered tree. The insertion pmcedurc is simply to insert the Rmaining path for the d e  to be inserad into the GDT tree.

C. C l m ~ f i n g  an unseen sample using a GDT  Funccion SmrchGDTfl: GDT, t: sample) 1. begin 2.

3.

max confidence and support value; 4. cod.

travnse T in depth-fmt ordcr; find a rule in T that matches t; The d e  gives  Function h h G D T ,  outline3 above is used to classify a given sample t. The tree is searched h m  the root node in a depth-6rst orcler. The maximized confidence value and support value variables are initialized to be m. Class label is set to be the defauit class in case if t h m  is no rule found for tuple 1. When searching the tree, wl~enevcr a class node is found, its corresponding rule will be examined to check if it matches the tuple and has a higher confidence and suppofi value. If so, it beMmw the m t  k t  matching rule.

Note that for a GDT, a given sample may match more than one d e .  TbM is, the full tree needs to be searched to fmd the rule with highest maximum cnnfidence. To reduce the effort of search, fM each 8tUiibutc in the internal nodes. the maximm mnfidencc of the paths passing through the node is stored The numbers after the name of amibutes in Figure 4 arc such mfidmce values. Aftcr a matching rule is found there is no nccd to search those branches if the maximum confidence of the matched rule is higher than the maximum value stored in the node.

N. A ~ O W C E S T U D Y  We implemented GDT tree by modifying an association based classification algorih GAGRDB [SI to evaluate its effectiveness. In this section, we report the muits of our experiments. All experiments reported in this section were performed on an IBM Thinkpad notebook computer with 700HZ CPU nmning Minosoft Windows 2000 professional and IBM DBZ version 7.0.

A. Classi$carion occuracy  As we discussed, it is expected that GDT will givcp higher classifmtion aceura~y than the classical decision tree.

During our experiments we run C4.S and our classification algorithms using a sct of data from the UCI Repository [6].

The continuous amibutes arc discretized using the MLC discretim based on entmpy discrctktion [S, 191. In most cases, IO-fold mss-validation is used The results of our system are shorn in Table 2. Fmm Table 2, we can sec that among 17 dafascb, GDT loses only S of them with average accuracy 0.033% higher than C4.S.

TabkZClanrificatonaccvrafyofGDTandC4.5  B. Esecutio# +enq One motivation of G M  is to address the issues of manipulating large number of classification rules in association-based classification The second oe*l of expCrimmts focus on the exextion efficiency of GDT. To indicate the effcaivcncsa of organizing rules in G M .  wc usc the association-based classification appmach withouf Wllsrmchn ' g GDT, GAC-RDB as thc ref- of comparison The data set wc wd is synthetic data set de- in [I]. Each rsord in the data sct wnsists of 9 atrribuDs and IO classification funetions defined on thex 9 atuihtes. Here we condun our qmhcntal  studies using hc t ion  IO shown blow, which is the most complex hction among the ten classification funetions.

hysars < 20 + equity=O h a y  2 20 + equity=O.lx hvalue x fhycarrs-20) disposable = (0.67 x ( salary +wmmission) -SO00 x elevel + 02 xcquity-IOk) ClasSA dispos8bleXl  Since GDT woks with categorical attributes, the non- categorical amiiur*l discretizcd first by a simple qui- width method for discrehtion The interval width and the number of intervals rcsulted are shown in Table 3.

Tabk 3. DiSSriking the Srmbute Val-  In order to compare the time used to search rule set. we p d c d  several rule set with diffmnt number of rules  20000 40000 6oMx)  %re 6. Ssarrh b e  &No. of da  0 60 120 180 240 300 Number of attributes  ~ ~~ ~~  Flgmre 7. Ssarrb h e  & No. of amibutcr  ranging from 4.921 to 52,133 for datasets with noise level of 0.1. For dataset with 500,ooO training tuples and 10,000 M fupleq the total time used in thc iterations of tinding classification rules to test is shown in Figurc 6. From the figure wc can sec that the searching time saved by using GDT increased b t i c a U y  as the number of rules incnascs.

The sewnd set of expnimnts is to sa the effects of the number of attributes on the scmhing time. We also use the synthetic datarrt, but we increase the number of atkibutcs horn 9 to 300. The noiss I m l  of the 200.000 training tuples is still 0.1 with the m e  number of 10,000 test tuples. Since those extra amibutcs other than the original 9 attributes are irrelevant to the classification hction IO, the n u n k  of rules produced for those dabsets with di&rcnt number of athibutes rrmains the same, and it is about 25,000 rules. The searching t i m  for the testing procedure of all iterations is shown in Figore 7. This figure indicates that the n u m k  of attributes does not a f k t  the searching time significantly for both data suuctwes. 'This is because we do not need to wmpare those exha attributes as they do not appear in the rule at all. At the sam t i m  the figurc also shows that the tree structure has a faster starching timS than tlat style suucture.

It is worth noting that, in the process of tinding classification rules, whenever a rule is found, we need to search the existing rule set to h o w  if the current rule is redundant. This pmemVe in similar to the searching pro~cdurs for an usem case. The diffmnce is that we only need to find the first rule covering i t  So wmparcd to searching all of matching rules, this p h  d y  spcnds less timc. Therefox, to so= ex- GDT can also speed up the rules' tinding p h  of association, based classification algorithms. The third sct of experimena demonstnttes i t  The data.Ws is as the m n d  set of ex-&. Figure 8 shows the time spent on searching    the existing tree for a -t rule to see if it is redundant and store it to the tree if not redundant It is obvious that as the number of amibutes incnascs, the time increases. It is impomnt to know that the me approach spcnt less time than the unstrucNnd approach  I 0 w 120 180 240 300  ~ l p ~  a Tic  m ~torr IUIW b b e r  of attributes

V. CONCLUSIONS  In this p~pcr. we proposed a gsncralizsd dccision trce (GDT), which is a natural e x h i o n  of decision tra. GDT unifies the decision Iree classification and association-based Classification. A G M  tra sncodcs all classifmton can be disurvsnd using association-bascd classification appmaches to address the issuc that the classical dccision tree classification may miss some classifications. On the omer hand. GDT makm mipulation of large number of classification rules more efficient as shown by the rcsulfi of OUrpcrfOImanes study.



VI. RE-EmNCEs  [IIR Agrawdl. T. Imielinski, and A. Swami. Database mining: A prformana paspcnive. IEEE Tmsactions on Knowledge and Data Enginemin& 5(6). Daonber 1993.

RI. Bayardo, Brute-Fa mining of high coniidmce classifration rules. In h e e d i n g s  of 3rdInmnational Confwence on Knowledge Direowry and Dafa Mining.

1997.

G. Dong. X Zhang, L. Wong, and 1. Li. CAEP Classification by aggregating emerging patterns.

InIernatio~I Confeence on Discowy science, 1999.

N. Friedman, D.Geiga, and M. Goldszmidt, Bayesian network classifier, Machinehrning. 29,1997,131- 163.

U.M. Fayyad and K.B. h i ,  Multi-Interval discretization of continuous-valued amibutes for classification learning, In Proceedings of the 13th Inmationalfoinf Conference an AmJicial Infelligence, 1993, 1022-1027.

CJ.  Mesq P. Murphy, UCI npositoty of machine learning databases. 1996.

Olnp:lhnvw.cs.uci.cdu/-mleamlMLReposito~.h~) B. Liu, W. Hsq and Y. M a  Intemting classification and association rule mining. In Proceedings of the Fowfh Intemafional Conference on Knowledge DiscowyandDafa Mining. New York, USA, 1998, 80-86.

[21  [3]  [4]  [SJ  [61   181 H. Lu. H. Liu, Decision tables: Sulable Chiincation Exploring EDBM CrpebiliUK. lo h&'& Of& 26th lo~umlalcQm-m0aoO vqLvgrDat?brPs. cab.

Egypt 2000.373-384.

H. Lu. R Setiono, and H. Liu. NnrroRule: A conneztionist approach to data mining. In Proceedinas Darahes, Zurich Switzerland, Sqtrmbcr 11-15 1995,478489,  [IO] M. Mehla, R Agrad ,  and 1. Rissanen. SLIQ: A fast scalable classifier for data mining. In Proceedings of the 5th I~ernnronal Conference on mending Dotabme T e c h l ~ ,  Avignon, France, March 1996.

[I I] D. Memalds and B. Wiithrich. Extending naive Bayes classifiers using long itnnsm. In Proceedings of Sfh Data Mining. San Diego, California, August 1999.

[I21 1. R Quinlan: Induction of Decision Trees. Machine karnihg, 1: 81-106,1986  [I31 J.R Quinlan. C4.5: Program for Machine Learning.

MorganKaufmann, 1993.

[I41 1. C. Shafer, R AgnwaL and M. Mehta. SPRINT: A scalable padlel classifier for data mining. In h e e d i n g s  of the 22nd InterMtioMl Conference on VayLmgeLJa&bases, Mumbai (Bombay), India, Septcdcr 1996.

[I51 K Wang, Y. He, J. Han. Pushing support cona&ts into frequent itemut mining. In Prmeeding of fhe 26fh hferMtiOMl Conference on Very large Dofabases, Cairo, Egypt, 2OOO. 43-52.

1161 M. Wan5 B. lycr. and J.S. V i e .  Scalable mining for classifration rules in relational databsses. In Proceedings ofthe 1998 IIIferMfiOMIhfabaSe Engineering and Applications Sympasium, cardiff, Wales, U.K. July 840.1998.

[I71 S.M. Weiss and C.A K&ou.jki Computer System that Learn: Cla&cation andPredidion Method$ fmrn Sratistics, Neural Neb, Machine Learning, and ~ S y s f e m r .  MorganKaufman, 1991.

[IS] K Wang, S. Zhou, Y. He, Growing decision trees on support-lcss association rules. In h e e d i n g  of fhe sixfh ACMSIGKDD infernational confireme on hnowledge discovery and &a mining, Boston, USA , Aupust2OOO, 265-269.3.

discmiation of continuous-valued amibutes for classitieation Icammg, In Proceedings of the 13th Infernnr'onalfoinf Confmnce on Ampcial hfelligence, 1993, 1022-1027.

