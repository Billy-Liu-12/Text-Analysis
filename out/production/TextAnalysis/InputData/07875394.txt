SPECIAL SECTION ON HETEROGENEOUS CROWDSOURCED DATA ANALYTICS

ABSTRACT Wireless sensor networks (WSNs) are almost everywhere, they are exploited for thousands of applications in a densely distributedmanner. Such deploymentmakesWSNs one of the highly anticipated key contributors of the big data nowadays. Hence, data aggregation is attracting much attention from researchers as efficient way to reduce the huge volume of data generated inWSNs by eliminating the redundancy among sensing data. In this paper, we propose an efficient data aggregation technique for clustering-based periodic wireless sensor networks. Further to a local aggregation at sensor node level, our technique allows cluster- head to eliminate redundant data sets generated by neighbouring nodes by applying three data aggregation methods. These proposed methods are based on the sets similarity functions, the one-way Anova model with statistical tests and the distance functions, respectively. Based on real sensor data, we have analyed their performances according to the energy consumption and the data latency and accuracy, and we show how these methods can significantly improve the performance of sensor networks.

INDEX TERMS Periodic sensor network (PSN), data aggregation, clustering topology, big-data sensing, similarity and distance functions, Anova model.



I. INTRODUCTION Speaking about more than hundreds, and sometimes thousands, of sensors, which are randomly distributed for monitoring target phenomenon, makes WSNs one of the big data producers. This fact has been supported by the report of ORACLE [1] where some examples of applications generating big sensor data were provided in the report. In addition, the authors in [2] and [3] give many real WSN applications where the scale of the sensory data has already exceeds several petabytes (PB) annually. However, such big data applications raise two problems: high energy consump- tion and complex data analysis. First, the sensing of big data volume leads to a great waste of sensors energy, which is usu- ally limited and not rechargeable, thus decreases the network lifetime. Second, it is a complicated mission for decision makers when dealing with a big amount of sensed data, that mostly contain a high redundancy level, to make the right decisions. In order to handle these problems, researchers has been focused on the data aggregation methods in WSNs. The main goal of these methods is to minimize the huge amount of data generated by neighboring nodes thus conserving net- work energy and providing a useful information for the end user [4], [5].

In this paper, we consider a cluster-based periodic sen- sor network (CPSN), where each sensor monitors the given phenomenon and periodically sends its collected data to its CH. Then, we introduce a complete data aggregation frame- work for CPSN. Two layer algorithms are introduced: at the node level and at the CH level. These algorithms aim at optimizing the volume of transmitted data thus saving energy consumption and reducing bandwidth on the network level.

At the first level, an aggregation process aggregates data on a periodic basis avoiding each sensor node to send its raw data to the sink. At the second level, we present and compare three different methods to search for redundancies between data sets generated by neighboring sensor nodes. The first method uses the similarity functions, such as the Jaccard function, to search the similarities between data sets. The second method searches the dependence of conditional variance between data sets based on the one-way Anova model and the Bartlett test. Finally, dissimilarities between sets are calculated in the third method based on distance functions, such as Euclidean and Cosine. Indeed, it is important to notice that first and second methods are already proposed in our previous works while, in this paper, we propose the distance functions as new data aggregation method for CPSN. Then, our objective is to  2169-3536 2017 IEEE. Translations and content mining are permitted for academic research only.

Personal use is also permitted, but republication/redistribution requires IEEE permission.

See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

VOLUME 5, 2017    H. Harb et al.: Comparison of Different Data Aggregation Techniques in Distributed Sensor Networks  compare the three methods in terms of energy consumption, data accuracy and data latency. Finally, we give the user the opportunity to choose the solution that matches the most its expectations and requirements.

The rest of this paper is organized as follows: Section II describes some of the existing data aggregation techniques proposed for WSNs. Section III provides a brief discussion of the data aggregation system based on clustering topology used in our proposed technique. In Section IV, we describe the aggregation at the sensor level, called local aggregation.

Sections V, VI and VII present the three aggregation methods at the CH level based on the similarity functions, the variance study and the distance functions respectively. Section VIII details the experimentations we have conducted in real sen- sors data with discussion of results of the three methods.

Finally, Section IX concludes the paper while highlighting some future scopes of work.



II. STATE OF THE ART Reducing energy consumption is a major issue in WSNs where sensors are usually battery-limited. Hence, data aggre- gation is an essential operation in WSNs used to decrease the data transmission, thus, to enhance the network lifetime.

It means computing and transmitting partially aggregated data to the end user rather than transmitting raw data in networks to reduce the energy consumption. Data aggrega- tion in wireless sensor networks has been well studied in recent years [6]?[10]. Indeed, the performance of any data aggregation technique is strongly dependent on the network?s topology. Hence, researchers have proposed many network?s topologies for WSNs, such as Tree-based [11], Cluster- based [12], Chain-based [13] or structure free-based [7] topology.

The authors in[6], [14] and [15], use the clustering meth- ods for aggregating data packets in each cluster separately.

In [16], the authors propose a data aggregation scheme named DMLDA, Dynamical Message List based Data Aggrega- tion, based on clustering routing algorithm. DMLDA mainly defines a special list structure to store history messages, which is used to judge the message redundancy instead of the period delay. In [17], the authors propose an aggregation and transmission protocol (ATP) based on clustering approach to conserve energy in PSNs. Instead of sending raw data to the CH, ATP allows each sensor to eliminate redundancy among its collected data and to adapt its data transmis- sion to the CH, using one way Anova model and Fisher test.

Other proposed techniques of data aggregation are based on a tree network topology, such as [8] and [9]. The authors in [8] use Genetic Algorithm (GA) to calculate all possible routes represented by the aggregation tree. The objective is to find the optimum tree which is able to balance the data load and the energy in the network. In [9], a semi-structured protocol based on the multi-objective tree is proposed, in order to reduce transmission delays and enhance the aggrega- tion probability. In such a work, the routing scheme explores  the optimal structure by using the Ant Colony Optimiza- tion (ACO).

Other works on data aggregation in WSNs are based on a chain routing topology [10], [18]. In [10], the authors propose a Cycle-Based Data Aggregation Scheme (CBDAS) in order to reduce the amount of data transmitted to the base station (BS). In CBDAS, the network is divided into a grid of cells, each with a head. The network lifetime is prolonged by linking all cell heads together to form a cyclic chain, where the gathered data move from node to node along the chain, getting aggregated. In [18], a chain-based routing scheme for application-oriented cylindrical networks is proposed. After finding local optimum paths in separate chains at each scheme, the authors formulate mathematical models to find a global optimum path for data transmission through their interconnection.

Finally, some works proposed recently on data aggrega- tion are based on a structure-free of the network [19], [20].

In [19], the authors propose a Structure-Free and Energy- Balanced data aggregation protocol, SFEB. SFEB features both efficient data gathering and balanced energy consump- tion, which result from its two-phase aggregation process and the dynamic aggregator selection mechanism. In [20], a virtual force-based dynamic routing algorithm (VFE) for data aggregation inWSNs is proposed. Motivated by the cost field and virtual force theories, VFE allows each node to select the optimal node to be the next hopwhichmakes data aggregation more efficient.

Subsequently, clustering is recently considered as an effi- cient topology control method in WSN that has many advantages, especially as far as scalability and network maintenance are concerned, compared to other topologies.

However, most of the existing data aggregation techniques based on clustering topology are dedicated to event driven data model [21], [22] and they mainly focus on the selection of CHs [23], [24]. In these techniques only CHs process and aggregate datawithout any processing at the level of the nodes themselves. In this paper, we propose three data aggregation techniques dedicated to PSNwhich achieve an aggregation of data at both nodes and CHs. They have two phases which are able to eliminate redundant data generated by nodes at each period.



III. NETWORK TOPOLOGY In this paper, we focus on the cluster-based network topology where the whole network is divided into several clusters.

Each cluster has a cluster-head (CH) which is responsible for managing the sensors in this cluster. Indeed, grouping sensor nodes into clusters has been widely adopted and studied by the research community to satisfy the scalability objective and achieve high energy efficiency and prolong the network lifetime [25]?[27]. Some of proposed techniques aim at forming and maintaining the clustered networks while optimizing cluster size [28], [29], others try to select the Cluster-Head (CH) or to change the entire cluster hierar- chies periodically [23], [25], [30], others are interested in  VOLUME 5, 2017 4251    H. Harb et al.: Comparison of Different Data Aggregation Techniques in Distributed Sensor Networks  communication among nodes and among clusters [26], [27] or in cluster joining [31]. Hence, in this paper, we adopt a cluster based architecture and we consider that the network is clustered and the CHs are defined using an appropriate clustering scheme.

FIGURE 1. Data aggregation based on clustering network topology.

In Fig. 1, we present a cluster-based sensor network topol- ogy where sensed data reach their destination (the sink) by traveling via CHs. In addition, the following constraints are respected in our network?s topology: ? Data transmission between member nodes and their appropriate CH or between CHs and the sink is based on single-hop communication.

? Member nodes collect data in a periodic manner. Subse- quently, each member node sends its data to the appro- priate CH at each period.

? Sensor nodes sense environment at a fixed sampling rate where each one takes ? measures at each period.

Then, our proposed data aggregation technique works in two levels: the first one at the sensor nodes level, called local aggregation, and the second at the CHs level (Fig. 1).



IV. AGGREGATION AT SENSOR LEVEL: LOCAL AGGREGATION In WSN, measures collected by sensor nodes are highly dependent on the monitored condition. Consequently, when the monitored condition slows down or speeds up, then, the measures collected by each sensor are more correlated and redundant. That fact is confirmed by the example shown in Fig. 2. In Fig. 2, sensors are deployed in the Intel Labora- tory [32] and they collect temperature measures. For a period of one day, readings collected by sensors S10 and S46 span over a range of [15.95, 21.45] and [14.01, 22.16] respec- tively. Small ranges of measures shown in Fig. 2 indicate that measures collected by each sensor are very similar in this period. Therefore, if sensors send all the collected measures to their appropriate CHs, their energy will be wasted and thus the network energy will be quickly depleted. Hence, data aggregation becomes a requirement in WSNs to minimize redundant data generated by sensor nodes.

A. DEFINITIONS AND NOTATIONS In PSN, each period p is divided into ? equal time slots as fol- lows: p = [s1, s2, . . . , s?  ] . At each slot sj, each sensor Si cap-  tures a new measure mij , then, it forms a vector of measures during the period p as follows: Mi =  [ mi1 ,mi2 , . . . ,mi?  ] .

Fig. 3 shows an example of periodic data collection where  FIGURE 2. Sensors in the Intel Laboratory.

FIGURE 3. Data collection in PSN.

the sensor node Si takes five measures (e.g. ? = 5) at each period pq (where q ? [1,3]) and sends its vector of collected data Mi =  [ mi1 ,mi2 ,mi3 ,mi4 , mi5  ] to the CH at the end of  each period.

As mentioned above, a data vector Mi formed by the sen-  sor Simay contain similar measurements, especially when the monitored condition varies slowly or when the slots are short.

In order to eliminate redundant values from the vectorMi, the sensor node Si searches formeasures similarities in the vector.

Thus, we assign to each sensor node the Similar function, i.e.

Similar(mij ,mik ), to identify if the two measures mij and mik captured by the sensor Si in the period p are similar or not.

The Similar function is defined as follows: Definition 1 (Similar Function): We define the Similar  function between two measurements captured by the same sensor node Si at a period p as:  Similar(mij ,mik ) =  { 1 if  ??mij ? mik?? ? ?, 0 otherwise.

where mij and mik ? Mi and ? is a threshold determined by the application. Furthermore, two measures are similar if and only if their Similar function is equal to 1.

4252 VOLUME 5, 2017    H. Harb et al.: Comparison of Different Data Aggregation Techniques in Distributed Sensor Networks  In order to save the integrity of the information, we define the weight of a measure as follows: Definition 2 (Measure?s Weight, wgt(mij )): The weight of  a measurement mij is defined as the number of similar mea- sures (according to the Similar function) to mij in the same vector Mi.

Based on the notations defined above, we describe the  local aggregation phase which is run by the nodes themselves at each period in the following manner: for each new cap- tured measurement, a sensor node Si searches for similarities of the new taken measurement. If a similar measurement is found, it deletes the new one and increments the cor- responding weight by 1, else it adds the new measure to the set and initializes its weight to 1.1 After applying local aggregation, Si will transform the initial vector of measures, Mi, to a set of measures, M ?i , associated to their cor- responding weights as follows: M ?i = {(m  ? i1 ,wgt(m?i1 )),  (m?i2 ,wgt(m ? i2 )), . . . , (m?ik ,wgt(m  ? ik ))}, where k ? ? .

Illustrative example: let consider a vector of measures generated by the sensor Si at the period p as follows: Mi = [10, 10.2, 10.3, 11, 11.1, 12, 11.3, 10.4, 12.1, 12.4].

By taking ? = 0.5, Similar function will transform the vector Mi to the following set of measures: M ?i = {(10; 4), (11; 3), (12; 3)}, where 4, 3 and 3 are the weights of the measures 10, 11 and 12 respectively.

Based on the setM ?i , we provide the following definitions: Definition 3 (Cardinality of the Set M ?i , |M  ? i |): The cardi-  nality of the setM ?i is equal to the number of elements inM ? i ,  i.e. |M ?i | = k .

Definition 4 (Weighted Cardinality of the Set M ?i ,  wgtc(M ?i )): The weighted cardinality of the set M ? i is equal  to the sum of all measures? weights in M ?i as follows:  wgtc(M ?i ) = ?|M ?i |  j=1 wgt(mij ) = ? , where mij ? M ? i .

At the end of each period p, each sensor node Si will have a set M ?i with no redundant measures. The second step is to send it to the appropriate CH which, in turn, aggregates the data sets coming from different member nodes. In the next sections, we present three different aggregation methods at the CH level.



V. AGGREGATION AT THE CH USING SETS SIMILARITY FUNCTIONS At the end of each period, each CH will receive several sets of measurements and their weights from different nodes.

The objective of the second aggregation level is to eliminate redundant data sets by identifying all pairs of sets whose similarities are above a given threshold. For this reason, we used in our previouswork [33] the Jaccard similarity function, which is one of the most widely accepted functions as it can support many other similarity functions [34]. The Jaccard similarity function returns a value in [0, 1] where a higher value indicates that the sets share more similarities. Thus we can treat pairs of sets with high Jaccard similarity value as  1Several methods could be applied like dichotomic search, compression etc.

near duplicate to reduce the size of final data sets transmit- ted from the CH to the sink. A Jaccard similarity function between two sets M ?i and M  ? j , generated respectively by the  sensors Si and Sj, can be formulated as follows [33]:  J (M ?i ,M ? j ) ? tJ ? |M  ? i ?s M  ? j | ? ? =  2? tJ ? ? 1+ tJ  (1)  where tJ is the Jaccard threshold defined by the application itself and ???s?? is defined as: Definition 5: Consider two sets of measurements M ?i and  M ?j , then we define the overlap, ?s, between them as: M ? i ?s  M ?j = {(m ? i,m ? j) ? M  ? i ? M  ? j with weight wgtmin(m  ? i,m ? j)such  that Similar(m?i,m ? j) = 1};  where wgtmin(m?i,m ? j) = min(wgt(m  ? i),wgt(m  ? j)), the mini-  mum value of the weight of m?i and m ? j.

Then, in order to prevent CH from enumerating and com- paring every pair of sets which has aO(n2) number of compar- isons, we proposed to use a prefix frequency filtering (PFF) technique. The PFF technique works in the following two steps to find the pairs of similar sets: ? Candidate pairs? generation: in this step, the CH searches the candidates (which may or may not be sim- ilar) sets for every data set. This step is based on the intuition that if all sets of measures are sorted by a global ordering, some fragments of them must share several common tokens with each other in order to meet the Jaccard threshold similarity (tJ ). Therefore, it first defines a prefix of length |M ?i | ? dtJ ? |M  ? i |e + 1 for  every setM ?i . Then two setsM ? i andM  ? j are candidates if  and only if they share at least ? measurements in their prefixes as shown in the following lemma [33]: Lemma 1: Assume that all the measures in the sets M ?i and M ?j are ordered in decreasing order of the measures weights. Let the p-prefix be the first p elements ofM ?i . If M ?i ?sM  ? j ? (2?tJ?? )/(1+tJ ), then p?M  ? i ?sp?M  ? j ?  ? = ?|p?M ?i |  k=1 wgt(m ? k )?  ( (1? tJ )/(1+ tJ )  ) ? ? where  m?k ? p?M ? i .

Proof 1: We denote by p-M ?i the prefix of the set M ?i and r-M  ? i the set of reminder measures where  M ?i = {p-M ? i + r-M  ? i }. We have:  M ?i ?s M ? j = p?M  ? i ?s M  ? j + r ?M  ? i ?s M  ? j  = p?M ?i ?s p?M ? j + p?M  ? i ?s r ?M  ? j  + r ?M ?i ?s M ? j  ?= p?M ?i ?s p?M ? j + r ?M  ? i ?s M  ? j  ? p?M ?i ?s p?M ? j  +  |r?M ?i |? k=1  (wgt(m?k ? r ?M ? i ))  In the second line we can omit the term p ? M ?i ?s r ? M ?j because we have assumed that it is negligible compared to the other terms in the equation. Indeed, if the two sets are similar then the measures having  VOLUME 5, 2017 4253    H. Harb et al.: Comparison of Different Data Aggregation Techniques in Distributed Sensor Networks  highest weights must be in the prefix set and not in the reminder, which means that the overlapping between the p-M ?i and r-M  ? j is almost empty. From the above  equations and equation (1)(similarity condition) we can deduce:  2? tJ ? ? 1+ tJ  ? p?M ?i ?s p?M ? j  +  |r?M ?i |? k=1  wgt(m?k ? r ?M ? i ) (2)  From the following equation:  |p?M ?i |? k=1  wgt(m?k ? p?M ? i )  +  |r?M ?i |? k=1  wgt(m?k ? r ?M ? i ) = ? (3)  We obtain:  p?M ?i ?s p?M ? j ?  |p?M ?i |? k=1  wgt(m?k ? p?M ? i )  ? 1? tJ 1+ tJ  ? ? (4)  The lemma is proved.

Based on the lemma 1, the CH calculates the overlap between the prefix of each pair of sets; the two sets are considered a candidate pair if their calculated overlap is greater than ?.

? Candidates? verification: once all the candidates pairs are found, the CH verifies the Jaccard similarity for each one in the second step. The two sets in a candidate pair are considered similar if their similarity is greater than the Jaccard threshold tJ .

Algorithm 1 describes the PFF technique to find simi- lar sets. Briefly, the CH searches similar measures between prefixes of every pair of sets using the Similar function (lines 3-21). Then, it assumes that the two sets are a candidate pair only if the overlap between their prefixes is greater than the score determined at lemma 1 (line 22). Finally, the two sets are considered similar if the overlap between their measures is greater than the Jaccard threshold (lines 23?25).

For more details about this algorithm, please refer to Algorithm 1 in [33].

In order to decrease the data latency, we provided sev- eral optimizations of the PFF technique based on the suf- fix filtering [35] and the k-means algorithm [36]. In [35], we propose a suffix filtering based on the measure weights in order to prune erroneous candidates that survive after applying prefix filtering. In [36], we propose to group data sets into clusters using k-means algorithm before applying PFF over each one. By this way, we minimize the num- ber of comparisons thus enhancing the aggregation process time.

Algorithm 1 PFF Algorithm Require: Set of measures? sets M ? = {M ?1,M  ?  2...M ? n}, tJ .

Ensure: All pairs of sets (M ?i ,M ? j ), such that J (M  ? i ,M  ? j ) ? tJ .

1: S ? ? 2: Ii? ?(1 ? i ? total number of measures in the prefixes  of all sets) 3: for each set M ?i ? M  ? do 4: p? |M ?i | ? dtJ ? |M  ? i |e + 1  5: Fs? empty map from set id to int 6: sumFreq? 0 7: for k ? 1 to p do 8: sumFreq ? sumFreq + wgt(m?k ),where  m?k ? p?M ? i  9: end for 10: for k ? 1 to p do 11: w? M ?i [k] 12: if (Iws exists such that Similar(w,ws) = 1) then 13: for each Measurement (M ?j [l]),wgt(M  ? j [l]) ? Iws  do 14: Fs[M ?j ]? Fs[M  ? j ]+ wgtmin(M  ? i [k],M  ? j [l])  15: end for 16: Iws ? Iws ? {p?M  ? i }  17: else 18: create Iw 19: Iw? Iw ? {p?M ?i } 20: end if 21: end for 22: for each M ?j such that Fs[M  ? j ] ? sumFreq ? ((1 ?  tJ )/(1+ tJ ))? ? do 23: if J (M ?i ,M  ? j ) ? ? then  24: S ? S ? {(M ?i ,M ? j )}  25: end if 26: end for 27: end for 28: return S

VI. AGGREGATION AT THE CH USING ANALYSIS OF VARIANCE Studying the variance between measurements in the data sets is another way of finding nodes that generate redundant data sets. In this section, our objective is to briefly explain our technique proposed in [37], based on the k-means algorithm adopted by the Anova model and the Bartlett test. Indeed, the one-way Anova model is used to identify if the variance (R) between measures in a group of data sets is significant or not. R can be calculated in different manners depending on the statistic tests proposed in the Anova model. In [38], we used the Anova model in order to detect all pairs of nodes with identical behavior which generate redundant data logs or sets. In a later time, we proposed an enhanced k-means clustering method adopted to the one-way Anova model in order to search groups of sensors that generate redundant data [37]. In such method, we used three tests (Fisher, Tukey and Bartlett) then we concluded that the Bartlett test gives the  4254 VOLUME 5, 2017    H. Harb et al.: Comparison of Different Data Aggregation Techniques in Distributed Sensor Networks  best results compared to Fisher and Tukey. Therefore, in this paper, the Bartlett test is compared to the other methods. Once R is calculated, the sets are considered duplicated if R is less than a threshold T (significance level defined in chi-square table) for some desired false-rejection probability (risk ?).

R is calculated according to the Bartlett test as follows [37]:  R = (? ? 1)(n? ln(? 2p )?  ?n j=1 ln(?  j ))  ? (5)  where n is the number of total sets, ? 2j is the variance of the set M ?j and:  ? = 1+ (n+ 1)  3? n? (? ? 1) (6)  and ? 2p is the pooled variance, which is a weighted average of the period variances and it is defined as:  ? 2p =  n? (? ? 1) ?  n? j=1  ? 2j  Thus the decision is based on the following rule: ? if R > T , the variance between the sets is significant thus the sets are considered redundant.

? if R ? T , the variance between the sets is not significant.

In order to apply the Anova model over the groups of  sets, we used the k-means algorithm to classify the sets in groups based on the means of these sets. Then, we proposed a new initialization method to find, dynamically, the optimal number of groups (K ) in k-means. The proposed method divides a parent group into b  ? n/2c children groups, where  n is the number of total sets at each period, every time the Anova model is not satisfied. Finally, the CH sends one data set from each group with the IDs of all the sensors in this group to the sink.

Algorithm 2 describes the k-means algorithm adopted by the Anova model and the Bartlett test, or simply the KAB technique, proposed in [37]. First, it starts, as explained previously, by grouping all the received sets at the initial same group (lines 4 to 6). Then, it searches the variance between measurements in all the sets in the initial group, using the Anova model and the Bartlett test (lines 9 and 10). If the test?s result indicates a low variance between the sets then, the algorithm considers this group as a final group and it puts it in the list of final groups (lines 11, 12 and 13). Else, it divides the initial group in K sub groups by applying the k-means algorithm (line 15). Once the final groups are obtained, CH sends only one useful information to the sink, e.g. the data set with the highest cardinality, and the IDs for the sensors that generate redundant data sets (lines 19 to 22).



VII. AGGREGATION AT THE CH USING DISTANCE FUNCTIONS In this section, we propose a new method to search redundant data sets generated by the sensors using the distance func- tions. Distance functions are an important method that can  Algorithm 2 K-means Adopted to Variance Study Require: Set of measures? sets M ? = {M ?1,M  ?  2...M ? n}, K .

Ensure: List of selected sets, L.

1: C ? ? // list of all final groups 2: Q? ? // a temporary list of groups 3: C1? ? 4: for each set M ?i ? M  ? do 5: C1? C1 ? {M ?i } 6: end for 7: Q? Q ? {C1} 8: repeat 9: compute R for Ci based on Equation 5 10: find T 11: if R ? T then 12: C ? C ? {Ci} 13: remove Ci from Q 14: else 15: Q? Q? k-means(Ci,K ) 16: end if 17: until no cluster Ci ? Q 18: L ? ? 19: for each cluster Ci ? C do 20: consider |M ?j | > |M  ? j?|; where M  ? j? ? Ci ? {M  ? j }  21: L ? L ? {( M ?j , ID(M  ? j ) ? ID(M  ? j?) )}  22: end for 23: return L  find duplicated data sets by searching dissimilarities between these sets. Hence, a great number of distance functions have been proposed in the literature [39]. In this paper, we are interested in two distance functions that are widely used in various domains: Euclidean and Cosine distances.

Let us consider two data sets M ?i and M ? j , gener-  ated by the sensor nodes Si and Sj respectively, at the period p as follows: M ?i = {(m  ? i1 ,wgt(m?i1 )),  (m?i2 , wgt(m ? i2 )), . . . , (m?iki  ,wgt(m?iki ))} and M ?j = {(m  ? j1 ,  wgt(m?j1 )), (m ? j2 ,wgt(m?j2 )), . . . , (m  ? jkj ,wgt(m?jkj  ))} where  |M ?i | = ki and |M ? j | = kj. Therefore,M  ? i andM  ? j are considered  redundant if the calculated distance between them is less than a threshold (td ) as follows:  Dist(M ?i ,M ? j ) ? td  However, two issues must be considered when using dis- tance functions with measures? weights: 1) Calculating the distance between two data sets with different cardinalities, e.g. ki and kj, and 2) integrating the weights when calculating the distance between sets. To face these challenges, we pro- pose to use the threshold ?, introduced in the Similar function (cf. Section IV), when computing the distance between the sets.

In order to find the distance between two sets M ?i and M ? j ,  the first step consists in dividing each set into two parts: over- lap and remained. The overlap part of the set M ?i (resp. M  ? j )  contains measures that are similar to those in M ?j (resp. M ? i )  VOLUME 5, 2017 4255    H. Harb et al.: Comparison of Different Data Aggregation Techniques in Distributed Sensor Networks  while the remained part contains the remaining measures of M ?i (resp. M  ? j ). Subsequently, the overlap part between two  sets has already been defined in Definition 5, i.e. M ?i ?s M ? j ,  while the remained part in each set is defined as follows: Definition 6 (Remained Part of M ?i , M  ? ir ): Consider two  sets of sensor measures M ?i and M ? j . We define the remained  partM ?ir (respectivelyM ? jr ) as all the measures inM  ? i (respec-  tively M ?j ) minus the measures in the overlap part of M ? i  (respectively M ?j ) as follows:????? M ?ir = M  ? i 	 (M  ? i ?s M  ? j )  and M ?jr = M  ? j 	 (M  ? i ?s M  ? j )  Where 	 is a new operation defined as: Definition 7 (Minus Operation, 	): We define the minus  operation, M ?i 	 M ? j , between two sets M  ? i and M  ? j as all the  measures in M ?i and not in M ? j as follows:  M ?i 	 M ? j = {m  ? i ? M  ? i , with wgt(m  ? i) = wgt(m  ? i) ?  wgt(m?j) for all m ? j ? M  ? i ?s M  ? j and Similar(m  ? i,m ? j) = 1}  In order to compute the distance between M ?i and M ? j , we  must to transform M ?ir (respectively M ? jr ) into a vector as  follows:  vM ?ir = [ m?i1 , . . . ,m  ? i1? ?? ?  wgt(m?i1 ) times  ,m?i2 , . . . ,m ? i2? ?? ?  wgt(m?i2 ) times  , . . . ,m?iki , . . . ,m ? iki? ?? ?  wgt(m?iki ) times  ]  Then, we order the measures in vM ?ir (respectively vM ? jr ) by  increasing order of their values to ensure a logical comparison when calculating the distance between them.

A. EUCLIDEAN DISTANCE In mathematics, the Euclidean distance is the ordinary dis- tance, e.g. straight line distance, between two points, sets or objects. It is used in many applications and domains, such as computer vision and prevention of identity theft [40]. Further- more, the Euclidean distance is already used in WSN during the deployment phase in terms of sensors? localization [41] and inter-sensors distance estimations [42]. In this paper, we use the Euclidean distance on the data sets collected by sensors while adapting it to take into account the measures? weights.

In general, the Euclidian distance (Ed ) between two data setsMi andMj, before applying the local aggregation, is given by:  Ed (Mi,Mj) =  ???? ?? k=1  (mik ? mjk )2 where mik ? Mi  and mjk ? Mj  Thus, Mi and Mj are said to be redundant if Ed (Mi,Mj) ? td , where td is a threshold determined by the application.

After applying the local aggregation phase, we consider that Mi and Mj are respectively transformed into M ?i and M  ? j .

Therefore, we calculate the Euclidean distance between M ?i  and M ?j as follows:  Ed (M ?i ,M ? j ) =  ?????|vM ?ir |? k=1  (m?ik ? m ? jk )  2 where m?ik ? vM ? ir  and m?jk ? vM ? jr (7)  Proof 2: Consider two sets of data M ?i and M ? j . Then,  Ed (M ?i ,M ? j )  =  ? (M ?i ?M  ? j )  =  ?(( M ?i ?s M  ? j + vM  ? ir  ) ? ( M ?i ?s M  ? j + vM  ? jr  ))2 =  ?(( M ?i ?s M  ? j ?M  ? i ?s M  ? j  ) + ( vM ?ir ? vM  ? jr  ))2 =  ? (vM ?ir ? vM  ? jr )   =  ??|vM ?ir | k=1  (m?ik ? m ? jk )  2 where m?ik ? vM ? ir and  m?jk ? vM ? jr  In the above proof, we consider that the Euclidean dis- tance between the measures in the overlap is equal to zero because they are redundant. Therefore, the Euclidean distance between two sets is equal only to distance between measures in the remained parts of M ?i and M  ? j , i.e. vM  ? ir and vM  ? jr  respectively.

B. COSINE DISTANCE Cosine distance is a measure of dissimilarity between two vectors that measures the cosine of the angle between them.

This kind of dissimilarity has been used widely in many aspects, such as the anomaly detection in web documents [43] and medical diagnosis [44]. Depending on the angle between the vectors, the resulting dissimilarity ranges from?1 mean- ing exactly the opposite, to 1 meaning exactly the same.

The Cosine distance (Cd ) between two sets Mi and Mj, before applying local aggregation, is given by:  Cd (Mi,Mj) = 1?  ?? k=1(mik ? mjk )???  k=1 m ik ?  ??? k=1 m  jk  where mik ? Mi and mjk ? Mj.

Thus, Mi and Mj are redundant if Cd (Mi,Mj) ? td .

Then, we adapt the Cosine distance to the measures?  weights in M ?i and M ? j as follows:  Cd (M ?i ,M ? j ) = 1?  A+ ?|vM ?ir |  k=1 (m ? irk ? m  ? jrk )?  A+ ?|vM ?ir |  k=1 m ?2 irk ?  ? A+  ?|vM ?jr | k=1 m  ?2 jrk  where A =  |M ?i?sM ? j |?  k=1  ( wgtmin(m?ik ,m  ? jk )? m  ?2 ik  ) (8)  4256 VOLUME 5, 2017    H. Harb et al.: Comparison of Different Data Aggregation Techniques in Distributed Sensor Networks  Proof 3: Consider two sets of data M ?i and M ? j . Then,  Cd (M ?i ,M ? j )  = 1? M ?i ?M  ? j?  M ?2i ? ? M ?2j  = 1?  ( M ?i ?s M  ? j + vM  ? ir  ) ? ( M ?i ?s M  ? j + vM  ? jr  )? (M ?i ?s M  ? j ) 2 + vM ?2ir ?  ? (M ?i ?s M  ? j ) 2 + vM ?2jr  = 1? (M ?i ?s M  ? j ) + (vM ?ir ? vM  ? jr )?  (M ?i ?s M ? j ) 2 + vM ?2ir ?  ? (M ?i ?s M  ? j ) 2 + vM ?2jr  = 1? A+  ?|vM ?ir | k=1 (m  ? irk ? m  ? jrk )?  A+ ?|vM ?ir |  k=1 m ?2 irk ?  ? A+  ?|vM ?jr | k=1 m  ?2 jrk  where A = ?|M ?i?sM ?j |  k=1  ( wgtmin(m?ik ,m  ? jk )? m  ?2 ik  ) .

C. DISTANCE NORMALIZATION In general, each distance function has its own method to calculate the distance between data sets. For instance, straight-line distance in Euclidean distance and the angle between data sets in Cosine distance. Therefore, normal- ization becomes essential to scale the distance between data sets into the range [0, 1] to have thus the same vari- ation between sets before comparing them. Hence, Gaus- sian normalization has been considered as a better approach to normalize data sets [45]. Consequently, in this paper, data sets sent by the sensor nodes to the CH are normal- ized using Gaussian normalization. Once the CH receives the data sets at each period, it calculates first the dis- tance, Euclidean or Cosine, for each pair of sets as follows: d = {d1(M ?1,M  ?  2), d2(M ?  1,M ?  3), . . . , d n?(n?1)2 (M ?n?1,M  ? n)}  where n is the number of total sets and n?(n?1)2 is the number of all possible distances. Then, it normalizes the returned distance values using the following Gaussian normalization equation:  d ?i = di ? Y 6? ?  +  (9)  where Y is the mean of all distances and ? is the standard deviation of pairwise distance over all data. Y and ? are calculated as follows:  Y =  ?|d | k=1 dk |d |  and ? =  ??|d | k=1(di ? Y )   |d | ,  where |d | = n? (n? 1)   After normalizing all pairwise distances, the CH will form the distance normalization vector between each pair of sets as follows: d ? = {d ?1(M  ?  1,M ?  2), d ?  2(M ?  1,M ?  3), . . ., d ?n?(n?1)  (M ?n?1,M  ? n)}.

D. DISTANCE-BASED ALGORITHM AT THE CH In this section we present our data aggregation method at the CH based on the distance functions. Algorithm 3 describes how the CH finds redundant sets of measures generated by sensors then how it selects, among them, data sets to be sent to the sink. After normalizing data sets based on Equation 9 (lines 2 to 11), the CH considers that two sets are redundant if the normalized distance between them is less than the threshold td (line 12 and 13).Dist function in line 5 represents Euclidean or Cosine distances and can be calculated based on Equations 7 and 8 respectively. Then, for each pair of redundant set, the CH chooses the one having the highest cardinality (line 18), then it adds it to the list of sets to be sent to the sink (line 19). After that, it removes all pairs of redundant sets that contain M ?i or M  ? j from the set of pairs  (which means it will not check them again). Finally, the CH assigns to each set its weight (line 21) when sending it to the sink.

Algorithm 3 Distance-Based Redundancy Searching Algorithm Require: Set of measures? sets M ? = {M ?1,M  ?  2...M ? n}, td .

Ensure: List of sent sets, L.

1: S ? ? 2: d ? ? // list of pairwise distance 3: for each set M ?i ? M  ? do 4: for each set M ?j ? M  ? such that j > i do 5: compute Dist(M ?i ,M  ? j )  6: d ? d ? {Dist(M ?i ,M ? j )}  7: end for 8: end for 9: compute Y and ? for d 10: for each di ? d do 11: d ?i = ((di ? Y )/(6? ? ))+ 0.5 12: if d ?i ? td then 13: S ? S ? {(M ?i ,M  ? j )}  14: end if 15: end for 16: L ? ? 17: for each pair of sets (M ?i ,M  ? j ) ? S do  18: Consider |M ?i | ? |M ? j |  19: L ? L ? {M ?i } 20: Remove all pairs of sets containing one of the two sets  M ?i and M ? j  21: wgt(M ?i ) = number of removed pairs + 1 22: end for 23: return L

VIII. EXPERIMENTAL RESULTS AND EVALUATION To validate our proposed data aggregation techniques, we developed a Java based simulator that is run on the data collected from 46 sensors deployed in the Intel Berkeley  VOLUME 5, 2017 4257    H. Harb et al.: Comparison of Different Data Aggregation Techniques in Distributed Sensor Networks  Research Lab [32].2 Mica2Dot sensors with weather boards collect timestamped topology information, alongwith humid- ity, temperature, light and voltage values once every 31 sec- onds. The objective of these experiments is to confirm that the proposed data agregation methods can successfully achieve desirable results for energy conservation, data latency and data accuracy in different monitoring applications. Data were collected using TinyDB in-network query processing system built on the TinyOS platform. In our experiments, we used a file that includes a log of about 2.3 million readings collected from these sensors. For the sake of simplicity, in this paper we are interested in one field of sensor measurements: the temperature.3 We assume that all nodes send their data to a common CH placed at the center of the Lab. First, each node periodically reads real measures while applying the local aggregation. At the end of this step, each node sends its set of measures with weights to the CH which in its turn aggregates them using the three proposed methods in our technique.

We evaluated the performance using the following parameters: (a)  1) the threshold ?, defined in Similar function, takes the following values: 0.03, 0.05, 0.07, 0.1.

2) ? , the number of sensor measurements taken by each sensor node during a period, takes the following values: 200, 500 and 1000.

3) the distance threshold td takes the following values: 0.35, 0.4, 0.45 and 0.5.

4) the threshold tJ of the Jaccard similarity function is fixed to 0.75.

5) ?, the false-rejection probability in the Anova model, is fixed to 0.01.

A. DATA AGGREGATION RATIO AT THE SENSOR?s LEVEL During the local aggregation, each sensor node searches the similarity between measures captured at each period and assigns for each measure its weight. Therefore, the result of the aggregation in this phase depends on the chosen thresh- old ?, the number of the collected measures in period ? and the changes in the monitored condition. Fig. 4 shows the percentage of remaining data, or aggregated data, which will be sent to the CH, with and without applying the local aggregation phase at the sensors? level. At each period, the amount of data collected by each sensor is reduced at least by 77% (and up to 94%) after applying the aggregation phase.

Otherwise, the sensor node sends all the collected data, e.g.

100%, without applying the aggregation phase. Therefore, our technique can successfully eliminate redundant measures at each period and reduce the amount of data sent to the CH.

We can also observe that, with the local aggregation phase, data redundancy among data increases when ? or ? increases.

2Our techniques has been also applied on real data collected from the ARGO project [46]. The obtained results were similar to those presented in this paper which indicate the efficiency of our techniques in underwater sensor applications. However, the results are not presented in order to not increase the number of pages of this paper.

3the others are done by the same manner  FIGURE 4. Percentage of data after applying local aggregation.

This is because the Similar function will find more similar measures to be eliminated in each period.

B. DATA SETS REDUNDANCY When receiving all the sets from its member nodes at the end of each period, CH applies the second aggregation level in order to find all pairs of redundant sets. Fig. 5 shows the number of pairs of redundant sets obtained at each period when applying Euclidean and Cosine distances, KAB and PFF techniques respectively. First, we fixed ? and ? and we varied td as shown in Fig. 5(a), then we fixed ? and td and varied ? as shown in Fig. 5(b) and, finally, we fixed ? and td and varied ? as shown in Fig. 5(c). The obtained results show that, the CH finds more redundant sets when applying the distance functions, i.e. Euclidean and Cosine, compared to the Jaccard function used in PFF technique for different values of parameters. This is because the distance condition (Equations 7 and 8) is more flexible compared to the similarity condition (Equation 1) used in PFF. On the other hand, theAnovamodel gives the least important number of redundant sets because it searches redundant data sets in groups instead of pairs in distances and similarity methods.

FIGURE 5. Number of pairs of redundant sets at each period. (a) ? = 500, ? = 0.07. (b) ? = 500, td = 0.4. (c) ? = 0.07, td = 0.4.

4258 VOLUME 5, 2017    H. Harb et al.: Comparison of Different Data Aggregation Techniques in Distributed Sensor Networks  Several observations can be made based on the obtained results in Fig. 5: ? The Euclidean distance finds more redundant sets com- pared to Cosine distance in all cases. This is due to the Euclidean distance equation which is more flexible compared with that used in Cosine distance.

? The number of pairs of redundant sets in Euclidean and Cosine distances increases when td increases (Fig. 5(a)).

This is because, we allowmore measures to be lost when td increases thus we allow more sets to be redundant.

? The number of redundant sets obtained in both distances and KAB technique is almost fix when fixing ? and td and increasing ?, while it increases in PFF (Fig. 5(b)).

This is because the data sets save the same distance (resp. variance) condition when changing ?. Otherwise, the results of PFF proportionally change to ? since they are strongly dependent on the Similar function.

FIGURE 6. Percentage of sets sent to the sink at each period. (a) ? = 500, ? = 0.07. (b) ? = 500, td = 0.4. (c) ? = 0.07, td = 0.4.

C. DATA SETS REDUCTION In this section, we show how the CH is able to eliminate redundant sets at each period before sending them to the sink.

Fig. 6 shows the percentage of the remaining sets that will be sent to the sink after eliminating the redundancy. Similarly to Fig. 5, we varied td and we fixed ? and ? in Fig. 6(a), then we varied ? and fixed ? and td in Fig. 6(b) and, finally, we varied ? and fixed ? and td in Fig. 6(c). Generally, the obtained results are dependent on the number of the redundant sets shown in Fig. 5; if more redundant sets are found, this will lead to more sets being eliminated. Therefore, Euclidean and Cosine distances allow the CH to eliminate more redundant sets, except when td is small (e.g. td = 0.35 in Fig. 6(a)), at each period compared to PFF and KAB techniques.

The results obtained in Fig. 5 allow us to conclude some observations shown in Fig. 6: ? The percentage of sets sent to the sink using Euclidean distance is inferior to that sent using Cosine distance, for  different values of parameters. This is because the CH finds more redundant sets by using Euclidean distance (see results in Fig. 5).

? The distance functions allow the CH to send 15% to 61% less of sets to the sink compared to PFF, due to the flexibility of distances regarding the redundancy compared to similarity functions. Similarly, KAB gives better results (36% to 49% less of sent sets) compared to PFF in all the cases. This is because KAB searches redundant sets in groups then sends one set of each group to the sink while PFF searches them by pairs.

? The percentage of sets sent to the sink in Euclidean and Cosine distances decreases when td increases (Fig. 6(a)) while it is almost fix when ? or ? increases (Fig. 6(b) and 6(c)).

? PFF sends less percentage of sets to the sink when ? increases while it is almost fix when using KAB tech- nique (Fig. 6(b)). This is because, the variance condi- tion in Anova is independent from ? while the Jaccard function in PFF is dependent.

FIGURE 7. Energy consumption in each sensor node. (a) ? = 200.

(b) ? = 500. (c) ? = 1000.

D. ENERGY CONSUMPTION STUDY In this section, our objective is to study the energy consump- tion at the sensor nodes and CH levels. In sensor networks, energy consumption is highly dependent on the amount of data sent and received. First, Fig. 7 shows the energy con- sumption comparison with and without applying the local aggregation phase by each sensor node and when varying ? and ?. Since the local aggregation significantly reduces the redundancy among data collected by the sensor node (see results in Fig. 4), it allows it to proportionally save its energy when transmitting its data to the CH at each period. This result is obvious in Fig. 7 when the sensor node applies the local aggregation phase and when ? or ? increases. It is important to notice that our technique can save from 76% (Fig. 7(a)) up to 94% (Fig. 7(c)) of the energy of a sensor node.

VOLUME 5, 2017 4259    H. Harb et al.: Comparison of Different Data Aggregation Techniques in Distributed Sensor Networks  FIGURE 8. Energy consumption at the CH. (a) ? = 500, ? = 0.07.

(b) ? = 500, td = 0.4. (c) ? = 0.07, td = 0.4.

Fig. 8 shows the energy consumption comparison at the CH when using distances method, KAB and PFF tech- niques, in function of td in Fig. 8(a), of ? in Fig. 8(b) and of ? in Fig. 8(c). Depending on the results obtained in Fig. 6, the distances method gives the best results, except for td = 0.35 in Cosine, regarding the energy consumption in the CH. Subsequently, Euclidean distance can reduce the energy consumed in CH up to 39% and up to 64% compared to the amount of energy consumed using KAB and PFF respec- tively. Otherwise, Cosine distance can reduce up to 33% and 60% of the energy consumption in the CH compared to that consumed using KAB and PFF respectively. On the other hand, energy consumption in the CH is reduced, using KAB technique, from 32% to 54% compared to that consumed using PFF.

Since the energy consumption is minimized when the per- centage of sets sent is minimized, several observations shown in Fig. 8 can be concluded: ? Euclidean distance decreases the energy consumption in the CH from 9% to 40% compared to the Cosine distance. This is because the Euclidean distance sends less sets to the sink compared to the Cosine distance.

? Using Euclidean and Cosine distances, the CH con- serves more energy when td increases (Fig. 8(a)).

? The energy consumption in the CH, using distances and variance methods, is almost independent from ? threshold (Fig. 8(b)). Otherwise, PFF reduces the energy consumption in the CH when ? increases (Fig. 8(b)).

E. DATA LATENCY: EXECUTION TIME In this section, we compare the execution time required for the three data aggregation methods used in our technique when varying td , ? and ? respectively (Fig. 9). The execution time is dependent on the normalization process of data sets in distancesmethod, on the number of iterative loops in k-means algorithm used in KAB technique, and on the number of  FIGURE 9. Execution time at the CH. (a) ? = 500, ? = 0.07. (b) ? = 500, td = 0.4. (c) ? = 0.07, td = 0.4.

candidates generated in PFF. The obtained results show that KAB significantly outperforms the other methods function in terms of computation, in all cases. This is because, searching the groups of redundant sets in KAB requires less compu- tation time compared to the computation time required for comparison by pairs used in distances and similaritymethods.

Consequently, KAB can accelerate the execution time at the CH from 23 to 57 times compared to distances and from 14 to 26 compared to PFF. On the other hand, PFF can accelerate the execution time at the CH twice faster than distances method; the reason for that is the normalization used in Euclidean and Cosine distances which needs to calculate all distances between pairs of sets while PFF only searches the similarity between the generated candidate pairs.

Several observations can be made based on the results shown in Fig. 9:  ? The Euclidean distance decreases the execution time at the CH more than the Cosine distance. This is due to the complexity of the calculation of Cosine distance (Equa- tion 8) compared to Euclidean distance (Equation 7).

? The execution time required for both distances is almost fix when varying td (Fig. 9(a)). This is because both distances must normalize all data sets independently from td value.

? The data latency at the CH is optimized when ? increases, in all methods (Fig. 9(b)). This is because the cardinality of a data set decreases when ? increases thus the computation between sets decreases as well.

? The CH requires, with the three aggregation methods, more execution time when ? increases (Fig. 9(c)). This is because the cardinality of sets increases which require more time to be compared.

F. DATA ACCURACY: INTEGRITY OF INFORMATION Data accuracy is an important factor in WSNs which repre- sents themeasure ??loss rate". In our simulation, data accuracy  4260 VOLUME 5, 2017    H. Harb et al.: Comparison of Different Data Aggregation Techniques in Distributed Sensor Networks  FIGURE 10. Data accuracy. (a) ? = 500, ? = 0.07. (b) ? = 500, td = 0.4.

(c) ? = 0.07, td = 0.4.

has been evaluated based on the percentage of loss measures at the CH; in other words, we divided the number of measures taken by the sensor nodes whose values (or similar values) do not reach the sink, after applying each aggregation method, over the whole measures collected by the sensors at each period. Fig. 10 shows the results of data accuracy for the data aggregation functions used in our technique for different values of td , ? and ? . We can notice that PFF gives the best results for data accuracy, 2.81% in the worst case, compared to the Euclidean (up to 12.52%) and Cosine (up to 21.75%) distances and KAB technique (up to 33.8%). The reason for this is that the Jaccard function used in PFF is a strong constraint regarding the loss measures compared to distance and variance constraints which are more flexible. We can also notice that, Euclidean and Cosine distances conserve the integrity of the information more than the KAB technique.

Indeed, KAB sends one set among a group of sets to the sink which increases the loss of measures. It is important to know that with KAB technique, the objective is to send the minimum amount of data to the sink, which allows decision makers to take the correct decision based on the received information.

The following observations can be made based on the results obtained in Fig. 10: ? The Euclidean distance conserves the integrity of data more than Cosine distance in all cases. This is due to the equation of Cosine distance which eliminates the sets that have high cardinality.

? The loss of measures using Euclidean and Cosine dis- tances increases when td increases (Fig. 10(a)). This is because the CH eliminates more sets when td increases (see results in Fig. 6).

? The data accuracy, in distances and KAB techniques, increases when ? increases (Fig. 10(b)) or ? decreases (Fig. 10(c)). On the other hand, using PFF, the data accuracy decreases when ? or ? increases.

G. FURTHER DISCUSSIONS In this section, we give further consideration to our proposed techniques. We compare the obtained results while applying the three proposed methods. We give some directions as to which method should be chosen, under which conditions and in which circumstances of the application.

From the energy preserving point of view at the CHs, the three proposed methods significantly reduce the energy consumption in the CHs (Figs 8). In addition, we observe that the distance method conserves more energy compared to the variance and the similarity methods. Subsequently, it reduces up to 39% and 64% of the energy in CH compared to KAB and PFF respectively. Therefore, in the applications where we need to conserve the energy of the network as long as possible, the distance method is more suitable.

From the data latency point of view at the CHs, we deduce that the variance method gives the best result in terms of min- imizing the data latency, compared to distance and similarity methods. Subsequently, KAB can accelerate the execution time at the CH up to 57 and 26 times compared to the best results obtained using distances and PFF. These results are logical because, searching the groups of redundant sets using KAB has a weak complexity compared to search them in pairs using distances methods or in candidates using PFF.

Consequently, when the priority for the application is to deliver data to the sink, the variance method is more suitable.

From the data accuracy point of view at the CHs, the sim- ilarity method can totally save the integrity of the collected data without any loss of information, e.g. up to 2.81%. On the other hand, the distance method gives better results in terms of data accuracy compared to the variance method. Hence, if the application does not permit flexibility regarding data accuracy, the similarity functions method is more suitable; else, distance and variance methods can be used as a compro- mise between energy saving and data accuracy flexibility.

To summarize this section, Table 1 shows the flexibility of each method regarding energy consumption, data latency and accuracy, and complexity of the method at the CHs.

TABLE 1. Comparison between distance, variance and similarity methods.

As an analytical study, each sensor node Si will form a set Mi of ? measures in each period. Due to Similar func- tion, the size of this set will be reduced from ? to |Mi|.

Therefore, our proposed technique has at most O(|Mi|2) as a computation complexity at the sensor and it will save at most (2?|Mi|) measures at each period in its memory. These complexities are suitable for the case of sensor node since col- lected measures are usually redundant thus, makes |Mi| small even in the worst case. This fact is showed clearly in Fig. 4  VOLUME 5, 2017 4261    H. Harb et al.: Comparison of Different Data Aggregation Techniques in Distributed Sensor Networks  where the data collected by each sensor in each period is significantly reduced. On the other hand, the complexity of our technique at the CH level is dependent on which data aggregation method the CH uses (see Table 1). Since the CH must enumerate and compare every pair of set, the complexity of the distance functions is in order of O(n2), where n is the number of all received sets. Otherwise, the complexity of KAB method is at most in order O(n) thanks to k-means algorithm which only compare the sets in the same cluster.

Finally, the complexity of PFF is dependent on the complexity of the prefix filtering calculated in [47] which it is at most in order of O(n? log(n)) since the comparison is reduced to the candidates pairs only.

Finally, the message complexity in our technique is highly dependent on the number of data collected in a period, e.g. ? , which is fixed by the application; in the case where a large ? is needed, different solutions could be applied like packet division, etc. On the other hand, the storage complexity depends on the memory size of the sensor in addition to the period ? , which can be treated similarly to the message complexity. Thus, the storage complexity is dependent on the sensor nodes capabilities and memories and then data collection frequency can be defined in function of that storage capacity.



IX. CONCLUSIONS AND FUTURE WORK Data aggregation is very essential for WSNs where the huge amounts of data collected by the sensors need to be mini- mized. In this paper, we proposed a data aggregation tech- nique for clustering-based periodic sensor networks. After eliminating redundant data collected by each sensor, we propose three different data aggregation methods allowing CH to eliminate redundant data sets generated by neighbor- ing sensor nodes. The proposed methods are based respec- tively on the sets similarity functions, the one-way Anova model with statistical tests and the distance functions. We have demonstrated through experiments on real data mea- sures the efficiency of our proposed technique in sensor networks in terms of energy consumption, data latency and accuracy.

In our future work, we plan to schedule the sensor nodes in the network in a manner that nodes generating redundant data will not be active at the same time. Thus, sensor nodes will conserve more energy and network lifetime will be extended.

