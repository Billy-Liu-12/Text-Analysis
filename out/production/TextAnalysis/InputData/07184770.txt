Mount Wuyi, Fujian, China; March 27-29, 2015

Abstract-Mining maximal frequent itemsets is a  fundamental and important issue in many data mining application. A new depth-first search algorithm for mmmg maximal frequent itemsets called DFMFI (depth-first search for maximal frequent itemsets) is proposed, which can reduce the  number of candidate itemsets and the cost of support counting.

DFMFI projects the dataset information stored by the compressed FP-tree into the conditional matrix, and improves  efficiency of support counting by using vector logic operation.

Global 2-itemset pruning and local extension pruning used to prune the search space effectively. The experiments results verify the efficiency and advantage of this DFMFI.

Keywords: Data mining, Maximal frequent itemsets, Conditional matrix, Compressed FP-Tree

I. INTRODUCTION  W ITH the development of data mining technology,  mining frequent itemsets has become a basic and  crucial step [1] in some data mining issues like association  rules mining, sequential patterns mining, multi-layer pattern  mining etc. When the support threshold is low or data sets are  very dense, there are many long frequent items in data sets,  the quantity of frequent itemsets generated by mining is too  big, so the performance of mining algorithms is greatly  affected. However, novel and valuable information is often  hidden among the long frequent modes in the dense dataset.

Since the maximum frequent item sets has been implied all  frequent item sets, and their number is much smaller than the  complete frequent item sets, so mining maximum frequent  item sets can effectively reduce the size of problem solving, it  is of great significance to a rapid discovery of the long  frequent patterns among dense datasets.

There are a number of mining maximal frequent item sets  algorithms at present. MaxMiner [2] is a breadth-first  algorithm, proposed the concept of the search space tree and  pruning to increase the efficiency of the use of  forward-looking dynamic ordering method. DMFI [3]  Manuscript received November 29, 2014. This work was supported in part by the Science and Technology Foundation of the Education Department of Jiangxi Province under Grant GJJ13086, in part by the Science and Technology Support Program of Jiangxi Province under Grant 2011ZBBE50029, and in part by the Undergraduate Teaching Quality Project of the College of Science and Technology of Nanchang University under Grant 2013JXTDOOl.



X. zhang is with the College of Science and Technology, Nanchang University, Nanchang, Jiangxi, 330029, China.

K. L. Li is with the College of Science and Technology, Nanchang University, Nanchang, Jiangxi, 330029, China.

P. Liao (corresponding author) is with the College of Science and Technology, Nanchang University, Nanchang, Jiangxi, 330029, China (e-mail: pinliao@ncu.edu.cn).

algorithm and DMFIA [4] algorithm combines the top-down  bottom-improved bi-directional pruning strategy, MaxMiner  algorithms is thus improved. Mafia [5] to represent the data  set is a projection, discusses three methods of pruning the  search space tree depth-first algorithm based on memory,  using a bitmap format. FPMax [6] is an effective depth-first  algorithm using FP-Tree to compress the data to indicate the  associated information frequent item sets.

This article offers a new depth-first search algorithm for  mining maximal frequent itemsets DFMFI (depth-first search  for maximal frequent itemsets). The use of Compressed  Frequent Pattern tree algorithm for CFP-Tree [7] compressed  representation of frequent itemsets information, it bring the  subsets of data into a binary matrix projection condition,  while in the pruning strategy, it using the global 2-itemset  pruning and local extension pruning. The experimental  results show that this algorithm execution efficiency is  superior to other similar algorithm.



II. FREQUENT ITEMSETS AND MAXIMAL FREQUENT ITEMSETS  Let 1= {i],i2, ... ,in} be a set of distinct literals, for every  X!;;; I, X is called itemset. An itemset X with k items is called  a k-itemset. Let D={T],T2, ... ,Tm} be a transaction database,  transaction Ti!;;; I in it, if itemset X!;;; Ti , said transaction Ti  containing itemset X. The transaction number of itemset X  included in D is called support number of itemset X in D,  marked sup _ count(X). The percentage of the itemset X  transaction number in total number contained in D named  support of itemset X in D, marked support(X), the minimum  support threshold specified by user marked min_sup. The  minimum support count marked min_count.

Definition 1: Let X be a itemset, for every X !;;; I, if  support(X) ;:::: min_sup, then X is called frequent itemset in D.

Definition 2: Let Y be a superset of frequent itemset X,  Xc Y, for every Y such that support(Y)<min _sup, then X is  called maximal frequent itemset in D, and MFI denotes the set  of all maximal frequent itemsets in D.



III. COMPRESSED FP-TREE  CFP-Tree(Compressed frequent pattern tree) is one of the compression storage form for FP-Tree(frequent pattern tree), CFP-Tree put all subtrees of the root node of FP-Tree in the Root (except the left sub tree) converge to the corresponding node on the left most branches, effectively reducing the number of nodes in the FP-Tree, so as to realize the compression storage of frequent patterns. All frequent items    in Item Table of CFP-Tree is in descending order according to the number of support. Node record of CFP-Tree frequent items in descending order after the corresponding sorting sequence number index. Count array of CFP-Tree node  record is from the root node to the current node in the path, all set to current node as a suffix of the sub path corresponding to the count. The structure CFP-Tree algorithm as shown in [ 8].

Example 1: Set transaction database D as shown in table 1.

TID  TABLE I TRANSACTION DATABASE D  Items TID a,b,c,e 7 a,b,d 8 a,b,e 9 a,c,e 10 a,c,e 11 a,c,f 12  Items a,c  b,c,d b,c,e b,c  b,d,e c,d,g  The support numbers of item a, b, c, d, e, f and g are 7,7,9,4,6, 1, 1, and the corresponding number were 2, 3, 1, 5,4. If the minimum support count is 2, then the generated CFP-Tree as shown in figure 1.

Level-O  ItemTable Inde Item onn Lin1'  I 9 a 7 ..- b 7  4 6 .??? d 4  Fig. l. CFP-Tree of transaction database D.

Theorem 1: Set I = {i], i2, ... , id is the frequent itemsets ordered by descending order in Item-Table of the CFP-Tree.

The corresponding number is {l, 2, ... , k} , for ij E I,  1 ::; j ::; k, starting from the node chain headed with j, traversing the node chain on all nodes of j count array j.array = [ co, c], ... , cr], 0::; r<k, then all paths ended with node j can be getted.

For every node n in CFP-Tree, it's count array n.array = [ co, c], ... , Ck_l]. If n.array[ i] ;::>: min_count, 0::; i ::; k-l, then the itemsets that corresponding path from node level-i to node n is the frequent itemsets.



IV. MAxIMUM FREQUENT ITEM SETS MINING ALGORITHMS  A. Conditional Matrix Definition 3: if I={i],i2'" .,id is CFP-Tree the frequent  itemsets in the table of the project, for ijE I, l::;j::;k ,there is  Ij={i],i2, ... ,ij} ,  Ij k I, according to the nature of the four, in the CFP-Tree traversal path to all nodes j as suffix, a collection of paths is Sj={S],S2, ... ,Sm} ' count of the path is CVj={C],C2, ... ,Cm} . If CM={lIpq} ,  1 ::;p::;m, 1 ::;q::;j, is a matrix mxj, make the node j as the suffix path Sj projection onto the matrix CM, when iqE sp, lIpq=l, when iqE Sp, lIpq=O,   said CMj as condition matrix of frequent items ij, called CVj as the count vector of condition matrix CMj.

Example 2: A CFP-Tree is shown in figure one, Condition  matrix C? and count vector CV 4 of frequent item e Numbered 4 as shown in figure 2.

c ab e CV4  CM4 l ? l ?? II 1 0 1 1 1 1101 2 '-'  Fig. 2. The condition matrix and count vector of frequent item e  Theorem 2: Set t the r column of condition matrix CMj corresponding column vector is Vn 1 ::;r<j, then the support number of 2-itemsets {ir, ij} is sup _ count({ir, ij} )=Vr - CVj, " - " is vector inner product operator.

Theorem 3: Set column vector of Vr], Vr2, ... , Vm, 1 <n<j, corresponding to columns rIo r2, ... , rn in condition matrix CMj, then the support number of (n+ 1 )-itemsets {ir], ir2, ... , im, ij} is expressed as sup _ count( {ir], ir2, ... , im, ij} )  = (VrinVr2n ... nVm) - CVj-  Example 3: In condition Matrix C? as shown in figure 2, column vector VI=(I,O,O,I,l), V2=(1,1,0,0,1), counting vector CV4=(1,1,1,1,2), then the support count of 2-itemsets {i], 4} is sup_count( {ii, i4} )=(1, 0, 0,1, 1) - (1,1, 1, 1,2) = 4, 3-itemsets{i], i2, 4} sup count ({i], i2, i4} ) = ?(1,0,0,1, l)n(l, 1, 0, 0,1))- (1,1,1,1,2) = 3 .

Set I={i],h, ... ,ik} is the CFP-Tree frequent itemsets in the table, the mining maximum frequent itemsets in the CFP -  Tree MFI process can be divided into k steps, including the first j for tectonic conditions matrix CMk+l_j, then in CMk+I_j mining all contain ik+l_j, but does not include the in r>k+ I-j, the maximum frequent itemsets MFh+l_j, l::;j::; k. So for arbitrary frequent items ij E I, 1 ::; j ::; k, includes the ij but does not contain the ij+ l,ij+2, ... ,ik maximum frequent itemsets exists and only exists in the condition of the ij matrix CMj.

In the CFP-Tree can tap into all maximal frequent itemsets MFI in the transaction database D.Note that:the j step find frequent itemsets X, there may be a maximum frequent itemsets, there also may be found a maximal frequent itemset subset of MFI, therefore, the need for frequent itemsets X be a superset of the detection, to determine whether the X is the maximum frequent itemsets.

B. Global 2-Itemset Pruning and Local Extension Pruning When apply depth-first search in the search space, the  reasonable use of pruning strategy can efficiently prune the search space, and to improve the algorithm performance. The basic pruning strategy, such as look-ahead pruning and parent equivalence pruning, only play a role in relative search space of each matrix, and DFMFI algorithm uses two new pruning strategies: Global 2-itemset pruning and local extension pruning. Global2-itemset pruning is using the correlation between the condition of the matrix item set support to determine whether a condition of the matrix corresponding to a subset of the overall enumeration tree can be pruned; Local extension pruning is a local extension  matrix corresponds to a subset of the conditions inside the    enumeration tree, according to the support of the relationship between the current node and its parent node, thus to determine whether the right of the current node branches can be pruned accordingly.

Theorem 4: For two condition matrixs CMj and CM" j>r, if sup _ count( {ij,ir} )=sup _ count( {ir} ), the correspondence subset enumeration tree of CMr can be Pruning whole.

By definition 3, in subset enumeration trees that make ij as the root node corresponding with CMj, ir E {ij} .tail, the extension subtree that make {ij,ir} as root contains a subset enumeration tree corresponds to CMj. If sup _ count( {ij,ir} ) = sup _ count( {ir}), then showed that all frequent itemsets in {ir} u {ir} .tail are included in {ij,ir} U {ij,ir} .tail, and traversal on {ir} U {ir} .tail will not produce the new maximal frequent itemset. Therefore the subset enumeration tree corresponding to CMr can be integrally pruning.

According the formula of theorem 2 can quickly calculate the support number of {ij,ir} , he support number of {ir}can find out in the project table (ItemTable) directly, using the support equal features of {ir} and global 2 itemsets, set CMr Pruning in whole, Called Global 2-itemset pruning.

Theorem 5: For node N and its parent node N.parent in subset enumeration tree corresponding to the condition matrix CM, if there exists itemset i, such that i E N.tail, and N's extended support number in itemset i is equal to N.parent's extended support number in itemset i, then all tree nodes and branches included in the subset enumeration tree node N's right branch can be pruned. Assuming N={ix, ... ,ip,iq} ,  then N.paretn={ix, ... ,ip} ,  project ijE N.tail, Form the structure of subset enumeration tree, we know 1 ::;j<q<p::; k, if N and N.parent have equal extension support in the ij, that is count(N u {ij} )=count(N.parent u {ij} ), means all set items contain in N.parentu {ij} all included in the extension focused of N u {ij} ,  So in subset enumeration tree, the right side branches of N node which contain all nodes and branches of N.parentu {ij} can be pruned.

In a single condition matrix search space, using characteristics that the node and parent node are equal in a project extension support, pruning the enumeration subtree, referred to as the local extension pruning. You need to construct a Local Pruning Itemset is used to hold a single condition matrix need to be in a subset of the enumeration tree matching set of items when the local extension pruning.

Based on the support is a technology, using global two pruning and local extension pruning can quickly cut a large number of repeated search space, to improve the efficiency of algorithm effectively.

C. Algorithm Description Under the structure of CFP-Tree, the problem of mining  maximum frequent itemsets can be decomposed into several same-process sub problems. Therefore, the basic ideal of DFMFI algorithm is to use CFP-Tree structure to compress and save transaction information in the database, then, in descending order of frequent itemsets 1={iJ,i2' ... ,ik} ,tectonic conditions CMj,1 ::;j::; k matrix in turn in the order ik, ik-J, ... , iJ.In the maximum frequent itemsets mining CMj contains, about CMj contains all the frequent items {iJ,i2' ... ,ij} ,in ij as a subject of the enumeration tree, using depth first strategy to   traverse the search space. On the pruning strategy, the algorithm in addition to using look-ahead pruning and parent equivalence pruning both effective pruning strategy, also uses two sets global and local extension pruning the pruning strategy.

DFFMI algorithm: Input: CFP-Tree of D, Frequent intemsets of project table  1={iJ,i2' ... ,id, The label array IA  Output: The maximum frequent itemsets collection MFI in  D  1) for each item ij in I from ik to iJ do begin 2) construct CMj Ilconstruct conditions matrix inturn 3) LPlj = 0 lllnitialize local pruning set 4) if (j<k and there exists r (j<r ::; k) such that  sup _ count( { ij} ) = sup _ count( {i"ij} )) then 5) continue IIGloba12-itemset pruning 6) construct maximal tail itemset TIj of ij liThe tail itemsets  7) sort {ij} .tail ending with TIj IISort items 8) if !(IA_SupCheck(TIj, MFI, IA)) then IISuperset  checking 9) add TIj to MFI and add line number to IA 10) for each item x in {ij} .tail (x? TIj) do  11) DF _Search(x, MFI); I/ Depth fIrst search 12) end 13) return MFI procedure DF _Search (itemset N, MFI, LPI)  Input: Node itemsets N; The maximum frequent itemsets  MFI; Local pruning set LPI  1) if (IA_SupCheck(N u N.tail, MFI, IA)) then return I / Look -ahead pruning  2) use PEP to trim N.tail and reorder by increasing support I I Father equivalence pruning  3) if there exists N' ?N such that N' E LPI then return II  Local extension pruning 4) if there exists item i (iE N.tail) such that count(N u {i})  = count(N. parent u {i} ) then  5) add N.parentu {i} to LPI IIGenerate local pruning set  6) for each item t in N.tail do  7) if Nut is frequent then 8) DF _Search (N u t, MFI, LPI)  9) if !(lA_SupCheck(N, MFI, IA)) then II Superset checking  1 0) add N to MFI and add line number to IA  v. EXPERIMENTAL RESULTS AND ALGORITHM ANALYSIS In order to validate the performance of the algorithm,  compare DFMFI algorithm, FPMax algorithm and Mafia algorithm by experiments. Algorithm using VC++ implementation, the experimental environment is 2.2GHz Intel Core i3-2330, 4GB memory and Windows 2003 Server operating system. The experimental data sets by using two relatively more dense data sets Connect (average transaction length 43) and Pumsb* (average transaction length 50.5). The experimental results shown in figure 3 and figure 4.

Fig. 3. The execution time of each algorithm under Connect.

Fig. 4. The execution time of each algorithm under Pumsb*.

Experimental results show that the DFMFI algorithm's efficiency is superior to FPMax algorithm and Mafia algorithms. FPMax algorithm will generate a large number of subtrees during execution conditions, and each tree's establishment will need to sub-scanning twice the condition subtree in parent node in search space, so it takes more time.

DFMFI project data on the condition matrix, and use of binary logic operations to simplify the support vector calculation, using two sets of new global and local extension pruning to effectively minimize the search space, thereby reducing the computation time. Meanwhile DFMFI is using CFP-Tree related information to save the original data set, which has better space efficiency in compare with FP-Tree structure used by FPMax algorithm.



VI. SUMMARY  This article presents an efficient mining maximum frequent item sets algorithm DFMFI. The CFP-Tree algorithm it used will sufficiently compress the database, conditions of using matrix data and using binary logic operations to simplify the entry vector set support calculation, while using the new global two sets pruning and local extension pruning strategy explores the full potential of depth-first search, and maximize the use of existing frequent information to minimize the search space, reducing the maximum number of candidate   frequent item sets. Experimental results show that the DFMFI algorithm has good time and space efficiency.

