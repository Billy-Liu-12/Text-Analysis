The Research of Text Categorization based on FP-tree

Abstract-Decision tree is a forecasting model with a tree shape.

The way from the root node to the leaf node forms the rule that  forecasts a class label to the object. But it is often used in the  condition of having not too much attributes. And FP-tree is a  fast and efficient structure to discover frequent pattern. The  paper proposes a new fast method for categorization through  FP-tree. It discovers frequent feature terms of documents and  categories using FP-tree and then forms a class decision tree  by using the top frequent feature term to be the test attributes.

So the decision tree is used in text categorization by the  frequent feature terms. Finally the paper gives the experiment  and the analysis of the method.

Keywords?decision tree; fp-tree, mutual information

I.INTRODUCTION  The sudden expansion of the Web and the use of the Internet have triggered a lot of research fields, text categ- orization(TC for short) is only one of them. TC is the distinction between predefined classes of text by identifying discriminating features. Given a set of cases with class labels as a training set, classification is to build a model (called classifier) to predict future data objects for which the class label is unknown. Many techniques have been applied in TC, such as Bayesian Networks, decision trees, support vector machines, etc.

Another aspect that motivates our research is the success of association rule mining. The use of association rules has been exploited for finding new and interesting hidden patterns in large transactional databases. Associa- tion rules are rules that identify associations between ite- ms in transactions.In recent years many algorithms have been proposed for computing association rules efficiently.

Among the proposed algorithms the best known are apriori[1] and FP-tree growth[2].The concept of association rules mining has been intensively used in market basket analysis and also in a myriad of domains. So we can also take it into text classification field. Decision tree learning  is a widely used method for pattern recognition. Many decision tree learning algorithms like C4.5[3] and ID3[4] are used to discover some new pat- terns and it not only can deal with discretization attributes but also can deal with binary discretization for continuous-valued features and multi-interval discretization attributes.

In this paper we exploit the use of association rules mining in building a TC system from a relatively large training set. At first we preprocess each document in the training data set by separating words according to the par- agraph in the document, eliminating the stopwords and st- emming and record the number of the feature term. Then we compute the mutual information (MI) between each word and the corresponding class label and select the fea- ture terms which MI is no less than the threshold ?. Then we discover the association relationships between object conditions and class labels using FP-tree by replacing the support number of the item with MI computed above. Fin- ally we construct the decision tree using FP-tree. In the decision tree every test attributes is the class feature term and the range of the MI is the branch of the tree. And the leaf node is the class label. So for each test document we can classify it to a class along the decision tree.

The remainder of the paper is organized as follows: Section 2 describes the method of decision tree. Section 3 proposes a new fast method for building a text classifier using decision tree using FP-tree. Section 4 gives the analysis of the method.



II. DECISION TREE  Decision tree learning is a widely used method for pattern recognition and image interpretation. It is a tree structure which is likely to be a data flow graph. Every inside node denotes a test on an attribute and every branch represents a test output. And every leaf node shows a class or the distribution of the class. The top level node of the tree is the root node.

DOI 10.1109/WISM.2009.43     There is an example of decision tree showing in figure 1. It suggests the relationship between the sports of playing golf and the weather. There are thirty four records in the table and every record have five attributes. Among these attributes the former four are conditional attributes such as outlook, temperature, humidity and windy showing in ellipse form in figure 1. The last one is class attribute which is called the result attributes showing in rectangle form in figure 1. It?s domain is {Play(for short ?P?),not play(for short ?N?)}. And the number in the bracket denotes the number of the instances which the node contains.

There are many algorithms about the decision tree such as C4.5[3],ID3[4] and so on. A decision tree was constructed for each category by using the test of a certain attribute. It is grown by recursive greedy splitting through selecting an attribute which has the best effect of classification. So the core problem of the decision tree is to select a split attribute. Also there are many method to select the test attribute such as information gain, the ratio of the information gain, gini index,?2 statistics and the Bayesian posterior probability and so on.

? A TEXT CATEGORIZATION METHOD BASED ON FP-TREE  A. Feature Selection via Mutual Information  Let c and w be binary random variables indicating whether or not the category c and the word w occurred.

The value of MI between c and w is defined as follows:  {0,1} {0,1}  ( , )( , ) ( , ) log (1) ( ) ( )w c  w c w ce e  w c  P e eI w c P e e P e P e? ?  =? ?  where ew and ec are Boolean indicator random variables of the word w and the category c, respectively.

Note that when evaluating I(w,c) from a sample of documents, we can compute P(ew,ec), P(ec) and P(ew) using the training document. Consider, for instance, ec = 1 and ew = 1.Then P(ec;ew) = Nw(c)|N(c) , P(ec) = N(c)|N , P(ew) = Nw|N , where Nw(c) is a number of occurrences of  word w in category c, N(c) is the total number of words in c, Nw is a number of occurrences of word w in all the categories, and N is the total number of words.

B. The method of text categorization based on FP-tree  1) FP-tree construction: Let I = {i1,?, im} be a set of items, and a transaction database D= <T1;?; Tn>, where Ti (i?[1,n]) is a transaction which contains a set of items in I. The support (or occurrence frequency) of a pattern A denoted as A.supD, which is a set of items, is the number of transactions containing A in D. A is a frequent pattern if A's support is no less than a predefined minimum support threshold ?.

Definition 1: For a item set X?I, if A.supD?? and for each Y?X there will be Y.supD<? then we will call X as the maximal frequent item set in D.

So we can apply it in TC to find frequent feature terms.

At first we should pre-process the document by dividing it into terms according to paragraph and register the number of the term at each paragraph and compute the value of MI between the words and the corresponding category. So we can view the document as the transaction database D, view the paragraph contained in the document as the transaction and view the words contained in the paragraph as the items in the transaction.

In the frequent pattern-tree of the algorithm every node consists of six fields shown in table 1 where node-name registers which item this node represents, node-count registers the number of transactions represented by the portion of the path reaching this node, node-fchild repres- ents the name of first child of the node. If it is a leaf node its value is null, node-MI registers the value of MI, and node-link links to the next node in the FP-tree carrying the same node-item, or null if there is none. In addition for convenience of visiting the tree it creates a frequent item head table Htable. It consists of three fields: item-name, item-next and item-head where item-head points to the first node that has the same name in the FP-tree. And the head point of the frequent item link table points the item which number of support is minimum.

The structure of the node in FP-tree is as follows: Table 1 the structure of the node in FP-tree  node-name,node-countnode-fchildnode-link, node-MI node-parent  Algorithm 1: FP-tree construction Input: A transaction database D and a minimum support  Figure 1  the decision tree  not Very  hot cool mild NormalHigh  Rain overcast  Sunny Outlook  windy  Humidit TemperaturP(7)  N(5) P(14) N(5) N(1)  N(1)P(1)     threshold ?.

Output: Its frequent pattern tree, FP-tree 1. Scan the transaction database D once. Collect the set of frequent items F and their supports and sort it in support descending order as the list of frequent items L.

2. Create the root of an FP-tree, T, and label it as null. For each transaction Trans in D do the following: select and sort the frequent items in Trans according to the order of L. Let the sorted frequent item list in Trans be [p|P], where p is the first element and P is the remaining list.

Call insert_tree([p|P]; T).

3. If P is nonempty, call insert tree(P;N) recursively.

The function insert_tree([p|P]; T) is performed as follows. If T has a child N such that N.item-name = p.item-name, then increment N's count by 1 and the value of MI is the sum of these two items; else create a new node N, and let its count be 1, its MI is the item?s MI; its parent link be linked to T, and its node-link be linked to the nodes with the same item-name via the node-link structure, And then do such judgment as following: if the value of node-fchild of T is null it suggests that T has no child node. Then we give the value of node-fchild of T with the value of node-name of node N. We can use it to judge whether the node is a leaf node or not. If the value of node-fchild of node T is null it is a leaf node; else it is not a leaf node. There is a database and its FP-tree is figure 2. And in figure 2 we only show three attributes for every node: item-name, node-count and node-fchild.

Table 2  the database  TID The item of  TID  TID The item of  TID  TID The item of  TID  P1 t1?t2?t5 P4 t1?t2?t4 P7 t1?t3  P2 t2?t4 P5 t1?t3 P8 t1?t2?t3?t5  P3 t2?t3 P6 t2?t3 P9 t1?t2?t3    2) Mining Frequent Patterns using FP-tree:There is a property of a node in fp-tree as follows: Property 1: if node-count of node is no less than the minimum support threshold min_sup, the node-count of predecessor node is necessarily no less than min_sup.

From the construction of FP-tree we can know that the value of node-count of the parent node is no less than the node-count of child node. The method search the maximal frequent item set according to the element of the item chain. Because the element of the chain is in an increase order of the support we should consider the item which has minimum support. And we do the following recursive- ly. It finds all nodes nd1,?,ndh which item-name is the same as the node we deal with and finds all paths P1,?,Ph which contain the item according to nd1,?,ndh. And then it deal with every node of nd1,?,ndh. Firstly we should judge if pi is a subset of any element of MFSD ie we judge that pi is in MFSD or is included by an element of MFSD.

If not then do the following:?1. if node ndi is a leaf node and its support number is no less than minimum support min_sup we will put pi into MFSD; else we would judge whether there exists any node ndj(i<>j) which has the same item name and ndi.node-count+ ndj.node-count>= min_sup and also pi is a subset of pj then we put pi into MFSD.?2 if node ndi is not a leaf node and its support number is no less than min_sup we will put pi into MFSD; else we will judge if there exists a node ndj(i<>j) which has the same item-name as ndi, ndi.node-count+ ndj.node-count>= min_sup and pj is a subset of pi we will put pi into MFSD. So we can get the maximum frequent item set.

Algorithm 2: Discovery Maximal Frequent Item set Algorithm DMFIA Input: The frequent pattern tree of D FP-tree; frequent item chain table Htable; the minimum support threshold min_sup; the frequent item list of D: LDF Output: the maximal frequent item sets of D: MFSD.

(1)MFSD=??(2)P=Head? (3)while(p!=nul1){ (4)find all nodes nd1,?,ndh which item-name is p.item-name in FP_tree; (5) find all paths P1,?,Ph which contain p.item-name according to nd1,?,ndh and its prefix node?s parent point.

(6)for(i=1;i<=h;i++){ Figure 2  the FP-tree of database  Null, 0,t2  t2,7, t1  t1,4 t5 t3:2,  nullt5:1, null  t4,1, null  t3,2, t5  t4,1, null  t5:1 null  t1,2 t3  t3:2, null  t2 t1 t3 t4  t5  ?     if(Pi is not a subset of any element of MFSD){ if(ndi.node-fchild==nul1){//if ndi is a leaf node if(ndi.node-count>=min_sup) {MFSD=MFSD?pi;} else for(j=1;j<=h;j++){ if(i?j)and(ndi.node-count+ndj.node-count>=min_sup)and( Pj includes Pi) {MFSD=MFSD?pi; The support number of ndk in path pi adds ndj.node-count and the MI adds ndj.node-MI;}} Else//if ndi is not a leaf node if(ndi.node-count>=min_sup) {MFSD=MFSD?pi;} Else for(j=1;j<=h;j++) if(i?j)and(ndi.node-count+ndj.node-count>=min_sup) and(Pj includes Pi) { MFSD=MFSD?pi; The support number of ndk in path pi adds ndj.node-count and the MI adds ndj.node-MI;}} p=p.item-next?}//next item of hashtable  3) The method of text categorization based on FP-tree  Algorithm 3: A TC method based on FP-tree(TCMFP) Input:A category sets C={c1,?,cm} and the document set D={D1,?,Dm}where Di={di1,?, dini}(1?i?m)and for each dij?Ci,1?j?ni. a threshold ?, minimal support threshold s Output: A decision tree for each category Ci(1?j?m).

1.we preprocess each document dij in the document set D by separating words according to the paragraph in the do- cument dij, eliminating the stopwords and stemming. We select the feature terms which weight in the training docu- ment is no less than the threshold ? and no more than the threshold ? through the traditionnal method tfidf. Then we can get a term co-occurence sets in the same paragraph of the document dij and compute the occurrence frequency of each term in the paragraph. Then we can compute the value of I(w,ci) between the feature term w in the parag- raph and the category ci according to the formula (1).

2. Get the frequent feature terms of each document dij(1?i?m, 1?j?ni) for each category.

For each category Ci we model paragraphs in the document dij as transactions where items are feature terms from the corresponding paragraph.We used the algorithm (1) for constructing the frequent pattern tree FP-tree of the document dij and then used the DMFIA algorithm for mining frequent item-sets in transactional databases in order to find frequent sets of words in the documents dij.

At one time we can get MI between the frequent feature words and the category Ci. Then repeating it as above we can get the frequent feature words of every document for each category Ci. Among the set of frequent feature terms for the document we select the set which the sum of MI is maximal as the feature terms to describe the document.

3.Get the frequent feature terms of each category Ci(1?i?m).

At the basis of the step (2), we model every document belonging to the category Ci(1?i?m) as transactions where items are frequent feature terms get from step (2). Like above we first construct the FP-tree by using the algorithm (1) and then mine frequent feature terms of the category Ci and get the value of I(w,Ci) by using the algorithm DMFIA.

4. Construct a decision tree for the category.

Among the set of frequent features for the category gained from step(3). We select those which have maximal MI to describe the category and integrate all the feature terms to an attributes list and view it as the test attributes to decide the branch of decision tree. When constructing the decision tree we select the test attribute according to the value of MI. Also we separate the samples into some parts according to the support number of the test attribute.

Then for the test attributes every branch is a set whose value of the corresponding attribute?s support number is in a discrete scope. Then we get the decision tree and get the classification rules for each category.

For a test document we only compute every feature term?s support number then it can be classified to a class using the decision tree.

The whole process of algorithm TCMFP is described in figure 3.

Figure3 the process of algorithm TCMFP  Training document  separate words according to the  paragraph  Eliminate the stopwords and  stemming  Construct decision tree for each categ- ory using FP-tree  select the words which weight is between ? and?  Mining the frequ- ent feature words of the category  Mining the frequent feature words of the  document using  Test documents  Preprocessing and compute the  support number  Evaluate the  algorithm  Compute the MI(w,ci), sort it in decrease order     IV THE EXPERIMENT OF  THE METHOD  Reuters-21578 is widely used in TC. The TOPICS gro- up of categories contains 135 categories. We first delete the documents which belong to two or more categories and select the 10 most popular categories as table 3 shows.

Table 3 ten categories that the algorithm select 0 earn 2 money-fx 4 crude 6 interest 8 ship 1 acq 3 grain 5 trade 7 wheat 9 corn  A. Data pre-processing  We eliminate the stopwords and the digital number and stem the remained feature terms using the algorithm Porter stemmer. These two methods are general methods to preprocess the training documents and they can compress the information without any loss and also they can?t loss the feature information which is in favor of the classification. As the feature words of the category is so much that we adopt the pre-pruning strategy in the construction of the decision tree. We stop the growth of the decision tree when it gets to a certain height h.

B. The Analysis of the method  From the FP-tree construction process, we can see that one needs exactly two scans of the transaction database D: the first collects the set of frequent items, and the second constructs the FP-tree. The cost of inserting a transaction Trans into the FP-tree is O(|Trans|), where |Trans| is the number of frequent items in Trans. We will show that the FP-tree contains the complete information for frequent pattern mining. Furthermore, without considering the (null) root, the size of an FP-tree is bounded by the overall occurences of the frequent items in the database, and the height of the tree is bounded by the maximal number of frequent items in any transaction in the database.

In the step one we delete the feature terms whose weight is not in the given scope using the traditional method tfidf. This can reduce the nodes of the FP-tree. So it can reduce the account of the computation in the process of constructing the FP-tree and discovering the frequent feature terms through FP-tree. Thus it can save some time.

In general the feature space of the document is very large and if the method can reduce the number of scanning the database it will decrease the training time to some certain extent. Further algorithm DMFIA computes the support not based on the original database but the FP-tree.

The whole algorithm compresses the original database into  a FP-tree. Based on the FP-tree DMFIA transfer the multiple scan of the original database into the visiting of the FP-tree in the memory. It avoids multiple scan of the original database in the hard disk. In the process of finding maximal frequent item set the time complexity of DMFIA is O(mn) under the best condition which m is number of the frequent item and n is the node number which item-name is the same as the item analyzing in FP-tree and the worst time complexity is O(mn2).

In addition the method of extracting the frequent feature terms is from the paragraph of the document. So the frequent feature terms gained can express the extent of the document better. It is of help of the text categorization.

Also when we construct the decision tree we select the test attribute according to the mutual information gained from the method. So it needs not some extra computation and it can?t add the time complexity of the method.

V CONCLUSION  In this paper, we have proposed a new method of TC based on FP-tree. The method discovers frequent feature terms of the document using FP-tree and moreover it gains the frequent feature terms of the category. On the basis of the above result it constructs the decision tree of the class by using the frequent feature terms as the test attributes. It forms a fast classifier. But there are some parameters that need artificial setting such as the threshold and the minimum support threshold s. It needs some experience or some experiments to decide.

