A Web Image Retrieval Re-ranking Scheme with Cross-Modal  Association Rules

Abstract   This paper proposes a new re-ranking scheme and presents experimental performance results for web image retrieval with integrated query. In our previous work, cross-modal association rule was designed for associating one keyword with several visual feature clusters in web image retrieval. Based on the cross- modal association rule, we implement an automatic re- ranking process online to integrate the keyword and visual features for web image retrieval, and gives experimental test. The experiment is carried out in a web image retrieval system named VAST (VisuAl & SemanTic image search), and the results show the effectiveness of the re-ranking method scheme.

1. Introduction   The images on the Web are generally exposed as part of web contents. Both text and image content can contain useful information that should be exploited in retrieving web images [1]. They bring information that are complementary and that can disambiguate each other [2]. Most of the past web image retrieval systems, such as WebMars [3], Cortina [4], and Atlas WISE [5], etc., integrate text and image visual features together by the Relevance Feedback (RF) mechanism, which adds the user?s burden for interaction.

Due to the laziness of web users [6] and for the sake of convenience, it is valuable to automatically associate and integrate the two modalities, i.e. keyword and visual feature, for web image retrieval.

In this paper we build upon our earlier work where we mined cross-modal association rule in web image collection [7]. In our earlier work, the cross-modal association rules associate one keyword with several visual feature clusters, which gives a possible way to realize an integrated query automatically in web image retrieval. Here, a new re-ranking approach is proposed based on the mined cross-modal association rule, and an experimental performance test is carried out in a web image retrieval system named VAST (VisuAl & SemanTic image search) [8].

In the rest of this paper: Section 2 discusses the related works. Our earlier work about the cross-modal association rule is briefly introduced in Section 3.

Section 4 describes the re-ranking method. Section 5 shows the experiment and the final section presents our conclusion.

2. Related works   To automatically integrate the textual and visual information of web images, previous works mainly include Pseudo-Relevance Feedback (Pseudo-RF) method and online clustering method. The pseudo-RF method [9, 10] always assumes the top-N images are the relevant ones for feedback. Its performance strongly relies on the quality of top-N images that are used for feedback. Meanwhile, the online clustering method [11, 12] clusters the initial text-based retrieval results based on visual feature online. It is based on majority rule. These methods can not improve the retrieval effectiveness when only a few correct images are contained within top-N results. Moreover, these  2008 International Symposium on Ubiquitous Multimedia Computing  DOI 10.1109/UMC.2008.25     online clustering methods remarkably increase the response time of the query for the online time- consuming clustering process, which affects the usability of the system.

We implement a re-ranking scheme based on the cross-modal association rule, which is an automatic integration from data mining prospective.

3. Cross-modal association rule   In our previous work [7], we described the cross- modal association rule that associate keyword with several visual feature clusters. The definition of support-confidence in our earlier work [7] is the standard support-confidence for the rules, and the support of the rule is extremely low, which affect the rules mining to some extent. Therefore, we re-define the support and confidence of the rule as follows:  Supp(Qj)=count(Qj)/|D| (1) Supp(Ci)=count(Ci)/|D| (2)  Supp(Qj => Ci) = count(Qj , Ci)/count(Qj) (3)  conf(Qj => Ci) = )),((  ),(  tjt  ij  CQcountMAX CQcount  (4)  where Qj is one keyword , Ci  is one cluster, D is the transaction database, count(A) is the count of itemsets that contain A in D. Obviously, |D| is the count of all itemsets in D. Similarly,  )( }),...,1|{,(  }),...,1|{(  j  ij  ij  Qcount miCQcount  miCQsupp  = =  =?  (5)  )),((  }),...,1|{,(  }),...,1|{(  tj  mt  ij  ij  CQcount  miCQcount  miCQconf  MAX ??  = =  =?  (6)  Based on our earlier work and the new definition, we obtain the cross-modal association rules. Some examples are shown in Table 1, where the C11 represents the visual feature cluster with ID 11 and the rest can be deduced by analogy.

Table 1. Examples of cross-modal association rule Cross-modal association rule Support Confidence (Great wall, C11, C34, C65) 10.8% 32.1% (car, C22, C56) 14.4% 56.7% (Great Wall, C65) 21.8% 72.8%  4. Re-ranking scheme with cross-modal association rule   The cross-modal association rules are used to identify interesting low-level visual feature clusters.

The images in these interesting clusters will rank higher. The clusters in these rules can be sorted by their weighted confidence in descending order. The weighted confidence of Ci for keyword Qj is defined as:  ? ?  = SCQ  iQ  ij  j  SconfSlength  Cconfw  ),( )(*)(  )(_ (7)  where S represents the rule that contains (Qj, Ci} and length(S) represents the length of the rule S.

The automatic re-ranking process, i.e. the re-ranking scheme, can be defined as fellows: 1) A user enters a query keyword Qj; 2) Run the plain textual ranked boolean query based  on the inverted index and get the initial retrieved images result R(Qj).

3) Look up the association rules table, if exists the rules containing the keyword Qj, like (Qj,{ Ci | i=1, ?, m}), then go on; else go to step 9).

4) According to formula (7), we order the clusters in these rules containing keyword Qj by their weighted confidence.

5) For the images in R(Qj), the images in the cluster with the largest weighted confidence ranks first, then subsequently the images in the cluster with the second largest weighted confidence, and the same is valid for the rest.

6) The images within one cluster are ranked by their visual distance (e.g. EMD) to the cluster centroid in ascending order.

7) The images not in these clusters are put in the last by their original order.

8) Get re-ranked results based on 5), 6) and 7).

9) Return the result to user.

It can be seen from the above, if exists the association rule for query keyword Qj, the images that both are contained in the interesting clusters of the rules and include the keyword Qj have higher ranks.

Otherwise, the re-ranking process regresses to the pure keyword-based search. In addition, the selection of visual features, which is another difficult problem in image retrieval area, is automatic and dynamic in our re-ranking process.

5. Experiments  5.1. Experimental setup   We have evaluated the proposed approach in VAST [8], with a database of about 1,000,000 web images collected from the websites listed in Table 2.

We selected 30 query keywords shown in Table 3 for experiments, which have both the largest frequencies and explicit meaning for objective evaluation. We gathered the top-100 images from     results for experiments, and evaluate the effectiveness in comparing the precision and recall value.

Table 2. Website list of our image database www.china-image.cn photolib.chinanews.com.cn www.fotosearch.cn www.phototime.cn www.chinatuku.com www.tour-magazine.com www.picture.com.cn www.southcn.com/travel/photo/  The keywords, the visual feature and clustering algorithm are same as the earlier work [7].

Table 3. Query keywords for experiment car, Great Wall, lion, tiger, seabeach, butterfly, bird, snow, sky, sunset, tulip, elephant, lake, baby, bear, boat, cat, cloud, dog, fish, frog, fox, horse, house, mountain, river, tree, rose, the Imperial Palace, Yellow River  We compared the method we proposed, i.e. the re- ranking by cross-modal association rule (CMAR) with other three methods, i.e. the keyword-only method, the pseudo-RF method, and the online clustering method.

In the pseudo-RF method, top-N images are used for feedback, where N=10. In the online clustering method, we adopt the best centroid-of-largest-cluster mode with the max radius of the cluster equal to ?/2 [11].

5.2. Top-K retrieval precision   Figure 1 shows the average precision/recall graphs.

It indicates that the average precision of the cross- modal association rule method is better than the other three methods, which suggests the effectiveness of our method. Moreover, the keyword only method is the worst, which confirms that the single-modal retrieval is inferior to the multi-modal retrieval. Furthermore, the online clustering method is better than pseudo-RF method, which is consistent with the conclusion in [11].

0 0.2 0.4 0.6 0.8 1  0.2  0.4  0.6  0.8   P re  ci si  o n  Recall      Keyword Only Pseudo-RF Online Clustering CMAR    Figure 1. Average Precision/Recall graph for the 30 query keywords  In fact, our method, the Pseudo-RF method and the online clustering method, are all based on the initial text-based retrieval results. Therefore, the quality of  the initial text-based retrieval results significantly affects the effectiveness of them. Therefore, we further divide the queries into three typical cases for comparison in detail. The query of ?car? represents the case of many relevant images in the initial top-K retrieved images (i.e. the good case) while the query of ?lion? represents the case of only a few relevant images (i.e. the bad case). The query of ?Great Wall? represents the case in the middle (i.e. the middle case), which is the most often cases in the queries.

Figure 2 shows the precision/recall graph for query keyword ?car?. In this case, the pseudo-RF, the online clustering method and our method all obtain a good retrieval precision, and the cross-modal association rule method is the best. Our method obtains strong cross-modal association rules in this case, which makes it perform excellently. The high precision of the pseudo-RF method and the online clustering method in Figure 2 lies in that the assumptions of them are satisfied in this case. However, the pseudo-RF method and the online clustering method are still inferior to the cross-modal association rule method.

0 0.2 0.4 0.6 0.8 1  0.2  0.4  0.6  0.8  P  re ci  si o  n  Recall    Keyword Only Pseudo-RF Online Clustering Semantic Rules   Figure 2. Precision/Recall graphs for query ?car?  Figure 3 shows the precision/recall graph for keyword ?Great Wall?. Our method is remarkably better than the pseudo-RF method and the online clustering method, and the online clustering method is a little better than the pseudo-RF method. Obviously, our method indicates significant superiority to the other methods in this case.

Figure 4 shows the precision/recall graph for query keyword ?lion?. Compared with the keyword only method and the pseudo-RF method, our method and the online clustering method obtain better effectiveness remarkably in this case, which demonstrates the effectiveness of the clustering for the initial retrieved results. Moreover, the pseudo-RF method doesn?t show the superiority to the keyword only method because only a few of the initial retrieved top-N images are relevant to the query. In addition, our method is a little better than the online clustering method only in the     low-recall stage, which also demonstrates the effectiveness of our method because it puts the relevant images to the front.

0 0.2 0.4 0.6 0.8 1  0.2  0.4  0.6  0.8   P re  ci si  o n  Recall    Keyword Only Pseudo-RF Online Clustering CMAR   Figure 3. Precision/Recall graphs for query ?Great Wall?  It is can be seen from the above figures that our method consistently performs better than the other three methods. It improves the retrieval effectiveness significantly in the good and middle cases or a little in the bad case compared with the other three methods.

Our method obviously outperforms the keyword only method due to the integration of the keyword and visual features. The online clustering method is the closest to our method because they are all based on the visual feature clustering. However, our method clusters on different visual features separately base on the whole inverted index for keyword, and automatically select the most appropriate visual feature clusters to represent the web image in the visual feature space.

0 0.2 0.4 0.6 0.8 1  0.2  0.4  0.6  0.8   P re  ci si  o n  Recall    Keyword Only Pseudo-RF Online Clustering CMAR   Figure 4. Precision/Recall graphs for query ?lion?   6. Conclusions   Based on our earlier work about cross-modal association rule, we propose a re-ranking scheme to  automatically re-rank the initial retrieval results in VAST system [8]. The experiment shows the re- ranking scheme improves the retrieval performance more or less in different cases, which demonstrates the effectiveness of the re-ranking scheme.

