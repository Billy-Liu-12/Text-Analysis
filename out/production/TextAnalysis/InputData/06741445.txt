Ensemble Method for Classification of High- Dimensional Data

Abstract? Ensemble methods, also known as classifier combination were often used to improve the performance of classification. Growing problem of data dimensionality makes a various challenges for supervised learning. Generally used classification methods such as decision tree, neural network and support vector machines were difficult to be directly applied on high-dimensional datasets. In this paper, we proposed an ensemble method for classification of high-dimensional data, with each classifier constructed from a different set of features determined by partition of redundant features. In our method, the redundancy of features was considered to divide the original feature space. Then, each generated feature subset was trained by support vector machine and the results of each classifier were combined by the majority voting method. The efficiency and effectiveness of our method were demonstrated through comparisons with other ensemble techniques, and the results showed that our method outperformed other methods.

Keywords?classification; ensemble; feature redundancy; high- dimensional data;

I.  INTRODUCTION The ultimate goal of supervised learning is to achieve the  best possible classification performance for the task at hand [1].

Classification algorithm is presented with a set of records, where each record is described by a fixed number of features along with a class label that denotes its target. The algorithm then outputs a concept description that represents underlying patterns in the data. Several classification algorithms have been developed such as decision tree [2], neural network [3], and support vector machine (SVM) [4] and so on. However, the increasing of the dimensionality may cause several problems to these classification algorithms with respect to scalability and learning performance. Moreover, the classification ability of a single classifier is limited.

In general, the ensembles of classifiers provide better classification accuracy than a single predictor produced.

Ensemble methods, also known as classifier combination, generate a set of base classifiers from training data and perform classification by combining the results of each base classifier.

In order to improve the accuracy of multiple classifiers, each base classifier should be diverse and independent. Ensemble classifier generation methods can be broadly categorized into  four groups [5] that are based on i) manipulation of the training set, ii) manipulation of the input features, iii) manipulation of the class labels, iv) manipulation of the learning algorithm.

Bagging [6] and boosting [7] are the widely used ensemble methods. They resample the original data to create multiple training sets based on some sampling distribution and build the basic classifier from each bootstrap samples. However, these methods cannot guarantee to generate fully independent individual base classifier [8]. Theoretical and empirical results [9], [10] indicate that the most effective method of achieving the independence is by training the base classifiers on different feature subsets [11]. The basic idea of feature subset-based ensemble is simply to give each classifier a different projection of the training set [12]. Compared with manipulating the training samples for base classifier construction, using independent feature subset for ensemble generation seems to be more efficient for the high dimensional data [13]. The reason is that i) feature subset-based ensemble can perform fast due to the reduced size of input space; ii) it can reduce the correlations among the classifiers.

In this paper, we proposed an ensemble framework for classifying high-dimensional data with each classifier constructed from a different set of features determined by partition of redundant features. First, we suggested a multiple subsets generation methods based on the feature relevance and redundancy to construct each of the classifier. Then, a number of classifiers learned from the generated subsets. Finally, the classification results of each classifier were combined by the majority voting. It was observed that the proposed ensemble method obtained promising classification accuracy compared with other ensemble methods.



II. PROPOSED ENSEMBLE METHOD An ensemble method with multiple independent feature  subsets is proposed to classify high-dimensional data. The framework of the proposed ensemble is shown in Fig. 1. The proposed method broadly consists of two steps: i) Generation of multiple feature subsets based on the correlations among features. ii) Learning of the model from each feature subset using machine learning algorithm as base classifier and combination of the results of each classifier by majority voting.

Next, we will illustrate each step in detail.

Fig. 1. Workflow of proposed ensemble methods  A. Feature Subset Generation High-dimensional data poses a severe challenge for data  mining. Feature weighting algorithms assign weights to features individually and rank them based on their relevance to the target concept. There are many feature ranking algorithms that have been developed such as chi-square test, mutual information, Pearson correlation coefficients, Relief and so on [14]. These methods are fast but lack of robustness against interactions among features. In [15], they proposed a Fast Correlation-Based Filter (FCBF) approach to remove the redundant features as well as irrelevant features. Symmetrical Uncertainty (SU) was used to measure the correlation.

Y)|H(X-H(X)Y)|IG(X ?                        (1)  ? ?  ? ? ?  ? ?  ? H(Y)H(X) Y)|IG(X2Y) SU(X,                        (2)  where IG(X|Y) is the information gain of X after observing variable Y. H(X) and H(Y) are the entropy of variable X and Y individually.  FCBF removes irrelevant features by ranking correlation between feature and class. Redundant features could be defined from meaning of predominant feature and approximate Markov Blanket. A feature is predominant if it does not have any approximate Markov Blanket in the current set. For two relevant features Fi and Fj, Fj forms an approximate Markov Blanket for Fi if  cj,ji,ci,cj, SU SU and SU SU ??                    (3)  where SUi,c is the correlation between feature and the class.

SUi,j is the correlation between feature and feature.

In our previous work [16], we proposed a hybrid feature selection algorithm and demonstrated that there are lots of feature subsets with good discriminative capability. Thus, we extend our previous work by training classifiers on the discriminative subsets instead of choosing a best subset. In the feature ranking and subset generation step, all the features are ranked by the symmetrical uncertainty value and a relevant  feature subset is selected. According to the correlation between features and class, SUi,c, all the feature are sorted in descending order. Then a relevant subset of features can be derived by a predefined threshold ? . If the SU value of a feature is larger than the threshold, the feature is considered to be relevant. Then, we iteratively split the redundant features in the subset of relevant features into several parts (subset1, subset2, ?, subsetk). In each iteration, the redundant features were removed as FCBF does and the removed features in previous stage were used in the next iteration. Among the removed features, we choose the most relevant one as the starting point, and then repeat redundancy analysis.

B. Learning Ensemble Model Over the past few years, the SVM has been widely used  because of its good performance on high-dimensional data.

SVM was developed by Vapnik to successfully solve the problems of handwritten digit recognition [17], object recognition [18], text classification [19], cancer diagnosis [20], and bioinformatics [21]. Thus, we used SVM as base classifier in our ensemble method. The goal of SVM is to find the hyperplane with a maximal margin illustrated as in Fig 2.

Fig. 2. Example of SVM  The learning task in SVM can be formalized as the following constrained optimization problem:  | |w| |min   w                                     (4)  n ..., 2, 1,i 1,b)x(wy subject to ii ????              (5)  This is known as a convex optimization problem, which can be solved using the standard Lagrange multiplier method:  ? ?  ??? N  1i iii  p 1)-b)x(w(y-| |w| |2  1L ?              (6)  where the parameters i? are called the Lagrange multipliers.

According to the Lagrange multipliers, the decision function is written as follows:     b)x),K(xysgn(f(x) ii n  1i i ?? ?  ?  ?                (7)  Additionally, the results of each classifier were combined by majority voting, which performs classification of unknown data based on the class label that obtains the most frequent votes. The mathematical function of ensemble method with k classifiers can be written as:  ))c ,(x)(fmax( argclass(x) i k  k??                  (8)

III. EXPEREIMENTAL RESUTLS  A. Datasets To evaluate the effectiveness of our method, we used two  publicly available gene expression datasets. In general, gene expression datasets share common characteristics, such as a very low sample/dimension ratio. The MLL_leukemia dataset [22] contains a total of 72 samples in three classes, acute lymphoblastic leukemia (ALL), acute myeloid leukemia (AML), and mixed-lineage leukemia gene (MLL), which have 24, 28, and 20 samples, respectively. The number of features is 12582. The prostate dataset was first published in [23]; it is a two-class classification problem and contains 102 samples and 12600 genes. One of the tasks addressed by the authors is to build a model that can discriminate between normal prostate and tumorous prostate tissue.

TABLE I.  DATASETS USED IN OUR EXPERIMENS  Dataset Instances Features Classes  Leukemia 72 12,582 3  Prostate 102 12,600 2    B. Performance Evaluation We compared our methods with widely used ensemble  methods: Bagging, AdaBoost and Random forest. For bagging, adaboost and our methods, SVM was used as the base classifier for fair comparison. The number of classifiers for each ensemble methods was set to 20. To obtain a statistically- reliable predictive measurement, we performed 10 runs of 10- fold cross validation on all the datasets. In 10-fold cross validation, each dataset was randomly partitioned into ten parts.

Nine parts were used as the training set, and the remaining one was used as the testing dataset. Selecting the kernel and appropriate parameters plays an important role in SVM classification performance. The RBF kernel is a commonly used kernel for three reasons [24]. First, the RBF kernel can handle nonlinear relationships between class labels and attributes. Second, it has fewer hyperparameters that influence the complexity of the model selection than the Polynomial kernel. Third, the RBF kernel has fewer numerical difficulties.

In our experiments, we chose the RBF kernel function and the parameters c and r must be optimized for the RBF kernel for each dataset. To determine the best values of c and r, we  conducted a grid-search approach using 10-fold cross validation. A number of pairs of (c, r) values were attempted, and the pair with the best accuracy was picked in the range of  }2 ..., ,2 ,{2c 15-3-5? and }2 ..., ,2 ,{2r 3-13-15? .

Table II and Table III exhibits the classification accuracy of four ensemble methods on two datasets. Each row indicates the accuracy of each run and the last row shows the average accuracy of ten runs. From Table II, it is clear that the proposed method is found to result in best average prediction accuracy on mll_leukemia dataset, which is 98.75% (standard deviation = 0.44), while the other methods are found to be 94.30% (standard deviation = 1.38), 96.80% (standard deviation = 0.67), 82.08% (standard deviation = 2.81) for bagging, adaboost, random forest, respectively. Similar results can also be found in Table III. On interesting observation is that random forest has relatively poor performance. It may be because random forest uses decision tree as base classifier, while other methods use SVM. It is well known that SVM has better performance on high-dimensional data than decision trees.

TABLE II.  COMEPARSION OF CLASSIFICATION ACCURACY ON MLL_LEUKEMIA DATASET (%)  Runs Bagging AdaBoost Random Forest Proposed Method  1 95.83 97.22 81.94 98.61 2 94.44 97.22 80.56 100 3 95.83 97.22 80.56 98.61 4 95.83 97.22 81.94 98.61 5 91.67 95.83 79.17 98.61 6 94.44 97.22 86.11 98.61 7 94.44 97.22 87.5 98.61 8 93.06 95.83 80.56 98.61 9 93.06 97.22 83.33 98.61 10 94.44 95.83 79.17 98.61  Avg. 94.30 96.80 82.08 98.75 a. Each row indicates accuracy of each run and the last row represents average accuracy of ten runs.

TABLE III.  COMEPARSION OF CLASSIFICATION ACCURACY ON PROSTATE DATASET (%)  Runs Bagging AdaBoost Random Forest Proposed Method  1 91.18 91.18 80.39 95.1 2 92.16 92.16 82.35 95.1 3 93.14 92.16 78.43 95.1 4 91.18 89.22 83.33 94.12 5 91.18 91.18 77.45 94.12 6 92.16 92.16 85.3 93.14 7 91.18 91.18 86.27 95.1 8 92.16 91.18 80.39 96.08 9 93.14 92.16 75.49 95.1 10 91.18 91.18 76.47 95.1  Avg. 91.87 91.38 80.59 94.81 b. Each row indicates accuracy of each run and the last row represents average accuracy of ten runs.

TABLE IV.  PAIRED T-TEST BETWEEN PROPOSED METHOD AND ADABOOST  Mean Standard Deviation Standard  Error Mean  95% Confidence Interval of the Difference t df Sig.

Lower Upper  Propsed method ? AdaBoost on mll_leukemia dataset 1.946 .718 .227 1.433 2.459 8.537 9 .000  Propsed method ? AdaBoost on prostate dataset 3.430 1.155 .365 2.604 4.256 9.391 9 .000      To test the statistical significance of differences among classifiers, a paired-samples t-test is performed regarding to adaboost and proposed method using SPSS. We selected adaboost because it showed the best average classification accuracy among the existing methods. From Table IV, the hypothesis, the mean accuracy of proposed method is equal to the mean accuracy of adaboost, has been significantly rejected (t=8.573, p-value=.000 and t=9.391, p-value=.000 on mll_leukemia and prostate dataset, respectively) with 5% significance level. It means that the differences between the proposed method and other methods are statistically significance.



IV. CONCLUSION In this paper, we presented an ensemble method, with each  classifier was trained from different feature space by dividing redundant features into different subsets. SVM was used as the base classifier and the results of each SVM were merged by majority voting method. During the experiments, we used two publicly available gene expression datasets which were taken from the Gene Expression Omnibus database and we have met our objective which is to evaluate and investigate the performance of the classifiers by using cross-validation. The results show that dividing the redundant features into several parts for ensemble construction can obtain better performance for classification on high-dimensional data and our proposed algorithm has higher prediction accuracy than other ensemble classification algorithms. Some future works are planned to extend the above algorithm by applying datasets which have high dimensionality.

