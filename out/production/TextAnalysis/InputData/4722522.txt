Privacy Preserving Decision Tree Learning Over  Vertically Partitioned Data

Abstract?Data mining over multiple data sources has become an important practical problem with applications in different areas.

Although the data sources are willing to mine the union of their data, they don?t want to reveal any sensitive and private information to other sources due to competition or legal concerns. In this paper, we consider a scenario where data are vertically partitioned over more than two parties. We focus on the classification problem, and present a novel privacy preserving decision tree learning method. Theoretical analysis and experiment results show that this method can provide good capability of privacy preserving, accuracy and efficiency.

Keywords Privacy Preserving; Data Mining; Decision Tree; Vertically Partitioned

I.  INTRODUCTION In present, great advances in networking and databases  technologies enable data to be distributed across multi parties and collected for sharing information. Distributed data mining such as association rule mining and decision tree learning are widely used by global enterprises to obtain accurate market underlying information for their business decision. Although different enterprises are willing to collaborate with each other to data mine on the union of their data, due to legal constraints or competition among enterprises, they don?t want to reveal their sensitive and private information to others during the data mining process. Privacy preserving data mining (PPDM) has emerged to address this problem, and become a challenging research area in the field of data mining (DM) and knowledge discovery (KD). The main goal of PPDM is to mine pattern or rules accurately without revealing any other private information [1].

The method of PPDM depend on the data mining task (i.e., association rule, classification, clustering, etc.) and the data sources distribution manner (i.e., centralize where all transactions are stored in only one party; horizontally where every involving party has only a subset of transaction records, but every record contains all attributes; vertically where every involving party has the same numbers of transaction records,  but every record contains partial attributes). In this paper, we particularly focus on applying PPDM method on the decision tree learning over vertically partitioned data.

The rest of the paper is organized as follows. In the next section, we will introduce the related work in PPDM and the contribution we did. In section 3, we provide some background of distributed decision tree learning and some secure multiparty computation. In section 4, we present our work of how to build a distributed decision tree, which doesn?t reveal privacy during the stages of building and classification.

Section 5 show the experimental result and privacy analysis.

Conclusion is given at the last section.



II. RELATED WORK Generally speaking, there are two approaches in privacy  preserving data mining. One is using randomization techniques [2,3,4], that is, adding ?noise? to the data before the data mining process, and using techniques that mitigate impact of the noise from the data mining results, however, recently there has been much debate about this kind of method, e.g., accuracy loss of mining results as altering the original data, inference problem can be derived from the reconstruction model, etc.

The other approach is using secure multiparty computation (SMC) techniques, such as secure sum, secure set union, secure size of intersection and scalar product, etc. In [6], Clifton has proposed to apply secure scalar product methods on association rules over horizontally and vertically partitioned data, respectively. In [7], Vaidya proposed algorithms on building decision tree, however, the tree on each party doesn?t contain any information that belongs to other party, the drawback of this method is that the resulting class can be altered by a malicious party.

The contributions in this paper are as follows: 1, Method proposed in this paper can preserve privacy not only in the stage of building tree, but also in the classification stage; 2, This method can be applied on more than two parties.

Supported by the Natural Science Foundation of China under Grant No.

69835001, Beijing Information Science and Technology University Foundation 2008.

DOI 10.1109/CSSE.2008.731    DOI 10.1109/CSSE.2008.731

III. BACKGROUND  A. Decision Tree Learning A decision-tree is a flow-chart-like tree structure, where  each internal node denotes a test on an attribute, each branch represents an outcome of the test, and leaf nodes represent classes or class distributions. Generally speaking, the basic algorithm for decision-tree induction is a greedy algorithm that constructs decision trees in a top-down recursive divide-and- conquer manner. The presentation here is rather simplistic and very brief and we refer readers to Ref. [1] for an in-depth treatment of the subject.

Obviously, the key of decision-tree induction is selecting the attribute that will best separate the samples into individual classes, for it plays an importance in not only the effectiveness of induction, but also the quality of mining rules. In this paper, we propose a privacy preserving distributed decision tree learning method based on ID3 [1], which is applied in mining concentrative database and uses information entropy to choose the best prediction attribute.

B. secure multiparty computation Secure multiparty computation (SMC) is the problem of  evaluating a function of two or more parties? secret inputs, such that each party finally hold a share of the function output and no more else is revealed, except what is implied by the party?s own inputs and outputs. SMC problem was firstly introduced by Yao and extend by Goldreich and others. These works use a similar methodology: the function f to be computed is represented as a Boolean circuit, and then the parties run a protocol for every gate in the circuit. Every participant gets shares of the input wires and the output wires for every gate.

Since determining which share goes to which party is done randomly, a party?s own share tells it nothing. Upon completion, the parties exchange their shares, enabling each to compute the final result.

In this paper, we proposed a PPDM method by applying PCIWL (Protocol for Comparing Information Without Leaking) and MNP (Mix Network Protocol), both of which belong to issues of SMC technology. We encourage readers who want deep understanding of the above two techniques to start with Ref. [8].



IV. PRIVACY PRESERVING DISTRIBUTED DECISION TREE LEARNING  In this paper we address the issue of privacy preserving distributed decision-tree mining. Specifically, we consider a scenario in which two or more parties owning confidential databases wish to run a data-mining algorithm on the union of their databases, without revealing any original information.

Our work is motivated by the need to both protect privileged information and enable its use for research or other purposes.

Privacy preserving can mean many things [5]: Protecting specific individual values, breaking the link between values and the individual they apply to, protecting source, etc. This paper aims for a high standard of privacy: Not only individual entities are protected, but to the extent feasible even the schema  (attributes and possible attribute values) are protected from disclosure.

A. Tree building Let R be the set of condition attributes and C be the class  attribute, we make assumptions that the database is vertically partitioned between k parties; each party Pi only knows its own attributes Ri, transaction ID and attribute C are known to all parties.

We take an example, as figure 1 shows, the class attribute is play, which is determined by four condition attributes, such as outlook, temp (possessed by Alice) and humidity, windy (possessed by Bobby).

(a) Training set in Alice   (b) Training set in Bobby  Figure1  Training set   If we use the traditional ID3 algorithm to mine on the union of datasets, we can obtain the public decision tree shown in figure 2, while each party?s private information are all revealed.

Figure 2    Public decision tree    In order to preserve each party?s private data, we introduce two new notions. One is Privacy-Preserving Decision Tree, as figure 3 shows, which is stored at the miner site. The semi- honest miner only knows the basic structure of the tree, (e.g., the number of the branches at each node, the depth of each sub- tree) and which site is responsible for the decision made at each node (i.e., only know which site possesses the attribute to make decision at the node, while without the knowledge of which attribute it is and what attribute values it has); the other is Constrain Set, e.g. {AR1, BR1}, it means that this path which is form the root node to the present node (the node with the value of BR1) has determined by those attributes in the Constrain Set. When beginning to build tree, all parties will send the numbers of local attribute to miner, and the Constrain Set is initialized as {}, as Constrain Set of the present node becomes full, i.e. {AR1, BR1, AR2, BR2}, it means R is empty [1], the next node should be leaf node, which with the class attribute value c assigned to most transactions with the certain transaction IDs.

Figure 3 Privacy-preserving decision tree   Now we?ll describe how it can be built and used to  classify testing sets. When the miner creates a root node, it sends signal to all parties. Each party obtains the local best prediction attribute Ri by information gain measurement, then sends the attribute serial number Ri and information entropy to the miner by PCIWL (Protocol for Comparing Information Without Leaking), which ensures that no original information would be revealed at miner site or any other parties. The miner applies PCIWL to get the maximum as the global best prediction attribution, while he doesn?t know the which attribute it is and what attribute values it has, he just has the knowledge that which site possesses that attribute and its? serial number, e.g., as it is shown in figure 3, the minor creates a root node AR1, which means Alice has the information at that node, and the first attribute possessed by Alice is the best prediction  attribution. At the same time, the minor set {AR1} as Constraint Set of the present node.

When creating the next node, whether it?s a leaf or internal node, the process is as following: Firstly, the miner sends token signal to the target party, which has possesses the best prediction attribute of previous node. Secondly, the target party receives token message, if the token signal is 0, which means Constraint Set is full, it only needs to compute the class attribute value c assigned to most transactions with the certain IDs, and send c to miner site; if the token signal is 1, which means that R isn?t empty, it firstly needs to judge if all the transactions with the certain IDs have the same class attribute c, if so, then sends c to miner site; otherwise works out the intersection of transactions used previously and transactions with best prediction attribute value, and sends IDs to other parties by MNP (Mix Network Protocol), by which it guarantees that the communication process is anonymous.

Thirdly, all parties compute information entropy of the local attribute corresponding to the certain IDs, and send the information entropy to the miner site by PCIWL. Finally, if the minor only receives attribute c from token party, it creates a leaf node with the value of c; if the miner receives information entropy form all parties, it chooses the maximum as the best prediction attribute, adds the attribute tag to Constant Set, and sends token to the next target party, which possesses the best prediction attribute of the present node.

B. Privacy-preserving algorithm Assume that there are three parties named A, B and C,  which respectively has ra, rb and rc condition attributes, and wants to collaboratively mining decision-tree. As the main idea we presented above, the algorithms, which comprise three parts, are as follows:  Local mining algorithm (performed by parties with token): Input: Local training samples, token.

Output: Sending class attribute distribution to miner site, or sending IDs to other parties and information entropy to miner.

1) If token=0, computes the class attribute value c assigned to most transactions with the certain IDs, and sends c to miner site;  2) If token=1, judges if all the transactions with the certain IDs have the same class attribute c, if so, sends c to miner site;  3) If not, works out the intersection of transactions used previously and transactions with best prediction attribute value, sends IDs to other parties by MNP, and do step 4;  4) Computes information entropy and sends it to the miner site by PCIWL;  Local mining algorithm (performed by parties without token): Input: Local training samples, transaction IDs.

Output: Sending information entropy to miner.

1) Receives transaction IDs form the token party, then computes intersection of IDs received and IDs used previously;  2) Computes information entropy corresponding to the certain IDs, and sends it to the miner site by PCIWL.

Miner site algorithm (performed by miner): Input: Class attribute distribution from token party, or information entropy from all parties.

Output: Creating node, updating Constraint Set, sending token signal to target party.

1)  If the receiving message is class attribute c from token party, creates a leaf node with the value c;  2)  If the receiving message is information entropy from all parties, applies PCIWL to obtain maximum, and do step 3;  3)  Creates an internal node with the value of target party?s name and serial number of the best prediction attribute, adds the attribute to Constraint Set, and do step 4;  4)  If Constraint Set is full, sends token=0 to the target party; otherwise sends token=1.



V. EXPERIMENT  RESULT The experiment was conducted with Pentium IV3.2 GHz  PC with 2GB memory on the Linux platform, and all algorithms were implemented in C/C++. We used the anonymous Web usage data of the Microsoft web site, which was created by sampling and processing the www.microsoft.com logs and donated to the Machine Learning Data Repository stored at University of California at Irvine Web Site.

We designed two sets of experiments. The first set is used to show that mining quality between non-privacy preserving approach and privacy-preserving approach in distributed decision-tree mining; the second set is used to show that privacy quality of privacy-preserving approach.

Figure 4 Mining quality comparison   Figure 4 shows that compared with traditional non-privacy  preserving approach, percentage of rules mined by privacy- preserving approach is at least 85%, and Figure 5 presents the privacy-preserving percentage is at least 82%. Experimental results show that the privacy-preserving approach we proposed can provide good capability of privacy quality without sacrificing accuracy.

Form the viewpoint of theoretical analysis, when building tree, the control is passing from site to site, except token party has the knowledge of best prediction attribute of the present  node, other party even the miner doesn?t know any relevant information. When classifying, the miner only knows the path of classifying process, i.e., which site handles the classifying in every step, while the information of which attribute is used to classify and values of transaction records in every party is protected. Based on theoretical analysis and experimental results, we can conclude that this method proposed in the paper is effective.

Figure 5 Privacy quality  ? CONCLUSION In this paper, we presented a privacy-preserving distributed  decision-tree mining algorithm, which is based on idea of privacy-preserving decision tree and passing control from site to site. Our experimental results show that it has good capability of privacy preserving, accuracy and efficiency.

For future research, we will investigate the possibility of developing more effective and efficient algorithms. We also plan to extend our research to other tasks of data mining, like clustering and association rule, etc.

