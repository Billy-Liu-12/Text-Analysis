2016 8th International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (lCUMT)

Abstract-The data mining issue in big data processing which is based on cloud computing has become a hot research topic.

Generally, most of the previous work directly analyzes the data through the existing mining approaches, which may cause problems such as redundant computation, high time complexity, and large storage space. Based on this argument, a novel heuristic approach called PASS (Pre-processing based on Small Sample) has been proposed for finding a small sample composed of the most frequent transactions in big data pre-processing.

Taking advantage of the cloud computing which can solve the bottleneck of data mining in distributed environment, PASS directly operates on the transaction database and groups all transactions according to different dimensions. By using the Bitmap-Sort, the most frequent transactions can be screened from each transaction set. Finally, the best-transaction-set is obtained through aggregating all the transaction-elects of each transaction set. The experimental results have shown that PASS significantly avoids producing plenty of candidate sets resulting from join operation, accelerates the maximal frequent itemsets mining, economizes the storage space, and improves the utilization rate of resources simultaneously.

Index Terms-Big data, best-transaction-set, cloud computing, data mining, maximal frequent itemsets, small sample.



I. INT RODUCTION  The data mining based on cloud computing [I] , [2] for big data processing has become an important method instead of conventional mining methods at present. Generally, the big data mining [3] , [4] , [5] , [6] is treated as a crucial domain of research on big data application and processing. And it can be used to discover the valuable information hidden in the mass, noisy, incomplete, random, and fuzzy data. However, to the conventional mining methods, there may be many problems and constraints resulting from the rapidly increasing, various and personalized requests of data mining in the big data mining process. In order to solve this problem, the big data mining in distributed environment becomes an effective method. And the cloud computing has been introduced to solve the bottleneck  School of Computer Science and Engineering  ChangChun University of Technology Changchun, China  Email: jia.zhao@inesctec.pt  of mining due to its sufficient storage capacity and elastically changeable computing capability [7] , [8] . Meanwhile, with the rapid rise of cloud computing, MapReduce framework [9] , [10] , [11] , [12] becomes one of most important technique for big data mining and it is beneficial to solve problems such as load balance, fault tolerance, etc.

In the process of data mining, the association rules mining (AR mining) [13] , [14] , [15] , [16] is an important work for data analysis. The associations or related links between itemsets in big data can be discovered by AR mining, and its general course contains two phases: searching out the frequent itemsets from the transaction database in the first phase and finding out the association rules according to frequent itemsets in the second phase. Thus the frequent itemsets mining can be regarded as a crucial step for AR mining. Currently, aiming at mining the frequent itemsets quickly and effectively, many approaches have been proposed by researchers. Apriori [17] , [18] , [19] has been proposed as one of the most important approaches in this domain, which is widely used in the data mining. In the meantime, many approaches based on Apriori have been presented. Ning Li et al. [20] have proposed a parallel Apriori algorithm based on MapReduce, which aims at processing huge datasets on distributable problems by using numbers of processing nodes. Ming-Yen Lin et al. [21] have proposed three mining algorithms to analyze the effect on the performance of different implementations and sum up a technique for improving the performance of MapReduce? based mining algorithms. However, numbers of candidate sets would generate in the implementation of Apriori due to frequent join operations. And the problem of information redundancy will occur because of the overmuch inclusion relations between candidate sets. The difficulty of storing and computing the large amount of generated itemsets will be increased. Moreover, constantly repetitive scanning the transaction database will slow down the speed of data mining and waste resources. Additionally, Xiaoting Wei et al. [22]     have proposed a parallelized incremental algorithm FP-Growth mining strategy based on MapReduce which can be used to process large-scale data, but higher speedup is still the pursuit.

Wang Yong et al. [23] have proposed a parallel association rules mining method based on cloud computing which has good communication and 1/0 overhead, but there is something undesirable, especially for smaller data sets or nodes.

In order to narrow the search scope of the maximal fre? quent itemsets as well as the sample space of transactions, a heuristic approach PASS has been proposed to find the small sample composed of the most frequent transactions. PASS is different with conventional methods of finding frequent itemsets. Instead, transaction database is directly operated in this paper. Firstly, transaction database is divided into a number of sub-databases and sent to different nodes based on the parallel computing feature of MapReduce framework.

Secondly, transactions in every sub-database transaction are grouped according to different dimensions. Then, the trans? action groups of different dimensions will be operated in parallel by different nodes. Finally, the idea of "Obtaining the best" is infused into each node to search out the most frequent transaction from each transaction set using bit vector operation. We gather the transaction-elects of each node to a unified set so that the best-transaction-set can be obtained. The best-transaction-set, which provides great potential for finding the maximal frequent itemsets, is exactly the small sample composed of the most frequent transactions. On the basis of such pre-processing, PASS can not only save the space but also speed up the mining process by avoiding producing a lot of unnecessary itemsets and frequent join operations. It exploits MapReduce processing framework in order to process large amounts of data quickly and effectively. In addition, the bit vector operation could substantially save the storage space and simplify the complexity of the search. It can not only guarantee the overall superiority, but further reduce the overall data processing time in the follow-up. Meanwhile higher speedup can be obtained. The efficiency of the approach can be significantly improved.

Other parts of this paper are as follows. In Section II, we formalize the proposed problem in this paper. And in Section III, the design and implementation of the proposed algorithm are introduced in detail. Besides, the theoretical analysis and discussion are also given. In Section IV, we will give the experimental results and analysis. Finally, in Section V, we summarize the whole paper and specify some future work.



II. PRELIMINARIES  Assume that I={h,h,h,." Jm} is the set of all consisting items, Ii (iE {I ,2, . . .  ,m}) denotes transaction items, and it can be treated as a commodity. Transaction database is denoted as TDB={T1,T2,T3,?? ,Tn}, which is an aggregation of all transactions. Transaction Tj (j E {l ,2, . . .  ,n }) is a subset of I (Tj r:;.I) and it can be regarded as a shopping list. Every trans? action has its unique identification TID. All items contained by each transaction Tj are transformed to a binary array named item[] and each bit item[i] is represented by a binary value  o or 1 on behalf of existence or non-existence of h Thus, transaction database can be converted into a set containing n elements, and each element is a bit vector whose length is m.

The frequency of the itemset X appearing in transactions of T DB is denoted as sup(X), while minimum support threshold, which is an empirical value given by users, is denoted as min_sup. If sup(X):;:'min_sup, then itemset X is called the frequent itemset. For a frequent itemset X and any non? frequent itemset Y, if X r:;.Y is satisfied all the time, we treat X as the maximal frequent itemset of transaction database.



III. ALGORITHM  In order to cope with the problem of generating redundant candidate sets due to join operations, accelerate the maximal frequent itemsets mining and improve resource utilization in the big data mining, we have proposed PASS as a pre? processing for mining the small sample composed of the most frequent transactions. The main idea is as follows. Firstly, partition transaction database into certain number of parts uniformly, and make them be processed on different nodes in parallel by MapReduce. Then group transactions of each part according to the dimension. Secondly, count the frequency of transactions in each transaction set. And then extract the transaction with the highest frequency from each transaction set exploiting the Bitmap-Sort. Finally, aggregate all the transaction-elects of each group to a unified set named the best-transaction-set. Thus, the small sample mining algorithm is realized according to above steps.

A. Specific Algorithm Stepl: Load the transaction database: assuming the number  of processing nodes in the data center is k. In order to process T DB in parallel thereby accelerating the speed of mining, we partition all n transactions of T DB into k sub-transaction sets uniformly, and process them on k nodes in parallel by MapReduce.

Step2: Each node performs Map and Reduce tasks on sub? transaction set of its jurisdiction: formalizing all transactions as <keyl,valuel> pairs first, where keyl is T _dim value of the transaction and value I is the corresponding TID.

Then group the sub-transaction set by T _dim, which means those transactions containing the identical T _dim value will be mapped to the unified group. T _dim is regarded as the dimension of a transaction Tj that can be calculated according to the equation (I),  m  T _dim = 2..: item[]. (1) i=l  Generated ]\;1 groups of transaction sets have different T_dim values are marked as I-item set 51, 2-item set 5 2, . " , ]\;1 -item set 5 M. ]\;1 is the number of items included in the longest transaction of T DB.

Step3: Redistribute 51-5 M to k nodes. An integer array counCil] is set and initialized for each 51-5M separately and it records the frequency of each transaction of current set.

Scan 5i, and classify transactions using < key2, value2 >     pair (key2 is binary sequence of item[] , and value2=1) in the Map phase. After intermediate process of shuffle, we obtain the frequency value of each Tj in the Reduce phase. Then update count_i[] according to statistical result and make sure that each element in array corresponds to Tj represented by the key value, jE{I,2,?? ,n}.

Step4: In this step, we aim to select the most frequent transactions from each Si. Since the frequency of each trans? action has been acquired in step3, we just need to compare element sizes of counC'i[] respectively. In consideration of conventional sorting methods are computationally intensive as well as complicated, we improve searching efficiency by the Bitmap-Sort. We create a bit array with appropriate-size as a hash table, and initialize all bits to O. Traversing array counCil]' assume that the current traversing element equal to integer h, then set the bit with subscript h of bit array to I until the entire counCil] has been traversed. If the bit value is 0, it indicates there is no such a transaction whose frequency is equal to the corresponding subscript value. Scan the bit array from the end, and get the subscript value L of the first bit where has been set 1. L is the highest frequency of Si. Identify all elements equal to L from counCi[], and then identify all the most frequent transactions on the basis of one-to-one correspondence between elements and transactions.

StepS: A dynamic array Ai[ AX is employed to aggregate and store the most frequent transactions of each Si, and it is defined as the best-transaction-set. Since transactions con? tained in each transaction set has a different T _dim, element lengths of 1\I1AX are not uniform. Put elects of SrSM into NI AX, and set up a dynamic array freq[] as a memory of the corresponding frequent values. 1\11 AX is the obtained small candidate sample composed of the most frequent transactions.

B. Discussion PASS is in line with the idea of heuristic approaches that  it ensures the efficiency and reliability of big data mining.

It is mainly because the process of grouping transactions by dimension and screening candidates with frequency, thereby reducing the search scope is consist with the feature of heuristic search using known information to narrow search and amend the search direction. Experience found that the resulting maximal frequent item sets of Apriori and its im? proved approaches tend to be generated from transactions occurs more frequently. Thus, we directly operate on the transaction database, treat the frequency of different dimension transactions as metrics, and gather the most frequent transac? tions in the small sample. This is actually an efficient pre? processing for finding the maximal frequent itemsets different from classical approaches. It can not only avoid the mass of unnecessary intermediate frequent sets, but also decrease the number of scanning database and the complexity of the task. At same time, Bitmap-Sort is introduced into process of screening the most frequent transaction in transaction sets of various dimensions. This makes it possible that the search process for each group can quickly find the target solution by scanning counCil] twice and bit array once. Besides, using I  bit to represent 4 bytes will compress plenty of storage space.

On the whole, this is a distributed processing of big data. The large scale operations of the data set are distributed to nodes on the network by MapReduce, we could achieve reliability of the approach and significantly improve the utilization of resources. By "obtaining the best" in each node, and gathering elected-candidates to unified set, we get the small sample.

Such a pre-processing can effectively control the size of data to be processed. From macroeconomic perspective, PASS completes efficient parallel computing on large databases.

And from microcosmic perspective, PASS directly operates on transaction database, classifies transactions according to the dimension, and screens candidates according to the frequency, aiming to effectively simplify the complexity of the small sample mining.



IV. EXPERIMENTAL RESULTS  In this section, aiming at verifying the efficiency and perfor? mance of proposed algorithm, PASS and Top-K are compared through experiments.

All the experiments are performed on a Hadoop cluster [24] , [2S] , in which each processing node has the configuration of Intel is-4S90S quad-core CPU with 8G main memory. And CentOS 7.0 is used as the software environment of each node.

One of the virtual machines is selected as the JobTracker, and others as TaskTrackers. The TaskTrackers can be used for mining the small sample which is most likely to contain the maximal frequent itemsets. The two approaches are both implemented in Java.

TABLE I has shown the datasets that are used to obtain the small sample for the following mining of the maximal frequent itemsets.

TABLE I EXPERIMENTAL DATASETS  Database Transactions Items  Tl0l4D100K 100000 870  T40l10DlOOK 100000 942  KOSARAK 990002 36841  RETAIL 88161 16469  ACCIDENTS 340183 468  In this set of experiments, we compare execution time of two approaches through five groups of datasets. As shown in Fig. 1, the execution time of two approaches increases gradually with the increase in the size of the database. The performance of execution time of PASS exceeds that of Top? K. It is because that there isn't a lot of calculation process in PASS. However, in Top-K, the weighted sum of dimension and frequency of transactions are calculated, and K transactions with the maximal query functions are searched out. Thus its whole execution time will inevitably increase with the increase of the amount of data. The experimental results have demonstrated that comparing with Top-K, PASS has a smaller     execution time under the same condition. When the data size becomes larger, the gap in the execution time of the two approaches becomes more apparent.

-D- Top-K -?PASS  250 0 ? ... " f!3, " 200 ? " 0 150 ? ... '" "  ? .. '" 100 3 0 '"'   0 '" "'-  ? ,,? "'? ?'"' ? ? $" ?'t! *? ?" ? ?'"' "' ..... "G "'" 'I' Fig. 1. Comparison of total execution time  As is well known, the sample selected from the transaction sets with the better representation is of great importance in the big data mining. Generally, the diversity is an important performance index to measure the representation of sample transaction sets. In the second scenario of experiments, we compare the PASS with the Top-K on diversity on five groups of testing datasets. T _types is defined as the number of dimension types of sample transactions. And T _nurn is defined as the total number of sample transactions. Thus we could define the diversity Divas follows:  Div = T - types  . T - nurn (2)  The proposed algorithm PASS is used for finding the most frequent transactions in transaction sets with different dimensions, thus the obtained sample transactions should have the same number of dimension types with all transactions in database. And the number of most frequent transactions from each group of transaction sets with different dimensions may be more than one thus it results in DivS:: l. Top-K takes both the dimension and frequency into account in the calculation so that its dimension types of sample transactions are not comprehensive enough. Therefore, its performance of diversity is relatively undesirable. However, the sample obtained by PASS can be more representative since it contains more dimension types of transactions. As shown in Fig. 2, the sample obtained by PASS has the higher diversity, thus the sample is more representative and authentic. Thus it is more likely to find the maximal frequent itemsets in subsequent work.

As is seen from Fig. 3, for the same data set T40IIODIOOK, the speedup trends of two approaches behave differently when adding computing nodes in parallel. Speedup refers to the ratio of execution time after adding several processing nodes and the  1.0  0.8  ? 0.6 .. " .e ?  0.4  0.2  0.0  Fig. 2. Comparison of diversity  time without increase in node. The line marked with triangle means that the speedup has a proportional linear growth with the increase of the nodes in the ideal situation. PASS as well as Top-K has a good performance of speedup, but PASS has relatively higher speedup under the same condition. This is because PASS has a smaller amount of calculation in contrast to Top-K. Thus, with the increase of the number of nodes, the speedup of PASS is more pronounced. However, the speedup with a proportional linear growth is difficult to achieve in actual circumstances since there is always the consumption of communication.

5.0 -{]-PASS --l>-Top-K  " 4.5 -o-Ideal .? '"' ? 4.0 ? " .? 3.5 '"' :5 3.0 II "" 2.5 = "'" " " 2.0 "" '"  1.5  1.0  Number of Nodes  Fig. 3. Comparison of speedup

V. CONCLUSION  Aiming at searching out a small sample composed of the most frequent transactions for the big data mining, a novel heuristic approach PASS, which is based on MapReduce, has been proposed in this paper. And the main idea, im? plementation and evaluation of the proposed approach are also expounded. The idea of PASS is different from that of recursively finding frequent itemsets of classic Apriori.

Instead, transaction database is operated directly in PASS.

Firstly, all transactions are grouped according to dimension.

Then count the frequency of transactions in each transaction set. And extract the transaction with the highest frequency from each transaction set exploiting the Bitmap-Sort. Finally, aggregate all the transaction-elects of each node to the best? transaction-set. The entire process described above is per? formed in parallel. Thus, it can not only avoid generating a lot of unnecessary intermediate itemsets, but also narrow the sample space at the greatest extent. Besides, the obtained small sample contains the most frequent transactions of each group of transaction database, thus there will be a high probability we can get the maximal frequent itemsets from it. In order to evaluate the performance of PASS, three sets of experi? ments have been conducted in Hadoop cluster. Eventually, the experimental results have shown that PASS can handle large amounts of data in parallel with a rapid processing capacity.

And with the increase of computing nodes, the efficiency is more pronounced. In the meantime, the obtained sample has the higher diversity, so that sample is more representative. As a result, it is more likely to find the maximal frequent item sets in subsequent work.

It is worth noting that there may be some omitted trans? actions which may eventually become the maximal frequent itemsets. Therefore we plan to introduce the support as a parameter to solve the problem in the following process so as to obtain the maximal frequent itemsets in the database for the big data mining .

ACKNOW LEDGMENT  Gaochao Xu, Yan Ding, Chunyi Wu and Yunan Zhai are supported by the National Science-Technology Support Project (2014BAH02F02), Project 2016029 supported by Graduate Innovation Fund of Jilin University, the Jilin Provincial Educa? tion Office (the 13th Five-Year Plan Science and Technology Research Project [2016] No. 347) and the National Natural Science Foundation of China (Grant No. 61133011). Jia Zhao, the corresponding author, is fully financed by the ERDF - European Regional Development Fund through the Opera? tional Programme for Competitiveness and Internationalisation - COMPETE 2020 Programme within project ?POCI-01- 0145-FEDER-006961?, and by National Funds through the Portuguese funding agency, FCT - Fundacao para a Ciencia e a Tecnologia as part of project ?UIDIEEA/5001412013?.

