Interestingness Preprocessing

Abstract  As the size of databases increases, the number of rules mined from them also increases, often to an extent that overwhelms users.

To address this problem, an important part of the KDD process is dedicated to determining which of these patterns is interesting. In this paper we define the Interestingness Preprocessing Step, and introduce a new framework for interestingness analysis. In a sim- ilar fashion to data-preprocessing, this preprocessing should al- ways be applied prior to interestingness processing. A strict re- quirement, and the biggest challenge, in defining Interestingness Preprocessing techniques is that the preprocessing will not elimi- nate any potentially interesting patterns. That is, the preprocess- ing methods must be domain-, task- and user-independent. This property diflerentiates the preprocessing methods from existing in- terestingness criteria, and, since they can be applied automati- cally, makes them very useful. This generic nature also makes them rare: Preprocessing methods are very challenging to deBne.

We also define in this paper the first two preprocessing tech- niques, and present the empirical results of applying them to six databases. The results indicate that Interestingness PreProcess- ing Step is very powerful: in most cases, an average of half the rules mined were eliminated by the application of the two Interest- ingness Preprocessing techniques. These results are particularly significant since no user-interaction is required to achieve them.

1 Introduction!

Builders of Knowledge Discovery in Databases (KDD) systems face the challenge of extracting interesting patterns from large masses of data [7]. Much attention has been ded- icated to the development of data mining algorithms to effi- ciently mine patterns from databases, for example, [3]. As the size of the databases increases, the number of patterns mined from them also increases. This number can easily in- crease to an extent it overwhelms users of the KDD process.

For the KDD process to successfully meet users? expecta- tions, an important part of it must be dedicated to determin- ing which patterns are interesting to its users.

Characterizing what is interesting is a difficult prob- lem. Numerous attempts have been made to formulate the qualities that define what is interesting; they range from evidence and simplicity to novelty and usefulness, as in  [ l l ,  18, 10,27, 1,4, 17, 15, 8,221.

Many distinct interestingness criteria (see Section 3) are  available to process mined patterns to determine which ones are interesting. Determining interestingness according to different criteria can result in different sets of patterns marked as interesting. This dependence - of the patterns marked as interesting on the domain knowledge - is clear when domain or prior knowledge is used explicitly (as in [IO, 27, 12, 19, 161, etc.). When prior and domain knowl- edge are applied implicitly in the interestingness criteria this dependence is not as clear, but still imposes a bias on the type of rules that will be outputted as interesting (see Sec- tion 3). This dependence should not be ignored.

1.1 Contributions  In this paper we introduce the Interestingness PrePro- cessing Step and show through an empirical evaluation the powerful results of its application. Interestingness PrePro- cessing is applied to the list of mined association rules (see the interestingness analysis framework in Figure 1) in order to reduce the size of the problem: the number of potentially interesting rules that need to subsequently be processed for interestingness.

The Interestingness Preprocessing criteria eliminate un- interesting rules independently of: (1) the data used in the mining process, (2) the domain of the problem, (3) the task addressed, and, (4) the specific interests and prior knowl- edge of the user. Thus, Interestingness Preprocessing meth- ods can be deployed automatically, without any human in- tervention. This generic nature of the Interestingness Pre- Processing criteria differentiates them from the existing in- terestingness criteria, the Processing Step (as in Section 3).

In this paper we also define the first two Interesting- ness Preprocessing techniques. Since Interestingness Pre- Processing methods do not impose any bias on the rules they mark as potentially interesting, they can be run se- quentially. This generic nature makes Interestingness Pre- Processing techniques rare. To demonstrate the power of these techniques, we provide an empirical evaluation of the results of the application of the two Interestingness PrePro-  0-7695-1119-8/01 $17.00 0 2001 IEEE 489    cessing techniques on six databases. These results are very encouraging, yielding, in almost all cases, an average dele- tion of more than half of the mined rules.

We stress that the Interestingness Preprocessing Criteria are meant to complement the existing types of interesting- ness process criteria and not to replace them. The Inter- estingness Preprocessing Step is the first step towards the comprehensive solution to determining which rules are in- teresting, independently of the domain, task and user, by eliminating some of the rules that are not interesting.

The rest of the paper is organized as follows: in Sec- tion 2 we provide needed definitions. We review the avail- able interestingness criteria in Section 3, and show the ex- plicit and/or implicit dependencies of these methods on do- main or prior knowledge. We define Preprocessing in the interestingness analysis framework in Section 4. We intro- duce the first two Interestingness Preprocessing techniques in Section 5, and present the powerful empirical results of their application on six different databases in Section 6. We end with our conclusions and future work in Section 7.

2 Definitions and Preliminaries  2.1 Attributes and Association Rules  Let A be a set of literals over the boolean domain. An attribute is any one of these literals. A set of attributes is a set, A,  such that A C A.

An association rule is a specific kind of pattern that is very popular due to its many practical applications. Let A and B be sets of attributes such that A ,  B C A and A n  B = 8. Let D be a set of transactions over A. A transaction is a subset of attributes of A that have the boolean value of TRUE. The association rule A + B is defined [3] to have support s% and confidence c% if s% of the transactions contain A A B,  and c% of the transactions that contain A also contain B. Given a support and a confidence threshold, [3]?s algorithm outputs the exhaustive list of all association rules that have at least those support and confidence levels.

For convenience we refer to A as the assumption of the rule A -+ B ,  and to B as the consequent of the rule.

2.2 Interestingness: Problem Definition  Definition 1 Let 0 be the set of association rules outputted by a mining algorithm run Eith support and conjidence thresholds of s and c. Find R C 0, the set of ?interest- ing rules ?*.

Definition 1 describes the general problem we are address- ing: finding the set of interesting rules. Our goal is not to infer from rules in R rules that have not been mined, i.e.,  * We do not attempt to formally define which rules are ?interesting?  that do not appear in R, that could be of interest to the user.

This is the only supposition we make of a user?s interests, a standard one often made implicitly.

Definition 1 is stated as a post-processing problem, a methodology with many benefits thoroughly discussed in 1231. Innovative approaches such as [4] and [24] incorpo- rate the interestingness measures into the mining process, but to achieve that, make assumptions on what attributes (or consequents) are interesting. In this work we introduce the first step towards the solution of the general problem of de- termining interestingness, as defined in Definition l .

3 Existing Interestingness Criteria and De- pendencies on Domain Knowledge  To determine which patterns are interesting, one needs to determine which interestingness criterion to use. User pref- erences to particular types of rules are implied by a user?s selection of an interestingness criterion, a selection often performed without considering the bias it imposes on the types of rules outputted as interesting. The task of choosing the subjectively ?right? interestingness criteria for a specific KDD process is far from trivial. Table 1 summarizes the use of explicit and implicit knowledge used to perform this task for different types of existing interestingness criteria. The details follow in this section.

3.1 Explicit Use of Domain Knowledge  The KDD literature offers three main approaches to ex- plicit use of domain knowledge. The first, and most popu- lar, approach proposes to have a domain expert or advanced user to formally (even if vaguely) express on demand, using a predefined grammar, what he or she finds (not) interesting or what a domain user already knows. For example, [lo] define templates of patterns that describe the structure of interesting rules, [ 121 define generalized templates that per- mit the expression of imprecise domain knowledge and [24] introduces user constraints. [ 13 determine interestingness according to user defined actionability and [27, 161 use user beliefs. The success of these strategies is conditioned upon the availability of a domain expert willing to go through the often significant effort of completing this task.

The second approach, taken in [25], constructs the knowledge base by having users classify every single rule.

This approach requires very intensive user interaction, this time of a mundane nature, and, as the author says, ?may be argued [to be] . . . tedious?. In some cases, the sheer amount of work involved in this approach may render it infeasible.

The third approach, presented in [19], is to eliminate a substantial portion of the un-interesting rules by having the user classify only a few simple rules. This technique re- quires very low-intensity user interaction. Since its output     Interestingness Method Used  Explicit methods (Section 3.1) Implicit Methods Ranking Patterns (Section 3.2) Pruning & Constraints  Preprocessing methods Summarization  Table 1. Dependency of Interestingness Methods on User Preference  Results affected by criterion selection initialization explicit user interests  Yes Yes Yes Yes No No Yes Yes No Yes No For analysis of results No No No  is a superset of the list of interesting rules, it is used as a first step in incorporating subjective interestingness.

3.2 Implicit Use of Domain Knowledge  Approaches that use domain knowledge implicitly do not compel users to specify directly what is interesting to them, but require information that only a domain user can provide: how to select the interestingness criterion to be used, how to initialize parameters used by these criteria, etc.

3.2.1 Ranking Patterns  In order to rank association rules according to their inter- estingness, a mapping, f, is introduced from the mined rules, R, to the domain of real numbers, f : R -+ R.

The number an association rules is mapped to is an in- dication of how interesting the association is; the larger the number a rule is mapped to, the more interesting the rule is assumed to be. For example, interest(A + B) = confidence(A -+ B)/Pr[B] (defined in [6]) ranks highly association rules with high confidence and low a-priori probability of the consequent. This characteristic is not unique to the interest criterion. AddedValue(A -+ B) = confidence(A -+ B) - Pr[B] (defined in [20]) shares this property. And yet, there is a significant difference between the order these two criteria impose on the rules they rank.

One difference is derived from the interest criterion?s indis- crimination to the same differences in orders of magnitude in the values of the confidence and that a-priori probabil- ity of the consequent+. This indifference to orders of mag- nitude may be exactly what a user is looking for. How- ever, a user unaware of this property may fail to choose the subjectively beneficial objective interestingness criterion in hisher particular case. The same applies to other differ- ences between the two criteria, such as interest?s symmet- rical treatment of the assumption and consequent. [9] de- scribed principles ranking criteria should satisfy and lists 13 criteria. More ranking criteria are listed in [20,4].

+For example, for A -+ B and C -+ D such that confidence(A -+ E )  = 0 . 5 ,  Pr[B] = 0.2 and confidence(C --t D)  = 0.05, Pr[D] = 0.02 , inreresf(A -+ E )  = 0 . 5 / 0 . 2  = 0.05/0.02 = interest(C + D).  However, AddedValue(A + E )  = 0.5 - 0.2 = 0.3 > 0.03 = 0.05 - 0.02 = AddedValue(C + D).

3.2.2  The mappings defined in Section 3.2.1 can also be used as pruning techniques. Using a user-defined threshold, all pat- terns mapped to an interest score lower than the predefined threshold are pruned or filtered out as not interesting.

Additional methods can be used to prune patterns that do not require the interest mapping criteria. Statistical tests, such as the x2 test, used are used to prune in [6, 131. These tests have associates parameters (significance, confidence, etc.) that need to be initialized.

A collection of instrumental pruning methods is detailed in [21]. These methods require the initialization, by a user, of several parameters (error, strength, etc.).

[4] present a novel algorithm that includes, in addition to the two usual constraints of minimum support and confi- dence thresholds, two new constraints: a user-specified con- sequent, and the second, unprecedented, constraint of a user specified minimum confidence improvement threshold.

Initialization of thresholds (improvement [4], type-I er- ror for the x2 test, etc.), implicitly requires users to use do- main knowledge. Defaulting to commonly used values, for example, 95% significance level for the x2 test, and even the choice to use the x2 test for pruning [6, 131 (as opposed to another method), are decisions that affect which rules will be outputted as interesting. As explained in [6], the use of the x2 test requires conditions that do not always hold, and thus, cannot be used in all cases. There is also the acute problem of multiplicity: when performing a series of tests at a given significance level, the overall probability of mak- ing at least one type-I error can be much higher than the predetermined level. When using the x2 tests, one should also be aware of the type-I1 errors, since only a minute frac- tion of the rules is expected to be interesting.

Pruning and Application of Constraints  3.2.3 Summarization of Patterns  Several distinct methods fall under the summarization ap- proach. [2] suggest a redundancy measure that summarizes all the rules at the given levels of support and confidence very compactly using the more ?complex? rulest. Under the  $The preference to complex rules is formally defined as: given rules A -t B and C -t D, C -+ D is redundant with respect to A -t B if (1) A A B = C A D, and A C C,  or(2)if C A D C A A B  andA E C .

assumption that the user has no preference to higher support and confidence levels than those specified, this summary in- cludes a relatively small number of rules that summarizes the entire list of rules without losing any information at all.

[13]?s summary of association rules with a single at- tributed consequent includes ?direction-setting? rules. The direction is calculated using the x2 test, which is also used to prune the mined outputted. This technique favors ?less- complex? rules. [ 141 provides an intuitive summary that simplifies the discovered rules by providing an overall pic- ture of the relationships in the data and their exceptions.

[26] suggest clustering rules ?that make statements about the same database rows [. . . I?, using a simple distance met- ric. [26] also introduce an algorithm to compute rule cov- ers as short descriptions of large sets of rules. For this ap- proach to work without losing any information, [26] makes a monotonicity assumption on the databases on which the algorithm can be applied, making this method domain inde- pendent but not task independent.

When a user decides to use summarization (a subjective decision in itself), further consideration needs to be given to what kind of summary is preferred. Are the ?more com- plex? rules suggested in [ 2 ]  to be favored over a summary such as the one suggested by [41 or [ 13]?

4 Interestingness Preprocessing  The Interestingness Preprocessing Step preprocesses the mined patterns in order to eliminate patterns that can be determined to be not interesting independently of the do- main, user and task. No information or user interaction is required by the Interestingness Preprocessing Step to per- form this analysis, other than the problem definition (Def- inition 1). The Interestingness Preprocessing Step there- fore precedes the Interestingness Processing, and follows directly after the data-mining process as depicted in the in- terestingness framework outlined in Figure 1. The Interest- ingness Preprocessing methods do not impose bias on the rules they output as potentially interesting, or on the nature of ?interestingness? in Definition 1.

The Interestingness PreProcessing Step has several ben- efits: (1) First, and most pragmatically, users can apply the Interestingness Preprocessing Step automatically, directly after the mining process, to all problem that conform to Def- inition 1. This enables a significant reduction to every case of the interestingness analysis problem. (2) Running the Interestingness Preprocessing comes with zero cost to users since these techniques do not require user interaction. (3) The output of the Interestingness Preprocessing Step (rules that were not filtered out during this step) does not require appraisal by users, as it is in other approaches, e.g., summa- rization. (4) The output of the filtering of the Interestingness Preprocessing Step can be used as the input for interesting-  ness processing methods, such as ranking, summarization, etc. (5) The Interestingness Preprocessing Step circumvents the pitfall of a precise definition of what is interesting.

5 Interestingness Preprocessing Techniques  5.1 Interestingness PreProcessing 1: Overfitting  The first Interestingness Preprocessing technique we in- troduce filters out overfitting by capitalizing on the basic characteristic of association rules: the implication as cap- tured by the confidence of the association. The confidence value of an association rule is not the best predictor for the interestingness of the rule; confidence does not take into ac- count the a-priori probability of the consequent. A rule may have a high confidence level, but this confidence value may be equal to the a-priori probability of the consequent, ren- dering the association rule meaningless. Yet, the confidence criterion captures an inherent property of association rules that may be exploited in some cases as detailed below.

Let the task to whose solution we are trying to approach be as defined in Definition 1.

Interestingness Preprocessing Method 1 The applica- tion of the first preprocessing technique to R consists of the deletion of any rule T = C -+ D E R if there exists a rule i: = A -+ B E R such that: ( I )  A c C, (2) B = D, and (3) confidence(?) 2 confidence(r). Of course, support(?) > support(r).

Verification: To show that the method above is an Interestingness Preprocessing method, we need to show that any rule that this method eliminates from R is an un-interesting rule (as in Section 2.2): a rule that does not provide any more information than rules that already exist in R. This follows directly from the definition of when rules are deleted according to this method. We ex- amine two rules, T I ,  ~2 E R such that T I  = C -+ D, ~2 = A -+ B where A C C (that is, A # C),  B = D, and confidence(r2) 2 confidence(r1). In the general case, given rules with the same consequent and assump- tions that strictly contain each other, not much can be con- cluded without making stronger assumptions. However, when confidence(A -+ B )  2 confidence(C -+ D ) ,  the rule C -+ D does not provide more information than A -+ B since we are adding more attributes to the assumption, A, to form a larger assumption, C, and diminishing the predic- tive power (confidence). In this case, given A -+ B,  we can delete C + D, as we wanted to show. 0 Example 1 Given T = (milkAshoes) -+ (cereal) and ? = (milk) -+ (cereal) such that Pr[milk] = 0.8, Pr[cereal] = 0.6, Pr[shoes] = 0.5, Pr[milk A cereal] = 0.4, Pr[milk A shoes] = 0.4, Pr[cereal A shoes] = 0.3, and Pr[milk A cereal A shoes] = 0.2, r can be deleted by the     Figure 1. Framework for Determining interestingness.

first Interestingness Preprocessing method, a s  it provides no more information than i (and both rules have the same conjidence level 0.5).

Note that Interestingness PreProcessing Method 1 does not eliminate T I  = C +  D, 7-2 = A + B where A = C, B C D (B # 0.): In this case, confidence(A + B) = Pr[A A B]/Pr[A] > Pr[A A D]/Pr[A] = confidence(C + 0 ) .  The relationship between the respective confidence values and the a-priori probabilities of the consequents can be measured in many ways (for example, interest and AddedValue as in Sec- tion 3). A possible scenario is that the a-priori probability of B is so large compared to that of D, that the rule A + B does not give significantly more information than + B, while A + D = C + D does. To remain generic, i.e., an Inter- estingness Preprocessing method, this technique does not eliminate either rule, r1 or ~ 2 .  Following the same reason- ing, for T I  = C + D, 7-2 = A + B such that A C C and B C D, no deletions are performed by this Interestingness Preprocessing method in order to avoid, in the general case, the elimination potentially interesting associations.

5.2 Interestingness Preprocessing 2: Transition  Interestingness PreProcessing Method 2 The applica- tion of the second preprocessing technique consists of the deletion of any rule T,  T = A + C E R, f o r  which 31-2, r3 E R such that ( I )  r 2  = A + C A D, and ( 2 ) ~3 = c + D where confidence(r3) = 1.

Verification: To show that the technique above is an Interestingness PreProcessing technique, we need to show that any rule that this technique eliminates from R is an uninteresting rule (as in Section 2.2): a rule that does not provide any more information than rules that already exist in R. This follows directly from the definition of when rules are eliminated according to this technique: from 7-3 = C + D : confidence(r3) = 1 we get that Pr[C A D] = Pr[C]. Hence, for r 2  = A + C A D  we have confidence(A + C) = confidence(rz), and we have support(A -+ C) = support(r~),  indicating that the rule  T = A + C does not contain any more information than the rule ~ 2 .  Since according to the definition we have that r 2 ,  ~3 E R, we have easily proven our claim. 0  Example 2 Given T = (raisins) + (cereal), ~2 = (raisins)+ (milkAcereal), and ~3 = (cereal) + (milk) such that Pr[milk] = 0.8, Pr[ceread, = 0.6, Pr[raisins], = 0.3, Pr[milkAcereal] = 0.6, Pr[milk A\ raisins]/ = 0.2; Pr[cereal A! raisins] = 0:1, and Pr[milk A cereal A raisins] = 01.1. We get that confidence(r3) = 1, indicating that the Interesting- ness Preprocessing 2 method would delete the rule T.  We note that support(r) = support(r2) and that confidence(r) = confidence(T2). This preprocessing method does not delete ~ 3 .  available for  users interested in a relationship between milk and cereal.

Note that given four rules, T = A + C A D ,  TZ = A + c, r3 = c -+ D such that confidence(r3) = 1, and, ~4 = A + D, ~4 cannot be deleted in the general case with- out risking losing a potentially interesting rule. In Exam- ple 2, ~4 corresponds to the rule (milk) + (raisins).

Now, confidence(r4) = 0.25, and users interested in the re- lationship between customers who purchase milk and cus- tomers who purchase raisins may find the higher confidence level of r4 interesting.

6 Experiments With Real Data  To determine the effectiveness of the Interestingness Preprocessing techniques we introduced in Section 5, we present the results of their application on six databases.

6.1 Data Description  In this section, we describe the different databases we used in this empirical analysis. We procured the WWW Database and the Grocery Database from independent sources. The other four databases were compiled from the UCI repository [ 5 ] .

WWW Proxy Logs Database: This database was com- plied from the logs of a World Wide Web (WWW) proxy     server. Each transaction in the database specifies the cate- gories of sites (for example, NEWS, PORNOGRAPHIC, etc.) a particular user browsed. The database represents the ac- cesses of the 2,336 heaviest users to 15 site categories.

Grocery Database: We obtained this real-life dataset from an Israeli commercial company that sells groceries via telephone, fax, and the Internet. The company has a list of approximately 95,000 items, not all of which are avail- able at all times. In order to make the dataset more man- ageable, we chose to work on the 1,728 items that cover 80% of the gross income of the store. We translated each transaction in the original dataset to one containing 1,757 boolean attributes: 1,728 attributes describe the items pur- chased, another 19 attributes are used to indicate different distribution centers in the country, and 10 attributes to indi- cate the range of money spent on a single shopping basket.

Thus, the processed database contains 67,470 transactions, each describing a single shopping basket: its contents, how much money was spent on it and where in the country the order was made. Note the very large number of attributes compared to the number of transactions in this database.

Adult Database: The Adult dataset contains 15 at- tributes, 6 continuous and 9 nominal, which we discreti- sized into 171 boolean attributes. We used the 45,222 en- tries in the original dataset from [5] that had no missing attributes as the basis for the compiled database we used.

Mushroom Database: The Mushroom dataset contains 23 nominal attributes (one of these attributes was an indi- cator of whether or not the mushroom is poisonous) which we converted to 113 boolean variables. We discarded the eleventh nominal attribute that contained missing values.

Thus we were able to use all 8,124 instances.

Nursery Database: The Nursery dataset was derived from a hierarchical decision model originally developed to rank applications for nursery schools. It contains 8 nomi- nal attributes which we turned into 32 boolean ones. There were no missing values for any of the attributes and we were able to use all 12,960 instances of the dataset.

Chess Database: This dataset, King and Rook (white) versus King and Pawn (the pawn is one square away from queening on a7), has 3,196 instances and 37 nominal at- tributes. Each instance describes a single board position.

6.2 The Data Mining Process  We implemented the association rule discovery algo- rithm (aprioriTid and the rule generation algorithm) in [3]. The algorithm outputs all the association rules in the dataset that have at least the given support and confidence  yielding 9,206 rules; for the Nursery database, 3% thresh- olds, yielding 8,314 rules; for the Chess database, 10% threshold, yielding 8,292 rules; for the Adult database, 20% thresholds, yielding 13,906 rules; and for the Mushroom database, 35% thresholds, yielding 6,356 rules.

6.3 Experimental Results  6.3.1  As explained in Section 5.1, the first Interestingness Pre- Processing method relies on relative values of the confi- dence, the predictability value, of the association rules in order to perform rule deletion. To demonstrate that this re- liance does not translate into a functional dependence of the criterion on a particular choice of confidence mining thresh- olds, we applied this preprocessing method to the outputs of mining the databases using different confidence thresholds.

That is, we mined each of the datasets using several differ- ent confidence thresholds and the most permissive (lowest) support threshold our computational resources allowed us, as detailed in Section 6.2.

Results: Interestingness Preprocessing Tech. 1  Figure 2. Results of the application of the first Interestingness Preprocessing technique.

levels. We ran this algorithm on each of the databases de- scribed above with the most permissive thresholds that the limited computational resources available to us permitted.

These support and confidence thresholds were as fol~ows: for the Grocery database, 3.5% thresholds were used, Yield- ing 3,046 rules; for the WWW database, 6% thresholds,  For each of the six databases, we applied the first Inter- estingness Preprocessing technique to the list of rules out- putted by mining the database with the given support thresh-     old (as in Section 6.2) and increasing confidence thresholds.

For example, the WWW database was mined with confi- dence thresholds of 0.06, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, and 0.9 using a 0.06 support threshold in all cases.

The results of the application of this preprocessing tech- nique on the association rules outputted by these ten min- ing processes of the WWW database are summarized in the upper left hand histogram of Figure 2. Each gray bar indi- cates the number of rules mined using a specific confidence threshold. For example, the first (leftmost) gray bar in the WWW histogram depicts the 9,206 association rules that were mined with 0.06 confidence level. The black bar su- perimposed on it indicates the number of rules deleted as a result of the application of this preprocessing technique on the respective list of rules, 3,793 in this case. That is, over 41% of the rules mined by from the WWW database us- ing 0.06 support and confidence thresholds were deleted by Interestingness Preprocessing technique 1. This deletion is very significant considering that no information of any sort was required to perform it. This Interestingness Preprocess- ing technique performed even better on the output of mining the WWW database using increasingly higher confidence levels, deleting as much as 65% of the rules mined with 0.9 confidence levels, as indicated by the rightmost bar in the WWW database histogram: out of the 1,503 rules mined (the rightmost gray bar), 977 rules (the superimposed black bar) rules were deleted. The other five histograms in Fig- ure 2 summarize the results of applying the first Interesting- ness PreProcessing technique to the other five databases.

The average deletion brought about by the application of the first Interestingness PreProcessing technique are 49.3% f 7.9% for the WWW database, 55.2% f 9.8% for the Mushroom database, 69.4% If 18.0% for the Nursery database, 57.7% f 7.3% for the Chess database, 72.6% f 8.6% for the Adult database, and 3.7% f 1.5% for the Gro- cery database. These results are extremely encouraging: a significant number, in most cases, an average of over half the rules outputted by the mining algorithm are deleted by the application of this preprocessing technique. This is a significant result not only because of the large number of rules that is deleted, but also because this deletion requires no information of any kind from a user of the KDD process.

6.3.2  The results of the application of the Interestingness PrePro- cessing 2 technique (Section 5.2) to the six databases were more modest than those of the application of the first tech- nique. Even these modest results are very significant: a re- duction even of a small percentage of the rules is both ef- fective and important when these results are achieved au- tomatically, without any interaction with any users. The average elimination brought about by the second prepro-  Results: Interestingness Preprocessing Tech. 2  Figure 3. Results of the application of Inter- estingness Preprocessing 2 technique  cessing technique was as follows: 3.3% f 3.4% for the WWW database, 17.4% f 5% for the Mushroom database, 2.5% f 2.7% for the Nursery database, 3.1% f 1.8% for the Chess database, 1% f 1.9% for the Adult database. We present the detailed results in a graphical representation in Figure 3. for the first two databases.

6.3.3 Results: Sequential Application of Interesting- ness Preprocessing Techniques  As depicted in Figure 1, the Interestingness PreProcessing techniques can and should be applied sequentially. We pro- ceeded to do so, applying the two preprocessing techniques we introduced in this paper sequentially. The results of this sequential application are depicted in Figure 4.

7 Conclusions and Future Work  In this work we defined an Interestingness Preprocessing Step, as part of the interestingness analysis framework, that eliminates association rules that can be determined to be not interesting in a user-, domain-, and task-independent man- ner. The generic nature of the preprocessing differentiates it from the interestingness processing that do impose a bias on the type of rules outputted as interesting.

In this work we also introduced the first Interestingness PreProcessing techniques. The deletion of un-interesting rules by these techniques is very significant since it is achieved without requiring a user to provide any informa- tion. Our empirical results indicate that these are powerful techniques: in most cases, more than half the rules were eliminated by their application. It is important to note that any filtering power, however weak, of any preprocessing technique is significant since all the Preprocessing methods are deployed sequentially immediately following the min- ing process on any set of mined rules, and these results add up. We hope that this work will encourage the development of more preprocessing techniques by the KDD community.

Figure 4. Results of sequential application of Interestingness Preprocessing  Our future work includes the development of other In- We are also  working on the incorporation of the preprocessing methods into the mining process and of running them in parallel.

terestingness Preprocessing methodologies.

