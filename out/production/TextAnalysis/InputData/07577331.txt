Approximate Frequent Itemset Mining for Streaming Data on FPGA

Abstract-Frequent Itemset Mining (FIM) is designed to find frequently occurring itemsets among a series of transactions.

It is extremely memory and time expensive. Frequent Itemset Mining from a Data Stream (FIM-DS) is even more challenging since storing the infinite data to memory is infeasible. In recent years, researchers have proposed various approximation algorithms for FIM-DS. However, the computation complexity is still high, and these methods are difficult to be accelerated using hardware accelerators. In this paper, we propose a Space-Saving based approximate algorithm for FIM-DS. It avoids exponential candidates generation and comparisons. We realize a hardware accelerator design and implement it on an FPGA platform. Experimental results show that our algorithm in software implementation achieves up to SAx speedup for transactions with small item database, and our hardware accelerator achieves up to SO,OOOx speedup for transactions with small number of items, and S.3x speedup for transactions with extremely large number of items.



I. INTRODUCTION  Frequent itemset mining (FIM) is firstly proposed by Agrawal [1] as a fundamental problem of mining association rules. It aims to determine which items frequently appear together among a series of transactions. FIM-DS stands for Frequent Itemset Mining from a Data Stream. It plays an important role in many data mining tasks, including mining association rules [2], sequential patterns [3], data indexing, classification, clustering, and so on [4].

There are many challenges in FIM-DS. First, the candidate space grows exponentially as the number of items increases.

Second, the varying width of transactions and their subsets present the difficulty in candidate comparisons, especially in a hardware system. If transactions come from an infinite data stream, which are infeasible to be stored in memory, FIM becomes more challenging. Most existing work either require to scan original data more than one times, like Aprior [2], FP-growth [5], or do not overcome the challenges presented above and can not be efficiently accelerated in hardware system [6] [7] [8] [9] [10] [11] [12].

To overcome these challenges, we propose a novel ap? proximate algorithm for FIM-DS and implement it on an FPGA platform. The main contributions of this work are summarized as the following  ? We propose a novel approximate algorithm for FIM? DS. It adopts the equivalent horizontal data representa? tion and is based on the Space-Saving algorithm [10].

This work was supported by Huawei Innovation Research Program, 973 project 2013CB329000, the Importation and Development of High-Caliber Talents Project of Beijing Municipal Institutions, National Natural Science Foundation of China (No.61261160501, No.61373026), and Tsinghua Uni? versity Initiative Scientific Research Program.

rexample:-------------1 ! Tra ntion: {A,B,C,} ' .. !

' v ' i Candidates: {A,B}{A,C} i i {B,CHA,S,C} : L __________ .? : __________ j  Figure 1. The Behavior of Recent FIM-DS Algorithms. 0(2L) candidates are generated from each received L-length transaction, and then processed one by one.

It applies simple bitwise operation to determine the relationships of different candidates, and avoids the exponential candidates generation and comparisons.

? We propose a Space-Saving based FIM-DS accelerator design. Hundreds of processing elements (PEs) are organized in a ring to achieve a high pipeline paral? lelism. In order to deal with transactions with large item database, the external memory is inserted in the PEs pipeline to improve the scalability.

? Our experiments based on the real and synthetic data sets show that, our software approximate algorithm achieves 8.4x speedup for small item database trans? actions. And our FPGA implementation achieves up to 50,OOOx and 5.3x speedup for transactions with small and extremely large number of items respectively.



II. RELATED WORK  In the last decade, lots of work on approximate FIM? DS have been proposed [6], [9], [11], [12]. In Figure I, exponential candidates are generated from each received transaction. They treat each candidate as an element e.

Then the problem becomes frequent item count (FIC) and can be addressed with the Space-Saving algorithm [10], [13]. Based on different candidate pruning techniques, these algorithms are classified into two types. The Lossy Counting (LC) algorithm [9] and the StreamMining algorithm [6] delete "unpromising" candidates whose support are lower in candidate table D. The Sticky Sampling algorithm [9] and the Chernoff-based algorithm [11], sample the input subsets so that the number of data to process is reduced. In order to deal with burst transactions, the Skip LC-SS algorithm [12] dynamically reduces the original stream to skip some error tolerable procedures. However, these algorithms can not avoid exponential candidates generation from each trans? action. And the performance of these software solutions is limited because of their low parallelism. Hence, we turn to parallel hardware solutions for FIM-DS acceleration.

FPGA has been used for FIM acceleration in some work.

Z. K. Baker [14] maps Apriori algorithm to a systolic array.

Transaction Item Transaction 10 Item 10 A B 0 E F G  1 1 1 1 0 0 0 0 2 A C E 2 1 0 1 0 1 0 0 3 A E F G 3 1 0 0 0 1 1 1 4 B C E G 4 0 1 1 0 1 0 1 5 0 F 5 0 0 0 1 0 1 0 6 A C D E 6 1 0 1 1 1 0 0  7 0 1 0 0 1 1 0  (a) (b)  Figure 2. The Data Representation of Transactions. (a) Example horizontal data representation. (b) The corresponding equivalent horizontal bitvector representation.

Sun et al. [15] implements FP-growth on FPGA by building a full size systolic tree. These work on FPGA consider the situation that original data are stored in memory and can be accessed for more than one times. These multi? scan approaches are not fit for the real-time data stream environment.



III. SPACE-SAVING BASED FIM-DS ALGORITHM  A. Data Representation  In order to avoid exponential candidates generation, the horizontal bitvector representation can be used. Figure 2 shows an example. We assume the received transactions are {{A, B, C}, {A, C, E}, {A, E, F, G}, {B, C, E, G}, {D, F}, {A, C, D, E}, {B, E, F}}. Figure 2(b) illustrates its equivalent horizontal bitvector representations. Bitvector is a binary sequence.

In the equivalent horizontal bitvector representation (E? HBR), one bitvector is generated after receiving one trans? action. It shows the relationship of the transaction with all items. Each bit in the binary sequence is corresponding to a unique item and reflects whether the item appears in the transaction. The bit is set true if its corresponding item is in the transaction, and false otherwise. The width of one bitvec? tor is equal to the number of unique items. If we use the bitwise AND operation among several bitvectors, we can get their intersection. Furthermore, we can determine whether one transaction contains a candidate with the bitwise AND operation on their bitvectors. For example, if bitvector _1 & bitvector _ 2 = bitvector _ 2, it means transaction bitvector_l contains candidate bitvectoL2. We use this format in our proposed algorithm for FIM-DS. First, the bitvector gener? ation is independent of other transactions but only itself.

Second, the transaction stream is infinite while the number of distinctive items is usually fixed, which creating the fixed wide bitvectors. Third, we can process transactions one by one from the stream, and update the results timely.

B. Space-Saving Based FIM-DS Algorithm  The Space-Saving algorithm is proposed Figure 3 illustrates the Space-Saving based FIM-DS al?  gorithm. In the initialization, parameters and the candidate table D are initialized. The initialization of D is to gen? erate different subsets of the first few transactions, and their supports are set to 1. We also can initialize D with history frequent candidates or candidates we are interested in. After initialization, the system steps into the period of running frequency counting and replacement alternatively.

In the frequency counting phase, we take the bitwise AND operation to each input bitvector (transaction) with bitvec? tors (candidates) buffered in table D. The support of one candidate is incremented if it is a subset of the transaction.

After processing one transaction, freq-table-swap( ) func? tion swaps the adjacent candidates in table D if they are in  tra nsactions  >-_y _____ .::..:;lnitialization Phase  ,...---- -----,  _ _ _ _ _ _ _ _ I Initialize various parameters: I I :1 I count_trans=O; count_item=O; ? I Count the number of received I Bitv_wid, freq_len, I ? transactions and every items: I I threshold trans/item/replacement I ? I count_trans=coun_trans+l; I I ...

- I  ..? I count_item=count_item+l; I I I  ? : I I Inliali,e fre<Ltable D use subsets I  ';.. Coont the candidate Sl4>port I I of the first b?vector: I ? I inDandke epthemin I I D,?sub=b?v_compute ?); I ::> I descending order: L!":::'? 1.:... _ _ ____ I g I frequency_counting() I at L!r?_?bl.::s?p? ___ ?  When results requi'ed, output  the bitvectors in D and their  supports, translate the  bitvectors into item sets:  bitv_translate( )  Figure 3. The Flowchart of the Proposed Space-Saving Based FIM-DS System.

ascending order. Function Count( ) counts the number of all received transactions and items. After threshold_ trans trans? actions are processed, the replacement phase starts. Items whose count are no less than threshold_item compose one itemset bitv_common. Then sub bitvectors of bitv_common are produced and compared with (bitwise AND operation) candidates in D. One sub bitvector replaces the candidate Db if it is not in D. After one replacement, freq-table? swap( ) function is activated again. It avoids the new inserted candidate is replaced though whose support may not be the least. The width of one bitvector is equal to the number of unique items. In our algorithm, one bitvector is composed of several 32-bit data with the unsigned integer type.



IV. FPGA IMPLEMENTATION  Figure 4 shows an overview of our hardware solution.

The whole system fits in one single FPGA chip and uses a DDR3 DRAM for external storage. The item_translator translates transactions into bitvectors, and sends them to the accelerator. The accelerator updates the support of candidates buffered in table and sorts them in descending order according to their supports. The counter counts the number of transactions and every items the accelerator has processed. When the transaction number reaches a user defined threshold, threshold_trans, it stops consuming input transactions. The sub-bitv-generator generates one bitvector, bitv_common, which contains the items whose supports in the counter are no less than another thresh? old, threshold_ item. Then it generates sub-bitvectors of bitv_common, and sends them to the accelerator. The ac? celerator replaces candidates which has lower supports with some sub-bitvectors. The bitv-translator translates the fre? quent bitvectors in PEs to itemsets. The controller controls    Figure 4. The Space-Saving FIM-DS Implementation Overview.

the behavior of each component in the system.

1) Translators: There are two kinds of translators: item?  translator and bitv-translator, as shown in Figrue S. Their targets are translating transactions (bitvectors) into bitvectors (transactions). Figure Sea) shows an example of an 8-bit bitvector generation. Each item is compared with numbers from 0 to 7. The bit is set true if the item is equal to the order of the bit in the bitvector. This circuit fits for applications with small item database. Another realization of item-translator is using the lookup-table method. The table records the results of the power of 2 by the different numbers. We get the power of 2 by the number of each item minus 1, and then the results of all the items are accumulated to generate the bitvector of the transaction. The translator consumes several cycles to complete the translation, and the PEs pipeline consumes bitvectors more quickly, we implement several copies of the item-translator. Each one deals with one transaction.

2) Sub-bitv-generator: The counter counts the number of all received transactions and items in each frequency counting phase. If the number of transactions reaches a user? defined threshold, it will generate a bitvector bitv-common.

bitv-common reflects the statistical information of recent received transactions. We use its sub-bitvectors to update the candidate table. After that, the counters of transactions and items are initialized. Sub-bitv-generator generates sub? bitvectors from one bitvector. It is activated in the candidate table initialization and before the candidate replacement.

Our system uses sub-bitvectors of the first few bitvectors to initialize the candidate table. In the candidate replacement phase, the least support candidate is replaced by one sub? bitvector generated from bitv-common. The lookup table method is also applied in the Sub-bitv-generator.

3) PEs Pipeline: As shown in Figure 6(a), the pipeline accelerator on FPGA is composed of lots of processing ele? ments (PEs), which are arranged in an "end-to-end" pipeline.

The router in the center of the PE-ring allocates the received bitvectors to idle PEs. A PE is the basic computation unit, as shown in Figure 6(b). Its responsibilities are decoding the encoded bitvectors, taking bitwise AND operation on them, and swapping bitvj and bitvj-l if necessary. Bitvectors bitvi and bitvj are received from the router and the previous PE respectively. Bitvectors bitvj and bitvj-l in one PE are current frequent candidates. All candidates in the pipeline form a ring candidates table. In the hardware system, the candidates flow and encounter every bitvi in every PE.

In the frequency counting procedure, if bitvi has encoun? tered every bitv j, the PE enters the idle state and waits for a new input. For transactions with small item database, the bievectors are not compressed. The candidates flow every  bitO ,........====="-,  bill  bitl  (a) (b) Figure 5. Architecture of the Item-translator and Bitv-translator. (a) Item? translator for small item database, and the example bitvector is 8-bit wide.

(b) The architecture of the Bitv-translator.

PE {Processing Element}  (a) (b) Figure 6. The Architecture of PEs Pipeline. (a) The PEs pipeline architecture. (b) The processing element (PE) architecture.

cycle. As a result, the latency of one transaction is the size of the table, freq_len. While for transactions with large item database, the latency is doubled. Because the bitvectors decoding takes one extra cycle.



V. EVALUATION A. Experimental Setup  Our software implementation runs on an Intel(R) Core(TM) i7-4790 CPU (@3.60GHz). The hardware accel? erator is built on the VC707 board which has a Xilinx FPGA chip Virtex7 48St working at ISO MHz. Our test datasets consist of synthetic datasets and real-world datasets. The synthetic datasets are generated by IBM's Synthetic Data Generator [16]. The real data sets are described in [l7] and from the KDD Cup 2000 data [18]. Table II characterizes the data sets in terms of the number of transactions and unique items, the size of the transactions.

B. Experimental Results  Software Implementation. Table I show our experimental results. In our software experiments, parameter freq_Ien is changed from S12 to 1024 x 10. As freq _len increases, the time overhead increases. If the item database is small, our software implementation achieves up to 8Ax speedup. If the item database is large, the performance decreases dramati? cally. It is because our algorithm is sensitive to the number of unique items. It equals to the width of one bitvector. As a result, the bitwise operations are increased and more time is consumed. The number of unique item in TlO.14.1000K is 10k, while the average length of transactions is 10. It means we use a 10000-bit wide bitvector to represent a 10- item transaction, which is only 1Ox32 bits. Moreover, we process 32-bit data one time in our algorithm. Our software solution consumes more than 6 minutes. On the other hand, the short average length of transactions make the number of candidates not very large. So they can achieve better performance.

Hardware Implementation. The resource utilization of our implementation is shown in Table III. We can tell that our FIM-DS accelerator with S12 128-bit PEs has almost    Table I PERFORMANCE COMPARISON WITH EXISTING WORK.

Dataset Algorithm Time(s) OurSW-512 OurSW-1024  Time(s) Speedup Time(s) Speedup  chess.dat [15] >3.3 0.072 45.8 0.375 8.8  connect.dat [15] >121.5 1.152 105.5 1.863 65.2  T4011 ODO 3N500K [19] 12.05 8.592 1.4 17.048 0.7  TI0.I4.1000K [9] 17.0 209.920 0.1 405.409 - [6] 4.0 209.920 405.409 -  Dataset Algorithm Time(s) OurSW-1024x6 OurSW-1024x7  Time(s) Speedup Time(s) Speedup  chess.dat [15] >3.3 2.538 I.3 3.003 1.1  connect.dat [15] >121.5 8.662 14.0 9.887 12.3  T40I I ODO_3N500K [19] 12.05 -  TI0.I4.1000K [9] 17.0 - [6] 4.0 -  Table II TEST DATASETS CHA R ATERISTICS  Dataset Num.Items Num.Trans. Average Length Size(MB)  chess 75 3196 37 0.342  connect 129 67557 43 9.300  T40110DO 3N500K 299 500K 40 214  TI0.14.1000K 10K 1000K 10 121  Table III FPGA RESOURCE UTILIZATION (BITVECTOR WIDTH = 128).

Resource Available Acc.128 Acc.256 Acc.512  Usea Utilization Usea Utilization Usea Utilization  LUTs 303600 60903 I 20.06% 121957 I 40.17% 270508 I 89.10% Regs 607200 48698 I 8.02% 104500 I 17.21% 231647 I 38.15%  fully utilized FPGA's hardware resource. As a result, the size of the candidate table in Acc.512 is 1024, and the data parallelism is 512. In our hardware solution, we set freq_Ien 10k, thresholLtrans 500, threshold_item 400 and threshold_ replacement 50. It means we take 50 potential high support sub bitvectors to replace the minimum support candidates in the table every 500 transactions.

We do not use the external memory when processing dataset chess.dat and connect.dat. Compared with FP-growth algorithm on FPGA [15], we achieve 17,OOOx to 50,000x speedup. For large item database transactions, we use the external memory to extend the candidate table size from 1024 to tens of thousands. Bitvector compression method can be used to reduce the width of bitvectors. Compared with [19] on dataset T40110DO_3N500K, we achieve the speedup of 57. [19] takes the equivalent vertical data representation and implement their exact algorithm on Gidel board FPGA.

Compared with work [6], [9] on dataset TlO.I4.1000K, the speedup is 22.6 and 5.3 respectively. Algorithms requir? ing exponential candidates generation are sensitive to the transaction length. So they can obtain good performance on dataset TlO.I4.1000K. Our algorithm is influence greatly by the number of items. In dataset TlO.14.1000K, the number of items is up to 10000, while the average length of transaction is only 10. In this situation, our system still achieve 5.3x speedup.



VI. CONCLUSION  In this work, we propose a Space-Saving based approxi? mate algorithm for FIM-DS, which can be implemented on an FPGA platform. In this method we use the equivalent horizontal bitvector representation to represent transactions.

We then take bitwise AND operation on bitvectors to find  OurSW-1 024 x 2 OurSW-1024x3 OurSW-1024x4 OurSW-1 024 x 5  Time(s) Speedup Time(s) Speedup Time(s) Speedup Time(s) Speedup  0.796 4.3 1.225 2.7 1.617 2.0 2.068 1.6  3.019 40.2 4.222 28.8 5.728 21.2 7.120 17.1  28.320 0.4  OurSW-1 024 x 8 OurSW-1024x9 OurSW-1024x 10 FPGA Time(s) Speedup Time(s) Speedup Time(s) Speedup Time(s) Speedup  3.487 0.9 3.942 0.8 4.398 0.75 1.9x l 0  1.7 x 10  11.441 10.6 I3.017 9.3 14.482 8.4 2.4x 10 -0 5.0x 104  0.21-0.42 57  0.75 22.6 0.75 5.3  their relationships. We couple these methods with the Space? Saving algorithm to complete frequent itemset mining. Fi? nally, we realize an implementation on the Xilinx VC707 board. Experimental results show that our algorithm in software implementation achieves up to 8.4x speedup, and our hardware accelerator achieves up to 50,000x and 5.3x speedup for transactions with small and extremely large number of items repectively.

