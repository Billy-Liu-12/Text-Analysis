A Weighted Load-Balancing Parallel Apriori Algorithm   for Association Rule Mining

Abstract   Because of the exponential growth in worldwide information, companies have to deal with an ever growing amount of digital information. One of the most important challenges for data mining is quickly and correctly finding the relationship between data.

The Apriori algorithm is the most popular technique in association rules mining; however, when applying this method, a database has to be scanned many times and many candidate itemsets are generated. Parallel computing is an effective strategy for accelerating the mining process. In this paper, the Weighted Distributed Parallel Apriori algorithm (WDPA) is presented as a solution to this problem. In the proposed method, metadata are stored in TID forms, thus only a single scan to the database is needed. The TID counts are also taken into consideration, and therefore better load-balancing as well as reducing idle time for processors can be achieved. According to the experimental results, WDPA outperforms other algorithms while having lower minimum support.

1. Introduction   With the rapid development of information technology, companies have been working on digitizing all areas of business to improve efficiency and thus competitiveness.  However, the consequences of full-digitization are that tremendous amounts of data are generated. It is important to extract meaningful information from scattered data, and data mining techniques are developed for that purpose.

There are many techniques being used for data mining, for example, Classification, Regression, Time Series, Clustering, Association Rules and Sequence.

Association rule [1, 2] is one of the most useful techniques in data mining. Generally, it takes long to find the association rules between datasets when a database contains a large number of transactions. By  applying parallel-distributed data mining techniques, the mining process can be effectively speeded up.

With parallel-distributed data mining the calculation is done in a distributed environment [3, 7, 8, 9, 12], but most of the time, irregular and imbalanced computation loads are allocated between processors and thus the overall performance is degraded.

In this paper the Weighted Distributed Parallel Apriori algorithm (WDPA) is presented as a solution for this problem. In the proposed method, a database has only to be scanned once because metadata are stored in TID tables. This approach also takes the TID count into consideration. Therefore, WDPA improves load-balancing as well as reduces idle time of processors.

The experimental results in this study showed that the running time of WDPA was significantly faster than that of previous methods. In some cases, WDPA only used about 2% of the time used in previous methods. This can be achieved because WDPA successfully reduced the number of scan iterations to databases and was able to evenly distribute workloads among processors.

The paper is organized as follows: In section 2, association rule and parallel distributed algorithms are explained. The WDPA algorithm is proposed in section 3. Section 4 gives the experimental results.

Finally, the conclusion is given in section 5.

2. Related Work   Frequent pattern mining problem is defined as follows. Let DB = {T1, T2, ?, Tk} be a database of transactions, where each transaction Te consists of  I, I = {i1, i2, ?, im} be a set of all items. Assuming A, B are itemsets, A, B ? I, A ? B=?, A?B denotes there is an association rule between A and B. Each association rule has support and confidence to confirm the validity of the rule. Support denotes the occurrence rate of an itemset in a DB. Confidence denotes the    proportion of data items containing Y in all items containing X in a DB. When the support and confidence are greater than or equal to the pre-defined minimum support and minimum confidence, the association rule is considered to be a valid rule.

The Apriori algorithm was proposed by R. Agrawal and R. Srikant in 1994 A.D. [2]. The Apriori algorithm is one of the most representative algorithms in mining association rules. It is based on the assumption that subsets of low-frequency itemsets must be low- frequency as well. Even though the Apriori algorithm takes lots of time to calculate combination of itemsets, the design of the data structure makes it easy for the algorithm to be parallelized. Therefore, some scholars propose that parallel- distributed Apriori algorithms be used [3, 7, 8, 9, 10, 11, 12, 13, 14]. For example, CD, DD, FDM, FPM, DMA etc. Recently, Ye [12] proposed a parallel-distributed algorithm using Trie Structure [6]. Ye?s algorithm distributes computing workload using the Trie Structure to speed up the computation, however, this causes significant variance between the sizes of candidate itemsets distributed among processors. Moreover, this method also requires a database to be scanned many times. The problems related to multiple-scan and load-imbalance gets worse when dealing with large databases and huge itemsets. Therefore, a Weighted Distributed Parallel Apriori algorithm (WDPA) is proposed. By storing the TIDs of itemsets and precisely calculating and distributing computation workloads, WDPA is able to effectively accelerate the computation of itemsets and reduce the required scan iterations to a database and balancing the load, thus significantly reduces processor idle time.

3. Weighted Distributed Parallel Apriori (WDPA) Algorithm   To avoid the problems associated with load- imbalance and multiple-scan, the WDPA algorithm is proposed so that a database only needs to be scanned once while maintaining load balancing among processors. In the algorithm, each transaction has a Transaction IDentification, called TID. By using hash functions to store TID in table structure, the number of itemsets can be quickly calculated without the need of rescanning the database.

In the WDPA parallel-distributed processing algorithm, the number of combinations for items, called a lattice, is first calculated. Lattice is the number of combinations calculated from candidate (k+1)- itemset counts by frequent k-itemsets. Equation (1) is the required count of itemset combinations of Ii, Equation (2) represents the total number of k-itemsets.

Using block division to do frequent itemset  distribution after calculating the number of combinations, is called Block_Lattice (BL) (Figure 1).

From Figures 1 and 2 we can see that the Lattice count decreases gradually at the lower-left part of the matrix, thus if itemsets are distributed sequentially, the load- imbalance distribution will occur. Therefore by cyclic partitioning of lattice, itemsets are distributed to the processors cyclically to balance the distribution of itemsets. This is called )(_ CLLatticeCyclic .

])1)([()(_ 1 ifreqlenILatticeCnt ki ??= ?   (1)  ? ?  =  ?  = 1)(    )(__ kfreqlen  i iILatticeCntLatticeTotalCnt  (2)     Figure 1. Block partitioning     Figure 2. Cyclic partitioning   By only calculating the Lattice number and  ignoring the length of the itemsets' TID, an uneven distribution of workload occurs. Therefore, this algorithm also takes TID length into consideration and regards it as a weight value, which makes the distribution of itemsets more accurate and more even.

Equation (3) calculates the weighted value of itemset    Ii. Equation (4) represents the total weight value of k- itemsets.

? ?  +=  ?  ?= 1)(    )()()(_ k  TIDTID  freqlen  ij jii IlenIlenIWeightTidValue  (3)  ? ? ?  =  ?  +=  = ?  ?= 1)(   1)(   1 1  )()(  _ k k  TIDTID  freqlen  i  freqlen  ij ji IlenIlen  WeightTidTotalValue (4)  There are two methods of partitioning weighted TID, the Block_WeightTid(BWT) partitions and distributes TID by block, the Cyclic_WeightTid(CWT) partitions and distributes TID in cyclic.

An example of the algorithms is given below:  For step1 and step2, P1 (MP), P2 (SP) read and scan database, then build level-1 candidate itemsets. (Figure 3)   Figure 3. Scan database and creating TID forms  Figure 4 shows that P1 collects itemsets that match  given support into frequent 1-itemsets, then uses CWT to calculate and distribute the itemsets on P1. P1: {C, B}, P2: {A, F, L, M, O}. (Step 3 and Step 4)   Figure 4. Distributing frequent 1-itemsets on P1   Figure 5 describes P1 and P2 combining level-2  candidate itemsets and calculating itemset counts according to the TID table. Figure 6 represents level-2 candidate itemset counts on P1 and P2. (Take level-2 candidate A and C for example, the intersection of A  and C on TID, [1, 5], is the resulting set, AC) (Step 5 and Step 6)     Figure 5. Itemsets are calculated by counting the TID forms     Figure 6. P1 and P2 level-2 candidate itemsets   Select the itemsets that match the given support  value, and save them as frequent 2-itemsets. Because the frequent 1-itemsets are larger, candidate itemsets that required combination computation will be larger, too. In this case, distributing the itemsets in Cyclic will produce better results. On the other hand, if there are frequent itemsets above level 1, candidate itemsets that required combination computation will be smaller, too.

In this case, distributing the itemsets in Block will produce better results.

P1 receives P2 itemsets, and repeats execution step 4 to step 9 until there are no more frequent itemsets.

Figure 7 illustrates the use of BWT to calculate and distribute the itemsets on P1. P1: {BC, AC}, P2: {AO, AP, CP}. (Step 3 and step 4)  Figure 8 represents P1 combined level-3 candidate itemsets matching given support value into frequent 3- itemset.

Figure 7. Distributing frequent 2-itemsets on P1     Figure 8. The resulting frequent 3-itemsets   Because the resulting frequent 3-itemset contains  only one itemset, no more combination operation can be made, the mining process ends here. The number of resulting frequent itemsets are: level-1: Eight, level-2: Six, level-3: One.

The algorithms are described in detail below:  Input: a transaction database DB = {T1, T2, ..., Tn}, and each transaction Ti ? I, I = {i1, i2, ..., im}. A given minimum support s. P is the number of processors. (p1 is master processor (MP), and p2, p3, ..., pp are salve processors (SPs)) Output: All frequent itemsets.

Method: Step  1. Each processor reads the database DB.

Step 2.Each processor scans DB and creates the  transaction identification set (TID).

Step 3. Each processor calculates candidate k-itemset  counts, when the count is greater than s, let freqk be frequent k-itemsets.

Step 4. MN equally divides the freqk into p disjointed partitions and assigns itemsetsi to pi. Itemsetsi denote that SPs were assigned to the itemsets from MN. The frequent pattern dividing method: (1) Block_Lattice (BL) (2) Cyclic_Lattice (CL) (3) Block_WeightTid (BWT) (4) Cyclic_WeightTid (CWT)  Step 5. Each processor receives the itemsetsi and the combination candidate (k+1)-itemsets.

Step 6. Each processor candidate itemsets is calculated by counting the TID forms.

Step 7. When itemset count is greater than s then it is a  frequent (k+1)-itemset, and itemset appeared in transaction id is saved to (k+1)-TID.

Step 8. SPs send frequent itemsets to MN.

Step_9.MN receives SPs itemsets, and repeats execution step4 to setp9 until there are no more frequent itemsets.

4. Experiments   In order to evaluate the performance of the proposed algorithm, the WDPA was implemented along with the algorithm proposed by Ye [12]. The program was executed in a PC cluster with 16 computing processors. Table 1 gives the hardware and software specifications. Synthesized datasets generated by IBM's Quest Synthetic Data Generator [4] were used to verify the algorithm. Moreover, the database T10I4D50N100K, T10I4D100N100K, T10I4D200N100K was used to examine the WDPA.

From the experimental results, our proposed method balances the workload among processors and saves on processor idle time because of the way CWT distributes itemsets. Therefore, the following experiments are calculated based on the CWT method.

Table 1. Hardware and Software Specifications  Hardware Environment CPU AMD Athlon Processor 2200+ Memory 1GB DDR Ram Network 100 Mbps interconnection network Disk 80GB IDE H.D.

Software Environment O.S. ReadHat Linux 7.3 Library MPICH2 1.0.3  Figure 9 shows the speed up of four WDPA  methods. From the results, it can be seen that the speeded up of the four methods is similar, but the CWT itemsets distributed used weighted TID and cyclic partition, therefore the CWT have more accurately parallel-distributed itemsets. According to the experiment, the CWT, regardless of processor numbers of 1,2,4,8,16, achieves better results than the other methods.

Figure 10 shows the execution time of the WDPA and Ye?s algorithm on different processors. WDPA(8) denotes that the WDPA algorithm used eight processors. Because Ye?s algorithm needs to repeatedly scan the database, the loads are imbalanced between processors. Therefore, from Figure 10, WDPA is nearly 120 times faster than Ye?s algoritm in the 16 processors case. The use of TID form in WDPA accurately parallelize the workloads, hence it effectively reduced the database scanning and saved on processor idle time.

Figure 9. Speedup of Four partition Methods of  WDPA (T10I4D100KN100K, minsup: 0.2%)   Figure 10. Each Processors Execution Time  (T10I4D50KN100K, minsup: 0.2%)  Figure 11 and 12 show the execution time and  speedup under different given supports. Ye?s algorithm requires that a database being re-scanned for every itemset to be counted during the mining process, so when there is lower support, Ye?s algorithm takes longer to re-scan the database. On the other hand, using the TID table with precise distribution of itemsets, the WDPA scans the database once only.

This greatly reduced the time spent on database scanning and balanced the computation workload among processors. Thus, there is an obvious performance advantage of the WDPA algorithm over Ye?s algorithm.

Figures 13 and 14 give the execution time and speed up with different databases. With the increased size of the database, the length of the TID of itemsets in the table will increase. Therefore, when the size of the database increased, the execution took longer.

Moreover, by parallel-distributing the processing, large databases can be more effectively mined and itemsets will be allocated to different processors to perform the calculation, this significantly speeding up the mining process.

Figure 11. Execution Time (WDPA vs. Ye?s  algorithm )(T10I4D100KN100K, minsup: 0.2%)   Figure 12. Execution Time (WDPA vs. Ye?s  algorithm )(T10I4D100KN100K, minsup: 0.3%)   Figure 13. WDPA Execution Time, minsup: 0.15%       Figure 14. WDPA Speedup, minsup: 0.15%   Figure 15 illustrates the execution time of various  minimum supports. The number of frequent itemsets as well as their length increased with a lower support in WDPA. Therefore, by using this method of calculation WDPA effectively accelerated the computation. Thus WDPA can achieve better speedup when there was lower minimum support.

Figure 15. WDPA Speedup   5. Conclusion   Determining the association between items in a huge database, is a worthwhile research topic.

However, the process of generating itemsets and confirming them is time consuming. Parallel- distributed computation strategies provide workable solutions to this problem.  In this paper, a Weighted Distributed Parallel Apriori algorithm (WDPA) is proposed, in which the TID of itemsets is stored in a table to compute their occurrence. WDPA effectively reduced the required scan iterations to a database as well as accelerated the calculation of itemsets. By taking the factor of itemset counts into consideration, this approach effectively balanced workloads among processors and reduced processor idle time.

Experimental results show that WDPA achieved higher speedups than pervious works in the case of high data volume and low support.

