SILVERBACK: Scalable Association Mining For

Abstract-Wei address the problem of large scale probabilistic association rule mining and consider the trade-otIs between accuracy of the mining results and quest of scalability on modest hardware infrastructure. We demonstrate how exten? sions and adaptations of research findings can be integrated in an industrial application, and we present the commercially deployed SILVERBACK framework, developed at Voxsup Inc.

SILVERBACK tackles the storage efficiency problem by proposing a probabilistic columnar infrastructure and using Bloom filters and reservoir sampling techniques. In addition, a probabilistic pruning technique has been introduced based on Apriori for mining frequent item-sets. The proposed target-driven technique yields a significant reduction on the size of the frequent item-set candidates. We present extensive experimental evaluations which demonstrate the benefits of a context-aware incorporation of infrastructure limitations into corresponding research techniques.

The experiments indicate that, when compared to the traditional Hadoop-based approach for improving scalability by adding more hosts, SILVERBACK - which has been commercially de? ployed and developed at Voxsup Inc. since May 2011 - has much better run-time performance with negligible accuracy sacrifices.



I. INTRODUCTION  Behavioral targeting refers to techniques used by advertisers whereby they can reach target audience by specific interests and/or users' activity history.

To increase the effectiveness of their campaigns, advertisers employ behavioral targeting of customers, by capturing data generated by user activities. In the context of social websites behavioral data [ 1 ] ,  [2] is generated in the form of likes, posts, retweets, or comments - however its foremost characterization is the large volume. For example in March 20 1 2, nearly 1 billion of public comments or post likes were generated by Facebook users alone, according to our estimation.

Mining valuable knowledge from behavioral data relies on the data mining techniques developed for more traditional data sources. However, it turns out that analyzing the public social web and extracting the most relevant items (i.e., frequent item? sets) is a valuable application of association rule mining to  I Yusheng Xie and Diana Palsetia contributed equally.

large behavioral databases for a particular commercial interest.

An interest could mean a group of online users, a brand or a product - e.g., the brand "Nikon" is a description of an interest in cameras. Given a set of interests and a large behavioral database of transactions of user activities in online social networks, an interesting task would be finding a list of relevant interests that share a similar demographic. As it may be observed, this operation is analogous to finding frequent item-sets and association rules from a large number of transactions of co-occurrences of the items [3].

The scale of the data in the online behavioral world is one factor posing challenges of a different nature, with respect to the existing works on association mining. We are challenged with a behavioral database containing over 10 billion trans? actions, up to 30,000 distinct items and growing by over 30 million transactions every day.

At the heart of the motivation for this project are the following two, in some sense, complementary observations: ( 1 )  Adding more hardware could help addressing the scalabil? ity - however, what if a Big Data startup cannot afford this "brute force" avenue of attaining sufficient computing power?

Contrary to large enterprizes like Facebook or Twitter, many of their smaller-in-scale partner startups have few database en? gineers challenged with designing a system that could handle inundating amount of data sent from their larger social network partners. Constraints on the budget and even considerations for energy-saving call for designing alternatives to the simple "put it on more machines and scale" approach, and are dictating careful designs on commodity hardware.

(2) The utility of extracted knowledge from the large scale behavioral data may be improved by proper exploitation of statistical techniques. Probabilistic approaches, for as long as they do not affect the accuracy of mining results past certain degree of quality assurance, do seem like viable avenues towards efficient storage schemes.

Our main contribution presented in this paper is SILVER? BACK - a probabilistic framework for accurate association rule and frequent item-set mining at massive streaming scale,     implemented on a commodity hardware. It relies on a column? based storage for managing large database of transactions, incorporating Bloom filters and appropriate sampling tech? niques to yield faster probabilistic database while maintaining satisfactory accuracies. Complementary to this, we develop an Apriori [4], [5] based algorithm to probabilistically prune candidates without support-counting for every candidate item? set. Our experimental findings demonstrate that S ILVERBACK is significantly more efficient than a generic MapReduce implementation. The framework and algorithmic implemen? tations have been successfully deployed at large scale for commercial use and progressively improved to the current version since May 20 1 1.

In the rest of this paper, after brief preliminaries and problem formulation (Section II), in Section III we position the work with respect to the related literature and present some observation justifying our approach. Sections IV and V address in greater detail the storage and infrastructure, as well as (versions of) algorithmic designs. In Section VI we present our comprehensive experimental observations, and in Section VII we conclude the work and outline directions for future work.



II. PROBLEM FORMULATION  We now give a more formal specification of the tasks addressed in this work.

Given large databases of users' activity log, the challenge in our application is to parsimoniously and accurately compute target-driven frequent item-sets and association rules, and provide a real-time on-demand response.

Let V denote a (large) list of users' activities across public walls in the Facebook network (or handles from Twitter), con? sisting of quadruples (Ui,wi,ti,ai) E V (i = O,l, . . .  ,IVI) denoting individual user's activity. The interpretation is that for i-th transaction, user Ui made activity of type ai on wall Wi at timestamp ti. Each Ui belongs to U, the set of all user IDs; each Wi belongs to W, the set of all wall IDs. In practice, IWI ? lUI ? IVI? Therefore, it is entirely expected that Ui = Uj or Wi = Wj for some i =I- j.

Aggregating the wall IDs in transactions from V by user ID generates Vu - which is a database of behavioral trans? actions. There is a clear analogy between Vu and the famous supermarket example of frequent item-set mining. User IDs in Vu are equivalent to transactions of purchase; walls that a particular user has activities upon are equivalent to the items purchased in a particular transaction. In this paper, we use wall and item interchangeably.

For a given (minimal) support level ex, a frequent item-set F, is a subset of W such that there are at least ex transaction in Vu. Fb a k-item-set, denotes a frequent item-set with exactly k number of items. A target-driven rule is generally defined as an implication of the fonn X =} Y where X, YeW, X n Y = 0, X U Y = Fb and Y is given as the target.

The goal, given a live and rapidly growing V and a target Y, is to efficiently discover rules that imply Y. As an  illustration, Vu in our settings is equivalent to an 800-million? by-30,OOO table that would have over 20 trillion cells in full representation.



III. RELATED WORK  This section gives an overview of a body of relevant works and casts our work in that context.

A. Association Mining  Association Mining aims at finding correlations between items in a dataset. And despite the recent advances in parallel association mining algorithms [6] [5],  the core technique is largely unmodified. The popular Apriori [4] algorithm iden? tifies the frequent items by starting with small item-sets, and only proceeding to larger item-sets if all subsets are frequent - incurring a cost-overheads because in every count step it scans the entire database. Several techniques have been proposed to improve issues of Apriori such as counting step, scanning and representing database, generating and pruning candidates and ordering of items, some of which we discuss in detail:  1) Max-Miner: Max-Miner [ 10] addresses the limitations of basic Apriori by allowing only maximal frequent item-set (long patterns) to be mined. An item-set is maximal frequent if it has no superset that is frequent. This reduces the search space by pruning not only on subset infrequency but also on superset infrequency.

Max-Miner uses a set enumeration tree which imposes a particular order on the parent and child nodes, but not its completeness. Each node in the set enumeration tree is considered as a candidate group (g). A candidate group consists of two item-sets. First called head (h(g?, which is the item-set enumerated by the node. The second called tail (t(g?, which is an ordered set and contains all items not in h(g). The ordering in the tail item-set indicates how the sub-nodes are expanded. The counting of support of a candidate group requires computing the support of item-sets h(g), h(g) u t(g), h(g) U {i},Vi E t(g). Superset pruning occurs when h(g) u t(g) is frequent. This implies that item? set enumerated by sub-node will also be frequent but not maximal, and therefore the sub-node expansion can be halted.

If h(g) u {i} is infrequent then any head of a sub-node that contains item i is infrequent. Consequently, subset pruning can be implemented by removing any such tail item from candidate group before expanding its sub-nodes.

Although Max-Miner with superset frequency pruning re? duces the search time, it still needs many passes of the transactions to get all the long patterns - becoming inefficient in terms of both memory and processor usage (i.e. storing item-sets in a set and iterating through the item-sets in the set) when working with sets of candidate groups.

2) Frequent Pattern (FP) Growth: FP-Growth [ 1 1 ] gains speed-up over Apriori by allowing frequent item-set discovery without candidate item-set generation. It builds a compact data structure called the FP-tree which can be constructed by allowing two passes over the data-set, and frequent item-sets are discovered by traversing through the FP-tree.

1 073    TABLE I COMPARISON WITH POPULAR ASSOCIATION MINING ALGORITHMS  Algorithm Transaction Freq. Itms. Db. Memory storage representation scans footprint  Apriori Row-based Row-based Many Large Max-Miner Row-based Row-based Many Large  Eclat Columnar Flexible A few Small FP-Growth Row-based FP tree 2 Enormous  SILVERBACK Columnar Flexible A few Tiny  In the first pass, the algorithm scans the data and finds support for each item, allowing infrequent items to be dis? carded. The items are sorted in decreasing order of their support. The latter allows common prefixes to be shared during the construction of FP-Tree. In the second pass, the FP-tree is constructed by reading each transaction. If nodes in the transaction do not exist in the tree, then the nodes are created with the path. Counts on the nodes are set to be 1 .  Transactions that share common prefix item, the frequent count of the node(i.e. prefix item) is incremented.

To extract the frequent item-sets, a bottom up approach is used (traversal from leaves to the root), adopting a divide and conquer approach where each prefix path sub-tree is processed recursively to extract the frequent item-sets and the solutions are then merged.

Allowing fewer scans of the database comes at the expense of building the FP-Tree - the size of which may vary and may not fit in memory. Additionally, the support can only be computed once the entire data-set is added to FP-Tree.

3) Eclat: Similarly to FP-growth, Eclat employs the divide and conquer strategy to decompose the original search space [ 12]. It allows frequent item-set discovery via transaction list (tid-list) intersections and is the first algorithm to use column-based, rather than row-based representation of the data. The support of an item-set is determined by intersecting the transaction lists for two subsets, and the union of these two subsets constitutes an item-set.

The algorithm performs depth-first search on the search space. For each item, in the first step it scans the database to build a list of transactions containing that item. In the next step, it forms item-conditional database(if the item were to be removed) by intersecting tid-list of the item with tid-lists of all other items. Subsequently, the first step is applied on item-conditional database. The process is repeated for all other items as well.

Like FP-Growth, Eclat reduces the scans of the database at the expense of maintaining several long transaction lists in memory, even for small item-sets.

4) Distributed and Parallel Algorithms: Discovering pat? terns from a large transaction data set can be computationally expensive and therefore almost all existing large scale associ? ation rule mining utilities are implemented on the MapReduce framework. Such examples include Parallel Eclat [8] ,  Parallel Max-miner [7], Parallel FP-Growth [9] and Distributed Apriori [6].

Table I compares our proposed method with other popular existing methods in many aspects including their scalability to  Cluster Empirical Support Lines of Accuracy scalability efficiency count code  Good[6] benchmark Yes ? 1 ,000 Exact Fair[7] ? 5x Yes Unknown Exact Poor[8] 3x ? lOx Yes ?2 ,000 Exact  Very Good[9] 5x ? lOx Yes 7,000+ Exact Good > l5x Const. time ?2,000 Probabilistic  more nodes.

B. Modern Applications of Bloom Filters  Capturing demographic between any two interests can be very high in space complexity as it requires membership operation to be performed. Bloom filter is a popular space? efficient probabilistic data structure used to test membership of an element [ l 3]. For example, Google's BigTable storage system uses Bloom filters to speed up queries, by avoiding disk accesses for rows or columns that don't  exist [ 1 4].  Similar to Google's BigTable, Apache modeled the HBase, which is a Hadoop database. HBase employs Bloom filters for two different use-cases. One is to access patterns with a lot of misses during reads. The other is to speed up reads by cutting down internal lookups.

A nice property of Bloom filters is that the time needed either to add items or to check whether an item is in the set is fixed - O(k), where k is the number of hash functions - independent of the number of items already in the set. The caveat, though, is that it allows for false positives. For a given false positive probability p, the length of a Bloom filter m is proportionate to the number of elements n being filtered: m = -nlnpj(ln2)2.

C. OLAP and Data Warehouse  Traditional OLAP (On-Line Analytical Processing) queries are usually generated by aggregation along different spatial and temporal dimensions at various granularities. For example, OLAP queries for a typical job posting website [ 1 5] include job views by day/week/month, job views by city, and job views by company. For efficiency, OLAP queries are issued against specifically designed data warehouses. Current indus? trial practices usually build a data warehouse in three steps: ( 1 ) cubifying the raw data; (2) storing the cubes; and (3) mapping OLAP queries into cube-level computations.

Web-scale real-time applications like Twitter and Facebook pose tremendous challenges to the three straightforward steps of building data warehouses.

Insights-seekers basically demand real-time summary statis? tics about the website at any granularity. To support such stringent demands, a modern data warehouse is typically first built from existing log data and incrementally updated by setting up a, so called, river - a persistent link between source and destination carrying real-time data stream. In addition, powerful and elastic MapReduce frameworks like Hadoop [ 1 6] are usually deployed to handle the first step, the cubification of web-scale data, and the third step, mapping queries to  1 074     ?L-?'?0??2 ?0??3 ?0??4?0??5 ?0??6?0 ??7?0 ??8? 0??9? 0??>=1 00 The number of brands a user makes comments  Fig. 1 .  Facebook user activity distribution (June 2008 to January 20 12,  with vertical axis in log scale)  cube-level computations. Durable and scalable key/value pair storages like Cassandra [ 1 7] are often necessary to fulfill the second step of building a web-scale data warehouse. In short, more hardware and scalability seem to be the two hosts that keep the system going - which, as indicated in Section I, is something that we aim to change.

IV S TORAGE AND I NFRAS TRUCTURE  Given the scale of Du in our settings, the traditional row-based storage assumed by [4] [ 1 1 ]  [ 1 8] would become out of depth. Our objective is to provide efficient storage scheme, however, we are not trying to invent a general-purpose advanced distributed storage engine to add to the already abundant list of such engines and file systems. Instead, we focus on an application/data-driven ad-hoc solution and we discover that a probabilistic column storage is very effective in tackling the massive data scale in our application domains.

A. Scalable Column Storage  The key observation that motivated our design is the sparsity of Du. The full representation of Du would require over 20 trillion cells (740M users by 32K walls), which is impractical even in distributed environment (notwithstanding the budget).

However, of the 20 trillion cells, less than 1% are populated.

According to our estimates, an average user accesses less than 1 4  of the 32K walls. The sparsity of Du is neither a coincidence nor a surprise us - in fact, the global sparseness in a social graph and the power-law decay in its node degree dis? tribution are part of the asymptotic behavior that we can safely assume. As an illustration, Figure 1 shows the distribution of Facebook users and the number of walls (items) they access, demonstrating that the number of users accessing x number of walls drastically decreases as x increases. Specifically, over 40% of the users only access less than 5 of the 32,000 walls.

We note that the "spike" on the right side is due to aggregating all users with more than 100 accessed walls into a single category. Hence, majority of the transactions in Du are likely to only contain a small number of items.

disk r------- :trnsc.

:irnsc.-- u2 -----  --- ----  -  :trnsc.

rirnsc.-- rirnsc?- u7 - - U-i -  -- ---  -- -rir;;;?.-- --US-- - -  - ---- -  rirnsc?? rirnsc?-  cache  ------ --- 1 , ,  ? ? ?0? ? ? ?] ??????? ]  u6 u7  u9  u10  .....

Q) Q.

E ct!

(j)  o 2: Q) C/) Q)  a:  Fig. 2. Illustration of the columnar storage in place of traditional row-based transactions(trnsc.) and its probabilistic enhancement  We use a sparse representation of the massive Du called "list of lists" (LIL) [ 1 9] (or "Column Family" in Cassandra [ 1 7]). LIL typically stores a massive sparse matrix by using a list to record the non-zero cells for each row. A column? based "list of columns" (LIC) representation is implemented for representing Du. That is, the LIC representation of Du contains a wall-column for each wall ID, and each wall? column only contains the active user IDs of the 800 million users. The upper part of Figure 2 illustrates how the traditional row-based transactions of items in a database are stored as columns. The LIC implementation is popular among columnar databases.

One of the advantages in this columnar storage is data independency. The LIC representation of the database Du can be partitioned by columns and we can store the columns as physically different files on different hosts. Inserts, deletes, and updates to any wall will only affect its column and therefore avoids database locks, which is particularly helpful when the database is live like Du.

B. Probabilistic Enhancement  An important consequence of the sparsity of Du is that in LIL representation, the lists/columns for the walls will have drastically different lengths. For example, the wall-list for Coca-Cola on Facebook contains over 30 million user IDs, whereas the small (albeit important) interests like ACM SIGMOD have less than 100 user IDs in their lists.

The main problem caused by the massive size differences is that the resource allocator would face a combinatorial problem - each host has a capacity and each column has different sizes.

The situation would be much easier to deal with if all columns are similar in size, which would allow the allocator to treat  1 075    all columns equally. To tackle this problem, two approaches seem appealing: ( 1 )  One may opt to shard the longer columns (e.g., Coca-Cola) - however, this introduces extra complexity as it diminishes the strong inter-column independency, which is important for us to scale easily. Extra locks would be required at column? level and shard-level for different chunks of a sharded column.

The situation becomes more complicated if the column is so big that its shards reside on multiple hosts. Indeed, sharding functionality is available in existing products like MongoDB [20]. But MongoDB 2.l generically implements readers-write lock and allows one write queue per database, which is not desirable in our case and may have unforeseeable impact at large scale.

(2) Another approach - which we adopted as our philosophy is to simply solve the locking problem by "avoiding it".

Namely, similar to [2 1 ] ,  we impose each column file to be single-threaded and therefore, no lock mechanism or extra complex management is required. The trade-off here is the need to make sure each column file size can be handled by a single thread with a reasonable delay. Sampling can alleviate the size difference among columns and make large columns controllable by a single-thread, and Reservoir sampler [22] is used for exceedingly long columns. In practice, we sample 500,000 IDs for columns with more than 500,000 IDs. A bonus of using Reservoir sampler is the ability to incrementally update the pool as new IDs are added to a given column and guarantee that the pool is a uniform sample of the entire column at any given moment. For each sampled column, an extra field is required to record the sampling rate.

However, the main problem now becomes that the column files still cannot fit into the main memory of our modest cluster, even after sampling - we note that loading all col? umn files of the described Du requires roughly 300GB after sampling. The practical goal is to reduce the representation of Du from 300GB down to approximately 25GB - without breaking data independency, performance or scalability. With such constraints, our options are limited due to "facts of life" such as: (a) sampling based techniques cannot be used since any sampling would have happened in the previous stage; (b) coding-based information compression is also undesirable because of its impact on performance and updatability.

Given these observations, Bloom filter [ l 3] with its proba? bilistic storage-efficiency seems a plausible choice. A Bloom filter is a space-efficient probabilistic data structure that is used to test whether an element is a member of a set. Hence, our idea is to construct a bloom filter for each column, as depicted in the bottom part in Figure 2. When the Bloom filters are built, they are meant to be cached in memory while the much larger columns can reside on slower disks. In our experience, Bloom filters' efficiency is about 5 to 7 bits per ID, where each ID is originally stored as a string of 1 0  to 20 ASCII characters, depending on the chosen column. In addition to drastically reducing the storage size, Bloom filter files can be incrementally updated as more IDs are added to the corresponding column file, which means no rebuild is  necessary for the filters.

Although the Bloom filters created for different columns can  use different number of hash functions, different false positive rate, or different number of set bits, we need to make sure all Bloom filter arrays are of the same size. In practice, we enforce the Bloom filter size to be 7,000,000 bits = 854.5 KBytes, which guarantees less than 0.1 % false positive rate with 500,000 expected inserts. Doing the same for all 30,000 columns would yield 854.5KBytes x 30,000 < 24.5 GBytes.

That is, we expect at most 500,000 (the number of max sample size) IDs to be added to any Bloom filter. Assuming that each ID sets 7 different bits in the filter, at most 50% of the bits in the Bloom filter will be set which, in turn, guarantees the bound on the false positive rate on the filters.

Together, the sampling limit and the size of the filter guarantee an acceptable/satisfactory level of accuracy. While this equal-in-size requirement might seem unnecessary and even superfluous - it is specifically imposed to enable bit operations between any two Bloom filters, which is critical in our association mining algorithm. As we will demonstrate in Section VI, both the sampling and Bloom filter have a very limited impact on the accuracy of the results.

C. Deployment of S ILVERBACK  The commercially deployed S ILVERBACK system consists of three major parts: ( 1 )  columnar probabilistic database of transactions; (2) a computation cluster; and (3) storage for output rules and frequent item-sets.

The database of transactional records, D, is implemented using modified versions of MySQL [23] and MongoDB [20] on top of 6 relatively powerful nodes. Since the database infrastructure is shared with other data warehousing purposes, databases are served from dedicated servers (free of other computational chores) to achieve high I/O throughput.

The computation nodes are the ones executing the SIL? VERBACK mining algorithms (cf. Section V), implemented as web services and served from scalable web servers like Tornado [24]. Therefore, most communication between the database and the computation cluster is through internal HTTP requests. About 30 nodes are deployed in this cluster, which is a shared resource among several computation-intensive purposes including association mining. The cluster is logically organized as master server, shadow master servers for fault? tolerance, and slave servers. However, physically several slave servers can reside on a same actual node; moreover, the master server is run alongside with slave servers on a same node as well. All the slave servers are designed to recover from crash and resume from its last checkpoint.

Two important design decisions in our computation in? frastructure are: ( 1 )  implementing the computation as web service-based transactions; and (2) the "ideological" separation between logical servers and physical nodes.

A substantial advantage of turning computation tasks into service-based transactions is the elimination of startup cost of loading dictionaries, lookup tables from disk, since the end points for those web services are persistent. More specifically,  1 076    the Bloom filter structures, which are small in memory foot? print, once fit into the main memory of the web servers can be tested, copied, and updated without modifying the disk as long as the hosting web services do not restart themselves.

Service-based system also makes logging much easier and can be readily integrated with frameworks like Scribe [25].

Another advantage, of a particular interest for our commercial application, is the web-servers ' built-in handling for timeout requests. Suppose the system is calculating frequent item-sets on-the-fty from end-clients' requests. Often, the desideratum is not to find complete/exact frequent item-sets in as efficiently (in time) as possible. To the contrary, the clients expect to explore as many frequent item-sets as possible after a tolerably short time-delay, say, 1 second. Service-based implementation makes it easier to achieve such expectations.

Separation between logical servers and physical nodes is  Algorithm 1: Column-oriented algorithm for finding two frequent item-sets and association rules  Input: a, minimal support, W, set of all items, Vu, the database of transactions  Output: 0, set of all frequent two item-sets 1 WI +--- {xix E W, length of x column 2: a}; 0 +--- {} 2 for each y E WI do 3 Uy +--- IDs from y column 4 for each x E WI and x >-- y do 5 8upportx,y +--- 0 6 bf +--- x column's Bloom filter 7 for each U E Uy do 8 if U in b f then 9 I 8upportx,y+ = 1  10 end 11 end  a powerful approach, enabling better utilization of resources among different services on a shared computation cluster. If 13 the cluster is split into smaller ones, each of which is dedicated    if 8upportx,y 2: a then I append {x, y} to 0 end  to a particular service, then service A cannot use the idle resources in cluster B even when service B is not actively using cluster B. Deploying both service A and B on the cluster as a whole can alleviate that. Moreover, dynamically reducing/increasing the slave servers running on each cluster node within just a few minutes can maximize the utilization of available resources and also reduce energy consumption in real time.

The execution of popular algorithms like Apriori [4] and FP-Growth [ 1 1 ] ,  even their distributed implementations [6], is row-based, where a transaction row is taken for granted as the execution unit. However, given the proposed storage scheme, this assumption is no longer valid and it is not straightforward to apply/generalize the existing algorithms to accommodate to our storage, due to the fundamental differences in data scanning between row-wise storage and columnar storage. In this section, we present the versions of our algorithms used in SILVERBACK.

D. Two Item-set Algorithm  We first demonstrate the column-oriented algorithm for find? ing frequent two-item-sets {X = {x}, Y = {y}}, where X and Yare both single item-sets, with a given minimal support a. The two item-set algorithm is often used in our commercial practice, where the owner of a brand y is interested in finding out other brands that are most frequently associated with y.

All the possible candidates for x are elements from W, the set of all items. Our algorithm starts by filtering out the unqualified candidates whose support is below a - a process can be done very efficiently by scanning 0 (IWI - 1) numbers, since the algorithm simply queries the length of each column file.

Let WI <;;; W denote the subset of W, which contains all the walls whose column size is above a. For each y E WI, the algorithm loads the user IDs from column y into a set Uy. Since the actual user IDs are not explicitly stored with the Bloom filter and reside on a much slower disk, reading  15 end 16 end 17 return 0  user IDs from disk only happens once per wall to avoid cost (note that Uy at each iteration is small enough to fit in memory). In other words, the algorithm scans the whole database from the disk only once. Then for each wall's Bloom filter representation bx, where x E WI, the algorithm tests whether u is a member of bx for VuE Uy. By testing Uy against bx, the algorithm effectively finds (with false positives introduced by the use of Bloom filter) y n x, the intersection between y column and x column. At this stage, confidence and support filtering is applied and all qualified y columns are put into the output set O. The x >-- y constraint says that x must come after y in atomic order, which guarantees that {x, y} and {y, x} are not calculated twice.

The equivalence between intersection of columns and union of item-sets allows us to compute other association mining concepts like lift, using the proposed storage and algorithm.

This equivalence is best illustrated in single item case, but the same property carries over to general case as shown in [ 1 2] and in the following section.

E. Two Issues With Apriori  Two particular operations in the Apriori algorithm signifi? cantly slow down its execution time. The first is the multiple scans of transactions. The other operation that significantly contributed to the temporal cost of traditional Apriori is candidate pruning, which requires counting support for each candidate generated. To overcome those two drawbacks, vari? ous pruning and optimization techniques have been proposed, as discussed in the related work section.

1) Minimizing scans of transactions: Apriori algorithm classifies candidate item-sets and explores their candidacy by  1 077    Algorithm 2: Apriori-gen algorithm for generating and probabilistically pruning candidates  Input: Fk-l, frequent (k - 1) item-sets; a, minimal support; HI(c), ... , Hf(c), sorted lists that holds the Bloom hash indices for Ic/c E Fk-l; Se for Ic/c E Fk-l, support counts for all frequent (k - 1) item-sets  Output: Ck> set of candidates for frequent k item-sets after pruning  1 Ck +--{} 2 for CI, C2 E Fk-l X Fk-l do 3 if CI and C2 satisfy Equation 1 then 4 for i E {1, ... , f} do 5 SIG(hi(CI)) +-- first m indices in Hi(cI) 6 SIG(hi(C2)) +-- first m indices in Hi(C2) 7 SIG(hi(CI U C2)) +-- find the smallest m        16 end 17 end  elements fro?IG(hi(CI)) U SIG(hi(c2)); Calculate Ji (CI, C2) based on Equation 5  end  J -( ) '\'f JJ;;;?e2) hybrid ?2 +-- L..,i=l f if Jhybrid (CI,C2)? (Sel + Se2) ;::: a then  C +-- CI U C2  end  order elements in C append C to Ck  18 return Ck  Algorithm 3: SILVERBACK - columnar probabilistic algo? rithm for finding general frequent item-sets  Input: a, minimal support, W, set of all walls, Vu, the database of transactions  Output: 0, set of all frequent item-sets 1 0+--{} 2 FI +--{xix E W, and supportx ;::: a} 3 F2 +-- Algorithm I (a, W, Vu) 4 0 +-- 0 U FI U F2; k +-- 2 5 for each C E F2 do  I Se +-- support counts from Algorithm I 's byproduct  7 H1(c), ... , Hf(c) +-- obtained from Algorithmi 8 end 9 while Fk i= 0 do  10 k + = 1 11 Ck +-- apriori-gen(Fk_I' a, 12 {HI (c), ... , Hf(C), supporte, 13 for Ic/c E Fk-d) 14 order elements in Ck 15 for each C E Ck do 16 HI ( c), ... , H f ( c) +-- empty ascending priority  queues each with capped capacity m 17 supporte +-- 0; bf +-- vector of Is 18 y +--first item in c; Uy +-- IDs from y column 19 for each x E c\y do 20 I bf +-- AND-mask(bf, x column Bloom filter) 21 end     for each u E Uy do   hI' ... ' hf +-- u's indices in bf, respectively if hI' ... ' hf all set in bf then  supporte+ = 1 append hl, ... ,hf to HI(c), ... ,Hf(c), respectively the cardinality of the item-set, where at each cardinality level,  the algorithm scans Vu (the entire database of transactions) for 27 counting the supports of the candidate sets at that cardinality 28 level. The problem then becomes obvious: the entire execution  end end  of the algorithm scans the database multiple times, which is   if supporte;::: a then I append c to Fk; append c to 0  end not desirable.

Minimizing the iterations of scanning the database is critical  in improving the overall efficiency of association mining algo? rithms, especially for large databases. FP-Growth [ 1 1 ] offers improvements partially due to the fact that it only scans the database of transactions twice in building the FP-tree structure.

However, as mentioned in Section III, the size of the FP-tree structure can be large and reading frequent patterns from the FP-tree requires traversing through the tree which, in turn, still incurs multiple loads. Benefiting from its columnar storage, Eclat [ 1 2] reads activities/transactions column by column and only the necessary columns and intersections of columns are retrieved into memory when checking the candidacy of each candidate. Similar to Eclat, our proposition only retrieves the necessary column files each time and further minimizes the 110 by replacing intersections of columns by AND-masked Bloom filters.

2) Candidate Generation and Probabilistic Pruning: Tra? ditionally, avoiding the exponential growth of candidate item-   32 end 33 end 34 return 0  sets (21w1 possible candidates) by the Apriori principle and other algorithmic improvements [ 10], was based on pruning the unqualified candidate item-sets. Apriori principle becomes especially effective when Vu is sparse and contains large number of items and transactions, which exactly suits our practical usage.

The Apriori-gen function in Algorithm 3 uses Fk-l x Fk-l method [26] to generate, Ck> the set of candidates for frequent k-item-sets. Apriori-gen function then uses a new, minHash? based [27] pruning technique to drastically reduce the candi? dates in Ck and to bring Ck as close to Fk as possible. Mini? mizing the cost of reducing Ck to Fk is key in achieving much  1 078    higher performance than previous Apriori-based techniques.

Fk-l x Fk-l method was first systematically described in  [26]. The method basically merges a pair of frequent (k - 1)? item-sets, Fk-l, only if their first k - 2 items are identical.

Suppose Cl = {ml, ... ,mk-d and C2 = {nl, ... ,nk-d be a pair in Fk-l. Cl and C2 are merged if:  mi = ni (for i = 1 ,  ... , k - 2), and mk-l i= nk-l' ( 1 )  The Fk-l x Fk-l method generates 0 (lFk_112) number of candidates in Ck. The merging operation does not guarantee that the merged k-item-sets in Ck are all frequent. Determining the Fk from the usually much larger Ck becomes a major cost in Apriori execution.

Can one efficiently determine if C E Fk for any C E Ck?

This is the question people have been trying to directly address. But we think one can alternatively ask, based on the Fk-l x Fk-l method, Can one efficiently determine if C E Fk for any c such that c = CI U C2 and CI, C2 E Fk-l? Dealing with C directly basically throws away the known information about Cl and C2. The important question then becomes how can Cl and C2 help determine the candidacy of c.

The key clue lies in S(c), the support set of c. S(c) = S(cd n S(C2)' From previous research, pruning based on the cardinality of S(c) is very expensive. Instead, we propose to consider the Jaccard similarity coefficient [28] in the Apriori? gen function:  J ( ) _ IS(cd n S(c2)1  Cl, C2 - IS(cd U S(c2)1 ' (2)  Measuring J (Cl, C2) is just as costly, so Apriori-gen uses minHash algorithm to propose a novel estimator for J (Cl, C2).

MinHash scheme is a way to estimate J (Cl, C2) without counting all the elements. The basic idea in minHash is to apply a hash function h, which maps IDs to integers, to the elements in CI and C2. Then hmin(Cl/2) denotes the minimal hash value among h( i), Vi E CI/2' Then we claim:  The above claim is easy to confirm because hmin(Cl) = hmin(C2) happens if and only if hmin(ClnC2) = hmin(Cl UC2).

The indicator function, :n.{hmin (C,)=hmin(C2)}' is indeed an unbiased estimator of J (CI, C2). However, one hash function is not nearly enough for constructing a useful estimator for J (Cl, C2) with reasonable variance. The original plan is to choose k independent hash functions, hI,"" hk, and con? struct an indicator random variable, :n.{hi,min(C,)=hi,min(C2)}' for each. Then we can define the unbiased estimator of J (CI, C2) as  (4)  Before the above estimator can be implemented, it is critical to realize its computational overhead in practice. Often k = 50 or more is chosen and the k hash functions need to be applied to each ID in the support of each candidate. At this  stage, typical applications of minHash often use the single? hash variant to reduce computation, Given a hash function h and a fixed integer k, the signature of c, SIG(h(c)), is defined as the subset of k elements of C that have the smallest values after hashing by h, provided that Ici 2: k. Then the unbiased, single-hash variant of Equation 4 is  J - ( ) _  ISIG(h(CI U C2)) n SIG(h(CI)) n SIG(h(c2))1 s.h. CI, C2 - ISIG(h(Cl U c2))1 '  (5) where SIG(h(CI U C2)) is the smallest k indices in SIG(h(cd) U SIG(h(c2)) and can be resolved in O(k).

In general, the single-hash variant is the best minHash can offer in terms of minimizing computational cost. However, one still needs to hash all elements in CI and C2 before he/she can find the signatures, which would make Equation 5 basically as costly as Equation 2. The key step that makes minHash estimation particularly efficient in our case is to link it with the Bloom filters assumed in our framework. Testing a member u in a Bloom filter essentially requires finding several independent hash values that map u to different indices in a bit array. Since the Bloom filter indices are comparable integers, the idea here is to avoid extra hashing in minHash calculation by re-utilizing these integer hash indices. Since all user IDs in the support sets of all frequent item-sets will be tested by the same Bloom hash functions, it guarantees the availability of these hash indices.

Suppose the Bloom filter test sets f number of bits (i.e. it runs the ID through hI, ... , h f for each ID, whose member? ship is to be tested). The direct attempt of utilizing the Bloom filter indices in minHash is simply:  f J (?C2) = L  :n.{hi,min(C11hi,min(C2)} (6) i=l  by replacing k in Equation 4 with f. A potential problem with this scheme is that, to achieve reasonable accuracies in Bloom filter and minHash, the expectations on f and k are very different. Indeed, we find f = 7 is sufficiently good for the Bloom filter while k is usually over 20 in order for minHash to give reliable estimates.

To overcome the empirical difference between f and k, we design a f-hash hybrid approach that uses the f already calculated Bloom hash indices. Choose k to be a fixed integer such that k > f, k = f?m, and m is also an integer. Let hi, for i = 1 ,  ... , f, denote the i-th Bloom hash function. Then the i-th signature of c, SIG(hi(c)) is the subset of m elements of C that have the smallest values after hashing by hi, provided that Ici 2: m. Applying the sig?res to Equat? 5, we obtain f independent estimators, !JCI, C2)" . .  , Jf (CI, C2). Finally, the hybrid estimator J hybrid (CI, C2) is derived as  f -- '" Ji (CI, C2) Jhybrid (CI, C2) = ? f .

i=l (7)  In fact, Equation 6 is a special case of the hybrid estimator.

When k = f and m = 1 ,  Equation 7 becomes equivalent to Equation 6.

1 079    Further, we have  J(Cl,C2) ' ( IS(Cl)1 + IS(C2)1) IS(cd n S(C2) I . IS(Cl) I + IS(C2)1  IS(CI) u S(C2)1 2: IS(cI) n S(c2)1?  (8)  Since IS(cd n S(c2)1 = IS(c)l, it follows that if IS(c)1 2: ex, then J (Cl, C2) . ( IS(cdl + IS(C2)1) 2: ? where ex is the min support. Replacing J (Cl, C2) with J (Cl, C2) gives us the rule Apriori-gen uses to reduce Ck closer to Fk. Observe that Apriori-gen applies the rule in reverse logical order, which introduces false positives. This is why Apriori-gen can only reduce Ck to some superset of Fk, but not exactly Fk.

F The S ILVERBACK Algorithm  The general association mining algorithm with the proposed pruning technique is presented in Algorithm 3. Schematically, it is similar to the original Apriori, but S ILVERBACK ef? fectively addresses the two issues brought up earlier in this section.

The iterations of transaction scans are minimized. The columnar database enables the algorithm to only load the necessary x column at each iteration. Further, by sorting the item-sets in each candidate set Ck and sorting the items in each item-sets, we can make sure each column is loaded only once from the disk and will stay in memory for iterations of all item-set candidates, to which this column belongs.

Probabilistic candidate pruning is key in our proposed algorithm. Indeed, we already show how it can prune off the unworthy candidates. But we are equally interested in its impact to the complexity of the algorithm. In Algorithm 3, the only temporal performance impact is line 26, where the hash indices (which we get for free when testing memberships with Bloom filter) are inserted in Hl(C)" ", Hj(c), each of which is a priority queue of capped length m. The temporal cost for each ID in the test of each candidate without insertions to priority queues would be O(J). The insertions introduce an additional complexity O(J log m). In the Apriori-gen function, for each candidate, lines 5 and 6 cost is O(Jm) and line 7 cost O(Jm log m) due to sorting. To claim that the temporal cost (and the spatial cost, which is bounded by temporal) is basically constant, we need to show that both f and m are small integers and the cost does not increase as the transactions or unique items increase.

f, the number of Bloom hash functions, is said to be 7 in previous section and it only grows logarithmically with respect to the total transactions. So f = 10 would be sufficient for some 1 trillion transactions. m, on the other hand, is determined by f and the minHash error rate. MinH ash introduces error E rv O( +.) to its Jaccard estimation J, which is between 0 and l. ?ose that E < 0.06 is satisfactory and f = 7, then m = 40 is sufficient. Further, if f increases to 10, m = 28 would be sufficient for achieving the same E.

S ILVERBACK is scalable and can be deployed on a cluster.

The column files and Bloom filter files are distributed across the slave servers of the cluster. An index file is stored  on the master server to keep track of the slave, on which a particular column file or Bloom filter is stored. A nice property of S ILVERBACK is that only the user IDs from one column are necessary to be loaded in memory at any given moment of the execution of S ILVERBACK. This implies that the uncompressed, large column files are never moved from slave to slave over the network. Only the compressed strings of Bloom filters are loaded from other slaves when necessary.

This property minimizes general intra-cluster I/O traffic and makes our algorithm scalable.



V. EX PERIMENTAL OB SERVATION  We now present the experiments that we conducted for evaluating the proposed methodologies.

A. Dataset  Our data is collected from two widely used social media platforms: Facebook and Twitter. Both Facebook and Twitter are a medium for individuals, groups or businesses to post content such messages, promotions or campaigns. The user comments/tweets, and user information from specific interests is publicly available and collected using Facebook API2 and Twitter AP13. In the experiments, the data collected over 20 12 is  used. Table I I  shows the size of  the databases we are maintaining using the proposed infrastructure and the amount of data used in the experiments.

TABLE II DATASETS SUMMARY STATISTICS  Statistic Facebook Twitter Unique itemslinterests 32K+ llK+ (used in experiments) 22,576 4,29 1 Total user activities lOB+ 900M+ (used in experiments) 226M 24.2M Unique users/transactions 740M+ 1 20M+ (used in experiments) 27 .4M 3 .7M  B. Errors from Sampling and Bloom Filter  As discussed earlier, a Bloom filter allows for false posi? tives. In this section we discuss how different capacity sizes and false positive probabilities affect the target-driven rule calculation. With the introduction of the probabilistic data structure, the computation of Supp{X U Y} i.e. the COlmnon users that have shown interests in both interests X and Y is affected, which in turn affects the order the relevant precise interests.

TABLE III COMMON USER COUNT  interest TM CM Ct EASPORTS 242399 1647 33197 techcrunch 202812 12295 32579 iTunesMusic 189568 7265 24171 googJe 149877 12022 21352 facebook 120724 8904 14212  2 http://deveIopers.facebook.coml 3https:lldev.twitter.comldocs/  C2 C3 C4 1647 10085 6611 12295 17105 15647 7265 10625 9698 12022 13797 13621 8904 9746 9859  CS C6 C7 1708 2136 1714 12950 13147 12496 7513 7640 7640 12605 12636 12636 9356 9365 9365  1 080    interest CI EASPORTS 3 1 550 techcrunch 20284 iTunesMusic 1 6906 google 9330 face book 5308  interest Cl EASPORTS 0.829 techcrunch 0.890 iTunesMusic 0.908 google 0.949 facebook 0.97 1  interest Cl EASPORTS 0.050 techcrunch 0.377 iTunesMusic 0.30 1 google 0.563 facebook 0.627  interest Cl EASPORTS 0.095 techcrunch 0.548 iTunesMusic 0.462 google 0.720 facebook 0.770  TABLE IV FALSE POSITI VES  C2 C3 C4 1 402 8438 4964 1085 48 1 0  3 3 5 2 5 6 8  3360 2433 648 1775 1 599 469 842 955  TABLE V ACCURACY  C2 0.992 0.994 0.997 0.996 0.997  C2 0.540 0.9 19 0.927 0.949 0.950  C2 0.701 0.958 0.962 0.974 0.974  C3 C4 0.954 0.973 0.974 0.982 0.982 0.987 0.990 0.99 1 0.995 0.995  TABLE VI PRECISION  C3 C4 0. 163 0.249 0.7 1 9  0.786 0.684 0.749 0.87 1 0.883 0.9 14 0.903  TABLE VII F-MEASURE  C3 C4 0.28 1 0.40 1 0.836 0.893 0. 8 1 2  0.88 1 0.93 1 0.952 0.955 0.964  CS C6 C7 6 1  489 67  655 852 20 1 248 375 375 5 8 3  6 1 4  6 1 4 452 46 1 46 1  CS C6 C7 1 . 000 0.997 1 .000 0.996 0.995 0.999 0.999 0.998 0.998 0.997 0.997 0.997 0.998 0.997 0.997  CS C6 C7 0.964 0.77 1 0 .961 0.949 0.935 0.984 0.967 0.95 1 0.9 5 1 0.954 0.95 1 0.9 5 1 0.952 0.95 1 0.9 5 1  CS C6 C7 0.982 0.569 0.980 0.974 0.932 0.992 0.983 0.929 0.975 0.976 0.964 0.975 0.975 0.970 0.975  Table III shows precise interests generated for target interest amazon for the period of July-December of 20 1 2. For each in? terest we provide Total Mentions(TM), which is the number of users who expressed interest, COlmnon Mentions (CM), which is actual number of common users who expressed interest for both interests (true positives), and different configurations of Bloom filters. Configurations C1 ,  C2, and C3 have false probability 0.10, 0.002, and 0.02 respectively and a filter capacity of 1 00,000. Configurations C4, C5, and C6 have false probability 0.1 0, 0.002, and 0.02 respectively and a filter ca? pacity of 200,000. Configuration C7 is the only configuration where the Bloom filter is built using sample(S) size equal to the capacity size (200,000) if the TM is over the capacity size and its false probability is 0.02. In configuration C7, the common mentions for the Bloom filter is then estimated proportionately based on the total mentions. Note the that total number of mentions for amazon is 1 84, 1 17.

Due to the probabilistic nature of the data structure, we use predictive analysis approach where we evaluate the effective measure of our system by formulating a confusion matrix, i.e., a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. The COlmnon mentions given by Bloom filter comprise of true positives and false negatives. Table  IV provides the number of false positive (jp), which deduced using conunon mentions from Bloom filter and true COlmnon mentions. The number of false negatives is always zero due to the nature of Bloom filter. Therefore, the true negatives (table not shown) are easily deduced. The accuracy, precision and F-measure is provided in Table V, VI and VII, respectively.

As expected, for a given capacity, as the false positive probability decreases, the accuracy ( (tp + tn)/(tp + tn + fp + fn? and precision (tp/ (tp + fp? both increase. The recall (tp/ (tp + fn? is always 1.0, i.e., all relevant users were retrieved because our system with Bloom filter does not permit false negatives. The precision for our system is always less than 1.0 as not every result retrieved by the Bloom filter is relevant. As the capacity is increased, the accuracy and precision further improve. Note that when the total mentions is greater than the capacity, the Bloom filter has higher inaccuracy for a fixed false probability. For example for EASPORTS , the accuracy is 15% lower for capacity of size l OOK vs. 200K for the false probability of 0.10. This is due to the property that adding elements to the Bloom filter never fails. However, the false positive rate increases steadily as elements are added until all bits in the filter are set to 1.

To counter this effect we sample data to be added to Bloom filter. Sampling can have an impact on the false positive rate of Bloom filters depending on the sampling quality. For example the number of false positives for EASPORTS, for Bloom filter configurations C5 and C7, are 6 1  and 67 respectively. But the false positives drop for techcrunch when sampling is used.

Due to probability of false positives, the interests order arranged in decreasing order of the common mentions count can be different. We use the Kendall Rank Correlation co? efficient or short for Kendall's tau (7) coefficient [29] to evaluate our results. Measuring the rank difference instead of absolute error that our probabilistic algorithm makes is due to practical interests. It is more often the case that our customers would ask queries like the top X number offrequent items associated with my brand. 7 is defined as the ratio of the difference between concordant and discordant pairs to the total number of pair combinations. The coefficient range is -1  :s; 7 :s; 1 ,  where 1 implies perfect agreement between rankings. Table VIII provides the Kendall statistics for two Bloom filter configurations. Both configurations approximately have 7 value of 0.98, implying that our rankings are very close in agreement compared to original rank. Also since the 2-sided p-value is less than 0.0000 1,  this implies that the two orderings are related and the 7 values are obtained with almost 1 00% certainty.

TABLE VIII KENDALL T RANK CORRELATION TABLE  Measure 200K, 0.02 200K, 0.002 Kendall T-statistic 0 .9825 1 0 .98455 2-sided p-value < 0 . 00001 < 0. 00001 S. Kendall Score 3847 3855 Var (S) 79624.33 79624.34 SIT. Denominator 3915 .5  3915 .5  1 08 1    C. Temporal Scalability and Efficiency  Fig. 3. Scalability comparison  In addition to evaluating the accuracy of our probabilistic algorithms, we still need to demonstrate their efficiency and scalability. After all, good efficiency and scalability are ex? pected trade-offs by sacrificing accuracy.

In Figure 3, we report the run times for different combi? nations of computing nodes, and minimum support threshold values, for four different algorithms. In the legend of Figure 3, HA denotes the naive implementation of Apriori in the MapReduce framework [5]. CS, CSBF, and S ILvERBACK de? note our proposed algorithm with progressively more features.

CS denotes a diminished version, where only the columnar storage is used but not the Bloom filter enhancement or the minHash pruning technique; CSBF is like CS but implements the Bloom filter enhancement for each column file; and finally, S ILVER BACK is the fully blown version that incorporates all techniques presented in our paper including the minHash prun? ing technique. In addition, a dashed line of ideal scalability is included for each of the four methods compared in Figure 3.

In both support levels (0.05% & 1 %), HA seems to have the most reliable speedup as the number of computation nodes increases. The CS method significantly deviates from the ideal speedup as we increase up to 32 nodes. We suspect its lack of scalability is due to the increase of I/O traffic, since the IDs in each column are not compressed like CSBF or S ILvERBACK and would pose significant load on the I/O. Both CSBF and S ILVER BACK exhibit superior scalability over CS, especially in the low support setup.

HA, the Hadoop solution, seems to have better scalability than all other algorithms, although its absolute run time is not  the lowest. Will HA be the fastest eventually if the number of nodes keeps on increasing? We think the relatively superior scalability in HA is mainly due to two aspects. First, HA, unlike the other three methods, is implemented on a Hadoop cluster with slightly better computational capability per node but much better inter-node connections (32 Gbitls InfiniBand).

The budget cluster, on which CS, CSBF, and S ILvERBACK are implemented, simply uses corporation-domain IP addresses as node identifiers. Second, S ILvERBACK still has room to improve its scalability to more nodes as this algorithm is only proposed in this paper while Hadoop Apriori is much more mature.

The ranks of performance for the four methods are con? sistent under both support levels. The two probabilistic ap? proaches, CSBF and S ILvERBACK, perform consistently faster than the exact ones, HA and CS, which is predicted as we expect sacrificing accuracy would significantly boost the temporal performance. CS performs consistently worst, which suggests that proposing a columnar storage by itself does not quite solve any problem.

Investigating the relative changes in the inter-method gaps under different support levels reveals more on the impact of minHash pruning and Bloom filter enhancement. First, the difference made by using Bloom filters, as illustrated by CS and CSBF, increases when min support level drops. Second, the use of minHash pruning technique also amplifies its impact as the support level decreases.



VI. CONCLUSIONS  We presented the S ILvERBACK framework, a novel solution for association mining from a very large database under constraints of a modest hardware. We proposed accurate prob? abilistic algorithms for mining frequent item-sets, specifically catering to the columnar storage that we adopted, which is en? hanced by Bloom filters and reservoir sampling techniques to enable storage efficiency. Our Apriori-based mining algorithm prunes candidate item-sets without counting every candidate's support. As our experiments showed, S ILvERBACK outper? forms Hadoop Apriori on a more powerful cluster in terms of run time, while our probabilistic approach yields a satisfactory level of accuracy.

The S ILvERBACK framework has been successfully de? ployed and maintained at Voxsup since May 20 1 1. Our ongoing efforts are focusing on further improvement our system performance and scalability. Specifically, in the near future we would like to develop more efficient inter-nodal communication solutions, which is critical to scale to hundreds of nodes.

AC KNOWLED GMENTS  This research was supported by Voxsup, Inc. and in part by NSF awards CCF-0833 1 3 1 , CNS-0830927, CNS-09 10952, III- 1 2 1 303 8, IIS-0905205, CCF-0938000, CCF-1029 1 66, ACI-1 l4406 1 ,  and IIS- 1 343639 ;  DOE awards DE-FG02-08ER25848, DE-SC000 1 283, DE-SC0005309,  1 082    DESC0005340, and DESC0007456; AFOSR award FA9550- 1 2-1-0458. We are thankful to our colleagues Zhengzhang Chen and Yu Cheng for their insights in our technical discussion.

