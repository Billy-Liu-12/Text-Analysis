Leveraging Memory Mapping for Fast and Scalable Graph Computation on a PC

Abstract?Large graphs with billions of nodes and edges are increasingly common, calling for new kinds of scalable compu- tation frameworks. Although popular, distributed approaches can be expensive to build, or require many resources to manage or tune. State-of-the-art approaches such as GraphChi and TurboGraph recently have demonstrated that a single machine can efficiently perform advanced computation on billion-node graphs. Although fast, they both use sophisticated data struc- tures, memory management, and optimization techniques. We propose a minimalist approach that forgoes such complexities, by leveraging the memory mapping capability found on operat- ing systems. Our experiments on large datasets, such as a 1.5 billion edge Twitter graph, show that our streamlined approach achieves up to 26 times faster than GraphChi, and comparable to TurboGraph. We contribute our crucial insight that by leveraging memory mapping, a fundamental operating system capability, we can outperform the latest graph computation techniques.

Keywords-graph mining; scalable algorithms; memory map- ping; single machine

I. INTRODUCTION  Large graphs with billions of nodes and edges are increas- ingly common in many domains, ranging from computer science, physics, chemistry, bioinformatics, to linguistics.

Such graphs? sheer sizes call for new kinds of scalable computation frameworks. Distributed frameworks become popular choices; prominent examples include GraphLab [1], PEGASUS [2], and Pregel [3]. However, such systems often demand additional cluster management and optimization skills from the user; and shared-memory systems can be expensive to build [4], [5].

Some recent state-of-the-art works, such as GraphChi [4] and TurboGraph [5] take an alternative approach by, instead, focusing on pushing the boundaries as to what a single machine can do. Their impressive results demonstrate that even for billion-node web-scale graphs, computation can be performed at a speed that matches that of a distributed framework, and at times even faster.

We agree that single-machine approaches are promis- ing, and indeed they can be attractive for researchers and practitioners who want scalable computation without having to use computing clusters. However, when analyzing these works, we observe that they often require sophisticated  techniques [4], [5] to do explicit memory allocation, edge file partitioning, scheduling, etc., in order to boost speed.

Can we streamline all these, and still achieve the same, or even better performance than the state-of-the-art approaches?

We believe we can. In the paper, we propose a minimalist approach that does exactly this and present our initial results to demonstrate its feasibility. Specifically, our major contributions and results include:  ? We contribute our crucial insight that by leveraging memory mapping, a fundamental capability from oper- ating systems, we can conduct high-speed graph com- putation that outperforms state-of-the-art approaches, while sidestepping common design complexities.

? We demonstrate through experiments on real, large graphs, including a 1.47 billion edge Twitter graph, that our streamlined approach, with only 184 lines of statements1, can be up to 26 times faster than GraphChi, and comparable to TurboGraph.

We note that we are not advocating replacing existing approaches with ours. Rather, we intend to highlight how much performance gain we can achieve by leveraging the memory mapping capability alone. We believe other ap- proaches can greatly benefit from integrating this technique into their implementations.

The rest of the paper is organized as follows: Section II briefly surveyed related work. Section III describes our main Memory Mapping idea for boosting graph computation speed. Section IV presents experiment results that shows how our approach compares with GraphChi and Turbo- Graph. Section V concludes and discusses future work.



II. RELATED WORK  We survey some of the most relevant works, which may be broadly divided into multi-machine and single-machine approaches.

Multi-machine. GraphLab [6] is a recent, best-of-the- breed distributed machine learning library for graphs. It exploits multiple cores to achieve high computation speed.

However, like many other shared-memory approaches, it requires the graph to fit in memory. For huge graphs that  1Number of statements measured by Eclipse?s Metrics plugin       do not fit in memory, distributed disk-based approaches are popular, such as Pegasus [2] (runs on Hadoop), and the Google Pregel system [3] (similarly, Apache Giraph).

Single-machine. This category is most related to our work. GraphChi [4] was one of the first works that demon- strated how graph computation can be performed on massive graphs with billions of nodes and edges on a commod- ity Mac mini computer, with speed matching distributed frameworks. More recently, Turbograph [5], improves on GraphChi, with greater parallelism, to achieve speed orders of magnitude faster. These systems use sophisticated data structures and memory management techniques. Our work aims to achieve an even greater speed, with a simpler design; the experiment results in Section IV demonstrate our success.



III. OUR APPROACH  A. Overview and Motivations  In this section, we describe our fast, scalable approach that leverages memory mapping to speed up graph computation.

Memory mapping is a fundamental capability in operating system (commonly used to support virtual memory). How- ever, it has not been exploited extensively by state-of-the- art approaches such as GraphChi and TurboGraph. Instead, they divide the edges into logical sections or separate files on disk, and selectively load them into memory.

Although fast, these approaches require explicit memory management and optimization in order to achieve high throughput and speed. They may also be harder to develop and maintain. For example, the GraphChi package contains about 8000 lines of code [4].

Can we streamline all these, and still achieve the same, or even better performance than the state-of-the-art approaches?

We believe we can. And this motivated us to investigate to the idea of leveraging memory mapping to achieve a minimalist approach that is not only faster, but also simpler than GraphChi and TurboGraph. Our implementation has only 184 lines of statements.

In the next few subsections, we briefly describe what memory mapping does, its benefits and how it can help with graph computation. We refer the reader to [7], [8], [9] for more details on memory mapping.

B. Memory Mapping and Its Advantages  Memory mapping is a mechanism that maps a file or part of a file into the main memory. By doing so, files on disk can be accessed the same way as if they were in memory [9]. This makes it possible to do I/O operations faster than accessing disk directly. The basic idea of the mechanism of memory mapping is illustrated in Figure 1.

1) Fast I/O Operations: The benefit of faster I/O speed provided by memory mapping is especially apparent when an application needs to execute a good number of operations on the same chunks of address space on disk. The OS  Figure 1: The mechanism of memory mapping. A portion of a file on disk is mapped into memory for use (blue); potions no longer needed are unmapped (yellow). In our approach, our file is a large edge list (on the left) which typically does not fit in the main memory (on the right). Our algorithm treats the edge file as if it were fully loaded into memory; programatically, it is accessed like an array. Each ?row? of the edge file describes an edge, identified by its source node ID (left) and target node ID (right).

typically keeps these frequently accessed chucks in memory automatically, so subsequent ?reads? from disk become high-speed reads from memory. In addition, as the OS does most of the work, additional low level optimization can be more directly provided by the hardware.

2) Less Overhead: Many programs that process large files requires a lot of manual optimization to reach good performance. Nevertheless, the OS does most of the work for memory mapping and depends less the developers for optimization. For example, as a rough comparison, GraphChi was written in more than 8000 lines of code [4]; our imple- mentation has only 184 lines, while achieving significantly better performance.

C. Our Idea: Memory-map Edge File for Fast Computation  As identified by GraphChi and TurboGraph researchers [4], [5], the crux in enabling fast graph computation is to design efficient techniques to store and access the graph?s edges, because many widely used graph algorithms eventu- ally boil down to become repeated matrix-vector multiplica- tions at their cores. The matrix concerned here is often the graph?s adjacency matrix (or its variants), which we store as an edge list (see Figure 1).

GraphChi and TurboGraph, among others, designed so- phisticated methods such as parallel sliding windows [4] and pin-and-slide [5] to efficiently access the edges. We show that we can forgo them and still achieve high speed,     Table I: Networks used in experiment  Name Nodes Edges LiveJournal 4,847,571 68,993,773 Twitter 41,652,230 1,468,365,182  at times significantly faster (up to 26 times faster) as shown in Section IV.

In more details, we first convert the raw, text-base edge list into a binary file, which consists of m integer pairs where m is the number of edges in the graph. Then we map the whole file into the main memory, even though we may not have enough main memory. For example, the Twitter network?s binary edge file is 11GB on disk, while we only have 8GB main memory. The reason is that the OS only reads sections from the file (and map them to memory) when they are needed, or expected to be needed by the process. Portions that are no longer needed are automatically unmapped by the OS (see Figure 1). To the algorithm users, and the algorithm authors, all these mapping and un- mapping operations are transparent. They can view the edge file as one large, contiguous file, and access it as if it were in memory.



IV. EXPERIMENT  A. Goal and Overview  We compare our Memory Mapping approach with two state-of-the-art approaches, GraphChi [4] and Turbo- Graph [5], by measuring the elapsed times of two classic graph algorithms: Connected Component and PageRank.

We first describe the graph datasets used for this experi- ment and our setup, then we present and discuss our results.

B. Datasets and Experimental Setup  Datasets: To understand how the three approaches per- form at different scales, we selected one smaller and one larger graph: a LiveJournal network [10] with 69 million edges, and a Twitter network [11] with 1.47 billion edges.

Table I shows the exact statistics of these two graphs.

Test computer: All tests are conducted on the same laptop computer with Intel i7-2620M quad-core CPU at 2.70GHz, 8GB RAM and 512GB SSD of Samsung 840 Series.

Since TurboGraph can only be run on Windows and GraphChi requires a library missing on Windows, we con- duct the tests for TurboGraph and Memory Mapping on Windows 8 (x64), and the tests for GraphChi on Linux Mint 15 (x64).

Implementations tested: ? GraphChi: v0.2.6 C++ version with default configura-  tions. The full GraphChi package contains about 8000 lines of code [4].

? TurboGraph: v0.1 Enterprise Edition We have varied its buffer size from 1GB to 4GB and report the best times recorded. TurboGraph?s source code is not available.

? Our Memory Mapping approach: Java 1.7 implementa- tion; 184 lines of statements.

Test Protocol: Each test is run under the same configu- ration for 3 times and the average is reported, as shown in Figure 2a and b. Page caches are cleared before each test.

C. Results on 69M edge LiveJournal Network  Figure 2a shows the elapsed times of finding connected components and running 1 and 5 iterations of PageRank on the LiveJournal Network with 69 million edges. Our Memory Mapping approach (in orange) shows great performance in all three tests. For 1-iteration PageRank, our approach is up to 26x faster than GraphChi and 3.4x faster than TurboGraph. We believe our significant speedup is due to the LiveJournal graph being relatively small (its binary edge file is around 526MB), so that the operating system can memory- map the entire file and keep it in the physical memory at all times, eliminating many loading and unloading operations that the other approaches may require.

This result suggests that low-level optimizations per- formed by the operating system may significantly out- perform explicit memory management that typical graph computation packages are employing.

D. Results on 1.47B edge Twitter Network  After testing on the LiveJournal graph, we test on a much larger graph?a Twitter graph with 1.47 billion edges.

Figure 2b shows the results. Similar to those for the Live- Journal network, Memory Mapping outperforms GraphChi, by at least 3 times for each test (e.g. 1,173s vs. 246s for 5 iterations of PageRank; 4.77 times as fast), and matches the speed of TurboGraph.

We were unable to run TurboGraph?s PageRank algorithm for more than 1 iteration. To estimate its 5-iteration timing, we extrapolate from its 1-iteration time, which gives 207 sec- onds. We use the formula 47?164400?37200 = 207 where 47 is the elapsed time, in seconds, we measured for one iteration, and 37200 and 164400 are respectively the elapsed time, in ms, of running 1 and 5 iterations of PageRank listed on TurboGraph?s website (http://wshan.net/turbograph/).

A possible explanation for Memory Mapping matching TurboGraph on the Twitter network is due to the its much larger binary edge file (11GB on disk). With only 8GB RAM total, the system cannot fully load it into memory; instead, it must load the edges from disk on demand. However, this behind-the-scene change is transparent to the algorithm user (or algorithm author). Our code remains the same, and our edge file remains as one single file on disk; re-sharding is unnecessary.



V. CONCLUSION AND FUTURE WORK  We contribute our crucial insight that by leveraging mem- ory mapping, a fundamental operating system capability,     Connected Comp 1-PageRank 5-PageRank    40.4   49.1  6.82  2.38 6.07  1.72 0.43 1.8E la  ps ed  Ti m  e (s  ec on  ds )  GraphChi TurboGraph Memory Mapping  (a) LiveJournal graph (69 million edges)  Connected Comp 1-PageRank 5-PageRank    1,000    1,173       E la  ps ed  Ti m  e (s  )  GraphChi TurboGraph Memory Mapping  (b) Twitter graph (1.47 billion edges)  Figure 2: Comparing the elapsed times (in seconds) of three approaches: GraphChi, TurboGraph, and our Memory Mapping, on (a) 69 million edge LiveJournal network, and (b) 1.47 billion edge Twitter graph. Graph algorithms evaluated are, from left to right: connected components, 1 iteration of PageRank, and 5 iterations of PageRank. Our approach, in orange, is up to 27 times as fast as GraphChi, for 5 iterations of PageRank on the LiveJournal graph (3.37 times vs. TurboGraph), and 4.77 times on the Twitter graph.

we can outperform state-of-the-art graph computation ap- proaches. Using large, real graphs of up to 1.5 billion edges, we compare our approach with two state-of-the-art single- machine computing systems: GraphChi and TurboGraph.

We demonstrate that our minimalist approach?one that forgoes explicit memory management and data structure design employed by other approaches?is up to 26 times faster than GraphChi, and comparable to TurboGraph. Our streamlined implementation has only 184 lines of statements.

Our work has shown us to an exciting, new direction that could push the single-machine graph computation speed to a new height. We look forward to seeing other approaches integrate our work. For the road ahead, we plan to explore several related ideas, such as to (1) port our Java implemen- tation to C++ for even greater speed; (2) investigate how using space-efficient data structures such as Compressed Sparse Row for storing the edges may help boost speed; and (3) explore how to support time-evolving graphs.

