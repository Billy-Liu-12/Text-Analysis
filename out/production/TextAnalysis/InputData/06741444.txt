Head Pose and Gaze Direction Tracking for Detecting  a Drowsy Driver

Abstract? This paper proposes a system that uses gaze direction tracking and head pose estimation to detect drowsiness of a driver. Head pose is estimated by calculating optic flow of the facial features, which are acquired with a corner detection algorithm. Analysis of the driver?s head behavior leads to three moving components: nodding, shaking, and tilting. To track the gaze direction of the driver, we trace the center point of the pupil using CDF analysis and estimate the frequency of eye-movement.

Keywords? Driver Drowsiness Detection, Gaze Direction, Eye Blinking, Head Pose Estimation

I. INTRODUCTION (HEADING 1) The major categories of car accidents are divided into  distraction and drowsiness. Drowsy driving refers to when a driver is half-sleeping after a long period of driving. Currently, more than 30% of deaths caused by car accidents are attributed to drowsy driving. In 2008, National Highway Traffic Safety Administration (NHTSA) estimates that 100 000 police reports on vehicle crashes were direct outcomes of driver drowsiness resulting in 1550 deaths, 71 000 injuries, and $12.5 billion in monetary losses [1]. For that reason, car manufacturers in the world are eagerly developing a system that can prevent drowsy driving.

Head pose estimation is a good index that directly shows the current state of the driver. When drowsy, a person?s head is leaned forward due to the weight of the head. However, then, it becomes difficult to breath and the head tries to regain the normal state, which results in nodding. Head pose estimation has been developed into POSIT(Pose from Orthography and Scaling with ITerations) algorithm [2]. The relationship between a 3D object and 2D feature can be estimated. However, it was difficult to build a continuous relationship among objects.

Meanwhile, La Cascia et al [3] estimated the head pose with a cylinder model using a texure-map method. Though it runs fast, there was a problem with the real-time use due to its low accuracy. Therefore, we adopted the POSIT plus auto reinitialization method [4], which is a model suitable for the real time case because it can reinitialize in a fast and automatic way.

The direction of gaze can be recognized by observing pupil of the eye. Since the position of the pupil?s center indicates the gaze direction. Pupil center detection includes the appearance- based method and feature-based method. The other approach would be using Adaboost algorithm that requires an extensive  training using a lot of pupil images [5]. It allows eye-tracking even with eyeglasses. However, it is not fast and the result varies according to the training data. Next, the feature-based method includes Zhou?s method based on GPF(generalized projection function) [6], which finds the center point by projecting the x and y coordinate axes. It has higher accuracy than the appearance-based method, but the problem is the processing time by domain processing. We have used the feature-based method that uses CDF(Cumulative Distribution Function) [7]. Although it does not allow tracking with eyeglasses, it was suitable for real-time processing because of its fast speed.

Also, gaze detection is widely used not only for drowsiness prevention systems but also for other areas including marketing, psychology, and HCI(Human Computer Interaction). For instance, in marketing, people?s level of interest can be measured by the time during which they directly look at a product. Already, large retailers have used for a system that decides product arrangement and sales. However, this kind of systems requires high maintenance costs. Moreover, gaze detection is actively used as study user interface in HCI research area. Gaze is used to control a computer mouse or for people with disabilities to communicate. However, there are problems with its accuracy and eye fatigue.

Additionally, Wierwille used PERCLOSE (Percentage of Eye Closure) based on the increase of blinking. It determines drowsiness by calculating the proportion of blinking in a designated frame [8]. Also, drowsiness can be determined based on the opening of mouth using BPNN (back propagation neural networks), which was introduced by Wang Rongben [9].

This study proposes a technology that directly determines the driver?s state using a camera, which is a non-contacting sensor. We will determine the drowsiness of a driver by tracking the gaze direction and using head pose estimation after detecting nodding.

II. HEAD POSE ESTIMATION.

We wish to calculate rotation vector of 3D in a 2D image  while automatically performing initialization (not training).To do so, a face with 3D coordinate, instead of 2D, has been  ( )=( Tilting, Shaking, Nodding)  This research was supported by Basic Science Research Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Education, science and Technology(NRF-2013R1A1A2006969) and by The Ministry of Knowledge Economy of Korea, under IT/SW Creative research program supervised by NIPA(National IT Industry Promotion Agency)? (NIPA-2013- H0502-13-1019)     (5.97 (7.05  Figur  creat mod POS obje Data  A. H T  rotat rotat axis)   T  for t acqu KLT Alig curre that initia corre  M extra vario and poin vecto desig  T stand POS calcu betw Equa  I and Cam poin  76,-22.104, -0.461 54,-22.774,-3.508  re 1. Head Pose E  ted to increase del created by SIT algorithm ct. The test  abase.

Head Pose Est The purpose tion vector o tion vector c ), and tilting(Z  R ?????	??, ?  To calculate th tracking will uired at first. F T(Kanade-Luc gnment is per ent face and the 2D image  alization is esponds to the  Motion trackin acted for the ous motion ta accuracy. M  nt. Later, Opti or is the sam gnating a certa  Third, the rot dard 3D coor  SIT algorithm ulates rotation  ween points on ation (2) is the  s ? ? ? ? ? ?  ??  In this formul (u,	v) that of  mera Matrix ha nts of image,  1) 8)  (1.274,-3.9 (0.994,-3.3  Estimation Result  e accuracy. It using FaceG is adopted to result is ev  timation of head pose f the face in  corresponds to Z axis). Equati  R ? ???????? , ?? ? ???????  he rotation ve be extracted For feature ex  cas-Tomasi) f rformed based the 3D avera e face and 3D  performed e extracted fea  ng is perform second time.

acking method otion vector ical Flow loo  me but only th ain block.

tation is calcu rdinate and cu  m is used to n and transla n the 2D and c e formula of P  0 ?? ?? ?? 0 1  ? ? ?????????  la, (X,	Y,	Z) r f 2D, which is as intrinsic pa (cx ,cy ); scal  933,-18.786) 381,-21.014)  t (top : Ground Tr  is based on a Gen[10]. To es o measure the valuated by u  e estimation i n the current o nodding(X ion (1) shows  ???????? ??, ???????,? ctor above, 30 from a 2D i  xtraction, Cor feature tracke d on the pupi age model. An D model corre  to the 3D ature points.

med regarding . We use Op ds as it has hi is calculated  oks for a vec he position ha  ulated by usi urrent 2D coo o obtain the ation based o corresponding  POSIT.

??? ??? ? ??? ??? ? ??? ???    epresents the s obtained ba arameters, wh le factor (s),  (28.959,9.492,4.

(31.489,7.213,8.

ruth, down: Estim  an average 3D stimate head p e rotation of a using the Bo  is to calculate image. Here axis), shakin  rotation vecto  ???????, ? 0-40 feature p image, which rner Detection er has been u il positions in nd then, assum espond one-to  coordinate  g the feature p tical Flow am igh tracking s  based on fe tor whose mo  as been move  ing the initia ordinate. Here  rotation. PO on the correl g points on the  ?????? ? ? ? ?? ?  coordinate o ased on projec hich are the c and focal le  .460)  .010)  mation)  D face pose, a 3D oston  e the e, the ng(Y  or.

????  points h was n and used.

n the ming -one,  that  point mong speed eature otion  ed by  alized e, the OSIT lation e 3D.

????  of 3D ction.

center ength  betw matr algo on th  A corr stan 2D i rotat  B. H W  eval of 7 Each vect show estim Figu of th  N MA Equ  ween pixels ( rix,?R|t?, are  orithm estimat he 3D.

Figure 2. H (to  As input data responds to th ndard model co image. With th tion vector co  Head Pose Es We used the p luate the estim 72 videos of h video has tor by installi ws the exact mation result.

ure 2 is the ro he video.

Next, to obta AE (Average M uation (3) show  M  ( fx , fy ). The extrinsic param tes the rotatio  Head Pose Estima op : Ground Truth  of POSIT, th e 3D object is oordinate and he obtained ro rresponding to  stimation Resu public databas  mation of head 200 frames a exactly calcu  ing magnetic ground truth Each rotatio  otation vector  in exact estim Mean Absolute ws MAE.

M?? ? 1?? ?  ???  rotation matr ameters that ha on matrix and  ation Result abou h, down: Estimati  he feature coo s received, an feature point  otation matrix o three axes is  ult se of Boston U d pose [2]. BU and uniform a culated rotatio  sensors on t th values in on vector is s  graph regardi  mation error, e Error) for the  ???? ? ???? ?  rix and transl averijandtij. PO  d translation m  t 200 frames ion)  ordinate of 2D nd these are th  extracted from as the determ  s calculated.

University (B U database con and varying li on and transl the head. Figu database and  shown as (?, ing the 200 fr  we calculate e entire 200 fr  lation OSIT  matrix  D that he 3D m the  minant,  U) to nsists ights.

lation ure 1 d our ?, ?).

rames  d the rames.

????     H truth MAE  A estim drow bigg  H We t mov findi detec of p poin face direc  A. P B  cente high used the d pupi the h The the d  H the mult  Here, N refers h and estimat E result of the  TABLE I.

MAE  As shown in th mated result o wsiness is dete ger in a short p  Human eyes m track the gaze  vement and th ing the cente ction based on  pupil center d nts, the averag  will be meas ction.

Figure 3. F  Pupil Center D Because the ga er detection b  hly accurate in d Face Adaboo detected area, ils are located hair and ears distribution m  designated RO  CDF?   Here, P(w) rep total size of tiplied by th  to the entire tion respectiv  e above graph.

MAE ACCO (DEGREES O  Tilting 1.467  he table above of the head ermined when period.



III. GAZE move very rap e direction bas he center. The er of the pup n the conventi detection using ge distance be sured to find  Flowchart of Pupil  Detection usin aze must be d based on CDF n calculation.

ost of Viola-Jo we designate  d, as ROI (Re so that we ca  map is created OI. Equation (4  ?r? ? ? ??w ?  ???			where ? 0 presents the h f it, which eq he height. T  e frames and ? vely. Table 1  ORDING TO THE GR OF RADIAN)  Shaking 1.856  e, there is high pose. Based the change in  E DIRECTION idly and the g ed on the vert direction trac pils. We use ional CDF. Fig g CDF. By u etween the ey an error and  l center detection  ng CDF detected real ti F analysis, w First, in the in ones to detect the top right a  egion of Intere an reduce the d based on the 4) shows the r  w? ? w ? 255  istogram of th quals the wid  The CDF dis  ??, ???  ground 1 shows the  RAPH ABOVE  Noddin 3.992  h accuracy wit on this accu  n nodding is 3  gaze varies wi ical and horiz cking begins  the pupil c gure 3 is flow using the dete yes on the cu d analyze the  using CDF  ime, we used p which is quick  nputted image t the face. Late and left, wher est). This incl error in detec e CDF analys esult.

he gray level a dth of the im stribution ma  ?  ng  th the uracy, 0? or  idely.

zontal from  center wchart  ected urrent  gaze  pupil k and e, we er, in re the ludes ction.

sis of  ????  and r mage ap is  gene hum  alwa distr  B whit prel beca refle diffe  L Min From desi calc of p  erated by accu man pupils hav  Figu  ays white. Ba ribution map i  I??x, y? ? By equation (5 te, including iminary region ause the dete ected on the p erent result.

Later, for exa nimum Intens m the center ignated and t ulate the exac upil center det  umulation of ve darker color  ure 4. Pupil cente  sed on this fa is extracted as  ? ?255 CDF0 5), the prelimi  g the pupil, n does not det ection rate ch pupil and diff  ct pupil cente ity) is extrac r of the ext the CDF ana ct center of the tection using C  Figure 5. Experim  brightness of r than the skin  er detection using  act, the section s a preliminary  F?I?x, y?? ? 0.

Otherwise  inary pupil reg , eyebrow, tect the exact  changes accor fferent pupil c  er detection, P cted from RO tracted PMI, alysis result i e pupil. Figure CDF  ment result using  f ROI. Most o n, and the scler  CDF  n until 0.05 o y pupil region.

.05? gion is extract and eyelid.

center of the p rding to the colors can pro  PMI (Pixel wit OI and comp  10?10 regio is accumulate e 4 shows the r  BioID  of the ra is  on the .

(5)  ted as The  pupil, light  oduce  th the pared.

on is ed to result     T used Equa  H perfo 0.2, detec BioI Figu  B. G G  in th gaze verti  F dista acqu with dista to de  S calcu unne remo regio detec  T How hidd of c deter  T detec the s redu nodd detec desig  H base indic issue mod but a  To determine t d and the accu ation (6).

???? ?  Here, ????mea ormed with ac and 0.25. N  cted in the exa ID DB and th ure 5 shows the  Gaze Direction Gaze direction he real time. W e direction usi ical and horizo  First, to reduc ance between uired in the re h the largest v ance is compar etermine the a  Second, the d ulated with the ecessary regio oved region is on of the scl ction. Figure 6  The pictures ab wever, when th den and it beco change in gaz rmined when t  This study pr ct gaze directi system is bas  ucing calculat ding error sh ction rate of p gnatingD???	w However, sin ed on subjec cators, there w e, and so we  dal system tha also the open-c  the accuracy o uracy of pupil  ?a?	???? ? ?? ???  ans the error ccepted error r  Next,Cl, Cl? and act center poin he pupil cent e test result.

n n is detected by We propose a ing the exact p ontal sclera.

ce error in det pupils is m  eal time base variance in th red with the d  accuracy of cur  distance of ve e detected pup on is removed s changed to b era is calcula 6 shows the re  bove show ho he person is l omes difficult ze direction i there is no cha

IV. CO resents a syst ion in order to  sed on the rea tion time. Fo howed high a pupil center s was	at 0.25.

ce drowsines  ctive indicato will need to b  plan to use at uses not onl closing of mo  of the algorithm center detecti  ??? ?, ??? ? ??? ? ???  in the result, rate ranges un Cr, Cr?  denote nt of each eye ter detected i  y using the pu a system that d pupil center a  tecting the pup measured with e, while exclu he pupil dista detected distan rrent detection  ertical and ho pil at the cente d based on th black and the ated to detect esult of gaze d  ow the gaze dir looking down t to detect the is analyzed a ange in about  ONLUSION  em to estima o recognize a al time basis, or head pos accuracy on showed 94%  ss measureme ors due to l be more resea it as a compo ly head pose a uth and eyes.

m, BioID DB ion is tested, u  ??  and each test ntil 0.05, 0.1, e the pupil c e (left and righ n the experim  upil center dete detects the cu and distance o  pil center, ave hin the 20 fra uding the dist ance. The ave nce between p n.

orizontal scle er. To reduce e he skin color.

distance in b t the current  detection.

rection is dete nward, the pup em. The frequ and drowsine 200 frames.

ate head pose drowsy drive the focus wa  e estimation, average, and of accuracy w  ent is determ lack of obje arch regarding onent for a m and gaze dire  [5] is using  ????  t was 0.15,  center ht) in ment.

ected urrent of the  erage ames tance erage  pupils  era is error, The  bright gaze  ected.

pil is  uency ess is  e and er. As as on , the d the when  mined ective g this multi- ection  [1]  [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10]  Figure 6  T. A. Ranney, driver distractio Traffic Safety A D. DeMenthon a code,? Int. J. Co M. La Cascia, S under varying texturemapped 3 22, no. 4, pp. 32 R. Oyini Mboun State and Head Intelligent Trans O. Jesorsky, K.

using the Hausd Person Authenti Z. Zhou and X.

Recognit., vol. 3 M. Asadifard an detection using MultiConf. Eng.

D. F. Dinges an measure of ale Highway Admin R. Wang., G. Li driver fatigue Transportation Conference, pp.

Singular Inversi  6. Result of gaze  E. Mazzae, R. G on research: Past  Admin., East Liber and L. S. Davis, ? mput. Vis., vol. 1  S. Sclaro, and V.

illumination: An  3D models?, IEEE 2?336, Apr. 2000  na, S. G. Kong, an d Pose for Drive sportation System Kirchberg, and  dorff distance,? i fication, vol. 3, p Geng, ?Projectio  37, no. 5, pp. 1049 nd J. Shanbezade g face detection . Comput. Scienti nd R. Grace, ?PE ertness as assess n., Washington, D ie, B, Ting., & L.

or distraction Systems, Proce 314-319. IEEE, O on, I. FaceGen. w  direction detectio  Garrot, and M. J t, present, and f  erty, OH, USA, T ?Model-based ob 15, no. 1/2, pp. 12 . Athitsos, ?Fast,  An approach bas E Trans. Pattern 0.

nd M-G. Chun, ? er Alertness Mon ms, vol 14, no .3,  R. Frischholz, ? in Proc. Audio V pp. 90?95 , 2001 on functions for 9?1056, May 200 eh, ?Automatic a n and CDF an ists, vol. 1, pp. 13 ERCLOS: A val sed by psychom  DC, USA, Tech. R . Jun.. Monitorin  with one ca eedings. The 7t October 2004  www.facegen.com  on experiment  J. Goodman, ?N future,? Nat. Hi ech. Rep., 2000.

bject pose in 25 li 23?141, Jun. 199 , reliable head tr sed on registrati Anal. Mach. Inte  ?Visual Analysis nitoring?, IEEE pp 1462-1469, 20 ?Robust face det Video Based Bio  eye detection,? P 04.

adaptive center of alysis,? in Proc 30?133, 2010 id psychophysiol  motor vigilance,? Rep., 1998.

ng mouth movem amera. In Inte th International  m.

NHTSA ghway  ines of 5.

racking ion of  ell., vol.

of Eye Trans.

tection  ometric  Pattern  f pupil c. Int.

logical ? Fed.

