DETERMINING CO-LOCATION USING A SEQUENTIAL HYPOTHESIS TEST ON PATTERNS OF SILENCE

ABSTRACT  In everyday meetings, automatic association of co-located mobile devices would ease sharing of web-links, media, and other information. We propose a method that compares pat- terns of silence from device microphones to detect co-location of those devices. This method works with unsynchronized audio capture, requires only 100bps and preserves privacy.

We show how to formulate pattern matching in a sequential hypothesis framework so that changes in co-location sta- tus (when people leave or join a meeting) can be determined promptly, and how to compute the likelihood ratio in practice.

Using 16 hours of captured audio, we show that our approach can correctly determine device co-location with a low error rate of 0.05%, and can detect co-location changes 10 seconds faster than a similar decision rule based on a constant time window. Compared to a prior audio signature method, we achieve higher accuracy at 1/7 the bit rate.

Index Terms? sequential hypothesis test, voice activity detection, mobile device association.

1. INTRODUCTION AND PRIOR WORK  Sharing text, pictures and other digital information in meet- ings is part of our daily lives, yet the process is often te- dious, requiring typing or emailing long URLs or codes. If we can reliably and continously determine which devices are co-located and are thus part of the same meeting, we can im- plement easier and more intuitve methods of content sharing, for instance through user interfaces in which drag-and-drop device icons automatically appear.

Zhang and Trott [1] argue for effectively determining device co-location by comparing device audio signatures.

This is because people hearing the same conversation pro- vides a human-centric concept of a meeting, while physical location alone, as determined for instance using WiFi-based methods, can inadvertently associate nearby people who are in different conference rooms. Device audio signatures also allow us to include the devices of people attending a meeting remotely, via teleconference, which WiFi-based methods do not. Nevertheless, a practical audio signature needs to be low bit-rate, preserve privacy by not revealing the content of  a conversation, support accurate and prompt determination of co-location, and must be implementable in a distributed fashion.

The binary silence signature offers all the above advan- tages. It is obtained using a voice activity detector to classify each 10 ms block into 1 bit of silence (0) or voice (1). We send this low bit-rate (100 bps) silence signature to a server that compares signatures across devices and returns the iden- tities of the co-located devices. Preserving privacy, the silence signature does not reveal the conversation.

One alternative to matching silence signatures is to corre- late either the audio signals or the windowed energy of those signals. We find this works poorly since people are at differ- ent locations in the room. The resulting mixed signal has au- dio components from each person but with different loudness at each device. Our approach using silence signatures works better because it is robust to these loudness differences.

The most similar prior work [1], proposes an audio co- location signature based on quantized phase. There are two essential advantages of a silence signature over quantized phase. First, compression can destroy phase information [1], meaning phase-based methods cannot generally extend to remote participants. Second, phase is inherently sensitive to alignment errors when the frames used to compute phase are not aligned temporally across devices. We show in the results section that this can cause significant drop in performance, and that a silence signature is more accurate overall. Further- more, a quantized phase signature contains more information than a silence signature, requiring seven times higher bit rate.

Audio signatures have been extensively used in music search [2]. One key difference between co-location and mu- sic search is the access to a clean reference signal for music search [1]. In contrast, audio signatures to determine device co-location are all computed from noisy, distorted signals.

A sound-emitting method [3] uses sentences automati- cally generated from public keys and vocalized by a text-to- speech system to establish secure pairing between devices with the aid of humans for manual authentication. In con- trast, our method uses the unmodified acoustic environment and does not require explicit user interaction.

There are non-audio ways of associating devices [4] such as bump [5]. These methods generally do not extend to re-     mote participants of tele-conferences, because they require physical bumping together of the devices.

Our contribution in this paper is three-fold. First, we pro- pose use of a silence signature for determining co-location and evaluate its effectiveness. Second, we develop practical approximations of the joint pdf for the two silence signatures we compare. We use statistical models of the transitions be- tween speech and silence from the two signals to allow com- putation of the likelihood ratio for hypothesis testing. Third, we apply sequential hypothesis testing [6, 7] to adaptively choose the smallest possible window size to reduce decision latency when co-location status changes (when someone joins or leaves the meeting). We compare the performance of our method with a constant window method, and a related prior method [1].

2. FORMULATION  Given current silence signatures s(0), s(1) from two devices, we want to decide between these two hypothesis  ?0: s(0) and s(1) are signals from different locations  ?1: s(0) and s(1) are co-located signals  by testing observations x = [s(0) s(1)]T over a causal window ? . A straightforward approach uses constant ? independent of the content of s(0) and s(1). However, a large ? entails long decision latency when co-location state changes, whereas a small ? is susceptible to higher mis-classification. Instead, we seek a sequential solution where progressively larger obser- vation windows of ?1 ? ?2 ? ?3 ? . . . are tried in sequence until we are confident about our decision. In a sequential anal- ysis framework [6, 7], this is the same as progressively testing  ?  1? ? < f(x(?i)|?1) f(x(?i)|?0)  < 1? ? ?  (1)  until the middle likelihood ratio term falls below ?1?? when we declare ?0 is true, or rises above 1??? when we declare ?1 is true. The quantities ? and ? are the desired mis- classification rate when ?0 and ?1 are true, respectively.

Taking the logarithm of (1), and choosing ? = ?, we can simplify the decision rule as  ?0 if LLR(?i) < ??  ?1 if LLR(?i) > ?  where LLR is the log likelihood ratio, and ? = log 1??? is the decision threshold. We try ?i+1 if |LLR(?i)| ? ?.

2.1. Likelihood under different locations: f(x(?)|?0)  We can model durations of speech and silence in conversa- tion by exponential and shifted exponential (p(t) = e?(t?t0)  for t ? t0) distributions, respectively [8, 9], where t0 is the minium silence duration produced by a voice activity detec- tor, and is set to 0.1s in this paper. Assuming independence of individual silence and speech durations [9], we can express f(x(?i)|?0) as products of the probability of each silence and speech duration over time window ?i across both signals.

We compute the parameters for the exponential and shifted- exponential distributions empirically from s(0) and s(1) as the reciprocal of the average voice and silence durations, respectively.

2.2. Likelihood under co-location: f(x(?)|?1)  The pdf of joint observation x(?i), given by f(x(?i)) = f(s(0)(?i), s  (1)(?i)), can be written as  f(x(?i)) = f(s (1)(?i)|s(0)(?i)) ? f(s(0)(?i)), (2)  where we can readily compute f(s(0)(?i)) from the statis- tics described in Section 2.1. We approximate the conditional term f(s(1)|s(0)) as follows. We first find all begin times b(0)j and end times e(0)j of each silence period in s  (0). We then find  the corresponding (i.e., closest) begin times b(1)(b(0)j ) and end  times e(1)(e(0)j ) in s (1). We then approximate the conditional  probability as  f?(s(1)|s(0)) = ?  pb(b (0) j ? b  (1)(b (0) j ))pe(e  (0) j ? e  (1)(e (0) j ))  where pb(d) and pe(d) are probabilities that co-located signa- tures have silence begin and end times that deviate by offset d, respectively, and are determined a priori via training.

We derive pb and pe from 7 co-located recordings from various devices in an actual project meeting that lasted one hour. We collected the silence duration as well as the offsets for all silence begin and end times. We find that the statistics of pb and pe are similar and not worthy of separate account- ing. The distributions of offsets for silence periods shorter than 0.3 seconds and longer than 0.3 seconds are shown in the left and right plots of Fig. 1, respectively. Their similar shapes and symmetry suggest that it is not necessary to con- dition on silence duration, and it suffices to consider offset magnitude.

With these simplifications, the resulting empirical pdf of offset magnitude is shown in gray in Fig. 2. This is further  ?3 ?2 ?1 0 1 2 3  0.05  0.1  0.15  0.2  offset (s)  pd f     silence duration <= 0.3s  ?3 ?2 ?1 0 1 2 3  0.05  0.1  0.15  0.2  offset (s)  pd f     silence duration > 0.3s  Fig. 1. Offset distribution in begin and end times of silence period of different durations for co-located silence signatures.

0 0.1 0.2 0.3 0.4 0.5 0.6  0.1  0.2  0.3  0.4  offset (s)  pd f  Fig. 2. Offset magnitude in silence begin and end times for co-located silence signature, with empirical distribution in gray and model in black.

approximated by the black curve in Fig. 2 by using an ex- ponential distribution for offsets less than 0.3 seconds, and a uniform distribution for offsets between 0.3 to 3 seconds.

Offsets larger than 3 seconds are clipped to 3 seconds. Both pb(d) and pe(d) are computed by looking up |d| against the black curve.

With f?(s(1)|s(0)), we can compute f(x). Rather than di- rectly using (2), we instead use  f(x) =  [f(s(1)|s(0)) ? f(s(0)) + f(s(0)|s(1)) ? f(s(1))] (3)  to ensure symmetry in the arguments s(0)(?i) and s(1)(?i).

3. RESULTS  This section shows results 1) comparing accuracy of se- quential versus constant windows; 2) comparing accuracy of sequential versus the method in Zhang [1]; 3) comparing decision latency for the sequential versus constant window method, including the case of a remote participant in a tele- conference.

We first compare the accuracy of using an adaptive win- dow as determined by the sequential method versus that of a constant window and that of the quantized phase method [1].

Our results use 5 audio recordings of 1 hour each. Three recordings, A,B,C, are from a different work meeting than  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.9  0.91  0.92  0.93  0.94  0.95  0.96  0.97  0.98  0.99   False Positive Rate (FPR)  T ru  e Po  si tiv  e R  at e  (T PR  )      adaptive silence (?=10, ave win=4.3) adaptive silence (?=20, ave win=7.3) adaptive silence (?=30, ave win=10.8) constant silence (win=5) constant silence (win=8) constant silence (win=11) quantized phase ? aligned, 2s quantized phase ? 50ms misalign, 2s quantized phase ? 100ms misalign, 2s quantized phase ? 100ms misalign, 11s  Fig. 3. Receiver operating characteristics for various ap- proaches zoomed in for 0.9 ? TPR ? 1.

that used to derive statistics in Section 2, while recordings D and E are from other unrelated meetings. For adaptive win- dows, we employ the sequential hypothesis testing starting with an intial window of ?1 = 3 seconds, and increase in 1 second steps until the decision threshold ? is crossed. Co- location of A with B to E is evaluated every second. Fig. 3 shows the results expressed in receiver operating character- istics (ROC). With ? = 10, the average window size is 4.3 seconds, and we already achieve a true positive rate (TPR) of 0.97, with a false positive rate (FPR) of 0.28%. Using a constant window of 5 seconds results in significantly worse TPR of 0.95 and FPR of 2.1%. Moving to ? = 30, with an average window size of 10.8 seconds, the adaptive windows method achieves TPR of 0.997 and a perfect FPR. In contrast, using a constant window of size 11 seconds would result in worse TPR of 0.978 and FPR of 0.1%. Further testing with 16 hours of recordings including more diverse settings such as a noisy cafeteria has shown that we can achieve a TPR of 0.9995 (0.05% error) and a perfect FPR of 0. We do not trace a curve as we vary ? since the trade-off is not between TPR and FPR. Instead, larger ? corresponds to improvement in both TPR and FPR at the expense of a larger average win- dow size.

The quantized phase method [1] computes quantized phase for 0.4s frames that are overlapping by 0.2s. Co- location is determined by comparing quantized phase in 2-second windows. With frames overlapping by 0.2s, a worst-case misalignment between two devices that deter- mines frame boundary independently is 0.1s, with an average misalignment of 0.05s. From Fig. 3, we see the performance of quantized phase using the default 2-second windows lags that of our silence signature significantly, even with frame alignment. The performance gap increases drastically as frame misalignment increases. Nevertheless, the performance of quantized phase increases significantly when the window size increases to 11 seconds, achieving near perfect TPR and FPR with aligned frames. When alignment cannot be guar- anteed, we see from Fig. 3 that silence signature provides significantly superior TPR and FPR over quantized frames with 11s windows and 0.1s misalignment while consuming only 100 bits per second compared to 700 for quantized phase.

Figs 4 and 5 shows the time trace of computed log like- lihood for adaptive window with ? = 30 and constant win- dow of 11 seconds, respectively. We see that using adaptive windows is much more successful in separating the true pos- itives (black) from true negatives (gray) than using constant windows. The corresponding trace for quantized phase using 11 seconds window and 0.1s frame misalignment is shown in Fig. 6. Again, silence signature with adaptive window pro- vides superior separation.

We next illustrate the reduction in decision latency when co-location status changes. In this experiment, Ramin and Mary are in a conference room talking with Dan, who is re-     500 1000 1500 2000 2500 3000 3500  ?50    time (s)  lo g  lik el  ih oo  d ra  tio     A?B A?C A?D A?E  Fig. 4. Time trace of log likelihood ratio using adaptive win- dow with ? = 30.

500 1000 1500 2000 2500 3000  ?50    time (s)  lo g  lik el  ih oo  d ra  tio     A?B A?C A?D A?E  Fig. 5. Time trace of log likelihood ratio for constant window of 11s.

500 1000 1500 2000 2500 3000      time (s)  sc or  e     A?B A?C A?D A?E  Fig. 6. Time trace of score computed using quantized phase with misalignment of 0.1s using 11s window.

mote (attending by telephone). At time 105 seconds, Mary leaves the room and returns at time 310 seconds. The adap- tive window sizes are shown in Fig. 7 between Dan-Ramin and Mary-Ramin. Due to more distortion caused by the phone system, we need a larger average window size of 16 seconds between Dan-Ramin versus that of 11 seconds between Mary- Ramin.

Fig. 8 shows the log likelihood ratio evaluated every sec- ond using adaptive windows. Mary is determined to have left the room at time 116 seconds, re-entering at time 320 sec- onds, for a decision latency of 11 and 10 seconds, respec- tively. Fig. 9 shows the corresponding result employing a constant window size of 14 seconds ? the average window size of Fig. 8. With constant window size, the maximum like- lihood decision is to choose ?0 or ?1 depending on whether  50 100 150 200 250 300 350 400 450 500   time (s)  w in  do w  ( s)     Dan ? Ramin (mean=16.32)  50 100 150 200 250 300 350 400 450 500   time (s)  w in  do w  ( s)     Mary ? Ramin (mean=11.02)  Fig. 7. Window size from sequential hypothesis test.

50 100 150 200 250 300 350 400 450 500 ?100    time (s)  lo g  lik el  ih oo  d ra  tio      Dan ? Ramin  50 100 150 200 250 300 350 400 450 500 ?100    time (s)  lo g  lik el  ih oo  d ra  tio      Mary ? Ramin  Fig. 8. Log-likelihood ratio using adaptive window.

50 100 150 200 250 300 350 400 450 500 ?100    time (s)  lo g  lik el  ih oo  d ra  tio      Dan ? Ramin  50 100 150 200 250 300 350 400 450 500 ?100    time (s)  lo g  lik el  ih oo  d ra  tio      Mary ? Ramin  Fig. 9. Log-likelihood ratio using constant window of 14 sec- onds, the average window size of Fig. 8.

50 100 150 200 250 300 350 400 450 500 ?100    time (s)  lo g  lik el  ih oo  d ra  tio      Dan ? Ramin  50 100 150 200 250 300 350 400 450 500 ?100    time (s)  lo g  lik el  ih oo  d ra  tio      Mary ? Ramin  Fig. 10. Log-likelihood ratio using a constant window of 50s.

LLR is negative or positve, respectively. We see that the small window causes four mis-classifications between Dan- Ramin, at times 79, 231, 380, and 449 seconds. One remedy to reduce such mis-classification is to use a larger constant window. We have determned empirically that a window size of 50 seconds is necessary to prevent mis-classification be- tween Dan-Ramin. Corresponding results for a constant win- dow of 50 seconds are shown in Fig. 10. Nevertheless, the times in which Mary is determined to have left and re-entered the room are 127 and 340 seconds, respectively. This repre- sents an additional 11 and 20 seconds worth of delay com- pared to the use of adaptive windows.

4. CONCLUSIONS  In this paper, we propose the use of silence signatures for de- termining device co-location, describe how to formulate this in a sequential hypothesis testing framework, and show that we achieve superior accuracy compared to a non-sequential test and a prior method based on audio signatures [1].

5. REFERENCES  [1] B. Zhang and M.D. Trott, ?Reference-free audio match- ing for rendezvous,? in Proc. ICASSP, March 2010, pp.

3570?3573.

[2] V. Chandrasekhar, M. Sharifi, and D.A. Ross, ?Survey and evaluation of audio fingerprinting schemes for mo- bile query-by-example applications,? in Proc. 12th Inter- national Society for Music Information Retrieval Confer- ence (ISMIR), 2011.

[3] M.T. Goodrich, M. Sirivianos, J. Solis, C. Soriente, G. Tsudik, and E. Uzun, ?Using audio in secure device pairing,? International Journal of Security and Networks, vol. 4, no. 1, pp. 57?68, 2009.

[4] M. Chong and H. Gellersen, ?Classification of sponta- neous device association from a usability perspective,? in Proc. The 2nd Int. Workshop on Security and Pri- vacy in Spontaneous Interaction and Mobile Device Use (IWSSI/SPMU), March 2010.

[5] ?Bump,? http://bu.mp/company, November 2012.

[6] A. Wald, ?Sequential tests of statistical hypotheses,? Ann.

Math. Statist., vol. 16, pp. 117?186, 1945.

[7] G.B. Wetherill, Sequential methods in statistics, Chap- man and Hall, second edition, 1979.

[8] P. Brady, ?A model for generating on-off speech patterns in two-way conversation,? Bell System Technical Journal, vol. 48, pp. 2445?2472, 1969.

[9] ITU-T P.59, ?Artificial conversational speech,? 1993.

