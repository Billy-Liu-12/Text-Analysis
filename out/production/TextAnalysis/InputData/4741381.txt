Association Rule Tree Algorithm   Based On Binary Information Granules

Abstract   The classic association rules, Apriori algorithm and  Fptree algorithm, are briefly illustrated to figure out the weakness of those algorithms.Then we develop an association rule tree algorithm from the idea of binary information granules. The algorithm retrieves the association rules from the association rule tree by computing of binary information granules, of which a system is converted from a transaction database.

Finally, we discuss the practicality and feasibility of this approach with the actual instances and relevant reasoning of theorem.

Key words:Granular Degree, Association Rule Tree, Algorithm, Binary Information Granule  1. Introduction In 1993, Agrawal firstly addressed the issues of  association rules in the itemsets of customer?s trade database, and proposed the Apriori algorithm with respect to the frequent set, in which the cardinal principle is the methodology of frequent set reasoning [1].Although Apriori algorithm is regarded as the best in the efficiency of computing, the algorithm still has to produce great volume of candidate sets and scan database repeatedly. The FP_Growth algorithm [2] of association rules, proposed by Jiawei Han, manifests higher efficiency of data mining than Apriori algorithm,  whereas it still has to scan the secondary transaction database. the cost of secondary scanning of transaction database is subject to upsurge. Thus, it is an improvement of efficiency to decrement the scanning frequency of database.

Granular computing  seems to be taken as a view and methodology to discover the target world[3,4,5]. In order to study the target world, granular computing can separate the original rough information granules of huge objects into several subtle information granules of trivial objects, and can also reverse the process to synthesize numerous original subtle information granules of trivial objects into a few rough information set of huge object. Furthermore, conceptual field composes of heterogeneous spaces with diverse granular degree, in which the information granules are the primary elements of solution. We leverage a binary  data string as an information granule.

2. Primary Definitions Presume that an Information System S, S = (U, AT),  includes two element, a training sample set U and an attribute set AT.

Definition 1: Providing that attribute set AT in the information system S is itemset, the itemset with K attributes is referred to as K-Itemset. A kind of record in training sample S, referred as transaction T, is an attribute subset, T ? AT. There is unique identity to each transaction, TID. Assuming that A is a sub-itemset of T, A ? T, then transaction T includes A.

Definition 2: Decision rules is organized as the embed equation, A?B. A ?  AT?B ?  AT?AND A ?B=?.

Definition 3: Support degree of itemset {A, B} from association rule A?B is the probability of P(A ?B). Thus, the support number is the amount of A ?B.

Definition 4: Confidence degree of rule A?B .It is conditional probability P(B|A), in which there are itemset B with itemset A from the system S.

Definition 5: Frequent Itemset, is referred as the itemset with sufficient minimum support degree, min_sup.

Definition 6: Association Rules. Association rule is the rules to suffice for minimum support threshold value, min_sup, and minimum confidence threshold value, min_conf. Moreover, the rule A?B is referred as association rule.

Definition 7: Granulating approach of attribute.

In the system S, and B ?  AT, given that the value interval of B is VB={ 1, 0},then B can be decomposed into the binary information sub-granule by quotient set U/IND{B}[6?7] .

For example, according to Definition 7, we have B:{1?0?0?1?0?0?1?0?0?1?1?0}, can be decomposed into the binary information granule, 100100100110.or 011011011001.

Definition 8: Granular degree of binary information granule. Assuming the amount of digit 1 in a binary information granule of attribute P is the granular degree of this information granule, |P|, then,   DOI 10.1109/CW.2008.57     the support number of an itemset of this attribute P is equal to the granular degree |P|.

For example, the granular degree |a| of binary information granule a: 101000011001, is 5.

Definition 9: Association computing of binary information granule and association degree between binary information granules.

In the information system S, if C1, C2 ?Ck is binary information granule with k attributes, then C1 ?  C2 ? ? ? Ck is the association computing of granules, and its results leverage to produce information granules with smaller granular degree. The granular degree| C1 ?  C2 ?  ?? Ck | is support number of k itemset {C1, C2,?,Ck}, then | C1 ?  C2 ?  ?? Ck | / |U| is support degree of k itemset { C1, C2 ?Ck }, and |U| denotes the total of object set. (Note that ? indicates the binary Boolean multiplication computing) .

For example, binary information granule of attribute a, b, c:  a={101000011001},b={100101001101} ,c={110101000101},then association computing is described as following: a ?  b ?  c?100000000001, support number of itemset, composed by a? b? c, is 2, |1000000000001|?2, and support degree  is 1/6, |1000000000001| / | U|?1/6.

Definition 10: Confidence degree of rule A?B is equal to the ratio between granular degree of binary information granule A ? B and granular degree of binary information granule A, Confidence degree C = |A ? B| / |A|.

3. Association Rule Tree BR-T Algorithm Based On Binary System 3.1. Rules Definitions  Definition 11: BR-T composes of a root node with NULL identity, and various nodes of tree, and each node can extend to x sub-node of tree, (x= 0, 1, 2?).

As x is equal to 0, the node is referred as leaf node.

Given that BR-T tree represent a transaction database, the tree nodes can be indicated as an attribute tag of transaction database, named as information system, and combination of all nodes traversed over the path of root node NULL denotes an itemset of database.

Definition 12: Except root node, each node of BR-T consists of 3 basic fields, Ii.cell, Ii.count, Ii.pointer.

Ii.cell indicates the tag of attributes. Ii.count indicates the support number of itemset and hold in curly bracket next to the left side of Ii.cell and Ii.pointer indicates the pointer of father node.

Definition 13: Assuming a combination of all k nodes traversed over the path from the node to root node NULL corresponds to k attributes, the support degree of sequence with k attributes, indicated n*k  itemset, is same as the association degree of binary information granule with these k attributes, and its association number is equal to the granular degree of smaller information granule, which is produced from association computing of binary information granule with k attributes. Therefore, the k itemset of combination, in which includes all nodes traversed over the path from the node to root node NULL, has been defined.

Definition 14: Candidate frequent mode set denotes as CF.

3.2. Algorithm description  (1) Constructing BR-T tree The construction of the BR-T tree can be   described  as follows.

Step 1: According to Definition 7, we can convert  transaction database  into binary style. 1 or 0 corresponds to whether each attribute exist or not, and each attribute is converted into a binary data string.

Step 2: According to Definition 8, we can compute the granular degree of each attribute to retrieve the support number of each attribute. As long as the granular degree is less than the threshold value which has determined before, the attribute can be reduced.

(Note that the threshold can be determined by actual computing situation.)  Step 3: This step is composed by the following sub- steps.

Step 3.1: First, creating root node NULL of BR-T tree. Computing the granular degree of each attribute to acquire support number, and retrieve the frequent itemset of transaction database. According to the support number, each attribute and its granular degree is sequentially inserted into BR-T tree as the leaf nodes of root.

Step 3.2: Combining itemset(k itemset) of each leaf node and the attribute of contiguous leaf nodes with less support number, to construct a new itemset(k +1 itemset). The support number of new itemset can be acquired from the definition 9 association computing of binary information granule of additional attributes and k nodes from specific leaf node to root node. As the support number of  (k+1) itemset is much than the threshold value, the new attributes can be inserted into the leaf node of  k itemset node above as a new leaf node in next tier.

Step 3.3: Repeating the process in which records insert into the tree as Step 3.2, until there is no any itemset of each leaf node in each sub-tree can be inserted into the BR-T tree. Therefore, the transaction database has been converted into a BR-T tree, and each path from any leaf node to root node corresponds to one or a few records in the database, while the association information between items of records has     been retained to facilitate the consequent data mining work of association rule.

Step 4: Creating a sequence TL to retain the address information of each leaf node from left to right.

(2) Mining BR-T tree  Step 1: Retrieving the first element from the  TL to get the itemset, pointed by this element, of leaf node.

According to Definition 9 and Definition 10, support degree and confidence degree of itemset are computed.

As the support degree is larger than the threshold value, itemset will be sent to the frequent mode set CF, and the pointer of parent node will be retrieved. If there is no identical pointer record, the pointer of parent node will be appended to the TL, and the first element in the sequence will be thrown out.

Step 2: Retrieving the next element from the TL, and repeating the process above until TL is empty and all elements have been thrown out.

Step 3: Compare to minimum support degree, min_sup, and deleting the combination in the CF, in which value below the min_sup. Thus, the left items in the CF are frequent mode needed, and all candidate association rules can be constructed.

Step 4: Finally, according to Definition 10, computing the confidence degree of each candidate association rules, and filtering to acquire the association rules needed by specific minimum confidence degree, min_conf.

4. Implement of the Algorithm and  Experimental Result 4.1. Experiment Specification  The developmental environment of experiments specified as following.

Platform of development: Windows XP system;  Hardware requirements: at least P4 2.0G CPU, 512MB Memory, 80G IDE hard disk;  Running Platform: Window XP; Environment of Compilation: VC++ 2008; Database: SQL_SERVER 2000;  The experimental data are derived from the trade records of small scale supermarket in a university, in which the information of 11 typical clients chosen 7 products is involved. The detail of trade records retrieved from the transaction database is represented as following Table 1:  Table 1: Transaction Database D    The process of algorithm is illustrated as following.

According to Definition 7, the granular degree is converted by the situation whether each attribute of transaction in transaction database D exists or not, whereas binary information granules of each single attribute itemset in D have been acquired. The results are as following:   According to Definition 8, the granular degree of  binary information granule of each single attribute in D has been computed, and ordered by descent as the support number. Providing the threshold value is 3, then e?f  and g can be reduced, and the frequent single itemset is represented as the following Table 2.

Table 2: Frequent Single Itemset (Support Number>=3)   Following the Step 3.1 of BR-T tree algorithm, each  frequent single itemset can be inserted into BR-T tree Shown in Figure 1. (Support Number>=3)   Figure 1: Establishment of Tree?s First Tier  Following the Step 3.2 and 3.3 of BR-T tree algorithm, the itemsets are inserted into the nodes of each tier.

Moreover, BR-T tree of transaction database D can be created, and candidate frequent mode set can be acquired by the Step1 and Step2 in the BR-T tree algorithm as the following Figure 2.

Figure 2: BR-T Tree of Transaction Database D     Finally, decision rule can be determined by the specific minimum confidence degree of transaction database D, min_conf (40%). Computing candidate association rules from candidate frequent mode set can be deduced as the following: {b,d}:3?p(d|b)=1/2, p(b|d)=1.

{a,d}:3?p(d|a)=1/3, p(a|d)=1.

{c,b}:3?p(c|b)=1/2, p(b|c)=3/7.

{a,b}:5?p(b|a)=5/9, p(a|b)=5/6.

{a,c}:5?p(c|a)=5/9, p(a|c)=5/7..

{a,b,d}:3?  p((b,d)|a)=1/3,p((a,d)|b)=1/2,  p((a,b)|d)=1, p(a|(b,d))= 1, p(d|(a,b))= 3/5, p(b|(a,d))=1.

Deleting the rules, in which confidence degree is  below 40%, and we can acquire the final association rules as following Table 3.

Table 3: final association rules   4.2. Analysis of algorithm  Supposing that the amount of attributes of transaction database is k, the total of record or the itemset is n, then the worst-timing complexity degree is equal to O (2k-k), and best-timing complexity is equal to O(k2). Therefore, BR-T tree algorithm is irrelevant to the amount of itemsets, and relevant to the exponential degree of the amount of attributes. The BR-T tree algorithm is more suitable to process the transaction database with large amount of itemset and limited attributes than FP_Growth algorithm.

By comparison of BR-T algorithm and Apriori algorithm based on 10,000 records of market?s database, the statistic results of all experiments is represents as following Figure 3.

1 0.75 0.5 0.25 0.1  BR-T  Apriori   Figure 3: Comparative Chart of computing time between BR-T algorithm and Apriori algorithm  The principles and experiments manifest the efficiency of BR-T algorithm, which resolves the problem of multi- scan transaction database by traditional association rule algorithm.

5. Conclusion We develop a novel association rule tree algorithm,  and testify its efficiency by many experiments. There are still lots of work left about how to improve the computing capability of BR-T algorithm further. thus, the result of binary information granular association computing concerning attribute set can produce smaller information granule and granular degree to increase the computing speed, while the individual leaf nodes inserts. Finally, next target of our research will be aim to integrate other intelligent algorithm into it.

