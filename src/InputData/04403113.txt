Addressing Missing Attributes During Data Mining Using Frequent Itemsets And Rough Set Based Predictions

Abstract  In this paper, we present an improved method for predict- ing missing attribute values in data sets. We make use of fre- quent itemsets, generated from the association rules algo- rithm, displaying the correlations between different items in a set of transactions. In particular, we consider a database as a set of transactions and each data instance as an item- set. Then frequent itemsets can be used as a knowledge base to predict missing attribute values. Our approach in- tegrates the RSFit method based on rough sets theory that produces faster predictions by considering similarities of attribute value pairs, but only for those attributes contained in the core or reduct of the data set. Using empirical stud- ies on UCI and other real world data sets, we demonstrate a significant increase in prediction accuracy obtained from our new integrated approach, referred to as ItemRSFit.

1 Introduction  How to process data containing missing attribute val- ues is an important task in the data preprocessing stage for data mining applications. Missing attribute values com- monly exist in real-world data sets. They may come from the collecting process, or from redundant diagnosis tests, change of the experimental design, privacy concerns, un- known data, etc. Various approaches on how to cope with the missing attribute values have been proposed in the past years. For example, in [5], proposed methods include ?us- ing the most common attribute value?, ?ignoring examples with unknown attribute values? and ?assigning all possi- ble values of the attribute restricted to the given concept?.

In [4], vectors of all attributes are compared and a dis- tance function is used to determine the most similar at- tribute pairs, in order to assign the missing value. These approaches have difficulties, however. For example, select- ing the most frequent value may lead to an inconsistent data set; assigning ?unknown? to an attribute may not be rea- sonable in some instances (e.g. for gender, in a health data  set).

We are interested in predicting missing attribute values  in the data preprocessing stage of the knowledge discovery process. Motivated to develop a technique that can predict all the missing attribute values with high precision, we inte- grate two techniques into our solution.

The first is the association rule algorithm [1], which is well known in data mining for discovering item relation- ships from large transaction data sets. Prior to the associa- tion rule generation, frequent itemsets are generated based on the item-item relations from the large data set accord- ing to a certain support. Thus the frequent itemsets of a data set represent strong correlations between different items, and the itemsets represent probabilities for one or more items existing together in the current transaction. In our research, we are considering a certain data set as a set of transactions. The implications from frequent itemsets can be used to find which attribute value the missing attribute is strongly connected to and frequent itemsets can be used for predicting the missing values. We call this approach the ?itemset-approach? for prediction. Apparently, the greater the number of frequent itemsets used for the prediction, the more information from the data set itself is used for predic- tion; hence, the higher the accuracy that will be obtained.

However, generating frequent itemsets for a large data set is time-consuming. Lower support requirements lead to a larger number of frequent itemsets, but usually cost a sig- nificant amount of computation time. Although demanding higher support requires less computation time, the resulting itemsets show restricted item relationships and the appli- cable number of itemsets are fewer; therefore, not all the missing values can be predicted. In order to balance the tradeoff between computation time and the percentage of the applicable prediction, another approach must be taken into consideration.

The second element of our solution is rough sets the- ory, proposed in the 1980?s by Pawlak [9] and used for at- tribute selection, rule discovery and many knowledge dis- covery applications in the areas such as data mining, ma- chine learning and medical diagnoses. Core and reduct are   DOI 10.1109/GrC.2007.144    DOI 10.1109/GrC.2007.144     among the most important concepts in this theory. The data set is viewed as a decision table where each data instance consists of condition attributes and decision attributes. A reduct contains a subset of condition attributes that are suf- ficient enough to represent the whole data set. The inter- section of all the possible reducts is the core. Therefore the attributes contained in the reduct or core are more important and representative than the rest of the attributes. By exam- ining only attributes within the same core or reduct to find the similar attribute value pairs for the data instance con- taining the missing attribute values, we can assign the most relevant value for the missing attribute. Since this method only considers a subset of the data set, which is either the core or the reduct, the prediction is faster than those consid- ering the complete data set. This approach ?RSFit? (Rough Set Fit) [7] is an alternative approach for fast prediction and it can be used to predict missing attributes that cannot be predicted by the frequent itemsets.

We integrate the prediction based on frequent itemsets and the RSFit approach into a new approach ItemRSFit (Itemset and Rough Set Fit) to predict missing attribute val- ues. Experiments on UCI [3] data sets and a real world data set demonstrate our proposed approach on assigning miss- ing attribute values obtains a high accuracy.

2 RSFit Approach to Assign Missing Values  In this section, we introduce the RSFit approach [7] for predicting missing values. We first make definitions to be used in the following descriptions of the proposed ap- proaches. The input to our approach is a decision table T = (C, D) containing missing attribute values, where C = {c1, c2, . . . , cm} is the condition attribute set, and D = {d1, d2, . . . , dl} is the decision attribute set. U = {u1, u2, . . . , un} represent the set of data instances in T .

For each ui (1 ? i ? n), an attribute-value pair for this data instance is defined to be ui = (v1i, v2i, . . . , vmi, di), where v1i is the attribute value for condition attribute c1, v2i is the attribute value for condition attribute c2, ..., vmi is the attribute value for condition attribute cm. Table 1 shows an example of a decision table, which may be used to decide the mileage category of a car, based on various features that describe the car.

The primary idea is to search only for attribute-value pairs within the core or the reduct of the data set. For each missing attribute value, we let the attribute be the ?target at- tribute?(represented as ck in the following). We assume that missing attribute values only exist in the condition attributes not in the decision attributes.

First, we obtain the core of the data set T = (C, D) based on Hu?s core algorithm introduced in [6]. If the tar- get attribute ck does not belong to the core, we include ck in the core. In case there is no core for T , we con-  sider the reduct of T . ROSETTA software [8] is used for reduct generation. Secondly, a new decision table T ? = (C?, D) is created based on the previous step, where C? = {c?1, c?2, . . . , ck, . . . , c?m?}, 1 ? m? ? m, 1 ? k ? m?, and C? ? C, C? is either the core or the reduct of C, U ? = {u1, u2, . . . , un?}, 1 ? n? ? n. A distance func- tion, such as Euclidean distance or Manhattan distance is then applied to compute the similarities between different attribute-value pairs. The best match has the smallest differ- ence from the target attribute-value pair. When this is deter- mined, we assign the value from the best matched attribute- value pair to the target missing value. Thus, a complete data set without missing values is obtained.

We demonstrate the RSFit approach by an artificial car data set which appeared in [6] as shown in Table 1. One missing attribute value for ?compress? is randomly selected across the data set as shown by Table 2 a). First, the core  Table 1. Artificial Car Data Set U Make model cyl door displace compress power trans weight mileage 1 usa 6 2 medium high high auto medium medium 2 usa 6 4 medium medium medium manual medium medium 3 usa 4 2 small high medium auto medium medium 4 usa 4 2 medium medium medium manual medium medium 5 usa 4 2 medium medium high manual medium medium 6 usa 6 4 medium medium high auto medium medium 7 usa 4 2 medium medium high auto medium medium 8 usa 4 2 medium high high manual light high 9 japan 4 2 small high low manual light high 10 japan 4 2 medium medium medium manual medium high 11 japan 4 2 small high high manual medium high 12 japan 4 2 small medium low manual medium high 13 japan 4 2 small high medium manual medium high 14 usa 4 2 small high medium manual medium high  is obtained for this data set as ?Make model? and ?trans?.

Since the core attributes exist and the missing attribute ?compress? does not belong to the core, we add attribute ?compress? to the core set. The new data set containing the core attributes, target attribute ?compress? and the de- cision attribute are created and shown in Table 2 b). Then we will find the match for attribute ?compress? in u8. u14 has the smallest difference, which is 0, from u8, therefore, u14 is the best match. We assign ccompress14 to ccompress8, which is ?high? (correct prediction).

3 ItemRSFit Approach  The RSFit approach cannot provide a very high predic- tion precision, although it is computationally faster than the ?closest fit? approach of [4] (see [7]). This is because this approach does not fully consider the item-item rela- tionships inside the data set. Take Table 1 as an exam- ple. The attribute value pairs such as (make modelusa, displacemedium, compresshigh, powerhigh) frequently appear together. Such frequently appearing value pairs can- not be extracted by simply looking at the distance com-     Table 2. a) With One Missing Attribute Value, b) New Decision Table  U . . . compress . . .

1 . . . high . . .

2 . . . medium . . .

3 . . . high . . .

4 . . . medium . . .

5 . . . medium . . .

6 . . . medium . . .

7 . . . medium . . .

8 . . . ? . . .

9 . . . high . . .

10 . . . medium . . .

11 . . . high . . .

12 . . . medium . . .

13 . . . high . . .

14 . . . high . . .

U Make model compress trans mileage 1 usa high auto medium 2 usa medium manual medium 3 usa high auto medium 4 usa medium manual medium 5 usa medium manual medium 6 usa medium auto medium 7 usa medium auto medium 8 usa ? manual high 9 japan high manual high 10 japan medium manual high 11 japan high manual high 12 japan medium manual high 13 japan high manual high 14 usa high manual high  parisons between different data instances by the RSFit ap- proach. RSFit uses only a subset of the transaction set as a knowledge base to find the similar object for prediction.

The association rule algorithm was first introduced in [1], and it can be used to find associations among items from transactions. For example, in market basket analy- sis, by analyzing transaction records from the market (i.e.

lists of items purchased, for each customer), we could use association rule algorithms to discover different shopping behaviours such as, when customers buy bread, they will probably buy milk.

Frequent itemsets generation is the first step for associa- tion rule generation. Itemsets that frequently occur together in the transactions are generated. Rules based on these item- sets are further extracted to represent the associated rela- tions. Many contributions such as [2] on how to efficiently generate frequent itemsets and rules have been reported.

Our approach is to consider data in the form of a de- cision table as the transaction set for generating the fre- quent itemsets. Each attribute value is considered to be an item in the set of transactions. For example, in Ta- ble 1, (cyl4, door2), (make modelusa, displacemedium, compresshigh, powerhigh) and (powerhigh, transauto, weightmedium) frequently occur together, and are consid- ered as the frequent itemsets. We define the following con- cepts:  Definition Transaction. The set of transactions to the frequent itemsets generation is in a form of a decision ta- ble T=(C, D), where C = {c1, c2, . . . , cm} is the con- dition attribute set where m is the number of condition attributes, and D = {d1, d2, . . . , dl} is the decision at- tribute set where l is the number of decision attributes.

U = {u1, u2, . . . , un} represents the itemsets in T , where n is the number of transactions in T. Each transaction contains (m + l) items. Therefore each attribute value is considered an item in the transaction.

An association rule [1] is a rule of the form ? ? ?. ? and ? represent itemsets which do not share common items.

? contains items from the condition attribute set C, and ? contains items from the decision attribute set D.

Definition Support. A support of an itemset is the percentage of the number of transactions containing the itemset to the total number of transactions. Support can be represented as |???||T | . For example, in Table 1, it would be reasonable to extract an association rule where ?1 is powerhigh, transauto, weightmedium and ?1 is mileagemedium. The support for {?1, ?1} from Table 1 would be 21.4%.

Frequent itemsets generation in an association rule algo- rithm first counts the frequencies of each individual item among the whole transaction set. Then based on the 1- itemsets whose support are no less than the predefined min- imum support, frequent 2-itemsets are generated. Those itemsets with occurrence no less than the minimum sup- port are selected for frequent 3-itemsets generation. Fre- quent l-itemsets are generated based on the frequent (l?1)- itemsets. The process continues until no new frequent item- sets are found.

To predict missing attribute values, let T = (C, D) be the decision table that contains missing attribute values, where C = {c1, c2, . . . , ck, . . . , cm}, 1 ? k ? m, and U = {u1, u2, . . . , un}, 1 ? n. First, the data input to the association rule algorithm is prepared. Data instances with missing attribute values are all removed from T , and we call the new decision table T ??. T ?? does not contain any missing values. Secondly, frequent l-itemsets are gen- erated based on T ?? with a given minimum support. Let Itemsets = {S1, S2, . . . , Sg}, where Si (1 ? i ? g) is a frequent l-itemsets generated based on T = (C, D) accord- ing to a minimum support, Si = {vi1, vi2, . . . , vil}, l is the number of items contained in Si, and vij (1 ? j ? l) is an attribute value in T .

We use the frequent itemsets generated in the previous step as our knowledge base to find a match for the missing value. Let ui = (v1i, v2i, . . . , vki, . . . , di) (1 ? i ? n) be the data instance in T containing the missing attribute value vki (represented as vki =?) for attribute ck (1 ? k ? m).

We search from Itemsets for all the itemsets containing the missing attribute vk, and check which itemset among the itemsets can be applied to ui. We say a frequent itemset can be applied to this data instance if all the items in this itemset, except the missing attribute, have exactly the same attribute values as contained by the data instance that has the missing attribute value. If this itemset can be applied, we assign the attribute value contained in this itemset to the missing attribute. In case there are multiple matched attribute-value pairs for the missing attribute, one of the values is randomly selected to be assigned to the missing value.

Suppose ui and uj are two of the data instances in T     that contain missing attribute values. ui = (v1i = 1, v2i = 2, v3i = 4, v4i =?, v5i = 1), and uj = (v1j = 0, v2j = 2, v3j =?, v4j = 5, v5j = 0)). The itemsets for T gen- erated from the second step are S1 = {v1 = 1, v3 = 4}, S2 = {v2 = 1, v4 = 7, v5 = 0}, and S3 = {v2 = 2, v3 = 4, v4 = 6, v5 = 1}. For the missing value in ui, S1 can- not be used for predicting v2 because it does not contain the missing attribute v4. S2 cannot be used for assigning the missing value either, because of the different values of v2. Since all the items in S3 can be applied to ui, we as- sign v4 = 6. For the missing value in uj , none of the three itemsets can be applied to find a match. The missing value will be further processed by other missing attribute value processing approaches such as RSFit.

Missing attribute values from some data instances in the original data set can be predicted by frequent itemsets. We call these data instances Compatible Records.

Definition Compatible Record. A compatible record (CR) is a record whose missing attributes can be predicted by an itemset. More formally, a record r with p miss- ing attributes is a CR if there exists an itemset I such that |I ? r| ? p. There also exist data instances for which no possible match can be found to predict the missing values.

If a record is not CR, the RSFit method is applied to predict the rest of the missing attribute values. We call this inte- grated approach ItemRSFit. The details on the integrated approach is shown in the following Figure 1. Stage A il-  Stage B  Stage A  Stage C  Remove Missing  Attribute Values  T?? (No Missing  Attribute Values)  Original Data T  Frequent Itemset  Generation Itemset  Prediction  Frequent  Itemsets  Complete  Data Set  RSFit  ROSETTA  Reduct and Core  Generation  Figure 1. ItemRSFit Approach  lustrates the itemset approach, in which the frequent item- sets, as the knowledge base, are generated based on using the apriori association rule algorithm from complete data instances. The reduct and core are generated in stage B for the use of the RSFit approach. In stage C, the frequent item- sets are used to predict the missing attribute first, then the  RSFit approach is applied to the rest of the missing cases.

4 Evaluation  We use the following approach to perform the evaluation process. We consider complete data sets as the transaction data set T . For each data set, we randomly select a cer- tain number of missing values among the whole data set to produce n missing attribute values per data set. We then ap- ply both the RSFit approach and the ItemRSFit approach on predicting missing values, and compare the accuracy of the predictions from these two approaches. For n missing at- tribute values, the prediction accuracy is defined to be the percentage of values that were correctly predicted (where the correct values are the ones removed to create the miss- ing values for the experiment).

The ItemRSFit approach is implemented by Perl and the experiments are conducted on Sun Fire V880, four 900Mhz UltraSPARC III processors. We use apriori frequent item- sets generation [2] to generate frequent 5-itemsets. The core generation in the RSFit approach is implemented with Perl combining the SQL queries accessing MySQL (version 4.0.12). ROSETTA software [8] is used for reduct genera- tion.

4.1 Experiments on Geriatric Care Data  We first perform experiments on a geriatric care data set.

This data set is an actual data set from Dalhousie Univer- sity Faculty of Medicine to determine the survival status of a patient giving all the symptoms he or she shows. The data set contains 8, 547 patient records with 44 symptoms and their survival status. We use survival status as the decision attribute, and the 44 symptoms of a patient as condition at- tributes, which includes education level, the eyesight, live alone, cough, high blood pressure, heart problem, gender, the age of the patient at investigation and so on. There is no missing value in this data set. There are 12 inconsis- tent data entries in the medical data set. After removing these instances, the data contains 8, 535 records. 1 The core attributes for this data set are eartrouble, livealone, heart, highbloodpressure, eyetrouble, hearing, sex, health, educa- tionlevel, chest, housework, diabetes, dental, studyage.

ItemRSFit approach is the new integrated approach in- troduced in this paper. Table 3 lists the prediction accu- racy for both RSFit and ItemRSFit. We also list the number and the percentage of CR by only using frequent itemsets as knowledge for prediction. In this research, we experiment on geriatric care with 50 to 200 missing attribute values.

From Table 3 we can see, the smaller the support be- comes, the more itemsets are generated and the larger the  1Note that the core generation algorithm cannot return correct core at- tributes when the data set contains inconsistent data entries.

Table 3. Comparisons for Geriatric Care Data on Prediction Accuracy  Data Sets Average Accuracy(Percentage%) Missing RSFit Support # CR % CR Integrated Values ItemRSFit  50 64.00% 90% 11 22% 64.00% 80% 22 44% 68.00% 70% 26 52% 68.00% 60% 38 76% 72.00% 50% 41 82% 70.00% 40% 43 86% 72.00% 30% 43 86% 78.00% 20% 46 92% 90.00% 10% 46 92% 96.00%  100 69.00% 90% 26 26% 69.00% 80% 53 53% 74.00% 70% 58 58% 74.00% 60% 69 69% 77.00% 50% 80 80% 75.00% 40% 87 87% 76.00% 30% 87 87% 81.00% 20% 95 95% 87.00% 10% 95 95% 96.00%  150 73.33% 90% 43 29% 75.33% 80% 85 57% 79.33% 70% 94 63% 79.33% 60% 120 80% 80.00% 50% 133 89% 81.33% 40% 137 91% 82.00% 30% 137 91% 83.33% 20% 142 95% 89.33% 10% 142 95% 96.67%  200 73.50% 90% 39 20% 73.50% 80% 103 52% 77.00% 70% 118 59% 76.50% 60% 146 73% 75.50% 50% 169 84% 73.50% 40% 182 91% 79.00% 30% 182 91% 79.50% 20% 192 96% 88.50% 10% 194 96% 96.00%  number of compatible records from frequent itemset be- comes. ItemRSFit approach always maintains or improves on the prediction accuracy of RSFit.

Figure 2 shows the comparison for the number of CR by Itemsets prediction according to different support for dif- ferent number of missing values. Frequent itemsets with lower support value can provide a larger knowledge base to find predictions. We can also see from Figure 2 that using itemsets alone cannot predict all the missing values. For instance, when there are 50 missing values existing in the data set, given support = 10%, there are still 8% missing instances that cannot be predicted by the itemsets.

In order to show that ItemRSFit approach obtains better prediction accuracy than RSFit, we show the prediction ac- curacy comparisons on geriatric care data set with 150 miss- ing attribute values, as shown in Figure 3. We can see when support value is lower, the prediction accuracy of ItemRSFit is significantly higher than RSFit.

10 20 30 40 50 60 70 80 90            Support (%)  P er  ce nt  ag e  (% )  50 Missing Values 100 Missing Values 150 Missing Values 200 Missing Values  Figure 2. Comparisons on the Percentage of Compatible Records for Geriatric Care Data using Frequent Itemsets to Predict  10 20 30 40 50 60 70 80 90        Support (%) A  cc ur  ac y  (% )  RSFit ItemRSFit  Figure 3. Accuracy Comparisons for Geriatric Care Data with 150 Missing Attribute Values  10 20 30 40 50 60 70 80 90          Support (%)  A cc  ur ac  y (%  )  50 Missing Values 100 Missing Values 150 Missing Values 200 Missing Values  Figure 4. Accuracy Comparisons for Geriatric Care Data with Different Number of Missing Attribute Values  Figure 4 demonstrates the prediction accuracy com- parisons for different number of missing attribute values with different support for the geriatric care data set using ItemRSFit. The ItemRSFit approach obtains higher accu- racy when the support value is lower and this is unaffected by the number of missing attribute values in the data set.

Observations. From the experimental results on the geriatric care data set shown in Figures 2, 3, 4, we ob- serve the prediction accuracy for the ItemRSFit approach     increases while the support value decreases. The frequent Itemsets approach can provide a higher prediction by itself.

But this approach cannot predict all the missing values in the geriatric care data set. For the ItemRSFit approach on geriatric care data, the highest accuracy is obtained when support = 10%; the lowest accuracy is obtained when support = 90%. This can be explained as follows. ?Sup- port? is a measure to evaluate the occurrence of both the antecedents and the consequents of an association rule in the data set. The higher the support is, the more frequent this occurrence has to be and the less knowledge for predic- tion is obtained. When the support value is increased, fewer matched cases are found from the itemset approach; there- fore, more missing values have to be predicted by the RSFit approach. The lowest accuracy of the ItemRSFit approach is equal to the accuracy from the RSFit approach. The RS- Fit approach gives the baseline prediction accuracy for the ItemRSFit approach. For different numbers of missing at- tribute values, the frequent itemsets with the lowest support brings the highest prediction accuracy. The frequent item- sets alone as the knowledge base to predict the missing val- ues cannot fully find all the matches for the missing value for geriatric care data.

4.2 Experiments on UCI Data Sets  In the experiments on the UCI data sets [3] we study how the ItemRSFit approach can be applied for predictions on different types of data sets. We experiment on data sets with no missing attribute values. For data sets with continuous attributes, we discretize the attribute values to discrete data before generating frequent itemsets. For each data set, we randomly select a certain percentage of the total possible missing values (total number of condition attributes ? total number of data instances) as missing attribute values, and list the prediction accuracy comparisons for the ItemRSFit and RSFit approaches according to different support values.

Experimental results from UCI abalone, glass, iris and lymphography data sets [3] have demonstrated the high pre- diction characteristics of the proposed ItemRSFit approach on processing data with missing attribute values as shown in Figure 5-8. Frequent itemsets can be used as a knowledge base to predict missing attribute values.

4.3 Discussions and Related Work  Experimental results from both the real-world geriatric care data set and UCI data sets have demonstrated the high prediction characteristics of the proposed ItemRSFit ap- proach on processing data with missing attribute values.

The frequent itemsets can be used as a knowledge base to predict missing attribute values.

We find the approach introduced in [10] close to our  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1           Support (%)  A cc  ur ac  y (%  )  RSFit ItemRSFit  Figure 5. Abalone Data with 0.5% Missing At- tribute Values  1 2 3 4 5 6 7 8 9 10        Support (%)  A cc  ur ac  y (%  )  RSFit ItemRSFit  Figure 6. Glass Data with 5% Missing At- tribute Values  work. An approach of using association rules generations on completing missing values is discussed. However, our proposed ItemRSFit approach is quite different. First, only frequent 1-itemsets and 2-itemsets are used in [10] to find the possible values for the missing data, and data associa- tions with missing attributes on the consequent part are used for prediction. What percentage of the missing data can be predicted with the data association is not discussed. We use frequent 5-itemsets as the knowledge base for predic- tion as described in the experimental design part in Section 4. We explore the relations between different support and the percentage of the compatible records using frequent itemsets as shown in Figure 2. Second, in case there is no match from the data association, the missing value is as- signed by the most common value of the missing attribute in [10]. We use frequent itemsets as the knowledge base for prediction, and the RSFit approach for the non-compatible records where the itemset cannot be applied, which guar- antee that more important attributes are taken into consider- ations while predicting attributes. The proposed ItemRSFit approach provides predictions based on the data domain it- self, which better preserves the originality of the data sets and avoids noise. Third, in [10], data associations, which are similar to associated rules, are generated according to     1 2 3 4 5 6 7 8 9 10           Support (%)  A cc  ur ac  y (%  )  RSFit ItemRSFit  Figure 7. Iris Data with 5% Missing Attribute Values  10 20 30 40 50 60 70 80 90           Support (%)  A cc  ur ac  y (%  )  RSFit ItemRSFit  Figure 8. Lymphography Data with 5% Miss- ing Attribute Values  both support and confidence and used as a knowledge base for predictions. Our approach is more efficient because we do not need to generate associated rules based on both sup- port and confidence for prediction. Only support is used for frequent itemsets generation in the ItemRSFit approach.

5 Concluding Remarks and Future Work  We explore a new usage of association rule algorithms to predict missing attribute values, combined with rough sets theory. We first introduce a new approach RSFit to assign missing attribute values based on rough sets theory.

Comparing to the ?closest fit? approach [4], this approach significantly reduces the computation time and a compara- ble accuracy is achieved. We then introduce an integrated approach ItemRSFit based on both the association rule al- gorithm and rough sets theory to assign missing attribute values. The experimental results show the new approach obtains high prediction accuracy. It relies on its own data as a knowledge base and therefore the predicted values are not biased.

In our research, we adopt the strategies used by [11] on balancing the computational cost and the prediction accu-  racy. Lower support value can bring a higher prediction accuracy; however, frequent itemsets with lower support requires more time for computation than frequent itemsets with higher support. In the future, we are interested in ex- ploring a satisfactory balance between the support value and the prediction accuracy. Given the available computational cost and the affordable computation time, it is interesting to explore what percentage of the missing attributes can be effectively predicted, and what are the most effective at- tributes to be predicted. In case of a higher prediction cost, the idea of giving more important attributes higher priorities for predictions can be applied as an heuristic.

Acknowledgements  Thanks to NSERC for financial support and to Dr. A.

Mitniski of Dalhousie University for the Geriatric Care dataset.

