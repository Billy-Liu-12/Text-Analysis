Semi-Supervised Based Training Set Construction  For Outlier Detection

Abstract?Outliers are sparse and few. It?s costly to obtain a training set with enough outliers so that existing approaches to the problem of outlier detection seldom processed with supervised manner. However, given a training set with sufficient outliers, supervised outlier detection perform better than other methods. Traditional training set need to label each sample, but we can only label out the outliers and the other unlabeled ones can be directly marked as inliers to construct training set. In most cases, the number of samples we can label is limited and a large number of samples can be easily obtained without labeling.

Semi-Supervised learning methods have a nature advantage in utilizing information of little labeled samples and large unlabeled samples to predict unlabeled instances. Based on this idea, we propose a algorithm CRLC constructing training set combining semi-supervised outlier detection. Our experiments show that our algorithm achieves better performance compared to other methods with the same cost.

Keywords?Pattern Recognition; Outlier Detection; Semi- Supervised Learning;

I.  INTRODUCTION Outliers are patterns in data that do not conform to a well  defined notion of normal behavior. For example, outliers in credit transaction are very likely premeditated fraud affairs; outliers in network communication may represent that our computer is being hacked. Thus, outlier detection is a subject of great significance in reality. Outlier detection technique based on the extent of the use of labeled data is broadly divided into three categories: using data which has labeled instances for normal as well as outlier class(Supervised), using data which has labeled instances for only the normal class(Semi-Supervised), using data which has no labeled instances(Unsupervised). Thus, the unsupervised and semi- supervised ones has been widely used in outlier detection.

Although outlier detection algorithm based on supervised has better performance, its application has been greatly restricted due to the difficulty of obtain training set including sufficient outliers.

The difficulty of constructing the training set for supervised outlier detection lies in that outliers are rare. A moderate training set contains a handful of outliers. The classifier supervised outlier detection algorithm train based on  the training set would not have a good classification performance. In order to improve the performance of the classifier, the training set need to include enough outliers which will increase the cost of labeling. Liu et al [3] proposed the algorithm BCISO to reduce manual label cost. BCISO approach focuses on labeling the suspected instances which have a great possibility to be outliers, while the others are regarded as normal, then, a training set constructed by this way. Based on the idea of semi-supervised learning methods, we can construct a training set by labeling few samples and predicting other unlabeled samples. In this way, we can greatly reduce labeling cost.

Our approach starts from a large set of samples without any labeled instances, then iterates increase the size of labeling pool, and ultimately constructs a training set including sufficient outliers. In each iteration, we select the most representative samples to label based on the idea of active learning that we can not only reduce unnecessary labeling, but also improve the performance of classifier.

Experimental results show that the tactics that we use iterative semi-supervised method and representative sampling can reduce labeling costs and improve the performance of classifier simultaneously.

This paper is organized as follows: In Section 2,we review and analyze related work. In Section 3,we describe our approach in detail. Then, empirical evaluation of the proposed method is presented in Section 4 and the last section we give our conclusion.



II. RELATED WORK Outlier detection research has always been a hot direction  in the field of machine learning. Outlier detection can be broadly divided into three categories based on the extent of the use of labeled data: unsupervised [2], semi-supervised [4][5][6][7], supervised [17]. Unsupervised outlier detection has been widely used due to its characteristic that do not need any labeled instances and can be basically divided into: distance-based [12], density-based [8], cluster-based [10], information theory-based [14]. Specific analysis can be found in literature [1][9]. Carrasquilla [11] evaluated several outlier detection algorithms on large datasets, and his experimental  *Corresponding author   DOI 10.1109/CLOUDCOM-ASIA.2013.96    DOI 10.1109/CLOUDCOM-ASIA.2013.96       results show that supervised outlier detection performs generally superior to unsupervised ones. This is a direct proof of our thoughts: outlier detection performance can be improved by constructing a training set.

Previously, Liu [3] proposed a innovative algorithm BCISO to construct the training set. BCISO scores the unlabeled instances according to its possibility to be outliers through unsupervised outlier detection algorithm and then labels instances one by one from high score to low score until the condition that there are not enough outliers exist in the unlabeled instances being met. Thus, they only have to label the high score regions, namely outliers intensive areas, so as to achieve the purpose of reducing the cost of labeling. However, there are still many normal points in the high score regions which will be labeled due to the process of BCISO is disposable labeling. Therefore, the intention of our algorithm is to reduce this part normal points size, thereby further reduce the cost of manual labeling.

Semi-supervised learning which is a method combined supervised learning and unsupervised learning is one of key issues in the field of pattern recognition and machine learning.

Semi-supervised learning has very great practical significance to reduce labeling costs and improve learning performance. In recent years, semi-supervised outlier detection methods [5] are more and more studied. Semi-supervised outlier detection can predict unlabeled instances with the use of label information of few labeled instances and statistical information of large unlabeled instances. In most scenarios, it?s easy to obtain a small amount of labeled samples and a large number of unlabeled samples that makes semi-supervised outlier detection can be well applied in various applications.



III. CONSTRUCT BY REDUCING LABEL COST  A. Problem Analysis The key of supervised outlier detection lies in that whether  the training set constructed has enough outliers. Since outliers are rare, constructing a training set including enough outliers would take a very big label cost. For example, assume the outliers rate is 1%. In order to obtain 100 outlier instances, we need to label about 10000 instances theoretically. In most cases, manual labeling cost is very expensive and the number of instances we could afford to label is very small. Thus, this results in the training set contains almost no outliers that will further affect the performance of supervised outlier detection.

Suppose we have a pool of n unlabeled samples with M outliers. Then the outlier rate is /M n  .If we can label out the all M outliers, the remaining unlabeled sample can be directly labeled as normal without labeling. Then we construct our training set with the above labeled-by-hand instances and labeled-by-predict instances. So the cost for building such training set is the size of labeled-by-hand instances, not the size of training set traditional. Liu [3] et al proposed their algorithm BCISO based on this idea. They first use unsupervised outlier detection algorithm Isolation Forest[2] to score each unlabeled instance. The higher score stands for the higher possibility of an instance being an outlier. Then, BCISO sorts the unlabeled instances descend according to the  score. Finally, they label the sorted instances one by one from high score to low score until the condition that there are not sufficient outlier in the unlabeled instances pool decided by the given outlier rate interval met. When BCISO stops, the training set are constructed by joining the remaining unlabeled samples predicted as normal point.

As the entire process is completed once labeled, label information of each step only help to decide the stop condition of the algorithm and do nothing help to the selection of samples to label. Thus, the quality of BCISO is directly related to the score quality which decided by Isolation Forest. That means, the normal instances resided in the high score regions will still be labeled after the initial scoring. Obviously, we can reduce the total labeling cost by reducing this part size of normal instances. Therefore, if we can use the information of labeled sample of each step to rearrange the remaining unlabeled instances making those normal instances contained in the high score zone scores lower and those outlier instances contained in the lower score zone scores higher, we can achieve the proposal to reduce the total labeling cost. For example, suppose that there are 10 normal instances contained in the first 100 high score instances after initial scoring, BCISO will label the 10 instances whereas the method, which uses information of labeled samples, perhaps label 5 instances average due to it lowers the scores of normal instances reside in the high score area.

B. Using information of labeled instances This section we will detail our algorithm how to use  information of labeled instances to achieve our proposal of reducing the labeling cost. Notice that the size of labeled instances is small and the size of unlabeled instances is large.

In order to reduce labeling cost,we need to considerate the information of labeled instances to influence the score or forecast classification of unlabeled instances. To achieve this, we use semi-supervised outlier detection SSOD.

In this paper, we use L  represent the pool of labeled instances and U  represent the pool of unlabeled instances. Let  1 2{ , ,... }, m  n iX x x x x R= ?  be a set of data points drawn from a specified domain. Let the first l  points be labeled  1{ ,... }lL x x= and 1{ ,..., }l nU x x+= be the other unlabeled points,  1 2{ , ,..., }lF u u u=  be a indicator vector of L  where 1iu =  if the i th?  point is a normal point and 0iu =  otherwise. We wish to predict the labels of these points in U . For this reason, we assume normal points form c  clusters and outliers do not belong to any clusters. Our target is to find a n c?  matrix  { |1 , 1 }n c ijT t i n j c? = ? ? ? ? , where 1ijt =  if ix  is contained in the cluster jc . We can get the matrix n cT ?  through solve a optimization problem that minimize the equation defined next:   1 1  1 2 1 1 1 1  ( , )  ( ) | |  n c  ij j i i j  n c l c  ij i ij i j i j  Q t d c x  r n t r u t  = =  = = = =  = ? +  ? + ?  ??  ?? ? ? (1)          . {0,1}  , 1  ij  c  ij j  st t  and for each i t =  ?  =?   where jc  is the center of j th?  cluster; ( , )j id c x  is the standard Euclidean distance between jc  and ix ; 1r , 2r are two weighting parameters that need experiments to determine. As for the objective function Q , the first term is directly inherited from the K-Means clustering objective function, which represents the sum squared error. But in our method, only normal points are partitioned into clusters, so outliers do not contribute to this term. We note that if only considering minimizing this term, it will classify every point as an outlier.

Therefore the second term introduced is to constrain the number of outliers not to be too large. The third term maintains consistency by mislabeled points. To make these three terms compete with each other, we incorporate two weighing parameters 1r  and 2r .Once the matrix n cT ?  obtained, we can predict ix  as an outlier if it satisfies the following function:    c  ij j  t =  =?  (2)  Our algorithm Constructing By Reducing Labeling Cost(CRLC) starts from the scene where L  is empty. We run SSOD to predict instances in U  with L . Suppose there have OS  instances predicted as outliers in U , then we choose k samples from OS  for labeling. After labeling, we add the k samples just labeled into L  and simultaneously remove them from U . Iterating the process until SSOD predicts all the instances in U  as normal points or the limit labeling cost user specified has reached. Thus, it will create a new k  samples with label at each iteration. When the k samples added to the L , the cluster centers are updated , making the cluster centers closer to the real centers. This process will also make outliers of U  becoming increasing explicit which equivalent increases the score of outliers in U . This will ultimately reduce labeling cost as it?s described before. Clearly, smaller k  means more iteration times and more computation, however, the performance for training is better.

Based on active learning [13], we should choose k representative samples from OS  at each iteration.

Representative we called is the information content metrics of current samples to be labeled in U . If the current sample can be better represent the remaining unlabeled samples, then we say that the sample has a high representative, namely, information content is higher. Representative measure the information density of current sample in U . Under the same labeling cost, one can improve the performance of learning to the utmost extent by labeling the representative samples. In our approach CRLC, we will cluster OS  samples to obtain k cluster centers, then we choose k  samples from each cluster to  label. In each cluster the instance, which is closest to the cluster center and therefore have more representative than the others, will be chosen.

The pseudo code of SSOD and CRLC can be found in Algorithm 1 and 2.

Algorithm 1 : ( , )SSOD L U  INPUT:   L -labeled instances  U - unlabeled instances  OUTPUT: Y -predict class of unlabeled instances /*Supposed Normal Instance can form m clusters*/ 1.   initialize m centers as 1 2{ , ,..., }mc c c c= 2.    While(T null== ||T has changed ) 3.         min ,  T T Q fixed c=  4.         min , c  c Q fixed T=  5.    End While 6.    For(each ix  in U ) 7.           iy =normal  8.            If(  m  ij j  T =  =? ) 9.                 iy  =outlier 10.          End If 11.   End For

IV. EMPIRICAL EVALUATION We empirically evaluate the effectiveness of the proposed  constructing training set for supervised outlier detection methodology, using a number of publicly available data sets.

For each of the data sets, we run our experiments 50 times.

The experiments aim to show the advantage of our proposed CRLC over BCISO. All experiments are on the assumption that the labeled points of training data is very reliable and the labeled points cannot to be misclassified. Therefore, we set parameter 2r  of SSOD the maximum integer according to section  B.

A. Experimental Setup Four datasets our experiments used all are downloaded  from UCI repository1.The information about these datasets can be found in Table I. Http and Smtp datasets are subsets data picked from KDD Cup 1999 data, which is widely used in the field of outlier detection. These subsets are also used by [2] and [3]. The original KDD Cup 1999 training data contain 41 attributes, however, they are reduced to 4 attributes(service, duration, src_bytes, dst_bytes) as these attributes are regarded as the most basic attributes. Using the ?service? attribute, the data is divided into five datasets(http, smtp, ftp, ftp_data, others) and we choose the largest two subsets Http and Smtp.

The outlier rate are 0.4% for Http and 1.2% for Smtp. As the size of Http and Covtype dataset is too large than the others,       we random sampling 1/10 samples from Http and 1/4 samples from Covtype as the final dataset for our experiment.

Algorithm 2 : ( , )CRLC X MAXCOST  INPUT:      X -unlabeled instance pool  MAXCOST -the maximum cost user specified OUTPUT: ( , )X Y - labeled training set 1.  L null= 2.  U X= 3.  cos 0t = 4.  While( cos t MAXCOST< ) 5.       ( , )uY SSOD L U= 6.        If(all in uY ==0) 7.              Break 8.        End if 9.       ( 1)uOS U Y= == 10.       If( ( )Num OS k< ) 11.           sample = OS ; 12.       Else 13.           sample = Re ( , )presentativeSampling OS k 14.       End if 15.      ( )Label sample 16.       L L sample= + 17.       U U sample= + 18.       cos cos ( )t t Num sample= + 19.  End While 20.  Y=YL+SSOD(L,U);    We run 50 random experiments on each data set.In each random run,each data set is divided into three folds randomly.

One third is used as unlabeled instances for algorithms to construct the training set. The remaining two thirds are used as testing data, which measures the performance of classification after training.

BCISO needs outlier rate interval [ , ]l ub b  which specified by user as the input parameters and CRLC needs two weighting parameters. These parameters vary with different datasets. In our experiment, we guarantee the true outlier rate included in the interval and the range of interval has the moderate size. We set   the maximum integer as described above. As for ,we first set a group of values and run single SSOD on testing data. Then we set the best value in the group which achieves the best performance of SSOD. Specific parameters with which we experiments for each data set can be found in Table . In each data set, we will conduct a comparative experiment under a group of labeling cost. The number of true outliers in each date set will be included in the interval of group.

TABLE I.  INFORMATION OF DATASETS  datasets number of instances  dimension outliers class  HTTP(KDD CUP 99)  623091 3 attack(0.4%)  SMTP(KDD CUP 99)  96554 3 attack(1.2%)  Covtype 286048 10 class4(0.96%) Shuttle 49097 9 classes2,3,5,6,7(6.1%)    TABLE II.  PARAMETERS FOR DATASETS  datasets lb  ub  1r  2r  HTTP 0.04 0.08 60 MAX_INT SMTP 0.005 0.01 15000 MAX_INT  Covtype 0.01 0.04 1200 MAX_INT Shuttle 0.005 0.01 5500 MAX_INT    B. Detection Performance After constructing the training set, we need to test the  performance of training set. The training set is used to train a classifier of supervised outlier detection, so the classification performance of the classifier on testing data can report the performance of training set. Remind that the training set contains a small noise. Here, we select Random Forest [16] as our supervised outlier detection method since Random Forest allows a certain degree noise present in the training set. As supervised outlier detection belongs to the two-class classification problem and one class is much larger than the other one that resembles imbalanced classification. Therefore, we choose BRF(Balanced Random Forest) [15] instead of Random Forest with which our experiments.

(a)Shuttle                                          (b)Smtp   (c)Covtype                                        (d)Http  Fig. 1. Detection performance AUC (y-axis) using different algorithms over various dataset  with the same cost (x-axis).

We evaluated the detection performances of all algorithms with Area Under Curve(AUC). AUC is widely used in evaluating performance of classifiers. The value of AUC is in the range of [0,1]. AUC value closer to 1 means the better classification performance and further means the better training set .

1http://archive.ics.uci.edu/ml/       We test three algorithms on each data set: BCISO, CRLC, CRLC(Random). CRLC(Random) means that when we select the k  samples from OS , we use random sampling instead of representative sampling as CRLC does. Experiments results can be found in Fig 1.We can see that CRLC outperform than BCISO in each data set .It proves that our method achieves better performance than BCISO when the labeling cost is small. And we also can see that representative sampling proves to be better than random sampling in general.

C. Experimental Analysis From the experiment results, we find that CRLC perfoms  significantly better than BCISO in Shuttle dataset while slight better in other datasets. Experimental datasets except shuttle has a very low outlier rate that BCISO only requires a few cost can label out all outliers while the cost CRLC reduces is limited and thus not obvious.

In addition, we observe that AUC value of CRLC is higher than BCISO when cost is zero. When cost is zero,the training set constructed by BCISO is all labeled as normal point. AUC value of the classifier trained with this training set theoretically almost equals the normal rate of dataset because it classifies each instance in testing data as normal instance.

But when cost is zero, AUC value of CRLC is higher than BCISO because the training set constructed by CRLC have some outlier instances labeled out due to the utility of SSOD.

Moreover, we observe that CRLC converges to a value when cost is high. We conduct a group of other experiments to explain this. We find that the converge value equals the AUC value we got when we labeled all instances in the training set , use this training set to train classifier, and test the classifier on testing data afterwards. This implicit proves that our approach indeed reduce the cost while maintaining the same performance.



V. CONCLUSION Liu et al ?s algorithms from [3] represents an innovative  point for constructing training set for supervised outlier detection in order to improve performance of outlier detection.

By combining semi-supervised outlier detection and representative sampling, we propose an algorithm CRLC which aims to reduce the labeling cost compared to BCISO.

Experimental results show that the proposed algorithm really achieves the target. Under the same cost, CRLC indeed perform better than BCISO.

