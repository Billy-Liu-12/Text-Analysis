Comparison and Analysis of algorithms for association rules

Abstract?Several algorithms for association rules are discussed which are AIS algorithm, SETM algorithm, Apriori algorithm etc. Their strengths and weaknesses are investigated.

Their performance are compared and analyzed. Among all the algorithms, the most inefficient one is SETM algorithm but it is the most convenient one to combine DBMS.

Keywords-data mining; association rules; itemset; support

I.  INTRODUCTION Since the issue of association rules mining was first  introduced in 1993 [1], it has attracted a great deal attention and it is one of the most important methods in data mining.

Database mining is an important new application area for databases, combining commercial interest with intriguing research questions.

Many algorithms have been discussed in the literature for discovering association rules. AIS [1] is the first algorithm for mining association rules between items. An algorithm called SETM [2] is proposed to solve the issue using relational operations. Apriori [3] is a popular algorithm in dealing association rules and there are two extensions of the basic Apriori called AprioriTID and AprioriHybrid. These algorithms achieve great improvement over the previous algorithms. DIC [4] algorithm is a further variation of Apriori which softens the strict separation between counting and generating candidates. The Partition-Algorithm [5] is an Apriori-like algorithm which use set intersections to determine support values. FP-growth [6] is an efficient FP- tree-based mining method to derive the support values of all frequent itemsets.

Let I be a set of binary attributes, called items and D be a database of transactions where each transaction T D is a set of items such that T is a subset of I. A transaction also has an association identifier called Tid. A set of items is called itemset and the number of items in an itemset is called the length of an itemset. An itemset with length k is called k- itemset. An association rule is an expression X=>Y, where X and Y are itemsets and X Y= . X=>Y expresses that whenever a transaction T contains X than T probably contains Y also. X is called the antecedent and Y is called the consequent of the rule. Supports for the candidate itemsets are determined as follows. For each transaction, the set of all candidate itemsets that are contained in that transaction are identified. The counts for these itemsets are then incremented by one. A transaction T support an itemset  X if X is a subset of T. The support of a rule X=>Y is defined as support(X=>Y)=support(X Y). The problem of mining association rules is to generate all association rules that have support and confidence greater than user-specified minimum support and minimum confidence.

In this paper we discussed the algorithms aspects of association rules. The algorithms dealing the association rules are analyzed and compared.



II. ALGORITHMS FOR ASSOCIATION RULES There are many algorithms for mining association rules  such as AIS, SETM, Apriori, AprioriTid, AprioriHybrid, DIC, Partition and FP-growth algorithms.

A. AIS algorithm The AIS algorithm is targeted at discovering qualitative  rules and generates all significant association rules between items in the database. The candidate itemsets are generated on the fly during the pass over the database. For every transaction, candidate itemsets are generated by extending the large itemsets from previous pass with the items in the transaction such that the new itemsets are contained in that transaction.

The AIS algorithm is composed with two sub problems: First finding all the large itemsets and second generating from each large itemset, rules that use items from the large itemset. The simple solution to the first sub problem is to form all itemsets and obtain their support in one pass over the data. However, this solution is computationally infeasible as the possible itemsets can easily be too much to compute.

The AIS algorithm uses a carefully tuned estimation procedure to determine what itemsets should be measured in a pass. This procedure strikes a balance between the number of passes over the data and the number of itemsets that are measured in a pass. It uses pruning techniques to avoid measuring certain itemsets, while guaranteeing completeness.

These are the itemsets that the algorithm can prove will not turn out to be large. There are two such pruning techniques which are called the "remaining tuple optimization" and "pruning function optimization". These techniques can prune out itemsets as soon as they are generated. Having obtained the large itemsets and their transactional support count, the solution to the second sub problem is rather straightforward.

2009 First International Workshop on Database Technology and Applications  DOI 10.1109/DBTA.2009.42   2009 First International Workshop on Database Technology and Applications  DOI 10.1109/DBTA.2009.42     B. SETM algorithm The SETM algorithm was motivated by the desire to use  SQL to compute large itemsets. Like AIS, the SETM algorithm also generates the candidate itemsets on the fly during the database scanning. It generates and counts the candidate itemsets as same as the AIS algorithm generates.

But SETM separates candidate generation from counting by using the standard SQL join operation for candidate generation. It saves a copy of the candidate itemset together with the TID (transaction identifier) of the generating transaction in a sequential structure. At the end of the scanning, the support count of candidate itemsets is determined by sorting and aggregating this se sequential structure.

Remembering the TIDs of the generating transactions with the candidate itemsets, SETM avoids the subset operation and uses this information to determine the large itemsets. If the database is ordered by TID, SETM can easily find the large itemsets in the next scanning using the relational merge-join operation.

The SETM algorithms can be expressed as SQL queries and uses only simple database primitives, such as, sorting and merge-scan join. SETM is simple, fast and stable over the range of parameter values. The SETM algorithms shows that at least some aspects of data mining can be carried out by using general query languages such as SQL, rather than by developing specialized black-box algorithms. The set- oriented nature of SETM facilitates the development of extensions.

C. Apriori, AprioriTid and AprioriHybrid algorithm The Apriori algorithm generates the candidate itemsets  counted in a pass by using only the large itemsets in the previous pass, not considering the transactions in the database. The first pass of the algorithm simply counts item to determine the large 1-itemsets. A subsequent pass called pass k has two steps. First, the large itemsets Lk-1 found in the (k-1)th pass are used to generate the candidate itemsets Ck. Next, the database is scanned and the support of candidate in Ck is counted. In Apriori, the candidate itemsets are compared with the transactions to determine if they are contained in the transaction. A hash tree structure is used to restrict the set of candidate itemsets compared so that subset testing is optimized. Bitmaps are used in place of transactions to make the testing fast.

The AprioriTid algorithm is an extension of basic Apriori algorithm. Apriori and AprioriTid differ based on the data structures used for generating the supports for candidate itemsets. In AprioriTid after every pass, an encoding of all the large itemsets contained in a transaction is used in place of the transaction. In the next pass, candidate itemsets are tested for inclusion in a transaction by checking whether the large itemsets used to generate the candidate itemset are contained in the encoding of the transaction. In Apriori, the subset testing is performed for every transaction in each pass.

However, in AprioriTid, if a transaction does not contain any large itemsets in the current pass, that transaction is not considered in subsequent passes. Consequently, in later  passes, the size of the encoding can be much smaller than the actual database.

Apriori and AprioriTid algorithm is combined to a hybrid algorithm called AprioriHybrid which uses Apriori for initial passes and switches to AprioriTid for later passes.

D. DIC algorithm A further variation of Apriori algorithm is DIC algorithm  which softens the strict separation between counting and generating candidates. Once a candidate reaches minimum support, the DIC algorithm begins to generate additional candidates based on it. Not like the hash tree, a prefix tree is employed. Each node including leaf node and inner node of the prefix tree is assigned to exactly one candidate respectively frequent itemset. In this way, each node which the itemset associated with is contained in the transaction. In addition, interlocking support determination and candidate generation decreases the number of database scans.

E. Partition algorithm The Partition Algorithm is fundamentally different from  all the previous algorithms in that it reads the database at most two times to generate all significant association rules.

The idea of Partition algorithm is as follows. Suppose we are given a small set, of potentially large itemsets. Then the support for them can be tested in one scan of the database and the actual large itemsets can be discovered. Clearly, this approach will work only if the given set contains all actual large itemsets. Partition algorithm accomplishes this in two scans of the database. In one scan it generates a set of all potentially large itemsets by scanning the database once.

This set is a superset of all large itemsets. During the second scan, counters for each of these itemsets are set up and their actual support is measured in one scan of the database. The algorithm executes in two phases. In the first phase, the Partition algorithm logically divides the database into a number of non-overlapping partitions. The partitions are considered one at a time and all large itemsets for that partition are generated. At the end of phase I, these large itemsets are merged to generate a set of all potential large itemsets. In phase II, the actual support for these itemsets are generated and t,he large itemsets are identified. The partition sizes are chosen such that each partition can be accommodated in the main memory so that the partitions are read only once in each phase.

F. FP-growth algorithm The FP-growth algorithm is based on a novel frequent  pattern tree (FP-tree) structure, which is an extended prefix- tree structure for storing compressed, crucial information about frequent patterns. It is an efficient FP-tree-based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth. Efficiency of mining is achieved with three techniques: First, a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans. Second, FP-tree-based mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets. Third, a     partitioning-based divide-and-conquer method is used to dramatically reduce the search space.



III. COMPARISON AND ANALYZE OF THE ALGORITHMS When the minimum support becomes smaller, the time  for implementation of all the algorithms becomes longer, because the candidate itemsets increase. Compared with other algorithms, time for implementation of the SETM algorithm appears to be too long. But SETM algorithms can be expressed as SQL queries and uses only simple database primitives and it is the most convenient one to combine DBMS.

In both synthetic and real-life data, the Apriori, AprioriTid and AprioriHybrid algorithm always outperform AIS and SETM.

Partition spends most of its time with determining the support values of the 2- and 3-candidates whereas Apriori is efficiently handling such small itemsets. In contrast for itemsets with size large than 4 the additional effort caused for Partition is to be neglected whereas this dose not hold for Apriori.

The advantage of DIC is that it reduces the number of database scans.

FP-growth algorithm is efficient and scalable for mining both long and short frequent patterns. It is faster than the Apriori algorithm.



IV. CONCLUSION In this paper, the algorithms dealing the association rules  are analyzed and compared.

