A Fuzzy Associative Classification System with Genetic Rule Selection for High-Dimensional

Abstract?The learning of Fuzzy Rule-Based Classification Systems for High-Dimensional problems suffers from exponential growth of the fuzzy rule search space when the number of patterns and/or variables becomes high.

In this work, we propose a fuzzy association rule-based classi- fication method with genetic rule selection for high-dimensional problems to obtain an accurate and compact fuzzy rule-based classifier with low computational cost. The results obtained from the comparison with other two genetic fuzzy systems over nine real-world datasets with different characteristics show the effectiveness of the proposed approach.



I. INTRODUCTION  There are many real applications in which Fuzzy Rule- Based Classification Systems (FRBCSs) [1] have been em- ployed. However, in most of them, the available data consists of a high number of patterns and/or variables. In this situation, the learning of FRBCSs suffers from exponential growth of the fuzzy rule search. This growth makes the learning process more difficult and, in most cases, leads to problems of scalability and/or complexity [2].

Association discovery is one of the most common Data Mining techniques used to extract interesting knowledge from large datasets [3]. Many efforts have been made to use its advantages for classification under the name of associative classification [4], [5], [6].

A typical associative classification system is constructed in two stages:  1) Discovering the association rules inherent in the database.

2) Selecting a small set of relevant association rules to construct a classifier.

In order to enhance the interpretability of the obtained classification rules and to avoid unnatural boundaries in the partitioning of the attributes, different studies have been presented to obtain classification systems based on fuzzy association rules [7], [8].

Supported by the Spanish Ministry of Education and Science under grant no. TIN2008-06681-C06-01.

In this paper, we present a Fuzzy Association Rule-based Classification method for High-Dimensional problems (FARC- HD) with genetic rule selection to obtain an accurate and compact fuzzy rule-based classifier with a low computational cost. This method is based on three stages:  1) Fuzzy association rule extraction for classification: A search tree is employed to list all possible frequent fuzzy itemsets and generate fuzzy association rules for classification.

2) Candidate rule prescreening: We consider the use of subgroup discovery to select the most interesting rules by means of a pattern weighting scheme [9].

3) Genetic fuzzy rule selection: We consider the use of Genetic Algorithms (GAs) [10], [11] to select a compact set of fuzzy association rules with high classification accuracy.

In order to assess the performance of the proposed approach, we have used nine real-world datasets with a number of vari- ables ranging from 19 to 60 and a number of patterns ranging from 208 to 7400. We have shown the results obtained from the comparison with other two genetic fuzzy systems (GFSs) and we have made use of some non-parametric statistical tests for multiple comparison [12], [13], [14] of the performance of these classifiers. Furthermore, we have analyzed the scalability of the proposed approach.

This paper is arranged as follows. The next section briefly introduces the fuzzy association rules for classification. Sec- tion III describes in detail each stage of the proposed approach.

Section IV shows and discusses the results obtained on the nine real-world datasets. Finally, in Section V some conclud- ing remarks are made.



II. PRELIMINARIES: FUZZY ASSOCIATION RULES FOR CLASSIFICATION  Fuzzy set theory has been used more and more frequently in Data Mining to represent and to identify dependencies between attributes in a database [1]. The use of fuzzy sets to describe associations between data extends the types of relationships    that may be represented [15]. For this reason, in recent years different studies have proposed methods for mining fuzzy association rules from quantitative data [16], [17].

Let us consider a simple database N with two attributes (age and weight) and three linguistic terms with their associated MFs (see Fig. 1). Based on this definition, a simple example of a fuzzy association rule could be:  Age is Low ?Weight is Middle  Fig. 1. Attributes and linguistic terms in a simple problem  Support and confidence are the most common measures of interest of an association rule. These measures can be defined for fuzzy association rules as follows:  Support(A? B) = ?  xp?N ?AB(xp)  | N | (1)  Confidence(A? B) = ?  xp?N ?AB(xp)? xp?N ?A(xp)  (2)  where | N | is the total number of transactions (patterns) in the database, ?A(xp) is the matching degree of the transaction xp with the antecedent part of the rule and ?AB(xp) is the matching degree of the transaction xp with the antecedent and consequent of the rule.

In the last few years, different studies have proposed meth- ods to obtain fuzzy association rule-based classifiers [7], [8].

The task of classification is to find a set of rules in order to identify the classes of undetermined patterns. A fuzzy association rule can be considered a classification rule if the antecedent contains fuzzy item sets and the consequent part contains only one class label.

A fuzzy associative classification rule A ? Clasj could be measured directly in terms of support and confidence as follows:  Support(A? Clasj) = ?  xp?Clasj ?A(xp)  | N | (3)  Confidence(A? Clasj) = ?  xp?Clasj ?A(xp)? xp?N ?A(xp)  (4)

III. FARC-HD: FUZZY ASSOCIATION RULE-BASED CLASSIFIER FOR HIGH-DIMENSIONAL PROBLEMS  In this section, we will describe our proposal to obtain a fuzzy association rule-based classifier for high dimensional problems. This method is based on three stages:  1) Fuzzy association rule extraction for classification.

2) Candidate rule prescreening.

3) Genetic fuzzy rule selection.

A scheme of this algorithm is shown in Fig. 2.

Fig. 2. Scheme of FARC-HD method  In the following subsections we will introduce the three mentioned stages, explaining in detail all their characteristics.

A. Fuzzy association rule extraction for classification  To generate the initial rule base (RB) we employ a search tree to list all the possible fuzzy itemsets of each class. The root or level 0 of a search tree is an empty set. All attributes are assumed to have an order, first setting the fuzzy items of the variable 1, then the fuzzy items of the variable 2, so on.

The one-itemsets corresponding to the attributes are listed in the first level of the search tree according to their order. The children of a one-item node for an attribute A are the two-item sets that include the one-item set of attribute A and a one-item set for another attribute after attribute A in the order, and so on.

An example with two attributes (V1 and V2) with two linguistic terms (L and H) is detailed in Figure 3. Notice that we can not joint the items set of a node with the items set of another previous node in the level because we will repeat nodes generated for this level. For instance, if we joint V2, L with V1, L we will obtained the node V2, LV1, L, which is the same as the node V1, LV2, L.

An itemset with a support higher than the minimum support is a frequent itemset. If the support of an n-item set in a node A is less than the minimum support, it does not need to be extended more because the support of any item set in a node in the subtree led by node A will also be less than the minimum support. Likewise, if a candidate item set generates a classification rule with confidence higher than the minimum confidence, it is again unnecessary to extend it further.

Once all frequent fuzzy itemsets have been obtained, the candidates fuzzy association rules for classification can be generated setting the frequent fuzzy itemsets in the antecedent of the rules and the corresponding class in the consequent. This process is repeated for each class.

Fig. 3. The search tree for two quantitative attributes V1 and V2 with two linguistic terms L and H  The number of frequent fuzzy itemsets extracted depends directly on the minimum support. The minimum support is usually calculated considering the total number of patterns in the dataset, however the number of patterns for each class in a dataset can be different. For this reason, our algorithm recalculates the minimum support for each class using the dis- tributions of the classes over the dataset. Thus, the minimum support of the class Clasj is defined as:  MinimumSupportClasj = minSup ? fClasj (5)  where minSup is the minimum support determined by the expert and fClasj is the pattern ratio of the class Clasj .

In this stage we can generate a large number of rules. It is, however, very difficult for human users to handle such a large number of generated fuzzy rules. It is also very difficult for human users to intuitively understand long fuzzy rules with many antecedent conditions. For this reason, the depth of the trees is limited to a fixed value (Depthmax) determined by an expert.

B. Candidate rule prescreening  In order to decrease the computational costs, we consider the use of subgroup discovery to select the most interesting rules from the RB obtained in the previous stage by means of a pattern weighting scheme [9]. Thus, in each iteration the rules are ordered according to a rule evaluation criteria from best to worst. The best rule is selected, covered patterns are reweighted, and the procedure repeats these steps until all patterns have been covered more than kt times or there are no more rules in the RB. This process is to be repeated for each class.

This scheme treats the patterns in such a way that covered positive patterns are not deleted when the currently best rule is selected. Instead the algorithm stores for each pattern a count i of how many times the pattern has been covered by a selected rule and decreases its weight according to the formula  w(ej , i) = 1i+1 (in the first iteration all target class patterns are assigned the same weight w(ej , 0) = 1). Covered patterns are completely eliminated when these have been covered more than kt times (for more information, see [9]).

In this paper, we have modified the measure used in [9] (wWRAcc?) to enable handling fuzzy rules. Thus, this measure for a rule A? Clasj is then defined as  n??(A ? Clasj) n?(Clasj)  ? (n ??(A ? Clasj) n??(A)  ? n(Clasj) N  ) (6)  where n??(A) is the sum of the product of the weight of each covered pattern by its matching degree with the antecedent part of the rule, n??(A ? Clasj) is the sum of the product of the weight of each covered positive pattern by its matching degree with the rules, and n?(Clasj) is the sum of the weights of patterns of class Clasj . Moreover, the first term in the definition of wWRAcc? has been replaced by n  ??(A?Clasj) n(Clasj)  to reward rules which cover uneliminated patterns of the class Clasj .

C. Genetic Fuzzy Rule Selection  We consider the use of GAs to select a compact set of fuzzy association rules with high classification accuracy from RB obtained in the previous stage. We consider the CHC genetic model [18] which has achieved good results for binary selection problems [19]. In the following, the main characteristics of this genetic approach are presented:  ? Genetic model.

? Codification and initial gene pool.

? Chromosome evaluation.

? Crossover operator.

? Restarting approach.

1) CHC Genetic Model: The genetic model of CHC makes use of a ?Population-based Selection? approach. P parents and their corresponding offspring compete to select the best P individuals to take part in the next population. The CHC approach makes use of an incest prevention mechanism and a restarting process to provoke diversity in the population, instead of the well known mutation operator.

This incest prevention mechanism will be considered in order to apply the crossover operator, i.e., two parents are crossed if their hamming distance divided by 2 is over a predetermined threshold, L. This threshold value is initialized following the original CHC scheme as:  L = (#Genes)/4.0 (7)  where #Genes is the number of genes in the chromosome (for more information, see [20]). In order to make this procedure independent of #Genes, in our case, L will be decremented by a ?% of its initial value (being ? determined by the user, usually 10%). The algorithm restarts when L is below zero.

A scheme of this algorithm is shown in Fig. 4.

Fig. 4. Scheme of CHC  2) Codification and Initial Gene Pool: Each chromosome is a binary vector that determines when a rule is selected or not (alleles ?1? and ?0? respectively). A chromosome C = {c1, ..., cM} represents a subset of rules, so that:  If ci = 1 then (Ri ? RB) else (Ri 6? RB),  with Ri being the corresponding i-th rule in the initial RB.

To make use of the available information, all the candidates rules are included in the population as an initial solution. To do so, the initial pool is obtained with an individual having all genes with value ?1? and the remaining individuals generated at random in {0, 1}.

3) Evaluation: To evaluate a determined chromosome we compute the classification rate (clas rat) and maximize the following function (defined for maximization):  Fitness(C) = clas rat? w1 ?NR (8) where NR is the number of selected rules (to penalize a large number of rules) and w1 is computed from the clas rat considering all the candidate rules,  w1 = ? ? (clas ratinitial/NRinitial) (9) with ? being a weighting percentage given by the system expert that determines the tradeoff between accuracy and complexity. We have empirically tested that a good neutral choice is for example 1.0 (good accuracy and not too many rules).

The fitness value of a chromosome will be ?w1 if there is at least one class without selected rules.

4) Crossover Operator: The half uniform crossover scheme (HUX) is employed [20]. This operator exactly interchanges the mid of the alleles that are different in the parents, ensur- ing the maximum distance of the offspring to their parents (exploration).

Figure 5 depicts the behavior of this operator.

5) Restarting Approach: To get away from local optima,  this algorithm uses a restart approach. In this case, the best chromosome is maintained and the remaining are generated at random in {0,1}.

The algorithm restarts when L is below zero, which means that all the individuals coexisting in the population are very similar.

Fig. 5. Scheme of the behavior of the HUX operator

IV. EXPERIMENTAL STUDY  In order to analyze the performance of the proposed ap- proach we have considered 9 real-world datasets. Table I summarizes the main characteristics of the 9 datasets, where Attributes is the number of attributes, Patterns is the number of patterns and Classes is the number of classes.

Furthermore, this table shows the link to the KEEL project webpage [21] from which they can be downloaded. Notice that we have removed the instances with any missing value in the dataset Dermatology.

TABLE I DATA SETS CONSIDERED FOR THE EXPERIMENTAL STUDY  Name Attributes Patterns Classes Segment 19 2310 7 Ringnorm 20 7400 2 Thyroid 21 7200 3 Wdbc 30 569 2 Ionosphere 34 351 2 Dermatology 34 358 6 SatImage 36 6435 6 Spambase 57 4597 2 Sonar 60 208 2  Available at http://sci2s.ugr.es/keel/datasets.php  This section is organized as follows: ? First, we describe the experimental set-up in subsec-  tion IV-A.

? Second, we show a statistical analysis obtained from the  comparison with another two GFSs in subsection IV-B.

? Finally, we analyze the scalability of the proposed ap-  proach in subsection IV-C.

A. Experimental Set up  In this study, we have compared the performance of our approach with another two GFSs, including FH-GBML [22] and SGERD [23] algorithms.

? FH-GBML: This method follows a Genetic Cooperative-  Competitive Learning (GCCL) approach and consists of two processes. The first process is used for generating good fuzzy rules while the second one is used for finding good combinations of generated fuzzy rules. This method simultaneously use multiple fuzzy partitions with different granularities for fuzzy rule extraction, using four homogeneous fuzzy partitions with triangular fuzzy sets and a don?t care condition.

? SGERD: It is a steady-state GA to generate a prespecified number of Q rules per class following a GCCL ap- proach. In each iteration, parents and their corresponding offspring compete to select the best Q rules for each class. Simultaneously, this method also uses multiple    fuzzy partitions with different granularities and don?t care conditions for fuzzy rule extraction.

To develop the different experiments we consider a 10-fold cross-validation model, i.e., we randomly split the data set into 10 folds, each containing 10% of the patterns of the data set, and used nine folds for training and one for testing 1. For each of the ten partitions, we executed three trials of the algorithms.

For each data set, we therefore consider the average results of 30 runs.

The initial linguistic partitions are comprised by five lin- guistic terms with uniformly distributed triangular membership functions giving meaning to them. The values considered for the input parameters of each method are: ? FARC-HD: MinSup = 0.05, MinConf = 0.8, Depthmax = 3, kt = 2, Pob = 50, Eval = 5, 000, ? = 0.02  ? FH-GBML: Nrules = 20, Fsets = 200, Gen = 1000, Pc = 0.9, Pdcare = {0.5, 0.8, 0.95}, Pmichigan = 0.5  ? SGERD: Q = heuristics These values were selected according to the recommenda-  tion of the corresponding authors within each proposal, which are the default parameters settings included in the KEEL software tool [21]. Notice that in the FH-GBML algorithm the authors used three different probabilities of don?t care (0.5, 0.8 and 0.95 depending on the size of the dataset) to obtain fuzzy rules with a few antecedent fuzzy sets. In these experiments, we have used these three probabilities of don?t care in each dataset and have shown in the tables the best average result obtained in each one.

B. Results and Statistical Analysis  The results obtained by the analyzed methods are shown in Table II, where #R stands for the average number of rules, #L stands for the average number of linguistic terms in the antecedent of the rules, and Tra and Tst respectively for the average accuracy obtained over the training and test data.

The results presented show that our proposal obtains the best global results for test. Furthermore, the average number of rules is low (13.5 rules on average) and the rules obtained only involve three or less attributes in their antecedents, giving the advantage of easier understanding from an user?s perspective.

In order to compare the results, we have used non- parametric statistical tests for multiple comparison [12], [13], [14] to find the best approach, considering the results ob- tained in the test partitions (Tst). First of all, we have used Friedman and ImanDavenport tests [24], [25] in order to find out whether significant differences exist among all the mean values. Tables III shows the Friedman and ImanDavenport statistics and it relates them to the corresponding critical values for each distribution by using a level of significance ? = 0.05. The p value obtained is also reported for each test.

Given that the statistics of Friedman and ImanDavenport are clearly greater than their associated critical values, there are  1The corresponding data partitions (10-fold) for these data sets are available at the KEEL project webpage [21]: http://sci2s.ugr.es/keel/datasets.php  significant differences among the observed results with a level of significance ? ? 0.05.

TABLE III RESULTS OF THE FRIEDMAN AND IMANDAVENPORT TESTS (? = 0.05)  Friedman test ImanDavenport test Statistic (X 2F ) Critical Value p value Statistic (FF ) Critical Value p value  14.001 5.991 <0.001 28.001 3.634 <0.001  Table IV shows the rankings (computed using a Friedman test) of the different methods considered in this study.

TABLE IV AVERAGE RANKINGS OF THE METHODS  Method Ranking SGERD 2.666 FH-GBML 2.333 FARC-HD 1.000  We now apply Holm?s test [26] to compare the best ranking method (FARC-HD) with the remaining methods. In Table V, the methods are ordered with respect to the z-value obtained.

Holm?s test rejects the hypothesis of equality with the rest of the methods (p < ?/i). Therefore, analyzing the statistical study shown in Tables IV and V we conclude that our model has shown itself to be the best performing method when compared with the remaining fuzzy GFSs applied in this study.

C. Analysis of Scalability Table VI shows the average runtime of the analyzed meth-  ods on the 9 real-world problems. The methods were imple- mented using Java and all of the experiments were performed using a Pentium Corel 2 Quad, 2.5GHz CPU with 4Gb of memory and running Linux.

Analyzing the results presented we can see that: ? The SGERD algorithm presents a very low average run-  time in all datasets, obtaining a good scalability when we increase the size of the problem. This method, however, obtains the worst ranking in the Friedman?s test when we compare the results obtained in the test partitions (see table IV).

? The FH-GBML algorithm expends a large amount of time when the number of attributes and patterns in the dataset is high.

? The FARC-HD approach presents a low computational cost in all datasets, obtaining a good scalability and the best performance in accuracy.



V. CONCLUSION In this paper we have proposed a fuzzy associative classifi-  cation method with genetic rule selection for high-dimensional datasets to obtain accurate and compact fuzzy associative classifiers with a low computational cost.

Analyzing the results obtained we can conclude that our model has shown itself to be the best performing method in the experimental study and presents a low computational cost in all datasets, obtaining a good scalability. Furthermore, our proposal obtains models with a reduced number of rules with few attributes in the antecedent, giving the advantage of high interpretability from an user?s perspective.

