MINING DATA ASSOCIATION BASED ON A REVISED FP-GROWTH

Abstract:  This paper introduces a new weighted Apriori based on a  revised FP-growth algorithm to mine association rules in a  relational database. The new algorithm is acquired by revising  the search mechanism of the well known Apriori weighted  multidimensional data mining algorithm which searches for  candidate item sets by repeatedly scanning in the database. The  effectiveness of our proposed algorithm is verified through a real  application of mining in the student achievement database.

Keywords:  Data Mining; Association rules; Multi-dimensional  Association rules; Apriori algorithm; FP-growth algorithm  1. Introduction  Data mining, also known as Knowledge Discovery in Database (KDD), extracts information from large databases which are of interest to people. The extracted knowledge represents concepts, rules, regularities and patterns in different forms [1]. Data mining can be applied in many real fields, for example, it can be used to discover the developmental trends, future prediction, and the analyses of the key factors which are needed to complete a given task.

Data mining algorithms to find association rules mainly focus on circular scanning algorithm, incremental updating algorithms, parallel algorithm, meta-pattern guided and constraint based mining algorithms. The main features of these algorithms are to compute the frequent items. The Apriori algorithm and the FP-growth algorithm are, in fact, the most typical ones.

1.1. Basic concepts and association rules  (1) The association rules are discovered from databases of the relevant task. Firstly, we set up a database DB with n records. Each record in the database is called as an item.

(2) LetJ={1J, /Z, ... 1m} be an item set (also known as itemset) with m items. The itemset containing k items is called k-itemset. A transaction T is an itemset and the database is the set of transactions. Each transaction is denoted by an identifier, called TID.

(3) An association rule is an implication of the form A=> B, where A c I and Bel, A (l B = 0. The support of rule A => B , sup( A => B ), is the percentage of transaction in the database that contains A u B . The confidence of rule A=> B , conf( A=> B), is the percentage of the transaction containing both A and B. Associate rules are called strong if they satisry both the minimum support threshold and the minimum confidence threshold. The minimum support threshold and minimum confidence threshold are selected by the domain expert.

(4) The frequent of the itemset is the number of transactions that contain this itemset. If the support of the itemset satisfies the minimum support threshold, then the itemset is a frequent itemset. The frequent k-itemset is denoted as Lk?  Usually, the association rule mining contains two steps: (1) finding all frequent itemsets in the database, and (2) using frequent itemsets to generate strong association  rules satisfying minimum support threshold and minimum confidence threshold.

1.2. The classical association rules mining algorithm  1.2.1 The Apriori algorithm  The Apriori algorithm can be used to mine the frequent itemset in the database. It is based on the fact that algorithm using the prior knowledge of the frequent itemset. Apriori algorithm is actually a layer-by-layer iterative searching algorithm, where k-itemset is used to explore the (k+ 1 )-itemset.

Apriori algorithm firstly scans the database to find out the number of each item. Those items satisrying the      minimum support threshold are frequent l-itemset. Then, a loop of two steps is performed: 1) the join step and 2) the prune step. In the join step, different combinations of items in the frequent k-itemset join together to generate the candidate itemset Ck+I for (k+l)-itemset LK+I. In the prune step, LK+I is generated from Ck+I by pruning those combinations of items which can not satisfy the minimum confidecet and minimum support thresholds. The property of Apriori states that all nonempty subsets of a frequent itemset must also be frequent items. This property is used to improve the efficiency of the prune step while the size of Ck+I is large. The procedure of the loop runs iteratively until no more frequent (k+ 1 )-itemset could be generated.

1.2.2 The FP-growth algorithm  In order to overcome the inherent defects of the Apriori algorithm which produce a large number of candidate itemsets and require scanning of the database iteratively, Han [9] proposed a frequent pattern growth algorithm (i.e.

FP-growth) based on Frequent Pattern (FP) tree. The algorithm only needs to scan the database twice. At the first scan of the database the frequent l-itemset will be generated.

At the second scan, the l-itemsets are used to generate the FP tree by filtering out infrequent items [13]. The FP tree contains all the frequent itemsets, so the higher order frequent itemsets can be mined from it. The mining of the FP tree starts with each frequent pattern of length 1. The conditional pattern base is constructed and then the (conditional) FP tree is grown. Then, the algorithm mines the tree iteratively to generate the frequent patterns.

The frequent pattern growth is achieved by connecting the frequent pattern with the pattern generated by the conditional FP tree. It uses the most frequent item, which provides a better selectivity. This method greatly reduces the cost of searching. The study on the performance of the FP-tree method shows that it is efficient and scalable to mine both long and short frequent patterns. Resulting frequent patterns are used to form frequent k-itemsets. It is about an order of magnitude faster than the Apriori algorithm's [15].

2. The revised FP-growth algorithm  By introducing an entropy heuristic and changing the search mechanism of the original FP-growth algorithm, we have the following revised FP-growth algorithm.

The revised FP-tree growth  Input: the transaction database D; minimum support threshold, min_sup.

Output: the complete set of frequent patterns (itemsets).

Procedure:

I. Construct FP- tree according to the following steps (a) Scan the transaction database once. Gather frequent  itemset F and their support counts. Sort F in descending order according to support counts and generate the frequency item list L.

(b) Construct root of the FP-tree and mark it with "null".

For each transaction in D, execute: Sort the frequent items in the order of L. Assuming that the list of frequent items is [p IPl, where p and P denote the first element and the remaining list, respectively.

Call the function insert_tree([PIPl,T). The procedure is performed as follows. If T has a child N which makes N item-name=p. item-name, N = N+I; otherwise, create a new node N and set the count to I. The newly created node is linked to its parent node T and also to the node with the same item-name value through the structure of the chain of nodes.

IfP is not empty, recursively calls the insert_tree (P, N).

2. Mining the FP-tree by calling the FP- growth (FP _tree,  null). The procedure is shown as follows: Procedure FP JIrowth (Tree,a) I) if Tree P contains a single path then 2) for each combination of node in path P(P) 3) generate pattern P U a and the support  count= the minimum support count in node p.

4) else for each ai in the head of Tree{ 5) produce a pattern fJ = a i U a and the support of  fJ = a i .. support Jount; 6) construct conditional pattern base of p, and then  construct conditional FP- tree of P;  7) if Tree fJ::;c 0 then 8) call FP JIrowth(Treep,p) }  The main difference between the original and the revised FP-growth algorithms is that the revised one uses the entropy heuristic during the generation of FP-tree and replaces the one-by-one scanning search mechanism with the batch search mechanism.

3. An algorithm of mining association rules in student achievement in the application of relational database structure  A relational database consists of a table which a row represents a record while a column represents an attribute of the database. For each attribute, it could be Boolean, numerical or character types [7].In a relational database, storage structure determines the acquisition of association rules with obvious features [5]:  (1) An attribute of the relational database does not take      Income Income Income Income ID  ( 0-lk) ( lk-2k) (2k-3k) (3k-4k)  Table 2 is Boolean after converting the transaction database  It can be seen by comparison that the transaction database contains items with the same dimension. 100 1 0 0 0  200 0 1 0 0 300 1 0 0 0 400 0 1 0 0 500 0 0 1 0 600 0 0 0 1 700 0 0 1 0  value of vector. Relational database association rule mining is a problem of multi-dimensional associate rules mining.

(2) Attributes in relational database could have different data types and could take on different values. Therefore, the multiple value association rule mining or quantitative association rule mining is the key feature here.

(3) In a multi-dimensional database, many dimensions can yield a concept hierarchy, and therefore, the relational database association rules tend to cover different attributes to multiple conceptual layers.

3.1. Type conversion based on Boolean Association Rule Mining in relational database theory  Boolean association rule mining technology has been developed for many years. It is the problem of association rule mining in origin, but the technique which can be used to construct table 1 is similar to the association rule mining [14].

It not only contains several conditional attributes, but also contains a large number of class attributes. Usually before mining, it maps the relational categories and numerical attributes into Boolean attributes, and converts the relational data table into database form (Table 1), and then uses the Boolean association rules mining technology [8]. Categories and numerical attributes Boolean mapping method includes the following 3 steps. (1) For categorical attributes with less values (including Boolean two attributes), each category is mapped to a project. (2) For attributes with more values, according to the concept of hierarchical principles, attribute values are divided into a plurality of subsets, and each subset of the attributes will produce a project; and (3) for numerical attributes, according to some numerical discretization methods for multiple sub-ranges, each section is corresponding to a project [9].

TABLE 1 TRANSACTION DATABASE TID II 12 I3 14 T100 0 1 0 0 T200 1 0 1 1 T300 0 0 1 0 T400 1 1 0 0  3.2. Weighted relational database mining algorithm of association rules  In the first mining before the use of predefined concept of hierarchy database, numerical attributes  discretization will be used. Values of interval instead of income concept hierarchy can be applied such as"0-2K", "2.1-3K", "3.1-4K" for numerical attributes of a dataset after discretization. It is similar to Table 2.

Each attribute of the database is treated as a predicate (dimension), according to the different attribute importance weighting. Mining algorithms at first temporarily do not consider the attribute weights. They only use the traditional Apriori algorithm to find the support no less than the minimum weighted support-threshold. For all the frequent itemsets in which the item set weights are considered to be less than 1, the proj ect sets of weighted support must be less than the support, and therefore, the generation of frequent item set is a superset of the weighted frequent itemsets.

Calculation of concentration of all items can generate the frequent item set with weighted support, and the weighted support measure is less than the minimum weighted support project. After obtaining all the weighted frequent itemsets, we then use a weighted frequent itemsets to generate all weighted association rules. The weighted support measure is defined as[3]:  wsup(x) = max {? ,? . . . . . .  hk} sup(x) .

Taking into account the Apriori algorithm for each generation of candidate itemsets, the procedure goes back to scan the database to judge whether these frequent itemsets are sufficient. It then determines which ones are the most frequent item sets. It may generate those which are resulting in an I/O expensive and efficiency very low, and therefore, an optimization process is necessary (Apriori optimization [6, 11]).

The optimization for training frequent itemsets is based on k-supporting transaction to calculate (K + 1 )-candidate with satisfying degree of support. It then finds the candidate set of frequent items, i.e. support (k+ l)-itemsets, supporting its K first-order subsets. In other words, support ( k+ 1) is a set of all subsets of k order affairs which must also support (k+ 1 )-itemsets. Here we define that, if the set D has k+1 elements comprising a non empty set called ensemble D k-order subset. If we can, from all k order subset support affairs, find the intersection of public sections and the equivalence of the ( K+1)-itemsets supports, it can avoid      scanning the database multiple times, which greatly improves the efficiency of the algorithm. With the minimal support services as the pruning criterion, we can define the minimum support services as the minimum support degree (minsup) [10, 12].

3.3 Algorithm description  Procedure of Weighted Apriori (Lk: findjrequent_k _itemset, S: dedescend power subset of Ck,SCk: set of Ck support transactions): {  }  }  Scan DB; Find item_sup_trans;  Cl =Generate_ Cl(DB); Ll =find_frequent_l_itemset; K=2;  While Lk-l*0 {  CK = apriorU?en (LK - 1 )  For each candidates itemset C E CK { Cksuptrans= SCk; For each descend power subset S {  Cksuptrans= Cksuptrans n S.suptrans;} C.count= ICksuptransl;  K++;  Lk = {c ECk l c.count ? wminsup}; } Return F=Uk Lk ;  function Apriori _ Gen( Lk ) {  For all itemset gl , g2 E Lk do {  (gj ?ij =g2 ?ij)J\ (gj ?i2 =g2 ?i2)J\?????? if  J\(gj . ik_2 = g2 . ik_2) J\ (gj . ik_j < g2 . ik_j) then {  C={gj.ipgj.i2'??????,gj?ik_pgj?ik_j} add c to Ck;  For all k-l itemset Xc C do if (X ? Lk_j ) then delete c from Ck ;  } } Return Ck;  4. The experiment results and analysis  To verify the effectiveness of our algorithm, we select a real database of student achievements in a middle school of Baoding City. The database which has more than 500 records with 12 features describes the achievements of year-2 students in this school.

First of all, the relational data model is related to a relational database. We convert the data to the database storage form, and then conduct the data preprocessing. For example, the following preprocessing: achievement deletion processing and performance loss, etc. It is mainly because of students dropping out of school or some reasons such as corrosion test [13]. The preprocessing also includes the missing data and incomplete data. Since our algorithm cannot handle the case in which some data are missing from the databases, we replace the missing data (scores) with the average of the column. The source database is shown in Table 3:  TABLE 3 SOME STUDENT ACHIEVEMENTS ID N_ Chinese """ Eo"", Hk,,,,>, Politic-. Phy.", C"="'ry S? Class rank G",&r.mk '" Han wenCii 108 108 116 100 96 95 go m 165 <7 183 Xic ya.o 109 108 117 100 95 OJ 97 719 183 49 183 Wani yibo 10< 111 111 99 " 96 92 713 107 <0 136 Tani chuh 97.5 111 115 " 96 go gs 112.5 1<7 " 183 Lou ",inn .. 105 108 '" 100 98 94 9' 710 39 19 18< r;uo roni 108.5 107 111 95 98 96 91 707.5 81 34 '" Du li. 102.5 112 110 gs go 95 92 102.5 193 51 187 hcn? ?i 98 118 '" 100 90 OJ 91 702 253 52 185 '''''21i 102 109 117 " 97 96 86 701 193 50 185 LiQ,iVlII 94.5 113 117 98 91 " 92 100.5 92 3T  The discretizing process for the student achievements is as follows. The thresholds are first defined and the scores according to a predetermined interval are categorized as: Poor [0, 60), Pass [60, 70), Medium [70, 80), and Good [80, 90), and Excellent [90,100]. After data preprocessing, the the dataset has the form of Table 4.

ID  1S3  1&3 1S6 IS] 1S4  1&3 1S7 1&5  1S3 1.6  TABLE 4 DATA AFTER PRETREATMENT OF THE TABLE N?.

Hanwenqi Xie yaa  Wan?yibo Tan chuhe Louxinna Guo ron  Duli Chen siyu  Wan Ii Liqi:mg  Hanwmqi Xie :v,aa  Wangyibo Tan chuhe  Chin?se (Excellent)  Chinese Chinese Chinese Chinue( hile} ( {ood) (Medium) (poor) 0 0 1 0 1 0  FP-growth on different length of the rule has very good adaptability, while the efficiency is also greatly improved.

The FP-growth algorithm is the central part of the weighted Apriori. It mines many rules which are identical with teacher common ideas and mines some unexpected rules such as that Math(Excellent) => Top rank.

Ph (e:-::cellent) , Ch(excellerrt) , En (good) ) lila (good)  Ch (excellent) , Ha (good) , En (lItediUJlt) ,Pol (excellerrt) ) top rank  lia(lItediUJII),Ch(excellerrt) , Ph (good) ) En (good) , His (good) , Pol (i:0od) ,  Ch (good) , top tali:  Ph(e::cellent) , Ch (good) , En (good) --) 1b (good) Ch (good) ,Ha (good) ,En (good) ) t? rank  lia(lItediUJII),Ch(excellerrt) ,Ph(good) ) En (good) ,His (good) ,Pol (good) ,  Ch (i0od) , top rank  Figure 1. Rules of FP-growth algorithm     According to the policy of the Boading City, the subjects in senior high school entrance examination will be assigned different weights. For example, the weight of Chinese or mathematics is 1, English is 0,7, chemistry is 0.5, the politics and history are 0.3 and 0.4 respectively. The mined results (achievement association rules) are dependent on these weights.

26 Ph (Excellent) , Ch (good) , En (good) --> Ma (good) 34 Ch (good) , Ma (good) , En (medium) --> top rank  Ma (medium) ,Ch (excellent) , Ph (good) > En (good) , His (good) ,Pol (good) , Ch (?ood) , top rank  Figure 2. Rules of weighted algorithm  5. Conclusions  67% 77%  67%  The revised PF-growth algorithm is proposed. Based on the PF-growth, a weighted Apriori is given. Experimental results show that, after an optimization, the weighted Apriori only needs to scan the database one time, rather than the multiple times originally. The effectiveness of our proposed algorithm is verified through a real application of mining in the student achievement database.

OJ 30 E  . ...

..., 20   g. 20 ? 15   ---- method 1 method 2  Wsupport Figure 3. Two Apiori running time Comparison  Apriori a1iori tlun.

