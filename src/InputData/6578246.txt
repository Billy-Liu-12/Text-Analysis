

Abstract?RAID stands for Redundant Array of Independent Disks. It is a storage technology that combines multiple disk drive components into a logical unit. RAID is now used as an umbrella term for computer data storage schemes that can divide and replicate data among multiple physical drives. Reliability and availability are big concerns for any data storage system, including RAID architectures.  In this paper, an alternative method estimating the reliability of various RAID architectures is presented, using the Markov models approach.  The reliability analysis of various RAID architectures is performed using CARE-Computer Aided Reliability Engineering, an industrial software tool. Estimating the reliability of RAID architectures and combining them with the reliabilities of other elements of the system, it allows estimating the overall reliability of the system.

Evaluating reliability and performance of RAID enable enterprises to identify the appropriate disk configurations, as part of their overall IT strategies.

Keywords-RAID disks; Redundancy; Reliability; Markov models

I.  INTRODUCTION   As research and business become more data driven, the need for high performance and high capacity storage systems grows. In many of these applications reliability and availability are extremely important also.

Currently there are no cost-effective ways to satisfy these performance needs with single hard drive systems.

Consequently drives are used in aggregate with the hope that each additional drive will result in an overall additive increase in system?s capacity and bandwidth/performance [1].

Disk arrays organize multiple, independent disks into a large, high performance logical unit. Disk arrays stripe data across multiple disks and access them in parallel to achieve both higher data transfer rates on large data accesses and higher I/O rates on small data accesses. Data striping also results in uniform load balancing across all the disks. Unfortunately, large disk arrays are highly vulnerable to disk failures.  A disk array with 100 disks is 100 times more likely to fail than a single disk array.  The solution is to employ redundancy in the form of error correcting codes to tolerate disk failures. This allows a redundant disk array to avoid losing data for much longer than an unprotected single disk [2].

The solution is RAID, where RAID stands for Redundant Array of Independent Disks. Data are distributed across the drives in one of several ways called "RAID levels", depending on what level of redundancy and performance is required.

RAID is now used as an umbrella term for computer data storage schemes that can divide and replicate data among multiple physical drives. The different schemes or architectures are named by the word RAID followed by a number (e.g.

RAID 0, RAID 1). Each scheme provides a different balance between the key goals: reliability and availability, performance, and capacity. RAID levels greater than zero provide protection against unrecoverable read errors, as well as whole disk failure.

Each option presents subtle tradeoffs among reliability, performance   and cost [2].

There is a rich body of research dedicated to the study of reliability of RAID architectures, using methods such as theoretical modeling and analysis, including machine learning technique and empirical studies. Reference [2] tried to show analytically the reliabilities of RAID, but it did not provide an in-depth analysis. Reference [3] uses simplified mathematical models and a reliability figure of merit of 0.9 to estimate the reliability of RAID disks supported on Dell ?PowerEdge? Servers.  A very popular method for modeling hard-drives and large storage-systems failures and consequently the reliability and availability of these systems are Markov models (processes). The Markov models are chosen for reliability analysis because it is relatively simple to analyze them. The simplicity comes from the Markov processes property which considers only the current state of the system when determining a transition to the next state. References [1], [4], [5], [6], [7] present modeling and analysis of RAID architectures using sophisticated and rigorous mathematical models.

The driving force behind the interest in reliable disk arrays architectures is the need for high performance storage systems for a large variety of applications. The applications range from image-intensive applications, which require huge data storage [8], [9], to long-term digital preservation systems which require highly reliable, cost-effective, easy-to-maintain archival units [10], [11]. Organizations whose mission encompasses the preservation of information and cultural heritage have very high reliability requirements for their digital preservation system [10].

This paper presents an alternative new method for estimating the reliability of the most popular RAID systems using the Markov models approach. CARE (Computer Aided Reliability Engineering) software tool is used to perform the reliability modeling   of RAID architectures.

The rest of the paper is organized as follows:  Section I presents the most important RAID architectures. Section II is an overview of the reliability concepts and Markov models for RAID. Section III presents the reliability estimation of RAID architectures using CARE tool, Markov modeling option.

Section IV presents the conclusions.



II. RAID ARCHITECTURE  A. RAID Level 0 (Nonredundant) RAID level 0   does not employ redundancy at all. It offers  the best write performance since it does not need to update redundant information. Without redundancy, any single disk failure will result in data loss. They are used in supercomputing environment where performance and capacity, rather than reliability, are primary concerns [2].

B. RAID  Level  1 (Mirrored) RAID level 1 consists of mirror disks. A RAID 1 array  requires a minimum of two drives. Whenever data is written to a disk, the same data is written to a redundant disk, so there are always two copies of the information. RAID 1 speed up read accesses by dividing them among both disks. When data is read, it can be retrieved from the disk with the shorter queuing, rotational delays, etc. Write accesses are slowed down because both disks must be updated. Mirroring is used in database applications where availability and transaction rates are more important than storage efficiency. RAID 1 is presented in figure 1.

C. RAID  Level 2 (Memory Style Error Correcting Codes) Raid 2 consists of a bank of data disks in parallel with  Hamming-coded disks. Hamming codes contain parity for distinct overlapping subsets of components. In one version of this scheme, four data disks require three redundant disks.  If a single component fails, several of the parity components will have inconsistent values, and the failed component is the one held in common by each incorrect set. Multiple redundant disks are needed to identify the failed disk, but only one is needed to recover the lost information.

D. RAID Level 3 (Bit-Interleaved Parity) RAID level 3 is a modification of RAID 2. It uses a single  parity disk rather than a set of parity disks to recover lost information. In a bit-interleaved parity disk array, data is conceptually interleaved bit-wise over the data-disks, and a single parity disk is added to tolerate any single disk failure.

Each read request accesses all data disks, and each write  request accesses all data disks and the parity disks.  Because the parity disk contains only parity and no data, the parity disk   Figure 1.  RAID  1 architecture and RAID 3 architecture  cannot participate on reads, resulting in slightly lower read performance than for redundancy schemes that distribute the data and parity over all disks. Bit-interleaved parity disks disk arrays are frequently used in applications that require high bandwidth, but not high I/O. Aldo they are easier to implement by comparison with other RAID configurations.

E. RAID Level 4 (Block-Interleaved Distributed Parity) The RAID Level 4 is similar with RAID level 3, except  that the data is interleaved across disks in blocks of arbitrary size, rather than in bits. In this setup, files can be distributed between multiple disks. The size of these blocks is called the stripping unit. Each disk operates independently which allows I/O requests to be performed in parallel, though data transfer speeds can suffer due to the type of parity. The error detection is achieved through dedicated parity and is stored in a separate, single disk unit. The advantage of RAID level 4 over RAID level 3 is that small read/write operations are faster because the operation is contained in just a single data disk rather than interleaved over all of them.

F. RAID Level 5 (Block Interleaved Distributed Parity) RAID 5 is a modification of the RAID4 structure and  requires three or more disks. Instead of having one parity disk the parity blocks are interleaved among all disks. The data are distributed over all the disks rather than only one. This allows all disks to participate in read operations in contrasts with to redundancy schemes with dedicated parity disks in which parity disks cannot participate in servicing read operations   G. RAID Level 6 (P+Q redundancy) RAID level 6 combines four or more disks in a way that  protects data against loss of any two disks.  As larger disks arrays are considered, multiple failures are possible and stronger codes are needed. Applications with more stringent reliability requirements require stronger error correction codes. Such a scheme, called P+Q redundancy uses Reed Solomon codes to protect against up to two disk failures using a minimum of two redundant disks. The P+ Q redundant disks arrays are structurally similar to the block interleaved distributed parity disk arrays and operate in a similar manner.

Figure 2.  RAID 5 architecture and RAID  6 architecture

III. RELIABILITY ANALYSIS  OF RAID  A. Reliability Metrics and Models The reliability of a system follows an exponential  distribution law during the useful life phase of the system:   R(t) = e-?t                                                (1)  where ? represents  the failure rate and is assumed to have a constant value  during the useful life of the system.

The mean time to failure (MTTF) of a system is the expected time of the occurrence of the first system?s failure. If the reliability function   is defined by (1), then:   MTTF = 1/ ?                                           (2)   The mean time to repair (MTTR) of a system is the average time required to repair the system. MTTR is commonly specified in terms of the repair rate ?, which is the expected number of repairs per unit time. Usually the assumption is that the repair rate is much greater than the failure rate. The MTTR of the system is defined by the following equation:   MTTR = 1/ ?                                           (3)   The mean time to data loss (MTTDL) is reliability metric specific to RAID architectures and represents the average time before a loss of data occurs in a given array [12]. Reference [13] presents the analytical expressions for MTTDL for various RAID architecture using the MTTF and MTTR of hard disk drives.

The reliability parameters of RAID architectures can be estimated using Markov processes.  Markov processes are a special class of stochastic processes. The basic assumption is that the behavior of the system in each state is memoryless.

The transition from the current state of the system is determined only by the present state and not by the previous state or the time at which it reached the present state. Before a transition occurs, the time spent in each state follows an exponential distribution. Markov processes are illustrated graphically by state transition diagrams (STD). For dependability analysis, a state is defined as a particular combination of operating and failed components. The aim of Markov processes analysis is to calculate Pi (t), the probability that the system is in state i at time t. To determine Pi (t), a set of differential equations is derived, one for each state of the system. These equations are the state transition equations.

State transition equations are usually presented in matrix form.

In reliability analysis, once a system fails, the system cannot leave the failed state.  Once the system of equations is solved and the Pi(t) are known, the system?s reliability, availability or safety can be calculated as a sum taken over all the operating states [9]. If the number of states in the Markov STD increases due to the complexity of the system, solving the Markov processes becomes computational intensive.

B. Reliability  for RAID using Markov Models Examples of reliability estimations for different RAID  architectures using Markov processes are presented. The assumption is that each disk fails independently with a constant rate ? and the time to repair each is exponentially distributed with mean 1/ ? and ? >> ? [12].

The   reliability of RAID 1 architecture can be estimated using a three-state Markov model The state of the system is the number of disks that are functional; it can vary between 0 (failed system) and 2 (both disks are functional).  State 1 represents actually two collapsed states (one disk failed, one disk is operational, both having the same failure rate ?). The unreliability at time t is the probability of being in the failed state, P0(t). Figure 3 presents the Markov state transition diagram for RAID 1 architecture.

Figure 3.  Markov state transition diagram for RAID 1  A set of differential equations is derived to calculate P1(t), P2(t) and P3(t), the probabilities of being in state 2, 1 or 0, using ? and ?.

d/dt (P2(t))= - ? P2(t)+ ? P1 (t)                            (4)  d/dt (P1(t))= - (?+ ?) P1(t)+ 2 ? P2 (t)                (5) P0(t)=  1 - P1(t) - P2 (t)                                       (6)  If ? >> ?, the mean time to data loss MTTDL can be approximated  as:   MTTDL = (3 ?+ ?)/(2 ?2)                                  (7)   The reliability function can be approximated  as:  R(t) = e-t/MTTDL                                                    (8)   For RAID level 3 architectures, the Markov State transition diagrams are  similar to those used for  RAID 1 architectures.

Instead of two disks per group we have d+1, where d=3,5, 7.

The analysis is similar to RAID 1 architecture analysis.

Figure 4.  Markov state transition diagram for RAID 3  ?+?=2? ?  ? 1 0    (d+1)? d?  ? 1 0            MTTDL can be expressed as:   MTTDL = [(2d+1) ?+ ?]/[d(d+1) ?2]       (9)   The reliability of RAID 3 architecture is estimated to:   R(t) = e-t/MTTDL                                         (10)   The reliability model for RAID 4 and RAID 5 architectures are similar with RAID 3 architecture. The models differ only in their performance. For hierarchical RAID architectures and different values for disks failure rates, the state transition diagram will include a large number of states and transitions, increasing the difficulty of solving the mathematical models.



IV. MARKOV MODELING OF RAID USING CARE-VRBD TOOL An alternative method for predicting the reliability  parameters of the most popular RAID systems is presented, using CARE-Computer Aided Reliability Engineering software tool. CARE is an engineering tool, developed by BQR Reliability Ltd., intended for reliability analyses and testing of electronic and mechanical systems.

Visual Reliability Block Diagram (VRBD) module of CARE tool is intended to define and calculate Reliability, MTBF, MTTR and Availability for systems as hierarchic combination of different functional components in a very simple way. VRBD   uses  analytical formulas and numerical algorithms providing quick calculations with controlled accuracy. It is based, partially on widely accepted principles, and also on proprietary innovative development effort at BQR Reliability Ltd., [14]. Markov model, part of  CARE VRBD module,  is described in terms of states and transitions  as described in  Section II of this paper.

Following CARE-VBRD methodology, the evaluation of the system?s reliability starts by editing user?s data such as: mission time, failure rate FPMH (failures per millions of hours), MTTR, accuracy for the desired model, states and transitions descriptions, etc.

The Markov state transition diagram RAID level 1 with collapsed states is presented in figure 5.  The results of the reliability calculations for this system for a set of data (MTTR=60 min, FPMH=2, mission/reliability definition time=1500 hours, etc. )   is presented in figure 6.

Figure 5.  Markov state diagram  for RAID 1      Figure 6.  Reliability graph for RAID 1  The Markov state transition diagram for RAID level 3 is similar with RAID 1, except the number of disks d. The reliability graphs for RAID 3 architectures for a possible set of data (MTTR=60 min, FPMH=2/disk) and d=3, 5, 7 are presented in figures 8, 9 and 10.

Figure 7.  Markov state diagram  for RAID 3      Figure 8.  Reliability graph for RAID 3, d=3                   Figure 9.  Reliability graph for RAID 3, d=5    Figure 10.  Reliability graph for RAID 3, d=7  Several reliability analyses were performed, changing parameters such MTTR, FPMH, mission time.  The results were consistent with the Markov models and calculations presented in references [12], [13]. Capacity, I/O performance and cost are other parameters to be considered when selecting RAID architecture for a specific application.



V. CONCLUSIONS In this paper, a new method estimating the reliability of  various RAID architectures is presented, using the Markov models approach using CARE (Computer Aided Reliability Engineering), an industrial software tool for reliability analysis.  A possible limitation of the method is the ?steep? learning curve of the CARE tool due to its complexity  and the associated cost of the software. Overall, a disadvantage of Markov reliability models is that the number of states necessary to model even simple disk arrays increases exponentially as new failure modes and system components are introduced. Because the repair rate for components of most  disk arrays is much higher than the failure rates, it is usually possible to simplify the Markov models by eliminating states that very rarely occur [1]. As future work I intend to extend the study to other RAID architectures and correlated failures in RAID architectures.

Estimating the reliability of RAID architectures and combining them with the reliabilities of other elements of the system, it allows estimating the overall reliability of the system. Evaluating reliability and performance of RAID enable enterprises to identify the appropriate disk configurations, as part of their overall IT strategies.

