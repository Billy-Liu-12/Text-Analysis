AN EFFICIENT MININGALGORITHM FOR FREQUENT PATTERN IN  INTRUSION DETECTION

Abstract: In data mining-based intrusion detection system, we  should make use of particular domain knowledge in relation to intrusion detection in order to efficiently extract relative rules from large amounts of audit records. This paper first gives a summary of the basic association rule algorithm and episode rule algorithm, then improves the basic algorithm considering the characteristic of audit records.

Key words: Data mining: Intrusion detection: Frequent pattern:  Association rule; Episode rule  With the development of the computer security, the security system progressed from static model to dynamic model. Intrusion detection is an active protection technology ,which can defend insider attacks. outsider attacks and the misuses of users in real time. Intrusion detection can research into intrusion accidents. intrusion measures and defects of the attack aim to construct an efficient system ,which can detect the intrusion and respond before the intrusion take effect. After decades of development, intrusion detection system can make use of some efficient technology including nerve network, data mining, etc. The data mining-based intrusion detection system analyses the data of network trafic and system log, and extracts intrusion modes and normal modes to detect intrusion. Nowadays the main mining algorithms for association rules implement a method named level-wise approximate mining that extracts frequent itemsets from the transactional database. However, the spending for scanning the transactional database twice is so lavish that the efficiency of the algorithm debases. This paper presents an extended algorithm that can make use of particular domain knowledge in relation to intrusion detection to lessen the amount of the candidate itemsets. So the efficiency of algorithm improves greatly.

1 The basic algorithm  In order to construct an accurate classifier, it is necessary to gather a sufficient amount of training data and identify a set of meaningful features. Both of these tasks need to extract the feature of audit records. Mining audit records is to extract frequent patterns, namely association rules and frequent episodes from audit records.

1.1 The basic algorithm for association rules  In the database, the association between data items means that we can infer that particular data item is in existence because of the appearance of some data items in a transaction. The purpose of mining association rules is to dig out all the latent association rules between the data items. The main object of mining association rules is the transactional database.

We provide the terms in relation to association rules with the standardization definition. As follows,  Definition 1.1 Let A be a set of attributes, and I be a set of values on A, called items. Any subset of I is called an itemset.

Definition 1.2 Let I be an itemset, and i,, be an item on I.

Any set of ial is call a transaction(T). Every transaction has a unique sign, namely TID.

Definition 1.3 Let D be a set of transaction(T). D is called a transactional database.

Definition 1.4 If T C I ,  X C I ,  T C X ,  then we assume transaction(T) supports itemset(X). In other words, T can meet X.

Definition 1.5 Define support(X) as the percentage of transactions(records) in D that contains itemset X.

Definition 1.6 Here X C_ I, Y I and X n Y= # , define implication formula X 3 Y as production rule based on the transactional database(D).

Definition 1.7 Given supp(X U Y)=S, we assume the support of rule X 3 Y in D(transactiona1 database) is S. Here is the expression  supp(X 3 Y)=S  0-7803-7865-2/031$17.00 02003 IEEE     Definition 1.8 If C% of D(transactiona1 database) can support both X and Y, we assume the confidence of rule X =j Y is C in D. Here is the expression  conf(X, Y)=C Definition 1.9 Let X 3 Y be the rule in D, and  supp(X U Y)=S, conf(X, Y)=C. Here is the expression of an association rule.

(X 3 y, s, c) In order to identify meaningful association rules, we  introduce minimum support and minimum confidence. The first limen is the minimum support that meets users? request. And the second is the minimum confidence that meets users? request.

Definition 1.10 The number of items in itemset X is k..

Define k as the length of X.

Definitionl.11 We call itemset L a frequent itemset if support(^) 2 minimum-support.

The purpose of mining association rules is to dig out the frequent association rules whose confidence and support is not less than the scheduled limen from transactional database D.

We can solve the mining problem in two steps.

(1) Dig out all the frequent itemsets from transactional database  D.

(2) Compute the requisite frequent association rules from  frequent itemsets. We must discover all the subsets of frequency itemset A. Note that subset a 4 . If supp(A)/supp(a) 2 minimum-confidence, then we find association rule a =2  (A-a). Here supp(A)/supp(a) is the confidence of rule a 3 (A-a).

In fact, the first step is the most difficult in the mining process. After discovering all the frequent itemsets, it is easy to compute the relative association rules. Many mining algorithm are advanced to solve the first step.

Algorithm Apriori is the most effective among these mining algorithms. The mining algorithms are based on the theory that any subset of a frequent itemsets also be a frequent itemset.

The algorithm starts with finding the frequent itemsets of length 1 ,  then iteratively computes frequent itemsets of length k + 1 from those of length k. This process terminates when there are no new frequent itemsets generated. It then proceeds to compute rules that satisfy the minimum confidence requirement.

The data structure for a frequent itemset has a row (ail) vector that records the transactions in which the itemset is contained. When a length k candidate itemset Ck is generated by joining two length k -1 frequent itemsets E 1 2 k-I  and lk - l  , the row vector of ck is simply the bitwise AND product of the row vectors of I k-l and 1 .

Function apriori_gen(Zk.,) performs to compute ck . The support of Ck can be calculated easily by counting the IS  in  I  its row vector, instead of scanning the database. There is also no need to perform the prune step in the candidate generation function. Since most @re-processed) audit data files are small enough, this implementation works well in our application domain.

Function aprioriAen(1 k- ,  ) is composed connection part and prune part. First it generates candidate itemset Ck, the program code is shown below  Insert into Ck select P.iteml, P.item2;.....P.itemk.1, q.itemk.l   I 2 from 1 k-1 p, 1k-I 4  of the  where P.iteml =q.itemk.l, - - e  P.itemk.z= q.itemk.2, P.itemk. I= q.itemk. I  Then delete all the items that contains one of length k-1 frequent itemsets from the candidate itemset Ckon the basis of the theory that any subset of a frequent itemsets also be a frequent itemset.

1.2 Mining episode rules  Episode is a set of sequential transactions in a given period. The support of frequent episode is not less than minimum-support. The problem of finding frequent episodes based on minimal occurrences was introduced in Reference 6.  Briefly, given an event database D where each transaction is associated with a timestamp, an interval [tl, t2] is the sequence of transactions that starts from timestamp tl and ends at t2. The width of the interval is defined as t2-tl. Given an itemset A in D, an interval is a minimal occur-rence of A if it contains A and none of its proper sub-intervals contains A .  Define support(X) as the ratio between the number of minimum occurrences that contain itemset X and the number of records in D. A frequent episode rule is the expression  X, Y 3 Z, [confidence, support, window] Here X, Y and Z are itemsets in D. Support(X U Y U  Z) is the support of the rule, and support(X U Y U Z)/support(XUY) is the confidence, window is the internal.

The width of each of the occurrences must be less than window. A serial episode rule has the additional constraint that X ,  Y and Z must occur in transactions in partial time order , i.e., 2 follows Y and Y follows X. This rule expresses sequential patterns between audit records. In an internal(window), after the occurrence of X and Y we assume that the probability of occurrence of Z is C and the probability of occurrence of this sequential patterns is S.

When minimum-confidence , minimum-support and window are known, algorithm for episode rules can compute all frequent episode rules.

Our implementation of the frequent episode algorithm      service telnet ftp  utilized the data structures and library functions of the association rule algorithm. Here any subset of a frequent itemset must also be a frequent itemset since each interval that contains the itemset as well as contains all of its subsets. We can therefore also start with finding the frequent episodes of length 2,then length 3, etc. Instead of finding correlations across attributes, we are looking for correlations across records. The row vector is now used as the intemal vector where each pair of adjacent is the pair of boundaries of an interval. A temporal join function that considers minimal and non-overlapping occurrences is applied to create the interval vector of a candidate length k itemset from the two interval vectors of two length k- 1 frequent itemsets.

src-host dst-host src-bytes dst-bytes flag A B 100 2000 SF C B 200 300 SF  2 Improvement on the basic algorithm  smtp IB  DHP(direct hashing and pruning) is an extended algorithm of Aprion. It is the most efficient among all unparallel algorithms. All the algorithms implement a method named level-wise approximate mining that extracts frequent items through scanning the transactional database repeatedly. However, the spending for scanning the database twice is so lavish that the efficiency of the algorithm debases. Therefore, we take the interestingness into account and improve on the basic algorithm.

Using the minimum support and confidence values to output only the ?statistically significant? patterns, the basic  D 250 300 SF I  algorithms implicitly measure the interestingness (i.e., relevancy) of patterns by their support and confidence values, without regard to any available prior domain knowledge. That is, assume Z is the interestingness measure of a pattem p, then  I @ )  =f(support@), confidence@)) where f is some ranking function. We propose here to  incorporate schema-level information into the interestingness measures. Assume ZA is a measure on whether a pattern p contains the specified important (i.e., ?interesting?) attributes, our extended interestingness measure is  le@) = fe(IA(p), f(support(p). confidence@))) =fe(IA@), I@>)  where fe  is a ranking function that first considers the attributes in the pattem, then the support and confidence values. For efficiency, we use interestingness lA as item constraints, i.e., conditions, dying candidate itemset generation. So we can generate rules in relation to intrusion detection.

telnet ( A  2.1 Using axis attributes  D 200 12100 SF  There is a partial ?order of importance? among the attributes of an audit record.

Some attributes are essential in describing the data, while others only provide auxiliary information. Consider the audit data of network connections shown in Table 1  smtp I B  Table 1  C 200 300 SF  timestamp  http I D  1.1  A I 200 0 REJ  2.0 2.3 3.4 3.7 5.2  Here each record (row) describes a network connection. A network connection can be uniquely identified by  < timestamp,src host, src port, dst host, sewice that is, the combination of its start time, source host,  source port, destination host, and service (destination port). These are the essential attributes when describing network data. We argue that the ?relevant? association rules should describe patternsrelated to the essential attributes. Patterns that include only the unessential attributes are normally ?irrelevant?. For example, the basic association rules algorithm may generate rules such as  src bytes = 200 --+ Jug = SF, [0.6, 0.21  These rules are not useful and to some degree are misleading. There is no intuition for the association between the number of bytes from the source, src bytes, and the normal status (flag = SF) of the connection.

We call the essential attribute axis attribute when they are used as a form of item constraint in the association rules algorithm. During candidate generation, an item set must contain value(s) of the axis attribute. We consider the correlations among non-axis attributes as not interesting. In other words,  1 if p contains axis attributes  0 Otherwise      In practice, we need not designate all essential attributes as the axis attributes. For example, some network analysis tasks require statistics about various network services while others may require the patterns related to the hosts. We can use service as the axis attribute to compute the association rules that describe the patterns related to the services of the connections.

It is even more important to use the axis attribute to constrain the item generation for frequent episodes. The basic algorithm can generate serial episode rules that contain only the ?unimportant? attribute values. For example  src bytes = 200, src bytes = 200 =3 dst bytes = 300, src bytes = 200, [0.4, 0.2, 2s]  Note that here each attribute value , e.g., src bytes =  200, is from a different connection record. To make matter worse, if the support of an association rule on non-axis attributes, A 3 B, is high then there will be a large number of ?useless? serial episode rules  2.2 Using reference attributes  Another interesting characteristic of system audit data is that some attributes can be references of other attributes. These reference attributes normally cany information about some ?subject?, and other attributes describe the ?actions? that refer to the same ?subject?.

Consider the log of visits to a Web site, as shown in Table 2.

Table 2  Here action and request are the ?actions? taken by the ?subject?, remote host. We see that for a number of remote hosts, each of them makes the same sequence of requests: ?/images?, ?/images? and ?/shuttle/missions/sts-71?. It is important to use the ?subject? as a reference when finding such frequent sequential ?action? patterns because the ?actions? from different ?subjects? are normally irrelevant. This kind of sequential patterns can be represented as  (subject = X, action = a). (subject = X, action = b) =3 (subject = x action = c), [c, s, w]  Note that within each occurrence of the pattern, the action values refer to the same subject, yet the actual subject value may not be given in the rule since any particular subject value may not be frequent with regard to the entire dataset. In other words, subject is simply a reference (or a variable).

To avoid having a huge amount of ??useless?? episode rules, we extended the basic frequent episodes algorithm to compute frequent sequential patterns in two phases:  (1 )  It finds the frequent associations using the axis attribute.

( 2 )  It generates the frequent serial patterns from these associations.

That is, for the first phase, find all unique attribute values in the database ; for the second phase, when forming  an episode, an additional condition is that, within its minimal occurrences,? the records covered by its constituent itemsets have the same value(s). In other words,  1 if the itemsets ofp refer to the same axis attribute  0 otherwise  ZA@) =  We get an episode rule, as follows (service=smtp,src-bytes=2OO,dst-bytes=30O,flag=SF), (service=telnet,flag=SF) (service=http,src~bytes=200),[0.2,0.1,2~] We in effect have combined the associations among  attributes and the sequential patterns among the records into a single rule. This rule formalism not only eliminates irrelevant patterns, it also provides rich and useful information about the audit data.

3 Analysis  We adjust the basic algorithm on the basis of discussed above. The main part of the algorithm keeps changeless. Function apriorisen is modified . The program code is shown below.

Insert into Ck select P.iteml, P.item2;..--.P.itemk -,, q.itemk.l  where ( IA(P) # 0) and (P.iteml =q.iteml) and (P.itemz =q.itemt)  1 2 from 1 k-1 p, Zk-1 4  and . . .and(P.itemk-2= q.itemk.2) , and( P.itemk-, 4 q.itemk.1)  That is, at first use interestingness as item constraints , i.e., conditions, during candidate itemset generation. This way can lessen the amount of candidate itemsets greatly.

4 Closing remarks  This paper advanced an extended mining algorithm for frequent patterns in intrusion detection. This algorithm use extended ?interestingness? measures on pattems that include information on the attributes, instead of just support and confidence values. These measures are used in our extended algorithms as various forms of item constraints in the mining process. We described in detail these extensions, namely, using axis attributes to compute relevant associations, using reference attributes to compute valid event episode pattems. Therefore, the efficiency of algorithm improves greatly.

5 Acknowledgements  This subject is supported by National Natural Science Foundation of Chinawo. 60273075).

