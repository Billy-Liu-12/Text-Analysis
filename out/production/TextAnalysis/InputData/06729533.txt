Mining Statistically Significant Sequential Patterns

Abstract?Recent developments in the frequent pattern mining framework uses additional measures of interest to reduce the set of discovered patterns. We introduce a rigorous and efficient approach to mine statistically significant, unexpected patterns in sequences of itemsets. The proposed methodology is based on a null model for sequences and on a multiple testing procedure to extract patterns of interest. Experiments on sequences of replays of a video game demonstrate the scalability and the efficiency of the method to discover unexpected game strategies.

Keywords?Sequential pattern; null model; significance test;

I. INTRODUCTION  Sequential pattern mining [1] is an important data mining task with many real applications. Most of the existing stud- ies, such as [14], [23], [33], [6], [25], focused on efficient algorithms and effective pattern representations. In the existing work, absolute or relative frequency (also known as support) is used as the sole criterion in selecting frequent patterns. While frequency often serves as a good preliminary filter to remove noise patterns of very low popularity, in many applications, one has to find relevant patterns whose interestingness is defined in a statistical way, and cannot be specified using only a support threshold. A pattern of high frequency may not be interesting if it is statistically expectable from other patterns.

At the same time, a pattern of low frequency may be interesting if it is statistically unexpected. Since a low support threshold often leads to a huge number of patterns, asking a user to select from patterns extracted using a low support threshold is overwhelming and impractical. This is a problem common not to only sequential patterns, but to frequent patterns in general.

To echo this challenge, several recent studies [20], [30], [3], [12] try to find patterns (i.e., itemsets or sequences) using some alternative interestingness measures or sampling representative patterns. A general idea, which is a framework of finding unexpected patterns, is to extract patterns whose characteristic on a given measure, such as the frequency, or more rarely the length [29], strongly deviates from its expected value under a null model. The frequency of a pattern is considered as a random variable, whose distribution under the null model has to be calculated or approximated. Then, the significance of the pattern is assessed through a statistical test that compares the expected frequency under the null model to the observed frequency. One of the key-points of this family of approaches is to choose an appropriate null model. It will ideally be a trade-off between adjustment to the data and simplicity: the model should capture some characteristics of the data, to integrate prior knowledge, without overfitting, to allow for relevant patterns discovery. A simple model, with low-order  dependency, often results in faster computations and clear interpretation of the unexpected patterns.

Specifically, Gallo et al. [9], Ha?ma?la?inen and Nyka?nen [13], and Mampaey et al. [19] extracted itemsets of unexpected frequencies in non-sequential data. On sequence data, finding over-represented substrings in strings has been extensively studied in statistics, with interesting applications in biology, as these patterns were found to coincide with binding sites in DNA sequences [24]. Here, every element in a string is a simple item (no itemsets). When itemsets are allowed in sequence data, the order dimension introduces serious complexity in computing the expected frequency, since a pattern can have numerous and overlapping occurrences, and sequences can have different lengths. To the best of our knowledge, Gwadera and Crestani [12] gave the only approach that extends the unexpected pattern finding framework to sequence data, which ranks subsequences of itemsets with respect to their significance given a null model. Their model is a mixture model obtained by conditioning on the length of the sequence combined with a maximal entropy model. This approach, while theoretically sound, may become inefficient with a large set of items, and does not include a complete statistical testing procedure for global significance.

In this paper, we develop a rigorous and efficient frame- work to mine statistically significant, unexpected patterns in sequences of itemsets. We make several contributions.

First, we develop an approach that allows us to avoid considering overlapping occurrences or conditioning on the length of the sequence. This method, which is combinatorial in nature, permits the development of an elegant dynamic computation procedure based on the expected frequency of the prefix of each pattern, and therefore considerably speeds up the computations. Second, we investigate the theoretical properties of the expected frequency under the null model and relate them to a very well-known concise representation, closed sequential patterns [33], to obtain a more parsimonious and less redundant set of patterns. In addition, we provide and discuss multiple procedures to test for the significance of the extracted sequences combined with correction procedures to control the error rate at a global level. Last, we evaluate our approach on synthetic and real large-scale sequence data.

We discuss in length the use of our algorithm SigSpan in an electronic-sport use case. We mine several large sequence bases of replays of Starcraft 2, a real-time strategy video game (Blizzard Entertainment). Our assessment shows an effective and scalable method. Helped by professional gamers, our framework is able to detect unexpected game strategies.

DOI 10.1109/ICDM.2013.124     The rest of the paper is organized as follows. Section II reviews the related work. Section III briefly reviews the preliminaries needed in our development. Section IV intro- duces our null model. In Sections IV,V and VI, we discuss the computation of the expected support of a sequence, the significance tests and the algorithmic details. An experimental study is reported in Section VII. We conclude our work and discuss several interesting directions in Section VIII.



II. RELATED WORK  A number of works have explored various notions of sta- tistical significance for itemsets and have proposed novel and efficient methods for their extraction. In [13], an approach is developed to mine for statistically significant association rules, under an independence assumption between the condition and the consequence of the rule. The expected frequency for the condition and consequence is derived analytically, based on the marginal frequencies of the dataset. In [9], the authors use two null models to calculate the expected frequencies. The first is an independence model, where items are mutually indepen- dent and their probabilities of occurrence are their observed marginal relative frequencies. The second one is obtained by considering a distinct distribution for each transaction, where each of the items has the same transaction-dependent probability of occurrence. This second model consider large transactions as less interesting, because two such transactions are likely to share items by chance. As the expected frequency is anti-monotone (the expected frequency of an itemset is greater than the expected frequencies of its subsets), the au- thors only consider closed itemsets (itemsets having no super- set with the same observed frequency). They then introduce the MINI algorithm to mine for non-redundant unexpected itemsets, by penalizing itemsets overlapping with itemsets with the highest p-values. An alternative to the independence model is the maximum entropy model used in [21]. A related framework is developed in [19], where unexpected itemsets given a maximum entropy model are iteratively used to update the model, resulting in a large coverage of the database and minimal redundancy between the itemsets.

In statistics, mining for over-represented patterns in sequences of items has been mainly developed for applications in biology, to find binding sites in DNA [24]. Because of the structure of DNA, where coding regions are usually made of consecutive bases, the patterns considered in these approaches are usually words (consecutive items), and the frequence considered is the total number of occurrences along the sequence. Under a Markov model, in [24] and [27], expected frequencies are computed given an exhaustive statistic for the model, in [22], by means of a convenient sequence associated to each word, and in [7], through generating functions. Note that [7] also considers words formed of items with variable-length gaps between them, the so-called ?hidden patterns?. In [26], the objective is to find specific patterns called structured motifs of unexpected frequencies in a set of DNA sequences given a Markov model. Structured motifs are formed of two boxes of consecutive nucleotides separated by a gap of variable length, but bounded. The probability of occurrence for one such motif at a given position is approximated by considering the past up to a fixed order. [34] extend this last work by using the inclusion-exclusion principle, but only for structured motifs with a fixed length gap. In both approaches, the probabilities of  occurrence are then compared to observed frequencies through a binomial test, as all sequences of the base are of the same length.

In the data mining community, a similar approach was developed by [11] to determine interesting frequent episodes in event sequences, i.e. subsequences with variable length gaps in sequences of items. Under an independence model, the authors use a sliding window to count all possible occurrences of an episode. Because of the dependency between overlapping occurrences, significance bounds for the frequency are set by using an asymptotic approximation for the variance and the Central Limit Theorem. In [16], the interest of subsequences is assessed by the difference between their observed and predicted frequencies, instead of the associated p-values. The most promising subsequences are then used to update a hidden Markov model through dynamic programming. Other solutions to reduce the set of frequent patterns to a smaller set of infor- mative patterns include Minimum Description Length (MDL)- based approaches [18], [31]. Both approaches are conceived as a framework to summarize data.

To the best of our knowledge, the only approach that extends the null model based pattern extraction framework to sequences of itemsets is [12]. In this work, the null model is obtained by combining two models at different levels: itemset-wise and sequence-wise. The sequence-wise model is a mixture model obtained by conditioning on the length of the sequence, as a subsequence is more likely to occur in a longer sequence. The weights of the mixture are determined by the proportion of sequences of the same length in the dataset.

For all the elements of the mixture, the authors use the same independence model as in their previous work [11], replacing items with itemsets. For the itemset-wise model, the authors choose a maximal entropy model, like in [21], as it makes mimimal assumptions about dependency between items, while setting the probability of an empty itemset to zero, unlike a simple independence model. The General Iterative Scaling Algorithm [4] can be used to calculate the expected frequencies of the itemsets under the maximum entropy model. The expected frequency of a sequential pattern is then obtained by combining the frequencies of its itemsets using the sequence- wise model. Finally, sequential patterns are ranked by a two- step algorithm which first searches for frequent patterns, then for each pattern, computes the expected frequency and uses the z-score for ranking unexpected patterns. As previously described in [19], the main difficulty of this approach is in the inference of an itemset?s probability which is infeasible to do for a large number of items. In fact, the author of [28] shows that querying the maximum entropy model is PP-hard.



III. PRELIMINARIES  Let I = {?1, ?2, . . . , ?m} be a finite set of items. An itemset X is a non-empty subset of I. A sequence s over I is an ordered list ?s1, . . . , s??, where ? > 0, si is an itemset for 1 ? i ? ?. ? is called the length of the sequence s, denoted by |s|= ?. Please note that the length of a sequence is the number of itemsets in it, instead of the number of item occurrences.

We denote by T(I) the set of all possible sequences over

I. A sequence database D of size n over I is a set of n sequences over I. Note that the sequences in a database can     s1 {a,b,c} {a,b} {b} s2 {a} {a,c} {a} s3 {a,b} {b,c}  TABLE I: A sequence database.

X1 X2 X3 X11 X12 X13 X21 X22 X23 X31 X32 X33  s1 1 1 1 1 1 0 0 1 0 s2 1 0 0 1 0 1 1 0 0 s3 1 1 0 0 1 1  TABLE II: The Bernoulli binary representation of the sequence database in Table I.

have different lengths. We denote by ?max the length of the longest sequence of D, that is, ?max = maxs?D{|s|}.

Definition 1 (Subsequence): A sequence s = ?s1, . . . , sk? is a subsequence of sequence s? = ?s?1, . . . , s?k??, denoted by s ? s? if k ? k? and there exist 1 ? r1 < r2 < ? ? ? < rk ? k? such that sj ? s?rj for all 1 ? j ? k. We also say that s is contained in s?, s? supports s. We call s? a supersequence of s. The greater prefix of a sequence s of size k denotes the subsequence ?s1 . . . sk?1?.

Definition 2 (Support and frequency): The support of a sequence s in a database D, denoted by Support(s,D) is the number of sequences in D that support s, that is,  Support(s,D) = |{s? | s? ? D, s ? s?}| (1)  The frequency of s, denoted by Freq(s,D) = Support(s,D)n , is the relative support.

We often omit the sequence database D if it is clear from context.

Definition 3 (Sequential pattern mining): Given a mini- mal frequency threshold 0 < minFreq ? 1, the problem of sequential pattern mining is to find all sequences s such that Support(s,D) ? minFreq. Every sequence s such that Support(s,D) ? minFreq is called a sequential pattern.

We also say s is to be frequent.

Definition 4 (Frequent closed sequence): A sequential pattern s is a frequent closed sequence if there does not exist a proper supersequence s? such that s ? s? and Support(s,D) = Support(s?,D). Frequent closed sequences directly provide the frequency of their subsequences and no additional processing is needed to extract this information.

This concept of closure can be seen as a lossless compression of frequent patterns.

We illustrate definitions in the following running example.

Example 1: Consider the sequence database D in Table I on the alphabet I = {a, b, c}. The size of D is n = 3. The longest sequence length is ?max = 3. Sequence ?{a, b}{b}? is contained in both s1 and s3, but not in s2.

Therefore, Support(?{a, b}{b}?) = 2. The greater prefix of ?{a, b}{b}? is ?{a, b}?. Given minFreq = 23 , ?{a, b}{b}? is a sequential pattern. The set of frequent closed sequences is {?{a}? , ?{c}? , ?{a}{c}? , ?{a, b}{b}? , ?{a, c}{a}?}.

The anti-monotonicity is an important property of sequen- tial patterns.

Property 3.1 (Anti-monotonicity): For any sequences s and s? such that s ? s?, Support(s) ? Support(s?). If s is infrequent, so is s?.



IV. A NULL MODEL  Let X = ?X1, X2, . . .? be a stationary stochastic process of random vectors, where each vector  Xi = (Xi1, . . . , Xim) (2)  contains m = |I| random variables of Bernoulli distribution (one for each item). This model corresponds to the unex- pected sequential pattern framework introduced and discussed in Section I. Then, each sequence s ? D, written under a binary representation, is an independent realization x of X , of maximal length ?max.

Example 2: Table II shows the Bernoulli binary represen- tation of the database D in Table I. For example, for i = 1, 2, 3, variable Xi1 indicates the presence or absence of item a in the corresponding itemset.

This binary representation is more convenient from a sta- tistical perspective. The null model for sequences of itemsets is then naturally decomposed into a model for items in each itemset, and a model for itemsets in sequence.

We choose a global independence model. We assume that itemsets X1, X2, . . . , X? are independent and identically distributed (i.i.d.). Therefore,  P (X) = P (X1)P (X2) ? ? ?P (X?) (3) We further assume that items are also independent. Thus,  P (Xi) = P (Xi1) ? ? ?P (Xim) (4) for all i = 1, . . . , ?. The indicator random variables Xi1, . . . , Xim are independent Bernoulli distributed, of param- eter p1, . . . , pm, the marginal observed probabilities of each item in the dataset.

Using the independence models for itemsets is not new, Gallo et al. [10] and Gionis et al. [9] successfully apply it.

Such an independence model allows useful interpretation as it can help identifying frequent co-occurrences of items that cannot be explained by their marginal frequencies. However, under independence assumptions, the null itemset has proba- bility (1? p1)? ...? (1? pm), a non-null quantity unless one of the items is present in each itemset. The maximum entropy (or log-linear) model [12] relaxes this constraint at the expense of much more costly computation. However, under the log- linear model, only one entry is removed from the sample space, with minimal effect on the remaining probabilities. Therefore, we choose the independance model which also respects the marginal frequencies of items and has a lower complexity.



V. EXPECTED FREQUENCY  Let x = ?x1 . . . xk? = ?(x11, . . . , x1m) . . . (xk1, . . . , xkm)? be a sequential pattern of k itemsets, under the binary repre- sentation. For example, if I = {a, b, c}, sequential pattern     X1 X2 X3 X4  x1 x2 . .

x1 x  c 2 x2 .

x1 x c 2 x  c 2 x2  xc1 x1 x2 .

xc1 x1 x  c 2 x2  xc1 x c 1 x1 x2  TABLE III: Possible positions of itemsets of x in S.

?{a, b}{b}? can be rewritten as ?(1, 1, 0)(0, 1, 0)?. Denote by ?xj = P (Xjr = xjr, r = 1, . . . ,m, xjr = 1) (5)  the probability of itemset xj under the null model, for all j ? {1, . . . , k}. For example, the event ?occurrence of itemset {a, b}? means that either {a, b} or {a, b, c} was observed, and the associated probability is ?{1,1,0} = P (X.1 = 1, X.2 = 1).

Example 3: Using the marginal frequencies of items in the dataset of Table II, under the independence model, we have P (a) = ?{1,0,0} = P (X.1 = 1) = 34 , as the item a appears in 6 itemsets out of 8, P (b) = P (X.2 = 1) = 58 and P (c) = P (X.3 = 1) =  8 . Therefore P ({a, b, c}) = ?{1,1,1} = 45256 .

The expected frequency of sequential pattern x in a single sequence, given the model defined above, is  p?(x) = P (? Xi1 , . . . , Xik , 1 ? i1 < . . . < ik ? ?, x1 ? Xi1 . . . , xk ? Xik) , (6)  where xj ? Xij if Xijr = 1 for all r ? {1, . . . ,m} such that xjr = 1. In other words, probability p?(x) is the probability that there exist k ordered itemsets in sequence X containing the itemsets of x. It depends on ?, since the longer the sequence, the greater the chance it supports x.

When the probability of all itemsets under the null model is known, computing the expected frequency p?(x) is a com- binatorial problem, where we have to enumerate all possible occurrences of x in a sequence, while adjusting for multiple occurrences in a same sequence. We adopt a counting strategy where all mutually exclusive occurrences of x are listed according to the first occurrence of each of its itemsets.

To fix the ideas, we describe this enumeration for a sequential pattern x = ?x1x2? of length 2 and a random sequence X = ?X1 ? ? ?X4? of length 4, such that X supports x. For each itemset xj , we denote by xcj the set of all possible itemsets that do not contain xj . Note that this set includes itemsets that contain some items of xj but not all of them.

We use a meta character ?.? to represent the set of all possible itemsets. Then X can be written as one and only one of the lines of Table III. The first possible configuration is that X1 supports x1 and X2 supports x2 (the first line of Table 2).

Note that x1 and x2 may occur further in the sequence. The next possible configuration is that X1 supports x1, X2 does not support x2 but X3 does (the second line). This case does not overlap with the previous one. The last possible case is that X2 and X3 do not support x2, but X4 does (the third line). A similar technique is used when the first occurrence of x1 is in X2 (the fourth and fifth lines) and in X3 (the sixth line). Thus, all possible occurrences are counted, without redundancy.

This procedure is easily extended to the general case of a pattern of length k and a sequence of length ?, where ? ? k.

First, note that if ? < k, p?(x) = 0. Then, for 1 ? j ? k, let qxj (i) be the probability that the first occurrence of itemset xj is at the ith position of the sequence. Then,  ?? ?  qxj (1) = P (xj ? X1) , qxj (i) = P (xj ? Xi, xj ? Xi? , 1 ? i? < i) , for 2 ? i ? ?.

(7)  Using the notation defined in Equation (5), we have{ qxj (i) =  ( 1? ?xj  )i?1 ?xj ,  for 1 ? i ? ?. (8)  By stationarity and independence of the stochastic process, qxj (i) is also the probability that xj first occurs at the i  th  position after x?j , for any itemset x ? j . Note that for each itemset,  this quantity can be computed by recurrence, since for i ? 2, qxj (i) =  ( 1? ?xj  ) qxj (i? 1). (9)  Example 4: From Table II, q{1,1,1}(1) = ?{1,1,1} = 45256 ? 0.18, q{1,1,1}(2) = (1??{1,1,1})?{1,1,1} = (1? 45256 )? 45256 ? 0.14, and q{1,1,1}(3) = (1? ?{1,1,1})q{1,1,1}(2) ? 0.12.

Listing all possible occurrences, the probability for x to occur in S is then  (10)p?(x) = ?  i1?I1,i2?I2,...,ik?Ik qx1(i1)qx2(i2) . . . qxk(ik),  where Ij = {1, 2, . . . , ?+ j ? k ? 1? ij?1} is the set of all possible positions for itemset xj . As first occurrences events are mutually exclusive, there are no compensating terms in this sum, unlike the inclusion-exclusion formula. This is the major advantage of the counting strategy presented above.

Example 5: The probability of occurrence of ?{a, b, c}{a, b}? in a sequence of length ? = 3 can be calculated as follows. First, we note that ?{1,1,1} = 45256 ? 0.18 and ?{1,1,0} = 1532 ? 0.47. Then q{1,1,1}(2) = (1 ? 45256 ) ? 45256 = 949565536 ? 0.14 and q{1,1,0}(2) = (1? 15/32)? 1532 = 2551024 ? 0.25. Therefore,  p3({1, 1, 1}{1, 1, 0}) = q{1,1,1}(1)q{1,1,0}(1) +q{1,1,1}(1)q{1,1,0}(2) +q{1,1,1}(2)q{1,1,0}(1)  ? 0.18? 0.47 + 0.18? 0.25 +0.14? 0.47 ? 0.20. (11)  The probability of occurrence p?(x) of each pattern x under the independence model can be calculated explicitly using Equation (10).

In the next section, we describe a dynamic strategy that calculates the support of any pattern given the support of its greater prefix, to reduce computational cost.



VI. SIGSPAN: AN ALGORITHM FOR UNEXPECTED SEQUENTIAL PATTERNS EXTRACTION  In this section, we propose an algorithm that integrates dynamic expected support computations and comparisons to observed support to extract patterns of unexpected frequency.

A. Dynamic Programming  For x = ?x1 . . . xk?, let Qx(i) be the probability that the first occurrence of x ends at the ith position of the sequence.

Let Qx be the vector of length ? ? k + 1 such that Qx = (Qx(i))|x|?i??. Then,  p?(x) =  ?? i=|x|  Qx(i), (12)  by reordering the terms of sum (10).

Example 6: The probability that the first occurrence of ?{a, b, c}{a, b}? ends at position 2 is  Q{1,1,1}{1,1,0}(2) = q{1,1,1}(1)q{1,1,0}(1). (13)  The probability that it ends at position 3 is  Q{1,1,1}{1,1,0}(3) = q{1,1,1}(1)q{1,1,0}(2) +q{1,1,1}(2)q{1,1,0}(1). (14)  Summing these two quantities gives (11)  We now turn to the computation of Qx(i), for all 1 ? i ? ?? k + 1: ? if k = 1: then x is reduced to a single itemset and  Qx = (qx(i))1?i?? is obtained as in (8).

? if 1 < k ? ?: then if x? denotes ?x1...xk?1?, the largest prefix of x, then for k ? i ? ?,  Qx(i) = i?1?  j=|x|?1 Qx?(j)qxk(i? j), (15)  which allows for using previous calculations for greater prefix x?. Therefore, the expected support computation framework is easily integrated in the prefix tree structure for frequent patterns extraction of Section III.

Probability (15) is calculated as follows: let  Mx? =  ? ?????  Qx?(|x| ? 1) Qx?(|x|) ? ? ? Qx?(?? 1) 0 Qx?(|x| ? 1) ? ? ? Qx?(?? 2) 0 0 ? ? ? Qx?(?? 3) ...

...

. . .

...

0 0 ? ? ? Qx?(|x| ? 1)  ? ????? .

(16)  Then, Qx is obtained as the matrix product of this matrix and the vector  (qxk((?? |x|+ 1)), ..., qxk(2), qxk(1))T (17)  Given the support of its greater suffix, the calculation of the expected support of x is reduced to its two last itemsets? positions enumeration, resulting in a complexity in O(?2).

Replacing ? with the maximal length ?max, and noting that p?(x) is obtained as a partial sum on the terms of p?max(x) for all ? ? ?max, the final complexity of computing p?(x) for all ? ? {1, ..., ?max} is O(?2max) for each x.

The expected frequencies are then compared to observed frequencies to extract unexpected sequential patterns.

B. Statistically Significant Patterns  For a sequential pattern x, and i = 1, ..., n, let Zix be the Bernoulli random variable such that:{  Zix = 1 if the i th sequence supports x,  Zix = 0 if not.

(18)  If all sequences have the same length ?, Zix = 1 with the probability p?(x) for all i and Support(x) is the sum of these i.i.d. random variables. The expected support of x under the null model is n ? p?(x) = E (Support(x)), and Support(x) has a binomial distribution.

In the general case, however, sequences of D have dif- ferent lengths and Support(x) is a sum of independent, non identically distributed variables of Bernoulli distribution B(p?i(x)), and of expectation E(Support(x)) =  ?n i=1 p?i(x).

The exact distribution of Support(x) can be calculated with a simple procedure of quadratic complexity. Instead of the exact distribution, we use Hoeffding?s concentration inequality ([15]) for sums of bounded, independent random variables:  (19)P {Support(x) ? Supportobs(x)} ? exp(? 2  n (Supportobs(x)? E(Support(x)))2),  This gives an upper bound for the p-value associated to the sequential pattern x. The drawback of this method is that the bound is not tight for small-variance random variables. If we fix a critical threshold ?, if  (20)Supportobs(x)? E(Support(x)) ? ? ?n log ?,  then x is statistically over-represented at level ?. For example, a data analyst may consider that sequences are significant if Supportobs(x)?E(Support(x)) ? 1000 in a data set of size 100, 000. Then the analyst needs to choose ? such that ? ? 1e20 .

As all frequent patterns are tested simultaneously, we apply a correction procedure for multiple testing. The Bonferroni correction controls the family-wise error rate by performing individual tests with a threshold of ?/N , where N is the number of tests. As Bonferroni correction is known to be very conservative, more recent and powerful methodologies propose to control the false discovery rate (FDR), i.e. the expected pro- portion of false positives. The Benjamini-Hochberg-Yekutieli procedure [2] allows for controlling the FDR at level ? for dependent tests by finding the largest q such that  ?(q) ? q N  ?N q?=1  q?  ?, (21)  where ?(1), ..., ?(N) are the N ordered p-values for all frequent patterns. If we use Hoeffding?s approximation, we have a sequence of bounds ?(1), ..., ?(N) from (19) such that ?(q) ? ?(q) for all q = 1, ..., N , and replacing ?(q) with ?(q) in (21) still allows for controlling the FDR. As N is only known once all frequent patterns are discovered, both corrections are applied a posteriori.

Using the difference between the observed support and the expected one has a bias due to the scale of the observed     support. To overcome the issue, an alternative measure is to use the deviation in percentage, that is,  Supportobs(x)? E(Support(x)) E(Support(x))  . (22)  Algorithm 1: SigSpan Data: A database DB, a minimal support minSupp and  a threshold ? Result: SF , a collection of unexpected sequential  patterns  SF ? ?;1 Compute probabilities of first occurrence qxk(i);2  /* mine frequent closed sequences */ CF ? ClosSpan(DB,minSupp);3 /* test if the closed sequences are  significantly over-represented */ for ?s ? CF do4  /* Using dynamic programming */ es := ComputeExpectedSupport(s)5 if s is significant then6  /* significance with Bonferroni test: Support(s)? es ?  ? ? |DB|2 log ?|CF|, or  FDR tests (see Equation 21) */  SF ? SF ? s7 end8  end9 return SF10  C. Closed Unexpected Sequential Patterns  Most of the algorithms for discovering sequential patterns use the antimonotonicity property of the frequence described in Section III. But the measure of interest defined in (20) is not anti-monotone: a supersequence of a non over-represented sequential pattern can be over-represented. However,  Lemma 6.1: The expected support is antimonotone.

Proof: For all ? ? 2, let x be a sequential pattern of length greater than 2, and x? its greater prefix. We have defined a probabilistic framework, where the event ?x occurs in sequence S? is included in the event ?x? occurs in S?, therefore, p?(x) ? p?(x?).

This property means that we can use the subset of closed unexpected sequential patterns as a summary of all the unex- pected sequential patterns, without losing information: let us suppose that for a sequential pattern x,  Supportobs(x?) = Supportobs(x), (23)  where x? is the greater prefix of x. As we are interested in over-represented sequential patterns, we also suppose that  (24)Supportobs(x) > E(Support(x?)) ? E(Support(x)).

Then,  (25)Supportobs(x?)? E(Support(x?)) ? Supportobs(x)? E(Support(x)).

Therefore, we can restrict the search to the set of closed sequential patterns, as they are always more interesting in the sense of (20) than their subsequences.

Algorithm 1 lists the different steps of SigSpan in order to extract unexpected sequential patterns, given a minimum support minSupp, and a threshold ? for the p-values. The algorithm starts by computing the probabilities of first oc- currence qxk then it calls the CloSpan algorithm to mine frequent closed sequences. Then, SigSpan does a depth-first traversal of the prefix tree of closed sequential patterns. For each pattern s, its support according to the model is computed using the support of its greater prefix. Therefore, we need to store the vector Qs? of the greater suffix of s. However, these vectors are discarded as we progress further on the tree. The different tests discussed in the previous section are evaluated for each closed sequence and if a sequence is significantly over-represented, it is stored in SF and outputted at the end.



VII. EXPERIMENTS  In this section, we report an extensive empirical evaluation of our algorithm SigSpan using real and synthetic datasets. All experiments are performed on a 1.8 GHz Intel Core i5 with 8 GB main memory running Mac OS X 10.7.5. We started from the original C++ version of CloSpan to implement the algorithm SigSpan (compiled with g++ and -03 optimization).

All the mathematical computations were processed using the uBlas Library1. The source code and the data sets are available at http://www.loria.fr/?raissi.

A. Synthetic Dataset  Finding injected patterns hidden in a generated dataset.

In order to assess the significance of our results, we check how SigSpan recognizes the unexpected sequence we introduced in a synthetic dataset on an alphabet of 1000 items.

In this synthetic data set the items? probabilities distributions are generated following a Beta distribution and the length of the sequences follows a Poisson distribution to allow a maximal of flexibility in the datasets generation process2. We then introduce ?{a, b, c}{a, b, c}? in 60% of the sequences.

The independence assumption of the null model means that it should account for the high supports of items a, b, and c, but not for their co-occurrence in the pattern ?{a, b, c}{a, b, c}?.

For each threshold, true positives are the number of sequences including ?{a, b, c}{a, b, c}? that are considered unexpected by SigSpan, false positives are other unexpected patterns, false negatives are patterns including ?{a, b, c}{a, b, c}? which are not outlined by the algorithm, and true negatives are the rest of the patterns. We draw a ROC curve on Figure 1 for different minimal frequency thresholds, with critical thresholds ? ranging from 10?100 to 10?1, using our algorithm. This  1http://www.boost.org/libs/numeric/ 2Due to space limitations, only the experiments with |D|= 100000, ? =  0.001, ? = 2 and ? = 7 are reported. The data generator along with some other datasets is provided with the SigSpan downloadable implementation.

0.2  0.4  0.6  0.8   0 0.2 0.4 0.6 0.8 1  Tr ue  p os  iti ve  ra te  False positive rate  60% 50% 40%  random  Fig. 1: ROC curve  D at  a  |D |  |I |  l m a x  l a v g  m a x (|s  i |)  a v g (|s  i |)  PvP 6,668 1,161 57 11,47 17 2,13 PvT 18,754 3,656 70 19,03 23 2,48 PvZ 22,784 3,749 94 19,59 28 2,66 TvT 7,457 2,202 67 20,73 18 2,56 TvZ 23,638 4,493 75 22,52 29 2,55 ZvZ 9,554 1,690 60 14,18 16 2,08  TABLE IV: Real world datasets  curves represent the fraction of true positives with respect to the fraction of false positives and provide a measure for precision and recall. The best possible prediction would be the point (0, 1) corresponding to no false positives and no false negatives. Figure 1 shows that the performance of the algorithm depends on the minimal frequency threshold, but not on ?, as all points overlap for all curves. All three curves are above the random guess line, thus demonstrating the accuracy of the algorithm. As expected, the curve for minFreq = 60% performs better.

B. The StarCraft II Use Case  Context Opponents modeling aims at simulating player behaviors. This study domain started in the seventies and focused on chess strategies [8] before shifting in recent years to video games, one of the most lucrative leisure activities.

Models of players behaviors are used within automated agents, to personalize a game environment for a specific user, to predict future actions of an opponent, etc. To tackle some of these problems, an important sub-problem is the automatic discovery of strategic patterns in a competitive video game environment [5]. We study one of the most competitive real- time strategy games (RTS) [17], StarCraft II (Blizzard Enter- tainment, 2010) which has its own world-wide players ranking system (ELO) and annual world cup competition series (WCS) with a US$1.6 millions prize pool for the year 2013 (among others premier tournaments).

A game of StarCraft II involves two players. Each player chooses a faction among Zerg (Z), Protoss (P) and Terran (T). As such, there are 6 different possible match-ups with different strategies of game. During a game, two players are  battling on a map (aerial view), controlling buildings and units to gather supply, build an army with the final goal of winning by destroying the opponent?s forces3. Such actions (training, building, moving, attacking) are done in real-time, see Figure 2 for an example. Each faction (Z,P,T) allows different units and buildings with distinctive weaknesses and strengths following a rock-paper-scissors principle. A strategy is hidden in large sequences of actions generated by players and called replays.

Our goal is to mine significantly over-represented patterns to detect surprising strategies in this game.

Fig. 2: An example of build-actions realized during a game.

Processing raw data We collected more that 300,000 replays from specialized repositories, and filtered them to keep the 90, 678 replays from professional players. We focus only on buildings order sequences (such as in Figure 2), the pillar elements of a strategy on which unit production type and attack timings depend [32]. All raw replays are partitioned into 6 datasets, one for each match-up. For each dataset, we derive one sequence for each replay: a sequence is seen as an ordered list of itemsets of buildings from B, where an itemset denotes a window of time of 30 seconds (the ordering of two items over a small period of time is insignificant in terms of strategy, hence the use of itemsets). Furthermore, both players actions are mixed in a sequence. Indeed, actions of both players are strongly correlated. We choose to distinguish both players within a sequence by adding the winner information. As such, an itemset of a sequence is a set of (B ? {winner, looser}).

Finally, since timings are very important in a strategy, we encode the window?s identifier for each item. As such, an itemset is a set of I = N ? B ? {winner, looser}. The first three itemsets built from the sample replay in Figure 2 are ?{(1, pylon, looser)}, {(2, gateway, looser), (2, spanningpool, winner)}, {(3, assimilator, looser), (3, pylon, looser)}}? . Table IV summarizes the main characteristics of the different datasets. For example, the dataset labeled ?PvT? encodes all sequences played by a Protoss player against a Terran player. The resulting datasets are described in Table IV. The sequences length distribution is given by Figure 3.

C. Quantitative Experiments  Performance and scalability Figures 5 and 4(a) give the execution times of SigSpan with different minimum supports.

The closed pattern extraction time is separated from the statistical computation time. In general, we observed that the statistical model computation is moderate except for extremely low supports (? 1%). The scalability tests reported by Figure 7 show that the total execution time grows linearly with the size of the data set when replicated (i.e. the same set of closed or  3http://en.wikipedia.org/wiki/Real-time?strategy             0  5  10  15  20  25  30  35  40  45  50  55  Sequence length  PvP ZvZ TvT          0  5  10  15  20  25  30  35  40  45  50  55  Sequence length  TvZ PvZ PvT  Fig. 3: Sequence length distribution of the StarCraft II datasets (sequence count in Y-axis).

significant patterns). For example, Figure 7(f) shows that the 200 times replicated ZvZ data set (2 millions of sequences) can be mined in less than 500 seconds with minSupp = 0.5%.

Closed vs. significant counts when fixing ? and vary- ing minimal frequency For each dataset, we set ? so that Supportobs(x) ? E(Support(x)) ? 500 in equation (20).

Figures 6 and 4 (c,d,e,f) reports the number of closed pat- terns and of significant closed patterns with several minimum supports. As expected, the number of closed patterns grows exponentially. Interestingly the number of closed significant patterns is several order of magnitude lower with low supports, and stays approximately constant. This is a very important remark as this collection of patterns can still be humanly processed and analyzed.

Closed vs. significant counts when fixing minimal frequency and varying ?. We now re-iterate the same experiment, this time fixing the frequency for our datasets4. Figure 8 reports these experiments. The number of frequent closed sequences remains constant. Both the number of unexpected sequences and ? decrease exponentially together.

D. Qualitative Interpretation  To assess the quality of the extracted patterns representing players tactics, we contacted a game expert. Our qualitative experiment is two-fold. Firstly, we show that classic and well- known strategy openings in StarCraft II are expected patterns.

Second, we show that some risky strategies are unexpected with high potentials for professional players.

Known Strategies, Expected Sequences Like in chess, Starcraft II openings are generally well-known and codified (given a name). We would like to observe if known openings in StarCraft II are translated as expected sequences (i.e., least unexpected sequences). We started with the TvZ dataset and extracted all closed patterns with minimum support higher than 0.4%, i.e. about 700, 000 patterns. We sorted them according to the ratio given by Equation (22): the closer the ratio to zero, the more expected the pattern and finally selected the top-100 expected patterns. According to the expert, most of these patterns (with a ratio lower than 0.5) are either known openings or standard actions the players need to do to develop normally their economy (i.e. non significant). The first pattern is ?{(5, winner,Barracks)}, {(7, winner,Barracks)}? is  4Due to space limitations, only ZvZ dataset results are reported.

a classical opening where the player favors a special kind of unit composition that appears only in the ?Barracks? building.

Another known opening is translated and found in the pattern ?{(6, looser, ComCenter)}, {(7, looser, ComCenter)}? (ranked 22). This strategy is called ?double expand?.

Consider now another scenario: the Terran (T) player has build a bunker (a defensive building). What is he expected to do next? We kept the 30, 414 patterns containing at least two items among one involving the bunker building. The most expected pattern is the following ?{(6, winner,Bunker)}, {(7, winner, CommandCenter)}? with an observed support of 113, and expected support of 67 (out of 22, 784 sequences in the original dataset). For the expert, this pattern clearly indicates either a so-called defensive-expand strategy in the early stage of the game, or an enemy rush (an all-in strategy that is deadly if not countered, equivalent to a blitz attack in chess on the f7 or f2 square early in the game, putting the king in check5).

In those examples, elements of the patterns are expected to occur together.

Surprising Strategies, Unexpected Sequences The so-called DT-rush is a risky Protoss gambling strategy that leads often to a win of the game when the opponent does not detected it (although it requires a strong technology coupled with long minutes of vulnerability). It is generally well detected and expected by the opponent given the information she gets by scouting the map. However, in the PvZ matchup, Protoss tends to use it, since Zerg needs some time to react to this piece of information. We mined the PvZ dataset accordingly with a minimum support of 0.04%. We kept only the patterns involving the building unlocking these invisible units (DT?s). On 14 remaining patterns, 12 patterns are of size 1 (only one itemset), and involve the target building in late state of the game with expected values almost iden- tical to the observed values. This is not surprising since all technologies are generally unlocked in mid/end game.

The 2 last patterns however, involve the target building in the early stage of the game leading to a victory. Both pat- terns ?{(6, winner, Canon)}, {(11, winner,DarkS)}? and ?{(11, winner, Concil)}, {(12, winner,DarkS)}? have an observed support of 94 while an expected support of 3 and 17. They however do not correspond to known timings of the DT rush (which occurs earlier). These patterns highlight an unexpected strategy that the expert calls ?delayed DT rush?.

5http://en.wikipedia.org/wiki/Fast chess             0.01  0.1  1  10  100  Minimum supports (%)  CloSpan Model+check  (a) Performance       1e+06  0.01  0.1  1  10  100  Minimum supports (%)  Closed Significant  (b) Count    1e-36 1e-27 1e-18 1e-09  1  Significant Closed  (c) 50%    1e-36 1e-27 1e-18 1e-09  1  Significant Closed  (d) 25%          1e-36 1e-27 1e-18 1e-09  1  Significant Closed  (e) 10%           1e-36 1e-27 1e-18 1e-09  1  Significant Closed  (f) 5%  Fig. 4: Synthetic dataset    0.1  1  10  Minimum supports (%)  CloSpan Model+check  (a) PvP         0.1  1  10  Minimum supports (%)  CloSpan Model+check  (b) TvT    0.1  1  10  Minimum supports (%)  CloSpan Model+check  (c) ZvZ        0.1  1  10  Minimum supports (%)  CloSpan Model+check  (d) PvZ    0.1  1  10  Minimum supports (%)  CloSpan Model+check  (e) PvT          0.1  1  10  Minimum supports (%)  CloSpan Model+check  (f) TvZ  Fig. 5: Performance (Y-axis in seconds)      1e+06  1e+07  0.1  1  10  Minimum supports (%)  Closed Significant  (a) PvP      1e+06  1e+07  0.1  1  10  Minimum supports (%)  Closed Significant  (b) TvT       1e+06  1e+07  0.1  1  10  Minimum supports (%)  Closed Significant  (c) ZvZ      1e+06  1e+07  0.1 1 10  Minimum supports (%)  Closed Significant  (d) PvZ      1e+06  1e+07  0.1 1 10  Minimum supports (%)  Closed Significant  (e) PvT      1e+06  1e+07  0.1 1 10  Minimum supports (%)  Closed Significant  (f) TvZ  Fig. 6: Pattern count   0.5  1.5  2.5  3.5   0  50  100  150  200  n  minFreq=0.2  (a) 20%         0  50  100  150  200  n  minFreq=0.1  (b) 10%         0  50  100  150  200  n  minFreq=0.05  (c) 5%        0  50  100  150  200  n  minFreq=0.025  (d) 2.5%        0  50  100  150  200  n  minFreq=0.01  (e) 1%         0  50  100  150  200  n  minFreq=0.05  (f) 0.05%  Fig. 7: Scalability test with dataset ZvZ replicated n times with different minimum support (Y-axis in seconds)    1e-240  1e-120  Significant Closed  (a) 5%    1e-240  1e-120  Significant Closed  (b) 4%      1e-240  1e-120  Significant Closed  (c) 3%       1e-240  1e-120  Significant Closed  (d) 2%       1e-120  Significant Closed  (e) 1.5%       1e+06  1e-33  1e-22  1e-11  1  Significant Closed  (f) 0.3%  Fig. 8: Closed vs. significant pattern counts for different minimum support (subfigures) and ? parameters (X-axis).

We contrasted this result with the PvT dataset: Terran is the only faction that has a counter to the DT-rush early in the game, with almost no technology required. For a minimum support of 0.04%, the one and only pattern that involves the target building is ?{9, looser,DarkS}? with an observed support of 95 and expected support of 94.7. This corroborates the fact given by the expert stating that DT-rush is not a viable strategy in the PvT matchup, but could be sometimes a surprising winning strategy in PvZ when delayed.



VIII. CONCLUSION  In this paper, we developed a new approach for extract- ing statistically significant sequential patterns. The proposed methodology is based on a null model for sequences of data and a combinatorial enumeration process of possible positions for the itemset of each subsequence. A statistical test is performed to determine over-represented sequential patterns, and an upper bound on the associated p-value is provided for greater efficiency. This probabilistic framework is seamlessly integrated to the tree structure of traditional frequent patterns mining algorithms. Correction procedures account for multiplicity of tests. Experimental results show the relevance of discovered patterns.

