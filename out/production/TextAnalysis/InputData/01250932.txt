A High-Performance Distributed Algorithm for Mining Association Rules ?

Abstract  We present a new distributed association rule mining (D-ARM) algorithm that demonstrates superlinear speedup with the number of computing nodes. The algorithm is the first D-ARM algorithm to perform a single scan over the database. As such, its performance is unmatched by any previous algorithm. Scale-up experiments over stan- dard synthetic benchmarks demonstrate stable run time re- gardless of the number of computers. Theoretical analysis reveals a tighter bound on error probability than the one shown in the corresponding sequential algorithm.

1 Introduction  The economic value of data mining is today well es- tablished. Most large organizations regularly practice data mining techniques. One of the most popular techniques is association rule mining (ARM), which is the automatic dis- covery of pairs of element sets that tend to appear together in a common context. An example would be to discover that the purchase of certain items (say tomatoes and lettuce) in a supermarket transaction usually implies that another set of items (salad dressing) is also bought in that same transac- tion.

Like other data mining techniques that must process  enormous databases, ARM is inherently disk-I/O intensive.

These I/O costs can be reduced in two ways: by reduc- ing the number of times the database needs to be scanned, or through parallelization, by partitioning the database be- tween several machines which then perform a distributed ARM (D-ARM) algorithm. In recent years much progress has been made in both directions.

The main task of every ARM algorithm is to discover the  sets of items that frequently appear together ? the frequent itemsets. The number of database scans required for the task has been reduced from a number equal to the size of  ? This work was supported in part by Microsoft Academic Foundation  and by THE ISRAEL SCIENCE FOUNDATION founded by the Israel Academy of Sciences and Humanities.

the largest itemset in Apriori [3], to typically just a single scan in modern ARM algorithms such as Sampling and DIC [17, 5].

Much progress has also been made in parallelized algo-  rithms. With these, the architecture of the parallel system plays a key role. For instance, many algorithms were pro- posed which take advantage of the fast interconnect, or the shared memory, of parallel computers. The latest develop- ment with these is [18], in which each process makes just two passes over its portion of the database.

Parallel computers are, however, very costly. Hence,  although these algorithms were shown to scale up to 128 processors, few organizations can afford to spend such re- sources on data mining. The alternative is distributed al- gorithms, which can be run on cheap clusters of standard, off-the-shelf PCs. Algorithms suitable for such systems include the CD and FDM algorithms [2, 6], both paral- lelized versions of Apriori, which were published shortly after it was described. However, while clusters may eas- ily and cheaply be scaled to hundreds of machines, these algorithms were shown not to scale well [15]. The DDM algorithm [15], which overcomes this scalability problem, was recently described. Unfortunately, all the D-ARM al- gorithms for share-nothing machines scan the database as many times as Apriori. Since many business databases con- tain large frequent itemsets (long patterns), these algorithms are not competitive with DIC and Sampling.

In this work we present a parallelized version of the Sam-  pling algorithm, called D-Sampling. The algorithm is in- tended for clusters of share-nothing machines. The main obstacle of this parallelization, that of achieving a coher- ent view of the distributed sample at reasonable communi- cation costs, was overcome using ideas taken from DDM.

Our distributed algorithm scans the database once, just like the Sampling algorithm, and is thus more efficient than any D-ARM algorithm known today. Not only does this algo- rithm divide the disk-I/O costs of the single scan by parti- tioning the database among several machines, but also uses the combined memory to linearly increase the size of the sample. This increase further improves the performance of the algorithm because the safety margin required in Sam-      pling decreases when the (global) sample size increases.

Extensive experiments on standard synthetic bench-  marks show that D-Sampling is superior to previous algo- rithms in every way. When compared to Sampling ? one of the best sequential algorithms known today ? it offers super- linear speedup. When compared to FDM, it improves run- time by orders of magnitude. Finally, on scalability tests, an increase in both the number of computing nodes and the size of the database does not degrade D-Sampling performance.

FDM, on the other hand, suffers performance degradation in these tests.

The rest of this paper is structured as follows: We con-  clude this section with some notations and a formal def- inition of the D-ARM problem. In the next section we present relevant previous work. Section 3 describes the D- Sampling algorithm, and section 4 provides the required sta- tistical background. Section 5 describes the experiments we conducted to verify D-Sampling performance. We conclude with some open research problems in section 6.

1.1 Notation and Problem Definition  Let ? ? ? ? ? ? ? ? ? ? ? ? be the items in a certain domain.

An itemset is a subset of ? . A transaction ? is also a subset of ? which is associated with a unique transaction identifier ? ? ? ? . A database ? ? is a list of such transactions. Let? ? ? ? ? ? ? ? ? ? ? ? ? ? " $ be a partition of ? ? into% parts. Let & be a list of transactions which were sam- pled uniformly from ? ? , and let & ? ? & ? & ? ? ? & " $ be the partition of & induced by ? ? . For any itemset + and any group of transactions , , & . / / 1 3 ? 5 + , 9 is the number of transactions in , which contain all the items of + and: 3 ; < 5 + , 9 ? A B C C E F G I J L M NO M O . We call  : 3 ; < Q + ? ? S U the local frequency of + in partition ? and : 3 ; < 5 + ? ? 9 its global frequency; likewise, we call  : 3 ; < Q + & S U the esti- mated local frequency of + in partition ? and : 3 ; < 5 + & 9 its estimated global frequency.

For some frequency threshold Z \ ^ ? % : 3 ; < \ a , we  say that an itemset + is frequent in , if : 3 ; < 5 + , 9 c ^ ? % : 3 ; < and infrequent otherwise. If , is a sample, we say that + is estimated frequent or estimated infrequent.

If , is a partition, we say that + is locally frequent, and if , is the whole database, then + is globally frequent.

Hence an itemset may be estimated locally frequent in thed G e partition, globally infrequent, etc. The group of all itemsets with frequency above or equal to f 3 in , is calledh i  F j , k . The negative border of h i  F j , k is all those item-sets which are not themselves in h i F j , k but have all theirsubsets in h i F j , k . Finally, for a pair of globally frequentitemsets + and m such that + n m ? q , and some con- fidence threshold Z s ^ ? % u 1 % f \ a , we say the rule  + x m is confident if and only if : 3 ; < 5 + { m ? ? 9 c ^ ? % u 1 % f  : 3 ; < 5 + ? ? 9 .

Definition 1 Given a partitioned database ? ? , and given ^ ? % : 3 ; < and ^ ? % u 1 % f , the D-ARM problem is to find all the confident rules between frequent itemsets in ? ? .

2 Previous Work  Since its introduction in 1993 [1], the ARM problem has been studied intensively. Many algorithms, representing several different approaches, were suggested. Some algo- rithms, such as Apriori, Partition, DHP, DIC, and FP-growth [3, 14, 11, 5, 8], are bottom-up, starting from itemsets of size a and working up. Others, like Pincer-Search [10], use a hybrid approach, trying to guess large itemsets at an early stage. Most algorithms, including those cited above, adhere to the original problem definition, while others search for different kinds of rules [5, 16, 13].

Algorithms for the D-ARM problem usually can be seen  as parallelizations of sequential ARM algorithms. The CD, FDM, and DDM [2, 6, 15] algorithms parallelize Apriori [3], and PDM [12] parallelizes DHP [11]. The major dif- ference between parallel algorithms is in the architecture of the parallel machine. This may be shared memory as in the case of [18], distributed shared memory as in [9], or shared nothing as in [2, 6, 15].

One of the best sequential ARM algorithms ? Sampling  ? was presented in 1996 by Toivonen [17]. The idea be- hind Sampling is simple. A random sample of the database is used to predict all the frequent itemsets, which are then validated in a single database scan. Because this approach is probabilistic, and therefore fallible, not only the frequent itemsets are counted in the scan but also their negative bor- der. If the scan reveals that itemsets that were predicted to belong to the negative border are frequent then a second scan is performed to discover whether any superset of these itemsets is also frequent. To further reduce the chance of failure, Toivonen suggests that mining be performed using some ? 1 ? f 3 s ^ ? % : 3 ; < , and the results reported only if they pass the original ^ ? % : 3 ; < threshold. He also gives a heuristic which can be used to determine ? 1 ? f 3 . The cost of using ? 1 ? f 3 is an increase in the number of candidates.

The Sampling algorithm and the DIC algorithm (Brin 1997 [5]) are the only single-scan ARM algorithms known today.

The performance of the two is thus unrivaled by any other sequential ARM algorithm.

The algorithm presented here combines ideas from sev-  eral groups of algorithms. It first mines a sample of the database and then validates the result and can, thus, be seen as a parallelization of the Sampling algorithm [17]. The sample is stored in a vertical trie structure that resembles the one in [14, 4], and it is mined using modifications of the DDM [15] algorithm, which is Apriori-based.

3 D-Sampling Algorithm  All distributed ARM algorithms that have been pre- sented until now are Apriori based and thus require mul- tiple database scans. The reason why no distributed form of Sampling was suggested in the six years since its pre- sentation may lie in the communication complexity of the problem. As we have seen, the communication complexity of D-ARM algorithms is highly dependent on the number of candidates and on the noise level in the partitioned database.

When Sampling reduces the database through sampling and lowers the ? ? ? ? ? ? 	 threshold, it greatly increases both the number of candidates and the noise level. This may render a distributed algorithm useless.

This is the reason that the reduced communication com-  plexity of DDM seems to offer an opportunity. The main idea of D-Sampling is to utilize DDM to mine a dis- tributed sample using ? ? ? ? instead of ? ? ? ? ? ? 	 . After? ?  ? ? ?  ? ? ? ? has been identified, the partitioned database isscanned once in parallel to find the actual frequencies of? ? ? ?  ? ? ? ? ? and its negative border. Those frequencies canthen be collected and rules can be generated from itemsets  more frequent than ? ? ? ? ? ? 	 .

We added three modifications to this scheme. First, al-  though the given DDM is levelwise, here it is executed on a memory resident sample. Thus we could modify DDM to develop new itemsets on-the-fly and calculate their esti- mated frequency with no disk-I/O. Second, a new method for the reduction of ? ? ? ? ? ? 	 to ? ? ? ? yielded two ad- ditional benefits: it is not heuristic, i.e., our error bound is rigorous, and it produces many less candidates than the rig- orous method suggested previously. Third, after scanning the database, it would not be wise to just collect the fre- quencies of all candidates. Since these candidates were cal- culated according to the lowered threshold, few of them are expected to have frequencies above the original ? ? ? ? ? ? 	 .

Instead, we run DDM once more to decide which candi- dates are frequent and which are not. We call the modified algorithm D-Sampling (Algorithm 1).

3.1 Algorithm  D-Sampling begins by loading a sample into memory.

The sample is stored in a trie ? a lexicographic tree. This trie is the main data structure of D-Sampling and is accessed by all its subroutines. Each node of the trie stores, in addition to structural information (parents, descendants etc.), the list of ?  " s of those transactions that include the itemset as- sociated with this node. These lists are initialized from the sample for the first level of the trie; when a new trie node ? and itemset ? are developed, the ?  " lists of two of the parent nodes are intersected to create the ?  " list of the new node.

The first step of D-Sampling is to run a modification of DDM on the distributed sample. Then, in order to set ? ? ? ? , the algorithm enters a loop; in each cycle through the loop it calls another DDM derivative called M-Max to mine the next ? estimated-frequent itemsets. ? is a tun- able parameter we set to about % & & . After it finds those additional itemsets, D-Sampling reduces ? ? ? ? to the es- timated frequency of the least frequent one and re-estimates the error probability using a formula described in section 4.

When this probability drops below the required error prob- ability, the loop ends. Then D-Sampling creates the final candidate set ( by adding to ? ? ? ?  ? ? ? ? ? its negative bor-der.

Algorithm 1 D-Sampling For node ? out of ? Input:? ? ? ? ? ? 	 , ? ? ? ( ? ? ? , " / 0 , 1 , ? , 2 Output: The set of confident associations between globally frequent itemsets Main: Set 3 ? ? ? ? ? 5 % , ? ? ? ? 5 ? ? ? ? ? ? Load a sample ? 0 of size 1 from " / 0 into memory Initialize the trie with all the size- % itemsets and calculate their ?  " lists? ?  ? ? ?  ? ? ? ? 5 ? " " ? > ? ? ? ? ? ? 	 A  While 3 ? ? ? ? ? D 2 1.

? ? ? ?  ? ? ? ? ? 5  ? ? ? ?  ? ? ? ? ? F ? ? H I > ? A  2. Set ? ? ? ? to the frequency of the least frequent item- set in  ? ? ? ?  ? ? ? ? ?  3. Set 3 ? ? ? ? ? to the new error bound according to? ? ? ? ? ? 	 , ? ? ? ? and ? ? ? ? ?  ? ? ? ? Let ( be ? ? ? ?  ? ? ? ? ? F L ? N H P ? S ? / ? ? U ? ? V  ? ? ? ?  ? ? ? ? ? W  Scan the database and compute ? ? ? 	 V X Y " / 0 W for each X [( . Update the frequencies in the trie to the computed ones  Compute ? \ 0 ^ _ ? ` a ? " / ? by running? " " ? > ? ? ? ? ? ? 	 A , this time with the actual fre-  quencies If exists X [ ? \ 0 ^ _ ? ` a ? " / ? such that X f[  ? ? ? ?  ? ? ? ? ?(i.e., from negative border) report failureg ? ? h i ? 1 V ? \ 0 ^ _ ? ` a ? " / ? Y ? ? ? ( ? ? ? W  Once the candidate set is established, each partition of the database is scanned exactly once and in parallel, and the actual frequencies of each candidate are calculated. With these frequencies D-Sampling performs yet another round of the modified DDM. In this round the original ? ? ? ? ? ? is used; thus, unless there is a failure, this round should never develop a candidate which is outside the negative bor-     der. If indeed no failure occurs, then all frequent itemsets will be evaluated according to the actual frequencies which were found in the database scan. Hence, after this round it is known which of the candidates in ? are globally frequent and which are not. In this case, rules are generated from? ? ? ? ? 	 ?  ? ? ? ? using the known global frequencies.If an itemset belonging to the negative border of? ? ? ? ? 	 ? ? ? does turn out to be frequent, this means that D-Sampling has failed: a superset of that candidate, which was not counted, might also turn out to be frequent. In this case we suggest the same solution offered by Toivonen: to create a group of additional candidates that includes all combina- tions of anticipated and unanticipated frequent itemsets, and then perform an additional scan. The size of this group is limited by the number of anticipated frequent itemsets times the number of possible combinations of unanticipated fre- quent itemsets. Since failures are very rare events, and the probability of multiple failure is exponentially small, the additional scan will incur costs that are of the same scale as the first scan.

3.2 MDDM ? A Modified Distributed Decision Miner  The original DDM algorithm, as described in [15], is lev- elwise. When the database is small enough to fit into mem- ory, the levelwise structure of the algorithm becomes super- fluous. Modified Distributed Decision Miner, or MDDM (Algorithm 2), therefore starts by developing all the locally frequent candidates, regardless of their size. It then contin- ues to develop candidates whenever they are required, i.e., when all their subsets are assumed frequent (according to the local hypothesis - ? ) or when another node refers to the associated itemset.

The remaining steps in MDDM are the same as in DDM.

Each party looks for itemsets for which the global hypoth- esis and local hypothesis disagree and communicate their local counts to the rest of the parties. When no such item- set exists, the party passes (it can return to activity if new information arrives). If all of the parties pass, the algo- rithm terminates and the itemsets which are predicted to be frequent according to the public hypothesis ? are the esti- mated globally frequent ones. If a message is received for an itemset which has not yet been developed, it is developed on-the-fly and its local frequency is calculated.

3.3 M-Max Algorithm  The modified DDM algorithm identifies all itemsets with frequency above ? ? ?  ! # $ . D-Sampling, however, re- quires a further decrease in the frequency of itemsets which are included in the database scan. The reason for this, as we shall see in section 4, is that three parameters affect the  Algorithm 2Modified Distributed Decision Miner For node ? out of ? Input:% ! ? the target frequency Output:? ? 	 ? ? ?Definitions:  ? ' ( * ? ? , - ./ 1 3 4 5 7 88 ? / 88  ! # $ ' ( * ? / ,= ? = >  ./ ?1 3 4 5 7 88 ? / 88  ! # $ ' ( * ? ? ,= ? =  ? C ( F - HIJ IK M / 1 3 4 5 7  88 ? / 88  ! # $ ' ( * ? / , M / 1 3 4 5 7 = ? / = G C ( F S- UV W X Y # ! [ ? ] #  Main: Develop all the candidates which are more frequent than % !

according to ? Do_ Choose a candidate ( that was not yet chosen and for which either ? C ( F b % ! d ? ' ( * ? ? , or? ' ( * ? ? , b % ! d ? C ( F_ Broadcast i - l ? n C ( F *  ! # $ ' ( * ? ? , p_ If no such itemset exists broadcast q s t ] ] v  Until = ? t ] ] # n = - xy z all ( with ? C ( F } % !

Return y When node ? receives a message i from party  : 1. If i - q s t ] ] v insert  into ? t ] ] # n 2. Else i - l ? n C ( F *  ! # $ ' ( * ? / , p If  ? ? t ] ] # n remove  from ? t ] ] # n If ( was not developed then: develop it, set ? C ( F -U , Calculate ( ? X ? n ? ? ] X by intersecting the ? ? ? lists of two of ( ?s immediate subsets and set  ! # $ ' ( * ? ? , -? 5 ? ? ? ? ? ? ? ? ?? ? ? ? Insert  to ? C ( F Recalculate ? C ( F and ? ' ( * ? ? ,     chances for failure. These are the size of the sample ? , the size of the negative border, and the estimated frequency of the least frequent candidate. The first parameter is given, the second is a rather arbitrary value which we can calculate or bound, and the last parameter is the one we can control.

The frequency of the least frequent candidate can be con- trolled by reducing ? ? ? ? ? . However, this must be done with care: lowering the frequency threshold increases the number of candidates. This increase depends on the distri- bution of itemsets in the database and is therefore nondeter- ministic. The larger number of candidates affects the scan time: the more candidates you have, the more comparisons must be made per transaction. In a distributed setting, the number of candidates is also strongly tied to the communi- cation complexity of the algorithm.

To better control the reduction of ? ? ? ? ? , we propose another version of DDM called M-Max (Algorithm 3). M- Max increases the number of frequent itemsets by a given factor rather than decreasing the threshold value by an ar- bitrary value. Although worst case analysis shows that an increase of even one frequent itemset may require that any number of additional candidates be considered, the number of such candidates tends to remain small and roughly pro- portional to the number of additional frequent itemsets. We complement this algorithm with a new bound for the error (presented in section 4). The combined scheme is both rig- orous and economical in the number of candidates.

The M-Max algorithm is based on the inference that changing the ?  ? ? ? ? threshold to the ? -value of the - largest itemset1 every time an itemset is developed or a hy- pothesis value is changed will result in all parties agreeing on the most frequent itemsets when DDM terminates.

This is easy to prove. Take any final state of the modi- fied algorithm. The ? value of each itemset is equal in all parties; hence, the final ?  ? ? ? ? is equal in all parties as well. Now compare this state to the corresponding state under DDM, with the static ?  ? ? ? ? value set to the one finally agreed upon. The state attained by M-Max is also a valid final state for this DDM. Thus, by virtue of DDM cor- rectness, all parties must be in agreement on the same set of frequent itemsets.

As a stand-alone ARM algorithm, M-Max may be im- practical because a node may be required to refer to item- sets it has not yet developed. If the database is large, this would require an additional disk scan whenever new candi- dates are developed. Nevertheless, at the ? ? ? ? ? correction stage of D-Sampling, the database is the memory-resident sample. It is thus possible to evaluate the frequency of arbi- trary itemsets with no disk-I/O.

1 ? is used when the ? largest ? is zero.

Algorithm 3M-Max For node ? out of Input:? ? ? ? ? Output: The M most frequent itemsets not yet in  ? ? ? ? ? ? ? ?  Definitions: same as for algorithm 3.2 Let ! denote the initial size of ? ? ? ? ? ? ? ?  , ? ? % ? ? ? ? ?Main: Do  1. call ' ? * ? ? 2. Choose + that was not yet chosen and for which either? - + / 1 ? ? 3 5 6 + 8 ? ; < or 5 6 + 8 ? ; < 1 ? ? 3? - + / Broadcast > % @ ? B - + / 8 ? ? ? ? 6 + 8 ? ; < J  3. If no such itemset exists broadcast K M O ' ' Q Until  R 5 O ' ' ? B R % ?T U all + in the trie with ? - + / Y ? ? which are not in? ? ? ? ? ? ? ?  Return T When node ? receives a message > from party ] : 1. If > % K M O ' ' Q insert ] into 5 O ' ' ? B 2. Else > % @ ? B - + / 8 ? ? ? ? 6 + 8 ? c < J If ] e 5 O ' ' ? B remove ] from 5 O ' ' ? B If + was not developed then: develop it, set g - + / %i , Calculate + j * ? B ? ? ' * by intersecting the k m o lists of two of + ?s immediate subsets and set ? ? ? ? 6 + 8 ? ; < %p q r s ; u ? ; w s pp x z p Insert ] to g - + / Recalculate ? - + / and 5 6 + 8 ? ; < call ' ? * ? ?  procedure ' ? * ? ? : Do M times:{ Select the next most frequent itemset outside? ? ? ? ? ? ? ?  and develop its descendants if they havenot been developed yet  Set ? ? to the ? value of the last itemset selected. For item- sets with ? % } consider 5 instead.

4 Statistical Analysis  The M-Max subroutine requires that we estimate the probability of an error ? i.e., an itemset which is actually frequent but appears with frequency of less than the lowered frequency threshold in the sample. An over estimation may result in an unnecassary decrease in ? ? ? ? ? which would result in a larger than required number of candidates. Here we describe the probability bound we have used in our im- plementation which outperforms the naive Chernoff bound discussed in the original paper which described the Sam- pling algorithm.

Let ? ? ? ? be the frequency of some arbitrary item-  set ? in ? ? . Consider a random sample ? of size ? from? ? . We will assume that transactions in the sample are in- dependent. Hence, the number of rows in ? which contain? can be seen as a random variable, ? ? ? ? ? ? ? ? ? ? " .

The frequency of ? in ? transactions, # ? ? $ ? ' ? , is  an estimate for ? ? , which improves as ? increases. The best-known way to bound the chance that # ? ? will deviate from ? ? is with the Chernoff bound. We use a tighter bound for the case of binomial distributions (see Hagerup and Rub [7]):  ) ? ? , ? ? . # ? ? , 0 2 " 4 6 8 ? . ? ?? . # ? ? ; < > ?

@ A 8 ? ?# ? ? ; ?

@ A B D Lemma 1 Given a random uniform sample ? of N transac- tions from ? ? , a frequency threshold E ? ? F ? H I , the low- ered frequency threshold ? ? ? ? ? , and the negative border of  J K L N @ A O ? Q , denoted ? ? , the probability S @ T U K V A W thatany ? Y ? ? will have frequency larger than or equal toE ? ? F ? H I (hence causing failure) is bounded by: , ? ? , [ 6 8 ? . E ? ? F ? H I? . ? ? ? ? ? ; < >  K L N @ A 8 E ? ? F ? H I? ? ? ? ? ; K L N @ A B D  For any specific itemset in ? ? , the probability that this itemset will cause failure is the probability that its esti- mated frequency is below ? ? ? ? ? while its actual frequency is above E ? ? F ? H I . Substituting E ? ? F ? H I for ? ? and? ? ? ? ? for # ? ? , the bound gives us:  ) ? ? , F ? H I ? ? ? ? ? " . F ? H I ? ? ? ? " , 0 c " 4 6 8 ? . E ? ? F ? H I? . ? ? ? ? ? ; < >  K L N @ A 8 E ? ? F ? H I? ? ? ? ? ; K L N @ A B D  As for the entire ? ? : ) ? ? e ? Y ? ? h ? ? i ? ? # " 4 mn o D p ) ? ? ? ? i ? ? # " 4  1 2 4 8 15          D?Sampling Speedup  Number of Computers  S pe  ed up  T20.I6.D200M, MinFreq = 1% T10.I4.D375M, MinFreq = 0.5% T5.I2.D600M, MinFreq = 0.25% Linear Speedup  Figure 1. D-Sampling speedup.

, ? ? , 6 8 ? . E ? ? F ? H I? . ? ? ? ? ? ; < > K L N @ A 8 E ? ? F ? H I? ? ? ? ? ;  K L N @ A B D Since calculating the negative border is in itself a costly  process, we choose to relax this bound by substituting, w , , J K L N @ A O ? Q , for , ? ? , . Obviously, any itemset inJ K L N @ A O ? Q can only be extended by at most , w , items, andthus this relaxed bound holds.

Corollary 1 (Toivonen 1996) If none of the itemsets in the negative border caused failure, then no other itemset can cause failure.

Any other itemset ? outside J K L N @ A O ? Q and ? ? must in-clude a subset from ? ? . Hence its frequency must be less than or equal to the frequency of this subset. It follows that if the frequency of each itemset in ? ? is below E ? ? F ? H I , so is the frequency of ? .

5 Experiments  We carried out three sets of experiments. The first set tested D-Sampling to see how much faster it is to run the algorithm with the database split among ? machines than to run it on a single node. The second set compared D- Sampling, DDM and FDM on a range of E ? ? F ? H I values.

The last one checked scale-up: the change in runtime when the number of machines is increased together with the size of the database.

We ran our experiments on two clusters: the first clus-  ter, which was used for the first, second and fourth sets of experiments, consisted of 15 Pentium computers with dual 1.7GHz processors. Each of the computers had at least 1     0.25 0.5 0.75 1 1.25 1.5 1.75 2         T5.I2.D600M, N=15  MinFreq  Ti m  e (M  in ut  es )  FDM time DDM time D?Sampling time  0.25 0.5 0.75 1 1.25 1.5 1.75 2        N um  be r o  f S ca  ns  Figure 2. Runtime of D-Sampling, DDM, and FDM for varying MinFreq.

gigabyte of main memory. The computers were connected via an Ethernet-100 network. The second cluster, which we used for the scale-up experiments, was composed of 32 Pentium computers with a dual 500MHz processor. Each computer had 256 megabytes of memory. The second clus- ter was also connected via an Ethernet-100 network.

All of the experiments were performed  with synthetic databases produced by the standard gen tool ? ? ? ? ? ? 	 ? ? ? ? ? ? ? ? ? ? ?  ? ? ? ? ? ! ? ? ? ? # ? $ ! ? 	 ? ! ( ? ? ? ( , ? - . ? , ? . The databases were built with the same parameters which were used by Toivonen in [17]. The only change we made was to enlarge the databases to about 18 gigabytes each; had we used the original sizes, the whole database would fit, when partitioned, into the memory of the computers.

The database T5.I2.D600M has 600M transactions, each containing an average of five items, and patterns of length two. T10.I4.D375M and T20.I6.D200M follow the same encoding. When the database was to be partitioned, we divided it arbitrarily by writing transaction number 1 2 3 into the 1 2 3 4 $ partition.

5.1 Speedup Results  The speedup experiments were designed to demonstrate that parallelization works well for Sampling. We thus ran D-Sampling with $ 5 7 (with $ 5 7 , D-Sampling reverts to Sampling) on a large database. Then we tested how splitting the database between $ computers affects the algorithm?s performance.

As figure 1 shows, the basic speedup of D-Sampling is  slightly sublinear. However, when the number of candidates  1 4 8 12 16 20 24 28 32       D?Sampling Scale?up  Number of Computers  Ti m  e (m  in ut  es )  T5.I2.D1200M, MinFreq = 0.5% T10.I4.D750M, MinFreq = 0.75% T20.I6.D400M, MinFreq = 1.5%  Figure 3. D-Sampling scale-up.

is large, the speed-up becomes superlinear. This is because the global sample size increases with the number of comput- ers. This larger sample size translates into a higher ? ? ? ? ? value and thus to a smaller number of candidates than with  $ 5 7 .

5.2 Dependency on 9 ? $ ; ? ? -  The second set of experiments (figure 2) demonstrates the dependency of D-Sampling performance on 9 ? $ ; ? ? - , which determines the number and size of the candidates.

We compared the D-Sampling runtime to that of both DDM and FDM. D-Sampling turned out to be insensitive to the re- duction in 9 ? $ ; ? ? - ; its runtime increased by nomore than 50% across the whole range. On the other hand, the run- time of DDM and FDM increased rapidly as 9 ? $ ; ? ? - is decreased. This is because of the additional scans required as increasingly larger itemsets become frequent. Because it performs just one database scan, D-Sampling is expected to be superior to any levelwise D-ARM algorithm, just as Sampling is superior to all levelwise ARM algorithms.

5.3 Scale-up  The third set of tests was aimed at testing the scalability of D-Sampling. Here the partition size was fixed. We used a database of about 1.5 gigabytes on each computer. A scal- able algorithm should have the same runtime regardless of the number of computers.

D-Sampling creates the same communication load per  candidate as DDM. However, because it generates more     candidates, it uses more communication. As can be seen from the graphs in figure 3, D-Sampling is scalable in two of the tests. In fact, for mid-range numbers of computers, D- Sampling runs even faster than with ? ? ? ; this is due to the superlinear speed-up discussed earlier. The mild slowdown seen in figure 3c is due to the smaller average pattern size and the smaller number of candidates in T5.I2.D1200M.

The larger the number of candidates, the greater the saving in candidates when the number of computers increases. If there are enough large patterns, this saving will compensate for the increasing communication overhead. Such is not the case, however, with T5.I2.D1200M.

6 Conclusions and Future Research  We presented a new D-ARM algorithm that uses the communication efficiency of the DDM algorithm to par- allelize the single-scan Sampling algorithm. Experiments prove that the new algorithm has superlinear speedup and outperforms both FDM and DDM with any ? ? ? ? ? ? value. The exact improvement in relation to previous al- gorithms depends on the number of database scans they re- quire. Experiments demonstrate good scalability, provided the database scan is the major bottleneck of the algorithm.

Some open questions still remain. First, it would be in-  teresting to continue partitioning the database until every partition becomes memory resident. This approach may lead to a D-ARM algorithm that mines a database by load- ing it into the memory of large number of computers and then runs with no disk-I/O at all. Second, it would be inter- esting to have a parallelized version of the other single-scan ARM algorithm ? DIC ? on a share-nothing cluster, or of the two-scans partition algorithm. Finally, we feel that the full potential of the M-Max algorithm has not yet been re- alized; we intend to research additional applications for this algorithm.

