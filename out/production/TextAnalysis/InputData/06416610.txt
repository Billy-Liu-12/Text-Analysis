Prediction of Risk Score for Heart Disease using Associative Classification and  Hybrid Feature Subset Selection

Abstract? Medical data mining is the search for relationships and patterns within the medical data that could provide useful knowledge for effective medical diagnosis. Extracting useful information from these data bases can lead to discovery of rules for later diagnosis tools. Generally medical data bases are highly voluminous in nature. If a training data set contains irrelevant and redundant features classification may produce less accurate results. Feature selection as a pre-processing step in used to reduce dimensionality, removing irrelevant data and increasing accuracy and improves comprehensibility.

Associative classification is a recent and rewarding technique that applies the methodology of association into classification and achieves high classification accuracy. Most associative classification algorithms adopt exhaustive search algorithms like in Apriori, and generate huge no. of rules from which a set of high quality of rules are chosen to construct efficient classifier. Hence generating a small set of high quality rules to build classifier is a challenging task. Cardiovascular diseases are the leading cause of death globally and in India more deaths are due to CHD.Cardiovascular disease is an increasingly an important cause of death in Andhra Pradesh.

Hence there is an urgent need to develop a system to predict the heart disease of people. This paper discusses prediction of risk score for heart disease in Andhra Pradesh. We generated class association rules using feature subset selection. These generated rules will help physicians to predict the heart disease of a patient.

Keywords-Andhra Pradesh, Associative classification, Feature subset selection, Genetic search, Heart disease

I.  INTRODUCTION  Data mining is a multidisciplinary field used to extract knowledgeable information from huge amount of data. Data mining is an interdisciplinary field which includes areas like machine learning, statistics, pattern recognition, artificial intelligence and data visualization [1].Data mining has attracted a great deal of attention in the information industry due to wide availability of huge amounts of data and there is a  need for turning such data into useful information. The information gained can be used in various applications like market analysis, telecommunication and health care applications. Data mining techniques have been applied to a variety of medical domains to improve medical diagnosis.

Medical applications of data mining include prediction of  diabetic, and heart disease and discovery of relationships among clinical and pathological data.

Associative classification is a recent and rewarding technique that applies association rule mining into classification and achieves higher accuracy than traditional classification methods and many of the rules found by AC methods cannot be discovered by traditional classification algorithms. Associative classifications are fit to applications where the maximum accuracy is desired and assist the domain experts in their decisions [2].Coronary heart disease is an epidemic in India and one of the disease burden and deaths. Mortality data from registrar general of India show that cardiovascular disease are a majority cause of death now.CVD cause about 30%of deaths in rural areas[3].medical diagnosis is an important yet complicated task that needs to be executed accurately and efficiently.

The automation of medical diagnosis is extremely advantageous. A classification system can assist the physicians to examine a patient .The system can predict if the patient is likely to have a certain disease. Associative classification is better alternative for predictive analysis [4].Genetic algorithms have played a major role in many applications of engineering science, since they constitute a powerful tool for optimization. This paper proposes a new approach for prediction of heart disease using associative classification and hybrid feature subset selection. The rest of the paper is organized as follows: section 2 explains basic concepts of associative classification, feature subset selection methods and heart disease. Section 3 gives insight into related work. Section 4 describes our proposed method.

The last section presents experimental results followed by conclusion.



II.  BASIC CONCEPTS  In this we will present basic concepts of associative classification, Feature subset selection and heart disease.

A. Associative classification Associative rule discovery and classification are two main functionalities in data mining with the exception that association rule mining discovers associations between attribute values and classification predicts the class label in a data set. In the last few years, association rule mining has been successfully used to build accurate classifiers, which resulted in a new approach coming to life known as associative classification [5].Studies showed that associative classification often builds more accurate classifiers than traditional classification algorithms[6]. Associative classification involves 2 stages. First stage adopts association rule mining algorithms like Apriori [7] or FP growth to generate class association rules. For example CBA [5] method employs Apriori candidate generation and other classification algorithms such as CPAR [6] and CMAR [9] method adopts FP Growth [8] algorithm for rule generation. The rule generation step generates huge number of rules. Experimental results reported by [10] has shown that Class Based Association algorithm which adopts Apriori concept generates more than 80,000 rules for some data sets which leads to memory exception and other problems such as over fitting. If all the generated rules are used in the classifier then the accuracy of the classifier will be high but the process of classification will be slow and time consuming. Hence several rule pruning techniques are proposed to choose am optimal rule set. A classifier is of the form A1, A2, ---An B, where Ai is an attribute and B is a class. Rule item that satisfy minsup are called frequent rule items, while the rest are called infrequent rule items.Assocaitive classification is to collect rules in training data set D, organize them in a certain order to form a classifier. When provided an unknown sample, the classifier selects the rule in accordance with the order whose condition matches the objects and assigns class labels of the rule to it.

B. Feature subset selection  A feature or attribute or a variable refers to an aspects of the data. Features can be discrete, continuous, or nominal.

Generally features are of three types 1) Relevant 2) Irrelevant 3) Redundant. Feature subset selection has been an active and fruitful field of research and development for decades in pattern recognition, machine learning and data mining. It has proven in both theory and practice effective in enhancing learning efficiency, increasing predictive accuracy and reducing complexity of learned results.

Feature selection has found success in many applications such as text categorization, image retrieval, CRM and intrusion [11] and to improve predictive accuracy [12].The features which are chosen to represent a set of patterns may be large or redundant in some way. In such cases it may be good to reduce the features. This features called feature selection or dimensionality reduction. If the dimensionality  is large, it is necessary to have a large training set to get good classification accuracy. This is due to curse of dimensionality or peaking phenomenon. For a particular no.

of training patterns, up to a certain point the increase in dimensionality leads to improvement in the classification accuracy .But after that there is a reduction in the classification accuracy [13].Feature selection algorithms fall into two categories. The filter model and wrapper model, the filter model relies on the general characteristics of the training data to select some features without involving any learning algorithms. Whereas the wrapper model requires one predermined learning algorithm in feature selection and uses its performance to evaluate and determine which feature are selected. The filter model is usually chosen when the no. of features becomes very large due to its computational efficiency.

Figure1.  Feature subset selection In the following subsections we will discuss feature subset selection techniques like information gain, symmetrical uncertainty of attributes and genetic search.

1. Information gain and symmetrical uncertainty of attributes  Information gain measure is based on the information theoretical concept of entropy, a measure of the uncertainty of a random variable.

))((log)()( 2 i i  i xpxpXH ?=   And the entropy of X after observing values of another variable Y is defined as  = j i  ))j/yi(p(x2)logj/yip(x)jp(yH(X/Y)   Where P(xi)is the prior probabilities for all values of X and P(xi/yi) is the posterior probabilities of X given the values of Y,and information gain is given by  IG(X/Y) =H(X)-H(X/Y) According to this measure, a feature Y is regarded more correlated to feature X than to feature Z if  IG(X/Y)>IG (Z/Y)      Symmetry is a desired property for a measure of correlations between features, however IG is biased in favor of features with more values, and the values have to be normalized to ensure they are comparable and have same effect.

Symmetrical uncertainty is a measure which compensates for IG bias toward features with more values and normalizes its values to the range[0,1].value 1 indicates that value of one feature predicts the value of other and 0 indicates that X and Y are dependent[14].

SU(X, Y) =2[IG(X/Y)/H(X) H(Y)]  C. Genetic algorithm  Genetic algorithms have played a major role in many applications of the engineering science, since they constitute powerful tool for optimization. A simple genetic algorithm is a stochastic method that performs searching in global search spaces, depending on some probability values. For these reasons it has the ability to converge to the global minimum or maximum depending on the specific application and to skip possible local minima or maxima.

Basically there are three fundamental operators in GA.selection, crossover and mutation within chromosomes.

As in nature, each operator occurs with a certain probability.

There must be a fitness function to evaluate individual?s fitness. The fitness function is a very important component of the selection process since offspring for the next generation are determined by the fitness value of the present population. Selection is an operator applied to the current population in a manner similar to the one of the natural selection found in biological systems. The fittest individuals are promoted to the next population and poorer individuals are discarded. Crossover allows solutions to exchange information in the same way the living organism use in order to reproduce themselves. Mutation operator applied to an individual single bit of an individual binary string can be flipped with respect to a predefined probability.

Figure 2. Block Diagram of Simple Genetic Algorithm  D.  Heart disease  Heart disease or coronary artery disease is a form of CVD that affects men and women. Heart problems are either acquired at birth or later in life. Heart problems acquired later in life are due to 1) narrowing or blockage of the coronary artery that nourishes is the heart muscle as in angina and heart attack 2) compromise of the muscles and valves of the heart lung system, which leads to weakling of the heart muscle 3) compromise of the electrical activities of the heart. There are several risk factors for heart disease1)smoking  2)high blood pressure  3)cholesterol 4) obesity 5)diabetes 6)psychological and social factors 7)heredity 8)age 9)gender 10)family history 11)Ethnicity.

According to the world health statistics report 2008, compiled by WHO, mortality due to cardiac causes has overtaken mortality due to all cancers put together. In India alone, we have about 4200 sudden cardiac deaths per lakh deaths annually. By 2030, India will rank among the highest risk countries [15].and Andhra Pradesh is in risk of more deaths due to CHD.



III. RELATED WORK  Large no. of research work is carried out for medical diagnosis for various diseases. In our proposed approach we attempted to predict heart disease using feature subset selection and classification. Data mining techniques have been applied to a variety of medical domains to improve medical decision making [16].Importance of feature subset selection was discussed in [17].Enhanced prediction of heart disease with feature subset selection using genetic algorithm was proposed by M.Anbarasi et al[18].Sellappan et al developed an intelligent heart disease prediction system using decision tree, naive Bayesian neural networks[19].Feature selection using FCBF in type II Diabetes data bases was proposed by Sarojini bala Krishnan et al.[11].Knowledge discovery using associative classification for heart disease prediction was proposed by M.A.Jabbar et al[2].Matrix based association rule mining for heart disease prediction was proposed in [20].Genetic algorithm based heart disease prediction was proposed in[21].Maximum clique based weighted association rule mining was proposed by  M.A.Jabbar et al[22].Cluster based association rule mining for heart disease prediction was proposed in [23].In this paper we propose a new approach which combines feature subset selection and associative classification to predict Heart disease prediction in Andhra Pradesh state.

Initial population  Decode the population  Find fitnes Selectio  Crossover  Mutation  Replace  New population

IV. PROPOSED METHOD  In our proposed method we classify the heart disease data and prediction of disease by applying feature subset selection. Our approach prunes irrelevant, redundant attributes and generates compact rule set. These generated rules will be built as classifier. Accuracy of our method improves over other classification algorithms.

Algorithm  1. Select the common attributes by applying feature subset selection measures like information gain, SU, and genetic search.

a) SU(X, Y) =2[IG(X/Y)/H(X) H(Y)] b)  IG(X/Y) =H(X)-H(X/Y) c) Genetic search conditions  i)  Cross over probability: 0.6 ii) Mutation probability: 0.033 iii) Max.generation 20 iv) Population: 20 v) Method: 10 fold cross 1  2. Generate the class association rules using information centric attributes using training data set.

3. Build the classifier using generated class association rules.

4. Predict the rules on test data  5. Find the accuracy of the classifier.

6. Accuracy = Accuracy measures the ability of the classifier to correctly classify unlabelled data.

Accuracy=   Number of objects correctly Classified Total No. of objects in the test set.



V. RESULTS AND DISCUSSION Experiments on 14 data sets from UCI [24] data were conducted using 10 cross fold validation. A brief description about data sets is shown in table 1. Number of selected features for each feature subset selection algorithm has been tabulated in table 2, 3, 9. Table 5 and fig 3 shows the accuracy of various data sets. Figure 4 and table 6, 7 depict the comparison of accuracy on various data sets.

Table 1 Description of Various Data Sets                        Table 2. Attributes selected based on Genetic search                                  Sl no  Data set Instances Attribu tes  1 Contact lenses 24 5 2 weather 14 5 3 Multiplexer 3X8 100 12 4 zoo 101 18 5 Students data   498 6  6 Monks 1 data 124 7  7 Monks 2 data 169 7  8 Monks 3 data 122 7  9 Iris data 150 5 10 Haber man?s  survival data 306 4  11 Pima diabetes data 768 9 12 Mushroom data 8124 23 13 Breast cancer 286 10 14 Hayes roth 132 5  Sl no Data  set Attribute selected based on Genetic  Search 1 Contact lenses Tear production rate 2 weather Outlook, Humidity  3 Multiplexer 3X8 Chooser2  4 zoo Animal hair, feathers,milk, toothed, back bone  5 Students data  E3,E4,E5  6 Monks 1 data Attribute1, 5,4  7 Monks 2 data Attribute 4,5,6 8 Monks 3 data Attribute 2,5,6  9 Iris data Petal length, petal width  10 Haber man?s  survival data  No. Of positively auxiliary nodes  detected, patients year of operation  11 Pima diabetes data Plas,mass,pedi,age  12 Mushroom data  Odor,gill,stack,ring number, spore print  colour  13 Breast cancer Node caps,inv nodes,deg malig  14 Hayes Roth Age, marital status, education level      Table 3. Attributes selected based on Genetic search                                   Table 4. Heart disease data sets                        Table 5. Accuracy of various data sets                     Table 6. Comparison of accuracy for various Non medical data sets               Table 7. Comparison of accuracy for various Medical data sets           Table 8.feature subset selection for Heart data set.

Sl no Data set  Attribute selected based on SU  1 Contact lenses Tear production rate, astigmatism 2 weather Outlook, humidity  3 Multiplexer 3X8 Chooser2,output  7,output 5,output 0  4 zoo  Legs,milk,animal,to othed,eggs,hair,feat  hers backbone  5 Students data  E5,E4,E3  6 Monks 1 data Attribute 1,5,4,3 7 Monks 2 data Attribute 4,5 8 Monks 3 data Attribute 2,5  9 Iris data Petal width, Petal length  10 Haber man?s  survival data  No. Of positively auxiliary  nodes detected  11 Pima diabetes data Plas,mass,age,insu,  preg  12 Mushroom data  Odor, spore-print- color, stalk-surface-  above-ring, ring- type,gill-size, stalk- surface-below-ring,  gill-colour   13 Breast cancer Node  caps,degmalig,inv nodes,irradiant  14 Hayes Roth Age, marital status, education level  Sl.no Attribute Name 1 Age 2 Gender  3 BP Systolic 4 BP Diastolic 5 Thalach 6 Hypertension 7 LDL Cholesterol 8 HDL  Cholesterol 9 Serum Cholesterol 10 Serum Triglycerides 11 Rural/Urban  Dataset name  Feature subset by IG  Feature  subset by Genetic search  Feature subset by SU  Heart Disease Dataset for A.P  Gender Gender Gender  Sl.no Dataset Accuracy 1 Contact lenses 83 2 weather 92.85 3 Multiplexer 3X8 76 4 zoo 77.5 5 Students data    6 Monks 1 data 72 7 Monks 2 data 56 8 Monks 3 data 76 9 Iris data 85.71 10 Haber man?s  survival data 70.76 11 Pima diabetes data 78 12 Mushroom data 92 13 Breast cancer 76 14 Hayes Roth 71.18  DATA SET Our approach CBA C4.5  Contact lenses 83 72 83.33 weather 92.85 85 50  zoo 77.5 40 72.2  Iris data 85.71 93.3 95.3  Mushroom data 92 91.89 99.96  Hayes Roth 71.18 53.73 -  DATA SET Our approach Na?ve bayes C4.5  Pima diabetes data 78 76.3 75.5  Breast cancer 76 71.67 72.52 Heart disease data for A.P  95 65 77.5          Figure3.Accuracyof various data sets      Fig 4 comparison of accuracy for various data sets  Table 9. Attributes selected based on IG     Justification for using various parameters of GA  1. Cross over rate: Crossover rate should be high and about 60% is the best  2. Mutation rate should be very low. Best rates reported are about 0.5%-1%  3. Population size: very big population size usually does not improve performance of GA.good population size is about 20-30.

4. Selection: Basic roulette wheel selection can be used.

The results clearly depicts the relevant attributes are identified by various feature subset selection approaches have indeed improved classification accuracy. In few cases the classification accuracy reduced marginally. But most of the case, results shows employing feature subset selection for associative classification enhanced the classification accuracy.

The heart disease data set was collected from various corporate hospitals from Andhra Pradesh. Attributes are selected based on the opinion from expert doctors. The pattern of heart disease in A.P has been reported as follows   a) The study participants compromised of males and females of all age groups   b) Majority of the people who attain at the age of above 45 are associated with Coronary heart disease.

c) Among the study 42%of the urban people are  associated with heart disease.

d) Analysis of our heart disease data showed that significant correlation between systolic blood pressure and heart disease.

e)  Lipid abnormalities are a widely accepted risk factor for heart disease.

f) Among the males 25% who had serum triglycerides above risk level of 150mg/dl are associated with heart disease, and only 6%of females had serum triglycerides above risk level.

g) 20%of males with high values of low density lipoprotein cholesterol are associated with heart disease and for females it is only 5%.

Slno Data set Attribute selected based on Information Gain  1 Contact lenses Tear production rate, Astigmatism 2 weather Outlook, Humidity 3 Multiplexer 3X8 Chooser2,Output  7,output 5,output 0 4 zoo Animal ,legs, milk,toothed,legs  5 Students data  E5,E3,E4  6 Monks 1 data Attribute5,1,4 7 Monks 2 data Attribute 5,4, 8 Monks 3 data Attribute 2,5 9 Iris data Petal length, petal width  10 Haber man?s survival data  No. Of positively auxiliary nodes detected,  patients year of operation 11 Pima diabetes data Plas,mass,pedi,age,insu  12 Mushroom data Odor,gill size, gill color,stack, ring type, spore print color, population  13 Breast cancer Node caps,inv nodes, deg malig,tumor size  14 Hayes roth Age, marital status education level     h) High density lipoprotein cholesterol levels were significantly higher in males (30%) than females (3%)  i) 22%of males who had hypertension were in the risk of heart disease.



VI.  CONCLUSION The objective our work is to predict the risk score of heart disease in Andhra Pradesh with reduced no. of attributes .We used feature selection measures such as SU, IG and genetic search to determine the attributes which contribute more towards the prediction of heart disease which indirectly reduces the no. of diagnosis tests which are needed to be taken by a patient. We employed associative classification to improve the accuracy of classification.

Associative classification is fit to application like medical data mining. Dietary advice to younger people should be addressed and combining campaigns to improve diet with efforts to increase physical activity may be needed to reduce coronary heart disease.

