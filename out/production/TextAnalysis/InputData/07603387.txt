

Abstract?Probabilistic frequent itemset mining is a challeng- ing problem when mining uncertain databases. We address this problem and present an algorithm named APFIMSample. It presents a probabilistic support estimating method to reduce the computing cost, which together with a sampling technique, can significantly improve the performance. We evaluate our algorithm over two datasets, and the experimental studies show that the APFIMSample algorithm is effective.



I. INTRODUCTION  Frequent itemset mining is a traditional and very important problem in data mining. A frequent itemset is an itemset with support less than a parameter specified by the users. [1] presented a comprehensive survey of frequent itemset mining.

Traditional frequent itemset mining methods mainly consider to perform over the exact datasets. Nevertheless, uncertain is everywhere in the real world[6]. The uncertain datasets have the probability to show the existence or the occurrence of each data, which can be denoted by the possible world. That is, each possible world is an exact database with a probability. The mining cost, however, is very high. So far the best mining algorithm to achieve an probabilistic frequent itemset has O(nlog2n) time complexity and O(n) space complexity. In this paper, we will focus on this problem and propose an approximate method to reduce the mining speed, moreover, we use a sampling approach for further improvement.

The rest of this paper is organized as follows. In section 2 we present the preliminaries. Section 3 introduces the related works illustrates our method in detail. Section 4 evaluates our method with experimental results. Finally, section 5 concludes this paper.



II. PRELIMINARIES  For distinct items ? = {i1, i2, ? ? ? , in} (|?| = n), a subset X ? ? is called an itemset; if an item xt(0 < t ? |X|) in X has an occurrence probability p(xt), we call X the uncertain itemset, denoted as X = {x1, p(x1);x2, p(x2); ? ? ? ;x|X|, p(x|X|)}, and the probability of X is p(X) = ?|X|i=1p(xi). An uncertain database UD has uncertain transactions UTs(0 < s ? |UD|).

The left table of Figure 1 is a simple uncertain database.

Using a possible world model, an uncertain database can be converted to exact databases. As can be seen, 2 transactions are  in the database, each of them has 2 items; thus, we materialize them to a possible world model presented in the right table of Figure 1, which includes 22+2 = 16 possible worlds. Each possible world has a probability.

To our knowledge, there are two concepts of the frequent itemset have been proposed. The first is expected frequqnt itemset, which cannot reflect the uncertainty of an itemset[3].

Consequently, the probabilistic frequent itemset was proposed.

Probabilistic Frequent Itemset[3] For an uncertain database UD, given the minimum support ? and the minimum probabilistic confidence ? , an itemset X is called a (?, ? )- probabilistic frequent itemset if the probability of its support not smaller than ?, denoted P?(X)??(X), is larger than ? .

Here  P?(X)??(X) = ?  PW??,?PW (X)??  pPW (X) (1)

III. APPROXIMATE PROBABILISTIC FREQUENT ITEMSET MINING METHOD  To address the problem of mining the probabilistic frequent itemset, Zhang et al. introduced the conception of probabilistic frequent items[2], and employed the dynamic programming technique to perform mining, which was improved by Ber- necker et al., who proposed ProApriori[3] with using the apriori rule for further pruning. Sun et al. regarded the probability computation as the convolution of two vectors and thus used the divide-and-conquer method TODIS[4] to conduct mining, in which the fast fourier transform(FFT) can reduce the computing complexity from O(n2) to O(nlog2n), so the cost was much saved.

We propose an algorithm named APFIMSample to find the probabilistic frequent itemsets approximately. As can be seen from the time complexity in TODIS algorithm, the performance mainly depends on the size of the database, if we can reduce it, the performance will be improved.

According to the theory of large numbers, when the data size N is large enough, the data is convergent to the normal distri- bution. It provides us an assumption. Since the occurrences of an itemset in the database can be organized with a probabilistic vector, which is called the probabilistic density function(PDF), we can compute the expectation and the variance and set them as the parameters as the normal distribution. Given an     ID    TRANSACTION  {A 0.6} {B 0.7}  {A 0.2} {C 0.3}  Ucertain DataBase  PID       POSSIBLE WORLD  {} {}  {A} {}  {} {A}  {B} {}  {} {C}  6 {A} {A}    {AB} {}  {A} {B}  PROB.

0.0672  0.1008  0.0168  0.1568  0.0288  0.0252  0.2352  0.0392  PID       POSSIBLE WORLD  {A} {C}  {} {AC}  {B} {C}  {AB} {C}  {B} {AC}  14 {AB} {A}    {A} {AC}  {AB} {AC}  PROB.

0.0432  0.0072  0.0672  0.1008  0.0168  0.0588  0.0108  0.0252  Possible Worlds  Fig. 1. Uncertain Transaction Database VS. Possible Worlds  itemset X , if the expectation of the PDF is e(X), and the variance is v(X), then we can describe that the occurrences of X O(X) fits the normal distribution with expectation e(X) and variance v(X), denoted as t=?(O(X)?e(X)?  v(X) ). Then  we can determine whether X is probabilistic frequent by computing the s = 1-t.cdf(?) and compare it with the minimum probabilistic confidence. Here cdf function is the cumulative density function of t, and ? is the minimum support. Actually, it has the similar idea in [5]. With such a method, we can estimate the probabilistic frequent itemset by O(N) time complexity and O(1) space complexity.

We further based on our assumption to improve the mining performance by sampling the database. If all the data is fitting the normal distribution, then we can uniform sample the database with a sampling rate ?; thus, the sampled database is also approximately fitting the normal distribution. This simple but effective method can further reduce the computing cost from O(N) to O(?N)(0 < ? ? 1).



IV. EXPERIMENTAL STUDIES  We studied the experiments to evaluate the performance of our algorithm APFIMSample with the state-of-the-art algo- rithm TODIS. Both algorithm were implemented with Python 2.7, Microsoft Windows 7 and running on a PC with a 3.60GHZ Intel Core i7-4790M processor and 4GB main memory. We used two datasets as the benchmark to test these two algorithms. The T25I15D320K dataset was generated with the IBM synthetic data generator, and he GAZELLE dataset has the click data from an e-commerce website. The former is a dense dataset, and the latter is a sparse one. We present the details in Table I. We also presented the mean and the variance of each dataset. As can be seen, both T25I15D320K and GAZELLE datasets have the high mean and low variance.

Fig. 2. APFIMSample VS TODIS  A. Runtime Cost Evaluation  Figure 2 and 3 showed the runtime cost of our algorithm over two datasets without sampling the database. As can be seen, when the minimum support decreased, the mining cost increased. Our algorithm was significantly better that TODIS.

Note that both algorithms were impacted little by the size of the database. Figure 4 and 5 presented the runtime cost of our algorithm when using different sampling rates(?=0.08 and ?=0.9). We can see that if the sampling rate decreased, the computing cost increased linearly. This also verified our theoretical analysis.

B. Memory Cost Evaluation  We evaluated the memory cost of our algorithm without sampling in comparison to TODIS algorithm in Figure 6 and 7.

In these two figures we can find that our algorithm spend a less memory that TODIS, which is because that we employed the probabilistic support estimating method. Furthermore, Figure 8     TABLE I UNCERTAIN DATASET CHARACTERISTICS  DataSet trans count average size min size max size items count mean variance T25I15D320K 320 000 26 4 67 1000 0.88 0.27 GAZELLE 59 602 3 2 268 497 0.94 0.08  Fig. 3. APFIMSample VS TODIS  Fig. 4. APFIMSample with different Sampling Rates  Fig. 5. APFIMSample with different Sampling Rates  Fig. 6. APFIMSample VS TODIS  Fig. 7. APFIMSample VS TODIS  and 9 showed the memory cost when setting various sampling rates. Over GAZELLE dataset we can see that the memory usage kept unchanged; the one over T25I15D320K, however, changed a little. This showed that our algorithm can be more impacted by the sampling method over dense dataset.

C. Accuracy Evaluation  Our algorithm achieved hundreds of speedups that the TODIS algorithm, the mining result will be not accurate. We evaluate the mining accuracy of our algorithm. The precision and recall were used to present the approximation of the mining results. The larger the precision and recall ,the closer between A and A?; as an example, when A = A?, P = R = 1.

Here A is the mining results of TODIS and A? is the ones of APFIMSample.

Fig. 8. APFIMSample with different Sampling Rates  Fig. 9. APFIMSample with different Sampling Rates  The minimum support ? was set to 0.08 and the minimum probabilistic confidence ? was set to 0.9. Table II showed the precision and recall. As can been, when the sampling rate was 1, that is, no sampling, the precision and recall are all 100%, which means our estimating method is highly useful.

Furthermore, when we set the sampling rate from 0.09 to 0.01, the precision and recall kept almost not changed over GAZELLE dataset, and changed a little over T25I15D320K dataset. Note that most of the values were above 90%, which presented our sampling method is also effective.



V. RELATED WORKS  Chui et al. proposed an apriori-like uncertain frequent itemset mining method namedU-Apriori[7], which used a data trimming framework to improve the mining efficiency; later, he used a decremented pruning method[8] to further improve the performance. Leung et al. proposed the UF-Growth method with a novel tree structure UF-tree[9], which was further improved in [11]. Moreover, Aggarwal et al. employed the H-struct, which was efficient in exact data mining, to perform uncertain frequent itemset mining, and proposed UHMine[10].

TABLE II PRECISION AND RECALL  Dataset Parameter SRate Precision Recall  GAZELLE minsup=0.08  1 100% 100% 0.09 100% 100% 0.08 100% 100% 0.07 100% 100% 0.06 100% 100% 0.05 100% 100% 0.04 80% 100% 0.03 100% 100% 0.02 100% 100% 0.01 80% 80%  T25I15D320K minsup=0.08  1 100% 100% 0.09 91.6% 91.6% 0.08 91.6% 100% 0.07 95.8% 95.8% 0.06 95.8% 100% 0.05 100% 96% 0.04 91.6% 100% 0.03 91.6% 95.6% 0.02 91.6% 95.6% 0.01 91.6% 88%  Again, Leung et al. presented a more tight upper bound of expected supports and then introduced an efficient algorithm BLIMP-Growth[12]. The constrained frequent itemset mining method U-FPS was also proposed in [13], which intended to use the user constrains to let the mining be more efficient.

Besides, Calders et al. also used sampling to achieve the ap- proximate results[14]. In addition, many studies have focused on the stream mining problem and proposed the incremental mining algorithms[15]-[18].



VI. CONCLUSION  This paper studied the problem of mining the probabilistic frequent itemsets over uncertain databases. We proposed a method to estimate the probabilistic support with a much less computing cost, and then introduced the sampling technique to further improve the performance. The time complexity can be reduced from O(Nlog2N) to O(?N). Our extensive experimental results showed that our algorithm significantly outperform the TODIS algorithm with a high accuracy.

