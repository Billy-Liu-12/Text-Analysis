

Abstract? The ability to mine large volumes of distributed  datasets enables more precise decision making. However, privacy concerns should be carefully addressed when mining datasets distributed over autonomous sites. We propose a new Privacy- Preserving Protocol for Association Rule Mining (P3ARM) over horizontally partitioned data. P3ARM is based on a distributed implementation of the Apriori algorithm. The key idea is to arbitrary assign polling sites to collect itemsets? supports in encrypted forms using homomorphic encryption techniques. A pair of polling sites is assigned for each itemset. Polling sites are different for consecutive rounds of the protocol to reduce the potential for collusion. Our performance analysis shows that P3ARM significantly outperforms a leading existing protocol.

Moreover, P3ARM is scalable in the number of sites and the volume of data.

Index Terms? Cryptography, Data Security, Privacy, Security

I. INTRODUCTION Data mining is the process of filtering through large  amounts of raw data for useful information. This information is made up of meaningful patterns and trends that are already in the data but were previously unseen. Different data mining techniques help analysts recognize significant relationships, trends and patterns in raw data in order to make better decisions. Distributed data mining algorithms apply data mining tasks on datasets distributed among different sites.

However, privacy concerns may prevent cooperative sites to provide their data for mining; a survey of Internet users? attitudes towards privacy [1] showed that 17% of the users are extremely concerned about any use of their data and generally unwilling to provide their data, even when privacy protection measures were in place. For 56% of the users, these concerns are often significantly reduced by the presence of privacy protection measures. Indeed, there is an increasing need to develop privacy-preserving solutions for different cooperative computation scenarios, including data mining.

This paper presents a new protocol for mining data that are distributed among mutually untrusting parties. We focus on   Manuscript received March 21, 2006.

Iman Saleh and Mohamed Eltoweissy are with the Bradley Department of  Electrical and Computer Engineering, Virginia Polytechnic Institute and State University, VA 22043, USA (e-mail: iman.saleh, toweissy@vt.edu).

Alaa Mokhtar and Amin Shoukry are with the Computer and Systems Engineering Department, Alexandria University, Egypt (e-mail: ahafez2001, shoukryamin @yahoo.com).

mining association rules on the union of datasets that are partitioned horizontally among autonomous sites. The data schemes of all partitions are the same, i.e., their records represent transactions on the same set of items. The goal is to produce a set of global association rules, while limiting the information shared between sites. Examples of such cases include the consumer transactions recorded by different supermarkets and medical records collected by competitive medical insurance companies.

In this model of cooperation, it is not necessary to transfer the whole datasets to apply association mining tasks. Only aggregating information on each data set is sufficient to calculate the itemsets? support counts and hence discovering association rules. However, exchanging aggregate information about the data may reveal sensitive rules. For example, suppose a public agency would like to mine health records.

Insurance companies have data on patients? diseases and prescriptions. Imagine a rule indicating a high rate of complications with a particular medical procedure. If this rule does not hold globally, the insurer would like to know this ? they can then try to pinpoint the problem with their policies and improve their care. If the fact that the insurer?s data supports this rule is revealed, the insurer could be exposed to significant public relations or liability issues [2]. The proposed protocol mines the data without leaking any information about the sites? private inputs.

The problem of privacy-preserving data mining is a special case of the Secure Multiparty Computation (SMC) problem.

The SMC problem aims to compute some function of inputs that are scattered among different parties, without having these parties reveal their individual inputs. In [3], Goldreich provided a proof that for such function, there is a SMC solution. The approach used is to represent the function as a combinatorial circuit. The parties then run a short protocol for every gate in the circuit. In this approach, as in data mining, the size of the protocol depends on the size of the circuit which can be highly inefficient for large inputs. Solutions should be developed to overcome the performance limitations of the general theoretical solution.

The main contribution of this paper is proposing P3ARM, a new protocol for privately mining association rules over data that is partitioned between three or more sites. P3ARM (Privacy-Preserving Protocol for Association Rule Mining) enables the participating sites to discover the correct and  P3ARM: Privacy-Preserving Protocol for Association Rule Mining  Iman Saleh, Member IEEE, Alaa Mokhtar, Amin Shoukry, and Mohamed Eltoweissy, Senior Member IEEE  Proceedings of the 2006 IEEE Workshop on Information Assurance  United States Military Academy, West Point, NY      complete set of rules over the union of their databases without disclosing any information about their individual inputs. The proposed protocol achieves the required level of privacy while imposing minimal communication and computation overhead to the mining task. P3ARM is based on a distributed implementation of the Apriori algorithm [4, 5]. We propose a performance enhancement to the scheme in [5] and add privacy preserving functionality In P3ARM, each itemset is arbitrary assigned to a pair of polling sites. The polling sites securely decide whether the itemset is globally supported.

This organization minimizes the probability of collusion and provides a scalable solution for large number of sites. Hence, P3ARM is a practical and applicable approach for privately mining data that is distributed among competing or mutually untrusting parties.

The rest of this paper is organized as follows. Section 2 provides background on the data mining algorithms forming the basis of our solution. The proposed protocol is presented in Section 3. Section 4 gives a security analysis and proof of privacy level achieved. Section 5 presents performance analysis in terms of the computation, storage and communication costs. Section 6 presents further discussion on P3ARM and compares it with the protocol proposed in [2].

Section 7 includes related work and finally Section 7 concludes the paper and outlines directions for future work.



II. BACKGROUND In this section we describe the Apriori algorithm [4] and a  distributed implementation of that algorithm [5]. As stated above, P3ARM builds on the distributed implementation of Apriori.

A.  Association rule mining and the Apriori algorithm The association rule mining problem was formally defined  in [4] as follows. Let I = {i1, i2... in} be a set of items. Let DB be a set of transactions, where each transaction T is an itemset such that T ?  I. Given an itemset X ?  I, a transaction T contains X if and only if X ?  T. An association rule is an implication of the form X ? Y where X ?  I, Y ?  I and X ? Y = ? . The rule X ? Y has support s in the transaction database DB if s% of transactions in DB contain X ? Y. The association rule holds in the transaction database DB with confidence c if c% of transactions in DB that contain X also contains Y. An itemset X with k items called k-itemset. The problem of mining association rules is to find all rules whose support and confidence are higher than certain user specified minimum support and confidence. In this respect, a transaction database DB can be seen as 0/1 matrix where each column is an item and each row is a transaction.

The Apriori algorithm has been developed for rule mining in large transaction databases by IBM's Quest project team [4]. They have decomposed the problem of mining association rules into two parts:  - Find all combinations of items that have transaction  support above minimum support. Call those combinations frequent itemsets, and  - Use the frequent itemsets to generate the desired rules.

The general idea is that if, say, ABCD and AB are frequent  itemsets, then we can determine if the rule AB CD holds by computing the ratio r = support(ABCD)/support(AB). The rule holds only if r >= minimum confidence. Note that the rule will have minimum support because ABCD is frequent.

The Apriori algorithm makes multiple passes over the database. In the first pass, it simply counts item occurrences to determine the frequent 1-itemsets. A subsequent pass, say pass k, consists of two phases. First, the frequent itemsets Lk-1 (the set of all frequent (k-1)-itemsets) found in the (k-1)th pass are used to generate the candidate itemsets Ck, using the apriori- gen() function. This function first joins Lk-1 with Lk-1, the joining condition being that the lexicographically ordered first k-2 items are the same. Next, it deletes all those itemsets from the join result that have some (k-1)-subset that is not in Lk-1 yielding Ck.

B. Distributed association rule mining Cheung et al. proposed a method for mining association  rules when data is partitioned horizontally between sites [5].

The problem can be defined as follows:  Let DB be a partitioned database located at n sites namely S1, S2. . . Sn. The database partitions at these sites are DB1, DB2. . . DBn , respectively. Let the size of DB and the partition DBi be D and Di, respectively. For a given itemset X, X has local support count of X.supi at site Si if X.supi of the transactions in DBi contains X.  The global support count of X  is given as X.sup =? =  n  i  iX  sup. . For a given minimum support  s, X is globally large if X.sup ? s ? D; correspondingly, X is locally large at site Si, if X.supi ? s ? Di .

In the following, we will use L to denote all the globally large itemsets in DB and Lk to denote all globally large k- itemsets in L. The problem of mining association rules in a distributed database DB can be reduced to the finding of all globally large itemsets. For a site Si, if an itemset X is both locally large at site Si and globally large, then we say that X is heavy at site Si. We will use HLi to denote the set of heavy itemsets at site Si, and HLik to denote the set of heavy k- itemsets at site Si.

In a straightforward adaptation of Apriori, in the kth iteration, the set of candidate sets would be generated by applying Apriori-gen function on Lk-1. We denote this set of candidate sets by CAk (which stands for size-k candidate sets from Apriori). In other words, CAk = Apriori-gen (Lk-1). At each site Si, let CHik be the set of candidates sets generated by applying Apriori-gen on HLik-1, i.e., CHik = Apriori-gen (HLik- 1, ), (CH stands for candidate sets generated from heavy itemsets). Hence CHik is generated from HLik-1, which is only a subset of Lk-1. For clarity, Table 1 lists the notations used in our discussion of the distributed association rule protocols.

Proceedings of the 2006 IEEE Workshop on Information Assurance  United States Military Academy, West Point, NY        The method can be summarized as follows:  1. Candidate Sets Generation: generate the candidate sets CHik = Apriori-gen(HLik-1). Each site generates candidates based on the intersection of globally large (k-1) itemsets and locally large (k-1) itemsets.

2. Local Pruning: For each X ?  CHik, scan the database DBi at Si to compute X.supi. If X is locally large at Si, it is included in the LLik set. It is clear that if X is supported globally, it will be supported in one site.

3. Support Count Exchange: LLik are broadcast, and each  site computes the local support for the items in i? LLik.

4. Broadcast Mining Results: Each site broadcasts the local  support for itemsets in i? LLik .From this, each site is able to  compute Lk.

The above method requires O(n2) messages for count  exchange for each candidate set, where n is the number of partitions. The protocol in [2] is based on this method. To ensure that only O(n) messages are required for every candidate set, an optimization technique has been introduced.

A simple assignment function, which could be a hashing function, is used to determine a polling site for each candidate set. For each candidate set X, its polling site is responsible for broadcasting the polling request, collecting the support counts, and deciding whether X is large or not. Since there is only one polling site for each candidate set X, the number of messages required for count exchange for X is O(n). The optimized method can be summarized as follows:  1. Candidate Sets Generation: generate the candidate sets CHik = Apriori-gen(HLik-1) as before.

2- Candidates sent to Polling Sites: Si acts as a home site of its candidate sets; for every polling site Sj, Si finds all the candidate sets in LLik whose polling site are Sj and stores them in LLi,jk (i.e., candidates are being divided into groups according to their polling sites), the local support counts of the candidate sets are also stored in the corresponding set LLi,jk; sends each LLi,jk to the corresponding polling site Sj.

3- Polling Site sends Polling Requests: Si acts as a polling site; Si receives all LLik sent to him from the other sites; for every candidate set X received, Si finds the list of originating sites from which X is being sent; Si then broadcasts the polling request to the other sites not on the list to collect the support counts.

4- Remote Site replies Polling Requests: Si acts as a remote  site to reply polling requests; for every polling request LLp,jk from polling site Sp , Si sends the local support counts of the candidates in LLp,jk back to Sp .

5- Polling Site Computes Heavy Itemsets: Si acts as a polling site to compute the heavy itemsets; Si receives the support counts from the other sites; computes the global support counts for its candidates in LLik and finds the heavy itemsets; eventually, Si broadcasts the heavy itemsets together with their global support counts to all sites.



III. PRIVACY-PRESERVING PROTOCOL FOR ASSOCIATION RULE MINING (P3ARM)  We will extend and enhance the distributed association rule algorithm in [5] (described above) to achieve privacy. The goal is to construct a secure version of the algorithm by preventing the disclosure of any information beyond the mining results.

Our work is inspired, in part, by the protocol recently proposed by the Kantarcioglu and Clifton Protocol (KCP for short) proposed in [2] for privately discovering association rules in horizontally partitioned data. Their protocol is based on cryptographic tools, namely the commutative encryption and secure comparison, to discover the frequent rules. We will discuss KCP in more detail in Section 6 along with a comparison with our P3ARM.

A. Problem definition Let n ? 3 be the number of sites. Each site i has a private  transaction database DBi. Given a support threshold s and confidence c as percentages, the goal is to discover all association rules satisfying the thresholds. To preserve privacy of data owned by participating sites, it is required to limit the information leakage so that each site ? after the execution of the protocol ? knows nothing that cannot be simulated knowing its own input data and the mining results.

We consider the following as private information and should not be revealed:  ? The itemsets supported at each site ? The local support count of an itemset at each site ? The global support count of an itemset at each iteration k ? Database size at each site Instead of using the actual support count to decide whether  an itemsets is heavy or not, we will use the excess support count of the itemset, i.e. by how much a support count at a site exceeds the threshold support s. For an itemset to be globally heavy, the following inequality must be true [2]:  ?? ==  ? n  j  j n  j  j DBsX  *sup. ? 0*sup.

??? =  j n  j  j DBsX  We will denote ( X.supj ? s * |DBj| ) by X.esupj  B. Assumptions - Public-Key Infrastructure. We assume that a public-key  infrastructure is available to all parties; Every party i have a public key Pi known by all parties, and a private key Qi  TABLE I NOTATION  D The number of transactions in database DB Di The number of transactions in the partition DBi Lk The set of globally large k-itemsets CAk The set of candidate sets generated from Lk HLik The set of heavy k-itemsets at site Si CHik The set of candidate sets generated from HLik-1 LLik The set of locally large k-itemsets in CHik

X.sup The global support count of an itemset X

X.supi The local support count of an itemset X at site Si    Proceedings of the 2006 IEEE Workshop on Information Assurance  United States Military Academy, West Point, NY      known only to party i. The keys are generated from a  cryptographic system homomorphic over addition, That is, the sum of two encrypted values is equal to the encrypted sum of  the values: Ek(x) * Ek(y) = Ek(x + y) Homomorphic encryption is used in voting protocols to  verify the tally of the ballots without revealing what those ballots are. Benaloh proposes in [6] a homomorphic encryption system. The work in [7] makes use of the homomorphism over the addition to construct cryptographic counters used to build a secure voting system.

- Semi-Honest Threat Model. The participating parties are assumed to be semi-honest. A semi-honest party follows the rules of the protocol using its correct input, but after the protocol is free to use whatever it sees during execution of the protocol to compromise privacy [3].

- Collusion. Initially, we assume no collusion between sites; in section 4.2, we will suggest a solution to protect against a potential collusion problem that can reveal the support information of sensitive itemsets.

C. Proposed protocol P3ARM is based on the distributed data mining algorithm,  however, additional processing is required to limit the disclosure of information. This is achieved by communicating support counts in encrypted form and assigning a pair of polling sites to each itemsets: we will call them the polling site and the co-polling site. The polling site is responsible for collecting and summing encrypted support counts, the co- polling site is responsible for decrypting the support count, securely compare it with the threshold and broadcasting the result to all sites. We will assume that, for a polling site Si, Si+1 mod n acts as its co-polling site. We will omit the mod n for simplicity.  The protocol can be summarized as follows:  1. Candidate Sets Generation: Since globally large (k-1) itemsets is known by all sites at each iteration k, each site can locally compute candidate sets CAk = Apriori-gen(Lk-1). CAk is the same for all sites.

2- Candidates sent to Polling Sites: For every polling site Sj, the site finds all the candidate sets in CAk whose polling site are Sj and stores them in CAjk . The excess support count of the candidate sets is encrypted with the public key Pj+1 of the co-polling site Sj+1 and stored in the corresponding set CAjk , the site sends each CAjk to the corresponding polling site Sj.

Note that in the non-secure version of the algorithm, only CHi,jk ? the intersection of globally large (k-1) itemsets and locally large (k-1) itemsets - is sent to the polling site. In the secure version all candidate sets CAjk are sent instead. The reason of that is to avoid revealing the information of which itemsets are supported at site i. However, this causes no overhead on local computation cost since all k-itemset supports are already calculated during the local pruning as described in [5]. Both steps 3 and 4 of the non-secure distributed data mining algorithm are no more needed in the secure version; since all sites send local supports for all k- itemset in CAk to the corresponding polling site.

3- Polling Site Computes Heavy Itemsets: Si acts as a  polling site to compute the heavy itemsets; Si receives the support information from the other sites encrypted using the key of its neighboring site i+1; Si computes the global encrypted excess support counts for its candidates, so for a candidate k-itemset X. Si computes:  EPi+1(X.esup) = EPi+1(? =  n  j  jeX  sup. ) = EPi+1(X.esup1)*  EPi+1(X.esup2) *?* EPi+1(X.esupn) Si generates a random number r that is used to conceal the  global support count; Si sends to Si+1 the value E(r+?  =  n  j 1  (X.esupj)). Si+1 decrypts the received value and  obtains r +? =  n  j 1  (X.esupj). To determine if X is globally heavy  itemset, Si and Si+1 engage in as secure protocol to test whether r +?  =  n  j 1  (X.esupj) > r where Si knows r and Si+1 knows r +? =  n  j 1    (X.esupj). This can be done using Yao method for secure comparison; an efficient algorithm for solving this problem is presented in [8]. Si+1 broadcasts the global heavy itemsets to all sites.  The protocol pseudo-code is given in Fig. 1.

P3ARM contains the following message types:  submit(k,esupp[]): sent by a site to the polling site at round k, the message contains the encrypted excess support counts esupp[] of the itemsets assigned to that polling site.

collect(k,sum[]): sent by a polling site to its co-polling site at round k, the message contains the encrypted sum of the itemsets concealed by random values.

scompare(k,r[],sum[]): a set of messages exchanged between the polling and co-polling sites, at round k, to securely compare the sum of supports with the random values.

result(k,flag[]): broadcast by the co-polling sites to all other sites, the message contains a 1-bit flag per itemset indicating whether the itemset is globally supported or not. A typical message exchange sequence is depicted in Fig. 2.

Proceedings of the 2006 IEEE Workshop on Information Assurance  United States Military Academy, West Point, NY         D2=50

X.sup2=10, Y.sup2=5

X.esup2 = 10-10%*50=5 Y.esup2 = 5-10%*50=0   submit(1,{ Ep4(0), Ep4(5)})  collect(1,{Ep4(0)+Ep4(5)+Ep4(15)+Ep4(7), Ep4(5)+Ep4(0)+Ep4(-4)+Ep4(2)})  D4=50

X.sup4=6, Y.sup4=2

X.esup4 = 6-10%*50=1 Y.esup4 = 2-10%*50=-3     result(1,{1,0})  D1=50

X.sup1=5, Y.sup1=10

X.esup1 = 5-10%*50=0 Y.esup1 = 10-10%*50=5  submit(1,{ Ep4(5), Ep4(0)})  D3=50

X.sup3=20, Y.sup3=1  Rx=7, Ry=2

X.esup3 = 20-10%*50=15 Y.esup3 = 1-10%*50=-4 3  scompare(1,{7,2},{28,0})  Site 3  Site 1  Site 4  Site 2  Co-Polling Site for the itemsets X, Y with Key Pair (P4, Q4) Polling Site for the itemsets X, Y     Fig. 2. Exchange of Messages in P3ARM per Round per Polling Site   Securely comparing the confidence of a rule  For a rule X ?  Y and under the restricted privacy constraints, the support counts are not revealed so no site knows the value of XY.sup or X.sup. They only know whether a support count exceeds the minimum support threshold or not. We will use the transformation proposed in [2] to be able to use above protocol to check the confidence of a rule:  c X  XY >  sup.

sup.

? c  X  XY  n  i  i  n  i  i  >  ?  ?  =  =    sup.

sup.

?  ?? ==  > n  i  i n  i  i XcXY  )sup.(*sup.

?  ?= >?  n  i  ii XcXY  0)sup.*sup.(   For a rule X ?  Y, a site j sends E(XY.supj ? c *X.supj) to the polling site and using the secure comparison algorithm, the confidence of the rule is checked against the threshold value without revealing the rule confidence or the support of any of its itemsets.



IV. SECURITY ANALYSIS  A. Proof of security level achieved We use the proof by simulation to prove the security of the  proposed protocols. The key idea is to show that a polynomial time simulator can simulate the view of the parties during the execution of the protocol based on their local inputs and the global result. We also use the composition theorem which states that if a function g is securely reduced to another function f, and f is computed securely, then the computation of f(g) is secure [3].

Theorem:  The proposed algorithm privately computes Lk in the semi-honest model.

Proof: Step 1: each site locally computes CAk based on the  globally known large itemsets Lk-1, so no communication occurs in this step, each site can simulate its view by running the algorithm on its own input.

Step 2: same as step 1; each site locally encrypt the support count for each candidate itemset.

Step 3: each polling site receives the encrypted support counts. Assuming the security of the encryption; the encrypted support counts are computationally indistinguishable from a number chosen from a uniform distribution. Hence, the polling site can simulate the encrypted supports received by a uniform random number generator. The polling site neighbor receives the encrypted sum of the support counts concealed by a random number r generated by the polling site, hence, the value obtained by the neighbor site after the decryption is computationally indistinguishable from a random number.

Each polling site engages in a secure comparison protocol with its neighbor to decide whether the global support exceeds the threshold value. Hence, no information is disclosed in this step that can not be simulated based on the site input and output.

Therefore, based on Steps 1, 2, and 3, we can conclude that the proposed protocol securely calculates Lk.

B. Collusion problem We propose a modification to ensure the secrecy of the  support count of an itemset that may be a part of a sensitive  P3ARM: Finding global heavy k-itemsets privately  Input: n>= 3 sites numbered 1..n ; the minimum support threshold s; The global heavy (k-1)-itemsets; Lk-1   Step 1 Candidate sets generation  For each site Si Generate CAk = Apriori-gen(Lk-1); Step 2  Candidate sets support count excess sent to the  corresponding polling sites For each polling S j For each candidate set X whose polling site is S j  Compute Epj+1  (X.esupi);  End for  Send encrypted support counts excess to S j; End for End for Step 3 Find Global heavy k-itemsets For each polling S j For each candidate set X whose polling site is S j  Generate a random r ; Send EPj+1( r +?  =  n  j 1  (X.esupj))  to S j+1 ;  S j+1  computes  Dqj+1(EPj+1( r +? =  n  j 1  (X.esupj));  S j and S j+1 securely compare r +? =  n  j 1  (X.esupj) against r;  If  r +? =  n  j 1  (X.esupj) >= r    S j+1 broadcasts X to all site  as a globally heavy itemset ; End for End for  Fig. 1.  P3ARM Pseudo-Code  Proceedings of the 2006 IEEE Workshop on Information Assurance  United States Military Academy, West Point, NY                  20 40 60 80 100  Num. of Pollling Sites  M es sa ge s         bi ts  Total Num. Messages Avg. Message Length  Fig. 3.  The Effect of Changing Number of Polling Sites on Number of Messages and Average Message Length          20 40 50 100 200  Num. of Itemsets per Polling Site  M es sa ge s        bi ts  Total Num. Messages Avg. Message Length  Fig. 4. The Effect of Setting a Minimum Number of Itemsets per Site on Number of Messages and Average Message Length  association rule (the complications associated with a certain medical procedure for example), the collusion between the polling and co-polling site for this itemset may reveal the support count at all other sites. For this purpose, we assume that the cryptosystem has the threshold decryption property.

For a sensitive itemset X, D co-polling sites are responsible for decrypting the sum X.sup instead of one co-polling site.

Each of  the D sites hold a secret share of the private key and if sufficiently many of them cooperate, they decrypt the support count. However, no coalition below the threshold value is able to decrypt the encrypted support count; this represents a higher privacy level for sensitive itemsets.



V. PERFORMANCE ANALYSIS We assume we have n sites, the total database size is |DB|  (it is enough to have an upper bound of this value). A support count is presented in m = ? ?DB2log  bits. Let t be the number of bits in the output of the encryption of a support count. The number of candidate itemsets at round k is |CAk|, and the number of polling sites is P (initially we have P = n, later we will suggest a modification in the basic scheme where P will be a subset of n for performance reasons)  A. Computation Cost At each site, the computation cost increase due to  encryption is O(t3 * |CAk|) where t3 represents the bit-wise cost of modular exponentiation, this is the cost of encrypting the support count of each candidate itemsets in |CAk|.  Each polling site performs multiplication of encrypted support counts for its subset of candidate itemsets (with upper bound of |CAk|). Hence, each polling site suffers a computation cost overhead of O(t2 * |CAk|). The computation overhead of the secure comparison algorithm is O(?t) where ? is a security parameter [8].

B. Storage Cost The storage overhead at one site is O(t * |CAk|) which is the  size of the encrypted supports. For a polling site, additional storage of O(t * n *|CAk|/P) is needed for the encrypted supports received from all other (n-1) sites. Finally, the co- polling site has an additional storage cost of O(t *|CAk|/P) which is the size of the encrypted sum of the support values.

C. Communication Cost At step 2 of the protocol, each site sends the support count  of the candidate itemsets to the corresponding polling site which is O(n* t * |CAk|) bits of communication. The polling site sends the encrypted support count to the decryption site, this requires O(t * |CAk|) bits of communication. At step 3, the broadcast of the global heavy itemsets can be done by broadcasting a |CAk|-bit string where a bit is set to 1 if the itemset is globally supported, 0 otherwise. This requires O(n * |CAk|) bits of communication. The overhead of the secure comparison algorithm is O(?t) bits of communication where ? is a security parameter [8].

Since all encrypted support counts are sent in one message  to the corresponding polling sites, the number of messages exchanged in our scheme is a function of number of polling sites. Fig. 3 shows the effect of changing the number of polling site P on number of exchanged messages and the average message length, in bits. For our evaluation, we used the mining results from [9] where a synthetic database was generated from the IBM Almaden generator. The database was created with parameters: |DB|=1M transactions with 1K distinct items. We will further assume that the database is distributed among n=100 sites, and since the results in [9] does not report the number of candidates at each round, we will assume that the number of candidates is 4 times the number of frequent itemsets.

Another possible extension is to set a minimum number of support counts per message, we will call it Lmin, Fig. 4 shows the effect of changing Lmin on both number of messages and the average message length.

Hence, setting number of polling sites as a system parameter has two advantageous results:  - By limiting the number of sites acting as polling sites, the communication overhead can be adjusted based on network conditions and consequently enhancing the performance.

- The protocol becomes scalable for large number of sites.

Proceedings of the 2006 IEEE Workshop on Information Assurance  United States Military Academy, West Point, NY       TABLE 2 COMPARISON BETWEEN KCP AND P3ARM SCHEMES  KCP P3ARM  C om  pu ta  tio n  C os  t - Encrypting candidate itemests generated by each site: O(t3*|CAk|*n) - Decrypting candidate itemests generated by each site: O(t3*|CAk|*n) - Secure comparison: O(t?)   - Encrypting the support count of each candidate itemsets in |CAk|: O(t3*|CAk|) - Multiplying encrypted support counts at polling sites: O(t2*|CAk|) - Secure comparison: O(t?)  C om  m un  ic at  io n  C os  t  - Finding union of locally large itemsets: O(n2*t*|CAk|) - Summing support count of itemsets: O(m*n*|CAk|) - Secure comparison algorithm: O(t?)   - Sending the support count of the candidate itemsets to the corresponding polling site: O(n*t*|CAk|) - The polling site sends the encrypted support count to the co- polling site: O(t*|CAk|) - Secure comparison algorithm: O(t?) - Broadcast of globally supported itemsets:  O(n*|CAk|)   C ol  lu si  on S  ce na  ri os    - The collusion between sites i+1 and site i-1, compromises the input of site i for all itemsets during the whole protocol execution.

- If site i colludes with site i?1, it can learn the size of its intersection with site i+1.

- Collusion between sites 0 and 1 may reveal the actual itemsets.

- The collusion between polling and co-polling site reveals the support count of an itemset at other sites.

Sc al  ab ili  ty   - Not scalable to large number of sites because of the communication overhead and the fact that the number of exchanged messages is a function of number of sites.

- Scalable to large number of sites because of the lower communication cost and the fact that the number of exchanged messages is a function of the number of polling sites.



VI. COMPARISON OF P3ARM AND KCP In this section we will discuss KCP proposed in [2] and  compare and contrast it with P3ARM. KCP runs in two phases to discover candidate itemsets and to determine which of the candidate itemsets meet the global support threshold. The first phase uses commutative encryption. Each party encrypts its own itemsets, then the already encrypted itemsets of every other party. These are passed around, with each site decrypting, to obtain the complete set. In the second phase, an initiating party passes its support count, plus a random value, to its neighbor. The neighbor adds its support count and passes it on. The final party then engages in a secure comparison with the initiating party to determine if the final result is greater than the threshold plus the random value.

P3ARM achieves many enhancements on this scheme as follows:  - In P3ARM, for each itemset, two sites are assigned as polling sites. This reduces the communication cost and the computation costs by an O(n). Also in KCP, fake items are added to the communicated itemsets which adds to the communication overhead. On the other hand, in our solution, all candidate sets are tested against the threshold, this is to avoid revealing the information of which itemsets are supported at a site. This causes no overhead on local computation cost but adds a communication overhead in our scheme especially for the 1-itemsets. This overhead is minimized in later rounds as itemsets are filtered out by the Apriori procedure.

- KCP dedicates two sites, namely sites 0 and 1, to collect the encrypted commonly supported itemsets from the even (odd) sites, this organization raises a potential collusion problem between these two sites. By randomly assigning the polling sites to the itemsets, the probability of collusion is minimized in P3ARM.

- Also, it should be noted that in KCP the number of messages exchanged is a function of number of sites, as the candidate itemsets must be encrypted (and later decrypted) by all the participating sites. So, as number of participating sites increases, the scheme becomes impractical. On the other hand, the number of messages in P3ARM depends on number of designated polling sites which is considered as a system parameter and can be adjusted to enhance the performance.

- P3ARM achieves high level of parallelism since comparisons of support counts are done between different pairs of polling and co-polling sites in parallel, which reduces the total execution time of the protocol.

- KCP suffers from three potential collusion problems: The collusion between sites i+1 and site i-1, which compromises the secrecy of site i for all itemsets during the whole protocol execution. Also, if site i colludes with site i?1, it can learn the number of commonly supported itemsets with site i+1.

Finally, the collusion between sites 0 and 1 may reveal the actual itemsets. A solution is proposed to solve the first collusion problem by splitting the input between the sites. On the other hand, our solution suffers from one collusion  problem; the collusion between the polling and co-polling sites. If these two sites collude, they can reveal the local supports of an itemset. However, the probability of such collusion is minimized in our protocol due to the fact that the itemsets are arbitrary assigned to polling sites, that is, it is not known, at the beginning of the protocol, which site will be assigned to which itemsets, as the supported itemsets are only discovered during the execution of the protocol. Table 2 summarizes the differences between the two approaches.



VII. RELATED WORK There is an increasing number of algorithms for privacy-  preserving data mining motivated by the increase in the need for applying mining tasks on sensitive data owned by different mutually untrusting autonomous parties. Mainly, privacy- preserving data mining solutions can be classified based on the core approach used into data randomization solutions and cryptography-based solutions. The former solutions reveal randomized information about each record in the data set in exchange for not having to reveal the original records. The  Proceedings of the 2006 IEEE Workshop on Information Assurance  United States Military Academy, West Point, NY      latter ones rely on a secure protocol using cryptography primitives and tools employed by the participating parties.

Privacy is achieved if at the end of the execution of the protocol no party knows anything except its own input and the mining results.

Methods presented in [10] and [11] are based on randomization approach. In [10], a data probabilistic distortion technique is used. A mining process for generating frequent itemsets from the distorted database was presented, along with a set of optimizations to address the fact that mining the distorted database is more expensive than mining the true database. The presented algorithm achieves privacy of over 80% and an error of less than 10%. In [11], the randomized response technique is used to conduct the data mining computation. The proposed method builds decision tree classifiers from the disguised data.

Recent approaches for privacy-preserving data classification are proposed in [12] and [13], where cryptographic tools are used to minimize the information shared. In [14], Vaidya and Clifton present a cryptography- based protocol that addresses the problem of mining association rules across two databases where the columns in the table are at different sites, splitting each row. There is a join key present in both databases and the remaining attributes are present in one database or the other, but not both. The goal is to find association rules involving attributes other than the join key. The method is based on a privacy-preserving scalar product protocol, and an efficient protocol for computing scalar product while preserving privacy of the individual values.

The advantage of the randomization approach is its performance, but it achieves this at the cost of accuracy. On the other hand, the cryptography-based approach ensures that the results are the same as the results obtained from the original algorithms - without the privacy concerns - but is generally more expensive than randomization [15]. Our work follows the second approach with the goal of improving the performance while achieving privacy.



VIII. CONCLUSION We proposed P3ARM, a new efficient protocol for mining  association rules over horizontally partitioned data. The protocol works for three or more parties under the semi-honest model. P3ARM introduces minimal overhead to the mining task due to the privacy requirements. The key idea is to arbitrary assign polling sites to collect the itemsets? supports in encrypted forms. We proved that our solution outperforms the one presented in [2] by an order of n (where n is the number of sites participating in the protocol) in both the communication and the computation overhead. It also minimizes the probability and effect of collusion between the participating sites.

Directions for future work include combining the data randomization and SMC approaches for privately mining association rules. This should enhance the protocol  performance, minimize the information leakage and realize high output precision. Another possible extension to reduce the number of communicated messages is to have the sites probabilistically send the local support value for an itemset.

Obviously, the mining results in this case will be an approximation. The accuracy of the results may be enhanced by providing information on the data distribution at each site to recover the missing values.

