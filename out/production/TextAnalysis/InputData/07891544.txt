See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

ABSTRACT | Remote sensing image scene classification plays  an important role in a wide range of applications and hence  has been receiving remarkable attention. During the past years,  significant efforts have been made to develop various data  sets or present a variety of approaches for scene classification  from remote sensing images. However, a systematic review  of the literature concerning data sets and methods for scene  classification is still lacking. In addition, almost all existing data  sets have a number of limitations, including the small scale  of scene classes and the image numbers, the lack of image  variations and diversity, and the saturation of accuracy. These  limitations severely limit the development of new approaches  especially deep learning-based methods. This paper first  provides a comprehensive review of the recent progress. Then,  we propose a large-scale data set, termed ?NWPU-RESISC45,?  which is a publicly available benchmark for REmote Sensing  Image Scene Classification (RESISC), created by Northwestern  Polytechnical University (NWPU). This data set contains 31 500  images, covering 45 scene classes with 700 images in each class.

The proposed NWPU-RESISC45 1) is large-scale on the scene  classes and the total image number; 2) holds big variations  in translation, spatial resolution, viewpoint, object pose,  illumination, background, and occlusion; and 3) has high within-  class diversity and between-class similarity. The creation of this  data set will enable the community to develop and evaluate  various data-driven algorithms. Finally, several representative  methods are evaluated using the proposed data set, and the  results are reported as a useful baseline for future research.

KEYWORDS | Benchmark data set; deep learning; handcrafted  features; remote sensing image; scene classification;  unsupervised feature learning  I .  IN TRODUCTION  The currently available instruments (e.g., multi/hyper- spectral [1], synthetic aperture radar [2], etc.) for earth observation [3], [4] generate more and more different types of airborne or satellite images with different resolutions (spatial resolution, spectral resolution, and temporal res- olution). This raises an important demand for intelligent earth observation through remote sensing images, which allows the smart identification and classification of land use and land cover (LULC) scenes from airborne or space platforms [3]. Remote sensing image scene classification, being an active research topic in the field of aerial and satellite image analysis, is to categorize scene images into a discrete set of meaningful LULC classes according to the image contents. During the past decades, remarkable efforts have been made in developing various methods for the task of remote sensing image scene classification because of its important role for a wide range of appli- cations, such as natural hazards detection [5]?[7], LULC determination [8]?[43], geospatial object detection [27], [44]?[52], geographic image retrieval [53]?[63], vegeta- tion mapping [64]?[68], environment monitoring, and urban planning.

In the early 1970s, the spatial resolution of satellite images (such as Landsat series) was low and hence, the pixel sizes were typically coarser than, or at the best, simi- lar in size to the objects of interest [69]. Most of the meth- ods for image analysis using remote sensing images devel- oped since the early 1970s are based on per-pixel analysis, or even subpixel analysis for this conversion [69]?[72].

With the advances of remote sensing technology, the spa- tial resolution is gradually finer than the typical object of interest and the objects are generally composed of many  Manuscript received November 11, 2016; revised January 10, 2017 and February 14, 2017; accepted February 26. 2017. This work was supported in part by the National Science Foundation of China under Grants 61401357, 61522207, and 61473231; and by the Fundamental Research Funds for the Central Universities under Grant 3102016ZY023 (Corresponding author: Junwei Han).

G. Cheng and J. Han are with the School of Automation, Northwestern Polytechnical University, Xi'an 710072, Shaanxi, P.R. China.

(email: junweihan2010@gmail.com)

X. Lu is with the Center for OPTical IMagery Analysis and Learning (OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi'an 710119, Shaanxi, P.R. China.

Remote Sensing Image Scene Classification: Benchmark and State of the Art By GonG ChenG, Junwei han, Senior Member, IEEE, and XiaoqianG Lu, Senior Member, IEEE    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Cheng et al . : Remote Sensing Image Scene Classif ication: Benchmark and State of the Art  2 Proceedings of the IEEE  pixels, which has significantly increased the within-class variability and single pixels do not come isolated but are knitted into an image full of spatial patterns. In this case, it is difficult or sometimes impoverished to categorize scene images at pixel level purely.

Having identified an increasing dissatisfaction with pixel level image classification paradigm, Blaschke and Strobl [70] in 2001 raised a critical question ?What?s wrong with pixels?? to conclude that researchers should focus on the spatial pat- terns created by pixels rather than the statistical analysis of single pixels. Afterwards, a new paradigm of object-based image analysis (OBIA) [71] or geographic-object-based image analysis (GEOBIA) [73] was raised for the object level deline- ation and analysis of remote sensing images rather than indi- vidual pixels. Here, the term ?objects? represents meaningful semantic entities or scene components that are distinguish- able in an image (e.g., a house, a tree, or a vehicle in a 1:3000 scale color airphoto). The core task of OBIA and GEOBIA is the production of a set of nonoverlapping segments (or poly- gons), that is, the partitioning of a scene image into meaning- ful geographically based objects or superpixels that share rela- tively homogeneous spectral, color, or texture information.

Due to the superiority compared to pixel-level approaches, object-level methods have dominated the task of remote sens- ing image analysis for decades [70]?[80].

Although pixel- and object-level classification methods have demonstrated impressive performance for some typi- cal land use identification tasks, pixels, or even superpixels, carry little semantic meanings. For semantic-level under- standing of the meanings and contents of remote sensing images, we take eight images from the very popular UC Merced land use data set [38] as examples, as shown in Fig. 1(a)?(h) (dense residential versus medium residential, freeway versus runway, river versus golf course, intersec- tion versus overpass), such pixel- and object-level meth- ods are distinctly not enough to identify them correctly.

Fortunately, with the rapid development of machine learn- ing theories, recent years have witnessed the emergence of another research flow, that is, semantic-level remote sens- ing image scene classification which aims to label each scene image with a specific semantic class [9]?[11], [15],  [16], [18], [23], [24], [26], [28], [33], [36]?[38], [53], [55], [59], [81]?[87]. Here, a scene image usually refers to a local image patch manually extracted from large-scale remote sensing images that contain explicit semantic classes (e.g., commercial area, industrial area, and residential area).

Since the emergence of UC Merced land use data set [38], the first publicly available high-resolution remote sens- ing image data set for scene classification, significant efforts have been made in developing publicly available data sets [9], [11], [17], [33], [38], [82] (as summarized in Table 1) or presenting various methods [9]?[11], [15], [16], [18], [24], [26], [28], [33], [36]?[38], [53], [55], [59], [81]?[87] for the task of remote sensing image scene classification. However, a deep review of the literatures concerning data sets and methods for scene classification is still lacking. Besides, almost all existing data sets have a number of limitations, such as the small scale of scene classes and images per class, the lack of image variations and diversity, and the saturation of accuracy (e.g., almost 100% classification accuracy on the most popular UC Merced data set [38] with deep ConvNets features [82]). These limitations severely limit the develop- ment of new data-driven algorithms especially deep-learn- ing-based methods.

With this in mind, this paper first provides a comprehensive review of the recent progress in this field.

Then, we propose a large-scale benchmark data set, named  Fig. 1. Eight scene images from the popular UC Merced land use data set: (a) dense residential; (b) medium residential; (c) freeway; (d) runway; (e) river; (f) golf course; (g) intersection; and (h) overpass.

Table 1 Comparison Between the Proposed NWPU-RESISC45 Data Set and Some Other Publicly Available Data Sets. Our Data Set Provides Many More Images per Class, Scene Classes, Total Images, and Scales (Spatial Resolution Variations) in Comparison With Other Available Data Sets for  Remote Sensing Image Scene Classification    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Cheng et al . : Remote Sensing Image Scene Classif ication: Benchmark and State of the Art  Proceedings of the IEEE 3  ?NWPU-RESISC45,?1 which is a freely and publicly avail- able benchmark used for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This new data set has a total number of 31 500 images, covering 45 scene classes with 700 images in each class. We highlight three key fea- tures of the proposed NWPU-RESISC45 data set when comparing it with other existing scene data sets [9], [11], [17], [33], [38], [82]. First, it is large scale on the number of scene classes, the number of image per class, and the total number of images. Second, for each scene class, our data set possesses big image variations in translation, spa- tial resolution, viewpoint, object pose, illumination, back- ground, occlusion, etc. Third, it has high within-class diversity and between-class similarity.

To sum up, the main contributions of this paper are threefold.

1)  This paper provides a comprehensive review of the recent progress in this field. Covering about 170 publications we review existing publicly available benchmark data sets and three main categories of approaches for remote sensing image scene classifica- tion, including handcrafted-feature-based methods, unsupervised-feature-learning-based methods, and deep-feature-learning-based methods.

2)  We propose a large-scale, publicly available benchmark data set by analyzing the limitations of existing data sets.

To the best of our knowledge, this data set is of the larg- est scale on the number of scene classes and the total number of images. The creation of this data set will ena- ble the community to develop and evaluate various new data-driven algorithms, at a much larger scale, to further improve the state of the arts.

3)  We investigate how well the current state-of-the-art scene classification methods perform on NWPU- RESISC45 data set. Current methods have only been tested on small data sets. It is unclear how they com- pare to each other on a larger data set with a large number of scene classes. Thus, we evaluate a num- ber of representative methods including deep-learn- ing-based methods for the task of scene classifica- tion using the proposed data set and the results are reported as a useful performance baseline.

The rest of the paper is organized as follows. Section II reviews several publicly available data sets for remote sens- ing image scene classification. Section III surveys three cat- egories of approaches in this domain. Section IV provides details on our data set. A benchmarking of state-of-the-art scene classification methods using our data set is given in Section V. Finally, conclusions are drawn in Section VI.

II .  A R E V IE W ON R EMOTE SENSING IM AGE SCENE CL A SSIFIC ATION DATA SETS  In the past years, several publicly available high-resolution remote sensing image data sets [9], [11], [17], [33], [38], [82] have been introduced by different groups to perform research for scene classification and to evaluate different methods in this field. We will briefly review them in this section.

A. UC Merced Land-Use Data Set  The UC Merced land-use data set [38] is composed of 2100 overhead scene images divided into 21 land-use scene classes. Each class consists of 100 aerial images measuring  256 ? 256  pixels, with a spatial resolution of 0.3 m per pixel in the red green blue color space. This data set was extracted from aerial orthoimagery down- loaded from the U.S. Geological Survey (USGS) National Map of the following U.S. regions: Birmingham, Boston, Buffalo, Columbus, Dallas, Harrisburg, Houston, Jacksonville, Las Vegas, Los Angeles, Miami, Napa, New York, Reno, San Diego, Santa Barbara, Seattle, Tampa, Tucson, and Ventura. The 21 land-use classes are: agri- cultural, airplane, baseball diamond, beach, buildings, chaparral, dense residential, forest, freeway, golf course, harbor, intersection, medium density residential, mobile home park, overpass, parking lot, river, runway, sparse residential, storage tanks, and tennis courts. This data set holds highly overlapping land-use classes such as dense residential, medium residential, and sparse resi- dential, which mainly differ in the density of structures and hence make the data set more rich and challenging.

So far, this data set is the most popular and has been widely used for the task of remote sensing image scene classification and retrieval [8], [10], [11], [14]?[16], [18]? [20], [24], [26], [28]?[30], [36], [38], [53], [55], [59], [61], [62], [81], [82], [84]?[98].

B. WHU-RS19 Data Set  The WHU-RS19 data set was developed in [33] on its original version of [83], extracted from a set of satellite images exported from Google Earth (Google Inc.) with spa- tial resolution up to 0.5 m and spectral bands of red, green, and blue. It contains 19 scene classes, including airport, beach, bridge, commercial area, desert, farmland, football field, forest, industrial area, meadow, mountain, park, park- ing lot, pond, port, railway station, residential area, river, and viaduct. For each scene class, there are about 50 images, with 1005 images in total for the entire data set. The image sizes are 600  ?  600 pixels. This data set is challenging due to the high variations in resolution, scale, orientation, and illu- minations of the images. However, the number of images per class of this data set is relatively small compared with  1http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.

html;https://1drv.ms/u/s!AmgKYzARBl5ca3HNaHIlzp_IXjs (OneDrive); http://pan.baidu.com/s/1mifR6tU(BaiduWangpan)    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Cheng et al . : Remote Sensing Image Scene Classif ication: Benchmark and State of the Art  4 Proceedings of the IEEE  UC Merced data set [38]. This data set has also been widely adopted to evaluate various different scene classification methods [9], [16], [18], [33], [37], [83]?[87], [97].

C. SIRI-WHU Data Set  The SIRI-WHU data set [11] consists of 2400 remote sensing images with 12 scene classes. Each class consists of 200 images with a spatial resolution of 2 m and a size of 200 ?  200 pixels. It was collected from Google Earth (Google Inc.) by the Intelligent Data Extraction and Analysis of Remote Sensing (RS_IDEA) Group at Wuhan University. The 12 land- use classes contain agriculture, commercial, harbor, idle land, industrial, meadow, overpass, park, pond, residential, river, and water. Although this data set has been tested by several methods [8], [10], [11], the number of scene classes is rela- tively small. Besides, it mainly covers urban areas in China and hence lacks diversity and is less challenging.

D. RSSCN7 Data Set  The RSSCN7 data set [17] contains a total of 2800 remote sensing images which are composed of seven scene classes: grass land, forest, farm land, parking lot, residential region, industrial region, and river/lake. For each class, there are 400 images collected from the Google Earth (Google Inc.) that are cropped on four different scales with 100 images per scale. Each image has a size of 400  ?  400 pixels. The main challenge of this data set comes from the scale variations of the images [14].

E. RSC11 Data Set  The RSC11 data set [9] was extracted from Google Earth (Google Inc.), which covers the high-resolution remote sens- ing images of several U.S. cities including Washington DC, Los Angeles, San Francisco, New York, San Diego, Chicago, and Houston. Three spectral bands were used including red, green, and blue. There are 11 complicated scene classes including dense forest, grassland, harbor, high buildings, low buildings, overpass, railway, residential area, roads, sparse forest, and storage tanks. Some of the scene classes are quite similar in vision, which increases the difficulty in discrimi- nating the scene images. The data set totally includes 1232 images with about 100 images in each class. Each image has a size of 512  ?  512 pixels and a spatial resolution of 0.2 m.

F. Brazilian Coffee Scene Data Set  The Brazilian coffee scene data set [82] consists of only two scene classes (coffee class and noncoffee class) and each class has 1438 image tiles with a size of 64  ?  64 pixels cropped from SPOT satellite images over four counties in the State of Minas Gerais, Brazil: Arceburgo, Guaranesia, Guaxupe, and Monte Santo. This data set con- sidered the green, red, and near-infrared bands because they  are the most useful and representative ones for distinguish- ing vegetation areas. The identification of coffee crops was performed manually by agricultural researchers. To be spe- cific, the creation of the data set is performed as follows: tiles with at least 85% of coffee pixels were assigned to the coffee class; tiles with less than 10% of coffee pixels were assigned to the noncoffee class. Despite there exists signifi- cant intraclass variation caused by different crop manage- ment techniques, different plant ages, and spectral distor- tions, there are only two scene classes, which is quite small for testing multiclass scene classification methods.

III .  A SU RV EY ON R EMOTE SENSING IM AGE SCENE CL A SSIFIC ATION METHODS  During the last decades, considerable efforts have been made to develop various methods for the task of scene clas- sification using satellite or aerial images. As scene classifica- tion is usually carried out in feature space, effective feature representation plays an important role in constructing high- performance scene classification methods. We can generally divide existing scene classification methods into three main categories according to the features they used: handcrafted- feature-based methods, unsupervised-feature-learning- based methods, and deep-feature-learning-based methods.

It should be noted that these three categories are not neces- sarily independent and sometimes the same method exists with different categories.

A. Handcrafted-Feature-Based Methods  The early works for scene classification are mainly based on handcrafted features [22], [23], [27], [38], [44], [51], [56], [62], [80], [82], [99]?[103]. These methods mainly focus on using a considerable amount of engineer- ing skills and domain expertise to design various human- engineering features, such as color, texture, shape, spa- tial and spectral information, or their combination that are the primary characteristic of a scene image and hence carry useful information used for scene classification.

Here, we briefly review several most representative hand- crafted features, including color histograms [99], texture descriptors [104]?[106], GIST [107], scale-invariant fea- ture transform (SIFT) [108], and histogram of oriented gradients (HOG) [109].

1) Color Histograms: Among all handcrafted features, the global color histogram feature [99] is almost the sim- plest, yet an effective visual feature commonly used in image retrieval and scene classification [38], [56], [80], [82], [99].

A major advantage of color histograms, apart from their ease to compute, is that they are invariant to translation and rotation about the viewing axis. However, color histograms are not able to convey the spatial information, so it is very difficult to distinguish the images with the same colors but different color distribution. Besides, color histogram feature    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Cheng et al . : Remote Sensing Image Scene Classif ication: Benchmark and State of the Art  Proceedings of the IEEE 5  is also sensitive to small illumination changes and quantiza- tion errors.

2) Texture Descriptors: Texture features, such as gray level co-occurrence matrix (GLCM) [104], Gabor feature [105], local binary patterns (LBPs) [84], [106], [110], etc., are widely used for analyzing aerial or satellite images [51], [56], [62], [100]?[102]. Texture features are commonly computed by placing primitives in local image subregions and analyzing the relative differences, so they are quite use- ful for identifying textural scene images.

3) GIST: GIST descriptor was initially proposed in [107], which provides a global description for representing the spatial structure of dominant scales and orientations of a scene. It is based on calculating the statistics of the outputs of local feature detectors in spatially distributed subregions. Specifically, in standard GIST, the images are first convoluted with a number of steerable pyramid filters. Then, the image is divided into a 4 ? 4  grid for which orientation histograms are extracted. Note that the GIST descriptor is similar in spirit to the local SIFT descriptor [108]. Owing to its simplicity and efficiency, GIST is popularly used for scene representation [111]?[113].

4) SIFT: SIFT feature [108] describes subregions by gradi- ent information around identified keypoints. Standard SIFT, also known as sparse SIFT, is the combination of keypoint detection and histogram-based gradient representation. It generally has four steps, namely, scale space extrema search- ing, subpixel keypoint refining, dominant orientation assign- ment, and feature description. Except for sparse SIFT descrip- tor, there also exist dense SIFT that is computed in uniformly and densely sampled local regions and several extensions such as PCA-SIFT [114] and speedup robust features (SURFs) [115]. SIFT feature and its variants are highly distinctive and invariant to changes in scale, rotation, and illumination.

5) HOG: HOG feature was first proposed by [109] to represent objects by computing the distribution of gradi- ent intensities and orientations in spatially distributed sub- regions, which has been acknowledged as one of the best features to capture the edge or local shape information of the objects. It has shown great success for many scene classi- fication methods [22], [23], [27], [44], [103], [116], [117]. In addition, in order to further improve the description ability of HOG for remote sensing images, several extensions are also developed [118]?[121].

These human-engineering features have their advan- tages and disadvantages [56], [90], [101], [102]. In brief, the color histograms, texture descriptors, and GIST feature are global features that describe the overall statistical properties of an entire image scene in terms of certain spatial cues such as color [56], [99], texture [104]?[106], or spatial structure information [107], so they can be directly used by classifiers for scene classification, whereas SIFT descriptor and HOG feature are local features that are used for the representa- tions of local structure [108] and shape information [109].

To represent an entire scene image, they are generally used as building blocks to construct global image features, such as  the well-known bag-of-visual-words (BoVW) models [6], [8], [9], [14], [19], [29], [36], [38], [39], [55], [93], [101], [122], [123] and HOG feature-based part models [22], [23], [27], [103]. In addition, a number of improved feature encoding/ pooling methods have also been proposed in the past few years, such as Fisher vector coding [10], [14], [84], [86], spatial pyramid matching (SPM) [124], probabilistic topic model (PTM) [11], [40], [42], [43], [92], [123], etc.

In real-world applications, scene information is usually conveyed by multiple cues including spectral, color, texture, shape, and so on. Every individual cue captures only one aspect of the scene, so one single type of feature is always inadequate to represent the content of the entire scene image. Accordingly, a combination of multiple complementary features for scene classification [8], [9], [11], [12], [20], [30], [33], [85], [88], [89], [92], [125] is considered as a potential strategy to improve the performance. For example, Zhao et al. [11] presented a Dirichlet derived multiple topic model to combine three types of features at a topic level for scene classification. Zhu et al. [8] proposed a local?global feature-based BoVW scene classifica- tion method, in which shape-based global texture feature, local spectral feature, and local dense SIFT feature are fused.

Although the combination/fusion of multiple com- plementary features can often improve the results, how to effectively fuse the different types of features is still an open problem. In addition, the features introduced above are handcrafted, so the involvement of human ingenuity in feature designing significantly influences their representa- tional capability as well as the effectiveness for scene clas- sification. Especially when the scene images become more challenging, the description ability of those features may become limited or even impoverished.

B. Unsupervised-Feature-Learning-Based Methods  To remedy the limitations of handcrafted features, learn- ing features automatically from images are considered as a more feasible strategy. In recent years, unsupervised feature learning from unlabeled input data has become an attractive alternative to handcrafted features and has made significant progress for remote sensing image scene classification [20], [26], [28], [33], [37], [54], [63], [87], [95], [126]?[134].

Unsupervised feature learning aims to learn a set of basis functions (or filters) used for feature encoding, in which the input of the functions is a set of handcrafted features or raw pixel intensity values and the output is a set of learned fea- tures. By learning features from images instead of relying on manually designed features, we can obtain more discrimi- native feature that is better suited to the problem at hand.

Typical unsupervised feature learning methods include, but not limited to, principal component analysis (PCA) [135], k -means clustering, sparse coding [136], and autoencoder [137]. It is worth noting that some unsupervised feature learning models such as sparse coding and autoencoder can be easily stacked to form deeper unsupervised models.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Cheng et al . : Remote Sensing Image Scene Classif ication: Benchmark and State of the Art  6 Proceedings of the IEEE  1) PCA: PCA is probably the earliest unsupervised fea- ture extraction algorithm that aims to find an optimal rep- resentative projection matrix such that the projections of the data points can best preserve the structure of the data distribution [135]. To this end, PCA learns a linear transfor- mation matrix from input data, where the columns of the transformation matrix form a set of orthogonal basis vectors and thus make it possible to calculate the principal compo- nents of the input data. Recently, some extensions to PCA have also been proposed, such as PCANet [138] and sparse PCA [126]. In [138], PCA was employed to learn multistage filter banks, namely PCANet, which is a simple deep model without supervised training but it is able to learn robust invariant features for various image classification tasks. In [126], Chaib et al. presented an informative feature selec- tion method based on sparse PCA for high-resolution sat- ellite image scene classification. However, the description power of linear PCA features is limited. It is impossible to obtain more abstract representations since the composition of linear operations yields another linear operation [139].

Some recent methods, such as sparse coding [136], autoen- coder [137] that will be reviewed below, have been devel- oped to extract nonlinear features.

2)  K -Means Clustering: The  k -means clustering is a method of vector quantization that aims to divide a collec- tion of data items into  k  clusters, such that items within a cluster are more similar to each other than they are to items in the other clusters. Since  k -means clustering is usually carried out when label information is not available concern- ing the membership of data items to predefined classes, it is seen as a typical unsupervised learning method. The algo- rithm iteratively repeats two steps: in the assignment step, each data item is assigned to the cluster with the closest cen- troid. In the update step, the centroids are recomputed as the means of the data items in the corresponding clusters.

The algorithm terminates when the assignments no longer change. To initialize the algorithm, a universal method is to randomly choose  k  data items as the initial centroids. Owing to its simplicity,  k -means clustering method is widely used for unsupervised-feature-learning-based scene image classi- fication. The most representative example are BoVW-based methods [8], [9], [14], [19], [29], [86], [87], [89], [91], [122], [123], [132], [140] where the visual dictionaries (codebooks) are generated by performing  k -means clustering on the set of local features.

3) Sparse Coding: Recently, sparse coding [136] instead of vector quantization has been applied to dictionary learn- ing. It is a class of unsupervised method for learning an over- complete dictionary from unlabeled training samples, such that we can represent an input sample as a linear combina- tion of these atoms efficiently [136]. The coefficient vectors [26], [33], [37], [95], [128] or reconstruction residuals [20], [47] for each sample form their new feature representations.

The core of sparse coding is to encode each high-dimen- sional input vector sparsely by a few structural primitives in  a low-dimensional manifold [141]. The procedure of seek- ing the sparsest representation for a sample in terms of the overcomplete dictionary endows itself with a discriminative nature to perform classification. Learning a set of basis vec- tors using sparse coding consists of two separate optimiza- tions: the first is an optimization over sparse coefficients for each training sample and the second is an optimization over basis vectors across a batch of training samples at once.

More recently, a large number of sparse coding meth- ods have been proposed for scene classification of remote sensing images [20], [26], [28], [33], [37], [95], [128]. For example, Cheriyadat [26] adopted sparse coding to learn a set of basis functions from unlabeled low-level features. The low-level features were then encoded in terms of the basis functions to generate spare feature representations. Sheng et al. [33] introduced a sparse-coding-based multiple feature combination for satellite scene classification. Zheng et al.

[28] designed a novel method for satellite image annota- tion using multifeature joint sparse coding with spatial rela- tion constraint. However, sparse coding is computationally expensive when dealing with large-scale data. Inspired by the observation that nonzero coefficients are usually assigned to nearby elements in the dictionary, Wang et al. [142] pro- posed locality constrained linear coding (LLC) method. LLC assumes that a data can be reconstructed by using its  k -near- est neighbors in the dictionary. Thus, the sparse coefficients can be computed through solving a least squares problem.

The weights for the remaining atoms are set to zero and the sparsity is replaced with locality [85], [142].

4) Autoencoder: Autoencoder [137] is another famous unsupervised feature learning method that has been suc- cessfully applied to land-use scene classification [63], [129], [133], [134]. It is a symmetrical neural network that is used to learn a compressed feature representation from high- dimensional feature space in an unsupervised manner. This is achieved by minimizing the reconstruction error between the input data at the encoding layer and its reconstruction at the decoding layer. The number of nodes in the encod- ing layer is equal to that of the decoding layer. To reduce the dimensionality of data, the autoencoder network recon- structs the feature representation with fewer nodes in the hidden layers. The activations of the hidden layer are usu- ally used as the compressed features. Gradient decent with back propagation is used for training the networks. To train a multilayer stacked autoencoder, the most important issue is how to initialize the networks. If initial weights are large, autoencoder tends to converge to poor local minima, while small initial weights lead to tiny gradient in early layer, making it infeasible to train such a multilayer net- work. Fortunately, a pretraining approach was proposed by Hinton et al. [137], which found that initializing weights using restricted Boltzmann machines (RBMs) is closer to a good solution.

In real applications, the aforementioned unsupervised feature learning methods have achieved good performance    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Cheng et al . : Remote Sensing Image Scene Classif ication: Benchmark and State of the Art  Proceedings of the IEEE 7  for land-use classification, especially compared to hand- crafted-feature-based methods. However, the lack of seman- tic information provided by the category labels cannot guarantee the best discrimination ability between classes because unsupervised feature learning methods do not make use of data class information. For a better classification per- formance, we still need to use labeled data to develop super- vised feature learning methods, which will be reviewed below, to extract more powerful features.

C. Deep-Feature-Learning-Based Methods  Most of the current state-of-the-art approaches generally rely on supervised learning to obtain good feature represen- tations. Especially in 2006, a breakthrough in deep feature learning was made by Hinton and Salakhutdinov [137].

Since then, the aim of researchers has been to replace hand- engineered features with trainable multilayer networks and a amount of deep learning models have shown impressive feature representation capability for a wide range of appli- cations including remote sensing image scene classification [13], [17], [45], [50], [82], [143]?[158].

On the one hand, in comparison with traditional hand- crafted features that require a considerable amount of engi- neering skill and domain expertise, deep learning features are automatically learned from data using a general-purpose learning procedure via deep-architecture neural networks.

This is the key advantage of deep learning methods. On the other hand, compared with aforementioned unsupervised feature learning methods that are generally shallow-struc- tured models (e.g., sparse coding), deep learning models that are composed of multiple processing layers can learn more powerful feature representations of data with multiple levels of abstraction [159]. In addition, deep feature learning methods have also turned out to be very good at discovering intricate structures and discriminative information hidden in high-dimensional data, and the features from toper lay- ers of the deep neural network show semantic abstracting properties. All of these make deep features more applicable for semantic-level scene classification.

Currently, there exist a number of deep learning mod- els, such as deep belief nets (DBNs) [160], deep Boltzmann machines (DBMs) [161], stacked autoencoder (SAE) [162], convolutional neural networks (CNNs) [163]?[167], and so on. Limited by the space, here we mainly review two widely used deep learning methods including SAE and CNN.

Readers can refer to relevant publications for other deep learning methods.

1) SAE: SAE [162] is a relatively simple deep learning model that has been successfully applied for remote sens- ing image scene classification [13], [134]. An SAE consists of multiple layers of autoencoders in which the outputs of each layer are wired to the inputs of the successive layer. To train an SAE, a feasible way is to use greedy layer-wise train- ing scheme [168]. Specifically, one should first train the first  layer on raw input data to obtain parameters and transfer the raw data into an intermediate vector consisting of activa- tions of the hidden units. Then, this process is repeated for subsequent layers by using the output of each layer as input for the subsequent layer. This method trains the parameters of each layer individually while freezing parameters for the remainder of the model. To obtain better results, after layer-wise training is completed, fine-tuning is performed to tune the parameters of all layers at the same time with a smaller learning rate. Compared to a single autoencoder as mentioned in the previous subsection, the feature repre- sentation power of SAE can be significantly strengthened.

This can be easily explained: with the composition of multi- ple autoencoder that each transforms the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level, we can learn very powerful representations. This has been proven in literature [13], [134], [169]?[171].

2) CNNs: CNNs are designed to process data that come in the form of multiple arrays, for example, a multispectral image composed of multiple 2-D arrays containing pixel intensities in the multiple band channels. Starting with the impressive success of AlexNet [163], many representa- tive CNN models including Overfeat [164], VGGNet [165], GoogLeNet [166], SPPNet [167], and ResNet [172] have been proposed in the literature. There exist four key ideas behind CNNs that take advantage of the properties of natu- ral signals, namely, local connections, shared weights, pool- ing, and the use of many layers [159].

The architecture of a typical CNN is structured as a series of layers.

1)  Convolutional layers: They are the most important ones for extracting features from images. The first lay- ers usually capture low-level features (such as edges, lines, and corners) while the deeper layers are able to learn more expressive features (such as structures, objects, and shapes) by combining low-level ones.

2)  Pooling layers: Typically, after each convolutional layer, there exist pooling layers that are created by computing some local nonlinear operation of a par- ticular feature over a region of the image. This pro- cess ensures that the same result can be obtained, even when image features have small translations or rotations, which is very important for scene classifica- tion and detection.

3)  Normalization layers: They aim to improve generali- zation inspired by inhibition schemes presented in the real neurons of the brain.

4)  Fully connected layers: They are typically used as the last few layers of the network. By removing con- straints, they can better summarize the information conveyed by lower level layers in view of the final decision. As a fully connected layer occupies most of the parameters, overfitting can easily happen. To pre- vent this, the dropout method was employed [163].

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Cheng et al . : Remote Sensing Image Scene Classif ication: Benchmark and State of the Art  8 Proceedings of the IEEE  In recent years, there has been an extensive popularity of CNNs in various remote sensing applications, such as geo- spatial object detection [45], [50], [173] and land-use scene classification [82], [143], [144], [146]?[150], [152], [153].

For instance, to address the problem of object rotation vari- ations, Cheng et al. [45] proposed a novel approach to learn a rotation-invariant CNN (RICNN) model for boosting the performance of object detection on the basis of the existing CNN models. This is achieved by optimizing a new objective function via imposing a regularization constraint term, which explicitly enforces the feature representations of the train- ing samples before and after rotation to be mapped close to each other. Castelluccio et al. [152] and Nogueira et al. [153] explored the use of existing CNNs for scene classification of remote sensing images by using three different strategies: full training, fine tuning, and using CNNs as feature extractors.

Experimental results show that fine tuning tends to be the best performing strategy on small-scale data sets.

I V.  THE PROPOSED N W PU-R ESISC45 DATA SET  Since the emergence of UC Merced land-use data set [38], considerable efforts have been dedicated toward the construction of various data sets [9], [11], [17], [33], [38], [82] for the task of remote sensing image scene classifica- tion. Despite the remarkable progress made so far, as we reviewed in Section II and summarized in Table 1, almost all existing remote sensing image data sets have a number of notable limitations, such as the small scale of scene classes, the image numbers per class and the total image numbers, the lack of scene variations and diversity, and the saturation of classification accuracy (e.g., ~100% accuracy on the most popular UC Merced data set with deep CNN features [82]).

These limitations have severely limited the development of new data-driven algorithms and also prohibited the wide use of deep learning methods because almost all deep learning models are required to be trained on large training data sets with abundant and diverse images to avoid overfitting.

Under such a circumstance, proposing a large-scale data set with big image variations and diversity is highly desirable due to the potential benefits for remote sensing community.

This motivates us to propose the NWPU-RESISC45 data set, which is a freely and publicly available benchmark data set used for remote sensing image scene classification.

A. Selecting Scene Classes for NWPU-RESISC45 Data Set  The first step of constructing the data set is to select a list of representative scene classes. To this end, we first investi- gated all scene classes of the existing data sets [9], [11], [17], [33], [38], [82] to form 30 widely used scene categories.

Then, we searched the keywords of ?OBIA,? ?GEOBIA,? ?land cover classification,? ?land-use classification,? ?geographic image retrieval,? and ?geospatial object detection? on Web  of Science and Google Scholar to carefully select 15 mean- ingful scene classes. In such a case, we obtained a total of 45 scene classes to construct the proposed NWPU-RESISC45 data set. These 45 scene classes are as follows: airplane, air- port, baseball diamond, basketball court, beach, bridge, chap- arral, church, circular farmland, cloud, commercial area, dense residential, desert, forest, freeway, golf course, ground track field, harbor, industrial area, intersection, island, lake, meadow, medium residential, mobile home park, mountain, overpass, palace, parking lot, railway, railway station, rec- tangular farmland, river, roundabout, runway, sea ice, ship, snowberg, sparse residential, stadium, storage tank, tennis court, terrace, thermal power station, and wetland.

Here, it should be pointed out that the term of ?scene? used for this scene classification data set refers to its generalized form, including land-use and land-cover classes (e.g., commer- cial area, farmland, forest, industrial area, mountain, and resi- dential area), man-made object classes (e.g., airplane, airport, bridge, church, palace, and ship), as well as landscape nature object classes (e.g., beach, cloud, island, lake, river, and sea ice). Accordingly, these classes contain a variety of spatial pat- terns, some homogeneous with respect to texture, some homo- geneous with respect to color, others not homogeneous at all.

B. The NWPU-RESISC45 Data Set  The proposed NWPU-RESISC45 data set consists of 31 500 remote sensing images divided into 45 scene classes. Each class includes 700 images with a size of 256  ?  256 pixels in the red? green?blue (RGB) color space. The spatial resolution varies from about 30 to 0.2 m per pixel for most of the scene classes except for the classes of island, lake, mountain, and snow- berg that have lower spatial resolutions. As same as most of the existing data sets [9], [11], [17], [33], this data set was also extracted, by the experts in the field of remote sensing image interpretation, from Google Earth (Google Inc.) that maps the Earth by the superimposition of images obtained from satel- lite imagery, aerial photography and geographic information system (GIS) onto a 3-D globe. The 31 500 images cover more than 100 countries and regions all over the world, including developing, transition, and highly developed economies. Fig. 2 shows two samples of each class from this data set.

The proposed NWPU-RESISC45 data set has the following three notable characteristics compared with all existing scene classification data sets including [9], [11], [17], [33], [38], [82].

1) Large Scale: The resurrection of deep learning has had a revolutionary impact on the current state of the arts in machine learning and computer vision. A major contributing factor to its success is the availability of large-scale data sets that allows deep networks to develop their full potential. Unfortunately, as can be seen from Table 1, almost all existing data sets are small scale. To address this problem, we propose a large-scale, freely and publicly available benchmark data set, which covers 31 500 images and 45 scene classes with 700 images per class. Taking the most widely used UC Merced data set [38] as comparison, the total image number of our data set is 15 times larger than it.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Cheng et al . : Remote Sensing Image Scene Classif ication: Benchmark and State of the Art  Proceedings of the IEEE 9  Fig. 2. Some example images from the proposed NWPU-RESISC45 data set, which was carefully designed under all kinds of weathers, seasons, illumination conditions, imaging conditions, and scales. Accordingly, these images generally have rich variations in translation, viewpoint, object pose and appearance, spatial resolution, illumination, background, occlusion, etc.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Cheng et al . : Remote Sensing Image Scene Classif ication: Benchmark and State of the Art  10 Proceedings of the IEEE  To the best of our knowledge, our data set is of the largest scale on the number of scene classes and the total image number.

The creation of this new data set will enable the community to develop and test various data-driven algorithms to further boost the state of the arts.

2) Rich Image Variations: Tolerance to image variations is an important desired property of any scene classification system, be it human or machine. However, most of the existing data sets are not very rich in terms of image vari- ations. On the contrary, our images were carefully selected under all kinds of weathers, seasons, illumination condi- tions, imaging conditions, and scales. Thus, for each scene category, our data set possesses much rich variations in translation, viewpoint, object pose and appearance, spatial resolution, illumination, background, occlusion, etc.

3) High Within-Class Diversity and Between-Class Similarity: Many top-performing methods built upon deep neural networks have achieved saturation of classification accuracy on most of the existing data sets owing to their sim- plicity, or rather the lack of variations and diversity. With this in mind, our new data set is rather challenging with high within-class diversity and between-class similarity. To this end, we obtained images under all kinds of conditions and added some more fine-grained scene classes with high semantic overlapping such as circular farmland and rectan- gular farmland, commercial area and industrial area, basket- ball court and tennis court, and so on.



V. BENCHM A R K ING R EPR ESEN TATI V E METHODS  Current methods have only been evaluated on small data sets.

It is unclear how they perform on a large-scale data set. In this section, we evaluate a number of representative approaches for scene classification on our NWPU-RESISC45 data set.

A. Representative Methods  In total, 12 kinds of previous and current state-of-the-art features, which have been widely used for scene classification, are selected for evaluation. Specifically, our selections include three global handcrafted features: color histograms, LBP, GIST, three unsupervised feature learning models: dense SIFT- based BoVW feature and its two extensions (BoVW+SPM and LLC), three deep-learning-based CNN features: AlexNet, VGGNet-16, GoogLeNet, and three fine-tuned CNN features.

1) Color histograms: Color histograms feature [99] is almost the simplest handcrafted feature that has been widely used for image classification. In our work, the color histograms feature is directly computed in RGB color space because of its simplicity. Each channel is quantized into 64 bins for a total histogram feature length of 192. The his- tograms are normalized to have an L1 norm of one.

2) LBP: LBP feature [106] is a theoretical simple yet efficient approach for texture description by computing the frequencies of local patterns in subregions. For an image, it  first compares each central pixel to its  N  neighbors: when the neighbor?s value is bigger than the value of center pixel, output 1, otherwise, output 0. This forms an  N -bit decimal number to describe each center pixel. The LBP descriptor is obtained by computing the histogram of the decimal num- bers over the image and results in a feature vector with   2   N dimensions. In our implementation, we set  N = 8 , hence resulting in a 256-dimensional LBP feature.

3) GIST: GIST feature [107] describes the global spatial layout of a scene image. To compute GIST feature, each image is first decomposed by a bank of multiscale-oriented Gabor filters (set to eight orientations and four different scales as the original work of [107]). The result of each filter is then averaged over 16 nonoverlapping regions arranged on a  4 ? 4  grid. The resulting image representation is a 512-dimensional feature vector.

4) BoVW: BoVW [174] has been possibly one of the most popular visual features during the last decade. Owing to its efficiency and invariance to viewpoint changes, it has been widely used by the community for geographic image clas- sification. There are usually five major steps in the BoVW framework used for image classification.

a) Patch extraction: With an image as input, the out- puts of this step are image patches. This step is implemented via sampling local areas of images in a dense or sparse manner.

b) Patch representation: Given image patches, the outputs of this step are their feature descriptors such as the popular SIFT descriptor.

c) Codebook generation: The inputs of this step are feature descriptors extracted from all training images and the output is a visual codebook. The codebook is usually formed by unsupervised  k -means clustering over all feature descriptors, as described in Section III-B.

d) Feature encoding: Given feature descriptors and codebook as input, this step quantizes each feature descrip- tor into a visual word in the codebook.

e) Feature pooling: This step pools encoded local descriptors into a global histogram representation for each image.

5) BoVW + SPM: SPM [124] is used to incorporate spatial information of a scene image. In brief, SPM divides each image into increasingly finer spatial subregions, con- structs the BoVW representations for each subregion, and then concatenates them to represent the image. In our implementation, we divide each image into  1 ? 1  and  2 ? 2 subregions. Thus, given a codebook with the size  K , we can obtain a  5K -dimensional feature vector for each image by using BoVW+SPM scheme.

6) LLC: LLC [142] is an integrated variation of sparse coding and BoVW. It utilizes the locality constraints to project each feature descriptor into its local-coordinate neighbors, and the projected coordinates are integrated by max pooling to form the final representation. Note that in our implementation of LLC, we adopted the same codebook as BoVW that was constructed    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Cheng et al . : Remote Sensing Image Scene Classif ication: Benchmark and State of the Art  Proceedings of the IEEE 11  simply by  k -means clustering without optimization. Therefore, LLC and BoVW have the same feature dimension.

7) AlexNet: AlexNet [163] was first proposed by Alex Krizhevsky et al. and was the winner of ImageNet large-scale visual recognition challenge (ILSVRC) [175] in 2012. It is composed of five convolutional layers and three fully con- nected layers. Besides, response normalization layers fol- low the first and second convolutional layers. Max-pooling layers follow both response normalization layers and the fifth convolutional layer. It is a landmark study for machine learning and computer vision since it was the first work to employ nonsaturating neurons, GPU implementation of the convolution operation, and dropout to prevent overfitting.

In our evaluation, we extracted the AlexNet CNN feature from the second fully connected layer, which results in a feature vector of 4096 dimensions.

8) VGGNet-16: VGGNet was proposed in [165] and has won the localization and classification tracks of the ILSVRC-2014 competition. It has two famous architectures: VGGNet-16 and VGGNet-19. In this evaluation, we used the former one because of its simpler architecture and slightly better performance. It has 13 convolutional layers, five pool- ing layers, and three fully connected layers. The VGGNet-16 CNN feature was also extracted from the second fully con- nected layer to obtain a feature vector of 4096 dimensions.

9) GoogLeNet: GoogLeNet [166] is another representa- tive CNN architecture that achieved new state of the art for the task of classification and detection in the ILSVRC-2014.

The main hallmark of its architecture is the improved utili- zation of the computing resources inside the network. By a carefully crafted design, the depth and width of the network were increased while keeping the computational budget con- stant. GoogLeNet has two main advantages: a) utilization of filters of different sizes at the same layer, which maintains more spatial information; and b) reduction of the number of parameters of the network, making it less sensitive to over- fitting and allowing it to be deeper. The 22-layer GoogLeNet has more than 50 convolutional layers distributed inside the inception modules. However, in fact, GoogLeNet has 12 times fewer parameters than AlexNet. In our work, we extracted the GoogLeNet CNN feature from the last pooling layer to form a feature vector of 1024 dimensions.

10) Fine-Tuned AlexNet, VGGNet-16, and GoogLeNet: Except for using the aforementioned three CNNs as universal feature extractors, we also fine-tuned them on our new data set to obtain better performance without using any data augmenta- tion technique. Table 2 summarizes the detailed parameters used for fine-tuning. Here, we used a bigger learning rate  (0.01) for the last layer to make the model be able to jump out of local optimum and to converge fast and a smaller learning rate (0.001) for other layers to allow fine-tuning to make pro- gress while not clobbering the initialization.

B. Experimental Setup  To make a comprehensive evaluation, two training?test- ing ratios are considered: 1) 10%?90%: the data set was ran- domly split into 10% for training and 90% for testing (70 training samples and 630 testing samples per class); and 2) 20%?80%: the data set was randomly divided into 20% for training and 80% for testing (140 training samples and 560 testing samples per class).

For BoVW, BoVW+SPM, and LLC methods, we adopted the widely used densely sampled SIFT descriptor to describe each image patch with the patch size set to be  16 ? 16  pix- els and the grid spacing to be 8 pixels to balance the speed/ accuracy tradeoff, which is the same as the research work of [86]. The sizes of visual codebooks were set to be 500, 1000, 2000, and 5000, respectively, to study how they affected the classification performance.

The AlexNet model, the VGGNet-16 model, and the GoogLeNet model, which were pretrained on ImageNet data set [175], are obtained from https://github.com/BVLC/ caffe/wiki/Model-Zoo for deep CNN feature extraction.

To further improve their generalization capability, we also fine-tuned them by using the parameters as summarized in Table 2. All three CNN models were implemented on a PC with 2 2.8-GHz 6-core CPUs and 32-GB memory. In addi- tion, a GTX Titan X GPU was also used for acceleration.

For fair comparison, the image classification was carried out by using linear support vector machines (SVMs) for all 12 kinds of image features. We implemented it with the LibSVM toolbox [176] and used the default setting in linear SVM ( C = 1 ) to tune the tradeoff between the amount of accepted errors and the maximization of the margin. Specifically, after extracting the aforementioned 12 kinds of image features, a linear one-versus-all SVM classifier was trained for each scene class by treating the images of the chosen class as posi- tives and the rest images as negatives. An unlabeled test image is assigned to label of the classifier with the highest response.

C. Evaluation Metrics  There exist three widely used, standard evaluation met- rics in image classification: overall accuracy, average accu- racy, and confusion matrix. The overall accuracy is defined  Table 2 Parameters Utilized for CNN Model Fine-Tuning    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Cheng et al . : Remote Sensing Image Scene Classif ication: Benchmark and State of the Art  12 Proceedings of the IEEE  as the number of correctly classified samples, regardless of which class they belong to, divided by the total number of samples. The average accuracy is defined as the average of the classification accuracy of each class, regardless of the number of samples in each class. The confusion matrix is an informative table used to analyze all the errors and confu- sions between different classes which is generated by count- ing each type of correct and incorrect classification of the test samples and accumulating the results in the table.

Here, it should be pointed out that our data set has the same image number per class, so the value of overall accu- racy equals to the value average accuracy. Thus, in this paper, we just used the metrics of overall accuracy and confusion matrix to evaluate all classification methods. In addition, in order to obtain reliable results for the metrics of overall accuracy and confusion matrix, we repeated the experiment ten times for each training?testing ratio and report the mean and standard deviation of the results.

D. Experimental Results  Fig. 3 first presents the overall accuracies of the methods of BoVW, BoVW+SPM, and LLC with the visual codebook sizes  being set to 500, 1000, 2000, and 5000, respectively, under the training ratios of 10% and 20%. As can be seen from it, the size of codebook affects the accuracy remarkably and 1) for the training ratio of 10%, the best results are obtained with the codebook sizes of 5000, 500, and 5000; and 2) for the training ratio of 20%, the best results are obtained with the codebook sizes of 5000, 500, and 5000, for BoVW, BoVW+SPM, and LLC methods, respectively. Consequently, in our subsequent evaluations, the results of BoVW, BoVW+SPM, and LLC are all based on these optimal parameter settings.

Tables 3?6 show the overall accuracies of three hand- crafted global features, three unsupervised feature learning methods, three deep CNN features, and three fine-tuned CNN features, respectively, under the training ratios of 10% and 20%. The following can be seen in Tables 3?6.

1) Handcrafted low-level features have the relatively lowest classification accuracy.

2) BoVW and its two variants, namely BoVW+SPM and LLC, improve significantly handcrafted low-level fea- tures. Actually, they act as mid-level image features that are  Fig. 3. Overall accuracies of the methods of BoVW, BoVW+SPM, and LLC with the visual codebook sizes being set to be 500, 1000, 2000, and 5000, respectively, under the training ratios of: (a) 10%; and (b) 20%.

Table 3 Overall Accuracies (%) of Three Kinds of Handcrafted Features Under the Training Ratios of 10% and 20%  TABLE 4 Overall Accuracies (%) of Three Kinds of Unsupervised Feature Learning Methods Under the Training Ratios of 10% and 20%  TABLE 5 Overall Accuracies (%) of Three Kinds of Deep-Learning- Based CNN Features Under the Training Ratios of 10% and 20%  TABLE 6 Overall Accuracies (%) of Three Kinds of Fine-Tuned Deep CNN Features Under the Training Ratios of 10% and 20%    This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Cheng et al . : Remote Sensing Image Scene Classif ication: Benchmark and State of the Art  Proceedings of the IEEE 13  built on low-level features such as SIFT descriptor [108] and hence provide more semantic and more robust represen- tations than the low level ones for filling up the so-called semantic gap.

3) Deep CNN features outperform all handcrafted features and unsupervised feature learning methods in very big margins (at least 30% performance improvement).

This demonstrates the huge superiority of the current-dom- inated deep learning methods in comparison with previous state-of-the-art methods.

4) By fine-tuning the three off-the-shelf CNN mod- els, the accuracy was further boosted by at least six percent- age points, resulting in the highest accuracy.

Figs. 4 and 5 show the confusion matrices of different methods under the training ratios of 10% and 20%, respec- tively, where the entry in the  i th row and  j th column denotes the rate of test samples from the  i th class that are classified as the  j th class. Limited by the space, we here just report the confusion matrices with the highest overall accuracies selected from each class of features (handcrafted features, unsupervised feature learning methods, CNN features, and fine-tuned CNN features). From Figs. 4 and 5, we observed the following.

1) In agreement with the overall accuracies as shown in Tables 3?6, handcrafted features have the lowest per-class accuracies, unsupervised feature learning methods take the  second place, and deep-learning-based CNN features have the highest per-class accuracies.

2) For color histogram feature, the relatively big confu- sions happen between ?golf court? and ?meadow? because they are characterized by green color.

3) For BoVW and deep-learning-based CNN features, the relatively big confusions happen between ?church? and ?pal- ace? and ?dense residential? and ?medium residential? because of their similar global structure or spatial layout. This suggests that a potential way to classify more challenging image scenes may be deep-learning-based methods in combination with dis- criminative attributes oriented methods such as [23].

V I.  CONCLUSION A ND DISCUSSION  The significant development of remote sensing technol- ogy over the past decade has been providing us explosive remote sensing data for intelligent earth observation such as scene classification using remote sensing images.

However, the lack of publicly available ?big data? of remote sensing images severely limits the development of new approaches especially deep-learning-based methods. This paper first presented a comprehensive review of the recent progress in the field of remote sensing image scene classifi- cation, including benchmark data sets and state-of-the-art methods. Then, by analyzing the limitations of the existing  Fig. 4. Confusion matrices under the training ratio of 10% by using the following methods: (a) color histograms; (b) BoVW; (c) VGGNet-16; and (d) fine-tuned VGGNet-16.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

Cheng et al . : Remote Sensing Image Scene Classif ication: Benchmark and State of the Art  14 Proceedings of the IEEE  data sets, we proposed a large-scale, freely and publicly available benchmark data set to enable the community to develop and evaluate new data-driven algorithms. Finally, we evaluated a number of representative state-of-the-art methods including deep-learning-based methods for the task of scene classification using the proposed data set and reported the results as a useful performance baseline for future research.

However, until now, almost all scene classification methods have relied on only traditional remote sensing, i.e., overhead imagery, to distinguish different types of land cover in a given region. In fact, the more recent develop- ment of social media and spatial technology has significant potential for complimenting the shortcoming of traditional means of scene classification. First, the rapid development of social media especially the online photo sharing web- sites, such as Flickr, Panoramio, and Instagram, have been collecting sorts of information of ground objects from geo- tagged ground photo collections. Second, by linking earth observation data coming from satellites and geographic information systems (GISs) to location-aware spatial tech- nologies, such as global positioning system (GPS), wire- less fidelity (WIFI), and smartphones, we are locating at a powerful geographic information system in which we can readily know, at any time, where every ground object is located on the surface of the Earth. Therefore, we can  say that these geo-tagged ground photo collections and location-based geographic resources act as a repository of all kinds of information, including who, what, where, when, why, and how. This allows us to perform knowledge discovery by crowdsourcing of information through these location-based social medial data. For example, using only remote sensing images, it is more difficult to tell where a certain object comes from, but with Twitter generating more than 500 million Tweets per day (of which a good portion are tagged with latitude?longitude coordinates), we can map ?what-is-where? easily on the surface of the Earth using the ?what? and ?where? aspects of the infor- mation. Besides, compared with remote sensing images, the ground photos uploaded by user hold higher resolu- tion and are quite different from satellite remote sensing in the observation direction, which can well capture the detail and vertical characteristics of ground objects. All the additional information is in fact very useful for the classi- fication and recognition of remote sensing images because it could better help individuals learn more powerful (or multiview) feature representations. Consequently, in the future work, we need to explore new methods and sys- tems in which the combination of remote sensing data and information coming from social media and spatial tech- nology can be deployed to promote the state of the art of remote sensing image scene classification. ?  Fig. 5. Confusion matrices under the training ratio of 20% by using the following methods: (a) color histograms; (b) BoVW; (c) VGGNet-16; and (d) fine-tuned VGGNet-16.

This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.

