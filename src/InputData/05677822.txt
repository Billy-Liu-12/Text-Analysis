Notice of Retraction

Abstract-new algorithm of mining frequent closed pattern about distributed data stream was proposed. The algorithm bases on the three-layer structure model. First, the leaf-layer nodes overlap the collecting item-sets. Second, the middle A-layer nodes expurgate the infrequent items of overlapped items again and again, and then insert them into prefix-sub-tree. Third, the top-layer nodes and the middle-layer ones answer the clients? request online in feedback way. Analysis and experimental results indicate: This algorithm really resolves the heavy workload between layers and has favorable application prospect.

Key Words-distributed data stream; frequent closed pattern; data mining; overlapped item-set; GCC algorithm

I. INTRODUCTION Compare with traditional database, it is difficult to mine  frequent closed item-sets in data stream system because of the enormous data and high real-time. In recent years, date steam appears in a distributed way as a result of widespread application of distributed network (such as sensor network, distributed network monitoring and so on), so it is named distributed data stream [1]. In the distributed environment, Mining frequent closed item-set doesn?t deal with the data stream that signal nodes produced, but need compute the data streams S1?S2???Sm that multi-nodes singly produced at the same time, and the data size is m times as big as the former in a unit time. As result, mining frequent closed item-set in data stream system will face a big challenge. At present it is one of research hotspots in data mining domain.

In 2005, Manjhi A put forward a typical mining model (HCS [2]) of distributed data stream. It mines frequent pattern of data stream by merger layer by layer. The result that this model produced can meet the given error, but workload between layers is so heavy. So, it is not applied. In literature [3], the algorithm of mining frequent closed pattern which is based on DSFCI-tree is further proposed. But the synchronization among the leaf nodes of layers should be taken into account, so the workload is heavy all the same. In literature [4][5], it effectively solved the communicate among layers in biased sampling. However, it only mined the prescriptive frequent set in the prescriptive probability. In another words, the algorithm is non-deterministic. In literature [6], the communication cost of the algorithm is low, but it only applied to mining frequent item-set, and was invalid to the frequent pattern.



II. DEFINITION AND DESCRIPTION  Definition 1 ? = { 1 2, ,..., ,...ix x x } is a set made up of all probable appeared items in distributed data stream DDS={S1?S2???Sm}. ix  is one of the items, and then any pure subset X of ?  is named item, and is called pattern. In DDS, the count of items including X  is named support count, labeled X.f; the support degree of X is X.s, X.s =X.f /|DDS|, |DDS| is the all items appeared in data stream S1? S2???Sm.

Definition 2 The given support degree s ( 0 1s? ? ) and the allowable error ? ( 0 s?< )?N is the count of items that come from data stream at present. If X.f ?(s-? )N, then X is called frequent item-set, a item-set is also called frequent item-set; if X.f > N? , then call X critical frequent item-set; if X.f ? N? , the X is not a frequent item-set.

Definition 3 If there is no item Y that meets both of conditions at the same time: (1) X Y? ?(2) X.f =Y.f, then call X frequent closed item-set, and also call it frequent closed pattern. Analogously, critical frequent closed item-set can be defined.

Definition4 DS is a list of data stream 1 2, ,..., ,...,k MX X X X , and kX (1 k M? ? ) is item-set.

On the assumption that there are N (1 ? N ? M) item-sets in DS, which are the same as in kX , those item-sets are expressed by a item [ ]kX N , then [ ]kX N is overlapped item-set, N is the frequency of [ ]kX N . For example, ABD [11] is overlapped item-set and 11 is its frequency.

Definition 5 the structure of GCC tree a) It makes up of three parts: root (not saving any item),  prefix sub-tree made of critical frequent item-set and the head-list of critical frequent item-set  b) Every node except root node in prefix sub-tree is make up of 7 domain groups (itemname, f, mark, next, parent, lchild, rchild), itemnam is the name of the item that the node represents. f is the support degree of the item that is made up of all the nodes, which are from direct sub-node of root to the node. Mark is a tag field avoiding repletion in mining overlapped item-set. The next is used for pointing to the next node which has the same item name. If there is none, then next equal to null. The parent points to its parent node, lchild     points to its first child node and rchild points to its brother node.

c) Every tuple of the head-list has three domains: i tem name, support count f and index that points to the f irst node that has the same item name in the tree.

All appearance , GCC tree is very similar to FP_DS tree[7], compared to FP_DS tree, its head-list has a additional support count domain and a different way in using mark domain of every node of the tree, which is to reduce the operations of the domain. The structure of GCC tree is shown as Fig 1, and the detailed application is described in the sequel algorithm.



III. GCC ALGORITHM  A. Basic idea of the algorithm This algorithm is based on the three-layer structure model,  and called GCC model. AS shown in Fig 2. In the model, the third layer nodes are leaf nodes, every leaf-node corresponds to a data stream. In order to describe easily, we suppose that every second-layer node exactly corresponds to the K (K>1) nodes of the third on   Any leaf-node ij continually check and embody the data stream connected to them, and then change them to lapped item-sets. After dealing with K 11/ ?? ?? ?  items, they sent all the lapped item-sets that frequency is bigger than K to the second node i. And the second nodes i collect lapped item-sets which were sent by connected leaf n-odes in polling model, and after expurgating the infrequent items of overlapped items again and again, then alternately insert them into GCC tree.

The second layer and the third one (node 0) communicate to each other in feedback way. This procure is made of 5 parts.

a) Node 0 send request to all the nodes of the sec ond layer;  b) All the nodes of the second layer respectively send frequent item-set in the head-list of GCC tree to the second one;  c) After collecting all the frequent item-sets, then node 0 merge them (only holding different ones), and them send the merged item list back to the second layer;  d) The second layer nodes respectively mine their saved lapped item-sets in GCC tree, and delete the items not in merged item list, then send them to node 0;  e) This communication is over after node 0 collecting all the lapped items.

The node 0 mines the frequent closed item from the collecting lapped items with the modified FP-growth algor ithm, and then makes them as a result.

B. Generation, updating and mining of the algorithm For the input data stream, GCC tree always deal with and  update them in denomination of segment. The count of items that every segment data Li contains is decided by memory space and the time that data source waiting for. The length of segment (the count of items in the segment) can change, but every segment length can?t less than 1/ ?? ?? ? .

Algorithm 1: initializing the GCC tree Input: null; Output: initialize GCC tree  a) Generating a null head list h_list; b) Generating a prefix sub-tree, only including a root  node ; c) The algorithm is over.

d) Algorithm 2: updating head list e) Input: data segment Li; present h_list; allowable  error  ? ; Output: updated GCC tree  a) N=0?? =0; b) Inputting a item{ c) N=N+ ei.f; /*every item is a item-set (ei.f ? 1) or  lapped item-set (ei.f is frequency )*/ d) Every element ei of item { e) If (ei?h_list )  then f) h_list ?ei.f = h_list ?ei.f + ei.f ; g) Else insert ei, and h_list ?ei.f= ei.f ?}} h) h_lis is ordered by un-ascending of support count ;  i) ? = N?? ?? ?? j) For  each  ei?h_list{ k) h_list ?ei.f = h_list ?ei.f-? ? l) If (h_list ?ei.f ? 0 )  then delete ei?} m) The algorithm is over.

Algorithm 3: updating GCC tree, in order to describe easily, we call the present existent GCC tree old tree, the newly created GCC tee is called new tree.

Input: data segment Li; present h_list; allowable error? ; Output: new tree  a) Building a new tree, including copying the head list of the old tree and make it as its own head list and generating a root node ;    i  11 1k i1 ij ik ? ? ?  ? ? Level 1  Level 2  Level 3  Fig 2 GCC model  root  Fig 1   the structure of GCC_tree  Prefix-subtree  a,50,0 f,20,0 c,10,0  f,20,0 c,5,0 b,1,0  c,20,0 b,1,0  b,1,0  a,50  Head-list  f,40  c,35  b,3    b) Calling the algorithm 2, making data segment Li and allowable error ?  as inputting, updating the head list of new tree;  c) Fetching item in reverse order of old tree head list; d) For  each  ei{ e) Finding out the pattern that made of all the nodes  from every branch ei of to root node of the old tree. They are 1X ? 2X ??? iX ??  f) For each iX {; g) If (ei.f > ei.mark)  then{ h) If(ei?new tree?s h_list)  then{ i) Sorting the items of iX by h_list of the new tree, and  delete these items not including in h_list, and then insert them into the prefix sub-tree of the new tree. If the node which save the corresponding item of this pattern already exist, making its support count adding (ei.f -ei.mark); if not exist, them create a new node, and its support count f is (ei.f -ei.mark), the value of mark is 0;}  j) The value of mark of every node prefix sub-tree corresponding in the old tree add (ei.f -ei.mark) ;}}}  k) Release the space old tree holding l) Every item of segment Li{ ? m) Sorting the items by the h_list of the new tree, and  delete these items not including in h_list, and then insert them into the prefix sub-tree of the new tree.

n) Deleting the item; o) The algorithm is over.

Algorithm 4: mining the lapped item-sets of GCC tree Input: GCC tree; the smallest frequency h of lapped item-sets; Output: all the item-sets which frequency is bigger than h  a) Fetching item in reverse order of GCC tree head list;  b) For  each  ei{ c) To find out the pattern that made of all the nodes  from every branch ei of to root node of GCC tree. They are 1X ? 2X ??? iX ??  d) For  each  ei{ e) If (ei.f > ei.mark+h)  then{ f) Saving the lapped item-sets iX , and these  frequency is (ei.f -ei.mark)? g) The value of mark of every node prefix sub-tree  corresponding in the GCC tree add (ei.f -ei.mark);} h) ei.mark =0;}} i) Outputting the saved result and algorithm is over.

Basing on algorithm 4, after mining all the lapped item-sets which frequency is not below 2 in the picture 1, the gotten results is {afc[20],ac[5],c[10],f[20],a[25]}?  C. The algorithm executed in leaf nodes Algorithm 5: changing the item-sets that node ij collected to lapped item-sets which frequency is not below K continually, and sending them to the second layer node i, changing once after dealing with K 1/ ?? ?? ? items Input: data stream connected to node ij ; the biggest count  Lmax (Lmax is always multiple of 1 / ?? ?? ? , but not bigger than K 1/ ?? ?? ? ) of items that memory is allowable; allowable error ? ; Output: sending lapped item-sets which frequency is bigger than K to node i continually.

a) While (data stream is not over ); b) Inputting a item and saving it, n ++?s ++?/* the  initializing value of n and s is 0?and be used to stat the count of item-sets */  c) If (s =Lmax ||n= K 1/ ?? ?? ? )  then{ d) If (s = n)  then {call algorithm 1?} e) Calling algorithm 3, and the saved items, existed  GCC tee and allowable error ? are inputting.

f) s =0,delete the saved items?  g) If (n= K 1/ ?? ?? ? )  then { h) Calling transmission process, and sending the  mining result in algorithm 4 to node i; i) Releasing the space that GCC tree occupied at  present, and n=0?}}  D. The algorithm executed in the second layer nodes Algorithm 6: the second node i collect lapped item-sets which were sent by connected leaf nodes in polling model, and compress them in a GCC tree. When variable Send =1( it indicates that node 0 has sent a new request ), then sending the frequent item-sets in the head list of GCC tree to node 0; When variable Sese=1( it indicates that node 0 has received a newly sent merged item list, in another words, the list is created after that node 0 merge all the item-sets of the second layer nodes ), then mining all the lapped item-sets in GCC tree, deleting the items not existed in the merged item list and sending them to the node 0. In addition, the count Ni of item-sets which has been dealt with at present should be also sent to node 0.

Input: lapped item-sets; the biggest count Lmax of items memory allowable; allowable error? ; given support degree s;  Output: continually answering the request of node 0, sending frequency item-sets and compressed lapped item-sets to node  a) Calling algorithm 1; b) While (data stream is not over ); c) If (Send =1)  then { d) According to the given support degree s, sending the  frequency items of the head list in the present GCC tree; e) While (Sese ? 1) wait()? f) Mining all the lapped item-sets in GCC  tree( frequency f ? 1), deleting the items not existed in the merged item list and sending them to the node 0;  g) Sending the frequency and Ni of the item-sets which has been dealt with at present to node 0; /*the initializing value of Ni is 0 */  h) Send =0?Sese =0?} i) Inputting a lapped item-set ei and save it, n ++?Ni=    Ni + ei.f?/* the initializing value of n is 0? ei.f is the frequency of the lapped item-sets. */  j) If (n =Lmax)  then{ k) Calling algorithm 3, and the saved items, existed  GCC tee and allowable error ? are input.

l) n =0,deleting the saved lapped item-sets;}}  E. The algorithm executed in the third layer nodes Algorithm 7: the third node(node 0) be responsible for answering the request of the user system, and collect the pattern that is probable to frequent closed item-sets from the second layer nodes, in another words, these patterns is frequent in the second layer nodes at least, And then mine the frequent closed patterns in existed algorithm. Suppose that Req=1 stand for them request of user system.

Input: compressed lapped item-sets; given support degree s;  Output: present frequent closed pattern a) While (data stream is not over ); b) While (Req ? 1) wait(); c) Sending request and support degree s; d) While(not finishing collecting frequent items) wait()? e) Merging the collected frequent items, then send  back to all the nodes in the second layer; f) While(not finishing collecting frequent items and  Ni ) wait()? g) With the modified FP_growth algorithm, mining  the present frequent closed patterns, the count of items N=   m  i  Ni = ? , m is the count of the second layer nods;  h) Releasing the space that the algorithm occupied, and deleting the collected data.



IV. PERFORMANCE ANALYSIS AND EXPERIMENT RESULT  A. Performance analysis 1) The analysis of workload Theorem 1 In the GCC model, the count of the items that any  leaf node output is below the input one 1/ k? ?? ?  Proving: In the GCC model, the items which input leaf-nodes is item-sets, and the output items is lapped item-sets and the frequency of these item-sets is above K. In another word, the output items are made of K+1 input lapped items at least.

With the above words, the Theorem 1 is true.

From the Theorem 1, we know that: Although the second nodes deal with the data which is got by combining K data streams, the count of received and dealt with data it needs is not more than leaf-node does. Therefore, the workload of the second is lighter.

Theorem 2 In a length of data stream DS, the count of potential frequent closed item-set is far less than the potential critical frequent closed item-set.

Proving: N is the count of item-set in the DS, then there is /N s? ?? ?  frequent closed item-sets and /N ?? ?? ?  frequent  closed item-set in the DS. Because s ? (s is the support degree, ? is the allowable error), so /N s? ?? ? /N ?? ?? ? .

With the above words, the Theorem 1 is true.

From the Theorem 2, we know that: The count of the  items that the second nodes sent to the third ones is far less than that stored in the second ones. And in the GCC model, the second owns a few nodes (the first is K times than the second), so the workload of the third is lighter.

2) The analysis of error Theorem 3 After being mined by the GCC algorithm, the support count of any item of the distributed data stream meet with the condition: 'f N?? , and ? = 1 2? ?+ , 1? is the error that the first allows, 2? is the error that the second does, and N is the count of the items in the DDS at present.

Proving: In the leaf-node of the GCC model, the support degree X that item-set reduces is not more than K after dealing with the items. In another word, in the leaf nodes, the support that item-set X reduced meets with the condition after dealing with N items: '1 1f N?? ; In the second nodes, the support any element ei reduced meets with this condition:  2N?? ? (according the algorithm 2), and the count of item-sets including ei meets with this condition: n ? 1. So the support of any item reduced in the second nodes is not more than ? , then we know that '2 2f N?? ? ? .From these words, we got 'f = ' '1 2f f+  ? 1N? + 2 N? = N? , the Theorem 3 is true.

From the Theorem 2, we know that: The output of GCC algorithm meets with the approximation? .

B. Experiment Result Here, we analysis the advantages and disadvantages further by comparing the performance of GCC and HCS, DSFCI. We executed the experiment in the LAN and connect them according to the structural model of each algorithm. The nodes of the model are replaced by PC (Intel Pentium dual-core processor E5200, 1GB memory, main-board P45-NEO3, Win 2000) the count of the leaf nodes in every model is 12. And the experiment data was produced by IBM data generator, and use the T12I8D1000k, T is the average length of the items, I is the average length of the critical frequent item, and D is the count of the items in the data set.

The leaf nodes will read the item data corresponding to the data set one by one, by which, we simulate the environment of data stream. And the allowance error ? =0.1s, s is the support degree. In the algorithm GCC, 1? = 2? =0.5? .

In the experiment, we compare the workload of the node which has the heaviest workload in the model (as shown in Fig 3) the dosage of the nodes which occupy the largest space (as shown in Fig 4) and the system response time (as shown in Fig 5).



V. CONCLUSION The mining of the frequent closed pattern will face to a  new challenge because of the emergence of distributed data stream. Although a lot of algorithms are disposed one after another, the workload among layers is not solved well all the time. This experiment demonstrate: the GCC algorithm availably solve this question with the pattern lapped and feedback, and answer the users? query request of frequent closed pattern on line, and has good time and space efficiency.

[1] BABCOCK B, OLSTON C. Distributed top-k monitoring[C]// Management of Data. New York: ACM Press, 2003: 102-103.

[2] Manjhi A, Shkapenyuk V. Rinding(Recently) Frequent Items in Distributed Data Streams. In: Proceedings of the 21st ICDE, 2005.

[3] CHEN Zhuan-liu, HU Wei-cheng, Hu Xue-gang. Mining frequent closed pattern about distributed data stream base on DSFCI-tree [J].

Micro-electronics and Computer, 2007, 24(9):120-125. (in chinese) [4] Kollios G, Gunopoulos D, Koudas N. Efficient biased sampling for approximate clustering and outlier detection in large data sets [J]. IEEE Transactions on Knowledge and Data Engineering, 2003, 15(5): 1170-1187.

