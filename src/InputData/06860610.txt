NEW BREAKDOWN DATA GENERATION AND  ANALYTICS METHODOLOGY TO ADDRESS BEOL AND

Abstract ? Both MOL PC-CA spacer dielectric and BEOL low-k dielectric breakdown data are commonly convoluted with multiple variables induced by process steps such as lithography, etch, CMP, cleaning, and thin film deposition. The traditional method of stressing one DUT per die or multiple DUTs per die, without careful data deconvolution, is incapable of addressing current complex MOL PC-CA and BEOL low-k dielectric breakdown modeling challenges. In this paper, a new big data generation method plus an analytics procedure method is proposed to soundly evaluate both MOL and BEOL dielectric time-dependent-dielectric breakdown data. A new diagnostic reliability concept is for the first time proposed for comprehensive process diagnostics and more accurate reliability failure rate determination.

Keywords ? low-k TDDB, low-k reliability, MOL, PC-CA breakdown, global die-to-die variation, local within chip variation, data deconvolution, compound Weibull distribution, compound Poisson area scaling, voltage acceleration

I.  INTRODUCTION The continuing aggressive scaling of device dimensions and  the introduction of new device configurations, have progressively challenged dielectric breakdown behavior between middle-of-line (MOL) polysilicon control gates (PC) and diffusion contacts (CA), and throughout the back-end-of- line (BEOL) low-k dielectric. Generally, both MOL PC-CA spacer dielectric and BEOL low-k dielectric breakdown data are commonly convoluted with multiple variables originating from multiple process steps such as lithography, etch, CMP, cleaning, and thin film deposition. Such variations could be significant from die to die, and even within dies. The actual die-to-die variation could be composed of several  subpopulations with distinct parameters that cannot be described by a single Poisson distribution. This could be why non-Poisson area scaling is often observed from MOL PC-CA and BEOL low-k dielectric breakdown data [1-2]. During technology qualification, both breakdown mechanisms can challenge process and reliability engineers to determine the true intrinsic breakdown behavior while correctly diagnosing process variations. This is important because process issues can appear to degrade measured intrinsic behavior. The traditional method of stressing one DUT per die or multiple DUTs per die, without careful data deconvolution, is incapable of addressing current complex MOL PC-CA and BEOL low-k dielectric breakdown challenges. In this paper, a new end-to-end big data generation method is proposed, together with a deconvolution procedure, to soundly evaluate both MOL and BEOL dielectric time-dependent-dielectric breakdown (TDDB) data. A new process and reliability quality index, die-to-die variation distribution, is quantitatively established. Furthermore, an aggregation of reliability stress data, diagnostics data, simulation data, and yield data together with analytics is for the first time proposed as a diagnostic reliability methodology for comprehensive process diagnostics and more accurate reliability failure rate determination.



II. EXPERIMENTAL The wafers investigated in this work were fabricated across  several different technology nodes ranging from 32nm to 14nm using metal?oxide?semiconductor (CMOS) processes in a 300mm manufacturing line. Various BEOL and MOL test structures were studied for development and validation of our method. For the BEOL low-k TDDB study, dielectric constant k =2.7 and k =2.55 carbon doped glass films were fabricated by chemical vapor deposition. Deposition of TaN/Ta bilayer     barriers and Mn-doped Cu seed layers were fabricated using ionized physical vapor deposition. For the MOL PC-CA voltage-ramp-dielectric-breakdown (VRDB) study, SiN film was used between gate and contacts as the insulation spacer film. All MOL structures were laid on STI oxide to isolate the PC-CA breakdown from the gate dielectric breakdown. In order to investigate the local within die variation, we designed a special set of structures with 10-40 clone copies per die. All TDDB and VRDB tests were performed at wafer level by regular single site wafer probers and a special multi-site wafer prober. Multi-site probing time was minimized by simultaneously probing 16 sites on a 300mm wafer using up to 320 DUTs in parallel (20 DUTs per die).



III. RESULTS AND DISCUSSIONS  A. Big Data Generation and Deconvolution It is well known that significant die-to-die variations cause  TDDB stress fields to become a non-constant at die level, which causes various forms of Weibull distributions and non- Poisson area scaling [2]. In actual BEOL and MOL TDDB applications, a mixture of many Weibull distributions often seems to be a more realistic case. Distributions resulting from mixing many component distributions are designated as ?compound?.  Figures 1 and 2 experimentally demonstrate various compound Weibull distributions (RED distributions of data) for BEOL low-k TDDB and MOL PC-CA dielectric VRDB, respectively. Without further data deconvolution, such compound distributions could be present in all kinds of shapes such as straight line-like, bimodal-like, concave curvature- like, and convex curvature-like shapes.  We also have found that such compound distributions exist for almost all the BEOL and MOL TDDB and VRDB datasets we have studied from 32nm to 14nm. Overall, Weibull slopes of such RED distributions were low or very low, and simply increasing the sample size without a careful deconvolution does not help to improve the overall Weibull slope at all.

Figure 1: Experimental proof of various compound Weibull distributions for BEOL low-k TDDB with a) k=2.7, b) k=2.55.

The complexity in data convolution makes it seem impossible to accurately determine a constellation of all relevant parameters just by blind data fitting. Therefore, the first step we took was to experimentally deconvolute them at die level. For this, we designed, then stressed 10-40 clone structures per die, and repeated each stress for all dies on the  entire wafer. In Figures 1 and 2, the 70-90 individual black and blue distributions are the actual local distributions from each die, each data point representing the breakdown value of one of the test structure clones. Therefore, each one of these local distributions captures the within-die variation of the breakdown. Generally, such massive parallel stressing data generation can easily collect a total of about 500-1000 data points per wafer for our next step of data analysis. We refer to this methodology as ?big data generation?.

Figure 2: Experimental proof of various compound Weibull distributions for MOL PC-CA dielectric Vbd with a) Vbd distribution, convex curvature like; b) Vbd distribution, concave curvature like.

Figure 3: t63 die-to-die distributions for low-k TDDB in Figures 1a and 1b.

Figure 4: V63 die-to-die distributions for PC-CA VRDB in Figures 2a and 2b.

By analyzing all the individual distributions separately, we can quickly construct a critical die-to-die variation index chart.

We do this by plotting all t63 or V63 data points from the population we want to study (e.g. one wafer or multiple wafers together). Figures 3 and 4 show such data, corresponding to the data shown in Figures 1 and 2 respectively. Since for a Weibull distribution, the characteristic scale parameter at 63.2% always has the highest confidence bounds, such t63 and V63 distributions represent the real die-to-die variation much better than the single DUT from one die approach does. In other words, the distributions of Figures 3 and 4 are much  3A.1.2    more accurate than the red lines on Figures 1 and 2. In order to establish a quantitative assessment of any process induced die- to-die variation, this t63 or V63 across-wafer distribution is now the new standard for further process diagnostics and reliability evaluation. It should also be noted that electrically measured t63 and V63 values obtained by this method capture all relevant breakdown parameters, not just spacing.

Therefore, such die-to-die t63 or V63 distributions provide more comprehensive information about die-to-die variation than does a spacing distribution alone. This t63 or V63 across wafer distribution can take various shapes as shown in Figures 3 and 4. From a reliability modeling perspective, those t63 and V63 data points together with within-chip local Weibull slopes ultimately determine the true chip level failure rate and are less conservative.

Figure 5: Comparison of Weibull slopes between 12-DUT and 24-DUT sample size.

Furthermore, as shown on Figure 5, our big data tells us that there is always a wide Weibull slope variation (i.e. 3-5X delta) across the wafer, and that such variation is inversely proportional to the number of DUTs per die stressed. With spacing the primary driver of t63 or V63 values, one should expect the Weibull slope to be a function of t63 or V63, yet, it seems unlikely that such a large spread on the same wafer would be caused solely by this intrinsic nature. In order to explore the primary root cause, we conducted a Monte Carlo (MC) simulation study assuming a fixed Weibull slope and a fixed ample size (i.e. 12 DUTs). If simulated Weibull slopes from 70-90 random runs could construct a wide Weibull slope variation similar to that experimentally observed, to the first order, we would conclude that our observed Weibull slope variation is simply due to the statistical variation of our relatively small sample size per die. Figure 6 does indeed show good agreement between the two. Alternatively, by plotting experimentally obtained t63 or V63 versus Weibull slope pairs, we can also directly validate whether the Weibull slope has an obvious t63 or V63 dependence. Figure 7 shows scattered data without any such obvious dependencies typical of most wafers. Since all the data points seem to come from distributions with the same Weibull slope but different t63 or V63 and in order to eliminate the substantial cross-wafer die- to-die variation but still utilize the big data we collected, we proceeded to normalize all data points with a fixed t63 or V63 point. By considering the fact of potential Weibull slope dependence on t63 or V63, we decided to use the median t63 or V63 as our normalization point. We proposed that even a  minor Weibull slope deviation existed for extremely low and high t63 or V63 data points from a wafer, and such deviation effect could be cancelled out at median t63 or V63. By linking the Weibull slope obtained from normalization to the corresponding median t63 or V63, our established Weibull slope should have a better physical meaning. Figure 8 illustrates this analytics procedure in detail. After such normalization, clean mono-modal distributions with substantially improved Weibull slopes were usually observed (Figures 9 and 10). As compared to the compound distributions shown in Figures 1 and 2, the Weibull slopes in Figures 9 and 10 are substantially improved (2-4X). For low-k TDDB, k=2.7 and k=2.55, dielectrics actually exhibit a similar intrinsic local Weibull slope, although the Weibull slope from the compound distribution for k=2.55 dielectric was much worse than the compound distribution for k=2.7 dielectric.

This analytics suggests that although, for a given electric field, the tBD of k=2.55 dielectric is slightly worse than tBD of k=2.7 dielectric, the real problem associated with the k=2.55 dielectric was really due to its higher die-to-die variability.

Figure 6: Weibull slope variation comparison between measured and simulated cases.

Figure 7: Experimentally measured t63 and V63 versus Weibull slopes for BEOL low-k TDDB and MOL PC-CA VRDB cases.

Figure 8: Data deconvolution procedure details and its outcome of local intrinsic Weibull slope determined from large sample size data.

3A.1.3      Figure 9: Local intrinsic TDDB distributions with improved Weibull slopes after deconvolution for BEOL low-k TDDB data in Figure 1.

Figure 10: Local intrinsic Vbd distributions with improved Weibull slopes after deconvolution for MOL PC-CA VRDB data in Figure 2.

Figure 11: Multiple MC simulations on to reproduce the measured BEOL low- k TDDB distribution shown in Figure 1b.

Figure 12: Multiple MC simulation based on extracted parameters to reproduce the measured bimodal like MOL PC-CA Vbd distribution.

Lastly, with the known within-die local Weibull slope and experimentally established cross-wafer global t63 or V63 distribution, we can subjectively construct compound distributions by MC simulations as many times as we want.

We can then compare simulated distributions to our experimentally established distribution to carefully validate our method and the extracted parameters. We found that, although the distribution per die varied every time during simulation due to sample size limited statistical variation, the combined big data distributions from MC simulations can always reproduce the details of our real data nicely as shown in Figures 11 and 12. This confidently justifies our proposed big data and analytics method.

Our data deconvolution method also offers a powerful way to further quantitatively establish a true Weibull slope versus t63 or V63 relation even using the data from the same wafer.

Two approaches can be used. The first approach is for the experimental data already exhibiting an obvious Weibull slope dependence on t63 or V63 as shown in Figure 13 as an example. In order to have a precise deconvolution, we can further group different t63s based on pre-determined t63 ranges, and then apply our proposed normalization method individually within each t63 group. As shown in Figure 14, three sections of dies with different t63 ranges were grouped to establish three Weibull slopes with three associated local mean t63 values (Red, Magenta, and Blue distributions) in the order of low to high t63 values. The Weibull slope decreases with decreasing t63, as expected, following an empirical relation of ?=C0+C1*t63+C2*Ln[t63] for t63 ? 0 (about 30% Weibull slope reduction for a 50x t63 reduction). Interestingly, if we still use the entire wafer as a single group for normalization, the Weibull slope, at the entire wafer mean t63, fits very well within the Weibull slope versus t63 curve. This further supports our method of using median t63 as a normalization point to extract the associated Weibull slope.

Using this sectional grouping concept, we can adjust data points with different normalization parameters during our normalization process to further improve our intrinsic Weibull slope determination if needed. Meanwhile, a relationship of Weibull slope and t63 could also be derived from the same wafer for reliability failure rate projection. The second approach is for data not showing any obvious Weibull slope dependence on t63 or V63. In order to establish a true intrinsic Weibull slope versus t63 or V63 relation, we then have to rely on a set of structures to forcibly generate different t63 or V63 values. Let?s use MOL PC-CA as an example. We purposely designed a set of ?wimpy? test structures with different PC- CA spacings. The wimpy value represented how far off- nominal the PC-CA spacing was designed per test structure.

As shown in Figure 15, by using such structures, different V63 values at different wimpy values, and therefore different Weibull slopes could be experimentally obtained by our big data analytics method. Then, by plotting Weibull slope versus V63 for all the wimpy values, a Weibull slope dependence on V63 could be quantitatively established as well. As shown in Figure 15, for PC-CA VRDB, a relatively weak dependence of Weibull slope on Vbd was found (~30% ? change for ~8V Vbd delta). By using those two approaches, it was experimentally confirmed that dies with lower t63 or V63 were usually prone to exhibit higher extrinsic defect  3A.1.4    population as compared to the dies with higher t63 or V63. In other words, extrinsic defect effect could be amplified for dies with lower t63 or V63.

Figure 13: A weak Weibull slope dependence on t63 observed for this particular low-k wafer.

Figure 14: Sectional grouping and normalization to determine Weibull slope at different t63 values for a BEOL low-k TDDB case.

Figure 15: Using wimpy structures to study MOL PC-CA Vbd Weibull slope versus V63 relation.

Traditionally, various data fitting methods, based on different models, are used to extract a set of relevant distribution parameters with the best fit to the data points for reliability modeling. With good confidence of our big data generation and analytics method, we can conduct a fair comparison of our method with other available fitting methods. The fitting methods we used for comparison are mono-modal MLE fitting, clustering model fitting, and bimodal 5-parameter MLE fitting. It should be noted that each of those fitting models assumes only one or two universal Weibull slopes for the entire dataset. Inherently, they cannot predict more than two Weibull slopes within the distribution, and they have no capability for establishing the critical t63 or V63 die-to-die distribution. As shown in Figure 16 using a low-k TDDB distribution as an example, it is obvious that all  those fitting models severely underestimate the real Weibull slope. Among those three data fitting methods, 5-parameter bimodal fitting seems to give the best Weibull slope while mono-modal fitting gives the worst Weibull slope for this specific case. By comparing those blind fitting methods to our big data generation and analytics method, it is clear that our method gives the best and also more meaningful Weibull slope. Due to the complexity in data convolution, it seems impossible for all other fitting methods to accurately determine a constellation of all relevant parameters just by blind data fitting.

Figure 16: Comparison of various methods for the same wafer data. Big data method provides the best Weibull slope.

B. Compound Poission Area Scaling for Compound Weibull Distribtuion  It has been reported that low-k TDDB could fail to follow traditional Poisson area scaling if process induced spacing variation is significant [2]. The traditional Poisson distribution is not always adequate to predict TDDB area scaling in IC. As shown in Figure 17, based on traditional one DUT per die TDDB stress method, compound Weibull distributions always exhibit non-Poisson area scaling. However, Poisson area scaling is preserved for local, within-die area scaling as experimentally demonstrated in Figure 18. There is no fundamental area scaling physics change for all BEOL low-k and MOL PC-CA TDDB, and single Poisson area scaling physics still works nicely at local per-die level. Therefore, the fatal defect concept could still be defined in terms of blocks and it could be assumed that they are distributed uniformly over each block. If a block size is equivalent to a die size, then, within a die, single Poisson area scaling of course could still be preserved. Globally, due to a severe die-to-die variation, without data deconvolution, single Poisson area scaling can never be applied to any compound Weibull distribution. It is proved that the traditional Poisson distribution is not always adequate to predict TDDB area scaling if die-to-die variation is significant. Data transformed by as-designed area ratio clearly underestimated the reliability of compound Weibull distributions. It is well known that for BEOL low-k and MOL PC-CA cases, the spacing could be composed of several subpopulations with distinct parameters that cannot be described by a single Poisson distribution.

3A.1.5    Therefore, the defect density in Poisson yield formula should not be a constant anymore. In order to account for this non- uniform defect distribution across the wafer, a new area scaling model, compound Poisson defect distribution model, is proposed. The expected number of fatal defects from an area is a random variable instead of being a constant when applying the compound Poisson model. The probability for a chip to have k fatal defects is [3-4]    (1)  where ? is fatal defect density regarded as a random variable, and f(?) is the defect density mixing function which can be in various forms such as Gamma, Seed, and triangular functions.

As an example, by applying gamma function as f(?) in equation 2, mathematically, it can be demonstrated that non single Poisson area scaling with compound Weibull distributions could exist.

(2)  where a and b are positive numbers, and ?(a) is the gamma function. This distribution has the mean a/b and the variance a/b2. The probability to have k fatal defects in Equation 2 becomes  (3)    The distribution in Equation 3 is commonly known as the negative binomial distribution. Since yield is degraded with a single defect, we set k=0 in Equation 3 with area A1 and defect density D as shown in Equation 4   ?  ? ?? ?  ? ? ? +  = +  = DA  b  p a 11   )11(  1)0(                    (4)   where A1D = a/b and ? = a. The parameter ? is called the cluster parameter. To incorporate the compound Poisson area scaling into TDDB data, the survival function can be equated with the reliability of a Weibull distribution with a known t63 and a single Weibull slope, ?, as shown in Equation 5.

(5)    Solving for D(t) gives     (6)   With equation 6, the relationship of t63 and area can be determined by assuming the same defect density D(t). Figure 19 illustrates the non-Poisson area scaling behavior for a single Weibull distribution with the compound Poisson defect distribution. From Figure 19, it is obvious that single Poisson area scaling underestimates the breakdown time, which is consistent with all of our experimental observations. In order to have the same tbd ratio with the same ?, the compound Poisson model requires a larger area ratio to achieve it as compared to the single Poisson model. In other words, the effective area ratio needed to achieve the same tbd change is smaller in the compound Poisson model as compared to the as- designed area ratio in the single Poisson model. Furthermore, the effect of decreasing ? is exaggerated in the single Poisson model as opposed to the compound Poisson model.

Since f(?) can be in various forms, it is impractical to derive the right f(?) with limited area scaling TDDB data (i.e. 3-4 areas) and/or to determine the actual fatal area scaling ratio instead of as-designed area ratio. On the other hand, based on our big data generation and analytics method, we already demonstrated that all the TDDB distributions we have dealt with were still Weibull distributions regardless of their single Weibull or compound Weibull format. One unique characteristic of Weibull distribution is that the Weibull slope will be preserved from both horizontal (Equation 7) and vertical (Equation 8) transformations. Both single and compound Weibull distributions have this characteristic [2].

Based on this theory, a simple graphic Shift & Compare (S&C) method was developed to experimentally determine the actual fatal area ratio for the compound Weibull case as shown in Equation 9 [2].

(7)    (8)    (9)    where nF is defined as fatal area ratio, which is different from as-designed area ratio for the adjustment of different global area scaling data based on the compound Poisson area scaling model. The key foundation for using S&C method is that the Weibull shape factor will be preserved for all areas regardless of single or compound Weibull distributions. Experimentally, this assumption was already proved as shown in Figure 17.

Realistically, the real product TDDB distribution could have the same shape as our test structure if deconvolution is not  ( ) ( ) ( ) ??? ? dfe k  kP k  ? ?  ?= 0 !

( ) ( )a ebf  baa  ? =  ?? ???  ( ) ( ) ( ) ( )( )( ) ak kk  bak bakdfe  k kP +  ? ?  +? +?== ? 1!!0  ??? ?  ?  ?  ? ?? ?  ? ? ? +  = ? ? ?  ?  ? ? ?  ? ?? ?  ? ?? ?  ? ?=  ? )(1  1)( 1163 tDAt  tExptR  ??  ? ? ?  ??  ? ? ?  ? ? ? ?  ?  ? ? ?  ? ?? ?  ? ?? ?  ? =  ?  11)(  ?  ? ?  t tExp  A tD  ?/1      ?  ?? ?  ? ?? ?  ? =??  ?  ? ?? ?  ? A A  t t  ( )( ) ( ) ? ?  ? ? ?  ? ?? ?  ? ?? ?  ? =?????   21 1(1 A  ALnFLnLnFLnLn  ? ? ?  ?  ? ? ?  ? ?? ?  ? ?? ?  ?  ? ??=?  ?  ?  ?  ? ?  ?  ?  ? ? ?  ?  ? ? ?  ? ?? ?  ? ?? ?  ? ??=  ?  ?  ?  /1  2 exp1exp1)( F  BD  n  BD  nt t  t ttF  F  3A.1.6    performed. Therefore, there is no need to change the distribution shape in order to align the data from different areas. Figure 20 illustrates the S&C method in detail. A computer automation program was developed to minimize the delta of Weibull slopes from both horizontal shift and vertical shift to determine the best nF. As shown in Figure 21, if f(?) is a gamma function, S&C could generate a similar nF as Equation 5 predicted. However, using S&C is a much simpler way with no guess of f(?) and its associated parameters. It should be noted that the S&C method also works naturally for single Poisson area scaling. Now the nF determined from S&C will be identical to the as-designed area ratio. In other words, if the as-designed area ratio is not known for some reasons, using S&C would allow one to experimentally determine the as-designed ratio for the Single Poisson case. Of course, if the big data generation method is used, and a compound Weibull distribution is carefully deconvoluted, then the single Poisson model would work flawlessly to model area scaling locally, and the S&C method would no longer be needed for those compound Weibull distributions.

Figure 17: Compound TDDB Weibull data transformed by Poisson area scaling.

Figure 18: Local area scaling by single Poisson area scaling.

Figure 19: tbd area scaling with single Poisson and compound Poisson defect distribution.

Figure 20: Shift & Compare method demonstration.

Figure 21: Comparison of Shift & Compare versus compound Poisson with f(?) = gamma function for nF determination.

C. Big Data Method Applications and Advantages With our careful data deconvolution, comprehensive  process diagnostics and accurate reliability projection can be conducted. First, a t63 or V63 wafer map could be established, which should provide more accurate information than does the traditional wafer map generated from one DUT per die. A t63 or V63 wafer map is important for the study of potential wafer regional dependence caused by process inhomogeneity.

Second, a clear separation of extrinsic early fail defects from intrinsic breakdown can be obtained as shown in Figures 10 and 22. Without this unquestionable separation, early fails, simply due to narrow spacing, could be treated as extrinsic defects. Third, our big data generation and analytics method allows us to combine data from different wafers, which is not possible for the traditional method if wafer-to-wafer variation exists. Therefore, an ultra large sample size can be achieved as demonstrated in Figure 22. Also with our data deconvolution and single Poisson area transformation, we can establish an ultimate intrinsic Weibull slope determination method with a wide range span of data points down to almost 1ppm level directly with only 4000 data points (Figure 23). Lastly, a more meaningful process split comparison and process diagnostics  3A.1.7    can be achieved. As shown in Figure 24 for a PC-CA case as an example, by comparing the raw data without deconvolution, split B is worse than split A. However, with further data deconvolution, intrinsically, both split A and B have the same and solid Weibull slope. The curvature shown from the raw compound distribution is mainly determined by the V63 distribution shape for both splits. The reason split B is worse than split A is its larger die-to-die variation and smaller absolute Vbd. Together, with our previously published diagnostic structures and method [5], we could also go a step further in determining the exact root cause, such as variation of overlay, CA size, LER or material breakdown strength to explain the observed V63 delta. Figure 25 illustrates our diagnostic method in detail. For a fixed contacted poly pitch (CPP), the highest VBD point, which is defined by the intersection of positive and negative misalignment curves, should represent the most centered position of CA placement between two poly gates after patterning. This VBD point should be at 0-misalignment position if overlay is perfect. However, if there is an offset of this highest VBD point from the 0- misalignment position, we can conclude that there is a shift of CA placement, xOL, simply due to an overlay misalignment.

From Figure 25, xPP is also an important parameter, which is defined as the distance between two zero breakdown positions at positive and negative sides. xpp shall represent the available electrical spacing between two poly gate edges for CA to freely move. CPP is a fairly constant number, and the spacing between two poly gates is usually also a constant. Therefore, the actual CA size could be estimated by subtracting xpp from the design poly gate space. The breakdown strength of a dielectric per a specific test condition could also be determined from Figure 25 [6]. The x-axis of all the data points, which represents the physical spacing, shall be independent of test condition, such as voltage ramp rate.

However, the y-axis of all the data points, which represents the breakdown voltage, is expected to be a function of voltage ramp rate. It was reported already that the faster the ramp rate, the higher the breakdown field [7]. Therefore, the Ebd dependence on voltage ramp rate is shown in Figure 25. As both Ebd and Vbd increase with increasing ramp rate, simply using Vbd/Ebd method to extrapolate the actual PC-CA spacing is also acceptable. It can generate the similar spacing as the xOL + xpp method does although the xOL + xpp method is recommended. Actually, with our proposed diagnostic approach, a self-consistent check can be performed using the Ebd and xOL + xpp methods to accurately determine the real PC-CA spacing. After the details of various process parameters are extracted, according to Figure 26, we can correlate our established V63 die-to-die variation to overlay, CA size, Ebd, and local variation separately (i.e. all other contributors that cause an actual PC-CA spacing difference from the as-designed spacing, in addition to overlay and CA size). As a consequence, we can determine which parameter is the primary factor to cause the degradation of split B in Figure 24. By looking at all the bottom plots in Figure 26, it is obvious that only overlay and CA size could cause lower Vbd and poor Vbd distribution. Between them, overlay clearly is  the primary root cause. With the same analysis of split A, and by comparing the overlay misalignment between split A and B, shown in Figure 27, it was confirmed that overlay misalignment was indeed the primary root cause as there was no obvious differences of CA size and Ebd between split A and split B. As the overlay misalignment problem was mainly a process control issue instead of a fundamental issue, the feasibility of split B was still demonstrated, as shown in Figure 24, from our careful big data generation and analytics method. Optimizing split B process control is critical and could equalize split B reliability with split A reliability.

Figure 22: 11 wafers combined to generate 15000 data points.

Figure 23: All dies on the same wafer from three areas after deconvolution and normalization with total 4000 DUTs.

Figure 24: Raw compound distributions versus deconvoluted distributions.

3A.1.8     Figure 25: Proposed diagnostic method for quantitative estimate of some critical process parameters.

Figure 26: Combination of massive diagnostic data and big stress data for powerful reliability and process analysis.

Figure 27: Overlay distribution for splits A and B.

D. Failure Rate Projection for Compound Weibull Distributions  Our new data generation and analytics method also has a profound impact on reliability failure rate determination.

Generally, the traditional "one DUT per die" stress method is only valid if all dies on the wafer are exactly identical.

Otherwise, the failure rate calculated from the traditional method is just a wafer-based failure rate, and has nothing to do with the required die-level failure rate. Furthermore, using a convoluted distribution at test structure level to predict a convoluted distribution at die level but without knowing the convolution details at both levels will most likely cause an erroneous projection. Based on our experimentally deconvoluted data, we proposed a probability associated TDDB concept as shown in Equation 10 and Figure 28 for TDDB failure rate calculation. The total failure rate of chips with different distribution parameters should be a sum of individual chip failure rates of all the chips and the  correspondingly different probabilities over the probability density function as the following  (10)  where P(x) is a specific probability density function of parameter x. We propose two methods to calculate the die level failure rate for a compound distribution case. The first method is for the breakdowns that are solely determined by line-to-line and via-to-line space. By applying our powerful big data method, we can first use one group of 10-20 DUTs per die for VRDB to construct a Vbd die-to-die distribution on the entire wafer. As Ebd could be determined from our proposed diagnostic method, the actual spacing per each die shall be determined. Next we can use another group of 10-20 DUTs within the same die for constant voltage TDDB. By plotting the t63 from TDDB versus the spacing from VRDB on the same die, and for all the dies on the wafer, the field acceleration factor could be accurately determined as shown in Figure 29 with 50-70 t63 data points. In contrast, the traditional way to determine field acceleration relies on collecting only 3 to 5 interval t63 data points at 3 to 5 fields, which will have wide error bounds. Furthermore, as such data points are potentially from convoluted distributions, more errors could be brought in. Therefore, our new method demonstrated in Figure 29 offers an improved accuracy with its larger t63 sample size. It should also be noted that the statistical variation nature of t63 induced by limited sample size per distribution is covered in our method. Using the electrically determined space to calculate the stress field has an obvious advantage as compared to using physical failure analysis or just nominal space to estimate the stress field by the traditional method. Our big data VRDB + TDDB method offers a powerful way to determine TDDB field acceleration factor with much less uncertainty. As shown in Figure 29, it is also unquestionably demonstrated that under a constant voltage of 12.5V TDDB stress, the actual stress field on each die is not a constant, and could vary from 0.47V/nm to 0.66V/nm (~40% variation) for this case. A detailed investigation of a low-k TDDB acceleration model based on our proposed big data VRDB + TDDB method will be published later. When a space dependent failure rate (Figure 29) with a known intrinsic Weibull slope and its dependence on spacing is established, then, combined with a known process assumption defined spacing distribution (usually a Normal distribution), the total failure rate of all the chips can be calculated by Equations 11 and 12. If the calculated total failure rate is higher than an acceptable reliability target, a minimum insulator spacing (MinIns) should be brought in to assure that the actual failure rate integrated from MinIns to infinite in Equation 12 could meet the reliability target. With Equations 11 and 12, it is interestingly found that there is always a critical spacing that exhibits the highest failure rate.

This peak failure rate spacing is found to be insensitive to the sigma and the mean spacing variations. However the failure rate itself at this peak failure rate spacing is a strong function of both sigma and the mean spacing as shown in Figures 30 and 31. Mature process control can usually target the mean  ( ) ( ) ( )? ? =  ?== m  i  x  xi dxxPxcdfxcdfchipCDF   max  min  )(  3A.1.9    spacing fairly well. However, the sigma in the space distribution function could vary with different products, which is the most uncertain source in this failure rate calculation method. Therefore, a careful evaluation of the peak failure rate spacing with different sigma values is extremely critical for reliability engineers. Carefully determining a MinIns to assure that product reliability can be kept away from a massive fallout condition is very useful.

(11)      (12)      Figure 28: Probability associated TDDB concept.

Figure 29: Combination of Vbd and TDDB big data to determine field acceleration factor with many t63 data points.

Figure 30: Calculated failure rate based on Equations 9 and 10. All cases show a critical spacing with the highest failure rate.

Figure 31: Examples showing the peak failure rate spacing is insensitive to sigma and mean change while peak failure rate is a strong function of both sigma and mean change.

On the other hand, the spacing may not be the only parameter to determine the overall TDDB. Based on Equation 13, it is well known that a Weibull CDF is a function of t63 or V63 and ?. As a t63 or V63 distribution and an intrinsic ? versus t63 or V63 relation can be exclusively determined by our proposed big data method discussed above, naturally a superposition based approach can be directly applied to calculate the realistic chip level failure rate without a bridge to the spacing. As t63 and V63 are determined by all relevant breakdown parameters, not just spacing, therefore this method should be a universal method for chip level failure rate calculation. The concept also can be applied to all reliability failure mechanisms including BEOL EM, BEOL SIV, and FEOL gate dielectric with either Lognormal or Weibull statistics. The total failure rate summing from all chips with different t63 or V63 values and correspondingly different probabilities over the probability density function could be described by Equations 14 and 15  ( ) nx  ett  x x  ExpxF ? ?  ?  ?  ? ?  ?  ?  ? ? ?  ?  ? ? ?  ? ?? ?  ? ?? ?  ? ??=  )( arg1  ?  (13)  ( ) ( ) ( )dxxFxPxF x xtotal ?=  max  min  (14)  ? ? ?  ?  ? ? ?  ? ? ? ?  ? ? ?????  ? ?  ? ? ? ?  ? ?=  ? 5.0/1  63 s VExp  A A  Ct dd ref  p use ?  ?  ( ) ( ) dse  t EOLF  uss  use total     63 2 1exp1 ?  ?  ??  ??? ? ?? ?  ?  ?  ?? ?  ?  ?  ? ?  ?  ?  ? ?  ?  ?  ? ?  ?  ?  ? ?  ?  ? ?? ?  ? ?? ?  ? ??= ?  3A.1.10      ( ) ( )  ( ) ? ? ?  ?  ? ? ?  ? ? ? ?  ? ? ???  ? ?  ? ? ? ? ? ?  ? ? ?=  ? ?  ? ? ?  ? ?? =  ? kk  mean  xExpxkxP  orxxExpxP  ???  ???    2 22  (15)   where n is the area scaling ratio, x could be either t63 or V63, k is the shape parameter and ? is the scale parameter of the Weibull distribution. For VRDB and TDDB, using P(x) in a Weibull format is recommended. As in Weibull, the distribution shape could be preserved after area scaling, which was supported by some experimental data. Also using P(x) in Weibull format generates a more conservative failure rate number as compared to P(x) in Normal format. However, regardless of which P(x) format is used, generally, the failure rates obtained by Equations 13-15 are much smaller as compared to the numbers calculated by the traditional method.

Table 1 and Figure 30 illustrate one low-k TDDB with k=2.55 and 64nm pitch interconnect as an example. With our new proposed method, the summation of failure rates from chips with t63=0 hour to chips with t63=100000 hours is significantly smaller than the single failure rate produced by the traditional method. Therefore, the Emax for a realistic reliability can be significantly lifted up.

Table 1:  Method ? ? vs. t63 used for FR calculation  t63 Failure Rate at End of Life  (A.U.) Traditional 0.62 No fixed 3050  New 1.69 Yes Varied 6.01     Figure 32: An example of calculated failure rate versus t63 based on Equations 12-14 for P(x) with a Weibull format.



IV. DISCUSSION AND CONCLUSIONS In conclusion, a new big data (aggregation of stress data, diagnostics data, simulation data, and yield data) generation and analytics method is proposed to address MOL PC-CA and BEOL low-k TDDB challenges. With this new method, without the introduction of any new TDDB acceleration model, reliability can be met with good confidence for various processes based on the square-root of E model. Since BEOL low-k TDDB and MOL PC-CA TDDB are so sensitive to  processing and to structural layout, different processes and different structure layouts could potentially require different TDDB models. The breakdown mechanisms at different stress voltage regions could also be different. Therefore, unless we perform long-term TDDB stresses to validate a TDDB model at all situations such as material change at different technology nodes, different low-k used at different levels, and critical process changes, the question about the correct TDDB model would still exist from different stress voltage regions, from different test structure layouts, and from one technology node to another.  Alternatively, restoring a true Weibull slope by our data deconvolution is an easier way to improve overall TDDB projection. Without any long-term TDDB stresses, a real and improved failure rate projection can be quickly established. Furthermore, wafer processing can also be carefully diagnosed and meaningfully compared with our proposed big data generation and analytics method. A new diagnostics reliability concept is naturally embedded in our method. Therefore in addition to a simple ?pass? or ?fail? reliability judgment for qualification and process development, a precise reliability failure root cause analysis with a potential process fix guideline can also be provided by our method. As a consequence, a huge cost and time saving for new technology development and qualification can be achieved. Lastly, as mentioned above, our new method could be a prerequisite to developing a reliable TDDB acceleration model if significant die-to-die variation is present in our stress data.

