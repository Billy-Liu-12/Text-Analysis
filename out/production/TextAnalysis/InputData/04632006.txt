

Abstract  ViSOM?s (Visualization induced SOM) final map can be seen as a smooth net embedded in the input space, where the distances among neurons are controlled by a regularization control parameter which is usually heuristically chosen on a trial and error basis. Empirical studies shown that ViSOM suffers from dead neuron problem, since a big number of neurons fall outside of the data region due to the regularization effect, even though the regularization control parameter is properly chosen.

In this paper, a modified Adaptive Coordinate (AC) approach that is able to preserve data structure is hybridised with ViSOM is proposed.  Experimental studies on benchmark datasets shown that the proposed method was able to eliminate the selection of regularization control parameter and minimizing the dead neuron for better data representation.

1. Introduction   Self-Organizing Map (SOM) [1] is able to preserve data topology in its visualization from N- dimensional space to the low dimensional display space. However, SOM?s visualization is not directly able to represent the data structure and inter-neuron distance in its topology preserved mapping [2].

Visualization induced SOM (ViSOM) [3] is then proposed to enhance SOM?s visualization to preserve data structure and inter-neuron distances. Empirical studies in [3] have proven ViSOM?s superiority among several other visualization methods such as Principal Component Analysis (PCA) [4], Multidimensional Scaling (MDS) [5] and Sammon?s Mapping [6] in terms of data structure, data topology and inter-neuron distance preservation.

ViSOM?s final map can be seen as a smooth net (mesh grid) embedded in the input space, where the distances among neurons are controlled by a regularization control parameter, which is usually heuristically chosen using trial and error approach [2]. This regularization control parameter controls the regular spread of the weight vectors in the data region in N-dimensional space. Empirical studies in [3] have shown that ViSOM suffers from dead neuron problem, since a big number of neurons fall outside of the data region in the produced map due to the regularization impact even though the regularization control parameter value is reasonably selected [3].

Empirical studies in [3] have also shown that, for a suitable regularization control parameter, ViSOM requires a large amount of neurons in the network to effectively represent the data. The big number of dead neurons produced in the ViSOM map results non-optimised network resource utilization even though the larger map size caused more computation efforts. Thus, this work scopes itself in defining a way of eliminating the impact of the regularization control parameter in the ViSOM network and optimise the dead-neuron problem, at the same time produce data structure, data topology and inter- neuron distance preserved data visualization.

In this work, a modified Adaptive Coordinate (AC) approach is hybridised with ViSOM to address the above-mentioned issues. The basic idea of AC approach [7][8] is to mirror the movements and locations of the neurons? weight vectors in the high dimensional input space in a low dimensional output space to reveal the clustering tendency of data learned by SOM [7]. Usually the AC is applied on the topology preserved SOM grid to reveal the clustering tendency of data by removing the rigidity of SOM grid [8]. A modified AC approach is also  proposed in [9][10] to produce data structure and inter-neuron preserved topology mapping.

In this paper, the integration of the modified AC and ViSOM network is proposed as AC-ViSOM.

Section 2 describes ViSOM in brief. Section 3       briefly describes modified Adaptive Coordinates (AC). In section 4, integration between AC and  ViSOM (AC-ViSOM) is presented. In section 5,  experimental results demonstrate that, the proposed AC-VISOM is able to eliminating the impact of the regularization control parameter in the ViSOM network and minimise the dead neuron problem.

2. Visualization induced SOM (ViSOM)  2.1. Self-Organizing Map (SOM)  ViSOM modifies the weight updating rules of SOM [3]. SOM [1] is able to preserve data topology of N-dimensional data space to a low (2D/3D) dimensional regular grid space. Each neuron i of the grid is assigned with an N-dimensional weight vectors [ ]Tw,.....,w,w,w Ni 321=w in N-dimensional data space. In the incremental training of SOM, each data sample )t(x in discrete time step t is presented to the network and the winner neuron is selected based on the closest similarity match as,  Let denote )t(hic  to represent the neighbourhood kernel function around the winner neuron c for time step t. Then the neighbourhood function can be described as,  Here, ci rr ?  represents the distance between neuron i and neuron c in the grid space and  )t(? represents the neighbourhood range monotonically decreasing with time step t. Then the winner along with a set of neighbouring neurons Nc gets the update by the following equation,     Here, SOM is briefly discussed according to  Kohonen [1].  The inter-neuron distances in the N- dimensional data space and the data structure are not directly visible in SOM?s visualization [2].

Therefore, to enhance SOM?s visualization with inter-neuron distance and data structure preservation, ViSOM was proposed in [3]. The following sections briefly describe ViSOM.

2.2. Visualization induced SOM (ViSOM)  The basic idea of ViSOM [3] is to regularize the weight vectors? distances to produce a smooth net (mesh) embedded in the N-dimensional data space.

ViSOM decomposes the force Fix=[x(t)-wi(t)] representing the force between neuron i and data  sample x(t) in discrete time step t into two parts, which was expressed in [3] as  (t)]-(t)[ (t)]-(t)[ (t)] -(t)[ icci wwwxwx += i.e.

cicxix FFF += . Fcx presents the force from the  winner neuron c and the data sample x, while Fci is the lateral force from the neuron i to the winner neuron c [3].

ViSOM then modifies SOM?s weight updating rule for cNi ??  described in Equation (3) as,  Here, cid  and ci?  are two distance metrics representing distances between neuron i and winner neuron c in the N-dimensional input space and low- dimensional output space respectively, and ?  is the regularization control parameter. The following section describes Adaptive Coordinate (AC) approach in brief.

3. Adaptive Coordinates (AC)  As described by Merkl and Rauber in [7][8], the basic idea of AC approach was to mirror the movements of the neurons? weight vectors in the high dimensional input space in a low dimensional output space. In the 2D output space each neuron is assigned a position by two adaptive coordinates ?axi, ayi? initially being identical to the neuron?s position in the map grid. During each training step, relative changes or movements in N-dimensional data space are mirrored in the output space using the adaptive coordinate vectors and adaptation criteria described in [7][8]. This AC approach requires to be integrated with a method that is able to preserve data topology in order to facilitate AC?s functionalities [7]. In [7] and [8], it has been proven that the mirroring ability of AC applied on the topology preserved SOM grid allows visualization of the clustering tendency of data learned by SOM.

A modified AC approach is also proposed in [9][10]. This approach produces data structure and inter-neuron distance preserved topology mapping from N-dimensional space to low dimensional display space. This modified AC approach is briefly described in the next section.

3.1. Modified Adaptive Coordinates  AC [7][8] used SOM?s grid space as it?s output space to create visualizations. In contrast, the modified AC [9][10] is able to produce data visualization by itself. Thus an output space needs to be defined to enable AC to create its visualization.

Let denote two normalization factors as ?in and ?out  { }N,....,,i,)t(minargc i i  21??= wx  (1)  ??  ? ? ?  ??  ? ? ? ?  ?  =   2 )t(  ci  ic e)t(h ? rr   (2)  ciii  ciici  N),t()t( Ni)),t()t()(t(h)t()t()t( i  ??=+ ???+=+  ww wxww  1 ?  (3)  [ ] [ ] ?  ? ?  ?  ?  ? ? ?  ?  ?  ? ? ?  ? ? ? ??  +? +  =+  ci  cici ic  c  ic  i  d )t()t(  )t()t( )t(h)t()t(  )t(  i  ?? ??? ww  wx w  w 1  (4)       to represent the maximum distances among data points in N-dimensional input space and a predefined maximum range of a side of the output space respectively. These two normalization constants enable the proposed modified AC to compute all relative changes in a normalization scale of 0 to 1.

In one complete AC adaptation cycle, each neuron i in turn acts as an adaptation centre and every other neuron j adjusts the output space distance towards current neuron i. Let denote, )t(dij  and  )t(ij?  to represent the distance between the current adaptation centre neuron i and any other neuron j in N-dimensional input space and low (usually 2D) output space respectively for any time step t.  Then the adaptation factor for the adaptive coordinates of each neuron j in respect with the current adaptation centre neuron i during time step t can be calculated as,    ji,)t(d)t()t(Dist in  ij  out  ij ij ??=+  ?? ?? 1  (5)  In Equation (5), )t(Distij 1+? represents the signed (+/-) differences between the normalized distances in input space and output space for any neuron j and current neuron (current adaptation centre) i. The positive value of )t(Distij 1+?  sets the direction of adaptation towards the neuron i while negative value sets it in the opposite direction. Now the adaptive coordinates of every neuron j ?axj(t), ayj(t)? are updated towards adaptation centre neuron i by the following equation,   (t)(t)].ax-(t)).[ax(tDist  (t) ax )(t ax jiijjj ?? 11 ++=+   (t)(t)].ay-(t)).[ay(tDist  (t) ay )(t ay jiijjj ?? 11 ++=+    (6)   In Equation (6), ?(t) is a monotonically  decreasing control parameter with (0<?(t)<1). More detail about the modified AC approach can be found in [9] and [10].

4. Proposed hybridisation of AC and ViSOM (AC-ViSOM)   4.1. Hybridisation of AC and ViSOM  The method of AC-ViSOM is proposed as hybridisation between modified AC and ViSOM. In AC-ViSOM, modified AC approach is first applied to preserve the data structure in visualization using randomly selected data samples as the initial positions of weight vectors. Once the data structure is preserved, the ViSOM network then adjusts the neurons? weight vectors? positioning in the N-  dimensional space according to the neurons? positioning in low dimensional display space.

In ViSOM [3] weight update, the regularization parameter ?  adjusts the neighbouring neurons? weight vectors? positioning in N-dimensional space according to their respective distances in the regular grid space (display space). In AC-ViSOM, the neighbouring neurons? distance is already preserved by AC in the display space, thus the ViSOM regularization parameter can be eliminated in the training.

The modified AC approach described in section 3.1 uses ?in and ?out as two normalization factors to preserve data structure by adjusting the neurons? location from N-dimensional space to low dimensional display space. In optimisation of distance adjustment, Equation (5) of AC adaptation can be expressed as,    out  in ijij  in  ij  out  ij  )t()t(d  ji,)t(d)t(  ? ??  ?? ?  =?  ??? 0 (7)  Here, )t(dij  and )t(ij?  represent the distance between neuron i and neuron j in N-dimensional input space and low-dimensional output space respectively for any time step t. If the expected distance between neuron i and j in N-dimensional input space is expressed with a scaling factor ? (regularization parameter) in respect with their corresponding distance in low-dimensional display space, then Equation (7) can be expressed as,    out  in  ijij .d  ? ??  ??  =?  = (8)   Then ViSOM?s weight update for AC-ViSOM  algorithm can be expressed as in Equation (9) for cNi ?? . Here, cid  and ci?  are two distance metrics  representing distances between neuron i and winner neuron c in the N-dimensional input space and low- dimensional output space. ?in represents the maximum distance among data points in N- dimensional input space and ?out represents a predefined maximum range of a side of the output space. This is how AC-ViSOM is able to eliminate the heuristics based trial and error process of scaling factor ?  (regularization parameter) selection. The AC-ViSOM algorithm is explained in the next section.

4.2. AC-ViSOM algorithm  Let us assume that all samples of x are derived from an N-dimensional data set and each neuron i is assigned with 2-dimensional adaptive coordinate vectors ?axi, ayi? randomly positioned in a predefined output space range. The maximum distance between among data points in N-dimensional input space is computed and presented by ?in, and the ?out is assigned with the maximum range of a side of the output space. Initially N-dimensional weight vector for each neuron is assigned in the data space.  Then the algorithm can be described in a two-phase structure as,   Phase 1: AC?s data structure preservation Step 1: Compute the adaptation factor for every  other neuron j for each neuron i using Equation (5).

Step 2: Update the adaptive coordinates of every other neuron j for each neuron i using Equation (6).

Step 3: Repeat for a predefined number of epochs (e.g. 50 epochs)   Phase 2:  ViSOM training Step 1: Select the winner for each data sample x(t)  using Equation (1).

Step 2: Update winner and its neighbouring  neurons using ViSOM?s weight update rule stated in Equation (9).

Step 3: Repeat Step 1 to Step 2 until all the data samples are presented to the network.

Step 4: Repeat Step 1 to Step 3 for a predefined number of epochs (e.g. 250 epochs).

5. Experiments  In this section the advantages of the proposed AC-ViSOM method are demonstrated using 2D simulated Gaussian data set, a 3D synthetic data set [2], and Wisconsin Breast Cancer (WBC) data set [11].

In the following experiments, data representation by weight vectors is measured using Mean Square Error (MSE). The MSE measurement refers to the average similarity errors for all data samples and their corresponding winners. The smaller the MSE value obtained, the better the data representation by the weight vectors would be. For M number of data  samples in N-dimensional input space, MSE can be calculated as,  ( )  1 1  /M  i  N  k ckikM  MSE ? ? = = ?  ? ?  ?  ? ? ?  ? ?= wx  (10)   5.1. Two-dimensional Gaussian data set  A 2D simulated Gaussian data set with two Gaussian sources centred at [1.0, 1.0] and [1.7, 1.7] and 300 data members for each sources with variance of 0.12, was generated for this experiment.

First a ViSOM network with size 20x20 was used with a regularization parameter ? = 0.15. The weight vectors were randomly selected from the sample data. Total iteration was set to 300. Learning rate ? and neighbourhood range ? was initialised to 0.3 and 10, which monotonically decreases to 0.0 and 0.05 respectively at the end of training. The regularized positioning of the weight vectors in the 2D data space were presented in Figure 1 which shows that, due to regularized spread of weight vectors, a big number of dead neurons is formed. Figure 2 then contains the grid space visualization of ViSOM for this 2D Gaussian data set. In this visualization circles represent Class 1 and square represents Class 2 in the data set.

Figure 1: Weight vectors? regularized positioning  of ViSOM for the 2D Gaussian data, dead neurons=67.7%, MSE=0.0639   Then an AC-ViSOM network was used with the  similar map size of 20x20 in this data set. Weight vectors were randomly selected from sample data.

The total iterations were set to (50+250) i.e. 50 iterations for AC?s data structure preservation in phase 1 and 250 iterations for ViSOM training in phase 2. The adaptive coordinates ?axi, ayi? for every neuron i was initialised randomly in the output space.

Learning rate ? and control parameter ? were initialised as 0.4 and 0.1 and linearly decreased to 0.0 at the end of phase 1. The normalization constant ?out was set to 20 and ?in value was computed over the data set. Then ViSOM training was applied in phase 2. Learning rate ? and neighbourhood range ?  [ ]  [ ] ?? ? ? ? ? ?  ?  ?  ?? ? ? ? ? ?  ?  ?  ? ? ? ? ?  ?  ?  ? ? ? ? ?  ?  ?  ?? ?  ? ?? ?  ?  ?? ?  ? ?? ?  ? ?  ?  +? +=+  out  in ci  out  in cici  ic  c  ic  i  d )t()t(  )t()t(  )t(h)t(  )t()t( i  ? ??  ? ??  ? ww  wx ww 1  (9)       was initialised to 0.3 and 10, which monotonically decreases to 0.0 and 0.05 respectively at the end of phase 2.  Weight vectors? positioning by AC-ViSOM network for this 2D Gaussian data set was presented in Figure 3.

Figure 2: ViSOM?s grid space visualization for  the 2D Gaussian data set    Figure 3: Weight vectors? positioning by AC- ViSOM for the 2D Gaussian data set, dead  neurons=17.5%, MSE=0.0282   Figure 3 shown that, AC-ViSOM was able to nicely embed the weight vectors in the data region, resulting minimize dead neuron produced as compared against ViSOM?s projection in Figure 1 and Figure 2. In Figure 4, AC-ViSOM?s display space visualization for this 2D Gaussian data set is presented.

Figure 4: AC-ViSOM?s display space  visualization for the 2D Gaussian data set   Figure 3 and Figure 4 demonstrated that, due to AC-ViSOM?s ability to embed weight vectors? positioning in the data region, the map produced better data representation and lower MSE value as compared with the ViSOM network.

5.2. Three-dimensional synthetic data set  In this experiment a three-dimensional synthetic data set [2] was used. This data set contains three Gaussian sources with 100 data samples in each source. The mean vector of three Gaussian sources were [5.0 7.0 6.0]T, [-2.0 5.0 -3.0] T, and [-10.0 6.0 2.0]T.  Their corresponding covariance matrixes were,  ? ? ?  ?  ?  ? ? ?  ?  ? ??  ?  0.45.13.0 0.13.00.1  3.00.10.5 ,  ? ? ?  ?  ?  ? ? ?  ?  ?  ? ?  0.32.03.1 0.10.50.2 5.01.00.1  and ? ? ?  ?  ?  ? ? ?  ?  ?  ?  ??  0.42.13.0 4.13.22.1 2.00.11.0    The view of the 3D synthetic data set is presented in Figure 5.

Figure 5: 3D synthetic data set   In this experiment, first a ViSOM network with  map size 15x15 was used with regularization parameter ?=0.12. The remaining parameter settings were kept the same as the previous ViSOM network applied on 2D Gaussian data set. ViSOM?s grid based visualization and positioning of weight       vectors? in 3D data space for this 3D synthetic data set were presented in Figure 6 and Figure 7 respectively. Figure 6 and Figure 7 highlight the dead neuron areas (oval shape line) produced by ViSOM mapping.

Figure 6: ViSOM?s visualization for the 3D  synthetic data set, dead neurons=68.4%, MSE=0.048      Figure 7:  Weight vectors? positioning by ViSOM  for the 3D synthetic data set    AC-ViSOM network with map size 15x15 is used  to perform the mapping. The normalization constant ?out was set to 15 and ?in value was computed over the data set. The remaining parameter settings were kept the same as the previous AC-ViSOM network applied on 2D Gaussian data set. AC-ViSOM?s visualization in the display space and the corresponding weight vectors? positioning were presented in Figure 8 and Figure 9 respectively.

Figure 8: AC-ViSOM?s visualization for 3D  synthetic data set, dead neurons=31.0%, MSE=0.024      Figure 9:  Weight vectors? positioning by AC-  ViSOM for the 3D synthetic data set   Figure 8 demonstrated that, AC-ViSOM was able  to produce more appealing data structure preserved visualization than that of ViSOM projected in Figure 6.  Figure 9 again demonstrated that, AC-ViSOM was able to nicely embed the weight vectors in the data region, resulting minimized number of dead neurons produced in the map, and lower MSE value than that of ViSOM.

The following section conducts a similar experiment to demonstrate AC-ViSOM in high- dimensional data set.

5.3. Wisconsin Breast Cancer data set  The Wisconsin Breast Cancer (WBC) data set [11] consists of 9-dimensional 683 data samples with 2 classes. In this experiment, ViSOM network with map size 20x20 was used with regularization parameter ?=0.25. The remaining parameter settings were kept the same as the previous ViSOM network applied on 2D Gaussian data set. ViSOM?s grid       based visualization for this WBC data set was presented in Figure 10.

Figure 10: ViSOM?s visualization for the WBC data set, dead neurons=84.7% and MSE=0.375     Then an AC-ViSOM network was used with map size 20x20. The normalization constant ?out was set to 20 and ?in value was computed over the data set.

The remaining parameter settings were kept the same as the previous AC-ViSOM network applied on 2D Gaussian data set. AC-ViSOM?s visualization for this WBC data set was presented in Figure 11.

Figure 11: AC-ViSOM?s visualization for WBC  data set, dead neuron=34.5%, MSE=0.193    Figure 11 demonstrated that, AC-ViSOM was able to produce data structure preserved visualization with more active neurons in the map than that of ViSOM projected in Figure 10 for this high dimensional data set. Figure 11 also demonstrated that, AC-ViSOM was able to produce a better data representation justified by low MSE obtained compared against that of ViSOM.

The above experiments have shown that AC- ViSOM is able to produced less dead neurons for  better network utilization, optimised data representation with lower MSE value, and appealing data structure preserved visualization as compared with ViSOM.

6. Conclusion and future works  In this paper, an extension of ViSOM was proposed by hybridising modified Adaptive Coordinate (AC) approach and ViSOM network as AC-ViSOM. Empirical studies have shown that, AC- ViSOM is able to eliminate the trial and error process of selecting suitable resolution control parameter of ViSOM. Besides, it is able to minimize the number of dead neurons in the map as well as optimise the data representation as compared with that of ViSOM. This better network utilization ability of AC-ViSOM has a potential to minimize the map size requirements to produce effective data visualization.

For the future works, the research will look forward to define a way of increasing AC-ViSOM inter-neuron distance preservation ability. The method on how to define a suitable initialisation process for the weight vectors to represent the data density information will also be investigated.

Acknowledgements  The authors would like to acknowledge the support of Fundamental Research Grant Scheme FRGS/02(06)/ 662/2007(27) and Universiti Malaysia Sarawak for funding this work.

7. References   [1]  T. Kohonen. Self-Organizing Maps, 3rd edition, New York: Springer-Verlag .2001.

[2] S. Wu and T. S. W. Chow, ?PRSOM : A New  Visualization Method by Hybriding Multidinesional Scaling and Self-Organizing Map? , IEEE Trans. Neural Netw., vol. 16, no. 6, pp.1362?1380, 2005.

[3]  H. Yin, ?ViSOM: A novel method for multivariate  data projection and structure visualization,? IEEE Trans. Neural Netw., vol. 13, no. 1, pp.237?243, 2002.

[4] R. A. Johnson and D. W. Wichern. Applied  Multivariate Statistical Analysis. Englewood Cliffs, NJ: Prentice-Hall, 1992.

[5] R. N. Shepard and J. D. Carroll, ?Parametric  representation of nonlinear data structures,? in Proc. Int. Symp. Multivariate Anal., P. R.

Krishnaiah, Ed. New York: Academic, pp. 561? 592.992. 1965.

[6] J. W. Sammon, ?A nonlinear mapping for data structure analysis,? IEEE Trans. Comput., vol. C- 18, pp. 401?409, 1969.

[7]  D. Merkl, and A. Rauber, ? Alternative ways for  cluster visualization in self-organizing maps?, In Proc. WSOM ?97, Workshop on SOM, Espoo, Finland, pp. 106-111, 1997.

[8] D. Merkl, and A. Rauber, ?The similarity of eagles,  hawks, and cows: visualization of semantic similarity in self-organizing maps?, Int?l Workshop on Fuzzy-Neuro-Systems, Soest, Germany, 1997.

[9]  Z. T. Sarwar and C. S. Teh. ?Hybridization of  Learning Vector Quantization (LVQ) and Adaptive Coordinate (AC) for Data Visualization and Intelligent & Advance Systems (ICIAS ?07). KL Convention Centre, Kuala Lumpur, Malaysia, 2007. [CD-ROM].

[10] C. S. Teh and Z. T. Sarwar. ?A Supervised ANN  for classification and data visualization?, In Proc.

of IEEE World Congress on Computational Intelligence (WCCI?08). Hong Kong Convention and Exhibition Centre, Hong Kong, June 1-6, 2008.

[11] D. J. Newman, S. Hettich, C. L. Blake and C. J.

Merz, UCI Repository of machine learning databases, Dept. of ICS, Univ. of California at Irvine. 1998.

