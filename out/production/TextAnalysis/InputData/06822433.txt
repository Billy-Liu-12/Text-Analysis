Concept Definition for Big Data Architecture   in the Education System

Abstract? Big Data is a huge phenomenon of last days.

Number of applications using this concept is still increasing and area of implementation is still wider and wider. There are different examples such as transport, health-care, industry or education. This work covers just the field of education or university?s environment. Purpose of this work is to identify Big Data sources from university?s environment and also design of procedure how to work with them. Paper also shows possible using concept of Big Data, but does not show what types of analyses should be performed to improve education process. It means that the main benefit of this work is design of architecture for this problem from the technical perspective. In addition, there are some specific software applications that can be deployed to solve this problem.



I. INTRODUCTION   The area of data analysis is still evolving the last period.

The reason for this fact is very simple ? the amount of data is growing incredibly fast and it is still necessary to find new technologies that could work with this situation.

Information obtained from this huge mass can be very important for concerned entities. On the other hand, there is still risk, that after complicated processes, information value will be still zero [1]. The amount of data is not the only one problem in recent years. Currently, there is a lot of new data formats and what is worse, a lot of data is even in the unstructured forms (images, audio, tweets, text messages, server logs, and so on). Moreover, the processing time between the requirement to process this data and the processing itself is very important. This aspect is called ? velocity. Together with the volume and variety, we have three basic aspects of concept called Big Data [2].

This topic is currently absolutely dominant in the field of IT Technologies. Potential of these analyses can be huge, not only for business but also for areas such as health, transport and education or all together in the smart city projects [3][4]. The area of education is a key factor to the success of every society and therefore it is important to think about how to improve. Is it possible to use the concept of BIG DATA for the acquisition of new knowledge that will improve processes in education? Yes, definitely. Currently, there are several case studies and works, confirming the fact that this idea is not quite crazy.

As an example could be the data analysis of 860,000 students in Colorado in 2000 high schools and 178 districts. The outcome of these analyses should be student?s readiness for college or career [5][6]. There are  also other studies about tracking student progress [7][8] and monitoring of behavioural patterns of students in learning [9].All these works already describe results from analysis of Big Data contrast to this paper. This is about technical architecture for solving this problem. Actually, this is a guide what must be done before a start of analysis in environment of education system or university.

II. WHAT IS BIG DATA?

A. Definition First, Big Data does not have only single definition.

Company SAS defines Big Data as follows:  ?Big data is a popular term used to describe the exponential growth, availability and use of information, both structured and unstructured? [10].

In contrast, IBM defines Big Data as follows: ?Data, coming from everywhere; sensors used to gather climate information, posts to social media sites, digital pictures and videos, purchase transaction record, and cell phone GPS signal to name a few?[11].

B. Dimensions of Big Data As mentioned, concept of Big Data has three basic  dimensions.

? Volume: Data volumes are doubling every  year. Reasons are simple. Significant percentage of organizations is storing historic data for many years. Social media provide a huge amount of text data (Facebook has more than 901 million active users generating social interaction data). RFID systems generate up to 1,000 times the data of conventional bar code systems.

? Variety: Data today comes in all types of formats - from a standard productions systems or transaction databases to OLAP (Online Analytical Processing) cubes. There are emails, stock or financial data and huge percentage of non-numerical data.

? Velocity: It means how fast data is being produced and how fast the data must be processed to meet requirements or demands [10][11][12][13].

? Companies, working with Big Data also talk about several other dimensions.

SAMI 2014 ? IEEE 12th International Symposium on Applied Machine Intelligence and Informatics ? January 23-25, 2014. ? Herl?any, Slovakia     ? Veracity: IBM Company argues, that 1 in 3 business leaders don?t trust the information they use to make decision. How can we work with data that we do not believe?

? Complexity: Big Data is from multiple sources. It is necessary to connect and find relations. Data should be controlled in order to achieve systematically integration structured and unstructured data [10][11][12].

C. Technology Processing of Big Data by traditional methods is too  complicated, time-consuming and not least financially unattractive. Therefore, there is a demand for new solutions. Currently, the focus is on parallel computing ? a form of computation in which many calculations are carried out simultaneously. Principle is that large problems should be divided into smaller pieces, which are then solved in parallel.

There are several important frameworks in Big Data area:  ? MapReduce: One of the most used frameworks, using for processing parallelizable problems across big datasets working with a large number of computers (nodes).  A MapReduce can work raw data stored in disk files, in relation databases, or both. The data can be in structured or unstructured forms (weblog records, e-commerce click trails, binary or multi-line records). This algorithm composes from two procedures. Map() procedure performs filtering and sorting (for example ? sorting students by last name into queues ? one queue for each surname). Reduce() procedure performs a summary operation (for example - counting the number of students in each queue, yielding surname frequencies).

MapReduce libraries have been written in many programming languages, but there are different levels of optimization. One of the most popular implementation is open-source Apache Hadoop.

? Hadoop:  It is an open source software project that enables distributed (parallel) processing of large amount of data sets across clusters of many computers. One server can scale up to thousands of machines, with a very high fault tolerance.

Hadoop derives from Google?s MapReduce and Google File System papers. This framework is written in Java.

? HDFS: The most common MapReduce usage pattern ? Hadoop Distributed File System. HDFS is a distributed, scalable and portable file-system for the Hadoop framework written in Java too.

HDFS stores large files (from gigabytes to terabytes) across a large amount of servers.

? Apache Hive: It is a data warehouse infrastructure built on top of Hadoop. This warehouse provides summarization of data, query and analysis. Hive supports analysis of big datasets stored in HDFS, Amazon S3 file system  etc. It provides an SQL ?like language called HiveQL, supporting indexes.

? NoSQL: Database system providing a mechanism for storage and retrieval of data with less constrained than traditional SQL (relational) databases. Impact of these databases is still higher, especially in Big Data concept [14][15].

Although an increase big data technology is huge, it doesn?t mean the end of classical BI tools like Cognos, QlikView, SPSS and so on. Trend is that BI tools should be work with new Big Data technologies side by side.

Here is a list of some applications, working with traditional data and Big Data:  ? Apache Hadoop, ? IBM InfoSphere BigInsights, ? Cloudera, ? IBM Cognos, ? IBM SPSS, ? SAS, ? Quest.



III. CONCEPT DEFINITION   The concept definition consists of three main parts: 1.  Identification of Big Data at university 2.  Design of algorithm for processing of Big Data 3.  Design of technical architecture  A. Identification of Big Data sources The starting point is the identification of Big Data and  determine the sources from which the come. We assume that we are dealing with Technical University of Ko?ice environment, but it can be very similar at any other universities. Bigger differences begin to emerge at the level of elementary and high schools. We divided this data to the groups based on its sources (Figure 1):  ? documents in non-electronic form, ? data from information systems, ? logs from university servers, ? opinions from social networks, ? data from public education portals.

The first data source is a little bit specific and due to  fact that these records are not in electronic form. It is questionable its usability in this form. If not, is it useful to transform it? We talk about historical writings, reports on exams, and records of students, whether actual written exams.

The second source of data is data from information systems available to universities. This is probably the least problematic source of data because of its format. This data is in structured form in tables. We mean primarily information about students, courses, exam?s marks. All this information is available in Modular Academic Information System (MAIS).

Very popular topic is currently analysis of server?s log.

There are a lot of server?s log at the university level.

Students are connected on web through university Wi-Fi  P. Michalik et al. ? Concept Definition for Big Data Architecture in the Education System      Figure 1. Big Data sources at university  Figure 2. Algorithm of processing big data  zones and networks. It is interesting to watch activity of every student in his free time. This information should be very helpful for creating a report about student?s hobby. If you know student?s hobby, you know which university course is interesting for him. If you have this report for 2 000 students, you know which university courses are not favourite.

Next one source is the most popular thing on the web at present. We talk about social networks ? the big phenomenon of our times. Facebook, Twitter and other social networks have a big potential to bring still new data from students, lectors, politicians from education zone etc.

Can you imagine situation that you know everything about all student?s opinion about his study? On the other hand, extraction of knowledge from this source is the most problematic. There are several reasons such as privacy, data is high unstructured, the amount of data is huge, too.

Last one, but not least is information from public portals, supporting education system. For example ? portalvs.sk  in Slovakia, official websites of Ministry of Education, Science, Research and Sport of the Slovak Republic ? www. minedu.sk and other public portals and discussion forums.

B. Design of algorithm After the identification of a data sources we can start  work with them. There is an algorithm for data processing in the picture below (Figure 2).

First step is obtaining data. As mentioned, it is highly dependent from data source. In case of internal data source like academic information system, problem is very easy. On the other hand, access to the data from social network is not simple task.

Next step of algorithm divides data to 2 groups. Data which is in structured format are sent directly to relation database management systems (RDBMS) [16][17][18].

This group includes data from MAIS in case of Technical University of Ko?ice. Second group consists of server logs, tweets and other unstructured data. This group of  data has to be sent to the Apache Hadoop for processing by MapReduce for example. Then, we can use Apache Hive to transform this data. Transformed data can be sent to RDBMS.

Now we have a RDBMS system with all data. It means, that we can create a data warehouse using by one of the classical way create it. As example should be star schema in data warehouse.

If you have a data warehouse, you can start to do OLAP operations and to create different types of reports and analysis. Of course, there is also option of data mining applications.

C. Design of architecture Design of architecture is closely related to algorithm.

As you can see in the picture 3 below, the solution has a several layers.

The lowest layer covers data sources. These data can be stored in traditional SQL databases (data from MAIS) or NoSQL databases (data from social network). For the storage of classical relation data can be use Microsoft SQL Server, Oracle databases or some several other products. Data that we need to save in any structure should be stored in NoSQL databases. One of the possible ways is to deploy Mongo database system.

In next layer is an Apache Hadoop. This system processes the huge amount of data from the lowest layer.

Advantage of this tool is fact that it can work with  SAMI 2014 ? IEEE 12th International Symposium on Applied Machine Intelligence and Informatics ? January 23-25, 2014. ? Herl?any, Slovakia      Figure 3. Technical architecture  NoSQL data and relation data. At the same level is Apache Hive tool. It would be easy to work with NoSQL data and transform them to RDBMS.

Now, we have a data warehouse in the middle layer. It does not matter, how technology is used. For example ? we can use Microsoft SQL Server technology or some product from Oracle, IBM and so on. Creating data cubes from this data is now easy. In addition this concept is used for many years.

If you have a data warehouse, you can start with data mining application a try to find suitable model for data.

There are a few good software applications for this job ? IBM SPSS or products from SAS Company.

The highest layer is only for creating useful analysis from obtained data. These analyses are output of process of possible change in education system at universities. In this layer may be several types of users such as teachers, politicians, administrators at university and so on.

Software application from IBM ? Cognos could be one of the possible ways.



IV.  CONCLUSION   Today, there are many software application and technologies supporting work with Big Data. This paper describes an option of using these technologies for processing of Big Data from technical perspective. It means that technical architecture has been designed. This architecture should be a suitable model for processing of Big Data from university?s environment. Further, very important asset of this work is an identification of data sources from education environment. These sources have been divided to 5 groups. After this identification, algorithm for processing of this data was designed.

Area of Big Data analysis from education data is not as simple as it might seem. Technical problem is only one puzzle piece in the mosaic. A view on processes is also very important. It would be great to identify all processes  of creating new knowledge from this education data. We talk about Business Process Management (BPM) view. It means that assignment of process owners, identification of resources (manpower, technical) and other aspects of BPM could be very helpful for further research.

