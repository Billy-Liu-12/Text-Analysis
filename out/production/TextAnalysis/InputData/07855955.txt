Revamped Market-Basket Analysis Using In-Memory  Computation Framework

Abstract- Data sets are growing day by day as they are being captured by information sensing devices such as mobiles, computers, wireless sensor networks, cameras, software logs, web logs , remote sensing in various fields such as medical, engineering, science and many more. These large data sets are now called Big Data. Working with Big Data is not a common task. As this large data set has information hidden within them, researchers cannot and they have not ignored the large data set.

Data mining is an interdisciplinary field in Computer Science which extracts information or the hidden patterns from data.

Association rule mining and frequent item set mining are popular data mining techniques that requires entire data to be in main memory. But large datasets does not fit into main memory. To handle this drawback, Hadoop MapReduce approach is used which has scalability and robustness features to handle large datasets. Apriori, Eclat and FP Growth are well known Frequent Itemset Mining algorithms. These algorithms are revised to work with Big Data using Hadoop MapReduce. But MapReduce framework has problems such as it stores the intermediate data in local disk. So the data needs to be accessed from the local disk which results in high latency problem. To address this issue Spark follows a general execution model that helps in in-memory computing and optimization of arbitrary operator graphs so that querying data becomes much faster when compared to the disk based engines like MapReduce. Thus the paper focuses on enhancing the performance of Frequent Itemset Mining using Apache Spark architecture and study the performance of this Revamped Market Basket Analysis based on FP-Growth by comparing it with Hadoop MapReduce implementation of Frequent Item set Mining task, BigFIM and also with different datasets.

Keywords-Frequent Itemset Mining, Hadoop MapReduce, Apache Spark

I. INTRODUCTION  World is now driven by large data sets generated and accumulated by information sensing devices like mobile phones, remote sensing devices, sensor networks. Data sets are generated in variety of fields including medical, research, WWW.science and so on resulting in Big Data. Big Data is structured or unstructured data which are collected from heterogeneous sources and hence it is difficult to process using traditional database management techniques. The processing of Big Data is very important because they contain hidden patterns or information or knowledge which is of most   H R Manjunath Prasad Department of CSE  NMAMIT, Nitte, Karnataka, India manjunath.prasad@nitte.edu.in  importance for any enterprise or organizations to make strategic planning for future.

Data mining is an analytic process and an interdisciplinary field in Computer Science for data analysis. There are several major data mining techniques [1] have been developed and are used in data mining: association, classification, clustering, prediction, sequential patterns and decision tree which help in discovering unknown patterns in the data sets. Mining tasks on large datasets are facilitated using Hadoop MapReduce framework. Association-rules have always been considered to be important in various fields like crowd-mining, crime- detection, recommender-systems and so on. Three well-known algorithms to facilitate association-rule generation are Apriori, Eclat and FPGrowth. There are various attempts that resulted in restructuring of these algorithms in-order to handle scalability and speed with the growing size of data sets.

Distributed versions are available as well as MapReduce versions are developed.

Market-Basket Analysis [2] is a process to identify the items that are bought together as a result of which; a retail store manager can make strategic decisions on marketing plans. The term association has become important in various other fields like crowd mining, where it helps in analyzing behavior of the people in social network. In medical , to identify association of symptoms to determine a disease and analyze the persistence of the disease region wise and cause of disease based on region and also various precautionary measures can be planned based on the analysis. Most common relevance of the market basket analysis is in recommender- systems which results in increased profits in e-commerce.

Numerous work on market-basket analysis has been carried out which satiated the needs of the mentioned applicable fields. Due to growing data sets the trend now is to use Hadoop for efficient data management and carrying out analytical tasks on the large data. However, the problem that needs to be addressed in Hadoop's MapReduce is latency which results due to numerous read and write operations to local storage which is basically Hadoop's Distributed File System.

Even though resource utilization Map Reduce the read write operation overcomes the latency problem MapReduce. Addressing latency will faster market-basket analysis. This  is good in Hadoop's is a bottleneck. Spark faced by Hadoop' s automatically result in increase in speed is     essential because the process deals with continually growing data sets which are in terabytes or petabytes. This speed up can greatly change the take recommender systems e-commerce to highly profitable sector.

The importance of market-basket analysis in various fields raises the need for efficient algorithms to facilitate the analysis process. Hadoop's MapReduce stands as a de facto solution for big data processing. But to add more efficiency in terms of speed and scalability further furnishing of the existing frequent itemset mining algorithms is required and one way to achieve this is using Apache's Spark.

The paper addresses the objectives in three folds. First, to study the existing algorithms developed to process big data and generate frequent-itemsets. Second, to develop the Revamped Market Basket Analysis based on FPGrowth using In-Memory Computation framework that is Apache's Spark. Third, to show that Revamped Market Basket Analysis based on FP- Growth using Apache Spark framework is faster than MapReduce implementation of frequent itemset mining - BigFIM by experimenting with different datasets of varying size and characteristics to explore variation in execution speed and with different support count that affects the number of frequent itemsets to be generated and thereby affects the execution speed.



II. BACKGROUND AND RELATED WORK  A. Frequent Itemset Mining  Frequent itemset mining [3] plays a prominent role as a data mining task. It is a method to find itemsets from the transaction database which occur frequently. The itemset is said to be frequent if it occurs in minimum number of transactions. This minimum number of transaction is indicated by support count. If the item set satisfies the support count it is said to be frequent. Minimum threshold is set for support count which is called as minimum support for the item set.

Finding frequent item sets help retail industries in making strategic plans about future trends. It can also be applied in various other fields like medicine, web usage mining, bio informatics. Once frequent itemsets are identified, association rules can be generated. A rule is said to be strong if it satisfies minimum confidence which says "If X => Y is a rule then confidence is number of transactions containing X that also contains Y". Strong association rules are required to reveal the elements that occur together in the data sets.

B. Hadoop MapReduce  HADOOP [4] is an Apache Software Foundation scheme that basically provides two components:  ? A distributed file system called HDFS (Hadoop Distributed File System)  ? A framework and API for building and running MapReduce jobs  HDFS is organized similar to customary UNIX file system with the exception of that data stockpiling is dispersed over few machines. It is not proposed as a substitution to a regular  file system, but instead as a file system like layer for substantial distributed frameworks to utilize.

Hadoop MapReduce is a programming framework which can be utilized for creating applications with requirements to process enormous amount of data. It likewise underpins parallel execution on vast groups. Clients indicate a map function that sorts out a key/value pair to produce an arrangement of intermediate key value sets, and a reduce function that deals with consolidation of all intermediate key value pairs with same key.

The outline is this: ? Map tasks perform a transformation.

? Reduce tasks perform an aggregation.

A Map Reduce work for the most part, parts the data set into independent units which are handled by the map tasks in a totally parallel way. The structure sorts the yields of the maps, which are then contributed to the reduce tasks. Ordinarily both the input and the output of the task are put in a file-system.

The structure deals with scheduling tasks, observing them and re-executes the failed tasks. Ordinarily the compute nodes and the nodes required for storage are same which is MapReduce structure and the Hadoop Distributed File System are running on the same cluster of nodes. This setup permits the framework to adequately plan tasks on the cluster where data is now present; bringing about high total transmission capacity over the cluster.

C. Apache Spark  One of the downsides of the MapReduce is that it doesn't give a proficient approach to execute calculations that need to perform numerous passes over the same data. The wastefulness emerges from the way that under MapReduce operations on information must be performed by a mix of MapReduce's three primary stages: map, shuffle and reduce. In the map stage, every information unit is changed and allocated a key. In the shuftle stage, the information is parceled by key and sent to the reducers. At long last, in the reduce stage, the reducers produce results for every gathering of information that has the same key.

Spark [5] was presented by Apache Software Foundation for accelerating the Hadoop computational computing software process. Spark is not an altered version of Hadoop and is not, generally, reliant on Hadoop in light of the fact that it has its own particular cluster management. Hadoop is only one of the approaches to execute Spark. Spark utilizes Hadoop as a part of two ways - one is storage and second is processing. Since Spark has its own computation for cluster management, it utilizes Hadoop for storage reason as it were.

Apache Spark is an exceptionally fast cluster computing technology, intended for quick calculation. It depends on Hadoop MapReduce and it extends the MapReduce model to proficiently utilize it for more sorts of computations, which incorporates intuitive queries and stream processing. The fundamental element of Spark is its in-memory cluster computing that expands the processing speed of an application.

Fig l. shows the general architecture of spark deployed on hadoop in a single node cluster. It should be clear that Hadoop's elements are HDFS, Jobtracker, Tasktracker, Namenode, Datanode and Spark's components are Driver which resembles Jobtracker of Hadoop and Executor which resembles Tasktracker. Resilient Distributed Datasets (RDD) [6] is an integral part of Apache Spark which enables in memory computation in Spark's environment. RDD is a fault tolerant collection of components which can be worked on parallely. There are two approaches to make RDDs. First approach is to parallelize existing collection in driver project, or it can be made by making a reference to a dataset in an external storage system, it can also read from HDFS, HBase, or any data source that offers an input in Hadoop required format. Spark's utilization of RDD is mainly to accomplish speedier and effective MapReduce tasks. Data Sharing is moderate in MapReduce.

SINGLE NODE CLUSTER  Fig 1. General Architecture of Spark Deployed on Hadoop in Single Node Cluster  D. Related Work  Finding association rules [7] from substantial database is one of the issues in Knowledge Discovery in Database. Size and intricacy of Big Data are difficulties for fmding association rules and frequent itemset mining. The entire database scan is essential in FIM, it may make challenge when datasets size is scaling i.e. there is persistent expansion in the data size, as extensive datasets does not fit into memory. To handle this limitation, Hadoop MapReduce approach which is a de facto standard that provides scalability and robustness features to handle large datasets. Apriori , Eclat and FP Growth are well known Frequent Itemset Mining algorithms. Several attempts were made to improve these iterative algorithms using Hadoop MapReduce in order to enhance the performance of the algorithm in terms of scalability and speed.

Parallel FP Growth, MRApriori, BigFIM, ClustBigFIM are some of the previous works in Hadoop MapReduce format.

Apriori algorithm has a disadvantage that it scans transaction every time the support count has to be updated. Eclat algorithm works to overcome this problem. It uses TID based scan for support count update and it is thus faster than Apriori.

But it involves too many join operations. FP growth overcomes the problems in Apriori and Eclat algorithms as it does not involve candidate generation and scans transaction only twice. MapReduce framework has problems such as it stores the intermediate data in local disk. So the data needs to be accessed from the local disk which results in high latency problem. Thus Revamped Market-Basket Analysis based on FPGrowth attacks the latency problem in Hadoop with it's in memory computation using spark.

Change of Apriori on MapReduce has been proposed by Lin et al. [8]; Single Pass Counting (SPC), Fixed Passes Combined - Counting (FPC) and Dynamic Passes Counting (DPC) which do counting step parallel by conveying distributed dataset to mappers. Li et al. [9] has proposed parallel variant of Apriori based algorithm with respect to MapReduce. Apriori based algorithm does not work with respect to extensive datasets having long continuous itemsets.

Hammoud has proposed MRApriori [10] approach for fmding frequent itemsets by exchanging amongst vertical and horizontal format iteratively which takes out need of repeated scanning of data. It rehashes checking for other intermediate data which decreases with iterations.

MREclat [11] is an altered Eclat algorithm on MapReduce system which creates a list of frequent itemsets, the list is divided into equivalence classes, then for every class frequent itemsets are processed utilizing MapReduce structure.

Parallel FP-Growth (PFP) [12] used for mining tag itemsets generated web page itemsets, and requires two scans on database. By utilizing MapReduce structure and its fault tolerant component, errand of mining huge scale data is changed over into other little assignments which are not subject to each other. It achieved scalability and direct speedup over shared memory parallel FP algorithms.

Gathering methodology of PFP has issues with memory and speed, to adjust the gatherings of PFP Zhou et al. [13] has proposed algorithm for quicker execution utilizing single items which is likewise not a productive way. Xia et al. [14] has provided Improved PFP algorithm for mining frequent itemsets from huge little files datasets utilizing small files handling system.

Moens et a1. [15] has proposed two techniques for frequent itemset mining for Big Data using MapReduce. First strategy Dist-Eclat is distributed form of pure Eclat strategy which advances speed by appropriating the search space equitably among mappers. Second strategy BigFIM utilizes both Apriori based strategy and Eclat with anticipated databases that fit in memory for generating frequent itemsets.

Riondato et a1. [16] work was PARMA algorithm which discovers accumulations of frequent itemsets in brief time utilizing sampling technique. Mined frequent itemsets are inexact but appropriate. It finds samples using k-means clustering calculation and the samples are called clusters.

Malek and Kadima [17] proposed a novel methodology which utilizes clustering strategy for mining of frequent item sets utilizing MapReduce system with expansion as a part of execution.

These algorithms have MapReduce implementations which have shown better performance compared to normal execution environment. ClustBigFIM is an extension of BigFIM, it uses k-means clustering and then applies Apriori and Eclat to the clustered objects to find frequent itemsets. But the problem with k-means is the selection of value "k" which is the number of clusters to be generated. Quality of cluster mainly depends on the value "k". Apriori algorithm has a disadvantage that it scans transaction every time the support count has to be updated. Eclat algorithm uses TID based scan for support count update and faster than Apriori. But it involves too many join operations. FP growth overcomes the problems in Apriori and Eclat algorithms as it does not involve candidate generation and scans transaction only twice.

One of the enormous downside of Hadoop's usage of MapReduce, is that the main correspondence that can happen between data processing steps, in data processing stream is via file system which can be Hadoop's file system. This correspondence confinement presents a high cost in terms of latency, on top of the officially unreasonable calculation.

From the above discussion it is clear that the researcher's emphasis was on implementing or restructuring frequent itemset mining algorithm using Hadoop MapReduce mainly focusing on improving scalability and speed of frequent itemset generation. Considering the latency problem of Hadoop MapReduce an attempt is made to restructure frequent itemset mining using Apache Spark [18] approach to improve the proclivity of scalability and speed.

The paper focuses on studying the performance of Revamped Market Basket Analysis based on FP-Growth with various datasets, using Apache Spark architecture and also compares it with the results obtained from Hadoop MapReduce implementation of Apriori and Eclat implemented as BigFIM.

Three datasets have been chosen for experimentation - retail, mushroom and foodmart. Advantage is mainly the time of execution which is reduced by 10 times.



III. PROPOSED WORK  Revamped Market-Basket Analysis based on FP-Growth is mainly for faster frequent itemset generation which can be considered to be a revolutionary approach in big data field.

The proposed work makes use of Spark environment for execution which is known for speed. It is an in memory computation framework which overcomes the latency problem in Hadoop MapReduce framework.

Fig 2. shows various components in spark execution environment that works together to perform mining on the input dataset using Revamped Market-Basket Analysis based on FP-Growth. The output is the frequent itemsets and the frequent itemset pattern. It is also observable that the dataset is in Hadoop' s HDFS from where it is read and worked upon.

Apache Spark  Revamped  Market-Basket  Analysis  Hadoop  HDFS  Fig 2. Architecture Depicting Revamped Market-Basket Analysis in Spark Environment  Fig 3. depicts the overall working of Revamped Market Basket Analysis based on FP-Growth in Spark. Input Data is converted into RDDs which is a basic element of Spark for in memory computation. There are various transformation and action functions to generate RDDs and to work on RDDs. Frequent itemset mining is performed on these RDDs as a result of which frequent itemsets and association rules are generated.

Input  Dataset  Generate ROD form  Frequent Itemset  Min ing based on  FP-Growth  Frequent Itemsets  Rules  Association Rules  Display Frequent  Patterns  Fig 3. Block Diagram of Proposed Model

IV. EXPERIMENTAL RESULTS  The experiment was carried out in a single node hadoop cluster with hadoop version 2.6.0. Spark was set up on hadoop with version 1.6. Spark provides a rich machine learning library, MLLib, which has various built in machine learning functions. It has built in functions for frequent itemset mining using FP-Growth. This makes it easier to develop a faster frequent itemset mining algorithm in speedier environment.

A. Datasets  Three datasets are used for experimenting the algorithm.

Retail dataset [19] is a transaction data from Belgian store containing 88162 transactions. Each transaction is data about purchase date, receipt number; number of items purchased, price, customer number. Analysis of this dataset can reveal interesting patterns like which products have highest sales in a particular month, customers who visit the store frequently and their purchase behavior. This dataset is chosen because it has average number of frequent item sets to be generated.

Mushroom dataset [19] is widely used dataset for data mining tasks. Mushrooms can be edible or poisonous.

Considering these classes, attributes of mushrooms are analyzed. Attributes considered are "cap-shape, cap color, odor, bruises, grill attachment, grill size, grill spacing, grill color, stalk-shape, stalk-root, stalk-surface-above-rings, talk- surface-below-ring, stalk-color-above-ring, stalk-color-below- ring, veil-type, veil-color, ring-number, ring-type, spore-print- color, population, and habitat." This dataset has many frequently occurring attribute values. This can greatly affect the time to generate frequent itemsets. There are 8614 transactions in this dataset.

Foodmart [20] is a retail dataset with 86647 transactions having no frequent itemsets. This dataset is chosen to learn the behavior in tenns of execution speed of the algorithm when no frequent item sets are found.

B. Results   700 == = ~  ~ = =   ~ === === 400 === 8 FPGrowth :::::::::  II BlgFIM ::::::= '00 E  ~ 200 l-  i=::=: E  1=  ~ ~ = 11- --- -~ --- --- --- ~ 0 = 10000 20000 30000 50000  Number of T ransactions  Fig 4. Performance of FPG row th in Spark versus BigFIM  Fig 4. is a bar chart showing the execution time of Spark implementation of FPGrowth and BigFIM in Hadoop. Both the algorithms are run on retail dataset which has 88162 transactions. The dataset is varied starting from 10000 transactions. Minimum support is set to 30% in both the algorithms. BigFIM does not produce association rules whereas FPGrowth produces association rules if minimum confidence is satisfied. Minimum confidence is set to 50% in FPGrowth. The graph shows that FPGrowth took 7 seconds to generate frequent itemsets but no association rules and BigFIM took 128 seconds to complete the execution. Input is increased to 20000 transactions. Minimum support is changed to 40% in FPGrowth and 50% in BigFIM. FPGrowth took 8 seconds to complete and BigFIM took 118 seconds. For 30000 transactions minimum support is 40% in FPGrowth as it revealed to be faster in previous runs and it is set to 75% in BigFIM so that not many frequent itemsets are generated and make runtime faster with increased transactions. FPGrowth took 8 seconds and BigFIM took 141 seconds. Transactions increased to 50000. Minimum support is set to 80% in BigFIM and 40% in FPGrowth. FPGrowth took 8 seconds and BigFIM took 709 seconds. When transaction is increased to 88162, FPGrowth shows a consistent speed of 9 seconds to complete with minimum support 40% and minimum-confidence 50% with no association rules. It took 9 seconds to complete with association rules when minimum-support is 20% and minimum-confidence 50%. BigFIM executed for an hour to complete its execution.

il ~ . .

E ;:  FPGrowth for three data sets 10 ,----------------  - FPGrowth  mushroom retail foodmart  Fig 5. Perfonnance of FP-Growth in Spark on Different Datasets  Fig 5. shows a line graph showing execution time of FPGrowth in spark for three datasets: mushroom, retail and foodmart. Mushroom dataset with 8614 transactions has many frequent-itemsets and association rules, retail dataset with 88162 transactions has few frequent itemsets and association rules and foodmart with 86467 transactions has no frequent itemsets. Minimum support is set to 50%, 40% and 20% for mushroom, retail and foodmart dataset respectively. Average execution time of FP-Growth in Spark is 8 seconds.

FPGrowth for mushroom with minconf=50%  20 50 80  Minimum Support in %  - FPGrowth for mushroom with m inconf=50%  Fig 6. Perfonnance of FP-Grow th in Spark on Mushroom Dataset  Fig 6. is a line graph that shows execution of FPGrowth on mushroom datasets with varying minimum support and minimum confidence is 50%. When minimum support is 20%, algorithm took 65 seconds as many frequent item sets and association rules were generated. When minimum support is increased to 50%, algorithm finished in 7 seconds and when minimum support is set to 80% algorithm took 7 seconds to complete.

In all the experimented cases, FP-Growth in Spark has shown faster execution with average execution speed of 8 seconds with average minimum support of 40%.



V. CONCLUSION  Proposed work focused on implementing Revamped Market Basket Analysis based on FP-Growth using Hadoop Apache Spark framework which provides rich set of machine learning libraries for machine learning algorithms and studies the performance of the algorithm in spark framework using different datasets like retail, mushroom, foodmart. The performance of the spark implementation is high in terms of execution speed and also with growing dataset size which is compared with BigFIM algorithm implemented in Hadoop.

This clearly shows that in order to deal with large datasets Hadoop MapReduce approach is used, but MapReduce has latency problem which can be overcome using Apache Spark.

