

Abstract-Many algorithms have been proposed to solve the problem of mining frequent itemset. However, the large size of the itemsets search space is still a challenging problem that eliminates the performance of any association rules mining algorithm. The size of search space is exponential to the number of items in the database. In this paper, an effective mechanism is proposed to reduce the search space. Moreover, an efficient algorithm is also devised for mining association rules based on this reduced search space. An accumulative support distribution of 2-itemsets is created over different levels with only one database scan. This distribution provides estimation for the support values of all itemsets in search space. The estimated supports are used in generating the candidate itemsets in all levels without extra database scan. The proposed algorithm reduces the execution time by reducing both CPU and I/O times.

CPU time is saved by reducing candidate sets size, whereas the I/O time, is reduced by reducing the required database scans.

The experimental and analytical results show a significant improvement of performance up to several orders of magnitude compared to Apriori algorithm. In addition, the number of generated candidate itemsets in proposed algorithm is less than the ones generated by Apriori algorithm in most cases.

Keywords - Data Mining, Boolean Association Rules, Reduced Search Space

I. INTRODUCTION   Association rule mining (ARM) is considered one of the  most critical data mining components. This is not only because it discovers frequently occurring patterns in databases, but also the associations between these patterns' attributes. Recently, many approaches have been proposed for ARM.

Association rules are divided into quantitative and Boolean rules, based on their attributes type. Quantitative association rules describe associations between either numerical or categorical attributes, whereas Boolean-based methods reveal associations pertaining to the presence or absence of items.

Based on the terminology [1], the Boolean ARM is briefly described as follows: I={i1 , i2 , ?.., in} is a set of n binary literals called items. A transaction Ti is a set of items in I such that Ti ?  I. A database D is a set of transactions. A transaction Ti is said to contain a certain set of items (itemset) C, iff C ?  Ti. Itemset that consists of K items is called K-itemset. The support of an itemset C, denoted ?(c), is the percentage of transactions that contain C. An itemset is considered as frequent one if its support is greater than or equal to user defined value minimum support (min_sup). The maximal frequent itemset is defined as the frequent itemset that is not a subset of any other frequent itemset. The Boolean association rule (hereafter refereed to as association rule) is presented in the form  A ? B  ( read: A implies B) where A and B are two itemsets and  A ? B = ? . The association rule support is the  support of an itemset C where C = A U B . The rule is frequent iff itemset C is frequent. The confidence in the rule is the conditional probability P(B|A) (probability that a transaction contains B, given that it contains A). An association rule is considered of interest if it is frequent and its confidence is greater than or equal to (user defined value) minimum confidence (min_conf).

ARM can be considered as a two-step process. In the first step, all frequent itemsets are discovered. In the second step, strong association rules are generated from the frequent itemsets. The first step is the more laborious, that is why most of the research work in the ARM field has been dedicated to improve the efficiency of discovering frequent itemsets.

Discovering frequent itemsets is a search problem on search space contains 2n-1 itemsets where n is the number of individual items in the examined database. The search space can be divided into levels, each level contains itemsets of the same length (e.g. Level-k contains all k-itemsets in the search  space). Number of itemsets at level-k = ?? ?  ? ?? ?  ? k n  . This large  search space and the cost of calculating the support of the itemsets are the most challenging problems in ARM.   To overcome these problems, ARM algorithms use different navigation techniques, data structures, and dataset layouts as shown in the related work section.

The performance of many ARM algorithms can be increased if the search space is reduced before applying these algorithms. The reduced search space (RSS) must contain all frequent itemsets for a given min-support to ensure that the ARM output (frequent itemsets) are complete. Moreover the required execution time for generating RSS must be small. In this paper, we provide an algorithm for generating the RSS and an ARM algorithm based on RSS.

This paper is organized as follows: related work is discussed in Section 2. Section 3 presents the problem definitions. Section 4 describes the proposed mining algorithm. Section 5 presents the experimental results and Section 6 concludes the work.



II. RELATED WORK   Since Agrawal et al. were the first to formalize the ARM concepts [1], many ARM algorithms were proposed. They may differ in the data structure they use, the way to compute support, the navigation technique, and/or the database layout.

In addition, they are proposed to discover different types of output, all frequent, closed frequent or maximal frequent for exact and approximate solutions. However, all ARM algorithms found in the literature can be categorized into two main categories: candidate generation based techniques and pattern growth based techniques.

REDUCED SEARCH SPACE BASED ASSOCIATION RULE MINING ALGORITHM   A. M. Ghanem2, B. Tawfik1, and M. I. Owis1  1Biomedical Engineering Department, Cairo University, Giza, Egypt 2Faculty of Information Systems, Suez Canal University, Ismailia, Egypt  e-mail: ahmed@optimal-sys.com     Candidate generation based techniques: In this category, the candidate set is first generated, and  then support values of the candidate itemsets are evaluated.

The resulting itemsets are used in next candidate generation.

The candidate generation based techniques can also be divided based on the database layout used into horizontal and vertical layout based techniques.

Horizontal layout based techniques:  In horizontal layout, the database is a list of transactions, and every transaction consists of set of items. The well known Apriori algorithm [2] is an example of this category.

Apriori algorithm is the foundation upon which many ARM algorithms are based on [3,4,5,6]. It calculates frequent itemsets at any level K in two steps: First, the candidate set is generated, then, database is scanned, and support of the candidate set is calculated. By definition, itemsets having a support greater than the minimum support are considered frequent. Apriori algorithm generates the candidate set for any level K+1 from the frequent itemsets in level K only.

This is a plausible assumption, since the super itemsets of any infrequent itemsets is also infrequent, and there is no need to evaluate them. Apriori requires a number of database scans equal to the length of the longest frequent itemset in the best case, and additional database scan is required in worst case.

Apriori is based on breadth first bottom-up searching approach, since the search space is examined level by level, and the start navigation level contains all individual items.

Many algorithms have been proposed to increase the ARM performance. Direct Hashing and Pruning (DHP) [3] use a hash table for pre-computing the approximate support for the K+1 itemsets, while the K-itemsets are processed. Despite its elegance, DHP still inherit the number of database scans from Apriori. Perfect Hashing and Pruning (PHP) [4] provide an optimized version of DHP. The partition algorithm [5] aims at reducing the number of database scans by dividing the database into non-overlapping partitions, and calculating the local frequent itemsets for these partitions. Then the global candidate set is generated from the local frequent ones, and additional database scan is performed to generate the global frequent set. However, the size of global candidate set, and the algorithm performance is very sensitive to inhomogeneity of the database. AS-CPA [6] based on partition, and provides anti-skew technique which reduces the sensitivity for inhomogeneity of database.  In the Dynamic Itemset Counting (DIC) [7] algorithm, the database is divided into equal-sized partitions. The DIC uses the frequent itemsets in one partition to generate the candidate itemsets in other partition. While this technique considerably reduces the number of database scans in homogeneous database, it is also sensitive to inhomogeneity in the database. Pincer Search Algorithm [8] provides a bidirectional navigation technique to discover the maximal frequent set. The first navigation direction is bottom-up -as in Apriori-, the second is top- down. The results of both directions are used in maximal frequent candidate set (MFCS) generation and pruning.

Vertical layout based techniques: In vertical layout, the database is organized as list of  transactions IDs TID which associated with each item. DLG [9] algorithm uses a bit vector to represent the TID list for each item. The support of any itemset is calculated using logic and resulting bit vector summation. Hierarchical Bit Map (HBM) [10] algorithm uses two levels, bit map -in first level- divides bit vector into group of unsigned short (16 bits). In second level, every unsigned short is encoded in bit which represent if the unsigned short is empty or not. The itemsets support is calculated into two stages, First intersections into the second level are provided, then intersections are performed for the resulting non empty short resulting in the first stage. The performance of these bit vector based algorithms, is sensitive to increasing the number of transactions and number of items. Eclat and Clique [11] algorithms divide search space into disjoint subsets. The subsets are examined independently. Max-eclat, Max-clique [11], MAFIA [12], and GenMax [13] are algorithms for discovering maximal frequent itemsets, as an alternative to all frequent itemsets, all frequent itemsets can be discovered from the maximal frequent itemsets by one extra database scan.

Pattern growth based techniques:  In these techniques, the database is represented in a compact tree based form. The process of discovering frequent itemsets is performed using this compact form without any extra database scan. Since the concepts of pattern growth were first proposed in [14], many algorithms [15,16] have been proposed to reduce the complexity of used data structure and/or increase the performance of discovering of frequent itemsets.



II. METHODOLOGY   Many algorithms have been proposed to enhance the association rules mining. They may differ in the way they propose to reduce the number of database scans, or in the way they use to generate the candidate itemsets. Also, they may use different data structures to enhance the mining process.

All of the developed algorithms achieve good results however; they have not directly addressed the problem of the large size of the search space which is the main problem that affects the performance of mining algorithms. This paper presents a mechanism to obtain a reduced search space based on overestimation for itemsets support, and it also presents the mining algorithm based on this reduced search space.

To facilitate the presentation of the proposed algorithm, some proposed definitions and lemmas will be introduced first, then itemsets? support estimation will be discussed afterwards. Finally, the proposed techniques -for reducing search space and discovering frequent itemsets- will be introduced.

A. Problem Description  Definition 1: ? L(I) is the support of itemset I at level L. ?L(I) is the probability of transactions of length L containing the itemset I.

N.B: To simplify the examples, we use number of transactions as alternative to probability.

Example: Consider dataset in Figure 1, there are five individual items {a,b,c,d,e}, and 6 transactions; ?3 (d) = 2 because d  is found in two transactions of length 3 with TIDs = 1, 4; While ?4 (d) = 1, because d  is found in only one transaction of length 4 with TID = 3.  Also, ?4(a, d) = 1 since the itemset a,d is found in one transaction of length 4 with TID = 3.

Fig. 1: Example dataset    Definition 2:  PI is the support distribution of itemset I over the search space levels, and it is defined as PI = [? 1(I)  ?2(I) ?.. ? n(I)], where n is the number of items, and equals to the maximum number of levels in the search space.

Example: Pe = [0  1  3  1  0] represents the number of occurrences of itemset e in database transactions of length 1, 2, 3, 4, and 5 respectively, and Pa,e = [0  0  2  1  0], also, represents the number of occurrences of itemset a,e in database transactions of length 1, 2, 3, 4, and 5 respectively.

Figure 2 shows the support distribution of all individual items at different levels. Each row of Figure 2, represents support distribution for the corresponding item.

L1  L2  L3  L4  L5  a  0  0  2  1  0  b  0  1  1  1  0  c  1  0  1  0  0  d  0  0  2  1  0  e  0  1  3  1  0   Fig. 2: The support distribution of individual items at different levels   Definition 3: ?L(I) is the accumulative support of itemset I, at level L. ? L(I) is the probability of transactions of length greater than or equal to  L, that contain the itemset I.

?L(I) = ? =  LMax  Lk  _  ?k(I)  where Max_L equal to the maximum  transaction size. If the maximum transaction size is unknown, Max_L will equal to the number of the items.

Example: ?3(e) = 4, because the number of transactions of a length greater than or equal to 3, that contain item e is four transactions with TIDs = 1, 3, 4, and 5; ?3(e) is equal to the summation  of  ?3(e)=3, ?4(e)=1, and ?5(e)=0; Similarly, ?3(a e) = 3 is the summation of  ?3(a e)=2, ?4(a e)=1, and ?5(a e)=0, and transactions of length greater than or equal to 3 have TIDs = 1, 3, and 4.

Definition 4: accI is the accumulative distribution of itemset I over the search space levels.

accI = [? 1(I)  ?2(I)  ?.. ? n(I)], where n is the number of items, and equals to the maximum number of levels in the search space.

Example: acce = [?1(e) ?2(e) ?3(e) ?4(e) ?5(e)] = [5  5  4  1 0] while Pe = [0  1  3  1  0] and acce[2] = Pe[2]+ Pe[3]+ Pe[4]=3+1+0 . accae = [?1(ae) ?2(ae) ?3(ae) ?4(ae) ?5(ae)] = [3  3  3  1  0]while Pa,e = [0  0  2  1  0] and accae[1]=Pae[1]+ Pae[2] + Pae[3]+ Pae[4] = 0+2+1+0.

Definition 5: ACCK is the accumulative distribution of all k- itemsets over the search space levels. K is the order of the distribution.

ACCK =  ? ? ? ? ?  ?  ?  ? ? ? ? ?  ?  ?  nacc  acc acc  .

.

Where ACCK [i][j] = ?j(i)  Example: ACC1, accumulative distribution of order one, is the accumulative distribution of items over the search space levels. Figure 3 is the ACC1 for the dataset in Figure 1. Each row in ACC1 represents accumulative distribution (acc) for an individual item.

ACC2, accumulative distribution of order two, is the accumulative distribution of 2-itemsets over the search space levels. Figures 4.a, b, c show only three layers of ACC2 for levels 2, 3, and 4. ACC2 is presented as three dimensional array. The first and the second dimensions are items indices which construct 2-itemsets, while the third one is the level index.

L1  L2  L3  L4  L5  a  3  3  3  1  0  b  3  3  2  1  0  c  2  1  1  0  0  d  3  3  3  1  0  e  5  5  4  1  0   Fig. 3: ACC1 accumulative distribution of order 1.

TID  Items  1  a, d, e  2  C  3  a, b, d, e  4  a, d, e  5  b, c, e  6  b, e      a b  c  d  e  a  3  1  0  3  3  b  0  3  1  1  3  c  0  0  1  0  1  d  0  0  0  3  3  e  0  0  0  0  5  (a)   a b c d e  a 3  1  0  3  3  b  0  2  1  1  2  c 0  0  1  0  1  d 0  0  0  3  3  e 0  0  0  0  4  (b)   a b c d e  a 1  1  0  1  1  b 0  1  0  1  1  c 0  0  0  0  0  d 0  0  0  1  1  e 0  0  0  0  1  (c)   Fig. 4: ACC2 accumulative distribution of order 2.

a:  accumulative supports for 2-itemsets at  level 2.

b:  accumulative supports for 2-itemsets at  level 3.

c:  accumulative supports for 2-itemsets at  level 4.

Lemma 1:  For all itemsets I of length L, ? L(I) = ? (I)  Proof: By definition, ? K(I) is the accumulative support of itemset I at level K, for K = the length of  itemset I  (L),  ?L(I) = the support of itemset I=?(I), because there is no transaction of length less than L contains itemset I of length L  Lemma 2:  For all itemsets A of length L, where I  ?  A,      ? L(I) ? ? (A)  Proof: We will first prove that  ? L(I) ? ? L(A).

Let TI,K , TA,K are the sets of transactions of length K, and contain itemset I, and itemset A respectively.

TI,K ?  TA,K Count (TI,K) ? Count (TA,K),  where Count (TI,K) is the number of transactions in set TI,K  ? =  LMax  Lk  _  Count (TI,K)   ? ?  =  LMax  Lk  _  Count (TA,K)   ? L(I) ? ? L(A)  From Lemma 1; ? L(A) = ? (A) then ? L(I) ? ? (A).

Definition 6: ?` (A) is the estimated value of support of A.

?`(A)= Min  AI ? ( ? L(I))  Where, L is the length of A.

The length of itemsets I, equals to the order of the accumulative distribution that is used in estimated value calculation.

Example: Using ACC1, in Figure 3,  the estimated support value of the itemset  a b e  is ?` (a b e)=Min(? 3(a), ? 3(b), ? 3(e)) from ACC1 ? 3(a)=3,  ? 3(b)=2, and ? 3(e)=4. ?` (a b e)= Min(3, 2, 4) = 2  while the real support of  itemset a b e is ?(a b e) = 1 since only one transaction , TID = 3, contains itemset a b e.

Using ACC2 ,in Figure 4.b, ACC2 at level 3, we have that ?3(a b)=1, ?3(a e)=3, and ?3(b e)=2, so that ?` (a b e )= Min(1 , 3 , 2) =1 .   ?` (a b e) =1 using ACC2 , and ?` (a b e) =2 using ACC1 , while the real support ? (a b e) = 2. This example shows that the estimated value from ACC2, is more accurate than the estimated value from ACC1, because ACC1 provides information -only- about the distribution of items in the search space levels, and does not provide information about the relations between the items, while ACC2 provides the accumulative distribution of all couple of items over search space levels.

Lemma 3:  For all itemsets A of length L,  ? (A) ? ?` (A)  Proof:  Based on Lemma 2,  ? L(I) ? ? (A) for all  I ?   A , Min  AI ? ( ? L(I)) ? ? (A)  ? (A) ? ?` (A)  Lemma 3, ensures that the estimated value calculated from accumulative distribution is an overestimation of the real support, so that the itemset that has an estimated value less than min-support, is infrequent, and there is no frequent itemset having an estimated value less than the min-support.

Definition 7: Itemset I is a frequent itemset at level K, iff ?k(I) ? min_support.

Example: If we have minimum support count equals to 3 transactions, from ACC1, we can notice that itemset e (?3(e)=4) is frequent in level 3, while it is infrequent in level 4 (?4(e)=1); from ACC2, we can notice that itemset a e is frequent in level 2 (?2(a e)=3), while it is infrequent in level 4 as shown in Figure 4.c (?4(a e)=1).

Lemma 4: If itemset I is an infrequent itemset at level K then all itemsets of a length, that is greater than or equal to K and contains I, are infrequent.

Proof: From definition 3,  ? K(I) ? ? L(I) for all K > L     From Lemma 2, the support of itemset A of a length L is less than or equal to ?L(I), where I ?   A. If I is infrequent itemset at level k then itemset A also is infrequent.

Using estimated values of the itemsets support that calculated by accumulative distribution, the search space could be reduced dramatically. That is, we can exclude any itemset that does not verify the minimum support constraint from the search space.

The size of resulting search space, and the impact of search space reduction on ARM algorithm performance are dependent on the goodness of estimations, and the cost of estimations calculations. As shown in numerical example on definition 6, using accumulative distribution of order 2, ACC2, provides better estimation than using order 1 , ACC1, .On the other hand, if the estimations are underestimations for the real values, then the result of ARM algorithm will be erroneous/false. This is because some of the itemsets that could be considered as frequent are not path the minimum support constrain and removed from the search space before applying ARM algorithm. As shown in lemma 3, the proposed estimation is overestimation.

B. Proposed Algorithm   In the proposed algorithm, the reduced search space based  ARM algorithm (RSS-ARM), first, the database is scanned once to generate the accumulative support distribution for the 2-itemsets over different levels (ACC2). This distribution provides estimation for the support of all itemsets in search space. Then ACC2 is used to generate reduced search space (RSS). The number of itemsets in the RSS is less than the number of candidate itemsets generated by Apriori algorithm in most cases, as shown in result section. Finally, the database is scanned, and the support of all itemsets in RSS is calculated and all frequent itemsets is discovered by applying the min-support constrain. Figure 5 shows the proposed algorithm. ACC2 is generated in step 2. According to Lemma 1, the support of itemsets in level 2 of ACC2 is equal to the support of 2-itemsets in the database, so that in step 3, AFS is initialized by the frequent 2-itemsets in ACC2 at level 2. In step 4, RSS is initialized by the frequent 2-itemsets. In steps 5-11, the RSS is generated. Finally, in steps 12 and 13 the itemsets in RSS are evaluated, and all frequent itemsets are discovered.

Input: a database and user-defined minimum support Output: AFS which contains all frequent itemsets  1. k := 2; AFS :=? ; RSSall :=? 2. call procedure to generate ACC2 3. AFS :=  frequent 2-itemsets at level 2 in ACC2 4. RSSk := AFS 5. while RSSk ?? 6.   RSSall :=  RSSall ? RSSk 7. call procedure to pre-prune RSSk based on  ACC2 8.    call join RSSk to generate RSSk+1 9.    call procedure to post-prune RSSk+1  10.   k := k + 1 11. end-while 12. read DB and count frequency of RSSall 13. AFS := AFS ?  frequent itemsets of RSSall 14. return AFS   Fig. 5: RSS-ARM algorithm.

Figure 6 shows the ACC2 generation method. The algorithm  is divided into two stages; the first one, includes  Fig. 6: ACC2 generation method   steps from 3 to 7. In this stage, the database is scanned and a distribution of 2-itemsets over levels is generated (definition 2). While the second stage includes steps from 8 to 14, the accumulative distribution is generated. The inputs to the method are the database, and the maximum number of levels in the accumulative distribution. Theoretically, if the database has 1000 items, then the maximum number of levels is 1000, however, this does not happen in real life. For example, if there is a transactional database of a supermarket, it never happens that one customer buys everything found in the supermarket at once. Accordingly, the number of itemsets in the database can not be chosen, as the number of the maximum levels. As a result, the maximum level number will be user defined, as a ratio from the average transactions length. For example, if the average value is 10, and the ratio is 1.2, then the maximum level is 12. The average length of transactions can be calculated from the database file size, and the number of transactions in the file. The effect of the  Procedure generate ACC2 Input: a database file, number of individual items, maximum number of levels in ACC2 Output: ACC2 /* ACC2 is stored in 3D array form the first and second indices addressee 2-itemsets will the third index determines the level as ACC2 [item][item][level] */  1. nItems := number of individual items in DB 2. nLevels := maximum number of levels in ACC2 3. for all transactions T in DB 4.         n := length of T 5.              for all items i ?  T 6.   For all items j?T 7.    ACC2[i][j][n] :=  ACC2[i][j][n]+1   8. for all items i of nItems 9.        for all items j of nItems 10.              k=nLevels-1 11.              while k>0 12.                  ACC2[i][j][k]:= ACC2[i][j][k] +  ACC2[i][j][k+1] 13.                   k := k - 1 14.             end-while 15. return ACC2     maximum number of levels on the size of the RSS is examined in result section.

Fig. 7: Pre-prune method.

The generated ACC2 is used to generate the reduced search space itemsets at all levels without any extra database scan.

The generation method is based on the concept that was introduced in lemma 4 for ACC2, which is, the super k- itemset of any infrequent 2-itemset at level k in the accumulative distribution is also infrequent. So the generation method uses only the frequent 2-itemsets in accumulative distribution. The reduced search space is generated level by level. At every level, the generation method is divided into three methods pre-prune, joint, post prune.

As shown in Figure 7, Pre-prune is done in two steps; in the first step, the accumulative distribution is pruned, in the second step, the k-1_itemsets that will joint to generate k- itemset are pruned. In the first step, the min_support constrain is applied to get frequent 2-itemset at level k (definition 6). In the second step, we compare the frequent 2- itemsets at level k, and level k-1, and get the 2-itemsets that are frequent at level k-1 and infrequent at level k. These 2- itemsets are denoted as delta itemsets, then we remove (label as not joinable). All k-1-itemsets that is a super set of any delta itemset (lemma 4).The joint and post-prune method like what is used in Apriori.



III. PRFORMANCE EVALUATION   In order to assess the performance of the proposed algorithm, two different sets of numerical experiments are conducted. The first set of experiments is designed to examine the effect of the number of levels in the accumulative distribution on both the execution time and the size of the reduced search space. In the second set of experiments, the performance of the proposed algorithm is compared to the Apriori performance with different datasets that varies in the number of items, the average transaction size, and the number of transactions. The examined datasets were generated using IBM synthetic data generator [17] that uses the following parameters: |D| is the number of transaction, |N| is the Number of items, |T| is the average size of transactions, and |I| is the average size of maximal frequent itemsets. For example, N1000.T10.I4.D100K specifies that  the number of items is 1000, the average size of transactions is 10, the average size of maximal frequent itemsets is 4, and the database size is 100,000 transactions.

The Apriori and the proposed algorithm are coded in java, and all the experiments are performed on a Core 2 Duo processor with 2 GB main memory running under Open Suse 11.0 Linux operating system. In implementing the Apriori algorithm, one-dimensional and two-dimensional arrays are used to reduce the required execution time for discovering frequent itemsets, in the first and second levels respectively as noted in [18]. For higher levels, the hash tree technique is used. However, in implementing the proposed algorithm, every two levels in the accumulative distribution are stored in one two-dimension array to reduce the used memory space.

A. Experiment One   In this experiment, the effect of the number of levels in  accumulative distribution on the proposed algorithm performance is examined. In Fact, the number of levels in accumulative distribution affects both the required memory for storing the accumulative distribution, and the size of the reduced search space. Increasing number of levels increases the required memory space while theoretically; reducing the number of levels increases the size of reduced search space.

Thus, it eliminates the effect of the pre-pruning method in reducing the search space for levels greater than the maximum number of levels in the accumulative distribution.

The experiment is applied on the datasets  T8I2L1000N1000D1000k and T10I6L1000N1000D1000k.

For both datasets, As shown in Figure 8 the execution time is reduced in the first part within exponential form with increasing the number of levels, while in the second part the execution time is increased with increasing the number of levels. As prescribed earlier, the number of levels affects both the size of RSS, and required memory space. The effect of RSS size on the execution time is more dominant in the first part. This is because both size of RSS and the execution time are reduced with increasing the number of levels until increasing the levels does not reduce the size of RSS. On the other hand, the required memory space becomes more dominant in the second part.

B. Experiment Two   In this experiment, the performance of the proposed algorithm is compared to Apriori. The experiment is divided into two parts; first, the execution time and number of candidate itemsets are measured for datasets have N1000, L1000, D100K and different T and I. While in the second part, the effect of number of transactions on the performance is measured. Figure 9 shows the results of datasets T8I4, T10I4, and T10I6 for N1000L1000D100k. As shown, the proposed algorithm has shorter execution time than Apriori in all min-supports. In Figure 10, the execution time is measured for different number of transactions, the result shows that the proposed algorithm is less sensitive to the number of transactions compared to Apriori, because the  Procedure to pre-prune RSS based on ACC2 Input: itemsets of RSSk, k-level and ACC2 Output: new pruned RSSk  1. CK := RSSk 2. delta _itemsets := all 2-itemsets which  were frequent in ACC2 level k-1 and became infrequent in level k  3. for all 2-itemsets I in delta_itemsets 4.   for all itemsets IR in Ck 5.    if  I is subset of IR 6.     delete IR from Ck 7. return Ck        proposed algorithm requires two database scans, while Apriori requires number of database scans not less than the length of the largest frequent itemsets.

Fig. 8: Effect of number of levels on RSS and execution time for RSS-ARM at minimum support = 0.002          Fig. 9: Change in execution time and number of candidate itemsets with different minimum support for Apriori, and RSS-ARM algorithms.

Fig. 10: Effect of number of transactions on execution time for Apriori and RSS-ARM algorithms at minimum support = .002

IV. COUNCLUSION  We introduced the concept of accumulative distribution of  itemsets supports over search space levels, and provided technique to generate reduced search space. The reduced search space can provide better starting point for many ARM algorithms; moreover, we propose ARM algorithm based on reduced search space. The proposed algorithm requires only two database scans to discover all frequent itemsets, and generate number of candidate itemsets less than ones generated by Apriori algorithm in most cases. The reduction in number of database scans, and candidate itemsets reduce the required I/O time and CPU time, which implies that increasing in the proposed algorithm performance up to several orders of magnitude compared to Apriori in most cases.

