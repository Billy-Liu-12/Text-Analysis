Interactive Event Recognition in Video

Abstract? In this paper, we propose a multi-modal decision-level fusion framework to recognize events in videos. The main parts of the proposed framework are ontology based event definition, structural video decomposition, temporal rule discovery and event classification. Various decision sources such as audio continuity, content similarity, and shot sequence characteristics together with visual video feature sets are combined with event descriptors during decision-level fusion. The method is considered to be interactive because of the user directed ontology connection and temporal rule extraction strategies. It enables users to integrate available ontologies such as ImageNet and WordNet while defining new event types. Temporal rules are discovered by association rule mining. In the proposed approach, computationally I/O intensive requirements of the association rule mining is reduced by one-pass frequent item set extractor and the proposed rule definition strategy. Accuracy of the proposed methodology is evaluated by employing TRECVid 2007 high level feature detection data set by comparing the results with C4.5 decision tree, SVM classifiers and Multiple Correspondence Analysis.

Keywords? Interactive event definition, decision fusion, event classification, event recognition, rule mining, semantic video analysis.



I. PROPOSED FRAMEWORK We propose a multi-modal, multi-layer, concept-based semantic  segmentation and event recognition framework. The main phases of the proposed system are event definition, temporal rule discovery, physical video segmentation and event recognition. Block diagram of the overall flow is given in Fig. 1. Video event is defined as a collection of temporal descriptors and certain feature occurrences in a scene. The user can define new events and the system can learn the descriptors and features of these new events through the event definition and temporal rule discovery modules which use a set of training videos. Keyword based event definitions (constructed by the user) are enhanced and converted to visual descriptors. Rule learning is applied to the (scene descriptor set, event label) pairs and the underlying rules of the form ?Scene descriptor subset => Event Label? are extracted. Once the learning phase is over the system is ready to recognize events in new videos. Scene identification is done through physical video segmentation and for each scene a feature bag is constructed. Text filters (domain keywords), audio analysis (silence intervals, amplitude patterns), and motion flow analysis (motion vector direction pattern) are employed for advanced feature construction. Event recognition is the final step, which employs rules discovered by the temporal association rule miner, on the constructed feature bag to detect any known event in each scene.

The contributions of the proposed approach are: ? The mined association rules are integrated with the multi-  modal decision strategy and ontology related interactive event modeling.

? Event definition and recognition are handled in a way that event model is constructed automatically under user supervision. As a result, video annotation requirement is eliminated.

? The proposed event definition strategy provides an ontology connection ability through which predefined system modeling requirement is reduced to simple descriptor definitions.

? Temporal association rules are used to recognize events. The proposed temporal representation is a combination of probabilistic models, first order logic, multi-modal decision fusion, and feature fusion.

? The framework also achieves high coverage by examining audio continuity, content similarity, context vocabulary and shot sequence characteristics together with visual video features. Various knowledge and decision sources and enhanced temporal rule mining strategy are integrated with event descriptor definitions in order to achieve accurate and precise event recognition.

Fig. 1: Block Diagram of Proposed Approach.

A. Event Definition Event descriptors are initialized as keywords and each keyword is  mapped to a set of image descriptors. Each event is examined and defined in terms of temporal descriptor occurrences. An event definition interface is constructed in order to achieve high and adaptable event coverage. The interface enables users to construct new event types, by importing the corresponding training video data, and assigning descriptors to that particular event. The user interaction results in an interactive, evolving and dynamic domain representation.

WordNet is used to enlarge the user defined event descriptors and ImageNet is used to convert textual descriptors into visual descriptors.

Although there are other image databases e.g. [3], its coverage of verified high resolution images makes ImageNet the best candidate.

B. Temporal Rule Discovery  In order to detect event in a scene, first the underlying temporal rules should be discovered using the user-defined event descriptors in the training videos. Frequent itemset generation and rule generation are the phases of rule discovery process. In itemset generation step, descriptors are mapped to the items of association rules. Event type is added as an item to each itemset. Association rule mining is performed to generate underlying rules. For each scene, previous itemsets are examined and new possible itemsets are also revealed. At the end of the training phase frequent itemsets satisfying defined thresholds are selected to define rules of the current event. The proposed approach requires only one pass on the training video set for association rule mining.

C. Scene Description  Each event is defined to occur in a scene, thus scenes should be analyzed to define best representative feature set for the event representation. Scene to feature sequence mapping, sequence matrix construction and sequence similarity definition are the components of the proposed scene description phase.

2013 IEEE International Symposium on Multimedia  DOI 10.1109/ISM.2013.24   2013 IEEE International Symposium on Multimedia  DOI 10.1109/ISM.2013.24       D. Physical Video Segmentation Physical video segmentation covers frame, shot, and scene  identification. Applied shot detection algorithm is a specialized version of threshold-based strategy [4]. When shots are identified, they are clustered in terms of the underlying bag of features. After shot clustering, the scenes are constructed from shots by employing pattern investigations on sequence alignment, audio, text and various other decision sources.

E. Event Recognition  Event recognition is the process of assigning scenes to the correct event types. The proposed event recognition process is utilized by enhanced event definition and multi-modal decision fusion schemes in which discovered rules and other decision sources are employed. User centered and ontology enabled event definition ability results in interactive and adaptable event recognition.

Each scene is represented as a bag of features and descriptors which are extracted from various feature sources such as audio pattern (silence intervals, amplitude patterns), text (subtitle descriptor existence), user defined descriptor occurrences, object existences (HOG results), and motion pattern (direction, magnitude) and so on.

When a bag of features is constructed for each scene, discovered association rules, sequence similarity results and given decision sources are employed and an event type is assigned to each scene.

The decision fusion of these decision sources employs association rules as the main source. In case of not having descriptive or precise rule sets, other decision sources are employed according to the computational requirements.



II. EVALUATION Decision tree, SVM, and MCA [5] are employed in the  comparative evaluation of the proposed system. WEKA Decision Tree and  ??????? ???? [6] are applied with default parameters, and MCA is used for the same dataset. The TRECVid 2007 [7] high level feature detection data set is used in the evaluation. The scene boundaries are detected and shots in the scene are constructed from the high level feature labels. The dataset is not designed for scene based concerns thus an initial calculation step is applied for conversion. The frames of most common label are kept in the scene and others are eliminated.

The evaluation of physical decomposition is carried out by the performance of shot boundary detection. Our algorithm results in 92.20% precision and 91.36 recall values. The results are in the range of top 10 submissions [8]. The performance results of event classification are given in Table 1. 3-fold cross validation is applied in order to test robustness of the algorithm and Table 1 contains the average of those applications in order to give representative results.

The results show that the quality of event descriptors and the description capability are strongly reflected in the performance of event classification.



III. CONCLUSION The proposed heuristic maps temporal characteristics of the  sequence into multi-digit number format. As a result of the domain translation, the temporal comparisons become integer comparisons and attached to the frequent item generation task. Instead of heavy string matching and occurrence considerations, the itemset generations are handled just by integer comparisons and sequence matrix examination. Interactive event definition and recognition capability is a promising approach for generalized, multi-purpose event recognition frameworks. The proposed approach could be improved and event definitions could be enlarged. An interactive event definition database with correspondence to WordNet and ImageNet make a novel improvement in video analysis. Event network construction would lead detailed event categorization and continuously improved event definitions.

The training data set includes various types of environments in order to cover a wide spectrum of events. However in specific domains such as surveillance videos, the environment is strongly defined and event types are limited. There are various image processing applications for video foreground/background detection.

Those methods could be applied to the training data in order to replace the existing background with the surveillance background in order to eliminate misleading effects of the backgrounds differences.

Table 1: Evaluation of the proposed approach.

