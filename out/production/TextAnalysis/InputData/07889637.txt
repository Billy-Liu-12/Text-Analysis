TSINGHUA SCIENCE AND TECHNOLOGY ISSNll1007-0214ll03/09llpp149?159

Abstract: With cloud computing technology becoming more mature, it is essential to combine the big data  processing tool Hadoop with the Infrastructure as a Service (IaaS) cloud platform. In this study, we first propose  a new Dynamic Hadoop Cluster on IaaS (DHCI) architecture, which includes four key modules: monitoring,  scheduling, Virtual Machine (VM) management, and VM migration modules. The load of both physical hosts  and VMs is collected by the monitoring module and can be used to design resource scheduling and data locality  solutions. Second, we present a simple load feedback-based resource scheduling scheme. The resource allocation  can be avoided on overburdened physical hosts or the strong scalability of virtual cluster can be achieved by  fluctuating the number of VMs. To improve the flexibility, we adopt the separated deployment of the computation  and storage VMs in the DHCI architecture, which negatively impacts the data locality. Third, we reuse the method of  VM migration and propose a dynamic migration-based data locality scheme using parallel computing entropy. We  migrate the computation nodes to different host(s) or rack(s) where the corresponding storage nodes are deployed  to satisfy the requirement of data locality. We evaluate our solutions in a realistic scenario based on OpenStack.

Substantial experimental results demonstrate the effectiveness of our solutions that contribute to balance the  workload and performance improvement, even under heavy-loaded cloud system conditions.

Key words: Hadoop; resource scheduling; data locality; Infrastructure as a Service (Iaas); OpenStack  1 Introduction  Cloud computing is one of the hottest areas of  ?Dan Tao and Bingxu Wang are with School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing 100044, and Jiangsu High Technology Research Key Laboratory for Wireless Sensor Networks, Nanjing 210003, China. E-mail: dtao@bjtu.edu.cn.

? Zhaowen Lin is with Network and Information Center,  Institute of Network Technology, Science and Technology on Information Transmission and Dissemination in Communication Networks Laboratory, National Engineering Laboratory for Mobile Network Security, Beijing University of Posts and Telecommunications, Beijing 100876, China.

E-mail: linzw@bupt.edu.cn.

?To whom correspondence should be addressed.

Manuscript received: 2016-10-01; revised: 2016-12-26; accepted: 2016-12-27  research at home and abroad, which integrates large- scale computing, storage, and network resource via a network, and provides these resources for different users on demand[1]. As an open-source framework for distributed system architecture, Hadoop can achieve large-scale data computing and storage, and is usually deployed on physical cluster. There are some drawbacks in traditional Hadoop clusters. First, its deployment and configuration are tedious tasks. When Hadoop starts running, the realtime monitoring on Hadoop consumes plenty of manpower and financial resources. Second, the fluctuation of tasks causes imbalance of resource utilization. With the appearance of peaks in the tasks, resource bottlenecks may be encountered. In contrast, the troughs in the tasks will bring idle resource. Hadoop cannot realize dynamic resource allocation. Third, the utilization of high-performance computers in physical    150 Tsinghua Science and Technology, April 2017, 22(2): 149?159  clusters is insufficient, particularly for computation and storage resource, resulting in severe resource wastage.

To solve the abovementioned problems, it is essential to deploy a Hadoop cluster on an OpenStack-based cloud as its service[2]. This study adopts OpenStack, which can provide an Infrastructure as a Service (IaaS) solution in the form of Virtual Machines (VMs). Sahara, an open-source project, is developed to rapidly deploy a Hadoop cluster in an OpenStack- based cloud environment. A virtual cluster, which can simplify cluster management, enables autonomic management of the underlying hardware, facilitating cost-effective workload consolidation and dynamic resource allocations for better throughput and energy efficiency. However, virtualization in such cloud platforms is known to cause performance overheads[3].

Understanding how to optimize the performance of a Hadoop cluster has attracted considerable attention.

Researchers have accumulated a series of research achievements on resource scheduling and data locality in the related context.

Scheduling techniques for dynamic resource adjustment have been recently addressed. Sandholm and Lai[4] presented a dynamic priority parallel task scheduler for Hadoop. It allowed users to control their allocated capacity by adjusting their spending time. Sharma et al.[5] proposed a MapReduce resource Orchestrator (MROrchestrator) framework, which dynamically identified resource bottlenecks and resolved them through fine-grained, coordinated, and on-demand resource allocations. However, the abovementioned studies focused on a resource scheduling-based traditional Hadoop cluster. Lama and Zhou[6] studied automated resource allocation and configuration of the MapReduce environment in the cloud without considering the load of physical hosts.

Zuo et al.[7] proposed a resource evaluation model based on entropy optimization and dynamic weighting.

The entropy optimization filtered the resources that satisfied user QoS and system maximization by goal function, constraints of maximum entropy, and the entropy increase principle, which achieved optimal scheduling and satisfied user QoS. Liu et al.[8]  presented an adaptive method aiming at spatio-temporal efficiency in a heterogeneous cloud environment. A prediction model based on an optimized kernel-based extreme learning machine algorithm was proposed for a quick forecast of job execution duration and space occupation, which consequently facilitates the process  of task scheduling.

For data locality, to address the conflict between  locality and fairness, Zaharia et al.[9] proposed a simple delay scheduling algorithm wherein a job waited for a limited amount of time for a scheduling opportunity on a node that has data on it. Experimental results showed that waiting can achieve both high fairness and high data locality. Jin et al.[10] proposed an availability- aware data placement strategy, and its basic idea was to dispatch data based on the availability of each node for reducing network traffic and improve data locality.

Both works were studied on a traditional Hadoop cluster. Thaha et al.[11] presented a data location- aware virtual cluster provisioning strategy to identify the data location and provision the cluster near the storage. However, multiple tasks might be executed on a same physical host, which negatively impacted system performance.

Motivated by this, we propose load feedback-based resource scheduling and dynamic migration-based data locality solutions based on a novel Dynamic Hadoop Cluster on IaaS (DHCI) architecture. The resource utilization can be improved by the load balance of physical hosts and the flexible scalability of VMs.

Moreover, based on the separated deployment of the computation and storage VMs, computation VMs can be quickly migrated to match their corresponding storage VMs in order to effectively guarantee data locality.

The remainder of this study is organized as follows.

In Section 2, we introduce a DHCI architecture. Based on this architecture, load feedback-based resource scheduling and dynamic migration-based data locality solutions are explored in Section 3. In Section 4, we perform a comprehensive evaluation to validate our solutions. Finally, we conclude this study in Section 5.

2 DHCI Architecture  There exists a huge difference on Hadoop?s running environment between a physical cluster and an IaaS cloud platform[12]. In the IaaS cloud environment, Hadoop is deployed on VMs provided by the cloud platform. In this case, the Hadoop cluster cannot sufficiently understand the resource usage of the underlying physical hosts, which will result in load imbalance and performance degradation. In addition, the scalability of the Hadoop cluster is not satisfactory.

In contrast, the virtual Hadoop cluster on the cloud    Dan Tao et al.: Load Feedback-Based Resource Scheduling and Dynamic Migration-Based Data Locality for : : : 151  platform is more convenient for the flexible adjustment of cluster scaling.

Motivated by this, we integrate Hadoop onto an IaaS cloud platform and propose a new DHCI architecture.

In the DHCI architecture illustrated in Fig. 1, we introduce four kernel modules besides the original packages of private cloud and Hadoop.

? Monitoring Module: Considering that different  clusters in a virtual environment are isolated, Hadoop cannot obtain the load of physical hosts at all. A monitoring module is introduced to periodically monitor the load on the physical hosts as well as the VMs. The load information collected can be used to provide the basis for resource scheduling.

? Scheduling Module: It is responsible for two  aspects: (1) periodically pushing the load information of the physical hosts to the scheduling node (e.g., ResourceManager) in Hadoop and (2) issuing the corresponding scalability strategy to the VM management module according to the load of the VM clusters.

? VM Management Module: It achieves dynamic  scaling of the VMs by adding or deleting operations. This is an execution module which takes instructions from the scheduling module and interacts with the VMs on the IaaS platform.

? VM Migration Module: It is used to detect a task?s  data locality and execution process. Once this module finds (1) the execution progress of a task is slower than a given threshold and (2) its CN and  SN do not meet the data locality, it will migrate this task to a suitable physical host by the storage of data duplication.

In summary, the DHCI architecture has two features: (1) joint load monitoring, and (2) flexible resource scheduling. The monitoring module monitors the physical and virtual resources with full awareness of the current system load conditions. This necessary data can be utilized to optimize subsequent resource scheduling.

Through the scheduling and VM management modules, the resource utilization can be optimized according to the load balancing of the physical hosts and the flexible scalability of the VMs. Based on the idea of ?mobile computing?, the reuse of VMs migration, achieved by the VM migration module in the DHCI architecture, can also reduce bandwidth consumption and improve system performance.

3 Resource Scheduling and Data Locality  3.1 Load feedback based resource scheduling  The resource scheduling selects appropriate resources assigned to different tasks for execution[13]. Currently, most evaluation indicators are static or predictive physical performance items, such as the computing power of CPU, storage capacity, and network bandwidth[14]. However, in the dynamic environment of cloud computing, it is difficult for these indicators to reflect the actual service ability of the physical resource.

In our solution, the load of physical hosts can be described from two aspects: CPU utility rate and load average. Load average is a kind of performance  Monitoring  module IaaS  Virtual machine  management module  Scheduling  module  Monitor physical hosts  Upload monitoring information  Issue scheduling strategy  Increase/decrease virtual machine  Push load information of physical hosts    Physical host    Virtual machine  Monitor virtual machines  Virtual machine  migration module  Migrate virtual machine  Fig. 1 DHCI architecture.

152 Tsinghua Science and Technology, April 2017, 22(2): 149?159  parameter (e.g., memory, disk, and network). This parameter denotes the average utilization rate of run queues. The higher the values of CPU utility rate and load average, the heavier the workload of a physical host. The VMs mentioned here run Linux OS; therefore, the performance of the system can be monitored every minute using the Top command in Linux. For the whole VM cluster, we adopt a unified script to collect the status of resource consumption.

In the DHCI architecture, for a physical host, its load information will be uploaded and fed back to the scheduling module periodically via the monitoring module. We adopt a single-level threshold method to compare the load information, and its workflow can be illustrated in Fig. 2. Once the load exceeds a preset threshold, the physical host is considered as stressed out, and the resource application using it will be canceled. Otherwise, the resource application will be supported.

One of the most significant advantages of integrating Hadoop onto the IaaS cloud platform is flexibility.

In other words, the scale of the virtual cluster can dynamically adjust according to its real-time workload.

Similarly, for the virtual Hadoop cluster comprising multiple VMs, the monitoring module in the DHCI architecture collects its load information and feeds it back to the scheduling module. A double-level threshold method is used to distinguish between the lowest load VM and the highest load VM, as shown in Fig. 3. If the load exceeds a ceiling value, the VM addition operation will be issued and a new VM will be created on the lowest load physical host. However, if it  Overload?

Yes  Unsuccessful  resource application  Resource match?

No  Successful resource  application  Yes  No  Compute the load of physical  host where resource is available  Resource application  Fig. 2 Single-level threshold resource scheduling algorithm.

Overload?

Yes  Select the lowest  load physical host  Create new virtual  machine  Underload?

No  Select the highest  load physical host  Delete excessive  virtual machine(s)  Yes  No  Compute the load of  virtual Hadoop cluster  Fig. 3 Double-level threshold resource scheduling algorithm.

is below a floor value, the VM deletion operation will be issued and excessive VM(s) will be deleted on the highest load physical host.

3.2 Dynamic migration based data locality  Large-scale distributed systems aim at processing data as close as possible to the storage location to reduce data movement between the computer and storage facilities, which is typically known as data locality[12]. Data locality is a crucial factor impacting the performance of a virtual Hadoop system. In a traditional Hadoop cluster comprising physical hosts, computation VMs (used for task computation, denoted by CNs), and storage VMs (used for data storage, denoted by SNs) are combined into a single VM. The advantage is that CNs can directly obtain data from SNs while avoiding data transmission across a network. However, this deployment is no longer an effective method for a virtual Hadoop system.

The scalability of a virtual Hadoop cluster can be achieved by dynamically adding or deleting VMs. The combination of CNs and SNs results in poor scalability.

This means that once CNs are added or deleted, their corresponding SNs should be applied with the same operation. Moreover, in the process of VM migration, VMs functioning as CNs and SNs will cause massive data movement, thereby reducing the efficiency of VM migration.

Therefore, in the DHCI architecture, the separation of CNs and SNs is adopted to improve flexibility. In particular, they are deployed as respective VMs. In this manner, CNs can be migrated to a ?suitable? place based on the idea of ?mobile computing?. It is obvious    Dan Tao et al.: Load Feedback-Based Resource Scheduling and Dynamic Migration-Based Data Locality for : : : 153  that this deployment form offers several advantages over a centralized method[15]: (1) strong scalability, which allows for respective fluctuating numbers of CNs or SNs and (2) flexible migration, i.e., CNs can be migrated without considering any other SNs.

Compared to that of the traditional Hadoop cluster, data locality in the DHCI architecture can be classified into three categories[16], as illustrated in Fig. 4.

? Host data locality: CNs and SNs are deployed on  the same host (e.g., VM1 and VM2 are on Host1).

? Rack data locality: CNs and SNs are deployed on  the same rack but different hosts (e.g., VM1 and VM4 are on Rack1).

? Across-rack data locality: CNs and SNs are  deployed on different racks (e.g., VM1 and VM10 are on Rack1 and Rack2, respectively).

Experimental results have shown that the speeds of task execution for meeting different types of data locality are significantly different under the same conditions[17]. In particular, the task completion time for meeting ?rack data locality? and ?across- rack data locality? approaches three and four times as long as that for meeting ?host data locality?, respectively. Data transmission between co-located VMs is often as efficient as local data access mainly because inter-VM communication within a single host is optimized by the hypervisor[18]. Hence, we consider to improve ?host data locality? in order to optimize the performance of the DHCI architecture. Considering that the separation of CNs and SNs influences data locality, we dynamically migrate the CNs to any host(s) or rack(s) where the corresponding SNs are deployed to guarantee data locality. During the migration process, a VM remains on and the program executed in this VM  VM1  Host1  VM 2  VM3  Host2  VM4  VM 9  Rack1  Host3  Host4  Rack2  VM7 VM8  VM5 VM6  VM10 VM11  Host data locality Rack data locality Across-rack data locality  Fig. 4 Three types of data locality in a virtual Hadoop cluster.

keeps track of the running state. Even if this VM is connected to a network, the network connection will not be affected. In fact, the cost of migrating the VM is considerably less than that of reading/writing operations among different VMs[16].

First, the initial resource allocation should keep data locality. In the Hadoop YARN adopted, ContainerAllocator is responsible for communicating with Resource Manager and applying resources for tasks. Usually, there exist three backups for each task in HDFS. Considering the level difference of data locality, there will be multiple resource requests. VMs can be allocated resource, prioritized by ?host data locality?, ?rack data locality?, and ?across-rack data locality?. A resource request for a task can be described as a tuple <Priority, Hostname, Capability, Containers>, where ?Hostname? can represent the ID of the host or the rack.

Consider the case in Fig. 4 as an example wherein a task applies a resource in order from the host to the rack.

If this task can acquire a resource from a certain host, the request will stop; otherwise, it will apply it one by one in the following manner: <20,?Host1(VM1,VM2)?,?memory:2G?,?1?> <20,?Host2(VM3,VM4,VM9)?,?memory:2G?,?1?> <20,?Host3(VM5,VM6,VM10,VM11)?,?memory:2G?,  ?1?> <20, ?Rack1?,?memory:2G?,?1?> <20, ?Rack2?,?memory:2G?,?1?>.

Second, data locality should be optimized in the  process of task execution. Hadoop monitors task execution and judges whether data locality is satisfied or not. If not, Hadoop continues to search whether there exist one or more hosts which can meet the requirement of data locality. Then, CN will migrate to the correct one.

The real-time dynamics and uncertainty of cloud resources make resource management and task scheduling challenging[19]. Parallel computing entropy is developed from Shannon information entropy, which has the characteristics of symmetry, nonnegativity, and scalability. Sun et al.[20] have proved that parallel computing entropy can be maximized if and only if the load is completely balanced in the homogeneous cluster or the grid environment. In this study, we extend the method of parallel computing entropy into the heterogeneous cluster or cloud computing environment. To accurately grasp the dynamic load and available information of resources, we propose    154 Tsinghua Science and Technology, April 2017, 22(2): 149?159  a parallel computing entropy based VM migration solution. In particular, the load of the i -th physical host can be represented as Li ,  Li D  nX jD1  !ij sj (1)  where n denotes the total number of VMs on the physical host i . For the physical host i , !ij denotes the ratio of the number of VMs with the task type j to the total number of VMs and sj denotes the computation of VM with the task type j .

The relative load Ki of physical host i , which is the ratio of the individual load to the total load of m physical hosts, can be represented as follows:  Ki D Li=  mX iD1  Li (2)  Considering the difference among performance parameters (e.g., CPU, memory, and bandwidth) of each physical host, we express the processing capacity of the physical host as follows:  Pi D fPcpu; i ; Ploadaverage; ig (3) where Pcpu; i and Ploadaverage; i respectively denotes the CPU utility rate and the load average of the available physical host i . We quantify the above equation as follows:  Ptotal D ?Pcpu; i C ?Ploadaverage; i (4) where ? and ? are constant coefficients and satisfies ?C ? D 1.

Hence, the relative load Ki of the physical host i can be expressed as follows:  Ki D Li=Ptotal, i  mX iD1  Li=Ptotal; i  (5)  Assume that there are m physical hosts in a cloud computing environment. At time t , the relative load of the physical host i is denoted as Ki (t ). Parallel computing entropy of the whole physical cluster at time t can be defined as follows:  H.t/ D  mX iD1  Ki .t/ln  Ki .t/ (6)  During the migration process, the migration can be selected by the maximum entropy increment at each step, and thus make the execution time of all the tasks the shortest. Therefore, the increase of parallel computing entropy causes (1) the decrease in calculation amount of physical host(s) with the biggest load and (2) a more balanced load of other physical host, and thus a decrease in the total execution time of  all the tasks. The goal of load balancing is to increase the parallel computing entropy as far as possible to reduce the total execution time of all the tasks.

The amount of computation for all the VMs on each physical host can be calculated using the sampling interval T , and the parallel computing entropy H (t ) can be calculated by Eq. (6). Once the value of H.t/ is less than the threshold of  , the load balancing of the system is unsatisfactory to some extent, the VM migration process is needed.

To achieve a completed CN migration, three major issues (?2W1H?) should be solved.

3.2.1 Which CN needs to be migrated?

The physical host needs to be migrated as it is the highest-loaded device. Similarly, the CN that needs to be migrated is the CN experiencing the heaviest- computation load. For a CN needs to satisfy the need to be migrated, it must satisfy two conditions: (1) it is the highest-computation CN, i.e., CN has the greatest amount of calculations and (2) CN and its corresponding SN are on a separate host or rack, as shown in Algorithm 1.

3.2.2 Where should a CN be migrated?

The destination host to which a CN is migrated should include its corresponding SNs store data replication. In a virtual Hadoop cluster, each task can be partitioned into several Map and Reduce tasks. Each Map task runs map functions processing one data block (128 MB by default in YARN). The data replication of each data block is three by default, and can be stored in different  Algorithm 1 Determine CN to be migrated Input: the load Li of m physical hosts.

Output: CN.

begin  1: Calculate the average load of the system Lavg DPm iD1 Li=m;  2: Calculate the load difference4Li D Li ? Lavg; 3: Sort 4Li from the largest to the smallest, and store q1, q2,  ..., qm into the queue Qph ; 4: Select the first element q1 in the queue Qph, q1 denotes the  ID of physical host with the highest load; 5: Calculate the corresponding computation amount cj of CNj  on the physical host q1, where 1 6 j 6 s; 6: Sort cj from largest to smallest, and store CN1, CN2, ...,  CNs into queue QCN; 7: Select the first element CN1 in queueQCN, CN1 denotes the  ID of CN with the heaviest-computation amount; 8: return CN1  end    Dan Tao et al.: Load Feedback-Based Resource Scheduling and Dynamic Migration-Based Data Locality for : : : 155  hosts even racks. Here, we choose the least-loaded host which satisfies the requirement of data locality as the destination host with the lowest cost.

3.2.3 How should one migrate a CN?

The selected OpenStack cloud platform can support VM migration very quickly. The whole migration process involves three kinds of physical hosts: the source, destination, and control hosts. We mainly utilize the Python interface function in the Libvirt tool to migrate the VM, and data transmission can be realized in a tunneled way. As space is limited, the further description will not be given.

4 Emulation and Analysis  In this emulation, we choose OpenStack as the cloud platform and Hibench as the Hadoop performance testing tool. Hibench can provide a series of typical Hadoop benchmark test cases, which can be directly used to conduct a performance test. Here, three classic benchmark test cases, WordCount (counts the words in the input files), TeraSort (sorts the data generated by TeraGen), and Sort (sorts the data written by the random writer), are adopted to evaluate the performance of the proposed DHCI architecture. The hardware configuration for the testing environment is listed in Table 1.

4.1 Comparison of running time under the same load  First, we compare the running times using three classic schedulers (FIFO Scheduler, Fair Scheduler, and Capacity Scheduler) for the traditional Hadoop cluster and DHCI architecture. It should be noted that FIFO- DHCI, Fair-DHCI, and Capacity-DHCI are defined as the three schedulers used in the DHCI architecture. The emulation results in Fig. 5 show that the running time in the DHCI architecture is less than that in the traditional Hadoop cluster with the same workload (data volume is 2 GB). Using Fair Scheduler as an example, for the WordCount, TeraSort, and Sort cases, the running  Table 1 Hardware configuration for the testing environment.

Parameter Configuration CPU type 4-core 2.4 GHz Intel(R) Xeon (R) Memory 32 GB  Network card Three 2 Gbps LANs OS Linux 14.04  VM images Virtual CPU, 2 GB RAM, and 30 GB HDD      R un  n in  g t  im e  (s )  FI FO  FI FO  -D H  CI Fa ir  Ca pa  ci ty  -D H  CI  Fa ir-  D H  CI  Ca pa  ci ty  Scheduler  Sort TeraSort  WordCount T es  t ca  se  Fig. 5 Comparison of running time for the three schedulers under the two architectures.

times in the DHCI architecture are decreased by 14%, 9%, and 8%, respectively. The proposed scheduling approach can outperform other traditional approaches mainly because in the DHCI architecture, the resource allocation can be avoided on overburdened physical hosts or the strong scalability of the virtual cluster can be achieved by fluctuating the number of VMs.

Second, we evaluate the running time under two architectures with different data volumes (from 256 MB to 2048 MB), as illustrated in Fig. 6. As far as WordCount is concerned, the emulation results show that the performance of the DHCI architecture is superior to the traditional Hadoop cluster. Obviously, the more the data volume, the greater the benefits of the DHCI architecture be apparent.

4.2 Comparison on running time under the load pressure  Once the workload approaches the peak value or valley value, the DHCI architecture will process tasks by dynamically increasing or decreasing the number of virtual machines. In this way, compared to the        R u nn  in g  ti m  e (s  )  Traditional Hadoop cluster  DHCI architecture  Data volume (MB) 256 512 1024 2048  Fig. 6 Comparison of running time under two architectures with different data volumes.

156 Tsinghua Science and Technology, April 2017, 22(2): 149?159  traditional Hadoop cluster, the DHCI architecture can process the same workload with less running time by optimizing resource utilization. To test the operational condition of the Hadoop cluster under certain load pressure in a private cloud environment, when the workload on virtual cluster 1 reaches the maximum, a new virtual cluster 2 will be added, as shown in Fig. 7.

Supposing that the original workload of the task (WordCount, TeraSort, and Sort) run on virtual cluster 1 is 2 GB. The effect on the operational efficiency of the task on the two architectures with different scales of virtual clusters can be illustrated in Fig. 8.

There is no doubt that the dynamic increase of virtual clusters allows for resource utilization as well as load balancing, and thus results in less running time. Under a specific workload, compared to the running time for the traditional Hadoop cluster, that for the traditional cluster under load pressure decreases to 67%, 71%, and 55% for WordCount, TeraSort, and Sort, respectively. This depicts that  OpenStack  Physical host 1 Physical host 2 Physical host n  Virtual cluster 1   Virtual cluster 2  Add a new virtual cluster  Fig. 7 Adding new virtual cluster(s) onto IaaS cloud platform for relieving load pressure.

WordCount TearSort Sort Test case  R un  ni ng  t im  e (s  )      Traditonal Hadoop cluster Traditonal Hadoop cluster under load pressure DHCI architecture under load pressure  Fig. 8 Comparison of running time under two architectures with different scales of virtual clusters.

multiple tasks can be simultaneously operated on a virtual Hadoop cluster with relatively low cost. In the case of load pressure, compared with the running time for the traditional Hadoop cluster, that for the DHCI architecture decreases to 56%, 59%, and 52%, respectively. Under a specific workload, we compare the running time for the traditional Hadoop cluster with that of the DHCI architecture under load pressure; they are close to 2:1 (e.g., 913 vs. 507, 633 vs. 371, and 2450 vs. 1266, respectively). The results clearly show that with the same available computation resource of the cloud platform, the DHCI architecture can significantly reduce the running time and thus improve the operational efficiency of tasks.

4.3 Comparison of CPU utility rate and load average between the two architectures  In this section, we evaluate the performance parameters of each physical host when the WordCount task is executed in the DHCI architecture.

Figure 9 shows the CPU utility rates of four physical hosts (Ph1, Ph2, Ph3, and Ph4) from the 1st minute to the 14th minute. In this experiment, we set 50% as a threshold. Once the CPU utility rate is greater than this threshold, we regard the workload of the physical host as excessive. In this case, the resource on this physical host will not be allocated for task(s) any more. From the curves in Fig. 9, we find that in the 3rd, 6th, 7th, 8th, and 10th minutes, the CPU utility rates of part of the physical hosts exceeded the preset threshold of 50%.

For example, in the 3rd minute, both the CPU utility rates of Ph1 and Ph2 are 53%, which is greater than 50%. After 1 min, the CPU utility rates of Ph3 and Ph4 increase gradually to accomplish workload sharing.

Correspondingly, their values of Ph1 and Ph2 decrease to 43% and 49%, respectively.

Figure 10 shows the load average of four physical hosts during a period of 14 min. This experiment sets     C P  U u  ti li  ty r  at e  (% )  Time (min) 1 2 3 4 5 6 7 8 9 10 11 12 13 14  Ph1 Ph2 Ph3 Ph4  Fig. 9 CPU utility rates of multiple physical hosts in the DHCI architecture.

Dan Tao et al.: Load Feedback-Based Resource Scheduling and Dynamic Migration-Based Data Locality for : : : 157  Time (min) 1 2 3 4 5 6 7 8 9 10 11 12 13 14  L o  ad a  ve ra  ge  1.5  1.0  0.5   Ph1 Ph2 Ph3 Ph4  Fig. 10 The load averages of multiple physical hosts in the DHCI architecture.

70% as a threshold. Similarly, when the load average exceeds this threshold, the workload of the physical host is regarded as severe. This striking trend in Fig. 7 shows that when a physical host?s workload rises and exceeds the preset threshold, other physical hosts in the same virtual cluster will take part in workload balancing and thus improve the efficiency.

Figures 9 and 10 give the performance parameters: CPU utility rate (CPU for short) as well as the load average (LA for short) of 4 physical hosts during a period of 14 minutes for two different architectures. We take their averages for each physical host respectively, as illustrated in Figs. 11 and 12. Then, we calculate their variances to reflect the fluctuations in workload of multiple physical hosts. The variance of CPU utility rate in the traditional Hadoop cluster and the DHCI architecture are 0.196 and 0.1, respectively. The efficiency of the cluster load balance in the DHCI architecture is superior to that in the traditional Hadoop cluster. From the perspective of load average, the similar conclusion can be drawn.

4.4 Test on data locality optimization  To verify the effectiveness of the data locality optimization strategy, we also use the benchmark  Ph1 Ph2 Ph3 Ph4      A ve  ra g  e C  P U  u ti  li ty  r at  e (%  )  Traditional Hadoop cluster  DHCI architecture  Fig. 11 Average CPU utility rate of each physical host in the two architectures.

1.00  0.75  0.50  0.25   A ve  ra g  e lo  ad a  ve ra  ge  Ph1 Ph2 Ph3 Ph4  Traditional Hadoop cluster  DHCI architecture  Fig. 12 Average load average of each physical host in the two architectures.

test cases, namely WordCount, TeraSort, and Sort with 2 GB data volume. The data in Fig. 13 shows the difference in testing data from the DHCI architecture with and without data locality optimization, respectively. We can conclude that the time taken to execute these tasks with data locality optimization is less than that without data locality optimization while under the same data volume condition.

5 Conclusion  In this study, we designed a novel dynamic Hadoop cluster IaaS architecture by introducing the following four modules: monitoring, scheduling, VM management, and VM migration modules. In particular, we proposed resource scheduling and data locality solutions. We assessed the efficiency of our solutions on the aforementioned virtual Hadoop cluster. Convincing experimental results show that our solutions can effectively balance the load and improve system performance.

