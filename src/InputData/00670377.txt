Binary Partition Based Algorithms for Mining Association Rules

Abstract Mining association rules is an important data mining problem. A binary partition based fast algorithm BPA for mining association rules in large databases is presented in this paper. Basically, the framework of BPA is similar to that of algorithm Apriori. In the first pass, all the frequent 1-itemsets can be divided into two disjoint parts.

Accordingly in each subsequent pass k, we partition the set of all the frequent k-itemsets into three subsets. Any two different partitions are disjoint. If necessary, this partition procedure can be a recursive one. Therefore we get a binary partition tree in the first pass and a corresponding ternary partition tree in each subsequent pass k. Due to such a partition, BPA can be very easily parallelized assuming a shared-memory architecture.

1. Introduction  Data mining, also known as knowledge discovery in databases, has been recognized as a new area for database research. This area can be loosely defined as the efficient discovery of previously unknown patterns or rules in large databases. Database mining is motivated by the decision support problem faced by most large retail organizations.

Progress in bar-code technology has made it possible for large supermarkets to collect and store massive amounts of sales data, referred to as the basket data. A record in such data typically consists of the customer-id, the transaction date and the items bought in the transaction.

Analysis of the big amount of past transaction data can provide very valuable information on customer buying behavior, and thus we can improve the quality of business decisions. [1, 2]  The problem of mining association rules over basket data was introduced in [1]. An example of such a rule might be that 90% of customers that purchase bread and butter also purchase milk. The intuitive meaning of such a rule is when customers purchase some items how they will tend to purchase some other items too. Finding all such rules is valuable for cross-marketing and attached mailing applications. Other applications include catalog design, add-on sales, store layout, and customer segmentation based on buying patterns. The databases involved in these applications are very large. Therefore, it is imperative to design efficient algorithms to mine association rules. [1, 2, 3, 4, 8, 9, 10, 11].

In this paper, an efficient algorithm BPA (Binary Partition Based Algorithm) is  presented. The rest of the paper is organized as follows. A formal description of mining association rules is first given in section 2. Then we discuss the related works in section 3.  In section 4, we describe the BPA algorithm in detail, and discuss scale-up problem and parallelization at the same time. The relative performance study is discussed in section 5. Finally we conclude with a summary in section 6.

2. Mining of Association Rules  The following is a formal statement of the problem of mining association rules: [2]  Let { }miiiI L21,=  be a set of literals ,  called items.

Let D be a set of transactions, where each transaction T is a set of items such that IT ? . Associated with each transaction is a unique identifier, called its TID . We say that a transaction T contains X , a set of some items in I , if TX ? . An association rule is an implication of    the form YX ? , where IX ? , IY ? , and =YX I ?. The rule YX ?  holds in the transaction  set D with confidence c if %c of transactions in D  that contain X  also contain Y . The rule YX ?  has support s in the transaction set D if %s  of transactions in D  contain YX U .

Given a set of transactions D , the problem of mining association rules is to generate all association rules that have support and confidence greater than the user- specified minimum support (called minsup) and minimum confidence (called minconf ) respectively.

The problem of discovering all association rules can be decomposed into two subproblems:  1.   Find all sets of items (itemsets) whose support is greater than the user-specified  minsup. Itemsets with minimum support are called frequent itemsets.

2.   Use the frequent itemsets to generate the desired rules. The general idea is that if, say, ABCD and AB  are frequent itemsets, then we can determine  if the CDAB ? holds by computing the ratio (AB)ABCDconf support/)(support= . If  minconfconf ? ,  then the rule holds.

Since the second subproblem is straightforward, much  of the current research has been focussed on the first subproblem [1, 2, 3, 4].

3. Related Works  Among the many algorithms that have been proposed for discovering frequent itemsets, the Apriori algorithm presented by R.Agrawal and R.Srikant is the most successful and influential [4, 7, 11]. To discover all frequent itemsets, Apriori makes multiple passes over the data. The first pass of the algorithm simply counts item occurrences to determine the frequent 1-itemsets. A subsequent pass, say pass k, consists of two phases. First,  the frequent itemsets 1?kL  found in the (k-1)th pass are  used to generate the candidate itemsets kC  , using the apriori-gen function. Next, the database is scanned and  the support of candidates in kC  is counted. At the end of the pass, Apriori determines which of the candidate itemsets are actually frequent. This process continues until no new frequent itemsets are found. The key of efficiency is based on not generating and evaluating those candidate itemsets that can not be frequent. To achieve this goal, all currently known algorithms use a same basic intuition that any subset of a frequent itemset must be frequent [2].

To deal with a kind of incremental updating problem of association rules, We have proposed to partition the  frequent 1-itemsets 1L  using a binary partition tree in paper [7]. In fact, when users interactively mine association rules, they may have to continually tune two thresholds, minimum support and minimum confidence, which describe the users? special interestingness.  Assume the transaction database does not change, we mainly consider the case that the new minimum support is less than the old one. Our partition idea is based on the following observation:  In the first pass, all the frequent 1-itemsets can be divided into two disjoint parts, all the new frequent 1-  itemsets 1  L  and all the old frequent 1-itemsets 2  L . And since any subset of a frequent itemset must be frequent too, then for every single item i  in a frequent k-itemset c , its corresponding frequent 1-itemsets {}i  is either drawn from 1  L or 2  L . Accordingly in each subsequent  pass k, we can partition all the frequent k-itemsets into three disjoint classes:  1) frequent  k-itemset { }kiiic K,, 21= , )1( kjj ??? , { } 11Li j ? ; 2) frequent k-itemset { }kiiic ,,, 21 K= , )1( kjj ??? , { } 21Li j ? ; 3) frequent k-itemset { }kiiic ,,, 21 K= , there must be two non-empty subsets 1c  and 2c  that ccc =21 U ,  =21 cc I ?, and 11 Lc ? ,  12 Lc ? .

The above partition procedure is at single level. In the final part of that paper, we have shown we can make a generalization by doing further partition at multiple levels. In this paper, Our goal is to use the same basic intuition to solve the mining of association rules itself and discuss the implementation problems of  the  generalized partition procedure in detail.

4. Algorithm BPA  Given a transaction database D , the support of an itemset can be taken as the number of transactions that  contain the itemset. Suppose the minsup is s , kL  is the  corresponding set of frequent k-itemsets, and kC  is the corresponding set of candidate k-itemsets. Associated with each itemset is a count field to store the support of this itemset.

4.1. Algorithm BPA    Basically, the framework of BPA is similar to that of Apriori, it needs to make multiple passes over the  database  too. In the first pass,  BPA  generates 1L . Then we use a binary tree (called a binary partition tree) to  partition 1L , and accordingly, in each subsequent pass k , we employ a ternary tree (called a ternary partition tree)  to partition kL .

Now we describe our partition procedure in detail. Let  maxl be the maximal level number which we?d like to  choose. We use the notation nlkL ? (l=1, 2, ?, maxl) to  denote a subset of frequent k-itemsets in the lth level, which is labeled by n. n is a sequence of integers in the  range of 1 to 3 with a length of  l. Let 1L  be the root of the binary partition tree and at level 0; its two disjoint  sons 111 ?L  and 211  ?L at level 1, etc. A internal vertex nlL ?1 at the lth level corresponds to the union of its two disjoint  sons 1)1(1 nlL ?+ and 2)1(1  nlL ?+ at the ( 1+l )th level . Likewise, In each subsequent pass k, let kL be the root of the corresponding ternary partition tree and at level 0; its  three mutually disjoint sons 11?kL , 21?  kL  and 31?  kL at level 1,  etc. A internal vertex nlkL ?  at the lth level corresponds to  the union of its three mutually disjoint sons 1)1( nlkL ?+  , 2)1( nlkL ?+ and 3)1( nlkL  ?+  at the ( 1+l )th level. However, the third son such as 31?kL  can not be further partitioned, it can only be a leaf vertex. Figure 1 illustrates an example of binary partition tree and ternary partition tree with maxl=2.

Level 0                 1L                           kL  Level 1         111 ?L       211  ?L           11?kL 31?  kL 21?  kL  Level 2 1121 ?L     1221  ?L       112?kL 132?  kL 122?  kL  Figure 1   Example of  binary partition tree and ternary partition tree  At the same level l, 11 plL ? , 21  plL ? , 1plkL ? , 2plkL  ? and 3plkL ?  (p is the common prefix of length ( 1?l )) must satisfy the following conditions which are similar to those of the three disjoint classes mentioned in section 3:  1) 1plkLc ??? , let { }kiiic ,,, 21 K= , )1( kjj ??? ,  { } 11 plj Li ?? ; 2) 2plkLc  ??? , let { }kiiic ,,, 21 K= , )1( kjj ??? , { } 21 plj Li ?? ; 3) 3plkLc  ??? , let { }kiiic ,,, 21 K= , there must be two non-empty subsets 1c and 2c  that ccc =21 U ,  =21 cc I ?, and  plLc ?? , 212  plLc ?? .

Clearly, based on the binary partition tree, the  generation of 1L  can be viewed as a procedure of 2-way merge. And in each subsequent pass k, it is a 3-way merge.

Suppose in each subsequent pass k, nlkC ?  is the  corresponding candidate k-itemsets of nlkL ? , and  1pl kC ? , 2plkC  ? and 3plkC ?  correspond to 1plkL  ? , 2plkL ? and  3pl kL ? respectively. We use function bpa-gen( nlkL  ? ) to  generate nlkC ? .The parameter nlkL  ?  corresponds to a vertex  in the ternary partition tree. If  nlkL ?  is a leaf  vertex, we  simply use the apriori-gen function to generate 1pl  kC ? and 2plkC  ? , and to generate 3plkC ? , we use iua-  gen( 1pljL ? ) presented in [7]. In fact, we can get every  candidate k-itemset in 3plkC ?  by simply concatenating a  frequent j-itemset )11( ??? kj  in 11 pl  kL ? ?  and a  frequent (k-j)-itemset in 21 pl  kL ? ? , assuming neither this pair  of itmsets ( 1pljL ? , 2pl jkL  ? ? ) is empty. Function iua-  gen( 1pljL ? )  takes two steps. First, in the concatenate step,  we concatenate 1pljL ? and 2pl jkL  ? ? :  insert into 3plkC ?  select  item.p ,  item.p ,?, j  p item. ,  item.q ,  item.q ,?,  jk q ?item.

from 1pljL ? p , 2pl jkL  ? ? q ;  Next in the prune step, delete all itemsets 3pl  kCc ?? such that some (k-1)-subset of c  is not in   pl kL ? ? . In the case of  nl kL ?  is a internal vertex, the  generations of 1plkC ? and 2plkC  ? are implemented  by the 3-  way merges of  the (l+1)th level, and we still use function    iua-gen to generate 3plkC ? . Using a recursive style, the  bpa-gen( nlkL ? ) function is described as follows:  bpa-gen( nlkL ? )  {  if  ( nlkL ? is a leaf vertex) then begin  1plkC ? =apriori-gen( 11  pl kL ? ? );  2plkC ? =apriori-gen( 21  pl kL ? ? );  end  else begin   /* nlkL ? is a internal vertex */  1plkC ? =bpa-gen( 1)1( nlkL  ?+ );  2plkC ? =bpa-gen( 2)1( nlkL  ?+ );  end 3pl  kC ? =?;  for  );1;1( ++??= jkjj do begin 3plkC  ? = 3plkC ? U iua-gen( 1pljL  ? );  end nl  kC ? = 1plkC  ? U  2pl kC ? U  3pl kC ? ;  return ( nlkC ? );  }  After generating kC , the database is scanned and kL is finally generated. This process continues until no new frequent itemsets are found. Figure 2 gives the primary framework of algorithm BPA:  1)  { }itemsets-1frequent 1 =L ; 2)  begin do );;2(for 1 ++??= ? kLk k 3)      kC =bpa-gen( kL );     /* kL  can be viewed as  00? kL ,  here l=0, n=0 */ 4)      forall transactions  do begint D? 5)           );,(subset tCC kt = 6)         do  candidates forall tCc ? 7)                c count. ;+ + 8)   end 9)   { }scountcCcL kk ??= . ; 10)   end 11)  Answer= ;kkLU  Figure 2   Algorithm BPA  4.2. Scale-up and Parallelization  In the candidate generation phase of pass k, we need  storage for 1?kL and kC . With the increase of the number of transactions, the number of frequent itemsets may greatly increase too. Then we have to do swaps between memory and disks. However, by means of the binary partition and the ternary partitions, we are always able to  keep both a subset of kC  and the corresponding subset of  1?kL  entirely available in memory. This aim can be taken  as a basic partition rule. Whenever a subset of 1?kL  does not entirely fit in memory, we do a partition of  that subset, therefore the corresponding leaf vertex in the binary partition tree or the ternary partition tree becomes a internal vertex. Apparently, it is a dynamic partition procedure. Whenever a new partition is done, the binary partition tree in the first pass and the ternary tree in each subsequent pass k have to do a vertex split accordingly.

For example, as shown in the figure 1, we can first divide  1L  into two disjoint subsets  ?L and 211  ?L . If 111 ?L  and  ?L can entirely fit in memory respectively, we then  suspend the partition. (We can also have another choice,  if 111 ?C  can not entirely fit in memory, we can choose to  further partition 111 ?L .) Suppose in each subsequent pass k,  whenever a subset of 1?kL , say  ? ?kL  can not entirely fit  in memory, we first partition 111 ?L into 1121  ?L and 1221 ?L ; then  in each subsequent pass k , we partition 11?kL into 112?  kL  and 122?kL  and 132?  kL .

Based on the binary partition and ternary partitions,  BPA can be very easily parallelized assuming a shared- memory architecture. That is algorithm PBPA. Suppose we have n processors which have a shard-memory. Based on multiple levels partition, we have a straightforward choice. For example, we can make the ternary partition tree have at least n leaf vertices. We then distribute the load equably among the n processors. Therefore we can take full advantage of the available processors.

5. Relative Performance  Basically, BPA is an extension of Apriori. Now we compare the relative performance of BPA with that of Apriori. The framework of BPA is similar to that of Apriori,  and there is not much extra overhead for the maintenance of  the binary partition tree and ternary partition trees .

BPA  does better than Apriori in the generating of the candidates k-itemsets corresponding to the third child    (such as 31?kL ) of a internal vertex. Apriori expands two frequent (k-1)-itemsets to generate candidate k-itemsets using the apriori-gen function, it needs (k-1) join conditions to implement complex join. However, BPA only needs to concatenate two sub-itemsets using the iua- gen function. Furthermore, in the prune step. Suppose  nl kL ?  is a leaf vertex, When generating nlkC  ? , BPA can  implement efficient prune by simply checking the  corresponding nlkL ? ?1 . However, Apriori has to check the  whole set 1?kL . Therefore, suppose a ternary partition tree has m leaf vertices, then Apriori has to check  the  scope to decide a prune  by (m-1) times 1?kL  more than  BPA. This can be more significant while single nlkL ? ?1 can  entirely fit in memory respectively but the whole of 1?kL can not.  In fact, at this time Apriori can no longer prune  those candidates whose subsets are not in 1?kL  [3].

Therefore, BPA can efficiently generate less candidates than Apriori. Based on the binary partition, BPA can always gain this advantage.

It is obvious that PBPA not only has all the above virtues of BPA, but also can do efficient parallel computation.

To get detailed relative performance data, related experiments are still under construction.

