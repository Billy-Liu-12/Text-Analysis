Abnormality Analysis of Streamed Log Data   Ashot N. Harutyunyan, Arnak V. Poghosyan, Naira M. Grigoryan, and Mazda A. Marvasti

Abstract?We examine the determination of abnormality of streamed data using the statistical structure of the meta-data associated with it. The vital need for such a subject within a he- terogeneous log based environment in real-time comes from the fact that most cloud based applications will use text-based logging as a means of reporting application behavior. The sheer volume of such logs makes retrospective analysis in- feasible due to large processing and storage requirements. Our approach is based on conversion of the original data stream in- to meta-data (graph) and revealing the dominating (normal) statistical patterns within it. Real-time analysis of the stream compared with the meta-data model determines the degree of anomaly of the current data. The resulting graph also reveals the fundamental structure (?behavioral footprint?) of the data beyond the sources (physical or virtual devices) and processes.

Index Terms?Cloud and virtualization management, log analysis, big data, events stream, normalcy structure, anomaly and change detection.



I. INTRODUCTION We treat the problem of statistical structure retrieval  from data streams within the ?big data? model [1]. The relevance of this kind of analysis is dictated by rapidly growing needs to make run-time decisions on abnormality of cloud based environments from which log data can be streamed. Examples include application logs, user behavior data, social network data, etc.

A fundamental approach to data-agnostic management of data centers applies the dynamic thresholding of monito- ring data collected from IT resources instead of out-of-date static or hard thresholding technique. The dynamic thre- sholds (DT) of a time series metric are the appropriate upper and lower bounds computed using variability, change point, and cyclicality of the data (see [2]-[4]).

In this paper we extend the concept of dynamic thresholding of time series data to any kind of data that is a stream of records and events. In other words, we introduce the notion of normalcy of those streams as an extracted statistical structure and develop a mechanism for their abnormality detection in run-time mode.

In our earlier paper [4] we employed directed graphs to extract patterns from unstructured data. Here we use the same graph model as a tool for capturing the meta-data patterns of streaming log files to determine system anomaly states, based on its historic ?normalcy structure?, accounting also for change-points occurring in the stream. Prior related work includes the papers by Lin and Siewiorek [5] and Rouillard [6], among others. In particular, [5] is devoted specifically to extraction of intermittent errors in error logs  and [6] studies modeling of normal/abnormal events in logs employing rule sets. Another related problem is the work- flow model discovery and its transactional improvement from event based logs [7]. In this regard, our analysis is generic in terms of relying on non-contextual and non-rule based pattern extraction in data streams compared to those works (see also references therein). Moreover, it builds on a global correlation structure of the stream to quantify degrees of abnormalities occurring run-time, as well as to localize ?responsible anomaly sources? with their contributions to the overall abnormality. No prior research to the best of our knowledge directly targets this setting.

Our approach is based on conversion of the original stream into a meta-data and revealing the dominating (nor- mal) statistical patterns within it. The meta-data is formed via a graph representing different ?event types? (nodes) which are detected in the stream along with ?event sources? they are associated within the stream. Two nodes in this graph are adjacent if the corresponding event types are in proximity by a time frame and an ?event source?. The be- haviors of those sources in terms of generating different event types create probabilistic correlations between graph nodes computed with application of that proximity measure.

The characterized probabilistic graph represents the normal or dominating common behavioral structure that the stream historically follows independently of the event sources.

Having this historical normalcy, we are then able to estimate the upcoming stream portions in terms of their deviation from the extracted structure. This is performed through matching the event types in the new data entries with histo- rical graph and checking whether conditionally the most probable event types are realized in the observation window.

From this we can quantify the deviation of the current data segment from the most typical (historical) patterns. This quantity actually measures the abnormality degree of the stream.

One of the main problems in processing of data belonging to ?big data? field is the limited availability of data for retrospective analysis. By its very nature, limits on storage and I/O can severely restrict algorithms that require the complete data set to be available for analysis. In terms of this restriction, our algorithm is highly scalable. Converting the data stream into a graph structure representing the meta- data, we retain the useful content of the data and build the wanted statistical structure without any need for retro- spective analysis.

We apply our method to a set of vCenter (VMware?s virtualization management software) logs. Using the     information on event types and on fleeting VM?s (or hosts) as sources of those event types, we are able to determine the statistical normalcy structure of the stream by the above mentioned graph. Comparison of real-time data to this graph allows us to then determine abnormality patterns. Through that comparison we estimate the degree of abnormality that can be used by an alerting engine within an infrastructure management system. Moreover, the algorithm can be applied to normalcy analysis of virtualized environments at different hierarchical levels (VM, host, cluster, etc.).

The normalcy structure represents the image of pure event type correlations independently of the event sources in the heterogeneous system. In other words, it is the funda- mental structure (?behavioral footprint?) of the data beyond underlying sources (physical or virtual) and processes. Thus another benefit of this method is that comparison of the cur- rent data portions with the meta-data graph may shed light on sources of abnormal situations.



II. STREAM-TO-META-DATA CONVERSION We interpret the streamed log data as a flow of text  consisting of events with associated event types and event sources (which can be detected via a log parsing procedure).

The procedure outlined for the automatic detection of event attributes in [4] is one example of an event detection me- chanism. This is the basic assumption behind our algorithm on processing of streamed log data in terms of extracting its fundamental statistical characteristics. Say the stream con- tains I different types iT  of events and K  different sources  kS  of events. We aim at investigating how those types are correlated along with emerging stream inputs independently of the sources they are associated with.

Two types iT  and jT are considered to be related with each other from short term perspectives, if they appear in the stream attached to the same source kS  and within a time window t? . In other words, to determine the correlations between event types we apply a proximity pair criterion  ),( tSk ? . Hence, the probability of appearance of iT  under an observed type jT  (both associated with kS ) can be estimated by the following frequency  ? =  ?=? K  k kji  j ji tSTTNTN  tTTP  ),|,( )(  1),|(  as ratio of joint occurrences ),|,( tSTTN kji ?  of the type  pair ),( ji TT  in the stream over the number )( jTN  of ob-  served type jT  independently of kS , Kk ,1= . Therefore, the conditional probability between two event types pro- voked by all possible event sources can be determined with hereinafter usage of concise notations )|( jiP  or jiP| .

The prior probabilities of event types can be also computed by the frequency LTNTP ii /)()( =  where Ii ,1= and L  stands for the number of events read from the stream.

In view of the above definitions, the conditional probability  )|( ji TTP  becomes a tool for measuring the ?common behavior? of event types abstracted from underlying event sources. Having computed the prior and conditional probabilities between event types we can build the structure in Fig. 1 showing a weighted graph of event types with those conditionals on edges. Note that the above mentioned frequencies can be updated cumulatively and the corres- ponding meta-data in form of a directed graph can be updated with the expansion of the stream. Interestingly, this stream-to-meta-data conversion can be implemented by the map/reduce programming model [8].



III. THE NORMALCY STRUCTURE The constructed meta-data graph of Fig. 1 can be viewed  as the dynamically and historically built structure of pair- wise correlations of event types. This is the graph used for processing historical normalcy and run-time decision making on abnormality behavior of the stream.

Flowchart 1 describes the main parts of our analysis and their interrelation. The overall algorithm behind this study will be described in detail below.

Flowchart 1. Main flows of the algorithm.

Training of the model. We define the normalcy  structure of the stream by its meta-data and in terms of the dominating statistical relations in the graph of Fig. 1. Ana- logous to outlier removal in time series data, small condi- tional probabilities are considered to be outliers and thus can be removed from analysis. Eliminating small conditional probabilities from the graph edges (a sensitivity parameter is applied) we reduce it to its dominating correlations sub- graph(s). We call this graph Dynamic Normalcy Graph (DNG). It can be easily seen that the DNG is the stream?s    historical footprint of common probabilistic behavior of event types that result from all possible event sources.

Scoring/Ranking. The event types in the current obser- vation window are mapped to the DNG to compare the mis- match between the run-time scenarios to those in historical mode. The degree of mismatch represents the degree of ab- normality of the real-time data.

Fig. 1. Meta-data graph of event types, .7=I   Let )(21 ,...,, wjTTT ? be the event types registered in the  current observation window w? . And let the subset )(,...,, 21 wjjj kTTT ?   be those event types that pair-wise are in  proximity for event source ,kS  )(,1 wkk ?= . In other words, the following holds:  ? ?  =  ?=? )(   )()( wk  k k wjwj .

Those event types can be matched on the DNG and the edges having matched nodes can be highlighted. We can then compute the probabilistic mismatch (i.e. the abnormality degree) between this matched sub-graph and the DNG (relying on the graph isomorphism concept) by the following formula  ? ?  =  ?=? )(   ),()( wk  k k wSMwM  where ),( wSM k ?  stands for a mismatch assigned to source  kS  and calculated by  %100  )|(  )|(  ),( )(   )(  ,1  )(   )(  ,1  ? ?  ? ? ?  = ?=  ?  = ?==? wj  j  jm  jii  wj  j  jm  jii k  jiP  jiP  wSM  k    and where we assume that nodes adjacent (in DNG) to j nodes are enumerated from 1  to )( jm . We also enumerate the absent nodes in w?  from 1  to )( jm . Note that the  summation in ? ?=  )(  ,1  )|( jm  jii  jiP  as well as in ? ?=  )(  ,1  )|( jm  jii  jiP  is  performed over the conditional probabilities on the directed edges arising from the node j  in DNG. )( wM ?  lies in the interval %].100 ,0[  Measuring the mismatch between the run-time flow of events and the historical normalcy allows us to control unacceptable deviations and generate alarms. For that purpose, first we keep track of historic anomalies and estimate their usual (normal) level. That can be performed by simple measures such as whiskers method (that recognizes the concentration of data points from out-of- range values) and high quantile cut of data, or the sophisticated DT computation technology.

In particular, let )}(),...,(),({ 21 wMwMwMM h ???=  be the series of subsequent mismatches calculated along the historical application of the obtained DNG to the stream with a moving w?  window. And let ),( MqQ  be the ?q th quantile of M . Then we define the historical abnormality  )(0 wD ?  according to the method of whiskers as  )(5.1),75.0()(0 MIQRMQwD +=? , where )(MIQR  is the interquartile range (the difference between 0.75 and 0.25 quantiles) of M . In an alternative way )(0 wD ?  can be  defined also as 0.9 or higher quantile of M  regulated by a sensitivity parameter ]1,0[?s , where 1=s  corresponds to  ),9.0( MQ  and the lowest 0=s  to the maximum of M .

The deduced level )(0 wD ?  is used in run-time estimate of abnormality. So only the abnormalities with their mismatch  )( wM ?  above )(0 wD ?  are reported. This means that the real and relative abnormality degree )( wD ?  above )(0 wD ? in run-time mode can be obtained with the following difference:  ).()()( 0 wDwMwD ???=? Here the approach can be extended to apply a dynamic  thresholding technique (e.g. [3]) for softer abnormality control (see also the study [9] and references therein). In other words, instead of )(0 wD ?  we can compute its dynamic (time-dependent) version ),(0 twD ? .



IV. INDICATING ABNORMALITY SOURCES During the abnormality detection by the above  mentioned mismatch calculation, the event types which contribute to the mismatch can be identified. Then those event types can be checked in terms of the event sources associated with them. An ordered list of recommendations    indicating the most likely sources of abnormality can be generated. To generate those recommendations and indicating the highly ?responsible? event sources in an unacceptable abnormality, let )(21 ,...,, wkSSS ?  be the event sources that proximate the event types )(,...,, 21 wjjj TTT ?  in  run-time mode. For each event source kS  we compute its mismatch ),( wSM k ?  and generate the following series:  ),(),...,,(),,( )(21 wSMwSMwSM wk ??? ? . Those sources are then prioritized with the corresponding mismatches and displayed for the final recommendation to the user as indica- tions of abnormality sources.



V. ON CHANGE POINT DETECTION Global changes in the stream which are able to skew our  abnormality analysis can also be detected with a procedure comparing two meta-data portions from the stream in terms of mismatch between their graph-wise representations.

For quantification of similarity between two historical streams we introduce the following measure:  ? ? ? ?  ?  ?  ? ? ? ?  ?  ?  ?  ?  ??  ? ?+? ?+?  =  ? ?   ,, 2|1|    )()(   ),(  EE  GPGP  VVEE VVEE  GGSim  jiji jiji  where ),( 111 EVG ? and ),( 222 EVG ? , composed from the set of vertices kV  and edges kE , 2,1=k . This measure ranges from ]1,0[  with maximum similarity .1  In the similarity formula above the first fraction is responsible for the geometric similarity of the graphs and the second for the probabilistic closeness. The notation  )(| kji GP  stands for the conditional probabilities on jointly  present edges for kG . We impose an additional constraint for the condition 021 =? EE  by setting 0),( 21 =GGSim .

To verify whether the stream has substantially changed during its development, we take advantage of its meta-data structure again, now for recent period of the original stream containing 2/L  events from total length L , assuming  2/L is large enough to provide sufficient statistics. This means that as soon as the sufficient statistics is available we initiate generating a parallel construction of a meta-data graph for  2/L ?recent size (tail) of the stream. Then we keep updating this graph along with the evolution of the data sliding the tail window with size 4/LC =? . The latter assumes that we refresh the graph nodes/connections obtained due to the scanning of the stream in the first 4/L   part of the tail with the new nodes/connections observed in the last 4/L  portion of the tail observed by the sliding window. At each stage of this moving structure update we have two meta-data graphs, basic one and the moving one, conventionally denoting them )(1 LG   and ( )2/2 LG , respectively. Estimate of their  similarity with ))2/(),(( 21 LGLGSim  tells us how close the adaptive data tail to the overall process is. As soon as we detect only a 50%  similarity (or another parameterized quantity), then we have to declare a change point and replace  the basic historic meta-data )(1 LG  with the moving one ( )2/2 LG  and proceed with the rest of algorithmic blocks as shown in  Flowchart 1. With these settings we don?t target a sensitive change point detection problem but only a global change identification in the stream occurred in its recent structure versus its overall structure. The tail size and C?  can be tuned for softer analysis of change.



VI. RESULTS FOR VIRTUAL CENTER EVENTS Here we discuss the application of the abnormality de-  tection algorithm to a parsed log data of vCenter consisting of 200,000 events (a time period spanning one month). In this case, the event sources are VM?s or hosts and the event types are the corresponding types from the log, such as VmEmigratingEvent or VmStoppingEvent with additional categories they are attributed to in the stream, for example, ?info? (i), ?error? (e), or ?warning? (w). So for our analysis the combinations such as ?VmEmigratingEvent+info? and ?VmEmigratingEvent+error? are interpreted as basic event types.

Fig. 2 shows the normalcy structure of the log processed on the above mentioned events data and Table 1 details its node description (A defines the attribute column). This DNG represents the pure event type correlations where the outlier relations are filtered out. Additionally, to compress the structure only strong correlations (higher 0.8) are illustrated. Similar graphs are obtained for a series of experiments that confirm that the vCenter has its inherent statistical and fundamental structure of event type behaviors independent of the applications that run on the VM?s.

Those experiments were performed for different portions of the vCenter log containing more than 1,000,000 event records as well as for the whole data set. In all cases it was possible to derive a DNG with high probabilistic connections between a subset of defined event types.

Several observations from the obtained DNG can be made:

I. DNG contains an unconnected fragment (nodes 32 and  33), i.e. a sub-graph, which means that the virtual center imprints isolatable event types. In case of the nodes 32 and 33, one can make a conclusion that most of the time (94%) ?VmBeingRelocatedEvent+info? results in ?VmRelocatedEvent+info? with 6% failure that would result in an abnormality situations.



II. There are event types with only outgoing connections (like node 29) and event types with only incoming con- nections (node 4). In other words, the composite event type ?VmRegisteredEvent+info? inevitably leads a collection of event types (23,51,49, etc.), meanwhile a series of event types (2,5,16, etc.) ultimately lead to ?VmResourceReallocatedEvent+info?.



III. An important class of correlations is related to determi- nistic connections. For example, ?VmInstanceUuid- ConflictEvent+error? (49) generates ?VmInstanceUuid- ChangedEvent+info? (51) without any alternative. The same happens with ?VmRenamedEvent+warning? (35) and ?VmReconfiguredEvent+info? (11), however these event types have no impact on other types and are of no influence to the rest of the system.

Fig. 2. Normalcy structure (DNG) of vCenter events.

The extracted DNG becomes the ?behavioral footprint? of the virtual center in terms of VMs behaviors. It means that the lifecycle of any VM should follow the structure that the DNG dictates. Any deviation introduced by VMs in the current stream becomes the abnormalities. It can be either an evolving critical abnormality or an out-of-normal state that  can also be inherent to the stream in terms of its historical behavior. That is why we evaluate typical out-of-normalcy (so called historical abnormality) of the stream in order to differentiate it from the abnormality that needs to become an alert.

Table 1. Nodes of Fig. 2.

ID Event Type A  0 vim.event.AlarmStatusChangedEvent i 1 vim.event.TaskEvent  i 2 vim.event.VmBeingHotMigratedEvent i 3 vim.event.VmEmigratingEvent i 4 vim.event.VmResourceReallocatedEvent i 5 vim.event.VmMessageEvent i 6 vim.event.DrsVmMigratedEvent i 9 vim.event.AlarmActionTriggeredEvent i  10 vim.event.AlarmSnmpCompletedEvent i 11 vim.event.VmReconfiguredEvent i 14 vim.event.VmAcquiredMksTicketEvent i 16 vim.event.VmStoppingEvent i 17 vim.event.VmPoweredOffEvent i 18 vim.event.VmStartingEvent i 20 vim.event.DrsVmPoweredOnEvent i 21 vim.event.VmPoweredOnEvent i 22 vim.event.VmDisconnectedEvent i 23 vim.event.VmConnectedEvent i 26 vim.event.VmResettingEvent i 29 vim.event.VmRegisteredEvent i 32 vim.event.VmBeingRelocatedEvent i 33 vim.event.VmRelocatedEvent i 35 vim.event.VmRenamedEvent w 41 vim.event.VmDiscoveredEvent i 49 vim.event.VmInstanceUuidConflictEvent e 50 vim.event.VmMacConflictEvent e 51 vim.event.VmInstanceUuidChangedEvent i 52 vim.event.VmMacChangedEvent w  For the example of Fig. 2 we compute the historical ab- normality estimate )(0 wD ?  and show it in the plot of Fig.

3. This figure depicts the mismatches )( wM ?  along the historical log for the extracted DNG with 30=?w  minute sliding by 5 minute intervals. Here the computed value for  )(0 wD ?  is %55.25  (for sensitivity 7.0=s ), therefore, abnormalities are indicated at run-time for values above this level.

Fig. 3. Historical abnormality estimate %55.25)(0 =?wD  4/28/2010 5/12/2010 5/24/2010            Date  M (3  m  in ut  e)    Fig. 4 shows abnormality jumps in run-time mode detected for the same log after its DNG extraction. For the two abnormality peaks in Fig. 4 we were then able to generate a list indicating the highly probable misbehaving VMs (Table 2). The first abnormality process occurred on 5/24/2010 at 04:17PM (time (T) point A in Fig. 4) and the second one on 5/24/2010 at 05:18PM (point B in Fig. 4). One VM was detected in each case that generated events, however, failed to generate the highly correlated events associated with them. Table 2 shows these highly culpable VMs (with mismatch scores (MS) of 40.9% and 100%, respectively) with columns of generated event (GE) ID and missing event (ME) ID.

Fig. 4. Run-time abnormality above )(0 wD ?  Table 2. Recommended VMs at abnormal state of vC log.

T VM Name GE ID ME ID MS  A communities-lt-db-1  0, 1, 10 - 40.9% 9  0  2, 3 4  B ora-dev2-ksdmmk-d1 23 0 100%  This essentially shows that breakage of expected correla- tions become abnormality events that can indicate when it?s appropriate to look into the log data.

To verify the robustness of our analysis against w? , we computed the function )(0 wD ?  for different window sizes, getting a near constant behavior.



VII. APPENDIX: ANALYSIS OF ECONOMIC DATA The methodology was applied to an economics data set  to show its agnostic nature. This data set consisted of key short-term economic indicators (EI) of OECD countries (available at http://stats.oecd.org/). OECD is an international economic organization of 34 countries founded in 1961 to stimulate economic progress and world trade. Also data of 8 non-member countries with large economies are included.

We aimed at extracting the normalcy structure of EIs for above mentioned countries within 1995-2007 and detecting the abnormality or change in that structure provoked by the recent economic recession. In this scenario, the event sources are the member countries and the event types are their main short-term (quarterly) EIs relative to the  preceding period. Table 3 shows the event types participating in the DNG of OECD+8 economic normalcy structure of Fig. 5. Overall, 20 key indicators (with theo- retically possible 60 event types ? positive, negative, and zero changes in indicators being different event types) are included in the analysis. However, only 17 positive change indicators constitute this DNG. The parameters used for this analysis were 1=?t  quarter, 1=?w  quarter.

Table 3. EIs in DNG relative to previous period.

ID Event Type/Key Economic Indicators 1 DomesticProducerPrices-Manufacturing_positive 2 GDP-PrivateFinalConsumptionExpenditure_positive 3 GrossDomesticProd_positive 4 Employment_positive 5 Financial-SharePrices_positive 6 GDP-Exports_positive 7 GDP-ImportsOfGoodsAndServices_positive 8 GDP-GovernmentConsumptionExpenditure_positive 9 GDP-GrossFixedCapitalFormation_positive  10 IntProduction_positive 11 IntTrade-ExportInGoods_positive 12 IntTrade-ImportInGoods_positive 13 LaborCompensation-HourlyEarnings_positive 14 Sales-RetailTrade_positive 15 ServiceExports_positive 16  ServiceImport_positive 17 UnitLaborCost-BusinessSector_positive  Fig. 6 shows the historic and runtime processing of the abnormality indicator (mismatch), illustrating the largest abnormality in Q4 of 2008. This, of course, is the ?Great Recession? event of 2008. Table 4 shows the abnormality sources and the missing events leading to the triggering of abnormality.

Fig. 5. DNG of economic data.

5/24/2010 A B 5/24/2010            Date  M (3  m  in ut  e)    The mismatch score column in this table indicates the fraction with which each source (or country) contributed to the abnormality indication. What is interesting to note from this list is that Greece is in first place in terms of having had experienced the greatest deviation from its normal economic activity (which since then has led to many rounds of austerity measures). Note that the United States is 23rd on this list indicating that the 2008 event was not as impactful to the US as it was on other countries.

Fig. 6. Historic (1995-2007) and runtime (2008-2011)  abnormalities of economic data.

Table 4. Abnormality sources for the peak at Q4-2008, %2.81)( =?wM , see Fig. 6.

Event Source GE ID ME ID MS Greece 7,4,14,17,9 13,2,3,6 4.65  N. Zealand 13,1,4,8,17,16 2,3 4.01 Iceland 13,1,10,11,8   12,2,3 3.74  Australia 13,12,4,14,11,8,17 2,3 3.71 Japan 7,8,17,16 13,2,3,6 3.36 Poland 4,14,2,8,17,9 13,7,3 3.31  S. Africa 4,8,9   7,2,3 3.20 Israel 4,8,9 7,2,3 3.20  Austria 4,14,8,17 13,2,3 3.19 Germany 13,14,8,17 2,3 2.88  UK 13,14,8,17 2,3 2.88 Finland 13,4,8,17 2,3 2.79  Netherlands 13,4,8,17 2,3 2.79 Sweden 13,4,8,17 2,3 2.78 Mexico 13,4,8,17 2,3 2.78  Czech Rep. 4,8,17 13,2,3   2.26 S. Korea 4,8,17 13,2,3   2.26 France 13,4,2,8,17 3 1.99  Belgium 13,8,17 2,3 1.96 Canada 13,8,17 2,3 1.96  Denmark 13,8,17 2,3 1.96 Italy 13,8,17 2,3 1.96 US 13,8,17 2,3 1.96

VIII. CONCLUSIONS We introduced a new decision making framework for  information retrieval from data streams under special constraints on complexity and scalability. By a convolu- tional extraction of a representative meta-data from the stream we create the stream?s ?behavioral footprint? and use  this in run-time evaluation of the underlying system on the current data flows. The meta-data can be stored in form of a graph based on an ?event source?/?event type? principle.

Application of our approach to IT data sets results in extension of the traditional normalcy analysis of time series data obtained from monitoring of IT infrastructures to include modern cloud infrastructures with high order of transiency. Furthermore, the method is applied to processing of log data generated by virtual centers. The new insight that we get here is that the dynamic environment produced by fleeting VMs can be projected via some probabilistic laws on behavioral event types. Matching this image (graph) with a run-time image one can come up with abnormality estimate of the transient infrastructure with additional capa- bility to localize the underlying sources of misbehaviors.

Due to universality of the proposed algorithm, the meta-data construction model and related analysis is applicable to data sets from various applications with similar anticipated results. This was demonstrated by applying the algorithm to economics data showing the 2008 ?Great Recession? event.

Determining ?event source? and ?event type? would allow a ?behavioral footprint? of the meta-data to be created which can then be used in real-time to determine brakeage of correlation as an abnormality indicator.

