Online Generation of Association Rules

Abstract  We have a large database consisting of sales transactions.

We investigate the problem of online mining of association rules in this large database. We show how to preprocess the data e?ectively in order to make it suitable for re- peated online queries. The preprocessing algorithm takes into account the storage space available. We store the pre- processed data in such a way that online processing may be done by applying a graph theoretic search algorithm whose complexity is proportional to the size of the output. This results in an online algorithm which is practically instanta- neous in terms of response time. The algorithm also sup- ports techniques for quickly discovering association rules from large itemsets. The algorithm is capable of ?nding rules with speci?c items in the antecedent or consequent.

These association rules are presented in a compact form, eliminating redundancy. We believe that the elimination of redundancy in online generation of association rules from large itemsets is interesting in its own right.

1 Introduction  The importance of discovering association rules as a tool for knowledge discovery in databases has recently been recog- nized. By using the data from bar code companies or sales data from catalog companies, it is possible to gain valuable information about customer buying behavior in the form of association rules. Such information can be used to make decisions such as shelving in a supermarket, designing well targeted marketing programs etc.

Let I = fi1; i2; : : : ; img be a set of literals called items.

The database consists of a set of sales transactions T . Each transaction T 2 T is a set of items, such that T ? I. In this paper, we consider the 0-1 case only; in other words a 0-1 variable indicates whether or not an item was bought.

A transaction T is said to contain the set of items X if and only if X ? T .

An association rule is a condition of the form X ) Y where X ? I and Y ? I are two sets of attributes. The in- tuitive implication of the association rule is that a presence of the set of items X in a transaction set also indicates a possibility of the presence of the itemset Y . Two notions for establishing the strength of a rule are those of minimum support and minimum con?dence, which were ?rst intro- duced in [2].

The support of a rule X ) Y is the fraction of transac- tions which contain both X and Y .

The con?dence of a rule X ) Y is the fraction of trans- actions containing X which also contain Y . Thus, if we say that a rule has 90% con?dence then it means that 90% of the tuples containing X also contain Y .

Starting with pioneering work in Agrawal et. al.[2], a host of work has been done in this area with a focus on ?nding association rules from very large sets of transaction data.

The primary idea proposed in [2] was an itemset approach in which ?rst all large itemsets are generated, and then these large itemsets are used in order to determine data dependencies. Subsequent work has primarily concentrated on this approach.

The itemset approach is as follows. Generate all com- binations of items that have fractional transaction support above a certain user-de?ned threshold called minsupport.

We call all such combinations large itemsets. Given an itemset S satisfying the support constraint, we can use it to generate rules of the type S?X ) X for each X ? S. Once these rules have been generated, only those rules above a certain user de?ned threshold called mincon?dence need be retained.

Faster algorithms for mining association rules were pro- posed in [3], while a hash-based algorithm was established in [17]. Generalized association rules were presented in [21].

Methods for mining quantitative association rules were es- tablished in [22]. Other related work may be found in [9, 11, 19]. An up-to-date survey on some of the work done in data mining may be found in [6].

In this paper we consider the problem of online mining of association rules. The idea in online mining is that an end user ought to be able to query the database for association rules at di?ering values of support and con?dence without excessive I/O or computation. In the itemset method, mul- tiple passes have to be made over the database, for each di?ering value of minsupport and mincon?dence, starting from scratch. Some sampling techniques exist which reduce the number of passes over the database to two [19, 23]. For very large databases, this may involve a considerable I/O and in some situations it may lead to unacceptable response times for online queries.

The problem of mining association rules is especially suit- able for an online approach. It is hard for a user to guess apriori how many rules might satisfy a given level of sup- port and con?dence. Typically one may be interested in only a few rules. This makes the problem all the more di?-    Rule Support Con?dence  X ) Y Z S(X [ Y [ Z) S(X [ Y [ Z)=S(X) XY ) Z S(X [ Y [ Z) S(X [ Y [ Z)=S(X [ Y ) XZ ) Y S(X [ Y [ Z) S(X [ Y [ Z)=S(X [ Z) X ) Y S(X [ Y ) S(X [ Y )=S(X) X ) Z S(X [ Z) S(X [ Z)=S(X)  Table 1: Redundancy in rule generation  cult, since a user may need to run the query multiple times in order to ?nd appropriate levels of minsupport and min- con?dence in order to mine the rules. In other words, the problem of mining association rules may require consider- able manual parameter tuning by repeated queries before useful business information can be gleaned from the trans- action database.

Another issue is that while mining association rules, a  large percentage of the rules may be redundant. It is useful to eliminate redundant rules simply from the point of view of compactness in representation to an online user. For example, if the rule X ) Y Z is true at a given value of minsupport and mincon?dence, then rules such as XY ) Z, XZ ) Y , X ) Y , and X ) Z are redundant. This can be easily seen from the Table 1 in which one can see that both the support and con?dence values of the rule X ) Y Z are less than the support and con?dence values for the rules X ) Y , X ) Z, XY ) Z, and XZ ) Y . In fact, in most cases, the number of redundant rules is signi?cantly larger than the number of essential rules, and having too many redundant rules defeats the primary purpose of data mining in the ?rst place. We note that this kind of redundancy arises when we consider rules which have more than one item in the consequent.

In recent years, an important application of database sys-  tems has been Online Analytical Processing (OLAP). The primary idea behind this approach has been the \preprocess once query many" paradigm. The idea is that it is time con- suming to compute results from raw transaction data each time a user makes a query. By preprocessing the data set just once, a user may be able to query the system e?ciently multiple times at the cost of a single phase of preprocessing.

Considerable work has been done in online analytical pro- cessing, as applied to the data cube [5, 7, 8, 10, 20]. This paper also discusses an approach for online mining by using one phase of preprocessing.

1.1 Contributions of this paper  In this paper, we present an intuitive framework for per- forming online mining of association rules. Past work has concentrated on a two phase approach:  (1) Large Itemset Generation: Controlling parameter minsupport.

(2) Rule Generation: Controlling parameter mincon?- dence.

The bottleneck in this procedure is the ?rst step, since most algorithms require multiple I/O passes in order to perform this step. Thus, the natural solution is to prestore as many itemsets as possible with the least support value possible given the memory available. This approach however, has  some obvious drawbacks. On the one hand, one might want to store as many of such itemsets as possible as constrained by the memory space or preprocessing time available, so that important information will not be lost. On the other hand, if too many itemsets are prestored, then the second phase of rule generation becomes the bottleneck. For ex- ample, while trying to mine rules containing speci?c sets of items, the number of relevant large itemsets may be a very small fraction of the total number of itemsets pre- stored. Yet, one may need to look at each and every pre- stored itemset in order to ?nd the relevant large itemsets.

Consequently, it becomes important to organize the item- sets along with support information in such a way that the online time required to mine the rules is small and is de- pendent on the number of large itemsets corresponding to a user query, rather than the number of itemsets prestored.

In this paper we shall discuss such a method. From now on, we shall refer to the prestored itemsets as primary itemsets.

The primary threshold is the minimum level of support for any prestored itemset. Thus the primary itemsets comprise all itemsets whose support is at least equal to the value of the primary threshold. At this stage we would also like to make a careful distinction between primary itemsets and large itemsets. A large itemset corresponds to an itemset for a user query, and is a subset of the primary itemsets.

More speci?cally, the contributions of this work are as fol- lows:  (1) We devise a framework for organizing the primary itemsets in such a way that online rules with very lim- ited I/O on the prestored data. The online time for mining the rules is independent of the size of the trans- action data as well as the number of itemsets prestored.

In fact, we shall see that the time required to process a query is completely dependent upon the size of the out- put. This feature is especially suitable for the online case.

(2) We give a technique which can quickly predict the size of the output at a given level of user speci?ed parame- ters. For a given level of user-speci?ed minsupport and mincon?dence, both the number of itemsets as well as the number of rules can be predicted. A reverse query such as predicting the level of minsupport for which a particular number of itemsets exist can also be per- formed.

(3) We discuss the issue of e?ciency in the generation of the rules. Since we include the possibility of generat- ing rules with more than one item in the consequent, it may often be cumbersome (at least from an online perspective) to look at each of the subsets of the large itemsets as a possibility for the antecedent. A large number of possibilities can be pruned by careful order of examination. It is also possible to e?ciently gener- ate only rules with exactly one item in the consequent.

Such rules are called single-consequent rules.

(4) We discuss the issue of generating rules with speci?c of items in them. The items may occur in the antecedent or consequent.

(5) We discuss the issue of redundancy in the rules gener- ated from large itemsets. We discuss the level to which essential rules may often get buried in hordes of redun- dant rules. Compactness of representation to an online    user is a very useful feature. This segment of the paper has both theoretical and practical signi?cance.

(6) We present an algorithm for ?nding the primary item- sets which automatically decides which itemsets to pre- store depending upon available memory capacity. For the sake of high level discussion, we shall ?x the maxi- mum number of itemsets rather than the memory space occupied by the itemsets. This is a slightly di?er- ent problem from that discussed in Agrawal et. al.

[2], where one needs to ?nd the itemsets with support above a particular value. The value of the primary threshold at which the best ?t to this maximum num- ber of itemsets may be found is not known in advance.

One may perform a binary search on the support value in order to ?nd the value of the primary threshold. We propose techniques for improving the e?ciency beyond simply performing a simple binary search.

We should note that it is not possible to perform online min- ing of association rules at support levels less than the pri- mary threshold. This is not necessarily a severe restriction since the primary itemsets are obtained within the prepro- cessing time constraints, which are signi?cantly more liberal than online time constraints. Thus, most useful itemsets are typically prestored.

1.2 Kinds of online queries  Assume that the kinds of online queries that such a system can support are as follows.

(1) Find all association rules above a certain level of min- support and mincon?dence.

(2) At a certain level of minsupport and mincon?dence, ?nd all association rules concerned with the set of items X.

(3) Find the number of association rules/itemsets in any of the cases (1), (2) above.

(4) At what level of minsupport do exactly k itemsets exist containing the set of items Z.

(5) For a particular level of mincon?dence c, at what level of minsupport do exactly k single-consequent rules ex- ist, which involve the set of items Z.

1.3 Overview  We introduce the concept of an adjacency lattice of item- sets. This adjacency lattice is crucial to performing e?ective online data mining. The adjacency lattice could be stored either in main memory or on secondary memory. We shall discuss more details about how this lattice is actually con- structed in a later section. The idea of the adjacency lattice is to prestore a number of large itemsets at a level of sup- port possible given the available memory. These itemsets are stored in a special format (called the adjacency lattice) which reduces the disk I/O required in order to perform the analysis. In fact, if enough main memory is available for the entire adjacency lattice, then no I/O may need to be performed at all.

We shall see that this structure is useful for both ?nding  the itemsets quickly and also using the itemsets in order to generate the rules. Redundancy in rules is eliminated, so  Itemset Support  A 1% B 2% C 2% D 1% AB 0.5% AC 0.7% BD 0.6% BC 0.4% ABC 0.3%  Table 2:  0.6%  ABC  NULL  1% A          B 2%           C 2%        D 1%  0.5% 0.7%  0.3%  AB          AC           BC           BD 0.4%  Figure 1: The adjacency lattice  that an online user may be presented with the most compact representation possible.

2 The adjacency lattice  Before we consider making a more detailed description, we shall discuss the concept of an adjacency lattice of item- sets. For future reference, we shall denote the adjacency lattice by L.

An itemset X is said to be adjacent to an itemset Y if one  of them can be obtained from the other by adding a single item. Speci?cally, an itemset X is said to be a parent of the itemset Y if Y can be obtained from X by adding a single item to the set X. Equivalently, Y may be considered to be a child ofX. Thus, an itemset may possibly have more than one parent and more than one child. In fact, the number of parents of an itemset X is exactly equal to the cardinality of the set X. This observation follows from the fact that for each element ir in an itemset X, X ? firg is a parent of X. It is easy to see that if a directed path exists from the vertex corresponding to Z to the vertex corresponding to X in the adjacency lattice, then X ? Z. In such a case, X is said to be an ancestor of Z, and Z is said to be a descendant of X.

The adjacency lattice L is constructed as follows: Con-  struct a graph with a vertex v(I) for each primary itemset

I. Each vertex I has a label corresponding to the value of its support. This label is denoted by S(I). For any pair of vertices corresponding to itemsets X and Y , a directed edge exists from v(X) to v(Y ) if and only if X is a parent    Algorithm FindItemsets(ItemSet: I, Support: s) begin LIST= v(I); OutputList= ?; while LIST 6= ? do  begin Select a vertex v(R) from LIST; f Assume that the children of a vertex are arranged in decreasing order of support g while the next child v(T ) of v(R)  satis?es S(T ) ? s do begin if v(T ) 62 OutputList do  begin LIST=LIST [v(T ); OutputList= OutputList[ (v(T ); S(T )); cardinality=cardinality+1; end  end; Delete the vertex v(R) from LIST end;  end;  Figure 2: The search algorithm for generating large itemsets  of Y . We denote the corresponding edge by E(X;Y ). The vertex v(X) is referred to as the tail of the edge E(X;Y ), while the vertex v(Y ) is referred to as the head.

Consider for example the group of primary itemsets illus-  trated in Table 2. The corresponding adjacency lattice is illustrated in Figure 1. Each vertex has a label correspond- ing to the value of its support. We make the following simple observations for the adjacency lattice L:  Remark 2.1 The adjacency lattice L is a directed acyclic graph.

Remark 2.2 For each vertex v(J) in L which is a descen- dent of v(I), we must have S(J) ? S(I).

The truth of Remark 2.2 follows from the fact that for each vertex v(J) which is a descendent of v(I), the corresponding itemsets must satisfy J ? I. Since the adjacency lattice is the primary structure which is used to represent the pre- processed data, it is useful to measure the memory which such a structure might require. We shall proceed to show that the space required to store the adjacency lattice is not the bottleneck, and is almost of the same order as the space required to hold the itemsets themselves.

Theorem 2.1 The number of edges in the adjacency lattice is equal to the sum of the number of items in the primary itemsets.

Proof: The number of edges may be obtained by summing the number of parents of each primary itemset. The number of parents of a primary itemset is equal to the number of items in it. The result follows.

3 Online generation of itemsets  In order to ?nd all itemsets which contain a set of items I and satisfy a level of minsupport s, we need to solve the following search problem in the adjacency lattice.

Problem 3.1 For a given itemset I (including fg), ?nd all itemsets J such that v(J) is reachable from v(I) by a directed path in the lattice L, and satis?es S(J) ? s.

Algorithm FindSupport(ItemSet: Z, Cardinality: k) begin LIST= v(Z); OutputList= ?; cardinality= 0 while (LIST 6= ?) and (cardinality? k) do  begin Select a vertex v(R) from LIST with largest value of S(R); OutputList= OutputList[ (v(R); S(R)); cardinality=cardinality+1; for each child v(T ) of v(R) do  begin if v(T ) 62 OutputList do  LIST=LIST [v(T ); end;  Delete the vertex v(R) from LIST end;  return (minfS(R) : (v(R); S(R)) 2 OutputListg;OutputList) end;  Figure 3: Finding the level of support for a ?xed number of itemsets  It is important to understand that the number of vertices reachable from a given vertex may be quite large, though the number of vertices which satisfy the level of minsupport smay be small. The idea is to use the lattice organization to restrict the number of vertices examined. Thus, when a user makes multiple queries to the database, this pre-processed data helps avoid the reading of the entire database from scratch. We shall now discuss the search algorithm which given the parameters I and s, ?nds all the itemsets con- taining I and having a support level of at least s. This algorithm is illustrated in Figure 2. The algorithm Find- Itemsets starts at a given itemset I and LIST= fv(I)g. The algorithm then adds all of its children v(J) with support S(J) ? s to LIST unless the vertex has been visited before.

The vertex v(I) is then deleted from LIST. This process is repeated until LIST is empty. Thus, all the vertices which are the unvisited children of a given vertex in LIST are re- cursively searched unless their support value is less than s. The itemsets for every vertex which is visited are also added to the OutputList. At the same time, a count of the cardinality of OutputList is maintained in order to handle the feature where a user may wish to ?nd the cardinality of the itemsets. At termination of the algorithm the Output- List contains all the itemsets J with support S(J) ? s and satisfying J ? I.

3.1 Finding the level of support for a ?xed  number of itemsets  A useful online feature is to ?nd the level of support at which exactly k itemsets (each of which contains the items Z = fi1 : : : irg) exist. This can be accomplished by making a few changes to the search algorithm of Figure 2. The resulting algorithm is illustrated in Figure 3. The primary idea is that while selecting a vertex v(R) on LIST which is to be examined in the current iteration, we always pick the vertex with the highest value of support. At that time, we add this vertex to OutputList. The algorithm terminates when k vertices have been found. It can be proved that at each stage of this algorithm, OutputList maintains r ? k itemsets containing Z with the highest support value.

Theorem 3.1 The algorithm FindSupport(Z, k) ?nds the k itemsets containing Z and having the highest value of sup- port. If less than k such itemsets are represented in the    FG=>DE  DEF=>G, DFG=>E  ,  EFG=>DDFG=>E  DF=>EG  DE=>FG, DEF=>G, EF=>DG,  DEFG  DEF         EFG    DFG        DEG  DF         DE           EF              FG       EG  E  Maximal ancestors of DEFG  Essential Rules Redundant Rules  E=>DFG  f  Ancestors of  DEFG which  have support  at most  S(DEFG)/c  EFG=>D, EG=>DF, DEG=>F  Figure 4: An illustration of the boundary itemset  adjacency lattice, then the algorithm ?nds all the itemsets containing Z.

Proof: The proof of this theorem is by induction. The induction hypothesis is that the r ? k items maintained in the OutputList are the r itemsets containing Z with the highest value of support. The induction hypothesis is triv- ially true when OutputList = ?. Each time an itemset is added to OutputList we pick the itemset on LIST with the highest support value. Any other itemset which we add to OutputList in the future, is either already on LIST, or is a descendent of some itemset currently in LIST. From Re- mark 2.2, the result immediately follows.

3.2 Finding the level of support for a ?xed  number of single-consequent rules  A single consequent rule is one in which the consequent con- tains only one item. It is also possible to use the algorithm described above to ?nd the level of support at which a par- ticular number (say k) of single-consequent rules exist for a prespeci?ed level of con?dence c. This can be achieved by making a minor modi?cation to the procedure FindSup- port of Figure 3. In this case, each time a vertex v(X) is selected from LIST, all the single-consequent rules which can be generated from v(X) at con?dence level c are added to OutputList. The count of the number of rules is main- tained. The ?rst time the count exceeds k, the procedure is terminated. The proof of correctness of this method is exactly analogous to the proof of Theorem 3.1.

4 Online generation of rules from item-  sets  In the previous section, we discussed how large itemsets may be generated from the adjacency lattice. In this sec-  tion, we discuss how rules may be generated from these itemsets. To generate the rules, we utilize the following observation: For each rule A ) B at con?dence level c, the label  (support) on the vertex v(A [ B) is at most 1=c times the label (support) on the vertex v(A). Thus, the con?dence of a rule may be obtained by comparing the labels on two vertices which satisfy an ancestor-descendant relationship in the adjacency lattice.

Conversely, let X = fX1; : : :Xkg be the itemsets gener-  ated in the ?rst phase of the online processing algorithm.

Let c be the level of mincon?dence at which it is desired to mine the association rules. For each Xi 2 X , rules may be generated by applying a reverse search algorithm start- ing from v(Xi) and ?nding all ancestors of v(Xi) which have support at most S(Xi)=c. For each such ancestor v(Y ) of v(Xi), it is possible to generate rules of the form Y ) Xi ? Y . Thus the problem of ?nding all rules gen- erated from a large itemset X is reduced to the following graph search problem in the adjacency lattice:  Problem 4.1 Find all ancestor vertices of v(X) which have support at least S(X)=c.

Unfortunately, many of the generated rules will turn out to be redundant. For example, if a rule X ) Y Z is included in the output, then the rule XY ) Z can be regarded as redundant.

De?nition 4.1 Let A) B and C ) D be two association rules. The rule C ) D is redundant with respect to the rule A) B if the support and con?dence of the former are both always at least as large as the support and con?dence of the latter, independent of the nature of the transaction data.

We shall ?rst classify the di?erent kinds of redundancy as follows:  Theorem 4.1 Simple Redundancy: Let A ) B and C ) D be two rules satisfying A [ B = C [D = X. The rule C ) D bears simple redundance with respect to the rule A ) B, if C ? A. In other words, if the rule A ) B is true at a certain level of support and con?dence, then so is C ) D, independent of the nature of the transaction data.

Proof: Omitted. See [1].

Thus, in simple redundancy, the support value for the two rules is the same, but the con?dence value for one is larger than the con?dence value for the other. The support values for the rules are the same since they are generated from the same itemset. As an example, the rule AB ) C bears simple redundance with respect to the rule A ) BC. We shall now discuss the case when one rule dominates the other based upon both support and con?dence.

Theorem 4.2 Strict Redundancy: We consider two rules generated from itemsets Xi and Xj respectively such that Xi ? Xj. Let A ) B and C ) D be rules satisfying A [ B = Xi, C [ D = Xj, and C ? A. Then the rule C ) D is redundant with respect to the rule A) B.

Proof: Omitted. See [1].

Thus, in strict redundancy, one rule dominates the other based upon both support as well as con?dence. As an exam- ple, the rule X ) Y bears strict redundancy with respect to the rule X ) Y Z. We shall introduce some additional de?nitions and notation here for the sake of future discus- sion.

Algorithm FindBoundary(ItemSet : X, Confidence : c) begin LIST= v(X); BoundaryList = ?; while LIST 6= ? do  begin Select a vertex v(R) from LIST; for each parent v(T ) of v(R) do  begin if v(T ) has not yet been visited and S(T ) ? S(X)=c do  LIST=LIST [v(T ); end;  Delete the vertex v(R) from LIST if v(R) is maximal add v(R) to BoundaryList end;  end;  Figure 5: Finding the boundary itemset  De?nition 4.2 A rule is de?ned to be essential at support level s and con?dence level c if it does not satisfy simple or strict redundancy with respect to any other rule which has support at least s and con?dence at least c.

As we shall see, the number of redundant rules may often be a signi?cant fraction of the total number of rules. We shall prove a result which quanti?es the number of redundant rules corresponding to a single rule X ) Y . For ease in notation, we shall denote the number of items in an itemset X by jXj.

Theorem 4.3 The number of rules bearing simple redun-  dancy with respect to X ) Y is 2jY j ? 2. The number of rules bearing either simple or strict redundancy with respect  to the rule X ) Y is 3jY j ? 2jY j ? 1.

Proof: Omitted. See [1].

As an example, consider the rule A) BC. There are 22?2 simple redundant rules, namely AC ) B, and AB ) C.

The strict redundant rules are A ) B, and A ) C. Thus the total number of redundant rules is 32 ? 22 ? 1 = 4.

Clearly, as the number of items in the consequent increases, the number of redundant rules explodes exponentially.

De?nition 4.3 A vertex v(Y ) is a maximal ancestor of v(X) at con?dence level c if and only if S(Y )=S(X) ? 1=c, and no strict ancestor v(Z) of v(Y ) satis?es S(Z)=S(X) ? 1=c.

Maximal ancestors are very relevant to the process of ?nd- ing rules which avoid simple redundancy.

Theorem 4.4 Let v(Y ) be a maximal ancestor of v(X) at a level of con?dence c. Then the rule Y ) X ? Y cannot exhibit simple redundancy with respect to any other rule at con?dence level c and any support level s ? S(X). Con- versely, if the rule Y ) Z does not exhibit simple redun- dancy with respect to any other rule at con?dence level c, then v(Y ) must be a maximal ancestor of v(Y [ Z).

Proof: Omitted. See [1].

Thus, ?nding maximal ancestors of large itemsets is nec- essary and su?cient to generate rules which avoid simple redundancy. As an illustration, consider the example in Figure 4. Only the relevant segment of the adjacency lat- tice is illustrated in the ?gure. Suppose that we wish to generate all the rules at a particular con?dence level c from an itemset DEFG. Also, assume that the itemsets which have support at most S(DEFG)=c are DEF , EFG, DFG,  Algorithm GenerateRules(Set of Itemsets: X , c) begin RuleSet = ? for each Xi 2 X do F(Xi; c) = FindBoundary(Xi; c) for each Xi 2 X do begin P(Xi; c) = F(Xi; c) for each child Xj 2 X of Xi do  P(Xi; c) = P(Xi; c)? F(Xj; c) For each itemset Y 2 P(Xi; c) do  RuleSet = RuleSet [ fY ) Xi ? Y g end;  return RuleSet end;  Figure 6: Generating the rules from the boundary itemsets  DEG, DF , DE, EF , EG, FG, and E. Thus, a total of 10 rules (corresponding to these 10 itemsets) can be generated, each of which satisfy the con?dence level c. However, as we see from Figure 4, only three of these rules are essential, while the rest bear simple redundancy to one or more of these rules. These three rules are generated by picking the three maximal ancestors of DEFG from these 10 itemsets and generating the corresponding rules. Thus the prob- lem of generating nonredundant rules with con?dence level c from a large itemset X reduces to the following graph search problem.

Problem 4.2 Find all maximal ancestors of v(X) with support at most S(X)=c.

We shall refer to all the maximal ancestors of a vertex as the boundary itemsets for the corresponding itemset at the given level of con?dence.

De?nition 4.4 The boundary for an itemset X at level of con?dence c is the set of all maximal ancestors of X at con?dence level c, and is denoted by F(X;c).

Finding the boundary for a given itemset X is simple enough by using a reverse search algorithm on the corre- sponding adjacency lattice starting at v(X), as illustrated in Figure 5. This algorithm does not incorporate the con- straints on having particular items in the antecedent or con- sequent. We shall discuss this issue in a later subsection.

In order to actually generate rules from the itemsets  X = fX1;X2; : : :Xkg, we apply the following method.

For each itemset Xi 2 X , we ?nd the boundary itemset F(Xi; c) and for each Y 2 F(Xi; c), we generate the rule Y ) Xi?Y . Unfortunately, this may result in strict redun- dancy while generating rules from two di?erent itemsets Xi and Xj which satisfy Xi ? Xj. First, we will discuss some simple results.

Theorem 4.5 Let X be an itemset, and let X1;X2; : : :Xk be the children of X. Let Y be any itemset in F(X;c) ? [ki=1F(Xi; c). Then, the rule Y ) X?Y cannot bear strict redundancy with respect to any other rule. Conversely, let Xi be a child of X such that Y lies in both F(X;c) and F(Xi; c). Then the rule Y ) X ? Y is strictly redundant with respect to one or more rules.

Proof: Omitted. See [1].

Thus, we have e?ectively shown in the above theorem that in order to avoid strict redundancy, it is necessary and su?- cient to prune the boundary of an itemset X so that it does    not share any itemsets with the boundary of any itemset Xk 2 X which is a child of X. In other words, for each child Xk 2 X of X, we remove from F(X;c), all member itemsets in F(Xk; c): Then these pruned boundaries may be used in order to generate the rules. The resulting algorithm is illustrated in Figure 6. This algorithm uses as input the itemsets X which are generated in the ?rst phase of the algorithm at the appropriate level of minsupport. The algo- rithm FindBoundary of Figure 5 may be used as a subrou- tine in order to generate all the boundary itemsets. These boundary itemsets are then pruned and the rules are gen- erated by using each of the itemsets corresponding to the boundary in the antecedent.

4.1 Rules with constraints in the antecedent  and consequent  It is easy enough to adapt the above rule generation method so that particular items occur in the antecedent and/or con- sequent. Consider for example the case when we are gener- ating rules from a large itemset X. Suppose that we desire the antecedent to contain the set of items P and the con- sequent to contain the set of items Q. (We assume that P [ Q ? X.) We shall refer to P as the antecedent inclu- sion set, and Q as the consequent inclusion set. In this case, we need to rede?ne the notion of maximality and bound- ary itemsets. A vertex v(Y ) is de?ned to be a maximal ancestor of v(X) at con?dence level c, antecedent inclu- sion set P , and consequent inclusion set Q if and only if P ? Y , Q ? X ? Y , S(Y )=S(X) ? 1=c, and no strict an- cestor of Y satis?es all of these constraints. Equivalently, the boundary set contains all the itemsets corresponding to maximal ancestors of X. It is easy to modify the algorithm discussed in Figure 5, so that it takes the antecedent and consequent constraints into account. The only di?erence is that we add an unvisited vertex v(T ) to LIST if and only if S(T ) ? S(X)=c, and T ? P . Also, a vertex v(R) is added to BoundaryList, only if it satis?es the modi?ed de?nition of maximality.

5 Generation of the adjacency lattice  In this section we discuss the construction of the adjacency lattice. The process of constructing the adjacency lattice requires us to ?rst ?nd the primary itemsets. There are two main constraints involved in choosing the number of itemsets to prestore:  (1) Memory Limits: In order to avoid I/O one may wish to store the primary itemsets and corresponding adja- cency lattice in main memory.1 Recall that Theorem 2.1 characterizes the size required by the adjacency lat- tice for this purpose. Assume that we desire to ?nd N itemsets. Note that because of ties in the support val- ues of the primary itemsets, support values may not exist for which there are exactly N itemsets. Thus, we assume that for some slack value Ns, we wish to  1Storing the adjacency lattice on disk is not such a bad option after all. The total I/O is still proportional to the size of the output, rather than the number of itemsets prestored. Recall that the graph search algorithms used in order to ?nd the large itemsets and associ- ation rules visit only a small fraction of the vertices in the adjacency lattice.

Function NaiveFindThreshold(NumberofItemsets: N, Slack: Ns) begin High = maxifSupport of item ig Low = 0; Generated = 0; while (Generated 62 (N ?Ns; N)) begin Mid = (High + Low)=2; Generated = DHP (Mid);  end; return(Mid);  end  Algorithm ConstructLattice(NumberofItemsets: N, Slack: Ns) begin p =NaiveFindThreshold(N, Ns) For each itemset X = fi1; : : : irg with S(X) ? p do Add the vertex v(X) to the adjacency lattice with label S(X) Add the edge E(X ? fikg; X) for each k 2 f1; : : : ; rg  end  Figure 7: Constructing the adjacency lattice  ?nd a primary threshold value for which the number of itemsets is between N ?Ns and N .

(2) Preprocessing Time: There may be some practical limits as to how much time one is willing to spend in preprocessing. Consequently, even if it is not possible to ?nd N itemsets within the preprocessing time, it ought to be able to terminate the algorithm with some value of the primary threshold for which all itemsets with support above that value have been found.

A simple way of ?nding the primary itemsets is by using a binary search algorithm on the value of the primary thresh- old, using the DHP method discussed in Chen et. al. [17] as a subroutine. This method is somewhat naive and simplis- tic, and is not necessarily e?cient, since it requires multiple executions of the DHP method. This method of ?nding the primary threshold is discussed in the algorithm NaiveFind- Threshold of Figure 7. The time complexity of the proce- dure can be improved considerably by utilizing a few simple ideas:  (1) It is not necessary to execute the DHP subroutine to completion in each and every iteration. For estimates which are lower bounds on the correct value(s) of the primary threshold, it is su?cient to terminate the pro- cedure as soon as N or more large itemsets have been generated at the level of support being considered.

(2) It is not necessary to start the DHP procedure from scratch in each iteration of the binary search proce- dure. It is possible to reuse information between iter- ations. Let I(s) denote the itemsets which have sup- port at least s. It is possible to speed up the prepro- cessing algorithm by reusing the information available in I(Low). Generating k-itemsets in I(Mid) is only a matter of picking those k-itemsets in I(Low) which have support at least Low. This does not mean that every itemset in I(Mid) can be immediately generated using this method. Recall (from (1) above) that the DHP algorithm is often terminated before completion, if more than N itemsets have been generated in that iteration. Consequently, not all itemsets in I(Low) may be available, but only those k-itemsets for which k ? k0, for some k0 are available. Thus, we have all    0 1 2 3 4 5 6 7 8 9  x 10   0.002  0.004  0.006  0.008  0.01  0.012  0.014  0.016  P rim  ar y  th re  sh ol  d  Number of itemsets prestored  T10.I4.D100K T10.I6.D100K T20.I6.D100K  Figure 8: Threshold varation with itemsets prestored  DataSet Conf. Sup. DHP Online  T10.I4.D100K 90% 0:3% 100 sec. instantaneous T10.I6.D100K 90% 0:3% 130 sec. instantaneous T10.I6.D100K 90% 0:2% 240 sec. 2 seconds T20.I6.D100K 90% 0:5% 100 sec. instantaneous  Table 3: Sample illustrations of the order of magnitude advantage of online processing  those k-itemsets in I(Mid) available for which k ? k0.

These itemsets need not be generated again.

6 Empirical Results  We ran the simulation on an IBM RS/6000 530H work- station with a CPU clock rate of 33MHz, 64 MB of main memory and running AIX 4.1.4. We tested the algorithm empirically for the following objectives:  (1) Preprocessing sensitivity: The preprocessing tech- nique is sensitive to the available storage space. The larger the available space, the lower the value of the primary threshold. We tested how the primary thresh- old value varied with the storage space availability. We also tested how the running time of the preprocessing algorithm scaled with the storage space.

(2) Online processing time: We tested how the online processing times scaled with the size of the output. We also made an order of magnitude comparison between using an online approach and a more direct approach.

(3) Level of redundancy: We tested how the level of re- dundancy in the generated output set varied with user speci?ed levels of support and con?dence. We showed that the level of redundancy in the rules is quite high.

Thus redundancy elimination is an important issue for an online user looking for compactness in representa- tion of the rules.

6.1 Generating the synthetic data sets  The synthetic data sets were generated using a method sim- ilar to that discussed in Agrawal et. al. [3]. Generating the data sets was a two stage process:  0 1 2 3 4 5 6 7 8 9  x 10           x 10   Number of itemsets prestored  R el  at iv  e C  om pu  ta tio  na l E  ffo rt  fo r  pr ep  ro ce  ss in  g  T10.I4.D100K T10.I6.D100K T20.I6.D100K  Figure 9: Computation variation with itemsets prestored  0 5000 10000 15000        Number of rules generated  R es  po ns  e T  im e  in s  ec on  ds  T10.I4.D100K T10.I6.D100K T20.I6.D100K  Figure 10: Online response time variation with rules gener- ated  20 30 40 50 60 70 80 90 100        Support fixed at 0.15%  Confidence  (T ot  al R  ul es  G en  er at  ed )/  (E ss  en tia  l R ul  es )  T10.I4.D100K T10.I6.D100K  Figure 11: Redundancy level variation with con?dence    0.1 0.15 0.2 0.25           Confidence fixed at 90%  Support  (T ot  al R  ul es  G en  er at  ed )/  (E ss  en tia  l R ul  es )  T10.I4.D100K T10.I6.D100K  Figure 12: Redundancy level variation with support  (1) Generating maximal potentially large itemsets: The ?rst step was to generate L = 2000 maximal \po- tentially large itemsets". These potentially large item- sets capture the consumer tendencies of buying certain items together. We ?rst picked the size of a maximal potentially large itemset as a random variable from a poisson distribution with mean ?L. Each successive itemset was generated by picking half of its items from the current itemset, and generating the other half ran- domly. This method ensures that large itemsets often have common items. Each itemset I has a weight wI associated with it, which is chosen from an exponential distribution with unit mean.

(2) Generating the transaction data: The large item- sets were then used in order to generate the transaction data. First, the size ST of a transaction was chosen as a poisson random variable with mean ?T . Each trans- action was generated by assigning maximal potentially large itemsets to it in succession. The itemset to be as- signed to a transaction was chosen by rolling an L sided weighted die depending upon the weight wI assigned to the corresponding itemset I. If an itemset did not ?t exactly, it was assigned to the current transaction half the time, and moved to the next transaction the rest of the time. In order to capture the fact that customers may not often buy all the items in a potentially large itemset together, we added some noise to the process by corrupting some of the added itemsets. For each itemset I, we decide a noise level nI 2 (0; 1). We gen- erated a geometric random variable G with parameter nI . While adding a potentially large itemset to a trans- action, we dropped minfG; jIjg random items from the transaction. The noise level nI for each itemset I was chosen from a normal distribution with mean 0.5 and variance 0.1.

We shall also briey describe the symbols that we have used in order to annotate the data. The three primary factors which vary are the average transaction size ?T , the size of an average maximal potentially large itemset ?L, and the number of transactions being considered. A data set having ?T = 10, ?L = 4, and 100K transactions is denoted by T10.I4.D100K.

We tested how the primary threshold varied with the  number of itemsets prestored. This result is illustrated in  Figure 8. The ?gure shows that the primary threshold ini- tially drops considerably as the number of primary itemsets increases, but it bottoms out after a while. We also illus- trate the variation of the computational e?ort required with the available storage space in Figure 9. We note that for the itemset T10.I4.D100K, the computational e?ort required in order to ?nd additional large itemsets after ?nding 20000 itemsets increases considerably with the number of itemsets prestored. This is because for this particular data set, the average size of a maximal potentially large itemset (or bas- ket) is only 4. Consequently, the total number of possible large itemsets is relatively limited. On the other hand, the computational e?ort for preprocessing required by the data sets T20.I6.D100K and T10.I6.D100K is relatively similar.

This shows that the computational e?ort required to ?nd a speci?c number of primary itemsets is more sensitive to the size of a typical basket in the data, rather than to the size of a transaction.

We also tested the variation in the online running time of the algorithm with the number of rules generated. We ran the online queries for varying levels of input parameters in order to test the correlation between the running time and the number of rules generated. This is illustrated in Figure 10. This result is signi?cant in that it shows that the running time of the algorithm increases linearly with the number of rules generated for all the data sets used.

The absolute magnitude of time required in order to gen- erate the rules was an order of magnitude smaller than the time required using a direct itemset generation approach like DHP. A brief summary of some sample relative ?nd- ings is illustrated in Table 3.

We also discuss the level of redundancy present in the rule generation procedure. Figures 11 and 12 illustrate that the number of redundant rules is often much larger than the number of essential rules. The benchmark for measuring the level of redundancy is referred to as the redundancy ratio, and is de?ned as follows:  Redundancy Ratio = Total Rules Generated  Essential Rules (1)  Thus, when the redundancy ratio is K, then the number of redundant rules is K ? 1 times the number of essential rules. The redundancy ratio has been plotted on the Y-axis in Figures 11 and 12. We see that in most cases the number of redundant rules is signi?cantly larger than the number of essential rules. This illustrates the level to which useful rules often get buried in large numbers of redundant rules.

Also, the redundancy level is much more sensitive to the support rather than the con?dence. The lower the level of support, the higher the redundancy level.

7 Conclusions and Summary  In this paper we investigated the issue of online mining of association rules. The two primary issues involved in online processing are the running time and compactness in representation of the rules. We discussed an OLAP-like approach for online mining association rules which avoids redundancy.

Acknowledgements  We would like to thank V. S. Jaychandran and Joel Wolf for their extensive comments and suggestions.

