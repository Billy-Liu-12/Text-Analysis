Mining Association Rules from Complex and Irregular XML Documents using  XSLT and Xquery

Abstract Currently, XML has been penetrating all areas of  Internet for exchanging data. This fast growing usage of XML makes great amount data sources of XML data available and raises the need for languages, methods and tools to extract knowledge through the collections of XML documents. To date, the famous Apriori algorithm to mine any XML document for association rules without any pre-processing or post-processing has been implemented using only the XQuery language. But the algorithm only can mine the set of items that can be written a path expression for. However, the structure of the XML data can be more complex and irregular than that. Consequently, it is difficult to identify the mining context. In this paper, we introduced XSL and XSLT which are also proposed by W3C to do some preprocessing of the input XML documents, transform the complex and irregular XML document into simple and regular XML document which can meet the needs of our mining algorithm. Our preprocessing makes the algorithm much more adaptable and universal.

Keywords: Apriori algorithm, XQuery, XML, XSL,  XSLT, Association Rules, Data mining  1. Introduction   Traditionally, data mining used to work on massive amounts of data which are stored in the relational database to discover useful information and knowledge.

So many data mining techniques such as decision trees, association rules, neural networks have been presented and become very popular. Among them, association rule is much more effective for discovering important relationships in great amounts of data. It is responsible to find correlation relationships among different data attributes in a large set of items in a database. Since its introduction, this method has gained a lot of attention.

Nowadays, Buying and selling items through Internet becomes more and more popular. In attempt to standardize the format of data exchanged over the web and to achieve interoperability between different technologies and tools involved, World Wide Web Consortium [1] introduced XML(Extensible Markup Language). Currently, XML has been penetrating all areas of Internet for exchanging data. This fast growing usage of XML makes great amount data sources of XML data available and raises the need for languages, methods and tools to extract knowledge through the  collections of XML documents. To date, there are some methods proposed to mine XML documents [2], but most of them still need to use the traditional relational database. For example, first map the XML data to the relational data and then use the techniques designed for relational database to do the mining. As a result, this task becomes more troublesome and time consuming.

To our great joy, W3C has proposed a perfect language called XQuery [3] in order to provide an efficient way to extract XML data from XML documents. It makes the interaction between the web and database more smoothly. By using only the XQuery language, Jacky W.W. Wan and Gillian Dobbie [4] have already implemented the famous Apriori algorithm [5] to mine any XML document for association rules without any pre-processing or post-processing. As they stated in their paper, although the research is a good starting point, there are still many issues that remain open. Their algorithm only can mine the set of items that can be written a path expression for. However, the structure of the XML data can be more complex and irregular than that. Consequently, it is difficult to identify the mining context. In order to solve this problem, we introduced XSL and XSLT which are also proposed by W3C to do some preprocessing of the input XML documents, transform the complex and irregular XML document into simple and regular XML document which can meet the needs of our mining algorithm. Our preprocessing makes the algorithm much more adaptable and universal.

We have done the job through several steps. Firstly we introduced a standard data source template called SDST and used it like an interface or adapter. The SDST is the right standard input data which our Apriori algorithm works on. Secondly we modified the Apriori algorithm which is used to generate large itemsets to adapt to the SDST. Thirdly, we introduced XSL and XSLT [7] which are also proposed by W3C to do the preprocessing of the input XML documents. XSL and XSLT can transform one XML document to another XML document with different structure. So here we use them to transform the complex and irregular XML documents into the SDST for our algorithm. In this way, association rules can be extracted from a great many complex and irregular XML documents existing in the Internet.

2. Background Concepts   2.1 Introduction to Association rules   DOI 10.1109/ALPIT.2008.48     Association rules were first introduced by Agrawal et al [6] to analyze customer buying habits in retail databases. Consider a retail store which sells a great many items. In order to increase the sales and profits, it will make some business decisions based on the knowledge about historical transactions data that gives the buying habits of the customer. The association rule is in the form of customer who bought item M may also bought item N. So the goal of association rules is to extract such rules from the given transaction data history stored in database.

The problem of association rules can be best explained using Transaction data in the Market-Basket.

A basket can contain many items that have been bought at a grocery store. For instance:   Transaction D(TID)  tems  Basket1 {pen,notebook,pensilebox} Basket2 {pen,notebook,pensile box ,ink} Basket3 {ink, milk } Basket4 {pen,notebook,pensile  box,ink,milk,paper,eraser } Basket5 {pen, ink, nodebook}  Table 1.  A Transactions Database  Consider I is the set of n distinct items where  1 2{ , ,..., }nI i i i= while an item is an article in a basket, or an attribute-value pair. A transactionD consists of all the items purchased together in one basket such thatD I? . It is called an itemset and usually has a transaction Id (TID) along with it. For example, transaction {pen, notebook, pensile box, ink, milk, paper, eraser } is an itemset whose TID is Basket4. A k-item-set is an item-set with exactly k items in it. So Basket4 is a 7-itemset. A transactions database is just the set of such transactions like Basket4. An association rule is about the relationship between two disjoint itemsets, ,X Y I?  and X Y? = ? . It is an implication of the form X Y=> and presents the pattern that when X occurs, Y also occurs. An association rule is characterized by two measures: the support which means the percentage of transactions in the transactions database that contain both items X and items Y; and the confidence which means the percentage of transactions in the transactions database containing the items X that also contain the items Y. So, for an association rule X Y=> , it can be calculated that support( X Y=> ) = support(XUY) while confidence( X Y=> )=support( X Y=> )/support(X).

Take Table1 for example, in this transactions database, association rule ?pen, notebook=>pensile box? holds the support of 0.6 and the confidence of 0.75. This rule implicates that customers who buy pen and notebook also buy pensile box in 75% of the cases and this rule holds in 60% of all the transactions in the database.

The number of association rules that can be extracted  from transactions database could be enormous and not all the rules found are useful to us. Interesting association rules are those whose support and confidence are greater than minSupp and minConf which are predefined. An itemset with the support greater than minSup is called large itemset or frequent itemset. The X Y=>  rule can be a powerful rule only when XUY is in the large itemset and its confidence is greater than minConf.

2.2 Introduction to Apriori algorithm  Usually, the steps involved in the mining association  rules are as follows: (1). large itemset generation  Discover all the large itemsets from the transactions database, in this section, we use Apriori algorithm to implement this.

(2). Rules derivation  Extract the association rules from the large itemsets which are generated by the first step.

The Apriori algorithm uses the bottom-up and breadth-first property pruing unnecessary branches for further consideration. It needs two parameters, minSupp and minConf. The minSupp is used for generating large itemsets and minConf is used for rules derivation. The following presents the Apriori algorithm.

The APRIORI algorithm  (1). k = 1; (2). Find large itemset kL  from kC  which is the  set of all candidate itemsets; (3). Generate 1kC +  from kL ; (4). k = k+1; (5). Repeat (2)-(4) until kC  is empty;  Step (2) is called the large itemset generation step and Step (3) is considered as the candidate itemset generation step. In step (2) scan transactions database and count the support of each itemset in candidate itemset. If the support is greater than minSupp, then add that itemset to large itemset. In step (3), there is much more work to do, shown as follows: For k = 1, 1C  = all itemsets of length = 1.

For k > 1, generate kC  from 1kL ?  as follows: (1). join  kC = join of 1kL ? with itself.

If both 1 2 1{ ,..., , }k ka a a? ?  and 1 2{ ,..., , }k ka a a?  are in  1kL ? , then add 1 2 1{ ,..., , , }k k ka a a a? ?  to kC .

These items are always stored in the sorted order.

(2). Prune Remove 1 2 1{ ,..., , , }k k ka a a a? ? , if there is any of  its subset is not a large itemset, since any subset of a large itemset must be large.

3. Mining Association Rules   In this part, we introduced XSLT and XQuery to  mine association rules from complex and irregular XML documents existing in the Web. In order to continue the discussion, we assume the reader already has the knowledge of XML, XQuery and XSLT. As stated in the introduction section, our task involves in three main steps:  (1). Introduce to SDST; (2). Improve the Apriori algorithm presented by  Jacky W.W. Wan and Gillian Dobbie in their former paper to adapt to the SDST;  (3). Transform the original data source of complex and irregular XML documents to SDST.

3.1 Introduction to SDST  SDST is the acronym of Standard Data Source Template. It is a simple and regular XML document which is used as the standard template for the input data source of our Apriori algorithm implemented using XQuery. The following is an example of SDST <?xml version="1.0" encoding="utf-8"?> <transaction>  <itemset> <item>pen</item> <item>notebook</item> <item>pensile box</item>  </itemset> <itemset>  <item>pen </item> <item>notebook</item> <item>pensile box</item> <item>ink</item>  </itemset> <itemset>  <item>ink</item> <item>milk</item>  </itemset> <itemset>  <item>pen</item> <item>notebook</item> <item>pensile box</item> <item>ink</item> <item>milk</item> <item>paper</item>  </itemset> <itemset>  <item>pen</item> <item>ink</item> <item>notebook</item>  </itemset> </transaction>  In the SDST, the set of transactions is identified by the tag <transaction> which is also the root node of the SDST XML document. Tag <itemset> represents each transaction in the transactions set while tag <item>  represents each purchased item in each transaction.

SDST is the right input source XML document and our Apriori algorithm works on the SDST to extract the most powerful association rules.

3.2 Improvement on the Apriori algorithm using XQuery  Although Jacky W.W. Wan and Gillian Dobbie have proposed an Apriori algorithm using XQuery in their paper, the algorithm couldn?t works on the SDST. In order to make the Apriori algorithm adapt to the SDST, we have made some improvements on all the fundamental functions based on which the Apriori algorithm was built as well as creating a completely different algorithm to generate the large itemsets.

3.2.1 Improvement on the six fundamental functions  In the Jacky and Dobbie?s previous paper, they have presented six important functions: join, commonIts, removeIts, candidateGen, prune and removeDuplicate.

Among them, join, commonIts and removeIts act like the traditional set operator union, intersect and except respectively. The reason for not using the built-in set operator in XQuery is that these set operators are used to combine sequences of nodes. CandidateGen, prune and removeDuplicate are used to generate the candidate set. For each step k, GenerateCandidate function generate k-itemsets by joining large (k-1)-itemsets with itself using join function provided above. In each step k, Not all the itemsets generated are potential large k-itemsets, say, the itemsets may contain more or fewer items. Therefore, we use prune function to eliminate them. What?s more, there may be duplicate k-itemesets in the Candidate itemsets. So it?s the removeDuplicate function?s responsibility to remove them.

Although the above six functions work very well, they aren?t proper for our SDST. In order to apply to SDST, the input arguments and return values of these functions must be formatted. So, we have made some improvements and defined our own functions: local:common($x as element(),$y as element()); local:join($x as element(),$y as element()); local:remove($x as element(),$y as element()); local:prune($x as element(),$y as element()); local:GenerateCandidate($x as element()); local:removeDuplicate($x as element()).

Unlike the previous functions, the arguments in these functions, such as $x and $y, can only be the itemset in SDST with the following structure.

For instance: <itemset> <item>a</item> <item>b</item>  <item>c</item> </itemset>  Also, these functions return an itemset with the same structure as the result. This restriction means any other     element with a different structure in XML document cannot be used as the argument. Thanks to this, the functions can strictly perform to SDST.

As Apriori algorithm stated, for each step k, once the Candidate itemset is generated, the Large itemset should be extracted from the Candidate itemset by joining it with itself. For each step k, we use the LargeItemsets function to achieve this goal. The function compose of LargeItemsetsCandidate and RemoveNonLargeItemset. The two sub functions work together to get the large itemsets we need.

LargeItemsetsCandidate joins the Candidate itemset with itself and generates the set of Candidate large itemsets. Not all the itemsets in this set are large itemsets that we need, only those whose support is equal or greater than the minSup.

RemoveNonLargeItemset remove those itemsets not eligible and return the final large itemsets.

Up to now, we have got the k-largeItemset in each step k, the following task is to combine all the steps using Apriori algorithm and generate all the large itemsets in the XML document. is our apriori algorithm.

Besides, we have created a function called GenerateLargeItemsets  works like an interface. What you need to do is just put the source XML document into the function argument, and then the function would extract all the large itemsets for you.

It is worth noting that we use preprocess Data to generate the large 1-itemsets. Because the large 1-itemsets is just all the distinct single item which support is greater than the minSup in the XML document, the method to get them is different from getting other lagrge k-itemset (k>1).

Let?s take the SDST showed above for example, its name is customer.xml and given the minSup=0.4, you only need to write the following command:  local:GenerateLargeItemsets(doc("customer.xml"), 0.4), it will generate all the large itemsets.

Afterwards, we use generateSupp(), to get large itemsets document as shown in the following.

<LargeItemsets>  <itemset> <item>pen</item> <sup>0.8</sup>  </itemset> <itemset>  <item>notebook</item> <sup>0.8</sup>  </itemset> <itemset>  <item>pensile box</item> <sup>0.6</sup>  </itemset> <itemset>  <item>ink</item> <sup>0.8</sup>  </itemset> <itemset>  <item>pen</item>  <item>notebook</item> <sup>0.8</sup>  </itemset> <itemset>  <item>pen</item> <item>pensile box</item> <sup>0.6</sup>  </itemset> <itemset> <item>pen</item>  <item>ink</item> <sup>0.6</sup>  </itemset> <itemset>  <item>notebook</item> <item>pensile box</item> <sup>0.6</sup>  </itemset> <itemset>  <item>notebook</item> <item>ink</item> <sup>0.6</sup>  </itemset> <itemset>  <item>pen</item> <item>pensile box</item> <item>notebook</item> <sup>0.6</sup>  </itemset> <itemset>  <item>pen</item> <item>ink</item> <item>notebook</item> <sup>0.6</sup>  </itemset> </LargeItemsets>  Large itemsets don?t mean association rules. One more step is required to convert these large itemsets into association rules. Association rules can be found from each large itemset X by using the following algorithm:  For each non-empty subset A of X (1)  Let B=X-A; (2)  A=>B is an association rule if  Confidence(A=>B) ?  minConf Where confidence(A=>B)=support(AB)/support(A) and support(A=>B)=support(AB).

And the following is the algorithm implemented using XQuery:  In the rules function, for each large itemset X in the large itemsets document, we search for other large itemset Y which satisfiesY X? and Z=X-Y and support(YUZ)/support(Y) ? minConf. And the rule Y=>Z is found correspondingly. Y is antecedent and Z is the consequent. Our XQuery algorithm for extracting the association rules is based on the theory any subsets of a frequent itemset are also frequent itemsets. The Apriori property (downward closure property) can prove that.

3.2.2 Transform the original data source of complex and irregular XML documents to SDST  We just discussed how to mine association rules from SDST. However, the structure of the XML data can be more complex and irregular in the real world. So it?s time for us to transform the complex and irregular XML source documents into SDST. Here we introduce XSLT to perform this task. We assume that the reader has some knowledge of XSLT.

For some XML document, we write the XSLT to transform it to our SDST.

The XSLT is the following: <xsl:stylesheet version="1.0" xmlns:xsl= "http:// www.w3.org/  1999/XSL/Transform"> <xsl:template match="/"> <transaction> <xsl:apply-templates/> </transaction> </xsl:template> <xsl:template match="customer"> <itemset> <xsl:apply-templates select="commodity"/> </itemset> </xsl:template> <xsl:template match="commodity"> <item> <xsl:value-of select="name"/>, <xsl:text /> </item> </xsl:template> </xsl:stylesheet>  In the SDST, there are only three different element, transaction, itemset and item. Therefore, we need to create three templates for each complex XML document. In XSLT, the <xsl:template> element is used to build our SDST templates. The three steps involved in are shown as follows: (1) The match=?/? attribute defines the template for  the whole document.

<xsl:template match="/">  <transaction> <xsl:apply-templates/> </transaction>  </xsl:template> The above template means the whole document  should be included in the root element <transaction> and </transaction>.

(2) the match=?customer? attribute defines the  template for the customer element.

<xsl:template match="customer">  <itemset> <xsl:apply-templates select="commodity"/> </itemset>  </xsl:template> The above template means tag <customer id=??>  </customer> are replaced by the tag <itemset> and </itemset> and among all the elements in the <customer  id=??></customer>,only element <commodity> is selected and put between <itemset> and </itemset>.

(3) The match="commodity"> attribute defines the  template for the commodity element.

<xsl:template match="commodity"> <item> <xsl:value-of select="name"/>,<xsl:text /> </item> </xsl:template>  The above template means tag <commodity > </commodity> are replaced by the tag <item> and </item> and among all the elements in the <commodity ></commodity>, only element <name> is selected and extracted by its value which is put between <item> and </item>. Through the above three steps, any complex and irregular transactions XML document can be transformed to SDST and then can be extracted association rules using Apriori algorithm.

It is worth noting that we cannot create a common XSLT for all the XML documents. This is determined by the XML itself. XML is a document which can be extensible so that its tags are not predefined and can be changed by anyone as they like. Therefore, it is our responsibility to create each XSLT corresponding to each XML document.

4. Apply the XSL transformation to the SDST   We implemented the transform under the Windows  and Microsoft.NET platform using Microsoft Visual Studio 2005 and C#. For preparing, we save source XML document as commodity.xml and save XSL file as commodity.xsl to the \Bin\Debug folder that is located underneath the folder in which the project is created.

(1) Create a new C# console application in Visual Studio 2005 and make sure that the project contains a reference to the System.Xml namespace, and add a reference if it does not. Import Xml and Xsl namespaces and remember prior to any other declarations.

(2) Declare an XslTransform object to transform commodity.xml as well as other appropriate variables would be used.

(3) Create a new XslTransform object. The XslTransform is an XSLT processor implementing the XSLT recommendation.

(4) Load the XslTransform object with the style sheet which transforms the commodity.xsl into the SDST using Load method.

(5) Initiate the transformation using Tranform method.

(6) Bulid and run the project, the result SDST file is in the \Bin\Debug folder under the project file?s folder.

5. Conclusion  Jacky W.W. Wan and Gillian Dobbie have already  implemented the famous Apriori algorithm to mine any XML document for association rules using only the Query language. But their algorithm can only mine the set of items which can be written a path expression for.

However, the structure of the XML data can be more complex and irregular than that. Consequently, it is difficult to identify the mining context.

In order to solve this problem, we introduced XSL and XSLT to do some preprocessing of the input XML documents, transform the complex and irregular XML document into simple and regular XML document which can meet the needs of our mining algorithm. Our preprocessing makes the algorithm much more adaptable and universal.

The work is just a starting point, Our future research will extend the problem by mining association rules from more than one XML document where documents have so many different structure. Besides, we will focus on implementing other algorithms like FP-growth or decision trees.

.

