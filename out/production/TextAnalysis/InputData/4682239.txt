A New Interestingness Measure for Associative Rules Based on the Geometric  Context

Abstract   Associative classification has arrested attention in  recent years and made significant improvement in related applications. This paper introduces the concept of a new interestingness measure and examines its utility in some application domains. Many interesting- ness measures have been presented before with different qualities, which make them useful for some applications. Some of these measures, such as support and Interest, do not concentrate on all properties of an association rule. Besides, some of them, such as J_Measure and Mutual Information, have complex computes. We present a new geometric measure which uses all basic terms of a contingency table values ),( BAP , ),( BAP , ),( BAP , ),( BAP to estimate the association of itemsets A and B. The fundamentals of this measure are based on a simple fact: Since sum of these terms is constant, increasing each term causes the decrement of the other terms. Then, for better understanding, we describe our new measure in semi Cartesian coordinates. Finally, we demonstrate the benefits of using the new measure for association rule mining based on results obtained from a random generated dataset.

1. Introduction  Finding relationships and rules between types of data is a fundamental task in many data mining problems. Finding these rules leads user to control one part of data by changing other parts. For example, there are lots of goods in a supermarket arranged in rows. Some goods have relation to each other by their nature. For instance, most of customers who buy A, will buy B if they see it. Therefore, it's better to put A and B together in the same place. Now, consider the  case which most of customers prefer to buy A rather than B. Then, if we put them nearby, B will not be sold. Therefore, it's better to put A and B far from each other. Similar rules could be found in other data sets such as article data sets, etc.

There are many rules in a data set, but all of them are not interesting for all users. For this reason, many measures have been presented. Each of them has focused on some of user's interests, and filters the other rules attending to these interests. Therefore, a data mining expert must be careful in choosing the measures for filtering rules. In fact, this choice is hard to make since rule interestingness measures have many different qualities [1].

This paper is organized as follows: In section 2 we briefly recall the context of association rules. In section 3 we introduce a list of interestingness measures and their properties. We propose in section 4 a new interestingness measure and prove its applicability in filtering association rules. In section 5 we present the results of some measures, including our proposed measure, for a random data set. Finally, we conclude in section 6.

2. Association rule mining  Given a typical market-basket database E, an associative rule A?B means if someone buys the itemset A, then he/she probably also buys itemset B.

Some properties of an associative rule A?B are listed below:  ? A and B are itemsets of a dataset, like goods in a supermarket.

? ?=? BA ? Support(A,B) is the ratio of records that  contains A and B to total number of records.

?  confidence (A,B) is the maximum of two   DOI 10.1109/ICCIT.2008.299    DOI 10.1109/ICCIT.2008.299     values: the ratio of  records that contains A and B to records that contains A, and the ratio of  records that contains A and B to records that contains B.

For example in market-basket data set, a sample is quality of sale in a time period (like one month), goods are itemsets, and each record is a customer. In Table 1, five samples of a virtual market-basket data set, with two itemsets Coffee and Tea have been shown.

Table 1. Five samples of a virtual basket- market data set  May Apr Mar Feb Jan 2120 4871 2654 3684 2548 Coffee 1500 1244 987 1542 874 Tea 997 356 116 744 564 Coffee & Tea 4300 7065 3687 5854 4584 Total Customer   In data mining, one of the main problems is to select  associative rules (A?B) from all of the associative rules extracted from a data set, which dependencies of A and B satisfies a defined threshold.

The well-known APRIORI algorithm proceeds in two steps within the support-confidence framework in order to extract association rules [3]:  ? Find frequent itemsets (the sets of items which occur more frequently than the minimum support threshold) with the frequent itemsets property (any subset of a frequent itemsets is frequent; if an itemsets is not frequent, none of its supersets can be frequent) for efficiency reasons. Thus, starting from k=1 APRIORI generates itemsets of size k+1 from frequent itemsets of size k.

? Generate rules from frequent itemsets and filter them with the minimum confidence threshold.

APRIORI tends to generate a large number of rules. It is hence impossible for an expert to filter these rules.

Therefore, it seems necessary to add an extra phase to APRIORI algorithm which can filter rules in a good manner. One of the classic methods which relies on the use of subjective measures is the user-driven in the sense that it takes into account the user's Apriori knowledge while objective measures are said to be data-driven and only take into account the data cardinalities. In this paper, we focus on objective measures.

As we mentioned, interestingness measures can be powerful tools for experts to filter and manage rules extracted by APRIORI algorithm. Some of these  measures are listed in section 3.

3. Existing measures  Interestingness measures for associative rules are usually defined using frequency counts or relative frequencies as presented in Fig. 1[4].

Given a rule A?B we have:   ? N=|E| the total number of records;  ? NA=|A| the number of records in A;  ? NB=|B| the number of records in B;  ? NAB=|A?B| the number of record are in both A and B;  ? N?B=|??B| the number of records are in B but not in A;  ? P(A)= NA/N  probability of A;  ? P(A,B)=NAB/N probability of A and B.

Fig 1. Diagram of E, A and B   The analysis of relationships between itemsets often  requires a suitable metric to capture the dependencies among itemsets. These metric are defined in terms of the frequency count tabulated in a 2?2 contingency table as shown in Table 2(Left).

Table 2. Left: General form of a contingency  table, Right: Contingency table of sample May  B  B B  B  2120 1123 997 A AN  BANABN A  2180 1677 503 A ANBANBANA  4300 2800 1500   N BNBN  As an example, we have presented contingency table  of sample May in Table 2(Right).

Some of common interestingness measures have  been introduced in Table 3 [3].

4. Proposed Measure  In this section we introduce a new measure which is able to find good rules among all produced rules.

A?B A B  E     Table 3. Some of common interestingness measures # Measure  Formula Range Independence value (?)                                                 ?-coefficient     Goodman-Kruskal    Odds ratio    Yule's Q    Yule's Y    Kappa     Mutual Information    J-Measure      Gini index    Support Confidence   Laplace   Conviction     Interest   Cosine    Piatetsky-Shapiro's Certainty factor   Added value   Collective strength     Jaccard   Klosgen   ?   ?   ?    Q    Y   ?     M   J      G   s c   L   V   I   IS    PS F   AV   S   ?   K  ( )  ))()|(),()|(max(),( ),()()(  ),( ),(),(1  )()()()(1 )()()()(  ),(),( )()|(),()|(max  ) )(1  )()|(, )(1  )()|(max( )()(),(  )()( ),( )()(  ),(  ) ),( )()(,  ),( )()(max(  ) 2)( 1),(,  2)( 1),(max(  ))|(),|(max( ),(  ))()(])|()|()[(])|()|()[( ,)()(])|()|()[(])|()|()[(max(  ) )(  )|(log),() )(  )|(log(),(  ), )(  )|(log(),() )(  )|(log(),(max(  ))(log)(),(log)(min( )()(  ),( log),(  )()()()(1 )()()()(),(),(  ),(),(),(),(  ),(),(),(),( ),(),(),(),( ),(),(),(),(  ),(),( ),(),(  )(max)(max2 )(max)(max),(max),(max  ))(1))((1)(()( )()(),(    APBAPBPABPBAP BAPBPAP  BAP BAPBAP  BPAPBPAP BPAPBPAP  BAPBAP APBAPBPABP AP  APBAP BP  BPABP BPAPBAP  BPAP BAP BPAP  BAP BAP BPAP  BAP BPAP  BNP BANP  ANP BANP  ABPBAP BAP  APAPBAPBAPBPBAPBAPBP BPBPABPABPAPABPABPAP  AP BAPBAP  AP BAPBAP  BP ABPBAP  BP ABPBAP  BPBPAPAP BPAP  BAP BAP  BPAPBPAP BPAPBPAPBAPBAP  BAPBAPBAPBAP  BAPBAPBAPBAP BAPBAPBAPBAP BAPBAPBAPBAP  BAPBAP BAPBAP  BPAP BPAPBAPBAP  BPAPBPAP BpAPBAP  jj jii i  i j ji  ji ji  kkjj  k kkjjkjjj kjk  ?? ?+  ?? ???  +  ? ?  ? ?  ? ?  + +  + +  ??+++ ??+++  +  +  ????  ??  ?? ??+  +  ? + ?  ?? ? ??? +  ?? ?     -1?1     0?1    0??    -1?1    -1?1    -1?1     0?1    0?1      0?1    0?1 0?1   0?1   0.5??     0??    0?1   -0.25?0.25 -1?1   -0.5?1   0??     0?1   0.12?0.38                         0.25 0.5   0.5        0.5     0.4    0.33    First, we explain two types of dependencies:  ? Two itemsets A and B are dependent if by happening of A, we expect happening of B which is called a positive dependency.

? Two itemsets A and B are dependent if we expect happening just one of them. This dependency is called negative.

Our interestingness measure can find and highlight rules which specify these types of dependencies. To understand how this measure works, we consider a semi-Cartesian axis. Each half axis in it, is used to show one of the values ),( BAP , ),( BAP , ),( BAP and ),( BAP .

In our approach right half axis is allocated to ),( BAP , left half axis is allocated to ),( BAP , up half axis is allocated to ),( BAP and down half axis is allocated to ),( BAP . Therefore, we have four points for each contingency table in this semi-Cartesian axis. These points form a quadrilateral which is shown in Fig 2.

Fig 2. Quadrilateral for sample May   It is clear that there exists one quadrilateral for each sample in data set. In these quadrilaterals diameters (which are called v and h) are perpendicular.

Therefore, their area will be compute by (1).

(1) ( )hvAr ?= 21  Since v and h are computed by (2) and (3), the area will be computed by (5).

(2) ),(),( BAPBAPh +=  (3) ),(),( BAPBAPv +=  (4) 1=+ hv  (5) ( ) ( )),(),(),(),(21 BAPBAPBAPBAPAr +?+= The minimum value of v and h is zero and the  maximum value of them is one. This area could be used as a measure to compute the dependency degree between two itemsets A and B.

It is obvious that the least value of Ar is zero which happens when one of v or h was zero. In this condition quadrilateral turns into a horizontal (or vertical) line with a length of one. To compute maximum of Ar we have:  (6) ( ) 1,21 =+?= hvhvAr  (7) ( )( ) ( )221121 vvvvAr ?=??=  (8) ( ) 21,2102121 ==?=?= hvvd d  v  Ar  (9) ( ) 125.081212121max ==??=Ar Thus, each quadrilateral's area is between 0 and  0.125. On the other hand, each record (customer) belongs to one of four classes in Table 2(Left). When we have N records which are distributed in these four classes normally, we expect each class to have approximately 4N  members. In this condition, quadrilateral turns into a lozenge and its area becomes maximum (0.125).

But, when gathered records in one of two groups v or h increases, quadrilateral will be pulled and its area will be decreased. In this condition, it is apparent that one of those two types of dependencies exists between itemsets A and B. Hence, it seems that there is a map between area of quadrilateral and dependency of A and B: the increment of dependency between A and B leads to the decrement of area.

We have proved that value of Ar is between 0 and 0.125. In addition to the simplicity of computing this measure, the two mentioned dependencies could be specified easily.

5. Results To analyze and evaluate the proposed measure in  this section, a random data set with 20 samples (E1- E20); has been considered. Each sample contains 1000 records and two itemsets A and B. For example in a supermarket, itemsets are goods, records are customers who buy goods and samples are monthly sales of A and B. This data set is shown in Table 4.

Table 4. Randomly generated data set  Ar v h  BAN  BAN  BAN  ABN  0.1236 0.553 0.447 178 229 324 269 E1 0.1025 0.288 0.712 589 133 155 123 E2 0.1142 0.647 0.353 152 44 603 201 E3 0.1233 0.442 0.558 113 194 248 445 E4 0.1070 0.69 0.31 253 458 232 57 E5 0.0962 0.26 0.74 31 122 138 709 E6 0.0648 0.153 0.847 379 19 134 468 E7 0.1228 0.433 0.567 164 223 210 403 E8 0.1048 0.701 0.299 184 283 418 115 E9 0.1040 0.295 0.705 598 141 154 107 E10 0.1194 0.394 0.606 73 232 162 533 E11 0.0866 0.223 0.777 528 99 124 249 E12 0.1060 0.695 0.305 78 371 324 227 E13 0.1168 0.628 0.372 158 438 190 214 E14 0.0997 0.275 0.725 689 139 136 36 E15 0.1197 0.603 0.397 99 388 215 298 E16 0.1139 0.649 0.351 350 4 645 1 E17 0.0877 0.227 0.773 721 12 215 52 E18 0.1086 0.681 0.319 313 451 230 6 E19 0.1104 0.671 0.329 157 11 660 172 E20   Table 5, contains rankings by measures for the  generated data set.

The proposed measure shows that least  independency belongs to E1, which has the most normal distribution of records. None of other measures has not selected this sample as the most independent sample. In this sample, customers have a normal random behavior about itemsets A and B and each class has approximately 4N  members.

On the other hand, most of the measures have selected E7 as the most dependent sample, so as the proposed measure. This means that most of customers buy both of A and B or none of them.

6. Conclusions  In this paper, we have described some of the most common measures which are used in many data mining procedures. We listed some properties which covered a large potential user's preference. Then, we presented two types of dependencies which are used in many cases such as supermarket datasets.

Table 5. Ranking of samples by different interestingness measures ? ? ? Q Y k M J G s c L V I IS PS F AV S ? K Ar E1 8 12 6 7 7 9 7 8 9 2 3 3 6 6 2 9 8 8 10 2 9 1 E2 13 1 16 11 11 14 12 13 13 8 1 1 16 17 3 13 13 16 3 3 16 15 E3 3 2 3 3 3 4 3 4 3 6 16 16 7 4 4 4 6 5 11 7 6 7 E4 2 3 2 2 2 3 2 2 2 13 8 8 3 2 11 2 3 2 8 11 3 2 E5 16 16 12 15 15 15 16 17 16 12 14 14 11 14 17 15 12 14 17 16 11 11 E6 4 4 4 4 4 5 4 3 4 20 17 17 4 3 16 5 4 4 2 19 4 17 E7 20 20 20 20 20 20 20 20 20 16 19 19 20 13 18 20 20 19 20 20 20 20 E8 7 5 7 6 6 8 6 6 7 11 7 7 8 5 10 7 7 6 7 10 8 3 E9 15 18 10 13 13 16 14 16 15 9 9 9 13 12 14 16 15 15 18 12 15 13 E10 11 6 15 10 10 12 10 12 10 10 4 4 15 15 6 11 11 12 5 4 14 14 E11 1 7 1 1 1 1 1 1 1 19 11 12 2 1 13 1 2 1 6 15 2 5 E12 19 19 18 17 17 19 18 18 19 1 10 10 17 18 12 19 17 18 1 13 19 19 E13 17 17 11 14 14 17 15 14 17 3 6 6 14 11 9 18 16 13 16 6 17 12 E14 10 11 8 8 8 10 9 11 11 4 2 2 9 9 7 12 9 9 14 5 10 6 E15 5 8 5 5 5 6 5 5 5 15 12 13 5 10 15 6 5 7 9 14 5 16 E16 12 13 9 9 9 11 11 10 12 5 5 5 10 8 1 14 10 10 13 1 12 4 E17 6 10 13 16 16 2 13 7 6 18 13 11 1 16 20 3 1 3 15 18 1 8 E18 14 14 19 18 18 13 17 15 14 14 15 15 19 20 8 10 19 20 4 9 18 18 E19 18 15 14 19 19 18 19 19 18 17 20 20 12 19 19 17 14 17 19 17 7 10 E20 9 9 17 12 12 7 8 9 8 7 18 18 18 7 5 8 18 11 12 8 13 9   We also introduced a new interestingness measure and proved its range and its relation to itemsets dependencies. Then we showed the results of ranking a random data set samples has been shown by various measures and it has been described that our proposed measure is able to find some independencies better than most of the other measures.

7. Acknowledgment  This work is partially supported by Data and Text Mining Research group at Computer Research Center of Islamic Sciences (CRCIS), NOOR co. P.O. Box 37185-3857, Qom, Iran  8. References [1] P. Lenca, P. Meyer, B. Vaillant, and S. Lallich, "On selecting interestingness measures for association rules: User oriented description and multiple criteria decision aid", European Journal of Operational Research 184, 2008, pp.

610-626.

[2] R. Agrawal, T. Imielinsky, and A. Swami, "Mining association rules between sets of items in large databases", In Proc. Of 1993 ACM-SIGMOD Int. Conf. on Management of Data, Washington, D.C., May 1993, pp. 207-216.

[3] P. Tan, V. Kumar, and J. Srivastava, "Selecting the right interestingness measure for association patterns." Technical Report 2002-112, Army Performance Computing Research Center, 2002.

[4] Y. Lan, D. Janssen, G. Chen, and G. Wets, "Improving associative classification by incorporating novel interestingness measures", Expert Systems with Applications 31, 2006, pp. 184-192.

[5] G. Pitatetsky-Shapiro, "Discovery, analysis and present- ation of strong rules." , In G. Piatetsky-Shapiro and W.

Frawley, editors, Knowledge Discovery in Databases, MIT Press, Cambridge, MA, 1991.

[6] F. Mosteller, "Association and estimation in contingency tables", Journal of the American Statistical Association, 1968, pp.1-28.

[7] C. Aggarwal, and P. yu, "A new framework for itemsets generation", In Proc. Of the 17th Symposium on Principles of Database Systems, pages 18-24, Seattle, WA, June 1998.

[8] T.G. Dietterich, "Approximate statistical tests for comparing supervised classification learning algorithms", Neural Computation, 1998, 10(7), pp.1895-1924.

[9] B?ttcher, M. et al., "Mining changing customer segments in dynamic markets", Expert Systems with Applications (2007),doi: 10.1016/ j. eswa. 2007.09.006.

[10] X. Yin, J. Han, "Classification based on predictive association rules", Proceedings of 2003 SIAM international conference on data mining, San Francisco, CA, 2003, pp.

331-335.

