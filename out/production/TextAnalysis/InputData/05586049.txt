?

Abstract?Variable Orderings (VOs) have been used as a restriction in the process of Bayesian Networks (BNs) induction. The VO information can significantly reduce the search space and allow some algorithms to reach good results.

Previous works reported in the literature suggest that the combination of Evolutionary Algorithms (EAs) and VOs is worth when learning a Bayesian Network structure from data.

However, most works on this area do not explore specific characteristics of the domain, thus, they simply apply classic evolutionary operators. In addition, most works did not report good results when applied to big BNs. This paper proposes a new mutation operator, named Distance-Based Mutation Operator (DMO), to be used with the Variable Ordering Evolutionary Algorithm (VOEA). Experimental results obtained by VOEA are compared to ones achieved by VOGA (Variable Ordering Genetic Algorithm), and indicated improvement in the quality of the obtained VO and in the BN induced structure.



VI. INTRODUCTION AYESIAN Networks (BNs) are graphical representations of probability distributions and have been widely used to model and represent uncertainty. The development of  automatic methods for learning BN structures directly from data is a relevant problem, and is considered a difficult task because the number of possible structures grows exponentially according to the number of variables.

Therefore, finding the BN structure that better represents the dependences among the variables is not a trivial task. This is a NP-Complete problem, thus it is hard to identify the best solution for all the application problems [5]. In order to reduce the search space, some constraints can be imposed during the induction process. One possible constraint is the definition of a Variable Ordering (VO).

The definition of a VO can significantly reduce the search space and may play an important role in the process of BNs induction. Defining a suitable VO is, however, a complex problem to be performed mainly because it requires prior knowledge about the model [3].

Evolutionary Algorithms (EAs) have characteristics that make them very efficient as search and optimization tools   Authors acknowledge the Brazilian research agency CNPQ and FAPESP for their financial support.

E. B. Santos is with the Federal University of Rio de Janeiro and Federal University of S?o Carlos - Brazil. E-mail: edimilson_santos@dc.ufscar.br  E. R. Hruschka Jr. is with the Federal University of S?o Carlos - Brazil.

E-mail: estevam@dc.ufscar.br  E. R. Hruschka is with the University of S?o Paulo ? Brasil. E-mail: e- rh@icmc.usp.br  N. F. F. Ebecken is with the Federal University of Rio de Janeiro ? Brasil. E-mail: nelson@ntt.ufrj.br    for many different types of problems [10] and some works have already proposed the use of EAs [19] to seek suitable VOs to induce BNs structures from data. However, to our knowledge, the proposed EAs are mainly based on canonical crossover and mutation operators to address this problem. In this paper we show that an operator specially designed to the VO definition problem can help to improve the quality of BN structures induced from data. In this sense, we propose a new mutation operator named Distance-based Mutation Operator (DMO) which dynamically controls the permutation of genes in the chromosomes of an EA. The proposed EA is called VOEA (Variable Ordering Evolutionary Algorithm) and extends the ideas presented in [19] trying to explore the influence of a single evolutionary operator specially designed to search for suitable VOs.

The remainder of this paper is organized as follows.

Section 2 provides a brief overview of BNs, as well as of the related structure learning problem. Section 3 addresses related work, whereas Section 4 introduces the Distance- Based Mutation Operator (DMO). Section 5 shows some preliminary experiments with the proposed operator. Finally, Section 6 brings the concluding remarks and points out some future work.



VII. BACKGROUND   Bayesian Networks (BNs), also known as belief networks or causal networks, are probabilistic graphical models in which the nodes represent random variables and the arcs represent the direct connection between them. A BN consists of two major components: i) a directed acyclic graph (DAG) ? or graphical structure ? and ii) a set of conditional probability table (CPT) ? or numerical parameters.

The DAG can be represented by G = (V, E) and consists of a set of nodes (V = {X1, X2, ..., Xn}) and an ordered set (E) of pairs of distinct elements in V. Each node in V corresponds to a discrete random variable in the domain.

The elements of E are named edges (or arcs). An edge, Xi ? Xj describes a parent and child relation, where Xi is the parent and Xj is the child. All parents of Xj constitute the parent set of Xj ? denoted by ?Xj. In addition to the graph, each node Xk has an associated CPT specifying the probability of each possible state of Xk given each possible combination of states of the nodes in ?Xk. If a node Xk is such that ?Xk = ?, its associated table gives the marginal probabilities of Xk.

In the BN induction process, the Variable Ordering (VO)  A Distance-Based Mutation Operator for Learning Bayesian Network Structures using Evolutionary Algorithms  Edimilson B. dos Santos, Estevam R. Hruschka Jr., Eduardo R Hruschka, and Nelson F. F. Ebecken  B          information may be used as a restriction to reduce the search space, as performed by the K2 algorithm [7]. K2 constructs a BN from data using a heuristic search procedure. It receives as input a complete dataset and a VO. K2 is broadly used due to its good performance in terms of computational time complexity as well as its good results when an adequate VO is supplied [15].

The attribute preorder assumption is a critical issue and it is used to reduce the number of possible structures to be learned. In this sense, K2 uses an ordered list (containing all the attributes including the class), which asserts that only the attributes positioned before a given attribute A may be parents of A. The first attribute in the list has no parent, i.e.

it is the root node of the BN.

The K2 algorithm uses a greedy process to search for the best structure. Given a dataset D with M objects and a VO, it begins as if every node has no parent. Then, beginning with the second attribute from the ordered list (the first one is the root node), its eligible parents are evaluated and those that maximize the whole probability structure are added to the network. The process is repeated for all attributes in order to get the best structure. K2 metric for evaluating each possible parent set to each variable is defined by equation (1):    ? ? ? ???  ? ?  i  i  q  j  ir  k ijk  iij  i X NrN  r iXg  1 1 !

)!1( )!1(),( 	  (1)   where each attribute Xi (i = 1,?,n) has ri possible values {vi1, vi2, ?, viri}. D is a dataset with m objects. Each attribute Xi has as its set of parents 	Xi, and qi is the number of instantiations of 	Xi. Nijk is the number of objects in D, in which Xi has value vik and 	Xi is instantiated as wij (wij represents the j-th instantiation relative to D of 	Xi). Finally, Nij = Nijk.

Having defined the best structure, the network conditional probabilities are determined using a Bayesian estimation of the (predefined) network structure probability.

The VO information may not be available in real world applications [4]. It is important to emphasize that an exhaustive search through all the orderings (n! to a problem with n variables) remains intractable for big problems.

Therefore, the VO is often defined as a random list to the learning algorithms, then, poor results tend to be produced [13]. Thus, some heuristics have been proposed to find a suitable VO. In this paper, an Evolutionary Algorithm (EA) using a specially designed mutation operator is proposed (details will be provided in Section 4).



VIII.RELATED WORK   Several works proposing solutions to search for a suitable VO to BN structure learning can be found in the literature.

Most of them, however, simply use the cannonical genetic algorithm operators and evaluate the obtained results in problems having a small number of variables. Considering  that, the number of optimal VOs depends on the number of variables in the problem and on the density of their relationships, the search for a suitable VO in problems having a small number of variables might be relativelly simple, thus, it might not be worth to use sophisticated techniques such as evolutionary algorithms. On the other hand, when the problem involves a high number of densily connected variables, the search for a suitable VO tends to be a harder problem, therefore being suitable the application of more sophisticate techniques such as evolutionary algorithms.

In the remaider of this section we present works related to the problem of identification of suitable VO for learning Bayesian Networks from data and give a brief overview of their main ideas.

In [4], authors present an algorithm that incorporates an approach based on information theory and another approach based on search and score to induce BNs. The VO information is found by the approach based on tests of conditional independence and then it is transmitted to K2 to induce the network structure.

In Campos and Huete?s work [3], methods to obtain a good approximation to the optimal ordering using only partial information are proposed. More specifically, the authors use only relations of conditional independence of order zero and order one, and search for the VO that best preserves this information. The search process is performed by genetic algorithm and simulated annealing.

The work presented in [18] uses two ?Estimation of Distribution Algorithms? (EDAs) to obtain the optimal VO for the K2 algorithm: UMDA (Univariate Marginal Distribution Algorithm) and MIMIC (Mutual Information Maximization for Input Clustering).

A. Evolutionary Algorithms using K2 for searching VOs In [16] the authors present a methodology to induce BN structures from data. The methodology is based on the search for better VO by using genetic algorithms based on K2. Since the problem of finding an optimal VO resembles the traveling salesman problem (TSP), genetic operators developed for TSP were used in the employed genetic algorithm. The authors in [16] use the Alarm network [2] in their experimental evaluation . None of the best orderings obtained using the searches with genetic algorithms was able to improve the evaluation of an initial ordering applied by the K2 algorithm.

Hsu?s work [14] is aimed at searching for topological VOs and applies the K2 algorithm at each ordering. The experiments have used the Asia [8] and Alarm [2] datasets.

The results obtained were not satisfactory for Alarm.

In [9], the author presents an algorithm named K2GA: a genetic algorithm that uses a modified version of the K2 heuristic to improve search efficiency.  Among the databases used, the Alarm (37 variables) has had less satisfactory results in terms of the quality of the learned structure, thus         revealing that the found VO might not be good enough for that specific dataset.

In [19] and [20], hybrid methods have been proposed.

They combine the theory of EAs and BNs. These methods aim at optimizing the induction of network structures by searching for a suitable VO and are named VOGA and VOGAC. Both methods use canonical genetic operators.

The crossover operator chosen has been OX (Order Crossover). For mutation, it is used an order based operator.

Both methods have presented good results for small and medium databases.



IX. DISTANCE-BASED MUTATION OPERATOR (DMO)   In this paper, a new mutation operator, called Distance- Based Mutation Operator (DMO), is being proposed. DMO has three variables as input: the first is the target chromosome c, the second is the target gene g, and the third is the distance d (see Fig. 1). Thus, based on d, DMO randomly selects a gene g? (g ? g?) in c and performs a permutation using g and g? values. To be more specific, suppose a chromosome c having n genes represented as integers as in [1, 2, 3, ?, n]. When performing a mutation operation using DMO, supose the target gene chosen to be changed is the 4th gene and the distance equals to 2 (g = 4 and d = 2). Then, g? must be selected from gene 2 to gene 6, including these.

Fig. 1. DMO Algorithm.

The initial idea of DMO is to maintain exploration and  exploitation of the EA and achieve greater efficiency without using the crossover operator.  Thus, the search can be guided according to the distance of permutation between two genes. If the distance is large, the permutation of genes can be considered global. Therefore, it may generate new chromosomes in which a variable that was the last one (in the ordered list) might even become the first. Thus, it can drastically change the parent/child tests performed by K2.

On the other hand, if the distance is small, the permutation tends to be local and to generate chromosomes closer to the average population, without deviating sharply from the region of search explored.

The DMO operator scans the search space and seeks for the suitable ordering in a dynamic fashion. A simple  permutation of nearby genes may lead to search for another area of the search space, starting the search in a new region, without drastically changing the generation. Thus, it is possible to search for suitable orderings dynamically avoiding local optima.

In the experiments presented in Section 6, the permutation distance of genes is altered during the course of generations.

The distance decreases according to the evolution of the best individuals, in such a way that the good chromosomes do not suffer abrupt changes in the last generations. This size can be dynamically set, and initially, it is equal to the size of the chromosome and decreases over the generations. It is calculate using the following function:    )( )(  * MinMax  AverageMax Maxactual kk  kk ss  ?  ? ?  (2 )   where sactual is the current distance between two genes of the chromosome; sMax is the maximum distance between two genes (initially it is equal to chromosome size); kMax is the maximum distance among orderings of the current generation; kAverage is the average distance among orderings and kMin is the minimum distance among orderings.

To measure the distance between two orderings, in this work, we employ two possible metrics as described below:   1) Kendall tau distance [6]: it is a metric widely used and  which counts the number of discordant pairs between two lists. The greater the distance, the more dissimilar are the two lists. The Kendall distance is defined as:    ? ?? Pji jiKK },{ 21,21 ),(),(   (3)  where: P is the set of unordered pairs of distinct elements in the lists  1 e 2; 0),( 21, ?jiK , if i and j are in the same order in1 e 2; 1),( 21, ?jiK , if i and j are in opposite order in 1 e 2.

2) Spearman's rank correlation coefficient [17]: it is often thought of as being the Pearson correlation coefficient between the ranked variables. In practice, however, a simpler procedure is normally used to calculate ?. The n raw scores Xi, Yi are converted to ranks xi, yi, and the differences di = xi ? yi between the ranks of each observation on the two variables are calculated. The Spearman?s coefficient ? is given by:  )1(  1 2  ? ?? ?  nn di?  (4)    DMO Algorithm  {Input: chromosome c, target gene g, distance d.} {Output: c?} ----------------------------------------------------------------  1. interval = a set of values from d-g to d+g 2. g' = random(interval) 3. c? = permutation(c, g, g?) /* c' is the new  chromosome where g and g' were permuted. */ 4. end{DMO}         The idea of using two different metrics to measure the distance between two different orderings (during the search for suitable VOs) is to identify how related the obtained empirical results are to a specific distance metric.



X.EXPERIMENTS AND ANALYSIS OF RESULTS   The experiments performed with the DMO operator involved seven knowledge domains that have its characteristics summarized in Table 2. The Alarm domain [2] is well-known in the Bayesian Networks community and has 37 variables. The Synthetic 1, Synthetic 2 and Synthetic 3 domains have 32 variables each and were created using a sampling strategy applied to the structures of Bayesian classifiers described in Fig 2. The Synthetic 50-50, Synthetic 50-100, Synthetic 50-200 and Synthetic 50-300 domains were randomly generated [21] and have 50 variables each, and 50, 100, 200 and 300 arcs, respectively. The structures of these four BNs are shown in Fig 3. The different numbers of arcs were generated to verify the impact of the network density in the task of obtaining optimal orderings.

TABLE 2. DATASET DESCRIPTION WITH DATASET NAME (DOMAIN), NUMBER OF ATTRIBUTES PLUS CLASS (AT), NUMBER OF INSTANCES  (IN) AND NUMBER OF CLASSES (CL).

Alarm Synth  Synth  Synth  Synth 50-50  Synth 50-100  Synth 50-200  Synth 50-300  AT 37 32 32 32 50 50 50 50  IN 10000 10000   0 50000 50000 50000 50000  CL 2 2 2 2 2 2 2 2   Considering that we intend to empirically assess the  impact of using a single specially designed mutation operator (DMO) instead of using both, mutation and crossover canonical operators (as done in VOGA), or using a single canonical mutation operator, the following setings were defined. For each dataset VOGA [19] and VOEA algorithms were executed. VOEA extends the ideas presented in [19] and tries to explore the influence of a single evolutionary operator for searching for suitable VOs.

Here, VOEA was implemented based on eight different approaches, namely VOEA, VOEA_CM-f, VOEA_CM-g, VOEA_DMO-Kendall, VOEA_DMO-Spearman, VOEA_DMO-g, VOEA_DMO-Kendall-rate and VOEA_DMO-Spearman-rate. All these eight different implementations are described in the sequel.

Synthetic 1     Synthetic 2     Synthetic 3  Fig. 2. Bayesian Networks representing Synthetic 1, Synthetic 2 and Synthetic 3 domains. The graphical representations were created using GeNie Software [8].

1) VOEA: this is VOEA algoritm implemented having the  canonical mutation operator based on order as the single evolutionary operator. In addition, the mutation rate has been mainted fixed.

2) VOEA_CM-f: in this implementation VOEA has the  canonical mutation (CM) operator (based on order) as its single operator, but it uses dynamic mutation rate which is calculated by function (5) for each generation:              )( )(  * MinMax  AverageMax Maxactual gg  gg rr  ?  ? ? ,   (5)   where ractual is the value of new mutation rate; rmax is the initial rate provided by user and the g values refer to the population chromosome fitness calculated by g in (1).

3) VOEA_CM-g: it also uses a single canonical mutation  operator (CM), but differently from VOEA_CM-f, its dynamic mutation rate is initialized by the user and it decreases over generation. Empirically, it was decided that the mutation rate from the previous generation is multiplied by 0.9 for each new generation, thus, decreasing the mutation rate in a linear way in each generation.

4) VOEA_DMO-Kendall: this is the VOEA algorithm  having the DMO operator as its single operator. The mutation rate is fixed and the permutation distance of genes is dynamically calculated using  (2) and (3).

5) VOEA_DMO-Spearman: this is the VOEA algorithm  having the DMO operator as its single operator. The mutation rate is fixed and the permutation distance of genes is dynamically calculated using  (2) and (4).

6) VOEA_DMO-g: it also has DMO as its single operator  and a fixed mutation rate, but differently from VOEA_DMO-f, the permutation distance of genes is initially equal to the size of the chromosome and decreases over generations. Empirically, it was decided that the size of the distance is multiplied by 0.9 in each generation.

7) VOEA_DMO-Kendall-rate: In order to estimate the  influence of dynamic mutation rate with the DMO operator, a VOEA_DMO-Kendall-rate has been developed to combine them. Thus, VOEA_DMO- Kendall-rate uses DMO operator as in VOEA_DMO- Kendall and the dynamic mutation rate as in VOEA_CM-f.

8) VOEA_DMO-Spearman-rate: it uses DMO operator  as in VOEA_DMO-Spearman and the dynamic mutation rate as in VOEA_CM-f.

Synthetic 50-50   Synthetic 50-100   Synthetic 50-200  Synthetic 50-300 Fig. 3. Bayesian Networks representing Synthetic 50-50, Synthetic 50-100, Synthetic 50-200 and Synthetic 50-300 domains. The graphical representations were created using Weka Software [21].

The following steps define the experimental methodology for each of the 11 employed datasets in more detail:   1) The experiments involved VOGA using crossover (OX)  and mutation (based on order) canonical operators and all VOEA aforementioned versions.

2) For a given dataset, all the algorithms were assessed in  a number of runs, using the same initial population .

3) The crossover rate has been set to 0.8 and the mutation  rate has been initialized with 0.3. These values have been defined empirically.

4) The Bayesian K2 ?g function? [7] has been adopted as  fitness function.

5) Since the algorithms have stochastic nature, more than  one run is necessary to verify the final solution. For this reason, in the results presented in this paper, for each dataset (except for the Synthetic 50-100, 50-200 and 50-300, due to their high running times), each algorithm was run 35 times. Table 3 and Table 4 present the average Bayesian score (g) and the number of necessary generations to reach the solution, respectively.

Considering the results shown in Table 3, some  observations are possible. Taking into consideration all the assessed domains, the Bayesian score (g function) obtained  with VOEA versions are better than the ones achieved by VOGA (except to the Synthetic 50-50 domain). Thus, as for these domains, the mutation operator alone tended to produce better VOs.  In addition, VOEA_DMO-Kendall, VOEA_DMO-Spearman, VOEA_DMO-Kendall-rate and VOEA_DMO-Spearman-rate obtained better results than VOEA in 5 datasets. It shows a tendency on the DMO operator finding better VOs than canonical mutation operator.

Table 4 revels VOGA converged faster than VOEA versions in 4 out of 7 datasets. VOEA versions have, however, achieved best g values for most of the data sets. It means DMO alone does not seem to allow a faster convergence for the VOs problem. This fact might not be surprising considering that DMO is a mutation operator.

Another interesting observation is that DMO favored network structures very connected where the nodes have many children. Denser network structures tend to have a reduced number of optimal orderings. DMO operator may create chromosomes closer to each other when it restricts the size of permutation distance between two genes. For instance, consider the chromosomes of final generations of network of Fig 4. These chromosomes should not be changed with distance greater than 1. Because it is perceived that the permutation of nearby genes from a chromosome like [1, 2, 3, 5, 4] (with permutation distance equal to 1) may generate the chromosome [1, 2, 3, 4, 5], which contain the optimal ordering to induce the network structure of Fig 4.

TABLE 3. BAYESIAN SCORES (G FUNCTION)..

VOGA VOEA VOEA_ DMO-Kendall VOEA_  DMO-Spearman VOEA_ DMO-g  VOEA_ CM-f  VOEA_ CM-g  VOEA_ DMO-Kendall-rate  VOEA_ DMO-Spearman-rate  Alarm -48349 -48092 -48042 -48493 -48147 -48090 -48043 -48054 -48342 Synth 1 -84823 -84794 -84792 -84806 -84796 -84792 -84794 -84793 -84797 Synth 2 -87834 -87833 -87833 -87834 -87833 -87833 -87833 -87833 -87833 Synth 3 -86415 -86415 -86415 -86416 -86415 -86415 -86415 -86415 -86415  Synth 50-50 -1227998 -1228080 -1228036 -1228126 -1228048 -1228176 -1228176 -1228176 -1228176  Synth 50-100 - 1221791 -1223841 -1221224 -1220121 -1219657 -1221224 -1220579 -1218931 -1221224  Synth 50-200 -597612 -588007 -596169 -596036 -594386 -586418 -585811 -578411 -589208 Synth50-300 -644560 -644078 -635861 -644058 -639799 -644058 -642906 -636604 -640950  TABLE 4. NUMBER OF GENERATIONS TO CONVERGENCE.

VOGA VOEA VOEA_ DMO-  Kendall  VOEA_ DMO-Spearman  VOEA_ DMO-g  VOEA_ CM-f  VOEA_ CM-g  VOEA_ DMO-Kendall-  rate  VOEA_ DMO-Spearman-  rate Alarm 25 54 78 19 39 57 78 73 34  Synth 1 26 44 45 39 41 50 46 43 47 Synth 2 12 20 24 13 21 21 19 24 17 Synth 3 25 32 31 25 30 29 26 32 33  Synth 50-50 21 50 19 13 22 11 11 11 12 Synth 50-  100 20 12 11 20 12 11 12 38 11  Synth 50- 200 19 37 18 19 21 35 49 77 47  Synth50-300 22 15 20 12 21 11 20 26 14              Fig 4. Structure of a hypothetical Bayesian network.



VI. CONCLUSIONS AND FUTURE WORK   This paper introduced a new mutation operator, named Distance-based Mutation Operator (DMO) to compose a ?Variable Ordering Evolutionary Algorithm? (VOEA). This algorithm is based on ideas of VOGA to optimize the process of learning a Bayesian Network from data.

The results obtained in the performed empirical analysis showed that DMO is able to improve the variable orderings quality. In addition, in these datasets, DMO tended to help finding best Bayesian Network structures (better g score) mainly in the bigger and more densely connected domains.

It can be seen as preliminary empirical evidence that more connected network structures might benefit of using a specific genetic operator as DMO which limits the permutation distance as far as the EA goes further in its search.

On the other hand, the use of crossover and DMO operators might be an interesting future line of investigation mainly to see whether such combination of operators might favor Bayesian network structures having many variables but not densely connected. In sparser BN structures, for instance, the permutation of genes of a particular chromosome may not influence the generation of a target ordering. Often, it might be better to combine them with a sequence of genes from another chromosome, using crossover operators. The BN named Synthetic 1 (see Fig. 2) can help in this understanding. It is possible to observe the structure of this network presents several node sets whose orders are not relevant. For example, sequential set formed from ?Node16? to ?Node32? produces the same BN structure in any order. The same idea applies to nodes that are between "Node8" and "Node16". Therefore, for cases like these, the combination of sequence of genes coming from two chromosomes may produce more interesting orderings which are closer to the optimal ones.

We intend to proceed along these lines of investigation by performing a deeper study about variable ordering for the optimization of Bayesian networks learning. New specific operators for crossover and mutation will be developed, and, in addition, different feature ranking measures will be studied to create new fitness functions which can be part of  a multiobjective evolutionary algorithm.

