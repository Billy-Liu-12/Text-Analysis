Optimization and improvement based on K-Means Cluster algorithm

Abstract?K-Means Cluster algorithm is one of important cluster analysis methods of data mining, but through the  analysis and the experiment to the traditional K-Means  cluster algorithm, it is discovered that its cluster result  varies along with the initial selected cluster central point,  and the difference is big. In view of this question, this text  proposed the method of seeking the initial cluster center  embarking from the data object distribution, Moreover in  order to accurately  appraise the cluster result, it also  proposed cluster assessment method based on the data  object. Through analyzes and contrast of the experiment,  the improved cluster algorithm surpasses the traditional  K-Means cluster algorithm, and also can obtain high and  stable classified accuracy.

Keywords-cluster; K-Means cluster algorithm; cluster  central point; assessment method;  1. INTRODUCTION  Recent years as one of important cluster analysis methods of data mining, clustering becomes increasingly a cause for concern. clustering  is assembling a large number of m-dimensional data object (n months) into a k-cluster aggregation (k <n),in order to make the similarity  of objects within the same cluster as great as possible, and the  similarity  of objects  in different clustering?s  as small as possible[1].At present, as the high efficiency of K-Means algorithm for clustering large-scale data, and the ability of dealing with numerical attributes and classification attributes, so it  is  widely used in  market research and data mining field[2]. In traditional K-Means algorithm, the parameter value k is given in advance, and randomly selecting k  data objects from data object clusters as the initial cluster centers, so that clustering results have strong dependence on the initial cluster centers, that is different  initial cluster center will produce different results, this can be proved from this text?s experimental results. It can be seen that the validity of clustering results directly depend on the choice of initial  cluster centers, how to choose suitable initial cluster centers is a problem worthy of studying.

2. TRADITIONAL K-MEANS ALGORITHM  Algorithm thinking [3]: Traditional K-Means algorithm first randomly select k data objects as the initial cluster center, initial each data object to represent the average of a cluster or center. Each of the remaining data objects, according to its distance from the cluster center, from which it is assigned to the nearest cluster, and then re-calculated the average of each cluster. This process is repeated until the convergence criterion function.

Concrete steps: Input: the number of cluster k, containing n data object  data sets.

Output: k cluster.

(1) From the n data objects, randomly select k data  objects as the initial cluster centers; (2)Calculate the distance of each data object to each  cluster center, and assign it to the nearest cluster; (3)The distribution of all data objects is completed;  Re-calculate the center of k cluster; (4) Comparison of the last, respectively, the  corresponding cluster center, if the cluster centers change, to (2), otherwise, to (5);  (5)Output the results of clustering.

K-Means algorithm summarized, including the three  process: The first is to select the initial cluster center; the second is classification of the data objects; third is the adjustment of cluster centers, in which the later two process alternating execution. K-Means algorithm in the advantages and disadvantages: Advantages: (1) simple and fast; (2) In the case of large data sets, algorithms better scalability, time complexity is O(tkn), in which t is the number of iterative algorithm, k is the number of clusters, n is the data number of data objects[4]. Disadvantages: (1) the traditional K-Means clustering algorithm is sensitive to initial clustering center, and different initial centers often  2009 Second International Symposium on Knowledge Acquisition and Modeling  DOI 10.1109/KAM.2009.185   2009 Second International Symposium on Knowledge Acquisition and Modeling  DOI 10.1109/KAM.2009.185   2009 Second International Symposium on Knowledge Acquisition and Modeling  DOI 10.1109/KAM.2009.185     correspond to different clustering results;(2) sensitive to the order of data input; from different initial cluster centers will be different from the results of the cluster and not the same as accuracy;(3)Vulnerable to the impact of noise and isolated points; K-Means algorithm for the noise and isolated points in the data sets is very sensitive, and such data will have a greater impact[5].

3. IMPROVED K-MEANS ALGORITHM  3.1. Improved ideas about the algorithm  The inadequacy of the traditional K-Means algorithm is selecting initial cluster centers slavishly, and the choosing of initial cluster center is the key factor that affects clustering results. When clustering data objects, only to select suitable initial cluster centers, and not to select isolate point as much as possible, and to select a focal point of the selected data object sets for the real distribution can obtain more accurate clustering results, for this reason, this text put forwards a method that optimizes choice of the initial clustering centers. the idea of selecting initial clustering centers in optimized K-Means clustering algorithm is: first of all, rule out the K most possibly isolated points( k data objects that be away from  other objects at the fastest adding Euclidean distance in the clustering data sets); Calculate center of all remaining object sets, select the nearest point of the center as the first initial center ,and carry on the  iteration to find an point that is farthest from the initial central point(the focal point that have been selected no longer participate in the choice of the next center). As the next initial focal point, until you find the farthest of the K initialization center points.

3.2. Improved K-Means clustering algorithm  K-Means Algorithm of the initial cluster center of Optimization described as follows:  Input: the number of cluster k, containing n data object data sets.

Output: k cluster.

(1)Calculate the Euclidean distance of each data object  between others in the clustering data object set; (2)Find out k data objects, which are the largest sum of  the Euclidean distance of each data object between others.

Apart from these k data objects of all objects, the rest record as D(X);  (3)Calculate the center of the D(X), record as Z 0 ;  (4)Find out the data object Z1  from the D(x), which is  the nearest to Z 0 , as the first initial cluster center point;  (5)Find out the data object Z 2  that has the farthest  distance to Z 1  in the D(x) other than Z 1 , as the second  initial cluster center point;  (6)Find out the data object Z 3  that has the farthest  distance to Z 2  in the D(x) other than Z 1 , Z 2 , as the third  initial cluster center point; (7) And so on, can be the first k initial cluster center  point, Z k , that is, Z k  has the farthest distance to Z 1?k  in  the D(x) other than Z 1 , Z 2 , Z 3 ,?, Z 1?k ;  (8)Start with above k initial cluster center points, with the traditional K-Means clustering algorithm, and obtain Clustering results.

Improved K-Means algorithm flow chart shown in Figure 1:   Figure 1 Flow chart of the improved algorithm     4. CLUSTERING EVALUATION METHOD  How to assess the clustering efficiency of clustering algorithm, it has great meaning to those of the old mechanisms, but the key point which to find a reasonable solution is through the result class sign of the clustering analysis. It?s blur correspondingly to the class sign of simple analysis result which has already known. Two different kinds of class sign may be the same in truly situation and vice versa. This will cause the efficiency of clustering assessment analysis is going down. Through analysis and come up with an assessment way based on clustering data content.

The corn theory of the assessment method which based on clustering data content is to ensure the correctly classified number according to the similarity factor of data.

That is to say after a certain kind of data being processed, it should still have the same data items of in a specified data class. Then accumulate all the correct number in each classified data group. The accuracy rate of the clustering classification is the ratio of above number and the total sample number.

Processing Steps as follows: (1)Deal with the classified consequence of those  already known sample data.

(2)Deal with sample data and get the classified result  after clustering analysis.

(3)Suppose there are k classes in those of already  known data sample, get the first classified identification number, then compare the each identification number in this class with the identification number of data object in each kind of CResult, get the object number which is in the  most similarity between CResult and SReult.

(4)Follow this processing way; can get the correct  classified amount of data object in each kind of SResult.

(5)The accuracy rate of this clustering classification is  a ratio of the sum of each kinds of correctly classified data object number and total number of samples.

5. COMPARISON AND ANALYSIS OF EXPERIMENTAL RESULTS  5.1. Experimental Description  Experimental environment: laptop computers, the key configuration: Intel Core Duo CPU@2.0GHz 2.0GHz, 2G RAM.

Experimental Methods: In the myEclipse development platform, realize the traditional K-Means algorithm and the improved K-Means algorithm using Java language.

Cluster analysis of the experimental data sets respectively, and also compare clustering results.

Experimental data: the selection of UCI database Iris, Wine, Pendigits, three groups of data sets as test data sets.

UCI database (http://archive.ics.uci.edu/ml/machine-learning-databases/) is a specialized for testing machine learning, data mining algorithms database. The data in the database has definitude classification, so, this paper clustering evaluation method can accurately evaluate the quality of clustering.

5.2. Experimental results  With the traditional K-Means algorithm and this paper proposed improved K-Means algorithm of optimization the initial cluster center, the results are as follows:  Table 1 the Clustering results of Iris as experimental data set algorithm time the initial  cluster center  Iteration  number  Correct  rate  randomly  chosen  initial  cluster  centers    1 (16, 45, 53) 8 82.00%  2 (25,42, 141) 4 83.33%  3 (37, 59, 96) 12 88.67%  4 (12, 11, 66) 5 76.67%  5 (7, 73, 145) 3 89.33%  average 84.00%  improved  (65, 119,14) 4 89.33%  Table 2 the Clustering results of Wine as experimental data set algorithm time the initial  cluster center  Iteration  number  Correct  rate  randomly  chosen  initial  cluster  centers    1 (67,19, 35 )  5 70.22%  2 (155, 8, 54 ) 7 70.22%  3 (31, 1, 162) 12 74.16%  4 (79, 70, 18) 4 73.60%  5 (54,70, 175) 6 72.00%  average 72.04%  improved  (175,19,81) 10 70.22%       Table 3 the Clustering results of Pendigits as experimental data set  algorithm time the initial cluster center Iteration number  Correct rate  Randomly Chosen Initial Cluster centers  1 (2744,1044,3034,2694,51,1393,2117,2132,3144, 1575) 36 72.33% 2 (2663,376,2512,2739,1144,1318,1475,305,3180,1407) 17 65.93% 3 (2324,1967,1402,1343,2524,2653,2713,2272,1911,11) 24 68.61% 4 (1086,2128,732,1150,1412,1736,2145,3015,176,2473) 22 76.39% 5 (1467,367,1474,2473,1877,3017,476,2727,966,1543) 16 70.41% average 70.51%  improved  (31,154,59,730,3165,1212,509,2607,1543,2817) 20 76.27%  5.3. Experimental Analysis  From the above experimental results: Table 1, Table 2, Table 3, which show that the traditional K-Means algorithm of randomly chosen initial cluster centers, With different initial cluster centers, complete Cluster analysis need different iterations, and the correct rate of clustering is not the same.

When the experimental data set is Iris data sets or Pendigits data sets, the clustering of the highest accuracy rate and the minimum difference between the correct rates in more than 10 percentage points, clustering results with different initial cluster centers more volatility. Application of improved algorithm can be the result of a stable cluster, 3 groups selected data sets, the accuracy rate of Iris data set than average accuracy rate significantly improved, and so does Pendigits data set, but Wine data set a bit lower.

Wine data set for that experiment, why the clustering accuracy rate of the improved algorithm below average?

Analyze characteristics of three test data sets, (1) Iris data set has 150 data objects, together into 3 categories, each data object has four attributes, all attribute value range is (0 ~ 10), relative concentration of the distribution of all data.

(2) Pendigits data set has 3498 data objects, together into 10 categories, each data object has 16 attributes, distribution of such data is scattered. (3) Wine data set has 178 data objects, together into 3 categories; each data object has 14 attributes, some attributes of a larger range of  disparities. Application of decision tree analysis [6] found that attribute 1, attribute 8, attribute 11, three most important attributes for classification, and their range are as follows: (0.34 ~ 5.08), (1.28 ~ 13), (11.03 ~ 14.83), and contribute little to the classification of the attribute is the range of attribute 13 (278 ~ 1680), in the calculation of distance, attribute 13 has played a role in the absolute. The  characteristics of three data sets and experimental results have been the inspiration that it is necessary to standardize the data objects and make data objects non-dimensional, so that is conducive to better clustering results.

Generally speaking, traditional K-Means clustering algorithm is an approach randomly selecting cluster centers, and does not take the distribution of data into account, this has led to completion of cluster analysis with the blind chosen initial cluster centers, there is no guarantee for Clustering results; And the difference is that the improved algorithm is choosing the initial cluster centers in purpose, so that the initial choices of cluster centers reflect the actual distribution of data as much as possible. The results of clustering analysis have strong stability, and clustering results better reflect the real distribution of the actual data.

6. CONCLUSION  This article introduces the general process of the K-Means algorithm, and analyze the impact  of randomly selecting the initial cluster centers on the results of clustering, then  put forward the idea of finding the initial cluster centers from the distribution of data objects, as well as the realization of this algorithm, and gives an accurate method of evaluating the effects of clustering, and through experimental comparison and analysis, we know that improved algorithm can be a higher rate of accuracy and stability. Compared to the traditional K-Means algorithm, the advantages of the improved algorithm is as follows:(1) The choice of initial cluster center more conform with the actual distribution of data sets;(2) Clustering results no longer fluctuate with the choice of the initial clustering Center;(3) the Clustering results have relatively high accuracy and stability;(4) the required number of iterations     for Completing clustering reduced, that is less time-consuming, and high efficiency.

The next step is to solve the problems found in test and to perfect the improved algorithm. At present the inadequacy of the improved algorithm is that the improved algorithm takes Euclidean distance as a measure of similarity between objects. And if some attribute in some data sets has a larger range of values, it will affect the result of classification. Therefore, before clustering in the data set, it is necessary to standardize the data objects and make data objects non-dimensional, in order to make the improved algorithm get more accurate clustering results.

This text mainly focuses on pre-treatment of the cluster data, which is how to carry on data cleaning and data standardization for clustering data sets appropriately.

