

Abstract: Frequent closed itemsets mining has become an  important alternative of association rule mining recently.

CloSET+ is a efficient algorithm to find Frequent closed itemsets without candidate generation.  However, CloSET+ must scan database two times. In order to enhance the efficiency of CloSET+ algorithms and reduce the I/O cost of database scanning in frequent closed itemsets mining, propose a novel algorithm called QCloSET+ which can mining Frequent closed itemsets with one database scanning.

Keywords: Data mining; Association rules; FP-tree; Frequent  itemsets  1. Introduction  The increase of data stored in many organizations has speeded up with the advent of Internet. Although the database technology is keeping on improving and competent of manipulating huge amount of data, what is important is the useful information or implicit knowledge embedded in data. To extract the embedded implicit knowledge from data, many data mining methods have been proposed.

At first, most of the data mining methods focused on mining association rules in transaction databases. A transaction contains a set of data items where there is not a special order between them, and association rules may exist between the items. Thus the methods to tackle this kind of problems are regarded to find transactional association rules. However, the data in some applications are sequential in nature, such as web logs and stock trends. Then it is more valuable to mine sequential patterns in this kind of data.

Mining frequent itemsets requires to discover all the itemsets in a given dataset D which have a support higher (or equal) than a given minimum frequency threshold min supp[1,2,3]. This problem has been extensively studied in the last years. Several variations to the original Apriori  algorithm [1], as well as completely different approaches, have been proposed[4,5,6]. All the algorithms proposed browse bottom-up the huge solution search space, by exploiting various strategies for pruning it. Among these strategies, the most effective regards the exploitation of the downward closure property: if a k-itemset is frequent, then all of its k-1 -subsets have to be frequent as well. On the other hand, if a k-itemset is discovered to be infrequent, then all its supersets will not be frequent as well. Thus, when during browsing we discover an infrequent itemset I, we can safely prune I and all its supersets. Moreover, transactions which do not contain at least a frequent itemset can be deleted from the datasets since they cannot contribute to the support of any frequent itemset.

This property makes it possible to effectively mine sparse datasets, also with very low min supp thresholds.

Sparse datasets in fact contain transactions with weak correlations, and the downward closure property allows to strongly reduce the search space and the dataset itself as the execution progresses. On the other hand, dense datasets contain strongly related transactions, and are much harder to mine since only a few itemsets can be in this case pruned, and the number of frequent itemsets grows very quickly while decreasing of the minimum support threshold. As a consequence, the mining task becomes rapidly intractable by traditional mining algorithms, which try to extract all the frequent itemsets.

Closed itemsets[7] mining is a solution to this problem.

Closed itemsets are a small subset of frequent itemsets, but they represent exactly the same knowledge in a more succinct way. From the set of closed itemsets it is in fact straightforward to derive both the identities and supports of all frequent itemsets. Mining the closed itemsets is thus semantically equivalent to mine all frequent itemsets, but with the great advantage that closed itemsets are orders of magnitude fewer than frequent ones. Using closed itemsets we implicitly benefit from data correlations which allow to strongly reduce problem complexity. Moreover, association rules extracted from closed sets have been proved to be      more concise and meaningful, because all redundancies are discarded.

In [7], the CLOSET algorithm was proposed for mining closed frequent patterns. Unlike Close and Pascal, CLOSET performs depth ?rst, column enumeration.

CLOSET uses a novel frequent pattern tree (FP-structure) for a compressed representation of the datasets. It then performs recursive computation of conditional tables to simulate the search on the column enumeration tree.

CLOSET is unable to handle long biological datasets because of two reasons. First, the FP-tree is unable to give good compression for long rows. Second, there are too many combinations when performing column enumerations.

CLOSET+ [14] is a recent improvement on CLOSET.

However, CloSET+ must scan database two times.

This paper propose a novel algorithm called QCloSET+ which can mining Frequent closed itemsets with one database scanning. So it can reduce the I/O cost of database scanning in frequent closed itemsets mining evidently.

2.  Related works  2.1.  FP-growth: mining AFIS from FP-tree  Han et al. proposed a data structure called frequent pattern tree or FP-Tree [9]. FP-growth mines frequent itemsets from FP-Tree without generating candidate frequent itemsets. FP-Tree is an extension of prefix tree structure. Only frequent items have nodes in the tree. Each node contains the item?s label and its frequency. The paths from the root to the leaves are arranged according to the support of the items with the frequency of each parent is greater than or equal to the sum of its children?s frequency.

The construction of FP-Tree requires two data scans. In the first scan, the support of each item is found. In the second scan, items within transactions are sorted in descending order according to the support of items. If two transactions share a common prefix, the shared portion is merged and the frequencies of the nodes are incremented accordingly.

Nodes with the same label are connected with an item link.

The item link is used to facilitate frequent pattern mining.

In addition, each FP-Tree has a header that contains all frequent items and pointers to the beginning of their respective item links.

Algorithm 1 (FP-tree construction)[9]  Input: A transaction database DB and a minimum support threshold ?.

Output: Its frequent pattern tree, FP-Tree Method: The FP-tree is constructed in the following steps.

1. Scan the transaction database DB once. Collect the set of frequent items F and their supports. Sort F in support descending order as L, the list of frequent items.

2. Create the root of an FP-tree, T, and label it as ?null?, for each transaction in DB do the following. Select and sort the frequent items in transaction according to the order of L.

Let the sorted frequent item list in transaction be [p|P], where p is the first element and P is the remaining list. Call insert_tree ([p|P], T).

Function insert_tree ([p|P], T) {If T has a child N such that N.item-name = p.item-name Then increment N?s count by 1; Else do {create a new node N; N?s count = 1; N?s parent link be linked to T; N?s node-link be linked to the nodes with the same item-name via the node-link structure;} If P is nonempty, Call insert_tree (P, N).

}  FP-growth partitions the FP-Tree based on the prefixes.

FP-growth traverses the paths of FP-Tree recursively to generate frequent itemsets. Pattern fragments are concatenated to ensure all frequent itemsets are generated properly. In this way, FP-growth avoids the costly operations of generating and testing of candidate itemsets.

As pointed out by the authors of FP-Tree, no algorithm works in all situations. The fact holds true for FP-Tree when the dataset is sparse. When the data is sparse, the compression achieved by the FP-Tree is small and the FP-Tree is bushy. As a result, FP-growth would spend a lot of effort to concatenate fragmented patterns with no frequent itemsets being found.

Algorithm 2 (FP-growth: Mining frequent patterns with FP-tree by pattern fragment growth)[9].

Input: FP-tree, and a minimum support threshold ? .

Output: The complete set of frequent patterns.

Method: call FP-growth(FP-tree, null).

Procedure FP-growth(Tree, ?) { if Tree contains a single prefix path P // Mining single prefix-path FP-tree  then for each combination (denoted as ?) of the nodes in the path P do generate pattern ?  ?  ?  with support = minimum support of nodes in ?; else   for  each item ai in Q, in the header of Tree  do { // Mining multipath FP-tree generate pattern ?  = ai ?  ?  with support = ai .support;      construct ??s conditional pattern-base and then ??s conditional FP-tree Tree? ; if Tree? is nonempty then call FP-growth(Tree?, ?); }  2.2. CLOSET+: MINING CFIS from FP-tree  To minie frequent closed itemsets from FP-trees..

CLOSET+[14 ] was proposed. It use the top-down pseudo tree-projection and upward subset-checking for sparse datasets, whereas for dense datasets, the bottom-up physical tree-projection and a compressed result-tree have been adopted. The CLOSET+ algorithm is described as follows.

First, scan TDB once to find the global frequent items and sort them in support descending order. The sorted fre- quent item list forms the f_list.  Second, scan TDB and build FP-tree using the f_list.  Third, With the divide-and-conquer and depth-first searching paradigm, mine FP-tree for frequent closed itemsets in a top-down manner for sparse datasets or bottom-up manner for dense datasets. During the mining process, use the item merging, item skipping, and sub-itemset pruning methods to prune search space. For each candidate frequent closed itemset, use the two-level hash indexed result tree method for dense datasets or pseudo-projection based upward checking method for sparse datasets to do closure checking. Finally, stop when all the items in the global header table have been mined. The complete set of frequent closed item-sets can be found either from the result tree or the output file F.

3. QCloSET+ : Mining frequent closed itemsets with one database scanning  As analyzed above, it is obvious that Constructing FP-tree is the key step in FP-tree based algorithms. FP-tree contains compressed message of frequent itemsets, it is easy to mining frequent itemsets from FP-tree. To construct FP-tree, we must scan data base twice, one for creating 1-itemset and one for build FP-tree. To mine useful information from database effectively, we needs to solve efficiency problem of mining,  when data quantity very large,  efficiency of mining algorithm become the key of mining problem, to increase efficiency of mining algorithm, developing one database scanning algorithm is an effective policy.

In this section we present a algorithm to construct FP-tree with only one database scanning.

First, we divide the datasets into n parts. For example, Every part contains 1000 transactions. Then, for each part of database, we scan transaction data one time,  read data into a memory buffer , create a local FP-tree and merge into  result FP-tree. When the loop finished, the result FP-tree is the global FP-tree.  We need not to create same node link for sub FP-Trees because it is only a local part of database, we create same node link after all sub FP-Trees are merged into one FP-tree.

Algorithm 3 (QFPC: Quick FP-tree Constructing Algorithm)  Input: A transaction database DB and a minimum support threshold ?.

Output: Its frequent pattern tree, FP-Tree Method: The FP-tree is constructed in the following  steps.

{ T=NULL,T1= NULL,T2=NULL  Divide database into n parts averagely. For example, Every part contains 1000 transactions.

For I=1 to n DO {Scan the  part(I) of database, read the transactions  into buffer, Collect the set of items F and their supports.

Sort F in support descending order as L.

Create local FP-tree as T1 FP_Merge(T,T1,T2)  //merge T1 and T2 into T T1=T2,T2=T,T=T1 //swap T,T2 T1= NULL,T2=NULL } Prune the items from T where its support is less then ? Create same node link for T Return T  }  Algorithm 4 (FP-merge FP-tree merging algorithm, do not create head table and node link in result FP-tree )  Input: Two sub FP-trees Output: a merged FP-tree named FP-tree<result> Method: call FP-merge( result, FP-tree1, FP-tree2) Procedure FP-merge(result, FP-tree1, FP-tree2:FP-TREE) { Count the item support number from FP-tree1 and FP-tree2 and sort the items in descending order,insert into head table of result  while FP-tree1 !=NULL, do { retrieve one item train in FP-tree1 from leaf to root; sort the items in descending order of head table n=leaf?s count;  Let the item train be [p|P], where p is the last element and P is the frontal list.

Call merge-insert ([p|P], result, n).

Remove the item train from FP-tree1; }  while FP-tree2 !=NULL, do { retrieve one item train in FP-tree2 from leaf to root; sort the items in descending order of head table      n=leaf?s count; Let the item train be [p|P], where p is the last element and P is the frontal list.

Call merge-insert ([p|P], result, n).

Remove the item train from FP-tree2; } }  Algorithm 5 (merge-insert: insert a items train into FP-tree)  Input: item trains, FP-tree node, count of trains Output: a merged FP-tree Method: call merge-insert (items train, node, support) Procedure merge-insert ([p|P], T, num) {If T has a child N such that N.item-name = p.item-name N?s count= N?s count+num; Else do {Create a new node N; N?s count = num; N?s parent link be linked to T; } If P is nonempty, Call merge-insert (P, N, num); }   To explain the algorithm, we use an example with the transactions shown in Table 1. Let the number of n be 2(Divide database into two pars)  and the minimum support threshold set to 2.  Figure 1 to Figure 9 show the FP-tree creating procedure.

Table 1. An example database DB TID Items DB part Sorted Items T001 c,f, a, m Part 1 f, c, a, m T002 a,f, c  Part 1 f, c, a T003 f, c, a Part 1 f, c, a T004 b,f Part 1 f, b T005 f, c, m,a Part 2 c, f, a, m T006 c, b, p Part 2 c, b, p T007 c,h,f Part 2 c,f,h T008 b,p, c Part 2 c,b,p  r o o t  f :4  c :3  a :3  m :1  b :1  r o o t  c :4  f :2  a :1  m :1  b :2  h :1 p :2     Figure 1. Create sub FP-trees T1(left tree) from DB part1, T2(right tree) from DB part2   r o o t  f : 3  c : 2  a : 2  b : 1  r o o t  c : 1  f : 1  a : 1  m : 1  ( c f a m ) 1   Figure 2. After moving (cfam1)1 from T1 to Result   r o o t  f : 1  b : 1  r o o t  c : 3  f : 3  a : 3  m : 1  ( c f a ) 2   Figure 3. After moving (cfa)2 from T1 to Result      r o o t r o o t  c : 3  f : 3  a : 3  m : 1  ( c f a ) 2 f : 1  b : 1     Figure 4. After moving (cfa)2 from T1 to Result   r o o t  c :3  f :3  a :3  m :1  f :1  b :1  r o o t  c :4  f :2  a :1  m :1  b :2  h :1 p :2     Figure 5. Before merging T1 to Result      r o o t  c : 4  f : 4  a : 4  m : 2  ( c f a m ) 1 f : 1  b : 1  r o o t  c : 3  f : 1 b : 2  h : 1 p : 2   Figure 6. After moving (cfa)2 from T1 to Result   r o o t  c : 5  f : 5  a : 4  m : 2  ( c f h ) 1 f : 1  b : 1  r o o t  c : 2  b : 2  p : 2 h : 1   Figure 7. After moving (cfa)2 from T1 to Result   r o o t  c :7  f :5  a :4  m :2  (c b p ) 2 f :1  b :1  ro o t  h :1  b :2  p :2   Figure 8. After moving (cfa)2 from T1 to Result     r o o t  c : 7  f : 5  a : 4  m : 2  f : 1  b : 1b : 2  p : 2     Figure 9. After Pruning the items h( Its support is less then ?)  4. Experiments and Results  The goal of the experiments is to find out the extent of different dataset properties that could affect the performance of QPFC algorithms and the relative performance compares with other FP-tree based algorithms.

In this section we will evaluate QPFC+FP-growth. and QPFC+CLOSET+ in comparison with three algorithms FP-growth,CLOSET+ and TFP. All the experiments were performed on Pentium 4 1.8GHz PC with 512Mb RAM running on Window XP. Datasets are generated with the data generator by IBM QUEST. FP-growth is provided by the original authors. All programs are complied with the same compiler. Experimental results show that QPFC and QFP-growth are competitive. It is efficient and scalable for mining large databases or data warehouses. See figure 10 to 11.

Figure 10. Memory Comparison of QFP-growth and  FP-growth            0.5 1 1.5 2 2.5 3  Support Threshold(%)  R u n  T i m e ( S e c .

)  CLOSET+with QPFC  CLOSET+   Figure 11. Run time Comparison of CLOSET+ and  CLOSET with QFPC      5. Conclusions  In this paper, we have introduced an efficient implementation of an FP-Tree constructing algorithm and an improved method QFP-growth for constructing frequent patterns from FP-tree. QFPC can create FP-tree with one database scanning, QFP-growth can generate frequent patterns without conditional pattern-base and conditional FP-tree generation, and the frequent patterns are generated in order. Our future research work is to use the method into implementation of SQL-based, highly scalable FP-tree structure, constraint-based mining of frequent patterns, sequential patterns, max-patterns, partial periodicity and other interesting frequent patterns mining.

