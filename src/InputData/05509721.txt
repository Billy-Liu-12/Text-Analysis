Indoor SLAM Based on Composite Sensor Mixing Laser Scans and

Abstract Vision sensors give mobile robots a relatively  cheap means of obtaining rich 3D information of their environ-  ment, but lack the depth information that a laser range finder  can provide. This paper describes a novel composite sensor  approach that combines the information given by an omnidi-  rectional camera and a laser range finder to efficiently solve the  indoor Simultaneous Localization and Mapping problem and  reconstruct a 3D representation of the environment. We report  the results of validating our methodology using a mobile robot  equipped with a 2D laser range finder and an omnidirectional  camera.



I. INTRODUCTION  A key issue in mobile robotics is to give robots the ability  to navigate in an autonomous way in unknown environments  based only on their perception. Thus, a mobile robot must  be equipped with a perception system capable of providing  accurate information of its current location and its surround-  ings, so that the robot is able to reconstruct a reliable and  consistent representation of the environment. There are two  interdependent tasks that any mobile robot has to solve:  localization and mapping. When neither the location of the  robot nor the map are known, both tasks must be performed  concurrently. This problem, known as Simultaneous Local-  ization and Mapping (SLAM), has been largely studied since  the seminal work of Smith and Cheeseman [18], [19], and  is closely related to the development of sensor technology.

Nowadays, laser range finders have replaced sonars when  possible because of its superior efficacy in estimating dis-  tances accurately and their better signal to noise ratio. Many  techniques have been developed to make the most of this  type of sensor for solving the SLAM problem. Since a laser  scan directly provides metric information of the scene, the  localization problem can be stated in terms of an odometry-  based method where the incremental displacement is found  by computing the best rigid transformation that matches two    successive scans. To match two scans it is necessary to link  the individual measurements in one scan with the corre-  sponding measurements in the other scan. This association  can be done either using an intermediate representation of the  laser data (e.g. a polygonal approximation [1]) or directly,  by exploiting the raw data [2].

Gabriela Gallegos gratefully acnowledges her funding by a CONACYT  (Mexico) grant.

Several methods can be found in the literature for 2D  and 3D scan matching. These methods are often categorized  based on their association rule such as feature to feature or  point to point matching. In the feature-based approach [3],  [5], features such as line segments and corners are extracted  from laser scans and then matched against each other. Such  approach requires the identification of appropriate features in  the environment. On the other hand, point to point matching  does not require the environment to be structured or contain  any predefined features.

The Iterative Closest Point [4] (ICP) algorithm is perhaps  the most widely used point to point scan matching method  that works with range sensors. ICP uses a nearest neighbor  association rule to match points, and least squares optimiza-  tion to compute the best transformation between two scans.

Two enhanced methods based on ICP were proposed by Lu  and Milios [6]: the Iterative Matching Range Point (IMRP)  and the Iterative Dual Correspondence (IDC) method. Al-  though ICP and its extensions are fast and in general produce  good results, they are only guaranteed to converge towards  a local minimum and may not always find the correct  transformation. Furthermore, these algorithms suffer from  computational complexity problems when dealing with large-  scale environments because the point to point association  rules they use result in a O(n log(n)) complexity in the  best case (where n is the number of points in a scan). To  overpass these constraints, Diosi and Kleeman proposed the  Polar Scan Matching method [7] which avoids searching  for point associations by simply matching points with the  same bearing. We will discuss this approach in more detail  in Section III.

Despite all the work that has been done to improve  techniques to use lasers to solve the SLAM problem, the use  of 2D lasers alone limits SLAM to planar motion estimation    and does not provide sufficiently rich information to reliably  identify previously explored regions. Vision sensors are a  natural alternative to 2D laser range finders because they  provide richer perceptual information. Many works have  pursued research on vision-based SLAM [9], either relying  on feature-based representations [11] or, more recently, on  a direct approach [10]. However, standard cameras only  have a small field of view (typically between 30? and  40?) and can be easily affected by occlusion. In contrast,   Anchorage Convention District  May 3-8, 2010, Anchorage, Alaska, USA  978-1-4244-5040-4/10/$26.00 2010 IEEE 3519  using a catadioptric camera [21], [22] one can obtain a full  360? view of the environment. Image acquisition with these  omnidirectional cameras has many advantages: it can be done  in real time, it is easier to recognize previously observed  places whatever the orientation of the robot is and it is also  less likely that the robot gets stuck when facing a wall or  an obstacle. Thus, vision sensors provide dense and rich  3D information about the environment. Nevertheless, vision  alone does not provide the depth information that a laser  range finder does, which is crucial for solving the localization  problem.

In this paper we describe a hybrid sensor combining the  advantages of a laser range finder and an omnidirectional  camera. In its formulation, our work is close to Bibers [20].

The major difference is that the process we describe is fully  automated and does not require manual postprocessing by an  operator.

The rest of the paper is organized as follows: in Section II  we describe the experimental testbed used to validate our  methodology and discuss the data acquisition and synchro-  nization process; Section III briefly overviews the Polar Scan  Matching method, while our SLAM approach is presented in  Section IV; in Section V we discuss the merging of omni-  directional images with laser range data to extract vertical  lines and build a 3D representation of the environment; we  end with some concluding remarks in Section VI.



II. EXPERIMENTAL TESTBED  Hannibal (Fig. 1) is our more recent robot from Neobotix  mobile platform (MP-S500). Hannibal is equipped with a    Sick LD-LRS1000 laser, capable of collecting full 360?

data. The laser head revolves with variable frequency from  5Hz to 10Hz and the angular resolution can be adjusted  up to 1.5? at multiples of 0.125?. The laser has a 30m  range. To perform a 360? scan with a resolution of 0.25?, it  was necessary to reduce the frequency of the rotor to 5Hz,  thus obtaining 1400 data points per scan. The perspective  camera is a progressive-scan CCD camera (Marlin F-131B)  equipped with a telecentric lens and a parabolic mirror (S80  from Remote Reality). Careful calibration of the laser and  the camera is required for merging image and laser data.

We used the Matlab Omnidirectional Calibration Toolbox  developed by Mei [13] to estimate the intrinsic parameters  of the camera and the parameters of the parabolic mirror. For  the calibration between the camera and the laser we used the  method described in [12]. Figure 2 shows the projection of  the laser range measurements on the omnidirectional image  after calibration.

Data acquisition and synchronization. Odometry data arrives  at a frequency of 50Hz, omnidirectional images at 10Hz and  laser measurements at 5Hz. Since data from the different  sensors that we use arrive at different frequencies, we im-  plemented a function to synchronize the data as it comes out  from the robot.

Fig. 1. Hannibal robot experimental testbed  Fig. 2. Laser data projected on an omnidirectional image after calibration.



III. POLAR SCAN MATCHING  Polar Scan Matching (PSM) [7] is a point to point laser  scan matching method that exploits the natural representation  of laser scans in a polar coordinate system to reduce the  complexity of the matching process. As other scan matching  approaches, like the Iterative Closest Point (ICP) method, the  PSM method finds the pose of a laser scan with respect to a  reference scan by performing a gradient descent search for  the transformation that minimizes the square error between  corresponding points. In contrast to other matching methods,  PSM avoids an expensive search for corresponding points by  matching points with the same bearing. The method assumes  the reference and current scans are given as sequences of  range and bearing measurements of the form {rri,?ri}ni=1  and {rci,?ci}ni=1, respectively, and requires an initial estimate  (xc,yc,?c) for the pose (position and orientation) of the    current scan in the reference scan coordinate frame. The  method may be best described by describing each of its  phases:  a) Preprocessing: To remove outliers and increase the  robustness of the method, the filter developed by A.Victorino  in [23] is first applied to both scans. The measurements  in each resulting scan are then classified into segments  according to simple criteria: two consecutive measurements  not further than a threshold or three measurements lying ap-   proximately on the same polar line are assigned to the same  segment. Segments consisting of a single point are discarded  (most mixed pixels). To aid the segmentation process, the  maximum range is limited so that two consecutive readings  belonging to the same segment cannot be too far apart.

b) Projection: To compute the error in the pose esti-  mate of the current scan the method needs to know how  the current scan would have been measured from the point  of view of the reference scan. The projection of the current  scan into the reference scan coordinate frame is a sequence  of measurements (r?ci,? ?ci)ni=1 computed as follows  r?ci =  ?

(rci cos(?c + ?ci)+ xc)2 +(rci sin(?c + ?ci)+ yc)2 (1)  ? ?ci = atan2(rci sin(?c + ?ci)+ yc,rci cos(?c + ?ci)+ xc) (2)  The bearings of the above sequence do not necessarily  coincide with bearings where the laser would have sampled a  reading. A range measurement r??ci is computed for each sam-  ple bearing by linear interpolation among points belonging  to a same segment. Points that would have been occluded are  not taken into account, only the smallest range measurement  for a bearing is kept.

c) Translation and Orientation Estimation: The  method alternates between translation and orientation  estimation. After making a correction to the pose estimate,  the projection phase is repeated with the corrected estimate.

The process stops when the magnitude of the last position  and orientation correction is smaller than a given threshold,  hopefully indicating that a minimum has been reached.

Translation is estimated using a standard weighted least  squares method. A correction (?xc,?yc) to the position  estimate is found by minimizing the weighted sum of the    square range residuals ?ni=1 wi(rri ? r??ci)2 while leaving  orientation unchanged. The weights are computed as  recommended by Dudek and Jenkin [8],  wi =  c2  (rri? r??ci)2 + c2  (3)  Orientation is estimated by computing the average range  residual for 1? shifts of the current scan in a 20? window.

The new orientation estimate is found by fitting a parabola  to the shift with the minimum average error and its left and  right neighbors.

The implementation of the PSM method provided by Diosi  is tailored to a laser with 1? angular resolution and 180?

bearing range. These assumptions are used when transform-  ing sample bearings from radians to indexes into arrays and  back. We generalized Diosis implementation to lift these as-  sumptions. Our implementation is parametrized so that it can  deal with lasers with arbitrary angular resolution and bearing  range. In addition, instead of just returning the pose estimate  at the moment the algorithm stops, our implementation keeps  record of the estimate with the minimum error and returns  it as a result.



IV. LOCAL AND GLOBAL MAPS WITH SLAM  AFFINE-TRANSFORMATION(x,y,? )  return  ?

??

cos? ?sin? 0 x  sin? cos? 0 y  0 0 1 0  0 0 0 1  ?

??

Fig. 3. Affine transformation for a translation (x,y) and a counterclockwise  rotation around the origin by an angle ? .

GLOBAL-MAP(scan[N])  1 SR ? scan[1]  2 T1 ? AFFINE-TRANSFORMATION(SR.x,SR.y,SR.? )  3 T3 ? T1TL  4 Map? APPLY-TRANSFORMATION(T3,SR)  5 for i? 2 to N    6 SC ? scan[i]  7 T2 ? AFFINE-TRANSFORMATION(SC .x,SC.y,SC .? )  8 T ? T?1L T?11 T2TL  9 (x,y,? )? (T(1,4),T(2,4),atan2(T(2,1),T(2,2)))  10 (x,y,? )? PSM(SR,SC,x,y,? )  11 T ?3 ? AFFINE-TRANSFORMATION(x,y,? )  12 T3 ? T3T ?3  13 Map?Map?APPLY-TRANSFORMATION(T3,SC)  14 SR ? SC  15 T1 ? T2  16 return Map  Fig. 4. Pseudocode of the procedure used to incrementally build a global  map from a sequence of laser range scans with odometry information.

We build 2D local maps of the environment using the  enhanced PSM implementation described in the previous  section. Local maps will be used both, in the localization  process and for mapping the environment. Later, these maps  will be used in SLAM to reconstruct a 2D global map from  which it is possible to recover the pose of the robot at each  instant.

Let TL be the rigid transformation between the laser  coordinate frame and the robot coordinate frame. We fix as a  global coordinate frame the coordinate frame of the odometry  data. Let (x,y,? ) be the current position of the laser scan  coordinate frame. The affine transformation matrix from the  laser coordinate frame to the global coordinate frame is given  by the procedure in Fig. 3.

We use the procedure in Fig. 4 to build a global map  and reconstruct the path of the robot from a sequence of  laser range scans with associated odometry data. We begin  by taking the first scan in the sequence as the reference scan  SR. Initially, the map consists only of the points in the scan  SR represented in the global coordinate frame, but it will  be incrementally enriched at each iteration of the loop. We  keep at every moment a transformation matrix T3, from the  coordinate frame of the laser in the reference scan frame  to the global coordinate frame. At the beginning of each  iteration we take the next scan in the sequence to update the  current scan SC. We then use the odometry data to obtain an  initial estimate for the pose of the laser in the current scan  with respect to the reference scan coordinate frame. We feed  this estimate to the PSM procedure described in the previous     section, and get as a result a new estimate of the pose. Using  this new estimate, we update the T3 matrix, transform the  points in the current scan to the global coordinate frame,  and add them to the global map. The current scan becomes  then the reference scan and the whole process is repeated  again.

Because the short-term odometry of the robot when trav-  eling on a flat surface is relatively accurate, in practice we  do not need to use scan matching to compute the pose of  the robot in every scan. Instead, we only use scan matching  to get a better estimate of the pose of the robot when it has  traveled a certain distance or rotated a certain angle, or when  a certain lapse of time has passed since the last time scan  matching was used.

Using the results obtained using the PSM algorithm, the  odometry data of the whole sequence can be recomputed.

It suffices to multiply after each iteration matrix T3 by the  transformation matrix T?1L , which gives the transformation  matrix from the robot (not the laser) coordinate frame of  the current scan to the global frame. The pose (x,y,? )  can be readily extracted from this last matrix. Figure 5  shows the position of the robot at several instants in the  sequence as given by the original odometry data (in red)  and as computed by SLAM (in green) superimposed on the  generated map. The sequence was obtained by manually  commanding the robot to explore the ground floor of a  building in a closed loop. Note that although we did not  perform closed-loop detection or corrections of any kind,  the results are quite satisfactory. The recomputed odometry  represents a big improvement over the original odometry that  even drifts out of the building.



V. VERTICAL LINE EXTRACTION FROM  OMNIDIRECTIONAL IMAGES AND LASER SCANS  This section explains the procedure we developed to  extract vertical lines from omnidirectional images and to  estimate their 3D positions using information from the laser  range finder. We first project the laser information on the  omnidirectional image in order to get an approximation of the  depth information missing in the image. To achieve that, the  unified projection model defined in [15] is applied, which is  an extension of Geyers [17] and Barretos [16] models. The    generalized camera projection matrix K is computed from  the generalized focal lengths (?1,?2) and the principal point  (u0,v0):  K =  ?

?

?1 0 u0  0 ?2 v0  0 0 1  ?

?

Using K, we can compute the normalized coordinates of a  point p in the image (represented in the camera coordinate  frame) as m = [x,y,1]T = K?1p. We then compute X s =  [Xs,Ys,Zs] as follows (see Fig 6):  X s =  ?

????

?+?1+(1?? 2)(x2+y2)  x2+y2+1 x  ?+?1+(1?? 2)(x2+y2)  x2+y2+1 y  ?+?1+(1?? 2)(x2+y2)  x2+y2+1 ? ?

?

????

p  X  Xs  ~zm  ~xs  ~zs  ~ymRm  Rp  Cm ~xm  ~ys  Cp  K  ?m  ?

m    ?p  Fig. 6. Unified projection model  Fig. 7. Detection of vertical lines and the corresponding laser measure-  ments.

where ? is the mirror parameter, which is equal to 1 for  parabolic mirrors.

We then extract the quasi-radial lines in the scene, cor-  responding to approximately vertical features (e.g. walls,  facades, doors, windows). As we set the camera-mirror  system perpendicular to the floor where the robot moves, we  can guarantee that vertical lines are approximately mapped to  radial lines on the camera image plane. To extract prominent  vertical lines, we first apply the Canny edge detector to obtain  a binary edge image and then apply the Hough transform to  detect lines in the binary image. To extract vertical lines we  compute the image center (i.e, where all radial lines intersect  in) using a circle detector, and filter out the lines detected  by the Hough transform that do not lie on radial directions.

As shown in Fig. 7, by overlapping in the omnidirectional  image the laser scan data and the radial lines we can find  the laser range measurements corresponding to vertical lines.

This gives us the depth information missing. We detect those  laser measurements and save them in the original camera  frame together with its corresponding point in the image  plane (which also corresponds to a point on a vertical line).

Fig. 5. Global map obtained by SLAM together with the original and recomputed position of the  robot at several key instants.

Let Ms0 = [X s0 ,Y s0 ,0]T be a laser measurement lying on  a vertical line expressed in the camera coordinate frame,  ? a 3D plane defined in the camera frame, and msi =  [xsi ,ysi ,zsi ]T , i = 1,2 the endpoints of the vertical line where  the laser measurement lies expressed in the sphere (mirror)  coordinate frame. These last points are computed by invert-  ing the projections of the unified model of Fig. 6.

We reconstruct the 3D lines as follows. Let us be the  director vector. For every Msi ? ?, the vector  ????

Ms0Msi is  colinear to us. Thus,  ????

Ms0Msi = ?ius =?

??

?

Xi?X s0 = ?iusx  Yi?Y s0 = ?iusy  Zi?Zs0 = ?iusz  (4)  ???

OMsi = i  ????

Osmi =?

??

?

Xi = ixsi  Yi = iysi  Zi = izsi  (5)  Substituting (5) in (4) we get the following system of  equations ??

?

ixsi ?X s0 = ?iusx  iysi ?Y s0 = ?iusy  izsi ?Zs0 = ?iusz  (6)  If ? is a vertical plane in the sphere frame Rs, i.e. us =  [0,0,1]T , then: ??

?

ixsi ?X s0 = 0  iysi ?Y s0 = 0  izsi ?Zs0 = ?i  (7)  Because we know [xsi ,ysi ,zsi ]T and [X s0 ,Y s0 ,0]T , we can com-  pute i for each i. We can then substitute in Equation (5) to  obtain the extreme points of the lines in ?. Finally, we apply  the homogeneous transformation to transform the coordinates  of those points to the global coordinate system and trace the  3D lines. The result is shown in Fig. 8. Observe how the  vertical lines are consistent with the 2D map.

Fig. 8. Environment with 3D lines

VI. DISCUSSION AND PERSPECTIVES  This paper describes an original composite sensor ap-  proach that takes advantage of the information given by  an omnidirectional camera and a laser range finder to ef-    ficiently solve the Simultaneous Localization and Mapping  problem for indoor environments, and to reconstruct a 3D  representation of the environment. The accompanying video  illustrates the incremental generation of a 2D map and the  estimation of the robot trajectory alongside the laser range  data projected on omnidirectional images. It also shows the  vertical lines detected in the images and their mapping into  a 3D reconstruction of the environment.

In order to show the robustness of the methodology, we  tested the algorithm with a sequence taken in a different in-  door environment with our old robot Anis which is equipped  with the same catadioptric camera and an AccuRange 4000  2D laser range finder. This laser is composed of a laser  telemeter with a rotating mirror that allows measurements  of points on 360?, except for an occlusion cone of ap-  proximately 30? caused by the assembly of the mirror. The  resulting 2D map is shown in Figure 9. The vertical line  extraction and the reconstruction of the 3D environment were   Fig. 9. Global Map obtained by SLAM in Borel Building  consistent as well.

The SLAM problem has been solved using many different  approaches, however some important problems need to be  addressed that are often directly linked to the sensors used.

Laser range finders cannot help in evaluating the translation  of a robot moving in a straight line in a corridor. Mapping in  dynamic environments is also hard with only laser data. On  the other hand, using visual sensors alone introduces issues  such as propagating correctly the scale factor, initializing  the range when using a monocular sensor, and merging data  when using multiples cameras.

In our approach, the laser provides metric information of  the environment that helps to fix a scale factor (removing  the difficulty of propagating the scale factor) without the  need to use multiple cameras. Throughout the paper we  have identified several advantages of combining laser and  visual sensors. Our experimental results are encouraging and  give us valuable insight into the possibilities offered by this  composite sensor approach.

We have considered several research directions that could  be pursued to improve the results obtained so far. We have  thought about extending our algorithm with loop closure de-    tection. This would allow the algorithm to detect previously  visited locations and improve the accuracy of mapping and  the precision in the estimation of the robot pose. Being able  to detect previously visited places is of great importance to  solve the problem of global localization and to recover the  robot from kidnapping, a situation occurring when the robot  is displaced by something out of its control (e.g. taking an  elevator, being transported from one location to another).

Therefore, solving the loop closure problem will not only  improve SLAM performance, but will as well enable new  capabilities.

Further work will concentrate on an extension of the  PSM algorithm to exploit the information about vertical lines  detected using omnidirectional images. Segmentation of the  ground (floor) to extract planes on the image would allow  a dense (textured) 3D reconstruction by warping the images  onto the geometric model of the world. Finally, we believe  the general approach can be extended to solve the full six  degrees of freedom (6DOF) SLAM problem, which is an  active field of research.

