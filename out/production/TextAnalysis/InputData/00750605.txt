New Algorithms for E?cient Mining of Association Rules?

Abstract  Discovery of association rules is an important data mining task. Several algorithms have been proposed to solve this problem. Most of them require repeated passes over the database, which incurs huge I/O over- head and high synchronization expense in parallel cases. There are a few algorithms trying to reduce these costs. But they contains weaknesses such as of- ten requiring high pre-processing cost to get a vertical database layout, containing much redundant compu- tation in parallel cases, and so on. We propose new association mining algorithms to overcome the above drawbacks, through minimizing the I/O cost and ef- fectively controlling the computation cost. Experi- ments on well-known synthetic data show that our algorithms consistently outperform Apriori , one of the best algorithms for association mining, by factors ranging from 2 to 4 in most cases. Also, our algo- rithms are very easy to be parallelized, and we present a parallelization for them based on a shared-nothing architecture. We observe that the parallelism in our parallel approach is developed more su?ciently than in two of the best existing parallel algorithms.

Keywords: data mining, association rule, frequent itemset, parallel processing.

1 Introduction  Discovery of association rules is an important prob- lem in the area of data mining. The problem is in- troduced in [2], and can be formalized as follows. Let I = fi1; i2; ? ? ? ; img be a set of literals, called items .

LetD be a collection of transactions, where each trans- action d has a unique identi?er and contains a set of items such that d ? I. A set of items is called an itemset , and an itemset with k items is called a k- itemset . The support of an itemset x in D, denoted as ?(x=D), is the ratio of the number of transactions (in D) containing x to the total number of transactions in D. An association rule is an expression x) y, where x; y ? I and x \ y = ;. The con?dence of x ) y is the ratio of ?(x[y=D) to ?(x=D). We use minsup and minconf to denote the user-speci?ed minimum support and con?dence respectively. An itemset x is frequent if ?(x=D) ? minsup. An association rule x ) y is  strong if x [ y is frequent and ?(x[y=D)?(x=D) ? minconf.

The problem of mining association rules is to ?nd all strong association rules.

?This work was partially supported by ARC Large Research Grant (1996-98) A849602031.

Since it is easy to generate all strong association rules from all frequent itemsets, almost all current studies for association discovery concentrate on how to ?nd all frequent itemsets e?ciently . Our paper will also focus on this key problem. Several algorithms [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12] have been proposed in the literature to solve this problem. However, most of them are based on the Apriori approach [1] and require repeated passes over the database to deter- mine the set of frequent itemsets, thus incurring high I/O overhead. In the parallel case, most algorithms perform a sum-reduction at the end of each pass to construct the global counts, also incurring high syn- chronization cost.

Some algorithms [11, 12] have been introduced to overcome the above drawbacks. However, they con- tains other weaknesses. First, they use a vertical database layout and assume that it can be available immediately. But in practice, a transaction database usually have a horizontal layout. Thus, to get a ver- tical layout, they need to build a new database with the same size. Clearly, this pre-processing step costs too much for a very large database and may increase their costs greatly. Second, they attach a tid-list on each candidate (potentially frequent itemset) in or- der to compute supports for its supersets. But, for a large database and a large amount of candidates, the space cost for storing tid-lists of all candidates is very huge and might not ?t in main memory. Third, in parallel cases, they generates clusters of candidates and then partition these clusters into di?erent proces- sors. However, lots of these clusters are overlapping and many candidates may occur in multiple and dif- ferent clusters. Thus, on one hand, these candidates will be counted for getting their supports at multi- ple processors so that much redundant computation is produced; on the other hand, to avoid synchroniza- tion cost, a large part of database has to be copied to multiple processors, which is not e?ective in both time and space costs. In short, parallelism can not be fully developed in these algorithms.

In this paper, we propose new e?ective association mining algorithms to overcome all the above draw- backs. First, they are more e?cient than Apriori by reducing I/O cost greatly and controlling computa- tion cost e?ectively. Second, they are based on the standard horizontal layout database and do not have any pre-processing step. Third, they are very suit- able for parallelization and parallelism is easy to be fully developed for them. We present a framework of our algorithms in Section 2; describe our algorithms in Section 3; study their performances experimentally in Section 4; propose a parallelization for them in Sec- tion 5; and conclude the paper in Section 6.

2 A Framework  We use L to denote the set of all frequent itemsets, and Lk to denote the set of all frequent k-itemsets. As mentioned before, the key problem of association dis- covery is to generate L. Almost all current algorithms require to compute supports for a set of candidates C, and L is formed by selecting all itemsets with sup- port exceeding minsup from C. Among them, Apriori [1] is one of the best because it minimizes the size of C: only those itemsets whose subsets are all frequent are included in C. However, to realize that, it has to employ a levelwise approach: at each level k, it needs a scan over data to count support for each c 2 Ck, where Ck is a superset of Lk; next it generates Lk from Ck and then use Lk generate Ck+1; after that, it can start to work for the next level k+1. Clearly, this approach contains high I/O costs and is not suitable for parallelization.

To obtain a low I/O expense and a high parallelism, a straightforward idea is to design a method as follows: generate a candidate set C (? L) at one or two times instead of uncertainly multiple times like Apriori . If we can ?nd such a method, we only need to use one or two scans over data to obtain all necessary sup- ports and then generate L immediately. Clearly, this method has a high parallelism because we can parti- tion not only the candidate set C but also the database into multiple processors to do support-computation in parallel. It is very easy to see that such a method does exist. The simplest one is to take all itemsets (i.e., all subsets of I) as candidates. Clearly, this naive method is extremely ine?cient due to the exponential size 2jIj  of the candidate set C. To design e?cient methods, we need to control the size of C.

Let c be an itemset. We use Pk(c) to denote the set of all k-subsets of c. Now we propose a faster method as follows: ?rst ?nd L1 and L2; next use L2 to gen- erate Call = fc ? I j P2(c) ? L2; jcj ? 3g, i.e., all k-itemsets (k ? 3) whose 2-subsets are all frequent are regarded as candidates; then count supports for all c 2 Call; ?nally generate all frequent k-itemsets (k ? 3) by checking supports for all c 2 Call. Actu- ally, this approach is very similar to ClusterApr al- gorithm proposed in [12]. The only di?erence is that ClusterApr uses a vertical database layout.

We assume that c is the maximal candidate (in size) in Call. Thus, the size of Call is at least in the order of O(2jcj), as all k-subsets (k ? 3) of c must also be in Call. Hence, on the condition that jcj is small, the above method runs e?ciently because of the limited computation cost and the minimized I/O cost. How- ever, when jcj becomes large, the computation cost increases greatly due to the huge size of Call so that the algorithm might break down.

To avoid this exponential bottle neck, now we pro- pose a framework which is based on the above method but counts support for only a small part of candidates in Call when the size of Call is huge. In practice, it is not wise to generate all candidates in Call, as it requires not only lots of time but also huge space.

An alternative method is to generate and store those maximal candidates called item cliques . An itemset c 2 Call is an item clique if any superset of c is not in Call. We always use Q to denote the set of all item cliques, Qi to denote the set of all item cliques with a  ?xed size i, maxcliqsize to denote the size of the max-  imal item clique. Clearly, we have Q = Smaxcliqsize  i=3 Qi, and Call = fc j c ? cliq; cliq 2 Q; jcj ? 3g.

An Complete Partition of Items (CPI in short) is de?ned as a list of non-overlapping itemsets such that the union of them contains all items. Let I0 ? In?1 be a CPI. By de?nition, we have (1) Ii \ Ij = ; for  i 6= j and i; j 2 f0 ? (n ? 1)g, and (2) Sn?1  i=0 Ii = I.

Furthermore, we say that this CPI has size n and item clusters (clusters in short) I0 ? In?1. An itemset c is called an intra-CPI itemset if c is a subset of any of I0 ? In?1; otherwise, c is an inter-CPI itemset.

The brief idea of our method is as follows. First, we generate L1, L2 and Q, and then use Q to produce an e?ective CPI. Second, we count supports only for all intra-CPI candidates in Call, and then all frequent intra-CPI itemsets can be obtained. We use Cintra to denote the set of all those intra-CPI candidates, i.e., Cintra = fc j c ? cliq; cliq 2 Q; jcj ? 3; c is intra- CPI g. Third, we build a set of inter-CPI candidates Cinter = fc j c ? cliq; cliq 2 Q; jcj ? 3; c is inter-CPI, all intra-CPI subsets of c are frequentg. It is easy to see that Cinter ? CallnCintra contains all frequent inter-CPI k-itemsets for k ? 3 because any subset of an frequent itemset must also be frequent. Therefore, ?nally, we can count supports for all c 2 Cinter , and all frequent inter-CPI itemsets can be obtained.

Clearly, this method is not only correct but also more e?cient because of Cintra [Cinter ? Call. Since jCallj is exponential (in the size of the maximal item clique), we should try to generate an e?ective CPI to keep both sizes of Cintra and Cinter under control.

We introduce a threshold maxcansize for controlling the size of Cintra. We say that a candidate c 2 Call is an oversize candidate if the size of c is greater than maxcansize. By setting maxcansize, we try to gener- ate a CPI such that Cintra contain as few oversize candidates as possible. Since a large maxcansize may imply a very huge Cintra and a small one may pro- duce a very huge Cinter , in practice, we often choose a medium-sized maxcansize (e.g., 8, 9 or 10) to keep both jCintraj and jCinter j reasonable. Based on the premise that jCintraj has been controlled not to be ex- tremely large by usingmaxcansize, we should let Cintra contains as many candidates as possible to reduce the size of Cinter greatly. To realize that, we can not only keep the CPI size as small as possible but also let each item clique be related to as few clusters as possible.

To sum up, we conclude the framework of our method by four phases as follows.

? Initialization: Generate all frequent 1-itemsets and 2-itemsets. Use all frequent 2-itemsets to generate all item cliques (potential maximal fre- quent itemsets).

? CPI Generation: Use all item cliques and a pre- speci?ed maxcansize to generate a CPI, based on the following principles: (1) let Cintra contain as few oversize candidates as possible; (2) keep the CPI size as small as possible; (3) let each item clique be related to as few clusters as possible.

? Intra-CPI Mining: Use all item cliques to gen- erate Cintra. Count supports for all c 2 Cintra and generate all frequent intra-CPI itemsets.

? Inter-CPI Mining: Use all item cliques and fre- quent intra-CPI itemsets to get Cinter . Count supports for all c 2 Cinter and get all frequent inter-CPI itemsets.

3 Algorithms  In this section, we ?rst design algorithms for com- pleting the above four phases respectively and then describe our algorithms for mining frequent item- sets. For simplicity, we always assume that I = f0; 1; ? ? ? ; N?1g, and use the form of hx0; x1; ? ? ? ; xn?1i to denote an itemset c such that c = fx0; x1; ? ? ? ; xn?1g and x0 < x1 < ? ? ? < xn?1.

3.1 Initialization  In this phase, we need to ?rst generate L1 and L2, and then use L2 to generate all item cliques. Simi- lar to [5], we use a one-dimensional array and a two- dimensional array to speed up the process of obtain- ing L1 and L2. This simple process runs very fast, since no searching is needed. After obtaining L2, we can use a method presented in [12] to generate all item cliques. To give a brief description, we ?rst in- troduce some useful concepts. Let x be a frequent item. The equivalence class of x, denoted by [x], is de?ned as [x] = fy j hx; yi 2 L2g. The cov- ering set of [x], denoted by [x]:cvset, is de?ned as [x]:cvset = fy 2 [x] j [x]\([y]n[z]) 6= ; for any z 2 [x]g.

The algorithm is as follows, please refer to [12, 7] for more details.

Algorithm 3.1 The item clique generation algo- rithm: GEN CLQ Input: (1) I = f0 ? (N ? 1)g, the set of all items; (2) L2, the set of all frequent 2-itemsets. Output: Q3 ? Qmaxcliqsize, where maxcliqsize is the size of the maximal item clique and Qi is the set of all item cliques with size i.

Algorithm GEN CLQ for each x 2 I do [x] = fy j hx; yi 2 L2g; for each x 2 I do [x]:cvset = fy 2 [x] j [x] \ ([y]n[z]) 6= ; for any z 2 [x]g; for (x = N ? 1;x ? 0;x??) do  // Generate all item cliques [x]:cliqlist = fhxig; for each y 2 [x]:cvset do  for each cliq 2 [y]:cliqlist do if cliq = [x] then mark cliq with remove; if cliq \ [x] 6= ; then insert (fxg[(cliq\[x])) into [x]:clqlist such that 6 9A;B 2 [x]:clqlist; A ? B;  for each x 2 I do // Put all item cliques in order  for each cliq 2 [x]:cliqlist without mark remove do  insert cliq into Qi, where i is the size of cliq;  maxcliqsize = the size of the maximal item clique;  3.2 CPI Generation  In this phase, we need to use all item cliques and a pre-speci?ed maxcansize to generate a CPI I0 ? In?1, based on the following principles: (1) let Cintra con- tain as few oversize candidates as possible; (2) keep the CPI size n as small as possible; (3) let each item clique be related to as few clusters as possible. Clearly, if maxcansize ? maxcliqsize, we can easily put all items into one cluster such that the generated CPI contains only one cluster I. In this case, all above principles have been satis?ed. Now we consider the opposite case, maxcansize < maxcliqsize.

We use cpisize to denote the size of the desired CPI.

Let cliq be the maximal item clique. Based on the ?rst principle, we hope that all intra-CPI subsets of cliq have sizes no greater than maxcansize. To meet this requirement, we should have at least maxcliqsize?1  maxcansize + 1  clusters in the CPI because each cluster can contain at most maxcansize items in cliq. To satisfy the second principle, we simply set cpisize = maxcliqsize?1  maxcansize +1 in our  method.

Let x be an item. We use x:cls 2 f0 ? (cpisize?1)g  to indicate that x is an item in cluster Ix:cls. We use IQ to denote the set of all items occurring in Q. Thus, our main task is to generate x:cls for each x 2 I.

After initially setting all x:cls = unknown, we can use a scan of Q to assign a value to x:cls for each x 2 IQ. Certainly, given an item clique cliq 2 Q, we only need to generate x:cls for each x 2 cliq with x:cls = unknown. We introduce such a procedure as follows.

Procedure assigncluster(cls: an cluster, cliq: an item clique)  Aunknown = fx 2 cliq j x:cls = unknowng;  if A0 = ; then return; // Each item x 2 cliq has been put in some cluster  for (i = 0; i < cpisize; i++) do Ai = fx 2 cliq j x:cls = ig; Choose k from 0 ? cpisize? 1 such that jAkj = maxfjA0j ? jAcpisize?1jg; if jAunknownj < jAkj then cls = k; for each x 2 Aunknown do x:cls = cls;  In this procedure, we ?rst partition the given cliq into Aunknown; A0; ? ? ? ; Acpisize?1 based on Ai = fx 2 cliq j x:cls = ig. Let Ak be the maximal partition (in size) among A0 ? Acpisize?1. If jAunknownj ? jAkj, we simply set x:cls = cls for each x 2 Aunknown, where cls is an input parameter value. However, when jAunknownj < jAkj, it is easy to see that there are quite a few items in cliq belonging to cluster Ik and the size of Aunknown is rather small. In this case, we put all elements of Aunknown into cluster Ik to maximize the number of inter-CPI subsets of cliq and let cliq be related to as few clusters as possible (the third princi- ple).

Now, by a scan of Q and calling assigncluster(cls, cliq) for each cliq 2 Q, we can generate x:cls for all x 2 IQ. We let cls have an initial value 0. During the scan over Q, if we call assigncluster(cliq1, cls) to pro- cess cliq1, we always call assigncluster(cliq2, (cls+ 1) mod cpisize) to process the next clique cliq2, where mod refers to the module operator. It is easy to see that our scan of Q should not start from any item    clique with size greater than maxcansize, because oth- erwise oversize intra-CPI candidates will be generated immediately. Our strategy is to scan Q in the follow- ing order:  Qmaxcansize ! Qmaxcansize?1 ! ? ? ? ! Q3 !

Qmaxcansize+1 ! ? ? ? ! Qmaxcliqsize,  where item cliques within each Qi for all i 2 f3 ? maxcliqsizeg are kept in lexicographic order. Clearly, after ?nishing scans for Q3 ? Qmaxcansize, most items in IQ usually have been assigned into some clusters.

Thus, the following scans forQmaxcansize+1 ? Qmaxcliqsize do not tend to generate oversize intra-CPI candidates in the CPI. In addition, by starting from Qmaxcansize, we try to let each item clique be related to as few clus- ter as possible to meet the third principle mentioned before.

After obtaining x:cls for all x 2 IQ, we propose two methods adjustcluster(I) and adjustcluster(II) to adjust these x:cls's and try to make Cinter contain as few oversize candidates as possible. We will describe these methods later. When we ?nish all the above work, it is easy to see that we have InIQ = fx 2 I j x:cls = unknowng. Note that all these items in InIQ  are uninteresting for us, because they won't occur in any intra-CPI or inter-CPI candidate that concerns us.

With this observation, we simply set x:cls = 0 for all x 2 InIQ, and thus all I0 ? Icpisize?1 can be obtained by scanning x:cls for each x 2 I. Now we can present the description of our algorithms for CPI generation as follows, where GEN CPI(X) denote the algorithm that calls adjustcluster(X) to do cluster adjustment and X may be either I or II.

Algorithm 3.2 The CPI generation algorithms: GEN CPI(I) and GEN CPI(II).

Input: (1) Q = Smaxcliqsize  i=3 Qi, the set of all item cliques; (2) maxcansize, a pre-speci?ed threshold for controlling the size of Cintra. Output: I0 ? Icpisize?1, a CPI.

Algorithm GEN CPI(X) // X 2 fI, IIg for each x 2 I do x:cls = unknown; cls = 0; cpisize = maxcliqsize?1  maxcansize + 1;  for (i = maxcansize; i ? 3; i??) do for each cliq 2 Qi do assigncluster(cls, cliq) and cls = (cls+ 1) mod cpisize;  for (i =maxcansize+1; i ?maxcliqsize; i++) do  for each cliq 2 Qi do assigncluster(cls, cliq) and cls = (cls+ 1) mod cpisize;  if X=I then adjustcluster(I) else adjustcluster(II); for (i = 0; i < cpisize; i++) do Ii = ;; for each x 2 I do  if x:cls = unknown then I0 = I0 [ fxg else Ix:cls = Ix:cls [ fxg;  Procedure adjustcluster(X) // X 2 fI, IIg for each x 2 I do x:adj = 0; for each cliq 2 Q do  for each A satisfying A is a maximal oversize intra-CPI subsets of cliq do  A0 = fx1 ? xng = fx 2 A j x:adj = 0g and A1 = fx 2 A j x:adj = 1g;  for (i = 1; i ? n; i++) do  if (jA1j+ i ? maxcansize) then xi:adj = 1 else xi:adj = 2;  if X=II then cpisize++; for each x 2 I with x:adj = 2 do  if X=II then x:cls = cpisize? 1 else x:cls = (x:cls+ 1) mod cpisize;  It is easy to see that the main procedure of our al- gorithm GEN CPI(X) employs the idea mentioned be- fore. Now we explain the adjustcluster(X) procedure, where X may be I or II. We initially set x:adj = 0 for all x 2 I, which means x has not been found in any oversize intra-CPI candidate. Then, we use a scan over Q to process each maximal oversize intra-CPI candi- date A. For each A, we will set either x:adj = 1 or x:adj = 2 for all x 2 A with x:adj = 0, with the goal of making the size of A1 as close to maxcansize as pos- sible, where A1 = fx 2 A j x:adj = 1g. After complet- ing the processing for all those A's, we move all x 2 I with x:adj = 2 to a new cluster. If X=I, x will be moved to the next cluster by setting x:cls = (x:cls+1) mod cpisize. If X=II, cpisize will be increased by 1 and then x will be moved to the new cluster Icpisize?1. The di?erence of these two methods is as follows: the ?rst one tries to satisfy the third principle, while the sec- ond one tries to satisfy the ?rst principle. It is easy to see that any old oversize intra-CPI candidate can be broken into two smaller intra-CPI candidates after this adjustment.

3.3 Intra-CPI Mining  In this phase, we will ?rst use the obtained CPI and all item cliques to generate Cintra; then count sup- ports for all candidates in Cintra; and ?nally generate all frequent intra-CPI subsets. First, we study the data structure for storing candidates. The hash-tree structure introduced in [1] can only be used to count supports for a set of candidates with the same size, and so is not suitable for our problem. Here, we use a pre?x-tree T to store a set of candidates with di?er- ent sizes. Let r be the root of T . Each node t 2 T contains four ?elds: t:item, t:cn, t:son[1 ? t:cn] and t:sup. We use t:item to store an item, except that r:item stores nothing. We use t:cn to store the num- ber of children of t, and use t:son[1 ? t:cn] to store the addresses of all its children. We always maintain the following property: t:item < t:son[1]:item < ? ? ? < t:son[t:cn]:item. Furthermore, each tn 2 T corre- sponds to a unique candidate, denoted by tn:can, such that tn:can = ht1:item; ? ? ? ; tn:itemi and (r; t1; ? ? ? ; tn) forms exactly a path (or pre?x) from root r to the current node tn. We use t:sup store the support of t:can.

Constructing and updating such a pre?x-tree is easy, and so we simply use instree(T , c) to denote the process of inserting an candidate c into a pre?x-tree T .

Note that when we insert a candidate hx1; ? ? ? ; xni into T , any its pre?x hx1; ? ? ? ; xmi for m ? n will also be regarded as a candidate in T . For example, Figure 1 shows a pre?x-tree generated by the following candi- date set: Cexam =f h1; 2; 5i; h1; 2; 6; 7i; h1; 2; 6; 7; 9i; h1; 2; 6; 8i; h1; 4; 9i; h1; 8; 9i; h2; 5; 8i; h3; 5; 9i; h3; 6; 7i; h3; 6; 7; 8; 9i; h3; 6; 7; 9i; h3; 7; 9i g. Note that h3; 6; 7; 8i is regarded as a candidate in the tree, though it is not in Cexam. Now we can present the following proce- dure to generate supports for all candidates stored in      5 6    2 4 8            root    Figure 1: An example of a pre?x-tree  pre?x-tree T .

Procedure CountSup(T : a pre?x-tree, D: a transaction database)  Set t:sup = 0 for each t 2 T and let r be the root of T ; for each transaction d 2 D do Let hx1 ? xni be the itemset containing exactly all frequent items in d; IncSup(r, hx1 ? xni);  Procedure IncSup(t: a pre?x-tree node, hx1 ? xni: an itemset)  t:sup++; if n = 0 then return; if 9i 2 f1 ? t:cng, t:son[i]:item = x1 then IncSup(t:son[i], hx2 ? xni); for (i = 2; i ? n; i++) do IncSup(t,hxi ? xni);  Based on the de?nition of the pre?x-tree, it is easy to see the correctness of the above method. Now we can present our algorithm for Intra-CPI mining as fol- lows.

Algorithm 3.3 The Intra-CPI mining algorithm: INTRA MINER.

Input: (1) Q, the set of all item cliques; (2) I0 ? Icpisize?1, a CPI; (3) L1 and L2, the sets of all fre- quent 1-itemsets and 2-itemsets; (4) minsup, a support threshold; (5) D, a transaction database. Output: Lintra, the set of all frequent intra-CPI itemsets.

Algorithm INTRA MINER Cintra = fc j c ? cliq; cliq 2 Q, jcj ? 3; c is intra-CPIg; Let T be an empty pre?x-tree and for each c 2 Cintra do instree(T , c); CountSup(T;D); Collect all frequent intra-CPI k-itemsets for k ? 3 into Lintra by a scan of T ; Lintra = Lintra [ L1 [ fx 2 L2 j x is intra-CPIg;  This algorithm is easy to understand. First we use Q to generate Cintra. Next, all candidates in Cintra are inserted into a pre?x-tree T . Then, we call CountSup(T;D) to count supports for all candidates in T . Finally, we can generate Lintra by a scan of T , L1 and L2. The correctness of the algorithm is straightforward.

3.4 Inter-CPI Mining  In this phase, we ?rst use all item cliques and all frequent intra-CPI itemsets to generate Cinter , next  count supports for all c 2 Cintrasuball and then generate all frequent inter-CPI k-subsets for k ? 3. The algo- rithm is as follows.

Algorithm 3.4 The Inter-CPI mining algorithm: INTER MINER.

Input: (1) Q, the set of all item cliques; (2) I0 ? Icpisize?1, a CPI; (3) Lintra, the set of all frequent intra-CPI itemsets; (4) L2, the set of all frequent 2- itemsets; (5) minsup, a support threshold; (6) D, a transaction database. Output: Linter, the set of all frequent inter-CPI itemsets.

Algorithm INTER MINER Cinter = fc j c ? cliq; cliq 2 Q; jcj ? 3, c is inter-CPI, all intra-CPI subsets of c are in Lintrag; Let T be an empty pre?x-tree and for each cliq 2 Q do instree(T , c); CountSup(T;D); Collect all frequent inter-CPI k-itemsets for k ? 3 into Linter by a scan of T ; Linter = Linter [ fx 2 L2 j x is inter-CPIg;  This algorithm is easy to understand. First we use Q and Lintra to generate Cintra. Next, all candidates in Cinter are inserted into a pre?x-tree T . Then, we call CountSup(T;D) to count supports for all candi- dates in T . Finally, we can generate Linter by a scan of T and L2. The correctness of the algorithm is straight- forward.

3.5 Algorithm Description  Now we can present our algorithms for ?nding all fre- quent itemsets as follows.

Algorithm 3.5 The frequent itemset mining algo- rithms: MINER(I) and MINER(II).

Input: (1) I, the set of all items; (2) D, a transaction database; (3) minsup, a support threshold; (4) maxcan- size, a pre-speci?ed threshold for controlling the size of Cintra. Output: L, the set of all frequent itemsets.

Algorithm MINER(X) // X 2 fI, IIg Use array technique to generate L1 and L2;  GEN CLQ; // Get Q = Smaxcliqsize  i=3 Qi if maxcliqsize ? maxcansize then  Call = fc j c ? cliq; cliq 2 Q; jcj ? 3g; Let T be an empty pre?x-tree and for each c 2 Call do instree(T , c); CountSup(T;D); Collect all frequent k-itemsets for k ? 3 into L by a scan of T ; L = L [ L1 [ L2;  else GEN CPI(X); // CPI generation INTRA MINER; // Lintra generation INTER MINER; // Linter generation L = Lintra [ Linter;  This algorithm is also easy to understand. First, we use array technique to generate L1 and L2. Next, we call GEN CLQ to generate all item cliques. If max- cliqsize ? maxcansize, we know that the size of Call is    limited. Hence, we use a pre?x-tree T to store all can- didates in Call and then count supports for them. Af- ter that L can be obtained immediately. If maxcliqsize > maxcansize, we call GEN CPI(X) to do CPI gener- ation, call INTRA MINER to obtain Lintra, call IN- TER MINER to generate Linter, and ?nally we have L = Lintra [ Linter. The correctness of the algorithm is straightforward.

4 Performance Study  We have implemented our algorithms MINER(I) and MINER(II) for evaluating their perfermances. In both implementations, we set maxcansize = 10. For com- parison, we have also implemented two Apriori ver- sions. One version denoted by ApriHash uses hash- tree structure suggested by [1] to store candidate sets.

The other version denoted byApriPref uses our pre?x- tree structure to store candidate sets. To assess the performances of MINER(I), MINER(II), ApriHash and ApriPref , we performed several experiments on a SUN ULTRA-1 workstation with 64 MB main mem- ory running Sun OS 5.5. To keep the comparison fair, we implemented all the algorithms using the same ba- sic data structures, except that ApriHash used a dif- ferent hash-tree structure to store candidate sets. In addition, for a fair comparison, all our test results do not contain the execution time for generating L1 and L2, because Apriori uses a much slower method to get L1 and L2 than our array technique.

The synthetic datasets used in our experiments were generated by a tool described in [1]. We use the same notation Tx:Iy:Dz to denote a dataset in which x is the average transaction size, y is the average size of a maximal potentially frequent itemset and z is the number of transactions. In addition, we generated all datasets by setting N = 1000 and jLj = 2000, where N is the number of all items, jLj is the number of maximal potentially frequent itemsets. Please refer to [1] for more details on the dataset generation.

Figure 2 shows the experimental results for four synthetic datasets to compare the performances of these algorithms. We observe that both our algo- rithms MINER(I) and MINER(II) outperform both ApriHash and ApriPref , by factors ranging from 2 to 4 in most cases. However, the di?erence between MINER(I) and MINER(II) is minor.

In theory, there are three reasons for that our meth- ods outperform the Apriori approach. First, our methods only need at most 2 scans of databases for obtaining all frequent itemsets with size greater than 2, while Apriori requires uncertainly multiple scans.

Hence, the I/O cost in our methods is usually much smaller than that in Apriori . Second, we design ef- fective methods to make the size of our candidate set reasonable so that the computation cost is also well- controlled. Third, using pre?x-tree structure to store candidates also has two advantages: (1) its space re- quirement is very low; (2) the support-increment op- erations can be made at each visited node to reduce the computation cost, while these operations can only be made at leaf nodes when hash-tree is used. To sum up, with these advantages, our algorithms are more e?cient than Apriori .

5 Parallelization  Since mining frequent itemsets requires lots of com- putation power, memory and disk I/O, it is not only interesting but also necessary to develop parallel al- gorithms for this data mining task. Here, we study how to extend our algorithm MINER(X) for paral- lelization, where X 2 fI, IIg. We assume a shared- nothing architecture, where each of n processors has a private memory and a private disk. The processors are connected by a communication network and can communicate only by passing messages.

The main idea of our parallelization is as follows.

First, the transaction database D is evenly distributed on the disks attached to the processors. We use Di to denote the set of transactions at processor Pi, for all i 2 f1 ? ng. Let c be an itemset. The local sup- port count of c at processor Pi refers to the number of transactions containing c in Di. We generate all required candidates and count their local supports at each processor. All the obtained local support counts will be sent to all other processors. Thus, when a pro- cessor completes its local support-counting and also receives all other local support counts from other pro- cessors, the global support counts for all candidates can be accumulated and all frequent itemsets can be obtained at this processor.

Now we introduce some concepts to help our dis- cussion. Let T1 and T2 be two pre?x-trees. We say that (1) T1 is a subtree in shape of T2, denoted by T1 v T2, if for each t1 2 T1 there exists t2 2 T2 such that t1:item = t2:item and the position (or occur- rence) of t1 in T1 is the same as that of t2 in T2; (2) T1 and T2 are in the same shape if T1 v T2 and T2 v T1.

We introduce the following procedure to do support accumulation for T1 and T2 in the same shape and save the result in T1.

Algorithm AccumSup(T1, T2) for each t1 2 T1 do  Let t2 2 T2 such that the positions of t1 in T1 and t2 in T2 are the same; t1:sup = t1:sup+ t2:sup;  Before giving our parallel solution, we ?rst paral- lelize INTRA MINER and INTER MINER as follows, where we assume that all required input data can be available.

Algorithm PAR INTRA MINER // A parallelization of INTRA MINER  for i = 1 to n at processor Pi do in parallel  Cintra = fc j c ? cliq; cliq 2 Q, c is intra-CPI and jcj ? 3g; Let Ti be an empty pre?x-tree and for each c 2 Cintra do instree(Ti, c); CountSup(Ti, Di) and send Ti to all other processors; Receive Tj 's from all other processors Pj 's; // Synchronization point for j = 2 to n do AccumSup(T1, Tj);  // T1 ? Tn are in the same shape Collect all frequent intra-CPI k-itemsets for k ? 3 to Lintra by a scan of T1; Lintra = Lintra [ L1 [ fx 2 L2 j x is intra-CPIg;              Minimum Support (in %)  T5.I2.D100K.data  ApriHash ApriPref  MINER(I) MINER(II)  T im  e( se  c)  0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05         Minimum Support (in %)  T10.I4.D100K.data  0.25 0.150.45 0.40 0.35 0.200.300.60 0.500.55  T im  e( se  c)  ApriHash ApriPref  MINER(I) MINER(II)        Minimum Support (in %)  T10.I6.D100K.data  ApriHash ApriPref  MINER(I) MINER(II)  T im  e( se  c)  0.60 0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.150.55            Minimum Support (in %)  T20.I6.D100K.data  ApriHash ApriPref  MINER(I) MINER(II)  0.951.00 0.90 0.85 0.75 0.65 0.550.600.700.80  T im  e( se  c) Figure 2: Performance comparison  Algorithm PAR INTER MINER // A parallelization of INTRA MINER  for i = 1 to n at processor Pi do in parallel  Cinter = fc j c ? cliq; cliq 2 Q, jcj ? 3; c is inter-CPI, all intra-CPI subsets of c are in Lintrag; Let Ti be an empty pre?x-tree and for each cliq 2 Q do instree(T , c); CountSup(Ti;Di) and send Ti to all other processors; Receive Tj 's from all other processors Pj 's; // Synchronization point for j = 2 to n do AccumSup(T1, Tj);  // T1 ? Tn are in the same shape Collect all frequent inter-CPI k-itemsets for k ? 3 to Linter by a scan of T1; Linter = Linter [ fx 2 L2 j x is inter-CPIg;  Both algorithms above employ the same idea. First each processor Pi for all i 2 f1 ? ng independently inserts all intra-CPI or inter-CPI candidates into a lo- cal pre?x-tree Ti. Next, Pi uses local transaction data Di to obtain the local supports for all candidates and sends the result tree Ti to all other processors. Then, it waits for receiving all Tj 's from all other processors Pj 's. After all T1 ? Tn are available. Pi does support accumulation for all candidates and store the global supports into T1. Finally, Lintra or Linter can be gen- erated easily at each Pi. It is not hard to see that the above procedure is complete. Thus we can present the parallelization of MINER(X) as follows.

Algorithm PAR MINER(X) // X 2 fI, IIg for i = 1 to n at processor Pi  do in parallel Count local supports for all 1- itemsets and 2-itemsets; send the results to all other processors; re- ceive local supports from all other processors; accumulate all local supports to get global supports; and then generate L1 and L2;  GEN CLQ; // Get Q = Smaxcliqsize  i=3 Qi if maxcliqsize ? maxcansize then  for i = 1 to n at processor Pi do in parallel  Call = fc j c ? cliq 2 Q; jcj ? 3g; Let Ti be an empty pre?x-tree and for each c 2 Call do instree(Ti, c); CountSup(Ti, Di) and send Ti to all other processors; Receive Tj 's from all other processors Pj 's;  // Synchronization point for j = 2 to n do AccumSup(T1, Tj);  // T1 ? Tn are in the same shape Collect all frequent k-itemsets for k ? 3 into L by a scan of Ti; L = L [ L1 [ L2;  else for i = 1 to n at processor Pi do in parallel GEN CPI(X);  // Get a CPI PAR INTRA MINER;  // Lintra generation at P1 ? Pn PAR INTER MINER;  // Linter generation at Pi ? Pn for i = 1 to n at processor Pi do in parallel L = Lintra [ Linter;    It is easy to see that PAR MINER(X) is a natural parallelization of MINER(X) on a shared-nothing ar- chitecture by using our main idea mentioned before, where X may be I or II. This parallelization uses a simple principle of allowing \redundant computations in parallel on otherwise idle processors to avoid com- munication", which is also employed by Count Dis- tribution algorithm [3]. Note that Count Distribu- tion is the fastest parallelization of Apriori . However Count Distribution contains uncertainly multiple syn- chronization points due to the sum-reduction at the end of each pass to construct the global counts. This fact greatly limits the development of parallelism. Our PAR MINER(X) has overcome this drawback because it contains at most 3 synchronization points. Further- more, di?erent from the methods in [11], all of our database partitions are non-overlapping, which guar- antees there is no redundant support-counting opera- tion in our method. Hence, our method develops the parallelism more su?ciently than both the above ex- isting methods.

6 Conclusions  We have proposed new algorithms for e?cient mining of association rules. Di?erent from all existing algo- rithms, we introduce a concept of CPI (Complete Par- tition of Items) and divide all itemsets into two types: intra-CPI and inter-CPI. After obtaining all frequent 1-itemsets and 2-itemsets, we generate and maintain a set Q of item cliques (maximal potentially frequent itemsets). Furthermore, we have designed two meth- ods for generating an e?ective CPI by using Q. Then, we can use Q and our CPI to get a set Cintra of intra- CPI candidates and count supports for them so that all frequent intra-CPI itemsets can be obtained. Fi- nally, we use Q, our CPI and all frequent intra-CPI itemsets to generate a set Cinter of inter-CPI candi- dates and also count supports for them so that all frequent inter-CPI itemsets can be obtained.

Our algorithms have several advantages. First, their I/O costs are quite limited because they only require at most 3 scans over database. Second, they can make both sizes of Cintra and Cinter reasonable so that their computation costs are also e?ectively controlled. Third, they use a pre?x-tree structure to store candidates, which can also reduce computation cost. As a result, they appear to be more e?cient than Apriori , one of the best algorithms for associ- ation discovery. To con?rm that, We have done the experiments to compare the performances of our algo- rithms together with Apriori . The test results show that our algorithms outperform Apriori consistently, by factors ranging from 2 to 4 in most cases.

Another advantage of our algorithms is that they are easy to be parallelized. We have also presented a possible parallelization for our algorithms based on a shared-nothing architecture. We observe that the parallelism can be developed more su?ciently in our parallelization than two of the best existing parallel algorithms.

