

Abstract?Laser Radar imagers can be designed to provide 2-D or 3-D images of scenes.  The 2-D imagers generally possess superior spatial resolution while providing a rough estimate of the range to the target.  Newly developed 3-D systems possess the ability to form images of the scene as well as range to every pixel in the scene.  The spatial resolution of these systems is typically many times less than those of the 2-D systems.  It is the goal of this work to present a statistical method for fusing 2-D and 3-D LADAR imagery.  The method is used on simulated LADAR sensor data in order to demonstrate its potential for improving both the range resolution and spatial resolution over what could be achieved alone from either sensor.

TABLE OF CONTENTS  1. INTRODUCTION.................................................................1 2. SENSOR MODELS..............................................................1 3. PROPOSED SENSOR FUSION ALGORITHM.........................3 4. RESULTS............................................................................4 5. CONCLUSIONS ...................................................................5 6. REFERENCES .....................................................................5 7. BIOGRAPHY.......................................................................6  1. INTRODUCTION LADAR (LAser Detection And Ranging) systems have potential applications across a wide range of fields including machine vision and long range reconnaissance.

The spatial resolution of LADAR imaging systems is ultimately limited by either the size of the aperture used to collect the light, the atmosphere in betw1een the sensor and the target or ratio of the pixel pitch to the focal length of the imaging system.  In modern 3-D LADAR imagers it is the pixel pitch of the image acquisition array that is generally the limiting factor.  2-D LADAR imagers are designed with denser focal planes than their 3-D counterparts.  This is due to the fact that the 2-D imaging arrays can be fabricated with more pixels per unit2 area because there are not other hardware components on the chip used in the high speed read out and integration function needed by the 3-D system.

Because 2-D imagers can obtain high spatial resolution while 3-D imagers possess ranging capability at low spatial resolution a pote3ntial sensor fusion opportunity arises  1 U.S. Government work not protected by U.S. copyright.

2 IEEEAC paper #1549, Version 3, Updated January 8, 2009   involving the fusion of these two types of sensors.  It is the goal of this research to fuse data from a 2-D sensor and a 3- D sensor in order to obtain both higher spatial resolution and better range accuracy.  Section 2 of this paper describes the sensor models used to simulate the LADAR data used to test the proposed sensor fusion technique presented in Section 3.  Section 4 of this paper shows results obtained on simulated data using the proposed sensor fusion technique.

Section 5 of this paper is used to discuss conclusions drawn from these results.

2. SENSOR MODELS The sensor models used in this study break down into two different categories.  The first category is the 2-D LADAR model.  This model relates the perceived intensity of the target as a function of position in detector plane, A(x,y), to the measured image in the 2-D focal plane array camera [1].

?? = =  ??= N  y  N  x ywxzhyxAwzi  1 1 ),(),(),(  (2.1)  In equation (2.1) the point spread function h(x,y) is convolved with the amplitude of the target to produce the intensity in the focal plane i(z,w).  The indices (x,y,z,w) refer to pixels in the focal plane array.  This simple model doesn?t take into account the radiometry of the scenario.

The scene amplitude, A(x,y), is the perceived intensity at the sensor receiver.  This paper is not concerned with estimating reflectivity of the target or other target parameters aside from the range to the target from the LADAR receiver.

The second sensor model is for the 3-D LADAR system.

This model contains elements of the 2-D LADAR model but adds the ranging component of the sensor.  The received pulse is modeled as a Gaussian.

)),((  ),(),,( ?  ??  yxrt  k  k  eyxAtyxp ?  ? = (2.2)  The received pulse p(x,y,tk) is a function of the range, r(x,y), as a function of position in the scene.  It is also a function of the intensity of the scene.  The observed intensity in the 3-D LADAR system contains the effects of diffraction.  The intensity of the scene in the 3-D LADAR system, I(z,w,tk) is       computed via equation (2.3).

?? = =  ??= N  x  N  y kk yLvxLuhtyxptvuI  1 1 ),(),,(),,(    (2.3)  In this equation tk represents the times at which the LADAR return is measured by the 3-D sensor.  L is the ratio between the pixel pitch of the 3-D sensor divided by the pixel pitch of the 2-D sensor.  This parameter captures the fact that the 3-D sensor possesses inferior spatial resolution to that of the 2-D sensor.

The statistical model for the measured data for both the 2-D and 3-D LADAR systems assumes the noise in the measurement is governed by Poisson statistics [2].  If all the measurements, d1(z,w), are statistically independent from each other, then the joint probability of the 2-D LADAR data is given by equation (2.4).

?? = =  ?  = N  z  N  w  wziwzd  wzd ewzidP  1 1 1  ),(),(  1 )!,( ),()(   (2.4)   The joint probability of all the measurements taken by the 3-D sensor, d2(u,v), is computed via equation (2.5).

??? = = =  ?  = M  u  M  v k k  tvuItvud k  tvud etvuI  dP kk  1 1   1 2  ),.,(),.(  2 )!,,( ),,(  )(  (2.5)   The parameter M is the number of pixels in each dimension of the 3-D imaging focal plane.  In this paper it is assume that the ratio of the dimension of the 2-D array over the 3-D array dimension is equal to L.  This makes M=N/L.  The specific simulations generated to provide data for this research use a simple target model shown in Figure 1.  The 3-D sensor is assumed to measure 20 samples in range.  The target has uniform reflectivity, thus uniform amplitude A(x,y).   The target region is represented with  40,000 pixels in a square region that is 200 by 200 pixels in size.  The simulated data generated in this paper uses a ratio of  L=5 for the number of pixels in the 2-D imaging sensor  over the number of pixels in the 3-D imaging sensor.  This ratio is consistent with the pixel pitch ratio between a typical 3-D sensor (100 micro-meters) and that of a typical 2-D sensor that might have a (20 micro-meter) pixel pitch.

Figure 1: Target board model used to generate simulated data.  The front surface in white is at range sample number 7 in the gate while the darker regions are at a range of 11 samples in the range gate.

Because of the target?s uniformity in reflectivity and the fact that it is further assumed that the illuminating laser provides a uniform pattern of illumination at the target surface, the 2- D data set is also uniform.  The 3-D observation of the target contains 20 images.  Figures 2 and 3 contain examples of what some of these images look like when viewing the target board shown in Figure 1.

Figure 2: Image of the target board taken with the 3-D sensor at a range equal to that of the front surface of the target board.  Notice how the loss of spatial resolution causes some of the bars to be significantly less visible.

Figure 3: Image of the target board taken with the 3-D sensor at a range equal to that of the back surface of the target board.  Notice how the loss of spatial resolution causes some of the bars to be significantly less visible.

The images shown in Figures 2 and 3 possess 40 by 40 pixels, while the target board is simulated with 200 by 200 pixels.  This is due to the fact that the detector array under samples the images formed on it due to its large pixel pitch.

Figure 4 shows an image of the target board at a range corresponding to the front of the board.

Figure 4: Image of the target board at a range of 7 samples in the range gate if the 3-D sensor had a pixel pitch of 20 micro-meters.  The 100 micro-meter pixel pitch causes a significant loss of spatial resolution as seen in Figures 2 and 3.

Figure 5 shows the impulse response of the imaging system that is used for the function h(x,y).  The blurring associated with this impulse response is due in part due to the diffraction effects of the optical system.  The other contribution to the blurring is the spatial extent of the pixel integrating the light on the detector array.  It is assumed in this work that the array possesses a fill factor of 100 percent.

Figure 5: Spatial impulse response of the 3-D imaging sensor.

3. PROPOSED SENSOR FUSION ALGORITHM The proposed algorithm is derived as a gradient ascent algorithm designed to maximize the log-likelihood of the 3- D LADAR data.  The log-likelihood of the 3-D data is computed from the joint probability of the data shown in equation (2.5).

??? = = =  ?= M  u  M  v  K  k kkk tvuItvuItvudrQ  1 1 1 2 ),,()),,(ln(),,()(  (3.1)  In order to maximize Q with respect to the range r we will differentiate Q with the respect to p in order to compute the gradient of the log-likelihood with respect to the pulse shape as shown in equation (3.2).  With the gradient computed, a strategy will be devised for moving in the direction of the gradient in order to ascend the log- likelihood function.

),()1 ),,( ),,((  ),,( )(  1 1  oo  M  u  M  v k  k  koo  yLvxLuh tvuI tvud  tyxp rQ ???=  ? ? ??  = =  (3.2)  Here we can identify two components of the gradient that will be denoted as Q+ and Q- which are defined in equations (3.3) and (3.4) respectively.

),( ),,( ),,(  ),( 00 1 1  2 yLvxLuh tvuI tvud  yxQ M  u  M  v k  k oo ??=??  = =  +  (3.3)  ),(),( 00 1 1  yvxuhyxQ M  u  M  v oo ??=??  = =  ?   (3.4)  ),(),( ),,(  )(  oooo  ko  yxQyxQ tyxp  rQ ?+ += ?  ? (3.5)         Since the gradient component in the denominator appears with a negative sign in front of it in equation (3.2), it represents the propensity for the overall gradient to be negative. At the same time the numerator, being strictly positive and appearing in (3.2) as a positive component to the gradient, represents the positive part of the gradient.  By forming a ratio of the positive part of the gradient over the negative part of the gradient and using this ratio to update the function, p, it becomes possible to form an update for the pulse function.

),( ),,( ),,(),,(),,(  1 1  oo  M  u  M  v k  k koo  old koo  new yLvxLuh tvuI tvudtyxptyxp ??= ??  = =  (3.6)  With the new pulse function, p, estimated from the data by conducting a fixed number of iterations of equation 3.6, (100 iterations is used in the results shown in Section 4) the range is estimated from the new pulse for each pixel (xo,yo).

The range estimation algorithm is accomplished via a correlation algorithm that shifts the reference waveform in equation (2.2) and calculates the correlation between the waveform and the new pulse, pnew(x,y,tk) as function of that shift.  The shift that produces the maximum correlation is the estimated range [3].  In this way the range can be updated for each pixel in the array.  With the updated range it is possible to repeat the process by using equation (3.6) to produce a new estimated pulse shape per pixel and then to recover a range from that pulse.  This process is repeated for a fixed number of iterations.  In this study this iteration is completed 20 times; however, a means of choosing a number of iterations automatically from the data is desirable.

4. RESULTS Results are obtained from the simulated data set shown in this section through a series of figures and graphs that document the improvement in range resolution and spatial resolution obtained by processing the spatially under sampled 3-D LADAR data with the proposed algorithm.

Figure 6 shows the recovered range of the target board as a function of position in the scene.  Figure 7 shows the recovered range as a function of position from the raw unprocessed data.  These range estimates would be what a traditional range only processing technique would produce from the data.  Figure 8 shows an image recovered at a range corresponding to the location of the first surface in the target.  It is obvious from this image that the spatial features of the scene have been largely restored by the proposed processing technique.  Figure 9 shows the true range image that was input into the simulation to generate the simulated sensor data.  Figure 10 shows the average range error across the entire scene as a function of the number of iterations used in the reconstruction.  The average range error in the first iteration is equal to the range error obtainable on the raw low-resolution data.  The  improvement in spatial resolution appears to have a correspondingly positive effect on the range error.  This is due to the fact that the ranging algorithm can better track the range to various places in the target scene when it has higher spatial resolution data to work with.

Figure 6: Reconstructed image of the target using the proposed algorithm.  Along the top row it appears that two bars in the upper right and left corners appear to be partially reconstructed.  They are completely missing in Figure 7 which is the range image recovered from the raw unprocessed 3-D LADAR data.

Figure 7: Range image computed from the raw sensor data.

Notice how the large bars near the bottom are thinner than in Figure 6 and Figure 8, which is the true range image.

This distortion of the bars is due to the spatial under sampling of the detector array.

Figure 8:  True range image input into the simulation.  This range map is used to compare the recovered range found using the proposed algorithm in Figure 6 to the ranges obtained from the raw data found in Figure 7.

Figure 9: 2-D Image of the target at a range at the front of the target.  Notice how the bars are much more similar in size and shape to those in Figure 4 (non-under sampled view of the scene) than those found in Figure 2.

Figure 10:  Range error as a function of iteration number using the proposed algorithm.  An average range error of 0.8 meters is obtained by ranging using the raw data.  The new algorithm takes that initial set of range estimates and improves on it through multiple iterations of the proposed algorithm.  This graph shows that the range error improves nearly 10 centimeters.

5. CONCLUSIONS The proposed ranging algorithm is shown in this paper to significantly improve both the spatial and range resolution of 3-D LADAR data.  The role of the 2-D LADAR data was to make the estimation of the amplitude of the scene A(x,y) unnecessary.  With a high-resolution map of the scene amplitude it has been shown that high resolution range estimates can be recovered from the coarsely sampled sensor data.  There are still a number of  issues to be dealt with in the application of this algorithm to measured sensor data.  One of the primary algorithmic concerns is that guidelines have yet to be developed for controlling the number of iterations to use in the image reconstruction.

The opinions and views expressed by the author are not necessarily those of the United States Air Force or the Department of Defense  6. REFERENCES [1] J.W. Goodman, Fourier Optics, New York: McGraw-Hill,  1968.

[2] J.W. Goodman, Statistical Optics, John Wiley and Sons, 1985.

[3] J. Khoury, C. L. Woods, J. Lorenzo, "Resolution limits for time-of-flight laser radar", Proceedings of the SPIE, vol. 5816, pp. 270-276, 2005.

7. BIOGRAPHY  Stephen Cain is an Associate Professor of Electrical Engineering at the Air Force Institute of Technology.  He received his B.S.E.E. from The University of Notre Dame, his M.S.E.E. from Michigan Technological University and his Ph.D. from the University of Dayton.

Dr. Cain has worked at Wyle Laboratories as a Senior Scientist and ITT Aerospace as a Senior Engineer.  He is currently working on LADAR imaging systems in the area of system characterization and ranging algorithm development.

