3rd Mediterranean Conference on Embedded Computing           MECO - 2014                                      Budva, Montenegro

Abstract? Frequent itemset mining has been a focused theme in data mining research for years. It was first proposed for market basket analysis in the form of association rule mining. Since the first proposal of this new data mining task and its associated efficient mining algorithms, there have been hundreds of follow- up research publications. In this paper we further develop the ideas presented in [1]. In [1] we consider two problems from linear algebra, namely set intersection problem and scalar product problem and make comparisons to the frequent itemset mining task. In this paper we formulate and prove new theorems that estimate the number of candidate itemsets that can be generated in the level-wise mining approach.

Keywords- frequent itemset mining; Apirori algorithm; scalar product; set intersection

I.  INTRODUCTION  We suggest a new model for frequent itemset mining which is based on linear algebra theory. In that way we give new mathematical foundation for frequent itemset mining task.

The idea comes from the definition of frequent itemset: frequent itemset is set of items that appear in sufficiently high number of transactions in given database. In linear algebra terminology it means that sufficiently high number of transactions have to intersect on particular itemset in order to make it frequent.

In this paper we, in more detailed manner, developed the idea that had been initially presented in [1]. In [1] we considered frequent itemsets, while in this paper we make upper bounds on the number of candidate itemsets in level wise mining approach. As in [1], we adopted results from set intersection and scalar product theory to estimate maximal number of candidate itemsets that can be generated in level- wise mining approach. To illustrate the idea we use the modification of well known Apriori algorithm [2] for frequent itemset mining, called Apriori Multiple algorithm. Details of the Apriori Multiple algorithm are presented in [3] and [4].

Results from this paper can be effectively applied to all algorithms that use level-wise mining approach [2].



II. PRELIMINARIES  This section contains usual mathematical foundation of frequent itemset mining. We primarily use notions from [5].

Suppose that I is a finite set; we refer to the elements of I as items.

Definition 1. A transaction data set on I is a function . The set  is the kth transaction T. The  numbers 1, ?, n are the transaction identifiers (TIDs).

Given a transaction dataset T on the set I, we would like to  determine those subsets of I that occur often enough as values of T.

Definition 2. Let  be a transaction dataset on a set of items I. The support count of a subset K of the set I in T is the number  given by:  . The support of the itemset K is the number  .

Definition 3. An itemset K is -frequent relative to the transaction dataset T if . We denote by the collection of all -frequent itemsets relative to the transaction dataset T and by  the collection of all - frequent itemsets that contain r items for .

Note that .

Definition 4. Frequent itemset mining problem consists of  finding the set for given minimal support  and transaction dataset T.

The following rather straightforward statement is fundamental for the study of frequent itemsets. It is known as Apriori principle.

Theorem 1. Let  be a transaction dataset on a set of items I. If K and  are two itemsets, then implies .

3rd Mediterranean Conference on Embedded Computing            MECO - 2014                                    Budva, Montenegro

III. LEVEL-WISE FREQUENT ITEMSET MINING  In this section we will briefly explain our Apriori Multiple algorithm for frequent itemsets mining from [3]. It is modification of well known Apriori algorithm from [2].

Apriori Multiple implements level-wise approach in frequent itemset mining. The main characteristic of this approach is in the following: it generates frequent itemsets starting with frequent 1-itemsets (itemsets consisted of just one item); next, it iteratively generates frequent itemsets of size 2, 3, etc.

Iteration consists of two phases: candidate generation and support counting.

In the candidate generation phase potentially frequent itemsets or candidate itemsets are generated. The Apriori principle is used in this phase (theorem 1). Regarding the original Apriori algorithm, in this phase, we have added new parameter named multiple_num that determines the ?length? of iteration. Actually, in the original Apriori algorithm, in the iteration r, set  is generated, while our Apriori Multiple algorithm in the iteration r generates sets  .

The Apriori Multiple can use any value for multiple_num parameter. If multiple_num = 0, our Apriori Multiple ?becomes? the original Apriori algorithm. If we want to ensure that Apriori Multiple finishes in just two database scans, we need to choose value for multiple_num parameter such that  holds, where  is the maximal size of -frequent itemsets. The value  is not known in advance, but we can use the following very simple approach to find it. In the first scan, Apriori Multiple generates frequent 1-itemsets. During this scan Apriori Multiple can determine the length of the longest transaction in the database: . It is clear that , so the algorithm can set multiple_num parameter to . Another approach is to set multiple_num parameter to average size of transactions. This does not guarantee that the algorithm finishes in two database scans, but it will generate less number of candidates than the original Apriori algorithm [3]. Also, Apriori Multiple can start with some value for multiple_num parameter and change this value in the following iterations.

The multiple_num parameter can also be defined by user, just like  threshold. It means that user, according to domain knowledge or some other estimate can specify the value for multiple_num parameter.

The support counting phase consists of calculating support for all previously generated candidates (which are not pruned according to the Apriori principle in the preceding candidate generation phase). In the support counting phase, it is essential to efficient determine if the candidates are contained in particular transaction , in order to increment their support.

Because of that, the candidates are organized in special structures like TS-Tree from [4, 6, and 7]. The candidates, which have enough support, are termed as frequent itemsets.

Pseudo-code for Apriori Multiple algorithm comes next.

Apriori Multiple Algorithm Input: T - transactional database;  (minsup) - minimal support; Output:  - Method: 1.  = frequent 1? itemsets in T 2. multiple_num =  AVG(transaction  length) 3. FOR (r=2;  ?; r+=multiple_num)  DO = candidate_generation( )  FOR i = 1 TO multiple_num - 1 = candidate_generation( )  END FOR FOR i = 0 TO multiple_num - 1  support_count( ) END FOR FOR i = 0 TO multiple_num - 1  END FOR END FOR 4.



IV. UPPER BOUNDS ON THE NUMBER OF CANDIDATE ITEMSETS IN APRIORI LIKE ALGORITHMS  In this section we further develop the ideas from [1].

Because of completeness, let us briefly introduce set intersection problem and make comparison between it and frequent itemset mining.

Let  be finite set that contains n different elements. Let  be set of subsets of  that satisfies , where k is in advance given integer. We choose  and define condition  . With  we denote , where maximum is taken on all M that satisfies previously defined conditions.

In frequent itemset terminology set  is set of items I, while set  can be considered as collection of candidate sets in the iteration k in Ariori like algorithms.

In [1] we have proved the following theorem. With we denote the maximal number of candidate r-  itemsets from set of items  such that any two itemsets intersect on  items.

Theorem 2. Let . Also, under the conditions  and  define the set  . In other words, the set  is collection of r-itemsets in such that at least  items is taken from .

If for some  holds     3rdMediterranean Conference on Embedded Computing                 MECO - 2014                                      Budva, Montenegro  (1)  we have .

In the following paragraphs, we analyze conditions from the theorem 2 and formulate new theorems in order to make upper bounds on the number of candidate itemsets in Apriori Multiple algorithm or any algorithm that relays on level-wise mining approach.

Conditions from the previous theorem just ensure that sets are defined correctly. First, consider .

The opposite is  and in that case we have that r- itemsets of  surely intersect on  elements which implies . Second, consider  that can be easily transformed to . This condition ensures that  . Finally, consider  that is equivalent to . The previous condition says that r-itemsets F for which is true, definitely exists.

We can now formulate theorem that estimates number of candidate itemsets in Apriori Multiple algorithm from the previous section. Recall that in Apriori Multiple we have longer iterations, which means that in th candidate generation phase algorithm generates the following sets of candidate itemsets: . Notice that all these candidate itemsets intersect on  items.

Figure 1. TS-Tree  Figure 1 illustrates the idea. For fast implementation of Apriori Multiple algorithm special tree structure is used. The tree is called TS-tree and it is based on Ryman set enumeration tree. Each level in the tree contains candidate itemsets; the 0th level contains just the root that represents empty set, the first level contains the set  of candidate 1- itemsets, the second level contains the set  of candidate 2-  itemsets, the level t contains the set  of candidate t-itemsets.

Details can be found in [6] and [7].

Theorem 3. Consider   th candidate iteration step in Apriori Multiple algorithm. Let . In Apriori Multiple algorithm holds .

Proof. Notice that the set  corresponds to the set of r- itemsets from  with property that any two itemsets from  intersect on  items. In other words,  can be estimated with . In order to prove our theorem we will apply results from theorem 2.

In order to find w, we have to satisfy condition .

If we allow different, i.e.  that is equivalent to  , we have  .   (2)  Condition (2) is not possible. We set and check  (1).

(3)  Recall that , so we have  (4)  From (4) we get  (5)  The previous is equivalent to the  (6)  Finally, we have  (7)  Condition (7) is satisfied for sufficiently large n as in our case, because n is typically several thousand while multiple_num and t are much less. It can be shown that integer w that satisfies (1) is unique [8], so we use , that implies  .

The set  is actually set of all r-itemsets in  such that any two of them intersect on . It is also illustrated on figure 1.

Notice that . If we use estimation from [8] we have  .

The theorem 3 can be reformulated in order to estimate upper bound on the number of candidate itemsets in any Apriori based algorithm that applies level-wise mining approach.

Theorem 4. Consider th candidate generation step in Apriori algorithm from [2]. Number of candidate itemsets in  th step, where  is .

3rdMediterranean Conference on Embedded Computing               MECO - 2014                                      Budva, Montenegro

V. FREQUENT ITEMSET MINING TASK IN VECTOR THEORY  In [1], as future work, we plan to define frequent itemset mining task with support of vector theory, i.e. using vector scalar product.

Scalar product is an algebraic operation that takes two equal-length vectors and returns a single number. This operation can be defined as the sum of the products of the corresponding entries of the two vectors of  numbers. Scalar product of two vectors  and y  is defined as:  (8)  In the following paragraphs we will make modification of the original scalar product problem from [8] in order to make it more familiar with frequent itemset mining task.

Let  be natural numbers that satisfy .

Consider the following set of vectors:  Obviously, the set  is correctly defined and  We fix  and want to estimate maximal size of any set  in which there is no pair of vectors with scalar product t. Parallel to the problem from the previous section is obvious.

We can now define the candidate itemsets maximal number estimation problem using scalar product theory. Let  be set of items. Any vector from  represents candidate -itemset because -vector corresponds to the canidate r-itemset   in the way that  if  and  if  and number of   is . At the same time vector product of vectors from  corresponds to size of intersection of any two candidate r- itemset from  .

In the next paragraph we formulate the theorem that is modification of the corresponding theorem from [8]. The proof  is similar to the proof of the Theorem 2 in [1].

Theorem 5. Let p be prime number,  set of items and  database of transactions on .

Additionally, let . Maximal number of candidate r-itemsets  in th iteration of level-wise mining approach with property that any two candidate itemsets  do have at least one item in common is  , where  .

The previous theorem provides estimation of the maximal number of candidate itemsets that can be generated from one common item in th iteration. That common item can be any node from the first level in TS-tree as it is illustrated in figure 1.

From [8] we will just take the following estimation  .

Notice that in theorem 5 we define the set  that contains vectors with property  and there is no pair of orthonormal vectors (vectors with scalar product 0).



VI. CONCLUSION  In this paper we present new mathematical model for frequent itemset mining problem. We use linear algebra method to estimate the size of candidate itemsets in Apriori based algorithms that implement level-wise approach in mining frequent itemsets.

Results from this paper can be effectively used in implementation of any level-wise algorithm because it gives upper bounds on the number of candidate itemsets, so we can know maximal memory requirements in advance.

As future work we plan to:  ? make much better estimation in theorem 3  ? incorporate minimal support parameter  in order to achieve better estimation and define eventually dependency between   and measures  or  .

