

LVFS: A Scalable Big Data Scientific Storage  System  Navid Golpayegani*t+, Dr. Milton Halemt, Edward J. Masuoka+, Neal K. Devine* and Gang Ye* *Sigma Space Corporation  Lanham, MD 20706 tDepartment of Computer Science and Electrical Engineering  University of Maryland Baltimore County  Baltimore, MD 21250 +NASA Goddard Space Flight Center  Terrestrial Information Systems Lab  Greenbelt, MD 20771  Ahstract-LVFS is a virtual scalable file storage system developed in response to a class of scientific data systems that over time continue to collect petabytes of data that begin to seriously impact the response time to user request services. The system has been operational in a real use case, the NASA MODIS Adaptive Processing System (MODAPS), and shown to double data throughput compared to the original system thanks to better performance and easier load balancing. The MODAPS operational life has been extended over a decade as of now and contains over four petabytes of data in over billions of files on over 500 different disks attached to multiple storage nodes. MODAPS is the processing system for delivering calibrated Level 1 data from MODIS instruments on two NASA satellites, each containing 36 channel multi-spectral visible and infrared changes launched over a decade ago. These system's life cycle operations are typical of many scientific instruments and experiments that continue to generate useful archival data well beyond their originial expected lifetime capabilities to meet current scientific user needs. The Level 1 Atmosphere Archive and Distribution System (LAADS) is responsible for distribution of products produced by MODAPS.

The LAADS Virtual File System (LVFS) has now replaced parts of LAADS and is responsible for the read only distribution of all LAADS data to the public. In this paper, we describe the unique design of LVFS and, additionally, describe our ongoing work to incorporate a Distributed Hash-based architecture into the LVFS design to transform LVFS into a full scientific storage architecture scalable to Exabyte sizes.



I. INTRODUCTION  Scientific data sets, such as those from the Large Hadron Collider, Sloan Digital Sky Survey, and Brain fMRIs, are growing constantly and producing Petabytes of data with some data records doubling every year [1]. Managing Petabytes of data in hundreds of millions of files presents complex storage  address the first aspect of this problem. LVFS is designed for scientific processing systems as it relies on the existence of an optimized metadata database server. Such servers are usually available for most data intensive scientific data processing and archival systems. The metadata servers in such systems usually contain detailed metadata information about the data produced and stored by the processing system represented in an fashion optimized for searching.

The second aspect of a storage system, storing and transfer? ring data on a distributed set of storage nodes in a reliable and resilient manner, is still under development to expand LVFS from a virtual file system to a full scientific storage system capable of scaling to exabyte sizes. By incorporating the notion of a distributed hash table based architecture into the existing LVFS system, we plan to implement a system that successfully manages to both locate and search data sets and provide data at reasonable speeds for exabyte scale sizes.

In this paper, we describe the existing LVFS and outline the planned work to incorporate distributed hash tables to produce an exabyte scalable storage architecture for scientific storage systems. In Section II, we describe the design of LVFS and DISHAS, an experimental distributed hash table storage system, as independent systems and how we propose to combine the two systems to create an exabyte scale storage architecture. Section III shows the performance of LVFS in LAADS and experimental results for DISAHS. Section I V  we describe similar systems and, finally, we conclude in Section V.



II. DESIGN  system problems that are present in many scientific storage In this section, we will look at the design of LVFS and the systems. However, when dealing with such massive scales of Distributed Hash Archive System (DISHAS). LVFS is now a data, traditional storage systems face two challenges in meeting production level virtual file system currently in use by LAADS the needs of scientists. First, is dealing with data discovery for to distribute MODAPS data. It is responsible for all read large scale file systems. Second, is dealing effectively with data only access to data being distributed to the public via various resiliency and distribution in a manner that minimizes resource protocols such as FTP, HTTP, OPeNDAP [2], Web Coverage usage. The LAADS Virtual File System (LVFS), originally Service (WCS) and others. DISHAS is a fully decentralized developed as an outgrowth of the Level 1 Atmosphere and storage and data distribution system still in the experimental Archive Distribution System (LAADS), has been dsigned to stage.

"dirLayout II : [ { "type" : "file", II name II : "README", "contents" : "Sample contents of README file" } { "type" : "dir", "name" : lallData", IdirLayout" : [  { "type" : "sqlDir", "query" : "select ArchiveSet from ArchiveSet_Table where public=' true''', "cacheTime" 600}  Fig. l: An sample L VFS configuration file  A. LVFS  LVFS is a virtual file system for scientific processing systems and is mainly responsible for providing a virtual directory tree for accessing existing data. LVFS' goal is to generate a file system directory structure to represent the scientific data sets available for read access. In order for LVFS to balance runtime and development speed it is written in C, makes use of the File System in Userspace (FUSE) [3] kernel module, and uses ISON for its configuration. The FUSE kernel module exposes basic file system operations, such as opening, reading, and writing, to a user space program. The ISON configuration is responsible for describing the directory layout represented by LVFS to the operating system via the FUSE kernel module.

Fig.l shows a simplified ISON configuration file used by LVFS. The sample configuration files defines a README file and a directory named aliData at the root level. The contents of the README file is provided in the configuration file. The contents of the aliData directory is specified as an sql query.

Each entry in the sql result is mapped to a new directory name inside the allData directory. This configuration file demonstrates a small set of the capabilities of the LVFS virtual file system. Each entry in the configuration file contains a type definition. This type definition specifies a particular module used that LVFS uses to parse the remaining configuration definitions.

While the sample configuration file describes the file, dir, and sqlDir modules there are many more modules bundled.

Additionally, custom modules can be added to LVFS without the need to recompile. A well defined API allows anyone to develop additional modules to be used for custom directory.

In the provided sample, the file module creates virtual files, the dir module generates virtual directories hard coded in the configuration. The sqlDir module generates virtual directories obtained via the provided sql queries. As these sql queries could be slow, the configuration allows these queries to be cached for a certain period. In the sample provided, the sql query is cached for 5 minutes before the query is re-run to obtain updated results. Other bundled modules with LVFS include a memory cache module, a file swap module, an HTTP module, a sql File module, and a statistics module.

With this simple design, LVFS is capable of creating an arbitrarily complex virtual file system directory structure to represent scientific. This unique design allows LVFS to represent the same data sets in multiple views to the user without the need for duplication, or the use of symbolic links. For example, the same data set can be represented by measurement time, processing time, measurement location, etc.

by supplying the configuration file with the approriate SQL queries for each representation.

Once a file is opened for reading, the ISON configuration file specifies how this file should be located and retrieved. For example, and sql query can provide the hostname and path of the remote host where the real file is located. Currently LVFS supports retrieving the file via any protocol supported by libCuri. This includes HTTP, HTTPS, FTP, and many other protocol. To speed up file access, a swap module stores remote files locally which allows multiple users to access the same file without the need to download the file multiple times. Due to the modular design, LVFS can be expanded to support other download protocols.

LVFS is currently being used by LAADS system to dis? tribute the entire dataset of MODAPS via a load balanced FTP server system. Each FTP server is currently handling several hundred FTP users simultaneously with several hundred open files at any given time. We will show detailed performance results of the public FTP server in section III. While LVFS currently uses LAADS' Postgres database it is flexible enough to support any database system. This flexibility ensures that it can continue to scale into exabyte sizes. For any scientific data processing system should the metadata server not scale to the required size it can be replaced with more modern systems capable of scaling and LVFS can be easily configured to use the new metadata system.

B. Combining DISHAS and LVFS  DISHAS is an experimental object based storage system. It uses a variation of the Chord Distributed Hash algorithm [4] to locate objects. Each object stored within DISHAS is assigned an ID out of a finite but large pool of available numbers. While we use the MD5 algorithm in our implementation, any hash function can be used. The finite range of numbers produced by the MD5 algorithm is distributed among all storage nodes in proportion to the total disk space provided by a storage node.

A global lookup table distributed among all nodes matches ranges of MD5 numbers to storage node. Depending on the requirements of the processing system various aspects of an object can be used to compute the ID. In the case of MODAPS it is preferred that files representing measurements for the same day be stored on the same storage node. This requirement is due to the fact that it is easier for MODAPS to regenerate lost data for the same day compared to regenerating lost data spanning multiple days. Therefore, for MODAPS, the MD5 for each file is computed based on a formula including the measurement day. This ensures all files representing measure? ments for the same day will compute to the same MD5 ID and, therefore, be assigned to the same storage node.

DISHAS communication is simple HTTP communication.

Objects can be retrieved from DISHAS via standard HTTP GET requests. Similarly, objects can be stored into DISHAS    via HTTP PUT operations. The use of standard HTTP protocol allows files to be retrieved or stored even with standard unix utilties or web servers. Additionally, the use of HTTP protocol allows for files to be retrieved or stored without any knowledge of the global lookup table. An HTTP GET or PUT request can be sent to any available storage node. If, by chance, the correct node is contacted, the requested operation is performed.

Should, however, the wrong node be contacted, then standard HTTP redirect methods are used to notify the client of the correct node to contact.

Storage nodes can be added and removed from the system by updating the global lookup table. When a new storage node is added to the system, a new global lookup table is generated with the range of MD5 redistributed so that all storage nodes are once again assigned MD5 ranges in proportion to the disk space they provide. The new global lookup table is distributed among all storage nodes. When a new global lookup table is sent to a storage node, the node identifies MD5 ranges that be? longed to it in the previous table but not the new table. Objects belonging in these ranges are transferred to the new responsible node. Similarly, when a storage node is to be retired then all MD5 ranges assigned to that node are distributed among the remaining nodes and a new table is pushed to all storage nodes.

Temporary outages, such as when a storage node crashes, are not dealt with in any specific manner. Standard HTTP and TCP communication timeouts are used deal with temporary outages. As the DISHAS design is completely decentralized, there is no single point of failure. Data stored on down nodes will be unavailable, unless replicated, but data on other storage nodes will still be accessible without any delay.

To incorporate a distributed hash storage model into LVFS requires LVFS to be converted to support write operations.

Currently LVFS only performs read operations both for file access and for metadata access from the metadata server.

LVFS configuration needs to be modified to incorporate write operations into the metadata server to update information such as file size, last modification time, etc. Additionally, standard POSIX write operations will need to be supported. These write operations are already exposed by the FUSE module.

LVFS configuration needs to support information on how to compute an ID for a given file. A simple example would be to configure LVFS to compute an MD5 based on the file name of the file to be written. In the case of MODAPS, however, the configuration would compute an MD5 based on the path elements that identify the date of measurement. Additionally, LVFS will need to retrieve and maintain a global lookup table to determine which remote storage node to store files on.

With these minimal changes LVFS can be converted into a full scientific storage system limited in scalability only by the metadata database system used by the data processing system.



III. PERFORMANCE  LVFS is currently used in the production system as the virtual file system for the MODAPS processing system at NASA Goddard Space Flight Center. It has been in production for the past year and has exceeded expectations. It is being expanded into more parts of the processing system as the need arises. Since it has been put into production for the LAADS system, it has been able to nearly double the volume of data   iii 200 1::.

OJ ;9i 150  ." '" ? 100 ? o 50  o  Monthly Download Volume  Fig. 2: Monthly average download from FTP Server in 2011, 2012 and 2013  being distributed to the public. This is partially due to the caching module for LVFS as well as LVFS ability to allow for easy mirroring of the virtual file system on additional hosts.

Fig.2 shows the monthly download volume of the public FTP server for LAADS in 2011, 2012, and 2013. Once LVFS is implemented between July and August 2012 to replace the legacy NFS based system, the download volume nearly doubles from approximately 90TB a month to approximately 180TB a month. This doubling in volume is due to the flexibility provided by LVFS. With LVFS we are capable of running load balanced FTP servers. Prior to LVFS maintaining NFS mount points on multiple FTP servers and proper symlinks was impossible due to the number of files maintained by LAADS.

With LVFS, however, a new FTP server can be setup in a matter of minutes allowing us to grow with demand.

Fig.3 shows the resource requirements of LVFS during various levels of load. For the same time period we can see in Fig.3a that LVFS at its peak maintains 220 actively open files while Fig.3b shows it utilizing approximately 20 Gigabytes of swap space for those files. During the entire period Fig.3c shows that the CPU is nearly fully idle.

I V. RELATED WORK  LVFS and DISHAS are not unique in attempting to solve the problem managing ever growing datasets. There is a lot of work being performed to solve the problems associated with exabyte storage. Popular storage systems such as the Hadoop Distributed File System (HDFS) [5], Parallel Virtual File System (P VFS) [6], and Lustre [7] deal with the problem of storing large volumes of data. However, once stored, they do not address the problem of locating or searching large volumes of data. Many of these file systems rely on centralized metadata servers that do not scale well with large files [8].

Some storage systems, like P VFS, attempt to address the centralized nature of the metadata server through replication but do not address the problem of searching for data in exabyte scales. Due to the generic nature of the metadata server incorporated file system only limited metadata information, such as the file size, timestamp, etc are stored. LVFS, however, takes advantage of the optimzed and detailed metadata server designed specifically for the scientific instrument. Additionally,    Nu.roer of Open Files  ::? no '00 lion 00:00 lion 12:00  CPU Idle  ' ?? (a) Number of open files (b) Swap space used by LYFS (c) CPU Load  Fig. 3: Resource usage by LVFS for a one day period on the public LAADS FTP Server  since LVFS uses the specialized metadata server designed for the instrument, it is capable of representing the same dataset in many different directory structures simultaneusly without duplicating any information.

More recent work has concentrated in decentralized storage systems such as Amazon's Dynamo [9], Amazon's S3 [10], and Apache's Cassandra [11]. Most of these systems are presented as database replacements as they act as simple key/value stores completely abandoning metadata information and, hence, no directory structure. Amazon's S3 system contains a fixed and basic directory structure organized by buckets and keys.

DISHAS as a standalone system is very similar to such storage systems. However, when combined with LVFS, DISHAS will provide the decentralized capabilities of Dynamo and Cassan? dra and LVFS will provide the virtual directory structure to produce a decentralized storage system capable of supporting standard POSIX compliance.

Systems such as iRODS [12] attempt to address data discovery of large scientific datasets. However, their approach is to create a new metadata server internal to iRODS. As a result information maintained by the project's metadata server is duplicated into the iRODS metadata server creating problems of synchronization. LVFS avoids such issues by using the projects metadata server.

Other systems, such as Spyglass [13] and Provenance Aware Storage System (PASS) [14] deal with the problem of metadata searching and locating existing files. However, they do not deal with the problem of storage of large volumes.

These systems are based on past research on improving meta? data lookup information [15], [16].



V. CONCLUSION  LVFS has proven to be an invaluable tool for the LAADS to distribute data in a flexible manner. Being able to restructure entire LAADS dataset on the fly with no downtime or the ability to stand up new hosts that appear to have all of the LAADS data available locally in minutes is very beneficial.

Not only has LVFS been proven to be beneficial due to its flexibility but benchmark results have shown that it is capable of making Petabyte scale datasets available on a single node while allowing hundreds of processes to access the datasets simultaneously with minimal load on the system.

Benchmarks have shown that we can double data throughput from the LAADS' public FTP server due to the ability to easily create load balanced server. We believe LVFS can scale to exabyte sizes without modification due to the fact that it currently manages Petabyte scales with minimal load. LVFS is only limited by the scalability of the metadata server which  can be replaced with better performing systems without any modification to LVFS.

Additionally, since LVFS makes use of the higly de? tailed existing metadata information present in most scientific processing system, LVFS has the unique ability to describe the same dataset in multiple views without requiring extra information or duplicating data sets. This gives LVFS the ability to organize the same dataset in a directory structure by measurement date, measurement region, or any other aspect relvant to the scientific dataset.

Due to LVFS success as a virtual file system we plan to expand LVFS to a full scientific storage architecture. In order to maintain the goal of being scalable to Exabyte storage, we will be using distributed hash tables architecture, as implemented by DISHAS, to ensure that the storage system can be entirely decentralized and distributed with minimal re? quirement in computational power. The underlying technology behind DISHAS has been shown to be scalable in real life systems such as those used by Amazon.

