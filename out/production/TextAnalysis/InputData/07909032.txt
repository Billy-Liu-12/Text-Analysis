2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

Abstract?In the environment of cloud computing, the data produced by massive users form a data stream and need to be protected by encryption for maintaining confidentiality. Traditional serial encryption algorithms are poor in performance and consume more energy without considering the property of streams. Therefore, we propose a velocity-aware parallel encryption algorithm with low energy consumption (LECPAES) for streams in cloud computing. The algorithm parallelizes Advanced Encryption Standard (AES) based on heterogeneous many-core architecture, adopts a sliding window to stabilize burst flows, senses the velocity of streams using the thresholds of the window computed by frequency ratios, and dynamically scales the frequency of Graphics Processing Units (GPUs) to lower down energy consumption. The experiments for streams at different velocities and the comparisons with other related algorithms show that the algorithm can reduce energy consumption, but only slightly increases retransmission rate and slightly decreases throughput. Therefore, LECPAES is an excellent algorithm for fast and energy-saving stream encryption.

Index Terms?Advanced Encryption Standard, flow control, low energy consumption, parallel encryption, scaling frequency, velocity- aware  F  1 INTRODUCTION 1.1 Motivation  H ETEROGENEOUS many-core architecture has the ad-vantages of better performance and energy efficien- cy compared to homogeneous many-core architecture [1].

This is because the coprocessors in the heterogeneous architecture, such as GPUs, have simple control and high throughput. Therefore, super computers, such as the TOP 500 [2] supercomputers, widely adopt heterogenous architecture currently. Therein the architecture of CPU + GPU becomes the mainstreams thanks to powerful computing capacity, low price, and supporting general computing of GPUs. Despite all of these, for example, Titan, the second place of the TOP 500 computers listed in Nov. 2015, employs Nvidia K20x and still consumes a large amount of 8.208 MW on average. Thus, the problem of lowering energy consumption of GPUs is significant, and then is hotly studied currently.

Cloud computing provides resources by the demands of users with perfect elasticity and scalability, so it is used widely. However, in this open environment, user data are off the control of the owners and more likely to be stolen by adversaries. Encryption is a basic technology to protect user data from stealing. There are massive users in cloud computing. These users produce a large amount of data in the manner of streams. If adopting  ? The authors are with the College of Computer Science and Electronic Engineering, Hunan University, Changsha 410082, China.

E-mail: {feixiongwei, lkl, yangwangdong}@hnu.edu.cn.

Xiongwei Fei is also with the School of Information Science and Engineer- ing, Hunan City University, Yiyang 413000, China.

Keqin Li is also with the Department of Computer Science, State U- niversity of New York, New Paltz, New York 12561, USA. E-mail: lik@newpaltz.edu.

encryption to protect these data, cloud servers will un- dertake heavy computation burden.

Cloud servers must efficiently deal with the encryp- tion of data streams; otherwise users experience will be impacted and services will even be disrupted. In order to improve efficiency of encryption, parallel technologies that use many heterogenous cores provide a feasible solution, but with the increase of frequencies and the number of cores, energy consumption increases corre- spondingly. This causes some problems of heat dissipa- tion, system stability, even environment, etc. Hence, it is emergent to parallelize encryption of data streams in a low energy consumption fashion.

However, in practice, user data streams have the properties of burstiness, variance, realtime, etc. Without considering these properties, parallelism technologies will consume more energy because slow streams will waste energy. For reducing the energy consumption, a feasible solution is first to sense velocity and then to scale the frequencies of GPUs correspondingly. The key point is how to sense the velocity of streams. In this work, we use a sliding window to buffer stream data and sense its used volume in real time.

With the rapid development of electronic commerce and network finance, encryption has become an impor- tant measure in protecting secret data of these appli- cations. AES is the symmetric encryption/decryption standard due to its higher security and performance compared to its competitors [3]. AES has several modes, of which the Counter (CTR) mode can be parallelized fully and have the property of provable security [4].

Thus, we choose the CTR mode of AES to encrypt stream data in this work.

In summary, the motivation of this paper is to en- crypt stream data of cloud users, especially the data of    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2697446, IEEE Transactions on Big Data   electronic commerce or network finance, in low energy consumption and high performance fashion using par- allelism and scaling frequencies methodologies. The key points include sensing the velocity of the data streams and scaling the frequencies of GPUs.

1.2 Our Contributions In this paper, our contributions include four aspects as follows:  1) We adopt dynamic scaling frequency technology to save energy consumption in stream data encryp- tion.

2) We adopt a sliding window to control streams and sense current velocity based on thresholds.

3) We design and implement a velocity-aware parallel encryption algorithm with low energy consump- tion for streams, called LECPAES.

4) We evaluate LECPAES from energy consumption, throughput, and retransmission rate through com- parisons of CPU Serial (CS), CPU Parallel (CP), and GPU Parallel (GP) AES algorithms for streams.

1.3 Organization The remainder of this paper is organized as follows.

Section 2 reviews some related works. Section 3 intro- duces some models which will be used in this work.

Section 4 gives the algorithms. Section 5 introduces the experiments. Section 6 discusses the experimental results.

Section 7 provides conclusions and a look to the future.

2 RELATED WORK Currently, there are two research directions on GPU ener- gy consumption. One is saving the energy consumption, and the other is evaluation and prediction of energy consumption. These two directions will be described in Subsections 2.1 and 2.2, respectively. In addition, some works on parallelizing AES are described in Subsection 2.3.

2.1 Saving Energy Consumption Some recent research on the first direction are described as follows. Abe et al. [5] analyzed GPU-accelerated sys- tems on power and performance and found that the total energy reduction is trivial using voltage and frequency scaling of CPUs, but can be achieved by scaling voltage and frequency of GPUs.

Ge et al. [6] found that GPU frequencies and ener- gy consumption have a linear relationship in compute- intensive applications. Therefore, scaling down GPU fre- quencies can reduce energy consumption effectivley.

Ma et al. [7] dynamically distributed workload to CPUs and GPUs based on the previous execution time and dynamically scaled the frequencies to reduce the energy consumption. Arora et al. [8] accurately predicted idle durations and then adopted power gating to save energy consumption of GPUs.

Tang et al. [9] proposed a secret sharing protocol using Elliptic Curve Cryptosystems (ECC) and a proactive share refreshing protocol, which are both efficient and can save energy consumption in communications and processing. Further, they derived a multi-party signature scheme suitable for low-power devices in wireless net- works. Liu et al. [10] introduced MoTE-ECC, a highly optimized yet scalable ECC library, which saves energy consumption of nodes through reducing the execution time of two scalar multiplications.

In this work, we adopt the idea of scaling voltage and frequency of GPUs in [5]. Because AES is also a compute-intensive application, which will be analyzed in Subsection 3.2, the energy consumption of AES can be saved by scaling down GPU frequencies based on [6].

Our work differs from [7] in that our work scales GPU frequencies based on the current velocity of stream.

2.2 Predicting Power and Energy The second direction is also a research key point because it can provide analyses and insight on how to save en- ergy. Some researchers have studied it based on specific models.

Ma et al. [11] adopted statistical analysis to model the power consumption of Nvidia GPUs but first required to analyze GPU workloads quantitatively. Nagasaka et al. [12] proposed a statistical model which used the GPU performance counters to estimate power consumption of GPUs. The model has high accuracy but is invalid for kernels with texture accesses.

Hong and Kim [13] proposed an analytical model to estimate the execution time of a GPU parallel program.

It is useful to understand the bottlenecks of performance in a GPU parallel program and can be used to estimate the energy consumption of the program.

Wu et al. [14] proposed a machine learning model capable of predicting the performance and power of GPUs across a range of hardware configurations. It first gathers a collection of kernels on a real GPU with various configurations, and then estimates the performance and power of new kernels using machine learning with an average error of 15%.

Kasichayanula et al. [15] analyzed per-component of power consumption of GPUs such as floating point units, shared memory, and global memory using NVML (N- vidia Management Library) to measure real-time power and energy consumption, and analyzed the power and energy consumption of three kernels on Nvidia Tesla C2075.

In summary, we can get inspiration on energy-saving by means of energy prediction and analyses. For exam- ple, we can consider the properties of an application and then correspondingly use computing resources energy efficiently.

2.3 Parallelizing AES CPU + GPU heterogeneous computers are suitable for the parallel computing on compute-intensive applica-    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2697446, IEEE Transactions on Big Data   tions, such as the SpMV (SParse Matrix and Vector mul- tiplication) problem [16] [17]. Because AES is a compute- intensive application too (see analysis in Subsection 3.2) and GPUs support general computing with the merits of low cost and powerful computing, naturally much attention is paid to improve the performance of AES by parallelizing it on the CPU + GPU architecture.

Manavski [18] first parallelized AES on GPUs us- ing CUDA (Compute Unified Device Architecture). His work is significant in two aspects. On one side, the difficulty of programming is reduced; on the other side, the performance of parallel AES is improved about 20 times compared to the serial AES.

Later, Maistri et al. [19] parallelized AES adopting CUDA also, and got good ratio of performance to price.

Then, Iwai et al. [20] optimized AES from the aspects of parallel granularity and storage distribution, etc., and found that the execution efficiency of parallel AES can be improved by using the granularity of 16 byte/thread and storing T tables in shared memory.

Some other works on optimizing and analyzing paral- lel AES on GPUs can be found in Refs. [21?29].

The above works [18?29] only deal with single data file, and do not consider the encryption of data stream of users. However, encryption of data stream becomes more and more usual in practice, especially in cloud computing. The data stream produced by massive users have the properties of burstiness, variance, realtime, etc.

Therefore, if the data stream is still encrypted in the pattern of single file, it is likely to result in the lack of efficiency and even to cause service failure.

3 MODELS 3.1 Stream Encryption Model In cloud computing environment, users? data are out- sourced to cloud servers. These data can be processed by or stored in a cloud server. If users want some of their data, these data should be transferred back to users. Be- cause there are massive users in the environment, these data transfers form data streams. Due to the openness of the environment, these data need to be protected by encryption to keep confidentiality. Fig. 1 illustrates this scenario, where massive users request some data from a cloud server. The cloud server responses these requests and will form a user data stream. This stream should be encrypted to guarantee data confidentiality.

In practice, encryption of data stream could face the following three problems:  1) If the average inflow velocity of data stream is faster than the average outflow velocity of encryp- tion, cloud servers will not achieve all the encryp- tion of the data stream. This will cause part failure of services.

2) If the instant inflow velocity of data stream is faster than the outflow velocity of encryption, but the average inflow velocity of the data stream is slower than the outflow velocity of encryption. This can  Cloud Server  User 0Data Stream Encrypted  Data  Encryption  User 1  User n  User n+1  User i  Fig. 1. Scenario of encryption of user data stream in cloud computing  be solved by buffering the burst throughput to stabilize.

3) If the average inflow velocity of data stream is slower than the average outflow velocity of en- cryption, sometimes cloud servers will be idle but still consume energy. This will cause high energy consumption.

For the first problem, because the velocity of tradition- al serial encryption is lower than the mainstream velocity of users? data streams, it can be considered to improve the velocity of encryption by parallel encryption using the existing cores of computers. Therefore, in this paper, we propose a solution which uses CPU parallelism or GPU parallelism to improve the efficiency of encryption in heterogenous environment. The details of the solution are described in Subsection 3.2.

For the second problem, we propose a solution which uses a sliding window to tame the burst data flow of users and achieve robust services. The details of the solution are described in Subsection 3.3.

For the third problem, we propose a solution, which first senses the velocity of data stream and then dynam- ically scales the frequencies of GPUs to reduce energy consumption. The details of sensing velocity and saving energy by scaling frequencies dynamically are described in Subsections 3.4 and 3.5, respectively.

3.2 AES Parallelizing Model  AES is a block ciper. Each block is long as 16 bytes.

The encryption process of a block includes multiple iterations, each of which is called a round. The number of rounds is determined by the version of AES. AES has three versions of 128, 192, and 256. For simplicity, we use AES-N to express the version, where N is one of 128, 192, and 256. A different version of AES needs a key with different length and will perform different number of rounds Nr. Specifically, AES-N needs a key of length N bits and will perform Nr = N/32 + 6 rounds.

Fig. 2 describes the encryption process of a block of plain text. Except round N/32+6 misses a MixColumn, each round has four procedures, i.e., SubBytes, ShiftRows, MixColumns, and AddRoundKey.

AddRoundKey in a different round needs a different round key. Note that before the first round, there is an AddRoundKey also. Therefore, AddRoundKey will    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2697446, IEEE Transactions on Big Data   be executed N/32 + 7 times totally. Correspondingly, N/32 + 7 round keys are required.

These round keys are extended from the key of N bits by the procedure KeyExtension as shown in the left part of Fig. 2. The procedure needs to be executed in serial due to data dependence. Nevertheless, once generated, the round keys can be used in other different blocks. This means that KeyExtension needs to be executed only one time in serial in the entire process of encrypting the same plain text.

In a round, SubBytes, ShiftRows, MixColumns, and AddRoundKey involve some operations as listed below respectively (Note: We use the following abbreviations.

O: XOR; L: lookup; R: rotation; M : multiplication.).

1) AddRoundKey executes 16 XOR operations (16O) on a plain text block, called state, and a round key.

2) SubBytes substitutes data using S-Boxes and can be accelerated by 16 T table lookups (16L).

3) ShiftRows rotates rows by 0, 1, 2, or 3 byte(s) respectively to confuse data, thus it has 6 byte rotations (6R).

4) MixColumns transforms and mixes data in columns which has 4?4 = 16 multiplications (16M ) and 3? 4 = 12 XORs (12O).

When encrypting a state, AES-N has the number of operations:  OPs = Nr?(OPsb+OPsr)+(Nr+1)?OPak+(Nr?1)?OPmc, (1)  where OPak, OPsb, OPsr, and OPmc represent the number of operations of AddRoundKey, SubBytes, ShiftRows, and MixColumns, respectively. For exam- ple, AES-256 has operations:  OPs = 14 ? (28O + 16L + 6R + 16M) + 16O ? 16M ? 12O = 396O + 224L + 84R + 208M,  in encrypting a state long as 16 bytes. If not considering the differences of operations, AES-256 has 912 operations.

Therefore, AES-256 has the computing complexity of O(N) = N2.46, where N represents the number of bytes which want to be encrypted. Similarly, AES-192 and AES-128 have 780 and 648 operations by using and their computing complexities are N2.40 and N2.33, respectively.

As a conclusion, AES is a computing-intensive applica- tion. It is suitable to be executed on GPUs, because GPUs can support these integer operations through CUDA (Computing Unified Device Architecture) and efficiently hide the overhead of transferring data by large-scale parallelism.

AES has five modes, i.e., ECB (Electronic Code Book), CBC (Cipher Block Chaining) , CFB (Cipher Feedback), OFB (Output Feedback), and CTR (Counter). ECB mode can be fully parallelized, but cannot hide the mode of plaintext and probably suffers active attacks from adversaries. CBC and CFB are hard to suffer active attacks with higher security than ECB but are hard to parallelize. OFB possibly suffers active attacks and hard to parallelize. CTR can be fully parallelized and has proved security, which is strongly recommended  AddRoundKey  ShiftRows  MixColumns  SubBytes  N bits Key  Key Extension  16 Bytes Cipher Block  16 Bytes Plain Text Block  I<=N/32+6  I=1  I=N/32+6  I=I+1  AddRoundKey  N  W[I?4 I?4+3]  W[0 3]  Round I  Y  Y  N  Fig. 2. Encryption process of a block  by Lipmaa et al. [4]. Naturally in this work, we adopt CTR mode and the encryption process of AES can be parallelized fully among blocks.

Fig. 3 shows the details of parallel encryption in CTR mode, where IV is the initial value of the counter, Pi represents the ith block of plain text, and Ci represents the ith block of cipher text. From the figure, we can see that, after the key extension executed in serial, different values IV + i are encrypted and then added Pi to get Ci in parallel by different threads. It is apparent that both IV + i and Pi are independent in different encryptions.

If the number of blocks of the plain text Nb is more than the number of threads Nt, these blocks will be distributed evenly, i.e., the thread Ti, 0 ? Ti < Nt, will undertake Nb/Nt+ i < Nb%Nt, where / and % represent the operation of integer division and remainder, respec- tively. For example, plain text with 8 blocks is encrypted by four threads. Each thread will encrypt 2 blocks. For another example, plain text with 10 blocks is encrypted by four threads. Each of the first two threads will encrypt 3 blocks and each of the other threads will encrypt 2 blocks.

2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2697446, IEEE Transactions on Big Data   Encryption  of IV  Encryption  of IV+1  Encryption  of IV+i  Encryption  of IV+n IV  Thread 0 Thread 1 Thread i Thread n  Key  Key  Extension  IV+1 IV+i IV+n  P0  C0  P1  C1  Pi  Ci  Pn  Cn  Fig. 3. Encryption parallelizing model of n blocks in CTR mode  3.3 Stream Control Model With the varying of behaviors of users, user data stream will have different velocity. Sometimes data stream has high velocity; sometimes data stream has low velocity.

For burst high-velocity stream, a control mechanism should be adopted to avoid the abruption of services.

Therefore, in this paper, we propose a methodology of employing a sliding window to control the burstiness of stream.

Fig. 4 depicts the model of a sliding window, where a user data stream D0, D1, ..., Dn, ... flows into the window and then flows out under the control of the sliding mechanism. The data stream will be encrypted to the corresponding cipher stream E0, E1, ..., En, ....

The window corresponds to a chunk of storage space with size of size, which can be set by the profile of the cloud server. The green part of the window represents the buffered data, i.e., the data stored in the window currently.

head points to the start byte of the buffered data and  tail points to the next byte to the end of the buffered data. As data flow in or out, head and tail will change cyclically.

Apparently, whether user data can flow in the window is determined by the current available size as of the window. How to calculate as? Firstly, the size of the buffered data bs can be calculated using Eq. (2):  bs =  { tail ? head if tail ? head  size? head+ tail otherwise (2)  Secondly, as = size?bs. Therefore, as can be calculated using Eq. (3):  as =  { size? tail + head if tail ? head  head? tail otherwise (3)  Based on the value of as, whether the data of a user Di with length of len(Di) can flow in the window isF i is decided by Eq. (4):  isF i = (tail < head ? head? tail ? len(Di))|| (tail ? head ? size? tail + head ? len(Di)), (4)  where ?||? is the operation of ?or?.

For example, assume that a window is with size of 10,  i.e., size = 10 and it locates at from addresses 20 to 29.

D1D2Dn  headtail  Buffered Data  Data Stream  Encryption  E1En Cipher Stream  Sliding Window  E2  winwin+size-1  Fig. 4. Stream Control Model  Here, we denote the starting address of the window as win, i.e, win = 20. Sometime head and tail are 23 and 26, respectively. Now data of a user with length of 6 want to flow in the window. Because isF i is true after the calculation of Eq. (4), these data can flow in the window.

Then, the data will be put into addresses 26, 27, 28, 29, 20, and 21. For conciseness, we use [x, y] to denote the space between addresses x and y. In other words, the data will be put into [26, 29] ? [20, 21], where ? is the operation of union. And then, tail will be changed to 22.

Note that once tail goes beyond the next byte to the end of the window, which will restart from the beginning of the window. Formally, if isF i is ture and tail+ len(Di) > win+size, the data will be put into two separate parts. One part is from tail to win + size ? 1 and the other is from win to tail + len(Di) ? size ? 1.

Then, tail will rollback to tail + len(Di) ? size. For the above example, because tail + len(Di) = 26 + 6 = 32 > win+ size = 20+10 = 30, the data will be put into from tail = 26 to win + size ? 1 = 29 and from win = 20 to tail+ len(Di)? size?1 = 26+6?10?1 = 21. And then, tail changes to tail + len(Di)? size = 26 + 6? 10 = 22.

In another aspect, i.e., isF i == true and tail + len(Di) < win + size, the data will be put into from tail to tail + len(Di) ? 1 and then tail will change to tail+ len(Di). For the above same window but different len(Di) = 3, the data will be put into [26, 28] and tail will change to 29.

In the last aspect, i.e., isF i == true and tail + len(Di) == win + size, the data will be put into from tail to tail+ len(Di)?1 and then tail will change to win.

For the above same window but different len(Di) = 4, the data will be put into [26, 29] and tail will change to 20.

In summary, if data of a user can flow in the window, the storage space s and tail can be calculated by Eq. (5):  s =  ????? [tail, tail + len(Di) ? 1] ? [win, tail + len(Di) ? size ? 1]  if isF i = true ? tail + len(Di) > win + size; [tail, tail + len(Di) ? 1]  if isF i = true ? tail + len(Di) ? win + size (5)  and Eq. (6):  tail =  ??? tail + len(Di) ? size if isFi = true ? tail + len(Di) > win + size;  tail + len(Di) if isFi = true ? tail + len(Di) < win + size; win if isFi = true ? tail + len(Di) = win + size  (6)    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2697446, IEEE Transactions on Big Data   respectively.

3.4 Sensing Velocity Model The sliding window can be used not only to tame a burst data stream, but also to sense the current velocity of the stream. We propose a methodology of first sensing the current bs of the window and then determining the level of the velocity by comparing to some pre-set thresholds.

Because the realtime velocity changes constantly, it is hard to sense the velocity precisely. But we can use bs of the window to approximate the velocity. Larger bs in the window means higher velocity of the data stream, and vice versa.

Fig. 5 illustrates an example of sensing velocity. At a given time, the sliding window has buffered bs data, where bs can be calculated by Eq. (2). Then bs com- pares with the thresholds. If the value of bs locates in the middle of two thresholds, i.e., thresholdi ? bs < thresholdi+1, then processors will scale to the corre- sponding frequency fi+1.

As shown in Fig. 5, bs of the buffer data locates in the middle of threshold2 and threshold3, so the processor will scale its frequency to f3. Generally, assume that a GPU can work at n fixed frequencies F = {f1, ..., fn} and n ? 1 thresholds can be set up to determine which one frequency f should be chosen. Eq. (7) describes the calculation method in detail:  f =  ??? f1, if bs < threshold1fi+1, if thresholdi ? bs < thresholdi+1 ? i ? {1, ..., n? 2}fn, if bs ? thresholdn?1 (7)  With the change of velocity, bs will change and result in the change of frequency dynamically. By this way, the energy consumption will reduce. The rational will be de- scribed in next Subsection 3.5. The bs of sliding window  Buffered Data  Used-volume: bs  Used-volume: bs  Threshold1 Threshold2 Threshold3 Thresholdn  Set frequency 3  Sliding window  Fig. 5. Sensing Velocity Model  can affect the current GPU frequency. Because a GPU must work at some fixed frequencies F = {f1, ..., fn} and a certain frequency fi represents a certain computation speed, a relationship can be established as Eq. (8):  fx fy  ? thresholdx thresholdy  , (8)  where fx and fy represent two different fixed frequen- cies, respectively; whereas thresholdx and thresholdy represent two different thresholds, respectively. If know- ing a fixed frequency fx and the corresponding  thresholdx, for any fy in the fixed frequency set, the cor- responding thresholdy can be figured out. Through this ratio relationship, a threshold can be roughly determined by another.

A GPU has two sorts of different frequencies, one of which is core frequency fc, and the other is memory frequency fm. For example, K20M has six pairs of frequencies as listed in Table 1.

TABLE 1 Characteristics of K20M  # fc (MHz) fm (MHz) tr condition  1 324 324 0.43 bs < 0.43? size 2 614 2600 0.81 0.43? size ? bs < 0.81? size 3 640 2600 0.84 0.81? size ? bs < 0.84? size 4 666 2600 0.88 0.84? size ? bs < 0.88? size 5 705 2600 0.93 0.88? size ? bs < 0.93? size 6 758 2600 NA bs ? 0.93? size  When calculating threshold ratio tr, we should consid- er these two sorts of frequencies together, but AES is a compute-intensive application as analysed in Subsection 3.2. Therefore, we can grasp the main contradiction, i.e., we can consider only the core frequency. Therefore we can get five different threshold ratios tri using Eq. (9) on K20M:  tri = fci fc6  , i = 1, ..., 5. (9)  These tri are calculated and listed in the fourth column of Table 1. If the sliding widow is large as size, then we can get the conditions for setting different frequency according to current bs. For K20M, these conditions are listed in the fifth column of Table 1. Specifically, because fc1 fc6  = 324758 = 0.43, when bs < 0.43 ? size, GPU core frequency and memory frequency are set to fc1 and fm1, respectively.

3.5 Saving Energy Model The data stream produced by massive users has the properties of burstiness, variance, realtime, etc. These properties will bring the possibility of reducing the energy consumption of streaming encryption. When the velocity of the data stream becomes slow, scaling down the frequency of processors appropriately will reduce energy consumption E. The rationale will be discussed as follows.

The power of a processor P is consist of static power Ps and dynamic power Pd, i.e., P = Ps + Pd. Ps is determined by some factors, such as circuit technology, chip layout, etc. Ps is not impacted by the frequency of the processor. Whereas, Pd is determined by the power formula [30] of CMOS (Complementary Metal Oxide Semiconductor) circuits shown in Eq. (10):  Pd = ACV 2f, (10)  where A is the switching activity factor, C is the ca- pacitance, V is the supply voltage, and f is the clock    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2697446, IEEE Transactions on Big Data   frequency. Furthermore, V and f have the relationship as shown in Eq. (11):  f = K (V ? VT )?  V , (1 ? ? ? 2), (11)  where VT is the threshold voltage while K and ? are the parameters related to manufacturing technique.

Because VT can be omitted because its value is usually very small compared to V [31]. An approximate relation formula is yielded as Eq. (12):  f ? KV ??1. (12)  Then, Eq. (13):  Pd = ACkf?, ? = ? + 1  ? ? 1 , k = K  1?? , (13)  can be yielded from Eqs. (10) and (12). Hence, Pd is proportional to ? power of f , where ? = ?+1??1 .

By default, GPUs are set at the second highest frequen- cies to keep high performance and good stability. After sensing velocity of sliding window, energy can be saved by scaling the frequency of GPUs corresponding to the velocity of data stream.

Fig. 6 demonstrates the rational. In the top of the figure, a data stream with variant velocity flows in a sliding window, and then it flows out and is shaped to a fixed velocity.

When the stream is encrypted, two different solutions are shown in the figure. One is non-scaling frequency as shown in the part pointed by the arrow marked b. The other is scaling frequency as shown in the part pointed by the arrow marked a.

The non-scaling frequency encrypts the data stream in the fixed frequency and consumes energy unchange- ably. The scaling frequency encrypts the data stream at different frequencies according to the velocity. Scaling down the frequency will cause low energy consumption according to Eq. (13).

In addition, because the default frequency is the sec- ond highest, the non-scaling frequency will consume more energy than scaling frequency. The reddish slash grid represents the static energy consumption, which is the same in the two solutions. The blue back slash grid represents the dynamic energy consumption. Therefore, the yellow crossover grid represents the saved energy.

4 ALGORITHMS  In this section, a Low-Energy Consumption Parallel AES (LECPAES) algorithm is proposed based on the above sections. As comparisons, GP (GPU Parallel) , CP (CPU Parallel), and CS (CPU Serial) AES are described in this section also.

Sliding Window  Speed Data Stream  Time  Flow in  Speed Data Stream  Flow out  Time  Power  At freq. 1  Time  At freq. 3 At freq. 2  Power  At fixed  freq.

Time Encryption at scaled frequencyEncryption at fixed frequency  Static Energy  Dynamic Energy  Static Energy  Dynamic Energy  ab  Saved Energy  Fig. 6. Comparison of energy consumption between scaled freq. and fixed freq.

4.1 LECPAES Alg. 1 describes the details of LECPAES in the pseudo code manner. The algorithm can encrypt users? plain text streams to cipher text streams.

Nvidia Management Library (nvml) [32] provides the abilities of setting frequencies of GPUs and retrieving the energy consumption. Therefore, in Line 1, the algorithm initializes nvml. If successful, then the algorithm gets the GPU device handle in Line 2. Because a GPU must work at some fixed frequencies, the algorithm retrieves all supported frequencies in Line 3. Based on these frequencies, threshold ratios can be calculated as Eq. (9) in Line 4. In Line 5, the algorithm establishes a sliding window with size of size and sets the initialize head and tail for buffering data and sensing velocity.

Because this algorithm needs to process users? data stream, from Lines 6 to 12, a ?while? loop will be performed continuously until a stop command is issued.

In Line 7, the algorithm selects a frequency combination f based on the current used ratio of the sliding window using the method in Eq. (7).

In Line 8, the algorithm uses nvmlDeviceSetApplication- sClocks in nvml to set GPU frequency to f . After that, the current plainStream in the window is encrypted by parallel AES on the GPU in Line 9. And then, the cipherStream can be flowed out in Line 10.

Because the state of window changes, head and tail are moved as described in Subsection 3.3 in Line 11. If stop command is issued, the loop will terminate. And then nvml should be shutdown as shown in Line 13.

4.2 GP Alg. 2 shows the pseudo code of GP. The algorithm is different from LECPAES in scaling GPU frequencies. GP does not need to scale GPU frequencies, so that it does not use nvml and does not sense the velocity of the current stream.

GP first initializes a sliding window in Line 1, and then circularly executes encryption task until a stop command is issued. The encryption task includes encrypting the users? data in current window on the GPU in parallel as    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2697446, IEEE Transactions on Big Data   Algorithm 1 LECPAES Algorithm Input:  Plain text stream, plainStream; Keys, keys;  Output: Cipher text stream, cipherStream;  1: Initialize nvml; 2: Get GPU device handle; 3: Retrieve supported n kinds of frequencies fc and  fm; 4: Compute threshold ratios using Eq. (9); 5: Initialize a sliding window with size size; 6: while (!stop) do 7: Select frequency f by Eq. (7); 8: Set GPU frequency to f ; 9: Encrypt plainStream in current window using  keys on GPU in parallel; 10: Flow out the current encrypted cipher  cipherStream; 11: Move head and tail as the description in Subsec-  tion 3.3; 12: end while 13: Shutdown nvml.

Algorithm 2 GP Algorithm Input:  Plain text stream, plainStream; Keys, keys;  Output: Cipher text stream, cipherStream;  1: Initialize sliding window with size size; 2: while (!stop) do 3: Encrypt plainStream in current window using  keys on the GPU in parallel; 4: Flow out the current encrypted cipher  cipherStream; 5: Move head and tail as the description in Subsec-  tion 3.3; 6: end while  shown in Line 3, flowing out the cipher stream as shown in Line 4, and sliding the window as shown in Line 5.

4.3 CP Alg. 3 exhibits the pseudo code of CP. The algorithm differs GP on parallelism method. CP employs CPU parallelism rather than GPU parallelism adopted in GP.

Therefore, CP derives multiple threads according to the number of CPU cores in Line 3. If the CPU supports hyper-threading, CP will derive twice as many of threads of the number of CPU cores. For example, Two Intel Xeon E5-2640 v2 CPUs support hyper-threading and have 16 cores totally, so that CP derives ? = 16 ? 2 = 32 threads when running on these CPUs. The derived threads will undertake the encryption evenly in parallel as shown in Line 4. After the encryption finished, the  current encrypted cipher cipherStream flows out. And then head and tail are moved as the description in Subsection 3.3.

Algorithm 3 CP Algorithm Input:  Plain text stream, plainStream; Keys, keys;  Output: Cipher text stream, cipherStream;  1: Initialize sliding window with size size; 2: while (!stop) do 3: Derive ? threads according to the numbers of CPU  cores; 4: Perform encryption by each thread in parallel for  even users? data; 5: Flow out the current encrypted cipher  cipherStream; 6: Move head and tail as the description in Subsec-  tion 3.3; 7: end while  4.4 CS  Alg. 4 describes the pseudo code of CS, which differs CP on whether adopting parallelism. CS does not adopt CPU parallelism and serially performs encryption for the buffered data in the window as shown in Line 3.

Algorithm 4 CS Algorithm Input:  Plain text stream, plainStream; Keys, keys;  Output: Cipher text stream, cipherStream;  1: Initialize sliding window with size size; 2: while (!stop) do 3: Perform encryption serially for the buffered users?  data; 4: Flow out the current encrypted cipher  cipherStream; 5: Move head and tail as the description in Subsec-  tion 3.3; 6: end while  5 EXPERIMENTS 5.1 Experiment Setup  5.1.1 Experiment Environment Experiments are conducted on a heterogeneous platform with two Intel Xeon CPUs and one Nvidia K20M GPU.

The configuration of the platform is listed in TABLE 2.

Experiments are conducted with four different algo- rithms for a series of user data streams. The purpose is to compare them and validate the effect of LECPAES,    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2697446, IEEE Transactions on Big Data   TABLE 2 Configuration of the Experimental Platform  GPU CPU  NVIDIA Tesla K20M (13 multiproces- sors)  Two Intel Xeon E5-2640 v2 (support hyper- threading)  Six groups of supported clock frequen- cies of core and memory in unit of MHz: (324, 324), (614, 2600), (640, 2600), (666, 2600), (705, 2600), (758, 2600)  Clock rate: 2.0GHz  Total: 2496 cores Total: 16 cores  and further find some useful conclusions. The four algo- rithms are CPU Serial (CS) AES , CPU Parallel (CP) AES , GPU Parallel (GP) AES , and LECPAES. They all employ sliding windows but differentiate in encryption manner.

CS executes serial encryption for the buffered data one user by one user on the CPUs, whereas CP executes parallel encryption for the current buffered data in the window on the CPUs. GP executes parallel encryption for the current buffered data in the window on the GPU. LECPAES differs GP in dynamically scaling the frequency to reduce the energy consumption.

The user data stream is mimicked as 12, 000 users sub- mitting their data cyclically. The period, called latency, represents different flow velocity. Longer latency means lower stream velocity, and vice versa. Thus, the exper- iments can implement different streams with different velocity. The users will produce data with length be- tween 35 KB and 150 KB to simulate the data in typical web accesses randomly, since this length range has been gathered statistics by Levering et al. [33].

When the velocity of the stream changes, sometimes the velocity of inflow may be faster than the velocity of outflow. If the sliding window has not enough space to load more data, this will make some data miss, i.e., can- not be encrypted this time. In this situation, these missed data will be retransmitted later. In the experiments, the time for retransmission is set to 2? latency.

In order to get the energy consumption of the GPU, the experiments use nvmlDeviceGetPowerUsage in nvml to get the instant power gpower every 50 ns and Event mechanism in CUDA to record the spent time gtime .

In another aspect, the experiments use PowerGadget provided by Intel to sample the instant power of the CPUs cpower. PowerGadget bases on RAPL (Running Average Power Limit) library which can acquire the data of power of Intel CPUs recorded in MSRs (en- ergy Model-Specific Registers). The spent time on the CPUs ctime are recorded by gettimeofday for Linux or QueryPerformanceCounter for Windows.

Further, the energy consumption Eg on the GPU can be calculated by Eg = gpower ? gtime. Similarly, the energy consumption Ec on the CPUs can be calculated by Ec = cpower ? ctime. Therefore, the total energy consumption on the CPUs and GPU E can be calculated as: E = Eg + Ec.

Finish  Start  Produce a user data  every latency  Get the buffered  data and record tail  Initialize  thread 0  Acquire power data  on the CPUs every  50 ns  Acquire power data  on the GPU every  50 ns  Monitor whether the  stream completes  thread 1 thread 2 thread 3 thread 4  Finished?

Y  N  Encrypt the buffered  data using CS, CP,  GP, or LECPAES  Record the finish  time  Record the start  time Window has  enough space?

Put the data to the  window and move  tail  Y  Move head  Wait 2 latency N  Fig. 7. Experimental Process  5.1.2 Experimental Process The experiments first derives five threads to execute different tasks. The first thread produces the plain text data for users every latency to form a data stream. If the sliding window has enough space, the thread will put the data into the window and move tail and then continue; otherwise the thread will wait 2? latency and then continue.

The second thread is in charge of triggering the encryp- tion task by one of CS, CP, GP, or LECPAES according to different experiments. Once the encryption completes for the buffered data, the thread will move head and then continue.

The next two threads acquire the data of power on the CPUs and GPU every 50 ns, respectively. The last thread terminates the application when the data stream has been processed completely. The full process flow can be seen in Fig. 7.

The experiments are conducted for the four algorithms separately. For implementing different velocities, the ex- periments choose seven different latencies, i.e., 100, 500, 1000, 1500, 2000, 2500, and 3000 us. The results will be reported in the next subsection.

5.2 Experimental Results  The experimental results will be reported from several aspects including energy consumption, throughput, re- transmission rate, and energy saving, in this section.

5.2.1 Energy Consumption In the experiments, CS, CP, GP, and LECPAES encrypt some users? data streams with different latency separate- ly. Their energy consumptions are drawn in Fig. 8.

From the figure, CS consumes far more energy than the other three algorithms. Moreover, CS consumes also the same energy for the streams with different latency.

The average energy consumption of CS is 10520.77 J.

2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2697446, IEEE Transactions on Big Data   0 500 1000 1500 2000 2500 3000       12000  CS  CP  GP  LECPAES En  er gy  C on  su m  pt io  n (J  )  Latency (us)  Fig. 8. Energy Consumption  CP, GP, and LECPAES increase energy consumption with the increase of latency. Specifically, CP increases energy consumption from 1259.91 J for the stream with latency of 100 us to 2218.47 J for the stream with latency of 3000 us. GP increases energy consumption from 381.31 J for the stream with latency of 100 us to 2839.52 J for the stream with latency of 3000 us. LECPAES increases energy consumption from 395.95 J for the stream with latency of 100 us to 1974.57 J for the stream with latency of 3000 us.

LECPAES consumes less energy than GP for the streams with latency larger than 500 us, but more energy than GP for the streams with latency less than 500 us.

For the stream with latency of 500 us, GP and LECPAES consume energy of 641.60 J and 524.89 J, respectively.

GP consumes less energy than CP for the streams with latency less than 1500 us, but more energy than CP for the streams with latency larger than or equal to 1500 us. For the stream with latency of 1500 us, GP and CP consume energy of 1522.14 J and 1378.80 J, respectively.

LECPAES consumes less energy than CP for all the streams in the figure, but their difference of energy consumption becomes less with the increase of latency.

For the stream with latency of 100 us, LECPAES and CP consume energy of 395.95 J and 1259.91 J, respectively.

Here, LECPAES consumes 899.96 J less energy than CP.

Nevertheless, for the stream with latency of 3000 us, LECPAES and CP consume energy of 1974.57 J and 2218.47 J, respectively. Now, LECPAES consumes 243.90 J less energy than CP.

5.2.2 Throughput In this Subsection, throughput will be reported. Here, throughput represents the efficiency of a stream pass- ing the encryption server. It is defined as a quotient throughput of the data volume of the stream dv dividing the pass time pt, i.e., throughput = dvpt . Note that dv refers to the available data volume, i.e., the same data retransmitted multiple times will be calculated only one time. In addition, pt represents the entire time of from  the time of the stream flows in to the time of the stream flows out.

Fig. 9 portrays the throughput of the four algorithms for the streams with different latencies. From Fig. 9, the throughput of LECPAES, GP, and CP decreases as the latency increases, but the throughput of CS keeps stable. Specifically, LECPAES decreases throughput from 282.13 MBps for the stream with latency 100 us to 29.32 MBps for the stream with latency 3000 us. GP decreases throughput from 287.56 MBps for the stream with laten- cy 100 us to 30.31 MBps for the stream with latency 3000 us. CP decreases throughput from 61.44 MBps for the stream with latency 100 us to 28.45 MBps for the stream with latency 3000 us. CS keeps the throughput between 5.55 MBps and 5.62 MBps.

0 500 1000 1500 2000 2500 3000 -100        Th ro  ug hp  ut (M  Bp s)  Lantency (us)  CP CS GP LECPAES  Fig. 9. Throughput  In the experiments, GP has the highest throughput, but LECPAES is only a little less than GP. CS has the lowest throughput. The average throughput of CS, CP, GP, and LECPAES are 5.57 MBps, 48.02 MBps, 99.18 MBps, and 97.59 MBps, respectively.

5.2.3 Retransmission Rate  When a stream has higher velocity of inflow than that of outflow, some users? data can be buffered in the sliding window. However, when the sliding window does not have enough space to load more data, current users? data will be lost and need to be retransmitted.

Retransmission will take place after 2 ? latency of the missing service. Retransmission will reduce the QoS (Quality of Service) and impact the experience of users.

Retransmission rate is defined as the ratio of retransmis- sion to all transmission and reflects the QoS.

Table 3 lists the retransmission rate processed by d- ifferent algorithms for the streams with different laten- cies. The retransmission rates decrease as the latencies decrease. CS has the highest retransmission rate, and GP has the lowest retransmission rate.

2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2697446, IEEE Transactions on Big Data   TABLE 3 Retransmission Rate  latency (us) CS CP GP LECPAES  100 61.13075 4.60500 0.64567 0.73242 500 14.30458 0.73592 0.01425 0.01492  1000 6.54617 0.40750 0.00650 0.00667 1500 4.50083 0.25417 0.00358 0.00458 2000 3.27467 0.18333 0.00133 0.00258 2500 2.62733 0.08543 0.00000 0.00000 3000 2.02175 0.03786 0.00000 0.00000  5.2.4 Energy Saving Energy saving ratio Esrij represents the saving energy ratio of one algorithm Ai over another one Aj in this paper. It can be calculated as Esrij = (EAj ?EAi)/EAj , where EAi and EAj are the energy consumption of Ai and Aj , respectivley. Table 4 lists the energy saving ratio of LECPAES over the other three algorithms. The second, third, and fourth columns represent the ratios of LECPAES over GP, CP, and CS, respectively. The last row represents the average ratios.

The energy saving ratio of LECPAES over GP (L2G) increases as the latency increases. Its average value is 23.4%. The energy saving ratio of LECPAES over CP (L2CP) decreases as the latency increases. Its average value is 33.7%. The energy saving ratio of LECPAES over CS (L2CS) decreases as the latency increases. Its average value is 89.5%.

TABLE 4 Energy Saving Ratio of LECPAES  latency L2G L2CP L2CS  100 -0.03842 0.69446 0.96222 500 0.18190 0.59456 0.94992  1000 0.28982 0.43694 0.92702 1500 0.29878 0.22588 0.89846 2000 0.30019 0.16544 0.86940 2500 0.30272 0.13169 0.84399 3000 0.30461 0.10994 0.81167 avg 0.23423 0.33699 0.89467  Table 5 lists the energy saving ratios of GP over CP, GP over CS, and CP over CS, respectively. The last row also represents the average ratios.

The energy saving ratio of GP over CP (G2CP) decreas- es as the latency increases. Its average value is 8.5%. The energy saving ratio of GP over CS (G2CS) decreases as the latency increases also. Its average value is 85.3%. The energy saving ratio of CP over CS (CP2CS) decreases as the latency increases. Its average value is 84.9%.

5.3 Summary  In summary, the comparison of LECPAES and GP from the aspects of energy consumption, throughput, and retransmission rate can be seen in Table 6. Because  TABLE 5 Energy Saving Ratio of GP and CP  latency G2CP G2CS CP2CS  100 0.70576 0.96362 0.87636 500 0.50442 0.93878 0.87647  1000 0.20716 0.89723 0.87038 1500 -0.10396 0.85519 0.86883 2000 -0.19255 0.81339 0.84352 2500 -0.24528 0.77626 0.82033 3000 -0.27995 0.72917 0.78840 avg 0.08509 0.85338 0.84918  energy saving ratio is a relative value and its statistical difference is meaningless, it will not be listed in the following Tables 6 to 8. The differences in the table are calculated by the corresponding value of LECPAES minus that of GP. From the table, LECPAES costs less energy, but with a little lower throughput and a little higher retransmission rate, than GP. The trend is that the amount of variation becomes smaller with higher latency.

On average, compared to GP, LECPAES saves energy of 435.37620 J, reduces throughput of 1.59055 MBps, and increases retransmission rate of 0.00109.

TABLE 6 LECPAES vs GP  latency (us)  Energy Consumption Difference (J)  Throughput Difference  (MBps)  Retransmission Rate Difference  100 14.64789 -5.43462 0.00233 500 -116.70678 -3.58103 0.00172  1000 -312.04820 -0.13804 0.00233 1500 -454.78482 -0.84615 0.00050 2000 -587.49921 -0.05752 0.00075 2500 -726.29618 -0.08636 0.00000 3000 -864.94611 -0.99010 0.00000 avg -435.37620 -1.59055 0.00109  The comparison of LECPAES and CP can be seen in Table 7, where the differences are calculated by the corresponding value of LECPAES minus that of CP. In this table, the tread has changed compared to that in Table 6. LECPAES costs less energy, but with higher throughput and lower retransmission rate than CP. On average, compared to CP, LECPAES saves 477.66191 J energy, increases throughput of 49.56776 MBps, and reduces retransmission rate of 0.78299.

The comparison of LECPAES and CS can be seen in Table 8, where the differences are calculated by the corresponding value of LECPAES minus that of CS. In this table, the tread is same with that in Table 7, but with larger different value. On average, compared to CS, LEC- PAES saves energy of 9410.76959 J, increases throughput of 92.02511 MBps, and reduces retransmission rate of 13.60646.

2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2697446, IEEE Transactions on Big Data   TABLE 7 LECPAES vs CP  latency (us)  Energy Consumption Difference (J)  Throughput Difference  (MBps)  Retransmission Rate Difference  100 -899.95776 220.68415 -4.40675 500 -769.74009 93.33504 -0.83745  1000 -593.37253 26.72644 -0.23183 1500 -311.44435 2.27806 -0.00317 2000 -271.50469 1.93408 -0.00174 2500 -253.71760 1.14931 0.00000 3000 -243.89636 0.86728 0.00000 avg -477.66191 49.56776 -0.78299  TABLE 8 LECPAES vs CS  latency (us)  Energy Consumption Difference (J)  Throughput Difference  (MBps)  Retransmission Rate Difference  100 -10085.40719 276.50737 -60.13958 500 -9955.36973 147.90988 -14.66937  1000 -9712.35478 77.32657 -7.34900 1500 -9444.21666 51.49978 -4.76192 2000 -9117.86565 37.80085 -3.47817 2500 -9050.31162 29.36779 -2.68150 3000 -8509.86151 23.76355 -2.16567 avg -9410.76959 92.02511 -13.60646  6 DISCUSSION Through the experiments, we have the following find- ings.

? For fast streams, LECPAES consumes little more en- ergy than GP. For slow streams, LECPAES consumes the least energy compared to GP, CP, and CS. This is because, for faster streams, LECPAES will result in higher retransmission rate and lower throughput, but the profits brought by scaling frequency cannot offset them. As the velocity of streams increases, LECPAES gets more profits from scaling frequency and becomes saving more energy than GP. This can be seen that LECPAES, GP, and CP will consume more energy with the increase of the velocity of streams. CS consumes almost the same energy with the increase of the velocity of streams. This is be- cause CS performs encryption in serial and its speed is far slower than the speed of inflow. Moreover, faster streams will cause data retransmission later, thus the energy consumption of CS is determined by the time of draining the stream. In the experiments, even the slowest stream is still faster than the speed of encryption outflow, thus CS will spend almost the same time of encrypting the data for streams with different velocity and consume almost the same energy.

? With the decrease of the velocity of streams, LEC- PAES, GP, and CP decrease their throughput, but CS keeps stable throughput. Faster streams will cause  higher retransmission rate, but when calculating throughput, only the successfully encrypted data are used. That is to say these streams have the same amount of data. Because CS encrypts the streams at the slowest speed and the speed is far slower than the veloctiy of inflow, CS produces the lowest throughput and keeps stable. CP produces the sec- ond lowest throughput because of higher encryption speed than CS but slower encryption speed than GP and LECPAES. For slower streams, CP, GP, and LECPAES will spend more time to complete the encryption, thus they decrease throughput for slow- er streams. Because GP and LECPAES have higher encryption speed than CP, thus CP and LECPAES produce higher throughput than CP. In addition, because LECPAES dynamically scales the frequency, LECPAES has a little slower encryption speed than GP and results in a slightly lower throughput than GP.

? GP and LECPAES are suitable for fast streams, but LECPAES consumes less energy than GP. For fast streams, GP and LECPAES consume less energy, pro- duce higher throughput, and lower retransmission rate compared to CP and CS. This is because, the GPU undertakes full workload and produces better energy efficiency. Because GP and LECPAES encrypt data at high speed, they have higher throughput and lower retransmission rate. Moreover, LECPAES consumes less energy than GP through dynami- cally scaling frequency, but only slightly decreases throughput and slightly increases retransmission rate. In short, LECPAES is the best algorithm among them for fast and energy-saving stream encryption.

7 CONCLUSION In this work, in order to expedite encrypting streams in cloud environment in energy saving manner, a velocity- aware parallel encryption algorithm with low energy consumption, called LECPAES, is proposed.

In LECPAES, a sliding window is adopted and used to sense the velocity of current stream. Then a scaling frequency scheme is employed based on the current sensed stream velocity. The encryption is performed on the GPU in many threads fashion. As comparison, CS, CP are also designed for suiting stream data and are compared with GP and LECPAES from the aspects of energy consumption, throughput, and retransmission rate.

Some experiments are conducted in a server with one K20M GPU and two Xeon CPUs and show the following results. (1) LECPAES can reduce energy consumption, slightly reduce throughput, and slightly increase retrans- mission rate compared with GP. (2) LECPAES can reduce energy consumption, increase throughput, and reduce retransmission rate compared with CP. (3) LECPAES can reduce energy consumption largely, increase throughput largely, and reduce retransmission rate largely compared    2332-7790 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TBDATA.2017.2697446, IEEE Transactions on Big Data   with CS. Therefore, LECPAES is suitable for fast stream encryption in energy saving manner through sensing velocity.

This paper represents our initial work in stream en- cryption with low energy consumption. We consider only the stable streams with fixed inflow speed. In future, we plan to extend this work to suit streams with complicated inflow speed.

