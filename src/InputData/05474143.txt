An Efficient Method for Incremental Mining of Share-Frequent Patterns

Abstract?The share measure of itemsets has been proposed to discover useful knowledge about numerical values associated with items in a transaction database. Therefore, share-frequent pattern mining problem becomes a very important research issue in data mining. However, the existing algorithms of share-frequent pattern mining are based on static databases.

Moreover, they are not suitable for interactive mining. In this paper, we propose a novel tree structure IncrShrFP-Tree (Incremental Share-Frequent Pattern Tree) for incremental and interactive share-frequent pattern mining. It is effective for incremental and interactive mining to utilize the previous tree structure and to use the previous mining results when a database is updated or a minimum support threshold is changed. It needs maximum two database scans to calculate the resultant share-frequent patterns in incremental databases.

Extensive performance analyses show that our method is very efficient for incremental and interactive share-frequent pattern mining.

Keywords-Data mining; knowledge discovery; share-frequent pattern; incremental mining.



I. INTRODUCTION  Data mining discovers potentially useful information from databases. Frequent pattern mining [1], [7], [9] plays an essential role in many data mining tasks such as mining association rules, discovering patterns having useful corre- lations, constraint based patterns, and so on. However, binary frequency (either absent or present) or support of a pattern may not be a sufficient indicator for finding meaningful patterns from a transaction database because it only reflects the number of transactions in the database which contain that pattern. In our real world scenarios, one user can buy multi- ple copies of items. Therefore, traditional frequency/support measure cannot analyze the exact number of items (itemsets) purchased. Accordingly, itemset share approaches [3], [4], [5], [6], [10], [11] have been proposed to discover more important knowledge from databases. The share measure can provide useful knowledge about the numerical values that are typically associated with the transaction items. In addition to real world retail markets, share-frequent pattern mining is also useful to discover interesting web path traversal patterns as a user may spend different time units at different websites. Moreover, the share measure can be applied to other application areas such as biological gene databases, stock tickers, network traffic measurements, web-server logs, data feeds from sensor networks and telecom call records.

The existing share-frequent pattern mining algorithms [3], [4], [5], [6], [10], [11] have considered fixed database and need multiple database scans to find the share-frequent patterns. They did not consider that one or more transactions could be deleted/inserted/modified in the database. By using incremental and interactive share-frequent pattern mining, we can use the previous data structures and mining results and avoid the unnecessary calculations when the database is updated or the mining threshold is changed. In our real world databases, new transactions can be added and old transactions can be deleted or modified frequently. Consider in our real world retail market customer X has bought three pens, four pencils, and one eraser, and customer Y has bought one computer mouse. After sometime, customer Z may buy two loaves of bread and one carton of milk, customer X may return two pencils, and customer Y may return the mouse. Therefore, new transactions in the real world market can be frequently added, and old transactions can be modified or deleted. Hence, it is necessary to design methods that can handle the dynamic nature of real world datasets.

Moreover, the data structures presented in the previous works do not have the ?build once mine many? property (by building the data structure only once, several mining operations can be done) which is very necessary for in- teractive mining. As a consequence, their data structures are designed for a particular minimum threshold. If the minimum threshold is 30%, then they can calculate only the result for that specified threshold only. If the user again wants to know the result for minimum threshold 20%, then they have to build their data structures again and start their calculations from the very beginning. In our real world scenarios, users need to extract knowledge using different minimum thresholds according to their application interest from a database. Therefore, the ?build once mine many? property of a data structure is necessary to discover this type of knowledge without building the data structure again.

Motivated by these real world scenarios, we propose an efficient solution of these major problems of the ex- isting algorithms of share-frequent pattern mining. In this paper, we propose a novel tree structure IncrShrFP-Tree (Incremental Share-Frequent Pattern Tree) for incremental and interactive share-frequent pattern mining. Our method exploits a pattern growth mining [7] approach to avoid the  2010 12th International Asia-Pacific Web Conference  DOI 10.1109/APWeb.2010.52     level-wise candidate generation-and-test problem [1]. It can handle the incremental data by scanning a database at most two times in contrast to scanning a database several times by the existing algorithms. Our proposed data structure has the ?build once mine many? property for interactive mining.

The remainder of this paper is organized as follows.

In Section II, we describe related work. In Section III, we describe the share-frequent pattern mining problem. In Section IV, we develop our proposed tree structure for incremental and interactive share-frequent pattern mining.

Here, we also describe the mining process for the proposed tree structure. In Section V, our experimental results are presented and analyzed. Finally, in Section VI, conclusions are drawn.



II. RELATED WORK  The problem of frequent pattern mining is to find the complete set of patterns satisfying a minimum support in a transaction database. The downward closure property [1] is used to prune the infrequent patterns. This property tells that if a pattern is infrequent then all of its super patterns must be infrequent. The Apriori [1] algorithm is the initial solution of frequent pattern mining problem and very useful in association rule mining. But it suffers from the level-wise candidate generation-and-test problem and needs several database scans. The FP-growth [7] algorithm solved the problem of candidate generation-and-test by using a tree- based (FP-tree) solution. It needs only two database scans to find the resultant set of frequent patterns according to a user given minimum threshold. Some other research [8], [9], [12] has been done for frequent pattern mining. This traditional frequent pattern mining problem considers only the binary occurrences (0/1), i.e., either absent or present of the items in one transaction. Even though Srikant et al.

[2] proposed a method to deal with quantitative attributes of patterns, it is still based on the support measure.

Carter et al. [6] first introduced the share-confidence model to discover useful knowledge about numerical at- tributes associated with items in a transaction. The ZP and ZSP [3], [5] algorithms use heuristic methods to generate all the candidate patterns. Moreover, they cannot rely on the downward closure property and therefore their searching methods are very time-consuming and do not work effi- ciently in large databases. Some other algorithms such as SIP, CAC and IAB [3], [4], [5] have been proposed to mine share-frequent patterns but they may not discover all the share-frequent patterns. The Fast Share Measure (ShFSM) [10] algorithm improves the previous algorithms by using the level closure property. This property cannot maintain the downward closure property. However, ShFSM wastes the computation time on the join and the prune steps of candidate generation in each pass, and generates too many useless candidates.

The Direct Candidates Generation (DCG) [11] algorithm outperforms the ShFSM algorithm by generating candidates directly without the prune and the join steps in each pass.

Moreover, the number of candidates generated by DCG is less than ShFSM. DCG can maintain the downward closure property by using the potential maximum local measure value (Definition 5) of an itemset which is actually the transaction measure value (Definition 6) of an itemset.

Nevertheless, DCG has a big problem of adopting the level- wise candidate generation-and-test methodology. Hence, its number of database scans is dependent on the maximum candidate length and it tests huge unnecessary candidate patterns. In the k-th pass, DCG scans the whole database to count the local measure value of each candidate k- itemset X and counts the potential maximum share value of each monotone (k+1)-superset of X [11]. Therefore, DCG generates and tests too many candidate patterns in the mining process. For example, if the number of distinct items is 10000, then it tests  (   ) two-element candidate patterns  in pass-1 to get the actual candidate patterns of pass-2. Its number of database scans is dependent on the maximum length of the actual candidate patterns. For example, if the maximum length of a candidate pattern is 4 (?abcd?), DCG has to scan the database 4 times to find all the share-frequent itemsets. If the maximum actual candidate length is 20, a total of 20 database scans are required. So, for a maximum actual candidate length N, a total of N database scans are required.

As for incremental mining, we mean a kind of mining technique which can be well applied for the dynamic en- vironment where a database grows and changes frequently.

Interactive mining means that repeated mining with different minimum support thresholds can be possible by utilizing the same data structure or previous mining results. Some research [14], [15], [17], [18] has been done for incremental and interactive mining algorithms based on the traditional frequent pattern mining. The IncWTP and WssWTP algo- rithms [16] are designed for incremental and interactive mining of web traversal patterns. However, these algorithms are not applicable for incremental and interactive share- frequent pattern mining.

None of the existing share-frequent pattern mining meth- ods proposed any solution for incremental mining, where lots of transactions can be added and deleted. Moreover, they need several database scans and do not have the ?build once mine many? property for interactive mining. Therefore, we propose a novel tree structure for incremental and interactive share-frequent pattern mining.



III. PROBLEM DEFINITION  We have adopted similar definitions presented in the previous works [3], [4], [5], [6], [10], [11]. Let I = {i1, i2, ......im} be a set of items and DB be a transaction     Table I EXAMPLE OF AN INCREMENTAL TRANSACTION DATABASE WITH COUNTING  TID Transaction Count Total count T1 {a, b, f, g} {2, 1, 2, 1} 6 T2 {a, c, e} {5, 3, 3} 11  Initial DB T3 {b, c, h} {3, 2, 2} 7 T4 {c} {5} 5 T5 {a, c, e, f, g} {1, 3, 1, 2, 1} 8 T6 {a, c, e, f} {4, 2, 1, 5} 12  db+ T7 {a, d} {1, 3} 4 T8 {b, c, d} {4, 3, 2} 9  database {T1, T2, ......Tn} where each transaction Ti ? DB is a subset of I.

Definition 1: The measure value mv(ip, Tq), represents the quantity of item ip in transaction Tq. For example, in Table I, mv(a, T2) = 5.

Definition 2: The transaction measure value of a transac- tion Tq denoted as tmv(Tq) means the total measure value of a transaction Tq and it is defined by,  tmv(Tq) = ?  ip?Tq mv(ip, Tq) (1)  For example, tmv(T7) = mv(a, T7) + mv(d, T7) = 1 + 3 = 4 in Table I.

Definition 3: The total measure value Tmv(DB) repre- sents the total transaction measure value of all the transac- tions in DB. It is defined by,  Tmv(DB) = ?  Tq?DB  ?  ip?Tq mv(ip, Tq) (2)  For example, Tmv(DB) = 62 in Table I. Here, DB = Initial DB + db+.

Definition 4: The itemset measure value of an itemset X in transaction Tq, imv(X,Tq) is defined by,  imv(X,Tq) = ?  ip?X mv(ip, Tq) (3)  where X = {i1, i2, .......ik} is a k-itemset, X ? Tq and 1 ? k ? m. For example, imv(bc, T3) = 3 + 2 = 5 in Table I.

Definition 5: The local measure value of an itemset X is defined by,  lmv(X) = ?  Tq?DBX  ?  ip?X imv(ip, Tq) (4)  where DBX is the set of transactions containing itemset X . For example, lmv(ac) = imv(ac, T2) + imv(ac, T5) + imv(ac, T6) = 8 + 4 + 6 = 18 in Table I.

Definition 6: The transaction measure value of an itemset X , denoted by tmv(X), is the sum of the tmv values of all the transactions containing X .

tmv(X) = Tmv(DBX) = ?  X?Tq?DBX tmv(Tq) (5)  For example, tmv(g) = tmv(T1) + tmv(T5) = 6 + 8 = 14 in Table I.

Definition 7: The share value of an itemset X , denoted as SH(X), is the ratio of the local measure value of X to the total measure value in DB. So, SH(X) is defined by,  SH(X) = lmv(X)  Tmv(DB) (6)  For example, SH(ac) = 18/62 = 0.29, in Table I.

Definition 8: Given a minimum share threshold,  minShare(?), an itemset is share-frequent if SH(X) ? minShare. If minShare is 0.25 (we can also express it as 25%), in the example database, ?ac? is a share-frequent itemset, as SH(ac) = 0.29.

Definition 9: The minimum local measure value, min lmv, is defined as  min lmv = ceiling(minShare ? Tmv(DB)) (7) In Table I, if minShare = 0.25, then min lmv = ceiling (0.25 ? 62) = ceiling (15.5) = 16. Hence, for any itemset X , if lmv(X) ? min lmv, then we can say that X is a share-frequent pattern.

The main challenge of share-frequent pattern mining area is the itemset share does not have the downward closure property. For example, ?a? is a share-infrequent item in Table I for minShare = 0.25 as SH(a) = 0.2096. How- ever, its super-pattern ?ac? is a share-frequent pattern as SH(ac) = 0.29. As a consequence, the downward closure property is not satisfied. This property can be maintained by using the transaction measure value of a pattern (Definition 6). For a pattern X , if tmv(X) < min lmv, then we can prune that pattern without further consideration. For example, in Table I if we consider minShare = 0.25, then tmv(g) = 14 < min lmv(16). According to the downward closure property, none of the super patterns of ?g? can be a candidate for share-frequent pattern and therefore we can easily prune ?g? at the early stage.



IV. INCRSHRFP-TREE: OUR PROPOSED TREE STRUCTURE  IncrShrFP-Tree arranges the items in lexicographic order and inserts them as a branch inside the tree. It explicitly     H-table  b:22  c:52  d:13  e:31  f:26  a:41  g:14  h:7  (a) After inserting initial DB (b) After inserting  db+  b:16  c:16  a:41  h:7  c:5  { }  b:6  f:6  g:6  c:31  e:31  f:20  g:8  d:4  d:9  b:7  c:7  a:25  b:6  f:6 h:7  g:6  c:19  e:19  f:8  g:8  c:5  { }H-table  b:13  c:31  e:19  f:14  g:14  a:25  h:7  Figure 1. Incremental maintenance of IncrShrFP-Tree  (a) Conditional-tree for item ?f?  H-table  c:20  e:20  a:26 a:26  { }  c:20  e:20  H-table  c:31  a:31 a:31  { }  c:31  (b) Conditional-tree for item ?e?  a:31  { }  b:16  H-table  b:16  a:31  (c) Conditional-tree for item ?c?  Figure 2. Mining process  maintains the tmv values of items in the header table and tree nodes. To facilitate the tree traversals, adjacent links are also maintained (not shown in the figures for simplicity).

Consider the initial database presented in Table I. The IncrShrFP-Tree constructed for the initial database (Table I) is shown in Fig. 1(a). The tmv value of T1 is 6 in Table I. When T1 is added into the tree, the first node in lexicographic order is ?a? with tmv = 6. The second node is ?b? with the same value and so on. After that T2 is inserted  Table II CALCULATION PROCESS OF SHARE-FREQUENT PATTERNS  No.

Candidate  tmv lmv SH Share-frequent  patterns patterns 1. af 26 16 0.258 Yes 2. cf 20 12 0.1935 No 3. ef 20 9 0.145 No 4. acf 20 17 0.274 Yes 5. aef 20 14 0.2258 No 6. cef 20 14 0.2258 No 7. acef 20 19 0.306 Yes 8. f 26 9 0.145 No 9. ae 31 15 0.2419 No 10. ce 31 13 0.2096 No 11. ace 31 23 0.37 Yes 12. e 31 5 0.08 No 13. ac 31 18 0.29 Yes 14. bc 16 12 0.1935 No 15. c 52 18 0.29 Yes 16. b 22 8 0.129 No 17. a 41 13 0.2096 No  into the tree. The tmv value of T2 is 11, and the branch in lexicographical order is ?a:11?, ?c:11?, and ?e:11?. Here ?a? is assigned the prefix sharing with the existing node ?a?, the tmv value of node ?a? is 6+11=17. For other items, new nodes are created. In a similar process, transactions up to T5 are added into the tree (shown in Fig. 1(a)).

Table I also shows that the initial database is incremented by adding one group of transactions DB+ (T6, T7, T8).

By following the same process adopted during the initial database construction, DB+ is inserted into the tree as shown in Fig. 1(b). Moreover, in a similar fashion, deletion and modification operations can be done in an IncrShrFP- Tree. However, an IncrShrFP-Tree holds property 1 that makes it suitable for pattern growth mining.

Property 1:: The tmv value of any node in the tree is greater than or equal to the sum of the total tmv values of its children.

According to Property 1, we can apply a pattern growth mining approach in the IncrShrFP-Tree by using the tmv value. Consider the database of Table I and minShare = 0.25 in that database. The final tree created for that database is shown in Fig. 1(b).

At first the conditional tree of the bottom most item ?f ? (shown in Fig. 2(a)) is created by taking all the branches prefixing the item ?f ? and deleting the nodes containing an item which cannot be a candidate pattern with the item ?f ?. Obviously, item ?b? cannot form any candidate pattern with item ?f ? as it has low tmv value with item ?f ?.

Hence, the conditional tree of item ?f ? does not contain item ?b?. However, candidate patterns (1) {a, f}, (2) {c, f}, (3) {e, f}, (4) {a, c, f}, (5) {a, e, f}, (6) {c, e, f}, (7) {a, c, e, f} and (8) {f} are generated here. The conditional tree of item ?e? is shown in Fig. 2(b). Candidate patterns (9) {a, e}, (10) {c, e}, (11) {a, c, e} and (12) {e} are generated here. The conditional tree of item ?c? is shown in Fig. 2(c) and candidate patterns (13) {a, c}, (14) {b, c} and (15) {c} are generated. No conditional tree is created for item ?b? as item ?a? has low tmv value with it. Therefore, only candidate pattern (16) {b} is generated here. Finally, the last candidate pattern (17) {a} is generated for the top-most item ?a?. The second database scan is required to find the share- frequent patterns from these 17 candidate patterns. Table II shows the calculation process and the 6 share-frequent patterns are, {a, f}, {a, c, f}, {a, c, e, f}, {a, c, e}, {a, c} and {c}. Figure 3 describes the IncrShrFP-Tree construction and mining algorithm.

Using the ?build once mine many? property of the IncrShrFP-Tree, it is possible to perform interactive mining.

After creating the tree for the first mining threshold, several mining operations can be done on that tree without rebuild- ing the tree. Decreases in mining time and second database scanning time for determining share-frequent patterns from the candidate patterns are also realized. For example, after obtaining the share-frequent patterns for minShare(?) =     Input: DB, group of db+ and db?, minShare (?) Output: Share-frequent patterns  Step 1: Begin  Step 2: (i) Scan the next transacton Ti (ii) Sort the items of Ti according to the  lexicographic order (iii) insert/delete Ti into IncrShrFP-Tree (iv) Update the header table  Step 3: If DB or any db+/db? is processed then perform Steps 4 to 8 Else goto Step 2  Step 4: Input ? from the user  Step 5: For each item ? having tmv value greater than min lmv do  (i) Add item ? in the candidate pattern list (ii) Perform recursive mining operation prefixing  item ? by creating conditional trees (iii) Add all the candidate patterns prefixing  item ? in the candidate list Step 6: Calculate the Share-frequent patterns from the  candidate pattern list  Step 7: If there is any other mining request, goto Step 4  Step 8: If no more db+/db? remains, goto Step 9 Else goto Step 2  Step 9: End  Figure 3. The IncrShrFP-Tree construction and mining algorithm  30%, if we perform a mining operation for larger values of ?, then the candidate set is the subset of the previous candidate set and the actual share-frequent patterns can be defined without mining or scanning the database again. If ? is smaller than the previous trial, then the candidate set is a super-set of the previous candidate set. Therefore, after mining the database can be scanned only for patterns that did not appear in the previous candidate set.



V. EXPERIMENTAL RESULTS  To evaluate the performance of our proposed approach, we conducted several experiments on IBM synthetic dataset T10I4D100K and real-life datasets mushroom and kosarak from the frequent itemset mining dataset repository [13].

These datasets do not provide the share values of each item for each transaction. Similar to the performance evaluation of the previous share-frequent pattern mining algorithms [10], [11], we generated random numbers for the quantity of each item in each transaction, ranging from 1 to 10.

The performance of our algorithm was compared with the existing algorithms ShFSM [10] and DCG [11]. Our programs were written in Microsoft Visual C++ 6.0, and run with the Windows XP operating system on a Pentium dual core 2.13 GHz CPU with 1GB main memory.

At first, we have tested the effectiveness of IncrShrFP- Tree in interactive mining with the T10I4D100K dataset.

1 2 3 4 5 minShare (%)  N o.

o f  ca nd  id at  es  ShFSM  DCG  IncrShrFP-Tree  Figure 4. No. of candidates comparison on the T10I4D100K dataset         1 2 3 4 5  minShare (%)  R un  ti m  e( se  c.

)  ShFSM  DCG  IncrShrFP-Tree  Figure 5. Runtime comparison on the T10I4D100K dataset  The T10I4D100K dataset contains 100,000 transactions and 870 distinct items. Its mean transaction size is 10.1 and around 1.16% ((10.1/870)?100) of its distinct items are present in every transaction and therefore it is a sparse dataset. Figure 4 shows the number of candidates comparison on the T10I4D100K dataset. The minimum share threshold range of 1% to 5% is used here. Figure 4 demonstrates that the existing algorithms generate too many candidate patterns because of the level-wise candidate generation-and-test methodology. Moreover, for the lower minimum share thresholds, the numbers of candidates of the existing algorithms sharply increase. On the other hand, our approach can avoid the generation of unnecessary candidate patterns by using a pattern growth method and it always needs maximum two database scans.

Figure 5 shows the runtime comparison on the T10I4D100K dataset. The minimum share threshold range of 1% to 5% is used here. We took the result in Fig. 5 as the worst case of interactive mining, i.e., we listed the thresholds in descending order. Our approach achieves a huge gain after the first mining threshold because our tree does not have to be constructed after the first threshold.

As the threshold decreases, an additional mining operation is needed. However, as candidate set of threshold 4% is a superset of the candidate set of threshold 5%, we can easily save the second database scan time for the candidates of threshold 5%. Share-frequent patterns from these are             10 15 20 25 30  minShare (%)  N o.

o f  ca nd  id at  es ShFSM  DCG  IncrShrFP-Tree  Figure 6. No. of candidates comparison on the mushroom dataset    10 15 20 25 30  minShare (%)  R un  ti m  e( se  c.

)  ShFSM  DCG  IncrShrFP-Tree  Figure 7. Runtime comparison on the mushroom dataset  already calculated for threshold 5%. We have to scan the database a second time only for the candidates newly added for threshold 4%. The best case occurs if we arrange the mining threshold in increasing order. Then candidate set of threshold 2% is a subset of the candidate set of threshold 1%.

Therefore, without mining and a second database scan, we find the resultant share-frequent patterns from the previous result. In that case, after the first mining threshold, the computation time for other thresholds is negligible compared to that required for the first. Fig. 5 also shows that runtime differences between the existing methods and our approach become larger as ? decreases because of the generation of too many candidates and the several database scans required by the existing algorithms.

Subsequently, we have tested the effectiveness of IncrShrFP-Tree in interactive mining with the mushroom dataset. The mushroom dataset contains 8,124 transactions and 119 distinct items. Its means transactions size is 23, around 20% ((23/119)?100) of its distinct items are present in every transaction and therefore it is a dense dataset. Dense datasets have too many long frequent as well as share- frequent patterns. The probability of an item?s occurrence is very high in every transaction. As a result, for comparatively higher threshold, dense datasets have too many candidate patterns. Actually, long patterns need several database scans.

Hence, when the dataset becomes denser, the number of         0.2 0.4 0.6 0.8 1  No. of transactions (in million)  R un  ti m  e (s  ec .)  ShFSM  IncrShrFP-Tree  Figure 8. Database increasing by db+ = 0.2 million transactions on the kosarak dataset         0.9 0.8 0.7 0.6 0.5  No. of transactions (in million)  R un  tim e  (s ec  .)  ShFSM  IncrShrFP-Tree  Figure 9. Database decreasing by db? = 0.1 million transactions on the kosarak dataset  candidates and total runtime sharply increase in the Apriori based existing algorithms.

Figure 6 shows the number of candidates comparison on the mushroom dataset. The minimum share threshold range of 10% to 30% is used here. Figure 7 shows the runtime comparison on the mushroom dataset. For the worst case analysis of interactive mining, we have also given the value of thresholds in descending order. Figure 6 and Fig.

7 demonstrate that the existing algorithms also generate too many candidate patterns and require a large amount of runtime in a dense dataset because of the level-wise candidate generation-and-test methodology. Therefore, our method outperforms the existing algorithms by exploiting a pattern growth approach, using only two database scans and with the power of interactive mining.

We have tested the effectiveness of our approach in incre- mental mining on the real life kosarak dataset. It contains 990,002 transactions and 41,270 distinct items. Its mean transaction size is 8.1. At first, IncrShrFP-Tree was created for 0.2 million transactions of this dataset, and then a mining operation was performed with ? = 5%. Another 0.2 million transactions were added to the tree and a mining operation was performed again with the same minimum threshold. In the same manner, all of the transactions in the dataset were     added and the mining operation was performed for each stage using ? = 5% as shown in Fig. 8. It is obvious in Fig.

8 that as the database increases in size, the tree construction and mining time also increase.

After adding all of the db+, we performed the deletion operation in that tree. Here db? size is 0.1 million. At first, 0.1 million transactions were deleted and a mining operation was performed using ? = 5%. The same operation was repeated four times as shown in Fig. 9. It is also obvious in Fig. 9 that as the database size decreases, the tree construction and mining time also decrease. We have compared the performance of our tree structure with the existing ShFSM algorithm [10]. As the DCG algorithm maintains an extra array for each candidate [11], we could not keep its all candidates in each pass in the main memory.

Figure 8 and Fig. 9 show that our method outperforms the existing algorithms in incremental mining.

Figure 8 also shows that our method has efficiently handled the 41,270 distinct items and around 1 million transactions in the real-life kosarak dataset. Therefore, this experimental result also demonstrates the scalability of our method to handle a large number of distinct items and transactions. Our tree structure can represent useful informa- tion in a compressed form in memory because transactions have many items in common. By utilizing this type of path overlapping (prefix sharing), our tree structure can save memory space. Moreover, by using a pattern growth approach, we generate fewer candidates compared to the ex- isting algorithms. Our approach needs 0.652 MB, 15.72 MB, and 202.347 MB memory for mushroom, T10I4D100K and kosarak datasets respectively. In contrast, the existing ShFSM algorithm needs 2.056 MB, 30.49 MB, and 478.391 MB memory for mushroom, T10I4D100K and kosarak datasets respectively. Therefore, our approach significantly outperforms the existing algorithms in memory usage.



VI. CONCLUSIONS  The existing algorithms of share-frequent pattern mining have a big limitation of assuming the static nature of the databases in all aspects. We have proposed an efficient tree structure IncrShrFP-Tree that can capture the incremental data very efficiently in a compact form. It has the ?build once mine many? property and highly suitable for interactive mining. It considerably reduces the mining time by using previous tree structure and mining results. Moreover, it exploits a pattern growth mining approach to avoid the level- wise candidate generation-and-test problem. It needs maxi- mum two database scans in contrast to several database scans needed by the existing algorithms. Extensive performance analyses show that our method is very efficient and scalable in incremental and interactive share-frequent pattern mining.

