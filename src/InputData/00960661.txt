An Efficient Hash-Based Method for Discovering the Maximal Frequent Set

Abstract  The association rule mining can be divided into two steps. The first step is to find out all frequent itemsets, whose occurrences are greater than or equal to the user-specified threshold. The second step is to generate reliable association rules based on all frequent itemsets found in the first step. Identifying all frequent itemsets in a large database dominates the overall performance in the association rule mining. In this paper, we propose an efficient hash-based method, HMFS, for discovering the maximal frequent itemsets. The HMFS method combines the advantages of both the DHP (Direct Hushing arid Pruning) and the Pincer-Search algorithms.

The combination leads to two advantages. First, the HMFS method, in general, can reduce the number of database scans. Second, the HMFS can filter the infrequent candidate itemsets and can use the filtered itemsets to find the maximal frequent itemsets. These two advantages can reduce the overall computing time of finding the maximal frequent itemsets. In addition, the HMFS method also provides an efficient mechanism to construct the maximal frequent candidate itemsets to reduce the search space. We have implemented the HMFS method along with the DHP and the Pincer-Search algorithms on a Pentium I11 800 MHz PC. The experimental results show that the HMFS method has better performance than the DHP and the Pincer-Search algorithms for most of test cases. In particular, our method has significant improvement over the DHP and the Pincer-Search algorithms when the size of a database is large and the length of the longest itemset is relatively long.

1. Introduction  The data mining refers to extract knowledge from a large database [IO].  In recent years, data mining has attracted a growing amount of attention in the database community. One of the most important reasons is the fast growing huge amount of data that is far from the ability of human to analyze. Discovering association  0-7695-1372-7/01 $10.00 0 2001 IEEE  rules from a database is an important technique in the data mining area. For example, for the retail business, a database may contain sales transactions. We can analyze customers buying habits through discovering the associations among products, that is, a customer who buys some products will also buy other products in the same transaction. These mined rules are very useful for the retailer to improve the marketing strategies. Other popular applications include mining web contents, mining web path traversal patterns [ 1 I], etc.

The process of mining association rules can be decomposed into two steps [ 131. The first step is to find out all frequent itemsets, whose occurrences are greater than or equal to the user-specified threshold. The second step is to generate reliable association rules based on all frequent itemsets found in the first step. The cost of the first step is much more expensive than the second step.

Therefore, much research focused on developing efficient algorithms for finding frequent itemsets. A well-known Apriori algorithm proposed by R. Agrawal and R. Sriank [13] was the first efficient method to find the frequent itemsets. The main contribution of the Aprinri algorithm is that i t  utilizes the downward closure property, i.e., any superset of an infrequent itemset must be an infrequent itemset, to efficiently generate candidate itemsets for the next database scan. By scanning a database k times, the Apriori algorithm can find all frequent itemsets of a database, where k is the length of the longest frequent itemset in the database.

Many methods based on the Apriori algorithm have been proposed in the literature. In general, they can be classified into three categories, reduce the number of candidate itemsets, reduce the number of database scans, and the combination of bottom-up and top-down search.

Reduce the number of candidate itemsets: Methods in this category try to generate a small number of candidate itemsets efficiently in order to reduce the computational cost. The hash-based algorithm DHP (Direct Hashing and Pruning) proposed by Park et al. [6] is an example.

The main contribution of the DHP algorithm is that i t  uses a hash table to filter the huge infrequent candidate itemsets before the next database scan. However, the  51 1    DHP algorithm needs to perform database scans as many times as the length of the longest frequent itemset in a database.

0 Reduce the number of database scans: Scanning a database iteratively is time consuming. Thus, methods in this category try to reduce database scans aiming at reducing disk I/O costs. The Partition algorithm proposed by Savasere et al. [l] generates all frequent itemsets with two database scans. The Partition algorithm divides the database into several blocks such that each block in the database can be fitted into the main memory and can be processed by the Apriori algorithm.

However, the Partition algorithm examines much more candidate itemsets than the Apriori algorithm. Brin et al.

[I71 proposed the DIC algorithm that also divides the database into several blocks like the Partition algorithm.

Unlike the Apriori algorithm, once some frequent itemsets are obtained, the DIC algorithm can generate the candidate itemsets in different blocks and then add them to count for the rest blocks. However, the DIC algorithm is very sensitive to the data distribution of a database.

The combination of bottom-up and top-down search: Methods in this category are also based on the downward closure. They obtain the frequent itemsets in a bottom-up fashion like the Apriori algorithm. In the mean time, they use the infrequent itemsets found in the bottom-up direction to split the maximal frequent candidate itemsets in the top-down direction in each round. The advantage is that once the maximal frequent itemsets are obtained, all subsets of the maximal frequent itemsets are also identified. Therefore, all subsets of the maximal frequent itemsets do not need to be examined from the bottom-up direction. Without the top-down pruning, they need to scan database as many times as the length of the longest frequent itemset. However, the improvement is not clear when the length of the longest frequent itemset is relatively short. The Pincer-Search algorithm proposed by D. Lin et al. [ 2 ]  and the MaxMiner algorithm proposed by R.J. Bayardo [6] are two examples.

In these two methods, the generation of the maximal frequent candidate itemsets is not efficient. They may spend a lot of time on finding the maximal frequent itemsets.

In this paper, we propose an efficient hash-based method to generate the maximal frequent itemsets (HMFS) in the category of the combination of bottom-up and top-down search. The proposed method combines the advantages of both the DHP and the Pincer-Search algorithms. Unlike the D H P  algorithm, the HMFS method is very efficient in reducing the number of database scans when the length of the longest frequent itemset is relatively long. Unlike the Pincer-Search algorithm, the HMFS method can filter the infrequent itemsets with the hash technique from the bottom-up direction and then can use the filtered itemsets to find the  maximal frequent itemsets in the top-down direction. In addition, the HMFS method also provides an efficient mechanism to construct the maximal frequent candidate itemsets. To evaluate the performance of the HMFS method, we have implemented this method along with the DHP and the Pincer-Search algorithms on a Pentium I11 800 MHz PC. The experimental results show that our method has better performance than the DHP and the Pincer-Search algorithms for most of test cases. In particular, our method has significant improvement over the DHP and the Pincer-Search algorithms when the size of a database is large and the length of the longest itemset is relatively long.

The rest of the paper is organized as follows.

Section 2 introduces the terminology used in this paper.

The detail of the proposed HMFS method is presented in section 3. The experimental results of the proposed method are presented in section 4.

2. Preliminaries  In this section, we introduce some basic definitions of the associ;ition rule mining. Let 1 = { i l ,  iz, .. ., i,,) be a set of distinct items.

Definition 1: A database D is a set of transactions, where each transaction T contains a set of items in 1.

Definition 2 :  A subset of I is called an itemset. An itemset is called a k-itemset if i t  contains k items.

Definition 3: The support of an itemset X I for a database, denoted as support(X), is the number of transactions in the database that contain all items in X.

Definition 4: An itemset is called a frequent itemset if its support is greater than or equal to some user-specified minimum support. Otherwise, it is an infrequent itemset.

The set of all frequent k-itemsets is denoted as Lk.

Definition 5: Given the set of all candidate k-itemsets, Ck, is defined as Lk., x LX., = (Xu Y I X, Y E LX.,, I X n  Y I =k -2} ,wherek>  1.

Definition f!: A frequent itemset is called a maximal frequent iternset if i t  is not a subset of any other frequent itemsets.

Definition 7: An association rule is defined as X 3 Y , where X ,  Y  The task of the association rule mining is to discover all association rules that satisfy the minimum support and the minimum confidence. We now give an example to explain the terms described above. Consider the database shown in Table 1. Assume that I = ( A ,  B, C, D, E ,  F )  and there are five transactions in the database D.

Let the minimum support and the minimum confidence be 40% and 100%, respectively. All frequent itemsets are shown in Table 2. The itemsets whose support smaller than 5 ~ 4 0 %  = 2 are infrequent itemsets. In this example, itemsets D ,  AB,  and AE are infrequent itemsets. Note  I ,  X ,  Y f  0 and X n Y = 0.

I 2 1 B. C .  E.  F I  A .  C .  D  I 3 1 A .  B. C . E .  F 1  Suppor t A  I 4  Itemsets AF, BC,  BF, CE, EF, ACF, BCE, BCF, BEF, CEF, BCEF  A ,  B ,  E, F, A C ,  BE, CF c  I 5 I A .  C .  F I  Table 2: All frequent itemsets.

3. The HMFS Method  Our HMFS method combines the advantages of both the +DHP and Pincer-Search algorithms. In the HMFS method, it uses the hash technique of the DHP algorithm to filter infrequent itemsets in the bottom-up direction.

Then lit uses a top-down technique that is similar to the Pincer-Search algorithm to find the maximal frequent itemsets. The main difference of the top-down techniques between the HMFS method and the Pincer-Search algorithm is that the HMFS method provides a more efficient mechanism to initialize the set of maximal frequent candidate itemsets than that of the Pincer-Search algorithm. By combining the advantages of the DHP and Pincer-Search algorithms, the number of database scans and the search space of items can be reduced. The algorithm of the HMFS method is given as follows.

Algorithm HMFS() 1.

2.

3. call cunstruc~~maximulfrrque~itcandidate_irem.~ets(C~, H2); 4.

5 .

6.

7.

8 .

9.

Enhuf-algorithm  In the first round, scan the database D to count the support of all  Cz is constructed by LlxLl and is filtered by Hz;  In the second round, divide D into several blocks; for all blocks h E D do  Count the supports of itemsets in C2 and MFCS; call process-cullision(C2, H 2 )  to process the collisions of the hash  Move the maximal frequent itemsets from MFCS to the hash tree;  l-itemsets and build a hash table H2;  buckets;  Apply the Pincer-Search algorithm to the rest of rounds;  Function cons truc t~maximalfrequen~~eandi~lu~~~iren~se ts (C~,  H2) I .  C,n,u = (x =XIXZX~.  . ..x, 1x1~2, xlx3,  . .., xlx. E C2, where n > 2 ) ; 2. m = 3 ; M F C S = 0 ; 3. for all x = xlxzx3.. .x, E C,",, do 4.

5 .

6.

7.

8.

9.

IO.

Push x into the stack initially; while the stack is not empty do  Popup an element x from the stack; while m f n do k = h2(xj,,,), for i = 2, 3 , , ,  ., m - I ; if ( H 2 ( k )  < minimum support) then  // h2 is a hash function  Split ~ 1 x 2 ~ 3 . .  ..x, into two (n-1)-itemsets, x'= xIx2x ?... x!.. . X , ~ . I ~ ~ ~ + I . .  .x, and xu=  x1x2xXJ.. .x,.lx8+l.. .X,n*;"+l.. .x,;  if i.~_maximal_carrdi~lulare_iremser(x ") = true then I I .

12. else discard x': 13.

14. then continue processing x'; 15. m++; 16. else x'is discarded; 17. break; 18.

19. return MFCS; E n d _ o f T o n s t r u c t _ m a x ~ m a l ~ r ~ q u e n t ~ ~ u ~ ~ ~ i ~ l a t e ~ i t e i n , ~ r t . ~  Function is-maximal-candi~lute_itrmsrr(itemset x) I .

2. if all items in x are also in s then return false; 3. else return true; End-of_is_maximul-candi~late-it~m.set  Function procrss_coIlision(C2, H2) I .

2.

3.

4.

push  into the stack;  if is-mw;imal-cuncii~lute-items~t(x 'j = true  if (m = n and the length of x > 2) then MFCS = MFCS U (x) ;  for all itemset s in the stack do  for all blocks h E D do for all H2(k) 2 minimum support do  for all c, E C2 that hashed into H2(k) ,  where i = I .  2 , .  . ., n do  if ( H , ( k )  - 2 suppon ( c , )  + support ( c , ) ,=I  < minimum support, V j  = 1,2 ,___, n ) 5 .

6 .

End-of- proces.y-colli.sion  then use the infrequent 2-itemset cj to split the itemsets in MFCS;  Remove the infrequent 2-items-t cj from CZ;  In the algorithm HMFS(), lines 1-2 use the hash technique to filter the infrequent itemsets in C2 in the bottom-up direction. Line 3 constructs the set of maximal frequent candidate itemsets MFCS. Line 6 counts the supports of itemsets in MFCS and C,. Line 7 splits the maximal frequent candidate itemsets if some conditions are satisfied. Line 8 moves the maximal frequent itemsets from MFCS to the hash tree. Line 9 performs the Pincer-Search algorithm to get the maximal frequent itemsets.

We first explain how the function construct-maxirnalJrequent-candidate-itemsets() works.

Line 1 constructs C,,,, with all 2-itemsets that have the same first item in C,. Lines 3-19 generate the set of maximal frequent candidate itemsets, MFCS. The generation process is as follows. Assume that an itemset x in C,, is denoted as x1x2x 3...x,. Consider the first m     items in ~ 1 ~ 2 x 3  .... x,, for m = 3, ..., n, and examine the 2-item subset x,xm of x, for i = 2, 3 ,..., m-I. If the number of 2-itemsets in the corresponding hash bucket of x,x,, is smaller than nunimum support, i.e., x,x, is not in C2,  split x into x? = XIXZX~ ..., Y ,... x , ~ . ~ x ~ + ~ . . . x ,  and x?  = ~ 1 x 2 ~ 3  ... x,.Ix,+I. .. x,,,x,,.I ... x,. Itemsets x?and x?are  then compared with elements in the stack. We have the following four cases.

Case 1 .  All items in x?and x??are also in any element in the stack. Both x?and  are discarded. The next itemset is popped up from the stack and the generation process continues.

Case 2. Only items in x?are also in any element in the stack. Itemset x? is discarded. The generation process continues to examine .x,+Ix, of x?: Case 3. Only items in x??are also in any element in the stack. Itemset x ?  is discarded. The generation process continues to examine the x,x,+I of x!

Case 4. Otherwise, itemset x?is pushed into the stack and the generation process continues to examine x,xm+~ of X!

The generation process continues until rn = n.

Then we get a maximal frequent candidate itemset.

Once one maximal frequent candidate itemset is generated, the next itemset in the stack is popped up and the generation process is applied until the stack is empty.

C. = (AB, AC. AD. AE. AF.

BC. BF. CD. CE. CF)  A B C D E F  c i  ? 4  [ A B C I D E F C, ( A  B C D  E F J  [ 4 B C I D E F .  BCisin C,  ( A B C D I E F i  1 .4 B C D 1 E F. BD IS no1 an c2  &- I A ?I? I E F .  CDisin c, I A B  C E I F. BEis no1 !n C, . . ..

I A C D E I F  ( A C D E 1 F. DE is not m C, discard ACEF c  Figure 1: An example of the split process.

An example of the generation process is shown in Figure 1 .  Let C2 = {AB, AC, AD, AE, AE BC, BE CD, CE, C F ] .  C,,,,, is {ABCDEF}.  Consider the first 3 items ABC in ABCDEF. Since BC is in Cz, we examine ABCD in ABCDEF. Since BD is not in Cr,  ABCDEF is split into ABCEF and ACDEF. Compare ABCEF and ACDEF with elements in the stack, we have Case 4.

ACDEF is pushed into the stack and the generation  process continues with ABCEF. Since BE is not in C2, ABCEF is split into ABCF and ACEF. Compare ABCF and ACEF with elements in the stack, we have Case 2.

ACEF is discarded. A maximal frequent candidate itemset ABCF is obtained. Since the stack is not empty, itemset ACDEF is popped up from the stack and the generation process continues in a similar manner.

Finally, all maximal frequent candidate itemsets, ABCF, ACD,  and ACE are generated from ABCDEF.

In HMFS method, the collision of the hash buckets cannot be avoided by using the hash technique. The collision may result in an infrequent itemset be used to construct the maximal frequent candidate itemsets. For example, assume that C 2 =  {AB, AC, AD, AE, AE BC, BE CD,  CE, C F )  is given. One of the maximal frequent candidate itemsets of C2 is ABCF. Assume that AC,  a frequent itemset, and AF, an infrequent itemset, are hashed into bucket 2. Since AC and AF are in the same bucket, AF cannot be filtered and will be used to construct the maximal frequent candidate itemsets. Function process-colli?sion() provides a solution of this problem.

In the following, we explain how it works. First, it divides the database into several blocks. In the second round, the supports of elements in C 2  and MFCS are counted. The number of 2-itemsets hashed into bucket k in H2 is denoted as H2(k) .  Assume that there are n 2-itemsets, c I ,  c2 ,..., c ,  in Cz, hashed into bucket k.  An infrequent itemset c, can be identified by the following equation:  H 2 ( k ) - ~ s u p p o r t ( ~ , ) + s u p p o r t ( c , )  < (1) , = I  minimum support, b?j = 1,2 ,..., n where the supports of c, and c, among the k blocks are denoted as mpport(c,) and support(c,), respectively. In each block scanning, all infrequent itemsets in C2 are identified and are deleted from C2. The identified infrequent itemsets are used to split itemsets in MFCS as well.

We now give an example to explain Equation (1).

Assume that the number of transactions in a database is 10,000, the minimum support is O S % ,  and H2(k) is 100.

After scanning several blocks in the database, supporf(AC) is 70 and support(AF) is 10. By applying Equation (l), 100-(70+10)+10 = 30 < 10,000X0.5% = 50. Thus, we can identify AF is an infrequent itemset and AF can be discarded. The purpose of dividing a database into several blocks is that some infrequent itemsets in Cz may be determined earlier when some blocks are scanned.

The maximal frequent candidate itemsets that contain these infrequent itemsets cannot be counted further.

Therefore, the division may lead us to identify those maximal frequent candidate itemsets that contain infrequent itemsets earlier and reduce the time of finding the maximal frequent itemsets.

4. Experimental results  D T I L  Number of transactions Average size of transactions  Average size of the maximal potentially large itemsets Number of potentially large itemsets  [ N I  Number of items  I W  0 2 5  050 071 u ," ,Mmrrppm 19,  To evaluate the performance of the proposed method, we have implemented the HMFS method in C language along with the DHP and the Pincer-Search algorithms on a Pentium 111 800 MHz PC with 512MB of main memory. The program designed by IBM Almaden Research Center is used to generate synthetic databases [ 5 ] .  This program has been widely used by many researchers [ l ,  2, 6, 7, 8, 9, 12, 14, 171. By setting up parameters of the program, we can generate desired databases as benchmarks to evaluate the performance of our method. Table 3 describes all the parameters used in the program. In our experiments, we set N = 1000 and L = 2000. The number of the hash buckets is 500,000.

We designed two tests. In the first test, we compare the relative performance and the number of database scans for the three algorithms on four databases. The results of the first test are shown in Figure 2 and Figure 3.

Figure 2 shows the execution time of these three algorithms for test databases with various minimum supports. In Figure 2, our method is a little slower than the DHP algorithm on TlOI4DlOOK when the minimum support is I % .  In this case, the execution time of the DHP algorithm and the HMFS method are 4 and 6 seconds, respectively. The reason is that the length of the longest itemset is two for T1014DlOOK when the minimum support is 1%, i.e., only two database scans are required for T1014D100K. The HMFS method and the DHP algorithm all require two database scans. However, the HMFS method needs to spend some time on constructing the maximal frequent candidate itemsets based on C2. Therefore, it takes more time than the DHP algorithm. For other test cases, the HMFS method outperforms the DHP and the Pincer-Search algorithms.

The summary reasons are given as follows.

1. In contrast with the DHP algorithm, the HMFS method finds the frequent itemsets not only in the bottom-up direction but also in the top-down direction.

The execution time is improved since the number of database scans is reduced. The number of database scans is shown in Figure 3 .  The number of database scans required by the DHP algorithm is the length of the longest frequent itemset. In general, the number of database scans of the HMFS method is half of that of the DHP algorithm when the minimum support = 0.25% and  j o x  O J O  07s  cm WlnirmmSvppm ,%I  0.5%.

2. In contrast with the Pincer-Search algorithm, the HMFS method still has better performance than the Pincer-Search algorithm even though the number of database scans required by the HMFS method is the same as the Pincer-Search algorithm. There are two reasons.

First, the HMFS method uses the hash table to filter a huge number of infrequent 2-itemsets in the C, instead of actually counting the supports of all 2-itemsets. Second, it constructs the maximal frequent candidate itemsets by using the hash technique instead of the combination of all distinct 1-itemsets in a database. The search space is reduced substantially.

(a) T1014D1 OOK (b) T1514D1 OOK . .

I W  1 021 050 075 I W w,"urum*poni%,  1 0 2 5 0 5 0 0 ' 5 u,"lmm*pprn IS,  (c) T2014D1 OOK (d) T2016DlOOK Figure 2: The execution time of the HMFS method, DHP, and Pincer-Search algorithms on various test databases with increasing minimum supports.

In the second test, we evaluate the performance. of the HMFS method and the DHP algorithm on the test databases with various database sizes. The results of the second test are shown in Figure 4. The performance of the Pincer-Search algorithm is not included since it takes     too much time to get the execution results for the test databases. In Figure 4, the number of transactions in the test databases is set from lO0K to 500K and the minimum support is 0.75%. From Figure 4, we can see that both the execution time of HMFS and DHP increases when the number of transactions increases. However, the execution time of the DHP algorithm is near linear to the size of test databases. The HMFS method is not so sensitive to the size of a database compared to the DHP algorithm. Therefore, our HMFS method performs much better when the database size is larger.

IWK XQK 3WK WOK S W K ,  IWX i o o Y  WOK IWX raor , I D . h C S E .  D r l l v  lur  (a) TlO14 (b) T1.514 +iuirs 4 - 0 ~ ~  I  l a  - ?. .k_ we ~- - Ll1*Da.sr.

(c) T2014 (d) T2016  Figure 4: The execution time of the HMFS method and the DHP algorithm on the test databases with various database sizes. (Minimum Support = 0.75%)  5. Conclusions  In this paper, we have proposed an efficient hash-based method, HMFS, for discovering the maximal frequent itemsets. The method combines the advantages of the DHP and the Pincer-Search algorithms. The combination leads to two advantages. First, the HMFS method, in general, can reduce the number of database scans. Second, the HMFS method can filter infrequent itemsets and use the filtered itemsets to find the maximal frequent itemsets faster. In addition, an efficient mechanism to construct the maximal frequent candidate itemsets is provided. To evaluate the performance of our method, we have implemented the proposed method along with the DHP and the Pincer-Search algorithms on a Pentium I11 800 MHz PC. The experiments were conducted on various benchmark databases. The experimental results show that our method has better performance than the DHP and the Pincer-Search algorithms for most of test cases. In particular, our method has significant improvement over the DHP and the Pincer-Search algorithms when the size of a database is large and the length of the longest itemset is relatively long.

