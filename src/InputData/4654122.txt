Mining Cross-Modal Association Rules for Web Image Retrieval

Abstract   To alleviate the known semantic gap, it is necessary to integrate the two-modal parts of web images, i.e. the low-level visual features and high-level semantic concepts (which are usually represented by keywords), for web image retrieval. In this paper, we associate the keyword and visual features of web images from a different prospective and a new approach based on the cross-modal association rules is proposed to automatically integrate the keyword and visual features for web image retrieval. A customized mining process is developed for the special association rule that crosses the two modals of web images. The cross- modal association rule effectively associates the keyword and visual feature clusters, and seamlessly integrates the two modals of web images in retrieval process. The proposed approach is utilized successfully in a web image retrieval system named VAST (VisuAl & SemanTic image search) [1].

1. Introduction   To effectively retrieve images in WWW, existing commercial engines, including Google image search, Lycos, and AltaVista photo finder, etc., use text (i.e.

surrounding words) to look for images, without considering image content [2]. In contrast, traditional content-base image retrieval (CBIR) systems, represented by QBIC [3], are proposed to utilize only low-level  features such as color, texture, shape to retrieve similar images. In fact, the images on the Web are generally exposed as part of web contents. It has two-modal characteristics, i.e. textual and visual features, which bring information that are complementary and that can disambiguate each other  [4]. Therefore, a web image should be specified not only by the image itself, but also with respect to the web contents surrounding the image [2].

Although efforts have been devoted to combine the two modalities, the semantic gap between them is still a huge barrier in front of researchers. The representative prior works to bridge the semantic gap in web image retrieval mainly focus on the Relevance Feedback (RF) mechanism [5-7] and online clustering method [8-11]. However, the RF is rarely employed by web users due to the additional efforts of human interaction, and the online clustering method would dramatically increases the response time of the query.

Moreover, the online clustering method can not improve the retrieval effectiveness when only a few correct images are contained within top-N results.

In this paper, we associate the keyword and visual features of web images from data mining prospective.

A new approach based on the cross-modal association rules is proposed here to automatically integrate keyword and visual features for web image retrieval.

The cross-modal association rule crosses two modals of web images and narrows the semantic gap in web image retrieval. It is successfully integrated into a web image search prototype system (i.e. VAST system [1]).

In the rest of this paper: Section 2 discusses the related works about the integration of the two modalities of web images. In Section 3, our proposed approach that mines the cross-modal association rules is described. In Section 4, we depict how the cross- modal association rules are exploited in our system.

Finally, we present our conclusion.

2. Related works   To integrate the textual and visual information of web images and thus bridge the semantic gap, some  International Symposium on Computer Science and its Applications  DOI 10.1109/CSA.2008.70     research prototypes for web image retrieval that have emerged in recent years, such as WebMars [5], ImageRover [7], Cortina [12], and Atlas WISE [6], etc., utilize the text and visual features together by the RF mechanism. However, according to the recent statistical analysis [13], web users rarely make use of feedback to revise the search. Relevance feedback, while just be effective in the real-life scenarios (e.g.

web search), is rarely employed by web users due to the additional efforts of human interaction.

Some works use the online clustering method [8, 10, 11] to integrate the keyword and visual feature, which cluster the first keyword-based retrieval results by the visual features of image and thus automatic re-rank the results set. They increase the response time remarkably, which affects the usability of the web system.

Moreover, the online clustering method can not improve the retrieval effectiveness when only a few correct images are contained within top-N results.

From above works, little attention has been drawn to associating and fusing the two basic modalities of Web images by association rule mining technology despite it is extensively used in the CBIRs and the text- based search engines. Our approach realizes the integrated query from data mining prospective and overcomes the drawbacks of RF method and online clustering method.

3. Mining cross-modal association rule   To discover the cross-modal association rule, we firstly quantize the visual features by clustering, then illustrate the cross-modal association in web images, and finally give the mining process.

3.1. Image clustering based on region features   To discover the cross-modal association, we aim to associate the keyword and visual feature clusters.

For the visual features, we obtain the average L*U*V color and three-level wavelet texture for each region of image. We use a fast image segmentation algorithm based on watershed algorithm and c-means algorithm. The number of the regions in an image is set to be 6. The EMD (Earth Mover Distance) is used to compute the similarity of two images. The distance metric of two regions in EMD is Euclidean distance.

The EMD distance distribution of our region feature can be match with a known normal distribution (?=209, ?=59) as Figure 1, which is based on the statistics of about 250,000 real web images randomly selected in the image collection of VAST system [1].

K-Means is chosen to cluster each visual feature.

Based on the EMD distribution of the images, 100 such clusters are created initially (the max radius of the cluster is set to be ?/2), which offer a good trade- off between speed and accuracy. Because the distribution of the images over the clusters is very uneven, we partition the big cluster that is larger than a threshold T (currently is 20,000) into smaller clusters of about 10,000 images per cluster.

0 100 200 300 400 500         x 10  -3  EMD distance  pd f      EMD pdf Norm pdf   Figure 1. EMD distance distribution of region feature  3.2. Cross-modal association   Figure 2 shows an illustration of cross-modal association between keyword and visual feature clusters. The existing inverted file, which relates keywords to their associated images, builds the links  Web Image 1 Web Image N...

Keyword 1 Keyword 2 ... Keyword M  Feature 1 Feature 2  Cluster(F1, 2) ... Cluster(F1, L  Feature 1 Feature 2  Cluster(F1, 1)  Cluster(F2, 1 ... Cluster(F2, LCluster(F2, 2   Figure 2. Cross-Modal Association  between the keywords and images. Meanwhile, each image belongs to P visual feature clusters and then the links between the images and visual feature clusters are built. Based on the two links, we can easily abstract the association between the keywords and visual feature clusters. Therefore, we can find interesting association rules among these associations between the keywords     and visual feature clusters by data mining technology.

For example, in Figure 2, keyword 2 is possibly associated with the clusters with red dash-line (i.e.

Cluster(F1,2) and Cluster(F2, L)).

3.3. Mining process   Based on the illustration in Figure 2, we give the mining process for cross-modal association rule as follows. Firstly, based on the keyword inverted index in our VAST system [1], a record of the keyword inverted index table I is defined as follows:  (Qj, {imagej,i,k | i=1,?, Nj; k=1,?,Mj,i}) (1) where Qj is one keyword and imagej,i,k is one image that contains the keyword Qj and belongs to one web page Wj,i. Therefore,  Wj,i ={ imagej,i,k | k=1,?,Mj,i } (2) Thus, formula (1) can be transformed as follows:  (Qj , {Wj,i | i=1,?, Nj }) (3) where Wj,i is the images containing keyword Qj and belonging to one web page. Considering the images with close size in the same web page always have semantic homogeneity and generally have some common keywords, we assume they belong to the same category.

For each imagej,i,k, we build an image-cluster table, i.e. one image must belong to one cluster, and a record of the table is defined as follows:  (imagej,i,k , Cp )     (4) where Cp is the cluster that the imagej,i,k belongs to.

According to (3) and (4), the itemset of transaction T in database D  is:  (Qj , {Ci | i=1,?, m })   (5) where {Ci | i=1,?, m } are the clusters set.  Therefore, many transactions can be deduced from one record of I.

Only the itemsets containing at least one keyword and one visual cluster are considered. To satisfy the support-confidence framework, we define the support and confidence of the association rule as follows:  supp(Qj)= |D(Qj)|/|D| (6) supp(Ci)= |D(Ci)|/|D| (7)  where D(Qj) is the set of images labeled with keyword Qj in database D, D(Ci) is the set of images in cluster Qi, and |D| is the number of images in database D.

Similarly,  supp(Qj=>{Ci | i=1,?, m }) =p(Qj , {Ci | i=1,?, m })  (8)  conf(Qj=>{Ci | i=1,?, m }) = p(Qj , {Ci | i=1,?, m }) /p(Qj)  (9)  The extended association rule generation algorithm is given by Table 1 based on A-priori algorithm and the above formulas. It is used to identify the frequent itemsets and generate strong association rules. Each  superset of frequent itemsets with cardinality K belongs to a set of candidate itemsets of size K, i.e. SK.

Table 1. Cross-modal association rule generation algorithm  Input:   Transaction Database D; Minimum Support Threshold min_supp;  Output:  Frequent Itemset L 1. L1 ={Qj | supp(Qj) > min_supp};  2. L1 = L1?  {Ci | supp(Ci) > min_supp}; 3. L2= { ( Qj , Ci )  | Qj ?L1 , Ci ?  L1 ,  supp( Qj  , Ci ) >  min_supp};  4. for (K=3; LK-1 ?? ; K++) do 5.     SK ={sk | sk  is K-itemset and a combination  of frequent sets from LK-1 };  6.     for )( DT ?? and )( Kk Ss ??  do 7.           if )( Tsk ?  do 8.                supp(ck) = supp(ck) + 1/|D| 9.            end if 10.       end for 11.     LK ={ ck | supp(ck) > min_supp}; 12. end for  13. ;  K K  LL ? ?  =  14. return L;  We can get the strong association rules like (Qj, {Ci | i = 1, ? ? ? ,m}) by the algorithm in Table 1 with defining two thresholds, i.e. min_supp and min_conf.

Table 2 shows the characteristic of association rules distribution. It is interesting that the 1% series seems to have more items with higher confidence than 20% and 40%, while the 10% series seems to have confidence around 1%. Of course, the percentage of association rules decrease with the support and confidence increasing.

Table 2. The number of cross-modal association rules with the different min_supp and min_conf min_supp 0.01% 0.1% 1% 10% #conf>1% 101 1,306 123,861 8,798 #conf>5% 0 120 22,409 1,425 #conf>20% 0 0 17,820 68 #conf>40% 0 0 12,414 7  Table 3 shows some examples of the cross-modal association rule. The C11 represents the cluster with ID 11, and the rest can be deduced by analogy.

Table 3. Examples of cross-modal association rules cross-modal association rules support confidence (Great Wall, C11, C34, C65) 1.2% 5.8% (car, C22, C56) 1.41% 5.74% (Great Wall, C65) 3.8% 36.8%      4. Retrieval by cross-modal association rule   We re-rank the initial keyword-based search results with the cross-modal association rules in VAST system [1] automatically. As to these rules containing keyword Qj have the same |D(Qj)| and |D| in formula (6) and (7), the clusters in these rules can be sorted by their confidence in descending order. The query process can be defined as fellows: 1) A user enters a query keyword Qj; 2) Run the plain textual ranked boolean query based  on the keyword-based inverted index and get the initial retrieved images result D(Qj).

3) Look up the association rules table, if there exist the association rules containing the keyword Qj, like the formula (5), then go on; else go to step 9).

4) According to the confidence of these rules, we order the clusters in these rules containing keyword Qj by their confidence.

5) For the top-N initial retrieved images in D(Qj), the images in the cluster with the largest confidence ranks first, then subsequently the images in the cluster with the second largest confidence, and the same is valid for the rest.

6) The images within a cluster are ranked by EMD distance to the cluster centroid in ascending order.

7) The images not in these clusters are put in the last by their original order.

8) Get the re-ranked results based on 5), 6) and 7).

9) Return the result to user.

It can be seen from above that, if exists the association rule for Qj, the images that both are contained in the clusters in corresponding rules and include the keyword Qj have higher rank. Otherwise, the query process reduces to pure keyword search.

5. Conclusions   We use the data mining technique to discover the cross-modal association rules between keyword and region-based image visual feature clusters in VAST web image retrieval system [1]. Our method seamlessly integrates the keyword and visual cluster and overcomes the drawback of RF method and online clustering method.

