MINING FUZZY ASSOCIATION RULES USING PARTIAL SUPPORT

Abstract: The paper presents a new approach of mining fuzzy  association rules. Most existing methods need to perform multiple scans of the database to get frequent itemsets and work poorly if the data are densely populated and duplicated.

Our approach only needs one scan to build the fuzzy P-tree, which is a variant of a set enumeration tree. The tree is stored with partial fuzzy support values of candidate itemsets, which facilitate the calculation of total fuzzy support values. We describe the implementation of our algorithm derived from Apriori algorithm. The fuzzy P-tree can be combined with many existing methods and significantly improve their efficiency.

Keywords: Fuzzy Association Rules; Fuzzy P-tree; Linguistic Terms  1 Introduction  Mining association rules, which may lead to previously unknown and interesting information, is an important field of data mining. An association rule is defined as the form A-B, where A and B are both sets of attributes. We always use two measures to evaluate the interestingness of an association rule. The support value for the rule is the number of database records which consist of A U B. The confidence value for the rule is the ratio of the support value for A U B  and the support value for A. If an itemset satisfies support threshold, it is called as frequent.

If an association rule satisfies both support threshold and confidence threshold, the association rule is considered interesting.

The problem of mining association rules was initially derived from supermarket basket data. So early research is mainly focused on Boolean association rules. At that time people often concern whether an item is available in a transaction or not and neglect related quantitative information. However, quantity is also a very important aspect of information. So people start to investigate quantitative association rules. As quantitative attributes always contain a lot of distinct values and the support value for one particular value is quite low, people have to  discretize quantitative attributes. But it may lead to sharp boundary between intervals and thus affect the correctness of final results. So the apply of hzzy set theory has become a common idea due to its simplicity and similarity to human reasoning. Hong puts forward an algorithm that integrate fuzzy concept with Apriori algorithm to find fuzzy association rules. Chan and Au [41[51 make use of adjusted difference and fuzzy set to discover association rules without user-defined support threshold and confidence threshold.

But existing methods of mining fuzzy association rule are similar to classical Apriori algorithm[']. The common characteristic of these methods is that they require multiple scans of the database. G Goulboume and his co-workers[21 propose a new algorithm using a special tree data structure, P-tree. His algorithm only need one database scan to built the tree and interim results are calculated and stored in the tree. As these interim results, partial support values, can be used to calculate the support count of candidate itemsets, extra database scan is unnecessary. This algorithm outperforms Apriori algorithm and even FP-tree Algorithm 16]. But his method is defined on binary data and cannot be used to discover k z y  association rules. We extend his method to mining fuzzy association rules and propose our implementation based on his method.

2 Related Work  Enlightened by the idea of set-enumeration tree, G Goulboume and his co-workers [21 develop a special tree structure, P-tree, which can be constructed by a single database scan. Every node of the tree consists of, two aspects of information: one itemset and the corresponding partial support value used for subsequent processing. For any candidate itemset, the calculation of the total support can be worked out just by traversing the tree and thus the extra scan of the database is not required any longer. The P-tree is a variant of a set-enumeration tree. The itemsets are stored according to their lexicographic order with a related incidence count. In order to avoid the potentially exponential scale of the tree, nodes can be added  0-7803-7865-2/03/$17.00 02003 IEEE     dynamically with the scan of database to include necessary nodes.

Figure 1 shows the tree of set I, I={a, b, c}. Suppose that the database of the tree comprises exactly one instance of 7 possible itemsets of three attributes. In this tree, each subtree contains all the superset of the root node, which following the root node in lexicographic order.

Figure 1. The P-tree for set (a, b, c)  PA is defined as the number of records whose content is identical to the itemset A. TA is defined as, the total support value for the itemset A. QA is define as the support value for subsets which contain the itemset A and follow the itemset A in lexicographic order. Its value is equal to the incidence count of corresponding node in the P-tree. The following equation can be concluded:  TA=QA+ C PB ( v B 113 A, B precedes A in lexicographic order )  Generally, the computation required is significantly reduced because partial support values of candidate itemsets have already been worked out in the procedure of constructing the tree.

3 Preliminaries  We now summarize fuzzy association rule methodology as follows. Note that we borrow some definition in [4].

Given a set of records, D, each of which contains a set of attributes, 1=(1,,12, I,,}, where I, (1 <i<n) can be a quantitative attribute or a categorical attribute. For every record d of D, dom(i) denotes the domain of the attribute I,.

A set of linguistic terms are defined based on the dom(i). L,, (j=1,2, ..., m,), is the linguistic terms of the attribute I,. m, denotes the total count of the linguistic terms of the attribute I,. L, is represented by a fuzzy set, whose membership function is U, (U,J: dom(i)-[0,1]). If the attribute I, is quantitative, the linguistic terms of it is defined as follows:  i Lij= l J J ( V )  L o m ( i )  , v E dom(i), if dom(i) is discrete Ug(v)  L o m ( i ) , v E dom(i) , if dom(i) is continuous  If the attribute I, is categorical, dom(i)={vijll SjSm,}, m, denotes the attribute has rq different values. The definition of the linguistic terms of it is defined as follows:  Vij Whether the attribute is quantitative or categorical, we  can use one uniform representation, a set of linguistic terms, to describe the data using the above definitions. Any record can be transformed to a set of pairs <linguistic term, value>, each of which consists of one linguistic term and its corresponding membership value.

An illustrative example. In this example, we show how to transform a relational database to its fuzzy representation The database is shown in Table 1  Table 1 .  A Sample Database Sex I Age I Salary I  50 5000 Let us first consider the categorical attributes: Sex,  which is defined as follows: 1 1  Sex: Male = - Female = - A4 F  For the remaining attributes, age and salary, their membership functions are defined in Figure 2.

Young Middle-aged Old  20 40 60  The Age Attribute     I Low Medium High  Sex  02000 4000 6000  Age Salary  The Salary Attribute  Figure 2. The definitions of membership functions  Table 2. The fuzzy version of sample database  The result after transformation is shown in Table 2. In order to simplify the representation, let al=Male, a2=Female, b 1 =Young, b2=Middle-aged, b3=01d,  cl=Low, c2=Middle, c3=High.

For one fuzzy association rule A-B, where A and B  are both as a set of linguistic terms. A= alAa2A--Aa,,,, B=b, A b2 A..  . Ab, . The support value and confidence value for the rule are defined in the following formulas.

UL(d) denote the membel?ship values for linguistic item L of the record d.

An illustrative example (continues). Using the data in Table 2, we want to calculate the support value of the association rule: al-c2. It means young men may have low salary.

1 x 1 + I x  0.5 F S(al+ c2) = = 0.375  4 Fuzzy P-tree  The fuzzy P-tree is an extension of the P-tree, which is also a variant of set-enumeration tree. Like the P-tree, each node of the tree contains two aspects of information, a set of linguistic terms and its corresponding fuzzy support    value. The algorithm of building the fuzzy P-tree is basically similar to the building of the P-tree [21. However, the fuzzy P-tree is based on more complex attributes (both quantitative attributes and categorical attributes) instead of just boolean attributes, it has its own characteristics.

First, unlike the P-tree, support values of the fuzzy P-tree are real numbers instead of integers. They are worked out according to the formula of support value defined in Section 3.

Second, we can?t insert the fuzzy data, a set of linguistic items, directly into the fuzzy P-tree when we build it. It may contain different linguistic items coming from one attribute due to the application of fuzzy theory. As the association between them is meaningless, we have to split the set of linguistic items into appropriate subsets. For example, the set of linguistic items of the first record in the sample database is a2blb2clc2, which bl and b2 come from the attribute ?age?, and cl  and c2 come from the attribute ?salary?. We need to split it into four itemsets, a2blc1, a2blc2, a2b2cl and a2b2c2. So we need to process four itemsets for the first record when building the tree.

Third, the nodes are sorted according to the ascending order of the number of linguistic items of different attributes. They aren?t stored according to their lexicographic order like P-tree. If there are two different attributes that have the same number of linguistic items, the linguistic items are sorted according to their corresponding attribute names. In this way we can decrease the number of non-leaf nodes. For example, according to this rule, the order of linguistic items of the sample database in Table 2 is ala2blb2b3clc2c3. The tree contains 26 (2t-2 X 3+2 X 3 X 3) nodes on condition that the sample database contains all possible itemsets. If we use another different order, blb2b3ala2clc2c3, the total count is 27.

The number is 30 ifwe use the order, blb2b3clc2~3ala2.

In the course of building the tree, parent nodes are subsets of their descendents and sibling nodes are sorted according the rule defined before. The algorithm scans the database row by row from the first record to the last one.

On each iteration the row is turned into its fuzzy representation, the fuzzy data iS split into proper subsets and the subsets with its fuzzy support values are inserted into the tree. If one itemset is already represented by one node in the tree, its support value is added to the support value of the node. Otherwise a new node is created for this itemset and inserted into the appropriate position in the tree.

Dummy nodes are inserted to preserve the tree structure and prevent the tree from forming a linked list. As the building the fuzzy P-tree is similar to the building the P-tree except the differences we discussed above, we don?t want to repeat the algorithm here. It is discussed in detail in PI .

5 ,, Computing Total Support  Apriori-TFP algorithmL3] follows the Apriori methodology of performing repeated scans, in each of which the support values of candidate itemsets of the same size are calculated. The result is stored in a second-enumeration tree, the T-tree, which is arranged in the opposite order to that of the P-tree. For candidate itemsets, the total support values can be carried out just by walking the P-tree once. In order to avoid creating too many nodes of T-tree, it is built level by level and pruned itemsets which are not frequent at the end of every scan.

We can take advantage of this algorithm to calculate fuzzy support values based on fuzzy P-tree.

We notice one problem of Apriori-TFP algorithm that the P-tree is static in the course of this algorithm. Whatever the support threshold is large, we still have to traverse the whole tree. In other words, the P-tree may contain many unnecessary nodes in some cases. If we can prune these nodes and add their support values to other nodes, we not only decrease the number of nodes but also reduce the height of the tree. The number of pruned node is decreased as the increase of the size of candidate itemsets. We may significantly improve the efficiency only through pruning singleton itemsets that are not frequent. For example, if we prune all nodes contain element ?a? in the Figure 1, the tree only leave 3 nodes.

We modify the fuzzy version of Apriori-TFP algorithm to prune nodes after the first scan of the Fuzzy P-Tree (in fact the improvement also apply to P-tree). In order to distinguish our algorithm with Apriori-TFP algorithm, we use the term Apriori-TFP-PN (Apriori-TFP with Pruning Nodes) to denote our algorithm. We make good use of the structure of the Fuzzy P-tree to reorganize the tree when we prune nodes. The node contains any singleton itemset that is not frequent was separated from the fuzzy P-tree with its subtree. Its original position is replaced by its sibling node or set to null. If the unattached tree still contains a different singleton itemset that is not frequent after eliminating the previous itemset, we need continue the last operation until the tree doesn?t contain any itemset that is not frequent.

Then we begin to merge unattached trees with the fuzzy P-tree. The nodes of unattached trees are processed in preorder traversal. The location of inserted nodes is located by traversing the fuzzy P-tree according to preorder traversal. We summarize all possible instances as four cases below, where m denotes the node of unattached tree under consideration  Case I An identical node n exists in the fuzzy P-tree.

1 .FS (n)=FS (n)+FS (m) 2.Compare the direct child node of m with the subtree  of n, and the next sibling node of m with the sibling nodes  of n Case I1 m is a parent of existing node n 1 .Set n with its subtree on the child branch of m 2.Set m as the new root node, or the child node or the  sibling node of the previously investigated node according to actual information, FS(m) = FS(m) + FS(n)  3.Compare the direct child node of m in the unattached tree with the nodes of the current subtree of m, and the next sibling node of m with the current sibling nodes of m  Case I11 m is a sibling of existing node n.

0 If m and n have a common leading substring S  that is not equal the parent node of n 1 .Create a dummy node k for S.

2.Set k as the new root node, or the child node or the  sibling node of the previously investigated node according to actual information, FS(k) = FS(m) + FS (n).

3.Set m and n as the child nodes of k 4.Compare the next sibling node of m in the  0 Else 1.Set n as the new root node, or the child node or the  sibling node of the previously investigated node according to actual information  2.Compare the next sibling node of m in the unattached tree with the current sibling nodes of m  Case IV m is a new and unique child of current node n. If m is not the unique child, it should be processed as previous cases.

unattached tree with the current sibling nodes of m  1 .Set m as the child node of n 2.Compare the next sibling node of m with the current  sibling node of n This algorithm is similar to the algorithm of the  building of P-tree to some extent. Note that we don?t need to traverse the tree from the root node every time when merging unattached trees. After one node is inserted, the location of its child nodes and sibling nodes can be located based on previous search. In some cases the node and its subtree can be merged once and we don?t need process nodes one by one, for example, Case 111. So the average time complexity is linear to the number of the nodes of the tree. It is obvious that the algorithm doesn?t change the original characteristic of the tree. We still can calculate support values through traversing the tree after pruning nodes.

6 Experimental Results  In order to evaluate the algorithm we have described above, we have compared Apriori-TFP-PN algorithm with two algorithms, Apriori (founded on a hash tree structure[?]) and Apriori-TFP. The comparison is based on our own      implementation of these algorithms, which are unoptimizable programs. We use a real-life database with about 50,000 records. It has seven attributes: four are quantitative and three are categorical. There are 25 linguistic terms in total after transformation. We have compared their execution time for five different support thresholds. The result is shown in Figure 3 .

+ Apriori zn I  Apriori-TF  Apriori-TFP-PN J + D  Support Value (%)  Figure 3 .  Comparisons of three algorithms  Both Apriori-TFP and Apriori-TFP-PN outperform our implementation of Apriori. This also verifies the experimental results in [3]. We notice our algorithm need less time than Apriori-TFP in some cases. When the support threshold is relatively large or small, the difference between them is not obvious. After analysis, we think the phenomenon is based on two reasons. On one hand, the algorithm may prune a small quantity of nodes because most singleton itemsets are frequent with a small threshold.

On the other hand, it doesn't need multiple scans because the size of the largest frequent itemset is small with a big one. But it is known that we will discover too much meaningless association rules with a small threshold and lose useful information with a high threshold. So even in some cases our algorithm doesn't show clear advantage, it is still a positive improvement on Apriori-TFP algorithm.

As we use memory-resident data in the experiment, we believe our algorithm will work better if the data cannot be retained in main memory.

7 Conclusion  We present an algorithm for computing fuzzy support value using a variant of a set-enumeration tree that is stored with partial support values. The tree is built by one database scan. The fuzzy support value of any candidate itemset can be worked out just by traversing the tree. Most existing methods of mining fuzzy association rules can take advantage of this data structure to improve their performance.

As the P-tree (or Fuzzy Pdree) has a simple structure and is an compressed representation of source data to some extent, it normally requires far less storage space than source data. HoweverFit still may create an exponential storage requirement when source data is densely populated, and thus the tree-is not possible to completely be retained in main memory. Coenen and his co-workers 13] recommend partitioning the tree to several subtrees and every subtree may b_e retained in main memory. As candidate subsets may belofig to different subtrees, we still cannot avoid multiple U 0  operations in this way. We put forward a new algorithm that may prune a large quantity of nodes and the P-tree can be memory-resident after the first traverse of the tree.

