Data Migration Ecosystem for Big Data  Invited Paper

Abstract? Data Migration is the process of moving data from a  system or systems to a new environment. Often, it is a sub-activity of  a business application deployment. Big data is defined as data that is  huge, has heterogeneous data dictionaries and involves complex  manipulation. Due to nature of the process complexity and its  resources hungry approach in migrating Big Data, special attention is  required to have a proven methodology and ecosystem to govern the  process. The Data Migration Ecosystem for Big Data is the  productive set of interacting processes, practices and environments,  to collect data from one location, storage medium, or  hardware/software system, to cleanse, transform and transfer it to  another. The processes and practices are governed by rules and  disciplines, with the goal of ensuring information is complete, of high  accuracy and consistent. This paper is based on our experience in  migrating data for a Malaysia government agency, which involves  approximately 1 billion rows of data from 31 heterogeneous sources /  systems. Some of the data migrated was created in the seventies  (1970), for which the business logic has since been enhanced or  changed. The challenge is further complicated by available data being  from proprietary databases that are non-RDMS compliance and  includes data that is manually maintained in Microsoft Excel  spreadsheets.



I. INTRODUCTION  Big data management is an indispensable activity in the information age. With evolvement of information collection mechanisms and advancement of storage and database technology, data sets in today?s institutions are often so large, complex and rapidly-generated, that they cannot be processed by traditional information and communication technologies. (T.

Kalil, Fen, Z. 2013) Big data is collections of data sets that are large and complex. According to the IDC forecast, data is expected to grow to 40 zettabytes (40 trillion gigabytes) in 2020 (IDC 2012) from an estimated 1.8 zettabytes in 2011.

With a growth rate of a factor of 300, special attention is required by institutions in capturing, manipulation, storing, searching, retrieval, sharing, transferring, analysis and visualizing big data. In industry practice, the big data challenge is triggered when the data of an institution is high in volume, has heterogeneous information stored, and involves heavy and complex business rules on data manipulation.



II. DATA MIGRATION  Data migration for big data is a challenging and resource demanding process. Data Migration is the process of moving data from a system or systems to the new environment. Often, it is a sub-activity of a business application deployment. Data migration plays a vital role in ensuring business continuity. It needs substantial focus and attention, and discipline, where :  (1)No data is lost due to the data migration process, (2) No data is polluted and (3) Change to data is governed by endorsed rules.

Data Migration is an important process and much literature has being published. Most of the literature is focused on the area of Data Cleansing. This literature discusses the techniques for relational data cleansing (Jaipal et al. 2011), processes of data cleansing (Mueller 2003), metrics to measure the data quality (Leo 2002, Loshin 2006) and detailed processes on records deduplication. Some literature discusses the algorithms to resolve a specific problem, like Levenshtein distance (Levenshtein 1966), Jaro distance (Jaro 1972), Jaro-Winkler distance (Winkler et al. 1991) on strings similarity detection, Soundex (Russell, Odell, 1918) on phonetic coding schemes, duplicate detection machine learning functions (Bilenko et al.

2003, Pinheiro et al. 1998, Tejada et al. 2001), etc.. Apart from literature, there are also patents filed: US7296011, US7516149, US20100005048, etc. on specific data cleansing methods. Most of this literature is focused on a specific aspects of the data migration problem, less on the overall data migration processes or the data migration ecosystem. The significant publications on overall processes are mostly from institutions e.g. US Department of Education (2007), DataFlux Corporation (2003), IBM (2007).



III. DATA MIGRATION ECOSYSTEM FOR BIG DATA  A data migration ecosystem is the productive set of interacting processes, practices and environments, to collect data from one location, storage medium, or hardware/software system, to cleanse, transform and transfer to another. The processes and practices are governed by  rules and disciplines, with the goal of ensuring information is complete of high accuracy and consistent.

Fig. 1. Data Migration Ecosystem for Big Data

IV. DATA MIGRATION PLANNING FOR BIG DATA  The data migration commences with the needs analysis of the Data Migration project. It is highly recommended to execute data migration as an independent project. Thorough activities planning with issues and risks mitigation plans are the foundation for a successful project.

The Data Migration Planning stage for Big Data is focused in defining the scope of the data migration project that specifies what data are to be migrated, the source of the data, and the target environment. Often complete listings of all systems or data sources are mandatory to govern the subsequent activities.

Within this stage, the Data Migration Team specifies the general project deliverables, the timeline, and governance information such as Quality Plan, Change Management Plan, Stake Holder List, Standard Operation Procedures and Communications Plan.

Fig. 2. Data Migration Planning for Big data

V. DATA MIGRATION PROFILING FOR BIG DATA  Data migration profiling is a process to study and understand the source systems data to clarify their structures, contents, data dictionaries, relationships and derivation rules.

Profiling helps to understand anomalies and to assess data quality and to discover, register and assess enterprise metadata.

The key objective of data profiling is to validate metadata when it is available and to discover metadata when it is not.

The result of the analysis provides valuable information for stakeholders strategically to determine the suitability of the candidate source systems, and tactically identify problems for later solution designs and setting sponsors? expectation.

A. Preliminary Data Analysis  Preliminary Data Analysis helps the Data Migration team familiarize itself with the metadata dictionary and dataset definition, and with light profiling assessment on sample data.

It gives an early indicator whether the right data is available at  the right level of detail and any potential anomaly can be handled subsequently.

B. Data Profiling Meetings  Data Profiling Meetings are interactive sessions conducted between the Data Migration Team and the Data Owner (Technical & Business) to communicate and clarify the data migration requirements and expectations, and to study and understand the business requirements and data / business history. A series of focus group meetings are recommended, where the meetings are organized based on source systems or based on business modules.

During data profiling meetings, data is analyzed per-table basic. The data is profiled by analyzing the data structures and the nature of data stores in the table (column profiling) and the relationships between tables (dependency profiling).

Requirements are usually captured based on Data Owners experience, on what they have encountered or what they are suspicious of, for example in our case, the Data Owner are aware of existence of duplicate person records due to the automatic registration process.

C. Detailed Data Profiling  Detailed Data Profiling is a process that involves detailed analysis of metadata of each identified table, identification of detection rules, characteristics of tables and action required.

The Detailed Data Profiling process is executed in multiple iterations, where each iteration introduces additional intensity to data detection requirements.

The Detailed Data Profiling process begins with the study of tables characteristics. Table characteristics refer to how a table is used in the source system and the type of data it contains and these include:  a. Transactional Tables ? tables that store business  transactional data which represents a snapshot of or a  specific transaction of an event.

b. Transactional Inactive Tables - similar to Transactional  table, but the data its content is archived state or  historical data. These data are static and no further  modification or change is expected.

c. Reference Static Tables ?  store  reference information  that seldom changes.

d. Reference Dynamic Tables ? store reference  information that are frequently changed or modified.

e. Temporary Tables ? are temporary, interim or staging  tables that exist to satisfy the processing requirements  of the application.

The Table characteristics study helps the Data Migration team and the Data Owner in deciding subsequent actions to be performed, which can be generalized into:  a. Migrate As-Is ? tables to be migrated to target system  without any transformation or cleansing.

b. To be Cleansed ? tables to be analyzed, cleansed and  transformed prior to being migrated to the target  system.

c. To be Merged - tables to be merged to other tables.

? Establish scope  ? Identify risks, constraints, dependencies and assumptions  ? Develop data migration risk mitigation plan  ? Develop data migration operating procedure  ? Develop data migration communications plan  ? Identify critical paths and success factors  Plan Data Migration Project  ? Determine business requirements and expectations  ? Determine technology and it infrastructure requirements  ? Determine data security and privacy requirements  Determine Data Migration Requirements  ? Identify and collect existing data related artifacts  ? Identify and collect target data related artifacts  ? Blueprint current state of data architecture  ? Determine data migration technology  Assess Current & Target Environments  ? Determine data migration timeline  ? Determine data migration method  ? Determine data cleansing method  ? Determine data quality plan  Develop Data Migration Plan  ? Identify stakeholders  ? Define roles and responsibilities  Define and Assign Roles & Responsibilities     d. Not to be Migrated - tables will not be migrated to the  target system.

For tables that identified as ?To-be Cleansed?, the metadata  of these tables are analyzed in detail with detection  requirements categorized as follow:  a. Missing Values ? fields with null or blank values.

b. Invalid Data ? fields with values that do not conform to  data types or formats.

c. Horizontal Constraint ? fields with value that do not  conform to the accepted record pattern after referential  checks with other fields value within the same tables.

d. Condition Detection ? refers to detection rules that are  based on business data specifications that are domain  and data specific. This detection often requires custom  developed detection mechanisms.

e. Address Cleansing ? refers to detecting, correcting,  transforming and harmonizing address values to pre-  defined formats and abbreviations.

f. Data Domain Violation ? checking on fields with value  that do not conform to a predefined acceptable value of  a trusted reference list.

g. Duplicate Record Detection ? refers to detection of  double-entry records based on similarity of fields  value.

h. Orphan Record Detection ? refers to detection of  records with missing referential (parent) record  (invalid foreign key), ot records with missing foreign  key values. This detection is similar to domain  violation, except that the exception is the record and  not a specific column value.

At the end of each detailed data profiling iteration, data  profiling reports are updated. Data profiling documents are structured based on modules of source systems, and subject to on-going modification whenever new requirements or rules are gathered. These reports describe the requirements, expectations, and source data structures, contents, data dictionaries, relationships and derivation rules, and govern the data migration team?s subsequent activities.



VI. DATA CLEANSING FOR BIG DATA  Data Cleansing is the process of executing cleansing rules identified in the Data Profiling Analysis to discover any potential bad data or inconsistency, and to perform any necessary rectification or correction to these bad or inconsistent data.

Fig. 3. Data Cleansing for Big Data  Data Cleansing activities are governed and tracked using a Data Analysis Report. The Data Analysis Report content is derived from the Detailed Data Profiling Document, where it  details the execution, detection and rectification of all identified rules captured during the Data Profiling exercises.

Data Cleansing is performed in multiple iterations in which each iteration introduces new data detection requirements to refine or to further discover any potential bad data or data inconsistency. Each iteration involves activities starting with Data Extraction & Loading, followed by Exception Detection and ending at Exceptions Rectification.

A. Data Extraction & Loading  Data Extraction and Loading are the processes of exporting data from the source system and loading it into the Data Migration Staging environment.

Data Extraction & Loading starts with export of data from source systems by the Data Owner. Data is extracted with ?AS IS? structure into predefined format for ease of data transport between platforms. In the case of our study, due to the heterogeneous form of source systems (e.g. MySQL database, UNISYS database, AS400 DB2 database, Microsoft SQL Server database, Microsoft Access database, CSV files, etc.) data is extracted into CSV file format (using tide (~) delimiter).

In the technical layout, it always recommended to have three staging databases:  1) T1 database Data is first loaded into T1 database with ?AS IS? structure  (whenever possible), with minimal transformation rules. The T1 database should at best mirror the state of data in the source system. No data manipulation should be performed on this database, except corrections that necessitate completing the data loading.

Exceptions that occur during loading are categorized as ETL Exceptions and that a report should be produced and forwarded to the Data Owner for rectification. The Data Owner is expected to rectify or correct these exceptions in the source systems, and re-extract the data. The Data Migration team will re-load the new data into the same table in the T1 database, and update exception statistics accordingly. The number of records in the T1 database must match with the source data extracted.

2) T2 database Data in T2 database may not be in ?AS IS? structure as is  the case for the T1 database or source data. All data manipulation, transformation and detection are performed in this database. Some tables may be altered (with additional columns, merge with other table or split into multiple tables) to enable execution of required detections.

3) T3 database T3 database contains only final tables that are required for  destination production database. The data and information in T3 should be in the structure and format specified as in final production database, all interim fields, data, information and tables should not be reside in this database.

B. Exception Detection  Exception detection is the process of executing data detection rules identified in the Data Profiling process. The exception detection activities validate the requirements of the Data Cleansing:       a. Validity ? the degree to which the rules conform to  defined rules or constraints, i.e. Data-type constraints,  Range constraints, Mandatory constraints, Unique  constraints, Set-membership constraints, Foreign-key  constraints, Regular expression patterns and Cross-field  validation.

b. Accuracy ? the degree of conformity of a rule based on  a standard or a true value. Accuracy is achieved by  applying various algorithms / logics and cross-  examination with a good reference set of data.

c. Completeness - the degree to which all required rules  are known. Data incompleteness is difficult to fix in the  Data Cleansing process, and always requires going back  to the original source of data for further investigation.

d. Consistency - the degree to which a set of rules are  equivalent across systems. Inconsistency occurs when  two data items in the data set contradict each other.

e. Uniformity - the degree to which a set of data rules are  specified using the same units of measure in all  systems.

Exception Detection for Big Data often requires high  resources and is time consuming, because the tables involved are often large and sometime may requires complex logic.

Special attention is required to choose the most appropriate mechanisms.

C. Exception Rectification  Exception rectification is the process of correcting or modifying data to make it conform to predefined format / rule, based on exception lists generated from the Exception Detection process.

There are a few potential rectification results:  a. Exceptions list are identified as valid data.

b. Exceptions list are identified as invalid data and  require correction.

c. Exceptions lists are identified as invalid data, no  correction on exception records but on other table  records.

d. Exceptions lists are identified as invalid data and  correction is not able to be carried out.

Table I has the recommended corrections by Exception Category.



VII. PRODUCTION DATA MIGRATION FOR BIG DATA  Production Data Migration is the process of extracting source data from source systems, cleansing at the staging environments, and transforming and loading the ?clean? data into the target system. This is the main process of the entire Data Migration activities and often the most challenging, due to timelines, volume of data and complexity of processes.

This stage objective involves provision of the full data set into the staging environment. As a prelude to this stage, all requirements, processes and programs used should be ready, prepared and tested.

TABLE I.  EXCEPTION CATEGORY  Error  Category  Error Type Proposed Correction /  Rectification Action  Missing values  Null required fields, e.g.

employee name  or employer name  Produce Rejection Listing User verify  Use tool to correct values  Null mandatory  fields  Produce Rejection Listing  User verify  Use tool to correct values  Invalid  Data  Zero data fields Use tool to nullify the value  Null numeric  field  Use tool to assign zero value  Invalid email format  Produce Rejection Listing User verify  Use tool to correct value  Format non- compliancy  Produce Rejection Listing User verify  Use tool to correct value  Invalid IC Produce Rejection Listing  User verify Manual rectification  Data  Domain Violation  Domain  violation  Produce Rejection Listing  User verify Use tool to correct value  Horizontal  Constraint  Horizontal  Constraint  Produce Rejection Listing  User verify  Use tool to correct value / Manual rectification  Condition  Detection  Condition  Detection  Produce Detection Listing  User to clarify  Address Cleansing  Wrong assignment  Produce Rejection Listing User verify  Use tool to assign new value  Standardization Produce Rejection Listing User verify  Use tool to assign new value  Duplicates Duplicates Produce Rejection Listing User verify  Use tool to deduplicate or move to  pending repository  Orphans Orphans Produce Rejection Listing User verify  Use tool to remove or move to  pending repository or archives  False  Records  False Records Flag records and notify user to verify      Fig. 4. Production Data Migration for Big Data       A. Commence Restrictive Usage Period  The Restrictive Usage Period, sometimes known as the Freeze Period, is a period of time ?before the target system goes live? to ?the date the target system goes live?. During this period of time, the functionalities of the source systems are limited to only search, retrieve and view of information, and prohibits all modification (add, update and remove) functionalities. The enforcement of the Restrictive Usage Period is vital in ensuring data integrity between the source systems and the target systems, because the source data extracted during this period is the foundation data for the target system. And, any change to the data via source system will not be picked up and reflected in the target system. With the limited functionalities of the source system, it is generally recommended to fall back to the manual processes, especially on any activities that involve updating or modifying of data, where any update or modification intention is recorded and later to be entered into the target system after its goes live.

B. Source Data Extraction  Processes in Source data extraction are similar to the Data Cleansing, Data Extraction & Loading sub-stage. The Key difference is that the source data extraction is performed during the Restrictive Usage Period, and all data to be migrated are involved.The extracted source data is loaded into the Production Staging environment for Data Cleansing purposes.

Due to the short Restrictive Usage Period, especially when Big Data is involved, it is recommended to commence data extraction prior to the period. Selected data may be extracted early, weeks before the target system?s live date. Following example Restrictive Usage Period is two weeks with T as the target system live date:    Fig. 5. Data Migration Periods  C. Staging Data Cleansing  In some Data Migration project, some detection and correction activities are carried out in the Production Data Migration Stage. These activities are necessary due to rectifications or corrections that are not-performable in the source system, as well as some new created/modified data yet to be cleansed. The key expectation of the detection and correction activities is to ensure data is in a ?clean? state prior to being migrated to target system.

The detailed processes of Data Cleansing are similar to the Data Cleansing stage, with the exception that only data detection rules that required subsequent corrections are  executed. Corrections are done in the staging environment, either via automated pre-program logic, or required manual intervention. As the limited timeframe for Production Data Migration, manual correction should kept to a minimum.

D. Transform and Loading to the Target System  This is the final stage of the Production Data Migration project. The stage involves taking of ?Cleaned? source date from staging environment, transforming according to the target schema data dictionary and loading into the target environment.



VIII. POST-PRODUCTION DATA MIGRATION  Post-production data Migration is necessary to upkeep the integrity of both the data in both source system and target system. The objective is to ?Identify, detect and extract changes from the source systems? database data, and provision of these changes to the staging database for migration to target system database in the Forward-Sync process. And, to identify, detect and extract changes from the target system?s database back to source systems? database via the Back-Sync process.?  Application of the Post-production data Migration commences immediately after going live with the target system database. This process is executed iteratively using a fixed interval (every day, weekly, etc.) until the source system is switched-off and replaced completely by the target application.

The execution timeframe of each cycle is normally measured in hours and should be completed within the same day.

The key activity within each cycle of the Post-Production Data Migration is similar with the Production Data Migration, with the exception of:  a. Minimal / no data analysis and profiling activity.

b. Identification, detection and extraction of delta  (changes) data, using pre-identified and agreed delta  data detection mechanisms.

c. All data transformations, patching, rectifications and  corrections are performed on the working database.

d. Migration of ?clean? data into the staging database for  production purpose. This staging database is delivered  to the Application vendor for migration into the live  database.

Within the objective of this iteration type, the destined  database (the new application?s database) data should be appended with the data provided by this cycle.



IX. RECAPITULATION AND CONCLUSION  Special attention is required to choose the appropriate mechanisms to execute Data Migration for Big Data. The most primitive mechanism is to construct custom SQL queries for each activity, which required that the Data Migration team to be highly skilled in PL/SQL knowledge and having an in-depth knowledge on how the SQL query syntax is to be executed in the database. For example, ?NOT EXISTS? syntax in the SQL query may result in a full table scan that absorbs considerable system resources.

To assists in our data cleansing and data migration project of a Malaysia Government agency that involving approximate 1 billion rows of data from 31 heterogeneous sources / systems,     we developed a tool named ?Mi-Morphe?. The ?Mi-Morphe? tool allows us to ETL source data from all the heterogeneous systems (MySQL database, UNISYS database, AS400 DB2 database, Microsoft SQL Server database, Microsoft Access database, CSV files, etc.), performs data exception detection, in addition to the generates exception reports for rectification purposes. In addition to the the full functionalities that allow dynamic configuration, the ?Mi-Morphe? utilization of the GPU hardware substantially reduces the processing duration required for orphan records detection, duplicate records detection, domain detections and other detections that involve complex mathematical formula. Most of these detection algorithms are not feasible even with highly tuned SQL queries.

Data Migration for Big data is a complex challenge. It needs careful planning and execution, and requires stakeholder close attention to ensure completeness and correctness. The selection of mechanisms is important especially when performing complex data detection that involved complex mathematical formulae.

