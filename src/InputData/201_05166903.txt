Study and Implementation of Association Rule Algorithm in Data Mining

Abstract?On the basis of in-depth study of the existing data mining algorithms, according to the disadvantages of them in relational databases, in this paper a new data mining algorithm based on association rules is presented. The algorithm can avoid redundant rules as far as possible, and the experimental results show that the performance of the algorithm can be obviously improved when compared with the existing algorithms.

Keywords-Data mining; association rules; Apriori

I.  INTRODUCTION With the rapid development of the information  technology, human beings can more fleetly and conveniently access to data, save data, and the volume and complexity of the data are unprecedented. And with that every walk of life are all beginning to adopt WEB site to be marketing and CRM tools, the data obtained is increased with the exponential speed. However, only part of the huge amount of data has been used for the distillation of knowledge, namely, embarrassing situation of a "rich data" and "lack of knowledge? has been appeared. However, the appearance of data mining which uses computer technology to analyze, deal with and extract useful and interesting knowledge makes it possible to solve this issue.

Data mining is a treatment process to extract useful and interesting knowledge from large amount of data. The knowledge modes data mining discovered have a variety of different types, the common patterns are: association mode, classification model, class model, sequence pattern and so  on ]2[  . Association rule mining is the current hot. In association rule mining algorithms, the most algorithms are based on Apriori algorithm to calculate, and in the mining process they can produce amount of option set, which will reduce the efficiency of the association rule mining; at the same time the association rule mining will obtain amount of redundant rules, which will reduce the validity of the association rule mining; and also the user interaction performance of the association rule mining is relatively poor.

On the basis of in-depth study on the existing data mining algorithms, according to the disadvantages of the association rule mining algorithms in the relational databases, a new data mining algorithm based on association rule is presented.



II. ASSOCIATION RULE MINING In the knowledge modes data mining discovered,  association rule mode is a very important and also the most active branch. Association rule refers to the rules of certain association relationship between groups of objects in the database. It can be used to find the contact among the different commodities (terms) in the transaction database, and so that the behavior patterns of customer purchases will be found. Fining that rules can be applied for merchandise shelf design, inventory arrangements and users classification according to the purchase patterns.

A. Association rule conception Association rule mining can be described as following:  assuming },,,{ 21 niiiI L= is n aggregates with different terms, then for a transaction database D , each element T in D  is a set composed by some terms in I , IT ? . The association rule is expressed as YX ? , hereinto, IX ? ,  IY ? , and ?=?YX . The association rule mining is to discover all condition implicative expression meeting the minimum degree of confidence and support users given, that is, association rules. The confidence and support degree of these rules are all greater than or equal to the minimum degree of confidence and support ]5[ .

The confidence and support degree of the association rules respectively reflects the correct degree and the support rate of them. Analyzing from semantic perspective, the degree of confidence refers to the trustworthiness of the rules; support degree refers to the possibility of the rule mode appearance, reflects the importance of the antecedent to the consequent.

In general, the user can define two thresholds which is respectively set as minimum support threshold and minimum confidence threshold, the support and confidence degree the data mining system generated are required to be not less than the two given thresholds, then we can say that this rule is valid, otherwise it is null and void. Thus a specific association rules can be uniquely identified by using a implication expression and two thresholds ]1[ .

B. Mining steps of association rules The steps of the mining association rules can be roughly  described by a two-step process ]3[ .

DOI 10.1109/ICSPS.2009.212     (1) Identify all the frequent term sets. That is, to identify all the term sets whose support degree greater than pre-given support threshold.

(2)To generate strong association rules on the basis of the found frequent term sets. That is, to generate those association rules whose support and confidence respectively greater than or equal to the pre-given support threshold and confidence threshold.

In the above two steps, the second step is relatively easier, because that it only needs to list all possible association rules on the basis of the found frequent term sets, and then measure these association rules by using support and confidence threshold, at the same time the association rules meeting the support and confidence threshold requirement are considered interesting association rules.

In fact, because that all association rules are generated on the basis of the frequent term sets, they have been automatically meet the support threshold requirements, thus only the requirements of the confidence threshold should be considered. The first step is the key steps for mining association rules and the overall performance of mining association rules is decided by it, so all the algorithms for mining association rules are focused on the study of the first step.

C. Mining algorithm of association rules Existing mining algorithm of association rules can be  broadly divided into search algorithms, hierarchical algorithms, data sets partitioning algorithm, and so on ]4[ .

1) search algorithms  Search algorithm is to deal with all the term sets contained in the affairs which were read into the data set, so it needs to calculate the support of all term sets in the data aggregate D . Search algorithm can find all the frequent term sets only through one scan on data sets, an affair contained n projects will generate 12 ?n term sets, and when the terms the data set D contained is very large, the quantity of the option sets should be calculated and stored is often very large. Therefore, such algorithms can only be applied in the association rule mining with relatively concentrative data.

2) Hierarchy algorithm The hierarchy algorithm with Apriori algorithm to be  representation is to find frequent term sets since childhood and until now in the terms contained. Apriori algorithm will find all the frequent k  term sets in the first k  scan on the data sets, the option sets in the first 1+k scan will be generated by all frequent k term sets through connecting computing. The number Apriori algorithm need to scan data sets is equal to the term numbers of the maximum frequent term sets. The hierarchical algorithm with Apriori algorithm to be representation can generate a relatively small option set, and the number of scanning database is decided by the term numbers of the maximum frequent term sets.

3) Partitioning algorithm Data set partitioning algorithm includes partition  algorithm, DIC algorithm, these algorithms will divide the entire data set into data blocks which can be stored in  memory to handle, in order to save the I/O spending of visiting rendering. Partition algorithm only requires two times of scan on the entire data set, DIC algorithm can identify all the frequent term sets through the two times of scan when appropriately dividing data blocks. The number of the option term set of the data set dividing the algorithm generally larger than it of Apriori algorithm, increasing the data distortion can reduce the number of the option term set.

Data set partitioning algorithm is the basis of the various parallel association rule mining algorithm and distributed association rule mining algorithm.



III. APRIORI ALGORITHM  A. Apriori algorithm idea Apriori algorithm is an effective algorithm to mine  Boolean association rule frequent term sets. Apriori algorithm uses the strategy of breadth-first search, that is, layer-by-layer search iterative method, first of all, find out the frequent term set with length of 1 which is recorded as 1L , 1L  is used to find the aggregate 2L  of frequent 2-term sets, 2L  is used to find the aggregate 3L  of frequent 3-term sets, and so the cycle continues, until no new frequent k - term sets can be found. Finding each kL needs a database scan. Finding all the frequent term sets is the core of association rule mining algorithm and it has the maximum calculating workload. Afterward, according to the minimum confidence threshold the effective association rules can be constructed from the frequent term sets. Apriori algorithm flow as shown in Figure 1 ]6[ :     Data initialization, generate l frequent item sets L1;order K=2  Lk-1<>?  Through (k-1) frequent item sets generate k option sets  Y  K=K+1  Frequent item set L  Scan database, solve out the support frequency of each option item set  N  END   Figure 1.  Apriori algorithm flow  B. Performance analysis Experiments show that Apriori algorithm has better  performance and scalability, especially in dealing with sparse data. Using layer-by-layer iterative search method, the algorithm is simple, and there is not complicated theory, but also easy to achieve. But there are also some shortcomings difficult to overcome ]7[ :  (1) There are too many times of scanning the database. In In the Apriori algorithm description, each option term set generated, the database should be comprehensively searched.

If a frequent term set with length M should be generated, the database should be scanned for M times. When many affair data are stored in the database, in the limited memory capacity, the system I/O load is considerable big, and then the time of each scanning database will be very long, so that the efficiency is very low.

(2) Apriori algorithm will have many middle term sets.

Using kL  to generate optional kC , the greater the k the generated optional term set will be increased with geometric level.

(3) Using unique support, the difference of each property importance is not considered. In real life, the occurrence of some affairs is very frequently and some is very sparse, thus it would have a question for our mining: if the minimum support threshold set too high, although the pace would be accelerated, but the data covered would be less, and the meaningful rules may not be found; if the minimum support  threshold set too low, then many of the rules had no practical significance will be filled in the entire mining process, which will greatly reduced the mining efficiency and rule usability.

Those all will mislead decision making.

(4) The fitness landscape of the algorithm is narrower.

The algorithm only considers the association rule mining of single dimension Boolean, but in practical application there may be multi-dimensional, quantitative and multi-level association rules. At this time, the algorithm is no longer applicable, needs to be improved, or even needs to be re- designed.



IV. APRIORI ALGORITHM IMPROVEMENT  A. Apriori algorithm improvement idea To solve the problem of the algorithm efficiency, the  strategy of divide and rule is adopted. In the Apriori algorithm, the support plays a decisive role, but now the confidence will be in the first place to mine some association rules with very high degree of confidence. The entire algorithm is divided into three steps: generate optional term sets, filter optional large term sets, and generate association rules ]9[ . In the three steps, the key is filtering candidate large term sets. After a first scan on the database, the initial optional term set is generated, and after the support is calculated the affair data term with low support is found, and then the confidence of it in the corresponding original database will be calculated through the method based on probability, if too high, the affair data term set which it belongs to will be filtered to reduce the number of records in next search; thus in the second scanning database, the scanning scope will be reduced and the searching time will be shorten, which can improve the algorithm efficiency; by the same way, finally a large optional set will be generated, resulting association rules, which will be output.

B. Apriori algorithm implementation 1) Generate optional large term set Combining all the large term set with the same first k-2  term, the different last term in 1?kL  to be kC . Set p , q  are two large term sets and the first k-2 terms of them are same, that is,  2211 ,, ?? ?=??=? kk itemqitempitemqitemp L , but the last terms are different, set 11 ?? ?<? kk itemqitemp , so combining p , q  an optional large term set can be generated. The first k-2 terms of the optional large term set should be maintained, the first k-1 term is 1?? kitemp  and the first k term is 1?? kitemq .

If the large term set 1?kL  is known, in order to solve kL ,  it does not need to calculate the support of all the kmC in k term sets, and only to calculate the k term set whose all k-1 term subsets are large term set. Because that only the support of these term sets is possibly more than infirm term set, so they are called optional large term set. If using all possible     terms to expand each term set in 1?kL , and then delete those term sets of k-1 term subset which contains terms not belong to 1?kL , the result surely is a superset of kL . The first step is to expand 1?kL  through each term in the database, and then delete part of them, for each term set in this part, if the first k-1 subset obtained through deleting the first k-1 term is not belongs to 1?kL , and the condition 11 ?? ?<? kk itemqitemp  is for no repetition.

Therefore, after the first step, kk LC ? .  In a similar way, in the second step of deleting kC  there are term sets of the k- 1 term subset which does not belongs to 1?kL , and each term set which possibly is kL is not deleted.

The generation of the optional large term set greatly reduces the number of the term set which needs for support calculation, the process of solving the optional large term set can be simplified as follows: firstly solve the term set 1L , then generate the optional large term set 2C  from 1L , and further calculate the support of each term set in 2C  and delete the part which is less than infirm term set, that is, obtain 2L ; and so on all optional large term set will be solved.

2) Calculate support The method to calculate the support is to obtain the  support of all term sets in kC  through scanning the database for a certain k value, set r to be the greatest possible length of the large term set, then the algorithm only needs to scan the database for r times. For a certain record t the optional term set it included should be calculated, recorded as tC . In order to narrow the search scope, kC  uses the organization form of hash tree. The leaf node of hash tree is term set sequence, and the inner node is hash table. When increasing a term set c, starting from the root node and down to certain leaf node, and at the inner node with depth d, according to the hash value of the first d term of c the corresponding barrel and then through the barrel pointer the lower branch will be determined. All the nodes should be initialized for the leaf nodes, when the number of the term set in the leaf node reaches a certain threshold, the node will be transformed into inner node.

From the root node of the hash tree, if they are leaf nodes, comparing with each k term set in the leaf node, the k term set containing t should be found; after hash calculation, if the first i term of reaches to inner node, all the terms after the i term of t will be carried out hash calculation and selected for lower nodes. Apriori algorithm fully reduces the term set and only calculates the support of the optional term set whose support may greater than or equal to minsup; In addition, the storage of kC  uses hashtree structure, the item set counting is faster.

3) Filter optional large term set After scanning on the database, the initial optional term  set is generated, and after the support is calculated the affair data term with low support is found, and then the confidence of it in the corresponding original database will be calculated through the method based on probability, if too high, the affair data term set which it belongs to will be filtered to reduce the number of records in next search; thus in the second scanning database, the scanning scope will be reduced and the searching time will be shorten, which can improve the algorithm efficiency. When calculating the efficiency of the affair data term set for the affair data term with lower support, if the efficiency higher this affair data term set will be filtered; if the efficiency is lower this affair data term set will be reserved. When filtering the affair data term sets in the database, according to the above two theorems to filter them, and ignore them in the following support calculation, thus the record number involved in calculating the support of the optional term set will be less than the original record number in the affair database, consequently the efficiency of the entire algorithm will be improved.

C. Experimental results and conclusion Using c++ and in the HP Pavilion g3628cx PC machine  with EMS memory of 2G, the above algorithm and APriori algorithm has been carried out experiment, and the experimental data is the order data of a certain large-scale company. The testing results as shown in table 1:  TABLE I TESTING RESULTS types Affair number  APriori algorithm  Optimization algorithm  20 thousand 7.3 minute 4.2 minute 40 thousand  8.1 minute 4.9 minute 60 thousand 12.3 minute 6.1 minute 80 thousand 16.4 minute 7.8 minute  100 thousand 20.6 minute 8.3 minute   From the testing results it can be seen that when very much business data records, the executive efficiency of the improved algorithm has increased almost 30% than the Apriori algorithm. Because that the Apriori algorithm is selected from the functions when 1?kL  generating option kC , and then delete those options having infrequent subsets by using Apriori character; once the options generated, the database will be scanned and all the optional subsets in the affairs will be found through subset function, and each such option will be cumulative counted. Finally all options meet the minimum support form in a frequent term set kL , but this will spending a lot of time. The improved algorithm sets the confidence to be the first place and directly filters the affair data in substantial transaction database which does not meet the requirements, so its performance can be greatly improved.



V. CONCLUSION With the rapid development of database technology and  the widely appliance of database management system, there     are more and more accumulated data. Faced the mass storage data, how to find valuable information or knowledge is a very arduous task. Data Mining is quickly developed to meet such a request. Data mining is to extract implicit, previously unknown knowledge and rules from large database or data warehouse, and these knowledge and rules have a potential value for the decision-making. Data mining is a product from the combination of artificial intelligence and database development, is one of the forefront research directions of the current international database and information decision- making system ]8[ .

The discovery of the association rule is a most successful and most vital duty in the data mining, is a very active research area in current data mining, its goal is to discover all frequent modes in the data set, and the current research work carrying on are mostly focused on the development of effective algorithm. On the basis of in-depth study of the existing data mining algorithms, according to the disadvantages of them, in this paper a new data mining algorithm based on association rules is presented. The algorithm can avoid redundant rules as far as possible, and the experimental results show that the performance of the algorithm can be obviously improved when compared with the existing algorithms.

