Maintenance of Discovered Informative Rule Sets:  Incremental Insertion*

Abstract - We propose here an efficient data-mining algorithm to discover the Informative Rule Set (IRS) when the transaction database is updated, i.e., when a small transaction data set is added to the original database. An IRS is defined as the smallest subset of an association rule set such that it has the same prediction sequence by confidence priority as the association rule set. A topdown level-wise approach for the discovery of IRS on static database has been proposed in [9].

Based on the Fast Updating technique (FUP) [5] for the updating of discovered association rules, we present here an algorithm to maintain the discovered IRS, under incremental insertion. Numerical comparison with the non-incremental informative rule set approach is shown ta demonstrate that our proposed technique requires less computation time, in terms of number of database scanning, number of candidate rules generated and processing time, to maintain the discovered informative ruieset.

Keyword data mining, prediction, informative rule set, and incremental discovery  1 Introduction  The discovery of association rules in transaction databases is an important data-mining problem because of its wide application in many areas, such as market basket analysis, decision support, financial forecast, collaborative recommendation, and prediction.

Prediction is a process, for example, given a set of rules that describe the shopping behavior of the customers in a store over time, and some purchases made by a particular customer, we wish to predict what other purchases will be made by that customer. Many techniques have been proposed for prediction in the past.

In addition to the classical decision-tree induction approach, there are Bayesian classifications, neural  ~~  * 0-7803-79527/03/%17.00 8 2003 IEEE.

network, nearest neighbor classifiers, case-based reasoning, genetic algorithm, rough set, fuzzy set, and data mining approaches. For data mining approach, the association rule set is usually used for prediction.

However, traditional association rule algorithms typically generate a large number of rules, most of which are unnecessary when used for prediction.

Enhancements on simplifying the association rule set [ 11 directly and indirectly have been therefore studied extensively. Most indirect algorithms simplify the set by post-pruning and reorganization of association rules.

The direct algorithms attempts to reduce the number of association rules directly, for example, the constraint association rule sets [2][10], non-redundant rule sets [4][13], and informative rule sets [9]. In this work, we are palticularly interested in improving the efficiency of mining informative rule sets when the transaction database is updated, i.e., when a small transaction data set is added to the original database. This problem is referred to as the maintenance of discovered informative N k  Set.

One possible approach to the maintenance problem is to re-run the data-mining algorithm on the whole updated database. However, this approach has some obvious disadvantage. All the computation done initially at fmding out old large itemsets are wasted and have to be computed again fiom scratch, for the case of association rules. In the case of IRS, support counts of candidate itemsets also have to be recomputed from scratch for the updated database. Therefore, more efficient algorithms for computing the large itemsets in the updated database, utilizing the information from old large itemsets, are quite desirable.

The problem of maintenance of discovered association rule set in large database has been studied extensively in the past [3][6][8][1 I]. Maintenance of discovered association rule set is a process by which, given a transaction database and its association rule set, when the database receives insertion, deletion, or   mailto:slwang@nyit.edu   modification, we wish to maintain the discovered association rules as efficiently as possible. In this work, we propose an efficient approach to update the discovered informative rule set when the database is updated by insertion. The proposed approach is based.

on the Fast Wdating technique (FUP) [SI for updating the discovered association rules. Numerical comparisons with the non-incremental informative rule set approach [9] will be shown to demonstrate that our proposed technique require less computation time, in terms of number of database scanning and number of candidate rules generated, to maintain the discovered informative rule sets.

2 Problem Statement  Informally, the prediction using association rule set can be described as follows. For a given association rule set R and an itemset P, we say that the predictions for P from R is a sequence of items Q. The sequence of Q is generated by using the rules in R which is descending order of confidence. For each rule r that matches P (i.e. for each rule whose antecedent is a subset of P), each consequent of r is added to Q. After adding a consequence to Q, all rules whose consequences are in Q are removed form R.

As an example, consider a small database shown in Table 1. For minimum support 0.5 and minimum confidence 0.5, a set of 12 association rules can be found, as shown in Table 3. For an itemset P = {a. b),  the predicted sequence of items will be Q = {b, c, a ) .  It can be observed that not all association rules are used to produce the predicted sequence Q.

Table 1 A sample database D  Table 2 New data set A?  Table 3 Association Rule Set obtained ffom Table I (AR I Support I Confidence  1 la=>b (0.67 10.8 2 la=>c 10.67 10.8  3 Ib=>a 10.67 10.8 4 Ib=>c 10.67 (0.8 5 IF>?  In 67 In r( 1  Basically an informative rule set is the smallest subset of association rule set such that it can make the same prediction sequence according to confidence priority. The definition of informative rule set introduced in 191 is given as follows.

Definition 1 Let RA be an association rule set and 4 the set of singletarget rules in RA. A set RI is informative over RA if (1 )  R, c Rf4; (2) V r  E RI but does not exist r ?  E RI such that r ?  C r and con@ ?) 2 conf(r); and (3) V r ?  E Rf, - R,.

exist r E RI such that r ?  3 r and conf(r.7 5 conf(r).

A topdown level-wise searching algorithm using candidate tree is proposed by [9] for the efficient discovery of informative rule set. Consider again the database shown in Table 1. The informative rule set for the same minimum support and confidence will be the first 6 association rules in Table 1, i.e. 9 = {a=>b, a=>c, b=>a, b=>c, c=>a, c=>b), which can make the same prediction q u e n c e  as the whole association rule set.

The problem of maintenance of discovered informative rule set is that, given a transaction database and its informative rule set, when the database receives insertion, deletion, or modification, we wish to maintain the discovered informative rule set as efficiently as possible. For example, consider the database shown in Table 1 and its informative rule set p, when a new data set in Table 2 is inserted, the updated informative rule set will b e p  = {a=>b, a=% b=>a, b=>c, c=>a. c=>b.

d=>a, d=>b, a=>d, b=>d}.

The following notation will be used in this paper  D original database A +: new database D?: updated database, Du A + I: items in the database, (e.g. P, F?) l?t transaction in database, (e.g. Ti@, nV*?

X itemset, XL I     s: support c: confidence Xsup: support ofX(e.g. XSUP~D,X.SU~PA+,XSUPPD~) C,: set of k-th candidate itemsets (e.g. C,", C r ,  cy ) 4: set of k-th large itemsets (e.g. LF, L r  , L r  ) T: candidate tree (e.g. P, P', P+) &: k-th level of T R: informative rule set (e.g. P, R&+, R"3  3 The Incremental Insertion Algorithm  This section presents the proposed incremental insertion algorithm for the maintenance of discovered informative rule set. Based on the concept of informative rule miner [9] and FUP technique [SI, the proposed algorithm generates directly the updated informative rule set level-by-level and stores them in a candidate tree. A candidate tree is an extended set enumeration tree such as Figure 1, where the set of items is {a. b, c, d). Each node in the candidate tree stores two sets {A, 4. ~ A is an itemset, the identity set of the node, and 2 is a subset of the identity itemset, called potential target set where every item can he the consequence of an association rule. For example, a node { [abc), {ab] )  is a set of candidates of two rules, namely, bc=>a and ac=>b.

Utilizing FUF' technique to update the supports of 1 -itemsets,.the proposed algorithm updates the level one nodes of candidate tree p. It then generates the candidate nodes of the next level. While the candidate nodes are not empty, it proceeds to update the support counts of the candidate nodes using FUP technique. It then prunes the unnecessary candidate rules or nodes.

This process repeats itself until no candidate node is generated. The following is the top level of the proposed incremental insertion algorithm of informative rule set.

Input: DatabaseD  the informative rule set RD candidate tree P .

new data set A +  Output: the informative rule set fl' I .  P=P 2. Update supports of 1 -itemsets 3. Update level one of candidate tree 4. Generate candidate nodes of next level 5. While (candidate nodes are nut empty) 6.

7.

8.

9.

Update supports of candidate node Prune the candidate rules or nodes Include qualified rule sets t o p Generate candidate nodes of next level  Retum rule set in RD' The functions for generating candidate nodes in  steps 4 and 9 and pruning the candidate nodes in step 7 are introduced in 191. The update support functions in steps 2 and 6 are given as follows.

Function: Update supports of 1 -itemsets (1) Initialization: PL:, CF =$, LY =$ (2) For allXE I"',  IfXE W, then update support ofXon D* l/XsuppD known //scan dh Else {If XE Cy, then CF =Cp. U {A'), and calculate support ofXon A')  IfX.suppD+> s*(D+ A+), then (3) For all X? W  LY=Lp' U {A') (4) For allXE Cy  IfX.suppA'<s*A', then cF=cy - ix)  (5 )  For allX6 C y  //scan D, Xis  non-frequent itemset in D  IfX.suppD+>s*(D+A 7, then L?=L? U {A')  Function: Update supports of Canchdate nodes Initialization: WT;, T,D'=$ cp =Candidate-Rule-generator (TE;)- T; 11 Apriori generator function For all noden,? W  If ei contains non-frequent subset in TP,' then W=W- [a)  Calculate support of ni on D' suppDi known /I scan dh  If .  n i s u p p ~ + > s * ( ~ + d ) then T,D' = T;' U {nil  For all niF W // ni  For allniF C r If ~ nisuppAt+d  then C F = C ~  - [ n i l  I f .  nisuppD+>s*(DtA7  then T:'=T;' U { n i l  For allniF C p  //scan D  Example  This section gives an example to show the proposed algorithm can he used to find the informative rule set     mcrementally.

Input: DatabaseD in Table 1 the informative rule set 9 = {a=>b, a=>c, b=>a, b=>c, c=>a, c=>b} candidate tree P new data set A + in Table 2  output: the informative rule set P  1.

2.

3.

4.

5 .

6.

7.

8.

In the following, we summarize the candidate rule sets and the number of scanning of database for the non-incremental and incremental insertion algorithm for the discovely of informative rule set in Figure 2. The candidate rule sets for nomincrementa1 approach are 4,6, and 3 for level I ,  2, and 3, respectively. However, the incremental approach has only 0,2, 2, respectively. In addition, there are some saving of database scanning for the incremental approach as shown in Table 4.

P = P  5 Experiment Results Update supports of 1 -itemsets: ampD+ = 7 ,  b.sup D+  The candidate nodes in level two are nodes b. c, d , which are descendant of node a of level one. Othex nodes are shown in Figure I .

While (candidate nodes are not empty)  two, as shown in Figure I .

Update supports of candidate nodes of level  Node d of descendant ofc is pruned.

= (a=>b, a=%, b=>a, b=>c, c=>a,  c=>b, d=>a, d=%, a=> d, b=>d}.

Generate candidate nodes of level three, as  shown in Figure 1.

All programs are written in C++ and run on the same 600Hz Pentium 111 PC with 256 MB of memory running Windows XP operating system. We ran the algorithms on a transaction data set of 50,000 records, which is generated by the synthetic data generator of QUEST from IBM Almaden research center. In this dataset, there are 100 different items and the average length of itemsets is 7.8 with maximum itemset length 18 and minimum itemset length 2.

In the following, we present the processing times The process repeals the while loop until there is no of the algorithm the candidate node, as shown in Figure I .  non-incremental algorithm under various minimum  supports and incremental data sets.

Level 1 Level 2 Level 3  . .

. . .  ... . . . .  :.

... . ,~  . .

Scanning of Database IIRS  C IRS IIRS IRS  4 0 @4)& A + (4) A + (4) 6 2 @6)& A + (6) A?(6) 3 2 o( l )&A?(l)  A?(1)  association rule set such that same prediction sequence by confidence priority can be achieved. In this work, we have presented an efficient data-mining algorithm for the maintenance of discovered informative rule set under incremental insertion. Numerical comparisons with the non-incremental informative rule set rpproach has bee shown that the proposed technique require less computation time, in terms of number of database scanning and number of candidate rules generated.

However, more numerical simulation needs to be canied out for justification. In addition, incremental deletion  ., musup and modification of discovered informative rule set may  FIGURE 2 Running time comparison of incremental and non-incremental approaches References  I I  .. FIGURE 3 Running time comparison of incremental and non-incremental approaches  Figures 2 and 3 show the running times for both approaches under various minimum supports. The size of original database D is 40,000 records. The minimum confidence is set at 20%. The size of inserted data set is 2,000 records and 10,000 records respectively. We can observe that for small minimum supports, the incremental approach performs better than the non-incremental approach. For minimum supports greater than 20%, the two approaches perform similarly.

This is due-to the fact that the number of candidate itemsets and rules is small and about the same, when the minimum support increases.

6 Conclusion  Informative rule set is the smallest subset of  [l] R Agrawal, T. Imielinksi and A. Swami, ?Mining Association Rules between Sets of Items in Large Database?, Proc. of the ACM SIGMOD Conference on Management of Data, Washington DC, May 1993, 207-2 16.

[2] R Agrawal, R Snkant, ?Fast Algorithms for Mining Association Rules?, Proc. of the 20th Int? I Conference on Very Large Databases, Santiago, Chile, September 1994, 487499.

[3] Necip Fazil Ayan, ?An Efficient Algorithm to Update Large Itemsets with early Pruning?, In Proc. 5th ACM Discovery and Data Mining, San Diego, CAUSA, August 1999.

[4] Yves Bastide, Nicolas Pasquier, Rafik Taouil, Gerd Stunme, Lot Lakhal, ?Mining Minimal Non-Redundant Association Rules Using Frequent Closed Itemsets?, Proc. DOOD?2000 conference, pp 972-986, LNCS, Springer-verlag, july 2000.

[5] David W. Cheung, Jiawei Han, Vincent T. Ng, C.Y.

Wong, ?Maintenance of Discovered Association Rules in Large Databases: An Incremental Updating Technique?, Engineering, New Orleans, Louisiana, 1996, pp 106-1 14.

[6] David W. Cheung, Vincent T. Ng, Benjamin W. Tam ?Maintenance of Discovered Knowledge: A Case in Multi-level Association Rules?, In Proceedings of 2nd     Data Mining, 1996, pp307-310  [7] David W. Cheung, S.D. Lee, Benjamin Kao, ?A General Incremental Technique for Maintaining Discovered Association Rules?, In Proceedings of Intemational Conference on Database Systems for Advanced Applications, 1-4 April 1997, pp 185-194.

[8] Guanling Lee, K.L. Lee, Arbee L.P. Chen, ?Efficient Graph-Based Algorithms for Discovering and Maintaining Association Rules in Large Databases?, Knowledge and Information Systems 3 (2001), 338-355.

[9] Jiuyong Li, Hong Shen, Rodney Topor, ?Mining the Smallest Association Rule Set for Predictions?, Proceedings of the 2001 IEEE Intemational Conference on Data Mining.

[lo] Chunhua Wang, Houkuan Huang, Honglian Li, ?A Fast Distributed Mining Algorithm for Association Rules With Item Constraints?, 2000 IEEE Intemational Conference on System, Man & Cybernetics, Vol 1-5, 1900-1905,2000.

[1 I]  G.. 1. Webb, ?Efficient Search for Association Rules?, Proceedings of the 6th ACM SIGKDD Data Mining (KDD-00) page 99-107, N.Y., Aug 20-23 2000.

[12] ShowJane Yen, Arbee L.P. Chen, ?An EEcient Approach to Discovering Knowledge from Large Databases?, In Proceedings of the IEEE/ACM Intemational Conference on Parallel and Distributed Information Syste, 1996, pp8-18.

[13] M.J. Zaki, ?Generating NorrRedundant Association on Knowledge Discovery and Data Mining, 2000.

[14] Mohammed J. Zaki, Ching-Jui Hsiao, ?An Efficient Algorithm for Closed Association Rule Mining?, 6th ACM SIGKDD Intemational Conference on Knowledge Discovery and Data Mining, pp 34-43, Boston, MA, August 2000.

