A Cloud-based Framework for Supporting Effective and Efficient OLAP in Big Data Environments

Abstract?Inspired by the emerging Big Data challenge, in this paper we provide the description of OLAP*, a Cloud- based framework for supporting effective and efficient OLAP in Big Data environments. OLAP* combines data warehouse partitioning techniques with Cloud Computing paradigms, and provides a suitable implementation on top of the well-known ROLAP server Mondrian where the main task consists in applying meaningful transformation of multidimensional database schemas.

We complement our analytical contribution by mean of a case study showing the effectiveness of our framework in a practical setting.



I. INTRODUCTION On-line Analytical Processing (OLAP) [1] lies heavily upon  a multidimensional data model kept within multidimensional databases (MDB) [2]. Compared to relational databases, MDB increase performance by storing aggregated data and enhance data presentation. An MDB schema contains a logical model, consisting of OLAP data cubes, where each data cube is a set of data organized and summarized into a multidimensional structure defined by facts, measures and dimensions. Building the data cube can be a massive computational task. Indeed, data warehouses, the reference data platforms storing OLAP data cubes, tend to be extremely large. Also, the workload is composed of business queries that tend to be complex and ad-hoc, and do often require computationally expen- sive operations such as star joins, grouping and aggregation.

Also, hurdles commonly known as the four V-dimensions, respectively for Volume, Velocity, Variety, and Variability are proper characteristics of Big Data [3]?[5], which, in turn, are meaningful data sets on top of which performing OLAP in the context modern data-intensive Cloud infrastructures [3], [6]? [8]. These experiences are directly correlated to some well- known research efforts falling in the area of OLAP analysis over data streams (e.g., [9]) and stream OLAP over high- performance data structures like Grids (e.g., [10], [11]) and BigTable (e.g., [12], [13]), which embed advanced OLAP data cube compression paradigms (e.g., [14], [15]). Following this major trend, in our research we focus the attention on strategies for improving query processing in data warehouses over Big Data, with the goal of an effective and efficient OLAP support in such environments, by exploring the applicability of parallel processing techniques, hence perfectly suitable to Clouds.

Dealing with huge data sets, most OLAP systems are I/O-bound and CPU-bound. First, this is due to hard drives I/O performances, which do not evolve as fast as storage, computing hardware (Moore Law) and network hardware (Gilder Law). Second, OLAP systems require high computing capacities. Since the 80?s with RAID systems, both practi- tioners and experts admitted that the more we divide disk I/O across disk drives, the more storage systems outperform.

In order to achieve high performance and large capacity, database systems and distributed file systems rely upon data partitioning, parallel processing and parallel I/Os. Besides high capacity and complex query performance requirements, these applications require scalability of both data and workload. It is well known that the Shared-Nothing architecture [16], which features independent processors interconnected via high-speed networks, is most suited for requirements of scalability of both data and workload. Other architectures do not scale well, due to contention for the shared memory. Following these considerations, in this paper, we investigate solutions relying on data partitioning schemes for parallel building of OLAP data cubes, and we describe the framework OLAP*, suitable to novel Big Data environments [3]?[5], along with the as- sociated benchmark TPC-H*d, an appropriate transformation of the well-known data warehouse benchmark TPC-H [17] which we used to stress OLAP*. Finally, we demonstrate the effectiveness of the proposed framework through a running example focusing on an architecture that builds along the principles of our framework and that it is developed on top of the well-known ROLAP server Mondrian [18].



II. RELATED WORK  Data partitioning mainly aims at minimizing the cost of execution time of the OLAP workload through enabling intra- query parallelism and inter-query parallelism. We recall that inter-query parallelism consists in simultaneously processing different queries in distinct nodes, and intra-query parallelism is obtained when multiple nodes process the same query at the same time. Also, data partitioning aims at minimizing the cost of maintenance of the data warehouse through targeted and parallel refresh operations, and the cost of ownership of a data warehouse through the use of commodity hardware  2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing  DOI 10.1109/CCGrid.2014.129     with a shared-nothing architecture rather than expensive server architectures. Several data partitioning schemes there exist in literature. Hereafter, we survey the five main kinds of such schemes.

First kind of partitioning scheme is based on a fully repli- cated Data Warehouse, where the data warehouse is replicated on a database cluster. Load balancing of the workload among nodes enables inter-query parallelism, and consequently de- creases the delay required to process a query. This schema does not reduce the time to execute the query. Second kind of partitioning scheme, performs primary horizontal partitioning of the fact table and replicates all dimensions [19]. Third kind of partitioning scheme proposes Derived Horizontal Partition- ing (DHP) of the fact table along selected dimension tables.

Consequently, a new issue emerges which is the choice of the dimension tables. The problem is denoted as the referential horizontal partitioning problem, and proved NP-hard [20].

Fourth kind of partitioning scheme is based on range and list-partitioning of the fact table along attributes belonging to hierarchy levels of the dimension tables. Stohr et al. [21] pro- pose MultiDimensional Hierarchical Fragmentation (MDHF) for partitioning the fact table. MDHF allows choosing multiple fragmentation attributes from different hierarchy levels of the dimension tables, with each fragmentation attribute refers to a different dimension.

On the other hand, these exist several state-of-the-art mid- dleware for distributed OLAP workload processing, which is also related to our research. Here, we briefly recall some well- known approaches:  ? PowerDB Ro?hm et al. [19] evaluate TPC-R benchmark in a database cluster. They compare the execution of OLAP queries in a fully replicated database system to a hybrid one combining partial replication with partitioning. Their partitioning scheme consists in partitioning the biggest table LineItem and in replicating all other tables. The system architecture is 3-tier, clients, DBMSs and a co- ordinator. The coordinator is responsible for processing TPC-R SQL workload.

? cgmOLAP Chen et al. [22] claim that cgmOLAP is the first fully functional parallel OLAP system able to build data cubes at a rate of more than 1 Terabyte per hour. The cgmOLAP system consists of an application interface, a parallel query engine, a parallel cube materialization engine, metadata and cost model repositories, and shared server components that provide uniform management of I/O, memory, communications, and disk resources.

? ParGRES Paes et al. [23] implements ParGRES mid- dleware. The main features of ParGRES are: automatic parsing of SQL queries to allow for intra-query parallel execution; query processing with inter- and intra-query parallelism; virtual dynamic partition definition; result composition; update processing; and dynamic load bal- ancing.

? SmaQSS DBC middleware Lima et al. [24] propose distributed database design alternatives which combine physical/virtual partitioning with partial replication. They adopt partial replication of data partitions using Chained Declustering. Finally, they assessed their partitioning  schemes with the implementation of a prototype sys- tem SmaQSS DBC middleware (Smashing Queries while Shrinking disk Space on a DataBase Cluster) and us- ing data of TPC-H benchmark, as well as a subset of the workload of TPC-H benchmark, namely Q1, Q4 ? Q6, Q12, Q14, Q18, Q21.



III. SUPPORTING OPTIMAL BUILDING OF OLAP DATA CUBES VIA DATA WAREHOUSE FRAGMENTATION  In this Section, we provide our theoretical results in the context of data warehouse fragmentation for supporting opti- mal building of OLAP data cubes. These principles are the basis of the proposed OLAP* framework. We consider a data warehouse schema, with one fact table F and n dimension tables (D1, D2, . . . , Dn), such that (i) each fact joins to one and only one dimension member, and a single dimension member can be associated with multiple facts and (ii) each fact references the n dimensions. We define SF as the scale factor of the data warehouse. We assume that the size of F is ? ? SF Bytes and the size of each dimension Di is either ?i ? SF Bytes (i.e., SF dependent) or ?i Bytes (i.e., SF independent). It is important to weigh the space/time trade-offs. These trade-offs are dependent on many parameters some of which are (1) number of dimensions, (2) sizes of dimensions, (3) cardinalities of dimensions? members and (4) degree of sparsity of the data cubes. Hereafter, we compare the storage and multidimensional analysis of four proposed fragmentation schemas, namely, Schema 1, Schema 2, Schema 3 and Schema 4. These schemas are characterized as follows:  ? Schema 1: the data warehouse is not fragmented and not replicated.

? Schema 2: the data warehouse is fully replicated over N nodes.

? Schema 3: the fact table F is fragmented along Primary Horizontal Partitioning (PHP) and all dimensions are fully replicated over N nodes.

? Schema 4: every dimension Di which size is SF inde- pendent is fully replicated; a selected dimension table Dk is fragmented along PHP partitioning, all tables in hierarchical relationship with Dk, including the fact table are fragmented using derived horizontal partitioning along Dk. Finally, other tables are replicated.

Table I summarizes the results of storage overhead analysis, for the four previous different schemas. Notice that Schema 3 overhead in storage is less than the overhead for Schema 4.

In general, every candidate schema maximizing the number of partitioned dimension tables will produce a smaller storage overhead.

Given the storage analysis above, Table II summarizes the results of our multidimensional analysis, for the four different schemas. Hereafter, ?? ? SF denotes the number of facts within F . Similarly, ??i denotes the number of rows within dimension Di and ??j denotes the number of rows within dimension Dj (being SF independent). The cube density is measured as the ratio of the actual cube data points to the product of the cardinalities of the dimension hierarchies. In the following, we consider an OLAP data cube built with one     TABLE I DATA WAREHOUSE VOLUME FOR THE FOUR FRAGMENTATION SCHEMAS.

Volume Schema 1 ?? SF +??i ? SF +  ? ?j ? SF  Schema 2 N ? (?? SF +??i ? SF + ?  ?j ? SF ) Schema 3 ?? SF +N ? (??i ? SF +  ? ?j ? SF )  Schema 4 ?? SF +N ? (??i ? SF +  ? ?j ? SF )?  (N ? 1)? (??k ? SF )  TABLE II CUBE CHARACTERISTICS AT EACH NODE.

Cube Size Cube Density  Schema 1 ?  ??i ? SF ?  ??j  ??SF? ??i?SF  ? ??jSchema 2  Schema 3 ??SF  N????i?SF ?  ??j  Schema 4  ? ??i?SF  ? ??j  N ??SF?  ??i?SF ?  ??j  measure and from all dimension tables, such that it includes each dimension primary key table as a member and fragmented dimension tables are evenly distributed among all nodes. Given an OLAP data cube, the cross product of dimensional members originates intersections for measure data. Nevertheless, in reality most of the intersections will not have data. This leads to density (inversely to sparsity). We assume that, for the OLAP data cube we consider (along with its dimensions), the number of non empty intersections depends on SF , and is equal to ??SF . Notice that, Schema 3 produces OLAP data cubes more sparse than Schema 1, while Schema 4 produces OLAP data cubes N times smaller than Schema 1 and Schema 3, and might have same density than Schema 1.



IV. PARALLEL OLAP OVER BIG DATA: THE OLAP* APPROACH  Following the results deriving from the analysis provided in Section III, in order to accomplish different requirements posed by these analysis, we designed and implemented a middleware for parallel processing of OLAP queries, OLAP*.

Figure 1 depicts the logical architecture of OLAP*. Sum- marizing, this architecture is devoted to query routing and cube post-processing over Big Data, and encompasses some emerging components like Mondrian (ROLAP server) and MySQL (Relational DBMS). It can be easily interfaced to other classical OLAP clients at the front-end side, such as JPivot, thanks to suitable APIs.

In order to assess the effectiveness of our framework OLAP* in comparison with the ones of other proposals, we designed an innovative benchmark specially targeted to multidimen- sional data, called TPC-H*d benchmark, which is inspired to the well-known TPC-H benchmark [17], the most prominent decision support benchmark. TPC-H benchmark consists of a suite of business oriented complex ad-hoc queries and concur- rent data modifications. The workload and the data populating the TPC-H database have been chosen as to have broad industry-wide relevance. The workload is composed by 22  Fig. 1. OLAP* Middleware Logical Architecture.

TABLE III TPC-H*D FRAGMENTATION SCHEMAS.

Relation Schema Customer PHPed along c custkey Orders DHPed along o custkey LineItem DHPed along l orderkey PartSupp, Supplier, Part, ReplicatedRegion, Nation, Time  SQL queries with a high degree of complexity. Existing TPC- H implementation allows the generation of raw data stored into eight TBL files, namely Region, Nation, Customer, Supplier, Part, PartSupp, Orders, LineItem, by using a specific scale factor SF ranging over the interval {1, 10, . . . , 100, 000}.

The latter determines the final TPC-H data size as over the interval {1GB, 10GB, . . . , 100TB}. Basically, TPC-H*d is a suitable transformation of the TPC-H benchmark into a multi-dimensional OLAP benchmark. Indeed, each business query of the TPC-H workload is mapped onto an OLAP cube, and a temporal dimension (Time table) is added to the data warehouse. Also, we translate the TPC-H SQL workload into a corresponding (TPC-H*d) MDX workload.

This approach involves into specific fragmentation schemas over the respective TPC-H tables. Table 3 shows the fragmen- tation schemas of TPC-H*d relational data warehouse, which both uses PHP and DHP fragmentation approaches, which have been discussed in Section II.

With respect to the partitioning scheme shown in Table III, typical business queries of TPC-H can be in turn partitioned into three different types of queries, and three corresponding different executions to be handled. These query classes are the following:  ? Class 1: as result of replication, business queries which involve only replicated tables are executed by any node.

? Class 2: business queries which involve only the parti- tioned tables are executed on one database node as result of partitioning.

? Class 3: business queries which involve both partitioned and replicated tables are executed on all database nodes.

In this class, we distinguish two types of cubes? post- processing, namely:  ? Sub-Class 3.1: cubes built at edge servers have com- pletely different dimension members, consequently the result cube is obtained by operating the UNION ALL of cubes built at edge servers;  ? Sub-Class 3.2: cubes built at edge servers present     shared dimension members, consequently the result cube requires operating specific aggregate functions over measures, respectively sum over sum measures, sum over count measures, max over max measures, and so forth.

V. OLAP* IN ACTION!

We demonstrate the effectiveness of our framework OLAP* through a running example of an architecture building along the principles of this framework, developed on top of the ROLAP server Mondrian [18].

OLAP* incorporates the process of generating the TPC-H*d data set from the original TPC-H benchmark, by means of deriving the initial MDB schema from the TPC-H workload directly, based on the approach [25]. In this case, a special component for recommending the most suitable MDB schema optimization, called AutoMDB [26], is exploited. Figure 2 shows the OLAP cube C8 derived from the SQL statement associated to the TPC-H query Q8,  Also, OLAP* incorporates two implementations of TPC- H*d. The first one is a Java client, which sends a stream of MDX statements to a RDBMS using the appropriate JDBC driver and OLAP4j [27]. The second one is a Web-based implementation of TPC-H*d which makes use of JPivot as OLAP client, Apache Tomcat as JSP container, Mondrian as ROLAP server and MySQL as relational DBMS. At the applicative side, the framework supports all classical opera- tions provided by a standard OLAP interface, such as slice, dice, pivot, and so forth. For instance, Figure 3 illustrates pivot tables obtained for the TPC-H*d-derived OLAP cube C8 shown in Figure 2.

Fig. 2. Turning Business Query Q8 of TPC-H benchmark into a TPC-H*d OLAP Cube.

State-of-the-art Business Intelligence solutions describe MDB schemas via XML. In line with this major trend, we implemented an XML parser (using SAX library) for loading the description of the cubes, namely measure data, fact data (i.e., physical tables or logical views), dimension data (i.e.,  Fig. 3. Pivot Table over the TPC-H*d OLAP Cube C8 of Figure 2, and the Corresponding MDX Statements.

hierarchies, levels and properties). Also, we implemented algorithms for cube merge and virtual cubes definition. Some of the major features of our architecture allows us to:  ? load tpch_multidim.xml ? the initial MDB schema of TPC-H*d benchmark;  ? perform comparison of OLAP cube characteristics ? for each pair of OLAP cubes, we show whether they have same fact table or not and compute the number of shared, different or coalesced dimensions;  ? run merge of OLAP cubes via using different similarity functions ? for instance, Figure 4 shows a simple simi- larity function implemented by AutoMDB, which groups OLAP cubes having the same fact table.

Fig. 4. Clustering OLAP cubes sharing same fact table in AutoMDB.



VI. CONCLUSIONS AND FUTURE WORK  In this paper, we have described OLAP*, a novel framework for efficiently supporting parallel OLAP over Big Data in the context of in modern data-intensive infrastructures like Clouds, along with the associated benchmark TPC-H*d, a suitable transformation of the well-known data warehouse benchmark TPC-H. Experiments have clearly demonstrated the effectiveness of OLAP*. Future work is mainly oriented towards devising a novel version of OLAP* capable of ef- fectively supporting analytics over Big Data (e.g., [28]) that are rapidly arising as an emerging and challenging research context for next-generation database research.

