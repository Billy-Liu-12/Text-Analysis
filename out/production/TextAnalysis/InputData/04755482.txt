Literature Clustering using Citation Semantics

Abstract Clustering is a common and powerful technique for  statistical data analysis, document categorization and topic discovery. The majority of traditional clustering methods, especially for document clustering, are based on the vector space model for distance measure, where the vector is the word profile of a document in the context of the entire corpus. However, algorithms using this measure achieve limited accuracy. In this paper, we propose a semantic measure which incorporates citation semantics (Citonomy) into literature (document) clustering. Our experimental results show that the performance of clustering can be substantially improved by combining Citonomy and vector space measures.

1. Introduction   It is important to find accurate methods for document clustering for several reasons. Such methods can aid in increasing both precision and recall in retrieving documents in response to a query. They can also aid in browsing applications where one would like to find documents similar or related to a document of interest. Further, analysis of each cluster may help identify distinct topics, some of which may indicate emerging areas. Finally, they can help one to keep up with the continued increase in the number of publications. A detailed summary of clustering algorithms can be found in [1]. Document clustering algorithms can generally be classified as hierarchical or partitional. Hierarchical algorithms find successive clusters iteratively using previously established clusters, whereas partitional algorithms determine all clusters at once. Hierarchical algorithms can be agglomerative ("bottom-up") or divisive ("top-down").

Agglomerative algorithms begin with each element as a separate cluster and merge them into successively larger clusters. Divisive algorithms begin with the whole set and proceed to divide it into successively smaller clusters. The K-means clustering algorithm is a well known partitional algorithm. It partitions a set of  data objects into k clusters such that the inter-cluster similarity is low but the intra-cluster similarity high.

Clustering is typically dependent on the use of a pair-wise distance measure between the individual documents to be clustered. The Vector space model (VSM) [2] is commonly used for the distance measure in document clustering. Each document is represented by a vector of frequencies of terms (words) after removing stop words and word stemming (reducing a word to its canonical form). In practice, the term frequency is usually the weighted frequency, e.g., TF- IDF (term frequency ? inverse document frequency).

The idea of combining IDF with TF is that if a term is highly frequent across different documents, then it would have little discriminating power, and vice versa [3]. To compute the similarity between two documents, the corresponding vector representations are used to calculate measures like the inner product, dice coefficient and cosine coefficient  Two major problems with VSMs are low accuracy and speed. First, since each vector is the list of frequencies of words of a document, the semantics may be misinterpreted. For example, suppose document A states ?A man is walking a dog,? document B states ?Man and dog are both animals,? and document C states ?A girl is riding a bike.? Documents A and C should arguably be in the same cluster since both describe people?s outdoor activity or exercise, while B may be a document on biological classification.

However, based on the vector space model, A and B would end up in the same cluster since they would have higher similarity with each other than with C.

Even though word stemming may be applied to infer semantic equivalence of lexical variants, terms (or words) representing synonyms in a particular domain may still be identified as different terms (or words); this leads to low similarity of documents on the same topic that use different terms (words) to refer to the same concepts, thus resulting in low accuracy ? low precision and/or low recall. Second, with large documents, the vector space model suffers from high dimensionality which makes the computation of        weighted frequencies and similarity of documents quite costly, thus leading to long running times.

To solve these problems, clustering methods have been proposed that can be categorized into two-stage clustering (e.g. [4]), term-frequency based (e.g. [5]) and domain knowledge (or ontology) based (e.g. [6, 7]). Term frequency based methods tend to overlook small clusters and ontology based approaches can still suffer from the problem of high-dimensionality, depending on the domain.

