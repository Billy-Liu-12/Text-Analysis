Hiding Association Rules based on Relative-non-sensitive Frequent Itemsets

Abstract-Association rules hiding algorithms often sanitize transactional databases for protecting sensitive information. Data modification is one of the most important sanitation approaches. However, the exist modification methods either focus on hiding sensitive rules only, or take measures to reduce the impact on non-sensitive rules from the whole database while hiding sensitive rules. In this paper, we propose a new algorithm which hides sensitive rules from the side of non-sensitive rules. It classifies the sensitive transactions by their degree of conflict. For the special group of transactions, a victim-item must satisfy: 1, in the sensitive rules; 2, not in the non-sensitive rules. Our algorithm selects different victim-items in different transactions that contain the same rule, which makes sure that removing the victim-items in the special group of transactions has no influence to non-sensitive rules. The experimental results show that our algorithm for sanitizing transactional database can achieve better results compared with others algorithms such as Naive, MinFIA, MaxFIA and IGA. In particular, our algorithm has the least impact on non-sensitive rules.

Index Terms-Privacy preserving data mining, Association rules, Frequent itemsets

I. INTRODUCTION  W ITH the recent advances in data mining and more andmore attention paid to privacy, privacy preserving data mining (PPDM) has become a novel research direction in data mining. The main considerations in PPDM are two folds [1]. First, sensitive raw data, such as names, addresses and identifiers, should be modified or deleted from the original database, so that the recipient ofthe data can not compromise other person's privacy; Second, the sensitive knowledge which can be mined from a database by using data mining algorithms should also be excluded, because such knowledge can equally well compromise data privacy. The main  Prlc.lib IEEE lIt eill. II eillidlellllr.adcs (leel'09)

I.Bacil, Y. Wall, Y.Y. Yal, W.lilsler, I.ehal & LA Zadeh (Eds.) 911-1-4244-4642-1/09/825.00 ?20091EEE   objective in the privacy preserving data mining is to develop the algorithms for modifying the original data in some way, so that the private data and private knowledge remain private even after mining process. Because the knowledge in database is not manifest, how to protect this information is a challenge to PPDM. There are many approaches based on the different dimensions which have been adopted for PPDM.

Privacy preservation techniques are the most important, which refer to the techniques used for the selective modifications of the data. Selective modifications are required in order to achieve higher utility for the modified data given that the privacy is not jeopardized. There are three main techniques as following: heuristic-based techniques like adaptive modifications that modifies only selected values that minimize the utility loss rather than all available values; cryptography-based techniques like secure multiparty computation where a computation is secure ifno party knows anything except its own input and the results at the end or computation; reconstruction-based techniques where the original distribution of the data is reconstructed from the randomized data.

In this paper, we address the problem of association rules hiding by decreasing the support of the patterns that product the association rules. To transform a transactional database into a new one that conceals some strategic patterns while preserving the general patterns, we present a new algorithm based on data modification. Our algorithm selects different items to delete according to different transactions; we delete the items which are in sensitive patterns, but not in the non-sensitive patterns. The relative-non-sensitive patterns are the patterns that include the items in sensitive patterns; our method to delete items can minimize the side effects on the general patterns when sensitive patterns are hided. The main contribution in this paper is we present a new method to select victim-items.

The rest ofthis paper is organized as follows: In Section 2, we present an overview of the association rule hiding algorithms in existence. In Section 3, we introduce our algorithm. We present the experimental results and    discussion in Section 4. Concluding remarks and future work are listed in Section 5.

Table II Large itemsets Obtained from D  Itemset Support  An association rule is an implication of the form X ~ Y, where X c I, Y c I,andXnY=0. We say that the rule Yeo Yholds in the database D with confidence c if IXUYI/IXI ~ c (where IAI is the number ofoccurrences ofthe set of item A in the set oftransaction D), and A occurs in a transaction t, ifand only if IXUYI/INI ~ s , where INI is the number of transactions in D. While the support is a measure of the frequency ofa rule or itemsets, the confidence is a measure of the strength of the relation between sets of items. All n-itemsets (n2:2) derived from database D are shown in Table II.



II. RELATED WORK  Association patterns give a description of the relation between the different products (items). Based on these patterns, we can get the customers' buying patterns, such as when customers buy bread, they also buy milk. Market Managers can use these rules to adjust store layouts (placing items optimally with respect to each other) for cross-selling, promotions and catalog design, or to identify customer segments based on buying patterns. It is obvious that the rules in transaction database have great impact on saleroom, so the managers would be interested to mine theses patterns, but others who do data mining from the transactional database will pose a threat to the data owners ifthese rules are not used properly. Because of the above reasons, association rule hiding became an important area in PPDM.

A. The basic concepts about association rules  Let 1= {i], i2 ??? in} be a set of literals, called items. Let D be a set of transactions which is database that is going to be disclosed. Each transaction te D is an itemset such that t ~ 1.

A unique identifier, which we call TID, is associated with each transaction. We say that a transaction t supports X, a set of items in I, if X ~ t. A sample database of transaction is shown in Table I. Each row in the table represents a transaction. There are four items and six transactions. An itemsetXhas support s ifs percent ofthe transactions support X. Support ofX is denoted as Supp(X).

TID Tl T2 T3 T4 T5 T6  Table I Sample Database D  Items ABCD ABC ABD ACD ABC BD   AB 66% AC 66% AD 50% BC ~% BD 50%  ABC 50%  Itemsets AD has 50 percent Support since it appears in three out of six transactions in D. Rules AB ~ C has 75 percent confidence since four transactions contain AB and three transactions contained ABC.

B. The basic concepts about association rules hiding  Let D be a transaction database, T be a set of all transactions in a transactional database D, s be the minimum support threshold, and R be a set of all association rules that can be mined from D based on a minimum support s, and Rh be a set of decision support rules that need to be hidden according to some security policies.

Sensitive Rules [2]: A set of association rules, denoted by R; is said to be restrictive if R, c R and if and only if Rr would derive the set Ri;  Non-Sensitive Rules [2]: r--R; is the set of non- sensitive rules such that r--R, U R, = R.

Sensitive Transactions [2]: A set of transactions is said to be sensitive, as denoted by Sf' IfSf c T and if and only if all restrictive association rules can be mined from Sf and only transactions in Sf contain items involved in the restrictive association rules.

Victim-item [2]: for each sensitive rule, identify a candidate item that should be eliminated from the sensitive transaction. This candidate item is victim item.

Degree of Conflict of a Sensitive Transaction [2]: the degree ofa sensitive transaction t, denoted by degree (t), such that te Sf is defmed as the number of restrictive association rules that have items contained in t.

Disclosure Threshold [2]: this threshold expresses how relaxed the privacy preserving mechanisms should be, as denoted by 'P. When 'P = 0%, no restrictive rules are allowed to be discovered. When 'P = 100%, there are no restrictions on the restrictive rules.

Lemma [3]: ifan itemset is frequent, then all its subsets are also frequent.

C. Association rules hiding  The most frequently studied problem in association rule mining is the process of discovering frequent patterns and, consequently, association rules. One simple and effective way to hide some sensitive rules is to decrease their support in a given database. The confidence plays an important role in processing of generating rules from patterns, so there is    another way to hide some sensitive rules-decreasing the rules' confidence.

We can hide sensitive rules by decreasing the support or confidence of the sensitive rules until their support or confidence is below the minimum support threshold or the minimum confidence threshold. This procedure of altering the transaction is called a sanitization process [4]. It is important that a data modification technique should be in sanitization process. To do so, a small number oftransactions have to be modified by deleting one or more items from them or even changing items in transaction. Methods of modification include: Perturbation [4, 5], which is accomplished by the alteration of an attribute value by a new value, blocking [6], which is the replacement of an existing attribute value with a"?", aggregation or merging which is the combination of several values into a coarser category, swapping that refers to interchanging values of individual records, and sampling, which refers to releasing data for only a sample of a population.

D. Association rules hiding algorithms  Most association rules hiding strategies depend on fmding transactions that fully or partially support the generating itemsets of a rule. Given a rule X ~ Y on a database D, the hiding strategies were indicated by Vassilios S. Verykios[5] can be summarized as follows: l,decrease the confidence of the rule by increasing the support of the rule antecedent X through transactions that partially support the rule and by decreasing the support of the rule consequent Y in transactions that support both X and Y. 2,Decrease the support of the rule by decreasing the support of either the rule antecedent X, or the rule consequent Y, through transactions that fully support the rule. They present five algorithms for the strategies, algorithm l.a increases the support ofthe rule's antecedent until the rule confidence decreases below the minimum support threshold; algorithms l.b hides sensitive rules by decreasing the frequency of the consequent until either the confidence or the support of the rule is below the threshold; algorithms 2.a decreases the support of the sensitive rules until either their confidence is below the minimum support threshold or their support is below the minimum threshold, and algorithm 2.b and 2.c hide rules by decreasing the support of their generating itemsets until the support is below the minimum support threshold.

Adding noise to the data will generate artifacts such as illegal association rules that would not exist had the sanitizing not happened. So Stanley introduces the approaches that solely remove items, among the approaches that remove information only, he distinguishes the pattern restriction-based approaches that remove complete restrictive patterns from sensitive transactions and the item restriction-based approaches that selectively remove some items from sensitive transactions. Sanitizing a transactional   database consists of identifying the sensitive transactions and adjusting them. To speed up this process, Stanley introduces a very efficient strategy for indexing text database-inverted file [2, 7]. Table III shows an example of an inverted file corresponding to the sample transactional database shown in the Table I.

Sanitizing algorithms for transactional database can be classified into two classes [8]: the algorithms that solely remove information from transactional database and those modify existing information. The first algorithms only reduce the support ofsome items, such as algorithms l.b, 2.a, 2.b, 2.c and Naive, MinFIA, MaxFIA, IGA, while the second may increase the support of some items, such as algorithms l.a.

Table III Inverted file of Sample Database D  Item Freq TIDS A 5 Tl,T2,T3,T4,T5 B 5 Tl,T2,T3,T5,T6 C 4 Tl,T2,T4,T5 D 4 Tl,T3,T4,T5  Many approaches presented in the literature focus on the hiding of restrictive rules but do not study the effect of their sanitization on accidentally concealing legitimate rules or even generating artifact rules. Stanley R.M. oliveira take into account the impact of sanitization both on hiding rules that should be hidden and on hiding rules that should not be hidden. However, his algorithms can not fmd out which non-sensitive rules will be affected, and which will not be affected, and just attempted to minimize the impact on the sanitized transactions from the whole set of patterns. In this paper, we will fmd the non-sensitive patterns be hidden accidentally.



III. A NEW ASSOCIATION RULES HIDING ALGORITHM: HarRFI  Our algorithm called HarRFI means hiding association rules based on relative-non-sensitive frequent itemsets.

HarRFI algorithm hides sensitive rules by decreasing the support of the patterns that generate rules. In the sanitizing algorithm, we only remove the items from the transactional database. Most important of all, our algorithm can select victim items which can not only hide sensitive rules, but also minimize the side effects on non-sensitive rules. We can hide a pattern n (n>2) items in length by hiding its children pattern 2 items in length based on the lemma.

A. Problem formulation and hiding strategy  In literature [2], the original restrictive patterns, denoted as src-sen, are a set of 10 restrictive patterns from the dataset ranging from two to five items in length. The author makes any patterns that contain restrictive patterns to be restricted, denoted as par-src. Thus, in their experiments the object to be    Itemset Freq TIDS  Table VI Inverted file of relative-non-sensitive frequent itemset  AB 4 TI,T2,T3,T5 AD 3 TI,T3,T4 BC 3 TI,T2,T5  We can fmd TIDS that items are located quickly through the inverted structure like Table III. For example, Table IV is the inverted structure of itemsets AC, it is the intersection TIDS of row A and row C in Table III.

TIDS TI,T2,T5  T4  Freq  Table VII Inverted file of Victim items  C C  Victim  Table V is the inverted structure of all the non-sensitive patterns, Table VI shows the inverted structure of the non-sensitive which includes the items in sensitive patterns, these non-sensitive patterns is the patterns which will be hidden by accidentally.

In our work, fmding the structure like Table VI is the basis work to minimize the impact on non-sensitive patterns. The column Victim in Table VII is the item to be removed in column transactions which TID is in TIDS.

It is clear that removing the item C in Tl, T2, T4 and T5 not only can hide the sensitive pattern AC but also can not take impact on non-sensitive patterns AB and AD. In this paper, we select the item that is included the sensitive patterns but is not include the non-sensitive patterns as victim item.

This strategy is novelty in our approach ofsanitizing database and can minimize the impact on the non-sensitive patterns.

We define the relative-non-sensitive patterns as the one that contain the items in sensitive patterns. Then, the sensitive transactions can be classified as follows:  (1) Including a sensitive pattern only; (2) Including a sensitive pattern and a relative-non-  sensitive pattern; (3) Including a sensitive pattern and more than one  relative-non-sensitive pattern; (4) Including more than one sensitive patterns and more  than one relative-non-sensitive pattern; Comparing with other algorithms, our algorithm can deal  with the second sensitive transactions better; other algorithms select the victim-item randomly, this leads to reduce the support of some non-restrictive, consequently, lose some non-sensitive patterns from the sanitized dataset. Our algorithm does good work if most of the sensitive transactions like the second sensitive patterns above.

B. Implementation ofHarRFI Algorithm  In our algorithm, as to the same sensitive pattern, we select different items to delete for the different transactions based on whether the item will affect the non-sensitive patterns. To do this, the victim-item must be included in the sensitive pattern and not in the non-sensitive patterns. This mainly differs from the other approaches of selecting victim-item, and it plays an important role in eliminating the side effect of sanitizing algorithms on transactional database. Another work of reducing the side effect on non-sensitive patterns is deleting the victim-item based on the conflict of sensitive transactions; we can remove the victim-item in the transactions that their conflict is below the min-conflict.

4 TI,T2,T3,T5 3 TI,T3,T4 3 TI,T2,T5 3 TI,T3,T6  Freq TIDS  4 TI,T2,T4,T5 Freq TIDS  Table IV Inverted file of sensitive frequent itemset  Table V Inverted file of non-sensitive frequent itemset  AB AD BC BD  AC  Itemset  Itemset  hidden is not the src-sen, but the patterns contain them, par-src. There are two good reasons to show that the reasonable hidden objective is the src-sen: 1, The biggest (smallest) support of a pattern in src-sen is not always the biggest (smallest) one in par-src; 2, There are some items in par-src, but not in src-sen, to delete these items will lead to accidentally hidden non-sensitive patterns. For example, in Table I, let AC be the original restrictive pattern, it is right that ABC is restrictive, too. But, when we hide ABC through sanitizing the database, there is another item B more, and the victim item in transactions that contain ABC will be the item B using the algorithms MaxFIA or MinFIA, because the support ofitem B is likely to the biggest or smallest one in the pattern ABC. This will fail in hiding ABC. In the meantime, BC is a non-sensitive pattern, removing B will lead to hidden the pattern that should not be hidden. Obviously, we can hide par-src by hiding src-sen. It is reasonable to select c to be the set of patterns to be removed. So, in our work, the hidden object is the original restrictive patterns. The experiments behind prove that hiding src-sen has less effect on non-sensitive rules.

Based on the above analysis, we chose src-sen as set of sensitive patterns, and make sure that the elements in it are 2-items. To illustrate how to select victim items in sensitive transactions, let us consider the sample transactional database in Table I. Suppose minimum s = 50%, we have a set of restrictive patterns src-sen = {AC}, then the rest of the patterns in Table II are non-sensitive patterns but ABC. Here, we can trim out the pattern ABC through hiding AC.

Table IX shows the result of the second experiment. We select seven groups ofrestrictive patterns, and each group has ten restrictive patterns, ranging from two to five items in length from the dataset DJ, D2 and D3. The first three groups ofpatterns mined from DJ with s = 0.4%; the last two groups of patterns mined from D2 with s = 1.5%; the rest of the  10000 transactions in which the average size per transaction is 10; D2 contains 1000 items, with 100000 transactions in which the average size per transactions is 40. D3 contains 1656 items, with 515597 transactions in which the average size per transactions is 6.53.

We performed two series of experiments: the first is to prove that the src-sen is the better object to be hidden than par-src in algorithm Stanly presented, and the second is to measure the effectiveness of Naive, MaxFIA, MinFIA, IGA and our algorithm on hiding sensitive patterns n (n2:2) items in length. Because the disclosure threshold 'II only controls the proportion of sensitive transactions to be sanitized for each restrictive patterns, but does not control the hiding result and affect non-sensitive patterns directly, we set 'II = 0 in our experiments. Obviously, it will hide all the sensitive patterns when 'II = O.

In the first experiment, we select three groups ofrestrictive patterns, and each group has ten restrictive patterns from the dataset DJ with s = 0.4%, ranging from two to five items in length. Table VIII shows the impact to non-sensitive patterns on the set of restrictive patterns between src-sen and par-src.

The results tell us ifwe take src-sen as the object to be hidden, the percentage of non-sensitive patterns that are not discovered from the sanitized dataset DJ' is lower.

MaxFIA MinFIA IGA Percentageof non-sensitivelosing  0.107 0.105 0.116 0.175 0.104 0.110  IGA  0.126 0.131 0.116 0.124 0.154 0.15  0.152 0.642 0.231 0.437  HarRFI  0.127 0.108 0.128 0.131 0.119 .0129 0.185 0.142 0.105 0.104 0.137 0.137  MaxFIA MinFIA Percentageof non-sensitivelosing  0.128 0.131 0.105 0.185 0.142 0.175 0.137 0.137 0.110 0.15 0.137 0.13 0.18 0.145 0.165 0.17 0.145 0.155  0.175 0.145 0.16 0.585 0.852 0.72 0.571 0.357 0.596 0.578 0.605 0.658  Table IX Percentageof non-sensitivelosing  Table VIII Percentageof non-sensitivelosing  0.212 0.222 0.234 0.263 0.175 0.184  Naive  0.222 0.263 0.184 0.223 0.19  0.185 0.186 0.932 0.656 0.794  Narve  Avg  Avg  Avg  Src-senl Par-srcl Src-sen2 Par-src2 Src-sen3 Par-src3  Sensitive patterns  Par-src  The first step builds an inverted index of the items in D in one scan of the database. The step 2 and 3 build the data structures needed in our algorithm based on the set of sensitive patterns src-sen. Step 4 shows how to select the victim-item, which is the main contribution ofthis paper. The function min-support returns the item that has smaller support.

Another contribution of our algorithms in section 5 is classing the sensitive transactions based on their conflict in the whole database; other algorithms sort the transactions that include the same sensitive pattern in ascending order of degree of conflict. The min-conflict is used to decide to sanitize the transactions which conflict is below min-conflict.

For example, ifmin-conflict = 1, we sanitize the transactions which degree of conflict is 1. Our algorithm scan database again to delete the victim-item in sensitive transactions in step 6.

END  Input: D, src-sen, min-conflict Output:D' Step 1: buildingthe invertedstructureof D  Item-Tids = invert-item(D); Step 2: buildingthe invertedstructureof src-sen  Inverted-Senfreq = invert-freq(Item-Tids,src-seny; Step 3: building invertedstructureof rei-nonsen  Inverted-RelNonsen = invert-relfreq(Item-Tids, src-seny; Step 4: selectingthe victim-item  For each sf E Inverted-Senfreq do For each rnsf E Inverted-RelNonsen do  If sfitemset n rnsfitemset is not NULL do victtm-item-r-sf.itemset n rnsfitemset; Delete rnsf; Delete sfTIDS E sfTIDS n rnsfTIDS  Else continue; If sfTIDS is not NULL do  victim-item =min-support(sf itemsety; Step 5: classifyingthe transactionsbased on their conflict  For each sensitivetransactionSfdo Computer its conflict  Step 6: generatingthe new databaseD' For each sensitivetransactionSfdo  If conflict :s min-conflict do Sf = Sf -victim-item  The sketch of the algorithm as follows:

IV. EXPERIMENTALRESULTS  All the experiments were conducted on a PC, which has an AMD Althlon(tm) 64 Processor 3000+, with 1GB of RAM running a Windows XP operating system. The datasets used in our experiments are IBM-Artificial, BMS-POS. The IBM-Artificial datasets, TI0I4DI00K and T40II0DI00K, which is often used in the association rule research community. The BMS-POS dataset contains several years' worth of point-of-sale data from a large electronics retailer, denoted by DJ, D2 and D3. DJ contain 1000 items, with     groups of patterns mined from D3 with s = 4%. In terms of average of percentage of non-sensitive losing, our algorithm is the best performing algorithm in most oftime, there is only algorithm MinFIA hides fewer non-sensitive patterns compared to our algorithm on the dataset D2' transformed from dataset D2. Because we do not add items to the dataset, there are no artificial patterns created by sanitizing. We set the value ofmin-conflict in our algorithm a max value so that we can sanitize all the sensitive transactions. We delete all victim-items in dataset, so all the restrictive patterns are hidden successfully.

v. CONCLUSIONS In this paper, we have introduced a new algorithm for  sanitizing a transactional database, which is an itemset oriented approach based on decreasing the support ofa set of large itemsets until it is below a user specified threshold, so that no rules can be derived from the selected itemsets. We also introduce a new method for selectively removing items from a database to prevent the discovery of a set of rules.

Differing from the related methods, our method select items to delete based on whether they impact on the non-sensitive patterns. So, we delete different items from the transactions for the same sensitive pattern, this can minimize the impact on the non-sensitive patterns mining from the sanitized transactional database. The other contribution of this paper is we classify the transactions by theirs conflict; the user can sanitize the special group of transactions which have the lowest impact on database. This avoid to sort the transactions based the conflict for each sensitive pattern to hide. We also introduce the notion of relative-non-sensitive pattern which will be affected when hiding sensitive patterns, it help us note that which non-sensitive patterns will be affected.

Our sanitation method represents a significant improvement over the previous algorithms presented in the literatures; however, it is proven that the problem of fmding an optimal sanitization ofthe source database is NP-Hard [4].

In our algorithm, how to select victim-items with no affection to the non-sensitive patterns when we sanitize the third and the fourth sensitive transactions defmed in Section 3, which will be tried to solve in our future work.

