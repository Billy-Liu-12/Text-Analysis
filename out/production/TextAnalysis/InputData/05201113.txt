LEVERAGING SOCIAL MEDIA FOR TRAINING OBJECT DETECTORS

ABSTRACT  The fact that most users tend to tag images emotionally rather  than realistically makes social datasets inherently flawed from  a computer vision perspective. On the other hand they can be  particularly useful due to their social context and their poten-  tial to grow arbitrary big. Our work shows how a combination  of techniques operating on both tag and visual information  spaces, manages to leverage the associated weak annotations  and produce region-detail training samples. In this direction  we make some theoretical observations relating the robust-  ness of the resulting models, the accuracy of the analysis al-  gorithms and the amount of processed data. Experimental  evaluation performed against manually trained object detec-  tors reveals the strengths and weaknesses of our approach.

Index Terms? Social media, object detection, weak an-  notations, Flickr  1. INTRODUCTION  Semantic object detection is one of the most useful operations  performed by human visual system and constitute an exciting  problem for computer vision scientists. Robust models capa-  ble of capturing the diversity of an object?s form and appear-  ance, need to be learned from a large number of highly de-  scriptive training examples. However, current literature had  showed us that such examples are not existent and therefore  very expensive to obtain.

In this perspective, semantic object detection can be  viewed as a problem of either supervised [1], [2], [3], [4] or  unsupervised learning [5], [6], [7], [8], [9], [10], [11], [12],  [13], [14]. In the first case a classifier is trained to recognize  an object category e.g., a face [1], [4], a building [2] or a car  [3], using a set of hand-labeled training images. The draw-  back of these schemes is that they require a large amount of  strongly annotated images, the generation of which is a labo-  rious and time consuming procedure. To tackle this issue, the  methods resorting to unsupervised learning attempt to solve  the problem by using weakly annotated training examples. In  this case, the idea is to estimate a joint probability distribution  on a space of semantic labels and visual characteristics.

A high number of diverse ideas has been proposed in  the literature for this purpose. In [15], [13] the problem is  viewed as a top-down image segmentation procedure where  the recognition of visual objects is incorporated as an inter-  mediate step of segmentation. Aspect models like probabilis-  tic Latent Semantic Analysis (pLSA) [7], [16] and Latent  Dirichlet Allocation (LDA) [17], [18] have been used with  weakly annotated datasets to estimate the joint probabilities  between semantic labels and visual features. In some cases  these models are coupled with conditional random fields [12],  [19] to incorporate spatial and hierarchical information orig-  inating from context, or use Probabilistic Graphical Models  (PGM) [11] to consider the role of structure within the detec-  tion process. Other techniques, that also rely on observations  statistics to estimate these joint probabilities, include [10], [8]  where Expectation Maximization is employed, and [9] where  stochastic processes are used. Some pioneer work in this  direction has been presented in [5] where much information  is learned from a handful of images by taking advantage of  knowledge coming from previously learned categories, and  [6] where the advantages of supervised and un-supervised ap-  proaches are combined by solving a multiclass classification  problem.

This work concentrates on social media and their poten-  tial to serve as the training examples of an object detection  scheme. Social sites like flickr, accommodate image corpora  that are being populated with hundreds of user tagged images  on a daily basis. We are interested on whether such corpora  can be leveraged to facilitate the robust estimation of mod-  els. By looking at the literature above, we realize that most  of the proposed schemes have been tested on purpose spe-  cific datasets. For instance [5], [18], [7], [10], are evaluated  using the Caltech dataset which is a set of images manually  organized in categories, while [16], [20] operates on images  collected from the web using key-word based search. Simi-  larly [6], [9], [8], [21], [13] use the Corel dataset, which is a  set of images annotated with realistic tags, while [11], [12],  [18] operate on Microsoft Research database which is a set  of strongly annotated images. Few are the attempts where  object detection schemes exploit social data, as in [22], [23],     [14] where photo collections obtained from flickr are used for  this purpose. The advantage of using social sites like flickr is  that we can obtain a high number of images without spending  much effort or time. Consequently, as opposed to supervised  approaches, there is no limitation on the types of objects that  can be trained, since social sites accommodate images depict-  ing a huge variety of objects.

Our work bears many similarities with [8], where seg-  mentation, visual feature extraction and region clustering are  applied on a set of tagged images to facilitate object detec-  tors? training. However, we examine from both theoretical  and experimental perspective, the way the robustness of the  generated detectors is affected by the relation associating the  accuracy of the image analysis algorithms with the size of the  processed dataset.

2. FRAMEWORK DESCRIPTION  The goal of our framework is to start from a set of user tagged  images, obtained from social sites, and automatically extract  training examples, suitable for learning an object detection  model. Social media processing, segmentation, visual fea-  tures extraction, clustering and machine learning constitute  the analysis components incorporated by our framework, as  shown in Fig. 1. We mainly focus on the components of social  media processing and clustering, with the intention to tackle  the reduced amount of supervision foreseen by our framework  and the low quality of tags contributed by the social users. In  Social Media  Processing Segmentation  Vis. Features  Extraction  Clustering Machine  Learning  Tag-based clustering  -Social Knowledge  -Semantic Knowledge  Un-supervised image  segmentation MPEG-7 Descriptor extraction  from image regions  Region clustering based on  visual features  Learn models for recognizing  specific objects  Focus of our work  Fig. 1. Analysis components incorporate by our framework.

our framework, we identify six analysis steps that are applied  consecutively on a set of user tagged images: a) Cluster im-  ages using their tags and acquire image groups each one em-  phasizing on a particular topic. The linguistic description of  this topic is usually reflected in the most frequent tag. b) Pick  an image group so as its most frequent tag to conceptually re-  late with the object of interest. c) Segment all images in the  selected image group into regions that are likely to represent  objects. d) Extract the visual features of these regions with  the expectation that all regions representing the same object  will share a relative high amount of common characteristics.

e) Perform feature-based clustering so as to create groups of  similar regions. We anticipate that the majority of regions  representing the object of interest will be gathered in one of  the clusters, pushing all irrelevant regions to the others. f) Use  the visual features extracted from the regions belonging to the  cluster representing the object of interest, to train a machine  learning-based object detector.

Although, there are issues to be addressed such as a) how  to derive image groups with an increased level of semantic  coherence, b) how to determine the number of clusters for  the feature-based region clustering procedure, and c) how to  select the cluster containing the regions depicting the object  of interest; our great advantage relies on the social aspect  of the analyzed dataset and its potential to grow particularly  large. It has been shown [24] that the majority of users tend  to contribute similar tags when faced with similar type of vi-  sual content. This is attributed to the common background  that most users share and is expected to lead the prevailing  concepts in tag and visual information space to convergence.

Based on this assumption we adopt the following solutions  in order to fully automate the aforementioned process. Se-  mantically coherent groups of images are generated using a  tag-based clustering approach that incorporates both social  and semantic knowledge, detailed in Section 4. The num-  ber of clusters for the feature-based region clustering step  is determined in an un-supervised manner by employing the  Maximin algorithm, tuned using cross validation as described  in Section 4. Finally, the most populated of the generated  region-clusters is chosen to provide the machine learning al-  gorithm with the necessary training examples, as explained in  Section 3.3.

It is evident that selecting the most populated of the  generated clusters would certainly constitute the appropriate  choice, if all analysis components of computer vision (i.e,.

segmentation, discrimination by visual features) worked per-  fectly. However, since current literature has shown us that  this is not true, we examine how the size of the analyzed  dataset affects the legitimate error space of the analysis mod-  ules, for letting the aforementioned cluster selection to be the  appropriate choice. The following section investigates the  issue from a theoretical perspective.

3. THEORETICAL ANALYSIS  3.1. Preliminary Definitions & Conventions  Table 1 summarizes the notations used throughout the pre-  sented analysis. Given the diversity characterizing an ob-  ject?s form and appearance, both segmentation and visual fea-  ture extraction are likely to introduce errors in the analysis  pipeline of Fig. 1. However, if we consider that our final goal  is to create clusters of image regions depicting the object of  interest, we can accept that all these errors are eventually re-  flected on the efficiency of the clustering procedure. Thus, we  will make the convention that the clustering error incorporates  all these sources of error.

Table 1. Legend of Introduced Notations  Symbol Definition  S The complete social dataset  N The number of images in S  L A particular topic  SL An image group, subset of S that  emphasizes on topic L  n The number of images in SL  Iq An image from S  RIq = Segments identified  {r Iq i , i = 1, . . . , m} in image Iq  fd(r Iq i  ) = Visual descriptor  {fi, i = 1, . . . , z} extracted from a region r Iq i  TIq Set of tags associated with image Iq  C = Set of objects that appear {ci, i = 1, . . . , t} in an image group S  L  W = Set of clusters created by the {wi, i = 1, . . . , o} feature-based clustering algorithm  pci probability that social media processing  draws from S an image depicting ci  Moreover, we will assume that there is a one-to-one rela-  tion between an image and an object (i.e., we do not consider  cases where the same object is depicted in two different loca-  tions of the image).

3.2. Social Media processing  The goal of social media processing is to cluster images into  semantically coherent groups, SL ? S. We are interested in the frequency distribution of objects ci ? C appearing in S  L  based on their frequency rank. If we focus on a single image  group SL, we can view this process as the act of populating  SL with images selected from a large dataset S using certain  criteria, (see Section 4). In this case, the number of images  in SL that depict the object ci, can be considered to be equal  with the number of successes in a sequence of n independent  success/failure trials, each one yielding success with probabil-  ity pci . Considering that an image depicts more than one con-  cepts we can say that the probabilities pci , ?ci ? C are inde- pendent from each other and they depend on the nature of the  dataset. Given that S is sufficiently large, drawing an image  from this dataset can be considered as an independent trial.

Thus, the number of times an object ci ? C appears in S L  can be expressed by a random variable K following the bino-  mial distribution with probability pci . In this way we can use  the corresponding probability mass function (Pr(K = k)) depicted in eq. (1), to estimate the probability that SL con-  tains k images depicting ci:  Pr(K = k) =  (  n  k  )  pk(1 ? p)n?k (1)  Moreover, since the social media processing aims at cre-  ating groups of images emphasizing on a particular topic, we  can assume that there will be an object c1 that is drawn with  c1 c2 c3 c4 c5            Objects  # a  p p e a ra  n c e s  (a)  c1    c2 c1    c2 c1    c2            Objects  # a p p e a ra  n c e s  n=50  n=100  n=200  df=10  df=20  df=40  (b)  Fig. 2. a) Distribution of #appearances of the objects C in  SL, for n=100 and pc1=0.9, pc2 = 0.7, pc3 = 0.5, pc4 = 0.3, pc5 = 0.1. b) Difference of populations between c1, c2, using different values of n  probability pc1 higher than pc2 , which is the probability that  c2 is drawn, and so forth for the remaining ci ? C. This assumption is experimentally verified in Section 5.1 where  the tag-frequency histograms of different image groups are  measured. Given the above, we can use the expected value  (E(K)) of a random variable following the binomial distribu-  tion (eq.(2)) to estimate the number of times an object ci ? C will appear in SL, if its drawn from the initial dataset S with  probability pci . This is actually the value of k maximizing the  corresponding probability mass function.

E(K) = np (2)  In this way, we are able to estimate how the number of ap-  pearances (#appearances) of objects ci ? C are distributed in SL, based on their frequency rank. Fig. 2(a) show how such  a distribution would look like given that (pc1 > pc2 > . . .).

Based on this distribution and given the fact that as N in-  creases n will also increase, we examine how the population  of the generated region clusters relates with the clustering er-  ror space and n.

3.3. Clustering  The goal of feature-based region clustering is to group to-  gether regions representing the same object. Ideally, the  distribution of clusters? population based on their population  rank, coincides with the distribution of objects? #appear-  ances based on their frequency rank. In this case, the most  populated cluster w1 contains all regions depicting the most  frequently appearing object c1. However, there is very lit-  tle chance that we will get perfectly solid clusters, each one  containing regions representing a single object.

Nevertheless, given the fact that object models can be ro-  bustly learned even from rather noisy training sets, we seek to  detect the point where w1, which is the cluster containing the  majority of regions depicting c1, will stop be the most pop-  ulated cluster and therefore not selected by our framework  to train c1. Clearly, this depends on the clustering error and    the difference in population separating the first two most fre-  quently appearing objects c1, c2 ? C. This difference de- pends on pc1 , pc2 and increases proportionally to n as derived  from eq. (2) and shown in Fig. 2(b). Here, we work under the  assumption that it is more likely for the second most highly  ranked cluster w2 to become more populated than w1 as the  clustering error increases. Thus, we only consider c1 and c2 and examine how their difference in population relates with n  and clustering performance.

In order to do this we make an initial assignment of ob-  jects to clusters based on their ranks ci ? wi, and express clustering error using the notations of Table 2.

Table 2. Notations for Clustering  Symbol Definition  TCi Number of regions depicting object ci tci Number of regions depicting ci,  correctly assigned to cluster wi Popi Population of cluster wi FPi False positives of wi with respect to ci FNi False negatives of wi with respect to ci  DRi = Displacement rate of wi, FPi ? FNi with respect to ci  Given the above, FPi = Popi?tci and FNi = TCi?tci.

By substituting tci we have Popi = TCi+FPi?FNi. How- ever, TCi is actually the number of times the object ci ap-  pears in SL (#appearances) and according to eq. (2) we have  TCi = npi. Now, w1 will be selected by our framework for learning c1 as long as:  Pop1 ? Pop2 > 0 ? TC1 ? TC2 + (FP1 ? FN1) ? (FP2 ? FN2) > 0 ?  n > DR2?DR1 pc1?pc2  (3)  The displacement rate DRi shows how the Popi of clus-  ter wi modifies according to the clustering error and with re-  spect to the ideal case where this error is zero. Positive values  of DRi indicates leakages in wi population, while negative  values indicate inflows. Using eq. (3) we 3D plot in Fig. 3  the space where Pop1 ? Pop2 > 0. Every horizontal slice of this volume corresponds to the legitimate values of DR1 and  DR2 for a certain value of n. As n increases, the surface of  the corresponding slices increases also and thus the legitimate  error space for clustering increases too.

4. IMPLEMENTING THE FRAMEWORK  Social media processing: For acquiring image groups with  an increased amount of semantic coherence we adopted the  SEMSOC approach introduced by Giannakidou et. al. in [25].

In this work, an unsupervised model for efficient and scalable  mining of multimedia social-related data is presented. The  Fig. 3. Space in which w1 remains the most populated of the  generated clusters, derived from eq. (3)  reason for adopting this approach is to overcome the limita-  tions that characterize collaborative tagging systems such as  tag spamming, tag ambiguity, tag synonymy and granularity  variation, and increase the semantic coherence of the gener-  ated groups. Each group emphasizes on a particular topic and  the set of its containing tags reflects the way users perceive  it. SEMSOC manages to create meaningful groups by jointly  considering social and semantic features. Its outcome is a set  of image groups SLi ? S, i = 1, . . . , m where Li is an indicator of the emphasized topic and m is the number of cre-  ated clusters. In this case the number of clusters is determined  empirically, as described in [25].

Every image Iq has an associated set of tags TIq . We  choose the image group SLi where its most frequent tag con-  ceptually relates with the object that we want to detect. In this  way, we obtain a semantically coherent group of images the  majority of which is expected to depict the object of interest.

Segmentation: Segmentation is applied to all images in  SL with the aim to extract the spatial masks of visually mean-  ingful regions. In our work we have used a K-means with  connectivity constraint algorithm as described in [26]. The  output of this algorithm is a set of segments RIq = {r Iq i , i =  1, . . . , m}, which in the ideal case correspond to meaningful objects, ci ? C.

Visual descriptors: Seven descriptors proposed by  MPEG-7 [27] capturing different aspects of color, texture and  shape were used. These descriptors namely mpeg7={Dominant Color (DC), Color Layout (CL), Color Structure (CS), Scal-  able Color (SC), Edge Histogram (EH), Homogeneous Tex-  ture (HT), Region Shape (RS)} were extracted ?r Iq i ? RIq  and ?Iq ? S L. Different descriptors? combinations were  composed by concatenating their normalized values on a  single vector, fd(r Iq i ) = {fi, i = 1, . . . , z}. In this case,  d ? mpeg7 determines the descriptors? combination and z the dimensionality of the feature space, see Section 5.3. The  concatenation approach was used only for training the object    models using SVMs.

Clustering: For performing feature-based region clus-  tering we applied k-means on all extracted feature vectors  fd(r Iq i ), ?r  Iq i ? RIq and ?Iq ? S  L. For calculating the  distance between two regions we have used the functions  presented in [27] by independently measuring the distance  in each feature space and summing their normalized values.

However, the problem that arises from the use of a parametric  clustering algorithm like k-means is that a) the number of the  clusters must be known in advance, and b) its performance is  sensitive to the initial positions of the cluster centers. In or-  der to overcome these problems, we employed the Maximin  algorithm as described in [26], both for selecting the number  of clusters and estimating the initial positions of their centers.

Learning model parameters: Support Vector Machines  (SVMs) [28] were chosen for generating the object detection  models, due to their ability in coping efficiently with high-  dimensionality pattern recognition problems. All feature vec-  tors assigned to the most populated of the created clusters  are used as positive examples for training a binary classifier.

Negative examples are chosen arbitrary from the remaining  dataset. Tuning arguments include the selection of Gaussian  radial basis kernel and the adoption of a brute force strategy  for selecting the kernel parameters.

5. EXPERIMENTAL STUDY  The goal of our experimental study is twofold. On the one  hand, we wanted to get an experimental insight on the error  introduced by the analysis algorithms and check whether our  theoretical claims stand. On the other hand, we aimed at com-  paring the quality of object models trained using the proposed  framework, against the ones trained using high quality, man-  ually provided, region-detail annotations. Experiments nec-  essary for tuning some of the employed algorithms are also  presented.

To carry out our experiments we utilized three datasets,  a strongly annotated dataset constructed manually by asking  people to produce region-detail image annotations, and two  weakly annotated social datasets obtained from Flickr. For  the first dataset SM , a lexicon of 7 objects CM ={Vegetation, Rock, Sky, Person, Boat, Sand, Sea}, was used to strongly annotate 536 images at region-detail. The output of this pro-  cess was to record relations associating an image segment r Iq i ,  identified automatically by the segmentation algorithm, with  an object from CM . On the other hand, two datasets from  Flickr were crawled using the wget1 utility and Flickr API  facilities. The first dataset S3K consists of 3000 images de-  picting among others C3K= {cityscape, seaside, mountain, roadside, landscape, sport-side}, while the second one S10K  consists of 10000 images, mostly related to C10K={jaguar,  1wget: http://www.gnu.org/software/wget       S ky  V eg  et at io n  B ui ld in g  P eo  pl e  ou td oo  rs  S to ne  , r oc  k( s)  N u  m b  e r  o f  im a  g e  s       V eg  et at io n  S ky  P eo  pl e  ou td oo  rs  S to ne  , r oc  k( s)  R oa  ds id e  N u  m b  e r  o f  im a  g e  s         S ea  P eo  pl e  ou td oo  rs S an  d  V eg  et at io n  W av  es ( se  a)  N u  m b  e r  o f  im a  g e  s       P eo  pl e  ou td oo  rs  R oa  ds id e  B ui ld in g  V eg  et at io n  S ky  N u  m b  e r  o f  im a  g e  s       S ky  V eg  et at io n  B ui ld in g  S to ne  , r oc  k( s)  S ea  N u  m b  e r  o f  im a  g e  s  (a) Sky        V eg  et at io n  R oa  ds id e  S ky  S to ne  , r oc  k( s)  A ni m al  N u  m b  e r  o f  im a  g e  s  (b) Vegetation         S ea  P eo  pl e ou  td oo  rs  ve ge  ta tio  n  B ui ld in g  S an  d  N u m  b e r  o f im  a g e s  (c) Sea       P eo  pl e  ou td oo  rs  P eo  pl e in do  or  R oa  ds id e  T ur ke  y co  un tr y  S ky  N u  m b  e r  o f  im a  g e  s  (d) Person  Fig. 4. Distribution of objects? appearance in an image group  SL, obtained from S3K (upper line) and S10K (lower line)  turkey, apple, bush, sea, city, vegetation, roadside, rock, ten-  nis}.

For the purposes of our experimental study and after ap-  plying SEMSOC [25] on both S3K and S10K , we ended up  with four object categories Cbench={sky, sea, person, vegeta- tion}, that exhibited significant presence in all three datasets.

These object categories served as benchmarks for comparing  the quality of different models.

5.1. Social media processing  As claimed in Section 3.2, we expect the gap between the  number of appearances of the first (c1) and second (c2) most  highly ranked objects of C, to broaden as the volume of the  analyzed dataset increases. In order to verify this experimen-  tally, we plot the distribution of objects? #appearances in an  image group SL. Each of the bar diagrams depicted in Fig. 4,  describes the distribution of objects? #appearances inside an  image group SL, as evaluated by human subjects. The im-  age groups are created by applying SEMSOC on both S3k  and S10K , and selecting the groups emphasizing in one of the  benchmark object categories . It is clear that as we move from  S3k to S10K the gap between the number of images depicting  c1 and c2, increases in all four cases.

5.2. Tuning Maximin  As mentioned before, Maximin is used to decide the num-  ber of clusters and generate an initial estimation of the cluster  centers, to be used by K-means. However, Maximin largely  depends on a parameter called ?, that specifies the threshold  according to which new clusters are created or not. The pur-  pose of this experiment was on the one hand to optimally tune  ?, in order to use it for all subsequent experiments, and on the  other hand to check whether this value deviates substantially  as the training examples and the object category vary. This is  to ensure that the tuned value can be safely used under vari-  ous contexts. For this purpose we use SM and apply 10-fold  cross validation, for all available objects of CM and all pos-    Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Fold 6  Fold 7  Fold 8  Fold 9  Fold 10 boat  vegetation  sky  sea  rock  sand  person  0.2  0.4  0.6  0.8   Concepts  Cross validation results for Cl, EH and RS  Testing Folds  ?  Fig. 5. Cross-validation results for descriptor combination  CL, EH and RS (?avg = 0.633, ?min = 0.4 ,?max = 0.8)  sible descriptor combinations d ? mpeg7. Given that SM is strongly annotated, clustering efficiency can be measured ex-  plicitly using typical classification metrics (i.e. F-Measure).

For every object ci ? C M , the subset of images Sci de-  picting this object is selected using the manually provided an-  notations. Images are segmented and visual features are ex-  tracted. Subsequently, the regions are divided in 10 folds, us-  ing each time one fold for ?testing? and 9 for ?training?. For  every run of the experiment we vary the value of parameter ?  within [0.2 0.96] using steps of 0.05. For each value of ?, the  number of clusters determined by applying Maximin on the  ?training? folds, is used to perform clustering using k-means  in the regions belonging to the ?testing? fold. The F-measure  of the most populated cluster w1 is calculated with respect to  the most frequently appearing object c1 in S ci . Given that  for each value of ? we can measure the clustering efficiency  Fi,j,? , on the basis of a S ci and fold j, we are able to deter-  mine the optimal value of ? as ?opt = argmax?(Fi,j,?).

Finally, the average of the optimal values among folds and  objects (? = 0.633) was used for the remaining of our exper- iments. Fig. 5 is a 3D plot summarizing the aforementioned  results for the feature space derived by combining CL, EH  and RS. It is clear that the optimal values of ? does not de-  viate substantially as the object category and the folds vary.

Similar observations were made for all other combinations of  MPEG-7 descriptors, the results of which are not included in  this manuscript due to lack of space.

5.3. Optimal Feature Space  Visual descriptors determine the attributes by which a model  tries to capture an object?s form an appearance. After tun-  ing the Maximin algorithm for all different combinations  of MPEG-7 descriptors, we utilized the strongly annotated  dataset SM to determine the optimal feature space, in terms  of clustering efficiency. As in the previous case ?ci ? C M , a  subset Sci ? SM of images depicting ci was selected to serve as the image group. For each of those image groups, cluster-  None DC  CL SC  CS DC_CL  DC_SC DC_CS  CL_SC CL_CS  SC_CS DC_CL_SC  DC_CL_CS DC_SC_CS  CL_SC_CS DC_CL_SC_CS  NoneHTEHRSHT_EHHT_RSEH_RSHT_EH_RS   0.5   1.5   2.5   3.5  Texture and shape descriptors  Clustering efficiency for all descriptor combinations  Color descriptors  C lu  s te  ri n  g e  ff ic  ie n  c y  Fig. 6. Clustering efficiency for all combinations of MPEG-7  descriptors  ing efficiency was measured by calculating the F-Measure of  the most populated cluster w1, with respect to the most highly  ranked object c1 in S ci . Finally, these values were summed  over all different objects ci ? C M , to form a cumulative  f-measure metric assessing the clustering efficiency for a cer-  tain combination of visual descriptors (i.e., feature space).

Fig. 6 summarizes the results by plotting in the z-axis the  value of cumulative f-measure obtained for the feature space  determined by combining the descriptors indicated by the x? and y?axis. We can see that clustering efficiency maximizes when CL, EH and RS are combined. This experimental ob-  servation is also compliant with human intuition since color,  texture and shape are considered important attributes of vi-  sual perception for discriminating between different objects.

The feature space determined by d = {CL, EH, RS} was utilized for the remaining of our experiments.

5.4. Cluster Selection  Having tuned the Maximin algorithm and selected the opti-  mal feature space, the purpose of this experiment was to val-  idate using real data our theoretical claim that the most pop-  ulated cluster contains the majority of regions depicting the  object of interest. In order to do so, ?ci ? C M we obtain  Sci ? SM and apply k-means clustering using ? = 0.633 and d = {CL, EH, RS}. In Fig. 7 we visualize the way regions are distributed among the clusters by projecting their feature  vectors in three dimensions using PCA (Principal Component  Analysis). The regions depicting the object of interest ci are  marked in squares, while the other regions are marked in dots.

Color code indicating a cluster?s rank according to their pop-  ulation (i.e., red: 1st, black: 2nd, blue: 3rd, magenta: 4rth,  green: 5th, cyan: 6th) is used. Thus, in the ideal case all  squares should be painted red and all dots should be colored  differently. Squares being painted in colors other than red,  indicate false negatives and dots painted in red indicate false  positives. We can see that our claim is validated in 5 (i.e., sky,  sea, person, vegetation and rock) out of 7 examined cases.

?4 ?2    ?4  ?2    ?3  ?2  ?1     red:911  black:712 blue:428  magenta:375  (a) Sky  ?4 ?2    ?4  ?2    ?3  ?2  ?1     red:875  black:741 blue:424  magenta:369  (b) Sea  ?4 ?2    ?2    ?3  ?2  ?1      red:602 black:487 blue:351  magenta:339 green:238  (c) sand  ?4  ?2    4 ?4 ?2    ?3  ?2  ?1     red:306  black:302 blue:283  magenta:217 green:189 cyan:138  (d) Person  ?4 ?2    ?4  ?2   ?3  ?2  ?1     red:105 black:100  blue:79 magenta:68  green:59  (e) Boat  ?4 ?2    ?4  ?2    ?3  ?2  ?1     red:311  black:243  (f) Vegetation  ?4 ?2    ?2    ?2  ?1     red:261  black:166  (g) Rock  Fig. 7. Regions distribution amongst clusters. This Figure is best viewed in color with magnification.

The visual diversity of objects boat and sand, causes segmen-  tation and visual feature extraction to introduce significant  error, that prevents clustering from gathering the regions of  interest into the most populated cluster.

5.5. Object models comparison  Assessing the quality of object detection models, generated  using both the proposed framework and the manually pro-  vided region-detail annotations, is the purpose of this exper-  iment. Additionally, we want to validate our claim that as  the scale of the utilized social dataset increases, the error al-  lowed to be introduced by the analysis components increases  also and the models produced by the proposed framework are  more robust. With this intention, we generated object models  using SM , S3K and S10K for the object categories of Cbench.

For each object ci ? C bench one model was trained in a fully  supervised manner using the strong annotations of SM , and  two models were trained without supervision using the weak  annotations of S3K and S10K and the proposed framework.

In order to evaluate the performance of these models, we uti-  lized a portion (i.e., 268 images) of the strongly annotated  dataset SMtest ? S M as ground truth, not used during training.

By looking at the bar diagram of Fig. 8, we note that  models trained in a fully supervised manner perform opti-  mally in all cases. However, the performance achieved by  the models trained without supervision, although inferior, is  still satisfactory, especially if we take into account the time  and effort gained using the proposed framework. Another  interesting observation concerns the improvement in perfor-  mance achieved in all cases, between the models trained us-  ing S10K and S3K , respectively. This tendency verifies our  claim that there is a relation between the size of the utilized  social dataset and the robustness of the generated models.

6. CONCLUSIONS & FUTURE WORK  Although the quality of the object models trained using the  proposed unsupervised technique is still inferior from the one  achieved using supervised approaches, we have shown that  under certain circumstances social data can be effectively  used to learn the parameters modeling an object?s form and  appearance. Moreover, as it is reasonable to expect that the  proposed framework would not graciously scale to every pos-  Sky Vegetation Sea Person           Manual annotation vs automatic annotation  F m  e a s u re  Manual  Flickr 10000  Flickr 3000  Fig. 8. Comparing the quality of different object models  sible object category, the social aspect of user contributed  content and its potential to scale in terms of content diversity  and size, advocates it?s use for the type of objects that ap-  pear frequently in social context. Our plans for future work  include exploiting more of the user contributed information  (e.g., Flickr groups) for obtaining suitable (from a computer  vision perspective) datasets, and the employment of outlier  detection techniques for training the models using less noisy  region-clusters.

7. ACKNOWLEDGMENT  This work was funded by the X-Media project (www.x-  media-project.org) sponsored by the European Commission  as part of the Information Society Technologies (IST) pro-  gramme under EC grant number IST-FP6-026978 and the  European Community?s Seventh Framework Programme  FP7/2007-2013 under grant agreement n215453 - WeKnowIt.

8. REFERENCES  [1] Paul A. Viola and Michael J. Jones, ?Rapid object de-  tection using a boosted cascade of simple features,? in  CVPR (1), 2001, pp. 511?518.

[2] Yi Li and Linda G. Shapiro, ?Consistent line clusters  for building recognition in cbir,? in ICPR (3), 2002, pp.

952?956.

[3] Bastian Leibe, Ales Leonardis, and Bernt Schiele, ?An  implicit shape model for combined object categorization  and segmentation,? in Toward Category-Level Object  Recognition, 2006, pp. 508?524.

[4] Kah Kay Sung and Tomaso Poggio, ?Example-based  learning for view-based human face detection,? IEEE  Trans. Pattern Anal. Mach. Intell., vol. 20, no. 1, pp.

39?51, 1998.

[5] Fei-Fei Li, Robert Fergus, and Pietro Perona, ?One-  shot learning of object categories,? IEEE Trans. Pattern  Anal. Mach. Intell., vol. 28, no. 4, pp. 594?611, 2006.

[6] Gustavo Carneiro, Antoni B. Chan, Pedro J. Moreno,  and Nuno Vasconcelos, ?Supervised learning of seman-  tic classes for image annotation and retrieval,? IEEE  Trans. Pattern Anal. Mach. Intell., vol. 29, no. 3, pp.

394?410, 2007.

[7] Josef Sivic, Bryan C. Russell, Alexei A. Efros, Andrew  Zisserman, and William T. Freeman, ?Discovering ob-  jects and their localization in images,? in ICCV, 2005,  pp. 370?377.

[8] Pinar Duygulu, Kobus Barnard, Joa?o F. G. de Freitas,  and David A. Forsyth, ?Object recognition as machine  translation: Learning a lexicon for a fixed image vocab-  ulary,? in ECCV (4), 2002, pp. 97?112.

[9] Jia Li and James Ze Wang, ?Automatic linguistic in-  dexing of pictures by a statistical modeling approach,?  IEEE Trans. Pattern Anal. Mach. Intell., vol. 25, no. 9,  pp. 1075?1088, 2003.

[10] Robert Fergus, Pietro Perona, and Andrew Zisser-  man, ?Object class recognition by unsupervised scale-  invariant learning,? in CVPR (2), 2003, pp. 264?271.

[11] Giuseppe Passino, Ioannis Patras, and Ebroul Izquierdo,  ?On the role of structure in part-based object detection,?  in ICIP, 2008, pp. 65?68.

[12] Jakob J. Verbeek and Bill Triggs, ?Region classification  with markov field aspect models,? in CVPR, 2007.

[13] Manuela Vasconcelos, Nuno Vasconcelos, and Gustavo  Carneiro, ?Weakly supervised top-down image segmen-  tation,? in CVPR (1), 2006, pp. 1001?1006.

[14] Till Quack, Bastian Leibe, and Luc J. Van Gool, ?World-  scale mining of objects and events from community  photo collections,? in CIVR, 2008, pp. 47?56.

[15] Thanos Athanasiadis, Phivos Mylonas, Yannis S.

Avrithis, and Stefanos D. Kollias, ?Semantic image seg-  mentation and object labeling,? IEEE Trans. Circuits  Syst. Video Techn., vol. 17, no. 3, pp. 298?312, 2007.

[16] Robert Fergus, Fei-Fei Li, Pietro Perona, and Andrew  Zisserman, ?Learning object categories from google?s  image search,? in ICCV, 2005, pp. 1816?1823.

[17] Fei-Fei Li, Pietro Perona, and California Institute  of Technology, ?A bayesian hierarchical model for  learning natural scene categories,? in CVPR (2), 2005,  pp. 524?531.

[18] Bryan C. Russell, William T. Freeman, Alexei A. Efros,  Josef Sivic, and Andrew Zisserman, ?Using multiple  segmentations to discover objects and their extent in im-  age collections,? in CVPR (2), 2006, pp. 1605?1614.

[19] Antonio B. Torralba, Kevin P. Murphy, and William T.

Freeman, ?Contextual models for object detection using  boosted random fields,? in NIPS, 2004.

[20] Keiji Yanai, ?Generic image classification using visual  knowledge on the web,? in ACM Multimedia, 2003, pp.

167?176.

[21] Kobus Barnard, Pinar Duygulu, David A. Forsyth,  Nando de Freitas, David M. Blei, and Michael I. Jor-  dan, ?Matching words and pictures,? Journal of Ma-  chine Learning Research, vol. 3, pp. 1107?1135, 2003.

[22] Alexander Jaffe, Mor Naaman, Tamir Tassa, and Marc  Davis, ?Generating summaries and visualization for  large collections of geo-referenced photographs,? in  Multimedia Information Retrieval, 2006, pp. 89?98.

[23] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic,  and Andrew Zisserman, ?Object retrieval with large vo-  cabularies and fast spatial matching,? in CVPR, 2007.

[24] Cameron Marlow, Mor Naaman, Danah Boyd, and Marc  Davis, ?Ht06, tagging paper, taxonomy, flickr, academic  article, to read,? in Hypertext, 2006, pp. 31?40.

[25] Eirini Giannakidou, Ioannis Kompatsiaris, and Athena  Vakali, ?Semsoc: Semantic, social and content-based  clustering in multimedia collaborative tagging systems,?  in ICSC, 2008, pp. 128?135.

[26] Vasileios Mezaris, Ioannis Kompatsiaris, and  Michael G. Strintzis, ?Still image segmentation  tools for object-based multimedia applications,?  IJPRAI, vol. 18, no. 4, pp. 701?725, 2004.

[27] B. S. Manjunath, J. R. Ohm, V. V. Vinod, and A. Ya-  mada, ?Colour and texture descriptors,? IEEE Trans.

Circuits and Systems for Video Technology, Special Is-  sue on MPEG-7, vol. 11, no. 6, pp. 703?715, Jun 2001.

[28] B. Scholkopf, A. Smola, R. Williamson, and P. Bartlett,  ?New support vector algorithms,? Neural Networks, vol.

22, pp. 1083?1121, 2000.

