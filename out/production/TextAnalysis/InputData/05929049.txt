Anticipatory Retrieval and Caching of Data for Mobile Devices in Variable-Bandwidth

Abstract?In the recent past, there have been rapid advances in the technology of processors, storage and networks, leading to technologies like cloud computing. However, amid all these advances, performance of clouds and cloud services continues to present challenges. Access latencies to the information on the cloud due to variable bandwidth continues to be a serious problem of research; more so in environments requiring mobile devices to stay connected to the cloud. One way to smooth out bumps in bandwidth available is to use anticipatory retrieval of data, and to cache data that is likely to be requested later.

The proposed anticipatory retrieval and caching system is a solution that takes this path. It offers a better experience to those mobile users who are connected to a cloud and make frequent access to the cloud?s datastore. The proposed method aims to provide ubiquitous access to data on clouds regardless of the bandwidth levels. This is done by locally caching all the one-hop related item-sets I1, I2, . . . , Ik semantically belonging to (or semantically linked to) a particular item-set I ?. Caching is done asynchronously in the background during times of high bandwidth. The proposed algorithms assess the semantic relevance of the data using semantic distances along with user priorities and availability of bandwidth, and then prioritizes anticipatory data downloads on to the cloud?s storage based on the relevance quotient.

Index Terms?cloud computing, data cloud, mobile systems, variable bandwidth, semantic link network, anticipatory down- loads

I. INTRODUCTION  Ever since the advent of the World Wide Web (WWW), there have been rapid advances in the technology of proces- sors, storage and networks, leading to cloud computing and related technologies. But cloud computing?s potential does not begin and end with the personal computer?s transformation into a thin client?the mobile platform (which involves devices like smartphones, handheld GPS, tablet PCs, etc.) is already thought to be heavily impacted by this technology, a view clearly indicated by ABI Research [1] and IBM [2]. It is theorized that the cloud will soon become a disruptive global force in the space of mobile systems too, eventually becoming the dominant way in which mobile applications operate.

Just like the traditional cloud computing system, mobile cloud computing also refers to a scenario wherein the com- puting and the data are typically hosted outside the system (mobile device) and therefore the data latency is not just determined by that of the storage disks and the data bus speeds, but also on the delay (however minimal) caused due  to the network speeds, data manipulations, etc. Add to this the fact that the entire data or part of the data should be encrypted and decrypted when it moves over unreliable and public networks, as well as the fact that data often has to be streamed live, latency soon adds up to be a considerable problem. Moreover, we also need to consider the problems of having scarce wireless channel bandwidth and not having sufficient numbers of wireless network access points, thus making it a tough challenge for the cloud service vendors who wish to provide ubiquitous data access to mobile users (who generally desire to access diverse data such as stock quotes, news headlines, email, traffic reports and video clips, via wireless networks).

In this paper, we propose a novel system that is capable of performing anticipatory data caching over cloud-based data stores or ?data clouds.? This enhances the notion of ?ubiquitous data access? for a mobile user accessing data from a cloud datastore. The proposed utility-based system and caching mechanism considers several characteristics of mobile systems, such as intermittent connection availability, bandwidth variation patterns, mobility hand-offs, data update and user request patterns, to achieve significant data avail- ability in mobile devices that make access to data clouds.

Based on this, we then derive an analytical model of an- ticipatory retrieval and caching of data, and also propose algorithms for cache replacement, passive prefetching (which is done asynchronously in the background, during times of high bandwidth), anticipatory caching (through the semantic relevance in the data using semantic link networks proposed by Zhuge, et al. [3]). These algorithms take into account user priorities, current availability of bandwidth, and then prioritize data downloads of data objects (like documents, web data, etc.) from a cloud datastore. The purpose and end result is the offering of a better experience to the users regardless of the network connection, code, and data access details. Our simulations demonstrate the proposed caching mechanism?s efficiency in terms of hit ratios and reduced access latencies.

The rest of the paper is organized as follows. In Section II we explore and acknowledge other related works in and around this topic of research. In Section III we describe the proposed system model in terms of its schematic structures, prefetch model, and pseudocode, and develop a comprehensive and novel framework for anticipatory retrieval and caching of data for mobile devices in variable bandwidth environments.

In Section IV, the details about performance analysis of the proposed algorithm are discussed, and lastly, in Section V we derive conclusions from this work and consider extensions.



II. RELATED WORK The motivation of prefetching for mobile devices over  clouds comes from two observations: Coda Research [4] has estimated that there were 84 million smartphone users in the US alone in 2010 (with significantly larger numbers in the rest of the world, with China and India also having huge numbers of users), and an estimated 149 million users will have WiFi- enabled mobile phones in the United States by 2015, con- tributing to over $23 billion of revenue through cloud-based m-commerce alone; secondly, ComScore [5] considers mobile internet usage in the US, Europe and Japan, and estimates that around 45% of the smartphone users in these places used phones to access digital data in the form of documents, financial reports, photos, videos, etc. These facts thus provide us with ample support and use cases for building a system such as the proposed one.

Caching and prefetching on the edge of the Internet has been one of the popular techniques to improve the scalability and efficiency of delivering dynamic web content, and have become essential components of the technology of the web.

There has been a lot of work done in this regard over the years.

However, most of it has concentrated in reducing the latency in the web as one of the key aims, and very little research has focused on leveraging these techniques for mobile devices in general, and on mobile devices that are connected to clouds in particular.

For example, in one of the earliest works of web caching, Bestavros [6] and Albrecht et al. [7] introduce a server- initiated prefetching approach, wherein the web server main- tains per-client usage statistics and determines the conditional probability of transitioning from one web page to another.

When a client requests a page, the server sends along with the page the names of the most likely subsequently accessed pages, and leaves the initiative for prefetching to the client.

As an improvement, Deshpande and Karypsis [8] propose a Markov model as a part of the clients system and which acts as an agent that would personalize the prefetching based on the clients distinct behavior. According to their model, each web application constructs a Markov model and sends it to the client, which assists the client in improving its prefetching ability on subsequent accesses.

On the other hand, Markatos and Chronaki [9] and Hine et al. [10] aim at shifting the concentration from a purely client- side caching and combine the client-side activities with the servers knowledge of its most popular pages (the Top-10) with client access profiles. In their approach, a client is designed to only determine how much it will prefetch from the list and when it will start prefetching; the decision of what should be prefetched is left to the server. Just along the same lines, Wu and Bauer [11] propose a main memory predictive caching system for web servers and show that the prefetching strategy improves cache hit rates, particularly for small cache sizes.

However, while most of the earlier works concentrated on the caching aspects from a non-mobile point of view, Gupta et  al. [12] concentrate on the aspects of data management over mobile ad-hoc networks and propose to estimate the global distribution and then predict and cache their most popular data in the hope of being able to provide it to other devices when asked by them. Along similar lines, Yin and Cao [13] propose a cooperative caching scheme for mobile ad hoc networks using caching of popular data so that the availability in mobile ad hoc networks is increased.

While most of these works have had an inclination towards various aspects of the web and web-based services, there has not been much work done towards implementing an efficient caching or a predictive caching system over the cloud and cloud-based services. In this paper, we propose a novel system that is aimed at filling this lacuna of the need for an anticipatory caching and retrieval system to predictively cache data from a data cloud for mobile devices that are either connected to a cloud or they tend to constantly access the data from the cloud. The purpose and end result is the offering of a better experience to the users regardless of the network connection, code, and data access details.



III. SYSTEM MODEL  A. Schematic Structure  The goal of our work is to build a system that performs in- telligent calculations on data that lies on the cloud: it identifies the set of data items relevant to the particular data item that is currently being accessed, and prefetches and locally caches them beforehand, in anticipation of future requests. Users do tend to access sets of inter-related data (more so in case of documents) rather than just single unconnected documents (as also pointed out by Amazon in their EC2 documentation that deals with load balancing based on locality of reference [14]).

Thus our proposed model aims to prefetch and cache these related data from the data cloud locally during times of high network bandwidth and network availability, so that if the users might actually need them at future times, they just need to make calls to the local cache rather than to the cloud, thereby smoothening out the bumps in the bandwidth available.

Figure 1 shows the overall schematic of our work. The proposed system consists of mobile clients connected to one end of the cloud application, with the server in the data cloud.

A mobile device is locally connected to a cache and has a component called the Anticipatory Caching Engine (ACE) that performs the task of prefetching on an event trigger. The cloud server (which we may refer to just as the cloud itself) receives requests from clients, and a client on the other hand gets data search requests from the end user, and makes queries to the cloud based on them. However, before making a query, it looks into its own local cache to find out if the data items have already been cached; if not, it tries to build a semantic linked network (SLN) [3] of the data items that are related to the present data search item, and proceeds to request not only for the current item of request, but also makes requests, subject to sufficient available bandwidth, etc., for prefetches of related data items as found by the SLN.

The Anticipatory Caching Engine adopts three policies in performing this activity.

Fig. 1: A Brief Architectural Overview  1) When to cache: The most important thing that happens on the anticipatory caching engine is prefetching. This is about anticipating what data users might want to access in the future, taking into consideration their data request patterns and priorities, and even the availability of network bandwidth at that very point in time. This also includes checking if the data has been modified on the cloud after it was last cached locally.

2) What to cache: Data items that are no longer needed are purged to accommodate new ones (as is necessary given finite cache sizes). The decision on when a cache has to be purged and how much of it is to be purged is dependent on the data prefetched. Typically, when the cache is full and when new data has been prefetched, the cache has to be emptied, and purging the least- frequently-used data and keeping the most-frequently- used ones would minimize the need to prefetch the same data items repeatedly, thereby improving the cache hit rate (CHR).

3) Processing: The processing stage involves the prepara- tion of the final list of downloads based on the available information about what items to cache and when to cache from the above stages. This policy governs the actions of the anticipatory caching system.

The task of prefetching is carried out by the ACE in four key steps.

? The process of data collection, where on receiving the users? data requests, the ACE looks into the user priorities and extrapolates the contexts of those requests.

? The relevance checking: not all data items related could be relevant to the data item under consideration. Some might be more relevant to the data items than the rest.

This is where an SLN is built (see Section III-B) and those data items that are semantically closer to the data item under consideration are calculated.

? The availability of data items is checked. Essentially this in itself is a two-step process.

? It checks the cache to see if the data items that are to be prefetched have already been cached, and if the cached versions of the data items are the newest available.

? It looks into the availability of bandwidth to cache  the required data items. Depending on the bandwidth levels, it automatically adjusts itself into caching those items that do not clog the network, if down- loaded with the available bandwidth. After checking the availability, the data items are prioritized.

? Prioritization: Prioritizing data items forms the fourth step of the process, where depending on relevance and availability, the final list of data items that could be downloaded are determined.

B. Semantic Pre-fetching Model  In this section, we describe the basic working model of the proposed system. The basic skeletal structure of the system model is as shown in Figure 1. It consists of the following components: the data source, the Semantic Link Network (SLN), a few meta-models, and finally the list of data items to be cached.

A data source basically forms the base component of cloud data that a user might require. A data source is typically associated with a number of Application Program Interfaces (APIs) which are designed for retrieving data from it on user request and then helps in sending it to other web pages that display it. This allows us to have separate mechanisms for retrieving data from the database, displaying data, filtering the displayed data, and paging.

Fig. 2: Various Stages involved in the Model  However, the data at the data-source is not structured.

Hence, when a user makes a data search, the data-source only gives the output that has the search-word or search- item. We then transform the search item into a semantic link network [3]. A semantic link structure reflects the relevance among semantic components, just as hyperlink structure re- flects the relevance of current web pages.

A semantic link network S can be formally stated as follows  S = ?{N,L,Rules,OP}?,where  ? N is a set of k nodes {n1, n2, . . . , nk} indicating a concept or a structure. A node further is composed of an attribute (att) and a class (c). An attribute is a unique    value given to the node that points to a data item, and a class defines the type of this data item. (Informally, the class could be any of the common types like text, image, video or audio.)  ? L is a set of semantic links. A semantic link indicates a relation between two nodes, n, n? ? N . A semantic link takes the following form: n???n? , where ? denotes the type of relationship that exists between n and n?.

? Rules is a set of rules on L which acts as a basis for determining what kind of relationship exists between SLNs; it may be intra-document link or inter-document link.

? OP = {AddLink,DelLink,AddNode,DelNode, AddRule,DelRule} is a set of basic operations on SLN.

Users can add semantic nodes, semantic links and rules to or delete them from SLN.

Now, with S = ?{N,L,Rules,OP}? modelled as a graph with nodes N indicating data items or vertices and links L indicating relationships between data items or edges, we propose to build a final list of nodes ? that need to be cached for a given data search ?, which is pointed by a node n? ? N .

Now, ? is defined as the set of all the nodes of an SLN S? = ?{N ?, L?, Rules,OP ?}?, where S? is a subgraph of S and ? edge l? = l(x, y) ? L, |L?| = 1 and n? ? N ?. In short, S? is a subgraph of the graph formed by S that contains all one-hop neighbors of the node n? ? N , with the attribute of n? pointing to the given data search ?.

The next step is to consider the list ? (of cardinality m) which is of the form ? = {n?,1, n?,2, ...n?,m} where n?,r indicates the rth one-hop node of n? .

We then expand the each of the n?,r ? ? into a set ? such that ? = {{|n?,1,text|, |n?,1,image|, |n?,1,audio|, |n?,1,video|}, {|n?,2,text|, |n?,2,image|, |n?,2,audio|, |n?,2,video|}, . . . . . . , {|n?,m,text|, |n?,m,image|, |n?,m,audio|, |n?,m,video|}}  where |n?,i,text|, |n?,i,image|, |n?,i,audio| and |n?,i,image| indicates the number of text, image, audio and video respectively that is pointed by n?,i . Finally, the n tuple {|n?,i,text|, |n?,i,image|, |n?,i,audio|, |n?,i,video|} is transformed into a weight-set of the form {{..., {wr,text, wr,images, wr,audio, wr,video}, . . .}, where, each of them indicates the weight of video, audio, images and text of the rth node respectively. The weight indicates how much of a task it would be for the system to download that particular class of item. Weight is a value greater than 0 and is formalized as follows:  wx,k = ?maxcount j=1 ((SizeOf(xj))  (?current))  where wx,k is the weight of class x in node k, Sizeof(xj) denotes the size of the item number j of class x in node k, and ?current indicates the average current bandwidth levels. Simi- larly, we find the corresponding value wx,k,threshold which is the same formula, but, with ?current = ?threshold, which is the threshold bandwidth defined by the system designer. Based on all these parameters, we now select a combination of the  weights from wx,k whose value is below the threshold. This list of all wx,k is now called the to-be-cached list.

The prefetching model forms a crucial part of the antici- patory caching interface. On a cloud, this prefetching model could be either embedding into the cloud controller or could be placed at the cloud?s network interface.

C. Pseudocode We now give the pseudocode for the semantic prefetching  model described in Section III-B. The detailed code for the flow of events is shown in Algorithm 1, which explains the working of the prefetching model from the activity of data collection to the generation of the final list of downloads.

while true do1 if EventTriggered then2  CalculatePriorities(); //Calculate the order of3 Priorities aggregating users requests CollectData(); //Identify and collect related data4 items pertaining to the user request CheckRelevance(); // Calculate Relevance of the5 collected Data Items ListOfData[] = RankData(); //Rank the data items6 in the order of relevance. The array ListOfData contains the list of these ranked data.

while i < ListOfData do7  if !checkAvailablity() then8 //Check the availability of data items;9 Reprioritize(); //reprioritize, if bandwidth10 is not adequate else11  Enqueue(PrefetchQueue); //enqueue12 the final list into the PrefetchQueue  end13 end14  end15 while TestBandwidth() == true do16  //check if bandwidth levels is above threshold17 Prefetch(PrefetchQueue); //begin prefetching18 of the data items.

if isFull(cacheDatabase) then19  //Check if the cache is full20 runLFU(); //Run the cache replacement21 algorithm StoreDataItem(cacheDatabase); //store the22 prefetched data in the cache else23  StoreDataItem(cacheDatabase); //store24 the prefetched data in the cache  end25 end26  end27 end28  end29 Algorithm 1: Pseudocode for Anticipatory Caching Engine  Our model is implemented by means of a trigger event. At lines 1?2, a trigger event is generated by users? data requests for data retrieval, or it could be a service call by a computer system after gathering the users? requests for data retrieval.

At line 3, the priorities assigned by the users to various types of data items such as video, audio, image, and text are looked into and are ordered in decreasing order. For example, if most users choose videos as their first priority followed by text, images and audio, then the order would be, videos, text, images and audio respectively.

Generally, data items are indexed by keywords or context identifier(s) to indicate topic(s) they are related to or pertain to.

At line 5, a Semantic Link Network (SLN) is built considering the related data items to the search item. The construction of the SLN is explained in the semantic prefetch model (see Section III-B).

At lines 7?15, the relevance of data items is checked and they are ranked appropriately according to the order of relevance based on their semantic relation to the search item.

The ACE also checks if any of the data items that are to be prefetched are already cached or otherwise. If they are already cached, then the ACE checks if there is a newer version of that data available since the last prefetch. In addition, at lines 16?18 we also determine if there is adequate bandwidth to prefetch the data items at that instant in time.

Meanwhile, at line 10, the data items are reprioritized depending on their sizes, priority and availability. Re- prioritization is needed because there might not be enough bandwidth to download all the data items in the list obtained from the earlier step because the data items that could be prefetched are dependent on the available bandwidth to the data source. In case of inadequate bandwidth, the list of data items to be prefetched are reprioritized to meet the bandwidth requirements. The list of the items are then put into a Queue called the Prefetch Queue, which holds the list of data items to be prefetched.

At lines 19?26, it is checked if the cache database is full or otherwise. If the cache database is full, some of the data items are purged by running a Least Frequently Used (LFU) cache replacement strategy to accommodate the prefetched data items. The download details serve as a means of understanding the difference between the initial list and the final list of downloads and assist the ACE in taking any further decisions by observing the download patterns, to determine what kind of items could be more relevant than the rest.



IV. PERFORMANCE ANALYSIS  The performance of the model is measured by the response time and the cache hit percentage, with and without caching at different bandwidth levels. There is always a trade-off between the size of the cache database and the size of items cached. The cache size required would of course grow, should the number of data items to be cached increase. The goal is to find the best model that can provide accurate predictions while maintaining a comparatively small cache size.

In order to verify our model, an application was designed and implemented using Perl, the Eucalyptus cloud framework, Freebase data and Freebase JavaScript APIs. These technolo- gies can be summarized as below.

? Eucalyptus cloud framework: Eucalyptus is an open-  source software infrastructure for implementing cloud  computing on clusters. It is implemented using commonly-available Linux tools and basic web-service technologies, making it easy to install and maintain.

(Available at http://eucalyptus.cs.ucsb.edu/.)  ? Freebase data source: Freebase contains more than 10 million topics and more than 3000 types. These topics in Freebase are very intricately connected, which means that topics in different domains (politics, business, sports, edu- cation, etc.) are linked together, spanning across virtually any combination of tables. The data in Freebase include images, documents, hyperlinks, videos, audios, etc., along with the hypertexts. (Available at www.freebase.com).

? Freebase JavaScript APIs: Freebase is not only a web site that people can use directly with their browsers, but is also a collection of web services that web applications can use.

Fig. 3: Semantic Linked Network of Data Set Under Consid- eration  Node# Property  0 Freebase 1 Websites 2 Facebook 3 Founders 4 Social Network 5 Startup 6 Cassandra 7 Mark Zukerberg 8 Dustin Moskovitz 9 Harvard Univ.

10 Programmer 11 Entrepreneur 12 Washington DC 13 Google 14 Programing Lang.

15 Perl 16 Larry Wall  TABLE I: Node Numbers And Their Properties    Simulation was carried out using data from Freebase as a data source. the data included images, documents, hyperlinks, videos, audios, etc. The simulations were carried out over a predetermined sequence of queries. The Semantic Link Network that was under consideration for the queries is as shown in Figure 3, and the details of the node properties are as shown in Table I. The exact sequence of the queries in the form of their path is specified in Table II.

The details of response time to access the data items with and without caching, the number of requests made to the data source in both scenarios, and other details have been calculated as shown in Figure 4 and Figure 5. Figure 4 shows the overall size of the data items cached against the Bandwidth TimeLine, and Figure 5 explains the variations in response time in accessing data items, with and without caching at low bandwidths.

Query# Path of Data Item  1 Freebase.com 2 Freebase/ Facebook 3 Freebase/ Facebook/ MarkZuckerberg 4 Freebase/ Facebook/ MarkZuckerberg/ Harvard 5 Freebase/ Facebook/ MarkZuckerberg/ Harvard/  Cambridge 6 Freebase/ Facebook/ DustinMoskovitz 7 Freebase/ Facebook/ DustinMoskovitz/  Programmer 8 Freebase/ Facebook/ DustinMoskovitz/  Programmer/ LarryWall 9 Freebase/ Facebook/ DustinMoskovitz/  Programmer/ LarryWall/ Perl 10 Freebase/ Facebook/ DustinMoskovitz/  Programmer/ LarryWall/ Perl/ Prog. Lang.

TABLE II: Query Numbers And Their Path  Fig. 4: Size of Items Cached Versus Bandwidth TimeLine  It may be seen that the graph in Figure 4 has three plotted curves. The thick solid curve represents the number of items cached across the timeline by using the proposed method, the  Fig. 5: Response Time At Low Bandwidths Versus Bandwidth TimeLine  dotted curve represents the number cached across the timeline by using only the browser caching mechanism, and finally, the thin soild line represents the trendline of the bandwidth in Kbps (a 3 second moving average). The graph is such that the graph of the number of items cached is superimposed by the bandwidth timeline graph. The units and the values of the y- axis of the bandwidth timeline is shown explicity on the right hand side of the graph.

The graph in Figure 5 depicts the plot of response time versus the bandwidth timeline. It also has three plotted curves, with the thick solid curve representing the response time across the timeline without using the proposed method, the dotted curve the response time across the timeline by using the proposed mechanism, and finally, the thin solid line the trendline of the bandwidth in Kbps?again, a 3 second moving average. The graph is again such that the number of items cached graph is superimposed with the badnwidth timeline graph. The units and the values of the y-axis of the bandwidth timeline is shown explicity on the right hand side of the graph.

The difference in response times with and without caching at low bandwidths may be observed from the above; and so also, subsequently, the increase in performance with caching.

It may be noted from Figure 5 that the average response time at high bandwidths, without caching, is about 2.26 seconds, while that with caching is about 1.5 seconds, showing a 47.50% decrease in response time. It may similarly be noted from Figure 4 that the average number of items cached is consistently in sync with the changes in bandwidth (i.e., the the bandwidth, the more the number of data items cached).

The increase in performance is quantified by the percentage of requests that have to be made to the server and the response time to access the data items. The above results clearly show an about 48% decrease in response time in case of high bandwidths and a 31.25% decrease in response time in case of low bandwidths, when used with caching. The proportion of server hits in both cases is about 21.5% with caching compared to 75.39% without caching. These results indicate that by using semantic linking and then pre-fetching, the    latency encountered could be reduced by a significant margin.

It also demonstrates an increase in the cache-hit percentage, which indicates that data items are being acquired before they are requested. In addition, the pre-fetchers accuracy is increased according to the client access frequency, since with the increase in the number of requests, the patterns could be observed more clearly.



V. CONCLUSIONS AND FURTHER WORK  This paper presents a model that extrapolates the users? data access requests by using a semantic linking over the data source to provide them with a better performance system regardless of network variations, code details and data access patterns. The proposed model is also bandwidth-adaptive, in the sense that it adapts to varying bandwidths and varies its caching capacity correspondingly. It is found to be successful in prefetching the data search items from the data cloud with good accuracy and relatively little overhead. Moreover, this model can also be used over a data source that contains dynamically changing data items by using the concept of merely comparing the update time of the content with that of the one in the cache.

Currently, we are investigating the use of various types of server-side statistics to promote or demote semantic links that will adapt to the overall access behavior of the web-application clients. In addition, we are considering possible adjustments that are to be made to the current algorithm when the data source contents are rapidly changing. We aim to find the costs that are to be incurred in such situations and then try to reduce them by using a hybrid version of this technique.

