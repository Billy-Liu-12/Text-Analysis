Association Rule Mining with Modified Apriori  Algorithm using Top down Approach

Abstract?Data Mining is a field of computer science that is concerned with extracting useful information from varied sources. In an era where information has become the inherent necessity of human beings, its increased relevance and usefulness has taken focus as need of the hour. The most important part of this association rule mining is the mining of item sets that are frequent. Market basket analysis is done by companies in order to retrieve itemsets that are frequent and often used together by customers. Apriori algorithm is a widely used technique in order to find those combinations of itemsets. However, when any of these frequent itemsets increases in length, the algorithm needs to pass through many iterations and, as a result, the performance drastically decreases. In this paper, we propose a modification to the apriori algorithm by using a hash function which divides the frequent item sets into buckets. Further, we propose a novel technique to be used in conjunction with the apriori algorithm by eliminating infrequent itemsets from the candidate set. In this top down approach, it finds the frequent itemsets without going through several iterations, thus saving time and space. By discovering a large maximal frequent itemset very early in the algorithm, all its subsets are also frequent hence we no longer need to scan them. Clearly, the proposed technique has an advantage over the existing apriori algorithm when the most frequent itemset?s length is long.

Keywords?Data mining, Aprori algorithm, Hash function, Association rule mining, Maximal frequent itemset

I. INTRODUCTION  The most widely used application of association rule mining is market basket analysis. The buying habits of all customers are analysed by finding correlations and associations between all the various items that have been bought by the customer. The knowledge and discovery of these correlations will help retailers and shopkeepers come up with marketing strategies. They will gain an insight into the items that are frequently bought together by a customer. For eg, if a customer buys milk, it will tell the retailer what is the tendency of the customer to buy bread as well. And what kind of bread also. In one trip to the market, what is the probability that the customer will buy both? Or other combinations or groups of items will the customer buy on a trip to the store. This information is very useful and can help the shop achieve greater sales by helping retailers market certain products and also organize their shelf space. They may keep certain products together. This market analysis is also performed on the sales data of the customers  buying things at the shop. The results can help change advertisement strategies or in designing a new catalogue for the store. It can help you try out different layouts of a store. In one style, all items that are bought together frequently can be placed next to each other to drive sales up of these combination of products. All customers who are purchasing computers also have a tendency to buy antivirus software along with it. So by that logic, keeping the hardware units in close proximity to the software display may help augment the sales of both items. In an opposing strategy, by keeping hardware and software displays at opposite ends of the shop may entice customers who are buying these items to pick up and buy other items put on display along the way and increasing sales. For eg, a customer buy an expensive computer, he wants to buy an antivirus for it but while walking, the home theatre system was on display. So he picks up this as well. Market basket analysis helps retailers decide the items to be put on sale at a discounted price. If it is noticed that customers buy computers and printers together then discounting the price of printers may increase the sale of both, printers and computers. All associations are grouped into 3 categories today.

1. Frequent pattern: A pattern consisting of a set of items, that occurs frequently together in a data set. For instance, milk and bread which have a correlation and are frequently bought together is a frequent item set.

2. Subsequence, is a itemset where the order matters. If a customer buys a PC first and then followed by a printer is called a frequent sequential pattern if they have a strong correlation and appear frequently.

3. Substructure refers to various structural forms like subgraphs, subtrees, or sub-lattices in a form of itemsets or subsequences. If it occurs frequently, it is a frequent structured pattern.



II. RELATED WORK  There has been a lot of development in the area of association rule mining. Abhijit Sarkar and Apurba Paul used a tree based approach in order to find useful patterns. Their technique is similar to FP growth algorithm which involves building a tree and performing association discovery on the nodes. Apriori algorithm being one of the most widely used algorithms faces numerous drawbacks. Hence many alterations were proposed to improve the efficiency and reduce the time.

[2] Agrawal came up with AIS algorithm which created candidate item sets on-the-fly on each pass of the database scan. Any itemets which are large from previous pass are checked if they are there in the current transaction.  Ke Sun and Feng Shan Bai also propose a unique technique to mine association rules without assigning any weights to the itemsets.

[3] Concept maps is another technique for finding frequent itemsets. The current method to construct concept maps considers only the rules of the questions which are not answered. It misses some information about about questions that are answered correctly. Shyi-Ming Chen [4] proposes a technique to find association rules which doesn?t build extraneous relationships or break relationships between concepts in concept maps.



III. ASSOCIATION RULES  Let I = (I1, I2,..., Im) be an itemset. Let D be a dataset of all the transactions involved in the calculation. Each transaction T is marked with a unique identifier TID. Let A be a set of items.

An association rule is an implication of the form A => B. The rule A => B in the transaction set  has a support s, where s is the percentage of transactions in D that contain A U B. It is the probability, P(A U B). The rule A -> B has confidence c in the transaction set D, where c is the percentage of transactions in D containing A that also contain B. This is taken to be the conditional probability, P(B/A).

Rules that satisfy both, a minimum confidence (min_conf) and support threshold (minsup) have a correlation and are called strong. All support and confidence values occur betwwen 0% and 100%.

A. Defining Support and Confidence A set of items is called an itemset. An itemset that contains  k items is a k-itemset. The set {printer,computer } is a 2- itemset. The support count of an itemset is the number of transactions that contain the itemset.

If the support of an itemset satisfies a pre-specified minimum support  threshold,  then it is a  frequent itemset.



IV. ASSOCIATION RULE MINING  Step 1: Here we try to find all itemsets that are frequent.

Each of these itemsets will be in more number of transactions as the min support count.

Step 2: Here we generate strong association rules from the itemsets which are frequent in the above step. All these rules must be above the minimum support and should be above the minimum confidence.

A. Problems in mining frequent itemsets A major hindrance and challenge in retrieving and mining these itemsets from a huge data set is that the outcome of mining often generates a large number of itemsets, all which satisfy the minimum support (min sup) threshold, especially when minimum support is set low. This is due to the apriori property which states that if an itemset is frequent and above threshold then each of its constituents will be frequent as well.

Solution: Mine closed patterns and max-patterns.

1) Closed Patterns An itemsets is closed if none of its proper supersets has an  equal support as it has. An itemset X is called a closed frequent itemset if X is both frequent and separately closed in the dataset.

For eg, If Freq. Pat.: = {A:3, B:3, D:4, E:3, AD:3}  From the above set {A:3,}  cannot be  closed frequent since  its superset { AD:3} has same support.

Closed set C= {B:3, D:4, E:3, AD:3}  2) Max Patterns An itemset X is called a max-pattern if X is frequent and  there is no more super-pattern that is frequent. Suppose that a database has only two transactions:  {a11, a12, : : : , a110}, {a1, a2, : : : , a50}.

Let the minimum support count be 1.

We find two closed frequent itemsets and their support counts C={a1, a2, : : : , a100}:1, {a1, a2, : : : , a50}: 2  There is only one maximal frequent itemset: M={ {a11, a12, : : : , a110} :>1}  We cannot include {a1, a2, : : :  ,  a50}  as  a  maximal frequent itemset because it  has  a  superset which is frequent, {a1,  a2,  :  :  :  , a100}.



V. APRIORI ALGORITHM  A frequent itemset and all its subset will be frequent. This is the apriori property. If an item-set is below the minimum support threshold, minsup, then it is not frequent. Now if an item A is added to the itemset, then also the final itemset will not be frequent as it will not occur frequently than the min sup count. Hence, I U A will not be frequent either, that is, P(I U A) < minsup. This is called as downward closure property. Eg: If { PC, printer, cartridge} is frequent, so is {printer, cartridge}.

A. Scan Let T be the set of transactions in the database. In this step  we first generate the candidate itemset. This is done by pairing and forming 2-itemsets. Then we count the number of times these itemsets occur in each transaction.

B. Prune Since the number of candidate itemsets can be large in  number, this leads to heavy computation. To reduce the size of     candidate itemset, we prune those itemsets which  occur below the minimum support. When the frequent itemsets from the database is achieved, it is simple to generate strong association rules. All these rules agree both minimum support and minimum confidence.

C. Illustration Total number of transactions:15  Min Support = 20%  Min Support threshold = 20*15/100=3  TID List of Items  1 1,5,6,8  2 2,4,8  3 4,5,7  4 2,3  5 5,6,7  6 2,3,4  7 2,6,7,9  8 5  9 8  10 3,5,7  11 3,5,7  12 5,6,8  13 2,4,6,7  14 1,3,5,7  15 2,3,9    Scan 1  Item set Support Count  1 2  2 6  3 6  4 4  5 8  6 5  7 7  8 4  9 2    Prune 1  Itemset Support Count  2 6  3 6  4 4  5 8  6 5  7 7  8 4    Scan 2  Itemset Support Count  Itemset Support Count  {2,3} 3 {4,5} 1  {2,4} 3 {4,6} 1  {2,5} 0 {4,7} 2  {2,6} 2 {4,8} 1  {2,7} 2 {5,6} 3  {2,8} 1 {5,7} 5  {3,4} 1 {5,8} 2  {3,5} 3 {6,7} 3  {3,6} 0 {6,8} 2  {3,7} 3 {7,8} 0  {3,8} 0    Prune 2  Itemset Support Count  {2,3} 3  {2,4} 3  {3,5} 3  {3,7} 3  {5,6} 3  {5,7} 5  {6,7} 3       Scan 3  Itemset Support Count  {2,3,4} 1  {3,5,7} 3  {5,6,7} 1    Prune 3  Itemset Support Count  {3,5,7} 3    The data contain frequent itemset X ={3, 5, 7}. Association rules are retrieved from X. The nonempty subsets of X are {3, 5, 7}, {I3, I5}, {5, 7}, {3, 7}, {3}, {5} and {7}.

The final association rules along with their confidene are listed below:  Association Rules Confidence  {3,5} -> 7 3/3 = 100%  {3,7} -> 5 3/3 = 100%  {5,7} -> 3 3/5 = 60%  3 -> {5,7} 3/6 = 50%  5 -> {3,7} 3/8 = 37.5%  7 -> {3,5} 3/7 = 42.8%

VI. LIMITATIONS OF APRIORI  We see that the apriori algorithm functions in a bottom-up, breadth-first search method. The computation starts from the smallest set of frequent itemsets and moves upward till it reaches the largest frequent itemset. The number of times the algorithm passes a database is equal to the largest size of the frequent itemset. When an itemset becomes longer and it is frequent then algorithm becomes slower and takes a performance hit. There are many methods to enhance the efficiency of the algorithm. Either we can remove a transaction that does not contain any frequent k-itemset. It is being useless in subsequent scans. Or we could  partition the itemset that is potentially frequent. It must be frequent in at least one of the partitions of DB. We could also mine on a sample of the dataset, compromising on accuracy but improving efficiency.



VII. MODIFIED APRIORI ALGORITHM  A popular enhancement to the apriori algorithm is the FP tree algorithm. By reducing the costly database scans, it decomposes the dataset into smaller ones. We focus here on a hash based technique which divides the dataset into parts using a hash function.

Hashing itemsets into its buckets: Here,a hash-based technique is used to decrease the size of the candidate k- itemsets, Ck, for k > 1. For instance, when scanning the dataset to generate itemsets which are frequent of size 1, we can generate the itemsets which are frequent of size 2 and hash (i.e., map) them into various buckets of a table structure, and count the corresponding bucket counts. An itemset of length 2 with a corresponding bucket count in the hash table which is below the support threshold cannot be frequent and is should be removed from the candidate set. This technique may substantially decrease the number of candidate k-itemsets examined (especially when k = 2).

TID List of Items  1 1,2,5  2 2,4  3 2,3  4 1,2,4  5 1,3  6 2,3  7 1,3  8 1,2,3,5  9 1,2,3    H(x,y)=((order of X)*10+(order of Y))mod 7  ???????   ?????  ? ?? ?? ?? ?? ?? ??  ??????? ??????  ?? ?? ?? ?? ?? ?? ??  ??????? ?????? ?????? ?????? ?????? ?????? ?????? ??????  ???????? ?????? ?????? ?????? ?????? ?????? ?????? ??????  ??????   ?????? ??????  ??????   ?????? ??????    For eg, the minimum support count is 3, then the itemsets in buckets 0, 1, 3, and 4 are not frequent and are removed. They should not be included in C2.



VIII. PROPOSED TECHNIQUE  We propose a method which tries to find the frequent itemsets in a bottom-up manner as well as a top down manner.

It maintains a list of al itemsets. While passing through the database, it counts the support of these large candidate itemsets to see check if they are actually frequent. In such an event, we know that all the subsets of these frequent sets are going to be frequent and so we can remove them from the list to be scanned. This will increase our performance. If we are lucky, we may discover a very large maximal frequent itemset very early in the algorithm.

Consider a pass k,which signifies the length of itemsets to be searched. If some itemset that is a maximal candidate itemset, say X, if frequent then all its subsets and consituents must be frequent as well. Hence, all of its subsets should be pruned from the set to be scanned. In this procedure,all candidate sets considered in the bottom-up direction in the pass are removed.

If this set subsumes all the candidate sets of level k, then we need not proceed further and thus we save many database passes. Clearly, the proposed technique has an advantage over apriori algorithm when the largest frequent itemset is long.

In every step, we prune all items whose support is less than  the min sup count from the maximal candidate itemset. Let us call the maximal candidate itemset M. After generating candidate itemsets, we divide them into 2 categories. One whose support is above the minimum support threshold go into category L and ones whose support is below the threshold into category S.

Using the same example we used in Apriori algorithm, we count the number of times each item appears in the set of transactions. Min Support = 20%= 20*15/100=3.

Itemset Support Count  1 2  2 6  3 6  4 4  5 8  6 5  7 7  8 4  9 2    Then we prune all itemsets in S from the maximal frequent itemset M. For each item in the itemset, we remove it from M in order to generate more maximal frequent itemsets.

Eg: S={a,b} M={a,b,c}  After pruning we get, M={ {b,c},{a,c} } Dividing the itemset into L and S, we get M={1,2,3,4,5,6,7,8,9}  L={2,3,4,5,6,7,8} S={1,9}  Prune S from M  M={1,2,3,4,5,6,7,8,9} => {2,3,4,5,6,7,8}  Next, we generate 2-itemsets and divide it into itemsets above and below the support threshold, into L and S respectively.

Itemset Support Count  Itemset Support Count  {2,3} 3 {4,5} 1  {2,4} 3 {4,6} 1  {2,5} 0 {4,7} 2  {2,6} 2 {4,8} 1  {2,7} 2 {5,6} 3  {2,8} 1 {5,7} 5  {3,4} 1 {5,8} 2  {3,5} 3 {6,7} 3  {3,6} 0 {6,8} 2  {3,7} 3 {7,8} 0  {3,8} 0    L={ {2,3},{2,4},{3,5},{3,7},{5,6},{5,7},{6,7} }  S={{2,5},{2,6},{2,7},{2,8},{3,4},{3,6},{3,8},{4,5},{4,6},{4,7 },{4,8},{5,8},{6,8},{7,8} }  Now our M = {2,3,4,5,6,7,8}  We proceed by dividing M based on S. We first see {2,5}.

First we remove 2 from M to get {3,4,5,6,7,8}. Next we remove 5 from M to get {2,3,4,6,7,8}.

{2,5}  =>  {2,3,4,5,6,7,8}  =>  Remove  2  =>     {3,4,5,6,7,8}  {2,5} => {2,3,4,5,6,7,8} => Remove 5 => {2,3,4,6,7,8}  Now M={ {2,3,4,6,7,8},{3,4,5,6,7,8} }  Next we see {2,6}.

{2,6} => {2,3,4,6,7,8} => Remove 2 => {3,4,6,7,8}  {2,6} => {2,3,4,6,7,8} => Remove 6 => {2,3,4,7,8}  We  won?t  divide  {3,4,5,6,7,8} as it  doesn?t  contain 2  from {2,6}  Now M={{3,4,6,7,8} , {2,3,4,7,8} , {3,4,5,6,7,8} }  But since {3,4,6,7,8} is a subset of {3,4,5,6,7,8}, we use the superset in this top down approach.

Using this approach we keep on pruning S from M in order to arrive at the maximal itemset M.

S M  {2,3,4,5,6,7,8}  {2,5} {3,4,5,6,7,8} {2,3,4,6,7,8}  {2,6} {3,4,5,6,7,8} {2,3,4,7,8}  {2,7} {3,4,5,6,7,8} {2,3,4,8}  {2,8} {3,4,5,6,7,8} {2,3,4}  {3,4} {4,5,6,7,8} {3,5,6,7,8} {2,4} {2,3}  {3,6} {3,5,7,8} {4,5,6,7,8} {2,3} {2,4}  {3,8} {3,5,7} {4,5,6,7,8} {2,3} {2,4}  {4,6} {3,5,7} {5,6,7,8} {4,5,7,8} {2,3} {2,4]  {4,7} {3,5,7} {5,6,7,8} {4,5,8} {2,3} {2,4}  {4,8} {3,5,7} {5,6,7,8} {4,5} {2,3} {2,4}  {5,8} {3,5,7} {6,7,8} {5,6,7} {4,5} {2,3} {2,4}  {6,8} {3,5,7} {7,8} {5,6,7} {4,5} {2,3} {2,4}  {7,8} {3,5,7} {8} {5,6,7} {4,5} {2,3} {2,4}    Hence we get M= { {3,5,7} {8} {5,6,7} {4,5} {2,3} {2,4} }  From this we see we have 2 3-itemsets, {3,5,7} and {5,6,7}.

We calculate its support and find that {3,5,7} has a support of 3 whereas {5,6,7} has a support of 1 which is below the threshold. Hence the frequent itemset is {3,5,7} and all its subsets are also frequent.

Using the proposed technique, we know {3,5,7} is a frequent itemset and all its subsets will also be frequent. Hence we can prune all itemsets containing 3,5 and 7. This saves a lot of space as well as time while determining frequent itemsets. One of the major shortcomings of the apriori algorithm is that we have to scan the database very frequently. It increases the time and decreases the performance of the database. We designed this algorithm to take this fact into account and minimize the time spent on passing through a database giving us a better outcome and result.



IX. CONCLUSION  We see that the apriori algorithm operates in a bottom-up, breadth-first search method. The computation starts from the smallest set of frequent itemsets and moves upward till it reaches the largest frequent itemset. The number of times the algorithm passes a database is equal to the largest size of the frequent itemset. When an itemset becomes longer and it is frequent then algorithm becomes slower and takes a performance hit. We proposed a modification to the existing apriori algorithm which reduced the number of iterations required hence increasing performance. Further, we proposed a new technique to increase efficiency and reduce the time taken for generating frequent itemsets. This top down approach maintains a list of candidate itemset to reduce the number of database scans. Through this technique we were able to significantly reduce the time required to aquire frequent itemsets. Hence increasing efficiency of the entire algorithm.



X. REFERENCES  [1] Han J, Kamber M. Data Mining : Concepts and Techniques.Higher Education Press,2001  [2] Abhijit Sarkar, Apurba Paul, Sainik Kumar Mahata, Deepak Kumar,Modified Apriori Algorithm to find out Association Rules using Tree based Approach  [3] R.Revathi, M.Geetha, Re-Modified Apriori Algorithm in E-Commerce Recommendation System  [4] Shyi-Ming Chen, Shih-Ming Bai, Using data mining techniques to automatically construct concept maps for adaptive learning systems  [5] Ke Sun and Fengshan Bai, Mining Weighted Association Rules without Pre-assigned Weights  [6] Han, Pei,Y Yin and R Mao. Mining Frequent Patterns without Candidate Generation:A Frequent-Pattern Tree Approach. Data Mining and Knowledge Discovery,2004  [7] Jong S P,Ming S C,Philip S Y.An effective hash based algorithm for mining association rules. In Proceedings of the Management of Data.2005  [8] Kong Fang , Qian Xue-zhong, Research of improved apriori algorithm in mining association rules, Computer Engineering and Design, 2008  [9] Li Qingzhong , Wang Haiyang, Yan Zhongmin, Efficient mining of association rules by reducing the number of passes over the database, Computer Science and Technology,2008  [10] Geetha,Sk. Mohiddin , An Efficient Data Mining Technique for Generating Frequent Item sets, International Journal of Advanced Research in Computer Science and Software Engineering Volume 3, Issue 4, April 2013 ISSN: 2277 128X.

[11] R. Agrawal, T. Imielinski, and A. Swami, ?Mining Association Rules between Sets of Items in Large Datasets,? Proc. ACM SIGMOD ?93, pp. 207-216, 1993.

