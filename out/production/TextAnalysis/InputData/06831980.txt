Accelerating Intersection Computation in Frequent Itemset Mining with FPGA

Abstract?Frequent itemset mining is an important researching area in data mining and Eclat is a typical and high performance frequent itemset mining algorithm. However, the large numbers of sorted-set intersection computation in the algorithm limit the performance of the algorithm seriously.

FPGA is a low-power and high-performance computing platform that has been applied to accelerate parallel data mining successfully. To deal with the problem of the large number intersection computation in Eclat, this paper proposed a FPGA solution to accelerate the intersection computation.

And a full comparison matrix structure is provided to perform the parallel intersection computation. And a hardware hashtable method is also proposed to eliminate the data dependency in the intersection computation. The experiment results show that our solution can achieve a speedup of 26.7x on intersection computation comparing to the best software implementation existed, and the hardware hashtable method can achieve a speedup of 103x. The full comparrison matrix have a better scalability, thus the entire running time of the Eclat algorithm can be decreased extremely.

Keywords-Parallel Data mining; FPGA acceleration; Frequent itemset mining; Sorted-set intersection; Full Comparison Matrix; Hardware Hashtable

I. INTRODUCTION Associating rule mining is an important kind of data  mining application, while its core is frequent itemset mining.

Eclat Algorithm in [1] is a kind of frequent itemset mining approach. Compared to typical Apriori algorithm in [2], the candidate itemset generation and the support value computation in Eclat are more convenient and efficient, and the performance of Eclat is one magnitude better than Apriori. Unlike the methods used in Apriori and FP-Growth [3] that using horizontal data format to express the relationship between transaction set and itemset, Eclat adopts vertical data format to express the itemset, make the support value computation of each itemset require only the its corresponding transaction set, rather than repeating traverse of the entire database, so that the support value computation of the candidate set could be finished in a short time.

Meanwhile, the analysis [4] of Eclat shows that the occupancy of sorted-set intersection during the computation will increase rapidly while the support value is declining. So the speed of intersection of two sorted-sets is the key point affecting the performance of frequent item mining algorithm.

Meanwhile, sorted-set intersection is not only the core of frequent item mining, but also the basis of the inquiring  processing in database and information retrieval, e.g.

inverted index table intersection [5] in information retrieval and RID-list merging [6] in database searching. The low delay of response processing is a key measurement in these applications, so how to decrease the intersection time is very important.

In recent years, FPGA that contains massive logic resource, flexible on-chip memory, large off-chip memory and high flexible programmable and low power features, is not only the verify device for ASIC, but also being applied to very large scale parallel structure to implement the customized computing functions according to certain application. Moreover, FPGA is applied more and more in accelerating data mining applications.

On the background of Eclat, the frequent itemset mining algorithm mentioned before, this work analyze the algorithm in detail, and proposed a parallel speculative prefetching strategy and a full comparison matrix structure that is fitting for hardware implementation for the large amount of serial intersection computation in the Eclat algorithm. The structure is implemented in FPGA and the implementation results show that the structure required only the basic comparators to achieve full comparing of two sets of data in one cycle. Besides this, we also propose a hardware hashtable to eliminate the data dependency in the traditional set intersection computation. Meanwhile, this work also estimate the performance of the structure in various support value and different size of full comparator matrix with the same test data set of frequent item mining, and the overhead of the hardware resource was also showed.

The rest of this paper is organized as follow: Section 2 introduced the related works of using FPGA to accelerate frequent item mining algorithm and basic data mining operation, and the recent researches of parallelization on accelerating intersection computation; Section 3 analyze the runtime of Eclat algorithm, the feature of serial intersection computation process, and the proportion of each operation; the parallel mechanism and hardware structure adopted in intersection computation are proposed in Section 4; Section 5 show the strategy of hardware hashtable and the detail structure.Section 6 show the performance estimation result of the structure proposed; the last section concluded our work.



II. RELATED WORK This section briefly introduces the related works of  accelerating frequent item mining algorithm and basic data  on Embedded and Ubiquitous Computing  DOI 10.1109/HPCC.and.EUC.2013.98   on Embedded and Ubiquitous Computing  DOI 10.1109/HPCC.and.EUC.2013.98     mining operation with FPGA, as well as the recent researches of parallel intersection computation:  A hardware structure that implements the data path with systolic array is presented in [7], to accelerate Apriori Algorithm for the first time. In this structure, the data input needs to be divided into multiple times while comparing a long candidate item in the structure, reducing the performance of the accelerator. At the same time, the observation of the algorithm showed that, there was just little difference between two candidate items generated continuously. So an improved structure was proposed in [8].

The structure was added with a bit mapping CAM, to map the first data of the candidate item and the indexes of the rest items into the starting address of the CAM and Bitmap separately, in order to decrease the input times and increase the parallelizing comparing speed of items. A Systolic tree is proposed to accelerate the FP-Growth algorithm in [9], and to simulate the data stream process inside FPtree and map the process to hardware directly. The hardware structure achieved a speedup of 24x comparing to software implementation while the number of frequent items was no more than 10, but the performance of software implementation was better than that of hardware implementation while the number of items is increasing, since it was too many for the hardware to generate larger scale systolic tree. At the same time, it would make a large number of PE idle and waste hardware resources while dealing with sparse transaction database.

Besides the researches on accelerating the entire data mining algorithm, there are a great number of works on hardware accelerating the operations with high frequency and high time occupancy. Since counting the items in data stream is one of the basic operations in frequent item mining, [10] propose a hardware scheme for the counting operation.

The scheme applied content memory to simulate the software process, achieving a speedup of 3x comparing to software implementation. [11] present an optimized FPGA design of merging sorting network for the sorting computation which appears frequently in data mining.

Meanwhile, the design was applied to accelerate the median finding operation in database.

Up to now, numbers of parallelized intersection computation methods on general processors and graphic processors were presented. [12] applies SIMD instructions to implement intersection operation on Intel processor, but the parallelized operation was limited by the bit width of the SIMD instructions in the general processor, making the improvement to be limited. Meanwhile, the operation by SIMD supported only 2-set of intersection operation, that limited its application range.

In the thread-level parallel approach, [13] propose a dynamic detecting method to decrease the range of binary searching in intersection computation. The method is applied to multi-core processor to improve the load balancing in parallelized processing. [14] propose an intersection computation method for multi-core processor based on thread pool. The method utilize skip list to divide each set into multi subsets, and the subsets are processed in parallel by the threads in the thread pool. [15] propose a batch  method which gathered a set of data to be intersected on CPU, and then issue them to GPU to complete intersection operation, so that take advantage of the high parallel of GPU.

[16] propose a data format called BATMAP for intersection computation on GPU. The idea of this method is that generate the hash value for each element and find out whether the hash values of the elements in two sets are at the same position to decide whether the element belong to the intersection result. Although this method achieve a high efficient performance on GPU, it requires a previous computation and generation for the hash table.



III. ECLAT ALGORITHM INTRODUCTION AND ANALYSIS Some definition for frequent itemset mining is as follows  [1]: let I  be the set of items, and T a database of transactions, where each transaction has a unique identifier tid  and contains a set of items. Let X be an itemset, and X I? , and an itemset with k items is called k-itemset.

Let Y be a tidset, that it consist of several tid andY T? . The support of X is denoted as supp(X) , and it is the number of transactions in which it occurs as a subset. An itemset is frequent if its support fulfills supp(X) min_supp? , and the min_supp is a user-specified value.

:1 [ ], :  (1) ([ ]) : (2) [ ] : (3) [ ] , :  (4) ;  (5) ( ) ( ) ( );  (6) ( ( ) ) (7) ; (8)  i  j  i j  i j  i i  Input itemset P min_supp Output Set C of frequent itemsets and support  Eclat P for all X P do  for all X P and j i do F X X T F T X T X if supp F min_supp then  C C F end fo  ?  ? ? ?  ?  ?  ? ?  ;  ( );(((((  ;  (9) ( ) (10) ( ); (11)  i  i  r if C then  Eclat C end for  ?  Algorithm1. Eclat Algorithm  Transcation Items  1 A C T W  2 C D W  3 A C T W  4 A C D W  5 A C D T W  6 C D T  Horizontal layout  A C D T W  1 1 2 1 1  3 2 4 3 2  4 3 5 5 3  5 4 6 6 4  5 5   Vertical layout  Figure1. The Horizontal Layout and Vertical Layout  Eclat algorithm breaks the original search space into independent sub tree based on the equivalence class     transformation, and adopts a depth first search approach to enumerate the frequent itemset, the specific algorithm flow as show in algorithm1. This algorithm is based on vertical tid-list database layout that as shown in figure1. The left part of figure1 is the traditional horizontal data layout that used in Apriori and FP-Growth, and the right part is the vertical data layout that an itemset X has a corresponding tidset Y , for example, itemset {A} is corresponding to the tidset {1,3,4,5}.

As the line (5) in algorithm1 shows, the tidset of new itemset is generated from the intersection of tidset of two old itemset.

And the support count for an itemset will not need to traverse the database like Apriori, but rather count the number of elements in its tidset. For example, we want to compute the support value of itemset {A,C}, first we should get the tidset of {A,C} by intersecting tidset of {A} and tidset of {C}, we get the tidset of {A,C} is {1,3,4,5} and can compute its support value is 4 easily.

Some definition about sorted-set intersection are as follows: Given a collection of N sets 1{ ,..., }NS L L? , where  iL ? , ? is the universe of elements in the sets. Let | |i in L?  be the size of set iL Goal is to compute  1 2 ... kL L L2 ... kL L2 ... kk  as fast as possible. In this paper, we only consider the situation that k=2.

In the implementation of Eclat algorithm [4], the author adopts the basic merge algorithm (actually this is a high performance approach [5]), and the pseudo code as shown in the algorithm2:  (1) while (i_a<La&&i_b<Lb) do { (2)      if(A[i_a] == B[i_b]) { (3) C[count++]=A[i_a]; (4)         i_a ++; i_b ++; } (5)     else { (6)         if(A[i_a] > B[i_b]) (7)            i_b = i_b ++; (8)         else (9)            i_a = i_a + +; } } (10) return C  Algorithm2. The Merge algorithm for sorted-set intersection  We simply introduce the algorithm 1 for investigate its algorithmic features, and omit the initial part. The input of this algorithm is two sorted sets and output a sorted-set that contains common values of both input sets. Let aL  and bL  is the length of sorted-set A and sorted-set B respectively. aindex  point to a data in sorted-set A and  bindex point to a data in sorted-set B. Whenever the two compared data that pointed by aindex  and bindex  is equal, the data is common value and two index will self-increment.

If the two compared data is unequal, then only one index will self-increment (this is depend on the order in set is ascending  or descending). We can observe that the increment of index depend on the equivalence relationship between two compared data, and this is the intrinsic data dependency of this algorithm. When any sorted-set is traversed completely, the algorithm is end. And this algorithm has a O(M+N) complexity, the worst case is to execute 1a bL L? ? comparison, and the best case is execute ( , )a bMin L L comparison.

The implementation of Eclat in [4] is generally regarded as state-of-the-art, and the latest version is v4.0 (2013-04-04).

We run this program to analyze the features of Eclat and get its execution time details. The test tool is Intel Vtune Amplifier 2013, and test platform is Intel Core I7 920 Quad- core processor with 3.0GHz. The test data we adopted is T40I10D100K (15MB), which has an average of 40 elements in a transaction over 100,000 entries. The test results in table1 demonstrate that the intersection computation is the most time consumption part in algorithm, and the percentage of intersection computation time is increase with the decrease of support value. So, it is imperative to accelerate the intersection computation with parallel approach to reduce the overall execution time of Eclat algorithm.

TABLE I. ECLAT ALGORITHM EXECUTION TIME ANALYSIS  Support Runtime(s) All Intersect Read Preprocess other  5% 6.704 4.69(70%) 0.479 0.691 0.844 2% 15.204 12.997(85%) 0.514 0.703 0.989 1% 24.328 21.898(90%) 0.484 0.757 1.189 0.5% 56.912 52.624(93%) 0.555 0.763 2.797  In order to further analysis the behavior of sorted-set intersection in Eclat algorithm, we statistic the input length for every Intersection function call in program, and count the amount of Intersection function call times. And the result is show in table 2, we can see that the intersection function call times is increased with the decrease of support value but have a gradual trend. This indicates the fact that the more candidate itemset is pruning with lower support value, and the reason for the shorter input set length is same.

TABLE II. THE STATISTIC OF INPUT SORTED-SET LENGTH AND INTERSECTION FUNCTION CALL TIMES IN ECLAT ALGORITHM  Support Ave Source Length Intersect TimesS1 S2  5% 10610 6907 45152 2% 7685 3974 185745 1% 4189 2145 284635 0.5% 1484 984 550703

IV. PARALLEL STRATEGY AND HARDWARE ARCHITECTURE  From the serial algorithm for sorted-set intersection, we can observe that the execution time of algorithm depends on the comparison times between two data elements that pointed by its index respectively. When find a common data the index of both sets will self-increment, otherwise only the one index will self-increment. Since one index can only point one data element, this scalar approach is inefficient relatively.

So the most straightforward parallel strategy is to reduce the number of scalar comparison in a fixed set length, and make an index point multiple data elements as much as possible.

Then, we can find the common values from these two groups of multiple data elements. More specifically, we can load the N successive data elements that are located in the back or front of the data pointed by index, and perform a full comparison for these 2N data elements. In spite of some unnecessary comparisons is introduce by this approach, but we can use the special instruction or more hardware resource to implement a full comparison operation as soon as possible, and this performance speedup can offset a great part of overhead of the unnecessary comparisons.

This straightforward parallel strategy can be easily implemented in parallel form on General Purpose Processor with the SIMD instruction, such as the SSE instruction set in Intel processor. The implementation work with SIMD instruction is proposed in [12], and it called this strategy as speculative execution. However, the degree of data parallelism of SIMD instruction is limited, for example one SIMD instruction can only process 4 32bit integer data once.

What?s more, the full comparison performed by SIMD instruction worked in a cyclic shifts mode to obtain the common values, computing one full comparison need 8 cycles at least. So the acceleration effect of SIMD approach for intersection computation is limited.

(1) while (addr_a End&&addr_b End) do { (2)      Load  VectorA from BRAM Group A; (3)      Load  VectorB from BRAM Group B; (4)      VectorI = ComparatorMatrix(VectorA, VectorB); (5)      if(Vecto  ? ?  rA[N-1] == VectorB[N-1]) { (6)         addr_a = addr_a + N; (7)         addr_b = addr_b + N; } (8)     else { (9)         if(VectorA[N-1] >VectorB[N-1]) (10)             addr_b = addr_b + N; (11)         else (12)             addr_a = addr_a + N; } (13)    Store VectorI to BRAM Group I; }  Algorithm 3. Parallel merge intersection algorithm in FPGA  Fortunately, we can leverage the dedicated hardware provided by FPGA to implement this strategy, and we called it as speculative prefetching parallel computation. Firstly, FPGA allow a high degree of data parallelism for its parallel properties and high memory bandwidth. Secondly, the abundant logic resources can implement the full comparison among multiple data elements in as few cycles as possible.

Last, circuit in FPGA can be easily implemented in a pipeline form and make the data prefetching and full compare in parallel, thus we can obtain a high throughput.

Aimed at the speculative prefetching parallel computation in FPGA, we can design a flow as this:  1, Parallel load the data elements of two different sorted- set from memory modules to two data vectors;  2, Send the these two vectors of data element to full comparison unit to find the common values, and update the index according the last data in vector;  3, Write back the common data to the memory modules.

The specific algorithm execution in hardware circuit is  described in algorithm3: As shown in the parallel algorithm, the parallel  intersection computation is composed with data load, parallel full comparison and data write back. The most important part is the full comparison and the index update (belong to data load part). In this paper, we proposed a full comparison matrix structure to perform the parallel intersecting of two groups of data elements. In addition, we also proposed a data generator to control the index updating that guarantee the right data load.

CP CP CP  CP CP CP  CP CP CP          Reg 2  Reg 1 Reg 2  Reg N   Reg 1 Reg N  M ux out 1  M ux out 2  M ux out 3  Bit Reg  Reg  Bit OR  Bit Reg  Bit Reg      Bit OR  Bit OR  Reg  Reg  Figure2. Full Comparison Matrix Structure  Reg_ a  Reg_b  Comparator  +Addr_a 16'h0001  M U  X  +Addr_b 16'h0001  M U  X  Figure3. Structure of Address Generator  The full comparison matrix can perform a pairwise comparison for a NxN data elements matrix, the function of this structure is described as (2)-(4) in alogrithm3. As the figure 2 shows, this structure is a comparator matrix, and the data sources of this matrix are VectorA and VectorB that is consist of N 32bit register respectively. In our full comparison matrix, one input port of the comparators in each row is one data elements Ai in VectorA, and the another input port is the B0 to Bn in VectorB. The output of every     comparator is a Boolean value, 1 is two data elements equal and 0 is unequal. We buffer this Boolean value for one cycle with Bit register, and input it into a Bit OR unit. Each row has one Bit OR unit that has N input port and perform an OR operation with these N Boolean value, and if this Bit OR unit output value 1 that means that the Ai valid and output a strobe signal to get the common value through the Mux unit.

Each row owns N comparators, a Bit OR unit and a Mux unit.

Figure3 is the structure of address generator, and the function is described as (5)-(12) in algorithm3. The input is two last data element of VectorA and VectorB, and the output is two address values for the BRAM Group.

According these two address valus, we can read the correct data from BRAM Group to update the content of VectorA and VectorB. If the value in the Reg_a is equal to the value in the Reg_b, the both address increment for N data elements.

If the value in the Reg_a is larger to Reg_b, then the addresses for VectorA increment for N data elements, otherwise the address for VectorB increment.



V. THE HARDWARE HASHTABLE FOR INTERSECTION COMPUTATION  From the algorithm of Merge, we can oberseve that the increment of index depends on the relationship of two compared data, and this intrinsic data dependency of algorithm cannot avoid. This algorithm includes a branched structure in itself and causes the uncertainty in the read the data during the execution of program, and from the result of our experiment we can see that the number of comparision is always towards the worst case.

Set A a1 a2 a3 a4 a5 aN  Key_a1 Key_a2 Key_a3 Key_a4 Key_a5 Key_aN  0 1 2 3 4 ...

Key_b1 Key_b2 Key_b3 Key_b4 Key_b5 Key_bM  b1 b2 b3 b4 b5 bM  Hash(A)  Set B  Hash(B)  HashTable          Figure4.The mapping between Set and Hashtable  We propose a method based on hashtable to improve the performance of intersection computation. And this idea is used in the pattern match for several years [17][18]. The original idea of this method is that each data in different data set can be mapped to a hashtable, and we can check the duplication of data written and confilict in each location of hashtable to determine whether the two sets contain the same elements. As the figure4 shown, we assume two set A and B, and A contains N integer elements, and B contains M integer elements. We use the same hash function to the data in these two set, and we can see a3 in set Awill be written in the 3rd location in the hashtable after computation. And the b2 in set  B will be written in the 3rd location too. This situation shows that the written conflict of 3rd location represents the two same data if they are completely hash value. So, we can obtain the common data in the two set througth this idea.

The computation flow is that each set can respectively compute the hash value of its elements from the first one, and can only check the location of hash value. This can avoid the stall in the data reading. In the software implmentation, this check cannot be completed in parallel way, but the hardware can complete it based on the customized multi-port memory. Meanwhile, this method can apply to the unsorted data for the reason that the hash value is not need to be sorted too. The execution of this method is can be predicted, the number of comparsion is the Max(La,Lb).

Based on the full comparison matix, we propose a structure of hardware Hashtable as shown in the figure5. It includes an update logic module and a HashMem that is used to store Hashtable. When the full comparison output the common data of two sets, the unequal data will output to the Updata Logic, and these unequal data will be computed to the hash value. Here, we use the data as the address value to reprsent the hash value, and the data can be computed in the form of pipeline.

Comparison Matrix  Update Logic  Hash Mem  Intersection Data  Intersection Data  Unequal Data  Figure5.Structure of Hardware HashTable  The figure 6 is the detailed structure of Updata logic, that we can use the input data as the address value for the read and write of hash mem. Every address of hash mem store a Boolean value, if the read value of the input address is 1, then this demonstrate that the value of this address is a intersection data. If the read value of the input address is 0, then this demonstrate that the value of this address is not exist, we should update it ,and write a value of 1 to this address.

Reg  M U  X Hash Mem  R_addr  R_data  W_addr  Reg  W_en  W_data  Intersection Data  Unequal Data     Figure6.Structure of Update Logic

VI. EXPERIMENT AND EVALUATION All the hardware blocks are programmed with Verilog  and simulated with Modelsim 6.6F, synthesized using Xilinx ISE13.3. The experiment board is Xilinx ML605, and the FPGA chip is Virtex LX240T-1, and use the DDR3 SDRAM DIMM to store the test data. DDR3 SDRAM peak bandwidth is 6.4GB/s, and can provide 256 bits input data per cycle at 200MHz frequency. The address generator loads the two sets data from DDR3 SDRAM to two BRAM Groups in the Ping-Pong operation.

DDR3 SDRAM  Comparator Matrix  Addr Generator  M em  or y  In te  rf ac  e  BR AM  M od  ul e  In te  rs ec  tio n  Ve ct  or  LX240T  Figure 7. FPGA acceleration system  In order to compare the performance of software, we adopt T40I10D100K data set to evaluate our hardware architecture. The intersection computation speedup of FPGA device over a CPU is as the ratio of the execution time requirements, and we called it Local Speedup:  Software execution timeLocal Speedup Hardware execution time  ?  And the Eclat algorithm speedup of FPGA device over a CPU is defined as:  *Overall Speedup Local Speedup Intersection percentage?  Table 3 shows the execution time of software versions mentioned in section 3 and our hardware implementation.

The support is 0.5% to 5% respectively, and we choose an 8x8 full comparison matrix to process all of these intersect computation. The hardware run at 200MHz. the results show that the Local Speedup increase as the support value decrease, and it can achieve 11.7x at the 0.5% support. And the Overall Speedup can be 10.88 x. The reason for this situation is that the low support value can generate short set for intersection computation, and the trend of occurrence times of intersection in Eclat is slow.

TABLE III. THE EXECUTION TIME AND SPEED UP RESULT FOR DIFFERENT SUPPORT VALUE WITH A 8X8 MATRIX  Support Hardware Execution Time(s)  Speedup Local Overall  5% 0.77 6 4.2 2% 2.04 6.4 5.44 1% 2.61 8.4 7.56 0.5% 4.5 11.7 10.88 In order to evaluate the scalability of our full comparison  matrix, we design three different size matrixes, 8x8, 16x16  and 32x32 respectively. Since ML605 cannot provide enough input data bandwidth for the latter two matrixes, and the simulation results of these two matrixes are using Modelsim 6.6F and also run at 200MHz. We still use T40I10D100K data set, and choose the 1% support for these three size matrixes. The hardware execution time is showed in table4, and we can see that the Local Speedup increase in a near-linear way as the increase of matrix size. And the Local Speedup can achieve 26.7x with a 32x32 matrix. The best speedup of SIMD implementation in [12] is 5.3x, and our design has an obvious advantage.

TABLE IV. THE EXECUTION TIME AND SPEED UP RESULT FOR DIFFERENT MATRIX SIZE WITH 1% SUPPORT VALUE  Matrix Size  Hardware Time(s) Speedup Local Overall  8x8 2.61 8.4 7.56 16x16 1.56 14 12.6 32x32 0.82 26.7 24 We synthesis these three size matrixes with ISE 13.3, and  the FPGA chip resource consumption summary and maximum frequency are showed in table 5. As we can see that even the largest size matrix only consume 8% logic resource and 21% Block RAM of LX240T. The longest signal path determines the maximum frequency at which the overall circuit can operate. And with the increase of matrix size, the maximum frequency has a slight decrease, for the reason that the longest path between input port and output port of each comparator is fixed. And only the logic delay of Bit OR unit has a little increase because the input signal doubled, but this have a small influence to the overall performance. So these three full comparison matrixes can all run at 200MHz on FPGA chip.

TABLE V. FPGA CHIP RESOURCE CONSUMPTION SUMMARY  Matrix Size  Slices BRAM 36K  Maximum FrequencyRegisters LUT  8x8 596 928 14 281.928MHz 16x16 1310 3466 44 277.778MHz 32x32 3102(8%) 12602(8%) 86(21%) 274.217MHz Table 6 shows execution time of software versions  mentioned in section 3 and our hardware implementation based on the hardware hashtable method. The support is 0.5% to 5% respectively, and we choose an 8x8 full comparison matrix to process all of these intersect computation. The hardware run at 200MHz. the results show that the Local Speedup can achieve 103x at the 0.5% support.

That means we have an 8 times performance improvement due to eliminate data dependency.

TABLE VI. THE EXECUTION TIME AND SPEED UP RESULT FOR DIFFERENT SUPPORT VALUE WITH A HARDWARE HASHTABLE  Support Hardware Execution Time(s)  Speedup Local Overall  5% 0.29 15.9 11.1 2% 0.89 14.7 12.5 1% 0.75 29 18.9 0.5% 0.51 103 96

VII. CONCLUSION AND FUTURE WORK In this paper, we presented a FPGA acceleration solution  for the massively intersection computation in Eclat algorithm, which is a classic frequent itemset mining algorithm. And design a full comparison matrix structure to process the parallel intersection computation based on a speculative prefetching strategy. Based on this matrix structure, we also design a hardware hashtable to improve it. As the experiment results show that, our solution can achieve 6x to 26.7x speedup under different support value for intersection computation in Eclat algorithm. And the hardare hashtable also can achevie a 8 times improvement to the orginal matrix structure. We believe that not only frequent itemset mining could be accelerated with our proposed approach, but also the query processing or information retrieval area, in which set intersection occupy a large part of execution time. In future work, we want to exploit the intrinsic parallelism of Ecalt algorithm, for example to parallel multiple intersection operation based on our approach in this paper.

