VIRTUALIZATION TECHNOLOGY AND PROCESS CONTROL SYSTEM  UPGRADES

ABSTRACT  Growing numbers of applications for replacement Process Control Systems is the evidence that the lifetime of these assets is not as long as our industry was used to in the last century. This unsatisfactory trend is the reverse of the ?cost saving by standardization-medal?, because standardization in the field of IT comprises no more than 5 years. High cost in the case of an upgrade is consuming the initial cost saving by far.

Personal Computer (PC) technology which predominated last two decades is -from many perspectives- neither efficient nor cheap. Main-frame-based and smart Start-up companies recognized those deficits and moved efficient resource utilization and eased system administration back into the focus. They called the rediscovered technology ?Virtualization?.

In a nutshell, with Virtualization, Information Technology today is moving back from decentralized to more centralized structures. The main target group of this trend is certainly the ?classic? IT, because main benefits are becoming important in big data centers and large IT environments. A by-product of virtualization is the possibility to run ?obsolete? applications on up-to-date hardware. This feature of virtualization attracted the attention of process automation.

Different industries and automation system vendors are aware of the benefits of virtualization and they are started using it for both engineering and on-line process control system purposes.

The migration from physical to virtual platforms turned out to be smooth and numerous cement plants are either already using virtualization or are considering implementing it.

The price to win is an ?in principle- unlimited extension of the lifetime of the existing Process Control System.

INTRODUCTION  In the early 1990ies HMI components of Process Control Systems left starting with the proprietary and vendor specific platforms and moved to ?standard? IBM compatible Personal Computer with Microsoft operating systems. The main motives for this choice were cutting hardware cost, achieving hardware independency and easing administration by standardization. The downside appeared years later - hardware and software support for ?standard hardware? turned out to be much shorter than industry was used to.

Jhampton Text Box    Cement plants which installed PCS ten years ago are applying for upgrades of their control systems today, mainly because of obsolete hardware. Ten years is just almost half the lifetime of process control systems which our industry installed in the 1980ies and early 1990ies..

The nuisance of short lifetime cycles of Windows based control system is amplified by engineering efforts.

Conversion of outdated process control systems to the latest version is in most of the cases partly or sometimes even completely impossible.

Analyses show that roundabout two third of process control system upgrade cost are accounting for engineering and commissioning and only one third are expenses for hardware, licenses and installation.

Beside those investment-related issues the continuous pressure to increase the efficiency of our plants is a challenge to our maintenance departments. Any relieve and simplification of work is welcome.

This paper consists in principle of two main parts.

Part number one illuminates the technical background and working principle of Virtualization Technology.

Knowledge on this is important on one hand to be able to understand and classify arguments of technical discussions and on the other hand to assess chances for success for future implementation.

Part number two covers mainly administrational aspects and features of virtualized computer environments and outlines a potential approach for process automation installations towards virtualization.

UNDERSTANDING VIRTUALIZATION  The idea of real-hardware virtualization is not new. already in the 1960ies IBM performed research work in this field on S360 computer hardware.

The primary goal of this work was to build a ?Multi user system? in order to achieve a much better utilization of the formerly extremely expensive hardware instead of using the hardware in batch mode.

Multiple users should have access to the computing power at the same time.

Picture 1: Underutilized computer resources initiated virtualization technology   The main efforts of the development at that time was to achieve proper memory management, because ?Virtualization? means at least that ?Operating System? and applications are working in well protected and separated areas of the memory.

The IBM engineers even took one more step forward. They did not only virtualize the memory but the entire computer. The main purpose was a better user separation.

In the 1970ies virtualization was used in professional applications. IBM renamed the system to VM/370 in 1979. VM/370 consisted of two main parts. The Virtual Machine Monitor (VMM) and the Console Monitor System (CMS).

The VMM runs on the physical hardware and virtualizes this hardware in such a way that a number of ?virtual machines? can run on one physical computer.

The Console Monitor System (CMS) provided the actual Operating System functionality. CMS was a single user system and is built upon the VMM. There were clear interfaces between CMS and VMM.

Every privileged instruction by a virtual machine caused on a VM/370 a "trap" - an error - as it was trying to execute a "resource management" instruction while running in a less privileged ring. The VMM intercepts all these calls and emulates the instruction, without jeopardizing the integrity of the other guests. In order to improve performance, the developers of the guest OS and VMM (both at IBM) tried to minimize the number of traps and reduced the time required to take care of the various traps. The principle design of IBM is still used for virtualization products today.

X86 ? MICRO AND INSTRUCTION SET ARCHITECTURE  OS kernels in modern computer systems (e.g. IA-32 Architecture of Intel) are usually running in the privileged Ring 0 (Kernel-Mode or Protected-Mode, additional modes are just supported for compatibility reasons) and have access to the entire instruction set of the CPU and the whole memory.

A (protection) ring is one of two or more hierarchical levels or layers of privilege within the architecture of a computer system. This is generally hardware-enforced by some CPU architectures that provide different CPU modes at the firmware level.

Rings are arranged in a hierarchy from the most privileged (most trusted, usually numbered zero) to the least privileged (least trusted, usually with the highest ring number). On most operating systems, Ring 0 is the level with the highest privileges and it interacts most directly with the physical hardware such as the CPU and memory.

Normal user processes (application programs) are running in the underprivileged Rings 1 to 3 (user mode) due to security reasons. Those rings provide fewer instructions so that software which is running there cannot process certain tasks directly.

If a user mode process intends to accomplish a job which is only possible with higher privileges (lower Ring), like for instance accessing the hard disk drive or other hardware, this is only possible by calling the kernel (system call) and initiate a ?context-change?. A process hands over the control of the CPU to the kernel, and is halted until the call is completed and the kernel returns the result. The user mode process is leaving at no time the de-privileged ring and cannot interfere with any other user process and cannot be a threat to the stability of the kernel, because only trusted code may run on Ring 0.

A system call may send information to the hardware, the Kernel itself or other processes, but may also want to receive information from them. To enable this it is necessary to copy data out of the private memory domain of the kernel into the memory domain of a process. A normal process does not have any access privileges to this data.  Application programs in this way have access to its data and the result of e.g. system calls.

x86 CPU hardware actually provides four protection rings: 0, 1, 2, and 3. Typically only rings 0 (Kernel) and 3 (User) are used.

Picture 2: Privileges and protection structure of a CPU   [1] To be precise: x86 CPUs support 3 modes of operation: Protected Mode, Real Mode, Virtual 8086 Mode    Unfortunately, virtualization as developed by IBM was not possible on x86-CPUs as the 32/64-bit Intel ISA does not trap every incident that should lead to VMM intervention. Consequently, the conclusion in the 1990ies was that x86 cannot be virtualized in the way the old mainframes were virtualized. Other CPU ISAs such as PowerPC and Alpha are ?clean? enough to be virtualized in the classic manner.

VIRTUALIZATION OF X86 COMPATIBLE CPUS  Seventeen instructions of the x86-architecture which are classified as ?sensitive? cannot be intercepted, because the ISA knows Ring-depending-instructions, which work differently depending on the context of their execution mode. All of those critical instructions are affecting either i/o or memory operations which are executed in kernel mode This fact would increase the complexity of a x86-VMM drastically.

Unlike IBM mainframes, where the VMM simply utilizes a CPU flag of a sensitive instruction in order to intercept critical calls, the VMM of a x86-CPU has to scan the entire code prior to execution for sensitive but non-interceptable calls.

This method is called ?Scan before Execution? (SBE) or shortened: Prescan.

Prescan or "Scan Before Execution The VMM translates the binary code which the kernel of a guest OS wants to execute on the fly and stores the adapted x86 code in a Translator Cache (TC). User-Mode applications will not be touched by a Binary Translator (BT) because the user code is considered to be safe. User mode applications are executed directly as if they were running natively.

Kernel-Mode instructions are intercepted and altered by the VMM and deferred to virtual hardware.

Changed kernel code is slightly longer than the original one. If the kernel of the guest OS has to run a privileged instruction, the BT will change this kind of code into "safer" user mode code. If the kernel needs to get control of the physical hardware, the BT will replace that binary code with code that manipulates the virtual hardware.

The technological core competence of companies which provide virtualization technology is to minimize binary code translations and code overhead in order to keep latencies down.

Essentially, software virtualization techniques move the operating systems (kernels) at a less privileged ring than normal.

The software layer that performs the interception and translation of kernel mode calls is (again) called Virtual Machine Monitor (VMM) or Hypervisor.

The VMM is a controlling instance in between the hardware and virtual machines. A hypervisor runs directly on the host hardware in Ring 0 and offers resources to ?Guest Virtual Machines?. The advantage of the hypervisor solution is that operating systems of the virtual machines remain (almost) untouched and therefore still execute privileged operations as though running in ring 0 of the CPU.

Furthermore, modern hypervisors provide interfaces for higher level administration and monitoring tools.

Virtualization by hypervisor, sometimes also named ?Full Virtualization?, delivers ?compared to pure hardware virtualization or even system emulation- much higher processing performance.

I/O Virtualization and Memory Management Virtual memory addresses need to be converted into physical ones in case of usage of virtual memory.

X86- compatible CPU perform this in three steps. (Three tables LDTR (Local Descriptor Tables Register) IDTR (Interrupt Descriptor Tables Register) und GDTR (Global Descriptor Tables Register). The OS maintains page tables to translate the virtual memory pages into physical memory addresses. All modern x86 CPUs provide support for virtual memory in hardware.  Different CPUs (ARM, PowerPC, MIPS etc.) work in a similar manner.

Translation from virtual to physical addresses is performed by the Memory Management Unit, or MMU.

The current address (instructions and data!) is available in the CR3 register (hardware page table pointer). The most used domains of the page table are cached in the Translation Look-Aside Buffer (TLB).

It is obvious that a guest OS running on a virtual machine cannot have access to the real page tables, because this lead to uncontrolled collisions. Instead, the guest OS sees page tables which run on an emulated MMU.

Every time the guest OS modifies its page mapping, the virtual MMU module will capture (trap) the modification and adjust the shadow page tables accordingly.

Depending on the virtualization technique and the changes made in the page tables, this bookkeeping takes 3 to 400 times more cycles than in the native situation   DIFFERENT FORMS OF VIRTUALIZATION  Paravirtualization The kernel of the Guest operating system is ported under Paravirtualization to run on a hypervisor.

With Paravirtualization hardware is neither emulated nor virtualized. Critical CPU-instructions which directly access hardware need to be modified in the Guest OS. An interception like mentioned in the chapter before is thus obsolete. But as a result, Paravirtualization is restricted to operating systems like Linux with openly available source code.

The advantage of Paravirtualization is that ported guests allow a much simpler design of a virtual machine (VM). The ability of the guest kernel to communicate directly with the hypervisor results in greater performance levels than in any other method of virtualization.

The latest commercial Hypervisors and Guest OSes of Microsoft are designed to cooperate as Para- virtualized guest and Hypervisor.

Finally, the hypervisor?s job is reduced to simple scheduling tasks for resources of the virtual machines.

Hardware Accelerated Virtualization: Intel VT-x and AMD-V The main problem with the application of virtualization software is the performance loss compared to a bare OS installation. Pure emulation of the computer hardware is very inefficient. Performance losses of 50% and more are not unusual.

The idea behind hardware virtualization has -as already outlined- its origin in the fact that the x86 instructions architecture cannot be virtualized. Hardware virtualization is based on the philosophy of trying to intercept all exceptions and privileged instructions by forcing a transition from the guest OS to the VMM, called a "VMexit". Trapped instruction would furthermore be processed by the VMM.

The idea to improve virtualization performance by executing more instructions than before natively on the CPU was not far to seek. AMD and Intel developed independently (AMD-V, Intel VT) instruction set extensions to overcome constraints of the old x86-ISA. Both extensions are not compatible but they are based on the same principles.

To put it simply, we can say that above all the formerly management and separation of kernel-calls is managed on silica instead of software. But in addition to this AMD and Intel implemented also mechanisms on their CPUs that allow guest OSes to modify memory page tables in order to decrease the emulation efforts of the VMM.

The improvement is based on the induction of a ring with a privilege level higher than 0. Guest OSes run at the originally intended privilege level (Ring 0). The CPU-cycle-consuming ?World-Switch?, which handles the processing of ring 0 calls, which arrive in a VMM, is avoided.

Picture 3: Additional Layer (Ring -1) of CPUs wit virtualization support   Native Virtualization thus provides certain parts of the physical hardware in the form of virtual hardware to the Guest OS.

This is why -compared to the host performance- only 60-80% of the theoretical performance of a CPU can be achieved by virtualization.

VIRTUALIZED COMPUTER INFRASTRUCTURES  Initially, IBM invented Virtualization to extend a single task system to multitasking systems. A modern hypervisor transfers the old ideas to state of the art hardware, considering the entity of an OS with all installed software as ?Guest? or task.

Vendors of virtualization software developed many sophisticated features and tools around the core of virtualization in order to the range of additional value for virtualization users.

Physical to Virtual - P2V Before any virtualization can commence, it is necessary to find ways to convert an existing physical machine into a VM. Conversion is a procedure of transferring a physical computer with all of its hardware, the installed OS and applications into a ?Virtual Machine?.

The definition of a Virtual Machine according to Forrester Research is as follows:  A virtual machine is stored as a set of files in a directory in the DATASTORE. A virtual disk inside each virtual machine is one or more files in the directory. As a result, you can operate on a virtual disk (copy, move, backup,?) just like a file. New virtual disks can be ?hot added? to a virtual machine without powering it off. In this case a virtual disk file (.vmdk) is created in a Virtual Machine File System (VMFS) to provide new storage for the hot added virtual disk or an existing virtual disk file associated with a virtual machine.

Vendors in the field of virtualization software are offering software tools for P2V purposes. Physical machines can be virtualized on the fly, i.e. while they are running and performing their usual work. Other methods are using boot-CDs, hard disks or hard disk images to convert a computer. Furthermore back-up file formats of most important back-up solution providers are supported as base for a P2V conversion.

Virtual Infrastructure The real benefits of virtualization become possible with the implementation of a virtual infrastructure layer.

Virtualization software vendors coined the term ?cloud computing? for this issue. One of these vendors even claims to offer the first ?cloud computing operating system?.

Picture 4: Set of host computers administrated by a Virtualization Layer with numerous "Guests?   The infrastructure layer abstracts hardware to a very high degree. The layer considers no individual elements of hardware but it pools all available rescores instead. The virtual infrastructure would provide the aggregate of RAM of MHz to potential guests. All features regarding redundancy and high availability    are configured and maintained within the ?Administration layer?. The ?administration layer? itself can even just be a Guest in the virtual environment.

Along with the implementation of the administration layer the storage philosophy needs to be adapted to benefit from virtualization. Virtual machine files should be no longer physically located on a hard disk of an individual server, but on a common Storage Area Network (SAN) instead. Now the critical point of failure has changed in virtualized environments from the hard-disk of an individual server to the SAN.

Flexibility Virtual machines handled by system management software must not continuously reside on an initially assigned computer. In fact a ?Guest? can move around while still providing all service from one physical machine to another. This feature enables hardware maintenance without risks and interruption of computer operation in a very simple way.

A growing demand of resources by additional applications or increased machine load can be covered by adding new machines and distributing the Guests differently in the extended cluster.

This process can even be automated, meaning that load balancing between computer resources can be realized without manual intervention (if the hardware is already in place).

Rapid provisioning and backup As said before, a virtual machine is in essence just a file on file system. This file represents a computer with everything in it, to run on a virtualization host.

This hardware independence enables administrators to create clones of working images for roll out purposes.

In case of tests of hot fixes or updates, snapshots of the running machine can be taken to which one can revert in case a test fails.

Backups are simplified a lot, because third party tools are almost obsolete.

Quick recovery After a failure ?no matter hard- or software caused- a virtual machine can reboot automatically elsewhere in the virtual environment in order to keep the service. As this type of quick recovery causes interruptions to computer operation it is not suitable for process automation purposes.

Failover Real failover has only been available in commercial products since approximately two years. In principle, it consists of a foreground Guest VM with a ?Shadow-VM? of this Guest running in the background on different hardware of the resource pool. The shadow VM is an identical bit-copy of the foreground machine. It would become activated in case the foreground machine suffers a hardware breakdown.

Both features, Failover and Quick Recovery make use of the mobility of Virtual Machines in administrated environments.

The applied principle to achieve failover is highly comparable with system backup software which is capable of backing up system partitions while the system is running.

The working principle is a method which takes a snap-shot at a point in time and monitors the deviation to the snap shot while the initial snap shot is taken. The procedure is applied as long as the delta file size (between snapshot and data to be copied) is zero. At this moment the Copy-VM takes over.

The dead time for taking over by the Shadow-VM is just some (we only found one) IP packets - thus just some milliseconds.

But beware: Breakdowns caused by ?buggy? software are not covered by this technology, because the shadow-VM would crash as well.

Impact on automation layout in a cement plant Next picture shows a generic control system layout as used at HeidelbergCement with logical segregation of production stages and an ?automation backbone? which connects subordinate control systems.

Each control system consists of a redundant set of servers for failsafe reasons.

Picture 5: Standard PCS layout   Process control systems are typically installations which are subject to evolution rather than to revolution.

Most cement plants would never do a ?one-shot replacement? of an installed system (or version) by another but proceed in steps.

Logically, the approach towards virtualization is logically very similar to PCS upgrades.

The reason for the reference plant which introduced Virtualization Technology in 2008 for online control purposes were hardware failures and obsolete hardware for Windows-NT-based-CEMAT-V4-control systems.

The first machine to be virtualized was not ?mission critical? but represented in principle the entire functionality of CEMAT V4. It was an engineering station which was set up as a combined ?server-client- engineering station?.

Almost at the same time, this plant introduced a Process Data Acquisition System (Management Information System). According to the vendor?s specification, the whole system should be installed on 5 different computers (Com-Server, OPC Server, Database Server,?). Although it was obvious that it could be done on fewer machines the plant decided to follow the vendor?s specification because of warranty issues. However, setting up 5 machines for a relatively unimportant job was considered to be excessive, and as the virtualization platform was available we utilized it for the PDAS installation.

Due to the positive experience after step one, it was decided, to migrate also one PCS Server of the redundant server-set into a VM and install it to a host. The process control system software runs untouched on both machines and realizes failover between a physical and a virtual machine.

The decisive steps at our pilot plant were also done at a relatively early point in time (immediately after the tests with the engineering station were completed) in September 2008.

The infrastructure software layer was installed ?although we could not use it extensively- almost at the very beginning, i.e. with the first host, in order to give our staff the opportunity to become familiarized with this new (to automation people) technology.

Picture 6: PCS layout with virtual infrastructure and SAN.

The second very important step was the installation and commissioning of a Storage Area Network (SAN), because only a network storage enables to make use of virtualization infrastructure features such as failover. This is due to the fact that virtual machine disk files may not exist on a certain host, as in case of failure it would not be accessible and available any longer to be restarted on a different physical host.

SANs are either based on Fibre Channel or iSCSI Technology. The idea of both technologies is to extend the (usually) internal data lines between hard disk controllers and hard-disks to external devices.

Fibre Channel has been a proven technology for this type application for years. It has a slightly better performance, but is more expensive than iSCSI.

iSCSI stands for Internet Small Computer System Interface. It means encapsulation of the SCSI protocol within TCP/IP packages. Today iSCSI delivers sufficient performance to build up SANs for process automation purposes at a cement works.

In such an arrangement, from a failure point of view, the SAN is the most critical peace of hardware in virtualized computer architecture.

Although SANs are built up internally fail safe (redundant power supply, NICs, hard disk controllers,?) and data sheets give impressive figures for availability, any concept for process control purposes should allow for setting up auto-synchronizing SANs at different spots.

User interface VMs running on a host are comparable to applications running on a Terminal Server. In other words, VMs obviously need input and output devices to be handled. Fat clients (PC?s) are no good choice if it is intended to simplify the maintenance of the process control system hardware. Moreover, they are costly and sometimes prone to error.

Another step forward -with limited risk only- was to replace some operator stations of a segment by Thin Clients without hard disk.

The connection between thin client and VM is either done by vendor specific client software, RDP or VNC.

Obstacles Minor issues may occur while transferring physical control systems into virtual ones.

Most of the problems the reference plant encountered had to do with the hardware interfaces such as serial and parallel ports and multi-media features.

Unresolved points (minor) are to date realizing failover with legacy interfaces and sound output with Windows NT guests.

CONCLUSION Virtualization is no new but a revitalized technology. First virtualization solutions (purely based on software) as well as computer emulation deliver poor results in terms of performance.

Hardware supported virtualization and pure software virtualization are totally different approaches.

Virtualization today is most powerful when it combines the strengths of the different technologies. CPU demanding applications are wasting less performance than memory-intense application. (TLB miss costs!).

VMware ESX is a good example of this. ESX uses Para-virtualized drivers for the most critical I/O components (but not for the CPU), emulation for the less important I/O and Binary Translation in order to avoid the high "trap and emulate" performance penalty. In this way, virtualized applications perform quite well, in some cases almost as if there were no extra VMM layer involved.

The user benefit of virtualization is manifold. Virtualization  ? decouples software from hardware (provides availability for ?legacy platforms?, enables software in a cloud)  ? utilizes installed hardware more efficient ? safes space, energy, and invest cost ? eases system administration extensively  ? but requires additional know-how of system administrators   At this point in time we can say that virtualization has met all of our expectations and offered ?in particular with respect to system administration- even more. The number of plants that started using Virtualization is growing.

The process automation staff is still on a learning curve with virtualization in order to take maximum benefit of this technology.

Unexploited potential is likely to be found in the field of setting up systems, handling of back-ups and disaster recovery procedures.

