Proceedings of ICCT2013

Abstract: Stream data is a very common data type in big data and in many data streams applications, users tend to pay more attention to the mode information of the data streams. So mining frequent patterns in data streams is a significative work. Meanwhile, finding frequent itemests in a data set with predefined fixed support threshold could be seen as an optimization problem. In this paper, the problem of frequent itemsets mining is derived as a non-linear optimization problem, then genetic algorithm is adopted to solve it. Through the formal and bitmap representation of frequent itemsets, the non-linear optimization problem is transformed to 0-1 programming. A set of experimental results show that unlike typical Apriori algorithm, the complexity of time and memory space grows exponentially as the support decrease, our proposed algorithm has a high time and space efficiency even with a very low support.

Keywords: Big data, data streams, genetic algorithm, frequent itemsets  1  Introduction Data mining is the process of discovering patterns in large data sets from different perspectives into interesting and usefull information [1, 2]. The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use. It involves database and data management aspects, data preprocessing, model and inference considerations, interestingness metrics, com- plexity considerations, post-processing of discovered structures, visualization, etc. The traditional data mining is on the relative static database. However, with the rapid development of modern information technology applications, modern data-mining applications, often called ?big-data? analyses, have created a need to manage immense amounts of data quickly [3]. Big data is an emerging research area which requires exceptional technologies to efficiently process large quantities of data within tolerable elapsed times. In big data, the datasets are often from various sources yet unstructured such as sensors, video and image surveillance, Internet texts and documents, business transactions and web logs, and so on. So stream data is a very common data type in big data. In data stream, data arrives in a stream or  streams [4], and if it is not processed immediately or stored, then it would lose forever. Moreover, the data always arrives so rapidly that it is not feasible to store it all in active storage, i.e., in a conventional database, and a data stream is an ordered sequence of instances that in many applications of data mining can be read only once or a small number of times using limited computing and storage capabilities. Data stream mining is the process of extracting knowledge structures from these continuous, rapid data records.

There are some recent studies on mining data streams, including classification of stream data and clustering data streams. Meanwhile, in many data stream applications, users tend to pay more attention to the mode information of the recent data stream.

Consequently, mining frequent patterns in recent data stream is a significative work [5]. However, it is challenging to mine frequent patterns in data streams because mining frequent itemsets is essentially a set of join operations. Since one can only maintain a limited size of time window due to the huge amount of stream data, it is difficult to mine and update frequent patterns in a dynamic, data stream environment. There are three frequently used time window processing models in data streams mining, which are landmark window, sliding window and damped window model [6]. On the other hand, finding the frequent itemests in a data set with predefined fixed support threshold could be seen as an optimization problem. So this paper represents the problem of frequent itemsets mining as a nonlinear optimization problem, which contains the new kind of addition, multiplication and norm. This optimization problem can be summed up to a 0-1 programming, and the authors try to adopt the genetic algorithm [7] to solve it.

2  Problem Definition  2.1 Formal Representation of Frequent Itemsets In transaction databases, the set of items is called itemset.

The itemset contained k items is called k-itemset. For example, set {a, b} is a 2-itemset, which contains two items, item a and item b.

The frequency of occurrence of a itemset is the number of transactions, which include this itemset. It also called  ____________________________________     Proceedings of ICCT2013    frequency of the itemset, support counting or counting.

If the support of itemset I is satisfied with the predefined minimum support threshold, i.e., the support counting of itemset I is not less than the predefined support threshold, the itemset I is called frequent itemset.

To describe the generic problem of frequent itemset mining formally, some mathematics definitions are given in the following.

Assume 1 2P { | [ , , , ]? ? ?, ]], ]T mj j j j mjP P p p p R and  {0,1}, 1,2, , ; }, {0,1}? ? ? ?, ;;, ;ijp i m j N k .

Definition 1 (Addition) 1 1 2 2 [ , , , ]? ? ? ? ?,,  T i j i j i j mi mjP P p p p p p p .

Definition 2 (Numerical Multiplication)  1 2[ , , , ]? ? ? ? ?,, T  j j j mjk P k p k p k p .

Definition 3 (Norm)   ( ) || || .? ?  ? ? m  j j ij i  P P P  In these definitions, the ? ? ? denotes ?Logic AND? operation, and the ?? ? denotes ?Logical OR? operation.

It can be seen that (A, ? ) constitutes a semigroup with identical element, where ? and ? operation satisfy the commutative and associative laws and ?(P) has the property of triangle inequality. Based on the above definitions, the nonlinearized representation of frequent patterns is discussed.

Assuming that  1 2   [ ] n  n  m mn  PP P P P P  P P   ? ?? ? ? ?? ?  1nP1  ?     ]  n ?] ? ??      ?  mnPm ???? ? ? ??  is a task  related transactional database in the problem of frequent patterns mining. There are n items and m records in P, where pij = 0 expresses the i-th record doesn?t contain the j-th item. Conversely, pij = 1 expresses the i-th record contains the j-th item. So a k-itemset could be indicated by a n-dimensions {0, 1} vector: R = (r1, r2, ?, rn), where ri = 0 or 1. ri = 0 signifies that set R contains the i-th item and ri = 1 means that set R doesn?t contain the i-th item. Suppose the minimum support threshold is min_sup, the k-itemset Lk and k-frequent itemset Fk also can further represent by vector: Lk = {R|R = (r1, r2, ? , rn), ri = 0 or 1 and ||R|| = n ? k}; Fk = {R|R = (r1, r2, ?,  rn), ri = 0 or 1 and ||R|| = n ? k and || ( )  ? ? ? n  i ii r p  ?  min_sup}, where ( )  n  i ii r p  ? ? ?  1 1 2 2( ) ( )r p r p? ? ? ? ?  ( )n nr p? ?( nn? ( . Every element R in Fk is referred to as k-frequent itemset. The mining problem of frequent itemsets could be expressed as finding all elements in Fk with predefined minimum support threshold min_sup.

With regard to a large database, to mining the whole frequent itemsets, because of the huge number of frequent itemsets, it doesn?t have great significance. In some specific applications, users may hope to find the  long frequent itemsets as far as possible. This problem could be formulated by the following model.

1 2   min || ( , ,..., ) ||  || ( ) ||: 0  1 1,  2,  ...,  n n  i ii  i  f r r r  r p min_supST r or i n ?  ?  ? ? ? ??  ? ? ? ??  (1)  It is very difficult to solve the Eq. (1) directly. For a boolean database with n items, it is to find some solutions in 2n elements space. With rapid growth of n, the amount of calculation increases exponentially. This is why the frequent itemsets mining problem complexity.

In the circumstance without accuracy guarantee, we can try to use random fast optimization strategy to resolve this kind of problems. So this paper adopts genetic algorithm to settle this model shown in Eq. (1).

2.2 Bitmap vector and Boolean matrix In genetic algorithm, the encoding of chromosomes is the first problem encountered to solve. Binary encoding is the most common type of chromosome code. In binary encoding, every chromosome is a string of bits, 0 or 1.

In this paper, bitmap vector [8, 9] is first adopted to represent the itemsets, so in genetic algorithm, binary chromosomes decoding can be utilized to indicate frequent itemset. Such as an itemsets I = {a, b, c, d}, I has a subset {a, c}. This subset can be uniquely determined by a bitmap vector ?1010?. And in the running process of genetic algorithm, the chromosome ?1010? represents the itemset {a, c}.

Meanwhile, to calculate the support of an itemset, it is needed to access the data sets. In general, this would spent lots of time if the data sets don?t store in RAM.

The structure of the data set compressed has been an important research issue. In this paper, a simple and practical based boolean matrix effective data compression method was adopted. Through a transaction database scanning, the transaction database is mapped to a 0 and 1 matrix in RAM. The itemset support product operation. One scan of the transaction database can significantly reduce the I/O times in database scanning and saving the storage space. The method of this paper is shown in the following.

Table I An example of a transaction database.

TID Items 100 abc 200 abcd 300 abc 400 d  For any given transaction database D, Let : ?f D R , i.e., R = f(D) = (tij)m?n, where  j1 .

i ij  j i  I t t  I t  ???? ? ??? (2)  i = 1, 2, ?, m; j = 1, 2, ?, n. ti is the transaction record    Proceedings of ICCT2013    in D. m and n denote the number of the transaction records and items, respectively. A transaction database is illustrated in Table 1, and the boolean matrix followed the item alphabetical ordering sequence in {a, b, c, d} is shown in Figure I.

Figure 1 The boolean matrix corresponding to Table I.

3  Algorithm Design  3.1 Fitness Function Devising In designing of genetic algorithm, fitness is an important concept. Fitness is used to measure the good degree of every individual for optimizing calculation achieved or approached the optimal solution in population. The individual with high fitness would have high probability inheritance into future generations. Conversely, the individual with a lower fitness would have low probability inheritance into future generations. The function is used to measure the individual fitness called fitness function.

At the later evolution stage of genetic algorithm, namely, the algorithm approximates convergence period, because the differences among individual fitness in population are very small, the potential of continuously optimizing reduces and some local optimal solutions maybe obtain.

So, the devising of fitness function is an important aspect of genetic algorithm.

In this paper, a penalty function is added to transform the nonlinear programming model shown in Eq. (1) to an unconstrained optimization problem, whose objective function is illustrated in Eq. (3), to be more convenient for applying genetic algorithm. Because the transaction database is mapped to a 0 and 1 matrix, the solution of the nonlinear programming model shown in Eq. (1) is a binary string. It could be spontaneously interpreted as the chromosome code in genetic algorithm.

1 2 1   || ( , ,..., ) || || ( ) || ( )  || ( ) ||  n  n i ii n  i ii  r r r if r p min_sup f r  n if r p min_sup?  ?  ?  ? ? ? ???? ?  ? ? ? ? ???  (3)  where ri = 0 or 1, i = 1, 2, , n. ? is taken as n or one-half of n in general, represents the degree of penalty.

In Eq. (3), to the chromosome satisfied the minimum support, its objective function is the sum of binary chromosome code. This indicates that in the following iteration, the chromosomes of longer frequent itemsets would be priority selected and the frequent itemsets as long as possible would obtain in the finally.

The objective function illustrated in Eq. (3) could also be converted to Eq. (4). Set up the objective function of  every chromosome satisfied the minimum support is the same. In this way, the chromosomes dissatisfied the minimum support would be deleted instead of priority select the longer frequent itemsets in each iteration.

Based on the objective function in Eq. (4), the frequent pattern ultimately gained very likely depends on the selection of initial population. The objective function associated with the counting of frequent itemsets is also taken to priority select the most frequent itemsets. The results of genetic algorithm would be the itemsets as far as possible frequently in this situation.

|| ( ) || ( )  || ( ) ||  n  i ii n  i ii  if r p min_sup f r  n if r p min_sup  ?  ?  ?  ?  ? ? ? ???? ?  ? ? ? ? ???  (4)  3.2 Initial Population Determining By using genetic algorithms to mine frequent itemsets, it is important to find chromosomes satisfied with fitness according to the objective function in each iteration and in the next generation genetic to be priority selected. So, the selection of initial population is very critical.

For a transaction database with short data items, for example, a transaction database with 8 items, every data item divides into existence or inexistence, there will have 28 = 256 different transaction combinations. If choose 300 initial random populations, all transactions would be very likely covered. The populations satisfied minimum support threshold are almost certain to appear in initial populations. In the following, genetic algorithm could find these desirable chromosomes to do meaningful genetic iteration.

However, transaction databases usually have a large number of items in practice. For example, the mushroom data sets obtained from the UCI Repository [10], there are 119 attributes and could have 2119 = 6.65 ? 1035 different transaction combinations. So it is impossible to cover all possible transactions by using so many initial populations. If the number of initial populations is too smaller than possible transactions, the initial populations may not contain the populations satisfied minimum support threshold. In the following genetic generation, the objective function and fitness of chromosome are the worst case and the genetic iteration would become meaningless.

To solve this problem, we propose a new initial population determining method based on apriori principle of frequent itemsets in Apriori algorithm [11].

The short itemsets have higher frequency of occurrence than long itemsets included them. So in the setting of initial populations, we take the initial populations are all short itemsets or single data items. By using this approach, every single data items and the randomness of its number are guaranteed.

In this paper, ?0? denotes that the data item is appear in the relative position of chromosome, on the contrary, ?1? denotes that the data item is appear in the relative    Proceedings of ICCT2013    position of chromosome. The detailed determination method of initial population is shown in the following.

1. Suppose that population size and the length of is chromosome code are N and n, respectively. A transaction database Pmn = (pij)m?n has m transaction records and n data items. Initialize the population R = ones (N, n), i = 1; 2. If i > N, terminate the initialization process. If i > n, go to 4, otherwise, go to 3; 3. Assign the first n populations, to assure each row and each column all have one ?0?, i.e., R(i, i) = 0, i = i + 1, go to 2; 4. From the (n + 1)-th to Nth populations, to assure every row randomly include a ?0?, i.e., R(i, ceil(rand() ? n)) = 0, i = i + 1, go to 2.

The actual experiments show that, the setting of initial populations has significance in genetic algorithm successfully and effectively iterating and mining frequent itemsets.

3.3 Detailed algorithm Design Form the objective function shown in Eq. (3). It can be found that ||(r1, r2, , rn)|| is a linear operation. The nonlinearity of the objective function model comes  mainly from  || ( ) || n  i i i  r p ? ? ? . So it is needed to consider  how to resolve this nonlinear expression.

Suppose there are two strings, m1 = (m11, m12, , m1n) and m2 = (m21, m12, , m2n), where mij ?{0, 1}, i = 1, 2, j = 1, 2, , n. If m2j ? m1j , j = 1, 2, , n, call m2 is a substring of m1, denote as m2 m1. With a given database Pmn = [q1, q2, , qn]T and a {0, 1} string r = (r1, r2, , rn), If set X = (x1, x2, , xn), where  1 2  1 r r , ,...,i n i  Otherwise x  r r q ?  ? ? ? ? ? ? ?? iq (5)  It can be got that  || ( ) || n  i i i  r p ? ? ? = ||X||. In Eq. (5),  || ( ) ||  n i i  i r p  ? ? ?  indicates the number of the occurrence of  pattern r in database P. Because rj = 1 shows the j-th item exclusive in r, xi = 1 denotes ?r is the substring of the i-th record of P. This means that ||X|| indicates that occurrence number of pattern r. Consequently,  || ( ) ||  n i i  i r p  ? ? ?  = ||X||.

A procedure is designed to find the pattern counting of every chromosome corresponding to transaction database. The pseudocode is shown in Procedure patternCounting().

Procedure patternCounting(population) 1) foreach Transaction records in database P do 2)  foreach Chromosomes chrom ?  Population R do 3)    if chrom is the substring of records then 4)      The corresponding pattern Objvi = Objvi + 1; 5)    endif  6)   endeach 7)  endeach 8)  return counting array Objv of all chromosome patterns.

In the following, on the basis of the above fitness function devising and initial population determining, we elaborate the steps and process of long frequent itemsets mining based on genetic algorithm in large data sets.

Algorithm 1: Long frequent itemsets mining based on genetic algorithm  Input: Minimum support min_sup, database D; Output: Possibly long frequent itemsets.

1. Convert the original database to a {0, 1} boolean matrix based on the method put forward in Sec. 2.3, determine the size of populations NIND, the length of code PRECI and the number of iterations MAXGEN.

Using the approach proposed in Sec. 3.2 to generate initial population; 2. Through the basic operation of genetic algorithm, i.e., crossover and mutation operator, to form new sample space; 3. Calculate the substrings matching counting of every chromosome with database transformed into memory, then according to the Eq. (5) to compute the fitness of each chromosome; 4. Roulette selection is adopted to carry out the fittest survive operation. The samples with high fitness pass genetically to the next generation samples space; 5. If the fitness of the best optimal individual satisfies the given conditions or the number of iterations is greater than MAXGEN, go to 6. On the contrary, the number of iterations increases 1, then go to 2; 6. Choose m chromosomes, whose summation is smaller, i.e., the long frequent itemsets, and satisfy minimum support from the obtained genetic result. Output them from binary code space to solution space.

The mining algorithm of long frequent patterns based on genetic algorithm could generate the as long as possible frequent itemsets in basically constant storage space, i.e., the size of initial population, and constant time, i.e., the number of iterations. However, because of the nondeterminacy of genetic algorithm, the finally results might have some subtle but acceptable errors.

In the following, the static data sets are divided into some parts and successively arrival to simulate the data stream. We separately mine and handle every part, then composite the results.

Algorithm 2: Long frequent itemsets mining over data stream based on genetic algorithm  Input: The successively arrived data streams P(p1, p2, ), minimum support min_sup;  Output: The list of possibly long frequent itemsets D (Itemsets, Counting), each item composed by itemsets and counting of frequency.

1. For a batch arrived data stream pi (i = 1, 2, ), mine the long frequent itemsets according to the Algorithm 1; 2. Take out the first m long frequent itemsets obtained    Proceedings of ICCT2013    from step 1 join the data structure D following the rules: A), if the current frequent itemsets already exists in D, update this frequent itemset in D with the sum of the frequent counting in pi and primary D; B), if the current frequent itemsets doesn?t exist in D yet, add a new item record included this frequent itemsets and its counting of frequency to D; 3. Judge the data streams whether terminate the arrival, if true, go to 5. If not, go to 1; 4. Fetch out the first w longest frequent itemsets, which is in the users specified, from the long frequent itemsets list D. Output them from binary code space to solution space.

In this paper, to improve the efficiency of algorithm, we adopt hash table to store the frequent itemsests list D to fast search an itemset whether exists in D.

4 Experiment And Performance Evaluation In this section, some comparison experiments are designed to evaluate the performance of the proposed frequent itemsets mining algorithm by using public and representative data sets. Our experimental environment is Matlab 7.0 on Windows 7 operating system with Intel Core 2 CPU 2.40 GHz and 2.00 GB memory.

Table II Long frequent itemsests mining with min_sup = 25%.

Algrithm Populs Iters Length of frequent itemsets  Time Hitting 6 7 8 9 10 11  Genetic  1000 30 Null Null 15 6 2 Null 357s 66.70%  1000 40 Null Null Null Null 9 1 397s 100%  500 40 Null Null Null Null 3 1 207s 100%  500 30 7 10 Null Null Null Null 195s /  250 40 3 2 2  Null Null Null 116s /  1500 30 Null Null 3 18 10 1 529s 92%  1000 20 ... 14 1 Null Null Null 288s /  Apriori / / .... ... ... ... 11 1 56.42s /  First evaluate the performance of long frequent itemsets mining algorithm based on genetic algorithm with Apriori algorithm in static dataset. The comparisons are on mushroom [10] testing data set. When the predefined minimum support threshold min_sup = 25%, the time consumption and types of frequent itemsets with varying initial populations and number of iterations are shown in Table 2.

From Table II, it can be seen that when the support threshold is relatively large, i.e., min_sup = 25%, Apriori algorithm has significant advantages to our proposed algorithm in time consuming. To our proposed based on genetic algorithm, if suitable initial populations and umber of iterations are selected, it can generate the  maximum frequent itemsets in very high hitting ratio.

On the other hand, in Table 2, genetic algorithm is adopted to mine long frequent itemsets and Apriori algorithm for frequent itemsets. But frequent itemsets from the long frequent itemsets dug can be generated by genetic algorithm. For example, to a long frequent itemsets with length 11, 211?1 = 2047 frequent itemsets could produce through apriori principle. The size of initial population has significant impact to running time of algorithm. On the contrary, the influence of number of iterations is less than the former. This is because large number initial populations would increase the diversity of algorithm results.

In memory consumption, because a large number candidate items produced in the running process of Apriori algorithm, the memory requirements persistently increase. 44MB memory space is needed when mining mushroom dataset with minimum support 25%.

However, candidate items aren?t required in genetic algorithm. Genetic algorithm just persistently increases the number of ?0? in chromosome and the requirement of memory space is constant and limited by the size of initial population. If the size of initial population is 500, the length of each chromosome is 119, the memory space required is 500 ? 119 = 59500 bytes = 58KB. In addition, along with the decreasing of minimum support, the memory requirement in Apriori algorithm is increasing. But genetic algorithm lacks sensitivity to support, its memory requirements wouldn?t have a significant growth and it just involves with the size of initial population.

Table III shows the time consumption of genetic algorithm and Apriori algorithm with the predefined minimum support threshold min_sup = 20%. The time efficiency of Apriori algorithm declined sharply, but the time and space efficiency is mainly constant in genetic algorithm. The superiority of parallel searching has been fairly reflected.

Table III Long frequent itemsests mining with min_sup = 20%.

Algorithm Populs Iters Length of frequent itemsets  Time 12 13 14 15  Genetic  1000 40 ... ... 1 Null 487.8s  500 50 Null Null 4 1 227.8  1000 50 Null 9 1 Null 481.9s  Apriori / / ... ... 16 1 1085.9s  Fig. 2 shows the fitness changing curve of genetic algorithm in long frequent itemsets mining with minimum support min_sup = 25% and 1000 initial populations. It can be seen that in early iterations, the convergence speed of genetic algorithm is very fast. Up to the 30-th generation, the fitness of best individual has confirmed already and the convergence speed of average    Proceedings of ICCT2013    fitness of population also becomes slower.

0 5 10 15 20 25 30 35 40    Optimal individual fitness Average fitness   Figure 2 Fitness changing curve in long frequent itemsets with  min_sup = 25%  0 5 10 15 20 25 30 35 40    Optimal individual fitness Average fitness   Figure 3 The optimal and average fitness curves in the first  part of mushroom data stream  In the following, we divide mushroom data set into three parts to imitate data streams. The previous two parts separately contain 3000 records and the last part contains 2124 records. Fig. 3 illustrates the optimal and average fitness in the first part, i.e. the early 3000 records, with minimum support min_sup = 25% and 500 initial populations. In the first part, the optimal solution is found in the 33th iteration, which is 31th iteration and 27th iteration in the second and third parts, respectively.

With the minimum support min_sup = 25%, 500 initial populations and 40 iterations, 462.57 seconds are consumed and 4183 frequent itemsets are mined by using the type of data streams. We also adopt the Lossy Counting (Manku et al., 2002) framework to handle the data stream. We divide the data set into three parts as before, with the minimum support min_sup = 25%, 500 initial populations and 40 iterations. The error parameter  = 0.01, so the width of bucket is 1/? ? ? ? ?  = 100. In the process of genetic algorithm, ?  = 30 buckets are read in one time. 535.3 seconds are spend and 3911 frequent itemsets are obtained. This result is very similar with without using Lossy Counting framework. This is because when the support is relatively large, the pruning function in Lossy Counting is very limited.

5  Conclusions And Future Work Frequent itemsets mining is one of the basic missions and hot issue in data mining. According to the requirements of practical applications, based on genetic  algorithm, the frequent itemsets mining over data streams is studied in this paper. Through fitness function devising and initial population determining, genetic algorithm solved the contradiction between decreasing support and the complexity of time and memory space exponentially increasing in typical algorithm by parallel searching.

In the complete frequent itemsets mining, there are some acceptable error existents because of the randomness of genetic algorithm. In our future work, we would improve the accuracy of the complete frequent itemsets mining in genetic algorithm by optimizing the object function, slowing the speed of iterations and designing the more reasonable initial populations.

Acknowledgements This work is supported in part by the National Natural Science Foundation of China under Grant Nos. 61171053 and 61300239; the Doctoral Fund of Ministry of Education of China under Grant No. 20113223110002; the Science & Technology Innovation Fund for Higher Education Institutions of Jiangsu Province under Grant No. CXZZ12_0480.

