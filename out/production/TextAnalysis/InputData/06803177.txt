A Big Data Correlation Orchestrator for Internet of Things

Abstract?Connecting embedded sensors with cloud infrastruc- ture could open enormous possibility for creating new services which eventually could have unprecedented impact in our way of living. To provide robust and reliable services, embedded sensor generally need to produce significant amount of data which could easily exceed storage capability of micro-servers.

To address this problem, in this paper, we present BigCO: a big data correlation orchestrator for internet of things. This orchestrator is implemented in a micro cloud server whose role is to manage centralized as well as distributed wireless sensor nodes. In this paper, we address how multifaceted data could be interrelated and analyzed using 3D modeling and present a streaming compression algorithm (extending Ramer- Douglas-Peucker heuristic). Applying our proposed compression algorithm, we have achieved as high as 99.86% compression of sensor data.



I. INTRODUCTION A constantly growing number of wirelessly connected sens-  ing devices are bringing new opportunities and challenges.

New opportunities appear in many areas such as, the medical field, smart houses and cities, automation and in the industrial workflows. Researchers adapted sensors and controllers to semantic concepts by introducing the Things as a Service (TaaS) paradigm or the more general ?Cloud of Things (CoTs).

The low acquisition cost of micro-sized programmable system- on-chip computers combined with reliable wireless technology and micro-sized MEMs sensors, have greatly contributed to the expansion of the automation industry. Advancement in distributed systems and availability of cloud services created the necessary infrastructure to connect wireless nodes as Internet of things (IoTs). These sensors can produce massive amounts of data which impose many challenges, such as data storage and analysis.

Wireless connected sensors (a prominent example of IoT) can produce a large amount of data in a short period of time.

On the one hand, sensors generally have an insufficient amount of memory or processing power. Sensors being scattered in remote locations makes them easy targets of vandalism or other environmental harms. Therefore, it is essential to store data produced in a secure location. One of the most secure place to store data would be an off-site database provided through Infrastructure as a Service (IaaS). Some of the main benefits of using an IaaS are their easy access, real time sensor data visualization capability and their ability to monitor and control wireless sensor networks. An active community has already built several ready-to-adapt templates for sensor-driven data applications intended for data on the Cloud. IaaS based  companies (Nimbits [8], ThingSpeak [9] and others) provides data storage, processing, and subscription on the Cloud for IoT [3], [4], [5]. Most of these service providers support users by accessing IoT sensors, while addressing individual devices they are bounded to the transmission control and Internet packet protocols (TCP/IP). This paper addresses the challenges involved in storing large amount of data and provides a holistic algorithm to manage big data in micro-cloud environment. We have implemented the algorithm in distributed Wireless Sensor Networks (WSN) and proven the validity and practicality through simulation. Our aim is to connect wirelessly linked tiny sensors to cloud infrastructure for developing multi- purpose applications in large scale distributed systems. Our developed framework connects micro wireless sensor nodes to the internet (public cloud) or intranet (private cloud).

As a whole, we focus on developing a framework for the orchestration of sensor nodes and the management of the large amount of data produced by sensors. We discuss how multi- faceted data can be connected (here, for instance we have mapped temperature, humidity and cooling power consump- tion data) and how these data elements can be interrelated and analyzed using 3D modeling, providing loss-less compression as well as database query optimization based on 3D surface mapping (vs. running database queries on multiple relational database tables). This orchestration provides benefits in big data management for IoT, here we utilized Ramer-Douglas- Peucker Heuristic (R-DPH)[1], [2]. R-DPH is an application dependent data compression algorithm to turn a low cost single board computer into a data warehouse. Our proposed data- type dependent compression technique also assists in fault reduction and the development of reliable networks.



II. BIG DATA ORCHESTRATION AND COMPRESSION FOR WSNS  Wireless sensors can produce a significant amount of data.

Accumulated data should be either transmitted or aggregated and sent to a remote server. Since wireless sensors have a limited storage capacity, they need to either dispose of old data or stop producing new ones. Based on the data logging and monitoring applications requirements, a constant transmission may be necessary. If a live operation is not required, it is important to reduce the number of times an on-board radio is turned on to transmit packets. This step is carried out to save battery consumption. At the same time, the transmission of live packets on the current cloud infrastructure requires  2014 IEEE World Forum on Internet of Things (WF-IoT)     extensive resources and sometimes becomes impractical. For example, in one experiment of our study, we have found that transmitting temperature, humidity, power consumption and status data (such as battery power level) from a single node can produce 16 bytes of data. We set the sampling for every second (assuming real time application scenario), total data (including time stamp and related meta tags such as node id and resolution) added to 2.9 GB over 30 days period with only four sensing nodes. Obviously an orchestrator with enough storage capacity can handle the required space for a small scale setup such as monitoring a greenhouse over a period of one month. However, we will need a larger data management and storage capacity for large scale deployments.

In our deployment, we experimented with a streaming com- pression algorithm using the Ramer-Douglas-Peucker Heuris- tic. R-DPH is used for reducing the number of points in a curve that is approximated by a series of vectors. The domain samplings done by sensors produce series of points which can be visualized in a 2D, 3D or nD Cartesian coordinate system.

These points are within the margin of error distance of ?>0.

In most relational database, each point is stored in a table row along with other values such as a unique key (i.e. time) to create relationships between tables [5].

A line graph is created using the temperature points with the goal of generating similar graph with fewer points. The algorithm (R-DPH) first looks at an acceptable maximum distance between the original and the simplified graph made of a subset of the defined coordinates. It starts with dissimilar Y values coordinates (temperature) and tries to find a vector that will closely pass by all points. The start and end coordinates are untouched whereas other coordinates are deleted.

R-DPH heuristic starts by ordering the points in time (sensors data stored in the database sorted automatically based on time stamp). Point to line distance calculation algorithm (called error distance) then generates a line cluster containing beginning and end points. The heuristic continues by breaking the cluster into smaller lines, then the points that fall within error margin are removed and only the beginning and end points are saved. This process is repeated recursively by breaking the line further and finding the outermost point from the line. While the start and end points are maintained, the distance to the drawn line of all the other non-fixed points is measured. If the distance error is more than ? then that point must remain and is called the worst point. This particular point becomes the end point during the following recursion. If there is no error distance greater than ? then all the point outside the line will be removed. The created vectors will generate the curve for future database queries.

Figure 1 shows error distance calculation as explained above for the three combinations of points with lines. The point- to-line calculation provides an efficient heuristic for temperature-time (2D) data management. However, in our case orchestrator will have to store data from different type of sensors (temperature, humidity, light) to analyze correlation between them.

Fig. 1. Distance calculation and measuring the error  A. Ramer-Douglas-Peucker Heuristic Shortest path Calcula- tion for Data Compression  Given a point P and a line L formulated as X(t) =A+t??b , the nearest point on the line to the P is the projection X  ( t )  of P onto the ??b for a value of t. Figure 1 shows the vector P - X  ( t )  is perpendicular to ??b thus,  0 = ?? b (P ? X(t)) = ??b (P? A ? t ??b )  = ?? b (P ? A) ? t ? ??b ?2  As a result, the projection of nearest point value or (param- eter of projection) t, on line b is calculated by dot product ( t = ??b (P? A )/ ? ??b ?2 ). The distance of point P is ? P? A? t ??b ?2 and finally distance of point P to line  L is ? P? A?2? ?? (b (P ? A))   ? ??b ?2 . If we assume ?? b to be  a unit vector ( ?????b|= 1) we can simplify the equations to ??m  X = constant. The closest point P? on the line satisfies the relationship P = K + s??m for some s(s is the correction factor).

Dot product with ??m yields ??m P = ??m K + s? ??m?2= c + s? ??m?2, so s = (??m Y - c)/ ? ??m?2 . The distance between the P and the line is ? P?K ?= |s| ? ??m ?, or distance(P, L)= |??m.P?c|?m?2 . If we consider ??m being a unit vector, we can simplify the equation, this will reduce amount of processing requirement: ? P ? A ?2 = (P - A)(P ? A) T =(P ? A) T I(P ? A) =  (P ? A)T (b?b?T+mm?T )(P?A) = (b?.(P?A))2+(m?.( (P?A) )2= (b?.(P?A))2+(m?.P?c)2  Here construct ( b?b?  T +m?m?T  ) is a 2x2 identity matrix. Fi-  nally product of b?b? T  will be a 2x2 matrix, this is proven since  { b?, m?  } are orthonormal basis for R2 and every  vector can be represented as IX= X =(b?TX)b?+(m?TX)m? = ( b?b?  T ) X+(m?m?  T )X .

Since we have to process a great number of points to line distances, it is important to make quick calculations. To this end, we can use the memory of the orchestrator to recalculate ? ??b ? and store it per segment, other method (for low memory applications and orchestrator) is to defer the calculation of  2014 IEEE World Forum on Internet of Things (WF-IoT)     Fig. 2. Cooling system energy consumption relationship with temperature and humidity, observation reveals how humidity encourage consumers to utilize more cooling power to create a more comfortable experience, while the lowest consumption point is achieved at 60% humidity and 18?C (for a particular participated household in California)  Fig. 3. 3D point distance to surface compression with large error distance, only five surfaces are recognized  1/? ??b ?2 until there is an absolute necessity (remaining points). Points that are identified to be deleted need no further calculation.

A new challenge arises when multiple data points are asso- ciated together. The values of temperature, humidity and light are plotted by setting them in a two dimensional surface. To map the elevation with the relative temperature and humidity in 3D, we need to create a planar base and define error. Fei Lifan [1] in their three dimensional Douglas-Peucker described an algorithm on how to create a base plate. This process will help execute a multi-point compression in one run rather than executing each 2D plane separately.

B. Cooling system energy consumption relationship with temperature and humidity  Using 3D visualization of related points, we have depicted energy consumption relationship with temperature and humid- ity for a subset of points measured in one month (shown in Figure 2). In order to remove points that are closer than the error we need to compute the distance from a point to a base surface (in Figure 3). Without loss of generality, here our assumption is that the surface is a no-uniform rational basis  spline (e.g., a NURBS surface). With a base surface S(u, v) and a point P, we need to find S closest distance to P. Our goal is to find a coordinate (u, v) for which the distance from S(u, v) to point P is minimum (Figure 1). It is known that shortest line (distance) forms a perpendicular from P to S. As we create a function of the parameters of the surface, we need to examine how a surface is built from multiple points.

There are multiple ways to build a surface by using Bezier triangle patches, polynomial curves, and by B-Spline surface.

The concept covers a triangle patch (there are also multiple types of B-Splines but we considered only the triangle patch in our experiment).

Let?s consider a set of points {Si}n0i=0 , Si ? Si+1 called knots and the vector connecting them called knot vectors.

Given a rectangular lattice of 3D points and controlling point P for 0? i0 ? n0, we will have 0? i1 ? n1, a B-Spline rectan- gle patch, X (s, t) =  ?n0 ii0=0  ?n i=0 B  (0) i0,j0 (s)B  (1) i1,j1(t)Pi0,j1  polynomial satisfies Cox-de Boor formula and finally we will end up with the polynomial components X (s, t).

B (1) i,0 (t) =  { 1, ti ? t < ti+1 0, otherwise  B (1) i,j (t) =  (t?ti).B (1) i,j?1 (t)  ti+J?1 ? ti + (ti+J ? t).B(1)i+j,j?1 (t)  ti+j?1 ? ti+1 Now that we could define the surface based on our point  cloud, we will try to find the distance from a point to a polynomial. Given a parametric surface and a point P, to apply the R-DRH, we want to find a point on the surface S that is closest to P. Our objective is to find the parameters (u, v) such that the distance from S(u, v) to P is minimal. Our approach begins with the geometric calculation of the line segment from P to Q(t) by drawing a right angle triangle on the tangent plane of the surface at S(u0, v0). The vector between the surface and the arbitrary point can be expressed as a function of the parameters of the surface.

r(u, v) = S(u, v) ?? P f (u, v)= r (u, v) Su (u, v) = 0  and g (u, v)= r (u, v) Sv (u, v) = 0  By using the Newton Iteration, we can find an initial estimation by evaluating a surface tile of size n x n at equally spaced points. With these parameters, we will have:  ?i=  [ ?u ?v  ] =  [ ui+1?ui vi+1?vi  ]  fi=  [ fuC fv(ui, vi)  gu(ui, vi) gu(ui, vi)  ]  =  ? ?????  ? Su(ui, vi) ?2+r(ui, vi).Suu(ui, vi) Su(ui, vi).Sv(ui, vi)+r(ui, vi).Svu(ui, vi)  ..

Su(ui, vi).Sv(ui, vi)+r(ui, vi).Suv(ui, vi)  ? Sv(ui, vi) ?2+r(ui, vi).Svv(ui, vi)  ? ?????  2014 IEEE World Forum on Internet of Things (WF-IoT)     TABLE I PERFORMANCE EVALUATION OF BIGCO  Error margin # of data points BigCO (R-DPH) Compression execution time(sec)  ?=0 1440 0 0 ?=2 427 37.5 88% ?=3 2 38.4 99.86 %  ki= ? [  f(ui, vi) g(ui, vi)  ]  Considering the fact that we have an initial estimation of (u0, v0) using Newton iterations, Piegl and Tiller and two zero tolerance of Euclidean distance and zero cosine measure, we will check the following formula to find the lowest distance.

? Su(ui, vi). (S(ui, vi)?P ) ? ? Su(ui, vi) ?? (S(ui, vi)?P ) ?  ? ?  ? Sv(ui, vi). (S(ui, vi)?P ) ? ? Sv(ui, vi) ?? (S(ui, vi)?P ) ?  ? ?  If the criteria below are not met then a Newton step is taken to make sure a ? (ui) ? b and c ? (vi) ? d.



III. EXPERIMENTAL RESULTS  The Orchestrator is implemented using Raspberry-Pi [10]) and it stores data from sensors into a MySQL database.

Python is used to develop command interface for sensors as well as to implement R-DPH algorithm to compress sensor data. The user interface and web development are done using PHP language while the Apache Web server engine is used to provide the necessary interfacing. Wireless sensor nodes are programmed to send data at periodic intervals to the orchestrator. WSN nodes from MAXFOR CM5000 [11] are used to setup WSN. Each node has temperature, light, and humidity sensors. The orchestrator is connected to the cloud using Wi-Fi and using a sink node it connects with deployed sensor networks. Sensors are categorized as sink node (base station), access points (supports multi-hop routing) and end points (sensing or actuation device). Nodes are identical and are compatible of IEEE 802.15.4. Micro-controllers, CC2420 RF Chip (radio) and sensors are mounted on MAXFOR CM5000, and they are programmed using TinyOS [6]. In our development, we use hierarchical gradient based routing described in [7] to form a mesh network among the sensor nodes. In the gradient based routing, the backbone network is initially formed by using controlled flooding. The base station initiates the network formation by broadcasting a packet in which it sets the gradient height to 1. Nodes that receive the gradient packet set their gradient to the received value and rebroadcast the packet by increasing the gradient value by one. This constructs a network organized into layers where the higher gradient nodes report to the lower gradient ones.

For a given temperature and a humidity taken over a period of time t, we identify series of points with t, h in a series of equally distanced in time scale, utilizing R-DPH a tolerance of error ? is defined, compression will implement the point reduction using the given error distance. Table 1 shows  compression result of 1440 sample points per day (once every one minute) and it clearly demonstrates the effectiveness of this approach. The ability to compress remains relevant to the diversity of the points, how far they are apart and the given error margin ?. In a real scenario such as a green house, where temperature spikes are not expected, the algorithm could be extremely effective.

In the experiment we inferred that interrelated points like temperature and humidity can be correlated to the amount of energy needed to cool down a room for an acceptable human comfort level. For example, a low temperature triggers a reduction in the ability of the air to hold moisture. In addition, the information extracted from their combination can provide easier combinations with other relevant data sets like power consumption (temperature feedback) by air conditioning units. By adding multiple measurements, we can process the compression as a 3D entity.



IV. CONCLUSION Embedded sensor networks can produce significant amount  of data. Accumulated data should be either transmitted or aggregated and sent to a remote cloud server. Since wireless sensors have a limited storage capacity, they need to either dispose of old data or stop producing new ones until old data is offloaded. Our study shows aggregating data from several sensors can produce gigabytes of data in a week. Obviously an orchestrator with enough storage capacity can handle the required space for a small sensor network setup for a short pe- riod of time. However, we will need a larger data management and storage capacity for large scale deployments. To address this problem, we experimented with a streaming compression algorithm extending Ramer-Douglas-Peucker heuristic and we achieved as high as 99.86% compression of sensor data. In future, a multi-orchestration will be designed to handle multi- cast and unicast data reception and parallel data warehousing (for higher availability and disaster recover) for large sensor- cloud architecture.



V. ACKNOWLEDGMENTS The authors would like to thank Kim Young Jin and  Sung-il Hwang from MAXFOR Technology Inc, Korea for generously donating sensor nodes for this project. Part of this work is funded by the research grant available from Electrical Engineering and College of Engineering, California State University at Long Beach.

