Influence Of Discrete Granularity On Using      Association Rules To Complete Missing Values

Abstract?this paper investigates the impacts of different discrete granularities of continuous attribute on the algorithms which employ association rules to complete missing value. The Chi2 algorithm is used to discrete the continuous attributes. By using three different discrete granularities, the impacts of the algorithm on completing missing value are explored.

Experimental results show that different discrete granularity has negligible impacts on using association rules to complete missing value. However, the completing accuracy has an inclination to decrease with the decrease of the discrete granularity.

Keywords- missing value; association rule; discrete granularity; Chi2 algorithm

I.  INTRODUCTION Missing values is very common in all kinds of science  research. Incomplete data set brings great difficulty in data using and analysis and it is one of the main reasons for indefinite information system. Reference [1] summarizes some definition of missing value as follows: error occurring in transferring or collecting data, null value, exceeding the range, value that can?t meet the standard and so on. There are many reasons for missing value, but the main causes are probably as follows: some information is unavailable at the moment; some information is omitting; some attributes of the object can?t be used; some information is viewed as unimportant; the cost to obtain those information is high; the system requires high real- time, i.e., decision must be made quickly before the information is available etc. Researcher propounded many methods to solve the missing value which base on different theory and are applied in different occasion. Generally, those methods are divided into such kinds: deleting the records where missing value exists and then obtaining complete data set, completing the missing value manually, treating Missing values as special values like [12], utilizing the statistics method, conducting data mining or inference in those data sets etc. In recent years, there comes up with the method using association rules to complete missing values(ARMV), see [4, 5, 9] and it achieves good performance compared with other methods used to complete missing values such as Robust Bayesian Estimator (RBE) [11]. So far, research of ARMV is mainly on discrete data set. This paper investigates how the discrete granularity of continuous attribute in non-discrete data set affects the performance of ARMV. We say some continuous attribute has  discrete granularity 2, we mean using 2 intervals to represent the value of that attribute. The greater granularity is, the fewer intervals are used to represent the original data.

The rest of this paper is organized as follows: Section 2 introduces ARMV. Section 3 introduces the data discretization algorithm. Section 4 provides experimental results and evaluates the influence of discrete granularity of continuous attributes on ARMV. Finally, conclusions are drawn in section

II. ARMV A. association rules  In 1993, R.Agrawal put forward the concept of association rules first time. Combined with the transaction database, R.Agrawal gave its formal description as follows [9, 10]. Let I = {i1, i2, ..., in} be a set of n distinct literals, called item set. Let DB be a transaction database, where each transaction Ti ? I is a set of items, which has a unique identifier associated with it, called TID. If a set of items X that fulfills the relationship ?X ? T?, we say T contains X. An association rule is an implication which has the form ?X?Y?, where X ? I, Y ? I and X?Y =?. An association rule has two attributes: support and confidence whose values must be no less than the user- specified minimum support (minSup) and minimum confidence (minConf) values respectively. The rule ? X?Y? that has a support value s% means the percentage of transactions containing X ? Y in the transaction database, denoted as  X YSup(X Y)  | DB |/|DB |?? = , where X YDB ? is the transaction set containing X?Y. The rule ?X?Y? that has a confidence value c% means the conditional probability, i.e., Pr (Y|X) = Pr (X?Y) / Pr (X).So, it means c% of transactions in database containing X also contains Y. In fact, association rule is an approximate dependency relationship in database, using support and confidence to evaluate this relationship.

The task of mining association rule in transaction database is to find all association rules whose support and confidence are no less than minsup and minconf respectively. The primary concept behind most association rule algorithms is a two phase procedure [10]: In the first phase, all frequent item sets are found. An item set is said to be frequent if it satisfies a user-     defined minimum support requirement. The second phase uses these frequent item sets to generate all the rules which satisfy the user-specified minimum confidence. The first phase discovering all frequent item sets dominates the performance of the mining association rules process. Algorithms to discover all frequent item set can be divided into two categories: one is producing the candidate set and the other is not producing the candidate set. The Apriori algorithm, a multiple-passes and producing candidate set algorithm, is a typical and famous technique to identify frequent item sets [10].

B. Applying ARMV on incomplete transaction database Association rule has the form of imputation ?X?Y?, where  X?Y =?, X is called antecedent and Y is called consequent.

Let us see an example how to apply association rules to complete missing value, where the Zoo database is obtained from UCI machine learning repository.

Given data: aquatic = y, tail = y, hairs = n, legs =?, fins = ?.

Supposed minsup = 0.8 and minconf = 0.15, we used association rules r1 ?aquatic = y, hairs = n ? legs = 0? to obtain the new data: aquatic = y, tail = y, hairs = n, legs = 0, fins = ?. Again, we used association rules r2  ?tail = y, legs = 0 ? fins = y? to obtain the complete records: aquatic = y, tail = y, hairs = n, legs =0, fins = y. But how to select the association rules to complete missing values need more complicated procedure. In the following, we will introduce methods converting the relation database to transaction database and give our algorithm of selecting association rules to complete missing values.

1) Transforming the relation database to transaction database  We use the method like [7] to transform the relation database to transaction database. D is dataset defined over n attributes 1,..., nA A  and contains m cases 1,..., mc c .

Let ic , 1 i m? ? , be transaction ID and cases of attributes 1,..., nA A , 1,... na a , be corresponding items, we can get transaction dataset TD. That is, let  { | 1,i ij ijIS A a i a= = ?   is a case of iA }  is an item set eliminating the missing values. After transforming, a transaction of TD looks like 1 1,{ ,..., }  i i i j n njc A a A a= = .

Then given a minsup, we utilize the Apriori algorithm to mine the large item sets of TD, denoted as 1,..., gL L , 1g ? . Then for each element uve  of uL ,1 , 1u g v? ?  ? , we compute the confidence ? \{ }uv uvw uvwe d d} ?{ ? for  each element  , 1uvwd w ? , of uve . If the confidence is no less than the user- specific threshold minconf,  the element of association rule is add to the set of association rules. Now, let?s see an example below.

TABLE I.  (A) DATASET OF 5 CASES?B?TRANSACTION FORM OF DATASET  Case A1 A2 A3  TID Items c1 2 1 ? t1 A1=2, A2=1 c2 2 1 2 t2 A1=2,A2=1,A3=2, c3 2 1 2 t3 A1=2, A2=1,A3=2 c4 ? 2 1 t4 A2=2,A3=1 c5 1 ? 1 t5 A1=1, A3=1  TABLE II.  THE  ASSOCIATION RULES PRODUCING BY TABLE 1, WHERE MINSUP = 0.3, MINCONF = 0.6  ID Asscotiaon rules sup conf r1 {A1=2} ?>{A2 =1} 60% 100% r2 {A2=1}?>{A1=2} 60% 100% r3 {A1=2}?>{A3=2} 40% 66.7% r4 {A3=2}?>{A1=2} 40% 100% r5 {A1=2, A2=1}?>{A3=2} 40% 66.7% r6 {A1=2, A3=2}?>{A2=1} 40% 100% r7 {A2=1, A3=2}?>{A1=2} 40% 100%    There are 5 cases and 3 attributes in table 1(a), where ??? represents the missing value. Table 1(b) is the transaction form of Table 1(a) in which the missing values is eliminated. Then, given the specific threshold minsup = 0.6 and minconf = 0.3, we applied the Apriori algorithm on Table 1(b) to find large item set and obtained the association rules.

2) Selecting the association rules to complete missing values  Firstly, we score the each association rules based on the following function.

S (length, support, confidence) = length + 0.5?confidence + 0.25?support.                                                                 ? (1)  And sort them based on the score of rule by decreasing order.

Reference [9] gave a good heuristic function to select the association rule to complete missing value. Here, the methods to select the rule will not affect our research. So, we adopt a simple method to select the rule. Here, length is equal to the number of element of antecedent in association rule. For example, the antecedent of the rule ?aquatic = y, hairs = n ? legs = 0? has two components, so the ?length? of the rule is two. Then, for each case of test data set, we scan the rules one by one and identify which rules? antecedent is implicated in the case. These identified rules form the matching rules set called Rm. If Rm is not null, we select the first rule where the consequent of the rule is missing value. If Rm is null, we use the value that appears most frequent in that attribute as the missing value. The following is algorithmic description of missing data completing:               Figure 1.  algorithmic description of missing data completing

III. DATA DISCRETE ALGORITHM INTRODUCTIONS Data discretization technique can be used to reduce the  number of values for a given continuous attribute by dividing the range of the attribute into several intervals. Interval labels can then be used to replace the actual data values. Replacing numerous values of a continuous attribute by a small number of interval labels can thereby reduce and simply the original data [6]. Several methods have been proposed to discretize data as a preprocessing step for the data mining process. Roughly, we can divide the approaches into two classes [5]:  ? Unsupervised methods. "Blind" methods, where we have no classification information available for the object being considered. These methods rely on assumptions of the distribution of the attribute values.

? Supervised methods. Classification information is available, and this information can be taken into consideration when discretizing the data. A common denominator can be used to minimize the number of objects from different decision classes into the same discretization class.

In 1992, Kerber put forward heuristic algorithm ChiMerge to discrete the continuous attribute which based on 2?  statistics. Different from the conventional top-down splitting strategy, ChiMerge algorithm begins with initialization, adopts bottom-up strategy and consists of a series of merging step. It continues to merge the intervals until a stop condition is satisfied. The stop condition depends on an important parameter ?  which is decided artificially. The ChiMerge algorithm have been widely used and referred in machine learning papers, and adopting this method came as a natural choice. The algorithm has been refined into a modified Chi2 version [4]. In this paper, we use Chi2 algorithm to discrete data. The more details about Chi2 algorithm see [4].



IV. EXPERIMENT RESULTS Chi2 algorithm is implemented with C language and visual  C++ is used as programming platform. The experiment is conducted in such environment: windows XP SP2 operating system, Intel(R) Pentium(R) ? 2.40GHz CPU, 768M memory.

The three medical data sets used in this experiment are gotten from UCI machine learning repository. The first data set, named bupa.data about liver function disordered, has six attributes. The second data set, named diabetes.data about diabetes, has five attributes. The third data set, named new- thyroid.data about thyroid, has eight attributes. 20% missing values are introduced in those data sets. The association rules is produced on those data sets under the condition minsup = 0.2 and minconf = 0.5 repectively. We are interested to find that some discrete granularity of all attributes can?t be the same on the same granularity. For example, in Table 1, in D1, A1, A2, A3, A4, A5, A6 has granularities as following: 2, 2, 2, 2, 2, 1 respectively. Why not the same? Because those two intervals used to represent the data of A6 are so ?similar? and the Chi2 algorithm will combines them. Fig.1 shows that different data discrete granularity has little effect on ARMV, but as the data discrete granularity decreases, accuracy of ARMV inclines to decrease.

TABLE III.  THREE DATA DISCRETIZATION OF BUPA.DATA  A1 A2 A3 A4 A5 A6 CA DD1 2 2 2 2 2 1 -------  DD2 3 4 3 4 3 3 ------- DD3 7 6 6 8 6 7 -------  (Remark: A = attribute, DD = data discrete granularity, CA = categorical atrribute. the ab. in following is the same as this)  TABLE IV.  THREE DATA DISCRETIZATION OF NEW-THYROID.DATA  A1 A 2 A 3 A 4 A 5 CA DD1 2 2 2 2 2 --------- DD2 4 4 4 3 4 --------- DD3 6 6 8 8 6 ---------    TABLE V.  THREE DATA DISCRETIZATION OF DIABETES.DATA  A1 A2 A3 A4 A5 A6 A7 A8 CA DD1 2 2 1 1 1 2 1 2 --- DD2 4 4 4 4 3 3 4 4 --- DD3 6 6 8 6 7 6 7 7 ---       1) Algorithm SelectARToCompleteMV 2) Begin 3) Rm = ? ; 4) for each case Ti ?  the test data set do  // finding matching rules set Rm 5)    for each rule ?  Rules set do 6)         if antecedent(rule) is implicated Ti then  7)             Rm= Rm ? rule; //adding the rule to Rm 8)        endif 9)   endfor // select the most priority rule 10)   if Rm?null then 11)       Ri=First(Rm); 12)   endif 13)   Imputation data=consequent(Ri); 14) endfor 15) end    10%  30%  50%  70%  90%  DD1 DD2 DD3  DD = data discretization  a c c u r a c y  r a t e  bupa New-thyroid diabetes    Figure 2.  accuracy change under different data discretization

V. CONCLUSIONS Association rules are approximate functional dependency. It  plays an important role in supermarket basket analysis. In recent years, the research on ARMV has been on the upgrade, seeing [2, 3, 7]. Compared with RBE (Robust Bayesian Estimator) [11], ARMV achieves a better performance in completing missing values and has a good applicative perspective. But most of researches are on discrete data sets.

This paper investigates influence of the different data discretization of continuous attribute on using ARMV.

Experimental results show that data discretization granularity of continuous attribute has no great effect on ARMV, but as the data discretization granularity decreases, the accuracy of ARMV has the trend to decrease. The result has some guide meaning for future research of ARMV on non-discrete data set.

Future research is conducting more jobs on more data set.

