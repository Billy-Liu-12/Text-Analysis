

Abstract?The ability to learn classification rules from data is important and useful in a range of applications. While many methods to facilitate this task have been proposed, few can derive classification rules that involve ranges (numerical intervals). In this paper, we consider how range-based classification rules may be derived from numerical data and propose a new method inspired by classification association rule mining. This method searches for associated ranges in a similar way to how associated itemsets are searched in categorical attributes in association rule mining, but uses class values to guide the search, so that only those ranges that are relevant to the derivation of classification rules are found. Our preliminary experiments demonstrate the effectiveness of our method.



I. INTRODUCTION  The problem of learning classifiers from data has been extensively studied within data mining and machine learning research. One approach is to derive a set of classification rules, either directly using rule induction, or indirectly from other learning outcomes, such as decisions trees. For example,  ???? = ???? and ????? = ????? ?????? = ??? is a classification rule which can be used to determine whether to play tennis or not based on temperature and humidity. Such rules allow explicit classification knowledge to be examined, thus are particularly useful where not only accurate prediction of an outcome is required, but also good characterisation of how this prediction is made is needed. For example, in the process industry, past performance data is often analysed to help determine how processes may be optimised. In so doing, it is important to establish how input variables should be set in order to achieve a certain output, rather than simply to predict what the outcome will be for any given input.

To derive classification rules from data, the majority of the proposed methods follow a ?cover and remove? strategy [8]. That is, a classification pattern (rule) is heuristically formed to cover (classify) a subset of the data as ?well? as possible, and this subset is then removed from the data. The process is repeated on the remaining data until all the data are covered. This strategy has been successfully applied to categorical attributes, but may be ineffective for numerical attributes. This is because a very large number of patterns can be formed from numerical data. To address this issue, the existing methods typically resort to discretisation or data split (binary or multiway) [6]. However, criteria for good data  discretisation or split are not easy to set, and the classification rules resulting from such techniques are not always easy to interpret.

This work focuses on how range-based classification rules may be derived from numerical data. A range-based classifi- cation rule has the form  ?1, ?2, . . . ?? ? ? where each ?? is an ??????????, ????????? pair and ? is a categorical class value. For example,  ??? ? [25, 30] and ???? ? [2000, 3000]? ????? = ??? is a range-based classification rule, which represents the knowledge that if someone?s age is in [25, 30] and their loan amount is [2000, 3000], then they will repay the loan. It is easy to see that this type of rule naturally captures the classification patterns in numerical data.

Ideally, each of the discovered range-based classification rules would cover as many cases in the dataset as possible and each of them would be highly correct too. This requires the determination of ?right? intervals, as observed by Srikant and Agrawal [7]:  ? if an interval is too small, a rule containing this interval may not cover many cases. As a result, rules that are nearly as specific as the data itself will be generated.

? if an interval is too large, a rule containing this interval may cover many cases, but a good proportion of them may not represent correct classifications. As a result, rules that are not correct enough will be generated.

Some attempts have been made to extract ?right? intervals from numerical data. Srikant and Agrawal [7] proposed to use equi-depth partitioning to group individual data items into initial intervals, and then to allow consecutive ranges to be combined into a single range based on a user specified threshold. This technique, however, does not work well when the data is skewed. In response, Wang et al. [9] proposed a method based on different merging criteria. Instead of requiring individual data items to be grouped initially, their method successively merges neighbouring values into a range.

However, this method requires a so-called template to be used, effectively requiring a potential classification rule structure to be specified before range analysis can take place. In [3]     Fukuda proposed a method inspired by image segmentation.

This technique can find rectangular and admissible (connected, x-monotone) regions from two dimensional numerical data efficiently, but it is limited to finding associated ranges from two attributes only.

In this paper, we propose a new method for finding range- based classification rules from numerical data. Our approach is inspired by the classification association rule mining ap- proach [6], [5] which searches for associations among items in categorical attributes with respect to class labels, and then turns such associations into classification rules. We follow the same principle, but search for associated ranges in numerical attributes instead to form range-based classification rules.

Our preliminary experimental results show that our technique works effectively on numerical attributes.

The rest of the paper is organised as follows. In Section 2, we give the necessary definitions while in Section 3, we intro- duce our algorithm. Section 4 briefly discusses experimental results followed by the conclusion in Section 5.



II. PRELIMINARIES  Without loss of generality, we assume that data is contained within a single table ? (?1, ?2, . . . , ??, ?), where each ?? , 1 ? ? ? ?, is a numerical attribute and ? is a categorical class attribute. We denote the ?-th tuple of ? by ?? = ???,1, ??,2, . . . , ??,?, ???, where ??,? ? ?? , 1 ? ? ? ?.

We may drop ?? from ?? when it is not needed in the discussion.

Definition 1 (range): Let ? and ? be two values in the domain of attribute ? such that ? ? ?. A range over ?, denoted by [?, ?]?, is a set of values in ? that fall between ? and ?.

Definition 2 (cover): Let ? = [?, ?]?? be a range over attribute ?? . ? is said to cover tuple ?? = ???,1, ??,2, . . . , ??,?? if ? ? ??,? ? ?. We denote the set of tuples covered by ? by ?(?).

Definition 3 (associated ranges): Let ?1 = [?1, ?1]?1 , ?2 = [?2, ?2]?2 , ? ? ? , ?? = [??, ??]?? be a set of ranges over attributes ?1, ?2, . . . , ?? respectively. ?1, ?2, . . . , ?? are associated ranges if we have ?(?1)? ?(?2)?? ? ?? ?(??) ?= ?.

Definition 4 (range-based classification rule): Let ? be a class value and ?1, ?2, . . . ?? be a set of associated ranges.

?1, ?2, . . . , ?? ? ? is a range-based classification rule.

According to Definition 4, many range-based classification rules can be formed from a given table ? , for example, each tuple of ? can be such a rule. Some of the rules will be too specific to be useful. To determine the quality of a rule, we introduce three measures below.

Definition 5 (Support): Let ? be a table and ? : ?1, ?2, . . . ?? ? ? be a range-based classification rule derived  from ? . The support for ? in ? is  ?(?) = ??(?1) ? ?(?2) ? ? ? ? ? ?(??)?  ?? ? where ? ? ? denotes the size of a set.

Definition 6 (Confidence): Let ? be a table and ? : ?1, ?2, . . . ?? ? ? be a range-based classification rule derived from ? . The confidence for ? in ? is  ?(?) = ??(?1) ? ?(?2) ? ? ? ? ? ?(??) ? ?(?)? ??(?1) ? ?(?2) ? ? ? ? ? ?(??)?  where ?(?) denotes the set of tuples that have class value ? in ? .

Definition 7 (Density): Let ? be a table and ? : ?1, ?2, . . . ?? ? ? be a range-based classification rule derived from ? . The density for ? in ? is  ?(?) = ??(?1) ? ?(?2) ? ? ? ? ? ?(??)? ??(?1) ? ?(?2) ? ? ? ? ? ?(??)?  The support and confidence measures are similar to those traditionally used in association rule mining. The density indicates a rule?s concentration (i.e. how often a tuple covered by a range supports the rule in ? ). The following example explains these definitions.

Example 1: Suppose that we have the following table:  ? ?1 ?2 ?3 ?  ?1 0.22 0.45 0.45 ?2 ?2 0.33 0.21 0.76 ?2 ?3 0.58 0.74 0.35 ?2 ?4 0.13 0.20 0.24 ?1 ?5 0.74 0.56 0.78 ?1 ?6 0.45 0.60 0.65 ?1  and we have a rule ? : [0.22, 0.45]?1 ? [0.45, 0.60]?2 ? ?1.

Then we have  ?(?) = ??([0.22, 0.45]?1) ? ?([0.45, 0.60]?2)?  ?? ? = ?{?1, ?2, ?4, ?6} ? {?1, ?5, ?6}?  = 1/3  ?(?) = ??([0.22, 0.45]?1) ? ?([0.45, 0.60]?2) ? ?(?1)? ??([0.22, 0.45]?1) ? ?([0.45, 0.60]?2)?  = ?{?1, ?2, ?4, ?6} ? {?1, ?5, ?6} ? {?4, ?5, ?6}?  ?{?1, ?2, ?4, ?6} ? {?1, ?5, ?6}? = 1/2  ?(?) = ??([0.22, 0.45]?1) ? ?([0.45, 0.60]?2)? ??([0.22, 0.45]?1) ? ?([0.45, 0.60]?2)?  = ?{?1, ?2, ?4, ?6} ? {?1, ?5, ?6}? ?{?1, ?2, ?4, ?6} ? {?1, ?5, ?6}? = 2/5     It is easy to see that a rule with high support, confidence and density is desirable. Unfortunately it is not always possible to obtain such rules. In the following, we describe a heuristic method for finding range-based classification rules that meet minimum support, confidence and density requirements.



III. A HEURISTIC METHOD  A brute-force solution to find range-based classification rules from a given table ? is to examine all possible com- binations of ranges across all attributes to see if they have sufficient support, confidence and density. Assuming that each attribute has ? distinct values and we have ? attributes, then the brute force solution will need to examine ?(?2?) number of associated ranges. This is too expensive to compute for a non-trivial ?.

In this paper, we describe a more efficient algorithm.

Our proposed method works as follows. Given a table ? (?1, ?2, . . . , ??, ?), we attempt to find range-based classi- fication rules ? : ?1, ?2, . . . , ?? ? ?? by first finding associated ranges ?1, ?2, . . . , ?? for a given class ?? ? ? that have at least minimum support (????) and density (????), and then checking if the rule has the minimum confidence (????). ????, ???? and ???? as specified by the user. Algorithm 1 outlines the method.

Algorithm 1 R-CARM  input: ? (?1, ?2, . . . , ??, ?), where each ?? is numerical and ? is categorical ????, ????, ????, the user defined thresholds  output: ?, a set of range-based classification rules  1. for each ?? ? ? do 2. ??? ? ??????(?, ??) 3. ??1 ? {[????, ????]?1 , [????, ????]?2 , . . . ,  [????, ????]??} from ??? 4. for (? = 1, ??? ?= ?, ?++) do 5. ??? ? ? 6. for each ?? ? ??? do 7. ? ? ??????? ????? (??, ??) 8. for each ? ? ? do 9. if ?(?) ? ???? and ?(?) ? ???? then 10. ??? ? ??? ? ? 11. if ?(?? ??) ? ???? then 12. ?? ? ? {?? ??} 13. ???+1 ? ???????? (???) 14. return ?  First, we select a set of tuples from ? that has a class value ?? and store them in ??? (step 2). Then, we derive a set of ranges [????, ????]?? , one for each attribute ??, from ??? (step 3). These ranges are formed by finding the minimum and maximum values of ?? in ??? . We call these ranges candidate ranges, as they may not have the required minimum support and density to form rules. To illustrate steps 1-3 of Algorithm 1, we provide Example 2 below.

Example 2: Consider the table given in Example 1 again.

For class ?1, we have ??1 as  ? ?1 ?2 ?3 ?  ?4 0.13 0.20 0.24 ?1 ?5 0.74 0.56 0.78 ?1 ?6 0.45 0.60 0.65 ?1  and for attributes ?1, ?2 and ?3 in ??1 we have [????, ????]?1 = [0.13, 0.74], [????, ????]?2 = [0.20, 0.60] and [????, ????]?3 = [0.24, 0.78], respectively.

In steps 4 - 13, we repeatedly perform the following. For each candidate range ??, we first perform a range analysis on ?? (step 7). To illustrate why this analysis is necessary, consider a single range [????, ????]?? . Observe that tuples not covered by this range cannot possibly contribute to classification accuracy for ?? , only the sub-ranges of this range may result in better classifications for ?? . To illustrate this, consider [????, ????]?1 = [0.13, 0.74] in Example 2.

The range covers the tuples in the table given in Example 1 as follows:  ?4 ?1 ?2 ?6 ?3 ?5  0.13 0.22 0.33 0.45 0.58 0.74 ?1 ?2 ?2 ?1 ?2 ?1  Clearly, sub-range [0.45, 0.74]?1 would give a greater classi- fication accuracy for ?1, although the support for it is reduced in this case.

To find such sub-ranges with respect to the given support, confidence and/or density constraints, we use a function anal- yse range in step 7. This function finds sub-ranges based on the solution to the max sum problem described in [1]. That is, for class ?? , it scores the tuples in ??? that are covered by the range with 1, if their classes are ?? , and with ?1 otherwise. The sub-ranges with the max sum of these scores that satisfy the given constraints are the sub-ranges that need to be returned. However, since multiple sub-ranges of ?? may satisfy the specified constraints, analyse range finds all such sub-ranges and stores the result as a set in ?.

In steps 8-9, we check for each sub-range ? in ?, if it has sufficient support and density (step 9). If it has, we store it in ???, which will be used to form ?larger? association of ranges in the next iteration (step 10). Since both support and density measures are monotonic, the associated ranges that do not have enough support or density are not considered.

This is because these ranges cannot be used to generate larger association of ranges in subsequent iterations. Then, in steps 11-12, we check if the ranges found can be used with ?? to form range-based classification rules that have sufficient confidence, and, in step 13, we generate candidate associated ranges from ??? for the next iteration.

Example 3: To explain the steps 4-12 of Algorithm 1, we show how the associated ranges for ?1 in the first iteration (i.e.

? = 1) can be computed. We have ??1 = {[????, ????]?1 = [0.13, 0.74], [????, ????]?2 = [0.20, 0.60], [????, ????]?3 = [0.24, 0.78]} from step 3 according to Example 2, and as- suming that ???? = 0.6, analyse range derives the following sub-ranges from the three ranges:  ? = {[0.45, 0.74]?1 , [0.56, 0.60]?2 , [0.65, 0.78]?3} The support and density for each range contained in ? are computed as follows:  ?([0.45, 0.74]?1) = ?{?3, ?5, ?6}? ?? ? = 3/6  ?([0.56, 0.60]?2) = ?{?5, ?6}? ?? ? = 2/6  ?([0.65, 0.78]?3) = ?{?2, ?5, ?6}? ?? ? = 3/6  ?([0.45, 0.74]?1) = ?([0.56, 0.60]?2)  = ?([0.65, 0.78]?3) = 1  Assuming that ???? = 0.5 and ???? = 0.7, then two ranges have sufficient support and density, hence we have ??1 = {[0.45, 0.74]?1 , [0.65, 0.78]?3}. From these ranges, we can then form two range-based classification rules:  ?1 : [0.45, 0.74]?1 ? ?1 ?2 : [0.65, 0.78]?3 ? ?1  Since both of these rules have required minimum confidence, they are both stored in ?.

How candidate ranges are generated from ??? in step 13 needs some further explanation. Once every candidate range in ??? is processed, the algorithm prepares ???+1 for the next iteration by extending current ranges in ??? with one additional range using the function generate. This function is similar in principle to the one used in the Apriori algorithm for generating candidate itemsets in association rule mining, but has an additional step to adjust ranges.

Algorithm 2 describes the generate function. After initializing ???+1 to the empty set (step 1), we scan through the associated ranges in ??? in turn (step 2). For each ??, we search through ??? again to find another set of associated ranges ???, such that ?? and ??? differ by only one attribute (steps 3-6). ???????1 and ???? are two functions for retrieving the first ? ? 1 and the last attributes from the associated ranges, respectively. Once such a pair is found, a new, extended set of associated ranges is formed (step 7).

In conventional mining of associated categorical items, this is all that is required to be done to extend associations. For ranges, however, we must adjust the combined ranges, as two separate sets of associated ranges may not cover the same set of tuples. This is illustrated in the following example.

Example 4: Consider the two ranges found in Example 3 again: ?1 = [0.45, 0.74]?1 and ?2 = [0.65, 0.78]?3 . We have ?(?1) = {?3, ?5, ?6} and ?(?2) = {?2, ?5, ?6} in the original table given in Example 1. When the two ranges are combined  Algorithm 2 Generate  input: ???, a set of associated ranges that have sufficient support and density  output: ???+1, a set of candidate associated ranges  1. ???+1 ? ? 2. for each ?? ? ??? do 3. ?? ?????????????(??) 4. for each ??? ? ??? do 5. ?? ? ?????????????(???) 6. if ???????1(?) = ???????1(??) and  ????(?) < ????(??) then 7. ?? ? [????, ????]?????(?), . . . ,  [????, ????]????(?), [????, ????]????(??) 8. ?? ? ?????? ????? (??) 9. ???+1 ? ???+1 ? ?? 10. break 11. return ???+1  to form [0.45, 0.74]?1 ? [0.65, 0.78]?3 , it is easy to see that ?3 has ?1 = 0.58 which is in ?1, but its corresponding value in ?3 is 0.35 which is not in ?2. So the class value for ?3 in the combined range is undefined, as we have not observed such a case in the training data. Unfortunately, we cannot simply focus on the intersection of the tuples covered by both ranges, as ?3 sits in the middle of ?1 - not considering ?3 would mean that we have to eliminate ?5 or ?6 as well.

To solve this problem, we adjust the ranges using the adjust range function in step 8. For the studies carried out in this paper, we have adopted a very conservative policy: when a tuple covered by one range is not covered by at least one other range in the set of associated ranges, we consider this tuple to be likely to produce a wrong classification and we change its class to null. In Example 4, for instance, the classes of ?3 in ?1 and ?2 in ?2 will both be changed to null in ?(?1) and ?(?2). This will allow the support, confidence and density measures for the extended associated ranges to be discounted in the next iteration.

Once the classification rules are derived, we can process them into a classifier. Various approaches are possible (e.g.

based on confidence of the rules [6], [5]), but for simplicity of explanation, we adopt a simple majority vote scheme in this paper. If no classification rules are applicable, then the test tuple is given the most popular class in the training dataset.



IV. PRELIMINARY EXPERIMENTAL RESULTS  In this section, we report some preliminary experimental results. We evaluated our method against the JRIP rule in- duction method implemented in WEKA [4]. We tested the methods on the Wine dataset [2] which contains 13 numerical attributes and one categorical class attribute, and 178 tuples.

For classification accuracy tests, we varied the amount of training data from 50% to 80% and used the remaining data     as test data. For the parameters used in our method, we fixed ???? = 0.5 so that our method is comparable to JRIP (which adopts ???? = 0.5) and ???? = 0.5, but varied ???? from 3 to 20 (for convenience we specified support as the number of tuples here). The results are shown in Figure 1.

Fig. 1. Accuracy of Classifications  As can be seen, apart from when ???? = 20, the accuracy achieved by our method is comparable to that of JRIP. The poor performance for ???? = 20 and training data set at 50% is expected as the Wine dataset is not large, and requiring ???? = 20 means that not many classification patterns can be obtained from the data. Note that we have used a rather basic method to turn a set of classification rules into a classifier in this experiment. We believe that using mechanisms similar to that reported in [5] could result in greater classification accuracy.

What is more interesting to observe from this experiment is perhaps the number of associated ranges found, as shown in Figure 2. Although we have not studied this fully yet, our observation of experimental results seems to suggest that the associated ranges found can be partitioned into groups, each representing a different way of classifying the data. This is useful as it allows alternative classifiers to be constructed and different interpretations to be obtained.

Fig. 2. Number of Associated Ranges Found

V. CONCLUSIONS  In this paper, we proposed a new method for finding range- based classification rules from numerical data. Our method is inspired by the classification association rule mining approach.

That is, we search for associations among numerical ranges, but our search is guided by class values. This allows accurate range-based classification rules to be derived efficiently. Our preliminary experimental results have shown that the new method is promising.

There are a number of issues still to be addressed in future research. First, there is a need to study how good classifiers may be constructed from the discovered classification rules.

Second, better data structures and alternative measures may be developed to speed up the computation of associated ranges.

Finally, more comprehensive performance studies are needed to verify our results. We plan to study these issues in the near future.

