Data Flow Testing of SQL-based Active Database Applications ?

Abstract  The relevance of reactive capabilities as a unifying paradigm for handling a number of database features and applications is well-established. Active database systems have been used to implement the persistent data require- ments of applications on several knowledge domains. They extend passive ones by automatically performing predefined actions in response to events that they monitor. These reac- tive abilities are generally expressed with active rules de- fined within the database itself. We investigate the use of data flow-based testing to identify the presence of faults in active rules written in SQL. The goal is to improve relia- bility and overall quality in this realm. Our contribution is the definition of a family of adequacy criteria, which re- quire the coverage of inter-rule persistent data flow asso- ciations, and its effectiveness in various data flow analysis precisions. Both theoretical and empirical investigations show that the criteria have strong fault detecting ability at a polynomial complexity cost.

1. Introduction  The manipulation of persistent data has an important role in software-based solutions. Data persistency is an attribute associated with the demand for non-volatile data. Most of the real world applications have that kind of requirement; consequently, database-enabled applications play an impor- tant role in the majority of modern organizations [2]. A database application is a program running in an environ- ment containing one or more databases [9]. Due to its im- portance it is unavoidable to consider quality issues related to the development of database applications.

Faults are inherently present in software production, they are introduced in every phase of the software development  ?This work was partially supported by a CNPq research grant.

process. Testing is the process of executing a program with the intent to make it fail. It is the search for program in- puts that contradict the assertion that the program is correct.

When a failure is perceived, there is an opportunity to look for and remove its causing fault (debugging), contributing to software quality improvement.

The aim of this work is to contribute to the quality im- provement of SQL-based applications. An SQL-based ap- plication interacts with the database using SQL (Structured Query Language), the most used language for database ap- plication development [6, 4]. Differently from imperative and object-oriented languages, such as C and Java, SQL has received little attention from the academic community.

It is a paradox, since there is a large amount of source code written in SQL that needs systematic testing methods.

An active database system monitors specific events and, as they occur, triggers appropriate responses at the appro- priate time. The expected behavior is represented by ac- tive rules, also called event-condition-action rules (or ECA rules), defined and stored in the database. When the trig- gering event occurs, the condition is evaluated against the database state and an action is activated if the condition is true. From the application?s point-of-view, part of its func- tionality is expressed in rules, such that the control and data flow are transferred from the application to the active rules during execution.

The question associated to this research is the lack of testing techniques in the context of active rules written in SQL. The interaction between active rules is frequently a source of faults, which makes their use to be avoided by developers of active database applications. Little is known about failures manifested when active rules interact. This motivates new testing techniques tailored to uncover faults in the interaction between active rules, which are not dis- covered during the testing of individual rules.

We propose and analyze testing criteria for active rules written in SQL. An interaction model of active rules is pro- posed to abstract interaction associations that make the ba-   DOI 10.1109/ICSEA.2008.57    DOI 10.1109/ICSEA.2008.57     sis for the testing requirements. In the context of data flow based structural testing, a family of adequacy criteria called Interaction Between Active Rules based Criteria is defined; these criteria require the coverage of interaction associa- tions. The criteria are an extension to the All-Uses crite- rion [14] and explore persistent data flow relations associ- ated to rule interactions. We investigated the complexity of the criteria and empirically evaluated their fault detect- ing ability at different data flow analysis precisions. A tool called ADAPT-TOOL (Active Database APplication Test- ing TOOL for active rules written in SQL) was built to support the experiment. The results indicate that: i) fault- detecting efficacy was 2/3 of the adequate test cases and reached higher values for the lower data flow analysis preci- sion; and ii) coverage of interaction associations at smaller granularities did not improve the fault detecting ability.

2 Active Rules in SQL  Active rules in SQL, called triggers, are activated by a database state transition. The source of events is limited to operations on the database structure, specifically a re- lation state transition. The rule condition consists of any SQL predicate, including sub-queries and user defined func- tions [10]. Three components make up the model: i) the event set: the set of data manipulation operations being monitored; ii) the condition: a predicate that references the current database state and the rule transition values; and iii) the action: a sequence of data manipulation operations.

The transition values associated to a given active rule exe- cution are transient data that are being inserted, updated or excluded by the event operation.

The notation ri(ei, ci, ai) indicates that the rule ri is described by event ei, condition ci and action ai. In the control flow context, similarly to conventional programs, a rule r is represented by a directed-graph, given by G(r) = (N,B, e, x), where: N is the set of nodes in the rule; B is the set of branches; the entrance and exit nodes are, respec- tively, e and x; the entrance node is also called event node.

Figure 1(b) shows the control flow graph of the SQL active rule in Figure 1(a); dashed edges represent exceptions due to the execution of SQL manipulation commands; nodes 1, 2, and 7 represent the rule event, the condition rule, and graph exit node, respecttively; and nodes 3, 4, 5, and 6 rep- resent the rule action.

3 Persistent Data Flow  Similarly to variables in regular programs, database vari- able occurrences, also called persistent variables, are clas- sified as definition and use. Due to their persistent nature, every definition and use are global to application programs:  CREATE TRIGGER Reorder AFTER UPDATE OF PartOnHand ON Inventory WHEN (:New.PartOnHand < :New.ReorderPoint) FOR EACH ROW DECLARE NUMBER X; BEGIN SELECT COUNT(*) INTO X FROM PendingOrders WHERE Part = :New.Part; IF X = 0 THEN INSERT INTO PendingOrders VALUES (:New.Part, :New.OrderQuantity, SYSDATE); END IF; END;         (b)(a)  Figure 1. Example of an active rule in SQL: (a) source code (b) control flow graph.

the variable state remains the same, available to others, even after the execution of the application. Persistent data flow in SQL occurs due to the execution of data manipulation com- mands: insert, delete, update and select. The occurrence of these commands is represented by an exclu- sive node in the corresponding data flow graph.

The data flow in manipulation commands is character- ized by: definition, use and use-definition. The first is the result of the execution of the insert command; the sec- ond corresponds to the execution of the select command; and the last one is the result of the execution of the update and delete commands. Data flow of implicit operations are also considered; for example, the opening of a cursor is a persistent data use.

The ddef notation represents persistent data definitions: ddef(i)= {variable v | v is a database variable defined in node i}. The definition of a persistent variable is a persis- tent definition. The persistent use may affect the control flow, due to the occurrence of exception conditions. Per- sistent uses occur in the output edges of SQL nodes, due to the potential exceptions. The duse notation represents the persistent data use: duse(i,j) = {variable v | v is a database variable used in the edge (i,j)}. The use of a persistent vari- able is a persistent use.

A persistent data flow association, persistent associa- tion, ddef-duse-association, or simply ddua, is a triple [i, (j, k), v], such that: v ? ddef(i); v ? duse(j, k); and there is a definition-free path with respect to (w.r.t.) v from i to (j, k). Alternatively, if ddu(v,i) = {edges (j,k) | v ? ddef(i), v ? duse(j,k) and there is a definition-free path w.r.t.

v from i to (j,k)}, a persistent association is represented by the triple [i, (j, k), v], such that (j, k) ? ddu(v, i).

4 Data Flow Associations based on Active Rule Interactions  The execution behavior of a rule set reflects the reactions of the system at run time: active rules are triggered in re- sponse to events raised by database applications and by the processing of active rules themselves. The advantages of active databases are reduced by the cost associated with the inherent complexity of their interactions and user submitted data manipulation [1]. Even a small number of active rules can be complex, difficult to understand and manage [18].

Rule interactions are dependencies between rules that arise at run time execution. Given two rules, ri and rj , not necessarily distinct, described by (ei, ci, ai) and (ej , cj , aj), respectively, the rule interactions are defined as:  ? A/E interaction: the execution of ai signals ej , or a component of ej ;  ? A/C interaction: the execution of ai modifies the same database objects accessed during the evaluation of cj ;  ? A/A interaction: the execution of ai and aj access the same database objects; explicitly, ai modifies one or more database objects accessed during the execution of aj .

Next we introduce the persistent data flow associations based on active rule interactions, called interaction associ- ations. A/E interactions are represented by the types of data flow associations presented below.

(i) ddef-duse-AE association: is the 5-tuple [x, (y, z), v, r?, r??], where r? and r?? are rules, not necessarily distinct, from an active rule set R; v is a persistent variable; x represents an action node of rule r? such that v ? ddef(x); y represents the event node of rule r??; (y, z) is an edge in the r?? graph such that v ? duse(y, z); r?? is triggered due to the definition of v in x; and this triggering causes the use of v due to the definition of transition variables of r??.

The interactions based solely on data dependency are in- troduced next. They require the execution of subpaths of rules that take part in the interaction. A subpath in G(r) is a finite sequence of nodes p = (n1, n2, ? ? ? , nk) such that, for each i, 1 ? i < k, (ni, ni+1) ? A. A subpath formed by the concatenation of two subpaths p1 and p2 is denoted by p1.p2. An initial subpath of rule r is a subpath in which the first node is the entrance node (event node) of r. A final subpath of rule r is a subpath in which the fi- nal node is the exit node of r. A complete path in G(r) is an initial subpath followed by a final subpath. The subpath (i, j, ? ? ? , nk) is an edge (i, j) initiated subpath. The sub- path (n1, n2, ? ? ? , i, j) is an edge (i, j) terminated subpath.

The subpath (ni, n1, ? ? ? , nm, nk), m ? 0, with no defini- tion of variable v in nodes n1, ? ? ? , nm, is called a definition free subpath w.r.t. v from node ni to node nk and from node ni to edge (nm, nk).

(ii) ddef-duse-AC association: is the 5-tuple [x, (y, z), v, r?, r??], where r? and r?? are rules, not necessarily distinct, from an active rule set R; v is a persistent variable; x represents an action node of rule r? such that v ? ddef(x); y represents the condition node of rule r??; (y, z) is an edge in the r??  graph such that v ? duse(y, z); there are a final subpath in r? graph initiating in node x and an initial subpath in r?? graph ending in the edge (y, z), denoted by p? and p??, respectively, such that p?.p?? is a definition free subpath w.r.t. v.

(iii) ddef-duse-AA association: is a 5-tuple similar to ddef-duse- AC association, with the difference that y represents an ac- tion node of rule r??.

The following associations are the result of control flow transfers due to inter-rule triggering. Call and return asso- ciations are introduced, according to location of the defini- tion, triggerer or triggeree rule, respectively.

(iv) call-ddef-duse-AC association: is the 6-tuple [x, (y, z),- v, m, r?, r??], where r? and r?? are rules, not necessarily dis- tinct, from an active rule set R; v is a persistent variable; x and m represent action nodes of rule r?, not necessar- ily distinct, such that v ? ddef(x) and m represents an event triggering operation; y represents the condition node of rule r??;(y, z) is an edge in the r?? graph such that v ? duse(y, z); there are a subpath in r? graph initiating in node x and ending in node m, and an initial subpath in r?? graph ending in the edge (y, z), denoted by p? and p??, respectively, such that p?.p?? is a definition free subpath w.r.t. v.

(v) call-ddef-duse-AA association: is a 6-tuple similar to call- ddef-duse-AC association, with the difference that y repre- sents an action rule node of r??.

(vi) return-ddef-duse-AA association: is an association similar to call-ddef-duse-AA association, with the difference that the persistence data flow is from triggeree to triggerer rules.

5 Definition of the Testing Criteria  A testing adequacy criterion C is a function C(R,S, T ) ? {true, false}; where true means that rule set R was adequately tested with respect to specification S by test set T according to criterion C.

To evaluate coverage of interaction associations we need to dynamically analyse the persistent data along executed paths. The effectiveness of coverage criteria depends not only on the selected paths but also on the test data for those paths [3].

Consider that the application of a test case set produces the tuple < ?,? > such that: ? is the resulting path set, ? = {?1, ? ? ? , ?1}, p > 0; and ? is persistent data defined and used along each path of ?, ? = {?1, ? ? ? , ?p}, such that ?k ? ? is persistent data defined and used along path ?k ? ?. Consider a rule set R with specification S; r? and r?? are rules of R, not necessarily distinct, described by     G(r?) = (N ?, A?, e?, x?) and G(r??) = (N ??, A??, e??, x??), respectively.

To introduce testing criteria that exploit the inter-rule control flow due to triggering between rules, each ?k ? ? is described by (e?, ? ? ? ,m).(e??, ? ? ? , x??).(succ(m), ? ? ? , x?) such that succ(m) is a successor node of m in r?, so that the execution of the operation represented by node m can raise the event of r??.

? The tuple < ?, ? > satisfies the criteria all-uses-AE, call- all-uses-AC, call-all-uses-AA, and return-all-uses-AA for rule set R with specification S if, for each pair of rules r?  and r?? of R, not necessarily distinct, it covers (satisfies) all interaction associations ddef-duse-AE, call-ddef-duse-AC, call-ddef-duse-AA, and return-ddef-duse-AA, respectively.

To introduce testing criteria based solely on persistent data flow dependency, consider that each ?k ? ? is de- scribed by (e?, ? ? ? , x?).(e??, ? ? ? , x??) due to sequential exe- cution of rules: the execution of r? is followed by the exe- cution of r??.

? The tuple < ?, ? > satisfies the criteria all-uses-AC and all- uses-AA for rule set R with specification S if, for each pair of rules r? and r?? of R, not necessarily distinct, it covers (satisfies) all interaction associations ddef-duse-AC and ddef- duse-AA, respectively.

5.1 Criteria Complexity  The complexity of testing criteria is used to determine their application cost; it is defined in the worst case sce- nario, even if in practice this situation is unlikely to occur.

Although Interaction Between Active Rules based Criteria are an extension to All-Uses criterion, in which the worst case scenario is a function of control flow structures, their complexities are indexed by manipulation operations that possess both definition and use of persistent data, such as the SQL command update. The discussion below demon- strates that the criteria has polynomial order complexity.

In the context of persistent data flow, test adequacy eval- uation requires the dynamic analysis of defined and used data along of paths exercised by test cases. Similar to pro- grams with pointers [13], a test set that exercises every path does not necessarily exercise every persistent def-use asso- ciation, in contrast to the original theory, where a test set exercising all paths is guaranteed to exercise every associ- ation. So that, in the worst case a persistent variable is not redefined in all paths from the definition location to the use location.

Given two rules,R1 andR2, not necessarily distinct, and consider m as the number of occurrences of both definition and use of persistent data (e.g., update command) for a given database entity. Such occurrences are nodes in graphs of rules R1 and R2. Each node has persistent data flow to output edges of all nodes including itself.

The complexity of criterion all-uses-AA is 2.m2. Con- sider the graph Gm?1, m ? 1 nodes, whose complexity is 2.(m? 1)2. The graph Gm is obtained by a node inclusion in graph Gm?1. As a result, new persistent associations are established: (i) 2.m associations between the new node and output edges of all nodes including itself; and (ii) 2.(m?1) associations between existing nodes and output edges of the new node. Therefore, Cm = Cm?1 + 2.m+ 2.(m? 1) = 2.m2. Using a similar reasoning, the complexity of cri- teria all-uses-AC, call-all-uses-AA, return-all-uses-AA, and call-all-uses-AC are 2.m, 2.(m3 ? m2), 2.(m3 ? m2) e 2.(m2 ?m), respectively.

6 Empirical Analysis  To analyze the data flow associations created from the database usage we should decide on the level of granularity of the database variables at which we will trace their defi- nitions and their later uses [4]. The granularity, also called data flow analysis precision, determines the rigor of data flow analysis and defines coverage levels for persistent data flow relations.

According to Kapfhammer and Soffa [9] the granular- ity levels are: database, relation, attribute, tuple, and at- tribute value. Therefore, in the context of persistent data flow based testing, testing requirements are pairs < crite- rion, granularity > which coverage analysis consider ex- ercised paths and manipulated persistent data along these paths.

The experiment was performed to investigate the appli- cability of the Interaction Between Active Rules based Cri- teria and to rigorously analyze their fault-revealing effec- tiveness at different data flow analysis precisions. A rule set of SQL rules (Rx) with 74 interaction associations was ap- plied to the Oracle database management system. The Or- acle system has been frequently used by the academic com- munity [4] and supports the interaction model described in Subsection 4. This subject is derived from a real applica- tion, so that it is representative of authentic persistent data flows.

As the basis of our empirical investigation we have de- fined two aspects: i) applicability in a real setting; and ii) effectiveness in revealing faults. The experimental setup was established to gather information to help the evaluation of such aspects. We will consider the proposed criteria as effective if the application of an adequate test data is capa- ble of revealing the existing faults in the subject programs.

In addition to that, we consider the criteria to be applica- ble if the number of test cases required in a real application is substantially smaller than that determined by their the- oretical complexity; that is, practical use of the criteria is feasible.

6.1 Experiment Design  In addition to the real faults identified, versions of the rule set Rx were built by seeding one fault in each version.

The manipulation fault type list we introduced [11] and the mutation operators of Tuya et al. [16] were used to derive 26 faulty versions of Rx. The set of manipulation commands in Rx that can cause persistent data flow of interaction as- sociations was targeted during fault seeding. The manipu- lation command and the fault type were randomly selected for each faulty version to avoid fault seeding bias.

In addition to the criteria effectiveness evaluation, all faulty versions were tested at granularities relation, at- tribute, tuple, and attribute value, to observe whether higher data flow analysis precisions improve the fault-revealing ability. The triggering commands were randomly gener- ated and covered all event rule operations of Rx. The test- ing database generation approach was based on Ostrand and Balcer [12] and Chays et al. [2]: basically, the tester pro- vided a list of values that were conceptually different for each database attribute, according to its domain. They were combined to derive commands of insertion into database relations. Commands whose execution did not satisfy any database integrity constraints were discarded.

6.2 Execution of the Experiment  All rules of Rx were enabled for triggering before the application of each test case; this establishes the real opera- tion ofRx. The execution of the faulty rule was not isolated, and the triggering between rules could occur in a chain.

The experiment application was supported by ADAPT-TOOL ? Active Database APplication Testing TOOL for active rules written in SQL. The tool builds a test database by using structures of the relational model and focuses on the following functions: test data generation (triggering commands and input database), control of rule set faulty versions, application (and re-application) of test cases, testing oracle, and evaluation of test cases by granularity.

6.3 Analysis of the Results  Table 1 shows the summary of application of 632 test cases. The table lists the number of test cases, coverage of interaction association per granularity and of raised ex- ceptions. The columns are labeled (I), (II), and (III); lines are labeled (a) to (j). Columns (I) and (II) dis- tinguish fault-revealing and non fault-revealing test cases, respectively; column (III) refers to all test cases. Line (a) refers to test cases that did not exercise the faulty node. Test cases that exercised the faulty node but covered no interac- tion association are summarized in line (b). Lines (c), (d),  (e), and (f) represent test cases related to interaction as- sociation coverage at granularities relation, attribute, tuple, and attribute value, respectively. To understand how the values in lines (c) to (f) were computed, consider the cell (I : d), column (I) and line (d); it states the number of fault-revealing test cases to the granularity tuple, those test cases in which tuple was the highest granularity obtained at all interaction associations covered by them. Raised ex- ceptions are exploited in lines (h) to (j). Line (i) refers to user exceptions ? those raised by the programmer using the command raise. Line (j) refers to persistent manipulation exceptions ? those raised due to manipulation command ex- ecution and not handled by the programmer. Line (h) refers to test cases applied with no exception.

The application of an adequate test data revealed all faults in the active rule set and the number of test cases was substantially smaller than that expected by the theo- retical complexity for each rule set faulty version. To eval- uate the effectiveness of Interaction Between Active Rules based Criteria, a measure was used, denoted by the number of fault-revealing test cases of all adequate test cases, stated by Equation 1.

(I : c) + (I : d) + (I : e) + (I : f) (III : c) + (III : d) + (III : e) + (III : f)  (1)  Equation 1 was used by all data flow criteria, using any model, deriving adequate test sets that include at least one fault-revealing test case. The value 0.6711 resulted from the expression above, denoting that 2/3 of the adequate test cases are fault-revealing. Computing the effectiveness per granularity, the values 0.7053, 0.6970, 0.6864, and 0.5517 were obtained for granularities relation, attribute, tuple, and attribute value, respectively, showing that the effectiveness reaches higher values for the lower data flow analysis pre- cision.

The coverage of interaction association at higher gran- ularities does not improve the fault detecting ability. The coverage at granularity relation was enough to reveal the presence of all faults. In some versions of Rx, the cover- age at granularities tuple and attribute value was non fault- revealing. Two scenarios were observed: i) two state chang- ing commands characterize the interaction association, such as update-update, so that the execution of the second one fix the error due to execution of the first one; and ii) the trig- gering between rules leads to failure elimination due to the sequential execution of state changing commands.

The raised exceptions were traced to help the oracle task.

From the fault-revealing test cases, 33.7% (143 out of 424) raised exceptions, the majority resulting from raise com- mand executions. User exceptions implement behavior ac- cording to the software specification and they should be used to decide on the presence of faults. The instrumen- tation of exception situations required special mechanisms,     Table 1. Summary of fault-revealing and non fault-revealing executed test cases, by granularity and exception type.

(I) (II) (III) Test cases Fault-revealing Non fault-revealing All  (a) Fault not exercised 0 8 8 (b) No coverage 20 2 22  (c) Relation coverage 213 89 302 (d) Tuple coverage 46 20 66  (e) Attribute coverage 81 37 118 (f) Attribute value coverage 64 52 116  (g) all 424 208 632 (h) No exception 281 198 479 (i) User exception 112 10 122  (h) System exception 31 0 31 (j) All 424 208 632  since exception occurrences usually can cause the loss of database transactions, erasing the instrumentation data.

The demonstrated effectiveness is an initial empirical ev- idence. Additional studies are required since several factors are involved in the investigation, such as: test data genera- tion strategy, active rules used in the experiment, and fault types and faults selected for active rules.

6.4 Threats to Validity  A threat is related to the fact that the experiment uses only one set of rules, with several different versions. To compensate for this, the set has several interaction points, which makes its behavior very complex at run time. Such interaction points, specially the triggering between rules, re- sult in some cases in the execution of more than a thousand nodes, increasing the chances of unexpected or incorrect be- havior.

Another limitation is related to the artificial seeding of faults, in addition to real ones, one fault for each version of the rule set, in spite of the random selection of manipulation occurrences and of fault types. In practice it is observed that faults do not occur in isolation; defective programs will in general have more than one fault. We have to point out that a secondary goal of this empirical investigation is to eval- uate different data flow granularities to detect the presence of faults. The isolation of faults allow a finer level of con- trol when analyzing the influence of each granularity in the detection of each fault.

6.5 Related Work  In the active rule context, Vaduva [17] established an ap- proach based on rules sequences, independently of the exis- tence of triggering between rules, to reveal inadequate rules  interactions or improper sequences of rules execution. That work was based on SAMOS [7], an object oriented database system. The goal of each test case is the execution of a spe- cific rule sequence from a rule set. SQL is not used and the structural elements of the rules, such as control and data flow, are not considered in the coverage of a particular se- quence.

Chan et al. [1] focus on testing of active rules written in a declarative and object-oriented language. Test data are generated by reasoning on the flow-graph of all possible branches. Branching arises at the points where the continu- ation or the end of rule processing is decided. The analysis of intra-rule control flow strongly depends on the particular- ities of the specification rule language. Consequently, the method cannot be straightforwardly applied to imperative condition/action languages.

Kapfhammer and Soffa [9] defined testing criteria based on data flow analysis for database applications exploiting several granularity levels. The list of data flow associa- tions is dependent on the database initial state: the size of the database determines the number of required elements, since associations are defined for each possible element of the database. The results of their empirical investigation in- dicate that the number of required elements varies with the precision of the data flow analysis. The authors based their empirical investigation on a couple of Java programs ac- cessing a MySQL database. There is a tendency for a high number of required elements when tuple and attribute value granularities are used, even with a reduced database. They do not investigate the fault-revealing aspect of the proposed criteria.

Additional work concentrate on the generation of testing databases [2, 4, 5] and on the definition of approaches to test database applications [8, 15].

7 Conclusions  We introduce an approach to testing active rules written in SQL, representing a valuable resource to quality assur- ance activities related to the development of active database applications. A family of adequacy criteria is proposed, called Interaction Between Rules based Criteria, which re- quires the coverage of interaction associations. The criteria promote the use of a systematic approach to testing and pro- vide support to the dissemination of active rules databases.

The experiment performed a rigorous evaluation of cost and fault-revealing ability in various data flow analysis pre- cisions. The results of the empirical investigation, sup- ported by the tool ADAPT-TOOL, show that the criteria are capable of revealing faults in manipulation commands, in addition to their applicability at several granularities. The effectiveness of interaction association coverage was 2/3 of all adequate test cases, and reached higher values for the lower data flow analysis precision. Furthermore, fault- revealing and non fault-revealing scenarios were identified at every granularity level. The coverage of interaction asso- ciations at higher granularities does not improve the fault detecting ability; consequently, the criteria promote low- cost high-quality testing.

This work investigates the fault-revealing aspects of per- sistent data flow based testing. The results suggest that inter-rule persistent data flow analysis is an effective fault- revealing technique for active database applications, and promote the use of rule interactions in the design of rule sets. They also reinforce the advantage of covering asso- ciations at the relation level, specially considering the cost reduction effect without a significant drop on effectiveness.

New empirical studies are required, since several factors are involved in the results of the study, such as: data flow generation techniques; set of active rules utilized; test data generated; faults and types of faults seeded in the active rules. Additional sets of active rules could be used to in- clude new samples of real rules to mitigate threats to the study?s validity.

