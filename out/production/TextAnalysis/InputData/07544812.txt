L Greeshma,  Assistant Professor, Department of CSE,

Abstract ?Itemset mining identifies group of frequent itemsets that signify possibly of relevant information. Unique constraints are usually forced to emphasis the analysis on most interestingness itemsets. In this paper we proposed unique constraint based mining on relational dataset. The constrained- based mining helps us to merge all itemsets, which are interrelated to each other. Specifically it chooses itemsets with same consequent part of an association rule and evaluates the highest itemsets with minimum coverage in that relational database. This paper mainly concentrates to propose a new Apriori-based algorithm, which satisfy the certain properties of constrained itemset based mining like anti-monotonicity.

Index Terms? Itemset mining, data mining, association rule mining, candidate key generation

I. INTRODUCTION Knowledge discovery from data and data mining are multidisciplinary domain that mainly focuses of achieving relevant association rules from unlabeled data to consistent data. To perform this task we require various data mining algorithms [1]. Data can be incorporated from various data sources such as files, data warehouse or databases, World Wide Web and other information repositories such as OLAP (Online Analytical Processing) which is performed by certain OLAP operations like roll-up, pivot, drill-down, slice and dice data is resulted by OLAM (Online Line Analytical Mining) technique which integrates OLAP with data mining and mining knowledge in interdimensional databases. Mainly it is required for identifying the periodic strategies, which are abstracted inside in database or data warehouse plays, a fundamental role in many data mining tasks, such as frequent itemset mining, high utility Itemset mining and weighted itemset mining [7], [9], [11]. One of most widely used data mining technique is Association Rule Mining (ARM)[2].

Association rule is defined as X ?Y, which infers that if a specific transaction in a database contains itemset X then there is chance it contains itemset Y. For instance, if X purchases a Laptop then there is a possibility that Y purchases software.

The most important association rules are estimated by using interestingness measures like support, confidence. In general the association rule contains IF THEN Conclusion. In other words the interesting measure support signifies the likelihood of IF and THEN part of an association rule may occur together in a data set and confidence indicates the conditional possibility of happening THEN part under conditional possibility of happening IF part of rule.

Dr. G Pradeepini,  Associate Professor, Department of CSE, K L University, Vaddeswaram, AndhraPradesh, India.

pradeepini.gera@gmail.com     The association rules mining difficulty has become a significant research zone by concept of the, mining frequent itemset using candidate key generation i.e. Apriori algorithm [2]. First phase, finding frequent itemsets is vital for making association rules [4]. In Second phase, proving these rules is important because most of acquired rules may have same IF part, which causes a duplicates. It also distorts the end user for taking exact decision. The problem to be overcome is to reduce time complexity of resulted data set. In general, Information Retrieval System retrieves the huge number of rules because of it various data mining algorithm focuses on mining interdimensional association rules using static discretization of quantitative features on positive dependencies rather than negative dependencies but in certain situations we can interrelate positive and negative dependencies [11]. Thus, time complexity for the algorithms increases which degrades the performance. Later on to represent frequent itemsets the concept of mining closed high utility itemset, compressed frequent pattern, crucial itemsets and frequent itemsets based on positive dependencies have been proposed in the literature [7], [9], [10], [11]. The problems associated with these association rules is it contain redundancy and also size of itemsets are summarized but not association rules. Those rules are often termed as multilevel association rule. Frequent itemsets signify persistent associations among data items [10], which are generally designated by considering their interestingness in the analyzed data [9], [10]. The manual analysis of the data mining result is a challenging task. To overcome this problem, item set mining with overall constraints intents to identify set of itemsets [7]. Instead of estimating and retrieving itemsets separately, pattern sets (i.e., sets of itemsets) are produced and calculated as a whole to analyze the associations among data from a topmost-level perspective. Moreover, instead of generating all the itemsets, for each representation only the major itemset should be considered, because all the others are reduced representations of the same data. However, to estimate itemset interestingness like support and confidence. All previous algorithms just calculate one itemset at a single stage. Therefore, they cannot mine for each representation only the best representative itemset unlike generating all the itemsets initially and then post prune the uninterestingness patterns. In this paper we addressed the issue of itemset mining with overall constraints from relational database. In order to find out itemsets containing all the relevant information related to a given aspect, we proposed a new global constraint, namely the  Unique Constraint Frequent Item Set Mining  2016 6th International Advanced Computing Conference  DOI 10.1109/IACC.2016.23   2016 6th International Advanced Computing Conference  DOI 10.1109/IACC.2016.23    DOI 10.1109/IACC.2016.23    DOI 10.1109/IACC.2016.23    DOI 10.1109/IACC.2016.23       constraint-based association mining. The constraint-based association mining chooses all the itemsets that are (i) collects only of frequent itemsets with the same representation and (ii) illustrated by greatest size among those corresponding to that representation. To provide a concise and lossless useful representation of different data sets we choice at most one item set per representation.  To improve efficiency of mining itemsets with constraint-based association, we present a new Apriori-based algorithm [9], namely Constrained Itemset Mining algorithm (CIM), which adopts a breath first search strategy to identify frequent itemsets at the time. Therefore, the itemsets of interestingness patterns are retrieved at once without the need for post processing. Therefore, even the resultant data can directly help domain experts for advanced analyses. Section 2 reviews on related work. Section 3 outlines necessary. Preliminaries for the representation of itemsets and CIM algorithms are proposed. Section 4 describes about experimental results. Conclusion is given in Section 5.



II. RELATED WORK  There are numerous existing algorithms to find frequent itemsets by surveying database for each transaction in the database and most important among them is Apriori Algorithm. Pei et al. [4] proposed CLOSET using the denser unit of data known as Frequent Pattern-Tree which is achieved by mining frequent itemsets without the help of a candidate key generation and applying the divide and conquer approach from Frequent Pattern-Growth algorithm [5]. Grahne and Zhu [6] proposed subsequent version of CLOSET+ termed as FP- CLOSE condenses iterative traversal over FP-Tree. These rules can be weighed by its coverage and accuracy. Major drawback is it is not necessary to produced association rule sets may or may not be less than user specified threshold. All these related works specified here are meant for retrieving high relevant rules without any redundancy and lossless information to the end users based on certain constraints. The issue of choosing discrete itemsets according to their features has already been addressed in [9]. In [9] the authors first formulate the problem of discrete itemset mining with certain constraints.

In this context, constraints are aggregations of boolean predicates which impose the presence or the absence of a given itemset permutation. Similarly, in [11] an attempt to constrain itemset mining according to the itemset representation in the presence of classes has also been completed. Unlike [11] this work emphases on retrieving collection of itemsets satisfying global constraints rather than discrete itemsets according to their interestingness. However, this paper focuses on retrieving itemsets based on constrained association mining.



III.  PRELIMINARIES AND PROPOSED ALGORITHMS  In this section we proposed fundamental concepts and definitions, which are required for defining proposed algorithms.

A. Preliminaries  Let I={I1,I2,I3,????..In} be set of n distinctive items in a relational database R where I1,I2,I3,??.In represents feature names. Each relation in a database consists of distinctive identifiers. In the perspective of relational data, a dataset R is a set of records and it is illustrated by a schema ?? ??1,??2,??3????..??n} which consists of set of attributes ?j ? ?. Each tuple t, with an identifier tid, is a set of items.

Item ij is a pair (fj ; vj ), where fj is an attribute that describes a given feature, and vj represents the associated value and belonging to the respective feature domain. Continuous feature values are discretized by the various preprocessing step.

A k-itemset I in R is a set of k items [10] with distinct attributes, i.e., I ={(??1,v1),(??2,v2),??.(??n, vn) } such that ?j ? ??q for all 	?j , Vj), (??q, vq) ? I. In the following we represents it as R(I) as relation of an itemset I i.e., the set of features of an items appearing in I. Coverage of an itemset I in a given tuple t ? R if and only if I t. The transaction identifier (tid) of itemset I, denoted as tid , is the set of tids is associated to the tuples which are  covered by I in R.

Table 1 represents some relevant data about the employee under analysis. Each tuple corresponds to a different employee and it reports the values of a subset of features. List of attributes are age, buys a computer, salary and work. To achieve their goal, data mining analysts mine from the input data itemsets like {(Age, Youth), (Buys: a computer, No)}, where each itemset is categorized by a given relation (e.g., {Age, Buys: a computer}). To generate relevant itemset, the mined itemsets must contain at least 30% of the employee, i.e., their frequency of occurrence (support) should satisfy min_supp = 30%. Itemsets with the same relation are appeared together because they represent the same dataset. For the reason of ease, let us consider the itemsets with a pair of features. Since data analysts do not have a priori information about the most significant representations to consider, they have to (i) finding all the itemsets satisfying minimum support threshold value, (ii) combine the mined itemsets according to their relation, and (iii) rank the item sets by arranging them in decreasing order based on their confidence and discarding those itemsets which are not satisfying threshold value.

Table 2 reports the subset of mined itemsets. Among the itemsets with pair of attributes, the item set with highest confidence is; {Buys: a computer, Work} (83.3%). For example, according to employee buys a computer and work, data analysts can find out different advertising policies for youth software employee and middle-aged professors.

Together, the above-mentioned sections covers 83% of the employee thus represent conceivable objectives of advertising promotions.

Table 1: Employee Relational Dataset   Rid Age Buys: a  Computer Salary Work  1 Youth Yes 20K Professor 2 Youth No 25K Software  Employer 3 Youth No 25K Software  Employer 4 Middle  Aged Yes 30K Professor  5 Senior No 15K Clerk 6 Middle  Aged Yes 35K Professor   Table 2: Itemsets satisfying the relation-based and minimum confidence  constraints, which are mined from Employee Relational Dataset. (min_supp = 30%, min_confidence = 60%)   ItemSet Support Count Confidence I {Age} {(Age, Youth)} (50%)  {(Age, Middle Aged)} (33.3%) 83.3%  I {Buys} {(Buys, No)} (50%) {(Buys, Yes)} (50%)  100%  I {work} {(Work, Professor)} (50%) {(Work, Software Employer)} (50%)  83.3%  I {Age, Buys} {(Age, Youth) (Buys, No)} (50%) {(Age, Middle Aged) (Buys, Yes)} (33.3%)  66.6%  I {Age, Work} {(Age, Youth) (Work, Software Employer)} (33.3%) {(Age, Middle Aged) (Work, Professor)} (33.3%)  66.6%  I {Buys, Work} {(Buys, No) (Work, Software Employer) } (50%) {(Buys, Yes) (Work, Professor)} (33.3%)  83.3%     B. Itemset mining set with conventional constraints  Itemset mining from a relational dataset R involves identifying subsets of items from R [7].

Definition 1 (Itemset): Let X be the set of all itemsets in a relational dataset R. I X is a itemset in R.

Hereafter we will denote by I the set of all the possible itemsets in a relational dataset R, i.e., I=2n. Since this work focuses on identifying interesting itemsets, the distinct items occurring in a item set will be denoted as itemsets throughout the paper. Itemsets are categorized by different interesting measures [7]. Hereafter we will consider two conventional interesting measures, namely the set cardinality and coverage, which is estimated particularly for a suitable itemsets adaptable by domain experts for handbook inspection [7].

Definition 2 (Itemset cardinality and coverage). Let Ii  ??I be an arbitrary itemset. The definitions follow.

(i) The cardinality of Ii , denoted as c(Ii), is the number of itemsets in Ii, i.e, c(Ii) = |I|.

(ii) The coverage of Ii, denoted as cover(Ii) is percentage of tuple in R covered by an itemset in Ii i.e.,   Cover (Ii) = {| t ? R | ? X ? Ii such that t X                (1) | R |   =   UX ? Ii Rid(I)            (2) | R |  Let us consider again the example itemsets reported in Table2.

Itemset   I {Age, Buys: a computer} r has cardinality=2 and coverage= 66.6%. It contains itemsets {(Age, Youth), (Buys a computer, No)} and {(Age, Middle-Aged), (Buys: a computer, Yes)}, which cover the tuples with rid {2,3} and {4,6} respectively. Hence, the coverage of the itemset is 66.6%.

Minimum coverage and maximum cardinality constraints are unique constraints that are usually enforced to select the most relevant itemsets [17]. They choose the itemsets that represent a large sufficient percentage of the analyzed data (i.e., cover(Ii) ??min_coverage)and that have adaptable size (i.e., c(Ii) max_cardinality) respectively.

Definition 3 (relation-based constraint). Let min_supp and Xf be the set of frequent itemsets in a relational dataset R according to min_supp. An arbitrary itemset Ii ? I satisfies the relation-based constraint if and only if:  (i) I contains only frequent itemsets in X with the same relation, i.e., for all Xj ,Xq ? Ii such that relation (Xi)=relation (Xq)  (ii) I contains all the frequent itemsets in X with the same relation, i.e., for all Xj ,Xq ? Xf such that relation (Xi)=relation (Xq)  then Xj ,Xq ? Ii  The anti-monotonicity property should following properties hold: (i) The minimum coverage constraint cover(Ii) ? ?min_coverage is anti-monotone w.r.t., i.e., if cover(Ii) ? ?min_coverage then cover(Ij) ??min_coverage.

(ii) The maximum cardinality constraint is anti-monotone w.r.t, i.e., if c(Ii)  max_cardinality then c(Ii) ? max_cardinality, if min_supp = 0 is required.

C. Constrained Item Set Mining Algorithm  (i) Candidate key and itemset generation: The procedure generates the k-itemsets and their resultant itemsets at the same time (line 12). Any itemset that satisfies the relation- based constraint must contain only itemsets with the same relation. Hence, to generate a candidate keys that contains k- itemsets and satisfies the relation-based constraint CIM a pairs of itemsets containing (k-1)-itemsets. Such items sets are       generated at the (k-1)-th iteration and composed into set CIk-1 (line 23). More specifically, for each itemset Ii ? CIk-1 the relation attributes are ranked in specific order. Two itemsets Ii; Ij ? CIk-1  are joined if they share the first (k ? 2) relation attributes. The resulting itemset It has relation where relation(Ii) U relation(Ij).

(ii) Itemset calculation and collecting the resultant data set: The support of k-itemset is calculated by scanning a dataset and the infrequent itemsets are removed(lines 13-17).

Therefore, the itemsets satisfying the unique constraint C, i.e., the coverage or the cardinality constraint, are selected (lines 18?23). Since discrete itemsets with the same relation cannot cover the same tuple, the coverage of a itemset can be simply calculated by summing the support count of its itemsets.

Algorithm: The CIM algorithm Input: Transactional database D, minimum support user specified threshold value min_supp, uniform constraint c Output: Itemsets satisfying the relation-based and unique constraints   1. K := 1 2. Identify all frequent 1-itemsets satisfying min_supp ,  denoted as FI1 3. Findout the candidate key itemsets for composed 1-  itemset satisfying relation-based constraints denoted as CI1  4. for each Candidate Itemset  I?? CI1 5. { 6. Calculate Itemset-1 with respect to interestingness  measures 7. } 8. CI1 = applying unique constraint (CI1, C) by deleting  Itemsets which are not satisfying unique constraints 9. While (CIk ? 0) 10. { 11. K=k+1 12. Findout the candidate key itemsets from CIk-1 to  evaluate CIk 13. for each Candidate Itemset I ? CIk 14. { 15. Calculate the support count of an itemset(I) 16. } 17. CIK = apply_support_constraint( CIk , min_supp) by  discarding the infrequent itemsets from Candidate keys  18. for each candidate Itemset I ? CIk 19. { 20. Calculate Itemset-1 with respect to interestingness  measures 21. } 22. CIk = apply_unique Constraints ( CIk , C) 23. CIk = applying unique constraint (CIk, C) by deleting  Itemsets which are not satisfying unique constraints 24. } 25. return Uk CIk       From above example, employee relation dataset in Table 1 and the items in Table 2 mined by considering min_supp = 33% and min- coverage = 50%. CIM first identifies itemsets I {age}, I{buys: a computer}, I {salary} and I{work}. I {salary} is removed, along with the corresponding itemset I {salary, 25K}, because it does not satisfy the minimum coverage constraint. The remaining itemsets are added to the resultant set and they are used to findout candidate sets I {Age, Buys}, I {Age, Work} and I {Buys, Work} For each pair of itemsets, their associated itemsets are merged. For example, itemset I {Age, Buys} contains 4 candidate keys by merging the itemsets in I {age} and I{buys: a computer},. Among them, only {(Age, Youth), (Buys a computer, No)} and {(Age, Middle-aged), (Buys a computer, Yes)} are frequent and, thus, they are included in I {Age, Buys}. Itemsets I {Age, Buys}, are selected because they satisfy the minimum coverage constraint. Finally, at the third iteration, I {Age, Buys } der and I {Age, Work}are merged because they have common the first relation attribute (i.e., Age) and the associated itemset I {Age, Buys: a computer, Work } generated. At the same time, frequent 3-itemsets {(Age, Youth), (Buys a computer, No), (Work, Software Employer)} and {(Age, Middle-aged), (Buys a computer, Yes), (Work, Professor)} are generated and included in I {Age, Buys: a computer, Work}.



IV.  EXPERIMENTAL RESULTS  We performed extensive analysis on real and synthetic dataset of CIM (Constrained Itemset Mining) when compared with the previous approaches.

TABLE 3: Characteristics of various Dataset   Data Set Number of tuples  Number of attributes  Vehicle 894 19 Electronic Voting 435 17  Letter_rec. 20000 17    TABLE 4: Based on CIM algorithm for various datasets: Number of Itemsets by imposing different coverage constraint value and min_support = 1%   Data set Minimum coverage  # Itemsets  Average Coverage  per Itemset  Pruned Itemset  Vehicle 50  3.21E+06 8.73E+05 6.80E+04  64.2 79.2 93.7  67.4 91.1 99.3  Electronic Voting   1.83E+06 6.34E+05 4.97E+04  66.1 79.7 93.3  33.6 76.9 98.2  Letter_rec. 50  4.08E+03 2.72E+03 1.96E+02  74.9 80.1 98.3  69.8 79.8 98.5           o ----  min_support = 0.035 % x ---- min_support = 0.025 %   Fig. 1. Vehicle dataset      o ----- min_support = 0.015 % x ----- min_support = 0.01 %   Fig. 2. Electronic dataset         o ----- min_support = 0.025 % x ----- min_support = 0.015 %   Figure 3: Letter_rec. dataset

V.  CONCLUSION  The problems of association rule mining is to retrieve relevant itemsets. This paper addresses the itemset mining problem with unique constraints. It presents a new constraint, called relation-based constraints, fitted to relational data. In Constrained Itemset Mining (CIM) algorithm helps us to identify candidate key itemsets and generates frequent itemsets by satisfying anti-monotonicity property i.e., minimum coverage and maximum cardinality which are restricted to a particular dataset. The experiments illustrate the choosiness of the proposed constraint as well as the algorithm increases efficiency and scalability.

