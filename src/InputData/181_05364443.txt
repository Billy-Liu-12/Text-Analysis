Concept Classification Using a Hybrid Data Mining Model

Abstract  Apriori is a well-known algorithm which is used extensively in market-basket analysis and data mining.

The algorithm is used for learning association rules from transactional data bases and is based on simple counting procedures. In this paper we propose enhancements to Apriori which allow it to perform concept classification similar to the way decision tree algorithms learn.

Specifically, training examples are modified and treated as transactional data and the results are verified and further improved by C4.5 decision tree and k-means clustering algorithms, respectively. To demonstrate the novelty of the enhanced Apriori algorithm, we present a hybrid data mining model (HDMM) which identifies at- risk students based on their academic performance and other pertinent data.

1. Introduction  Much work has been done to date to enhance association rule mining using artificial intelligence techniques such as particle swarm optimization and genetic algorithms. Rough particle swarm optimization algorithm (RPSOA) uses the notion of rough patterns with upper and lower intervals to represent a set of values for automated mining of numeric association rules [1]. A hybrid model consisting of the Apriori algorithm and a constraint-based genetic algorithm (ACBGA) was designed for efficient classification of electronic ballast data [2]. In another approach, association rules generated by an Apriori miner are further refined by the aid of a genetic programming module [3,4]. Using support vector machines (SVM), a constructive method for extraction of association rules is proposed in which the classification knowledge is encoded in to an SVM classification tree (SVMT) and linguistic association rules are then decoded by using the trained SVMT [5]. Finally, dynamic mining of association rules using genetic algorithms (DMARG) is a method for discovering large itemsets in a dynamic transaction database using genetic algorithms [7].

In this paper a novel hybrid data mining model (HDMM) for concept learning is presented which combines the Apriori data mining, the C4.5 decision tree, and the k-means clustering algorithms. In the proposed  methodology, unlike many other techniques which typically rely on evolutionary algorithms to evolve a set of association rules, an enhanced Apriori algorithm learns a set of association rules. Each extracted rule can essentially be viewed as a conjunctive rule containing a combination of nominal as well as numeric attributes. The numeric attributes are discretized using the k-means clustering algorithm so the enhanced Apriori can generate quantitative association rules. The generated rules are compared with decision rules discovered by the C4.5 decision tree algorithm for verification and the overall classification accuracy is further improved by using a rule selection algorithm that combines the mined association and decision rules using a heuristic method. The benefits of the proposed system will be highlighted by an application that was created as an early warning system for identification of academically at-risk students.

2. A hybrid data mining model  The devised hybrid data mining model (HDMM) contains several modules as shown in Fig. 1:  Figure 1. The Hybrid Data Mining Model  Preparation of training data involves discretization of the numeric attributes so that the Apriori module can perform the necessary large itemset calculations. In general, discretization can be performed by one of the three methods of partitioning, fuzzy sets, and clustering [1].

In the partitioning method, numeric attribute domains are initially divided into small intervals and later adjacent intervals are combined into larger ones with statistically significant support values [8]. Fuzzy association rules, which rely on the fuzzy set theory, are another possibility but are very sensitive to shapes of membership functions  Rule Selection  (RS)  Training Data Preparation  Data Mining Module  K-Means  ANOVA  Apriori  C4.5   DOI 10.1109/ICTAI.2009.41     and do not support automated discretization. Finally, clustering methods such as the Birch method can successfully identify meaningful intervals for numeric attributes with better semantics for the intervals as opposed to the partitioning methods [1]. The method of discretization in this work is the k-means algorithm, which is efficient and can also be used adaptively to suit a specific application?s needs.

To prepare the training for data miners (see Fig. 1), numeric attributes are first discretized as follows. A single attribute dataset is created for each numeric feature which is then clustered with k-means, and the number of clusters K is dynamically adjusted until the resulting intervals have the highest support in the transactional data set.

Attribute values in each dataset are replaced by their respective cluster membership labels, and the individual datasets are finally merged into a single training data set.

Further, analysis of variance (ANOVA) is used to reduce the attribute space dimensionality and select factors that have the highest leverage on concept variance.

Specifically, an input parameter x% is used such that only the attributes with the highest leverage (whose sum of contributions to overall concept variance is above the threshold x) are included in concept formation.

During the data mining phase the Apriori and C4.5 algorithms are applied, and the generated association rules and decision rules are stored for further analysis. In the case of Apriori, a modification was made so that only large itemsets involving the classification labels (Ci) are considered. The reason for this modification is to direct the miner toward discovery of association rules of the form X Y such that or  As Fig.1 depicts, rule selection (RS) is the last stage of learning in the hybrid data mining model. The main purpose of the RS module is to find the rule that best classifies an unseen test case, and it uses the Best-Rule Strategy [9]. Suppose AR and DR are the discovered association and decision rules respectively such that AR = {AR1, ?, ARn} and DR = {DR1, ?, DRm}. Assuming that R = DR the coverage of R can be visually represented as seen below.

Figure 2. Rule Coverage of Two Test Cases  Each rule is associated with a cluster of training examples that have their own centroid. When the model encounters an unknown test case T, not only the rule that covers T is added to the candidate rule set, but so are all  neighboring clusters (rules) whose centroids are within a prescribed distance from T. To demonstrate, Ti in Fig. 2 will be classified as Rk?s concept label. However, Tj will be associated with rules R1, R2, and R3 because of its close proximity to these rules? centroids. In this case, the rule with the highest quality is selected for classification, where quality (Q) of a rule Ri is heuristically defined as the linear combination of the two rule properties of consistency (CONS) and completeness (COMP):  Q(Ri) = .CONS(Ri) + .COMP(Ri)        (8) Where and  CONS(Ri)  = (9)  COMP(Ri) = (10) The weights and are selected empirically to provide a desirable level of flexibility [9].

Fig. 3 shows the devised algorithm for testing unseen cases, which is used by the mining model in its last stage.

A hash tree is used for efficient retrieval of rules candidate sets. As association and decision rules are generated by the two miners, the examples covered by each rule are entered into the hash structure with an appropriate rule label. When an unseen test case Tk is processed by the model, all training examples from clusters whose centroids are close to Tk are identified using the k-nearest neighbor algorithm.

Figure 3. The Testing Algorithm  The examples are then run through the model and their corrsponding association and decision rules are recorded in a set of candidate rules called CR = {CR1, ?, CRm}.

Quality of each rule in CR is calculated using Eq. 8 and the best-rule strategy is applied to select the rule, Rk:  ( ) (11) Thus, the unseen test case Tk is classified according to the best rule?s class label.

3. Classification of at-risk students  The training cases used in the experiment contained information about academic performance of 1619  R1  R2 R3  Ti  Rk R Tj  /* ************************************* Initially, create an empty hash tree HT Execute loops during the mining phase: for each association rule ARi do  for each case Ek covered by ARi do HT.insert(ARi, Ek)  for each decision rule DRj do for each case Ek covered by DRj do HT.insert(DRj, Ek) ************************************* */ CR = candidate rule set (nearest neighbors(Tk)) class(Tk) = class of Best-Rule(CR)     undergraduates in fall 2008. For each student there were a total of 13 nominal and numeric attributes (independent variables) such as age, GPA, SAT scores, major, residency status, ethnicity, gender, etc. The single dependent variable Outcome indicates the academic status of each student and has 4 possible values: None, Probation, Warning, and Dismissal. The outcome was assigned to each case by the appropriate staff at Admissions and was almost entirely based on GPA. The main objective of this study was to determine which factors attributed to low GPA so an early warning system for identification of at-risk students could be developed.

Before running the proposed model, it was necessary to determine how well the two miners could classify the admissions data in isolation. Table 1 shows concept classification accuracy rates for the best results obtained using 10-fold cross-validation on the entire training set.

Table 1. Classification Results (GPA Excluded)  Method Number of Rules  Number of Attributes  Classification Accuracy  C4.5 120 12 55% Apriori 213 12 52%  After removal of GPA from the classification task, the obtained results shown in Table 3 clearly indicate that the predictive ability of each miner considerably diminished due to the absence of GPA and also the high level of skewedness in concept distributions. However, this trade- off in performance is well worth the extraction of rules that represent more useful knowledge involving attributes other than GPA.

In the next experiment, the proposed hybrid model was put to operation. As explained in Section 3, each iteration of the model performs analysis of variance (ANOVA) to isolate attributes that contribute at least x% to the overall concept variance (x is an input parameter).

This will allow the learner to efficiently focus in on the most promising regions in the concept space where factors with the highest leverage on overall variance are selected for rule formation. Using a 90% minimum acceptable threshold for variance (x), the 5 selected factors in the initial round in order of statistical significance included Ethnicity, Residency, Gender, and the verbal (SAT_V) and mathematical (SAT_M) SAT scores. The k-means algorithm was used to verify and ensure that the within-cluster squared error terms remained at acceptable pre-defined levels. Reducing high dimensionality in the attribute space is another objective of the proposed model, which in turn will facilitate the learner?s task by removing statistically extraneous information.

After application of Apriori and C4.5 algorithms on the reduced data set, the model achieved higher classification rates as shown in Table 2.

Table 2. Classification Results (Reduced Attribute Space)  Method Number of Rules  Number of Attributes  Classification Accuracy  C4.5 96 5 66% Apriori 100 5 62%  A sample of the generated association and decision rules is shown in Fig. 4. Note that the generated rules involving the concept None are not of interest and are therefore omitted due to space considerations. Also, there were no significant findings regarding the concept Dismissal due to the fact that very few training examples were actually labeled as such.

Figure 3. Extracted Rules  Figure 4. Generated Rules (Ethnicity=Black)  Comparing the two rules in Fig. 4, it is evident that the two miners were able to closely focus on similar clusters within the space of training instances. In general, the model was able to learn comparable association and decision rules involving other values of ethnicity which are omitted due to space considerations.

In the last stage of operation, the 100 generated association rules and 96 decision rules were processed by the rule selection algorithm and 10-fold cross validation testing was performed. The rule quality parameters and  were empirically set to 0.5.

Table 3 shows the overall capabilities of C4.5,  classification and regression tree (CART) [6] and the enhanced Apriori used in isolation. To further test the true mettle of the developed hybrid model it was decided to also run the HotSpot segmentation algorithm on the same data set. HotSpot learns a set of rules that maximize/minimize a target variable/value of interest.

With a nominal target, segments of the data are searched for where there is a high probability of a minority value occurring given the constraint of a minimum support [10].

As seen in Table 3, none of the techniques were able to learn the Dismissal concept due to the fact that very few training examples were labeled as such. The results demonstrate that HDMM achieved the highest overall classification accuracy for all of the outcomes of interest.

C4.5, CART and the quantitative Apriori algorithm applied in isolation produced comparable results although Apriori was not able to learn Probation and Dismissal outcomes mainly due to the skewed distribution of these values in the training set.

Ethnicity=Black & Gender=Male SAT_V=[432.0, 524.0)  Warning  if Ethnicity=Black if SAT_V < 520 Warning else                       Probation     Table 3. Summary of Classification Results  Method Learned Outcomes Accuracy  C4.5 CART  None, Warning, Probation None, Warning, Probation  66% 57%  Apriori HotSpot HDMM  None, Warning None  None, Warning, Probation  62% 90% 77%  HotSpot achieved 90% accuracy but only for the None outcome, which was the most statistically present concept. Interestingly enough, none of the employed methods were able to learn the highly skewed Dismissal concept. A close examination of the obtained true and false positive/negative classification rates for the three concepts that were learned by both HDMM and C4.5 reveals that, because the hybrid learner selected the most qualified rules, it better classified unseen cases during testing.

In terms of the actual knowledge learned in this experiment about early identification of academically at- risk students, the most interesting rules suggest that, in addition to GPA, two other indicators should be put in place. First, in the case of white males it is important for the University?s academic councilors to consider these students? SAT mathematical score; and second, in the case of black males, special attention should be given to careful examination of their SAT verbal score.

4. Conclusions  In this paper a novel hybrid learning model was presented which relies on three widely-used data mining algorithms: an enhanced Apriori association rule miner algorithm, the C4.5 decision tree algorithm, and the k- means clustering algorithm.

For preparing the training data, since the original Apriori algorithm cannot handle real-valued attributes, the k-means algorithm discretizes numeric attributes. Also, to ensure higher classification accuracy the model utilizes analysis of variance (ANOVA) to identify and select attributes that have the most effect on output variance.

During the learning phase, the HDMM trains its two mining modules in isolation. The novelty of the developed model is that unlike previous hybrid approaches to learning, which rely on evolutionary algorithms to further enhance a given set of association rules, this model uses the same set of association and decision rules. Each generated rule is characterized by a heuristic measure of goodness or quality depending on its levels of support and completeness. During the testing phase (rule selection), a rule candidate set is formed and the best rule in that set is selected using the Best-Rule strategy.

Experiments on a real-world application involving identification of academically at-risk students demonstrated that the hybrid model?s classification accuracy was higher, not only than those of several mining modules trained in isolation, but also than that of the popular HotSpot segmentation profiling algorithm.

Furthermore, even in presence of skewed concept distributions HDMM was robust enough to learn as many concepts as C4.5 and CART but with higher accuracy.

5. References  [1] Alatas, B. and Akin, E., ?Rough particle swarm optimization and its applications in data mining,? Soft Computing, Vol. 12, 2008, pp. 1205-1218.

[2] Chiu, C., Hsu P.L. and Chiu N.H., "Combining Apriori Algorithm and Constraint-Based Genetic Algorithm for Tree Induction for Aircraft Electronic Ballasts Troubleshooting," ICNC 2006, 2006, pp. 381-384.

[3] Niimi, A. and Tazaki, E., ?Combined Method of Genetic Programming and Association Rule Algorithm,? Applied Artificial Intelligence, Vol. 15, 2001, pp. 825-842.

[4] Niimi, A. and Tazaki, E., ?Rule Discovery Technique Using Genetic Programming Combined with Apriori Algorithm,? LNAI, 2000, pp. 273-278.

[5] Pang, S. and Kasabov, N., ?Encoding and Decoding the Knowledge of Association Rules Over SVM Classification Trees,? Knowledge Information Systems, Vo. 19, 2009, pp. 79- 105.

[6] Safavian, R. and Landgrebe, D., ?A Survey of Decision Tree and Cybernetics, Vol. 21, No. 3, 1991, pp 660-674.

[7] Shenoy, P.D., Srinivasa, K.G., Venugopal, K.R. and Patnaik, L.M., ?Dynamic Association Rule Mining using Genetic Algorithms,? Intelligent Data Analysis, Vol. 9, 2005, pp. 439- 453.

[8] Srikant, R. and Agrawal, R., ?Mining Quantitative Association Rules in Large Relational Tables,? Proceedings of ACM SIGMOD, 1996, pp. 591-600.

[9] Torgo, L., "Rule Combination in Inductive Learning," Lecture Notes in Artificial Intelligence, Vol. 667, 1993, pp. 384- 389.

[10] Weka and Pentaho, ?HotSpot Segmentation-Profiling,? http://wiki.pentaho.com/display/DATAMINING/HotSpot+Segm entation-Profiling, 2009.

