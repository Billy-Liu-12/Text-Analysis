Constructing Term Thesaurus using Text Association  Rule Mining

Abstract- This paper presents a new algorithm called ?concept- grouping? that adapts an association rule mining technique to construct term thesaurus for data preprocessing purpose. Similar terms, which are written differently, can be grouped together into the same concept based on their associations before they are used for subsequent analysis. This data preprocessing is important since it has an impact on the quality of other data mining techniques such as data clustering. The algorithm is applied to bibliographic databases such as INSPEC and EI Compendex toward the objective of enhancing traditional bibliometrics and content analysis. From the experiments with a set of publication abstracts, applying the proposed algorithm to combine similar terms into a pertinent concept before clustering process yields better cluster quality.



I. INTRODUCTION  Bibliometrics is the study that uses statistical and mathematical methods to analyze the literature of a discipline as it is patterned in its bibliographies. The fields of bibliometrics and content analysis have offered ways to analyze large amounts of Science and Technology (S&T) information resources over the years, but acceptance has been limited.  Recently, bibliometrics activity transitions into ?text mining? ? exploring the content of abstracts or full-text document sets. Such analytical approaches can aid management of technology and competitive technological intelligence in many ways.  For instance, they can help researchers in mapping and profiling their research domain [1] and technology managers in providing foresight analysis for their target technologies [2, 3]. One of the most accessible sources of S&T information that can be used in text mining process is publication abstract database. These scientific publication abstract databases such as INSPEC, EI Compendex, Science Citation Index, and IEEE Xplore include peer- reviewed articles from scientific and technical journals, primary research articles along with reviews, conference papers, and so forth. The information provided from such databases is well structured into distinct fields such as title, author, affiliation, keywords, and abstract. We assume this type of information is rich and suitable for discovering knowledge from a body of literature in support of S&T management.

There are approximately five major technique categories in the text mining process: document retrieval, data extraction,  data preprocessing (cleansing), data analysis, and data visualization [4]. For the context of this paper, document retrieval is a method concerned with the organizing, storage, searching, and retrieval of bibliographic information. Data extraction is the activity of automatically pulling out pertinent information from large volumes of texts.  Extraction can take two forms; one is to identify the specific field of entity extracted such as name, date, or address, and the other one is to identify the parts of speech from text corpus using natural language processing (NLP) technology. Data preprocessing, or data cleansing, is the algorithm that detects and removes errors or inconsistencies from data (e.g., phrases or words) and consolidates similar data in order to improve the quality of subsequent analyses.  This cleaned data will then be fed to the analysis process. Data analysis step involves dimensionality reduction techniques since each document can be represented as a vector of terms in a high dimension space [5]. Several data reduction techniques include factor analysis and cluster analysis, among others. Finally, the analyzed data can then be represented/visualized into a meaningful form.

For this paper, we focus on data preprocessing step. We propose a new algorithm called ?concept-grouping? which is based on the ?tree-structured networks? algorithm [6] to construct a thesaurus from related terms.

The intentions for using a concept-grouping algorithm to construct thesaurus for phrases extracted from abstract or title fields (which will be called ?abstract phrase? or ?title phrase? throughout the paper) are to combine similar terms into a pertinent concept and to increase the frequency (number of relevant documents) of abstract or title phrases before they are used in clustering.  Abstract or title phrases with technical words sometime capture the technical themes, which the author intends to present, better than keywords indexed by a database provider.  The bias and error introduced by the database providers can affect the performance of the techniques using keywords only.  However, the problem with using abstract or title phrases in the analysis is that the frequencies of the phrases are quite low because of the variation in the terms that are used.  The low frequency terms might be discarded in other analyses such as clustering. Other data preprocessing methods apply word stemming to group like terms into one. Stemming algorithm attempts to reduce a word to its stem or root form  Proceedings of ECTI-CON 2008     since, in most cases, morphological variants of words have similar semantic interpretations and can be considered as equivalent. However, this method is insufficient since it cannot detect the similarity between ?Internet? and ?world wide web.? Our proposed approach is believed to be more efficient since similar terms are grouped together into a concept based on the probability that they occur together.



II. TREE-STRUCTURED NETWORKS ALGORITHM  Tree-structured network algorithm is implemented based on association rule mining. Reference [7] introduced the new algorithm called Object-Oriented Association Rule Mining (OOARM) to effectively discover association rules from bibliographic data.

The basic tree-structured networks algorithm works as follows:  1. Find all frequent term-clusters 2. For each cluster, generate association rules with any  other clusters 3. To obtain Parent-Child relations, find association rules  that satisfy: confidence(X ? Y) = P(Y|X) ? minParent, where minParent ? 1  and support(X) < ? * support(Y) , where 0 < ? < 1 Y is then said to be ?parent? of X  4. To obtain sibling relations, find association rules between term clusters with (minSibling ? confidences < maxSibling)  where  0 < minSibling < maxSibling = minParent where  ? X and Y are set of terms and ?=?YX ? An itemset is collection of one or more items ? Each frequent term-cluster is each k-frequent  itemset that is generated by the OOARM algorithm ? Support is frequency of occurrence of an itemset ? Confidence of the rule X ? Y is the P(Y|X) ? All the threshold values are determined by  empirical analysis between related term clusters.



III. CONCEPT-GROUPING ALGORITHM  To make the algorithm more efficient, only the informative phrases should be used.  Hence, stopwords (i.e., terms that occur too frequently and are unimportant to the database content) should be removed from the list of phrases.  These terms are determined by a published list of the most frequently used words from two to ten letters [8].

We first explore the algorithm by deriving synonymous relation between abstract phrases. In other words, we find phrases that satisfy the following condition:   P(Y|X) = P(X|Y) = 1   (1)  where T = {t1, t2, ?, tn} is a set of n distinct phrases, X ,Y T? and X Y? =?  Equation (1) implies that X and Y are synonyms if they always occur together.  However we relax the constraint in (1) to cover some cases when phrases almost always occur together. The relaxed constraint is defined as,   P(Y|X) and P(X|Y) ?  minSynonym  (2)   The quantity minSynonym is determined by empirical  analysis among a set of related phrases.

The proposed algorithm is tested on a set of publication  abstracts. The results obtained are promising, however, we encounter two minor problems associated with this proposed algorithm. The first problem is that, for some data sets, the minimum support may have to be set at a very low level for the synonymous terms to be captured.  In such cases where the minimum support is very low (e.g., terms occurring with one another in 2 records), one cannot be confident to conclude that these terms are synonyms. The second problem is that it appears to be problematic to assign a name or concept to each group of synonymous terms. As a result, an improved method to group similar terms into a meaningful concept is proposed.

The improved algorithm will probe for strong parent-child relationships between keywords and abstract phrases.  Note that this analysis therefore uses two distinct data fields together.

We assume that synonymous terms can be grouped into a meaningful concept by discovering strong parent-child relationships between keyword and abstract phrases where keyword is a ?parent? and abstract phrases are its ?children.? Since a parent is considered to be the more general topic compared to children terms, it is reasonable to represent that particular keyword as the parent concept to those synonymous terms (children).  The improved concept-grouping algorithm is summarized as follows:  1. Find all parent-child relationships between keywords (parents) and abstract phrases (children).

2. Keep only 1-itemset parent and 1-itemset child relationships, discard the others.

3. If phrases (children) occur with multiple parents, choose the one with higher confidence.  If both confidences happen to be equal, choose the one with less support parent.

4. If again both supports are equal, discard those phrases.

The reason to choose the parent-child with higher confidence  in step 3 is that the child has more probability to occur with that parent.  If this probability is equal between two parent- child relations, then the less support parent is chosen because the child tends to be closer to that parent.



IV. EXPERIMENTAL RESULTS  The data used in our experiment are keywords and abstract phrases extracted from the 971 association rule mining (ARM)     research abstracts and the 817 information visualization (InfoViz) research abstracts. Table I and Table II illustrate experimental results from these two different data sets.  The results are obtained from applying parent-child algorithm presented in Section II.  For both data sets, minsup and minParent are set to 5 and 0.8 respectively.

TABLE I SYNONYMOUS TERMS FOR ARM RESEARCH DATA SET  Group name Synonymous Terms knowledge acquisition characterizations, computational costs query processing mining query parallel algorithms parallel algorithms, skewed data, processors graph theory association graphs electronic commerce e-commerce asynchronous transfer mode  data-intensive applications, parallel data mining   TABLE II  SYNONYMOUS TERMS FOR INFOVIZ RESEARCH DATA SET Group name Synonymous Terms user interfaces spreadsheets, large information  spaces, virtual worlds Internet Internet, world wide web virtual reality virtual reality, VR, virtual  environments graphical user interfaces graphical user interfaces, references information retrieval search engines multimedia computing mobile environments geographic information systems GIS splines(mathematics) smoothness Spreadsheet programs Visualization spreadsheets computer science education educational courses  The above tables illustrate that most of the synonymous  terms can be grouped together into meaningful concepts even when the minimum support is set to be higher (i.e., minsup = 5).

For instance, the virtual reality group, from Table II, consists of similar terms: ?VR? and ?virtual environments.? These terms would not be captured from the previously proposed algorithm where minsup is equal to 4. Hence, this improved concept-grouping algorithm promises to outperform the previously proposed method by reducing the noisy data and assigning appropriate concept name to the derived synonymous terms.

Note that this algorithm is collection-dependent, i.e., dependent on the domain of interest.  Hence, it does not imply that all the synonymous concepts captured from one data set can be generalized and used for a different data set.



V. PERFORMANCE EVALUATION  To assess the performance of this new concept-grouping algorithm, a factor analysis of abstract phrases with and without the proposed algorithm will be compared.  Factor analysis seeks to maximize similarity within clusters and minimize similarity between clusters.  However, factor analysis allows overlapping terms, meaning terms can be included in multiple clusters.  This method is suitable for analyzing multidisciplinary research domains.

There are two types of cluster validation studies.  External criterion functions derive the clustering solution by focusing on optimizing a function that is based on how the various clusters are different from each other.  Internal criterion functions try to determine if the structure is intrinsically appropriate for the data.  This means it focuses on producing a cluster solution that optimizes a function defined only over the data within each cluster and does not take into account the data assigned to different clusters.

We do not utilize the external quality measure since the cluster solution from factor analysis is allowed to have the same terms in multiple clusters.  Hence, there will always be linkages between clusters, which are not suitable for external assessment.  As a result, only internal assessment will be performed.  A commonly used internal measurement, the cohesion [9, 10], will be employed to evaluate the accuracy of the produced clustering solutions.

A. Cohesion  Borrowing the concept of the vector-space model, the cosine similarity can be used to compute the cohesion between two abstract phrases.  In this evaluation, each term, t, is considered to be a vector, t, in the document space. t = (df1, df2, ?, dfn), where dfi is ?1? if document i contains term t and ?0? if it does not.  To account for vectors of different lengths, the length of each term vector is normalized so that it is of unit length.

There are a number of possible measures for computing the similarity between term vectors, but the most common one is the cosine measure, which is defined as      )(),cos( tt tttt ?=   (3)   where 	 indicates the vector dot product and t is the  length of vector t.  The cosine formula can be simplified to vectors? dot product when the term vectors are of unit length.

This measure becomes one if the terms are identical, and zero if there is no document in common between them (i.e., the vectors are orthogonal to each other).

The average pairwise similarity between all points in a cluster will be used to determine each cluster?s cohesion, which is defined as     i j r  r i j t ,t Sr  cohesion cos( t ,t ) m ?  = ?   (4)  where mr is the number of terms in cluster r and Sr is the set  of terms in cluster r.  Note that this equation includes the similarity of each point with itself, which is equal to 1.

The total cohesion can be calculated as the weighted average of cohesions for each cluster:      i j r  k  total r i j r t ,t Sr  Cohesion m cos( t ,t ) m= ?  ? ? = ? ?  ? ? ?  ? ? (5)   where k is the total number of clusters.  This evaluation attempts to maximize the total cohesion.

B. Evaluation Results  Four data sets are used to evaluate the clustering results.

The summary of these data sets is shown in Table III.  For all data sets, a stop-list is used to remove common words, and the words are also stemmed.

TABLE III SUMMARY OF DATA  SETS USED TO EVALUATE THE CLUSTERING RESULTS Data Set Source Number of  Records Number of Terms used in Factor Analysis  1. ARM INSPEC and ENGI 971 162 2. InfoViz INSPEC 817 147 3. FuelCell ENGI 1002 164 4. Thai-SW INSPEC and ENGI 820 85  The ARM and InfoViz data sets are the same as the data sets  used in Section IV. The other two data sets are records related to fuel cell research (FuelCell) and software research published by Thai organizations (Thai-SW). For each data set, we perform factor analysis on:  ? Cleaned abstract phrases (using Stemming algorithm).

? Concept-grouped abstract phrases (results from the  proposed algorithm in Section III) We expect that the clusters? quality for concept-grouped  abstract phrases will be better than the quality for cleaned abstract phrases.  The relevant terms can be captured and combined by the proposed algorithm.  This would increase the number of relevant documents in a cluster.  As a result, the cohesion between terms within that cluster will increase.

Fig. 1 shows the cluster cohesion results for each data set.

The results suggest that the quality of clusters generated from concept grouped abstract phrases is better than the cleaned abstract phrases.  Even though, the cohesion of the concept grouped abstract phrases for the ARM data set is smaller, it is not significantly worse.  On average, the clusters generated from concept grouped abstract phrases are more compact and less noisy because the relevant phrases are grouped together into a meaningful concept before the analysis.  This leads to the larger cohesion between terms in the clusters.

0.34 0.35 0.36 0.37 0.38 0.39  0.4 0.41 0.42 0.43 0.44 0.45  1 2 3 4  Data Set  C oh  es io  n  Concept-grouped abstract phrases  Cleaned abstract phrases  .



VI. CONCLUSION  In this paper, a concept-grouping algorithm used for constructing a thesaurus from synonymous terms is proposed.

This algorithm adapts an association rule mining (ARM) technique to capture the synonymous terms.  Similar terms, which are written differently, can be grouped together into the same concept based on their associations before they are used for subsequent analyses.  A performance evaluation for the proposed algorithm is conducted.  To assess the performance, a factor analysis of cleaned abstract phrases is compared to one based on the concept-grouped abstract phrases.  The cleaned abstract phrases result from applying stemming algorithm. The cluster quality is then validated by a measure called cohesion, which focuses on maximizing the similarity between terms within the clusters.  The evaluation results show higher cohesions for the concept-grouped abstract phrase clusters compared to the cleaned abstract phrase clusters.  Hence, the proposed concept-grouping algorithm can help improve the accuracy of the representation of abstract phrases.

