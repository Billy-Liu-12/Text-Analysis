Disjunctive Combined Causal Rules Mining  Manal Alharbi

Abstract? Causal discovery is a well-studied problem due to an urgent need for systems that predict, explain, and make proper and necessary decisions in many domains including epidemiology, biology, medicine, economics, physics, and social sciences. Existing techniques such as learning Bayesian networks (BNs) and Randomized controlled trials (RCTs) are expensive and time consuming. In addition, they only find single cause rules from certain data. There are numerous important applications wherein we have to generate disjunctive causal rules from uncertain data. In this paper we propose an algorithm called DCCRUD that employs frequent itemsets mining algorithms to discover disjunctive combined causal rules from uncertain data. To the best of our knowledge ours is the first paper to address this important problem. Discovering causal rules where targets are disjunctions of variables might be equally important. DCCRUD applies to uncertain databases.

We evaluate the performance of the proposed algorithms on real datasets.

Index Terms? Data mining, Algorithms, Frequent itemsets, Disjunctive rules mining, Causality, partial association, causal rule, Uncertain databases.



I. INTRODUCTION While traditional association rules mining algorithms  identify the relationships among variables in general, the aim of causal discovery is to identify profound relationships such as ?a change of antecedent is the cause for a change in the consequent?. Association rules mining uses the well-known support?confidence framework which does not necessarily signify causation [1]. For example, the support?confidence framework can show that milk and eggs in the same basket are related but cannot indicate that buying eggs was caused by buying the milk. Causal relationships discovery is widely used in the prediction processes where the prediction of causes have been used to prevent harmful consequences [2]. The importance of discovering causal relationships can be felt in many areas, such as economics, physical, behavioral, medical, biological, and social sciences. Thus, significant advances in exhibiting and finding causal rules have been made in many areas. One of the most powerful tools for causal discovery is the Randomized controlled trial (RCT) [5]. RCT is an experimental approach, and the main challenge in this approach is the difficulty of conducting experiments because of ethical concerns or cost issues [4]. Causal discovery with observational data is an alternative solution when the experimental approaches are infeasible. Due to a rapid expansion of observational data, researchers have focused on attempts to reduce costs and help in decision making by predicting vital indicators that prevent harmful consequences [4]. In spite of advances made in finding causal rules, existing methods are unable to handle big datasets. Several variations of causal discovery with observational data have been investigated in the literature. Most of the previous works were  based on statistics [6-24]. The main challenge in these approaches on observational data is that statistical correlations discovered from observational data may not really form a causal relationship [4].

A. Causal Discovery Most of the existing studies [6-17] mainly focus on  inferring causal relationships in observational data using a directed acyclic graph (Bayesian networks) or undirected probabilistic graphical models (Markov networks). It is known that Bayesian networks based formulations are NP-hard and therefore algorithms based on these are only applied on low dimensional data sets [17]. Constraint based approaches have been used as optimization methods as they do not search for a complete Bayesian network [18-22]. Unfortunately, they have two problems [3, 4]: they only discover single causes, and they fail to discover causal relationships on non-fixed structures.

Integrating partial association tests along with association rules mining is a solution that has been introduced by [3, 4, and 21]. These solutions link causality with continuity, where the association between two variables is not affected by other variables. For example, it is reasonable to conclude that change of gender is the cause for salary differences, if there is always salary differences between male/female workers whatever the circumstances are (e.g., different ages, domains, and different qualifications). In this case, the association between being female and receiving low salaries holds [4].

The CR-PA algorithm [3] has been proposed to discover causal relationships with both a single cause variable, and multiple cause variables.

B. Finding Frequent Patterns from Uncertain Data Association rules mining [30] is a well-studied problem  where the input is a database  consisting of transactions . Each   is a subset of the set of all items  . An implication of the from  is said to be an association rule, where , , and . An association rule has to satisfy two conditions to be of interest.

The support of the rule has to be at least minsup. The support is defined as the fraction of transactions that contain both X and Y.

The confidence of the rule has to be at least user-defined threshold namely minconf. The confidence is defined as the number of transactions in which both X and Y occur divided by the number of transactions in which X occurs.

Unlike certain data where items in transactions are definite in terms of their occurrence in the data. In an uncertain database, we don?t know for sure which items belong to the transaction. We only know a probability for each possible item that this item belongs to the transaction [25-29]. Let  be the set of all items of an uncertain dataset  2015 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)     UDB that contains a set of  transactions, .

Let be the probability that the transaction   contains the item x, for any item x . The expected support of an itemset X can be defined as follows:  . A rule will be of interest if the expected support of the corresponding itemset  is at least  C. Disjunctive Association Rules Mining In several real life applications, rules in which both the  antecedent and the consequent consist of disjunctions of items might be equally important. The problem of generating disjunctive rules has been introduced in the literature in the context of certain data [30, 31]. In our previous work [33], we introduced disjunctive rules from uncertain data for the first time and presented an algorithm called DRMUD. In our DRMUD algorithm, we defined a k-disjunctive rule as:  When item a occurs with enough support and confidence, one or more of the items  are also expected to occur. We defined the rule  to be interesting, if each of the rules , has enough minimum support. This is because when the rule  has enough support, then obviously the rule  also will have enough support whatever the support of the rule  is.

When the rule  does not have enough support, then the rule  may not be interesting even if the rule has sufficient support. To identify interesting rules, we introduced two support thresholds minsup1 and minsup2. Here minsup1 is the minimum support that each of the rules (for 1 ? j ? (k-1)) should have, and minsup2 is the minimum expected support that is required between the item a, and the set of items , for the rule to be interesting.

DRMUD algorithm consists of two phases. In phase1, we mine pairs of items that have at least a minimum expected support (minsup1). In phase2, we use these pairs to generate k- disjunctive rules that satisfy the minimum expected support (minsup2).

In this paper, we propose a novel approach that adopts some of the strategies from previous works on association rules mining [21] and partial association rules mining [3] to discover causal relationships in observational data. Our new algorithm, DCCRUD aims to discover disjunctive combined causal rules from uncertain. Our target consists of k- disjunctive variables, where k is the length of the rule and it is chosen by the user. Since previous works have shown that the vertical layout representation of a dataset helped to speed up the process of generating the candidate rules (see e.g. [35]), DCCRUD algorithm also uses vertical layout.



II. RELATED WORKS In this section, we introduce the statistical definition of  correlation between two variables, and present the concepts for inferring causality from partial associations.

A. Support-Confidence Association Vs. Correlations Traditional support-confidence associations use  downward closure property to reduce the search space.

Downward closure property deletes any superset of an infrequent itemset [30]. In reality support-confidence framework might be misleading since it ignores the negative correlations [34]. The well-known Chi-square statistic test is  widely used for testing the correlation.  Two items are said to be correlated or dependent when its Chi-square value  is higher than a significance threshold .

Unlike the traditional support-confidence associations that take advantage of downward closure property, correlation using Chi-square statistic test is upward closed in the itemset lattice. Upward closure property is a property of dependence (correlation) that states that if an itemset A is dependent, then its superset will be also dependent. Thus, the search space will be reduced since adding items to a correlated itemset A will not cancel the correlation [34]. The Chi-square statistic test is easy to calculate. However, it has two major limitations: first, the expected values in all the cells in the contingency table must have a value greater than one; second, the expected values in at least 80% of the cells in the contingency table must be greater than 5. Brin, et al. [34] solved this problem by using the support-confidence framework as an additional pruning condition for finding correlation rules. Zhou, et al. also used the support-confidence framework as an additional pruning condition for finding correlation rules [3, 4]. They also, used combined contingency tables for more than two variables instead of using multi-way contingency tables. Suppose we have three dichotomous variables gender (female or male), disease one (having Rett syndrome or not), and disease two (having Alport syndrome or not). In the contingency table, entries for cells like being female and having Alport syndrome, and being male and having Rett syndrome will be close to zero, as it is very unlikely to be female and having Alport syndrome and being male and having Rett syndrome.

In the process of Causal discovery, we are more concerned with the positive outcomes of dichotomous variables. For instance, physicians are more interested in smoking subjects more than non-smoking subjects [3]. Considering all the cells with values close to zero will introduce many redundancies, and subsequently unreliable results. Zhou, et al. [3] called any association identified with a positive outcomes Chi-square value  that is higher than a significance threshold as positive association. If the Chi-square value  is less than a significance threshold , then this will form a zero association.

B. Rules discovery using partial association tests As mentioned in the previous section, positive association  plays an important role in identifying a Causal relation.

However, it is not easy to signify causality from correlations without human intervention [34].  Partial association test [23, 24] is a powerful statistical tool that can be used to test conditional independence of random variables when a controlled experiment is impossible. It tests the association between two random variables I and J, when a third random variable C is present. When the association between I and J does not hold, given the different combinations of C, we refer to this as zero partial association. Zero partial association means, either C is a common cause of both I and J, or I causes C which causes J but there is no direct Causal relation between I and J [24]. Brich in [24] proved that Mantel-Haenszel test [23] is an optimal method for testing a partial association test against the other methods. It excludes the non-causal relationships, and only the potential causal relationships are     included. Thus, it is suitable for testing the partial association of Causal discovery [3]. To test the partial association  where x, and y are two dichotomous random variables, and  we use the following  equation: = . If the  partial association is greater than a significance threshold, then the association between  holds (non-zero partial association), and  is a Causal rule [3]. Note that the number of possible combinations of , where  is the total number of variables. The worst memory usage and run time for conducting the partial test could thus be large. For instance, with  =3 variables, there are eight 2 2 possible contingency tables for the partial test. However, instead of testing all the combinations, we only consider the items that come in the same transaction with  or . Those rows or columns in the contingency table with zero values are not considered [24]. Zhou, et al. [3] have proposed the CR-PA algorithm for discovering causal rules in observational data.

The basic idea of the CR-PA algorithm is to identify positive associations using association rules mining as we have mentioned earlier. These positive associations are considered as causal hypotheses rules. Then, they employed the partial association tests on these association rules to exclude non- persistent associations.



III. PROPOSED METHODS: In this section, we first present our proposed algorithm  namely the DCCRUD algorithm for discovering disjunctive causal rules (both single and combined) from uncertain data.

Then, we discuss its time complexity.

Here are the components of the DCCRUD algorithm: Positive association: we use Chi-square statistic test to signify the positive association between two random variables.

Cause effect relation: to test whether the positive association between two random variables is persistent or not, we use the well-known Mantel-Haenszel test.

Combined casual rules: we use the Fk-1 ? F1 method to merge attributes in ascending order of variable IDs in order to generate combined rules from the zero associations sorted in ZA.

Disjunctive causal rules: we use the same assumption as in our previous work on disjunctive rules mining (DRMUD algorithm [33]), where each causal pair (i.e.,  , must have a  (for 1 ? j ? (k-1)), and the rule  must have a where .

A. Disjunctive Combined Causal Rules from Uncertain Data (DCCRUD) Algorithm                                  Phase 5: Generate the k-disjunctive combined causal rules  Note is the -value that each of the rules (for 1 ? j ? (k-1)) should have to be a causal rule, and  is the - value that is required between the item a, and the set of items , for the rule to be interesting causal rule.

*:  stores the result of the partial test for each   B. Complexity Analysis Phase 1: the first step takes  time, where  is the  average number of in a record, and  is the number of records. Each is allocated a bucket indexed with the same id as the . In a single scan through the database we can figure out for all 1-Varaibles. The second step takes time  for only a single scan through the set of all 1- variables is done. The time complexity for this phase is   In phase 2 the algorithm generates frequent pairs using  method by merging a 1-frequent variables with a 1- frequent variables in ascending order of item IDs to generate 2-frequent variables, and this can be done in linear time. Also,     DCCRUD counts the expected support for a new 2-variable itemset by making intersection between record lists. Due to ordering of the records this intersection can be done in time that is linear in the total length of the two lists. Phase 2 takes  , to find out positive and zero associated pairs using the method,  Where = number of variables, and average size of each variable list.

In phase 3: DCCRUD creates the 2?2 contingency tables for each positive association pair with all combinations of  (all variables that are already associated with and calculate the partial association test, using Mantel- Haenszel partial association test. To generate combined rules from the zero associations, in phase 4, we adopt the same strategy as in [3]. The rule  can be combined if both and have a zero association. This will reduce the search space since we will exclude all the positive associations from further combining. Based on observations in [3, 4], if the rule  is an association rule but it fails in the partial test to be a causal rule, this means, the association between and is either interrupted by other variables, or the other variables are a common cause of both  and . Clearly, there is no direct causal relation between and . Thus, it is improbable that combining another variable with the LHS will lead to a direct association. Moreover, any superset of a causal rule is considered as a redundant rule, and doesn?t give any new information [3, 34]. This will reduce the search space since once a causal rule is discovered we will not generate any superset of this rule.  Also, the regular frequent association itemsets serve as pruning conditions that reduce the search space and time.

Phase 3, and Phase 4 are the most crucial in the algorithm because these phases are iterated. Phase 3, and phase 4 take time  In phase 5, following the same assumptions as in our proposed algorithm on disjunctive rule mining (DRMUD [33]), given a chi-square calculation for two causal rules (i.e.

, the rule , must have a and each rule  must  have a  (for 1 ? j ? (k-1)), where . In Phase 5, while considering the set S=  if there is any j (1?j?k-1) such that ( , tj)< . , then the set S is not considered. Phase 5: takes  time , where  is the size of the  rule. Note that ?q ? q as ? ? [0,1].



IV. EXPERIMENTAL RESULTS We show the results of DRMUD algorithm [33] under the  vertical format, and we show the comparison between our proposed algorithm and the algorithm in [3] for single target under certain database.

A. Horizontal Vs. Vertical DRMUD algorithm In our previous work [33], we introduced disjunctive rule  mining under uncertain database (DRMUD) algorithm.

Subsequently [35] we showed that the vertical approach outperforms the horizontal approach in mining weighted frequent patterns for the case of conjunctions. Since the DRMUD algorithm was in horizontal format, and our proposed  algorithm is based on the idea of DRMUD  algorithm but under vertical format, we compare DRMUD in the two formats.

For the sake of comparison, we have used the same setting that we have used for the DRMUD algorithm. Both algorithms have been implemented and compiled using Microsoft?s Visual Studio C++ 2013, and run on an Intel(R) Core(TM) i7 3.40GHz PC with 8GB Main memory, operating on Microsoft Windows 7. We also have used the same dense dataset with 40,000 transactions, associated with 994 different items that we used for testing the performance of DRMUD in horizontal format. We have also run our vertical algorithm with the same values of k, and configuration parameters: minsup1, and minsup2 that we used in horizontal format. Figure 1 shows a comparison between the vertical and horizontal versions of DRMUD in terms of run times, where the runtimes differ by the parameters specified by the users.

Fig. 1. Comparisons between Horizontal and Vertical for DRMUD  algorithms under 40k transactions associated with 994 different items  B. CCRCD algorithm vs the CR-PA algorithm To the best of our knowledge, we are the first ones to  introduce disjunctive causal rules from uncertain data, where our target consists of k-disjunctive variables (where  is the length of the rule and it is chosen by the user). We have developed the algorithm CCRCD for discovering combined causal rules from certain data. Both the CCRCD algorithm and the CR-PA algorithm [3] discovered the same causal rules. We have used two datasets. Dataset1 is downloaded from [36].

This dataset has also been used in [4] for discovering causal rules using PC [38], HITON-PC [39] and CR-PA [3] algorithms with eight attribute variables and one target variable, within 100k records. Dataset 2 has been generated using the software tool Tetrad [37] that allows the construction of a graphical model for inferring causal reasoning. It consists of 29 attribute variables and one target variable within 856 records. Table 5 shows the results for both CR-PA and CCRCD algorithms under dataset 1 and minimum support=0.05, and p-value= 0.05 (95%). Note that in the original data the names for the attributes were (A, B, C, D, E, F, G, H), and the target name was (Z). We changed the attribute names to the values {1, 2?, 8}, and the target to the value {9}.

minsup1=0.007, minsup2=0.01  minsup1=0.0075 minsup2=0.015  minsup1=0.009, minsup2=0.02  Ti m  e in  S ec  on ds  k=2  DRMUD Horizontal DRMUD Vertical     minsup1=0.007, minsup2=0.01  minsup1=0.0075 minsup2=0.015  minsup1=0.009, minsup2=0.02  Ti m  e in  S ec  on ds k=3  DRMUD Horizontal DRMUD Vertical     Table 6 shows the results for both CR-PA and CCRCD algorithms under dataset 2 with minimum support=0.01, and p-value= 0.05 (95%). Here the attributes names are {1, 2?, 29},  and the target name is {30}.

Table 7 shows the results for both CR-PA and CCRCD algorithms under dataset 2 with minimum support=0.01, and p-value= 0.1 (90%).

Table 5: Dataset 1 Causal Rules under p=0.05, min. support=0.05  CR-PA CCRCD B  Z 2  9 C Z 3 9 F Z 6 9   Table 6: Dataset 2  Causal Rules under p=0.05, min. support=0.01 Causal rule CR-PA CCRCD  3  30 ? ? 6  30 ? ? 22  30 ? ? 23  30 ? ? 27  30 ? ? 29  30 ? ?  11 28  30 ? ?   Table 7: Dataset 2 Causal Rules under p=0.1, min. support=0.01  Causal rule CR-PA CCRCD 2  30 ? ? 3  30 ? ? 6  30 ? ?  15  30 ? ? 22  30 ? ? 23  30 ? ? 27  30 ? ? 29  30 ? ?  9 17  30 ? ? 11 28  30 ? ?  C. DCCRUD algorithm Since there is no existing algorithm for generating  disjunctive combined causal rules from uncertain data, we have combined our method -DRMUD algorithm [33]- with CR-PA algorithm [3] and proposed the DCCRUD algorithm to mine disjunctive combined causal rules from data without uncertainty.

DCCRUD is implemented and compiled using Java.

Following is the execution environment:  Intel(R) Core(TM) i7 3.40GHz PC 8GB Main memory Operating System is Microsoft Windows 7.

For testing the performance of DCCRUD over uncertain databases, we have used the following procedure:  We generated two datasets using [37]. Dataset 1 consists of 5142 records with 20 attribute variables and five target variables, and dataset 2 consists of 26380 records with 40 attribute variables and ten target variables  For each dataset, we have run our algorithms with different size of the rules , and w and different configuration parameters of minimum expected support (0.001, and 0.0009), and p1?value: 0.1 (90%), along with p2?value: 0.05 (95%), and p1?value? 0.025 (97.5%), p2?value: 0.01 (99%). Table 8 shows the results of disjunctive single and combined Causal  rules under p1-value= =0.025, p2-value= =0.01, minimum support=0.001, with rule maximum size k=2.

Table 8: Dataset 2 Disjunctive single and combined Causal Rules DCCRUD  [5]  [21] [13]  [23] [6]  [22]  [24] [14]  [21]  [7]  [22] [19]  [23] [8]  [21] [4 5]   [25]  [10]  [21] [3  16]  [21] [12]  [22] [17]  [22]  [24]  Figure 2 shows the performance of our proposed algorithm. Note that the runtimes can vary based on other parameters provided by the users. Figure 3 shows the scale-up of the attributes. The run time increases as the number of attributes increases. Also, the run time decreases as the minimum support increases as is shown in figure 4.



V. CONCLUSIONS Causal discovery is a well-known problem that also has  been addressed using association rules mining under certain data. In our prior work [33] we were the first to introduce disjunctive rules mining from uncertain data. In this paper we have introduced and studied the problem of mining disjunctive combined causal from uncertain data. To the best of our knowledge we are the first ones to introduce this problem. We have proposed the DCCRUD algorithm to tackle this problem.

Our algorithm DCCRUD works under vertical layout. We have presented theoretical and experimental results for the proposed method.

Fig. 2. Performance of DCCRUD algorithm       0.1  0.2  0.3  0.4  minSup=0.001 minSup=0.0009 minSup=0.001 minSup=0.0009  Ru nn  in g  Ti m  e (s  ec on  ds )  k=2  p1=0.1, p2=0.05  p1=0.025, p2=0.01  (a) 5142 Records, 20 attributes and 5 targets   0.5  minSup=0.001  minSup=0.0009  minSup=0.001  minSup=0.0009  Ru nn  in g  Ti m  e (s  ec on  ds ) k=2  p1=0.1, p2=0.05  p1=0.025, p2=0.01  (b) 26380 Records, 40 attributes and 10 targets k vs. time   0.5  p1=0.025, p2=0.01  p1=0.1, p2=0.05  p1=0.025, p2=0.01  p1=0.1, p2=0.05R  un ni  ng T  im e  (s ec  on ds  )  (c) Scale-up of attributes, K=2 20 Attrib., 5 Targ.

40 Attrib., 10 Targ.

