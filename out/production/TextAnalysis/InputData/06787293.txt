Device-driven Metadata Management Solutions for Scientific Big Data Use Cases

Abstract?Big Data applications in science are producing huge amounts of data, which require advanced processing, han- dling, and analysis capabilities. For the organization of large scale data sets it is essential to annotate these with metadata, index them, and make them easily findable. In this paper we investigate two scientific use cases from biology and photon science, which entail complex situations in regard to data volume, data rates and analysis requirements. The LSDMA project provides an ideal context for this research, combining both innovative R&D on the processing, handling, and analysis level and a wide range of research communities in need of scalable solutions. To facilitate the advancement of data life cycles we present preferred metadata management strategies.

In biology the Open Microscopy Environment (OME) and in photon science NeXus/ICAT are presented. We show that these are well suited for the respective data life cycles. To facilitate searching across communities we discuss solutions involving the Open Archive Initiative - Protocol for Metadata Harvesting (OAI-PMH) and Apache Lucene/Solr.

Keywords-Metadata Management, Scientific Big Data, Light Microscopy, Photon Science

I. INTRODUCTION Microscopes in biology and detectors in photon science  generate vast amounts of potentially precious data about molecules, organisms and physical events. The format of the data is partly proprietary and dependent on the applied instruments and methods, whereas information needed for further analysis is independent of the format. Metadata re- ferring to a metadata scheme is used to describe the content of data for a specific domain in a meaningful way in different phases of the data life cycle and allow for the efficient search of data elements. The volume and data acquisition rates in these highly complex scenarios are drastically increasing by orders of magnitudes, and the data, even in an individual field of study, becomes much more diverse.

Challenges in metadata management are manifold. Cur- rently many scientific communities do not deal with meta- data at all, but the growing amount of files lets them realize the need. In order to integrate increasing amounts data well  with the working environments of scientists, systems and standards have to be conceptually developed and tightly integrated. It is of paramount importance to automate as many steps as possible - the individual annotation of millions of data sets by hand is impossible. Since many metadata standards [1] there exist already, designing and implement- ing new formats and systems should be avoided wherever possible. It is often preferential to store metadata alongside or inside the data, since the link to the data is always maintained, which makes sharing, archiving, and re-using much easier.

The major challenge to create meaningful metadata is addressed in the context of the project LSDMA (Large Scale Data Management and Analysis) [3] by the Helmholtz Association of German Research Centers. More than ten key players, Helmholtz centers and universities, working on Big Data challenges, are participating. It aims to optimize the process of gaining scientific knowledge in close cooperation with applied scientists by improving data life cycle manage- ment methods. On one hand Data Life Cycle Lab (DLCL) conduct research, which is specific to the involved communi- ties like climate research or engineering. Five DLCLs with more than 25 communities are participating. On the other hand the Data Services Integration Team aims at generic solutions and services across DLCLs.

We present the evaluation for two use cases. One is light microscopy within life sciences with the Open Microscopy Environment (OME) [4] and the other is photon science using the ICAT metadata catalogue [5].



II. LIGHT MICROSCOPY AND THE OPEN MICROSCOPY ENVIRONMENT  The field of light microscopy is characterized by a large variety of conventional and new high-resolution mi- croscopes. Interpretation and processing of this data has become increasingly complex [6]. OME has become the de- facto standard for description and exchange of biological   DOI 10.1109/PDP.2014.119     microscopy data. OMERO [7], a tool based on a client-server model for data management, visualization and standardized analysis, implements the standard.

In the following we describe the field of light microscopy and present main aspects of the compliant file specification related to metadata for large data sets [8] and the OME- XML format [4]. Then we evaluate OME in detail and in comparison to DICOM.

A. Light Microscopy  Highly resolution fluorescent microscopes, such as lo- calization nanoscopes, are novel instruments providing un- matched insights into molecular arrangements and nanos- tructures of cells and cell nuclei. The advantage of local- ization nanoscopy comes from the combination of visibil- ity using standard preparation techniques and sophisticated computer image analysis with standard high quality optics and fast detectors. This offers new perspectives in biomedi- cal research resulting in novel aspects of diagnostics based on large scale data sets [9].

For localization nanoscopy, biomolecules are specifically tagged by fluorescent markers. By image processing the spatial positions of all individually tagged events can be determined with nanometer precision, resulting in a merged image of thousands of points (see Figure 1). From such point patterns characteristic nanostructures can be calcu- lated. Within the LSDMA activities two fields of biomedical research are addressed: a) Understanding mechanisms in the arrangement and clustering of highly expressed recep- tor molecules in cellular membranes of tumor cells and the influence of chemotherapy drugs; b) Understanding nanoscaled chromatin arrangements in cell nuclei during tumor genesis and after radiotherapy.

Figure 1 shows a membrane of a breast cancer cell after labelling. On the left an overlay of a standard wide-field image (green) and the result of localization imaging (red) of a membrane section is shown, where the human epidermal growth factor Receptor 2, a typical breast cancer marker, is labelled. The right image only shows the result of local- ization imaging, which was obtained from a time series of 1000 images. Here, each fluorescent point represents a single fluorochrome or antibody, attached to a receptor molecule.

In contrast to the wide-field image the resulting localization image reveals details of the formation of receptor clusters or linear arrangements of receptors.

B. The OME Metadata Model and Database  OME was designed to use the XML format for storing metadata related to a microscope image and the environment it was captured in. XML can be modified directly by XML methods or on a text-editor basis. Furthermore, XML can be mapped to relational databases or directly stored in XML databases. These offer the option to realize complex queries  Figure 1. Images of the membrane of breast cancer cell after specific la- belling of the Her2-receptors by means of fluorescence labelled antibodies.

and therefore a high degree of automation. The database structure of OME is visualized in Figure 2.

Figure 2. Database structure of OME XML: Input consists of raw data and metadata, stored in XML, related to image, experiment, and result.

The basic idea of most metadata definitions in micro- scopic imaging applications is threefold. The first class of fields includes a unique description, including a unique identifier, of the imaging setup like image descriptors, def- inition of the microscope hardware used, and parameters used during acquisition. The next metadata set describes the experiment and the name of the experimenter. A third class of fields allows for the definition of free comments, which may represent valuable information not fitting in the given scheme. This class, however, is non-standardized and therefore limits the portability.

Another specific property of the OME format is that each step of the data processing and visualization pipeline can be represented. It supports multiple image stacks at various pro- cessing stages. The dimensions are currently limited to 5D, i.e. three spatial dimensions are represented by multiple 2D images, one time dimension, and one application dependent parameter space.

OMERO [7], based on OME, is a client-server model intended to enable data handling, metadata editing, and data visualization. Its three components are a client, a server, and a database. The server is a middleware to interprete OME-XML schemas and to convert them to interact with both, database and client. As client OMERO provides a web browser interface and a standalone Java application.

C. Evaluation of OME in Comparison with DICOM  In LSDMA we apply OME for handling localization nanoscopy with medical applications where microscope data is used for diagnosis purposes in pathology. With respect to these application areas, OME competes with the DICOM [10] standard, which is widespread in radiological imaging.

Hence, it is of interest to compare both standards in context of Big Data.

DICOM shares some similarities to OME. First of all, it also uses unique identifiers. Instead of XML data is written in a binary tagged format with an address, description, and entry. Tagged fields are not human readable and are only usable in applications with format conversions. With a stricter definition and the possibility to map a production workflow into the format DICOM is oriented towards routine work in clinics. OME on the other hand is more suited to research applications with its greater flexibility.

A second issue in OME is the data format which is based on stacked 2D data like in DICOM. Stacked data has the advantage that it does not require contiguous memory for access. Especially methods for analysis and visualization that access 3D, 4D, or 5D information require a scattered read of the memory, which renders processing of huge chunks of data inefficient. The limitation to 5D data concerns both, OME and DICOM.

A significant advantage for DICOM is the option to attach intermediate data from the workflow process into the overall structure. This allows keeping data together, which is re- quired in well-defined productive environments. Concerning workflows, support for diverse applications and networking is native in DICOM, with the a client-server models for com- munication and high quality printing services, and available in OME via additional tools like OMERA.

In conclusion, OME is a first and necessary step towards uniform handling of metadata for microscope data sets. It is based on a modern concept like XML and its open source strategy allows vendors and research groups to freely extend and generalize it. DICOM in contrast enforces more strict definitions and the standardization of usage patterns regardless the underlying hardware and operating system.

OME has not been designed for huge data sets, while high- resolution microscopy like localization nanoscopy produces huge data sets. This shortcoming of OME marks a challenge for substantial further developments.



III. PHOTON SCIENCE USE CASE AND THE ICAT METADATA CATALOGUE  With many synchrotron facilities switching from particle physics to photon science the organizational characteristics, data rates, and data amounts of the new experimental se- tups differ greatly from the large collaborations in particle physics. In order to adapt to these changes, computing facilities need to re-think their established data management  concepts. We describe photon science as a scientific field, ICAT as a solution, and evaluate the latter.

A. Photon Science  The term ?photon science? encompasses facilities that use high-energy light sources, often X-ray lasers powered by particle accelerators, to investigate small-scale details of matter and organisms. These facilities generally have a similar experimental setup and data workflow, and members of the PaNdata project agree that their current challenges in the field of data handling are quite similar. PaNdata aims to deploy and test a common data infrastructure for 14 European institutes. The aim is to provide users with common software and a federation of their data catalogues, which enables users with appropriate access rights to search and download data located at any facility. Typically, exper- imental stations in photon science labs are built close to detectors. Such setups are called beamlines. Detectors used may either generate images or raw numeric data. Data rates may differ greatly between detector types. Modern detectors can generate image streams in the order of 10 GByte/s and up to 100 images per second.

?Beamtime?, the time granted for each experiment at a beamline, is generally very tight as the facilities are in high demand. So, to easily to data management scientists stored data directly on portable media. With the new high rate detectors, this is getting less and less feasible. Hence, the photon science community is in need for an integrated data management that will move data directly from detectors to large and robust backend storage systems. Access needs to be possible via network file systems and interactive download facilities. An integration of such data storage systems with analysis facilities is also highly desired. The ideal situation would be that scientists start by immediately setting up their experiment and starting to use the beam. The data would be automatically copied to the storage systems, annotated with metadata, and stored in a catalogue system.

Later the scientist would use the catalogue to retrieve their data for further analysis. This would allow the scientist to fully concentrate on their science and not data management.

B. The ICAT Metadata Model and Catalogue  ICAT?s metadata model [5] closely follows the real-life situation at most photon science laboratories. After the principal investigator submits a proposal, which, if accepted, grantes beamline time. Studies consist of one or more experiments. An experiment is carried out at an instrument, which is part of a facility, and produces datasets. To account for the real-world behavior of photon science detectors, datasets consist of one or more data files. Finally, the col- lected data is aggregated and published. This bottom-up data model is augmented by auxiliary information that allows for the parameterization of the experiments, instruments, and description of the samples. Also user information and access     rights are stored. Although this schema very closely models reality there is inflexibility that might lead to problems during adoption. For example, there is the requirement in the PaNdata cooperation for assigning more than one instrument to an experiment. Also, it needs to be understood how to map complex access rules to the internal ICAT rule engine.

The ICAT metadata catalogue is the core component in an open source suite of tools developed by several institutes. These tools are intended to provide photon science communities with an integrated data handling system for downloading data files, metadata handling and querying as well as provenance tracking and analysis support. ICAT is the most advanced of these components and is considered to be production-ready by the developers. It is the catalogue of choice for the PaNdata project.

Figure 3. ICAT follows the standard J2EE architecture. Catalogue, GUI, and data server run as applications on the same Glassfish instance. The database is connected via JDBC and interfaces with the outside via WSDL.

The catalogue system itself follows the standard J2EE architecture (Figure 3) and relies on a underlying Java EE application server. The reference configuration includes the Glassfish application server. It connects to either Oracle or MySQL databases and offers a web service interface to interact with other system components, like the download manager, the TopCat web GUI, or custom ingestion modules.

A plugin interface system accepts custom authentication modules. ICAT ships with authenticators for LDAP, a local database or anonymous accounts, which are useful for open- access systems.

C. Evaluation of ICAT in Photon Science  By offering a standard interface to the catalogue, ICAT is completely agnostic about stored file types. Files on a backend storage system are referenced by a string, so that backend data stores might support POSIX, or not, like object stores. Since with this approach metadata is only present in the catalogue, special care must be taken to ensure that it is always available and consistent throughout the lifetime.

Furthermore, a method to hand out the metadata along with the data sets has to be offered for download. An approach to facilitate this is to store the metadata directly in the data files. Going in this direction the PaNdata and CRIPS projects  are focussing on establising the NeXus data format [2] as a standard for photon science. NeXus is a specialization of the HDF5 format [11], widely used in high performance computing, so that every well-formed NeXus file is also a valid HDF5 file and can be handled by the already present large HDF5 software ecosystem. HDF5 is a hierarchical container file format that allows grouping of scientific data, as well as flexible selective operations on stored data.

NeXus augments this format by sets of metadata entries called ?application definitions?, which specify a minimal mandatory subset of metadata. More user-defined metadata may be attached to a file, but application definitions ensure that certain information is always present.

DESY, as a major German particle accelerator facility adopted NeXus as standard file format for its photon science beamlines. Already detector manufacturers start to directly implement NeXus as standard format. For other beamlines the necessary meta information for annotion has to be gath- ered from as before. The integration means that ingestion into any catalogue is greatly facilitated and data, including its metadata, can be transferred between storage systems.

The catalogue becomes a modular search tool and can easily be replaced.

In the future, several auxiliary tools are planned for the ICAT ecosystem. The most important is the Integrated Data Server (IDS), which will allow the upload and download of files to and from backend storage systems. Others include a Job Portal, for submitting analysis jobs on datasets to com- puting resources, and administration tools. Also, improve- ments to the ICAT itself are envisioned within PaNdata, such as support for access control mechanisms, an installation wizard, and a web-based administration interface.



IV. DISCUSSION  The Open Archive Initiative - Protocol for Metadata Harvesting (OAI-PMH) [12] is a standardized protocol for the collection of distributed metadata. The minimal set of required data is Dublin Core and any representation in XML is supported. OAI-PMH facilitates cross community metadata exchange with many communities already making use of XML. It should be easy for new communities to transfer their metadata to XML using XSLT transformations.

This facilitates the use of OAI-PMH for collecting metadata.

Furthermore, existing legacy tools can still be applied, simply using a different schema. Another aspect of OAI- PMH is that metadata databases can be set up on a smaller scale and then aggregated using OAI-PMH.

The most important aspect of metadata is the possibility to quickly search in large volumes of data by specifying metadata criteria. Here Apache Lucene [13] is the most widely used library which is open source. With Solr we already see enterprise grade search platforms, built on top of Apache Lucene. It is highly scalable and supports even highly advanced features like faceted search and dynamic     clustering. For searching in generic, community-created metadata in Solr [13], tools like ?vygr2vfind? emerge. It allows harvesting metadata via OAI-PMH and distribute it via Solr using a future workflow like show in Figure 4.

Figure 4. Workflow for generic search with Solr, using several sources of metadata and distributed harvesting.



V. CONCLUSION AND OUTLOOK  As discussed in the previous sections, sophisticated and production ready solutions for Photon Science and Light Microscopy are deployed in selected communities. However, some communities only use a rudimentary set of content metadata stored in plain text files or encoded in file and directory names, while many communities do not make use of metadata at all. The presented solutions are specifically adjusted to the needs of their users and therefore hardly transferable to other communities. However, with OAI-PMH there is a protocol available, which is in common use. To- gether with Apache Lucence/Solr it provides comprehensive gathering, indexing, and searching capabilities. The concept presented offers the possibility to add metadata for different communities in a lightweight way. This standardized and pluggable approach allows setting up more metadata servers and thereby facilitating distributed searches across Big Data use cases.

An open topic is the Authentication and Authorization Infrastructure (AAI) for defining access permissions to metadata. The LSDMA project is currently working on an integrative approach to support several federated identity management systems. Once LSDMA-AAI is production- ready, access to the metadata can be secured, based on dynamic group structures.

One further challenge is the utilization of term decla- rations. These will be varied if no declarations exist. For example, the project name can be declared by an ID, the project name or an abbreviation. As a result a comprehensive search becomes difficult. On one hand, this problem can only be solved by strict conditions. On the other hand, strict conditions do not always serve the purpose in the research community. Therefore, ways to transparently unify semantics have to be found.

