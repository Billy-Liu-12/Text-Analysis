Multiagent Reinforcement Learning Using OLAP-Based Association Rules Mining

Abstract In this paper we propose a novel multiagent learning  approach, which is based on online analytical processing (OLAP) data mining. First, we describe a data cube OLAP architecture which facilitates effective storage and processing of the state information reported by agents. This way, the action of the other agent, even not in the visual environment of the agent under consideration, can simply be estimated by extracting online association rules from the constructed data cube. Then, we present a new action selection model which is also based on association rules mining. Finally, we generalize states which are not experienced sufficiently by mining multiple-levels association rules from the proposed data cube.

Experiments conducted on a well-known pursuit domain show the effectiveness of the proposed learning approach.

1. Introduction  One approach to model multiagent learning is to augment the state of each agent with the information about other existing agents [7, 9, 10]. However, as the number of agents in a multiagent environment increases, the state space of each agent grows exponentially. This way, even simple multiagent learning problems become computationally intractable by standard reinforcement learning approaches.

Although Q-learning is the most widely used method in multiagent learning, it has some drawbacks, including modeling other learning agents present in the domain as part of the state of the environment and experiencing some state- action pairs much less than others during the learning phase.

To overcome these problems, in this paper we propose a novel multiagent learning approach that integrates OLAP based data mining into the learning process.

OLAP mining integrates online analytical processing with data mining in a way that substantially enhances the power and flexibility of data mining and makes mining an interesting exploratory process [2, 5].

The rest of the paper is organized as follows. The necessary background on Q-learning, association rules mining and OLAP is provided in Section 2. A variant of the pursuit problem is described in Section 3. The proposed OLAP mining based multiagent learning approach is presented in Section 4. The conducted experiments and the results achieved are discussed in Section 5. Summary and conclusions are included in Section 6.

2. Background Information  In this section, we provide an overview of the three basic concepts utilized in this paper, namely, Q-learning, association rules mining and OLAP technology.

First, Q-Learning is an incremental reinforcement learning method that can be used online and does not need a model for its application. The Q-learning algorithm stores the expected reinforcement value associated with each state-action pair in a look-up table. An agent selects its action based on an action-value function, called Q-function, which is updated using agent?s experience. So, consider action a and state s, the Q-function of action a in state s is formally defined as:  )),(max(),()1(),( ?? Aa  asQrasQasQ ? ?  ++?= ???  where ? (0??<1) and ? (0???1) denote learning rate and discount parameter, respectively; and ),( ll asQ is the value of  action al in state sl.

Second, association rules form an important class of  regularities that exist in databases [2, 3]. Formally, given a set of items I, an association rule is a correlation of the form X?Y, where IYX ?)( U  and ?=)( YX I . The intended meaning of X?Y is that a given transaction T? I, which contains items in X is likely to contain items in Y as well.

A rule X?Y is generally rated according to several criteria, none of which should fall below a certain (user-defined) threshold. In common use are the following measures where DX={T?D | X?T} denotes transactions present in database D and contain items in X, and |DX| is its cardinality:  A measure of support defines the absolute number or the proportion of transactions present in D and contain YX U :  ||)sup( YXDYX U=?    or ||  || )sup(  D  D YX YX U=? .

The confidence is the proportion of correct applications of the  rule: ||  || )(  X  YX  D  D YXconf U=?  Third, OLAP provides users with the flexibility to view data from different perspectives, and data cube is the most popular data structure for OLAP. A cube is a data organization similar to a multidimensional array of values. It represents measures over several dimensions. Each dimension of the cube contains |1| +iA  values, where || iA  is the number of distinct values at dimension iA . The first || iA  rows represent the distinct values of iA . Each cell in these rows stores the count value generated from relations between the dimensions. The ?+1? term in the above formula represents a     special ?Total? value in which each cell stores the aggregation value of the previous rows. These aggregation values show one of the essential features of cube structure.

(a)     (b) Figure 1 a) A sample initial position in 15x15 pursuit  domain    b) A sample goal state  3. The Problem Domain  Samples of the multiagent environment considered in this paper are shown in Figure 1. It is a variant of the well-known pursuit domain and has the following characteristics. First, it is fully dynamic, partially observable, non-deterministic and has a homogeneous structure. Second, two hunter agents and a prey agent exist in a 15?15 grid world; the initial position of each agent is determined randomly. Third, at each time step, agents synchronously execute one out of five actions: staying at the current position or moving from the current position north, south, west, or east. More than one hunter agent can share the same cell. However, a hunter agent cannot share a cell with the prey. Also, an agent is not allowed to move off the environment. The latter two moves are considered illegal and any agent that tries an illegal move is not allowed to move. Further, hunters are learning agents and the prey agent selects its own action randomly or based on a particular strategy such as Manhattan-distance measure.

Finally, the prey is captured when the two hunter agents are positioned at two sides of the prey. Then, the prey and the two hunter agents are relocated at new random positions in the grid world and the next trial starts.

4. OLAP Mining Based Multiagent Learning  4.1 Internal Model Association Rules Mining from the Proposed Data Cube  In our work described in this paper, as a given agent executes an action, we also consider actions of the other agent. For this purpose, an internal model database is required to hold the actions of the other hunter agent. This database is built using OLAP architecture. It is represented using the data cube shown in Figure 2, which has three dimensions to represent the internal model of a hunter agent. While two dimensions of this cube deal with the states of a hunter and the prey in the visual environment?, the third dimension  ? Visual depth is assumed 3, unless otherwise specified.

represents the action space of the other hunter.

With reference to the cube shown in Figure 2, as long as a  hunter agent observes new states, the learning process continues and the observed action of the other agent in the corresponding state is updated. So, each cell stores the count value generated from the current state and the observed action. Finally, in order to explicitly express the dependency of the other agent?s action, the hunter?s Q-function is adjusted as follows.

Consider a hunter h1, which tries to estimate the action of an agent h2. The corresponding Q-function is represented as  ),,,( otherself aasQ  where s is the state that hunter h1 can observe; )( selfself Aa ? and )( otherother Aa ?  are, respectively, actions of h1 and  h2,; here, selfA and otherA represent sets of all the possible actions for h1 and h2, respectively. Accordingly, the action  othera  is a hidden and major variable in selecting the action  selfa . In this study, the action othera  is estimated based on the  association rules extracted from the constructed data cube. If one hunter observes the other hunter in its visual environment, then the association rule otheras ?  is easily mined from observations of the other hunter?s past actions.

On the other hand, if one hunter could not perceive the other hunter, a prediction is done based on the following proposition in order to estimate the direction of the action of the unseen other agent.

D  (-3, -3)  (-3, -2)  (3, 3)  N ot  A vailable  Not Available  othera  othera  othera  othera  othera  ...

...

Hunter  Prey  Action  Both Not Available  count 2  count 1  count 3  count 3 Total Count  ???? ????  ???? ????  ??? ???  ??? ??? ???? ???????? ????C  ???? ???????????? ????  ??????? ???????  ???? ???? ????  (-3, -3)  (-3, -2)  (3, 3)  Figure 2 The proposed data cube  To satisfy data mining requirements as mentioned in Section 2, at the beginning of the learning process, the user specifies a minimum support value for action count, indicated as count3 in Figure 2. If the count value of a state reaches this minimum support value, then it is assumed that the state has been experienced sufficiently. In such a case, the hunter agent under consideration estimates the action of the other agent with respect to the highest confidence value. If a state is not experienced sufficiently, then the agent under consideration estimates the action of the other agent with respect to the user specified confidence value. If the number of occurrences of a state-action pair is less than the user specified minimum confidence value, then this action is not selected in the corresponding state. If there are more actions exceeding the minimum confidence value in a state, then the     possibility of selecting an action ai is computed as:  ? ? ? ?=  )( )(  )( )|(  MinConfAa j  i i  j  asconf  asconf sap , where )( iasconf ?  is the  confidence value of the rule ias ? , and )(MinConfA  is the set of possible actions that exceed the minimum confidence value for the corresponding agent.

4.2 Multiagent Learning by Mining Association Rules from the Data Cube  In this section, we present our approach of utilizing the data cube structure in mining OLAP association rules that show the state-action relationship. The dimensions of the data cube are specified as the state information and the actions of both hunters. The mining process developed for this purpose is described next in Algorithm 1.

Algorithm 1 (Mining Based Multiagent Learning) The proposed mining based learning process involves the following steps: 1. The hunter agent under consideration observes the current  state s and estimates othera  based on the association rules extracted from the data cube. As mentioned in Section 4.1, if the number of occurrences of s is greater than the minimum support value, then othera  with the highest confidence value is selected; otherwise, othera  is selected by considering every action of the other hunter, say ai, which exceeds the minimum confidence value in s, based on the value of )|( sap i .

2. The action selfa  is selected according to the estimated othera .

Similar to the previous association rules mining process, if the count of a state- othera  pair is greater than or equal to the specified minimum support value, then it is assumed that the relevant state and othera  were experienced sufficiently.

In this case, the hunter agent under consideration selects the action with the highest confidence value.

3. If the state- othera  pair is not experienced sufficiently, the agent selects its action based on the value of )|( sap i , in a way similar to the ?otherwise? part of step 1.

4. The hunter under consideration executes the action selfa  selected either in step 2 or in step 3.

5. Simultaneously, the other hunter executes the action *othera .

6. The environment changes to a new state s1.

7. The hunter under consideration receives a reward r from  the environment and updates the data cube.

8. If s1 satisfies a terminal condition, then terminate the  current trial. Otherwise, ss ?1  and go back to step 1.

4.3 Mining Multiple-Levels Association Rules from the Data Cube  Experiments showed that at the end of the learning process, some state-action pairs are experienced sufficiently, and some are never visited, depending on the escaping policy followed by the prey. Recall that an agent selects its own  action based on the information obtained from the environment. However, for some states, this information is not sufficient to mine association rules. To overcome this, the information is generalized from a low level to a higher level.

We decided to use reduced minimum support at lower levels.

In general, most of the work done on OLAP mining consider reduced minimum support across levels, where lower levels of abstraction use smaller minimum support in the mining process.

5. Experimental Results  We conducted some experiments to evaluate our approach, i.e., to test the effectiveness of learning by extracting association rules that correlate states and actions. Further, in order to show the superiority of our approach, we compare it with the standard Q-learning approach, abbreviated as SQL in the rest of the paper.  In our experiments, we concentrated on testing changes in the main factors that affect the proposed learning process, namely minimum support, minimum confidence and visual depth. All the experiments have been conducted on a Pentium III 1.4GHz CPU with 512 MB of memory and running Windows 2000. Further, in all the experiments, the learning process consists of a series of trials and each reported result is the average value over 10 distinct runs. Each trial begins with a single prey and two hunter agents placed at random positions inside the domain and ends when either the prey is captured or at 2000 time steps. Upon capturing the prey, individual hunters immediately receive a reward of 100. Finally, we used the following parameters in the Q-learning process: learning rate ?=0.8, discount factor ?=0.9, the initial value of the Q- function is 0.1, and the visual depth of the agents is set to 3, unless specified otherwise.

0 2000 4000 6000 8000 10000 12000 14000  Trials  T im  e st  ep s  to c  ap tu  re th  e pr  ey  MinSup3K MinSup5K MinSup7K SQL  Figure 3 Learning curves of the hunter agents when the prey escapes randomly  In the first two experiments, the prey follows random escaping policy. The first experiment investigates the learning process for different minimum support values, with the minimum confidence value fixed at 0%, until the number of occurrences of each state reaches the minimum support value. The learning curves of the steps required to capture the prey with respect to different values of minimum support are plotted in Figure 3.

From Figure 3, which also demonstrates the superiority of     our approach over SQL, it can be easily seen that the learning curve when the minimum support value was set to 5K (labeled MinSup5K in Figure 8) converges to the near optimal solution faster than the curve labeled MinSup7K.

Finally, it can also be easily observed from Figure 8 that the hunter agent almost learned nothing when the minimum support value was set to 3K.

0 2500 5000 7500 10000 12500 15000  Trials  T im  e st  ep s  to c  ap tu  re th  e pr  ey  MinSup5K  MinSup7K  MinSup9K  SQL  Figure 4 Learning curves of the hunter agents when the prey escapes using Manhattan-distance  In the second experiment, the escaping policy of the prey is Manhattan-distance to the located hunter. The results of this experiment are shown in Figure 4. Comparing this with the results of the previous two experiments, it can be easily observed that each state has to be experienced more for more complex multiagent environments. Also, it is enough to select the minimum support value as 7K because the hunter agent cannot learn for lower minimum support values, and it is unnecessary to increase the minimum support value. Here note that at convergence time the difference in the number of steps and trials between our approach and SQL are larger than those of the first experiment. This situation shows that our approach gives better results in more complex domains.

0 2500 5000 7500 10000 12500 15000  Trials  T im  e st  ep s  to c  ap tu  re th  e pr  ey  TotalCount100K(7K)  TotalCount150K(9K)  TotalCount150K(7K)  Figure 5 Learning curves of the hunter agents with generalized states  The last experiment is dedicated to the multiple levels case, while the prey escapes using Manhattan-distance. In this experiment, our goal is to generalize a state that has been experienced enough compared to other states. In such a case, the agent observes the environment from a higher level and decides on the best action. Here, if the total count reaches the pre-determined threshold value and the minimum support value of the current state is below the threshold, then the agent goes up to a higher level in order to get more detailed information from the environment. The results of this experiment for two different total count values and two  different minimum support values are plotted in Figure 5, which also considers generalizing only states of the other hunter agent.

6. Summary and Conclusions  In this paper, we proposed a novel multiagent reinforcement learning approach based on association rules mining in an OLAP architecture. To achieve this, we defined a data cube for holding all the environment related information obtained by agents. Using the constructed data cube, we estimated the action of the other hunter agent, even when it is not in the visual environment. Also, we proposed a method to generalize states that were not experienced sufficiently. In addition, we presented a new action selection method in order for the agents to take the most appropriate action. Experimental results showed that the proposed learning approach could be used more effectively to achieve high quality optimal solutions. As a result, we can state that the proposed data cube and association rules mining based learning algorithms work well for multiagent domains.

Currently, we are investigating the possibility of applying our methods to problems that require continuous state space and to develop and improve different corresponding algorithms.

