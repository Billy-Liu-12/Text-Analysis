Memory-Aware Sizing for In-Memory Databases Karsten Molka??, Giuliano Casale?, Thomas Molka?, Laura Moore?

Abstract?In-memory database systems are among the tech- nological drivers of big data processing. In this paper we apply analytical modeling to enable efficient sizing of in-memory databases. We present novel response time approximations under online analytical processing workloads to model thread-level fork- join and per-class memory occupation. We combine these approx- imations with a non-linear optimization program to minimize memory swapping in in-memory database clusters. We compare our approach with state-of-the-art response time approximations and trace-driven simulation using real data from an SAP HANA in-memory system and show that our optimization model is significantly more accurate than existing approaches at similar computational costs.

Index Terms?Optimization; In-memory Databases; Perfor- mance; Closed Queueing Networks; Approximation, SAP HANA.



I. INTRODUCTION  In-memory database systems leverage new technologies, such as SDRAM, flash storage, FPGAs and GPUs, to sharply optimize database throughputs and latencies. Case studies show that in-memory databases can achieve tremendous speedups, outperforming traditional disk-based database sys- tems by several orders of magnitude [1]. As a result, in- memory systems are in high commercial demand, in particular as part of cloud software-as-a-service offerings [2]. This poses new challenges regarding the management of these applications in cloud infrastructures, since there is virtually no architectural design, sizing and pricing methodology focused explicitly on in-memory technologies.

This paper tackles this problem by introducing a novel provisioning framework specifically tailored to in-memory databases. We propose a novel optimization-based method- ology to provision these systems at a reference timescale, minimizing costs. Our methodology can be applied to in- memory database clusters that are continuously monitored and feed performance measurements into our framework. Our framework enables what-if analyses for various in-memory database configurations regarding performance and cost with- out the need to set up experiments physically. In particular, we seek for load-dispatching routing probabilities that can load balance in-memory instances for a set of clients respecting the service level agreement (SLA) in place with the customer.

We use a queueing modeling approach to describe the levels of contention at resources, in order to establish the likelihood that a sizing configuration will comply to SLAs.

In particular, since in-memory systems are memory-bound applications, it is crucial that their sizing models can capture memory constraints, as memory exhaustion and swapping are more likely to happen in this class of applications. Conversely, existing sizing methods for enterprise applications have pri- marily focused on modeling mean CPU demand and request response times. Memory occupation is difficult to model as it requires the ability to predict the probability of a certain mix of queries being active at a given time. However, probabilistic models tend to be expensive to solve, leading to slow iteration speed when used in combination with numerical optimization.

To cope with this issue, we introduce a framework based on approximate mean-value analysis (AMVA), a classic method- ology to obtain performance estimates in queueing network models [3]. We observe in particular that current AMVA methods are unable to correctly capture the effects of variable threading levels in in-memory database systems and propose a correction that markedly improves accuracy. Our approach retains the same computational properties of AMVA and it is simple and inexpensive to integrate in optimization programs.

We also demonstrate that multi-start interior point methods and evolution strategies can be effectively used to solve the resulting optimization programs, offering different tradeoffs between accuracy and scalability. In particular, we propose a simple yet fast mutation function for our evolution strategy that turns out to be competitive with interior point methods.

Finally, we validate our approach using real traces from a commercial in-memory database appliance, SAP HANA [4].

The remainder of this paper is organized as follows. Section II motivates our research objective and gives the problem statement. Section III introduces the system characteristics of our in-memory database. A novel response time approximation is developed in Section IV, combined with a non-linear optimization program in Section V and evaluated in Section VI by numerical tests. Finally, Section VII outlines related work, while Section VIII concludes this paper and gives future work.



II. MOTIVATION AND PROBLEM STATEMENT  In-memory databases are a completely new type of big data analysis systems capable of processing heavily memory intensive workloads in a parallel fashion. Their resource man- agement is a complex and difficult task that includes memory- aware sizing of these systems across heterogeneous clusters.

1 2 3 4 5 6 7 8       Workload Scenario  M e a n  R e la  ti v e  R e s p  o n  s e T  im e  E rr  o r  in %      AMVA  FJ-AMVA  Fig. 1. Relative Response Time Error compared with Simulation  support these sizing decisions by enabling what-if analyses for various hardware configurations. It is therefore essential to develop models that are able to capture the behavior of in-memory databases across several dimensions. In particular, performance measures such as response times and throughputs are key metrics of in-memory systems that need to be modeled accurately. However, the extensive and variable threading-level we are faced with cannot be correctly captured by existing analytical approaches, such as AMVA [3], widely used to model the performance of multi-tier applications [5], and state- of-the-art techniques, i.e. fork-join AMVA (FJ-AMVA) [6].

To demonstrate this, we parameterized these two methods from real traces of our in-memory database SAP HANA and compared their response time predictions with a validated in- memory database simulator [7]. We give an excerpt of our results in Figure 1, which depicts the relative response time error of AMVA and FJ-AMVA compared with our simulator under different workloads. We observe that using both AMVA and FJ-AMVA can result in prediction errors of more than 50%. We will therefore develop a new performance model that captures in-memory database characteristics more accurately.

Our second challenge is coined by a capacity planning problem, assigning resources to in-memory databases subject to memory and utilization constraints. Optimizing memory occupation for such systems can be computationally expensive and can introduce local optima due to non-convexity. Hence we address this by proposing intelligent optimization strategies and combine these with our new performance model.

Summarizing, our main contributions are: ? A novel analytic response time approximation for in-  memory databases that considers thread-level fork join ? An optimization-based formulation for seeking load-  dispatching routing probabilities to minimize memory swapping for such systems subject to resource constraints  ? An experimental validation that reveals the applicability of local and global search strategies  ? Parameterization and evaluation of our models with real traces of an in-memory database system  To the best of our knowledge there are no methods that com- bine thread-level fork join models with non-linear optimization of in-memory systems, and thus represents a research novelty.



III. IN-MEMORY DATABASE CHARACTERISTICS  A. OLAP Workload Characteristics In-memory databases are optimized to execute analytical  business transactions, i.e. OLAP. These types of transactions represent read-only workloads and can thus be entirely pro- cessed in main memory. Due to their analytical nature OLAP  0 5 10 15 20  0.2  0.4  0.6  0.8   1.2  N o  rm a li z e d  T h  re a d  L e v e l P  a ra  ll e li s m     Query Class     Queries 1...22  (a) Thread Level Parallelism  0 5 10 15 20         N o  rm a li z e d  E x e c u  ti o  n T  im e  Query Class     Queries 1...22  (b) Execution Times  0 5 10 15 20      W a  rm -u  p M  e m  o ry  O c  c u  p a  ti o  n i  n G  B     Queries 1...22  0 5 10 15 20      N o  rm a  li z e  d P  e a  k M  e m  o ry  O c  c u  p a  ti o  n  Query Class  (c) Memory Occupation  1 2 3 4 5 6 7 8  0.5   N o  r m  a li z e d  T h  r e a d  E x e c u  ti o  n T  im e s  t r                         s t   1 2 3 4 5 6 7 8  0.5       s t   1 2 3 4 5 6 7 8  0.5   Thread ID      s t   1 2 3 4 5 6 7 8  0.5       s t   (d) Thread Times for t ? 8 Fig. 2. Workload Characteristics normalized by Query Class 1  workloads are not only computationally intensive but also show high variability in their threading levels. To emphasize these diverse characteristics, we analyzed trace logs obtained from benchmarking experiments running SAP HANA on an IBM X5 4-socket database server configured with 1TB main memory [8]. The benchmark was run with a scale factor of 100x and comprised a set of 22 OLAP queries introduced by SAP-H, an extension to the TPC-H benchmark with emphasis on analytical processing. We provide the results of our trace log analysis for all 22 query classes in Figure 2(a)-(c). All values are obtained from isolated query runs, normalized by class 1 for confidentiality and shown with their respective standard deviations. In Figure 2(a) we present the average number of CPU cores used by each query class and denote this with thread level parallelism. As expected, we see a strong variability of the parallelism across all query classes, which can increase contention for resources under OLAP workload mixes. This attains further distinction due to the varying computational expense of OLAP queries, depicted in Figure 2(b). In addition, we reveal the memory intensive character of OLAP workloads in Figure 2(c) by showing the physical mem- ory temporarily occupied during the processing of queries, which varies on gigabyte scale. To emphasize the importance of compression during the execution of OLAP workloads, we demonstrate in Figure 2(c) that our corresponding benchmark dataset with a size of 1.3 TB was reduced to approximately 65GB after a warm-up run for each query class.

B. Request Handling and Demand Characterization Query planning and execution are important stages during  the processing of OLAP workloads. The first stage involves a query planner analyzing the query structure and creating an appropriate execution plan. In the second stage, queries are executed depending on their assigned execution plan, which    c1=4 3 0 2 2 c6=3  Number of active Cores  C o  re ID  Processing Phases b1 b6  Case 1bCase 1a b2 b3 b4 b5  Job Execution Time dr  C o  re ID  Core Activity  Change of Thread Affinity  Thread Execution Time sr  Th re  ad t  Thread TimeCase 2a  Thread Execution Time sr  Th re  ad t  Thread Time - orderedCase 2b  0.2s               sr          t t  discarded  Fig. 3. Service Demand Estimation for an OLAP Query  defines the number of threads to be requested from an internal thread pool in order to service a query. All threads pertaining to a query process are then assigned to processing cores for execution and are synchronized before a query can leave the system. Kraft et al. [7] captured this behavior from our traces to parameterize their in-memory database simulator.

We therefore review this process briefly and subsequently extend it for use with our analytical model. Since our traces contain information from isolated runs for all 22 available query classes, also denoted as job classes, we can estimate two important model parameters on a per-class basis. In particular, we are considering the service demand dr and the parallelism lr. While dr accounts for the average time required by our in-memory system to service one job of class r, lr describes the number of CPU cores used on average by query class r.

In Section IV we want to compare our performance model with FJ-AMVA and the simulator developed in [7]. However, all three approaches require a different representation of dr.

Hence, we will show in the following how to extract dr appropriately. To better illustrate this process, we represent our traces in Figure 3 by an exemplary job that consists of 7 threads and is executed on a 4-core system. Figure 3 Case 1a shows the core activity, which was sampled during the execution of our job. We see that over time, all 4 cores were differently utilized, i.e. attributable to stalling threads or changes in thread affinity. Based on the sampled core activity, we divide the execution process of a query into P processing phases, as illustrated in Case 1b. Each processing phase is defined by its duration bp and its number of active processing cores cp, i.e. 4 active cores in processing phase 1 and no active cores in processing phase 3. As mentioned above, the extraction of processing phases and active cores was done in [7], since their simulator requires the parameters bp and cp.

However, we extend this process for use with our analytical model and determine dr and lr as aggregates of these mea- surements, dr =  ?P p=1 bpr and lr = dr  ?1? p bprcpr.

In addition to the core activity, our traces record the number of threads Jr pertaining to a class r job execution process as well as the execution times of each individual thread, excluding the duration in which a thread was not active. This information was not considered by [7], and thus prompted us to extract it from the raw traces. We illustrate this in Figure 3 Case 2a, which lists all 7 threads that belong to our exemplary job. We denote the execution time of each thread t pertaining to a job of class r with str and since FJ-AMVA specifically requires this representation, we will use str for its  parameterization in our experiments. Additionally, FJ-AMVA assumes that Jr ? I . However, for some classes and also for our example, with J = 7 and I = 4, this is not the case.

Hence, we sort str and use only the first t ? I longest running threads, shown in Case 2b. We justify this, as for the majority of classes in our traces, where Jr > I , str ? 0.2s for t > I .

This means that these threads were not sampled accurately, since [8] used a sampling interval of 0.2 seconds.



IV. FORK-JOIN MODEL In this section, we study queueing performance models  for in-memory databases based on multi-core processors. In addition, we propose an efficient analytical approximation to these and present all relevant notation in Table I.

A. Modeling an in-memory Database Performance models for in-memory databases need to be  aware of the complexity introduced by OLAP workloads and require a contention model that accurately captures hardware and application characteristics. Emphasized by the high level of query parallelism shown in Figure 2(a), fork-join queues prove to be an appropriate choice for modeling an in-memory database. We therefore apply fork-join queues to model the processing cores of such a system. In particular, we consider processor sharing (PS) queues in the sense of Baskett et al.

[9], i.e., where service times are i.i.d. generally distributed.

We employ multiclass closed queueing networks (QN) with a think time model that represents an abstraction of client think time and inter-activation times of worker threads in the database, which are dependent on the admission buffer and thread pool size. In Figure 4 we present our queueing model. It captures the behavior of jobs split into several tasks on arrival at the system, which are then assigned to processing cores in a probabilistic manner. This includes the synchronization aspect of parallel siblings at the join point and the return to the think time buffer once a job is completed.

Approaches to solve this type of QNs via simulation, e.g. in [7], emphasize the difficulty in finding analytical solutions. We will therefore discuss available approximations to QNs, before we introduce our novel analytical response time correction to fork-join queues.

B. Approximations to Fork-Join Queues The widely used exact analytical solution for closed QNs,  known as mean-value analysis (MVA), determines the re- sponse time Wir for a job of class r at queueing center i depending on the total number of per-class jobs ~N in a system    Database ServerClient think time  Core 1p1r  p2r  p3r  Core 2  Core 64  Fork Join  Fig. 4. Multiclass Fork-Join Queueing Model of a Database Server  as follows [10]: Wir = dir  ( 1 +Air( ~N)  ) . (1)  Here, the definition of Wir includes the queueing time and the service demand dir = virsir, the product of per-class service time sir and visits vir. The arrival instant queue Air( ~N) counts for the total number of jobs queuing or being serviced at i at the arrival instant of a job of class r. Based on the arrival theorem for closed QNs, Air( ~N) can be expressed as Qir( ~N?1r), which designates the queue length with one class r job less. MVA is applied in recursive fashion, but despite being analytical it gets intractable for problems with more than a few customer classes. This is addressed by Bard-Schweitzer [3], proposing an approximate MVA (AMVA) that employs a fixed-point iteration and estimates Air via linear interpolation:  Air( ~N) ? (Nr ? 1) Nr  Qir( ~N) +  R? s=1,s6=r  Qis( ~N). (2)  Synchronization in fork-join queues introduces temporal delays that cannot be described with the above product-form models. As MVA and AMVA are not applicable in that case, more recent approaches tried to address this aspect [6], [11].

Alomari et al. [6] propose a response time approximation called FJ-AMVA that sorts per-class residence times in de- scending order and scales them by an appropriate coefficient for better estimation of the synchronization overhead. This approach assumes sir to be the mean of the exponentially distributed service times S?ir. It can be shown that if sir are the same at every queue for a particular class r, maxi(sir)? HJr equals s? = E[maxi(S?ir)], where s? becomes the maximum service time of a job and HJr =  ?Jr j=1 j  ?1 denotes the jth harmonic number for job class r. In the heterogeneous case, sir can vary across the queues for each job class, which results in less synchronization time. FJ-AMVA approximates this by multiplying Wir with 1/i instead of HJr . However, the fork- join approximations in [6], [11] are less suitable for our model, as both assume exponential distributions of sir. By contrast, our service times sir and str show a generally low variability.

We point this out in Figure 2(b) and 2(d), by listing the per- class execution times and their standard deviations as well as the first 8 longest running threads for a subset of our query classes. This justifies the need for a response time correction that does not rely on exponential service times.

C. Response Time Correction Thread-level fork-join cannot be expressed with (1). We  therefore propose an analytical response time correction called TP-AMVA, which considers the placement of tasks in fork- join queues. In particular, we approximate the fork-join con- struct with only one single queue, which decreases processing  TABLE I MAIN NOTATION  Symbol Description Workload Parameters  R Number of query classes bp, cp Length of processing phase p and number of active cores during p dir, sir Service demand and service time of class r at queue i lr Number of cores used on average by class r (average degree of parallelism) str Service time of thread t of class r Jr Number of threads per class r ~N Population vector with number of jobs per customer class: N1, ..., NR ~Z Vector of per-class think times Z1, ..., ZR  Additional Parameters Ii Number of available processing cores at server i pir Probability of class r jobs being routed to queue i  Performance Measures Xir Per-class throughput at queue i Wir Per-class residence time at queue i Air Queue length at arrival instant of class r at queue i Qir Per-class queue length at queue i Uir Per-class utilization of queue i Mir Per-class memory utilization at server i  time and simplifies its integration into our optimization pro- gram. This abstraction does not consider the state of individual queues, but rather the average state of the system, which follows the MVA paradigm. Since we assume queues to be all with the same processing rates and equal class routing probabilities, their mean queue-length will be the same. Thus, if we want to enforce SLAs, it is sufficient to consider the expression of just a single arbitrary queue. Moreover, since we consider jobs to not cycle within the fork-join construct, dr = vrsr = sr. Our correction has the following form:  Wr = dr  ( 1 +  R? s=1  Qs ?rs ls I  ) , (3)  where the response time Wr is calculated as the service demand dr inflated by a factor that represents the service rate degradation under processor sharing due to jobs, which already compete for resources at the same queue. The arrival queue As is estimated by employing Bard-Schweitzer:  ?rs =  ??? Nr ? 1 Nr  , s = r  1, s 6= r, ?s, r.

(4)  Since we record thread-level information for each query class, we are able to better approximate the fork-join feature. For this we correct As by the factor ls/I to estimate the per-core queue length in a system with I cores. The performance measures Wr, throughput Xr and Qr can then be resolved by employing the AMVA fixed-point iteration. In addition we approximate the utilization in a fork-join system with:  U =  R? r  Ur lr I . (5)  Before we evaluate this model, we present an alternative approximation to (3), which is an empirical calibration. It follows the idea that an arriving class r job affects Wr depending on its probability pr being routed to a particular queue in the fork-join construct. Hence, we correct the class r queue length Qr by multiplying with pr:  Wr = dr  ( 1 +  R? s=1  Qs ?rs ls I prs  ) , (6)    TABLE II RELATIVE ERROR (%) COMPARED WITH SIMULATION FOR SCENARIO Si  Method S1 S2 S3 S4 S5 S6 S7 S8 AMVA with (lr/I)? sr 91.2 59.3 59.9 46.6 54.5 26.1 58.6 97.1 AMVA with sr 3166.6 554.3 127.3 97.5 367.3 471.9 277.4 649.2 FJ-AMVA 18.4 43.6 27.0 21.5 26.8 67.5 14.2 41.8 probabilistic TP-AMVA 1.8 3.9 11.0 7.9 18.1 5.9 10.6 5.1 static TP-AMVA 4.3 11.2 8.2 11.1 20.4 18.2 12.4 3.9  where prs is defined as:  prs =  ??? lr I , s = r  1, s 6= r, ?s, r.

(7)  We will show experimentally that our two approximations produce reasonable results under different workload mixes and are highly competitive compared with FJ-AMVA. During our evaluation we denote the implementation of (3) with ?static TP-AMVA? and (6) with ?probabilistic TP-AMVA?.

D. Evaluation In this section we will evaluate our correction against the  in-memory database simulator in [7] under different scenarios and include the FJ-AMVA into our comparison.

a) Experimental Setup: We implemented FJ-AMVA, and TP-AMVA in MATLAB and conducted several experiments for different workload scenarios based on the categories: light, medium and heavy. Whereas light mixes contain mostly query classes with small degrees of parallelism and shorter execution times, heavy mixes comprise query classes with high parallelism and longer execution times. To further vary the workload, we increased the number of concurrent users from 1 to 32. Throughout all scenarios we used a fixed think time extracted from the single user scenario in our trace logs.

We used the following parameterization for the simulator, TP-AMVA and FJ-AMVA. To increase its capability of captur- ing resource contention more accurately, we parameterized the simulator with the fine grained query characteristics defined by bpr and cpr, introduced in Section III-B. For TP-AMVA we used the aggregated service demand dr. In contrast, FJ- AMVA needs to be parameterized with the service times of jobs at each queue sir. We therefore mapped str, which naturally represents the service times needed by FJ-AMVA, onto sir. As a problem of our traces, there was no information about the placement of threads available. Hence we addressed this by applying a Monte Carlo Simulation choosing random permutations of str = {s1r, ..., str} with 1 ? t ? Jr and assigning them to queue t, 1 ? t ? Jr, before running FJ-AMVA. We then took the average response time of 100 iterations, which seemed reasonable to produce stable results.

Moreover, the task scheduling system in the simulator required equal routing probabilities to each core, as does our implementation of TP-AMVA. FJ-AMVA in contrast defines its routing probabilities pr as probability that a single queue in the fork-join construct is visited by job class r. For this case we assumed Jr/I to be a suitable approximation of pr and thus we used pr = Jr/I to parameterize FJ-AMVA.

b) Results: We show the results of our experiments in Figure 5 accompanied by Table II, which depicts the mean relative response time error compared with simulation. To give  0 10 20 30      Query Mix: Light  Number of Concurrent Users  R e s  p o  n s  e T  im e  i n  S e  c o  n d  s     FJ-AMVA probabilistic TP-AMVA static TP-AMVA Sim  (a) Scenario 1  0 10 20 30     Query Mix: Medium  Number of Concurrent Users  R e s  p o  n s  e T  im e  i n  S e  c o  n d  s     FJ-AMVA probabilistic TP-AMVA static TP-AMVA Sim  (b) Scenario 2  0 10 20 30        Query Mix: Medium  Number of Concurrent Users  R e  s p  o n  s e  T im  e i  n S  e c  o n  d s     FJ-AMVA probabilistic TP-AMVA static TP-AMVA Sim  (c) Scenario 3  0 10 20 30         Query Mix: Heavy  Number of Concurrent Users  R e  s p  o n  s e  T im  e i  n S  e c  o n  d s      FJ-AMVA  probabilistic TP-AMVA  static TP-AMVA  Sim  (d) Scenario 4 Fig. 5. Response Time Results under OLAP-based Workloads  an impression of how the standard AMVA implementation (2) performs, we list its mean relative error for a run with dr = sr and a run that takes the visit ratio into account by dr = lr/I ? sr. As expected, AMVA clearly shows a poor overall performance. In contrast, both static and probabilistic TP-AMVA perform reasonably well throughout all scenarios and follow the trend of the simulator. Both approximations tend to be more pessimistic once the number of users in- creases, whereas the response time prediction under light load appears to be slightly optimistic, i.e. scenario 3. In general, our probabilistic TP-AMVA captures contention effects better than its static version, staying below a 20% error rate.

Surprisingly, FJ-AMVA lacks in its accuracy across most scenarios. Its response time prediction is too optimistic under medium mixes, i.e. scenario 3, and too pessimistic under light- medium load (scenarios 1 and 2). In most of our scenarios, we observed a very pessimistic start for FJ-AMVA, when only few concurrent users are active. This can be explained, when looking at the parallelism of our query classes. Some of our queries, such as class 1, are highly parallel with str ? dr,?t, and thus contain almost no synchronization time. This is why the summation over Wir in FJ-AMVA, despite its scaling factor 1/i, results in a response time that is too high. However, this effect seems to diminish when the load grows and better reflects the increasing congestion for those cases. From the results, we conclude that FJ-AMVA in its proposed form is not suitable for modeling OLAP-based query workloads, whereas our correction turns out to be reasonably accurate and due to its simplistic model a good choice for the optimization program we present in the next section.



V. OPTIMIZATION PROBLEM Given a parallel system with memory and resource con-  straints, such as an SAP HANA in-memory database clus- ter, we are interested in how to dispatch workloads while    In-memory Database Cluster  Load Dispatcher  p1r  p3rp2r p4r  p5r  Fig. 6. Model of an in-memory Cluster Subject to Load Optimization  minimizing memory swapping. We further include utilization constraints to avoid under-provisioning in such environments by limiting the CPU utilization per server. In order to solve this constrained problem, we develop a non-linear optimization program based on our approximation probabilistic TP-AMVA, taking above mentioned constraints into account.

A. Cluster Model We consider a simplified model of an in-memory database  cluster with K servers, see Figure 6. This model contains a separate fork-join closed QN for each server, which simplifies the evaluation. However, the servers in our cluster are not completely independent, but share the same workload ~N . We therefore define Nir = Nr?pir, 1 ? i ? K as the percentage of workload that goes to server i, with pir designating the probability of routing a class r request to server i.

B. Non-linear Optimization Problem Our optimization program aims to reduce memory swap-  ping by minimizing the overall memory occupation M of an in-memory database cluster. In particular, we are seeking routing probabilities pir that allow for near optimal workload placement. This results in the optimization problem given by Equation (8), with its decision variables pir, throughput Xir and response time Wir. Note that two of the three performance measures Qir, Xir and Wir must be chosen as decision variables given that we have two degrees of freedom in the relations of a closed queueing network. To reduce the model complexity, we estimate M for each server by multiplying the per-class queue length with the per-class physical peak mem- ory consumption mr, (8b). We hereby make the pessimistic assumption that memory occupation grows as a function of the queue length Q and neglect that query classes could share data residing in main memory. Additionally, we assume that forking of new threads and joining is not related to the change of memory consumption.

We integrated our probabilistic TP-AMVA in (8c),(8e) and (8f), based on the steps of an AMVA fixed-point iteration.

We further defined ?irs = (Nir ? 1)/Nir ? (lir/Ii) for s = r and ?irs = 1 in case of s 6= r. In addition, we added memory and utilization constraints in form of Mmaxi and Umaxi and ensure correct routing probabilities with (8i).

From a performance point of view, our method uses less variables compared with FJ-AMVA, which would introduce M2 additional binary variables to sort the response times.

This gets further attention, when looking at the nature of our optimization problem, which is non-convex. Hence, we expect the number of local optima to grow when increasing the number of classes and servers as well as introducing different constraints for each server. This exacerbates the problem of  finding a globally optimal solution and requires strategies such as multi-start optimization.

Mmin = min pir,Xir,Wir  K? i=1  Mi (8a)  s.t.: Mi = ? r  Qirmir, ?i (8b)  Ui = ? r  Xir lir Ii dir, ?i (8c)  Nir = pir Nr, ?i, r (8d) Qir = XirWir, ?i, r (8e)?  r  Qir = ? r  Uir  ( 1 +  R? s=1  Qis ?irs lis I  ) , ?i (8f)  Qir = Nir ?Xir Zir, ?r (8g) Wir ? dir, ?i, r (8h)?  i  pir = 1, ?r (8i)  pir, Xir,Wir ? 0 ?i, r (8j) Mi ?Mmaxi , ?i (8k) Ui ? Umaxi , ?i (8l)

VI. NUMERICAL EVALUATION  Having developed the optimization-based formulation, our goal now is to solve this problem numerically to get insights into how workload placement effects the memory occupation in a cluster of in-memory databases under given resource con- straints. We are further interested in how the performance and accuracy of multi-start based approaches for our optimization problem compare with each other.

A. Evaluation Scenarios We varied the number of server instances and classes in  K,R = 2, 4, 8, 16 and the workload N in 8K (light load) and 32K (heavy load). Our per-class populations Nr are obtained by equally dividing N across all classes, allowing fractional Nr. To investigate how R affects the total memory occupation M , we clustered our set of 22 classes with k- means (a priori normalized with z-score) across the three dimensions parallelism lr, service demand dr and memory occupation mr, depicted for R = 2, 4, 8 in Figure 7. Finally, we set different constraints to affect the workload placement: Mmaxi = 512GB, U  max i = 0.95 for i ? K/2 and Mmaxi =  128GB, Umaxi = 0.99 for i > K/2.

B. Solution Methods We compare the minimization of memory swapping for two  different methods (local and global): ? fm: fmincon configured with the interior point algorithm ? es: (?+ ?) - evolution strategy [12]  We chose fm, as we are dealing with a constrained non-linear optimization problem and favored es due to its global search capabilities and due to the large state space we are faced with when considering more than 8 servers and classes. Both methods fm and es, call our external solver to approximate the response time Wir and throughput Xir. As our methods rely on AMVA, which supports only cases with Nir ? 1, we    0.5  1 0  0.5   0.5   m r  R=2  d r  l r  0.5  1 0  0.5   0.5   m r  R=4  d r  0.5  1 0  0.5   0.5   m r  R=8  d r  Fig. 7. Normalized k-means Clusters for different Numbers of Query Classes  applied the approximation proposed in [13], setting Qir := 0 when Nir < 1. With method es we implemented an evo- lutionary algorithm called evolution strategy. In contrast to genetic algorithms, which represent solution candidates by a string of bits, evolution strategies employ a real valued encoding of solution candidates and therefore require self- adaptive mutation functions. In particular, we implemented a (? + ?) evolution strategy (es), configured with a uniform parent selection and a ?best selection? as environmental selec- tion. Individuals (i.e. solution candidates) are represented by the routing probability matrix pir. On a population of ? = 20 parent individuals we perform a uniform selection 100 times to produce ? = 100 mutants in each generation. In particular, we had to develop our own adaptive mutation to alter individuals obeying the constraints (8i) and (8j). To create a mutant we randomly select a server x and a class y and subsequently modify the routing probability pxy by adding a new value ?p. We choose ?p uniformly from the interval [pxy ? pxy/ ((g mod 20) + 1) , pxy + (1? pxy) / ((g mod 20) + 1)].

The amount of change introduced by this mutation in form of ?p is adapted throughout the optimization depending on the current number of generations g. Moreover, we determine the fitness of each individual by its memory occupation M .

To ensure compliance with the given constraints, our imple- mentation penalizes violations of (8k),(8l) by impairing the fitness. Finally, from the 120 individuals after mutation, the 20 fittest are selected into the next generation. We have also tested a different implementation of our optimization based formulation in (8), using YALMIP [14]. However, we have found that the results using this approach were qualitatively very similar. Due to limited space we do not report the full details of these additional experiments, which may be found in our technical report [15].

C. Evaluation Methodology We implemented our approaches in MATLAB employing  fmincon and its interior-point algorithm for fm, while es relies on an evolution strategy. Moreover, method es does not depend on MATLAB proprietary toolbox functions, and therefore could be implemented in another programming language to further decrease processing time.

Our scenarios were evaluated on an Intel Core i5 CPU with 2.60GHz and two physical cores. To cope with different local optima, we randomized P = 10 initial points for every tuple (K,R,N/K) and ran fmincon using MATLAB?s MultiStart solver. Subsequently, we report the average of the cumulative execution time for all P local solver runs at a timeout of 1800 seconds to understand the performance at short time scales.

TABLE III MEMORY OCCUPATION AND EXECUTION TIMES. TIMEOUT: 1800S? .

Instances Memory Occupation in GB Time in s K R N/K fm es es-fS fm es es-fS 2 2 8 78.9 78.9 79.4 2.3 1800.0 0.1 2 4 8 43.4 43.4 45.4 3.9 1800.0 0.1 2 8 8 23.5 23.5 24.3 20.5 1800.0 0.3 4 2 8 157.8 157.9 158.1 3.5 1800.0 0.1 4 4 8 85.7 85.7 87.5 30.0 1800.0 0.2 4 8 8 46.1 46.2 47.7 320.9 1800.0 0.4 8 2 8 315.7 315.7 315.8 21.6 1800.0 0.1 8 4 8 170.6 170.9 176.8 240.1 1800.0 0.2 8 8 8 91.6 91.6 95.3 1800.0 1800.0 0.4 2 2 32 316.4 316.4 322.5 1.5 1800.0 0.1 2 4 32 237.6 239.1 270.3 6.2 1800.0 0.3 2 8 32 121.8 123.4 146.3 31.4 1800.0 0.7 4 2 32 632.4 632.7 640.8 4.1 1800.0 0.1 4 4 32 379.6 379.8 443.0 82.1 1800.0 4.2 4 8 32 215.9 216.2 237.5 389.7 1800.0 0.7 8 2 32 1264.7 1265.2 1265.8 34.0 1800.0 2.1 8 4 32 752.5 755.6 765.9 965.5 1800.0 14.5 8 8 32 414.0 416.0 542.0 1800.0 1800.0 1.7  16 2 32 2529.3 2529.4 2531.8 576.9 1800.0 3.2 16 4 32 1504.8 1486.7 1552.0 1800.0 1800.0 51.2 16 8 32 N/A? 818.6 847.8 timeout 1800.0 25.0 16 16 32 N/A? 385.4 442.2 timeout 1800.0 3.1 ?fm,es were stopped after 1800s or in case of fm when P = 10 solutions found ?No single solution found by fm  D. Results  We show the results of our optimization program for the two methods fm and es in Table III. We further report the cumulative execution time for fm and the time until the first non-violating solution, es-fS, was found by es.

At first, we were interested in how clustering of classes affects memory optimization. In our case we observe an inversely proportional behavior of the memory occupation M once the number of classes R increases, i.e. instances (2,2,8) and (2,8,8). This can be explained by the way we calculate Nr from the fixed ratio N/K, but comparing the instances (2,2,32) and (2,8,32), where Nr = 8 for both cases, we see that a large difference in M remains. We also discover non-monotonicity in the overall cluster utilization U with increasing R. We show this in Table IV, for a light and heavy load scenario.

We impose this on classes with high parallelism and long execution times that are less often merged into a cluster with short running and sequential queries when R increases, as depicted in Figure 7. From this we conclude that the more classes are aggregated, the more inaccurate gets the estimate for M and U , given that the ratio N/K remains constant.

Another question we wanted to address is how our opti- mization program handles workload placement under the given constraints Mmaxi and U  max i defined in section VI-A. We  therefore investigated the two instances (8,2,8) and (8,2,32) in more detail, which represent light and medium load scenarios.

Table V shows the routing probabilities pir and per-server memory occupation Mi for the two solutions found by fm.

Under light load we see that fm tries to achieve as little interference as possible between the two classes, resulting in maxi(Mi) = 52.4GB. Moreover, as no constraints are violated, the placement can be an arbitrary permutation across i, but needs to remain fixed for r. Once the workload grows to Nir = 128, which is a normal scenario for SAP HANA dealing with more than 128 parallel connections, the memory    TABLE IV AVERAGE SERVER UTILIZATION FOR 8-SERVER SCENARIO (K=8)  N/K=8 N/K=32 R=2 R=4 R=8 R=2 R=4 R=8  0.05 0.24 0.22 0.18 0.76 0.71  constraints for server 5-8 are violated. At this point we observe a workload shift towards servers 1-4, occupying 250.9GB on each. This suggests that our approach is able to optimize constrained workload placement reasonably well.

Our methods fm and es produce similar results regarding M for instances where all P solver runs completed successfully, i.e. (8,2,8). We explain this due to the same algorithm that is used to solve the queueing models. For cases, such as (16,4,32), fm produced slightly worse results than es, as in the fixed amount of time only one starting point could be processed. In general, given a sufficiently large number of initial conditions, fm seems to be more accurate. We impose this on the capability of interior point to better converge at small step lengths during the optimization process. Looking at the execution times, we see that fm is able to find solutions in less than 1800 seconds for 8 servers and 8 classes, if not necessarily for all P initial conditions. Our evolution strategy seems more efficient, since it reports the first solution after 25 seconds in scenarios, such as (16,8,32), where fm was timed out before a single successful solution was found. We explain this due to the implementation of es, which involves less evaluations of the objective function per iteration.

Summarizing the results, we showed based on empirical evidence that class aggregation can lead to inaccurate memory occupation and utilization estimates; that our optimization program can load balance in-memory systems respecting given resource constraints; and finally that interior-point based meth- ods using multi-start approaches perform more accurately than global optimization strategies, but become intractable under large scenarios of more than 16 servers and 4 classes.



VII. RELATED WORK Research into in-memory database performance started in  2002 when [16] introduced fundamental cost models including the entire memory hierarchy in a database system. Nowadays, on-demand provisioning of these systems drives research fur- ther into database optimization employing QNs [17].

In [18] classification-based machine learning is used to schedule tenants in multi-tenant databases. The authors char- acterize tenant and node-level behavior based on performance metrics collected from database and OS layer and validate their framework in a PostgreSQL environment. However, in this work scheduling constraints are only approximated. Workload characterization and response time prediction via non-linear regression techniques for in-memory databases are proposed in [19]. The authors derive tenant placement decisions by employing first fit decreasing scheduling, but evaluate on small scale only. [20] propose a new framework for managing performance SLOs under multi-tenancy scenarios. Their work combines mathematical optimization and boolean functions to enable what-if analyses regarding SLOs, but relies on brute force solvers and ignores OLAP workloads. In [21]  TABLE V WORKLOAD PLACEMENT UNDER SCENARIOS (8,2,8) AND (8,2,32)  N/K i=1 i=2 i=3 i=4 i=5 i=6 i=7 i=8 8 pi1 0 0.167 0.167 0 0.167 0.167 0.167 0.167 8 pi2 0.500 0 0 0.500 0 0 0 0 8 Mi 0.6GB 52.4GB 52.4GB 0.6GB 52.4GB 52.4GB 52.4GB 52.4GB  32 pi1 0.200 0.200 0.200 0.200 0.100 0 0.100 0 32 pi2 0 0 0 0 0 0.5 0 0.5 32 Mi 250.9GB 250.9GB 250.9GB 250.9GB 128.0GB 2.5GB 128.0GB 2.5GB  query demands are quantified by a fine-grained CPU sharing model including largest deficit first policies and a deficit-based version of round robin scheduling. The methodology applies to database-as-a-service platforms and is validated on a prototype of Microsoft SQL Azure. This work neglects characteristics for memory occupation. [22], [23] introduce frameworks for non-linear cost optimization regarding SLA violations and resource usage, applied to web service based applications and cloud databases. Their work either relies purely on constraint definitions or does not consider closed QNs. [24] proposes a framework for multi-objective optimization of power and per- formance. The methodology applies to software-as-a-service applications and it is validated using a commercial software, SAP ERP. The approach is based on simulation and does not consider thread-level fork-join.

[25]?[27] use multi-variate regression and analytical mod- els of closed QNs to predict query performance based on logical I/O interference in multi-tenant databases. However, these methods require detailed query access patterns and are evaluated for small numbers of jobs and batch workloads only.

Ignored by the latter, thread-level fork join is addressed by [7] and [6], but despite using similar techniques, their approaches are either computationally expensive or rely on exponential service time distributions.



VIII. CONCLUSIONS AND FUTURE WORK  In this paper we have made several contributions, which include a novel analytic response time approximation that models thread-level fork join and per-class memory occupa- tion in in-memory systems. In addition, we have developed an optimization based formulation that facilitates our ana- lytic approximation and efficiently seeks for load dispatching routing probabilities to minimize memory swapping in in- memory database clusters. Furthermore, we have shown that our models exceed the accuracy of existing approaches using real traces from a commercial in-memory database appliance, SAP HANA, for validation.

Possible directions for future work include further validation of our models with different hardware configurations as well as an evaluation of the applicability of our models with respect to other in-memory systems, e.g. Shark/Spark [28], [29]. In addition, our provisioning framework could be implemented in a real in-memory database system and extended by new features to include shared memory access and multi-tenancy.

