2014 IEEE Workshop on Electronics, Computer and Applications

Abstract-With the development of information society, data is in exponential growth. How to mining useful information from huge amounts of data become a big problem companies facing. Data mining algorithm deal with data and mine useful information, which is advantageous to the company development, but current mining algorithm need take a long time to process huge amounts of data or even unable to process huge amounts of data. Combine traditional algorithm migration with the cloud platform for parallel improvement can effectively solve the problem. In this paper, we study the Hadoop platform technology and common data mining algorithm.

Keywords-Cloud Computing; Map-reduce; Data mining; Hadoop  LCLOUD COMPUTING  A.Definition of cloud computing  Cloud computing is a computing platform based on Internet, it provide the hardware and software resources on-demand to demanders through this platform. Typical cloud computing providers typically offer a general application, the user access application through a browser or other web, and software and data application need are stored in the cloud. Cloud computing has the branch of narrow and broad. Narrow cloud computing refers to the delivery of IT infrastructure and usage patterns, Taking the infrastructure as a kind of service provided through the network, the user can use this service according to their need and pay according to usage. While generalized cloud computing refers to the service delivery and use model. [1] Compared with the narrow sense of cloud computing, cloud computing' services more broadly defined, not just the infrastructure, but all the services that can provided through the network,  B. Characteristics of cloud computing  At present, cloud computing has the following features:  (1) From the cloud scale, since the beginning of birth the cloud computing have very large scale server. The international famous IT companies, such as Microsoft and   Google have millions of servers, large-scale server makes the "cloud" has the amazing ability to calculate.

(2) From the user's use, cloud computing block the concrete implementation in the bottom and work for users in the form of services, The user can get service in different forms in different position without needing to understand the specific location,  (3) From the data reliability, "cloud" improve service reliability by data redundancy backup, isomorphism compute nodes swaps and other measures makes the data stored in it as reliable as local storage, or even higher than local storage reliability.

(4) From the dynamic extensions, cloud computing capacity support dynamic extensions, users can expand capacity as needed,  (5) From the cost performance, cloud computing platform consist cheap nodes and use the fault-tolerant mechanism to provide high performance services. User does not need pay much to build the data center while can still achieve the same performance.

CHadoop  Hadoop is a distributed foundation infrastructure developed by the Apache. [2]Users can easily develop and operate applications of mass data on the Hadoop, Its core is HDFS, graphs and HBase. HDFS is open source implementation of Google file system GFS, HBase is open source implementation of Google BigTable.

D.HDFS  HDFS (Hadoop Distributed File System) is a distributed file system run on the cheap hardware; the whole system is composed of hundreds of thousands of servers. The server failure is frequent, HDFS ensure high reliability of the system by automatically detect fault and repair it. On the data access, the application must access data set on HDFS in streaming way, HDFS is suitable for processing large data sets; the emphasis is on data throughput. When designing HDFS on POSIX support is eased in order to obtain more data throughput.

2014 IEEE Workshop on Electronics, Computer and Applications  To get the data To get metadata  File system management  Figurel. HDFS master-slave structure  E. MapReduce  MapReduce a kind of parallel programming framework put forward by Google. It hides the complex part of parallel programming and the task is divided into Map and Reduce phase. In the Map phase, read data from the file blocks according to the specified InputFormat class and generate key-value pairs, and then call the self-defined Map function for processing and generate the intermediate results in local. In the Reduce phase, remotely read the intermediate results generated in Map phase, call the self-defined Map function for processing and store the eventual results in HDFS. Users only need to realize the Map and Reduce functions according to the concrete situation, which greatly reduces difficulty of parallel programming for users.



II.RESEARCH OF LOGISTIC REGRESSION BASED ON  GRAPHS  Logistic regression is the most common machine learning methods, which are frequently used to estimate the possibility of something, the possibility of a certain users to buy goods, for example, the possibility of a patient suffering from a disease, as well as the possibility for users to click on an advertisement. But along with the sharp increase of user or regression characteristic, therefore the algorithm experienced Map Reduce, which with the help of a cloud platform can greatly improve the efficiency of analysis.

A. Introduction of logistic regression  Logistic regression analysis is to analyze the characteristic vector in a regression way, used for classification problems of 0/1, namely to predicate the binary classification problem whose result is 0 or 1. If the final result of the problem can use 0 s and 1 s to distinguish between then we can use data of these problems to get logistic regression models for prediction.

Suppose there is a set of such  data {(?, Yi )},1 -s; i -s; N ,? is characteristic of the input   vector, Yi E {O,l}. Logistic regression expresses the probability of events as a linear model:  log -- =j3'X ( P J --1- P (1) P here is expected probability P(y = I) , j3 is  regression coefficient of feature vector transformation P can get equivalent formula:  p.; e P = P(y = I) = f(x,j3) = --  1+ efJx  After  (2)  From the above formula it can be seen that logistic regression model is, in fact, according to the regression  coefficient j3. And to draw j3 need to make maximum likelihood estimation to likelihood function:  L(X,(J) = 2::1 ?i Inf(;,(J) +(1-yJln(l-f(;,pb) (3)

III.STOCHASTIC GRADIENT DESCENT ALGORITHM BASED  ON SIMULATED ANNEALING  A. Gradient descent algorithm  If real function F(x) is differentiable and is defined at point a, then the function F (x) drops fastest at a point along the opposite direction - V F( a) of the gradient.

Therefore, if b = a - tv F(a) When r > ? is small enough it IS true,  F(a) ? F(b) . [3] With this in mind, we can from initial estimates Xo of the local minimum value of the function F and consider the following sequence X 0' Xi' X 2'" to make Xn+1 = Xn - rn V F(Xn)' n ? ? So F(Xo) ? F(Xj) ? F(X2)'" can be obtained.

2014 IEEE Workshop on Electronics, Computer and Applications  Hopefully sequence (Xn) converges to the desired extreme value. Note that each iteration step length r is changeable.

B.AIgorithm to improve  Considering the shortcoming of gradient descent algorithm trapped in local optimal solution. Simulation of the effectiveness mobile of the annealing based solution can have help solve the problem. The improved gradient algorithm is as follows:  1. Set the initial temperature t = tmax, initial solution  for X K' the largest number of iterations m , cooling rate for the 17 .  Accept the probability function  is pet) = exp( !(xk) - t !(Xk+1) ) .

2. Xk+1 = Xk - f/F(Xk). If F(Xk+1) < F(Xk) , then jump to step 4.

3. If  and pet) > random(O,l), accept the solution Xk+1 and jump to step4; otherwise, select a solution in the field  X K and assign value to Xk+1 to perform this step again.

4. Perform cooling processt = tX17.

5. If a number of consecutive data processing are not  accepted, terminate the algorithm, or jump to step 2.

From the above algorithm we can clearly see that, in  the same temperature, if the candidate solution makes the objective function value decreased, the possibility it is accepted is larger than that of the candidate solution to make function value rise. As the temperature drops, the possibility of a candidate solution accepted by the probability function will be more and smaller. When temperature is close to zero, the possibility of accepting the solution that make the objective function is close to zero. Because of its randomness of accepting solution, algorithmic has a great probability that jump out of local optimal solution, and obtain the global optimal solution.

SUMMARY  After years of development, the cloud computing platform now has basically formed a relatively mature system. It is used in real commercial cloud computing cluster, such as application running on Amazon's EC2 include a variety of monitoring, fault tolerance and scheduling measures and can timely find and handle node failure. It is more reliable than ordinary distributed applications, and cost of hardware and software, including the management maintenance is cheaper. Cloud computing is prepared for the huge amounts data mining.

