Functional Hierarchical Search Results Data Analysis Mohana M. Gurram

Abstract 'NASA Ames, in collaboration with JSC ISS MOD, has been developing novel systems for searching across multiple heterogeneous data systems in order to facilitate rapid retrieval of relevant information for flight controllers on console and in their off-console duties.

Traditional data mining techniques to identify document similarity rely upon brute-force O(n2) cosine similarity of vectors representing text contained in the documents--an expensive and time-consuming operation that becomes unfeasible on large data sets such as those in use by the Shuttle and Station programs at NASA. This paper details a technique developed as part of this activity to identify documents similar to documents relevant and of interest to the user by leveraging the prior search results to produce a weighted graph of document relationships based upon search terms. This paper details the algorithms used, the user interface prototypes developed to evaluate the usefulness of similarity to the browsing activities of users, and the evaluation of the accuracy of this technique as compared with traditional data mining techniques  TABLE OF CONTENTS  1. INTRODUCTION.................... 1  2. BACKGROUND.................... 2  3. APPROACH.................... 3  4. ANALYSIS AND CONCERNS.................... 4  5. CONCLUSION.................... 5  6. REFERENCES.................... 5  1. INTRODUCTION  Search-Driven data analysis method proposed in this paper relies on search results produced by user specific queries, i.e. its user driven. Both naive and sophisticated user demands for a simple search result that one can understand and should address the means of one's search. The systems available in the market perform data-analysis based on predefined classifications or ad-hock classification based on datasets; the results may or may not be relevant to users demand. The approach in this paper is a different perspective of data-analysis using not predefined rules of classifications but from user defined classification rules.

2. BACKGROUND  The more the data is structured the better the applications that can be built on top of it, with the advancement in XML databases, many such applications are addressing the needs of performing data-analysis at middle-tire level as a real- time data analyzer or a standalone application service. The approach in this paper depends on 2NETMARK extensible DataBase (XDB), a schema-less approach to save and index all kinds of textual data semi-structured, full-structured and un-structured. The advantage with NETMARK is it converts even the semi-structured data into a meaningful structured format, the sequence diagram in Figure-2.0 shows the process, NETMARK saves the meta-data associated with the document information as the context of the data. The search results produced by NETMARK are in well structured XML format. Some of the data-sets that ISS MOD search for are PVCS and Flight Controller procedure documents. PVCS documents are in well structured HTML format, the procedure documents are semi-structured documents in Word or PDF format.

7. BIOGRAPHY........6  U.S. Government work not protected by U.S. copyright, based on work  supported by NASA under awards NCC2-1426 and NNA07BB97C.

IEEEAC Paper# 1090, Version 8, Updated 2007:12:17 2A patent pending database technology from NASA Ames Research Center.

User InterFace  Oraq & Orop documents :into web-folder  DAV Server  IF Microsoft OfFice Docurr  or FOF DocumentIIor FEF Dlocument -  Pipeline Server Most of the text-mining processes for document similarity analysis rely on numerical comparison functions like Cosine-Similarity function.

-F'  Decompose  XML, HTML Document  IIructured Decompose Iocument  NjETMARK parses and saves if  Figure-2.0  Traditional 3Text-Mining processes derive high-quality information by identifying patterns and trends, the bottom line is data-mining and text-mining extract previously unknown useful information, the process is unaware of what its is suppose to look for, as shown in Figure-2. 1, the categorization is uncontrolled, the rules to define the categorization are not pre-defined, the rules are been formulated by the discovery of an unknown pattern runtime within the process. In case of clustering approach the rules are pre-defined at the design level. These data-analysis process produced results are very useful for pattern recognition problems in scientific environments but when it comes to a general user wishing to find related documents specific to ones search criteria this such process would be too expensive for the operation.

Similarity (D, ,Dj) = Cos(V(D,) ,V(Dj))  Where D, and Dj are two different documents and V(D,) and V(Dj) are vectors associated with the documents. Such numerical process are computationally intense O(n2), particularly expensive when computing similarity on remote corpus unless corpus is cached. All the text is included in calculating the vectors in order to improve accuracy pre- processing techniques are been applied like Normalization (removal of stop words), establish semantic equivalence (taxonomies/ontology) etc.

This approach is suited for snapshot of data but if the data is changing dynamically the solution may not be applicable, as each updated document will require another O(n2) comparison, or at best an O(n) per updated document, depending on the specific algorithm utilized. Traditional text-mining processes cannot address the following needs with respect to document-similarity, Cost-effective data- analysis for dynamically changing data. User-defined categorization rules for data-analysis.

3. APPROACH  The approach comprised of multiple phases, the first level can be mapped to traditional text-mining approaches, and will be explained in the analysis section of the document.

The second phase depends on the first phase but performs data-analysis at a very granular level and so does the higher- levels.

Documents  Data DJiSCOueF Analysis Categorization  lIIl  DS  Each Shape Represernts a Differernt Category  The following Figure-3.0 gives a high-level picture of NETMARK client-server-services architecture, the focus of our approach is in the red-box section in the Figure-3.0.

This approach can be used on any XML database as long as the search result sets produced by the database are structured result sets. Other then the first level data-analysis all other levels depend on the tag-elements in the XML data, the tags determine the context of the data. So in the result set XML hierarchy every level maps to a level for data-analysis, the more the levels, the granular the analysis is performed i.e. the more the data is structured the better the analysis results are. NETMARK is the best case for this approach because it keeps track of the contexts as well as the level associated with the context.

Figure-2. 1 Both naive and sophisticated users provide search keywordsto a search engine and that determines the search criteria. In the First-level the search criteria itself defines the categorization rule to do data-analysis on search results.

3 Wikipedia http:Hen.wikipedia.org/wiki/Text_mining   nent    HTTP  Document Deoomposition  Netmark  Desktop  Browser  Web Folder  Other Clients  s = {R1, R7, R6, R5,R3}  Then  Isl = 5 , the cardinality of set "s" is the weighted relationship between D, and Dj.

The more the user defines the search criteria, the stronger the relationship among similar document gets, such that the document relationships are directly proportional to number of search criteria that users define, the more the user uses the system the better the data-analysis results they find. This data-analysis technique results in an undirected weighted graph, with each node corresponding to a document and the edge determining the weight and relationship among documents as shown in Figure-3. 1.

4F(5T ) -> { 6Do, D1, D2, D3.}  Where "S" is the search term and F(S) defines the search criteria that results to a finite set of documents that qualify the search criteria. Logically every document in the set is related to the other document in the same set. As the user keeps defining more search keywords the more result sets are produced.

F(To) -> 7Ro  F(T1) -> R  F(T2) -> R2  Accumulating these sets of results, we have the set of result sets:  8S = { Ro, R, R2 , Rn }  Now let's consider a smaller set "r"  r= {Di, Dj)  Where D. and D. are two different documents, there is a number of result sets wherein "r" is contained.

s e S: where r C Rn  In the Second-phase NETMARK plays a vital role, the search function returns not only a document reference but a set of contexts wherein the search term has been found  So if  F (T) -* Do, D1, D2}  Do= { 9cO0, cO1, c02} D1 = {cO1, c04, C05}  D2= {C04, C02, C05}  Where cij is the context at Level "i" and "j" is a unique identifier for each context at level "i".

In other words  F (T) -> { { cO0, cO1, c02}, {cO1, c04, C05}, {c04, c02, C05} }  Let's assume that the similarity of document D, and Dj is 5 at first level i.e. no context involved. The Documents D, and Dj may not be similar with weight 5 at second level.

As per our example  CDODI Do nD1 {c= 1c }  CDO D1 1 1  CDOD2 Do n D2 {c02}  The numbers of result sets determine the weight of relationship between document Di and document Dj. In the specific example, if F: Denotes a Function  T: Denotes a Search String 6D: Denotes a Result item, a Document  R: Denotes a set of Documents  : Set of result sets  CDOD2 1  And  CDID2 = D1 nD2 { COI, C05}  CDID2 2   9c :Denotes a Context Element   Document Analysis  Figure-3.0    In the second level document Do and D1 are similar at context co1 and the weight is 1 because the documents are similar only at one context. The more the contexts are similar in the document sets the more the documents are contextually related, and this applies for level three (c1j), level four (c2%) and so on.

The number of levels contextual data-analysis can be performed depends on the maximum depth of the XML result set.

.._a 2NJ5  mW . i:M . ,  w MI S ';  SM Ft  10 'S *.sS  X0 200I  M    27 19 t.- mm  'l19  1pt .

1W1301  .....-  mm~2701 ..2  11CT702  3-.5000  201 0  ;60r0  30 0  1%2S00300 10Figure-3.1  If document D, and Dj are not related at the level "i" then can not qualify for "i+1" level contextual analysis.

4. ANALYSIS AND CONCERNS  Certain advantages associated with the approach are  Similarity is user controlled, i.e. it's only measured using terms of interest to the user as shown in the Figure-4.0, each circle represents a result set, the more the circles the better relations among documents is established.

This image is a screenshot from a working prototype based on the approach.

Figure-4.0  * In the above Figure-4.0 the documents that happen to occur in most of the intersections can be considered as most relevant document to the user.

* Using this approach if all the possible search criteria's are defined at First-level then the resulting document similarity metrics can be mapped to the similarity metrics of a traditional document-similarity text-analysis process. Most of the traditional document-similarity approaches are one-shot calculations, any change in the state of the data leads to massive computations, this approach is incremental.

* Traditional text-mining algorithms require taxonomies/ ontologies to leverage data-analysis with semantic equivalence but with this approach user can define such relations among keywords within the search criteria using simple algebra, so the data-analysis algorithms will be smart to identify the relations, for example in a search criteria user can define  Plasma Control Unit OR PCU  In this case the result set for the search strings will be a union of result sets for each string and the relationship will be noted and user for further data- analysis.

F(TO) OR F(T1) = Ro U R1  * Similarity can be calculated removing the need to maintain result sets.

PO Pi  P2  gm    * Cost is 0 (n) significantly less then statistical and traditional text-mining algorithms.

* With proper algorithms the relationship weights among documents can be adjusted for dynamic data changes in the document.

The concerns associated with this approach are  * Only a few searches will not produce useful similarity metrics because calculation of similarity among documents is directly proportional to the number of search criteria definitions.

* Naive users will pollute similarity metrics.

* The dynamic changing documents may significantly impact the process of data-analysis algorithm because all the relationship with the document will have to change.

* Need to validate method, compare algorithmically/ qualitatively with other techniques.

5. CONCLUSION  The approach can be very useful for search related problems as the approach can be leveraged to work on any XML database with standard query languages like XQuery or any other query language as long as the result sets produced are well structured. A prototype is been built based on this approach using Netmark as the XML database, .NET framework to build data-analysis algorithms and AJAX (Asynchronous JavaScript XML) as a presentation layer for visualizing the weighted graph. The prototype is been tested on International Space Station (ISS) related data sets. Further developments include evaluating the approach against traditional text-mining approaches, integrate standard visualization techniques with data-analysis metrics, design an optimal algorithm to address the need for dynamic changing data.

[3] Rie Kubota Ando, "Latent Semantic Space: Iterative Scaling Improves Precision of Inter- document Similarity Measurement", Annual AMC Conference on Research and Development in Information Retrieval. Athens, Greece, Pages: 216- 223, Publication Year-2000, ISBN:58113-226-3.

[4] Maluf, A. David, Bell, David, Ashish, Naveen, "Semi-structured Data Management in the Enterprise: A Nimble. High-Throughput, and Scalable Approach," 9th International Database Engineering and Applications Symposium, 2005.

[5] Ming-Syan Chen , Jiawei Han , Philip S. Yu, "Data Mining: An Overview from a Database and Data Engineering, v.8 n.6, p.866-883, December 1996  [6] Maluf, David A. and Tran, Peter B., "NETMARK: A Schemal-Less Extension for Relational Databases for Managing Semi-Structured Data Dynamically," Foundation of Intelligent Systems, Ning Zhong, Zbigniew Ras, Shusaku Tsumoto, Einoshin Suzuki Eds, Springer , ISBN 3-540- 20256-0, 2003.

[7] Maluf, David A., " Implementing Articulation Rules for Object Request Brokers as an Extension to Production Systems ," IEEE Knowledge and Data Engineering Exchange Workshop, Newport Beach, California, pp 36-44, 1997.

[8] Maluf, A. David, et Al., "Knowledge Mining Application in a IVHM Testbed," IEEE Aerospace Conference, Montana, 2006.

6. REFERENCES  [1] D. Maluf, P. Tran, NETMARK: A Schema-Less Extension for Relational Databases for Managing Semi-structured Data Dynamically ISMIS 2003  [2] Maluf, David A. and Tran, Peter B, "An Extensible 'Schema-less' Database Framework for Managing High-Throughput Semi-Structured Documents", IASTED Conference on Computer Science & Technology (CST), Cancun, Mexico, Conference Proceedings, 2003.

7. BIOGRAPHY  Mohana M Gurram is a Computer Scientist for Universities Space Research Association at NASA Ames Research Center. He has worked with NASA missions like Mars Exploration Rover, International Space Station (ISS) and has been presented with awards like TGIR, Honors Award at NASA. His area of interest is Data Analysis, Knowledge Management especially context-sensitive data. Mohana is a co-inventor of multiple patent-pending information system technologies including NASA Program Management Tool, Query-Based document composition, Context-Based Configuration Management System. Mohana earned his Masters in Computer & Information Science.

Christopher D. Knight is a computer engineer at the NASA Ames Research Center.

He is one of the key developers of the NETMARK XDB system. Before joining NASA, he worked for a number of Silicon Valley companies on web-based shopping, gaming, and collaborative systems. His interests are focused on asynchronous collaborative technologies and architectures for rapid application development tools for enabling collaboration. He received a bachelor's degree in computer science from Cal Poly, San Luis Obispo in 1996.

