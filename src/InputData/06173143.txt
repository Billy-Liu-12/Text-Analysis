A New Approach of Self-adaptive Discretization to Enhance the Apriori  Quantitative Association Rule Mining

Abstract?Apriori algorithm was widely applied in association rule mining. Generally, we have to specify different ranges manually to discretize numeral fields to nominal fields, which may weaken the result due to unfit partitions. This paper introduced an approach to make discretized partitions in a self adaptive way to enhance the numeral quantitative association rule mining result.

Keywords-Apriori algorithm; Self-adapting discretization, Association rule mining

I. INTRODUCTION Association rule mining is a kind of data mining which  discovers associative business patterns which can support making business strategic decisions, such as product recommendation, bundling sale and production planning. On the other hand, the user to product association rule mining can discover the latent association between user's selection and his individual property. For example, netizens(net citizens) in the age group 15-20 are more likely to buy smartphones, followed by those between 25-29 years old, According to a survey by The Nation  The most classic algorithm in association rule mining is apriori algorithm[1], it finds item sets iteratively which satisfied with the minimum support. the item sets which has less support than minimum support will be removed every time when an iteration is over. Finally, it gives some frequent itemsets which has k items in each set. According to the confidence and support calculated from the k-itemsets, the very relevant rules will be found.

Generally, the apriori algorithm is usually applied to transactions (for example, a set of items bought by customers, or a series of songs in a music fans' play list), to find the answers to the questions like "Which product set will users most probably put together in a shopping cart?" or "Which products will a customer also buy after bought a laptop computer", this is called boolean association rule. In some cases, it can also mine the association rules from different kinds of data fields, such as the association between user property and his preference, which is called quantitative association rule.

When mining quantitative association rules, considering the continuous numeric properties like age, salary and body- height, it's not appropriate to consider every appeared value as a member of a itemset due to the support will be greatly reduced. In a general way, we have to divide the continuous numeric values to different partitions in advance, in order to avoid reducing support, by appointing the partition method  artificially or by clustering. This is called discretization.

There're some shortcomings in these methods(will be discussed in the following text) and the paper will introduce a new method to generate the partitions self-adaptively.

The self-adaptive approach is aimed to generate much reasonable partitions which can give a high confidence to the calculated association rule while guaranteeing the relatively high support..



II. APRIORI ALGORITHM TO THE ASSOCIATION RULES  A. Association rule Support that I = {i1, i2, ... , im} is the set of data items,  wile D = {t1, t2, ... , tn} is the collection which has the relevant data, T is the sub set of I. There are identifiers for each transaction. The association rule is the implication in the form of A?B, which A, B ? I and A B = ?????  There're two important metrics here: support and confidence, which were used to judge the association rules.

For the association rule A?B, the support S equals the percentage of the A and B in the association which was included in the transaction database D, which can be considered as probability P(AB). The confidence C represents the probability of D contains B if D contains A too, which is the conditional probability P(B|A).

The purpose of the association rule mining is to find the strong association rules which satisfy both minimum support threshold and minimum confidence threshold, itemsets which satisfy minimum support are called frequent itemsets.

Frequent K-Itemsets were denoted by Lk.

Generally, there're two subproblems for association rule mining: find out all the frequent itemsets and then generate corresponding strong association rules by the frequent itemsets. The focusing problem is the former one, which is the target the apriori algorithm.

B. The two steps of apriori algorithm 1.Connecting the frequent (n-1)-itemsets to generate  candidate n-itemsets. If the first k-2 items of two (k-1)- itemsets are equal and the (k-1)th item of the former (k-1)- itemset is less than the (k-1)th item of the latter (k-1)-itemset, they can be connected to the candidate k-itemset.

2.Pruning the k-itemsets according to the apriori character : a candidate k-itemset won't be a frequent k- itemset if any (k-1) item subset of the k-itemset doesn't belong to any frequent (k-1)-itemset.

As introduced above, the algorithm needs plenty of work to judge whether the first k-1 items are same and the last   DOI 10.1109/ISdea.2011.163    DOI 10.1109/ISdea.2012.540     items are different for two k-itemsets, on the other hand, it spends much time when judging whether a itemset is a subset of another. Many methods had been brought forward to improve the efficiency of apriori algorithm.

C. Quantitative association rule In this paper, the quantitative association rule was  focused, because the discretization step only appears in quantitative association rule mining. In this way, the numeral properties were partitioned in to different intervals. For instance, the following rule is an example of the quantitative association rule.

status(X,"netizen") age(X,"20~25")? buys(X, "smartphone")  After the discretization, numeral properties are now converted into nominal properties, and will be able do quantitative association rule mining in the similar way as boolean association mining.



III. THE USUAL WAY OF  DISCRETIZATION As introduced above, in the quantitative association rule  mining, numeral properties must be converted to nominal properties to improve the support as a single numeral value is too small compared with the whole possible numeral values. Then, a problem arose: how to discretize the value felicitously in order to maximize both the support and confidence.

Usually, there doesn't exist an exact range for some numeral properties, for instance, "The man who has height more than 190cm is tall", but how about a man whose height is 189cm? it's not proper to say that he's not tall. In the quantitative association rule mining, the problem can be much complex for the questions like "People in which age range with some other properties are more likely to purchase a treadmill?"  Most of the association rules in data mining is latent including the quantitative association rule, so it's hard to gave a proper discretization method before we know the rule, for instance, people who are teenager or children can be discretized by age to under 12, 12 to 18 and 18 to 25 or under 14, 14 to 20 and 20 to 25. The two different discretization methods may lead to quite different mining result.

Here we exploit a set of data for example. The data is about the consume record of a bookstore, which contains 2000 book purchase records from customers aged between 5 and 90. As the books are various in kind, to make the rule clear to find, we'll reduce the dimension by concluding the book into five main categories: cartoon/storybooks, reference/dictionary, technical/occupational planning, art/self-cultivation, health. If a customer bought two or more books which are in the same category, the category will be considered only one time for this customer. The age distribution of the customers is not a homogeneous distribution:   Figure 1.  Age distribution of the original data  To make the procedures clear to understand, we only consider two fields: age and the book items.

Firstly, the numeral field age should be discretized, here we use  frequently-used methods: discretize by increment of 10, the discretization result is shown in figure 2   Figure 2.  Age distribution of the first discretization method  In contrast, we also discretize the age field by increment of 15:   Figure 3.  Age distribution of the second discretization method  Then all the data fields will be nominal, and can be applied apriori algorithm now.

As the paper discussing the method for discretization, to make the result clear, only the rule mining results with the discretized fields age which shows the preferred books of each age group will be recorded.

The association rules found were recorded on table 1:      Discretization Method Association Rules Found  Increment of 10  age=(X,?10~20?) ?buys(X, ?reference/dictionary?) confidence=64% support=12% age=(X,?20~30?) ?buys(X, ?technical/occupational planning?) confidence=78% support=16% age=(X,?40~50?) ?buys(X, ?art/self-cultivation?) confidence=71% support=13%  Increment of 15  age=(X,?0~15?) ?buys(X, ?cartoon/storybooks?) confidence=49% support=11% age=(X,?15~30?) ?buys(X, ?reference/dictionary?) confidence=54% support=16% age=(X,?15~30?) ?buys(X, ?technical/occupational planning?) confidence=57% support=14% age=(X,?30~45?) ?buys(X, ?art/self-cultivation?) confidence=72% support=16% age=(X,?45~60?) ?buys(X, ?art/self-cultivation ?) confidence=55% support=12% age=(X,?45~60?) ?buys(X, ?health?) confidence=63% support=13%  Table 1. The mining result using fixed discretization  The association rules were found in a minimum support 10% and minimum confidence 40%. According to the result, no rules showing consumers in which age group prefer to buy cartoon/storybooks and health books were found using the discretization method of increment of 10, the second method of discretization helped to find more eligible association rules but the confidence are still relatively low.

Comparing the two results, the different methods of symmetrical discretization has their own advantages and disadvantages, wider ranges of discretization usually help to find association rules with more support but relatively lower confidence while narrower ranges were the opposite.

Usually, support and confidence were two conflicting values, to guarantee both high support and confidence, the range and the position of boundary should be optimized instead of generate it in a fixed way.



IV. THE SELF-ADAPTIVE WAY FOR A BETTER FIT DISCRETIZATION  The target of the new approach is to generate the ranges and boundaries in a much flexible way, for instance, the ranges in the discretization of age can be both 10 years and 14 years, rest with the probability density of the confidence.

Assume A and B are two fields in the transaction database, and A is a numeral field while B is a nominal field.

To find the association rules of A ? B the conditional probability P(B|A) can be precomputed. As A can be  distributed widely, the probability P(A) can be very small, which may lead to a quite fluctuate conditional probability density curve. To mitigate the problem, both the P(A) density curve and the P(B|A) density curve will be smoothed.

To the numeral values which don't have a clear boundary for discretization, such as age, the smooth can be done considering the weighting of the distance between near values, for instance, if a child who is 12 years old bought a cartoon, then the children of 10, 11, 13, 14 or more near ages will be affected too by different levels. So the precomputed conditional probability is a weighted average according to the numeral field.

Let A denote age and B denote an choice of a book, there're only two values in B - true and false, here we only consider the situation that people in age group A has bought B, that is to consider only true values to be P(B)  According to the formula of support and confidence, we can see that partitions which includes values near the peak of the curve will help to find association rules with high confidence and wider partition can help to find association rules with higher support. For instance, figure 4 showed the conditional probability density of association between age and art/self-cultivation.

Figure 4.  Conditional probability density of association between age and  art/self-cultivation  From the figure, we can see that people aged 37 are more likely to buy books about art/self-cultivation, so if people in the partition have ages near 37, the confidence will be high, but considering support, the partition shouldn't be too narrow.

For field age and all the book category fields, the conditional probability density curves are shown in figure 5:   Figure 5.  Conditional probability density of association between age and  other fields     To generate the partitions in a self-adaptive way, the boundaries will be use the x coordinate of the intersection point, which can guarantee that each partition will include a peak of the curve, two partitions can be merged if their two neighbor peaks are too close in distance. Similarly, if a partition is too small, it can be merged into the partition with the nearest peak  In contrast, if a partition is too large, which was caused by too far distance of two neighborhood intersection points, it can be split into several small partitions with the part which has low conditional probability separated.

By this method, the age field were discretized automatically, shown in figure 6.

Figure 6.  Automatically generated age partitions  In the automatically generated partitions, the ranges of partition are not fixed ? there?re both partitions with 6 years and 20 years, instead of only a fixed increment number, the boundaries are also automatically generated.

To evaluate the new discretization method, we use the new discretized age group to  apply a same apriori association rule mining as chapter III. The association rules found were recorded on the table 2.

Discretization Method Association Rules Found  Self-adaptive discretization method  age=(X,?6~15?) ?buys(X, ?cartoon/storybooks=true?) confidence=62% support=11% age=(X,?6~15?) ?buys(X, ?reference/dictionary?) confidence=70% support=12% age=(X,?23~32?) ?buys(X, ?technical/occupational planning?) confidence=80% support=17% age=(X,?33~43?) ?buys(X, ?art/self-cultivation?) confidence=76% support=13% age=(X,? 44~63?) ?buys(X, ?art/self-cultivation ?) confidence=56% support=16% age=(X,?44~63?) ?buys(X, ?health?) confidence=62% support=18%  Table 2. The mining result using the new discretization method  Generally, the result showed that the new method of discretization improved the quality of association rule  mining. Comparing the method of increment of 10, the new method helped to find more strong association rules with high confidence, comparing with the method of increment of 15, the new method helped to find equal number of association rules with much higher confidence, except the last rule. The rules are age=(X,?45~60?) ?buys(X, ?health?) and age=(X,?44~63?) ?buys(X, ?health?), although the former rule 1% higher in confidence (63% to 62%), the latter rule, which is generated by the second method, has the support 5% higher than the former one (18% to 13%), which is generally better.



V. CONCLUSIONS The paper discussed a new approach of self-adaptive  discretization method for apriori association rule mining, the evaluation proved that the new method can make better partitions automatically than traditional methods, which can help to find more strong association rules and enhance the confidence and support. In this paper, only the situation of single numeral to nominal associations were discussed, in the future research, the new approach will be improved to satisfy multiple numeral fields and numeral to numeral fields association rule mining.

ACKNOWLEDGEMENT This research work is supported by "Natural Science  Foundation of Liaoning Province"(20092006).

