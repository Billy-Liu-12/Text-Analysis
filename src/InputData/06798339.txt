ICROIT 2014, India, Feb 6-8 2014

Abstract- In today's network-based cloud computing era, software applications are playing big role. The security of these software applications is paramount to the successful use of these applications. These applications utilize cryptographic algorithms to secure the data over the network through encryption and decryption processes. The use of parallel processors is now common in both mobile and cloud computing scenarios.

Cryptographic algorithms are compute intensive and can significantly benefit from parallelism. This paper introduces a parallel approach to symmetric stream cipher security algorithm known as RC4. The algorithm is very popular and used in many key applications such the Wi-Fi connections, Skype, and Bit Torrent protocol systems. We present an efficient parallel implementation to the compute intensive PRGA that is pseudo? random generation algorithm portion of the RC4 algorithm and the resulted algorithm will be named as PARC4. The algorithm divides a message into fix sized large blocks and encrypts these  blocks concurrently on multi core machine.

Key Words - Parallel Algorithms, Encryption, RC4, Symmetric, WEP, PRGA, Stream Cipher.



I. INTRODUCTION  To secure information communications over the network, different encryption algorithms have been used. The encryption algorithms are further categorized into two broad categories: Symmetric and Asymmetric. In symmetric algorithms, same key is used for both encryption and decryption. RC4 [9][ 1 1- 12] is a very popular symmetric stream cipher algorithm. Some other algorithms include DES, 3-DES, and AES [ 1].

Asymmetric algorithms use different keys for encryption and decryption. The public-key infrastructure-based algorithm such as the RSA [ 1  ][3] is an example of an asymmetric encryption algorithm.

RC4 is a stream cipher algorithm and operates on individual bits to secure the algorithm. We will review it in detail in the next section. The speed of the encryption and decryption is a very important aspect of security algorithms [ 1] in working with applications. A slow cryptographic algorithm can slow the speed of an application and reduce its effectiveness.

Sequential security algorithms can be made faster using parallelization. Fortunately, with the advent of parallel processors in computing, we now have easi Iy avai lable means to parallelize the algorithms to make them faster. The symmetric   multiprocessors (SMP)[8] such those readily available from companies such as Intel can used in conjunction with parallel programming APIs such as OpenMP [2] to make security algorithms parallel and faster. It is possible to use parallel algorithms for any of the cryptographic techniques [8] currently in use.

This paper introduces a parallel approach to execute RC4 in parallel to encrypt a large set of data. The main purpose of the research is to improve the throughput of RC4 stream cipher. We discuss the RC4 algorithm in the next section to highlight some of the key processes used in the algorithm. We follow this up with a discussion on our research methodology to improve performance of cryptographic algorithms. This includes a discussion on how we detect parallelism in the RC4 algorithm and how we then parallelize it to gain performance benefits. In Section 5, we discuss our approach to measure speedup measurements for parallel algorithms. We are presenting a theoretical approach to what we expect to have gains as we parallelize the algorithm. We then conclude the paper and summarize the results.



II. RC4 ALGORITHM  Rc4 is the most common algorithm and is used in popular protocols like secure socket layer (SSL) to protect web browsing and in WEP to protect the wireless networks [5]. Other application areas of RC4 are Skype and Bit Torrent protocol system. RC4 generates key stream that is random stream of bits.

The key stream is combined with the plaintext using bit-wise XOR to generate the encrypted text. The algorithm has two main parts: the key scheduling algorithm (KSA) and the pseudo random generation algorithm (PRGA) [5].

The KSA is used to initialize the permutations in the 'S' array. The "key length" is the num ber of bytes in the key and the  for iji?om 0 to 255 Sri) := i  endfor  j:= 0 for iji?om 0 to 255 j:= 0 + Sri} + keyri mod key_length}) mod 256 SlVap values of Sri} and SOl  endfor  Figure I: The Key Scheduling Algorithrn(KSA)    As shown in Figure 1, the S array is first initialized with digits 0 to 255. Then with the help of S array elements and the keys, the j values are calculated. S[i] and S[j] are then swapped to generate a permuted array. The whole process is executed 256 times to generate a random key stream.

The iterations in the PRGA algorithm, as shown in Figure 2, depend on the input size. In each of the iterations, there is a different value for ranging from 1 to 255. If the input length is more than 255 then the process again starts from 1 and continues until the last byte. For each of the iterations, the value for j is calculated, S[i] and S[j] are swapped, and the sum of S[i] and S[j] mod 256 is looked up in the S array to return one byte. This byte is then XORed with one individual letter in plaintext to convert it into cipher text.

i:= 0  j:= 0  while GeneratingOut put:  i := (i + J) mod 256  j := (j + Sri}) mod 256 swap values o/S{i] and S!J]  Figure 2. The pseudo-random generation algorithm (PRGA)

III. RESEARCH METHODOLOGY  The methodology for speeding up the RC4 algorithm and making it more energy-efficient is based on the parallel computation techniques. The first step towards implementing the parallelism is to fmd the parallelism in algorithm. If the algorithm does not support parallelism then modification in structure is required so that it can support parallelism. Next, we look into these steps that we have taken to make RC4 parallel and faster.

A. Detection ofParalielsim inRC4 Rc4 is a stream cipher algorithm. The algorithm is sequential in structure and the flow of steps is sequential in nature. Consider the PRGA algorithm in Figure 2 that generates the sequence of bytes as output shown: In 5th line of code, j's previous value is being added withs[i] array in each of the iterations. After that, there is a call made to swap function which interchanges the values at indices i and j. This swap method is inherently sequential.

Ifi?2 andj?7. values at Sri]. S[j]  will interchange  Figure 3. Swapping between Sri] and S[j]   As shown in Figure 3, the S array's values are changed after each swap. The procedure is repeated for n times, where n is size of message/plaintext. Thus, to decompose the algorithm into sub tasks and to run the algorithm on multi-core machines isn't feasible with this current structure.

B. Method for adding parallelism The input text is divided into fixed size blocks, where each  block has 256 bits. Afterwards, each individual block is encrypted using RC4 algorithm, and finally, the output of each block is concatenated to make the complete cipher text. These operations are being done by multiple cores to have good speedup. If input data is not the multiple of 256 bits then the last block is padded to make it a 256-bit block. The algorithm can use any specific number of cores and is scalable by structure.

Figures 4 depict the complete mechanism:  BLOCK?1  Bit stream of this IllJek wil encrypted using  ? =BL :;: O=CK  :::; .2 ==?1 ?I :::;::B=LO:;: CK=-n=?  Bit stream of this block will be encrypted using  Bit stream of this block will be encrypted using  Here, k is the multiple of 256 and p is the size of the input  Figure 4. The parallelization process in PARC4

IV. TOOLS AND TECHNIQUES USED FOR PARALLELISM  There are different steps involved in parallelization of an algorithm. First and foremost condition is that the algorithm must be architected in a way so that it can support parallelism.

One must consider following points before executing algorithm in  ? How many functions are the most time-consuming  functions in the sequential implementation?

? Is there any type of data dependency in the algorithm?

These questions can be answered with the help of a profiler tool. The code profiler will tell you the functions that are compute intensive in your algorithm, It tells you how many functions are calling other functions, how many times these functions are called, and where the loop dependencies exist. For our research, we have used GNU's Gprof [ 13- 14] tool for the profil ing purposes.

Gprof is a profiler program that collects and arranges statistics on programs. It generates a "gmon.out" data file which has all of the details of your program including which function executed how many times, which function makes call to which other functions, and how many times. It provides many options to get detai Is about the program,  OpenMP[2] is the application programming interface that    supports shared memory parallel programming using C/C++ and FORTRAN. It provides a portable and scalable model for parallel applications. It comprises of three common components: the environment variables, the compiler directives, and the runtime library routines. These constructs extend a sequential programming language with single instruction multiple data models, synchronization, and work sharing models.

Using OpenMP it is possible to operate on data that is private to a specific processor as well as while sharing with other processors. OpenMP uses fork join model for parallel programs which means only a single thread starts execution and the moment it encounters a parallel region, it distributes the tasks among the team of other threads depending upon the constructs and the data in the region. At the end of parallel region, all threads are terminated after the completion of their respective tasks and only the master thread will continue execution until the next parallel region encountered in the program.

The parallel algorithm PARC4 utilized the following parallelization techniques:  1) Data decomposition: Data decomposition is a commonly used technique for deriving concurrency for the algorithm where dataset is large. In the implementation of PARC4, we have used "input data partitioning" method as the data decomposition technique.

2) Mapping technique for load balancing: For our parallel algorithm, the data decomposition is the first key step. After the decomposition of data, the next step is to map that decomposed data onto different threads (which then utilize different processors) to complete the whole task in a parallel and faster fashion. There are two types of mapping technique: static mapping and dynamic mapping.

? Static mapping: In this approach the independent tasks are being distributed among different processors prior to the execution of the program. Commonly this type of mapping is being used when we have a known data set.

? Dynamic mapping: In this approach the independent  tasks are being distributed among different processors at the  time of execution of the program. This approach is being used  when we have unknown data set. The input data set for PARC4  at a specific point of time is unknown and, as a result, the  dynamic mapping is used here. The dynamic mapping can be  further classified in two categories: centralized and distributed.

In the centralized dynamic mapping, we have a task pool. A  master thread initiates the task and then each processor  independently takes some portion of task or task itself  depending on the structure of the program. In PARC4, we have  applied this dynamic mapping scheme to make sure that each  processor has equal load and the work is more balanced among  the processors.

3) Shared memory architecture: Finally, for coding we use the  OpenMP API that targets the shared memory architecture   of a typical PC processor. Using variable privatization,  synchronization techniques, and work sharing constructs  we have implemented PARC4 the using OpenMP APIs.



V. SPEED UP MEASUREMENTS  There are different methods to calculate the efficiency of parallel algorithm with respect to speed:  Using Amdahl's law: Amdahl's law[2] is named after Gene Amdahl, a computer architect. This is a theoretical concept that is used to predict the maximum speed up by parallelprocessors.

Speedup can be calculated using the formula shown in Figure 5:  PIN +S  P: parallelizabe code S: Sequential code  N: Number of processors  Figure S. Amdahl's Law to calculate speedup  In this formula, P is the fraction of code that could be parallelized, S the fraction of code that is sequential, and N is the number of parallel processors. Parallelizable portion is assumed to be fully parallelizable through the use of parallel processors.

In the Table- 1 below, the speedup calculated using Amdahl' law is listed for up to 8 processors. The speedup will depend on the amount of parallelizable code. According to the Table-I, if 50% portion of the code is parallelizable, we will get l.3X speedup on 2 cores and 1.8X on 8 cores. Similarly if 75% portion of the code is parallelized, we will get up to 3.3X speedup on 8 cores.

The mentioned speedup is based on the fact that the portion of code is completely parallelizable. In PARC4 Algorithm, we have parallelized only PRGA portion. The PRGA swap functionality cannot be executed in parallel. We can only execute the complete functionality of PRGA on different data blocks in parallel. So speedup for PARC4 can differ from the speedup mentioned according to Amdahl's Law.

Table-I: Speed up using Amdhal's law  Parallel Code  Cores/Processo rs 50% 75% 80%  2 1.3 X l.6 X l.6 X  -  4 l.6 X 2.3 X 2.S X  -  8 l.8 X 2.9 X 3.3 X  Linux has a time utility which can be used as a tool to get the elapsed time of a program. Elapsed time is the total time that a program will take to complete. OpenMP has a runtime library routine OMP _GET _ WTIME which is used to get the total elapsed seconds for a specific core. It is used to calculate the total time that a single core is using to execute some portion of data.

For our research, we have set up the machine with the following configuration: ? Processor - AMD FX(tm) - 8320 , eight core  processor running @ 3500 Mhz ? RAM-8 GB ? System Type - 64 bit operating system,x64- based  processor ? Operating System - Linuxlubuntu 12.04 version ? OpenMP - 4.0 ? Compiler- GCC ? Programming Language - C with OpenMP  Using the mentioned setup, we executed the PARC4 algorithm on multiple cores (2 to 8 cores) and found increasing performance of the algorithm with the increasing number of cores as we had expected. For the largest file, we see a speedup of over 3 .5X with the 8-core configuration.

Table-2 lists the runtime impact as we the change the number of cores for each run.

Table-2: Speed up gained by PARC4  File Sequential 2 Core 4 Core 6 Core 8 Core Size  512 KB 0.01037Ins 0.01015 0.00366 0.00261 0.00193 InS InS InS InS  1 MB 0.01457Ins 0.00664 0.00641 0.00483 0.00196 InS InS InS InS  1.5MB 0.01457Ins 0.01331 0.00965 0.00783 0.00525 InS InS InS InS  2.5MB 0.02749Ins 0.02559 0.01808 0.01098 0.00807 InS InS InS InS  3.0MB 0.03957Ins 0.03260 0.02261 0.01389 0.01074 InS InS InS InS  Figure-6 shows the graphical representation of performance improvement by PARC4.

Vi E  OJ E  i= c  ? :J u  OJ x w  0.05  0.04  0.03  0.02  0.01   512 KB 1 MB 1.5 MB2.5 MB3.0 MB  File Size  ? Sequential  .2 Core  .4Core  .6 Core  .8 Core  Figure 6. Graphical representation of perfonnance improvement By PARC4

VI. CONCLUSION AND FUTURE WORK  This paper introduces PARC4 which is a parallel approach to the well-known RC4 cipher algorithm. We have used the ideas of profiling, data decomposition, and the dynamic mapping as the key means to parallelize and make the algorithm faster. OpenMP is used to parallelize the algorithm on a shared memory multicore architecture such as a 8-core machine that we have used for implementation. Our early results on experimentation show promising results on parallelization efforts. We plan to investigate alternative parallelization techniques for the same algorithm and also measure the gained energy efficiencies of the parallel algorithms.

