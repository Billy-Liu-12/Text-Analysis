A Data Clustering Algorithm for Mining Patterns  From Event Logs

Abstract- Today, event logs contain vast amomts of data that can easily overwhelm a human. Therefore, mining patterns from event lags is an important system management task. This paper presents a novel clustering algorithm for log file data sets which helps one to detect frequent patterns from log files. to build log file profiles, and to identify nnomalow log file lines.

Keywords-system monitoring, data mining, data clustering  1. INTRODUCTION Event logging and log files are playing an increasingly  important role in system and network management. Over the past two decades, the BSD syslog protocol [ I ]  has become a widely accepted standard that is supported on many operating systems and is implemented in a wide range of system devices. Well-written system applications either use the syslog protocol or produce log files in custom format, while many devices like routers, switches, laser printers, etc. are able to log their events to remote host using the syslog protocol.

Normally, events are logged as single-line textual messages.

Since log files are an excellent source for determining the health status of the system, many sites have built a centralized logging and log file monitoring infrastructure. Because of the importance of log files as the source of system health information, a number of tools have been developed for monitoring log files, e.g., Swatch [Z], Logsurfer [3], and SEC [4].

Log file monitoring techniques can he categorized into fault detection and anomaly detection. In the case of faull detection, the domain expert creates a database of fault message patterns. If a line is appended to a log file that matches a pattern, the log file monitor takes a certain action.

This commonly used approach has one serious flaw - only those faults that are already known to the domain expert can be detected. If a previously unknown fault condition occurs, the log file monitor simply ignores the corresponding message in the log file, since there is no match for it in the pattern database. Also, it is often difficult to find a person with sufficient knowledge about the system. In  the case of anomaly detection, a system profile is created which reflects normal system activity. If messages are logged that do not fit the profile, an alarm is raised. With this approach, previously unknown fault conditions are detected, but on the other hand, creating the system profile by hand is time-consuming and error-prone.

This work IS supported by the Union Bank ofEstonia  In order to solve the knowledge acquisition problems, various methods have been employed, with data mining methods being one of the most popular choices [5,  6, 7, 8, 91.

In  most research papers, the focus has been on mining frequent patterns from event logs. This helps one to find patterns that characterize the normal behavior of the system, and facilitates the creation of the system profile. However, as pointed out in [SI, the mining of infrequent patterns is equally important, since this might reveal anomalous events that represent unexpected behavior of the system, e.g., previously unknown fault conditions. Recent research papers have mainly proposed the mining of temporal patterns from event logs with various association rule algorithms [5, 6, 7, 8, 91. These algorithms assume that the event log has been normalized, i.e., all events in the event log have a common format. Typically, each event is assumed to have at least the following attributes: timestamp of event occurrence, the event type, and the name of the node which issued the event (though the node name is often encoded in the event type). Association rule algorithms have been often used for detecting temporal associations between event types [5,6,7, 8 ,9 ] ,  e.g., if events oftype A and B occur within 5 seconds, they will be followed by an event of type C within 60 seconds (each detected temporal association has a certain frequency and confidence).

Although association rule algorithms are powerhl, they often can't he directly applied to log files, because log file lines do not have a common format. Furthermore, log file lines seldom have all the attributes that are needed by the association rule algorithms. For example, the widely used syslog protocol does not impose strict requirements on the log message format [I]. A typical sydog message has just the timestamp, hostname, and program name attributes that are followed by a free-form message string, hut only the message string part is mandatory [I ] .  A detailed discussion of the shortcomings of the syslog protocol can be found in [ I ,  IO].

One important attribute that log file lines often lack is the event type. Fortunately, it is possible to derive event types from log file lines, since very often the events of the same type correspond to a certain line pattern. For example, the lines  Router myrouterl interface 192.168.13.1 down Router myroulerZ interface IO. IO. 10. I2 down Router myrouter5 interface 192.168.22.5 down  0-7803-8199-8/03/$17.C? 0 ZW3 IEEE IPOM2003 Page 119    represent the event type "interface down", and correspond to the line pattern Router + interface * down. Line patterns could be identified by manually reviewing log files, but this is feasible for small log files only.

One appealing choice for solving this problem is the employment of data clustering algorithms. Clustering algorithms [ I I ,  121 aim at dividing the set of objects into groups (clusters), where objects in each cluster are similar to each other (and as dissimilar as possible to objects from other clusters). Objects that do not fit well to any of the clusters detected by the algorithm are considered to form a special cluster of outliers. When log file lines are viewed as objects, clustering algorithms are a natural choice, because line patterns form natural clusters - lines that match a certain pattern are all similar to each other, and generally dissimilar to lines that match other patterns. After the clusters (event types) have been identified, association rule algorithms can be applied for detecting temporal associations between event types.

However, note that log file data clustering is not merely a preprocessing step. A clustering algorithm could identify many line patterns that reflect normal system activity and that can be immediately included in the system profile, since the user does not wish to analyze them further with the association rule algorithms. Furthermore, the cluster of outliers that is formed by the clustering algorithm contains infrequent lines that could represent previously unknown fault conditions, or other unexpected behavior of the system that deserves closer investigation.

Although data clustering algorithms provide the user a valuable insight into event logs, they have received little attention in the context of system and network management.

In this paper, we discuss existing data clustering algorithms, and propose a new clustering algorithm for mining line patterns from log tiles. We also present an experimental clustering tool called SLCT (Simple Logfile Clustering Tool).

The rest of this paper is organized as follows: section 2 discusses related work on data clustering, section 3 presents a new clustering algorithm for log file data sets, section 4 describes SLCT, and section 5 concludes the paper.

11. RELATED WORK Clustering methods have been researched extensively over  the past decades, and many algorithms have been developed [ I I ,  121. The clustering problem is often defined as follows: given a set of points with n attributes in the data space R", find a partition of points into clusters so that points within each cluster are close (similar) to each other. In  order to determine, how close (similar) two points x and y are to each other, a distance function d(x, y) is employed. Many algorithms use a certain variant of L, norm (p = 1, 2, ...) for the distance function:  Today, there are two major challenges for traditional clustering methods that were originally designed for clustering  numerical data in low-dimensional spaces (where usually n is well below IO).

Firstly, quite many data sets consist of points with categorical attributes, where the domain of an attribute is a finite and unordered set of values [13, 141. As an example, consider a categorical data set with attributes car-manufacturer, model, type, and color, and data points ('Honda', 'Civic', 'hatchback', 'green') and ('Ford', 'Focus', 'sedan', 'red'). Also, it is quite common for categorical data that different points can have different number of attributes.

Therefore, it is not obvious how to measure the distance between data points. Though several popular distance functions for categorical data exist (such as the Jaccard coefficient [12, 13]), the choice of the right function is often not an easy task. Note that log file lines can be viewed as points from a categorical data set, since each line can be divided into words, with the n-th word serving as a value for the n-th attribute. For example, the log file line Connection from 192.168.1.1 could he represented by the data point ('Connection', 'from', '192.168.1.1'). We will use this representation of log file data in the remainder of this paper.

Secondly, quite many data sets today are high- dimensional, where data points can easily have tens of attributes. Unfortunately,.traditional clustering methods have been found not to work well when they are applied to high- dimensional data. As the number of dimensions n increases, it is often the case that for every pair of points there exist dimensions where these points are far apart from each other, which ,makes the detection of any clusters almost impossible (according to some sources, this problem starts to be severe when n ? 15) [12, 15, 161. Furthermore, traditional clustering methods are often unable to detect natural clusters that exist in subspaces of the original high-dimensional space [15, 161. For instance, data points (1333, 1, 1, 99,25, 2033, 1044), (12, 1, 1, 724,667, 36,2307), and (501, 1, I ,  1822, 1749, 808, 9838) are not seen as a cluster by many traditional methods, since in the original data space they are not very close to each other. On the other hand, they form a very dense cluster in the second and third dimension ofthe space.

The dimensionality problems described above are also relevant to the clustering of log file data, since log file data is typically high-dimensional (i.e., there are usually more than just 3-4 words on every line), and most of the line patterns correspond to clusters in subspaces. For example, the lines  log: connectionfrom 192.168.1.1 log: RSA key generation complete log: Password authenticationfor john accepted.

form a natural cluster in the first dimension of the data  space, and correspond to the line pattern log:  *.

During past few years, several algorithms have been  developed for clustering high-dimensional data, like CLIQUE, MAFIA, CACTUS, and PROCLUS. The CLIQUE 1151 and MAFIA [I71 algorithms closely remind the Apriori algorithm for mining frequent itemsets [IS]: they start with identifying all clusters in I-dimensional subspaces, and after they have identified clusters C,, ..., Cm in @-/)-dimensional subspaces,  IPOM2003 Page 120    they form cluster candidates for k-dimensional subspaces from Cl,..&, and then check which of those candidates are actual clusters. Those algorithms are effective in discovering clusters in subspaces, because they do not attempt to measure distance between individual points, which is often meaningless in a high-dimensional data space. Instead, their approach is density based, where a clustering algorithm tries to identify dense regions in the data space, and forms clusters from those regions. Unfortunately, the CLIQUE and MAFIA algorithms suffer from the fact that Apriori-like candidate generation and testing involves exponential complexity and high runtime overhead [19, 201 - in order to produce a frequent m-itemset, the algorithm must first produce Zm - 2 subsets of that m- itemset. The CACTUS algorithm [I41 first makes a pass over the data and builds a data summary, then generates cluster candidates during the second pass using the data summary, and finally determines the set of actual clusters. Although CACTUS makes only two passes over the data and is therefore fast, it is susceptible to the phenomenon of chaining (long strings of points are assigned to the same cluster) [ I  I], which is undesirable if one wants to discover patterns from log files. The PROCLUS algorithm [I61 uses the K-medoid method for detecting K clusters in subspaces of the original space. However, in the case of log file data the number of clusters can rarely be predicted accurately, and therefore it is not obvious what is the right value for K.

Though several clustering algorithms exist for high- dimensional data spaces, they are not very suitable for clustering log tile lines, largely because they don't take into account the nature of log file data. In the next section, we will first discuss the properties of log file data, and then we will present a fast clustering algorithm that relies on these properties.

Data Set  111. CLUSTERING LOG FILE DATA  A .

The nature of the data to be clustered plays a key mle when  choosing the right algorithm for clustering. Most of the clustering algorithms have been designed for generic data sets such as market basket data, where no specific assumptions about the nature of data are made. However, when we inspect the content oftypical log files at the word level, there are two important properties that distinguish log file data fmm a generic data set. During our experiments that revealed these properties, we used six logfile data sets from various domains: HP OpenView event log file, mail server log file (the server was running sendmail, ipopd, and imapd daemons), Squid cache server log file, Internet banking server log file, file and print server log file, and Win2000 domain controller log file.

Although it is impossible to verify that the properties we have discovered characterize every logfile ever created on earth, we still believe that they are common to a wide range of logfile data sets.

Firstly, majority of the words occur only a few times in the data set. Table 1 presents the results of an experiment for estimating the occurrence times of words in log file data. The results show that a majority of words were very infrequent, and a significant fraction of words appeared just once in the data set (one might argue that most of the words occurring once are timestamps, but when timestamps were removed from data sets, we observed no significant differences in the experiment results). Also, only a small fraction of words were relatively frequent, i.e., they occurred at least once per every 10,000 or 1,000 lines. Similar phenomena have been observed for World Wide Web data, where during an experiment nearly 50% of the words were found to occur once only [21].

The Nature ofLog File Data  Mail server log file Cache  ewer log HP OpenView lntemet banking File and print server Domain controller (Li"W) file (Linux) event log file sewer log file log file(Win2000) logfile(WinZ000)  (Solaris) (Solaris)  Data set size 1025.3 ME, 1088.9 ME, 696.9 MB, 2872.4 MB, 2451.6M6, 1043.9 ME, 7,657,148 lines 8,189,780 lines 1,835,679 lines 14,733,696 lines 7,935,958 lines 4,891,883 lines  IPOM2003 Page 121    Secondly, we discovered that there were many strong correlations between words that occurred frequently. As we found, this effect is caused by the fact that a message is generally formatted according to a certain format string before it is logged, e.g.,  sprinf(messuge, "Connection from '366 port %d': ipuddress, portnumber).

When events of the same type are logged many times, constant parts of the format string will become frequent words which occur together many times in the data set. In the next subsection we will present a clustering algorithm that relies on the special properties of log file data.

B. The clustering algorithm Our aim was to design an algorithm which would he fast  and make only a few passes over the data, and which would detect clusters that are present in subspaces of the original data space. The algorithm relies on the special properties of log file data discussed in the previous subsection, and uses the density based approach for clustering  The data space is assumed to contain data points with categorical attributes, where each point represents a line from a log file data set. The attributes of each data point are the words from the corresponding log file line. The data space has n dimensions, where n is the maximum number of words per line in the data set. A region S is a subset of the data space, where certain attributes i l ,  ..& ( 1 3 9 4  of all points that belong to Shave identical values v1 ,..., vk: Vx E S, x,, = v,, ..., xIk = vk. We call the set {(il,vl), ..., (ik,vk)} the set of fried atlributes of region S. If kl (i.e., there is just one fixed attribute), the region is called /-region. A dense region is a region that contains at least N points, where N is the support fhreshold value given by the user.

The algorithm consists of three steps like the CACTUS algorithm [I41 - it first makes a pass over the data and builds a data summary, and then makes another pass to build cluster candidates, using the summary information collected before.

As a final step, clusters are selected from the set of candidates.

During the first step of the algorithm (data summarization), the algorithm identifies all dense I-regions. Note that this task is equivalent to the mining offrequent words from the data set (the word position in the line is taken into account during the mining). A word is considered frequent if it occurs at least N times in the data set, where N is the user-specified support threshold value.

After dense I-regions (frequent words) have been identified, the algorithm builds all cluster candidates during one pass.

The cluster candidates are kept in  the candidate table, which is initially empty The data set is processed line by line, and when a line is found to belong to one or more dense I-regions (i.e., one or more frequent words have been discovered on the line), a cluster candidate is formed. If the cluster candidate is not present in the candidate table, it will he inserted into the table with the support value I ,  otherwise its support value will he incremented. In both cases, the line is assigned to the cluster candidate. The cluster candidate is formed in the following way: if the line belongs to m dense I-regions that have fixed attributes (il,v,), ...,( im,v,,,), then the cluster  candidate is a region with the set of fixed attributes {(il,vl), ...,( im,vm)). For example, if the line is Connectionfrom 192.168./ . / ,  and there exist a dense I-region with the fixed attribute ( I ,  'Connection') and another dense I-region with the fixed attribute (2, 'from'), then a region with the set of fixed attributes { ( I ,  'Connection'), (2, 'from')} becomes the cluster candidate.

During the final step of the algorithm, the candidate table is inspected, and all regions with support values equal or greater than the support threshold value (Le., regions that are guaranteed to he dense) are reported by the algorithm as clusters. Because of the definition of a region, each cluster corresponds to a certain line pattern, e.g., the cluster with the set of fixed attributes { ( I ,  'Password'), (2, 'authentication'), (3, 'for'), (5, 'accepted')) corresponds to the line pattem Password authentication for * accepted.

Thus, the algorithm can report clusters in a concise way by just printing out line patterns, without reporting individual lines that belong to each cluster. The CLIQUE algorithm reports clusters in a similar manner [15].

The first step of the algorithm reminds very closely the popular Apriori algorithm for mining frequent itemsets [IS], since frequent words can he viewed as frequent I-itemsets.

Then, however, our algorithm takes a rather different approach, generating all cluster candidates at once. There are several reasons for that. Firstly, Apriori algorithm is expensive in terms of runtime [19, 201, since the candidate generation and testing involves exponential complexity. Secondly, since one of the properties of log file data is that there are many strong correlations between frequent words, it makes little sense to test a potentially huge number of frequent word combinations that are generated by Apriori, while only a relatively small number of combinations are present in the data set. It is much more reasonable to identify the existing combinations during a single pass over the data, and verify after the pass which of them correspond to clusters.

It should he noted that since Apriori uses level-wise candidate generation, it is able to detect patterns that our algorithm does not report. E.g., if words A, B, C, and D are frequent, and the only combinations of them in the data set are A B C and A B D, then our algorithm will not inspect the pattern A B, although it could have the required support. On the other hand, by restricting the search our algorithm avoids reporting all subsets of a frequent itemset that can easily overwhelm the user, hut rather aims at detecting maximal frequent itemsets only (several pattern-mining algorithms like Max-Miner [I91 use the similar approach).

In order to compare the runtimes of our algorithm and Apriori-based algorithm, we implemented both algorithms in Per1 and tested them against three small log file data sets.

Table 2 presents the results of our tests that were conducted on I,5GHz Pentium4 workstation with 256Mf3 of memory and Redhat 8.0 Linux as operating system (the sizes of log files A, B, and C were 180KB, 1814KB, and 4005KB, respectively).

The results obtained show that our clustering algorithm is superior to the Apriori-based clustering scheme in terms of runtime cost. The results also indicate that Apriori-based clustering schemes are appropriate only for small log file data sets and high support thresholds.

IPOM2003 Page 122    TABLE 11. THE RUNTIME COMPARISON OF OUR ALGORITHM A N 0  APRIORI-BASED ALMRlTHM  Although our algorithm makes just two passes over the data and is therefore fast, it could consume a lot of memory when applied to a larger data set. In the next subsection we will discuss the memory cost issues in more detail.

C. The Memory Cost of The Algorithm In terms of memory cost, the most expensive part of the  algorithm is the first step when the data summary is built.

During the data summarization, the algorithm seeks for frequent words in the data set, by splitting each line into words. For each word, the algorithm checks whether the word is present in the word table (or vocabulary), and if it isn't, it will be inserted into the vocabulary with its occurrence counter set to 1. If the word is present in the vocabulary, its occurrence counter will be incremented.

If the vocabulary is built for a large data set, it is likely to consume a lot of memory. When vocabularies were built for data sets from Table 1, we discovered that they consumed hundreds of megabytes of memory, with the largest vocabulary occupying 653 MB (the tests were made on Sun Fire V480 server with 4 GB of memory, and each vocabulary was implemented as a move-to-front hash table which is an efficient data structure for accumulating words [21]). As the size of the data set grows to tens or hundreds of gigabytes, the situation is very likely to deteriorate further, and the Vocabulary could not fit into the main memory anymore.

On the other hand, one of the properties of log file data is that a majority of the words are very infrequent. Therefore, storing those very infrequent words to memory is a waste of space. Unfortunately, it is impossible to predict during the vocabulary construction which words will finally be infrequent.

In order to cope with this problem, we use the following technique - we first estimate which words need no1 to be stored in memory, and then create the vocabulary without irrelevant words in it. Before the data pass is made for building the vocabulary, the algorithm makes an extra pass over the data and builds a word summary vector. The word summary vector is made up of m counters (numbered from 0 to m-I) with each counter initialized to zero. During the pass over the data, a fast string hashing function is applied to each word. The function returns integer values from 0 to m-I, and each time the value i is calculated for a word, the i-th counter in the vector will be incremented. Since efficient string hashing functions are uniform 1221, i.e., the probability of an arbitrary string hashing to a given value i is llm, then each counter in the vector will correspond roughly to W I m words,  where W is the number of different words in the data set. If words WI, ..., Wk are all words that hash to the value i, and the words wI,  ..., wk occur tl, ..., tk times, respectively, then the value of the i-th counter in the vector equals to the sum tl+ ...+ ti.

After the summary vector has been constructed, the algorithm starts building the vocabulary, but only those words will be inserted into the vocabulary for which their counter values are equal or greater than the support threshold value given by the user. Words that do not fulfill this criterion can't he frequent, because their occurrence times are guaranteed to be helow the support threshold.

Given that a majority of the words are very infrequent, this simple technique is quite powerful. If the vector is large enough, a majority of the counters in the vector will have very infrequent words associated with them, and therefore many counter values will never cross the support threshold.

In order to measure the effectiveness of the word summary vector technique, we made a number of experiments with data sets from Table 1. We used the support thresholds of I%, 0.1%, and 0.01% together with the vectors of 5,000, 20,000, and 100,000 counters, respectively (each counter consumed 4 bytes of memory). The experiments suggest that the employment of the word summary vector dramatically reduces vocabulary sizes, and large amounts of memory will be saved.

During the experiments, vocabulary sizes decreased 9.93-99.36 times, and 32.74 times as an average. On the other hand, the memory requirements for storing the vectors were relatively small - the largest vector we used during the experiments occupied less than 400 KB of memory.

If the user has specified a very low support threshold, there could be a large number of cluster candidates with very small support values, and the candidate table could consume a significant amount of memory. In order to avoid this, the summary vector technique can also be applied to cluster candidates - before the candidate table is built, the algorithm makes an extra pass over the data and builds a summary vector for candidates, which is later used to reduce the number of candidates inserted into the candidate table.



IV. SIMPLE LOGFILE CLUSTERING TOOL In order to implement the log tile clustering algorithm  described in the previous section, an experimental tool called SLCT (Simple Logfile Clustering Tool) has been developed.

SLCT has been written in C and has been primarily used on RedhatX.0 Linux and Solaris8, but it should compile and work on most modem UNIX platforms.

IPOM2003 Page 123    SLCT uses move-to-front hash tables for implementing the vocabulary and the candidate table. Experiments with large vocabularies have demonstrated that move-to-front hash table is an efficient data structure with very low data access times, even when the hash table is full and many words are connected to each hash table slot [Zl]. Since the speed of the hashing function has a critical importance for the efficiency of the hash table, SLCT employs the fast and eficient Shift-Add- Xor string hashing algorithm [22]. This algorithm is not only used for hash table operations, hut also for building summary vectors.

SLCT is given a list of log files and a support threshold as input, and after it has detected a clustering on input data, it reports clusters in a concise way by printing out line pattems that correspond to clusters, e.g.,  Dec I X  * myhost mydomain * connect from Dec 18 * myhost mydomain * log: Connection from *port Dec I8 * myhostmydomain ' log: The user can specify a command line flag that forces SLCT  to inspect each cluster candidate more closely, before it starts the search for clusters in the candidate table. For each candidate C, SLCT checks whether there are other candidates in the table that represent more specific line patterns. In the above example, the second pattern is more specific than the third, since all lines that match the second patlem also match the third. If candidates C,, ..., Ck representing more specific patterns are found for the candidate C, the support values of the candidates CI ,  ..., Ck are added to the support value of C, and all lines that belong to candidates CI ,  .... CL are also considered to belong to the candidate C. In that way, a line can belong to more than one cluster simultaneously, and more general line patterns are always reported, even when their original support values were below the threshold. Although traditional clustering algorithms require that every point must be part of one cluster only, there are several algorithms like CLIQUE which do not strictly follow this requirement, in order to achieve clustering results that are more comprehensible to the end user [15].

By default, SLCT does not report the lines that do not belong to any of the detected clusters. As SLCT processes the data set, each detected outlier line could be stored to memory, hut this is way too expensive in terms of memory cost. If the end user has specified a certain command line flag, SLCT makes another pass over the data after clusters have been detected, and writes all outlier lines to a file. Also, variable parts of cluster descriptions are refined during the pass, by inspecting them for constant heads and tails. Fig. 1 depicts the sample output from SLCT.

If the log file is larger, running SLCT on the data set just once might not be sufficient, because interesting cluster candidates might have very different support values. If the support threshold value is too large, many interesting clusters will not be detected. If the value is too small, interesting cluster candidates could be split unnecessarily into many subcandidates that represent rather specific line patterns and have quite small support values (in the worst case, there will be no cluster candidates that cross the support threshold).

Des 1B f myhoar.mydomain 8shdI.I: connect from 1 1 2 . 2 6 . 2 4 2 . 1 1 8 SYpport: 2 6 2  s Y C  -I outliers 168 oYCllers  Figure 1. Sample output from SLCT  lauth.critl log in l"* l :  pam-krbl: authenticate error: InpurioutpYr error 151  1mil .a lerLI  l lnapdlfffl: Fatal disk hoaL-ff. I.**, rabx-"': Disk quota exceeded  Figure 2. Sample anomalous log file lines  IPOM2003 Page 124    In order to solve this problem, an iterative approach suggested in [8] could be applied. SLCT is first invoked with a relatively high threshold (e.g., 5% or IO%), in order to discover very frequent patterns. Ifthere are many outliers after the first run, SLCT will be applied to the file of outliers, and this process will be repeated until the cluster of outliers is small and contains very infrequent lines (which are possibly anomalous and deserve closer investigation). Also, if one wishes to analyze one particular cluster in the data set more closely, SLCT allows the user to specify a regular expression filter that will pass relevant lines only.

Fig. 2 depicts sample anomalous log file lines that we discovered when the iterative clustering approach was applied to one of our test data sets (the mail server log file from Table 1). Altogether, four iterations were used, and the cluster of outliers contained 318,166, 98,811, 22,807, and 5,390 lines after the first, second, third, and fourth (the final) step, respectively. At each step, the support threshold value was set to 5%. The final cluster of outliers was then reviewed manually, and the lines representing previously unknown fault conditions were selected. The lines in Fig. 2 represent various system faults, such as internal errors of the sendmail, imapd, and syslogd daemon, but also unsuccessful attempts to gain system administrator privileges (for the reasons of privacy and  detecting interesting patterns from log files. Table 3 presents the results of some our experiments for measuring the runtime and memory consumption of SLCT. The experiments were conducted on I,5GHz Pentium4 workstation with 256MB of memory and Redhat 8.0 Linux as operating system. For all data clustering tasks, a word summary vector of size 5,000 counters was used. Since SLCT was also instructed to identify outlier points, four passes over the data were made altogether during the experiments. The results show that our algorithm has modest memory requirements, and finds many clusters from large log files in a relatively short amount of time.



V. FUTURE WORK AND AVAILABILITY INFORMATION For a future work, we plan to investigate various  association rule algorithms, in order to create a set of tools for building log file profiles. We will be focusing on algorithms for detecting temporal patterns, hut also on algorithms for detecting associations between event attributes within a single event cluster.

SLCT is distributed under the terms of GNU GPL, and is available at http:likodu.neti.eel-ristolslcti.

