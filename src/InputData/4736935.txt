Solving Cross-selling Problems with Ensemble Learning: A Case Study

Abstract   This paper shows our solution to PAKDD Competition 2007 as a case study of cross-selling problems. Following a brief description of the data mining task, we discuss several difficulties to be confronted with in the task from the view of data mining. Then, we show how to do the data pre- processing. In the solution we proposed, to weaken class imbalance of the modeling dataset externally, we combine under-sampling and over-sampling techniques. Besides, we adjust the parameters of each base learner internally to solve cost-sensitivity. Next, we get an ensemble of base learners to achieve a better predicting performance. Experimental results on prediction dataset of real world, provided by PAKDD Competition 2007 show that our solution is effective and efficient with its AUC value 60.73%.

1. Introduction   Cross-selling is becoming a more and more important and fashionable selling style in recent years.

Cross-selling can be categorized to two classes: customers based cross-selling and business based cross-selling. Most of research concentrates on the former [1-3]. Customers based cross-selling discovers business demand of each customer by mining association rules, according to different customers.

However, there are few successful cross-selling models based on business to guide cross-selling by mining customers? datasets of some type of business. In the later class, customers from the same dataset have the same type of business. This paper concentrates on predicting possible business based cross-selling.

The cross-selling problem of PAKDD competition 2007 is described as follows. The company currently has a customer base of credit card customers as well as a customer base of home loan (mortgage) customers.

Both of these products have been on the market for many years, although for some reason the overlap  between these two customer bases is currently very small. The company would like to make use of this opportunity to cross-sell home loans to its credit card customers, but the small size of the overlap presents a challenge when trying to develop an effective scoring model to predict potential cross-sell take-ups. The data mining task is to produce a score for each customer in the prediction dataset, indicating a credit card customer's propensity to take up a home loan with the company (the higher the score, the higher the propensity).

The reminder of the paper is organized as follows.

Section 2 describes the difficulties to address the task briefly. Section 3 shows how to pre-process the modeling dataset and prediction dataset. Section 4 presents our solution. Section 5 provides some representative evaluation results by PAKDD Competition 2007 and section 6 concludes the work.

2. Analysis and Understanding to Cross- selling Problems   Based on our understanding to the dataset provided, there are several difficulties to be dealt with in the cross-selling task from data mining aspect.

2.1. Nominal Attributes and Missing Values   In both of the modeling and prediction datasets, there are many nominal attributes which cannot be used, when modeling, by the majority of the most non- probability based algorithms directly, such as SVM, k- nearest neighbors, and so on. Besides, there are many missing values in several variables. Simply removing all the samples with missing values will lose too much information. Then, how to pre-process the dataset seems an interesting and complex challenge.

2.2. Class Imbalance    DOI 10.1109/ICACTE.2008.86    DOI 10.1109/ICACTE.2008.86     In the modeling dataset, the ratio between the number of credit card customers and that of home loan customers is 40,000:700, so severe class imbalance exists in the modeling dataset. It is known that class imbalance makes traditional learners behave badly, especially for minority class prediction. However, minority class always causes our more attention in most cases. Then, how to deal with such class imbalance effectively?

2.3. Cost-sensitivity   From the potential interest of the consumer finance company, it may be anticipated that all potential home loan customers are scored high, because a higher score represents a greater possibility to become a home loan customer, who will bring profit to the company, and in the same time, credit card customers are scored as low as possible, so that they will be paid less attention in the cross-selling process. However, a predictor cannot always predict correctly, and may producing a high score for a credit card customer and a low score for a home loan customer, but the costs are quite different, because mis-producing high scores for credit card customers will introduce some cost, such as advertising fees, but mis-producing low scores for home loan customers will introduce profit loss. And profit loss may be hundreds or thousands times of advertising fees regarding to average cost for one customer. In the cost sense, we would produce high scores for dozens of credit card customers rather than producing a low score for a home loan customer. This suggests that the task is cost-sensitive. But the cost information is unknown, then, how to deal with such cost-sensitivity?

2.4. Scoring   In the modeling dataset, target flags for all modeling samples are given, whose value may be 0 or 1, so it may be easy to do classifying. But the task is to produce a score for each customer in the prediction dataset, not to simply predict a target flag. In decision- making of cross-selling, predicting each sample with a score, which implies the possibility a credit card customer will became a home loan customer, gives decision maker more freedom than providing a predicted label simply. Then, how to produce a score for each customer in the prediction dataset?

3. Data Pre-processing   Table 1. Statistical results of missing values  in the datasets provided  Modeling dataset  Prediction dataset  Count Perc. Count Perc.

AMEX_CARD 3707 9.09 721 9.01  DINERS_CARD 3884 9.52 760 9.50 VISA_CARD 2455 6.02 477 5.96  MASTER_CARD 2937 7.20 570 7.13 RETAIL_CARDS 3693 9.05 724 9.05  DISP_INCOME_CODE 28742 70.45 5672 70.9 CUSTOMER_SEGMENT 1740 4.26 338 4.23  By statistical methods, we observed that missing values mainly exists in seven attributes in the modeling dataset1, as well as the prediction dataset, as shown in Table 1. The left column is the attributes with missing values and two columns in the middle and the two columns in the right are counts and proportions of samples with missing values in corresponding attributes for modeling dataset and prediction dataset separately. Considering specialization of the seven attributes, we use corresponding filling methods for different attributes. By the data dictionary, we find that there are close relationships among the attribute TOTAL_NBR_CREDIT_CARDS and the five attributes related to credit cards (i.e., AMEX_CARD, DINERS_CARD, VISA_CARD, MASTER_CARD and RETAIL_ CARDS). There may be some cases in the following, which cause missing values in the attributes. Some customers might only mark corresponding attribute with ?Y? in the five credit cards when he or she had some kind of card, while did not mark the attribute with ?N? in the five credit cards when he or she did not have such kind of card. In order to keep the attribute TOTAL_NBR_CREDIT_CARDS consistent with the attributes related to five cards, we propose the method, as shown in Table 2, to deal with missing values in the five attributes. To make the best use of samples in the modeling dataset, Group Mean Imputation [4] has been adopted to fill missing values in the two attributes DISP_INCOME_CODE and CUSTOMER_SEGMENT. We also use Group Mean to transform nominal attributes to numerical attributes, and finally we rescale all the attribute values of the modeling dataset and the prediction dataset to the interval [0, 1].

Table 2. Method to fill missing values in the  five attributes related to credit cards  1 Besides, there is only one missing value in the attribute CHQ_ACCT_IND and  ANNUAL_INCOME_RANGE in modeling dataset separately. We fill the two missing values manually on the  base of Group Mean.

For  each sample of modeling dataset and prediction dataset  do  if  TOTAL_NBR_CREDIT_CARDS.value >= The number of ?Y? in these five cards  then  complete the missing value in the five cards with char ?N?  else replace TOTAL_NBR_CREDIT_CARDS.value with the number of ?Y? in these five cards  complete the missing values in the five cards with char ?N?  end for  4. Solution   We propose an ensemble method to solve the cross- selling task. The pseudo-code of the solution is shown in Table 3, where the sub-algorithms SMOTE [5] and Bootstrap [6] are shown in Tables 4 and 5, respectively.

Table 3. Ensemble method to the cross-  selling task   Ensemble of base learners Input: D:  Modeling dataset  L:  Number of base learners Process: 1. +D  ?  the positive subset(i.e. home loan  customers) of D 2. ?D  ?  the negative subset(i.e. credit card  customers ) of D 3. +SMOTED  ? SMOTE( NKD ,,  + )  4. for  i ? {1,?, L } do 5.   ?iD  ? Bootstrap( ND ,  ? )  6.    +? ?? SMOTEi i DDD  7.   Training base learneri with iD  on the base of  AUC 8. end for  9. ? =  = L  i xSVMixv  )()(  Output ( )(xh : a score of sample x ): 1. if )(xv >= Threshold then  2.   ? =  += )(   )( )(  1)( xv  j j xyprobabilitxv  xh  (base learnerj 1)( =x )  3. else  4.   ? =  += )(   )( )(  1)( xv  j j xyprobabilitxv  xh  (base learnerj 1)( ?=x )   Re-sampling   As analyzed in section 2, the concerned task is high class-imbalanced. In the modeling dataset, the number of samples of the negative class (i.e., credit card customers class) is about 57 times more than that of the positive class (i.e., home loan customers class). In this case, standard machine learning algorithms tend to be overwhelmed by the majority class and ignore the minority class since traditional classifiers seeking an accurate performance over a full range of instances. To deal with such class-imbalance, we re-sample the modeling dataset by combining over-sampling and under-sampling methods. First, we over-sample the positive class to a moderate extent, then under-sample negative class to the similar size [7].

SMOTE [12] as an over-sampling method, has been shown better than random over-sampling with replacement, for it can avoid over-fitting. In the solution, we adopt SMOTE to over-sample the positive class, as shown in the step 3 of Table 3. And we set the number of nearest neighbors K for each a sample in the positive class to be 2. That?s we want to over-sample the positive class from 700 samples to 2100 samples.

The detail process of the SMOTE algorithm is shown in Table 4. Readers who are familiar with it can skip it over.

Table 4. The SMOTE algorithm   SMOTE Input:   +D : the positive subset of D  K :  number of nearest neighbors for a sample in +D  N :  number of samples in +D Process: 1. erpolatedD int  ? null 2. for i?{1,?, N } do 3.   { iK  i xx ?,1 } ?  the K nearest-neighbor instances of the i-th instance ix in  +D 4.     for ijx ?{  i K  i xx ?,1 }do 5.   ijD  ?  positive samples randomly     interpolated between ix  and ijx  6.    erpolatedD int  ? ij erpolated DD ?int  7.    end for 8. end for Output: (positive set after interpolated)  erpolatedDD int?+  In order to keep over-sampled positive class and  under-sampled negative class with similar size, that is to get a balanced distribution, we propose Bootstrap to under-sample the negative class, as shown in the step 5 of Table 3. In particular, we set the size of the negative class N to be 2100 after the bootstrap process. The detail process of the Bootstrap algorithm is shown in Table 5.

Table 5. The Bootstrap algorithm   Bootstrap Input:  ?D ?the negative subset of D N :  size of the positive class after Bootstrapped  Process: 1. for i ?{1,?, N } do 2.   x  ? randomly choose a sample from ?D  with replacement 3.   'D ? }{' xD ? 4. end for Output: (negative set after Bootstrapped) 'D   In the step 6 of Table 3, we unite size-matched over-  sampled positive class with L  under-sampled negative classes individually as training datasets for base learners.

4.2. Base Learners Selection   SVM (support vector machine) has established itself  as a successful approach for various machine learning tasks. One of its advantages is that it maximizes the margin between two classes, Besides, SVM enable a considerably easier parameterization when compared to other learning machines like for example multi-layer perception neural networks [8]. So we select SVM [10, 11] as base learner, as shown in the step 7 of Table 3.

As analyzed in section 2, the concerned task is also cost-sensitive, while the relative misclassification costs function are unknown. To deal with cost-sensitivity in the task we adjust the cost parameter C of SVM internally [8]. In the process of training SVM, we  select RBF (radial base function) [10, 11] as the kernel function of SVM.

In the proposed method, we set the number of base learners L  (i.e. the number of SVMs) to be 20. Note that in the first for loop (i.e. the steps 4 to 8) of Table 3, we obtain L  different under-sampled negative classes through L  iterations. Farther, we obtain L different training sets by uniting the L  under-sampled negative classes with the same over-sampled positive class. Then, in the step 7 of Table 3, we train L  SVMs on the L  training datasets.

4.3. Parameters selection   The preprocessed modeling dataset is treated as  testing dataset when selecting parameters for SVM and the threshold of ensemble. We use the area under the ROC curve (AUC) for evaluation, as is adopted by the organizing committee when evaluating submitted entries in PAKDD Competition 2007.

In order to get better performance for base learners, we test the two parameters C and ?  for the first base learner (i.e., SVM1), where C varies from 1 to 1001 step by 50 and ?  varies from 10 to 30 step by 0.5, as shown in Fig.1.

Fig.2 shows that there is a climax when the parameter C is at 5 regardless of the parameter? , and that the AUC value will rise quite gently certainly when the threshold of the AUC value is 0.98. So we train SVM1 when C equals 5 and ?  equals 21. Optimal parameters for the other nineteen support vector machines can be obtained in the same way when the threshold of the AUC value is 0.98.

Fig.1. Relation between the AUC value and the parameters for SVM1     Fig.2. Feature of the relation  4.4. Ensemble   Ensemble learning as one of the most research directions in machine learning has received much attention [13-15]. In the step 9 of Table 3, we improve prediction performance with an ensemble of base learners. The difficulty of scoring, as analyzed in section 2, is tackled in the segment. In ensemble, we classify a sample by threshold voting and then compute its score on average according to the classification result. The detail process is as follows.

When scoring, we adjust the threshold variable v .

For a sample, if it is classified to the positive class by no less base learners than v , we will classify it to the positive class, and its score is the average of probabilities2 of the base learners who classify it to the positive class; Otherwise, it will be classified to the negative class, and its score is the average of positive probabilities of base learners who classify it to negative class.

Fig.3. Relation between the threshold v and accuracy  Fig.3 reflects that when v  is 15, the true positive fraction achieves the highest value, and the false negative achieves a higher value. Considering producing a low score for a potential home loan customer is with a higher cost than producing a high  2 In LIB_SVM, SVM can provide two probabilities for each sample: one is the probability of the sample  classified to the positive class, and the other is that of the sample classified to the negative class. The sum of  the two probabilities is 1.

score for a credit card customer, as analyzed in section 2, we prefer to misclassify negative samples other than positives when we must make a choice alternatively, so we set the threshold v  to be 15. The accuracies of the two classes for different thresholds are showed in Fig.3.

5. Evaluation   Prediction results have been reported on dataset of real world provided by PAKDD competition 2007 with AUC as the evaluation metric3. Some representative modeling algorithms and their AUC values are shown in shown in Table 6. It is shown that our solution is effective with its AUC value 60.73%. It is evident that ?TreeNet + Logistic Regression? and ?MLP + n-Tuple Classifier? are much better than the other algorithms.

The grand champion team of the competition propose ?TreeNet + Logistic Regression? to solve the cross- selling task with the AUC value is 70.01%. And our solution works better than ?Neural Network?, ?Decision Tree? and ?Predicted Apriori Association Rules + Nearest Neighbors?.

Table 6. Predicting results of some  representative algorithms   Algorithms AUC TreeNet + Logistic Regression 70.01% MLP + n-Tuple Classifier 69.62% Our solution 60.73% Structural Resonance in Multi-dimensional Data 56.35%  Neural Network 55.96% Random Forest 55.16% Decision Tree 55.53% Tree Boosting 52.42% Customer Growth Model Clustering + Support Vector Machine 52.24%  Predicted Apriori Association Rules + Nearest Neighbors 50.06%   6. Conclusion   The solution to cross-selling problems proposed in the paper is an ensemble method based on majority voting, whose essence can be interpreted as follows.

Since we train twenty different base learners with various training datasets, we can get twenty groups of different support vectors, each of which establish a   3 The competition result can be seen from http://lamda.nju.edu.cn/conf/pakdd07/dmc07/results.htm, and the  ID of our team is P060.

classification hyperplane in a 40-dimension space 4 .

Ensembling base learners with the threshold v equals 15 means combining at least 15 hyperplanes to separate a group of subspaces for positive class from the 40-dimension space. These combined hyperplanes in the 40-dimension space works as a piecewise linear classifier in a 2-dimension space in some sense. The only difference is these combined hyper-planes are not static, but dynamic when classifying a given sample.

Formally, let NLLL ,,, 21  represent the N hyper- planes. For a sample x, if x in the subspace vLLL ??? 21 , where ?jL  },,{ 21 NLLL and thresholdpredefinedv ?? , our algorithm will classify it to the positive class; Otherwise, it will be classified to the negative class.

In the paper, we proposes an ensemble method to solve cross-selling problems. The task is of class imbalance and cost-sensitivity. We combine over- sampling and under-sampling methods to solve the class imbalance problem, and solve cost-sensitivity by adjusting the parameters of base learners. At last, we adopt majority voting to get an ensemble of base learners. Experiment on prediction dataset provided by PAKDD Competition 2007 shows our solution is effective and efficient.

Though we adopt SVM as the base learner and it has been shown effective, there may be more suitable algorithms to solve cross-selling problems, since our solution is a wrapper ensemble method. So exploring more effective and efficient methods for the task seems to be interesting. Besides majority voting, more advanced ensemble strategies are anticipated to get a better performance. Under a deeper analysis to the dataset, it is found that there are several redundant and irrelevant variables in cross-selling datasets, so effective variables selection will improve performance of the solution proposed in this paper. Since the task is to predict a score for each customer, logic regression and learning to rank methods can be anticipated to exhibit better performances in the task, and it is implied from the competition result in some sense. In the view of decision making, learning to rank methods may be a better solution compared to classification and regression methods, since the decision makers have more freedom to the quantity or the proportion of cross-selling customers according to their ranks.

7. References  [1] S.B. Li, B.H. Sun and R. Wilcox, ?Cross-selling sequentially ordered products: an application to consumer   4 The modeling dataset is 40 dimensions, so it is a 40-dimension space.

banking services?, Journal of Marketing Research 42(2), 2004, pp.233-239.

[2] R.C.W. Wong, A.W.C. Fu, K. Wang, ?Data mining for inventory item selection with cross-selling considerations?, Data Mining and Knowledge Discovery, 11(1), 2005, pp.81- 112.

[3] B. Bhasker, H.H. Park, J. Park and H.S. Kim, ?Product recommendations for cross-selling in electronic business?, In Proceedings of the Nineteenth Australian Joint Conference on Artificial Intelligence (AUS-AI?06), Lecture Notes in Computer Science (LNCS) 4304, Springer, 2006 pp. 1042- 1047.

[4] J. Scheffer, ?Dealing with missing data?, Research Leters of  Information and  Mathematical Science, vol.3, 2002, pp.153-160.

[5] R. Akbani, S. Kwek, and N. Japkowicz, ?Applying support vector machines to imbalanced datasets?, In Proceedings of the Fifteenth European Conference on Machine Learning (ECML2004), Lecture Notes in Artificial Intelligence, pp.39-50.

[6] Davison and D. V. Hinkley, ?Bootstrap Methods and Their Application?, Cambridge University Press A.C.,1997.

[7] S. Lessmann, ?Solving imbalanced classification Problems with Support Vector Machines?, In Proceedings of (ICAI?04), pp.214-220.

[8] S. F. Crone, S. Lessmann, and R. Stahlbock, ?Empirical Comparison and Evaluation of Classifier Performance for Data Mining in Customer Relationship Management?, In Proceedings of the IEEE 2004 International Joint Conference on Neural Networks (IJCNN2004),2004, pp.443- 448.

[9] The LAMDAer Team, ?Predicting Future Customers via Ensembling Gradually Expanded Trees?, presented at PAKDD?06 Competition, 2006.

[10] Z.Q. Bian and X.G. Zhang, ?Pattern Recognition?, Tsinghua Publish House, Beijing, 2000, pp.299-303. (in Chinese) [11] Y. Xiao and C.Z. Han, ?Method of Reducing False Positive Alerts Based on Support Vector Machine in Intrusion Detection?, Computer Engineering, Vol.32 No.17 2006, pp.25-27.

[12] N. V. Chawla, L. O. Hall, K. W. Bowyer, and W. P.

Kegelmeyer, ?SMOTE: Synthetic Minority Oversampling Technique?, Journal of Artificial Intelligence Research, 16, 2002, pp. 321-357.

[13] Z.-H. Zhou and W. Tang, ?Clusterer ensemble?.

Knowledge-Based Systems 19 (1), 2006,  pp.77-83.

[14] H. Lappalainen, J. W. Miskin, ?Ensemble Learning?, Advances in Independent Component Analysis, M. Girolami, Ed., Berlin, Springer Verlag Scientific Publishers, 2000, pp.75-92.

[15] A. Krogh, P. Sollich, "Statistical mechanics of ensemble learning", Physical Review E,  55(1),1997, pp.811-825.

