A Parameterised Algorithm for Mining Association Rules

Abstract  A central part  of m a n y  algorithms f o r  mining asso- ciation rules in large data sets is a procedure that f inds so called frequent i temsets.  Th i s  paper proposes a new approach to  finding frequent i temsets .  T h e  approach reduces a number of passes through a n  inpu t  data set and generalises a number of strategies proposed so far .

The idea is to  analyse a variable number n of i temset lattice levels in p scans through a n  input  data set. It is shown that f o r  certain values of parameters ( n , p )  this method provides more flexible uti l isation of fas t  access transient memory  and faster elimination of i temsets with low support factor.  The  paper presents the results of experiments conducted to  f ind how performance of association rule mining algorithm depends o n  the val- ues of parameters ( n , p ) .

Keywords: Data Mining, Association Rules, Fre- quent Itemsets, Algorithms  1 Introduction  The algorithms for mining association rules in large data sets attracted a lot of attention in the recent years. The original problem [l] was to find the corre- lations among the sales of different products from the analysis of large set of supermarket data. Association rule is an implication that determines co-occurrence of the objects in a large set of so called transactions, e.g. customer baskets, collections of measurements, etc. At present, the research works on association rules are motivated by an extensive range of application ar- eas such as banking, manufacturing, health care, and telecommunications. Association rule discovery tech- niques are used to  detect suspicious credit card transac- tions, money-laundering activities [9] in banking and fi- nancial businesses. The same techniques are applied in manufacturing, controlling, and scheduling of technical  production processes [5].  The other application areas include health care [7] and management of telecommu- nication networks [6].

The discovery of association rules is typically done in two steps [l]. Analysis of experimental data performed in the first step provides a minimal set of objects ( i tem- sets)  such that frequency of their co-occurrence is above a given threshold ( m i n i m u m  support). These itemsets are called as frequent itemsets. The second step uses the frequent itemsets to  construct the association rules.

I t  has been shown that computational complexity of the problem is buried in the searching for a minimal set of frequent itemsets in the first step. Generation of association rules from frequent itemsets has a linear complexity and it has no impact on the overall perfor- mance.

A number of algorithms finding frequent itemsets in large data sets have been already proposed. Ma- jority of them counts one category of itemsets, e.g.

all IC element itemsets in one pass through an input data set. For instance, Apriori algorithm [2] counts n element itemsets in the n-th pass through a data set. All frequent itemsets identified in the n-th pass are used to generate the hypothetically frequent item- sets (candidate i temsets)  for verification in the next pass. Frequent itemsets obtained from the n-th pass and being the subsets of frequent itemsets identified in the next pass are pruned. The process continues until no new frequent itemsets are found. Sampling for frequent itemsets algorithm [lo] extracts a random sample from a data set and finds all frequent itemsets there. Next, it tries to verify the results on a complete data set. A top-down approach [ll] applies the max- imum clique generation algorithm to find a ceil ing of the minimal set of frequent itemsets. Next, the sub- sets of all frequent itemsets included in a ceiling are counted in each pass through a data set. DIC algo- rithm [3] stops counting itemsets as soon as there is no chance for an itemset to  be frequent. Each elim-  1530-0919/01 $10.00 0 2001 IEEE    inated itemset is immediately replaced with another itemset. A new technique recently proposed in [4] uses FP-tree to store compressed crucial information about frequent itemsets. This technique needs a huge volume of transient memory if a number of frequent itemsets is too large. Partition algorithm [8] transforms an input data set from a horizontal layout to a vertical layout and uses a list intersection technique to identify the frequent itemsets.

An approach presented in this paper considers a hy- pothetical perfect algorithm capable of guessing and verifying all frequent itemsets in one scan through an input data set. An input to the perfect algorithm is a set of frequent and non-frequent itemsets called as a perfect guess. A perfect guess includes both frequent and non-frequent itemsets because for each frequent itemset found we have to show that none of its su- persets is frequent. For example, if a set of all items is {A,  B ,  C} and {A} ,  { B } ,  {C}, { A ,  B }  are frequent itemsets then to verify that { { A ,  B} ,  {C}} is the mini- mal set of frequent itemsets we have to check in a data set that {A ,B} ,  {C} are frequent and that {A ,C} , { B ,  C} are not frequent. As the result a perfect guess consists of the candidate itemsets from many levels of itemset lattice. The quality of association rule min- ing algorithms is determined by two factors. The first one is a number of passes through an input data set.

The other one is a number of comparisons of candidate itemsets with input transactions in order to find which candidate itemsets should be counted. The perfect al- gorithm minimises both parameters. It needs to read an input data set only once and it needs to perform the smallest number of comparisons to verify a perfect guess. For instance, elimination of any candidate item- set in order to reduce a number of comparisons results with a different solution.

We are aware that implementation of the perfect al- gorithm is unrealistic because probability of making a correct guess in a large data set is very low. Our idea is to treat a concept of perfect algorithm in a way sim- ilar to how a concept of "absolute zero temperature" is treated in physics. It is going to be the ultimate goal, i.e. a point which cannot be achieved and in the same moment a point that can be used to measure the quality of the realistic algorithms.

One of the objectives is to construct an algorithm that makes a good guess, i.e. a guess that is not per- fect and in the same moment it does not contain too many errors. To make a good guess we need to get some information about the properties of an input data set. It leads to a strategy where a data set is read once, the statistics are collected and used to guess all 2 , 3 , .  . . , n  - th element candidate itemsets. Next, a  guessed set of candidate itemsets is minimised and ex- tended by a minimal set of non-frequent itemsets that have to be tested to prove its correctness. At the end an input data set is scanned for the second time to verify a guess. Due to a fact that initial guess is not perfect some of the items that suppose to be frequent appear not to be frequent and vice versa. A set of mis- takes detected during verification is used to generate a new set of candidate itemsets that should be verified again. The third scan through a data sets eliminates all mistakes and provides the final solution for a range of 2 , 3 , .  . . ,n - th element itemsets. Then, the same procedure is repeated for the next range of itemsets.

To implement such an algorithm we need a procedure capable of guessing frequent itemsets from the statis- tics collected in the first scan of input data set. To our best knowledge none of the algorithms proposed so far has such properties.

A problem with the approach sketched above is that we make more errors in guessing of itemsets that con- tain more items. This is because the errors done at the lower levels of itemset lattice multiple themselves very fast at the higher levels. A number of error has an im- portant impact on performance because each of them requires the additional comparisons of candidate item- sets with transactions from an input data set. These observations lead to a parameterised version of the al- gorithm. In order to decrease a number of errors at the higher levels, we parameterise a range of itemsets for which a guess is done. On the other hand, smaller guessing range increases a number of passes through an input data set. The parameterised ( n , p )  algorithm finds all frequent itemsets from a range of n levels in itemset lattice in p passes (n >= p )  through an input data set. A classical Apriori algorithm is a special case of ( n , p )  algorithm where n = p = 1, i.e. the candidate itemsets from one level of itemset lattice are verified in one pass. An interesting question is what combina- tions of n and p values provide the best performance.

Intuitions are such that as a ratio n / p  increases we have to perform more unnecessary comparisons of candidate itemsets with transactions from an input data set. On the other hand, if ratio n / p  decreases then we perform less unnecessary comparisons and in the same moment we read an input data set more frequently.

The rest of this paper is organised as follows. A detail of (n, p )  algorithm, including guessing, verifying procedures, and an example, is given in Section 2. Ex- periments of ( n , p )  algorithm is demonstrated in Sec- tion 3. A summary and a discussion of future research are provided in Section 4.

2 Finding frequent itemsets  This section presents a parameterised (n, p )  algo- rithm for mining frequent itemsets. It also contains the description of guessing and verification of candi- date itemsets.

2.1 Problem description  Let I = { i l , i 2 ,  . . . , im} be a set of literals, called i t e m s .  Let D be a set of transactions, where each transaction t E D consists of transaction identifier t i d and set of items It I .  We assume that the items are kept ordered within each transaction. We call an itemset that contains k items as k-itemset.

Association rule is an expression X j Y where X, Y are itemsets and X ,  Y C I and X fl Y = 0. The support for an itemset is defined as a fraction of all transactions that includes X U Y .  The confidence of a rule X + Y is defined as (X U Y ) / X .  We accept a rule X + Y as true if its confidence exceeds a given threshold value.

A candidate i temset is an itemset selected for veri- fication of its support in a data set. An itemset is a positive candidate i temset  when it is assumed (guessed) to be frequent. Otherwise, it is called as a negative candidate i temset.  Both positive and negative candi- date itemsets are verified in single pass through a data set. A candidate itemset becomes a frequent i temset when verification shows its support level above a given threshold value. A remaining candidate i temsets is can- didates verified in another scan.

In the rest of the paper candidate k-itemsets are de- noted by C k ,  positive (negative) candidate k-itemsets are denoted C z  (C;), remaining candidates are de- noted by C i ,  and frequent k-itemsets are denoted by L k .

2.2 The algorithm  The algorithm starts from an initial pass through an input data set in order to find all frequent 1-itemsets (L1) and to  collect the statistics of the total number of 1-, 2-, . . . , n-element transactions that contain the elements from L1. The statistics are stored in table T, e.g. see Table 1. Then, the initial value of current level k is set to 2 and initial result is set to L1. If Lk-l is not empty, a procedure guess-candidates is called to guess the candidate itemsets from the next n levels. The procedure returns a set C of positive and negative candidate itemsets. The elements of C are verified in an input data set by a procedure verify-candidates. The procedure finds all errors  done by guess-candidates in one pass through an input data set. Then, it corrects the errors and finds the solution for levels from IC to k + n - 1 in the second pass through the data set. A minimal set of frequent frequent itemsets found is added to the result set.

The value of k is then increased by n. These steps are repeated until L ( k P l )  is empty. A pseudo-code of the algorithm is given below  n := number of lattice levels traversed at  a time; sup := minimum support; Results := 0; generate L1; generate statistics table T ; Result := Result U L 1 ; k := 2; while Lk-1  # 0 do  guess-candidates(T, L k - 1 ,  t f ,  t t ,  n, k, C); verify-candidates(C, sup, n, k); Result := Result U {Lk,  Lk+i,. . . , Lk+n-l}; k := k + n;  end;  2.3 Guessing candidate itemsets  The procedure guess-candidates finds all candi- date itemsets from a range of levels from k to  k + n - 1 that accordingly to our guessing method would ver- ify as frequent itemsets. The procedure takes on input statistics table T ,  frequency thresholds ( t p ) ,  m-element transaction threshold ( t t ) ,  set L k - 1  of frequent item- sets, level k it starts from, and number n of levels to be considered.

Guessing starts at level k .  The procedure uses apri- ori-gen function proposed in [2]  to  generate a set Ck of candidate k-itemsets from L k - 1 .  Then, it uses the statistics from table T to decide which candidate item- sets in c k  are positive (cz) and which one are negative (CL). A frequency threshold is applied to all transac- tions that consists of k or more elements. The output of this step is a set of single items whose frequencies satisfy the frequency threshold. If any itemsets in Ck are subset of the output set, then we put them into a set of positive candidate k-itemsets. We repeat this step until it reaches transaction length m. Finally, if there are any k-itemsets in C k  which are not in a set of positive candidate k-itemsets then they are appended to a set of negative candidate k-itemsets.

In the next step apriori-gen is applied to  (7: to form set of c ( k + 1 ) .  This time we consider from (k+l)- element to m-element transactions. The sets of Cgtl) and CG,,), are then generated. Next, C$+,) is used to  form C ( k + 2 ) .  This procedure is repeated until we     reach level ( k  + n - 1). Finally, all subsets of itemsets in C&+n-l) at lower levels are pruned.

For example, assume that procedure guess-candidates is called with the following parameters: item frequency threshold equals to SO%, m-element transaction threshold equals to five (5-element transaction), number of levels to traverse equals to three, starting level equals to  two, and table statistics table T as follows.

Item freq. according to  tr. length  3 els. I 4 els. I 5 els. 1 Total freq ~ B C  4 2 3 9 3 2 3 9  D E F  1 1 1 3 3 1 3 7 1 0 3 4  Suppose we are at level k ,  and the apriori-gen function, generated Cr, = { A B ,  AC, AD, A E ,  AF, BC, B D ,  B E ,  BF,CD, CE,CF,  D E ,  DF, E F }  As there is no k-element transactions in table 1, we consider (k+l)-element transaction. 80% of the total number of (k+l)-transactions, i.e. five, is four. The output set of single items whose frequencies satisfy the frequency threshold is { B } .  Consequently, there is no itemsets in Ck which is subset of this set. Next, applying the frequency threshold to transaction length 4, and this time the output set is { A B C } .  As there are some itemsets in Ck are subset of { A B C } ,  they are put into a set of positive candidates. We repeat this step in the transaction length 5 .  Finally, set of Cz and C i  are as follows: C t  = {AB, AC, A E ,  AF, BC, B E ,  BF, CE ,  CF, E F } C i  = {AD,  BD,  CD,  D E ,  D F }  no. of m-els trs.

We use two thresholds to guess the candidate item- sets: item?s frequency threshold and m-element trans- action threshold. The accuracy of candidate guessing is determined by both of them. If a value of item?s frequency threshold is high then accuracy of candidate guessing will be high as well. However, we will get less frequent itemsets from the first scan because we have less positive candidates. Consequently, the ex- tra database passes may be needed to determine large number of remaining candidate itemsets. On the other hand, if the value of item frequency threshold is low, we have too many errors. In addition, the higher value m-element transaction threshold is, more errors of can-  5 2 3 10  didate itemsets are generated.

2.4 Verification of candidate itemsets  Verification of candidate itemsets includes verifica- tion of candidates provided by guessxandidates pro- cedure and elimination of errors done at the guessing stage. The procedure verifyxandidates takes on in- put a set C with positive and negative candidate item- sets, minimum support (sup) ,  starting level ( I C ) ,  and number of lattice levels traversed (n).

In the first stage, the procedure scans an input data set and finds all positive candidate itemsets which ap- pear to be negative and vice versa. Due to errors in guessing it has to  construct a new set of candidate item- sets and verify them once more. If certain Cj? appears t o  be not frequent then all its subsets from levels k to  ( I C  + j - 1) are generated. ?Then, they are trimmed by supersets which appear to be frequent. Similarly, if certain CjT appears t o  be not frequent then all its supersets from levels j + 1 to IC + n - 1 are generated and trimmed by the verified frequent itemsets. In the next stage, the confirmation procedure scans an input set for the second time and verifies the final solution.

Although the (n, p )  algorithm moves n levels at a time, the total number of candidate itemsets is more or less the same as other algorithms moving level by level. It is because itemsets in lower levels will not be subsets of any candidates in higher levels, both in positive and negative candidate itemsets.

2.5 Example  This subsection describes a sample execution of ( n , p )  algorithm for n = 3, p = 2 , and the statistics given in Table 1. Suppose that frequency threshold t f  = SO%, m-element transaction threshold tt = 5, and the sets of positive and negative candidates at level 2 are: C$ = {AB AC AE AF BC BE: BF CE CF EF} CT = {AD BD CD DE DF}  Using only set of C$ and apply the thresholds to Table 1, set of C$ and C; are as follows: C: = {ABC ABE ABF ACE ACF AEF BCE BCF BEF CEF} c,- = {>  c,- = {I  The procedure is repeated a.t level 4.

C z  = {ABCE ABCF ABEF ACEF BCEF}  Set of final positive and negative candidate itemsets after pruning all subsets of positive superset are: C$ = {ABCE ABCF ABEF ACEF BCEF} c: = c,+- = {}     I Parameters I no. database scans I  t l   CT = {AD BD CD DE DF} c, = c,- = {}  The database are scanned to verify these candidate itemsets. With 20% of minimum support, partial fre- quent 2-, 3-, 4-itemsets are as follows: Lz = {BD CD DE}  L4 = {ABCE ABCF ABEF ACEF BCEF} L3 = {I  Then set of remaining candidate itemsets are gener- ated,  C f  = {BCD BDE CDE} Cf = {BCDE BCDF}  Verifying sets of remaining candidate by scanning the database, frequent 2-, 3-, and 4-itemsets are gener- ated.

c2? =  Lz = {AB AC AE AF BC BD BE BF CD CE CF DE  L3 = {ABC ABE ABF ACE ACF AEF BCD BCE  L4 = {ABCE ABCF ABEF ACEF BCEF}  EF}  BCF BDE BEF CEF}  By using frequent 4-itemsets, candidate itemsets of another three levels are formed. As there is only one 5-itemset, there is no need to form sets of candidate 6-, 7-itemsets.

Cs = {ABCEF}  Scanning the database, frequent 5-itemsets are, fi- nally, determined.

L5 = {ABCEF}  (n,  2) 3 5 7 8 10 12 np sup Apr  10 20 9 6 4 4 3 -  -  3 Experimental Results  L  12 10 20 1 1 7 4 4 4  3 - 14 10 20 1 3 7 6 4 4 4  3 2 0 1 0 0  10 8 5 4 3 -  - -  To assess the performance of ( n , p )  algorithm, we conducted several experiments on different data sets.

The algorithm was implemented in C language and we tested it on Unix platform. The experiments used the synthetic data sets generated by IBM?s synthetic data generator from Quest project. We considered the following parameters: number of transactions in a database (ntrans), average transaction length (tl), number of patterns (np), and a minimum support  We have tried a range of number of transactions, average transaction length, and a number of patterns.

As we expected, the results show that for n # 1 and p # 1 our approach provides better results than Apriori algorithm (n = 1 and p = 1) both in terms of execution time and the total number of database scans.

(SUP).

between Apriori and (n, p )  algorithm  Table 2 shows a number of database scans of ( n , p ) algorithm compared with Apriori in different distribu- tions of data sets.

1 - 800 != E  s  YI   100 1,000 10,000 50,000 100,000 150,000 200,000  ntrans  Figure 1. Performance of Apriori and (n,p) with tk10 np=lO sup=20%  Figure 1 presents the results for different numbers of transactions and fixed number of candidate item- sets are. We compared Apriori with ( n , p )  algorithm by moving several levels in two passes. With small size of databases, the performance of Apriori and ( n , p )  al- gorithm is approximately the same. When an input data set is larger, the performance of ( n , p )  algorithm is much better than Apriori. It is because a number of database scans of ( n , p )  algorithm is less than in Apri- ori. In addition, three to four levels are the optimal movings which are the best performance of this data set. We also conducted the other experiments with dif- ferent data distributions, as shown in Figures 2 and 3.

When the data distribution is more scattered, the exe- cution time of ( n , p )  algorithm is not much different to Apriori. It is because both algorithms have to deter- mine many candidate itemsets which are not frequent.

Figure 4 shows the performance of ( n , p )  algorithm with the increasing of the ratio of n l p .  We parame-     1 14000 i=  - k! 12000 g 10000 3 8000 #     100 1,000 10,000 50,000 100,000 150,000 200,000  ntrans  Figure 2. Performance of Apriori and (n,p) al- gorithm with t k l 4  np=lO sup=20%  terised the algorithm by moving one level in one pass of data set and moving more than one levels in three passes. It showed that when a ratio increases, the per- formance decreases due to  the itemset guessing with more elements, which resulted in getting more errors.

Finally, we illustrated performance of ( n , p )  algo- rithm by varying number of database passes ( p )  and fixing number of levels moving a time (n  = 8), as shown in Figure 5 ,  to confirm that we should not move too many levels in a few database scans, as well as should not move one level in one database pass.

4 Summary and future works  This work proposes a new approach to  finding fre- quent itemsets in mining association rules. The im- portant contribution of our method is the reduction of number of scans through a data set. The main idea of our new algorithm are to  guess candidate itemsets in each level of itemset lattice starting from level k up to IC + n - 1 and to verify such candidate itemsets. To have a good guess, some statistical data from input data are corrected during the database is scanned. By using such information, the candidate itemsets are gen- erated. Next, these candidate itemsets are verified by scanning the database. If there are some errors from the guessing, another scan through a database will be needed to  eliminate such errors and produce the final solution of frequent itemsets. Experiments based on different data sets have been conducted to evaluate per- formance of the algorithm.

r" 400 I/{ 2oo 0 I  2000 10,om 50,000 100,000 150,000 200,000  ntrans  Figure 3. Performance of Apriori and (n,p) al- gorithm with tk20 np=100 sup=lO%  As the central point of the algorithm is precise guess- ing of candidate itemsets the future works include sig- nificant improvements in collecting statistics and ac- curacy of guessing. It is necessary to measure what are the costs of getting more complex statistics in the first pass through an input data set and what benefits may be achieved from such statistics in the remaining part of the algorithm. It is also necessary to  improve the internal data structures of the algorithm in order to  eliminate an impact of inefficient searching methods on the overall performance.

