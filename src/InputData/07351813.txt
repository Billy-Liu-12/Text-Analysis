Review of Frequent Itemsets Mining in High Dimensional Dataset

Abstract? Nowadays, there are abundant of big data collection and to understand its patterns would need a thorough analysis.

Analyzing big data would depend highly on the purpose and the tasks involved would be various. One of the significant tasks is frequent itemsets mining and the strategy has been evolved in many ways in order to improve the efficiency and effectiveness of the mining process. In this paper, we briefly reviewed mining frequent itemsets algorithms from year 1998 until year 2013 that focus on maximal and closed frequent itemsets. We discussed these algorithms based on three main areas namely: the searching strategy, space reduction method, and data representation. These three main areas are concluded as the optimization strategy and are designed to improve the efficiency and scalability using a different approach in different areas to adapt to numerous growth of the dataset.

This work is beneficial for researchers in designing and enhancing the algorithm based on their own purposes.

Keywords- Data mining; high-dimensional dataset; frequent itemsets

I.  INTRODUCTION Mining frequent itemsets is the basic process in association rule mining [1]. The aim is to discover the frequent patterns that exist with a frequency not less than the minimum support of thresholds determined by a user [1-5].

Mining frequent pattern can be represented in several structures such as substructure, graph and itemset. Apriori and Frequent Pattern-Tree (FP-Tree) are considered the classic algorithms of frequent itemset mining. Till date, there are numerous algorithms that have been proposed in discovering the frequent itemsets, and they are usually constructed based on these two classical algorithms. The target is to have better efficiency and scalable mining algorithm of useful patterns in discovering knowledge.

The need for analyzing the high-dimensional (big data) data has been asserted since the emerging of various new application domains [6]. One of the tasks in data mining is to mine frequent itemsets. Frequent itemsets is a group of items in the dataset that occur frequently with the given thresholds of the minimum support. Most algorithms are Apriori variants, which enumerate all possible frequent itemsets [7].

In a dataset, it contains 2n (where n is the number of items in the dataset) of all possible subsets of frequent itemsets to be mine. In a real world domain for instance, gene expression studies, network intrusion, web content and usage mining contain typical long itemsets [7]. For example, if a dataset  contains five items, it generates 32 (25) subset of frequent patterns where it still looks effortless to compute.  However, in real-world application, a dataset might contain 40 items or even more than that, and these will generate 1,099,511,627,776 (240) subsets which obviously giving a computational problem. Furthermore, full set of frequent itemsets contains redundant information that causes a large number of frequent patterns [4].

In order to understand and facilitate other researchers, we reviewed several algorithms and propose a table of summarization on these algorithms. This review paper will discuss the available algorithms that have been developed for frequent itemsets mining. In section 2, we describe on big data and the effects of these criteria in mining frequent itemsets. In section 3, we present the algorithms and in section 4, the analysis of the optimization strategy is discussed. Finally, we conclude with the summary of this review paper in section 5.



II. BIG DATA AND ITS IMPACT ON ALGORITHMS Lately, we have been introduced by many active researches to the term of ?Big Data?. Big Data is described as the very huge data collections [8] and based on the three V?s: data volume, data velocity, and data variety and these three V?s based have been discussed in [9]. The massive huge volume of data is obviously giving a problem in analyzing the data. If our objective is to mine the frequent itemsets, then the increment of data size is affecting the efficiency in mining task. Big Data are characterized by two factors, namely, high-dimensionality and large sample size [8].

These characteristics are referred to data volume where the numbers of features (dimensionality) and number of records (rows) can massively expand to high volume depends on the applications. When the dataset has thousands of dimensions or samples, it will create a great challenge to developer in producing a good algorithm to analyze the data. For example, the searching method in Big Data is time consuming and when we used conventional method, it might not be practically efficient in high volume of data.

According to [6] and [10] there are two challenges in mining the big data. The first challenge is the curse of dimensionality, when the dimensionality becomes large; the volume of the search space is increasing. Due to this problem, the existing data representation is inefficient and many not applicable to work with a high dimensional space or not. How the algorithm managed to load these huge of   DOI 10.1109/ICAIET.2014.19     dimension into the memory for mining process is questionable [11]. Another issue from this challenge is the processing time since the computational time increases as dimension increases. The main memory consumptions can affect the algorithm?s efficiency [2], which means if the large dataset are mining, it requires more memory space. The second challenges that may arise is the specificity of similarity between points in high dimensional space. We haven?t looked into the second challenge because it is more related to the clustering issue and beyond our scope. From the first challenge, it has driven many researchers to come out with novel mining algorithm to handle these dataset characteristic problems since the traditional algorithm becomes incompetent.



III. FREQUENT ITEMSETS ALGORITHMS Frequent itemsets are discovered by determining the minimum support value and occurred in investigating dataset. It is part of association rules mining [12], which involves two main tasks: 1. Find all frequent itemsets in the dataset.

2. Each itemset found, generate all association rules with confidence equal or larger than minimum_confidence that has been determined.

Currently, there are two scopes to mine these frequent itemsets problems instead mining all possible frequent itemsets. The first scope is by mining only the maximal frequent itemsets. Maximal frequent itesets are frequent pattern that has no frequent superset [7, 13]. The second problem scope is to mine closed frequent patterns. Closed frequent patterns is a frequent pattern that has no superset with the same support [7, 13]. The idea to mine the closed and maximal frequent itemsets is to speed up the searching of all frequent itemsets, without to enumerate all possible frequent itemsets. Example of closed and maximal frequent itemsets is, let?s say a transaction dataset has only two transactions :{( a, b, c, d, e, f, g); (a, b, c, d)}. Then, we set the minimum support thresholds = 1. We find two closed frequent itemsets (CFI) and its frequency = {(a, b, c, d, e, f, g):1; (a, b, c, d):2}. Therefore, in maximal frequent itemsets there is only 1 frequent itemset MFI = {(a, b, c, d, e, f, g):1}.

Itemsets {a, b, c, d} is not included because it has frequent superset {a, b, c, d, e, f, g}.

With these two problems scopes introduced, [14] has divided frequent itemsets algorithms into three categories, which are 1) mining frequent itemsets, 2) mining maximal frequent itemsets, and 3) mining closed frequent itemsets. In this paper, we briefly review the evolution of closed frequent itemsets and maximal frequent itemsets within 16 years starting year 1998 until year 2013 by focusing on large scale dataset. We did not claim this work as a complete review rather it is a partial review that analysed the work that related to our interest.

A. Maximal Frequent Itemsets Algorithms One of the earliest algorithms that used mine maximal frequent set is Pincer-Search [15]. Pincer-Search is a combination of bottom-up and top-down search directions.

The authors introduced a novel of auxiliary data structure  called maximum frequent candidate set (or MFCS) in order to create new efficient top-down searching. The algorithm showed a significant result in reducing the number of times to read the database and number of times to enumerate the candidates. However, there is still a slight problem in discovering maximal frequent set when maximal frequent itemsets are distributed in a scattered manner.

MaxMiner [16] is another algorithm that mines only the maximal frequent itemset. By mining maximal frequent itemset, we can find out all frequent itemset from the maximal frequent itemset since all frequent itemset is a subset of maximal frequent itemset [7]. The success of this algorithm is by evading a strict bottom-up traversal of the search space and uses the heuristic to search long frequent itemsets. To work out a good lower-bound on the number of transactions contain in the itemset, MaxMiner used a technique to identify the new candidate itemsets before accessing the database by retrieving the stored information that has been collected from the database [16].

Similar to MaxMiner, MAFIA [17] mines maximal frequent itemsets by adaping the vertical bitmap data representation for counting and pruning mechanisms in searching the itemsets lattice. The algorithm implements a pruning technique to boost the efficiency of mining maximal frequent itemsets.  Three pruning techniques were introduced which are parent equivalence pruning (PEP), frequent head union tail (FHUT) and HUTMFI. The analysis has shown, pruning techniques applied to the algorithm such as PEP and FHUT were very useful in reducing the search space.

MAFIA can also generate frequent itemsets and closed frequent itemsets.

SmartMiner has been introduced by [18] to find the exact maximal frequent itemset for large datasets.

SmartMiner uses tail information similar to depth-first search strategy. Nevertheless, it works a bit different from dynamic reordering depth-first search strategies (DFS) whereby the SmartMiner will only create a node if the prior node is checked. Moreover, SmartMiner uses heuristic select function where the tail information and the frequency of the each item as their condition. With these additional features, SmartMiner creates less subtrees compare with dynamic DFS and skip the superset checking by passing the tail information.

FPMax algorithm [19] is a variation of the FP-growth, which aims to find maximum length frequent itemsets.

FPMax uses a global data structure called Maximal Frequent Item set tree (MFI-tree), to keep track the frequent pattern and apply the novel Lattice Graph maximum length itemsets to prune the search space more efficiently.

GenMax [20] is the algorithm that used backtracking search to enumerate all maximal pattern expeditiously. The algorithm applied the novel progressive focusing technique to eradicate non-maximal pattern and diffuse propagation to boost up the frequent checking. This progressive focusing technique narrows the search space to only the most related maximal itemsets by adding the check_status flag where superset checking is performed only if the flag is true. The idea behind diffset propagation is to avoid from storing all each tidset into combination set.

LFIMiner is an algorithm that mines the maximal frequent itemsets [21]. Searching optimization such as Conditional Pattern base Pruning (CPP), Frequent Item Pruning (FIP) and Dynamic Reordering (DR) are implemented to make the pruning search space efficient in LFIMiner. To reduce the TP-tree size, DR is used by keeping more frequent item closer to the root while to make the pruning search space efficiently CPP and FIP method are used.

B. Closed Frequent Itemsets Algorithms Mining closed frequent itemsets looks more sensible, and we can still generate all the frequent itemsets together with their support values [13]. The early closed frequent pattern algorithm that has been proposed unable to deal with a dataset that contain a high number of items (example: 10,000 or more items) [13]. Lee [22] has categorized the closed frequent itemsets mining algorithms into four categories which were test-and-generate, divide-and- conquer, hybrid, and hybrid-without-duplication.

A-Close [12] is an algorithm in mining frequent itemsets based on pruning closed itemset lattices concept. The main idea of this algorithm is to reduce the search space and memory consumption by implementing closed itemset lattices that mining closed frequent itemsets instead of mining all frequent itemsets [23].

CHARM [24] is another algorithm to find the closed frequent itemsets. It is different from the other mining algorithms which focus on itemset search space.  CHARM explores both itemset space and transaction space concurrently. The novel searching method gives CHARM the ability to skip levels to find the closed frequent itemsets.

Furthermore, by using union and intersection of two itemsets space and transaction space, CHARM is able to identify where the itemsets is located and does not require the internal data structure like Hash-trees or Tries.

Closet [25] is an algorithm to discover a closed frequent itemsets that applied three main techniques which are applying compress frequent pattern in the FP-tree structure for mining closed itemsets without candidate generation, developing single prefix path compression technique to identify frequent closed itemset faster and exploring a partition-based projection mechanism for scalable mining in large databases. Wang et al. [26] has proposed Closet+ which is an enhancement of Close that mines only closed frequent itemsets. Closet+ uses FP-tree to represent the frequent pattern and search it by Depth-First-Search (DFS) strategy.

The Carpenter has proposed by [27], and the design is to mine closed frequent itemsets in biological dataset such as microarray. Experiments have shown the Carpenter outperforms its competitor such as CHARM and the CLOSET which was tested on biological dataset. The Carpenter used a novel search strategy where it enumerates frequent pattern using row rather than by column enumeration. It consists of two main parts which are transposing the data into table and creating row enumeration tree search.

COBBLER [28] is an algorithm that mined closed frequent itemsets targeted for dense and parse dataset. The  COBBLER is a combination of row enumeration and column enumeration searching strategies. It is designed to dynamically switch between row and column enumeration while mining the closed frequent itemsets.  The idea of these algorithms is to mine closed frequent itemsets for a dataset that have different characteristics. Row enumeration is used for dense dataset that have a large number of columns and a few rows while column enumeration is for fewer numbers of columns and huge amount of rows. TFP [29] are closed frequent itemsets mining algorithm that mines base on top-k mining. Instead of mining frequent itemsets that above the min_support thresholds, it uses top-k frequent itemsets with a minimum length min_l, where k is a user defined value.

TD-Close [30] is an algorithm that mined closed frequent itemsets for high dimensional dataset which has a small number of rows and high numbers of columns. This algorithm used novel row enumeration tree which is more suitable for high dimensional dataset. The beauty of the TD- Close is by using the novel top-down search strategy where it speed up the frequent closed itemset searching and reducing the memory usage.

PGMiner [31] is an algorithm that mined closed frequent itemsets based on two key features: prefix graph and pruning strategy (inter-node and intra-node pruning) in order to reduce the search space. Analysis has shown, PGMiner uses low memory usage even at the low support thresholds.

PTClose or Patricia Tree Close [1] uses the PTArray Technique to reduce the tree scanning. Patricia Tree structure is a compact representation of the information frequency in the dataset. By using this data representation, PTClose algorithm required less memory and time consumption.

ICMiner [22] is an algorithm to find closed frequent itemsets by mining closed inter-transaction itemsets. The algorithm implements a novel data structure called itemset- dataset pair (ID-pair) where the aim is to store information in inter-transaction dataset tree or known as the ID-tree.

TTD-Close [32] is an algorithm that mined closed frequent itemsets based on top-down searching strategy. The key feature of this algorithm is the row enumeration tree in top-down approach that minimizes the number of scanning.

Additionally, TTD-Close implements a pruning technique called closeness-checking and other several pruning strategies targeting to reduce the search space. TTD-Close is aiming to mine in a very high dimensional dataset.

CFIM-P [33] is an algorithm that mined closed frequent itemsets, targeted to overcome the drawback of the FP-tree.

CFIM-P maximizes the mining closed frequent pattern by eliminating the redundant patterns and minimizing the depth of the tree without losing important information. This algorithm works in three stages.  In the first stage, it eliminates the null transactions to reduce the processing time. In the second stage, it generates closed frequent itemsets and lists all the relevant itemsets. Lastly, all the frequent itemsets are formed based on the closed frequent itemsets that has been discovered.

Different with previous algorithms, Moment [39] uses to mine closed frequent itemsets and claimed as the first that mine over data stream sliding windows. The authors highlighted new data structure, closed enumeration tree     TABLE I.  EVOLUTION SEARCHING STRATEGY  M ax  M in  er (B  ay ar  do ,   )  Pi nc  er -S  ea rc  h (L  in , 1  8)    A -C  lo se  d (P  as qu  ie r,   )  C H  A R  M (Z  ak i,   )  C lo  se t (  Pe i,   )  M A  FI A  (B ur  di ck  , 2  1)   Sm ar  tM in  er (Z  ou , 2  2)    FP M  ax (G  ra hn  e,  3)    C lo  se t+  (W an  g,  3)    C ar  pe nt  er (P  an , 2  3)    C ob  bl er  (P an  , 2  4)   G en  M ax  (G ou  da , 2  5)    TF P  (W an  g,  5)    M om  en t (  C hi  , 2  6)   TD -C  lo se  d (L  iu , 2  6)    PG M  in er  (M oo  ne si  ng he  , 2  6)   PT -C  lo se  d (N  ez ha  d,  7)    IC M  in er  (L ee  , 2  8)   LF IM  in er  (H u,   8)   TT D  -C lo  se d  (L iu  , 2  9)   PC P-  M in  er (L  ee , 2  0)    C FI  M -P  (2  1)   A -N  ew M  om en  t ( M  in , 2  3)    data representation        ? ?   ?           ?   ? ? ?   ? ?   ?  search strategy ? ?   ?     ?   ? ? ? ? ?  ?         ?   ?  space reduction method  ? ? ?   ? ? ? ? ? ?     ?   ? ?   ? ?       ?  (CET). The novel data structure is used to store all closed frequent itemsets in the current sliding window. Because of the CET, the Moment algorithm create the number of nodes less than total number closed frequent itemsets.

A-NewMoment [38] is an algorithm that used to mine closed frequent itemsets in data streams. The A-NewMoment introduced novel combinative data structure called bit-vector dictionary frequent itemsets list or known as BV-DFIlist which consist of bit-vector array, dictionary list and hash table. This algorithm is aimed to improve the time-efficiency problem while mining closed frequent itemsets in data streams. Using the novel data structure BV-DFIlist, this algorithm however required more memory space in order to have fast computational time. A-NewMoment algorithms creates smaller search space compared to Moment after applying its new data structure strategy. Next, we will discuss the analysis of features of each algorithm and optimization strategy.



IV. ANALYSIS AND DISCUSSSION Finding frequent itemsets is a crucial task in several  application fields such as in rules discovery, market basket analysis, collaborative filtering, etc. Numerous algorithms have been proposed to mine frequent itemsets efficiently.

Many approaches have been proposed in literatures with the aim to optimize these frequent itemsets mining algorithms such as minimizing the memory cost by using compression data representation (FP-tree) and reducing search space by applying the pruning method (inter-node and intra-node pruning)[31]. Based on these literatures, we concluded that there are three main approaches that were commonly discussed, namely searching strategy, space reduction method, and data representation.  Table I summarizes the approaches for both closed and maximal frequent itemsets.

The next subsections discussed each of these approaches.

A. Searching strategy.

Majority researchers are trying to improve the frequent pattern mining by introducing a novel search stragegy where we defined method as searching of candidates or without candidates of frequent itemsets. Various tree search strategies have been applied into the algorithms such as column enumeration, row enumeration, top-down traversal, bottom-up traversal, breadth-first-search and depth-first- search. For instance CLOSET+ that uses hybrid tree projection automatically build tree either bottom-up tree for dense dataset or top-down tree for the parse dataset [26].

Novel row enumeration has been verified by Mutalib [40] where searching frequent itemsets in very high dimensional dataset where the numbers of columns are larger than the number of rows work efficiently compared to columns enumaration. While the SmartMiner made an additional feature by introducing tail information to guide the depth- first-search (DFS) searching.

Obviously, the depth-first-search is mostly applied in developing mining frequent itemsets algorithms. In breadth- first-search, the searching was done by visiting all nodes in each level of the tree and made a new dataset scanning to count the k-itemsets support. Different from the depth-first- search that search in depth-first order, it will check the current node (k itemset) whether it is frequent or not, if the node is frequent, it will traverse to the node?s subtree (k+1 itemset). Due to this multiple dataset scanning by breath- first-search, depth-first-search is more suitable in mining long patterns itemsets [26, 36].

B. Space reduction method.

Based on our review, pruning technique is used to reduce the search space reduction where it is an important feature in frequent itemsets mining algorithm to overcome the efficiency problem. Taking the advantage of the pruning power, we could summarize that all suggested algorithms     used pruning techniques to reduce the search space and the generated candidates, are also to speed up the searching itemsets. We also noticed that most of the algorithms use the novel pruning technique such as item_skiping in the CLOSET+, MinePattern used by Carpenter, set-enumeration pruning applied in MaxMiner and many more. Pruning is a technique used to remove unnecessary branch from different data representation, either tree or lattice. The power of pruning is for improving the frequent itemsets searching even though the dataset becomes complex by reducing the search space and ignoring the unnecessary itemsets.

C. Data representation.

When the dataset grows exponentially, the size of the data becomes a problem and memory cost is increasing and this will lead to another problem, which is memory size.

There are limited algorithms that are focusing on the problem. FPMax for example utilized the maximal frequent itemsets tree (MFI-tree) to keep track all maximal frequent itemsets. Another example is the implementation ID-pair and ID-tree in ICMiner that can accelerate the speed in the mining process. We noticed that many of these novel algorithms enhance the data representation since it gives high impact in efficiency of searching frequent itemsets. The existing algorithm nowadays are using frequent pattern tree (FP-tree) structure, ventricle TID-list, and vertical bit vectors as data representation [31]. By creating a new data compression and representation such as vertical bit-vector, vertical representation, MFI-tree (modification of the FP- tree) is looking for another approach to cater the memory problem. Moreover, A-NewMoment showed the contradictory relation between time efficiency and memory usage where it uses more memory space to perform the computational to get batter computational time. There are two key features for an efficient maximal frequent itemsets algorithm which is the pruning technique and the representation of the data so it can perform fast computational [19]. The MAFIA uses vertical bitmaps to compress the data representation. Each bitmap represents an itemset in the dataset.  For example, if we have an itemset of {A, B}, it simply performs AND operation on all of the bits in the bitmaps for A and B to check either itemset {A, B} exist in corresponding transaction or not.

There are two types of formats in recording the transaction data. The first format is called horizontal data format where the items are recorded into particular transaction-id and the second format is the vertical data format where transaction-id is recorded into particular items.

Both data formats have their own advantages. Using vertical format, the frequent itemsets can be counted using transaction-id intersection compare to horizontal format that used complex internal data structure like candidate generation [37].

Horizontal format also looks relevant if the suitable compressed structure is applied in the FP-tree, which is not used so much memory space since the itemsets have common prefix that sharing the same path. Since Pasquier [12] introduced subset lattice, most of the proposed algorithms nowadays used tree-base structure in their  algorithm implementation.  Based on our analysis, PGMiner [31] applied different approach for its compression and representation in mining closed frequent itemsets known as PrefixGraph.  This lead to a new exploration strategy in improving the efficiency of the algorithms and also in improving the scalability to mine the large dataset, by reducing memory usage or reducing computational time in mining frequent itemsets (all itemsets, closed itemsets and maximal itemsets). In summary, a good representation structure and compression method are crucial for an efficient algorithm beside the pruning technique used.



V. CONCLUSION In this paper, we briefly described the key features of  frequent item mining algorithms focusing on maximal and closed frequent itemsets.  We then identified key features into three approaches, which are searching strategy, space reduction method and data representation/compression. We believed that, these three approaches influence the design of the algorithm strategy. From our observation, all the reviewed algorithms were designed to improve the efficiency and scalability of the algorithm by using different approaches to accommodate the growth and the complexity of datasets.

We strongly believe that understanding the dataset characteristic also is the key in having better algorithms. In future, we plan to propose an algorithm in mining frequent itemsets following these particular approaches.

