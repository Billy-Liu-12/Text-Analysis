Automatic Technical Term Extraction Based on Term Association

Abstract   This paper proposes a new automatic Chinese term extracting algorithm combining both statistics-based and rule-based methods. This algorithm firstly uses a statistical method to extract two-word candidates from raw corpus, and then extends these candidates forward to obtain multi-word candidate terms. We propose a new metric named Term Association (TA) that can measure the combining degree between words in a string very well. In the second subsystem it filters these candidates to get domain-specific technical terms based on defined rules. Our purpose is to achieve a higher precision of the domain-specific Chinese term extraction task by the?hybrid method than the previous approaches. This algorithm implements an extractor with an unprocessed corpus as input for technical papers of ethanol fuels. The results of experiments are analyzed and evaluated, and the precision and recall are 84.26% and 63.86% respectively.

? 1. Introduction   Automatic term extraction has been a critical problem in natural language processing (NLP), especially in Chinese and multiple language processing.

Unlike English, Chinese language does not have explicit word boundaries in written sentences. Automatic term extraction from Chinese texts is quite difficult especially for unknown words and compound key words, such as names, locations, translated terms, technical terms, abbreviations etc [1, 2]. Therefore, researchers in many countries have explored a number of new algorithms and techniques to solve the problems of automatic term acquisition in Chinese information retrieval (IR) and NLP.

At present, there are two main methods for detecting unknown key words or terms: statistical and rule-based approaches. Statistical approaches gather statistical features such as the frequency of words and their co-occurrence. Previous researchers focus on the use of one or two of these features [8]. For example, Su (1994) and Zhang (2000) used mutual information (MI) to measure the association of character strings which compose a term or a candidate [9, 10]. Wu (2002) used another feature: the likelihood score [11] that represents,  given a word as well as its part-of-speech tag and length, how likely a character appears in certain position within the word. Other features include in-word probability [12], context dependency [13], and relative frequency [14]. They are almost domain-independent, although they require a large amount of training data, which should sometimes be prepared manually.

Rule-based approaches detect unknown words by using a dictionary and heuristic rules for forming words.

These type of methods need to make amount of templates for complex term structures, so that they are not easily adapted to other domains. It is neither possible for a dictionary to contain all the words in Chinese nor to specify all the rules for word formation.

However, for a domain-specific term extraction task, rule-based approaches are quite effective in a certain degree, and we do take this method in our research work.

We think either statistical or rule-based methods have advantages and disadvantages and in many cases can complement each other. So we introduced both of these two approaches and proposed an approach which combined the two types of methods together. The hybrid approaches have been applied in English term extraction systems already and get well results. But for Chinese, most automatic term selection methods are statistics-based. Du (2005) [3] designed a multi-strategy based term extracting algorithm with a hybrid method and improved the statistical algorithms.

Our strategy is to extract unknown compound key words from text documents with Chinese included and to select the highly domain-specific compound words to be technical terms by rules. During the statistics part, we proposed an extraction algorithm based on a new statistical metric called Term Association (TA) which measured the combined degree of two neighbor strings.

2. Overview of the Proposed Approach   The approach proposed in this paper is a four-step process. Each step can be seen as a dependent module.

The corresponding flowchart of this automatic process is given by Figure.1.

Firstly, word segmentation by using the backward-maximum-method and part of speech (POS) tagging was carried out on the collected texts. And for   DOI 10.1109/FSKD.2008.40    DOI 10.1109/FSKD.2008.40     higher precision of our method, we filtered a few special characters and symbols that are rarely used.

Since the traditional N-gram language model is not a meaningful unit in linguistics, N-gram-based indexing tends to cause inconsistencies between training data and testing data when a term appears in the training data but not in the testing data [4]. So for the efficient search and convenient string update, in the second step, we built PAT-tree as the data structure for the corpus storage.

Thirdly, a statistical method was used to extract the candidate terms and we got a candidate list after the extraction.

In the last step, final terms were detected by the term filter using the rules we had defined. And the last two steps will be further discussed in Section 3.

In addition, experimental results and their evaluation will be given in Section 5. Finally, some further study and work we can do in the coming future will be presented in Section 6.

Figure 1. The process of our term extraction approach   3. Automatic Term Extraction Approach  Based on PAT-tree  3.1. Corpus Storage in PAT-tree Structure ?  PAT-tree is an efficient data structure for indexing a string of any length in the corpus and locating every possible position of a prefix in this string. It was developed by Gonnet [5] from Morrison?s PATRICIA algorithm (Practical Algorithm to Retrieve Information Coded in Alphanumeric) [6]. The PAT-tree is conceptually equivalent to compressed digital search tree but smaller. The superior feature of the PAT-tree data structure is most resulted from the use of so-called semi-infinite strings [7] in storing the substring values in the nodes of the PAT-tree. Distinguished with N-gram language model, the searched strings in the  PAT-tree do not restricted by the length. Using this data structure to index the full-text of documents, all possible character strings, including their frequencies in the documents, can be retrieved and updated in a very efficient way, yet not every character string with arbitrary length is needed to be stored. At present, this vary-gram language model is successfully used in the area of IR. There is an example in Figure. 2 which shows the semi-infinite strings generated for the Chinese character sting ????????????? ?? and its storage in the PAT-tree.

Data string:      PAT-tree:   Figure 2. An example of the Chinese PAT-tree   Basically, although PAT-tree is a very efficient data  structure to record all of the substrings of documents, it really demands large space overhead and takes time to build. In Section 4 and 5, some empirical results on the construction of PAT-tree will be reported.

3.2. Statistics-based Candidate Extraction   After data structure construction for the corpus, the further analysis is to choose reasonable statistical features to detect term candidate heuristically. Although the features proposed in Section 1 are all valuable for term extraction, only one feature used separately in the approach may be not quite satisfied in the value precision and recall. In order to make our approach more feasible, we proposed combining two complement features (MI and Log-Likelihood Ratio) into a new metric called Term Association (TA) and explore an estimation function to acquire term candidates. These candidates are ultimately validated as entries of term selection. All the statistical and linguistic information such as word?s frequency and character word number of corpus can be easily obtained from the PAT-tree we have just established.

3.2.1. Mutual Information (MI). The Mutual Information of two words(x and y) is defined as follows:  1 2 3 4 5 ?? ???? ?? ?? ???     ( , ), log ( ) ( ) P x yMI x y  P x P y ( ) =  (1),  where ( , )( , )  (* ) C x yP x y  C =  , ( ( ) )( ( ) )  (* ) C x yP x y  C =  ,  is the number of occurrence given the variable x as  is the number of occurrence  given the variable xy.  represents the total number of words in the corpus.

( )C x  ( , )C x y (*)C  Theoretically, MI measures the information that x and y share: it measures how much knowing one of these variables reduces our uncertainty about the other.

The larger MI of xy?s co-occurrence, the more chance it has to be a term candidate.

3.2.2. Log-Likelihood Ratio (logL). The logL of two words(x and y) is defined as follows:  1 2 1 2 1 2 1 1 2 2 1 1 2 2  1 2 1 2 1 2  log ( , ) ( , , ) ( , , ) ( , , ) ( , , )k k k k k kL x y l k n l k n l k n l k n n n n n n n  + + = + ? ?  + +  (2),  where , and ,  ,  ,  .

( , , ) log ( ) ( ) log (1 )l p k n k p n k p= + ? ?  1 ( , )k C x y= 1 ( , *)n C x= 2 ( , )k C x y= ? 2 ( ,*)n C x= ?  Mostly,  approximately equals with , so in this paper we use ,  , 1  in our calculation.

( ,*)C x ( )C x  1 ( ,*) ( )n C x C x= ?  2 1( , ) (*, ) ( , ) ( )k C x y C y C x y C y k= ? = ? ? ?  2 ( ,*) (*) ( ,*) (*)n C x C C x C n= ? = ? = ?   3.2.3. Term Association (TA). We have already observed that ranking the candidate terms merely by the two statistic measures mentioned before are not quite satisfied. So we use a new statistic called ?Term Association? (TA) that combined the two explored features into a new statistical estimation function to measure the strength of association between two neighbored character strings. For any two neighbored strings x and y in the corpus, their TA(x, y) can be calculated as follows:  0, ( , ) 0, ( , ) log( , ) log , ( , ) log  2 2  C x y cThread C x y cThread and L lThreadTA x y  MI L C x y cThread and L lThread  <? ?? ?     = ? ? + ?     >??  ?  (3),  where cThread and lThread are two constant threshold values chosen by experience.

With a threshold tThread of TA, we can determine whether two words can be connected as a new candidate.

If > tThread, x and y will form a new phrase xy as a candidate term.

( , )TA x y  The whole procedure of term candidates? extraction obeyed the forward-maximum-combination rule.

Pseudo codes of the TA calculation process can be shown in Figure 3. :   Figure 3. Pseudo codes of term TA calculation process   3.3. Rule-based Term Selection   However, not all the acquired candidates are the domain-specific terms we need after the above extraction algorithm. In some cases it may not work only based on the above TA value. Some compound candidates that have large TA values are not key words in the specific field. For instance, the new phrase ??? ????? is a candidate but not a term. Since the composed words ?????? and ???? co-occurred frequently that their TA value is larger than tThread.

While previous researches show that most terms have some linguistic and statistical features and they do help in term detecting. For instance, in technical documents, the majority of domain-specific terms are noun phrases or compound nouns consisting of a small number of single nouns. We then investigate several common patterns that can be used in term selection to improve the performance of our approach. In this case, we define four rules to filter unwanted candidate terms from the candidate list. With these rules and a well prepared stop-word list, our system can determine whether a character string is a term or not automatically.

Rule1: Terms are all nouns or noun phrases. A term can be formed as the lexical pattern of ((A|N)*(NP)*(A|N)*)N, where A is representation of adjective , N of noun and NP of noun phrase. (.)* in the rules represents that the pattern in the parentheses appears at least once.

Rule2: Candidates start or end with words and symbols in the stop-word list are not terms.

Rule3: Candidates appear in the corpus too frequently are not terms. If the number of a candidate?s occurrence is larger than a threshold (hThread), this candidate will be removed from the term list.

Rule4: Candidates only appear in few documents are not terms. If the document frequency (df) of a candidate term is smaller than a threshold (dThread), the candidate will be removed from the term list.

For easy understanding, these rules can be explained as follows:  For ,  c C L is t? ? (1) if c is not composed as the lexical pattern of  {((A|N)*(NP)*(A|N)*)N }, then ; c TList? (2) if c is composed as ?asb?, and a?STList or b?  STList , then ; c TList? (3) if C(c)> hThread, then c is not a term; (4) if D(c)<dThread, then c is not a term.

D(x) represents the document frequency (df) of x and the CList, TList and STlist are representation of candidate list, term list and stop-word list respectively.

4. Experiments   In order to evaluate the performance of our approach, an application of technical domain was proposed. The experiments on this application were done to extract terms from a set of technical papers in the field of ethanol fuels. The papers had been indexed manually first and a total of 285 terms extracted. Then we did three parallel experiments to compare our approach with the statistic-based method and Du?s extractor. The chosen thresholds are shown in Table 1.

Table 1. Thresholds chosen in the experiments  cThread 3 lThread 20  Thresholds used in the statistical part  tThread 30 hThread 15 Thresholds used in  our rules dThread 5  In statistics-based extractor, only the first three  thresholds were selected. And in Du?s extractor, only the first two parameters and hThread are used. All the application procedures were executed by JAVA programs automatically, and Table 2 shows the space and speed performance of our extractor ran on JAVA platform.

Table 2. The performance obtained in our experiment   The statistic-based extractor can be seen as a  sub-module of our extraction system, so the candidate terms we got can be taken as its final terms. Du?s  extactor combined MI and logL in a different way from us, and in the term selection part, it chose only one parameter to filter the candidates. In Section 5, we will compare the experiment results of the three extractors.

5.  Conclusion  5.1. Evaluation   Basically, it is not easy to evaluate the performance of any term extraction approaches. In this study manual work has been carried out in order to realize the performance. As the target documents have been manually extracted and we got a term list. We made a program to estimate whether the result term we got after the experiments and the one in the correct term list are match exactly or not. Table 3 gives the results of our evaluation task which compared with the performance of statistical method and Du?s term extractor.

Table 3. Some results obtained from the experiments   5.2. Results ?  The proposed PAT-tree does reduce the difficulty of term extraction in Chinese-included documents, which is fundamental to our task. Using this data structure all possible character strings can be easily retrieved or updated. The statistic-based filtering algorithm can be performed well on this condition also.

Under the help of the TA-based mixed approach, the application of technical paper term extraction got a good result with the precision of 84.26% and the recall of 63.86%. The precision is lager than the statistics-based methods (60.47%) and this is a significant improvement over the precision of 64.49% and the recall of 40.64% given by Du?s extractor [3].

6. Further Study   Term extraction can be used in many fields of NLP and IR. It is also valuable in concept acquisition during the knowledge base construction. We have planned to cluster terms after the extraction task and get abstract concepts for ontology building which is a critical step in the process of automatic knowledge base building.

Text size  1.93(MB) PAT-tree size  5.20(MB) Time of building PAT tree 5366610 (ms) Time of extracting candidate terms 7382938(ms) Time of selecting final terms 1889(ms) Number of extracted candidate terms 430 Number of selected final terms 215 Number of documents in the corpus 183  Statistic-based Extractor  Du?s Our Extractor Extractor  Number of Selected Terms 430 180 216  Number of Correct Terms Extracted 177 116 182  Obtained Precision 0.4116 0.6449 0.8426 Obtained Recall 0.6211 0.4064 0.6386     7. Acknowledgment   The research work in this paper is supported by the research Fund for PhD students, Ministry of Education (20060013007), Beijing Municipal Natural Science Foundation (4073037).

8. References  [1] Keh-Jiann Chen et al., ?Word Identification for Mandarin Chinese Sentences?, COLING?92.

[2] Lee-Feng Chien. ?PAT-Tree-Based Keyword Extraction for Chinese Information Retrieval,? Proceedings of SIGIR?97, 1997, pp. 50-58.

[3] Du B, Tian HF, Wang L, Lu RZ. ?Design of domain-specific term extractor based on multi-strategy?.

Computer Engineering, 2005, 31 (14), pp.159?160.

[4] Yu-Sheng Lai and Chung-Hsien Wu, ?Meaningful Term Extraction and Discriminative Term Selection in Text Categorization via Unknown-Word Methodology?, ACM Transactions on Asian Language Information Processing, Vol. 1, No. 1, March 2002, pp.34-63.

[5] Gaston H. Gonnet, Ricardo A. Baeza-yates and Tim Snider, ?New Indices for Text: Pat Trees and Pat Arrays?, Information Retrieval Data Structures & Algorithms, Prentice Hall, 1992, pp. 66-82.

[6] Morrison, D., ?PATRICIA: Practical Algorithm to Retrieve Information Coded in Alphanumeric?, JACM, 1968, pp. 514-534.

[7] Manber, U. and R. Baeza-Yates, ?An Algorithm for String Matching with a Sequence of Don?t Cares?, Information Processing Letters, 37, 1991, pp. 133-136.

[8] Hongqiao Li, Chang-Ning Huang, Jian-feng Gao and Xiao-zhong Fan, ?The Use of SVM for Chinese New Word Identification?. In IJCNLP-04. Sanya City, Hainan Island, China, March 22-24, 2004.

[9] Keh-Yih Su, Ming-Wen Wu, and Jing-Shin Chang, ?A Corpus-based Approach to Automatic Compound Extraction?, In Proceedings of ACL 94, 1994, pp.

242-247.

[10] Jian Zhang, Jian-feng Gao, and Ming Zhou, ?Extraction of Chinese Compound Words: An Experimental Study on a Very Large Corpus?, Proceedings of the Second Chinese Language Processing Workshop, 2000, pp. 132-139.

[11] Andi Wu. ?Chinese Word Segmentation in MSRNLP?. In   proceedings of the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan, July 11-12, 2003.

[12] Aitao Chen. ?Chinese Word Segmentation Using Minimal Linguistic Knowledge?. In proceedings of the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan, July 11-12, 2003.

[13] Sheng-fen Luo, Mao-song Sun. ?Two-Character Chinese Word Extraction Based on Hybrid of Internal and Contextual Measures?. In proceedings of the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan, July 11-12, 2003.

[14] T. H. Chiang, Y. C. Lin and K.Y. Su. ?Statistical models for word segmentation and unknown word resolution?. In proceedings of the 1992 R. O. C.

Computational Linguistics Conference, Taiwan, 1992, pp. 121-146.

