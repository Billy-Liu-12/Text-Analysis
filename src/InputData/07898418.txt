1545-5971 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

Abstract?Cloud computing provides individuals and enterprises massive computing power and scalable storage capacities to support a variety of big data applications in domains like health care and scientific research, therefore more and more data owners are involved to outsource their data on cloud servers for great convenience in data management and mining. However, data sets like health records in electronic documents usually contain sensitive information, which brings about privacy concerns if the documents are released or shared to partially untrusted third-parties in cloud. A practical and widely used technique for data privacy preservation is to encrypt data before outsourcing to the cloud servers, which however reduces the data utility and makes many traditional data analytic operators like keyword-based top-k document retrieval obsolete. In this paper, we investigate the multi-keyword top-k search problem for big data encryption against privacy breaches, and attempt to identify an efficient and secure solution to this problem. Specifically, for the privacy concern of query data, we construct a special tree-based index structure and design a random traversal algorithm, which makes even the same query to produce different visiting paths on the index, and can also maintain the accuracy of queries unchanged under stronger privacy. For improving the query efficiency, we propose a group multi-keyword top-k search scheme based on the idea of partition, where a group of tree-based indexes are constructed for all documents. Finally, we combine these methods together into an efficient and secure approach to address our proposed top-k similarity search. Extensive experimental results on real-life data sets demonstrate that our proposed approach can significantly improve the capability of defending the privacy breaches, the scalability and the time efficiency of query processing over the state-of-the-art methods.

Index Terms?Cloud computing, privacy preserving, data encryption, multi-keyword top-k search, random traversal  F  1 INTRODUCTION  C LOUD computing has emerged as a disruptive trend inboth IT industries and research communities recently, its salient characteristics like high scalability and pay-as- you-go fashion have enabled cloud consumers to purchase the powerful computing resources as services according to their actual requirements, such that cloud users have no longer need to worry about the wasting on computing resources and the complexity on hardware platform man- agement [1], [2]. Nowadays, more and more companies and individuals from a large number of big data applications have outsource their data and deploy their services into cloud servers for easy data management, efficient data mining and query processing tasks.

But when the companies and individuals enjoy these advantages in cloud computing, they also need to take the privacy concern of the outsourced data into account. Be- cause data sets in many applications often contain sensitive information like e-mails, electronic health records and finan- cial transaction records, when the data owner outsourcing such sensitive data to the cloud servers which are consid- ered to be partially trusted, the data can be easily accessed and analyzed by cloud service providers illegally. Since the analysis of these data sets may provide profound insights into a number of key areas in society (such as e-research, healthcare, medical and government services), thus data  Xiaofeng Ding, Peng Liu and Hai Jin are with the Services Computing Technology and System Lab, Cluster and Grid Computing Lab, the School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan 430074, China.

E-mail: {xfding, liup, hjin}@hust.edu.cn.

owners need effective, scalable and privacy-preserving ser- vices before releasing their data to the cloud.

Data encryption has been widely used for data privacy preservation in data sharing scenarios, it refers to mathe- matical calculation and algorithmic scheme that transform plaintext into cyphertext, which is a non-readable form to unauthorized parties. A variety of data encryption models have been proposed [3], [4], [5] and they are used to encrypt the data before outsourcing to the cloud servers. However, applying these approaches for data encryption usually cause tremendous cost in terms of data utility, which makes traditional data processing methods that are designed for plaintext data no longer work well over encrypted data.

The keyword-based search is such one widely used data operator in many database and information retrieval applications, and its traditional processing methods can- not be directly applied to encrypted data. Therefore, how to process such queries over encrypted data and at the same time guarantee data privacy becomes a hot research topic. Fortunately, many methodologies based on searchable encryption have been studied. For example, [6], [7], [8] deal with the single keyword search, and works [9], [10], [11], [12], [13] support the multi-keyword boolean search.

However, the single keyword search is not smart enough to support advanced queries and the boolean search is unre- alistic since it causes high communication cost. Therefore, more recent works like [14], [15], [16] focus on the multi- keyword ranked search, which is more practical in pay-as- you-go cloud paradigm. But most of these methods cannot meet the high search efficiency and the strong data security simultaneously, especially when applying them to big data encryption poses great scalability and efficiency challenges.

1545-5971 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TDSC.2017.2693969, IEEE Transactions on Dependable and Secure Computing   Motivated by this, in this paper, we focus on a special type of multi-keyword ranked search, namely the multi- keyword top-k search, which has been a very popular database operator in many important applications, and only needs to return the k documents with the highest relevance scores. For supporting multi-keyword search, we introduce the vector space model which represents documents and queries as vectors. In order to support top-k search, the relevance scores between documents and queries should be calculated, therefore, the TF?IDF (term frequency ? inverse document frequency) model is introduced as a weighting rule to compute the relevance scores for ranking purposes.

In addition, to improve the query efficiency for better user experiences, we propose a group multi-keyword top- k search scheme (GMTS), which is based on partition and supports top-k similarity search over encrypted data. In this scheme, the data owner divides the keywords in the dictio- nary (suppose that the dictionary contains all the keywords that could be extracted from all documents) into multiple groups and establishes a searchable index for each group.

On the other side, to better control the size of indexes, we adopt champion lists [17], [18] into our scheme, where the index of a keyword group only stores the top-ck documents of the corresponding keyword (the top-ck documents of a keyword represent the c ? k documents that have the highest relevance scores to this keyword, where c is a pos- itive integer). Furthermore, we propose a random traversal algorithm (RTRA) to strengthen the data security, where the data owner builds a binary tree as searchable index and assigns a random switch to each node, so the data user can assign a random key to each query. Therefore, the data user can change the results and visiting paths of queries by using different keys, which maintains high accuracy of queries.

Finally, we combine the GMTS and the RTRA together into an efficient and secure solution to our proposed problem.

Our contributions can be summarized as follows:  ? We first propose the random traversal algorithm which makes the cloud server randomly traverse on index and returns different results for the same query, and in the meantime, it maintains the accuracy of queries unchanged for higher security.

? Based on the random traversal algorithm, we present one both efficient and secure searchable encryption scheme, which can support top-k similarity search over encrypted data. In this scheme, the data owner can control the level of query unlinkability without sacrificing accuracy.

? Our experimental results show that our methods are more efficient than the state-of-the-art methods and can better protect data privacy. Especially, our proposed method has good scalability performance when dealing with large data sets.

The rest of this paper is organized as follows: In Section 2, we review and analyze the related works. Section 3 intro- duces the system model and threat model, the preliminaries and our design goals. The random traversal algorithm is introduced in Section 4. Section 5 describes the framework of the GMTS and describes the RGMTS framework in Section 6. Section 7 shows and analysis our experimental results. Section 8 concludes this paper.

2 RELATED WORK  Searchable encryption (SE) is a hot research field, especially with the emergence of cloud computing. In this section, we review and analyze the existing searchable encryption schemes. SE can be divided into public key searchable encryption [4], [9], [19], [20] and symmetric searchable en- cryption (SSE) [3] [7], [8] according to different cryptog- raphy primitives. In this paper, we focuses on the sym- metric searchable encryption because public key searchable encryption usually are computationally expensive [15], [21].

Abundant works [3], [7], [8], [22], [23] are proposed to deal with symmetric searchable encryption. Song et al. [6] first defined the problem of searching on encrypted data and proposed a symmetric searchable encryption scheme with linear complexity. After that, Goh et al. [7] formulated a security definition for SSE and proposed a secure index which is based on pseudo-random functions and Bloom filters, but the time cost of Goh?s scheme is O(n). Curtmola et al. [3] introduced two formal definitions of SSE and pro- posed a method which is based on inverted list to improve the query performance, their method is proved to be more efficient than other works. However, most of these works can only support single keyword boolean search, which is not advanced enough to support complex functionalities. In recent years, many works have been proposed to achieve different kinds of complex queries like similarity search, multi-keyword search, etc. In general, the literatures [22] and [24] used wildcard-based techniques, [25] based on Bed- tree and [26], [27], [28], [29] applied the locality sensitive hashing (LSH) to deal with similarity search. Works [10], [11], [12], [13] support multi-keyword boolean search, but boolean search is inefficient because it returns all the doc- uments that satisfy the query criteria. Hence, some recent works are proposed to deal with the bandwidth-saving multi-keyword ranked search [14], [15], [16], [23], [30].

Cao et al. [15] proposed the multi-keyword ranked search over encrypted data for the first time and built a searchable index based on the vector space model, and chosen ?coordinate matching? to measure the similarity between queries and documents. However, in their schemes, the time complexity of search is O(nm) (n is the number of keywords in dictionary, m is the size of the documents that stored in the cloud server), and the time complexity of trapdoor construction is also very high. Sun et al. [14] proposed a tree-based index structure which is based on the vector space model and the TF?IDF model. This structure achieves sub-linear time complexity, but it is vulnerable in protecting data privacy. One step further, Xia et al. [16] proposed a Greedy Depth-first Search tree-based searchable encryption scheme EDMRS, which achieved more efficiency than early works, but the cost of search remains high and the time complexity of creating trapdoor is high O(n2).

The works [14], [15], [16] add random numbers ?j in indexes or queries to disturb the relevance scores between queries and documents, and they claimed that the value of  ? ?j can be adjusted to control the level of query un-  linkability. But they can not protect the query unlinkability thoroughly, because in order to guarantee the accuracy of queries, the level of query unlinkability is usually get lim- ited. Actually, the cloud server can easily link two identical    1545-5971 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TDSC.2017.2693969, IEEE Transactions on Dependable and Secure Computing   Data Owner Data User  Encrypted  Index Trapdoor  ResultsEncrypted  Documents  Cloud Server  Secret keys  Fig. 1. The system model of searching over outsourced encrypted data.

queries by analyzing and comparing the results and visited paths. For instance, if the data user submits two identical queries to the cloud server, and sets the correct ratio to 80%.

Even though the relevance scores are disturbed by adding random numbers, it is expected that at least 60% of the results and visited paths are the same.

3 PROBLEM FORMULATION 3.1 System Model  As shown in Fig. 1, the system model we considered in this paper contains three parts: the data owner, the data user and the cloud server. The data owner uploads docu- ment collection D to the cloud server, but this collection may contain sensitive information. To protect data privacy, the data owner has to encrypt D before outsourcing it to the cloud server. Furthermore, in order to enable the cloud server to process query efficiently over the encrypted document collection C , the data owner constructs an en- crypted searchable index Ie locally. Finally, the data owner outsources both the encrypted document collection C and the encrypted searchable index Ie to cloud, and shares the secret key of trapdoor generation and document decryption to authorized data users with secure channels.

When the data user wants to search with a query, s/he generates the trapdoor T for this query firstly by query encryption, and then submits the trapdoor to cloud server for query processing. After receiving T , the cloud server calculates the relevance scores between trapdoor T and the documents in index Ie, and returns k documents with the highest scores to the data user.

Note that, the search control is outside the scope of our paper. Therefore, similar to works [16], [22], [27], [31], [32], we assume data users are trusted entities and the trapdoors are generated by data users themselves.

3.2 Threat Model  In this paper, we treat data owner and data user as trusted entities, but cloud server is considered to be ?honest-but- curious? as adopted in most works on secure cloud data search. The server is honest as it runs the programs and algorithms correctly, it is curious since the cloud service providers can easily access and analyze the encrypted data, and even record queries to learn additional information.

Based on the information which can be learned by cloud sever, we consider two threat models as [15], [31].

Known Ciphertext Model. This threat model corre- sponds to the ciphertext-only attack, as the cloud server  TABLE 1 Notations  Notation Description D the plaintext document collection, denoted as  D = {D1, D2, ..., Dm}, Di is a document of D C the encrypted document collection, denoted as  C = {C1, C2, ..., Cm} W the dictionary which contains n keywords  which appeared in the document collection D, denoted as W = {w1, w2, ..., wn}  Wq the keyword set which is a subset of the dictio- nary W and contains t keywords that data users want to search  WG the keyword group, denoted as WG = {WG1,WG2, ...,WGb}, where each group WGi contains d keywords  b the number of groups in the keyword group WG, it means b = ceiling(n/d)  I the unencrypted form of searchable index Ie the encrypted form of the searchable index I Q the query which is constructed based on the  keyword set Wq T the trapdoor, which is the encrypted form of  query Q R?e the search results that the cloud server returns  to data users, denoted as R?e = {R1, R2, ..., Rk} Score(Q,Di) the relevance score between query Q and docu-  ment Di  only knows the encrypted document collection C, encrypted searchable index Ie and trapdoor T .

Known Background Model. Compared to the known ciphertext model, this model is more stronger, as the cloud server here not only knows the ciphertext of document collection, searchable index and query, it is supposed to have other background knowledge like statistic information about the document collection, which will expose more knowledge to cloud. For instance, when the cloud server know the normalized TF distributions of certain keywords, it can identify these keywords by comparing the normalized TF distributions [14], [15], [16], [30], [33].

3.3 Preliminaries 3.3.1 Multi-keyword top-k Search Let D be the plaintext document collection that the data owner will outsource to cloud servers, and Di represents a document in D. W is a dictionary and Score(Q,Di) is the relevance score between query Q and document Di (the mainly used notations in this paper are summarized in Table 1). The multi-keyword top-k search [17] is used to find the k documents with the highest relevance scores to query Q, the formal definition is given as follows: Definition 1. (Multi-keyword Top-k Search) Given a query  Q and a document collection D, find k documents R?e = {R1, R2, ..., Rk} in D with the highest relevance scores, this is to say, ?Di ? D/R?e, Score(Q,Di) ? Score(Q,Rj)(1 ? j ? k).

e.g., document collection D has four documents, as  shown in Fig. 2, where each document is represented as a vector and these vectors store the TF values of their corresponding keywords. For a query (0, 0.5, 0, 0, 0.1, 0.6), the relevance scores of the documents D1, D2, D3 and D4 are 0.33, 0.24, 0.12 and 0.63, respectively. It is obvious that    1545-5971 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TDSC.2017.2693969, IEEE Transactions on Dependable and Secure Computing   0.3  0.5  0.7  0  0.8  0D1  D2  D3  D4  0.6  0  0.2  0  0.6  0.3  0.2  0  0  0.1  0  0.2  0  0.4  0  0  0.7  0.6  Fig. 2. A document collection, where each document is represented as a vector.

documents D1 and D4 are the top 2 documents, as their scores are higher than others.

3.3.2 Vector Space Model and TF?IDF Vector space model [34] and TF?IDF [17] are widely used in information retrieval. In vector space model, each document and query is represented as a vector (but in this paper, each document and query is represented as a group of vectors), which supports multi-keyword search quite well.

TF?IDF is used as a weighting function to calculate the similarity between documents and queries. Assume wi is a keyword in the dictionary W , the term frequency (TF) of wi in a document Dj is denoted as TFi,j , which measures the weighting of keyword wi in document Dj . Inverse document frequency (IDF) is used to measure the overall importance of a keyword in the entire document collection, we use IDFi to represent the IDF of keyword wi. The relevance score between query Q and document Dj is:  Score(Q,Dj) =  |Dj | ? wi?Q  TFi,j ? IDFi (1)  Here TFi,j = 1 + ln fj,i, where fj,i is the number of times that the keyword wi occurs in document Dj . And IDFi = ln(1 + mmi ), where mi is the number of documents which contain the keyword wi. |Dj | denotes the Euclidean length of document Dj , and it can be calculated as:  |Dj | = ? ?  wi?Dj  (1 + ln fj,i)2  3.4 Design Goals  Our goals contain three aspects: 1) Supporting multi- keyword top-k similarity search over encrypted data; 2) Search with high efficiency; 3) Privacy-preserving. The de- tails are listed as below:  Multi-keyword top-k Search: To design a searchable encryption scheme that enables the cloud server to support multi-keyword top-k similarity search over encrypted data;  Search efficiency: Our scheme should be efficient in index construction, trapdoor generation and search process- ing, and it should be more efficient and effective than the state-of-the-art methods;  Privacy-preserving: Our scheme should protect the pri- vacy of indexes and queries at the same time. They are  ? Index security and Query security: The plaintext infor- mation of encrypted searchable index and trapdoor should be protected.

? Keywords Privacy: The cloud server cannot identify whether a certain keyword is contained in a query by analyzing indexes or search results.

r1  r3  N3 N4  r  NN   NN   rr   5 6  r  r22r  D1  43N1 N22  D2 D3 D3  Fig. 3. An example of tree-based index for document collection D = {D1, D2, D3}.

? Query Unlinkability and Access Pattern: The cloud server cannot distinguish whether two identical trap- doors are from the same query, which needs us to hide the visiting paths on index and access pattern of query, where access pattern represents the available information in search results [15].

4 THE RANDOM TRAVERSAL ALGORITHM Fig. 3 is a tree-based index, where N1, N2, N3, N4 are the leaf nodes. Both the stored value of nodes N3 and N4 are document D3. If the result of a query contains documents D1 and D3, then we have to path through edge 1 and 3 to visit D1. On the contrary, visiting D3 has two options: from edge 2 to 5 or from edge 2 to 6. Inspired by the above example, we propose a random traversal algorithm (RTRA).

In RTRA, giving two identical queries, their visiting paths in index and search results can be different, but maintain the accuracy of queries unchanged. The main idea is as follows: 1) enlarging the whole document collection E times, hence each document in result has E options; 2) assigning a switch to each document; 3) building a tree-based index for the whole document collection, where document identifiers are stored in leaf nodes. 4) assigning a random key to each query. Therefore, data users can control the visiting paths and search results by assigning different keys. Next, we further discuss the details of RTRA.

4.1 RTRA Framework 4.1.1 Enlarging Document Collection Firstly, the documents in collection D are randomly divided into L groups with the same size, and the divided document collection is represented as DG = {DG1, DG2, ..., DGL}.

Then, each document group is copied E times and each document is assigned with a unique document identifier.

We use DGx to represent the enlarged document collection, where DGx = {DG11, .., DGE1 , ..., DG1L, ..., DGEL} and DG  j i  represents j-th copy of document group DGi. After D is enlarged, each document has E copies and were distributed in different groups.

For example, assume L = 2, E = 2 and document collection D has four documents D = {D1, D2, D3, D4}.

We divide D into two groups DG1 = {D1, D2} and DG2 = {D3, D4}. After D is enlarged, we get  DGx = {DG11, DG21, DG12, DG22} = {{D11, D12}, {D22, D21}, {D13, D14}{D24, D23}} = {D11, D12, D22, D21, D13, D14, D24, D23}  Where D1i and D i are two file copies of document Di (1  ? i ? 4).

Notice 1). The order of documents in each group is  random (e.g., the order of copy DG11 is D 1 , D  2 , while in    1545-5971 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TDSC.2017.2693969, IEEE Transactions on Dependable and Secure Computing   DG21 is D 2 , D  1 instead of D  1 , D  2 ). The reason is that  if the order of two group copies are the same, visiting path in the low levels of index may be the same. 2). For disturbing the visiting path of query on index, the order of each group in DGx is still selected randomly, such as the order of DGx could be {DG11, DG22, DG12, DG21} instead of {DG11, DG21, DG12, DG22}.

4.1.2 Assigning Switch Each document of the enlarged document collection DGx is assigned a switch which is a vector with length r (where r = L?E). For switch formulation and describe conveniently, we give the definition of The Same Switch Form: Definition 2. (The Same Switch Form) Given two nodes N1  and N2, if not all bits of their switches are zero, and both N1.switch[i] and N2.switch[i] are equal to zero or bigger than zero at the same time (where i = 1, 2, .., r), we call the two switches have the same form and the two nodes have the same switch form.

If two documents belong to the same group and they will be assigned with the switches that have the same form, otherwise they are different. switchji represents the switch of these documents which belong to document group DGji and it is calculated as follows:  switchji [?] =  { 0 if ?! = i ? E + j ? + |Rand()|%? if ? = i ? E + j  (2)  Where ? = 1, 2, ..., r. ? and ? are two positive integers, and they are set to 5 and 10 in this paper. Rand() is a random number generator and |Rand()| is its absolute value.

4.1.3 Building Index We build a binary tree I for document collection DGx as index, where document identifiers are stored in leaf nodes.

Let N represents a node in I , and we denote its form as ?fid, lc, rc, switch?. If N is a leaf node, fid is the document identifier, lc and rc are null. Otherwise, fid is null, lc and rc point to its left and right child, respectively. If the children of node N have the same switch form, we add node N to the document group where its children belong to, furthermore, we calculate the switch of this node by Equation 2. Otherwise, all the bits of the switch are set to zero.

4.1.4 Assigning Keys For getting different visiting paths and search results when processing a query at two different periods, the query is assigned with a random key, where the key is a vector with the same length as switches and represented as key. When generating a key, data user selects one dimension from each E dimensions of key, and the selected dimensions are set to zero, while the others are set to different random negative integers. The reason is that each document has E copies, but when processing search, the cloud server only needs to traverse one copy. The traversed copy is controlled by the values of key, if key[i ? E + j] equals to zero then the documents in DGji will be traversed (where i in [1, L] and j in [0, E ? 1]). The value of key is defined as below:  key[?] =  { ?? ? |Rand()|%? if ?! = j ? E + ?j 0 if ? = j ? E + ?j  (3)  r1  r2 r3  r4 r5 r6 r7  D1 D2 D4 D3 D2 D1 D3  r  D  r  D   D   D  666 0000 000 0 666 00000 000 0 000 000 0000 5 000 0000 000 5 000 777 00000 0 000 77777 0000 0 0000 000 6666 0  0 0000 5 0000 0 0 000 000 0  D  00 6 000 999 0  D  00 0  000 000 666 0  0 0 0 0  0 000 0 0  00 0 0 0   3 4      D4   555 00  D    r  00 000 000 00    r 000 0   r  D     rr    D   D  00 00   r  D       r  r  0000 000 000 0   r  r  000 00   r  rr   1 1 2 2 2 2 1 1  (a)  r1  r2 r3  r4 r5 r6 r7  D1 D2 D4 D3 D2 D1 D3  r  D  666 000 0000 0 666 0000 000 0 0000 000 000 5 000 00000 0000 5 000 777 0000 0 000 7777 00000 0 0000 00000 666 0  0 000 5 0777 000 0 0 0 000 0 6 0 999 0  000 000 666 0  0 0 0 0  0 0 0 0  000 0 0  2 3  4 5    8 9  D4     5555 00  DD   r  00 000 0    r 000 0   r  DD  0000 0  r  D   D  00 00  r  D  0000 00   D   D  00 666    r   r r  D  000 99   D    D     00 0  r  rr  0000 000 000 0   r  000 000   r  r   r  1 1 1 12 2 2 2  (b)  Fig. 4. An example of the random traversal algorithm with document collection D = {D1, D2, D3, D4}, E = 2 and L = 2. The search process starts at the root node r1 and uses depth-first traversal method to visit all nodes. (a) shows the visiting path and query results with key [0,?6, 0,?7], the search starts from r1, and first reaches leaf nodes D11 and D  2 through r2 and r4, because the scores of r2  and r4 are equal to 0. Then, the nodes r5, r3, r6, r7, D13 and D  are visited. The nodes D24 , D 3 , D  2 and D  1 are not traversed, be-  cause the scores of r5 and r6 are less than 0. Finally, we get results {D11 , D12 , D13 , D14}. (b) shows the visiting path and results of query with key [?8, 0,?5, 0], the results are {D24 , D23 , D22 , D21}, the visited nodes are {r1, r2, r4, r5, D24 , D23 , r3, r6, D22 , D21 , r7}.

Where ? = 1, 2, ..., r, j = floor(?/E). ?j is a random integer in [0, E?1] which represents the selected dimension by the data user.

For example, if we assume L = 2, E = 2, and the divided document collection is DG = {DG1, DG2}. Thus, the key is a vector of length 4 and each document has two copies. Where key[1], key[2], key[3] and key[4] are used to control the document groups DG11, DG  1, DG   and DG22, respectively. Such as that when key[1] equals to zero, the document group DG11 will be traversed, and so on. Therefore, there have four possibilities {DG11, DG12}, {DG11, DG22}, {DG21, DG11}, {DG21, DG22}, and the cor- responding keys are [Z,RN,Z,RN ], [Z,RN,RN,Z], [RN,Z,Z,RN ], [RN,Z,RN,Z], where Z is zero and RN can be any random negative integer.

4.1.5 Query Processing  Search starts from the root to the leaf nodes in the tree. For any node N , only when key ? switch ? 0 can the cloud server continues to walk along this node. As shown in Fig. 4, the switch of r5 is switch5 = [0, 0, 0, 6] and the switch of node r7 is switch7 = [0, 0, 5, 0]. If the key of a query is key1 = [0,?6, 0,?7], then documents D13 and D14 will be traversed, while D23 and D  4 will be ignored, because key1 ?  switch7 = 0 and key1 ? switch5 = ?42. On the contrary, if the key is key2 = [?8, 0,?5, 0], documents D13 and D14 will not be traversed, but D23 and D  4 will be traversed since  key2 ? switch7 = ?25 and key2 ? switch5 = 0.

Note that, even though documents D13 and D  3 have  different identifiers, since both of them are copies of original    1545-5971 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TDSC.2017.2693969, IEEE Transactions on Dependable and Secure Computing   document D3. Hence, D13 and D 3 have the same content.

Therefore, for two identical queries with different keys, the cloud server may visit different paths and return different results, but their query accuracies are the same.

4.2 Analysis We have introduced the unencrypted RTRA, but for the concern of more strict security guarantee, the following analysis is based on the encrypted RTRA (the encryption method will be introduced in next section) such that an adversary cannot distinguish two copies just rely on their ciphertext.

4.2.1 Functionality and Information Leakage When processing queries in the index, if the switch of one node is zero, then this node will be always traversed by the cloud as its product with any key always equals to zero.

On the contrary, if the product is less than zero, this node and its children will be pruned. Therefore, to reduce the number of traversed nodes, we try to keep the neighboring leaf nodes in the index to have the same switch form.

Thus when building index, these documents come from one group are assigned with the same switch form. Although the above method may expose the grouping information, the adversaries (cloud servers) cannot identify which copies are from one certain document, since the documents order in each group is randomly assigned. Furthermore, since each switch and key contains a set of different random numbers, thus the product of one key with different switches or the product of one switch with different keys are different.

Therefore, adversaries cannot directly identify the copies of one certain document by comparing the products.

In addition, document collection D is divided into L groups and each group is copied E times. Therefore, a data user can generate EL different keys, thus the expectation of the same document in the query result with different keys is |R?e|/E, where |R?e| is the size of search results. It is obviously that the more number of different documents in two search results, the more difficult for adversaries to judge whether the two queries come from the same request or not. Therefore, the data owner can raise the level of security strength by increasing the value of E.

4.2.2 Space Consumption The storage space occupied by RTRA is E times of original index, as the value of E increases, the space consumption also becomes larger. Thus, there is a trade-off between space and security, but with the rapid development of computer hardware, the space will not be the main problem. However, an approach to reduce the size of indexes is also introduced in the next section.

5 THE SCHEME OF GMTS In this section, we first introduce the unencrypted group multi-keyword top-k search scheme (UGMTS), which is designed to support multi-keyword top-k similarity search over encrypted data and to improve query efficiency. Then, we describe the EGMTS which is an encrypted variation based on UGMTS.

5.1 UGMTS The UGMTS framework involves three parts: 1) Building searchable index locally; 2) Query construction based on the search interests of data users; 3) Search processing in cloud.

5.1.1 Building Index A data owner builds a searchable index I in local before outsourcing the data to cloud. The index I contains two index groups, that is I = {IC, IR}, where IC is used to select effective candidate documents, and IR is used to calculate the final relevance scores between queries and candidate documents. The details are as follows.

Firstly, the data owner creates an inverted index V for the dictionary W , the inverted index consists of a set of inverted lists and it is denoted as V = {vl(w1), vl(w2), .., vl(wn)}. Where vl(wi) is the inverted list of keyword wi and represents the top c ? k documents of keyword wi (where c is a positive integer).

Note that, we have adopted the champion lists to our scheme as each inverted list only stores the top c ? k docu- ments of each corresponding keyword, which can improve query efficiency and save storage space, but it may also result in lower query accuracy. However, data owners can control the side-effect by adjusting the value of c (in this paper, c is set to 1, in performance analysis we show that c has limited impact on query accuracy).

Secondly, the data owner divides dictionary W into mul- tiple groups as WG = {WG1,WG2, ...,WGb}, where each group only contains d keywords. The data owner also finds the top c ? k documents of each word group based on the inverted index V , denoted as V G = {V G1, V G2, .., V Gb}, where V Gi is the top c ? k documents of word group WGi.

To formulate this problem, we give the definition of top-k documents of word group: Definition 3. (Top-k Documents of Word Group) Giving a word  group {w1, w2, ..., wd}, the top-k documents of this word group equal to U(vl(w1) ? vl(w2) ? ... ? vl(wd)) , where function U() is used to remove duplicated documents.

Thirdly, the data owner builds a keyword balanced bi- nary tree [16] as index for each keyword group from the corresponding top-ck documents (e.g. ICi is the index of keyword group WGi, and it is built from V Gi). The indexes combine as IC = {IC1, IC2, ..., ICb}, Ni represents a node in index ICi and it has form as < fid, lc, rc, val >, where lc and rc are children node of Ni, and val is a data vector with d dimensions. If Ni is a leaf node, then fid is its corre- sponding document identifier and val stores the TF values of keyword group WGi (e.g., the val[j] equals to the TF value of keyword WGi,j in document Dfid, where WGi,j is the j-th keyword in keyword group WGi). Otherwise, if node Ni is an intermediate node, then the fid is empty and the val is computed as below:  val[j] = max{lc.val[j], rc.val[j]}, j = 1, 2, ..., d (4)  Where lc.val and rc.val are the stored vectors of the left and right children of Ni, respectively.

Finally, the data owner constructs another index group IR for the document collection D, where IR = {IR1, IR2, ..., IRm}. We use IRfid to represent an index in IR. The index IRfid is built based on document Dfid, and    1545-5971 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TDSC.2017.2693969, IEEE Transactions on Dependable and Secure Computing   D1 D2  D4  a  b  c  d  e  f  D11 D2  D11 D4  D11 D2  D3  D11 D4  D22 D4  D  D  D  D  D  0.6666660.5  0.666660.5 0 DDDDD  0.4  DDDD 0.3  DDDD 33333330.5 0.6  DD  6666 0  6660.5  00000 0   .5  0 6666666666666666  0.666  6666600000 5   0.66  00 555    D1 D2  D4  0.88888880.6  0.888880.3 0.7 DDDDDDDDD  7770.6  DD  0.888888888 0 DD  0 0.3  8880.6  00 777777777777788880.3  000 0  8888888000000 30.88    0.5  0.1 0.6    D11 D4  D22 D4   + =  D22 D4DD1  D1  D2  D3  D4  0.333 0.555 0.777777 0 0.8888 0  0.666 0 0.222222 0 0.6666 0.3  0.222 0 0 0.111 0 0.2  0 0.444444 0 0 0.7777 0.6    Search {b,e,f}  QC={QC1,QC3}  QR={QR1,QR3} Building index tree  IC={IC1,IC2,IC3}Document collection Get V={vl(a),vl(b),?,vl(f)}  Get top-k documents  of each query and  merge themment co   (a),vl(b   C1,IC2,I  3 4  rg   a b c d e f      W  ...

IC1  IC3  list1  list3  CList6  Fig. 5. This is an example of UGMTS. 1) The document collection D contains four documents {D1, D2, D3, D4}, the dictionary W has six keywords {a, b, c, d, e, f}. 2) The data owner builds an inverted list for each keyword and divides W into three groups where each group contains two words, then merges the inverted index V into three groups. 3) The data owner builds a searchable index for each keyword group. 4) The data user submits the search keywords {b, e, f} which only contains the keywords in WG1 and WG3, so we build query groups QC = {QC1, QC3} and QR = {QR1, QR3}, then QC and QR were send to cloud server. 5) The cloud server searches the top-2 documents on the index IC1 for query QC1 and searches the top-2 documents of QC3 on the index IC3, it gets results list1 = {D1, D4} and list3 = {D2, D4}. Then the cloud server merges all results into collection CList as candidate documents, where CList = {D1, D2, D4}. Finally, the cloud server uses Equation 6 to calculate the final scores between the query group QR and all the documents in CList, and returns k documents with highest relevance scores to the data user as results.

it can be denoted as IRfid =< fid, val1, val2, ..., valb >, where vali is a vector with length d and vali[j] = TF (WGi,j , Dfid) (j = 1, ..., d).

5.1.2 Query Construction When the data user wants to search with keyword set Wq , s/he generates query group Q. The query group is represented as Q = {QC,QR}, where QC is used to search on index group IC and QR will be processed in IR.

Query group QC is denoted as {QC1, QC2, ..., QCb}.

QCi represents a query in QC and its a query vector with length d. The j-th dimension of query QCi corresponds to keyword WGi,j . If WGi,j exists in the keyword set Wq , the value of QCi[j] is the IDF of keyword WGi,j , otherwise it is 0. Note that when all dimensions of query QCi are 0, the data user would remove QCi from QC. The other query group QR is denoted as QR = {QR1, QR2, ..., QRb} and it is the same as QC in UGMTS. Finally, the data user submits query group Q to the cloud server.

5.1.3 Query Processing The details of search procedure in cloud servers are shown in Algorithm 1. When the cloud server receives query Q.

Firstly, it processes QC on index group IC to get the candidate documents CList. Note that, each query in QC is only processed on its corresponding index (e.g. the query QCi only be processed on index ICi). The relevance scores between QCi and the nodes of ICi are calculated by Equa- tion 5.

Score(QCi, Ni) = (QCi) ? (Ni.val) (5) Secondly, the cloud server uses Equation 6 to calculate the final relevance scores between query group QR and the documents in CList on the index group IR, and returns k documents with the highest scores to the data user as results. An example is shown in Fig. 5.

Score(QR, IRi) = ?  QRj?QR (QRj) ? (IRi.valj) (6)  Algorithm 1 Search Process of UGMTS Require: The query Q, the searchable index I ; Ensure: Return k documents with highest scores to the data  user; 1: function SEARCH(Q, I , k) 2: for query QCi in query group QC do 3: FINDTOPK(QCi, root of ICi, 0, k) 4: Merge top-k documents listi of QCi into CList 5: end for 6: for document Di in CList do 7: if Score(QR, IRi) > k-th score in R?e then 8: Insert i into R?e 9: end if  10: end for 11: return top-k documents of R?e 12: end function 13: 14: function FINDTOPK(QCi, node, sco, k) 15: if sco < k-th score in listi then 16: return 17: end if 18: if node is leaf node then 19: Insert the fid of node into listi.

20: else 21: leftScore = Score(QCi, node.lc) 22: rightScore = Score(QCi, node.rc) 23: if leftScore > rightScore then 24: FINDTOPK(QCi, node.lc, leftScore, k) 25: FINDTOPK(QCi, node.rc, rightScore, k) 26: else 27: FINDTOPK(QCi, node.rc, rightScore, k) 28: FINDTOPK(QCi, node.lc, leftScore, k) 29: end if 30: end if 31: end function    1545-5971 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TDSC.2017.2693969, IEEE Transactions on Dependable and Secure Computing   5.2 EGMTS The UGMTS is efficient but less effective in privacy preservation for both data owners and data users.

Therefore, for protecting the real value of indexes and queries, we add some phantom terms and random values into them to disturb the real relevance scores, and adopt the secure kNN algorithm [32] to encrypt them. The framework of the encrypted group multi-keyword top-k search scheme (EGMTS) is as follows:  Setup(d, u, r): The data owner generates two secret keys sk1 and sk2, where sk1 = {S1,M1,M2} and sk2 = {S2,M3,M4}. S1 contains a group of (d + u + 1)- bit vectors and was denoted as S1 = {S11 , S21 , ..., Sb1}.

S2 contains (b + 1) randomly vectors, and the length of the first b vectors is (d + r)-bit, while the last vector occupies (r + u + 1)-bit, thus we denote S2 as S2 = {S12 , S22 , ..., Sb2, Sb+12 }. Each bit of vectors in S1 and S2 is randomly set to 1 or 0. M1 and M2 are two groups of matrices, both of which contain b invertible matrices that are (d + u + 1) ? (d + u + 1). While M3 and M4 both contain b invertible matrices with (d + r) ? (d + r) and one (r + u + 1) ? (r + u + 1) invertible matrices. When the secret keys are constructed, the data owner shares them with authorized data users.

BuildIndex(D,W, d): The method of building index I (where I = {IC, IR}) follows the same procedure as in UGMTS. Except that, some phantom terms are added into all the data vectors of index I , the details are:  1. Magnify the Values of IC: In UGMTS, each index of the index group IC is a keyword balanced binary tree, where the values of data vector of intermediate node are the max value from its children. But such relation may introduce security concerns as the cloud server can build more linear equations to calculate the plaintext information of indexes. To hide the relation, we magnify the value of data vectors by adding random numbers, such that Equation 4 is changed into val[j] = max{lc.val[j], rc.val[j]} + |rand()|%max{lc.val[j], rc.val[j]}.

2. Extending IC: The dimension of each data vector in index group IC is extended from d to d+u+1, where u is the number of phantom terms. In addition, the values of phantom terms are randomly set to 0 or 1, and the (d + u + 1)-th dimension of all these data vectors are set to 1.

3. Extending IR: Each vector in the index group IR is extended from d to (d+r), and the values of extended r dimensions are the same within one index, but varies for different indexes. Data owner also adds a vector of length (r + u + 1) to each index, where u is the number of phantom terms, and the values of extended r dimensions are the same as other vectors in the same index. Note that, both the values of extended dimensions and phantom terms are set to 0 or 1, and all the (r + u+ 1)-th dimension of added vectors are set to 1.

EncryptIndex(IC, IR, sk1, sk2): The index groups IC and IR are encrypted before outsourcing. We use Ni to denote  a node in index ICi and NVi to represent the stored data vector. Furthermore, we use S1,i to denote the i-th vector in S1. Firstly, the data owner splits vector NVi into two random vectors {NV ?i , NV  ??  i } based on the value of S1,i.

Specifically, if S1,i[j] is 0, NV  ?  i [j] and NV ??  i [j] are the same as NVi[j]; if S1,i[j] is 1, NV  ?  i [j] and NV ??  i [j] are set with two random numbers, but their summation equals to NVi[j].

After splitting process is complete, node Ni stores two encrypted vectors {MT1,iNV  ?  i ,M T 2,iNV  ??  i }, where M1,i and M2,i represent the i-th matrices in the matrix groups M1 and M2, respectively. The encryption form of index group IC is denoted as:  I?C = {MT1 IC ? ,MT2 IC  ?? }  = {{MT1,1IC ?  1,M T 2,1IC  ??  1 }, ..., {MT1,bIC ?  b,M T 2,bIC  ??  b }}  The data owner also encrypts IR with secret key sk2, where the encryption method is the same as encrypting IC .

We use I?R to represent the encrypted IR. Finally, the data owner outsources I?C , I?R and the encrypted document collection C to cloud.

CreateQuery(Wq, key). The method of generating query groups QC and QR is similar to the UGMTS. However, the query vectors in QC and QR need to be extended, and the details are:  1. Extending QC: Each query in QC is independent when processing the search request, hence phantom terms are added to them all. Thus, the query vectors in QC are extended from d to d+ u+ 1 dimensions, and the phantom terms are stored in the first u dimensions of the extension. The values of phan- tom terms are set to random numbers ?i,j and the (d + u + 1)-th dimension is set to another random number ?i (where i = 1, ..., b and j = 1, ..., u).

Besides, the first d+ u dimensions of each vector are multiplied by a random positive number ?.

2. Extending QR: Before extending QR, a phantom query QRb+1 which contains a vector of r + u + 1 dimensions was added to QR by data users. More- over, to improve the query accuracy such phantom terms are only added into this phantom query. When calculating final relevance score, the cloud servers need to treat all the queries in QR as a whole, thus we add a number of r dummy keywords into each query and restrict their summation to be zero, which means that the stored vectors of the first b queries are extended to d + r, and the values of extended dimensions satisfy the requirement such as QRb+1[j]+  ?b i=1 QRi[d+j] = 0 (where j = 1, ..., r).

Note that, the above restriction is used to prevent the cloud servers from learning the final relevance scores between candidate documents and any single query, if the cloud server processes QR as a whole, it will get the real score, since the summation of all dummy keywords is zero. Otherwise, the final score will be the real score plus the score of partial dummy keywords. In addition, the (r + j)-th dimension of query vector QRb+1 is set to random number ?j (where j = 1, ..., u) and the (r+ u+1)-th dimension    1545-5971 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TDSC.2017.2693969, IEEE Transactions on Dependable and Secure Computing   is set to a random positive number ?. Finally, all the query vectors in the query group QR are scaled by a random positive number ? except the last dimension.

EncryptQuery(QC,QR, key). After query groups QC and QR are generated, the data user encrypts them with secret keys sk1 and sk2, respectively. Firstly, the data owner splits query QCi into two random vectors {QC  ?  i , QC ??  i }according to the value of vector S1,i. If S1,i[j] is 0, the sum of QC  ?  i [j] and QC  ??  i [j] equals to QCi[j], otherwise QC ?  i [j] and QC ??  i [j] are the same as QCi[j] (where j = 1, ..., d + u + 1). Finally the query group QC is encrypted as below:  Q?C = {M?11 QC ? ,M?12 QC  ?? }  = {{M?11,i QC ?  i ,M ?1 2,i QC  ??  i } | QCi ? QC}  The data user also uses secret key sk2 to encrypt QR with the same method applied to QC . In the end, the data user submits < Q?C, Q?R > to the cloud server as trapdoor where Q?R denotes the encrypted QR.

Query(Q?C, Q?R, I?C, I?R, k). The query processing method over the encrypted index groups I?C and I?R is similar to UGMTS, except the relevance scores between the nodes of index I?Ci and query Q?Ci are calculated by Equation 7, and the score between the query group Q?R and index I?R? is calculated by Equation 8.

Score(Q?Ci, N?i)  = {MT1,iNV ?i ,MT2,iNV ??i } ? {M?11,i QC ?  i ,M ?1 2,i QC  ??  i }  = ?(Score(QCi, Ni) +  u? j=1  ?i,j) + ?i  (7)  Where N?i represents a node in index I?Ci, MT1,iNV ? i and  MT2,iNV ?? i are the stored data vectors of N?i.

Score(Q?R, I?R?)  = ?  Q?Ri?Q?R  (I?R?.val ?  i) ? (Q?R ?  i) + (I?R?.val ??  i ) ? (Q?R ??  i )  = ?(Score(QR, IR?) + u?  j=1  ?j) + ?  (8)  Where I?R? represents an index in I?R, I?R?.val?i and I?R?.val  ?? i are the stored query vectors in I?R?.

5.3 Security Analysis  In this paper, we do not discuss the security of document collection, because it is encrypted by the data owner before outsourcing to the cloud server, and the encryption method could be any traditional encryption method that is suitable for the concern of data owners. Hence, we assume the encryption methods are secure and can guarantee strong data privacy. Next, we analyze the security of our scheme in known ciphertext model and known background model, respectively.

0.05 0.10 0.15 0.20 0.25    Nu mb  er of  do cu  me nts  Similarity Score  The distibution for =0  -0.05 0.00 0.05 0.10 0.15 0.20 0.25    Nu mb  er of  do cu  me nts  Similarity Score  The distibution for =0.03  -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4   Nu mb  er of  do cu  me nts  Similarity Score  The distibution for =0.1  Fig. 6. The distribution of similarity score for keyword ?network? with different values of ?.

5.3.1 In Known Ciphertext Model  Adversaries can calculate the real value of indexes and queries by establishing liner equations from the exposed ciphertext [15]. Assume I?Ci represents an index in the index group I?C , and it is encrypted by the secure kNN algorithm with secret key sk1, where each data vector is randomly split into two different vectors. The number of equations that es- tablished from the ciphertext of this index is 4?m(d+u+1), where 0 ? ? ? 1. But index I?Ci contains 2?m nodes and 2(d+u+1) unknown numbers in each node, there also have 2(d+ u+1)2 unknown numbers in matrices M1,i and M2,i.

It is obvious that the size of unknowns numbers is more than the known equations. Similarly, for the index group I?R and trapdoor, the number of unknown numbers is also more than the known equations. Hence, adversaries have no sufficient equations to calculate the plaintext of indexes and trapdoors without secret key sk1 and sk2.

According to Yao et al. [35], the secure kNN algorithm is not secure against the chosen-plaintext attack. But next we prove our EGMTS is secure.

Proof: Yao et al. use the known plaintext-ciphertext pair of queries to construct linear equations to calculate the values of index, but in our scheme, the relevance score learned by the cloud server is  Score(Q?Ci, N?i) = ?(Score(QCi, Ni) + u?  j=1  ?i,j) + ?i  where Score(QCi, Ni) is real score, but it is disturbed by 2 + u random numbers (two random numbers ? and ?i, u random numbers ?i,j), which means that even for two identical queries, the relevance scores are different. Besides, each linear equation may introduce 2+u unknown random numbers, therefore the unknowns in equations are always more than the number of linear equations, so adversaries cannot calculate the real value of index, and also cannot infer the real value of secret key.

In summary, the EGMTS is strong enough to protect the security of index and query in known ciphertext model, and it is more secure than the secure kNN algorithm.

1545-5971 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TDSC.2017.2693969, IEEE Transactions on Dependable and Secure Computing   5.3.2 In Known Background Model  In the known background model, cloud servers have more knowledge about the stored data, such as the normalized TF distribution of some keywords, therefore, the cloud server can identify these keywords by comparing the normalized TF distributions [14], [15], [16], [30], [33]. In addition, cloud server may learn the search interests of data users by linking queries and exploiting access pattern.

Keyword Privacy: In EGMTS, keyword privacy can be guaranteed by inserting random numbers ?i,j and ?j into queries to obscure the normalized TF distributions. More- over, random numbers ?i,j and ?j follow the uniform dis- tribution U(?? ? ?, ?? + ?). According to the central limit theorem, the  ?? j=1 ?i,j and  ?? j=1 ?j follow the normal  distribution N(?, ?2) (where 2? = u, ? = u??/2 and ?2 = u?2/6). As shown in Fig. 6, the bigger is the value of ?, the higher is the level of interference, but the lower is the query accuracy. Therefore, data users can balance the trade off between query accuracy and keyword privacy by adjusting the value of ?.

Query Unlinkability: In EGMTS, even though each query only contains one keyword, it is also represented as a fixed- length vector. Hence, the cloud server cannot determine which keywords the data user wants to search. In addi- tion, each query vector is randomly split into two different vectors which are encrypted before processing, and the relevance scores are also disturbed by inserted random numbers. Hence, the cloud server cannot distinguish even the same search request just rely on ciphertext and relevance scores. Moreover, according to [14], data users can control the level of query unlinkability by adjusting the value of phantom terms.

However, the cloud server can link two queries by com- paring and analyzing the access pattern and visiting paths on the index. Even though the relevance scores and visit- ing paths are obscured by inserting random numbers, the accuracy of queries usually get reduced with the increasing interference, which is impractical. To better ensure the sys- tem availability, the accuracy of queries cannot be too low.

But with the increase of query accuracy, the access pattern and visiting paths of two identical queries are becoming more similar (e.g., in order to guarantee the correct ratio is above 80%, two identical queries must share at least 60% common results and visiting nodes). Hence, the EGMTS and EMTS [14], MRSE II [15] and EDMRS [16] cannot perfectly protect the query unlinkability.

6 THE SCHEME OF RGMTS  In Section 4 we have introduced the random traversal algo- rithm which can change the visiting paths and search results of two identical queries by using different keys. In last section we also have introduced that EGMTS has weakness in query unlinkability protection, since the cloud server can link two queries by comparing and analyzing their visiting paths and search results. In this section, we propose a ran- dom group multi-keyword top-k search scheme (RGMTS) which absorbs the advantages of both RTRA and EGMTS, and provides more data security than EGMTS.

6.1 Building RGMTS Index First, the data owner enlarges the document collection D to DGx and assigns a random switch to each docu- ment, where the method is the same as RTRA. For ex- ample, when both E and L are set to 2, the document collection D = {D1, D2, D3, D4} is extended to DGx = {D11, D12, D24, D23, D22, D21, D13, D14}.

Then, similar to the index construction in GMTS, the data owner divides all the keywords in dictionary W into several keyword groups and finds the top-ck documents of each word group. But in RGMTS, all the top-ck document groups are further extended, such as that V Gi is extended to V Cxi where V Gi is the top-ck documents of keyword group WGi, and V Cxi is a subset of DG  x which contains all the copies of documents belong to V Gi. Note that, the documents that belong to V Gxi keep the same order as DG  x.

e.g., keyword group WG1 contains two keywords {a, b}  and its top-ck documents are V G1 = {D1, D2, D4}, after V G1 is extended to V Gx1 , the data owner gets V G  x 1 =  {D11, D12, D24, D22, D21, D14}, where V Gx1 ? DGx.

The data owner uses the extended top-ck documents  to build a searchable index for each keyword group, by using the method which has been applied to EGMTS. For instance, the data user uses V Gxi instead of V Gi to build a searchable index for keyword group WGi. Suppose we use < fid, lc, rc, val > to represent one node of these searchable indexes, where fid is the document identifier, lc and rc are the left and right child, respectively, val is a data vector of e (where e = d+ u+ r + 1). We also specified that the first d dimensions of the vector are the TF of its corresponding keywords, the (d + j)-th (where j = 1, ..., u) dimension stores phantom terms, the (d + u + j)-th dimension stores the switch of this node (where j = 1, ..., r), and the e-th dimension is set to 1. The data owner also builds an index group IR for document collection DGx, where the switch is stored in the added data vector. Finally, the data owner encrypts collection DGx, index groups IC and IR, and sends them to cloud.

6.2 Search Process of RGMTS When the data user wants to search with keyword set Wq , s/he constructs two query groups QC and QR using the method which is similar to EGMTS, except that:  1. The query vector in query group QC is extended from (d+ u+ 1) to e;  2. Each query of QC is assigned a random key, and these keys are stored in the (d+ u+1)-th dimension to (d+ u+ r)-th dimension of each vector.

3. The data user assigns a random key to the phantom query in the query group QR.

The data user encrypts QC and QR, and sends them to cloud as trapdoor T . When processing the query Q?Ci, which represents a query of Q?C , the cloud server calculates the relevance scores between Q?Ci and the nodes of index I?Ci from the root to the leaf, but only when the score of one node is larger than zero, its children nodes will be traversed.

After that, the cloud server merges all the results into CList as candidate documents and calculates the relevance scores between the documents in CList and the query group    1545-5971 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TDSC.2017.2693969, IEEE Transactions on Dependable and Secure Computing   Q?R. Finally, the cloud server returns k documents with the highest relevance scores to the data user as search results.

6.3 Security Analysis The RGMTS takes the advantage of RTRA which make sure that if the data user submits two identical queries with different keys, our search procedure in the cloud server must have different visiting paths and results, and in the meantime it maintains the accuracy of queries unchanged.

In addition, it is easy to conclude that the probability of getting the same query results and visiting paths of two identical queries is less than 1/EL, and the expectation of the number of common documents between the two search results is less than |R?e|/E. Therefore, we can control the level of query unlinkability by adjusting the value of E and L, and without sacrificing the correct ratio.

In order to completely hide access pattern and visiting paths, which requires the probability of getting the same results and visiting paths of two identical queries must be less than or equal to that the probability of two different queries. Therefore, the value of E would be very large, but with the increasing of value E, the index space is also becoming larger (even though we can decrease the storage space by only store the top-ck documents), thus data owners have to balance the trade-off between data security and storage space by adjusting the value of E.

In summary, the RGMTS trades space for data security, which can better protect the query unlinkability and access pattern than most existing works (such as [14], [15], [16]).

7 PERFORMANCE ANALYSIS In this section, we first describe the experimental setup and scenarios, then we analyze the performance of our schemes from two aspects: 1) the precision and privacy; 2) the efficiency of index construction, trapdoor generation and query processing. As one reference point, for query efficiency, we compare the time cost of our solution with the approach EDMRS as proposed in [16], which represents the latest research finding with high query efficiency.

7.1 System Implementation The overview of our system is shown in Fig. 1, the main functions of three module are briefly summarized as follow: 1) the data owner encrypts raw collection D to get encrypted version C, and builds searchable index Ie based on C; 2) the data user encrypts the query to construct trapdoor T , by using the key as shared by the data owner, and get the encrypted query results from the cloud server; 3) the cloud server stores the outsourced C and Ie from the data owner, it traverses the index to process encrypted queries, and returns those documents with top-k highest scores.

To test the performance of our schemes, we implement our system based on GMTS and RGMTS. In particular, the former two modules are implemented with C language and Python on a Windows 10 PC with Intel(R) Core(TM) i5-4590 CPU 3.30GHz and 4 Gigabyte memory. The cloud server module is implemented with C language on a Linux Server with Intel(R) Xeon(R) CPU E5620 Processors 2.40GHz and 24 Gigabyte memory. For convenience and fairness, we  1 3 5 7 9       pr ec  is io  n (  % )  the value of c  m=4000 m=8000 m=12000  (a)  50 70 90 110 130 150       pr ec  is io  n (  % )  # of retrieved documents  m=4000 m=8000 m=12000  (b)  Fig. 7. The impact of c on precision, we set n = 4000. (a) For the different values of c with the same k, we set k = 50. (b) For the different values of k with the same c, we set c = 1.

implement EDMRS and MRSE on the same programming languages and platform as that of GMTS and RGMTS.

Dataset: We randomly select 12000 emails as our collec- tion D from a publicly available real-life data set: the Enron Email Data Set [36]. In addition, we randomly extract 12000 keywords from these emails as our dictionary with Python, where each keyword is processed by Porters stemming algorithm [37] which excludes the stop words.

7.2 Precision and Privacy Without loss of generality, we adopt the definition of ?pre- cision? from [15], in which precision Pk is defined as Pk = k  ?/k, where k? is the number of real top-k documents in query results.

As described in Section 5.1.1, to decrease the size of in- dexes and improve the query efficiency, we adopt champion lists to our schemes, where each index only stores the top-ck documents of its corresponding keyword group. It is obvious that the previous methods may result in lower accuracy of queries. However, the data user in our scheme can control the query accuracy by tuning c. As shown in Fig. 7 that the larger value of c is, the higher is the precision we get. Fig. 7 also indicates that c has only limited impacts when its value beyond a certain point, that is because most of the top-k documents in a multi-keyword query appear in the union of search results for one single keyword query.

The works [14], [15], [16] are not designed to protect access pattern, and they only add random number ?j into index or queries to control the level of query unlinkability.

However, as we know the adversaries can link two queries by comparing and analyzing the access pattern, thus these works cannot protect the query unlinkability perfectly. In our work, we not only adopt random numbers to control the query unlinkability, but we also proposed RTRA to hide the access pattern. Although the access pattern is hard to hide thoroughly, we can reduce its leakage and increase the difficulty of cloud servers to link two identical queries. One obvious observation is that if the number of common docu- ments between the query results of two identical queries becomes smaller, then it will be more difficult for cloud servers to distinguish from these two queries. Hence, we define the level of query unlinkability as di = 1 ? k??/k, where k?? is the number of common documents between the two query results. In RGMTS, we know that the number k?? for two identical queries decreases as the value of E increases. Therefore, as shown in Fig. 8(a), the level of query    1545-5971 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TDSC.2017.2693969, IEEE Transactions on Dependable and Secure Computing   1 2 3 4 5 6 7 8   le  ve l o  f q ue  ry u  nl in  ka bi  lit y(  % )  the value of E  RGMTS EDMRS MRSE  (a)  50 70 90 110 130 150       pr ec  is io  n (  % )  # of retrieved documents  0.03 0.1  (b)  Fig. 8. We set n = 4000, m = 4000. (a) The level of query unlinkability with fixed value ? = 0.03 and different values of E. (b) The precision with different values of ?.

4 6 8 10 12       tim e  of in  de x  co ns  tr uc  tio n  ( x1  s)  # of documents in the dataset (x103 )  GMTS RGMTS EDMRS  (a)  4 6 8 10 12        tim e  of in  de x  co ns  tr uc  tio n  ( x1  s)  # of keywords in the dictionary (x103 )  GMTS RGMTS EDMRS  (b)  Fig. 9. Time cost of index construction. (a) for different sizes of the document collection D and fixed size of the dictionary W with n = 4000.

(b) for different sizes of dictionary W and fixed size of D with m = 4000.

unlinkability of RGMTS is increased with larger E, but EDMRS [16] and MRSE [15] do not have such property. On the other hand, to protect the privacy of keyword in the query, random number ?j was added to each query, which can directly affect the query precision. Fig. 8(b) shows the influence of ?j on precision, where ?j follows the normal distribution N(?, ?2), and ? is the standard deviation.

7.3 Efficiency  7.3.1 Index construction The procedure of index construction can be divided into two steps: 1) building a tree-based index group IC for all the keywords and constructing an index IR for document collection D; 2) encrypting all nodes in the indexes with secret keys sk1 or sk2.

As introduced in previous section, when building the index IC , each index only stores the top-ck documents of its corresponding keyword group. Therefore, the above op- eration only generates O(?mb) nodes, where ? is a decimal which is less than or equal to 2. On the other hand, our scheme generates O(mb) nodes when building index IR.

Overall, there are totally O(?mb) nodes will be generated during the index construction, where ? = 1 + ?.

Node encryption needs a splitting process and two multiplications of e ? e matrix, where e is the length of vector in each node, which equals to (d + u + 1) in GTMS and (d + u + r + 1) in RGMTS (we ignore the different length of vectors in IC and IR). The splitting process takes O(d) time and the two multiplications takes O(e2) time.

Hence, we can conclude that the time complexity of index construction is approximately equal to O(?mbe2). Note that,  4 6 8 10 12         tim e  of tr  ap do  or c  on st  ru ct  io n(  m s  )  # of keywords in the dictionary( x103 )  RGMTS EDMRS  (a)  5 10 15 20 25    tim e  of tr  ap do  or c  on st  ru ct  io n(  m s  )  # of keywords in the query  RGMTS EDMRS  (b)  Fig. 10. Time cost of trapdoor construction. (a) for different sizes of dictionary W and fixed size of query with t = 10. (b) for different sizes of query and fixed size of dictionary W with n = 4000.

TABLE 2 Size of index  Size of dictio- nary  2000 4000 6000 8000  GMTS (MB) 159 314 473 574 RGMTS (MB) 345 682 1027 1232  in this paper, we compare our schemes with EDMRS, which is more efficient than MRSE [15] and other methods. The time complexity of EDMRS is O(n2m). It is obvious that our schemes are mainly influenced by e, but EDMRS is proportional to the square of n, where n = |W |. On the other hand, the index of EDMRS is encrypted by two n? n matrices, which is time-consuming. But the indexes of our schemes are encrypted by several e? e matrices, where e is smaller than n. So, our schemes usually spend less time in encrypting the indexes. As shown in Fig. 9, our schemes are more efficient than EDMRS in index construction. Table 2 shows the storage overhead of our indexes (m = 4000, E = 2 and c = 1).

7.3.2 Create Trapdoor  In EDMRS, no matter how many keywords are contained in the query Wq , the length of trapdoor is always equal to the size of dictionary, where the trapdoor is a vector. However, in most of the time, people are likely to search just with five keywords or less [14], [38]. Therefore, most dimensions of the trapdoor are equal to 0, which wastes the computing resources greatly. In our schemes, the trapdoor is divided into b parts, like the dictionary W , and each part is called a query which is a vector with length d. If all dimensions of a query are equal to 0, we remove it from the trapdoor.

For example, assume our dictionary W contains n = 8000 keywords, and we divide it into b = 100 groups where each group contains d = 80 keywords. If the size of Wq is 5, then the generated trapdoor at most contains 5 queries, where each query includes one 80-dimensional vector. Therefore, the total length of our trapdoor is 400 at most, but the length of the trapdoor in EDMRS is 8000. Hence, in our schemes, the data user can take less time in trapdoor encryption. The time complexity of trapdoor construction in both GMTS and RGMTS are O(te2), where t is the number of queries in trapdoor. In the worst case, t equals to the size of Wq .

Obviously, the time complexity is independent of the size of dictionary, and it is only affected by the size of vectors in queries and the size of trapdoor. In Fig. 10, we can    1545-5971 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TDSC.2017.2693969, IEEE Transactions on Dependable and Secure Computing   4 6 8 10 12        tim e  of s  ea rc  h (  m s  )  # of documents in the dataset (x103 )  GMTS RGMTS EDMRS  (a)  5 10 15 20 25       tim e  of s  ea rc  h( m  s )  # of keywords in the query  GMTS RGMTS EDMRS  (b)  Fig. 11. Time cost of search. (a) for fixed size in query Wq (t = 10) and fixed size of dictionary (n = 4000) in different sizes of document collection. (b) for different sizes t in query Wq with fixed size of dictionary (n = 4000) and fixed size of document collection (m = 4000).

observe that our proposed scheme significantly outperforms the EDMRS in trapdoor construction.

7.3.3 Search We improve query efficiency in three ways: 1) we build a searchable index for each keyword group instead of the whole dictionary; 2) each index only stores the top-ck doc- uments of its corresponding keyword group; 3) every index is structured as a keyword balanced binary tree.

As mentioned above, the data user divides the origi- nal query into several queries and only sends non-empty queries to the cloud server. Therefore, with the first method, the cloud server does not need to search the indexes of all keyword groups. On the other side, the number of nodes in indexes was decreased with the second method, which avoid excessive search on extraneous nodes. Besides, with the third method, when we calculate the relevance scores between any node and queries, if the score of one node is less than the minimum score in CList, then its children nodes will not be traversed, thus many nodes could be pruned during our traversal process. With these methods, the overall computational cost is greatly reduced in our search procedure, and in the meantime we can guarantee the query privacy. Because the number of keywords in a query can be ranged from 1 to d, the cloud server cannot identify which keywords the data user wants to search.

We compare the query efficiency of our methods with EDMRS under different parameter settings. In particular, we study m (dataset size), t (query size), n (dictionary size) and the effect of k (parameter k in our top-k query) on real datasets. All results in Fig. 11 and Fig. 12 demonstrate that our methods are much more efficient in search time.

In particular, Fig. 12(a) shows that the query time of each method increases with k since they all need more time to process the data. Similarly, both figures in Fig. 11 show that the query time of each algorithm increases with m and t, respectively. On the other hand, the time cost of query in our methods is independent of the dictionary size. So, as shown in Fig. 12(b), the efficiency of query in EDMRS drops sharply with the increased size n of dictionary, but our methods still maintain high efficiency.

8 CONCLUSION In this paper, we focus on improving the efficiency and the security of multi-keyword top-k similarity search over  30 50 70 90 110       tim e  of s  ea rc  h (  m s  )  # of retrieved documents  GMTS RGMTS EDMRS  (a)  4 6 8 10 12       tim e  of s  ea rc  h (  m s  )  # of keywords in the dictionary (x103 )  GMTS RGMTS EDMRS  (b)  Fig. 12. Time cost of search. (a) for fixed n and m with different values of k ( where k is the number of documents that the data user wants to retrieve, and n = 4000, m = 8000, t = 10 ). (b) the time cost of search with different sizes of the dictionary W , we set the size of the document collection D as m = 4000, and t = 10.

encrypted data. At first, we propose the random traversal algorithm which can achieve that for two identical queries with different keys, the cloud server traverses different paths on the index, and the data user receives different results but with the same high level of query accuracies in the mean time. Then, in order to improve the search efficiency, we design the group multi-keyword top-k search scheme, which divides the dictionary into multiple groups and only needs to store the top-ck documents of each word group when building index. Next, to protect the query unlinkability, we apply the random traversal algorithm to get the RGMTS, which can increase the difficulty of cloud servers to conduct linkage attacks on two identical queries, and we can also tune the value of E to make the level of query unlinkability flexible for data owners. Finally, the ex- perimental results show that our methods are more efficient and more secure than the state-of-the-art methods.

