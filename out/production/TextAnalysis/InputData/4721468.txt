A Hybrid Method for Frequent Itemsets Mining

Abstract Discovering frequent patterns is one of the essential topic  data mining. A new algorithm based on the two-way-hybrid search for frequent itemsets mining is proposed. 1) A hierarchical search space organization is presented, based on which the original search space can be recursively decomposed into some smaller independent pieces. 2) A novel HFMI algorithm, which explores a flexible two-way-hybrid search method, is given. It executes the mining in both the top-down and bottom-up directions. Information gathered in the bottom-up can be used to prune the search space in the other top-down direction. Some efficient decomposition and pruning strategies are implied in this method, which can reduce the original search space rapidly in the iterations. 4) Experimental and analytical results are presented in the end of this paper.

?. INTRODUCTION Association rule mining, the task of finding  correlations between items in large databases of transactions, has received considerable attention, particularly since the publication of the AIS algorithm[1].

Frequent Itemset Mining (FIM) is the most fundamental and essential problem in mining association rules. It started as a phase in the discovery of association rules, but has been generalized independent of these to many other patterns. For example, frequent sequences, episodes, and frequent subgraphs. Since there are usually a large number of distinct single items in a typical transaction database, and their combinations may form a very huge number of itemsets, it is challenging to develop scalable methods for mining frequent itemsets in a large database.

1.1 Related works  Apriori algorithm is the best known previous level- wised algorithm [2]. It requires multiple database scans, as many as the longest frequent itemset. There have been extensive studies on the improvements or extensions of Apriori algorithm, e.g., hashing technique [3], partitioning technique [4], dynamic itemset counting [5].

FP-growth is another novel algorithm for mining frequent itemsets[6]. In the method, a data structure called  the FP-tree is used for storing frequency information of the original database in a compressed form. The FP- growth algorithm transforms the problem of finding long frequent patterns to searching for shorter ones recursively and then concatenating the suffix. It uses the least frequent items as a suffix, offering good selectivity. This makes the FP-growth method much faster than Apriori.

There are many alternatives and extensions to the FP- growth approach, including H-Mine which explores a hyper-structure mining of frequent itemsets[7]; dynamically considering item order, intermediate result representation, and construction strategy, as well as tree traversal strategy by Liu et al. [8]; and an array-based implementation of prefix-tree-structure for efficient pattern growth mining by Grahne and Zhu [9].

Eclat, an aprioro-liked algorithm, explores the vertical data format [10]. The computation is done by intersection of the tidsets of the frequent k-itemsets to compute the tidsets of the corresponding (k+1)-itemsets. This process repeats, until no frequent itemsets or no candidate itemsets can be found. A diffset technique is introduced by Zaki and Gouda[11], for reducing the memory requirement. Bitmap and Granular are also vertical format for frequent patterns mining [12-13]. A BitTableFI algorithm is presented recently, where data structure BitTable is used horizontally and vertically to compress database for quick candidate itemsets generation and support counting [14].

1.2 Contributions  We present a new algorithm for discovering frequent itemsets. The main contributions are as follows: (1) A structure of hierarchical search space organization  is presented, based on which the original search space can be recursively decomposed into some smaller independent pieces.

(2) A higher flexible two-way search method is given.

This search begins the mining procedure from the top-down and bottom-up ways at the same time, which can discovery more frequent itemsets as soon as possible. In this method, the top-down search uses a number of optimizations to quickly prune     away a large portion of the search space by sharing the result of the bottom-up search.

1.3 The Roadmap  The remaining of this paper is organized as follows.

Section 2 introduces the description and decomposition strategies of search space. Section 3 presents a hybrid algorithm for mining all of the frequent patterns, and some pruning strategies. The feasible optimizations and experimental results are presented Section 4. Finally, we conclude this paper in Section 5.

?.  HIERACHICAL SEARCH SPACE Given a transactional database D, supposed I is an  itemset of it, then any combination of the items in I could be frequent and all these combinations compose the search space, which can be represented by a set enumeration tree [15]. The process of frequent patterns mining is a searching process of the enumeration tree, and it is important to prune the tree efficiently while searching.

For the item set I, the corresponding search space of this tree structure would contain exactly I2  nodes. If the sets I is large enough, then the search space is explosive. Thus new description and organization mechanisms for the search space are imperative.

Definition1 Let YXT ?=  be an itemset in database D, where IYX ?, and ?=?YX . And let P(Y) be the power set of Y, denoted by }{)( YZZYP ?= . For this itemset T, then set of all the itemsets obtained by concatenating X with the elements in P(Y) is called the search space of T. That is })({]:[ YPZZXYX ??= ,  denoted by XYS  or S. We call X is an ancestor element of S, denoted by S.anc, Y is a child element of S, denoted by S.chi, and YX ?  is the border of S, denoted by S.bor.

Definition2 Let S and Si? ki ??1 ?be the search spaces. Iff kSSSS ???= 21  and ?=? ji SS , we call ),,,( 21 kSSS  is partition of S, denoted by  kSSSS +++= 21 .

Theorem1 Let IRYX ?,, be the distinct itemset  and ]:[ RYXS =  be a search space. For itemset },,,{ 21 kiiiR = (where Ii j ? and YXi j ,? ), the  search space S can be partitioned into the following subspaces ? = ++  k j kjj YiiXiYX 1 1 ]:[]:[ by R. That is ? = ++=  k  j kjjk YiiXiYXYiiiX  1 121 ]:[]:[]:[ .

Proof:  Let V be a itemset, ? Iw? , the power set of  wV is }{)( wVZZwVP ?= = },{}{ wVwVZZ ?? . It follows from the definition1 and definition2, that the search space ]:[]:[]:[ VXwVXwVX += . The above  search space [X:RY] can be partitioned via item Ri j ? sequentially. The result follows.

For example, following the theorem1, then search space ]:[ bca  can be partitioned into ]:[]:[ cacab + , furthermore it can be partitioned into  :][:][]:[ aaccac ++ .

For item set I, if we had enough main memory, we  could enumerate all the (maximal) frequent itemsets by traversing the search space [:I]. With only a limited amount of main memory in practice, we should decompose the original search space into some smaller pieces, such that each one can be solved independently in main memory. Following the above description and partition mechanisms, the original enormous search space could be partitioned into some little ones as flexibly as possible. Furthermore, theorem1 can be used to prune the search space.

The search space for item set I={a,b,c,d} is S=[:I]. It can be partitioned into some little ones step by step. The iterative results of a hierarchy of search space are shown in figure 1. For any decomposition degree K, each set contain to all of the leaf nodes this hiberarchy as shown in figure 1 is a partition of the original search space S. For example, let K=1, the set {[a:bcd],[b:cd],[c:d],[:d]} is a partition of S.

For a given item set I in database D and the minimum support threshold minsup, the task of mining FI is in the follow: finding the set  })(    ][:{ minsupXSuppandIXX ??  in the search space S=[:I]. For each transaction DT ? , all of its subsets compose a search space, which can be decomposed for any degree. We can check whether this T (or subsets of T) is frequent in different partitions of the original search space. For example, let T=abcde,  ]:[ bcdeaS = and ]:[ deabcS =? are the two search spaces.

Apparently we can check T in these two search spaces    [ac:d] [a:d] [bc:d] [b:d] [ab:cd  [:abcd]  [a:bcd] [c:d] [d:]  [abc:d] [ab:d]  [b:cd]  Figure 1.  Hiberarchy of search space on I?{a,b,c,d}  K=0  K=2  K=1  K=3     discretionarily. Following definition1 we have known 162 4 ==S  and 42 2 ==?S . Standing to reason, the  mining is more efficient in S ?   whose size is only 25% of that of S.

By decomposing a search space, we can prune the original search space. For example, the search space  ]:[ bcdeaS =  can be decomposed into ]:[]:[]:[]:[ dadaccdabbcdaeS +++=?  or  ]:[]:[]:[]:[ eaeaddeaccdeabS +++=?? ( 16=??S ). If  the itemset P?ae is infrequent, then S ?  could be pruned to ]:[]:[]:[ dadaccdab ++=? ( 8=? ). The size of search space reduces to the half. The first method is more efficient than the second ones.

?.  Mining Frequent Itemsets  In general, it is possible to search the frequent itemsets in either top-down manner or bottom-up one. The bottom-up approach is good for the case when all frequent itemsets are short, and the top-down approach is good when all frequent itemsets are long. If some frequent itemsets are long and some are short in the mining database, then both search approaches will not be efficient. An algorithm that can efficiently find both long and short frequent itemsets is presented.

3.1 Pruning Strategy  A key idea of our two-way hybrid approach is the use of information gathered in the search in the bottom-up to prune search space during the top-down search. It uses infrequent itemsets found in the search in the bottom-up to prune search space during the top-down search. Some efficient prune technologies will be discussed later.

The following corollary can be deduced by theorem1.

Corollary1: Let IYX ?, be distinct itemset in database D, and Iu ?  be an item. The search space  ]:[ uYXS =  can be decomposed by u. That is ]:[]:[]:[ YXuYXuYX += .

Given a search space ]:[ YXS =  and Yu ? , it can be decomposed into })]{(:[})]{(:[ uYXuYXu ?+?  by Corollary1. If itemset R=Xu is infrequent, then each itemset ]:[ YXuZ ?  is in frequent by property2.

Therefore the first subspace [Xu:(Y-{u})] can be pruned entirely.

Lemma1: Let IYX ?,  and Iu ? , and ]:[ uYXS =  be a search space on X. If the itemset R=Xu  is infrequent, the space S can be pruned by R. And the remained space is ]:[ YXS =? .

The theorem2 shows how to prune the search space [X:Y] by the infrequent itemset R.

Theorem2: Let X and Y be distinct itemsets, and ]:[ YXS =  be a search space. If there exists an item  Yu ?  satisfied that itemset XuR =  if infrequent, the partition of })]{(:[})]{(:[ uYXuYXu ?+?  is the most efficient ones.

Proof: For such search space ]:[ YXS = , the size of S lies on value Y . Let },,,{ 21 kiiiY = , there are 2  k subsets in S, that is S =2k. The original search space S can be decomposed into ? = +  k j kjj iiXi1 1 ]:[  by  theorem1. For the jth subspace ]:[ 1 kjjj iiXiS += , if the assumable infrequent itemset is Xij, then Sj can be pruned entirely by lemma1. Obviously, when j=1(i.e. i1=u), the value of jS  comes to the top. The result follows by Corollary1.

Theorem3: Let ]:[ YXS =  be a search space, and X be a (k-1)-itemset. Let kL~  be the set of all infrequent k- itemsets. If there exists such a itemset kLZ ~?  satisfied with XZ ? , search space S can be pruned by kL~ . And the resulting space )](:[ RYXS ?=? ?  ? }~    { kLZandXZXZR ???= .

Proof: ? Yu ? , there exists  })]{(:[})]{(:[]:[ uYXuYXuYX ?+?=  by Corollary1.

If there has no kLZ ~?  satisfied with XZ ? , .i.e.

kLXu ~? , the search space S can not be pruned any part by Z.

Let },,,{ 21 MiiiR = , the result follows by pruning S via Ri j ? ( Mj ??1 ) sequentially as in Corollary1.

3.2 Hybrid Frequent Itemsets Mining Algorithm  This two-way-hybrid approach includes two search phases for every pass. Frequent patterns are enumerated in both bottom-up and top-down directions.

Consider a pass k, the set of frequent k-itemsets Lk and the set of infrequent k-itemsets ~Lk are to be classified in the bottom-up direction. This procedure repeatedly uses Apriori-gen algorithm to generate candidates like the Apriori. The top-down procedure discovery frequent itemsets by search space decomposition. During the kth pass, every search space S?[X:Y] where X is an itemset of size k-1 can be decomposed into some little pieces, whose ancestors are k-itemsets. For search space S? [X:Y], the top-down procedure check whether the border element (i.e. YX ? ) of S is frequent firstly, if not, S is decomposed.

Implementation of this two-way-hybrid approach is shown in algorithm1.

Algorithm1. Hybrid Frequent Itemsets Mining  Procedure: HFIM (Transaction Set: D, Item Set: I)     Output: FI Begin  k=1; Ck=I; FIB =? ; Lk= Partition (Ck, minsup); ~Lk= Partition (Ck, minsup); FIB = FIB ?Lk;  ?=TFI ; ]}{[:1 I=? ; While ???k  do  //Top-Down Search ?=? +1k ;  Forall kS ??  do B=S.bor; If countsupport(B)<minsup then  If ???? }    { TFIRandBRR  then ? = Decompose(S, k, Lk, ~Lk); ???=? ++ 11 kk ; Endif;  Else BFIFI TT ?= ;  Endif; Endfor; //Bottom-Up Search Ck+1= Apriori-gen(Lk);  //candidate (k+1)-itemsets Lk+1= Partition (C k+1, minsup); ~L k+1= Partition (C k+1, minsup); FIB = FIB ?Lk+1;  k++; End; Return  FIT?FIB;  End;   Procedure: Partition (Ck, minsup) //classify Lk and ~Lk from set of candidate k-itemsets Ck Output: Lk, ~Lk Begin  ?=kL ; ?=kL~ ; Forall kj CX ? do  if CountSupport(Xj) ? minsup then jkk XLL ?= ;  else jkk XLL ?=~~ ;  Endif; Endfor;  End;  Procedure: Decompose (S=[X:Y], k, Lk, ~Lk) Output: search space set: ? Begin  ?=? ; }~  Z  {~ kk LandXZZP ??= ;  }  U  { kk LandXUUP ??= ;  }~{ kPVVYR ???= ; }{ kPTTRR ??= ;  Rn = ; Forall (i=1; i=n-1; i++) do If ki PXu ?  and kuuXu kii >+1 then  ]:[ 1 kii uuXu +??=? ; Endfor; Return ? ;  End;   ?. EXPERIMENTS and ANALYSIS  We run the simulation on PC: Intel P4 2.93G CPU, 1GHz main memory and Windows XP. The test databases are generated synthetically by an algorithm designed by the IBM Quest project in IBM Almaden Recearch Center, referring to Http://www.almaden.ibm.com/cs/quest/syndata#AssocSynData.

Figure 2 shows the perfermence both HFIM and Apriori algorithm. The performance measure was the execution time of the algorithm on the T10.I6.D10K dataset with different support threshold. From the figure we can make the following observation: our HFIM algorithm has the better performance Apriori.

Figure 3 shows the relative times of this two-way-  hybrid HFIM algorithm at varying minimal supports on the datasets of T15.I8.D10K.

To illustrate expandability of this algorithm, we  performed an experiment varying the database size from 5K to 20K. The average size of transactions is 10, and the   1 0 0 0  2 0 0 0  3 0 0 0  4 0 0 0  5 0 0 0  7 .0 % 6 .0 % 5 .5 % 5 .0 % 4 .5 %  M in im al Sup p o rt (T 1 5 I8 D 1 0 K )  Re sp  on se  T im  e( se  c.

)  H F IM  Figure 3. Times varying support thresholds      5% 4% 3% 2% 1% Minimal Support (T 10I6D10K)  T ot  al T  im e(  se c.

)  Apriori  HFIM  Figure 2. Total times varying Support thresholds     average size of potential maximal frequent itemsets is 6.

For the experiment we fixed a minimum support of 4%.

Figure 4 shows the result for the datasets.

?. Conclusions The two-way-hybrid approach could reduce both the  number of times the database is scanned and the search space rapidly. It can discovery the longer and the shorter frequent itemsets in earlier passes, and the infrequent (or frequent) itemsets discovered in the bottom-up can also be used to prune the search space in the other top-down direction. Furthermore, this algorithm can be parallelized easily on this hierarchical search space organization.

The set of frequent itemsets derived by most of the current itemset mining methods is too huge for effective usage. There are proposals on reduction of such a huge set, including closed itemsets, maximal itemsets, etc. However, it is still not clear what kind of itemsets will give us satisfactory itemsets in both compactness and representative quality for a particular application. So deriving a compact but high quality itemsets that are most useful in applications is our another future work.

