A Reconfigurable Computing Architecture for  Semantic Information Filtering

Abstract?The increasing amount of information accessible to a user digitally makes information retrieval & filtering difficult, time consuming and ineffective. New meaning representation techniques proposed in literature help to improve accuracy but increase problem size exponentially. In this paper, we present a novel reconfigurable computing architecture that addresses this issue, outperforms contemporary many-core processors such as Intel?s Single Chip Cloud computer and Nvidia?s GPU?s by ~20x for semantic information filtering. We validate our design using industry standard System-on-chip virtual prototyping and synthesis tools. Such a high performance reconfigurable architecture can form a template for a wide range of content- based and collaborative filtering engines used for big-data analytics.

Keywords?semantic comparison, Bloom Filter, SCC, GPGPU, reconfigurable computing, information filtering, recommendation systems

I. INTRODUCTION The amount of digital information created, captured or  replicated worldwide is expected to reach 3500 exabytes by 2020 [1]. This rate of information growth (~10x) makes search difficult, time consuming and unsatisfactory [2]. Data centers employ distributed computing frameworks such as MapReduce [3] to provide scalability for these problems through coarse grained task parallelism. However, this makes the overall system energy inefficient, resource intensive and failure-prone.

In recognition of these problems, mainstream server OEMs such as HP [4], Dell [5] & AMD [6] are exploring microservers for several data-intensive applications. The fundamental premise with microservers is that thousands of less powerful but highly power efficient System-on-chip (SoC) can get more done than hundreds of monolithic traditional processors. At the same time, there has been a clear recognition among processor/SoC manufacturers that many integrated cores (MIC) are more realistic than higher clock speeds [7, 8]. It is therefore likely that many-core SoCs will form the compute engines for datacenters of the future.

Many-core accelerators have evolved in two distinct paths (Fig. 1). On one hand, a large number of legacy uniprocessors (Fig. 1(e) such as x86) have been put together on the same die to form a cluster-on-chip (Fig. 1(c)) such as the Intel?s Single Chip Cloud Computer [9] (SCC). The SCC shares off-chip  memory access between a subset of cores and expect applications to be designed using message passing [9].

Fig. 1. Many-core architectures for data-intensive computing  On the other hand, Graphics Processing Units (GPU?s) employ massive thread-level parallelism (TLP) to mask memory latency (shared-memory MIC). The emergence of MIC as compute nodes of the future necessitates a revisit of the computational model, memory hierarchy and I/O pipelines to fully exploit the available concurrency within these processors.

Semantic information filtering (SIF) as a big-data application has been explored on a GPU and SCC in [10, 11].

Despite providing substantial speedups on both, they still remain memory-bound. Poor spatial and temporal locality of memory accesses leads to suboptimal performance levels on an SCC and GPU. Can this be improved? Secondly, is it possible to extract higher performance from each compute core by custom-designing its functionality in an application-aware manner. Thirdly, is it then possible to build a reconfigurable many-core computing machine that is a hybrid both shared & distributed memory MICs? Will such an architecture be scalable, provide high-throughput and enable higher performance gains at a low energy budget for big-data applications?

In this paper, we present a novel reconfigurable hardware methodology which addresses some of these challenges while considering semantic information filtering (SIF) as a case study. Our prototype System on Chip (SoC) reconfigurable processor core for SIF was designed from the ground up, evaluated on an industry-standard virtual prototyping platform for performance. This paper makes four key contributions:  CONTROL ALU ALU  ALU ALU  CACHE  DRAM  DRAM  D R A M  D R A M  D R A M  D R A M  (f)  (c) (d)  ALU ALU  ALU ALU  D is pa tc he r  C ol le ct or  (a)  (e)DRAM HDD  BUS  (b)  Conventional Distrib. System  Conventional SMP Processor  Distributed-Memory MIC - SCC  Cores in a Distributed- Memory MIC  Shared-Memory MIC - GPU  Cores in a Shared-Memory MIC      ? A reconfigurable hardware architecture which decouples computation and communication enabling multiple outstanding memory requests.

? A 5-stage reconfigurable computing kernel combining coarse-grained and fine-grained parallelism  ? An in-depth performance evaluation showing ~20x speedup over contemporary many-core processors  ? Extensive hardware-software codesign on an industry standard virtual prototyping tool to demonstrate scalability of proposed architecture.



II. APPLICATION CONTEXT: SEMANTIC INFORMATION FILTERING  In this section, we describe the need for hardware acceleration of semantic information fitering, the challenges involved and motivate the algorithm proposed in this paper.

A. Limitations of Vector-model & Scalability challenge with the Tensor-model Search engines traditionally deploy vector-based models as  to represent and compare unstructured feature vectors (using techniques such as TF-IDF, PageRank or LSI), ignoring the semantics involved. Consequently two phrases such as: ?American woman likes Chinese food? and ?Chinese woman likes American food? are considered similar (100%) because they contain the same keywords although they refer to distinct concepts. To alleviate this problem, new techniques have been proposed to represent composite meaning in the semantic computing community [12, 13]. While they rely on tensors (multi-dimensional vectors) to represent and successfully discriminate between complex concepts, they have been shown to increase the problem size super-exponentially [10].

Fig. 2. Semantic Information Filtering tool-chain  B. Representation of Semantics Fig. 2 outlines the overall sequence of steps necessary to  calculate similarity between the profiles of an item-user pair (Item1, User1) using semantic techniques. Free text feature vectors (comments, plot synopsys etc.) are converted using a concept (ontology) tree and natural language processing techniques (semantic processing) to generate a concept tree. A concept tree is a hierarchical n-ary tree whose leaves represent terms whereas its structure encodes meaning or semantics.

These trees are made to undergo a subsequent transformation to generate tensors using rules defined in [12, 14] without loss of semantics. The equivalent tensor form of a concept tree can be described as a large table of terms and coefficients where the terms represent distinct concepts (called basis-vectors) and the coefficients represent the relative importance of each concept in the tensor. For the purposes of this paper we assume that the basis-vectors are represented in their 64-bit MD5 Hash forms and the coefficient is a 32-bit number. It is easy to see that the  representation of an item or a user profile can be arbitrarily large; consequently the size of the tensor shall be in the hundreds of thousands [10].

C. Designing a massively parallel semantic comparator SoC Semantic information filtering (SIF) proceeds with the  accurate computation of similarity between every pair of user- item profiles represented as tensors. Computation of semantic similarity (s12  in Fig. 2) proceeds with: (1) identification of the common terms (say k) in the two tensors of size p,q respectively, (2) multiplication of the corresponding coefficients of the respective common terms to yield k interim products and (3) summation of these interim products to yield s12.

Given the temporal nature of the underlying data ? high performance and energy efficiency in computation of semantic similarity can result in large economic benefit and better user experience.  The rest of this paper focusses on the efficient computation of this semantic similarity using many-core processors given two feature tensors in their tabular form as an input (in memory).

If a sequential processor is used to compute semantic similarity on two tensors of size p,q respectively the identification of common terms has a time complexity of O(p logq) or O(pq) depending on whether or not a binary or linear search tree is used. However, for massive parallelization on MICs, we use a randomized algorithm using Bloom Filters (Algorithm 1).

Fig. 3. Set Intersection using a Bloom Filter (BF)  A Bloom filter (BF) [15] is a probabilistic space efficient data structure that enables a compact representation of a set and the fast computation of the intersection between two sets. It is described in the form of an n-bit long bit-vector. Elements of one set are inserted into an empty Bloom filter using k independent hash functions (hashk). The hash functions generate k distinct index values (BFIk) for every element which are then turned ?1? in the BF. The resulting bit-vector after inserting m elements of a set is a compact representation of the set. Elements of a second set can now be tested- iff all index positions due to a test element are ?1?, it is considered to be positive result (candidate match).

Fig. 3 shows a schematic view of the above operation with the basis vector terms of Tensor 1 (Set A) and Tensor 2 (Set B).

Elements of Set A are ?inserted? into the BF bit vector (shown in the center) by turning to ?1? the index positions returned by the hash bank. When elements of Set B are ?tested? with the BF and all the index positions are true, we can claim that the test element is a candidate match. The insertion & testing operation can be performed in parallel (one per processing element or thread) provided the Bloom filter is shared.

Interim products?Concept Tree  conversion  Tensor1  Tensorm1  Tensor1  Tensorm2  Tensor conversion  Term1 Coeff1 Term2 Coeff2  Termq Coeffq  Table Aggregation  Common1 Coeffq?Coeff2  Commonk Coeff1?Coeffp    Identification of Common Terms  Calculation of Similarity (s12)  Semantic Processing  Item Profile  User Profile  Term1 Coeff1 Term2 Coeff2  Termp Coeffp  Computation of Semantic Similarity  Term1 Coeff1  Term2 Coeff2  Termp Coeffp  Term1 Coeff1 Term2 Coeff2  Termq Coeffq    0th  2th  3th  4th  5th  6th  7th  (m-3)th  mth B  F In  de x  Po si  tio n  BFBF Set Operation  BF Test Operation  Set A Set B  1th  Hash Bank  Hash Bank  Tensor 1 Tensor 2  (m-2)th  (m-1)th     Testing whether an arbitrary element of the test set is present can result in false positives (return true, when the element is not present) ? a feature of randomized algorithms.

However, it is guaranteed to never return a false negative (return false, when an element is actually present). The probability of false positives (pfalse+ve [15]) can be made arbitrarily low for a given number of elements to be inserted (m) by choosing a large size for the BF bit-vector (n). In this paper, we chose the target pfalse+ve = 0.0001 and calculate n=f(m) accordingly for all experiments.

With the above analysis in mind, we propose a randomized algorithm to compute semantic similarity between two tensors (Algorithm 1). This approach uses a common shared BF bit vector of size m (lines 1-3, BF set operation on Tensor1?s terms t1i). An analogous test operation can then be performed on Tensor2?s terms t2j using the same hash bank (lines 6-7). If all the k BF indices (BFIk) return true, t2j is a candidate match.

Now we locate the corresponding coefficient of t2j in Tensor1 i.e c1m, if it exists (lines 8-11); multiply and sum this intermediate result. Location of this corresponding coefficient can be carried out using an off-chip content addressable memory [16] (CAM) lookup mechanism with t2j as the key; a single cycle operation. This eliminates the need for the loop between lines 8-12. However since the CAM lookup operation is going to be sporadic and involve significantly longer latencies, this stage has been pipelined ? to avoid core stalls.

Each of these functional units would then retain only a partial sum. This sum can be obtained using parallel reduction or centrally on a host processor/controller.

Algorithm 1: Massively parallel BF based SIF  Inputs: Tensor1(t1i,c1i), Tensor2(t2i,c2i) Output: Semantic (dot) product s12 1 parallel foreach t1i  Tensor1 do 2  compute k, BFIk =hashk(t1i) 3  k BF[BFIk] = 1 4 end for 5 parallel foreach t2j  Tensor2 do 6  compute k, BFIk =hashk(t2j) 7  if k BF[BFIk] = 1 then 8   parallel foreach t1m  Tensor1 do 9    if t1m == t2j then 10    s12 += (c2jxc1m) 11    end if 12   end for 13  end if 14 end for 15 return

III. RECONFIGURABLE COMPUTING ARCHITECTURE The computational and memory access requirements for  large-scale data intensive problems are significantly different from mainstream parallel applications, requiring new architectural solutions for efficient parallel processing. They are generally characterised by short parallel paths/threads with a small memory footprint, irregular, unpredictable memory access requirements.

The need for a reconfigurable processor occurs because the same processing units can be reused to execute a different  phase of the computation, while sharing the same interconnect network and conserving die area. It is possible to design specialized functional units (SFU) within a reconfigurable processing element (RPE) thereby providing both coarse and fine-grained parallelism required by an application. Several big-data applications may not require reuse of data, so cache memories and branch prediction logic is unnecessary. A simple memory hierarchy which allows cores/RPEs to issue multiple outstanding memory access requests to off-chip memory is advantageous.

Fig. 4. Reconfigurable hardware architecture template for data-intensive  algorithms  The overall architecture of the reconfigurable computing solution is illustrated at a high level in Fig. 4. It comprises of an execution controller (EC), multiple reconfigurable processing elements (RPEs), two distinct interconnection networks : core- core and memory-core. The RPE?s (shown with dotted lines) contain application-specific logic, are replicated and can independently execute application logic. The RPE?s are configured based on context word(s) delivered to it (shown in blue as configuration registers) in a similar manner to coarse grained reconfigurable arrays (CGRAs) [17]. The difference in the proposed architecture being: (1) the base processing elements (PE?s) in a CGRA contain ALU, multipliers, shift logic and registers whereas the RPE?s will contain software configurable application specific logic (further described in Sec. IV); (2) the interconnection structure of a PE array in a CGRA is pre-defined and fixed for ease of modeling generic applications whereas the proposed RPE?s can use generic memory-mapped cross-bar or packet-based (NoC) interconnect. In this paper, we use a cross-bar interconnect.

Two distinct interconnection networks are specified because the memory-core load and core-core loads are application dependent. The memory-core interconnect network links the RPE?s to r off-chip memory banks. These memory banks may be filled in using the DMA controller independent of processor interaction. The EC manages the operations of the RPE?s, including orchestrating initialization, task assignment, synchronisation; it also provides an interface to the host CPU processor. A group of RPE?s can also have shared memory accessible to more than one RPE. Each RPE is capable of independently reading and writing from/to its memory bank based on the configuration data sent to it as a bus-master.

The following section describes how we parallelized the SIF algorithm using the above reconfigurable architecture template. The following sections uses ARM and AMBA bus specifications only as a case-study because of it is an open  DRAM Bank1  Core-Core Interconnect Network  M em  or y-  C or  e In  te rc  on ne  ct   N et  w or  k  E xe  cu tio  n C  on tro  lle r  DRAM Bank2  DRAM Bank3  DRAM Bank4  DRAM Bank5  DRAM Bank r  Configuration Registers  Specialized Functional Unit (SFU)  Execution Controller  Reconfigurable Processing Element (RPE)  RISC Processor HDD Controller DMA Controller  System BUS     standard, and ease of integration with other AMBA-compatible IP blocks in our validation toolchain.



IV. SOC DESIGN FOR SEMANTIC INF. FILTERING Fig. 5 shows a high-level overview of the proposed SoC  architecture. In particular, we have chosen ARM Cortex A9 as the low-power RISC processor. When designed with TSMC?s 65nm generic process, it can be clocked at 1 GHz and consume <250mW. The rest of the figure describes the RPE matrix with 128 elements (RPE0-RPE128). Each RPE is provided configuration instructions via an execution controller. The RPE?s have been designed to use the AMBA APB [18] because  of the few signals necessary and limited I/O needs. The execution controller has  an AMBA AXI [19] master interface which are translated into the APB domain using an AXI to APB bridge [20]. Each RPE consists of two AXI Master ports which are connected via two interconnects (Memory-Core & CAM-Core) to the off-chip DRAM and CAM banks respectively. A separate p-bit bus from each RPE feeds into a separate RPE-Sync block on the SoC. Each of the components and rationale for designing them is explained in the subsections below. In the following description, BURSTx_READ and BURSTx_WRITE refer to AXI transactions which read or write x bytes in a single transaction respectively.

Fig. 5. Proposed reconfigurable SoC architecture for SIF    A. Role of the host processor (ARM Cortex A9) The host processor orchestrates the entire operation of the  SoC, partitions and loads data into the memory units, delegates and responds to interrupts from the Execution Controller (EC) and performs the final sum operation. Depending on user?s requirements and system constraints, it will partition the input tensor data (Tensor1 and Tensor2) into the RAM units via the DMA Controller at different addresses (64-bit addressing is used in the system). Tensor1?s terms and coefficients (t1i,c1i) are loaded into the CAM unit whereas terms alone (t1i) are loaded into the RAM units at their respective start addresses. Tensor2?s terms and coefficients (t2j,c2j) are both loaded at consecutive addresses into the RAM units immediately following Tensor1.

At the conclusion of the operation of each participating RPE, the execution controller generates an interrupt; triggering the core to issue a single BURST1_READ transaction to fetch the partial sums from the RPE?s and accumulate s12 (line 15 of Algorithm-2).

B. Design of the Execution Controller (EC) The execution controller initializes RPEs, delivers  configuration information, monitor RPEs progress and generate an interrupt to the host processor when the delegated task is complete. Its three key operations are summarized below:  1. Configuration & monitoring for BF set phase. The EC sends the following configuration registers to the RPE : (read_start_address1, num_data1, operation_id ) ? the address  for where to read t1i?s from, how many entries to read and operation_id=32?h1. This is performed as a BURST3_WRITE transaction from AXI_Master ports on the EC  AXI to APB Bridge  RPE?s APB slave port. The execution controller also receives a completion signal (32 bit, equal to operation_id) from the RPE?s once the BF set phase is complete. On receiving this signal, the EC proceeds to stage 2.

2. Configuration & monitoring of BF test phase. The EC sends the following config. registers to the RPE: (read_start_address2, num_data2, CAM_address, core_id, operation_id). These are similar in function to the above except that read_start_address and num_data now represent from where the RPE would read (t2j,c2j) and how many entries it would read. The operation_id=32?h2. The EC waits for the completion signal line from the core to learn that the BF test phase is complete and proceed to stage 3.

3. Retrive SUM, generate interrupt. The BF set phase will execute concurrently on all RPE?s unless there is a interconnect bottleneck. However, the BF test phase on the RPE?s will run asynchronously because it is data-dependent. If a particular RPE has a large number of potential matches (test_success=1), it will wait on the CAM units longer to return the corresponding coefficient(s) of the candidate matching tensor term(s) (c1m?s in line 10 of Algorithm 1). Thus different RPE?s will terminate at different times and will in that order inform the EC. The EC will generate an interrupt to the host processor, which in turn will fetch the partial sums (BURST1_READ) from the terminated RPEs.

DRAM Bank1  RPE-Sync  M em  or y-  C or  e In  te rc  on ne  ct   N et  w or  k  E xe  cu tio  n C  on tro  lle r  DRAM Bank2  DRAM Bank3  DRAM Bank4  DRAM Bank5  DRAM Bank r  ARM-CortexTMA9 HDD Controller DMA Controller  System BUS  CAM Bank1  C A  M -C  or e  In te  rc on  ne ct  N et  w or  k  CAM Bank2  CAM Bank3  CAM Bank4  CAM Bank5  CAM Bank s  AXI Master0 AXI to APB  Bridge  AXI to APB BridgeAXI Master   AXI Master??  AXI Master??-1  APB Slave AXI Master 1  AXI Master 2  BF Addr1  RPE0  APB Slave AXI Master 1  AXI Master 2  BF Addr1  RPE8  APB Slave AXI Master 1  AXI Master 2  BF Addr1  RPE7 RPE0 RPE1  RPE??  APB Slave AXI Master 1  AXI Master 2  BF Addr1  RPE112  APB Slave AXI Master 1  AXI Master 2  BF Addr1  RPE120  APB Slave AXI Master 1  AXI Master 2  BF Addr1  RPE15  APB Slave AXI Master 1  AXI Master 2  BF Addr1  RPE119  APB Slave AXI Master 1  AXI Master 2  BF Addr1  RPE127 AXI to APB  Bridge  AXI to APB Bridge  RPE0 RPE1  RPE??  On-Chip Off-Chip  BF_Addr0 BF_Addr63  BF_Addr64 BF_Addr128     C. Design of the RPE   Fig. 6. Reconfigurable Processing element (RPE) design for SIF  Fig. 6 presents a schematic overview of each RPE. Each RPE executes 5 distinct operations as part of two phases ? set and test. During the set phase, stages 1,2 & 3 are executed serially whereas during the test phase stages 1,2,4 and 5 are executed serially. Configuration instructions dispatched from the EC are stored in 32-bit configuration registers. These configuration registers drive the RPE state machine. Memory read requests to the memory banks are issued via the read channel of an AXI master subcomponent. We use a BURST16_READ in the increment mode for highest throughput. Once a basis_vector term is received, it is passed onto stage 2 to generate the required k Bloom Filter indices (BFI). Each BFIk is 22 bits long (corresponding to the bit- address of a m=222=4Mi wide BF bit-vector). We use k=7 indices per basis_vector term, which corresponds to a BF_Address bus p=154-bits wide. A BF_Address line for every core is routed externally to a BF-Sync module (described subsequently) for setting or testing a Bloom filter.

The test phase proceeds in an analogous manner. In this phase however, each core is expected to read both the (basis_vector,coefficient c2j) corresponding to a subset of Tensor2 allocated to it. The key difference being that once the BFIk bits  for the test basis_vector are generated, they are now tested for prescence in the BF. When Test_success is a true (shown as an input from the RPE_Sync module), it automatically triggers a lookup for the corresponding coefficient of Tensor1 (lines 8-11 of Algorithm 1). This lookup operation is issued by the RPE from its second AXI Master port as a BURST1_READ on a 128-bit address. The lower 10- bits of the 128-bit address are used to identify the cores identity (coreID). The next 64-bits carry the basis_vector of the candidate match. In case a CAM request is issued, the core does not stall; it proceeds forward in testing the next basis_vector in queue (pipelined). When the CAM request is returned with the corresponding coefficient of the candidate basis_vector  i.e. c1m, it is then multiplied with c2j yielding the partial sum (line 10 of Algorithm 1). We now describe the procedure for computation of BFI and its use in RPE-Sync.

1) Computation of Bloom Filter Indices (BFI) A Bloom Filter requires the use of k independent hash  functions to generate k index values. Alternately, k independent hash values can be produced from the output of only two hash functions h1(x) and h2(x) with the formulation BFIk=h1(ti) + ih2(ti) where i  without any degradation in false+ve probability [21]. In Table I, we show different alternative  mathematical operations that could be used to combine two initial hash functions and generate BFIk. , , + rot(A,j) represent the bitwise XOR operation, multiplication, addition and rotation of A by j bits. Each method was verfied in a statistical simulator to experimentally measure its pfalse+ve.

TABLE I.  METHODS TO GENERATE BLOOM FILTER INDICES (BFIK)  Method Operation Power Randomness  1 h1(ti) + rot(h2(ti), i) 557 W Fair  2 h1(ti) + i  h2(ti) 88 W Poor  3 h1(ti) + 2i  h2(ti) 637 W Poor  4 h1(ti)  i  h2(ti) 58 W Poor  5 h1(ti)  rot(h2(ti), i) 61 W Excellent  6 h1(ti)  2i  h2(ti) 92 W Poor    A multiplication (effectively a bit-shift to the left) introduces zeroes into vacated bit positions reducing the overall entropy. A subsequent XOR operation with these 0?s would retain the previous value, several bit-positions would become deteministic; thereby reducing the effecitveness of the bloom filter indices.  In contrast a cicular rotation operation preserves the entropy in the original data. Further, bit-wise XOR has a significantly lower power draw than an adder; hence method 5 was chosen as the preferred method to generate BFIk.

2) Design of the RPE-Sync module   Fig. 7. Construction of the RPE-Sync module  Fig. 7 shows the internal construction of the RPE-Sync module. This module enables the creation of a synchronous shared on-chip Bloom Filter for the RPE?s. RPE-Sync receives a set/test signal (active high) signal and a 154-bit wide BF_Addr from each RPE. The BF_Addr bus consists of k=7, 22-bit address bits for setting/testing the BF. Since there is no possibility of race conditions (BF bits are never set to 0) and that the , 22-bit address lines are guarenteed to be distinct, we perform the setting/testing of BF_Addr lines in a single cycle.



V. VALIDATION METHODOLOGY Initial functional verification of the design was performed  using ModelSim from Mentor Graphics after being implmented in RTL (Verilog). The core components of the design were then synthesized using Synopsys Design Compiler at a clock speed of 3 GHz using the 90nm technology library from TSMC. Given the complexity of the reconfigurable IP and the need to examine its behavior in the context of a data-intensive application, a full SoC virtual prototype was created using Carbon Model Studio & Carbon SoC Designer from Carbon Design Systems [22]. Carbon Model Studio and Carbon  2. Calculate BFIk  5. Calculate local SUM s12  RPE State Registers AXI Master  subcomponent (Port 1)  To Interconnect 1  AXI Master subcomponent  (Port 2)  DRAM Memory Bank  CAM Bank  APB Slave subcomponent  Execution Controller  1.Issue Multiple non-blocking memory load requests  3.Set Bloom Filter  Config Registers  Operation done  154 To RPE-Sync moduleBF_Address  Test_success Test_valid  From RPE-Sync module   4.Test Bloom Filter  To Interconnect 2  Set/Test  BF_Addr0  BF_Addr64  BF_Addr63  BF_Addr128  RPE-Sync Address Translation LogicTest_valid0  Test_Success0  1 1 0 1 1 1  1 1    BF  RPE0  Test_valid128 Test_Success128  RPE128  RPE0Set/Test0  RPE128Set/Test128  0 0 0 0  1 0 1 0 0 0 0 0       Compiler enables the creation of a high-performance linkable software object that contains a cycle and register accurate model of RTL. An object library file, header and database with information on all signals, top-level I/O?s together (Carbon Models) were created for the RPE?s, RPE-Sync and CAM modules. These carbonized models were run on Carbon SoC Designer (an Instruction Set Simulator). Carbon Models for basic components such as ARM Cortex-A9, AXIv2 compliant memory controller, interconnect infrastructure etc. was obtained with permission from ARM Holdings Inc., configured using AMBADesigner and then custom built on Carbon?s online IP Exchange.



VI. RESULTS & DISCUSSION To validate the effectiveness of our reconfigurable  computing methodology, we measure the performance of our SIF design on Carbon?s SoC Designer simulator. Our experiments focus on parallelizing a single semantic comparison. We experiment for (1) N varying from 25k to 160k and (2) similarity c varying between 10% to 100%.

TABLE II. shows the latencies for some basic operations.

TABLE II.  LATENCIES OF BASIC OPERATIONS - AXI PROTOCOL  Basic Operations Latency  RPE read from memory (BURST16_READ) 16+5 = 21 RPE read from CAM - (BURST1_READ) 9  RPE set configuration - (BURST4_READ) 10  Processing delay at RPE - (Multiply, partial sum) ~1 cycle  RPE calculate BFI 1  CAM lookup 1    A. Execution Time 1) with varying size of dataset   Fig. 8. Execution Time with increasing Tensor size (#Cores = 32)  Fig. 8 shows the averaged overall and phase-wise exeuction time for Tensor sizes p=q=N=25k to 160k when using 32 RPE?s. In this case, the number of RPEs used =32 i.e. each RPE has access to an independent memory bank (r=32), an independent CAM bank (s=32) ? an ideal case for the architecture. We would expect the set phase to execute for exactly 160000/32 = 5000 cycles at each core, which is indeed the case. Likewise during the test phase, each core will receive (basis_vector,coefficient) data (for Tensor 2) from its dedicated memory bank for (5000*2)=10000 64-bit entries. This will take  BURST16_READ transactions in  10000+625*5=13125 cycles. Each core will then issue separate CAM lookup requests depending on the number of Test_success it receives (generation of Test_success happens in 1 cycle). In this case, a total of 16000 entries (10%) in Tensor2 are expected to return it (distributed across the 32 cores). On an average, we determined that each core generates ~858 Test_success. This yields an additional 858*9=7772 cycles. An additional latency of ~1 cycle (Processing delay at RPE) occurs for each of the 10,000 elements. This value is quoted as ~1 cycle because of deep pipelining between the stages. Therefore the total cycles taken should be 13125+7772+10000 = 30897 (agrees with the results above).

Fig. 9 shows the overall and phase-wise execution time for Tensor sizes p=q=N=25k to 160k when using #cores = 128.

Total execution time is lower (~2x as compared to ~4x for #cores=32) because of cores stalls. Since #Memory banks (r) and #CAM banks = 32 has been kept fixed, several of the cores are starved for data.

Fig. 9. Execution time with increasing Tensor Size (#Cores = 128)  2) Sensitivity to change in characteristics of dataset   Fig. 10. Execution Time with varying similarity 10%-100% for a fixed size  Fig. 10 shows the sensitivity of the proposed architecture to variation in similarity (number of common basis vector terms) between two tensors of size p=q=N=160k. The execution time in the worst case (100% similarity) is ~60000 cycles decreasing to ~35000 cycles for 10% similarity. These experiments were performed with #cores= =32.

B. Comparision with contemporary Many-Core processors In TABLE III. we present a comparison of SIF on three  contemporary many-core processors for p=q=N=50k and sim=10%. Algorithm 1 when run on an Intel SCC, Nvidia Tesla C870 and Nvidia Kepler GTX680 provide a speedup of ~19x, ~10x and ~2x respectively for the proposed many-core SoC architecture. The 48-core SCC performs worst because it has no mechanism to maintain a shared BF across cores.

Nvidia?s Tesla & Kepler GPUs lack an efficient CAM lookup mechanism leading to worse performance.

se t  te st   to  ta l  se t  te st   to  ta l  se t  te st   to  ta l  se t  te st   to  ta l  se t  te st   to  ta l  se t  te st   to  ta l  25600 50000 74000 96000 130000 160000  Ex ec  ut io  n Ti  m e(  cy cl  es )  Size of Data - N (with ??=32, %sim=10%, r=32, s=32)  Overall Execution Time Core Active(Test) Core Active(Set) Core Stall(CAM read) Core Stall(Mem read)    se t  te st   to  ta l  se t  te st   to  ta l  se t  te st   to  ta l  se t  te st   to  ta l  se t  te st   to  ta l  se t  te st   to  ta l  25600 50000 74000 96000 130000 160000 Ex  ec ut  io n  Ti m  e (C  yc le  s)   Size of Data - N (with ?? =128, %sim=10%, r=32,s=32)  Overall Execution Time Core Active(Test) Core Active(Set) Core Stall(CAM read) Core Stall(Mem read)       se t  te st    to ta  l  se t  te st    to ta  l  se t  te st    to ta  l  se t  te st    to ta  l  se t  te st    to ta  l  10% 25% 50% 75% 100%  Ex ec  ut io  n Ti  m e(  cy cl  es )  Percent Similarity Between Tensor 1 & 2 - sim (with N=160k, ??=32, r=32, s=32)  Overall Execution Time Core Active(Test) Core Active(Set) Core Stall(CAM read) Core Stall(Mem read)     TABLE III.  COMPARISON OF RPE-SOC WITH INTEL SCC & NVIDIA GPU FOR SIF  Intel SCC  Nvidia Tesla [10] Nvidia Kepler  [11] Core  Architecture x86 Pentium - I C870 GTX680  Number of Cores 48 240 512  Main Memory 32 GB 4GB 64 GB Memory  Bandwidth 800 Mbps 76.8 Gbps 192.4 Gbps  Clock Speed 533 MHz 772MHz 1006 MHz Execution Time  (Cycles) 190k 100k 15k  Speedup ~19x ~10x ~2x

VII. RELATED WORK [23] describes the parallel implementation of a document  similarity classifier using Bloom Filters on two contemporary many-core platforms: Tilera?s 64-core SoC and Xilinx Virtex 5-LX FPGA. Although this work does not use tensors, they use a large array of Bloom filters at each processing element.

Secondly, this work assumes Bloom filters can be statically created. [24] presents a MapReduce method for accelerating tensor analysis by 100x over a single-node implementation.

[10, 11] describe a method to use a BF-based algorithm on a GPU, which is (1) limited by memory throughput, (2) uses general purpose GPU cores to port Algorithm 1, (3) has no capability to interface off-chip CAM. [25] presents a fine- grained parallel ASIC to realize Algorithm-2. This paper does not demonstrate scalability beyond p=q=N=1024, does not consider the impact of memory latency, does not consider the scalability of intermediate arbiters.



VIII. CONCLUSIONS & FUTURE WORK Workload specific server configurations for Big-Data  applications are now a reality. Workload specific accelerators are expected soon. Workload specific accelerators for Big-Data will need to be software reconfigurable to be applicable to a wide array of similar applications, provide extra-oridinary energy savings and high-performance to be a compelling alternative. In this paper, we described a novel software reconfigurable computing architecture for efficient and scalable semantic information filtering. We have shown that it is possible outperform the state-of-the-art SIF implementations on contemporary many-core processors (GPU, SCC) by more than ~20x times.

ACKNOWLEDGEMENT We would like to thank Carbon Design Systems, ARM Inc., Synopsys Inc., for supporting this research with required tools and models.

