WCCI2012 IEEE World Congress on Computational Intelligence  June, 10-15,2012 - Brisbane, Australia IJCNN

Abstract-In a previous work (Mohemmed et al. [11]), the authors proposed a supervised learning algorithm to train a spiking neuron to associate input/output spike patterns. In this paper, the association learning rule is applied in training a single layer of spiking neurons to classify multiclass spike patterns whereby the neurons are trained to recognize an input spike pattern by emitting a predetermined spike train. The training is performed in incremental fashion, i.e. the synaptic weights are adjusted after each presentation of a training pattern. The individual neurons are trained independently from other neurons and on patterns from a single class. A spike train comparison criterion is used to decode the output spike trains into class labels. The results of the simulation experiments on a synthetic dataset of spike patterns show a high efficiency in solving the considered classification task.



I. INTRODUCTION  One recognized feature of the neurons in the brain is the  use of spikes to exchange information. How the spikes are  utilized by the neurons to communicate is an important field in  neuroscience research [5], [16]. Understanding the neural code  helps in understanding how the brain performs its computation  functions and also in emulating it in order to design more  efficient artificial intelligent methods.

It has long been assumed that the information is represented  in the firing rate [4]. However, over the last two decades a  large volume of physiological and behavioral data has emerged  supporting a key role for temporal coding in the brain [2],  [15], [17]. For example, the temporal coding is known to  be effective in information transmission in the visual system.

The neurons in the early visual system from the retina to  the Lateral Geniculate Nucleus (LGN) to area VI produce very precise spike sequences in response to a repeated input  stimulus that cannot be interpreted within the formulation  of the firing rate [1], [15], [14]. Additionally, some pattern  recognition tasks such as colors, visual patterns, odours and  sound quality are not possible with rate coding [8].

There is no precise definition of temporal coding but in  general it relies on the time of the spikes and on the signifi?  cance of each spike rather than the number of spikes to carry  the information. Temporal coding assumes different formats  including the time latency to first spike, the order of the  spikes, the inter-spike interval and the precise timing of the  spikes [18].

U.s. Government work not protected by U.S. copyright  Institute for N euroinformatics  ETH and University of Zurich  Email: nkasabov@aut.ac.nz  Recently a number of learning algorithms based on temporal  coding have been proposed [3], [7], [11], [13]. Despite their  algorithmic simplicity, they are proved to be efficient in  complex spike computation tasks including precise time spike  sequence generation and spike pattern classification. These  algorithms are based on a network architecture consisting of  a single spiking neuron stimulated by many synapses. The  weights of the input synapses are tuned in a number of training  epochs until the neural spike output matches a desired pattern.

In a previous work [11], [12], the authors proposed SPAN,  Spike Pattern Association Neuron, a Leaky and Integrate  Firing (LIF) neuron that uses a supervised learning rule  for input/output spike pattern association. The main idea  of SPAN is converting the input, actual output and desired  spike patterns into analogue signals by convolving with a  real valued function, such as the alpha function, and then  applying the Widrow-Hoff rule [20] to adjust the synaptic  weights. The main advantages of this rule are its simplicity  and its straightforward implementation. With a single neuron,  it is possible to associate output spike patterns to complex  input spatio-temporal spike patterns. Although the algorithm  is efficient in precise time sequence generation, the main  envisioned application of the learning rule is spike pattern  classification. Because it is based on temporal coding, the class  label of a pattern is identified not only by the neuron firing  or not firing but also by the precise time of the output spike  pattern.

This paper focuses on extending the application of SPAN  to training a single layer of spiking neurons to classify spike  patterns. The input is given as spatio-temporal spike patterns  grouped into a number of categories. A single layer of SPANs  is trained to associate each category to a specific target spike  sequence.

The training procedure described in this paper has three  main differences over our previous work [11]. Firstly, the  training is performed in incremental fashion, i.e., the synaptic  weights are adjusted after each presentation of training pattern,  rather than batch learning, enabling the algorithm to be used  for online learning. Secondly, because multiple neurons are  used the decoding mechanism of the output spike sequences  into class label is based on the time and space distribution  of the output neurons adding redundancy to the class deter-    class 1 input .

.

class 1 group  ..

.......

.

..

..

'? .. , . . . . . ............ ? . : ... .?'" class 2 group  ..

.....

.

.

.

..

..

'? .. , . . . .

?  ? .. ? ? ? ? ? ? ? ? ? ? ?f ...

...........

. ? ...

desired pattern  group 1 output  group 2 output  desired pattern Fig. I. The architecture of a single layer spiking neural network. The dot lines refer to more neurons can be added to the network to detect different features of the input stimulus.

mination mechanism. Thirdly, the training of the individual  neurons is performed independently of other neurons, that is  only patterns of a single class are used to adjust the synaptic  weights of a neuron.

The paper is structured as follows. The next section briefly  describes the SPAN learning rule II. In section III, we dis?  cuss the network architecture, the training procedure and the  class label mechanism to identify the class label of unseen  pattern. Section IV and V present the experimental work and  discussion respectively. Section VI concludes the paper.



II. BRIEF DESCRIPTION OF SPAN LEARNING RULE  For the sake of completion, we briefly describe the learning  rule of SPAN and the reader is referred to [11], [12] for more  detailed explanation.

The neural model is a Leaky Integrate and Fire (LIF)  neuron with membrane potential u described by the following  differential equation:  (1)  where T m = RC is the membrane time constant of the neuron, Rand C are the resistance and capacitance of the membrane  respectively and Isyn (t) is the total current induced by the synaptic inputs.

SPAN is able to associate an input spike pattern to a desired  spike pattern by adjusting the weight (w) of the input synapse i in a number of epochs based on Widrow-Hoff learning rule:  (2)  where A E IR is a small real-valued positive learning rate, Xi is the input transferred through synapse i, and Yd and Ya refer to the desired and the actual neural output respectively.

To apply 2 on a spiking neuron where the signals are spikes,  the input, output and target spike patterns are convolved with  the alpha kernel function, other functions also can be used.

The final rule for adjusting the synaptic weight is given by  (see [12] for a mathematical derivation of the rule):  A G) 2 [L L(lt{ - t?1 + Ts)e-I'{ ?';)I 9 f  1,1 _'hi] -LL(lt{ -t?I+Ts)e-? h f  (3)  where Ts is the kernel function time constant, e is the ex? ponential constant, ti, ta and td are the times of the input, actual output and desired spikes respectively,. In [11] the rule  was used in a batch learning mode, i.e, in every epoch the all    input patterns are presented to the network sequentially, then  the error produced for each training sample is accumulated  and applied to compute the synaptic change tlWi for the next epoch. The next section describes an incremental version of  the rule applied on multiple neurons.



III. INCREMENTAL LEARNING OF SINGLE LAYER SNN  It is known that a single layer feed-forward neural network  (Perceptron) consisting of simple threshold or sigmoidal nodes  is not capable of solving nonlinear separable problems [10].

However, this limitation can be overcome if the nodes are  equipped with more advanced information processing com?  pared to threshold/sigma nodes or increasing the representation  dimensionality of the input vectors [19]. To some extent, both  of these features are applicable in a single layer network  consisting of spiking neurons. The spiking neuron in the  simple LIF model is controlled by a system of differential  equations and the input consists of high dimensional spike  patterns.

In the following subsections we discuss the application of  SPANs learning rule to train a single layer feed-forward SNN.

The envisioned computation task is that of spatio-temporal  spike pattern classification. The inputs are class labeled spike  patterns, each pattern consists of a number of spike trains.

The spike patterns are assumed to come from specific labeled  data (speech, video, etc.) converted into spikes. The conversion  mechanism of raw analogue data into spike patterns is an im?  portant issue that is left for future investigation. The objective  of the task is to train a single layer of SPANs to classify the  spike patterns into their categories by firing a target spike train  assigned to each class.

A. Training procedure The architecture of the network is depicted in Fig. 1. The  network consists of a number of SPAN groups equal to the  number of classes, and each group consists of one or more of  SPANs. A single group of neurons is assigned to identify one  class. Each neuron is trained to fire a specified target spike  sequence when a spike pattern of a certain class is presented  at its input synapses. The neurons are connected fully to the  input nodes. The question is why more than single SPAN per  class exist. The answer lies in increasing the redundancy to the  criteria of deciding the class label of the unseen patterns. For  example, the neurons of a single group can be trained to emit  different target spike trains to better capture the time structure  of the input pattern, or to train the neurons on different features  of the stimulus.

We note that each neuron of a group class is trained on  its class patterns independently from other neurons, i.e. the  synaptic weights of a single neuron are affected only by its  respective class patterns.

Because SPAN's learning rule is based on the Delta rule,  each neuron is trained by incremental learning steps. In a  single epoch, the input patterns that belong to a single class  are presented sequentially to a single SPAN, where after each  pattern presentation the produced spike trains and the target  spike trains are used to change the synaptic weights according  to Eq. 3.

B. Decoding the spike sequence output The second issue is how to interpret the spike sequence  output of the trained network into a class label to identify an  unseen spike pattern. The output of each neuron is a spike  train and there are multiple neurons for each class providing  a space-time relationship to the decoding criteria. The spatial  factor is represented by the neuron group and the time factor  is represented by the time accuracy of the output sequence  with respect to the desired sequence. A number of approaches  can be followed to decide the class label of a pattern. In this  paper we adhere to the following procedure.

We measure the dissimilarity between the actual sequence  output of a neuron and its assigned target pattern using  Eq. 4; once the average error for each neuron group has been  calculated, the neuron group that has the minimum error will  claim the class label of the pattern. The dissimilarity formula  is given by:  E = 100 !Yd(t) - Ya(t)! dt (4) where Yd and Ya are the kernel convolved signals of the desired and actual spike patterns respectively. For example,  assuming there are three neuron groups (GI, G2, and G3)  corresponding to three classes, each group consists of two  SPANs. Assuming that after presenting the network with an  input spike pattern, the computed errors, i.e. the difference  between the actual output train and the target train, of the  three groups are EI = {50.,50.}, E2 = {40.,80.} and E3 = {100., (}, where ( is a large number given as error for the neuron which does not spike. Given these error values, the  input pattern is decided to be of class E 1, since the average  error of this group is the smallest. This criterion gives the best  results for our experiments although other approaches such as  adopting the class of the SPAN that produces the smallest error  among the all neurons, could be followed.



IV. EXPERIMENTAL WORK  We performed a number of simulation tests to investigate  the performance of the learning rule in a classification task.

We measure the performance of the network in term of the  classification accuracy, i.e., the percentage of spike patterns  that are classified correctly in the testing phase. The number  of testing patterns is fixed to 100 per class. The simulation is  conducted using NEST [6]. Each experiment is repeated for  20 runs, where each run has different synaptic weights and  different input patterns initialized randomly. Other parameters  of the simulation and the spiking neurons are listed in Table I.

A. The input dataset The following procedure is used to generate the spike  patterns to train and evaluate the network. Firstly, a number  of spike patterns equalling the number of classes are gener?  ated randomly based on a uniform distribution. Each pattern    TABLE I TABULAR DESCRIPTION OF THE NEURAL MODEL AND EXPERIMENTAL SETUP.

Model Summery Neural model Leaky integrate-and-fire Synaptic model ex shaped synaptic currents Input Random input Connectivity All input neurons are connected fully to the output neurons  Neural Model Type Leaky integrate-and-fire (LIF) neuron Description Dynamics of membrane potential u(t): ? Spike times: t(f) : u(t(f?) = 1'J ? Sub-threshold dynamics: Tm ?? = -u(t) + R !"yn(t) ? Reset & refractoriness: u( t) = Ur V f : t E (t(f) , t(f) + Tref ) ? exact integration with temporal resolution dt Parameters Membrane time constant Tm = 10ms  Membrane resistance R = 333.33MQ Spike threshold 1'J = 20mV Reset potential Ur = Om V Refractory period Tref = 3ms Time resolution dt = O.lms Simulation time T = 200ms  Synaptic Model Type Current synapses with ex function shaped post-synaptic currents (PSCs) Description Synaptic input current !"yn(t) = L W LJ ex(t - t(f?)  Parameters  ex(t) = s , { e T-1 t e-t/Ts if t > 0 0, otherwise  Synaptic weight w E JR, uniformly randomly initialized in [0, 10] The kernel time constant Ts = Sms  Class 1 Class 2 Class 3 ?l ':"'?" :"; :" ".?":' .. ;:.:.? ... :,.?.: .. ? ....... . :.;:::> .:.;?.;'. ?: : -' j I"?' ' . ' :' . .  ?.". ,, ', .. ::\., ? 1 I .. . :::'::?.' .. -;''':' '?:'.?' :'? :.;' E ?"?"'"' ?." ., . -'--'---'..J'.:" . . ...... : ..... :, .?.. < ..... :.: ... . . , . . ..

.

, -..... : .... '? . ... :, .. :,: ... :.: ' ;  .

'.- .. ;:.-.:,: .. :? . ... ::.;::.;, ... '.

: ? .

::.'.,.,.: .. ? .... ;: .. :' ....... ',<: .. :,  .

. : .. , .

. : .? ,: ..

..... ..

.

::: .. ' .. , : ..... .... . Q.? ?? ' ?? ,.?.: :'.: . :.: ?.'.!, .; .?... : .?..? :::.:. , ..

.

: . ...

. -, ?.: ..... :,' .. ::, .'.. ..... " "  - '. , . . . ..

; . . ??:: . . :;? ...

1 1 , . " . .... : " ;',';':;;,0'> I [,., :<.'?:'..": I [?' ' . . / :?t : . [ [ ,.' '.?', ; ','::"": ,' .' 1 o 50 100 150 200 0 Time (ms) 50 100 150 200  Fig. 2. Example of spike patterns generated for training. The patterns are generated by adding a Gaussian time jitter of zero mean and 30 ms standard deviation to base patterns generated from a uniform distribution, see section IV-A.

consists of 64 spike trains, in which each train has 10 spikes  chosen randomly in the period of simulation. From each base  pattern, a number of training patterns are created by adding a  time jitter to each spike. The jitter is drawn from a Gaussian  distribution with a mean value of zero and a specified variance.

Following the same procedure, a set of unseen patterns is  created for each class to test the generalization of the network.

To give a visual impression of the created spike patterns,  three classes with three patterns generated by the mentioned  procedure are displayed in Fig. 2.

B. Experiments The following tests are conducted to evaluate the perfor?  mance of the network.

Test 1: Number of Training Patterns. This test investigates  how the number of training patterns affects the classification  accuracy. It is known that if the number of patterns is small,  over-fitting on the training patterns may occur, but if the  number of training samples is large then the network might  tend to forget previously seen patterns. The training dataset  consists of three classes, hence the network constitutes three  neurons, each assigned to identify a single class by producing  a target spike train = {132., 142., 155., 165.} ms in response to the pattern that belongs to its assigned class. The timing  of the target spikes is set arbitrary after a few trial and error  attempts, but in general the target spikes are located at the end  of the spike pattern period. The number of training patterns is  changed from I to 50 in 5 patterns steps. The accuracy results of this test on the testing patterns are shown in Fig. 3.

Fig. 3 shows the average accuracy of classification for three    'if.

? ? a ? '" co ? ?  100.-----?------?------?------_r------_n     *-* Jitter std = 50ms .-. Jitter std = 70ms  200?----??----??-----3?0=======4?0======?50 No. of Training Patterns pe r Class  Fig. 3. The classification accuracy vs. the number of training patterns per class for three datasets of different amount of jitters.

cases that differ in the amount of jitter added to generate  the training patterns: jitters with standard deviation of 30ms,  50ms, and 70ms. Clearly, the network efficiently recognizes  the unseen patterns with accuracy of 97% when the jitter is 30%, reducing to 60% when the jitter is 70ms. Because the training is performed in a single presentation of the training  patterns, a sufficient number of patterns is required to train  the network to achieve acceptable generalization ability. For  the given setup and with a jitter of 30ms, 30 patterns per class  were required.

Test 2: Number of Neurons per Class. This test is performed to check the impact of increasing the number of  output neurons trained to classify a single class. We compare  the accuracy when two neurons are assigned to each class with  that of a single neuron per class.

In the case of two neurons, the first neuron is trained  to produce the same previous spike sequence of Test I:  = {132., 142., 155., 165.} ms, and the second one is  trained to produce a different sequence: {125., 147., 159., 170.} ms.

The training dataset consists of 50 patterns per class generated  from a jitter of 50ms. The accuracy of this test is shown in  Fig. 4.

The figure shows that the two neurons network has better  generalization accuracy than a single neuron. Changing the  time of the target spikes (Single Neuron Train I vs. Single Neuron Train 2 in Fig.5) influenced the performance; the  second target train is better aligned to capture the time  structure of the input which is reflected in a better accuracy.

Combining two neurons each having different output spike  patterns improved the accuracy over the performance of single  neurons. However, training two neurons is computationally  more expensive than a single neuron as the number of synapses  to be trained is doubled.

Test 3: Number of Classes. This test examines the effect of increasing the number of classes on the classification per-     'if. 70 ? ? a 60 ? '" co ? 50 ?    200 30  Single Neuron Train 1 Single Neuron Train 2  40 50 No. of Training Patterns pe r Class  Fig. 4. Accuracy performance vs. number of SPANs per class (Test 2). The (Single Neuron Train I) and (Single Neuron Train 2) are accuracy plots for networks with single output neuron per class but with different output spike sequence. The (Two Neurons) plot is for a network with two neurons per class.

formance. Because the training of the neurons is independent,  increasing the number of classes could increase the chance  of misclassification since no mechanism is used to inhabit  the other neurons when one of the output neurons fires.

The network uses two SPANs per class and the patterns  are generated by adding a jitter of 30.ms. The accuracy of  classification for 3 and 6 classes is shown in Fig. 5.

Class Labe l  Fig. 5. Accuracy performance vs. number of classes (Test 3).

Increasing the number of classes did not produce a notice?  able change in the overall accuracy of classification (97.5% for 3 classes compared with 97.4% for 6 classes). Also, we have tested a 12-class network in which the average accuracy  was found to be 95%. However, in a separate test with patterns generated from a jitter of 50ms, we noticed that increasing the  number of classes caused a significant decrease in accuracy.

Thus, the network has a certain capacity to perform the  classification properly.



V. DISCUSSION  A number of issues arose from applying the SPAN learning  rule to training a single layer SNN. First, the training is  performed independently on each neuron. During training a  single SPAN learns only the patterns of its class. This is  possible because during training many synapses will adapt  to lock on the temporal structure of the corresponding class  patterns and will automatically reject, by firing differently,  other patterns. However, this property might highly depend  on the intraclass separation of the patterns.

The second issue is deciding properly the spike times of  the target trains in order to more accurately classify the given  dataset. Here, the times of the target spikes were selected  arbitrarily after few trials and were preferentially chosen to  be near the end of the pattern time period.

The third issue is decoding the class label from the output  spike sequences. As pointed out in the Introduction, biological  neurons produce very similar spike sequences in response  to the same input stimulus, but it is not clear how these  sequences are associated with different categories of the  input stimulus [9]. The proposed criterion, which might be  biologically implausible, is based on measuring the strength  of association between the actual output and the target trains.

The multiple neurons assigned to each class can have not only  different target trains but different parameters, to give more  flexibility in tuning the neurons to recognize specific classes  and reduce misclassification. One advantage of this could be  training the neurons to detect different features of the input  stimulus. However, further study is needed to analyze how and  to what extent adding more neurons can assist in improving  the performance.

It is noted that the obtained accuracy is higher than that  obtained in our earlier paper [11]. This is attributed mainly  to the classification criterion used in the present work which  is more relaxed and aimed at achieving higher classification  accuracy, rather than producing very precise spike sequences.

In addition, the network architecture consists of several SPANs  rather than a single SPAN to perform the classification as was  done in [11]. Using a spike sequence instead of single spikes  to encode the class label further improves the performance.



VI. CONCLUSION  This paper investigated the plausibility of using the precise  timing of spikes to perform spike pattern classification. The  network consists of a single layer of spiking neurons trained by  a supervised learning algorithm [11]. Each neuron is trained  incrementally where the synaptic weights are adjusted after  each pattern presentation and independently in a single class  of patterns. The pattern class of the pattern is determined  by the output neurons producing a sequence of spikes rather  than firing or not firing. We have demonstrated a potential  advantage in training multiple neurons per class with different  characteristics such as their output sequence.

Future work will consider applying the algorithm to real?  world application in particular to the field of computer vision.

