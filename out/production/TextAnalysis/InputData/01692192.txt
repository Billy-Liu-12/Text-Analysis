Efficient Improvement of FT-tree Based Frequent Itemsets Mining  Algorithms

Abstract  The Z/O cost of database scanning has been a bottle-neck problem in data mining . Many algorithms proposed recently are based on FP-tree. These algorithms include all frequent itemsets mining, closed frequent itemsets mining and top-k closed frequent itemsets mining. However, creating FP-tree from database must scan database two times. In order to enhance the e f f i cncy  of FP-tree based algorithms, using bufer and merging methods, a novel algorithm called QFPC was proposed which can create FP-tree with one database scanning.

1. Introduction  A fundamental component in data mining tasks is finding frequent patterns in a given dataset[l].

Frequent patterns are ones that occur at least a user- given number of times (minimum support) in the dataset. They allow us to perform essential tasks such as discovering association relationships among items, correlation, sequential pattern mining. Algorithms proposed in [1,2,3,4] find all frequent sets in a dataset.

The Apriori algorithm [ I ]  accomplishes this by employing a bottom-up search. But almost all previous Apriori-like algorithms use the candidate set generate-and-test approach. FP-tree based mining [4] is an exception. It has performance improvements over Apriori since it uses a compressed data representation and does not need to generate candidate sets.

FP-growth[4] can mine all frequent itemsets(AF1S) in database. However, when a transaction database is very dense and the minimum support is very low, i.e., when the database contains a significant number of large frequent itemsets, mining all frequent itemsets might not be a good idea. To solve this problem, another type of a frequent itemset, called closed frequent itemsets (CFIS), was proposed in [5]. A frequent itemset X is closed if none of its proper  supersets have the same support. Any frequent itemset has the support of its smallest closed superset. The set of all closed frequent itemsets thus contains complete information for generating association rules. In most cases, the number of CFIS is far less than the number of AFIS.

But there still have a problem: to come up with an appropriate min-support threshold, one needs to have detailed knowledge about the mining query and the task-specific data. To solve this mining Top- k frequent closed itemsets(TCF1S) was proposed in[6,8], where k is a user-desired number of frequent closed itemsets to be mined (which is easy to specify), top-k refers to the k most frequent closed itemsets .

All the algorithms mentioned above are based on FP-tree. However, creating FP-tree from database must scan database two times. In order to enhance the efficiency of FP-tree based algorithms, we propose a novel algorithm called QFPC which can create FP-tree with one database scanning.

2. FP-tree based algorithms Han et al. proposed a data structure called frequent  pattern tree or FP-Tree [4]. FP-growth mines frequent itemsets from FP-Tree without generating candidate frequent itemsets. The construction of FP-Tree requires two data scans. In the first scan, the support of each item is found. In the second scan, items within transactions are sorted in descending order according to the support of items.

Algorithm 1 (FP-tree construction)[4] Input: A transaction database DB and a minimum support threshold 5.

Output: Its frequent pattern tree, FP-Tree Method: The FP-tree is constructed in the following steps.

1. Scan the transaction database DB once. Collect the set of frequent items F and their supports. Sort F in support descending order as L.

2. Create the root of an FP-tree, T, and label it as "null", for each transaction in DB do the following. Select and sort the frequent items in transaction according to the order of L. Let the sorted frequent item list in transaction be [pip], where p is the first element and P is the remaining list. Call insert-tree ([PIP], T).

Function insert-tree ([PIP], T) {If T has a child N such that Nitem-name = pitem-name Then increment N's count by 1;  0-7695-2616-0/06 $20.00  ? 2006    Else do {create a new node N; N's count = 1; N's parent link be linked to T; N's node-link be linked to the nodes with the same item-name via the node-link structure;) If P is nonempty, Call insert-tree (P, N).)  To minie frequent closed itemsets from FP-trees..

CLOSET+[8] was proposed. It use the top-down pseudo tree-projection and upward subset-checking for sparse datasets, whereas for dense datasets, the bottom-up physical tree-projection and a compressed result-tree have been adopted. The CLOSET+ algorithm is described as follows.

First, scan TDB once to find the global frequent items and sort them in support descending order. The sorted frequent item list forms the f-list. Second, scan TDB and build FP-tree using the f-list. Third, With the divide-and-conquer and depth-first searching paradigm, mine FP-tree for frequent closed itemsets in a top-down manner for sparse datasets or bottom-up manner for dense datasets. During the mining process, use the item merging, item skipping, and sub-itemset pruning methods to prune search space. For each candidate frequent closed itemset, use the two-level hash indexed result tree method for dense datasets or pseudo-projection based upward checking method for sparse datasets to do closure checking. Finally, stop when all the items in the global header table have been mined. The complete set of frequent closed item-sets can be found either from the result tree or the output file F.

TFP[9] can mines the set of top-k frequent closed itemsets(TCF1S). TFP first builds the FP-tree from the input database DB: If the number of frequent items in an input transaction T is no less than the minimal length, T is inserted into FP-tree, in the mean time, it uses the closed node count method to raise minimum support, min-sup, and uses the raised min-sup to prune infrequent items from the FP-tree. After the FP- tree has been constructed, the descendant-sum method is adopted to further raise min-sup and prune FP-tree .

Then, we begin the mining process: Top-down traverse each item in the global header table. If the corresponding item's I-count is no less than the current min-sup, treat this item as a frequent prefix itemset, build conditional FP-tree (together with its header table) for it, and mine the frequent closed itemsets.

Finally, output the top-k frequent closed itemsets fiom the result pattern tree by traversing it in a bottom-up manner and in the support descending order.

3.Create FP-tree with one database scanning  As analyzed above, it is obvious that Constructing FP-tree is the key step in FP-tree based algorithms. FP- tree contains compressed message of frequent itemsets, it is easy to mining frequent itemsets fiom FP-tree. To construct FP-tree, we must scan data base twice, one for creating 1-itemset and one for build FP-tree. To mine useful information from database effectively, we needs to solve efficiency problem of mining, when data quantity very large, efficiency of mining algorithm become the key of mining problem, to increase efficiency of mining algorithm, developing one database scanning algorithm is an effective policy.

In this section we present a algorithm to construct FP-tree with only one database scanning.

First, we divide the datasets into n parts. For example, Every part contains 1000 transactions. Then, for each part of database, we scan transaction data one time, read data into a memory buffer , create a local FP-tree and merge into result FP-tree. When the loop finished, the result FP-tree is the global FP-tree. We need not to create same node link for sub FP-Trees because it is only a local part of database, we create same node link after all sub FP-Trees are merged into one FP-tree.

Algorithm 2 (QFPC: Quick FP-tree Constructing Algorithm) Input: A transaction database DB and a minimum support threshold 5.

Output: Its frequent pattern tree, FP-Tree Method: The FP-tree is constructed in the following steps.

{ T=NULL,TI= NULL,TZ=NULL Divide database into n parts averagely. For example, Every part contains 1000 transactions.

For 1=1 to n DO {Scan the part(1) of database, read the transactions into buffer, Collect the set of items F and their supports. Sort F in support descending order as L.

Create local FP-tree as TI FP-Merge(T,Tl ,T2) //merge T1 and T2 into T Tl=T2,T2=T,T=Tl //swap T,T2 TI= NULL,T2=NULL) Prune the items from T where its support is less then 5 Create same node link for T Return T) AIgorithm 3 (FP-merge FP-tree merging algorithm, do not create head table and node link in result FP-tree ) Input: Two sub FP-trees Output: a merged FP-tree named FP-tresresulP Method: call FP-merge( result, FP-tree], FP-tree2) Procedure FP-merge(result, FP-tree], FP-tree2:FP-TREE) { Count the item support number from FP-tree1 and FP-tree2 and sort the items in descending order,insert into head table of result wh~le FP-tree1 !=NULL, do { retrieve one item train in FP-tree1 from leaf to root; sort the items in descending order of head table n=leaf s count; Let the item train be [PIP], where p is the last element and P is the frontal list.

Call merge-lnsert ([PIP], result, n).

Remove the item train from FP-tree];) while FP-tree2 !=NULL, do { retrieve one item train in FP-tree2 from leaf to root; sort the items in descending order of head table n=leaf s count;  0-7695-2616-0/06 $20.00  ? 2006    Let the item train be [PIP], where p is the last element and P is the frontal list.

Call merge-insert ([pip], result, n).

Remove the item train from FP-tree2;)) Algorithm 4 (merge-insert: insert a items train into FP-tree) Input: item trains, FP-tree node, count of trains Output: a merged FP-tree Method: call merge-insert (items train, node, support) Procedure merge-insert ([pip], T, num) {If T has a child N such that N.item-name = pitem-name N's count= N's counwnum; Else do {Create a new node N; N's count = num; N's parent link be linked to T;) If P is nonempty, Call merge-insert (P, N, num);)  To explain the algorithm, we use an example with the transactions shown in Table 1. Let the number of n be 2mivide database into two pars) and the minimum   Table 1. An exampleabase  DB  a Figure 1. Create sub FP-trees Tl(1eft tree) from  1, ~ 2 ( r i ~ h t  tree) from DB  Figure 2. After moving (cfam1)l from T1 to Result  Im : I  I Figure 3. After moving (cfa)2 from T1 to Result  *I Figure 4. Afier moving (cfa)2 from T1 to Result  Fi ure 6. 8 [m : 2  1  After moving (cfa)2 from,Tl to Result  Im : 2  1 7. After moving (cfa)2 from T1 to Result  a Figure 8. After moving(cfa)2 from T1 to Result  +I Figure 9. After Pruning the items h  0-7695-2616-0/06 $20.00  ? 2006    4. Experiments and Results  The goal of the experiments is to find out the extent of different dataset properties that could affect the performance of QPFC algorithms and the relative performance compares with other FP-tree based algorithms. In this section we will evaluate QPFC+QFP-growth., QPFC+CLOSET+ and QPFC+TFP in comparison with three algorithms FP- growth,CLOSET+ and TFP. All the experiments were performed on Pentium 4 1.8GHz PC with 512Mb RAM running on Window XP. Datasets are generated with the data generator by IBM QUEST. FP-growth is provided by the original authors. All programs are complied with the same compiler. Experimental results show it is efficient and scalable for mining large databases or data warehouses.

-+- C W + w  ith QPFC I-caOSCI*!

Figure 10. Run time Comparison of CLOSET+ and CLOSET with QFPC  C  2 3    0.5 1 1.5 2 2.5 3  Support Thresho Id(%)  5. Conclusion  In this paper, we have introduced an efficient implementation of an FP-Tree constructing algorithm.

QFPC can create FP-tree with one database scanning, Our future research work is to use the method into implementation of SQL-based, highly scalable FP-tree structure, constraint-based mining of frequent patterns, sequential patterns, max-patterns, partial periodicity and other interesting frequent patterns mining.

6. References  R. Agrawal and R. Srikant. Fast algorithms for mining association rules. In VLDBY94, pp. 487- 499.

R. J. Bayardo. Efficiently mining long patterns from databases. In SIGMOD'98, pp.85-93.

K. Wang, Y. He and J. Han, Mining Frequent Itemsets Using Support Constraints , Proc. Int.

Conf. on on Very Large Data Bases (VLDB'OO), Cairo, Egypt, Sept. 2002.

J. Han, J. Pei, and Y. Yin, Mining Frequent Patterns without Candidate Generation(PDF), (Slides), Proc. 2000 ACM-SIGMOD Int. May 2000.

Grahne G, Zhu JF. High performance mining of maximal frequent itemsets. In: Proc. of the 6th SIAM Int'l Workshop on High Performance Data Mining (HPDM 2003). 2003.135-143.

N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal, "Discovering Frequent Closed Itemsets for Association Rules," Proc. Int'l Conf. Database Theory, pp. 398-416, Jan. 1999.

