

RECOMMENDATION OF NEW ITEMS BASED ON INDEXING TECHNIQUES  JIAN CHEN, JIAN YIN, JIN HUANG  Department of Computer Science, Zhongshan University, Guangzhou 5 10275, China E-MAIL: ellachen@l63.com  Abstraft.

The amount of information in the World Wide Web is  increasing far more quickly than our ability to pmeess.

Recommender systems apply knowledge discovery techniques to help people find what they really want. These techniques include collaborative filtering (0, association rules discovery and Bayesian networks, etc. Unfortunately all of these approaches have an important drawback: items or pages which being added to a site recently cannot be found. This is generally referred to as the ?new item problem?. In this paper, we introduce a general framework for solving this prohlem and present a single index strnctnre X.Features-Tree for using heuristic information retrieval technique to find the right items for the right users.

Keywords:  nearest neighbor; indexing techniques Recommender system; web usage mining: reverse k  1. Introduction  The World Wide Web continues to grow at an amazing rate as information gateway and as a medium for conducting business. All of us have known the feeling of being overwhelmed by the number of news, advertisements and all kinds of products for sale in E-commerce sites. So it is necessary to find technologies that can dramatically reduce the useless information and help us sift through all the available information to find which is most valuable to us. Recommender system is one of these technologies by identifying particular items that are likely to match each user?s tastes or preferences.

One of most widely used technologies for building recommender systems is collaborative filtering (CF) [?I. It is based on the idea that the active user is more likely to prefer items that like-minded people selected. Given a target user?s records of activity or preferences, CF-based techniques compare those records with the historical records of other users in order to find the top k users who have similar tastes or interests. The mapping of a visitor record to its neighborhood could be based on similarity in  rating of items, access to similar content or pages, or purchase of s ihlar  items.

Other techniques which have been successfully applied in recommender system include association rules discovery, Bayesian networks and Horting. An association rule is an implication expression which describes the relationships between the accessed pages and the purchase items. Once the association rules are extracted from user transaction database, prediction of the active user?s preference will be generated according all these rules[*?. Bayesian networks create a model based on a kaining set with a decision tree at each node and edges representing user?s information and focus on a single relationship between the users. The main idea of the approach is in the context of the Web there is often more relationship information than the simple person-item relati~nship[~?. Horting is a graph-based technique in which nodes are users and edges between bodes indicate degree of similarity between two users.

Prediction will be generated by traveling the graph to nearby nodes and combining the opinions of the nearby users[41?.

Unfortunately, all of these existing approaches for recommender system must rely on the usage history of users and just focus on the current demands of user, so it?s an inevitable thing that they have an important drawback items or pages which are added to a site recently cannot be found. Because for these new items or pages, there are no accessed records, no ratings from users and still no relationship are found by algorithms, all of which make it difficult for people discover them. This is generally referred to as the ?new item problem?. In this paper, we present a new framework aiming at solving this problem. The main idea is extracting semantic feature from items or pages content firstly, and clustering the users who appear to have similar preferences according to access similar content.

when a new item or page appears, we find its ?influence? user clusters by using new information indexing concepts and technologies. Finally, this item (page) will be recommended to these groups of users that seems to have  0-7803-8403-2/04/520.00 WOO4 IEEE  mailto:ellachen@l63.com     the similar tastes or interests matching with the semantic feature of this item (page).

2. Data Preparation  The critical start for our successful target based on Web is data preparation. Data cleaning is the task of removing log entries that are not necessary for the mining process, including eliminating irrelevant and unreasonable items and removing all log entries with filename suffixes representing images and sounds. Pageviews identification is the task of determining which page file accesses contribute to s single browser display. User transaction identification is to identify semantically meaningful groupings of pageviews in each user session, based on an underlying model of the user's browsing behavior. Finally, removing very low support or very high support pageviews references, i.e., references to those pageviews that do not appear sufficient number of the transactions, or those that are present in nearly all transactions, can further filter the transaction file.

3. Integrating Indexing Technologies to Recommender System with Semantic Features  3.1. Extracting semantic features from pageviews  Pageviews are semantically meaningful entities such as pages or items. User transactions are relevant subsets of pageview in each user session. Data preparation results in a set of n pageviews, P = {p , ,  p2 ,.._, p n )  and a set of m user transactions,T=[t,,t, ,.... t , ]  where each r, E T (with a unique identifier TID) is a subset of P. We use the concept described in reference [2] to define P and T. Each transaction f was viewed as an l-dimensional vector over the space of pageview reference  t = ( w ( P , A . w ( P ~ . ~ ) .  .... w(P,.o) where eachpiEPfor  some Cl{l ,  ..., n ) ,  and w(p, , t )  is the weight associated with pageview p,  in the transaction f representing its significance. The weights can be determined in a number of ways meeting the demand. For example, association patterns mining just use binary weight to represent the existence or non-existence of a product- purchase or a pages access in the transaction. On the other hand, content mining may use a function of the duration of the associated pageview in order to capture the user's interest in a content page. Thus, the set of all user transaction can be viewed as an mxn transaction-pageview matrix, denoted by TP.

Semantic Features can be extracted from both text and  meta-data. General text mining can be used to find the features in the text. For example, classification of content based on a concept hierarchy can be used to limit the discovered patterns just containing those pageviews which are about a certain subject or class of products. For features extracted from text, we follow a commonly used method in information retrieval to use a standard function of the term frequency and inverse document frequency to evaluate features weights. For features extracted from meta-data, because we assume the features weights are provided as part of the domain knowledge specified by the designer, it becomes particularly important when dealing- with product-oriented pageviews or those involving non-textual content.

Given the above concepts, each pageview p can be represented as a k-dimensional feature vector, where k is the total number of extracted features from the site in a global dictionary. Each dimension in a feature vector represents the corresponding feature weight within the pageview. Thus, the feature vector for a pageview p is given by:  where is fw(p,f,) the weight of the jth feature in P = (fw(P,f,).  fw(P. f?)  ..... M P ,  f, ))  pageview P E P ,  for some j U [ l ,  ..., k]. For the whole collection of pageview in the site, we have the nxk pageview-feature matrix PF = [p l ,  p ?,.... p. 1. Then, if we map each pageview of PF in a transaction t8 to one or more content features, we can get a new matrixTF=[t',,l'2,..., f',,,], where each t' ,  is a k-dimensional vector over the feature space. Thus, a user transaction can be represented as a content feature vector, reflecting that user's interests in particular concepts or topics.

3.2. User transaction clustering based on a efficient index structure  In contrast to C& clustering user transactions based on a transaction-feature matrix TF does not require explicit raings or inieractiou with users.

Ccmplete user transamon database  . *  .

After the CIUstering based on thn transaction features sun ilarity  . 0.

. *  . .  . . .. - 0  . .  0 : .

.I . . .. ..

. .. . ..

0 .  :. . .

- 9 .  . . .

Figure 1. Clustering Algorithm partitions user transactions into several groups  Traditional clustering techniques generally have had accuracy and low performance for very large-scale problems. .Because the number of features in a transaction vector usually is in tens to a few hundreds in typical applications. Some techdques l i e  SVD"' have been widely used to increase density and reduce the dimensionality of high-dimensional feature space. However in many cases, the resulting from reducing the dimensionality will still have a quite large dimensionality.

And the remaining dimensions are all relatively important which means that any efficient indexing method must guarantee a good selectivity on all those dimensions. Based on our studies and observations in indexin techniques on high- dimensional data, we choose X-Tree as the basic structure and user transactions clustering algorithm in our framework.

To cluster transactions we need a measure of distance between two transactions. Given two transactions t and s, we define the similarity sim(t,s) as the normalized cosine of the angle between the two feature vectors.

$61  I t n s I  Dist(t ,s)  = sim(r,s).=- * Such a clustering algorithm will result in a set  TC=(c,,c2, ..., ckJ of clusters, where each ci is a subset of T.

Ideally, each cluster represents 'a group of users with similar interest which described by a vector of features.

33. Recommending new items based on the concept of Influence Sets  K Nearest Neighbor (k") problem is an important topic in real work applications a d  research domains such as in information retrieval, multimedia systems, spatial databases, and data minmg etc. Recently, more and more attentions have been paid on its reverse version, which is known as "Influence Sets" problem. The Reverse k Nearest Neighbor problem is to find all points in a data set that take a given query point as one of their k Nearest Neighbor"'.

This notion arises in examples such as finding the set of customers affected by the opening of a new store outlet location, or deciding the service scope managed by a certain call center, etc. In our case, recommender system will notify the subset of Web users who will find a newly added page or item most relevant.

Firstly, we give the formal definitions of these two problems. Given a set S of n 'data points in some d dimensional space and a query point q,  Definition 1: The k Nearest Neighbor of q is defined as:  k " ( q ) =  [S' I S ' E  s h p, E S'A pz E s : D i s t ( q , p , ) . ~ D i s t ( q , p , ) , I S ' l = k ~ n ]  Definition 2 The Reverse k Nearest Neighbor of q is defined as:  R m N  (4) = { P  E S I E W N  ( P ) } What is worthy of notice is RkNN(q) may be empty, or  Theorem 1: PE R k " ( q )  w qc k" ( p ) have one or more elements.

This is .obvious regarding the definitions of k" and  Corollary 1: Suppose k", is the distance between  D i s t ( p , q ) < k " ,  = , p ~ R k h " ( q ) ,  Dist (p.4)  > k", pe  RkNN(q) ' Proof: I f D i s f ( p , q ) <  kNNP,  then q is a k" or is a tie  of kh" of p. According Theorem 1, p is R k "  of q.

Otherwise, p is NOT R W N  of q and vice versa.

In Section 3.2, we cluster all user transactions by utilizing an index structure X-Features-Tree. During the generation of this tree, we add some necessary amihutes into different type of nodes respectively,  Rk".

p and its k Nearest Neighbor,  Figure 2. Structure of X-Features-Tree      1 ) For each leaf node I E  T , we determine its N N p  and generate circle fp, M N J ,  where p is center and kNNP is radius.

A directory node contains an array of branches of the form (Childgtr*, SPHERE, m-k").

If Chi ld j t r  points to a non-leaf node, SfHERE is the minimum bounding hypersphere of all hypersphere that are entries in the child node; m-kNN=nurr(klvn6, where p are points contained in the subtree rooted at this node.

2)  '. .' I., .

Figure 3. S P H E R x u m  bounding sGhere of all leaf nodes in this Branch(Circ1e in 2 Dimension)  In the structure described above, we use minimum bounding spheres instead of minimum bounding rectangle, for reducing the overlapping of regions.

For each newly added page or item, we also can extract its feature vector and represent as:  - page = ( f,. f, ,.... f,) We can take this page or item as a query point q. and  fmd its potential "influence" users by traveling the whole X-Features-Tree. It can be accomplished by using a heuristic Rk" query algorithm which is described as  For a leaf node, we need to examine each point (p.kNNp,) in the node. If Disr (q ,p , )<k"p , , , i.e. q is a k" of p,  , then p, is one of the Reverse k Nearest Neighbor of q and add it to the results set  For a directory node, we compare the query point q with each branch B=(ChildJrr*, SPHERE, m 9 N N ) .  Here m-k." plays a crucial role. By definition, the distance from each pointp, to its k Nearest Neighbor is not greater than m-k" (nm-k" is the largest of them) and all points in the subtree rooted at B are contained in a SHfERE. Hence if the distance from q to the center of SHPERE is greater than m-k", then branch B need not to be visited. This is because any point in B cannot be closer to q than to its k Nearest Neighbor in the whole  following: *  X-Features-Tree. Otherwise, we call these two search processes recursively.

Before the algorithm performs, we must specify a  certain matching score of the user transaction and the new item (page). In our case, the score is the weighted number of matched features in their feature vectors. It will be reflected in the specified value of k. In another WO&, for each point p,, how many points it will take as its "neighbor".

Too low or too high threshold will cause a bad accuracy and low performance. An appropriate value of k is worthy of discussion.

When Rk"(q) is found, i.e. the influence sets of this newly added page or item are acquired, we capture the potential user groups whose tastes or preferences are matched with the feature of this page or item. In contrast to be searching passively by Web users, the information about this page or item can be recommended directly to them online or offline.

3.4. The Maintenance of X-Features-Tree  The X-Features-Tree structure is dynamic and can be updated incrementally. The processes of insertion, deletion and update can be complete easily by combining several k" and Rk" query operations. We first take a look at insertion. When a p i n t  p '  is to be inserted into a generated X-Features-Tree. We perform a .WN operation to find its kJVNp., and create the circle @', W N P , )  which is the form of leaf node. Next, an Rk" query can give us the information of those points that are affected, i.e. Rk"(p'). For each p, E R k N N ( p ' ) ,  we recomputed its kNNp, by perform kh7N(pZ) and the field SPHERE and mx-k" of its ascendant nodes will also need to adjusted up to the root of the tree. This can be done in a way very similar to the R k " query algorithm. At last, we insert p'  the same as in a normal tree. Now, we tum our attention to deletion. Firstly we delete the target point p" from the tree. Just like insertion operation, those affected points in Rk"(p") must be found and recomputed the k" field and their relevant directory nodes must be adjusted. The update operation is composed of two steps: after a deletion operation, insertion supervenes.

4. Conclusions  This paper presents a different view of integrating semantic knowledge into the recommendation process based on information retrieval techniques. We also give the necessary theoretical notions of a new framework and relevant efficient structure, which focus on capturing the underlying common properties and relations among the      users and items (pages). Instead of being found passively, items and pages can discover their potential Web users automatically and be recommended to these users actively in this general approach.

Acknowledgements  This work is supported by the National Natural Science Foundation of China (60205007), Natural Science Foundation of Guangdong Province (001264, 031558).

Research Foundation of Science and Technology Plan Project in Guangdong Province (2003C50118) and Research Foundation of State Key Laboratory for Novel Software Technology at Nanjing University.

Referencis  .[11 Herlockei, J. Understanding and Improving Automated Collaborative Filtering Syaems. Pb.D.

Thesis, Computer Science Dept., University of Minnesota.

Bamshad Mobasher, Honghua Dai, Tao Luo, Miki Nakagawa. Effective personalization based on association rule discovery from web usage data. In the Proteedings of the ACM Workshop on Web   Information and Data Management (WIDM2001). Pp 9-15  [3] laronski W., Bloemer I., Vanhoof K., Wets G. Use of Bayesian belief networks to help understand online audience. In: Proceedings of Data Mining for Marketing Applications Workshop at E C W K D D 2001,3-7  [4] Agganual, C. C., Wolf, J. L., Wu K., and Yu, P. S.

Honing Hatches an Egg: A New Graph-theoretic Approach to Collaborative Filtering. In Proceedings of the ACM KDD99 Conference. San Diego, CA. pp.

201-212.

Sarwar, BM, Karypis, G., Konstan, JA, and Riedl, I.

Applicafion of Dimensionality Reduction in Recommender System -- A Case Study. In ACM WebKDD 2000 Web Mining for E-Commerce Workshop.

[6] Stefan Berchtold, Daniel A. Keim, Hans-Peter Kriegel.

The X-tree: An Index Structure for High-Dimensional  ? Data. Proceedings of the 22nd International Conference on Very Large Databases. pp 28-39 F. ?of? and S .  Muthukrisbnan. Influence sets based on reverse nearest neighbor queries. In Proc. ACM SIGMOD Int. Conf. on Management of Data, Dallas, USA, May 2000.

