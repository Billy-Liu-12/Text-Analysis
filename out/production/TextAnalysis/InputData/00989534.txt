Frequent Subgraph Discovery*

Abstract rectly modeled with the traditional market-basket transac- As data mining techniques are being increasingly ap-  plied to non-traditional domains, existing approaches for $finding frequent itemsets cannot be used as they cannot model the requirement of these domains. An alternate way of modeling the objects in these data sets is to use graphs.

Within that model; the problem of finding frequent pat- terns becomes that of discovering subgraphs that occurfre- quently over the entire set of graphs. In this paper we present a computationally efJicient algorithm for finding all frequent subgraphs in large graph databases. We evalu- ated the performance of the algorithm by experiments with synthetic datasets as well as a chemical compound dataset.

The empirical results show that our algorithm scales lin- early with the number of input transactions and it is able to discover frequent subgraphs from a set of graph trans- actions reasonably fast, even though we have to deal with computationally hard problems such as canonical labeling of graphs and subgraph isomorphism which are not neces- sary for traditional frequent itemset discovery,  1. Introduction  tion approaches.

An alternate way of modeling the various objects is  to use undirected labeled graphs to model each one of the object's entities-items in traditional frequent itemset discovery-and the relation between them. In particular, each vertex of the graph will correspond to an entity and each edge will correspond to a relation between two entities.

In this model both vertices and edges may have labels asso- ciated with them which are not required to be unique. Using such a graph representation, the problem of finding frequent patterns then becomes that of discovering subgraphs which occur frequently enough over the entire set of graphs.

The key advantage of graph modeling is that it allows us to solve problems that we could not solve previously.

For instance, consider a problem of mining chemical com- pounds to find recurrent substructures. We can achieve that using a graph-based pattern discovery algorithm by creat- ing a graph for each one of the compounds whose vertices correspond to different atoms, and whose edges correspond to bonds between them. We can assign to each vertex a label corresponding to the atom involved (and potentially its charge), and assign to each edge a label corresponding to the type of the bond (and potentially information about their relative 3D orientation). Once these graphs have been created, recurrent substructures across different compounds become frequently occurring Related Work Developing algorithms that discover all frequently occurring subgraphs in a large graph database is particularly challenging and computationally intensive, as graph and subgraph isomorphisms play a key role through- out the computations.

The power of using graphs to model complex datasets has been recognized by various researchers in chemical do- main [18, 17,5,4], computer vision [ 12, 141, image and ob-  Efficient algorithms for finding frequent itemsets-both sequential and non-sequential-in very large transaction databases have been one of the key success stories of data mining research [2, 1, 22, 9, 31. We can use these itemsets for discovering association rules, for extracting prevalent patterns that exist in the datasets, or for classification. Nev- ertheless, as data mining techniques have been increasingly applied to non-traditional domains, such as scientific, spa- tial and relational datasets, situations tend to occur on which we can not apply existing itemset discovery algorithms, be- cause these problems are difficult to be adequately and cor-  *This work was supported by NSFCCR-9972519. HA-9986042. ACI- 9982274, by Army Research Office contract DmAAG55-98-1-0441* by the DOE ASCI program,.and by Army High Performance Computing Re-  ject retrieval [6], and machine learning [ 10,201. In particu- lar, Dehaspe et al. [5] applied Inductive Logic Programming  search Center contract,number DAAH04-95-C-0008. Access to comoutine (ILp) t' Obtain frequent patterns in the toxicology evalua- . Y  facilities was provided by the Minnesota Supercomputing Institute tion problem [18]. ILP has been actively used for predicting  0-7695-1 119-8/01 $117.00 0 2001 IEEE 313  mailto:cs.umn.edu   carcinogenesis [17], which is able to find all frequent pat- terns that satisfy a given criteria. It is not designed to scale to large graph databases, however, and they did not report , any statistics regarding the amount of computation time re- quired. Another approach that has been developed is using a greedy scheme [20, 101 to find some of the most prevalent  formance of FSG on the same chemical compound dataset used by AGM. Our results show that FSG is able to find all the frequent connected subgraphs using a 6.5% minimum support in 600 seconds.

2. Frequent Subgraph Discovery  find all frequent induced subgraphs in a graph database that satisfy a certain minimum support constraint. A subgraph G, = (Vs, E,) of G = (V, E )  is induced if E, contains all the edges of E that connect vertices in V,. AGM finds all frequent induced subgraphs using an approach similar to that used by Apriori [2], which extends subgraphs by adding one vertex at each step. Experiments reported in [ 1 13 show  Notation D t  k-(sub)graph g k Ck Fk  c l ( g k )  Description A dataset of graph transactions A transaction of a graph in D A (sub)graph with k edges A k-subgraph A set of Candidates with k edges A set of frequent k-subgraphs A canonical label of a k-graph g k  datasets, and it required 40 minutes to 8 days to find all fre- quent induced subgraphs in a dataset containing 300 chem- ical compounds, as the minimum support threshold varied from 20% to 10%.

Our Contribution In this paper we present a new algo- rithm, named FSG, for finding all connected subgraphs that appear frequently in a large graph database. Our algorithm finds frequent subgraphs using the same level-by-level ex- pansion adopted in Apriori [ 2 ] .  The key features of FSG are the following: (1) it uses a sparse graph representation which minimizes both storage and computation, (2) it in- creases the size of frequent subgraphs by adding one edge at a time, allowing to generate the candidates efficiently, (3) it uses simple algorithms of canonical labeling and graph isomorphism which work efficiently for small graphs, and (4) it incorporates various optimizations for candidate gen- eration and counting which allow it to scale to large graph databases.

We experimentally evaluated FSG on a large number of synthetic graphs, that were generated using a framework similar to that used for market-basket transaction genera- tion [2]. For problems in which a moderately large number of different types of entities and relations exist, FSG was able to achieve good performance and to scale linearly with the database size. In fact, FSG found all the frequent con- nected subgraphs in less than 500 seconds from a synthetic dataset consisting of 80000 graphs with a support threshold of 2%. For problems where the number of edge and ver- tex labels was small, the performance of FSG was worse, as the exponential complexity of graph isomorphism dom- inates the overall performance. We also evaluated the per-  The key restriction in our problem statement is that we are finding only subgraphs that are connected. This is mo- tivated by the fact that the resulting frequent subgraphs will be encapsulating relations (or edges) between some of entities (or vertices) of various objects. Within this con- text, connectivity is a natural property of frequent patterns.

An additional benefit of this restriction is that it reduces the complexity of the problem, as we do not need to con- sider disconnected combinations of frequent connected sub- graphs.

In developing our frequent subgraph discovery algo- rithm, we decided to follow the structure of the algorithm Apriori used for finding frequent itemsets [2], because it achieves the most effective pruning compared with other algorithms such as GenMax, dEclat [22] and Tree Projec- tion [I].

The high level structure of our algorithm FSG is shown in Algorithm 1. Edges in the algorithm correspond to items in traditional frequent itemset discovery. Namely, as these algorithms increase the size of frequent itemsets by adding a single item at a time, our algorithm increases the size of frequent subgraphs by adding an edge one by one. FSG initially enumerates all the frequent single and double edge graphs. Then, based on those two sets, it starts the main computational loop. During each iteration it first generates candidate subgraphs whose size is greater than the previous frequent ones by one edge (Line 5 of Algorithm 1). Next, it counts the frequency for each of these candidates, and  'The algorithm presented in this paper can be easily extended to di- rected graphs.

prunes subgraphs that do no satisfy the support constraint (Lines 7-1 1). Discovered frequent subgraphs satisfy the downward closure property of the support condition, which allows us to effectively prune the lattice of frequent sub- graphs.

Algorithm 1 fsg(D, 0 )  (Frequent Subgraph) I :  F' t detect all frequent I-subgraphs in D 2: F 2  t detect all frequent 2-subgraphs in D 3: k t 3 4: while Fk-' # 0 do 5:  C k  t fsg-gen(Fk-') 6: 7: gk.count t o 8: 9: IO: 1 1 : 12: k t k + l 13: return F', F 2 , .  . . , Fk--2  for each candidate @ E C k  do  for each transaction t E D do if candidate 8 is included in transaction t then  gk .count t gk .count + 1 F~ t {gk E ck I gk.count 2 0 1 ~ 1 )  In Section 2.1, we briefly review some background is- sues regarding graphs. Section 2.2 contains details of can- didate generation with pruning and Section 2.3 describes frequency counting in FSG.

2.1. Graph Representation, Canonical Labeling  Sparse Graph Representation Our algorithm uses sparse graph representation to store input transactions, in- termediate candidates and frequent subgraphs. Each one of the transactions, candidates and discovered frequent sub- graphs is stored using adjacency-list representation, while our canonical labeling described in Section 2.1 is based on adjacency matrix representation. Thus, after determining canonical label for a subgraph, we convert its canonical ad- jacency matrix back into adjacency lists. This adjacency-list representation saves memory when input transaction graphs are sparse, and speeds up computation.

Canonical Labeling Because we deal with graphs, not itemsets, there are many differences between our algorithm and the traditional frequent itemset discovery. A difference appears when we try to sort frequent objects. In the tra- ditional frequent itemset discovery, we can sort itemsets by lexicographic ordering. Clearly this is not applicable to graphs. To get total order of graphs we use canonical labeling. A canonical label is a unique code of a given graph [ 16,7]. A graph can be represented in many different ways, depending on the order of its edges or vertices. Nev- ertheless, canonical labels should be always the same no matter how graphs are represented, as long as those graphs have the same topological structure and the same labeling of edges and vertices. By comparing canonical labels of graphs, we can sort them in a unique and deterministic way, regardless of the representation of input graphs. We denote a canonical label of a graph g by cl(g). It is easy to see  and Isomorphism  that computing canonical labels is equivalent to determin- ing isomorphism between graphs, because if two graphs are isomorphic with each other, their canonical labels must be identical. Both canonical labeling and determining graph isomorphism are not known to be either in P or in NP- complete [7].

We use a straightforward way of determining a canoni- cal label. Using ajuttened representation of the adjacency matrix of a graph, by concatenating rows or columns of an adjacency matrix one after another we construct a list of in- tegers. Regarding this list of integers as a string, we can obtain total order of graphs by lexicographic ordering. To compute a canonical label of a graph, we try all the permu- tations of its vertices to see which ordering of vertices gives the minimum adjacency matrix. To narrow down the search space, we first partition the vertices by their degrees and la- bels, which is a well-known technique called vertex invari- ants [16]. Then, we try all the possible permutations of ver- tices inside each partition. Once we determine the caninical adjacency matrix, we convert our adjacency-list represen- tation of the subgraph so that the ordering of vertices in the canonical adjacency matrix is reflected. All compar- isons of subgraphs are done with the canonically reordered adjaceny-list representation. Further details can be found in [13].

Isomorphism In our algorithm, we need to solve both graph isomorphism and subgraph isomorphism. Graph iso- morphism is a problem to determine whether given two graphs g1 and g2 are isomorphic, namely, to find a map- ping from a set of vertices to another set. Automorphism is a special case of graph isomorphism where 91 = 92, which means to find a mapping from a graph to itself. Subgraph isomorphism is to find an isomorphism between 91 and a subgraph of g2. In other words, it is to determine if a graph is included in the other larger graph. A well-known algo- rithm for subgraph isomorphism is proposed in [19]. As suggested in [7], graph isomorphism can be directly solved in practice, although it is not known to be either in P or in NP-complete. On the other hand, subgraph isomorphism has been proved to be in NP-complete [8]. Thus, there is no scalable algorithm to solve it. When the size of graphs is small such as 10 vertices or less, however, it is also known that subgraph isomorphism can be feasible even with a sim- ple exhaustive search [7, 191.

We solve graph isomorphism by a simple way, which is, starting from a single vertex in one graph, to try to find a mapping to one of the vertices in the other graph, that is consistent with the labeling. Then, we keep the same pro- cess by adding vertices one by one until either we find a complete mapping or we end up with exhausting the search space. When we seek for the next mapping, we have to be careful to keep the consistency of edge and vertex la- bels. We can reduce the search space more if there are more     labels are assigned to edges and vertices, which leads to restriction against mapping. This approach can solve both graph and subgraph isomorphism.

2.2. Candidate Generation In the candidate generation phase, we create a set of can-  didates of size k + 1, given frequent k-subgraphs. Can- didate subgraphs of size k + 1 are generated by joining two frequent k-subgraphs. In order for two such frequent k-subgraphs to be eligible for joining they must contain the same (k - 1)-subgraph. We will refer to this common (k - 1)-subgraph among two k-frequent subgraphs as their core.

Unlike the joining of itemsets in which two frequent Ic- size itemsets lead to a unique (k + 1)-size itemset, the join- ing of two subgraphs of size k can lead to multiple sub- graphs of size k + 1. This is due to three reasons. First, the resulting two (k + 1)-subgraphs produced by the join- ing may differ in a vertex that has the same label in both k-subgraphs. Figure l(a) is such an example. This pair of graphs g;f and g t  generates two different candidates gz and g:. The second reason is because a core itself may have multiple automorphisms and each automorphism can lead to a different (k + 1)-candidate. An example for this case is shown in Figure l(b), in which the core-a square of 4 vertices labeled with vo-has more than one automorphism which result in 3 different candidates of size 6. Finally, two frequent subgraphs may have multiple cores as depicted by Figure l(c).

(a) By vertex labeling  (b) By multiple automorphisms of a single core  (c) By multiple cores  Figure 1. Three different cases of candidate joining  The overall algorithm for candidate generation is shown  in Algorithm 2. For each pair of frequent subgraphs that share the same core, the fsg-join is called at Line 6 to gen- erate all possible candidates of size k + 1. For each of the candidates, the algorithm first checks if they are already in C"'. If they are not, then it verifies if all its k-subgraphs are frequent. If they are, fsg-join then inserts it into C k+l, otherwise it discards the candidate (Lines 7-16). The algo- rithm uses canonical labeling to efficiently check if a partic- ular subgraph is already in Ck+' or not.

The key computational steps in candidate generation are (1) core identification, (2) joining, and (3) using the down- ward closure property of a support condition to eliminate some of generated candidates. A straightforward way of implementing these tasks is to use subgraph isomorphism, graph automorphism and canonical labeling with binary search, respectively. The amount of computation required by the first step, however, can be substantially reduced by keeping some information from the lattice of frequent sub- graphs. Particularly, if for each frequent k-subgraph we store the canonical labels of its frequent ( IC - 1)-subgraphs, then the cores between two frequent subgraphs can be deter- mined by simply computing the intersection of these lists.

Also to speed up the computation of the automorphism step during joining, we save previous automorphisms associated with each core and look them up instead of performing the same automorphism computation again. The saved list of automorphisms will be discarded once C'+' has been gen- erated.

Note we need to perform selfjoin, that is, two graphs g t and g$ in Algorithm 2 are identical. It is necessary because, for example, consider transactions without any labels, that is, each transaction in the input is an undirected and un- labeled graph. Then, we will have only one frequent 1- subgraph and one frequent 2-subgraph regardless of a sup- port threshold, because those are the only allowed struc- tures, and edges and vertices do not have labels assigned.

From those F1 and F 2  where IF1 I = IF21 = 1, to generate larger graphs of C k  and Fk for k 2 3, the only way is the self join.

2.3. Frequency Counting  Once candidate subgraphs have been generated, FSG computes their frequency. The simplest way of achieving this is for each subgraph to scan each one of the transaction graphs and determine if it is contained or not using sub- graph isomorphism. Nonetheless, having to compute these isomorphisms is particularly expensive and this approach is not feasible for large datasets. In the context of frequent itemset discovery by Apriori, the frequency counting is per- formed substantially faster by building a hash-tree of can- didate itemsets and scanning each transaction to determine which of the itemsets in the hash-tree it supports. Develop- ing such an algorithm for frequent subgraphs, however, is     Algorithm 2 fsg-gen(Fk) (Candidate Generation) Ck+' t 0; foreachpairofgF,g; E Fk,Z <jsuchthatcl(gf) <cl(g:)do  for each edge e E 9," do {create a (k - 1)-subgraph of 91" by removing an edge e}  gt-' t gt - e if gf-' is included in $ then {gf and g$ share the same core}  T ~ + '  t fsg-join(gt, gf ) for each g:" E Tk+' do  {test if the downward closure property holds for $+'} flag t true for each edge fi E g!+' do  hf t g;+1 - fi if hf is connected and hf $Z F k  then  flag t false break  if flag = true then c k + l  + C k + 1  U { g k + l }  return ~ k + '  Notation (DI IT1   ILI  challenging as there is no natural way to build the hash-tree for graphs. For this reason, FSG instead uses Transaction ID (TID) lists, proposed by [23,21,22]. In this approach for each frequent subgraph we keep a list of transaction iden- tifiers that support it. Now when we need to compute the frequency of gk+', we first compute the intersection of the TID lists of its frequent k-subgraphs. If the size of the in- tersection is below the support, gk+l is pruned, otherwise we compute the frequency of g k+l using subgraph isomor- phism by limiting our search only to the set of transactions in the intersection of the TID lists.

Parameter The total number of transactions The average size of transactions (in terms of the number of edges) The average size of potentially frequent subgraphs (in terms of the number of edges) The number of potentially  3. Experiments We performed a set of experiments to evaluate the per-  formance of FSG. There are two types of datasets we used.

The first type was synthetically generated, and allowed us to study the performance of FSG under different conditions.

The second type contains the molecular structures of chem- ical compounds, which is used to evaluate the performance of FSG for large graphs.

All experiments were done on 650MHz Intel Pentium I11 machines with,2GB main memory, running the Linux operating system.

3.1. Synthetic Datasets For the performance evaluation, we generate synthetic  datasets controlled'by a set of parameters shown in Table 2.

The basic idea behind our data generator is similar to the one used in [2], but simpler.

First, we generate a set of 1151 potentially frequent con- nected subgraphs whose size is determined by Poisson dis- tribution with mean 111. For each frequent connected sub- graph, its topology as well as its edge and vertex labels are chosen randomly. It has a weight assigned, which becomes a probability that the subgraph is selected to be included  Table 2. Synthetic dataset parameters  I frequent subgraphs I The number of edge and vertex labels N  in a transaction. The weights obey an exponential distribu- tion with unit mean and the sum of the weights of all the frequent subgraphs is normalized to 1. We call this set of ILI frequent subgraphs a seed pool. The number of distinct edge and vertex labels is controlled by the parameter N .  In particular, N is both the number of distinct edge labels as well as the number of distinct vertex labels.

Next, we generate ID1 transactions. The size of each transaction is a Poisson random variable whose mean is equal to IT1. Then we select one of the frequent subgraphs already generated from the seed pool, by rolling an ILI- sided die. Each face of this die corresponds to the proba- bility assigned to a potential frequent subgraph in the seed pool. If the size of the selected seed fits in a transaction, we add it. If the current size of a transaction does not reach its selected size, we keep selecting and putting another seed into it. When a selected seed exceeds the transaction size, we add it to the transaction for the half of the cases, and discard it and move onto the next transaction for the rest of the half. The way we put a seed into a transaction is to find a mapping so that the overlap between a seed and a transac- tion is maximized.

In the following experiments, we use the combinations of the parameters shown in Table 3.

Table 3. Parameter settings Parameter Values  3,5,7,10  N 3.5.10.20.40  Table 4 shows the amount of time required by FSG to find all the frequent subgraphs for various datasets in which we changed N ,  111, ITI, and o. In all of these experiments, the number of transactions ID1 was fixed to 10000 and the number of potential frequent subgraphs ILI was set to 200.

If the average transaction size IT1 is smaller than that of potential frequent subgraphs 111, we omitted such combina- tions because we can not generate transactions. In some     cases, we aborted computation because the running time was too long or because the main memory was exhausted, which are denoted by dashes in the table.

By looking at the table, we can observe a number of in- teresting points regarding the performance of FSG for dif- ferent types of datasets. First, as the number of edge and vertex labels N increases, the amount of time required by FSG decreases. For example, when IS = 2%, N = 3, (11 = 3 and 12?1 = 10, it takes 143 seconds, while the run- ning time drops to 16 seconds for N = 20. This is because as the number of edge and vertex labels increases there are fewer automorphisms and subgraph isomorphisms, which leads to fast candidate generation and frequency counting.

Also by having more edge and vertex labels, we can effec- tively prune the search space of isomorphism because they work as constraints when we seek for a mapping of vertices.

Second, as the size of the average transaction IT1 increases the overall running time increases as well. The relative in- crease is higher when N is small than when N is large. For example, going from 17?1 = 5 to IT1 = 40 under the set- ting of N = 5, 111 = 3 and (T = 2%, the running time increases by ,a factor of 20, whereas for the same set of pa- rameters when N = 40, the increase is only by a factor of 4. The reason for that is again having many edge and vertex labels effectively decreases the number of isomor- phisms and the search space. With small N and large ITI, we can not narrow down efficiently the search space of sub- graph isomorphism for frequency counting and the running time increases drastically. Third, as 111 increases the overall running time also increases. Again the relative increase is smaller for larger values of N and smaller values of IT1 by the same reason described above.

To determine the scalability of FSG against the number of transactions we performed an experiment in which we used (DI = 10000, 20000, 40000 and 80000 with ILI = 200, 111 = 5 and IT1 ranging from 5 to 40. These results are shown in Figure 2. As we can see from the figure, FSG scales linearly with the number of transactions.

3.2. Chemical Compound Dataset We obtained a chemical dataset from [15]. This was  originally provided for the Predictive Toxicology Evalua- tion Challenge [ 181, which contains information on 340 chemical compounds in two separated files. The first file named atoms . p l  contains definitions of atoms in com- pounds. For example, ?atm(d1, d l  1 ,  c, 22, -0.133)? means that a chemical compound d l  has an atom whose identi- fier is d l l ,  of element carbon, of type 22 and with par- tial charge -0.133. The other file bonds . p l  provides bonding information between atoms. A line in the file ?bond(d1, d l l ,  dl2,  7)?, for instance, states that in the com- pound d l  its atoms d l l  and dl2 are connected by a type 7 bond. There are 4 different types of bonds and 24 different  Figure 2. Scalability on the number of transaction  atoms, and there are 66 atom types.

We converted the data into graph transactions. Each  compound becomes a transaction. Thus, there are 340 trans- actions in total. Each vertex corresponds to an atom, whose label is made of a pair of the atom element and the atom type. We did not include partial charge to vertex labels be- cause those values were not discretized. Each edge is placed for every bond. Edge label directly corresponds to the bond type. By the conversion, there are 4 edge labels and 66 ver- tex labels produced in total. The average transaction size was 27.4 in terms of the number of edges, and 27.0 in terms of the number of vertices. Because the number of edges is very close to that of vertices, this dataset is sparse. There are 26 transactions that have more than 50 edges and ver- tices. The largest transaction contains 214 edges and 214 vertices.

The experimental results by FSG for finding frequent subgraphs are shown in Figure 3. Figure 3(a) shows the running time required for different values of support thresh- old and Figure 3(b) displays the number of discovered fre- quent subgraphs on those support levels. With (z = 7%, the largest frequent subgraph discovered has 13 vertices.

With the support threshold (T below lo%, both the run- ning time and the number of frequent subgraphs increase exponentially. FSG does well even for 7% support as it requires 600 seconds. AGM, a frequent induced subgraph discovery algorithm, required about 8 days for 10% and 40 minutes for 20% with almost the same dataset on 400MHz PC [ l l ] .

Comparing the performance of FSG on this dataset against those on the synthetic datasets, we can see that it requires more time for this chemical dataset, once we take into account of the difference in the number of transactions.

This is because in the chemical dataset, edge and vertex labels have non-uniform distribution. As we decrease the minimum support, larger frequent subgraphs start to appear which generally contain only carbon and hydrogen and a     Table 4. Running times in seconds for synthetic data sets. We omitted parameter combinations where (11 > 12'1, because transaction size is too small for potential frequent subgraphs. A dash in the table means we had to abort the computation for the set of parameters because of either memory exhaustion or taking too long time.

N 111 12'1  3 3 5  3 5 5  3 7 10  3 10 10  143 434  RunningTime[sec] u = 2 %  u = 1 %  12 22 30 40  112 390  18 32 51 102  189 736  66 4512  5817 -  6110 -  1953 - 4 0 -  -  8290 - - -   2 5 5  2 7 10  2 10 10  U = 2%  10 16 20 35  - - - -  27 52 25 1 2246  - - 4 0 -  -  557 6203 - -  4 0 -  - - -  - -   I88 10 10 190   ime[secl U = 1%    I506 - -  53 71 196 279  N 111 IT1 u = 2 %  u = l %  10 16 28 20 34 38  RunningTime[sec] u = 2 %  I u = 1 %   4 0 5 5  40 7 10  40 10 10  27 44 44 47 84 89 20 28  , 29 60 55 131  177 234 197 1236 861 5273  2456 9183 9687 - - -   20 5 5  single bonding type. Essentially with U < lo%, this dataset becomes similar to the synthetic datasets where N = 2.

3.3. Summary of Discussions We summarize the characteristics of FSG performance.

First, FSG works better on graph datasets with more edge and vertex labels. During both candidate generation and frequency counting, what FSG essentially does is to solve graph or subgraph isomorphism. Without labels assigned, determining isomorphism of graphs is more difficult to solve, because we can not use labeling information as con- straints to narrow down the search space of vertex mapping.

We can confirm it by comparing the results in Table 4 with various values of the number of edge and vertex labels, N .

Second, the running time depends heavily on the size of frequent subgraphs to be discovered. If input transactions contain many large frequent patterns such as more than 10 edges, the situation corresponds to the parameter setting of 111 = 10, where FSG will not be likely to finish its compu- tation in a reasonable amount of time. The same thing hap- pened with the chemical dataset with a support threshold less than 10%. If we compare Figure 3(a) and Figure 3(b), we notice the running time increases at a higher rate than the number of discovered subgraphs does, as we decrease the minimum support. With a lower support criteria, we start getting larger frequent subgraphs and both candidate generation and frequency counting become much more ex- pensive. On the other hand, as for the cases of 111 < 5 in  10 19 20 51  u = 2 %  u = 1 %  10 20 25   20 7 10  48 117 182 233 193 804  Table 4, FSG runs fast. The result of the chemical dataset is consistent with it. For example, if we use U = 10% for the chemical dataset, FSG spends 28 seconds to get 882 fre- quent subgraphs in total. The largest frequent graphs among them have 1 1  edges, and there are only 10 such frequent 11- subgraphs discovered.

Another important factor is the size of a transaction. If the average size of transactions becomes large, frequency counting by subgraph isomorphism becomes expensive re- gardless of the size of candidate subgraphs. Traditional fre- quent itemset finding algorithms are free from this problem.

They can perform frequency counting simply by taking the intersection of itemsets and transactions.

As of the number of transactions, FSG requires running time proportional to the size of inputs under the same set of parameters. This is the same as frequent itemset discovery algorithms.

4. Conclusion  In this paper we presented an algorithm, FSG, for finding frequently occurring subgraphs in large graph databases, that can be used to discover recurrent patterns in scientific, spatial, and relational datasets. Our experimen- tal evaluation shows that FSG can scale reasonably well to very large graph databases provided that graphs contain a sufficiently many different labels of edges and vertices.

