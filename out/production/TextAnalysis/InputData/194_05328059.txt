Maximizing the Efficiency of   Parallel Apriori Algorithm

Abstract - In this paper we attempt to maximize the efficiency  of the parallel Apriori Algorithm. The paper analyzes the performance of the algorithm over different datasets and over n processors on a commodity cluster of machines. In the Apriori Algorithm all processes need to synchronize after every pass. If any process is assigned more load than other processes in the system, the slowest process will dictate the speed of the program.

It is therefore important to ensure that load is equally balanced among all processes. Our algorithm determines the no. of running processes and divides the load equally so as to maximize the system performance and its efficiency. The experiments conducted show that the parallel algorithm scales well to the number of processes and also improves on the efficiency by effective load balancing.

Keywords - Data Mining, Association Rules, Parallel Mining, Apriori Algorithm, Frequent Item-set Generation, Load Balancing.



I.  INTRODUCTION Data Mining is the efficient discovery of previously  unknown patterns in large databases. One of the most important problems in data mining [1] is discovering association rules. There has been considerable research in designing parallel algorithms for mining association rules [2] [3] [4]. One of the key features of all the previous algorithms is that they require multiple passes over the database. For disk resident databases, this requires reading the database completely for each pass resulting in a large number of disk I/Os. The effort spent in performing just the I/O would be considerable for large databases and would result in poor response times. Efficiency Improvement[6] over the serial Apriori algorithm [1] [2] and the parallel CD algorithm [3] to generate all significant association rules based on the reduction in the time spent on scanning elements in the data set have been worked on. In this paper, we analyze the performance of the algorithm over n processors on a commodity cluster of machines. The extensive experiments carried out show that the parallel algorithm scales well to the number of processors and also improves on the efficiency by effective load balancing.

A. Problem Decomposition The problem of discovering all association rules can be  decomposed into two sub-problems [1]:  1. Find all sets of items (item-sets) that have transaction support above minimum support. The support for an item-set is the number of transactions that contain the item-set. Item- sets with minimum support are called large item-sets, and all others, small item-sets.

2. Use the large item-sets to generate the desired rules. Here is a straightforward algorithm for this task. For every large item- set l, find all non-empty subsets of l. for every such subset a, output a rule of the form a  (l-a) if the ratio of support(l) to support(a) is at least minconf. Consider all subsets of l to generate rules with multiple consequents.



II. DISCOVERING LARGE ITEM-SETS Algorithms for discovering large item-sets make multiple passes over the data [1] [2]. In the first pass, support of individual items are counted and it is determined which of them are large, i.e., has minimum support. Every subsequent pass starts with a seed set of item-sets that are found to be large in the previous pass. Use this seed set for generating new potentially large item-sets, called candidate item-sets, and count the actual support for these candidate item-sets during the pass over the data. At the end of the pass, determine which of the candidate item-sets are actually large, and they become the seed for the next pass. This process continues until no new large item-sets are found.

A. Apriori Algorithm Figure 1 gives an overview of the Apriori Algorithm [1]  [2], using the notation in Table 1. The first pass of the algorithm simply counts item occurrences to determine the large 1-item-sets. A subsequent pass, say pass k, consists of two phases. First, the large item-sets Lk-1 found in the (k-1)th pass are used to generate the candidate item-sets Ck. Next, the dataset is scanned and the support of candidates in Ck is counted. For fast counting, we need to efficiently determine the candidates in Ck that are contained in a given transaction t.

TABLE1. NOTATIONS  k-item-set An item-set having k items.

Lk Set of frequent k-item-sets(those with  minimum support).

Ck Set of candidate k-item-sets (potentially  frequent item-sets)   DOI 10.1109/ARTCom.2009.73    DOI 10.1109/ARTCom.2009.73    DOI 10.1109/ARTCom.2009.73     Pi Processor with id i Di The dataset local to the processor Pi Cki The candidate set maintained with the  Processor Pi during the kth Pass   1) L1 = { Large 1-item-sets}; 2) k =2; // k represents the pass number 3) while ( Lk-1 <> 0 ) do 4) begin 5)         Ck = new candidates of size k generated from Lk-1; 6)        forall transactions t ? D do 7)             Increment the count of all candidates in Ck that are  contained in t; 8)        Lk = All candidates in Ck with minimum support; 9)       k = k +1; 10) end 11) Answer = UkLk;  Figure 1: Apriori Algorithm  B. Parallel CD Algorithm The CD Algorithm [3] uses a simple principle of allowing  ?redundant computations in parallel on otherwise idle processors to avoid communication?. The first pass is special.

For all other passes k >1, the algorithm works as follows: 1. Each processor Pi generates the complete Ck, using the complete frequent item-set Lk-1 created at the end of pass k-1.

Ck is generated by joining Lk-1 with itself.

2. Processor Pi makes a pass over its data partition Di and develops local support counts for candidates in Ck.

3. Processor Pi exchanges local Ck counts with all other processors to develop global Ck counts. The global counts are achieved by adding the individual counts gathered by every processor. This step forces the processes to synchronize as no process will be able to continue until all have taken part in the collaboration.

4. Each Processor Pi now computes Lk from Ck.

5. Each Processor Pi independently makes the decision to terminate or continue to the next pass. The decision to terminate is taken if the candidate set for the next pass generated from large item set of the current pass results in a null set.

In the first pass, each Processor Pi dynamically generates its local candidate item-set C1i depending on the items actually present in its local data partition Di. Hence candidates counted by different processors may not be identical and care must be taken in exchanging the local counts to determine the global C1.

C. Performance Consideration Each Processor Pi makes a pass over its data partition Di  reading one tuple at a time and builds Cki. Since in every pass each Processor has to go through all the items in every transaction, this becomes a compute intensive step. After every pass, processes need to synchronize by exchanging counts. This implies that the slowest process will dictate the speed of the algorithm. Hence the data should be divided in  such a way that every processor gets equal amount of work so that the efficiency of the parallel algorithm is maximized.



III. PROPOSED ALGORITHM In the serial Apriori and the parallel CD algorithm, for every pass made by a Processor Pi, it requires a scan of all the elements of individual transaction read from the data partition.

This affects the scalability of the algorithm as there can be cases where a transaction does not contain frequent k-item-set in the k+1 pass. If such transactions can be identified then the algorithms would not read the transaction and move ahead with the next transaction. This would save considerable amount of processor time.

The proposed formulation identifies the transactions that contain frequent k-item-sets in the 1st scan and checks whether a transaction should be scanned when reading the database for subsequent scans. The overhead required is that one has to keep track of the transactions that probably would contain frequent item-sets. But the advantage of saving the transaction scan time would outperform the overhead as the database size increases.

To maximize the efficiency of the parallel algorithm the data is divided equally among the number of processes in the system so that each process can be utilized to its maximum.



IV. EXPERIMENTAL RESULTS To conduct experiments a pseudo-random data generator  has been developed that generates transactions of random length and elements are also randomly generated within a given range. Data set containing 20 million transactions generated using the above software has been used for the experiments. Each transaction contained data items of variable length from 1 to 15 from an item-set {1?25}. The parallel versions were implemented using the MPICH2 library functions under windows environment. Figure 2 shows the experimental results of the time taken to generate k-frequent item-sets by a dual core processor running n processes in parallel for the given data set. Figure 3 shows the experimental results of the time taken by n processors running in parallel for the given data set. These experiments are conducted on a 3.0 GHz, duo core processor systems with 2GB of RAM. Both the experiments were done with a support of 15%. As the data was generated using a pseudo-random data generator, the support was taken to be low so that there could be at-least 3 to 4 frequent-item sets generated. The first experiment shows that the time taken remains constant even after increasing the number of processes as the processor is being utilized to its maximum for the given data set. The second experiment shows that with the increase in the number of processors, the algorithm scales well and the efficiency is maximized. As seen in Figure 3, the maximum efficiency is obtained. Since the experiments are conducted on a duo core processor, the processor utilization is verified for a single process and dual process using the windows performance measurement tool.

Figure 4 and Figure 5 shows the cpu utilization using a single processor and both the dual core processors respectively.



V. CONCLUSION The parallel Apriori algorithm?s efficiency depends on the number of processors running and not the number of processes running. Due to its compute intensive scans there is no improvement if data is partitioned and parallel processes are used to run the algorithm. The performance benefit comes only when every partition of data is run by process on different processors. Also it is important that the data is partitioned equally so that the efficiency is maximized. In the next phase of research, we are in the process of extending the algorithm so that it improves the efficiency and scalability by partitioning the data as per the speed of the processors.

