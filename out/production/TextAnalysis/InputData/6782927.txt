An Efficient Transportation Architecture for Big Data  Movement

Abstract?Big data are transferred, routed and forward to their destinations as a combination of a series of packets one by one, without considering the data in their entity in today?s packet network. In this paper, we firstly summarize those transportation methods of bid data on packet networks and optical networks, respectively, and then propose a new data transportation architecture to collect, store and reshape the data flows in large time scale until it find the connection segments available on schedule. It leverages both circuit and storage merits together to form a now network node, which is termed as SSS in this paper. The intermediate storages act to relay and bridge these sub-connections into an equivalent entirety.

Keywords-Big Data; Optical Transport Network; Optical Switching; data transportation

I.  INTRODUCTION (HEADING 1) It is well accepted that the Internet data traffic increases 20-  50% yearly [1]. As the stable exponential growth of the data traffic, handling so large data sets?so-called big data?will become a key basis of competition according to McKinsey report [2]. Big data is a collection of data sets so large and complex that it becomes difficult to process using on-hand data management tools. The challenges include capture, curation, storage, search, sharing, transfer, analysis, and visualization [3].

Big data spans four dimensions: volume, velocity, variety and veracity, and is moved across the network more frequently, and often, in larger size.

In particularly, modern frontier science is increasingly data- driven and collaborative in nature. Scientists regularly encounter limitations due to large data sets in many areas, including genomics, connectomics, meteorology, complex physics simulations, and biological and environmental researches [4].  All of this is big data at high velocities. Some workflows require the ability to move a single 100TB data set in just a few hours. This will require hosts with 100Gbps interfaces and extremely efficient data transfer protocols. As the network pipelines are expanded in scale, efficient protocols and tools are necessary to move vast amounts of scientific data over high-bandwidth networks in such collaborations [5].

In today?s packet networks, data are transferred across the network as a combination of a series of packets, delivered one by one, without considering the data in their entirety with respective service quality requirements. Consequently, the  quality of service of interactive big data applications is hard to provision, and the utility of network is low [6] and the enhancement in transmission protocols becomes much urgent.

Intuitively, by relaxing the delivery time of big data files according to their respective priorities, the network resources may be used more efficiently and network congestion can be alleviated, and optical network running in circuit switching pattern could play a new role to do so.

In this paper, we propose and discuss a new data transfer model on optical network to complement the existing per- packet forwarding paradigm on packet network. In the new data transfer model, instead of transferring these data on per- packet bases immediately upon entering the network, the optical network stores the data until it find necessary, or enough network resource is available for that transfer, or delivery the data in step-by-step ways with the help of relay storage in each switching nodes. The scheduled data transfer is realized through the use of dynamic circuit switching paradigm [12-18].

The remaining of this paper is organized as follows. In section II, we will provide a brief overview on the data transportation methods on packet networks. In section III, we will discuss the big data file transportation methods on optical networks with generalized multi-protocol label switching (GMPLS) controlling based on our earlier work in this field. In section IV, we will discuss the as-proposed new model with the combination of optical circuit pipelines in each link and temporary storages in each node. It will be seen that the performance is improved for the big data movement.



II. BID DATA TRANSPORTATION ON PACKET NETWORK  Most data movement tools today rely on transmission control protocol (TCP) over sockets, which limits flows to some bound on today?s hardware and software. Efficient protocols and tools are necessary to move vast amounts of big scientific data over high-bandwidth long-distance networks in such collaborations. Some of these efforts are briefly overviewed in this section.

A. Standard TCP  This work is supported by the 863 Program.

The TCP is a well-known transport layer network protocol and used for reliable communications on packet network. In general, TCP performs congestion and flow controls jointly by using the concept of windows to throttle the amount of data that the sender injects into the network, depending on the receiver?s available capacity and link traffic. When TCP detects a packet loss, it decides to perform congestion control.

When the receiver does not have sufficient memory for receiving data from the sender, it performs the flow control. In the basic version, it uses an additive-increase multiplicative- decrease (AIMD) strategy for changing its windows as a function of network conditions [7].

In order to continuously fill the link capacity, TCP must maintain the window size into the round trip time (RTT) multiplied by the bandwidth, called the Bandwidth-Delay Product (BDP) [7-8]. The increase in link speed and distance (e.g., optic fibers) has led to paths of large BDP. TCP window must be able to reach such large values in order to efficiently use the available bandwidth. However, at larger windows, congestion may lead to the loss of many packets from the same connection. As a result, TCP has performance problems over long-distance high-bandwidth networks in wide area scenario.

In some cases, for example, basic TCP performance on long distance may be lower with 10Gbps, e.g., 9.2 Gbps at 0.2 miles, 8.2 Gbps at 1400 miles, and 2.3-2.5 Gbps at more than 6600 miles [11].

B.  Enhancement TCP Most current congestion control algorithms in TCP family  detect congestion and slow down when they discover that packets are being dropped. But packet loss only provides a single bit of information about the congestion level, whereas delay is a continuous quantity and in principle could provide more information about the network condition. Much effort is paid to enhance the TCP performance for decades. Among them, for example, a FAST TCP flow seeks to maintain a constant number of packets in queues throughout the network routers. If too few packets are queued, the sending rate is increased, while if too many are queued, the rate is decreased.

FAST TCP has the ability to rapidly stabilize high-speed long- latency networks into steady, efficient and fair operating points, in dynamic sharing environments [9].

On the other hand, parallel data transmission can establish multiple connections simultaneously, and, therefore, data sets are split and transferred in parallel through each connection. It should be noted that this technique operates at the application level. For example, GridFTP uses this technique and complement the shortcomings of TCP buffer tuning, and simultaneously achieve maximum throughput on high- performance long distance network environments. Parallel data transferring, unlike buffer tuning, can obtain the maximum achievable throughput by using a sufficient number of streams, is stronger than the buffer tuned stream against packet loss, is effective regardless of RTT, and at last, increasing the number of parallel streams reduces the traffic interference [8].

C. InfiniBand WAN Since InfiniBand came onto the scene, researchers have  firstly focused on the high performance network fabric to  connect compute and storage equipments within datacenters.

Later on, researchers have extended InfiniBand connectivity for wide area networks (WANs). From the application's viewpoint, the remote compute and storage nodes look and act as if they are sitting right next to each other [10].

The benefits of long distance InfiniBand mirror its advantages in the datacenter, namely high bandwidth and low latency. But the WAN InfiniBand performance will not always match the local performance. It is demonstrated a data rate of up to 8 Gbps over thousands of miles across 10 Gbps backbone.

Both bandwidth and latency tends to drop if it goes farther, but unlike the TCP implementation, quality of service is maintained [10].

D. RDMA over Converged Ethernet (RoCE) Remote Direct Memory Access (RDMA) over Converged  Ethernet (RoCE) is a network protocol that allows remote direct memory access over an Ethernet network. RDMA operates on the principle of transferring data directly from the user-defined memory of one system to another. The advantages of RDMA are lower latency, lower CPU load and higher bandwidth. In effect, RoCE is InfiniBand protocols made to work over Ethernet infrastructure. The notion of ?converged Ethernet?, also known as enhanced or data center Ethernet, includes various extensions to the IEEE 802.1 standards to provide prioritization, flow control and congestion notification at the link layer [11]. Experiment proves that RoCE is a promising technology for high-performance network data movement with minimal CPU impact over circuit-based infrastructures. Researchers compared the performance of TCP and RoCE over high latency 10 Gbps and 40 Gbps network paths, and showed that RoCE-based data transfers can fill a 40 Gbps path, outperform the TCP implementation [11].



III. BIG DATA TRANSPORTATION ON OPTICAL NETWORK  In the deployed backbone network for telecom operators and the test-beds for large science projects, optical networks are inevitably used as the fundamental transport platforms for both science collaborations and packet network bearings. It means that some of the optical network pipelines are used to bear the packet networks, and some for the science applications at the same time. The latter will be summarized and discussed in this section.

A. Data Driven Dynamic Optical Pipelines Some e-Science projects over the world need high-speed  networks provided by the optical network test-beds, such as the user controlled light paths (UCLP) in CA*net4, Starlight network in Europe, circuit-switched high-speed end-to-end transport architecture (CHEETAH), dynamic resource allocation via GMPLS optical networks (DRAGON), OptiPuter, optical metro network initiative (OMNInet), 3Tnet in China, and some others [12-16].

In general, optical pipelines are in the form of either wavelength channels or sub-wavelength Ethernet/SDH (synchronous digital hierarchy) connections. A widely and implicitly assumption is that at any specific time, there exists    only one communication task in each pair of end systems, and it will occupy the whole bandwidth. So the dynamic provisioning performances have deterministic influence on these applications [17].

Motivated by the above discussion, and also by our involvement in building and testing a GMPLS network test-bed of 3Tnet in the cities centered at Shanghai, we have proposed to define a set of metrics to characterize the dynamic optical pipeline provisioning performance of GMPLS networks. Two documents submitted to the IETF working group were published as Request for Comments (RFC) with number 5814 and 6477. In these documents, we defined a set of performance metrics, including unidirectional and bidirectional setup delays, graceful release delay and their statistic features in both the data and control planes [18-19]. The performance to transfer big data files is promising as field trail has demonstrated [20].

B. Multiple Connections Per Optical Interface In more actual scenario, end hosts are assumed to have  concurrent communication requirements with different remote parities. So we extended the GMPLS-controlled network by proposing a data flow-based VLAN (virtual local area network) tagging and switching technique to provide virtual interfaces with multiple connectivity in a single optical interface [13].

Figure 1.  Network model with a multiple connectivity interface (MCI) and implementation setup [16].

Figure 1 shows the network model with multiple connectivity interfaces (MCIs). On each end host, Ethernet frames are tagged according to their network or transport layer addresses before they are actually sent to the link. As an example, all the frames generated by one host and destined for other hosts are tagged with different VLAN ID numbers. In addition to VLAN tagging on end hosts, we introduces dynamic VLAN-to-VCG (virtual concatenation group) mapping into Ethernet over SDH devices. Whenever a communication task is scheduled on end hosts, a free VLAN ID will be allocated, and a circuit is established in the form of a routed VCG. At the same time, the ingress network element is informed of VLAN and VCG correspondence. After VLAN-to- VCG mapping is established on the ingress network element, tagged Ethernet frames are processed subsequently.

The as-mentioned approach enables an Ethernet interface to establish more than one connection with different remote interfaces and, therefore, realizes the MCIs. In our  implementation, the minimum switching granularity is VC-4 (155Mb/s), and a gigabit Ethernet interface can establish connectivity with up to eight different remote MCIs. Each of these connections has dedicated bandwidth of a single VC-4. In short, an MCI-enabled end host can establish connectivity with different remote interfaces through the circuits of varied smaller granularity [13, 20].

In the wavelength division multiplexed (WDM) network, a good percentage of sub-wavelength traffic will still need to be transported over the one wavelength with muxponder solutions.

The optical transport network (OTN) hierarchy provides a rich foundation for flexibly aggregating mixed-rate client signals onto higher rate wavelengths and switches. Non-OTN client circuits of one gigabit and higher rates in Ethernet and other formats are transparently wrapped, groomed, and mapped into higher capacity wavelengths in flexible combinations [21].

C. Flexible Optical Pipelines More recently, researchers are discussing the software  defined optical transport network (SD-OTN) as evolution of next-generation optical networks with flexible granularity.

With the help of software defined network (SDN) method, OTN can have the features of instant bandwidth provisioning, flexible bandwidth pipeline, and programmable transport controller, which meet the demands of forthcoming high-speed and highly-dynamic traffic boom and bring network operators with benefits of 100% resource utilization. It leaves room for future implementation and standardization [21].

Under the MCI model discussed earlier, because portions of physical interface bandwidth can be provisioned to multiple TCP sessions, the available bandwidth is not constant. To maximize the link utilization, it would be significant to investigate how active packet transport sessions respond to bandwidth change when the number of active sessions changes.

On the other hand, mechanisms can be devised so that once an upper layer notices a decrease in its bandwidth consumption, it can notify the underlying optical provisioning entity to adjust the provisioned bandwidth such that high bandwidth utilization efficiency can be achieved. Some novel cross-layer mechanisms can still be investigated to further increase end-to- end performance [13].



IV. BIG DATA TRANSPORTATION ON OPTICAL NETWORK WITH STORAGE RELAYS: A NEW  PARADIGM The number of connections established on circuit network  at a given moment is limited due to the add/drop interfaces and the line capacity. To increase it, one way is to provide parallel connections per each end interfaces as discussed in the MCI model. The second way is to tune the bandwidth of connections and to release more unoccupied bandwidth to accommodate more connections as discussed in SD-OTN model. In this section, we will discuss another way by segment the long connection which bypasses more nodes into several short sub- connections in its path with the storages to relay and bridge these sub-connections as an equivalent entirety.

A. A Store, Schedule and Switch Architecture The as-proposed architecture is called a Store, Schedule and  Switch (SSS) model by employing both advantages of storage and circuit switching. For conceptual understanding, herewith we take a one-dimension topology for example, as showed in figure 2. Five nodes with label 1 to 5 have both cross- connection matrix and storage inside. Assume the capacity of each link between neighboring nodes is equal for simplicity.

When two connection requests arrives simultaneously, which are labeled C1(1, 4) between node 1 and 4, and C2(3, 5) between 3 and 5, clearly, both requests cannot be meet simultaneously due to the overflow between node 3 and 4. So only one of them, such as C1(1, 4) is satisfied first, and the other one C2(3, 5) is rejected, or delayed to the second time slot.  In our paradigm, we segmented the rejected C2(3, 5) into two sub-connections as C2sub(3, 4) and C2sub(4, 5), respectively. Now the C2sub(4, 5) can be provisioned with the C1(1,4) together, and only the C2sub(3, 4) is delayed to the subsequent time slot. By intuition, short connections in the following time slot have lower probability to overlap and collide with each other, and, as a result, the performance is improved, as discussed in the following sections.

Figure 2.  The as-proposed Store, Schedule and Switch (SSS) Model.

B. The SSS Schedulling Method [23] Figure 3 shows the scheduling in the as-proposed SSS  Model. When there is data ready to send, the data source sends a request to a centralized scheduler (e.g. SDN controller) through the type (1) interface, providing information such as data size, destination and priority (e.g., deadline of delivery).

Upon receiving such a request, the scheduler makes a decision on whether the request can be admitted, according to historical request information. If the request is admitted, the data source sends the data to the nearest switching node (the ingress node) and the data is then stored in the network. Data from multiple sources under the same switching node are aggregated in the storage. The data for the same destination egress node and with close delivery deadlines form a delivery batch. As the deadline of a batch approaches, the scheduler configures a (virtual) circuit for the batch through the type (2) interface, and moves it to the storage on the egress node. After arriving at the egress node, the data in the batch are delivered to their destination applications through their respective procedures. It is worth noting that although type (1) interface exists between the scheduler and all data sources/sinks, and type (2) interface exists between the scheduler and all switching nodes, only a few are illustrated for simplicity. Also for simplicity, the other network devices that connect multiple data sources/sinks to a  SSS switching node through adding and dropping interfaces are not shown in the figure.

Figure 3.  Scheduling in the as-proposed Store, Schedule and Switch (SSS) Model [23]  It is apparent that the introduction of storage into circuit switching networks relaxes the delivery timeliness of data, and hence is helpful to use the network resources in a more uniform and balanced manner. The higher the storage capacity is, and the less tighter requirements application data have on delivery deadline, the higher capacity an SSS network might have. Here, high capacity means that more data delivery request can be admitted and accomplished. In this sense, the storage on switching nodes helps to reduce conflicts among those data delivery requests arrives closely in time, a very close analogy to the use of buffers in routers to avoid packet contentions. The former could scheduling and storing the data in minutes or even hours owing to its big storage, whereas the latter in microsecond due to its small buffer size and best-effort mechanism.

In particular, when the network is large, it might be difficult to establish a high capacity (virtual) circuit across the whole network. This often happened in the deployed networks which are always divided into different administration domains with different technologies and operated by different carriers. In that case, installing storage on intermediate nodes and especially at the boundary can improve the network performance by delivering data in more stops with the intermediate storages to relay and bridge the sub-connections as a final equivalent entirety in a larger time scale.

It is also worth noting that the SSS architecture does not impose any particular requirements on the underline (virtual) circuit switching network. Any network technology that is capable to provision bandwidth guaranteed pipes can be used to implement SSS. For instance, SSS can be implemented on a wavelength switched optical network, on a packet network with Traffic Engineering, or on a SDN with OpenFlow control. It is apparent that such a capability will give the scheduler more flexibility in scheduling different sized data batches in larger time scale and at the cost of higher computing complexity.

Fortunately, the computing ability is available with the help of computer cluster.

Scheduling  Switching node  Storage on Switching Nodes  Scheduled Data Delivery  Control Interface               C1(1,4)  Node 1        Node 2         Node 3        Node 4         Node 5  C2(3,5)  C2sub(3,4) C2sub(4,5)    C. Preliminary Performance Evaluation [23] A preliminary performance evaluation of SSS networks is  conducted through simulations on the 14-node NSFNET. In the simulation, every switching node is equipped with storage.

Link speed in the network is assumed to be 10 Gbps. Figure 4 shows how the number/ratio of discarded data is related to storage equipped on each switching node and to the traffic load.

It shows that increasing the storage capacity on switching nodes is helpful to reduce the number of discarded data caused by lack of storage. Meanwhile, increasing the storage capacity will increase the number of accepted data flows and will ultimately increase the number of data flows that cannot be delivered without the node storage. So the overall discard ratio decrease slightly as the storage capacity increases.

Figure 4.  Number of discarded data vs. storage capacity

V. CONCLUSIONS In this paper, we discuss the big data transportation on  different network layers with different transferring paradigm.

Traditionally, big data are delivered by using the enhancement TCP and similar tools on packet network. Some other researches discussed the data transportation on optical network.

We have demonstrated GMPLS controlled network delivered big data files dynamically on field trail. Herewith we proposed and discuss the combination of circuit and storage together to form a now network node, which is termed as SSS. This will be a new paradigm promising for the future big data transportation.

