Mixed analog-digital design of a learning nano-

Abstract?The association of  CNTFET and adjustable resistive devices is investigated to  implement programmable logic using neural  networks. A learning cell, based on switched capacitors principle, is designed in order to apply a simplified learning rule to program the resistances associated to each synaptic weight of the network. Electrical simulations validate the relevance of  such approach.

Keywords: Neural networks, CNTFET, solid electrolyte, on-chip learning, programmable logic.



I. INTRODUCTION  Many research teams intend to take advantage of nanotechnologies in order to reach ultimate integration density of future integrated circuits. The specific manufacturing technologies that will be developed to produce those circuits at a reasonable cost will benefit from a redefinition of the system architectures and the development of new computing paradigms.  Among the expected properties of these paradigms, it can be cited the tolerance to the high dispersion of these device characteristics, the ability to support the bottom-up assembly techniques. Thus, bio-inspired architectures, especially neural networks, are currently revisited.  Relying on there distributed features, neural nets are natively robust and the learning process enables to manage highly dispersed characteristics and high defect rate foreseen for nano- components. However, a more detailed analysis shows that numerous difficult challenges remains to be faced before this concept could be actually demonstrated. First, we cannot forecast the technology which finally could replace CMOS at the end of the roadmap; secondly we don?t know how the components will be precisely aligned or self-organized, and finally, device characteristics evolve as the technology progresses and physical or compact models are currently under development. Nevertheless, circuit and system designers cannot be satisfied to wait for the technology to become stable.

They have to forestall future design challenges and identify the key parameters of the emerging nano-components. So, despite this moving framework,  we choose to propose a neural architecture and to contribute to fill the gap between the concept of a neural-inspired architecture using nano- components and the constraints of the detailed design of such architectures. We focus here on the feasibility of a neuronal learning process, using exclusively nano-components.

In this paper, we present the design of a mixed analog-digital circuit intended to implement the learning stage. In section II, we introduce the global architecture.  In section III, we focus on the neural learning rules. In section IV, we describe and model the device considered as adjustable conductance to implement  synapses. In section V, we present the design of the synaptic configuration unit. In section VI, we exhibit simulation results.

Finally, section VII is dedicated to discussion.



II. NEURAL ARCHITECTURE  Today?s FPGA integrate special memory block called look up table (LUT) to implement any logic function tabulated during the programming step. Here, we focus on programmable architecture comparable to those FPGAs, especially in the implementation of those generic logic block using small neural networks. A neural network is a graph; neurons are edges and synapses are vertexes of the graph.

Classically, the neuron state xj of neuron j stems from a non- linear decision operator (e.g. Eq. 2), applied to the linear combination Vj of all its input neurons xi weighted by their synaptic weight wij (Eq. 1).

Figure 1. A monolayer neural network  Vj = ?N  i=0 xi ? wij (1)  xj = sign(Vj)  (2)  In the context of programmable logic, as long as the logic functions are decomposed using linearly separable functions (when an hyper-plan can be found in the input space to separate the two output classes), very simple monolayer networks, can be considered (see Fig. 1). In such networks, each output cell (neuron) is directly connected to all the inputs though adjustable resistive devices (see Fig. 2). The conductance Gij of the resistive devices implement the synaptic weights wij that connect the neurons i and j. In addition, as the conductance cannot be negative, each state is coded using a dual representation, positively and negatively. The resulting voltage is a linear combination of the inputs and is compared to a  INPUTS  OUTPUTS x1  x2  x3  x4  w1,4  w1,5  w2,5  w3,5 w3,5  +x5 x5  V5      Figure 2. A resistive implementation of a simple neural network.

reference threshold to determine the binary output voltage.

Thus, the actual logic function is fully defined by the conductance state of the resistive devices that connect inputs to outputs. These conductance states are determined during the learning process, using pairs of input-output patterns that compose the training set.  The following section deals with the learning algorithms.



III. LEARNING RULES  In the context of neural networks, various training algorithms minimize the error of the network from a given training set. A detailed analysis demonstrates that most of the popular training algorithms, when applied to this single layer neural net architecture, falls in one of the two following classes. In the first class of algorithms, the modification of the conductance is obtained from the input-output pattern only. In this case, the learning algorithms implement an open loop process. In the second class, the modification of the conductance is based on the difference between the actual output of the network and the expected output related to the current input. As only the second class of learning algorithms can compensate the dispersion of characteristics and the defects of the network, we choose to study their implementation.

Monolayer-Perceptron and Boltzmann machines fall in this second class. Concretely, the synaptic weights (and the conductances) of the network are updated with respect to the following rule:  ?Gij = ?Wij = ? ?N  i=0 xi ? (yj ? xj) (3)  where ? is the modification step, xi ,  the input pattern,  xj , the  current output state, and yi  the expected (target) output for xi input. As the three parameters are binary, there are actually  eight configurations. Among these, two configurations lead to  increase the conductance by ?  and two others lead to decrease the conductance by the same quantity. In the others four cases,  there is no output error ( xj = yj ) and the conductance should remain stable (see Table I).

The following sections deals with the design of the mixed digital-analog cell dedicated to the implementation of this learning rule. In a previous paper [3],  we showed that the electrical characteristics of solid electrolyte adjustable resistive devices (CBRAM) [4] and carbon nanotube field effect transistor (CNTFET) are compatible. To go further, we need to precise the behavior of this memory element especially the dynamics of the resistive state switching.

TABLE I. TRUTH TABLE OF THE CONDUCTANCE UPDATE  xi xj yi G  1 -1 1 ?  -1 1 -1 ?  1 1 -1 ??  -1 -1 1 ??  X -1 -1 0  X 1 1 0  x1  -x1 +x1  INPUTS  OUTPUTS  x2  -x2 +x2  x3  -x3 +x3  Threshold  -1 +1  Resisitive  Device  Synapse Neuron cellDual inputs   - 2 -

IV.FUNCTIONAL MODELING OF ADJUSTABLE RESISTIVE STATE  The analysis of the CBRAM behavior shows that multi- level resistance effect can be demonstrated [4].  When the voltage across the CBRAM exceeds a threshold voltage (VT+), the resistance decreases to a minimum value. Conversely, when the voltage falls below a negative threshold (VT-), the resistance increases to the maximum value.

Figure 3. I-V characteristic of solid electrolyte based adjustable resistive device.

Nevertheless, continuous value of resistive state can be obtained when the programming current is limited to Ilimit during the resistive state switching. Indeed, figure 5 of [4] shows that the resulting resistance R is given by the following  rule: R ? Ilimit = VT+  In order to implement the previously described learning algorithm, we need not only to program a given conductance state but also to increment or decrement the conductance. For that purpose, the knowledge of the conductance switching dynamics is required, but not yet really studied.  Despite this lake of knowledge, we had to make reasonable assumptions in order to develop a realistic model for our electrical simulation.  We suppose that the temporal  variation dG dt  of the conductance G=1/R is proportional to the  voltage beyond a given threshold (positive VT+ or negative VT-).

if (V > VT+and G < Gmax), dGdt = K+(V ? VT+)  else if (V < VT?and G > Gmin), dGdt = K?(V ? VT?)  else dG dt = 0.

Provided this assumption, we have developed a Verilog-A behavioral model of such synapses. The simulation results given figure 4 make use of this model and demonstrate the resistance modification capability. The parameter VT+ and VT-, Rmin and Rmax have been extracted from literature, while parameter K+ and K+ have been chosen arbitrarily due to the lake of precise data about it.

I  V VT+  VT-  Rmax  Rm in  Figure 4. Simulation of a resistance adjustment.

Consequently, we can demonstrate now that narrow programming pulses lead to increment or decrement the conductance state and can be used for the learning step.



V. DESIGN OF THE LEARNING CELL  The learning cell is made of two parts. Both are built from CNTFET, using the compact model from IMS laboratory [5] and capacitors.  The first part is purely digital. It provides two binary control signals (increment / decrement) for the second part.  The truth table is deduced from table I. Figure 5 shows the digital part of the learning cell.  It makes use of 2 XOR2, 2 NOR3 and an inverter. The architecture of each gate is directly translated from its CMOS counterpart.

The second part, inspired from [6] is purely analog and delivers precise current pulses to increment or decrement the conductance of the resistive devices. As long as the signal  LEARN  is negated, signals INC and DEC are disabled and the capacitors C1 and C2 are shorted. When the signal  LEARN  is asserted, if xj ?= yj  one of the signals DEC or INC will be asserted. Consequently, one of the capacitors C1 or C2 is loaded. During the charge of the capacitors, the same current is drive to the resistive device in order to modify its conductance. When the signal INC is asserted, the current mirror P2-P3 drives a positive programming current to the resistive device, leading to increase its conductance.

Conversely, when the signal DEC is asserted, thank to the combination of two mirrors (P5-P6 and N3-N4) the current is extracted from the resistive device, leading to decrease its conductance.

During the programming pulse, using a simple resistance to model the CNTFET  conductance, the charge current decreases exponentially :  I(t) = VP  RCNT exp(  ?t RCNT .C  )  Thanks to the mirror gain, the voltage across the multi-level resistance is given by the following equation.

V (t) = Nmirror ? I(t)/G(t) As long as ?G << G(t) , the conductance value used to compute the voltage across the resistance can be considered  Vprog (V) 1.0   0 1.0 2.0  -1.0        -200  Time (?s)  Iprog(?A)  R(kOhm)   - 3 -    Figure 5. Architecture for learning and schematic of the learning cell.

constant and equal to its value at the time t0  corresponding to the beginning of the programming pulse :  V (t) = Nmirror ? I(t)G(t0) = V0 exp( ?tRCNT .C ) with V 0 = NmirrorG(t0) ? VPRCNT Then, the variation of conductance is given by the following  integral ?G = ? t0+?T  t=t0 K(V (t) ? VT )dt  where ?T = RCNT CLog( V0VT )  is the time for the voltage  V (t)  to fall bellow the threshold VT . In this case, the conductance modification is given by the following relation :  ?G = KRCNT C(V0 ? VT ) = KRCNT C( NG(t0) ? VPRCNT ? VT ).

This design leads to a relatively high area cell, especially due to capacitors. Fortunately, the learning cell can be time- multiplexed to be shared by a set of synapses. For example, in the architecture presented on Fig. 5, the learning cell is included in the neuron cell, to be shared by an entire row corresponding to the synapses connected to the inputs of this neuron. Thus, the input state corresponding to the selected  column (xsel ) must be routed to the learning cells of the array.

On one hand, as the learning cell is only required during the programming step, time multiplexing does not affect the performances of the circuit during normal operation. On the other hand, sharing the learning cell implies to disconnect the  normal input while an access transistor selects the synapse to program.

Simulation results are given below.



VI.SIMULATION  The simulation demonstrates the ability to learn a 3-input logic function using this learning cell.  Learning cells included in their neuron cell are time-multiplexed for all their row of synapses. Consequently, the programming scheme of the connection matrix cannot be fully parallel and require selecting the columns of synaptic connection sequentially for each pattern applied. The simulation shows 6 training steps (epochs), each one corresponding to the presentation of the whole training set. Obviously, the training set is composed of 8 patterns for a 3-bit logic function.  In this example, the network is trained to learn a NOR3. For visibility, the inputs are not shown, but the inputs states follow the sequence of binary- coded numbers: 000 001, 010 ...  111 for each training step.

During the learning steps, for each pattern, the inputs are sequentially selected to apply a programming pulse to each synaptic connection.

In this simulation, learning the logic function requires only 5 steps to reach the convergence. During the 6th step, no programming pulses are applied to the synaptic connections but actual and target neuron states are equal. We can conclude that learning of linearly separable function can be achieved using such architecture within a few learning steps and that remains compatible with the conductance range of resistive devices.

Learning Cell (LC)  Iprog  Iprog4  Iprog5  Positive and  negative  current  mirror  N1 P1  P2P3  P4  P5  P6  N2  C1  C2  N3N4  LEARN  DEC  INC  Vdd  Vdd  Vss  LC  Resisitive  Device  SYNAPSE  SEL1  SEL1  SEL  SEL  SEL  x1  x1  xsel ( xi )  yj  xj  x1 x2  INPUTS  OUTPUTS  x3  xsel  y4 x4  V4  x5V5  y5  xsel  LC   - 4 -    Figure 6. Simulation of neural-inspired learning of the NAND3 logic function. The learning process is completed at the 6th step.



VII.DISCUSSION  This work intended to contribute to the identification of the difficult challenges that remain on the way of the nanocomponents integration for neural-inspired programmable logic. In this context, three remarks emerge.

First, surprisingly, the design of both logic and analog circuit did not required very innovative design methodology.  Indeed, the same architecture could be build from CMOS transistors without major differences. However, one has to notice that the placement of CNTFET and the design of the physical layout remains to be addressed.

Second, simulation relevance is conditioned to the validity of our models, especially for resistive device. Nevertheless, various degrees of freedom in the circuit (e.g. capacitance values, current gain of mirror, programming voltage) permit to adjust the design to drive the required programming current.

Finally, using switch capacitors leads to large area. This can be partially solved when the cells are shared among a set of synapses, but it requires at least one selection transistor per synapse. We are currently working on a new learning scheme, fully parallel, where the selection transistor is unnecessary.



VIII.CONCLUSION  Numerous challenging breakthrough remains to be done before the efficiency of bio-inspired concept could be actually demonstrated in the context of nanodevices.  In this paper,  we investigated the association of CNTFET and adjustable resistive devices to implement on-chip learning of neural networks. Resistive devices featuring multilevel capabilities implement the synaptic connections between neurons. We presented a behavioral model for this devices taking into account the dynamic of the resistive state switching. Then, we designed a new learning cell based on CNTFET compact  model from IMS-Bordeaux. It appears that CMOS circuit architectures could be easily translated to CNTFET circuits.

Finally, simulations assess the compatibility, from the electrical characteristic point of view, of this components and circuits with neural learning algorithms for programmable logic.



IX.ACKNOWLEDGMENT  This work is supported by the French Research National Agency (ANR) as part of the PANINI project (ANR-07- ARFU-008).



X. REFERENCES  [1] Kohonen T, Self organization and associative memory, springer verlag Berlin 1984.

[2] Rosenblatt F, Perceptron simulation experiments, proceedings of the IRE, 3, 48, pp.301-309 1960.

[3] He M. et al, Architecture of Neural Synaptic Array, Design and Nanotechnology (IEEE-Nano?07), 2-5 august 2007, Hong Kong, (China), in , vol. 1, p. 601, 4 pages, 2007.

[4] Kund M. et al. Conductive bridging RAM (CBRAM): An emerging non- volatile memory technology scalable to sub 20nm, in IEEE International Electron Devices Meeting, 2005 (IEDM) Technical Digest., 5-7 Dec. pp.

754 - 757  [5] O'Connor et al. CNTFET modeling and reconfigurable logic-circuit 2365-2379  [6] Madani, K.; Garda, P.; Belhaire, E.; Devos, F., Two analog counters for neural network implementation,, IEEE Journal of Solid-State Circuits, 26 (7), Jul 1991 pp. 966 - 974.

