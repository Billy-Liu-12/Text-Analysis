An Approach to Grid Scheduling Optimization Based on  Fuzzy Association Rule Mining*

Abstract  This paper presents a grid scheduling optimization technique based on knowledge discovery. The main idea is to transform the grid monitoring data into a performance data set, extract the association patterns of performance data through fuzzy association rule mining, then construct optimization logic according to the mining results, and finally optimize the grid scheduling. In the process of data mining, a method of association rule mining is proposed based on time- window and fuzzy set concepts, which can mine data for quantitative attribute value based on the attribute and time dimensions in grid performance data set.

1. Introduction  Grid computing is not only an aggregation of resources, but also a system with some dynamic characteristics. It has the ability of self-improvement, and can optimize itself according to such historical information as configuration optimization, distribution and utilization optimization of the resources. The process of optimization could take advantage of the grid monitoring data. In order to get high performance, the reasonable decisions for the execution of the applications are needed.

In a grid system, applications share various resources with others. Therefore, how to make these applications get high performance is the problem of how to deal with grid schedule. Since grid has some unique features such as the resources in the grid are always dynamic, heterogeneous and diverse, schedulers have to deal local issues, and the grid scheduling technique is more complex than those conventional scheduling technique. Grid scheduling is one of the major factors that affect the grid performance. If a   * This paper is supported by National Science Foundation of China under grant 60273076 and 90412010.

scheduler works well in resource selection and task scheduling, each task may get the most suitable resource, thus the mean response time of the tasks will decrease. This will guarantee each task is completed within the limited period.

In grid environments, the behaviors of some objects or entities, being involved in the execution of applications, are much the same as those in a stochastic system. Actually, activities of those entities in a grid system follow certain rules. If we study carefully on these activities, an intrinsic relationship between two or more entities will be found, which will consequently enable us to predict future activities of the entities. For example, when a data visualization service program is executed, large amounts of input data that might be located in different places are required. Thus, the high network bandwidth utilization of the partial network in a certain period occurs running such service. Therefore, we can summarize the rules between some entities according to the grid monitoring data, and predict possible status of entities in the future. Furthermore, we can make preparation for the future activities in advance, and take advantage of free resources and also avoid the excessive requests for the specific resources, which improve the performance of the system.

The grid monitoring and accounting tools could record the historical status and activities of grid objects to get large amounts of monitoring and accounting data. These data include the execution information of grid applications, the utilization information of grid services, and the status information of grid system. In order to take good advantage of these data, it is necessary to classify and analyze them in multiple levels. Therefore, we apply data mining technique to grid scheduling optimization system based on the historical data to find out important association information from massive data, and optimize the scheduling process.

The data mining technique enables the system to handle a large amount of data without the evaluation     information provided by the user, and enables the system to find out the information that might be ignored by the people. Association rule mining is an ideal approach to solve the above problem. For the data with quantitative values, we introduce the concept of fuzzy set into the mining process. Meanwhile, according to the features of grid monitoring data, we apply the mining method based on time-window to perform fuzzy association rule mining.

The rest of the paper is organized as follows: the related works are discussed in section 2. Section 3 describes the related concepts of association rule and fuzzy set. Section 4 discusses the process of fuzzy association rule mining. Section 5 illustrates the application of association rule in grid scheduling optimization. Section 6 concludes the paper.

2. Related Works  The researches of grid scheduling optimization mainly include three methods [1]: optimization based on model analysis and simulation, optimization based on real-time performance prediction, and optimization based on online application reconfiguration [2].

Dimemas [3] is a performance prediction simulator for message passing applications. It reconstructs the time behavior of a parallel application on a target architecture. Performance Analysis and Characterization Environment (PACE) is a modeling toolset for high performance and distributed applications. It includes tools for model definition, model creation, evaluation, and performance analysis [4]. These methods have some limitations and cannot meet the dynamic characteristics in real environment.

In grid scheduling research, most schedulers get predicted resource parameters from Network Weather Service (NWS) [5] predictions. NWS is an agent system deployed on the grid to periodically monitor resource and performance. The goal of the PRAGMA [6] project is to realize a next-generation adaptive runtime infrastructure capable of support self- managing, self-adapting and self-optimizing applications on the grid [7][8]. These methods can predict the performance of the system according to grid monitoring data. However, the prediction methods that have been used right now merely focus on a single element, not considering the role of historical monitoring information played in this regard.

Autopilot [9] integrates application and system instrumentation with resource policies and distributed decision procedures. The resulting closed loop adaptive control system can automatically configure resources based on application request patterns and system performance. The goal of Grid Application  Development Software (GrADS) [10] framework is to provide good resource allocation for grid applications and to support adaptive reallocation if performance degrades because of changes in the availability of grid resources. These methods can reconfigure accurately the execution of the applications in time. But application programming needs to be changed for grid scheduling. Consequently, the developers have to be involved in the process of performance optimization.

3. Related Concepts of Association Rule and Fuzzy Set  3.1. Association Rule  Let I={i1, i2, ?, im} be a set of items. Let D be a set of database transactions where each transaction T is a set of items such that T?I. Let A be a set of items. A transaction T contains A if and only if A?T. An association rule is an implication of the form A B, where A?I, B?I, and A B= . The rule A B holds in the transaction set D with support s, where s is the percentage of transactions in D that contain A?B. This is taken to be the probability, P(A?B). The rule A B has confidence c in the transaction set D, c is the percentage of transaction in D containing A that also contain B. This is taken to be the conditional probability, P(B|A). Rules that satisfy both a minimum support threshold (min_sup) and a minimum confidence threshold (min_conf) are called strong.

A set of items is referred as an itemset. An itemset that contains k items is a k-itemset. The occurrence frequency of an itemset is the number of transactions that contain the itemset. This is also known, simply, as the frequency, support count, or count of the itemset.

An itemset satisfies minimum support if the occurrence frequency of the itemset is larger than or equal to the product of min_sup and the total number of transactions in D. If an itemset satisfies minimum support, then it is a frequent itemset. The set of frequent k-itemsets is commonly denoted by Lk.

Given a data set D, a min_sup, and a min_conf, the association rule mining is to find out all the association rules that satisfy min_sup and min_conf specified in the data set D.

3.2. Fuzzy Set  A fuzzy set expresses the degree to which an element belongs to a set. The characteristic function of a fuzzy set is allowed to have values between 0 and 1, which denotes the degree of membership of an element in a given set. If X is a collection of objects denoted     generically by x, then a fuzzy set A in X is defined as a set of ordered pairs: A={(x, ?A(x))|x?X}, where ?A(x) is called the membership function for the fuzzy set A. The membership function maps each element of X to a membership value between 0 and 1. Let x1, x2, ?, xn be the elements of the fuzzy set A and ?1, ?2, ?, ?n denoted the membership value of the elements. The fuzzy set A is commonly denoted as follows: A=?1/x1+?2/x2+?+?n/xn.

Given a fuzzy set A in a finite universe X, its cardinality, denoted by Card(A), is defined as Card(A)= ?A(x), where x?X. Card(X) is referred as the scalar cardinality or the count of A.

Union, intersection and complement are the basic operations in classical set. The union of two fuzzy sets A and B is a fuzzy set C, written as C=A?B, whose membership function ?C(x) is related to those of A and B by ?c(x)=max(?A(x), ?B(x)), ?x?X. The intersection of two fuzzy sets A and B is a fuzzy set C, written as C=A B, whose membership function is related to those of A and B by ?c(x)=min(?A(x), ?B(x)), ?x?X.

The complement of a fuzzy set A, denoted by A?, is defined by the membership function as ?A?(x)=1-?A(x), ?x?X.

4. Process of Fuzzy Association Rule Mining  4.1. Data Preprocessing  The primary source of grid monitoring and accounting data mainly includes system logs, user logs, service logs, performance records, etc. Generally speaking, the original data cannot be mined and needs to be preprocessed. A fraction of grid monitoring and accounting data is shown in Table 1.

When preprocessing the grid monitoring data, the major work includes data cleaning and data joining.

Data cleaning aims at cutting dirty and useless data, and revising the possible mistakes in data set, while data joining aims at combining various data sets that are needed in accord with the purpose of mining, actually transforming original semi-structure monitoring data into a new form for mining.

We extract various performance data from different original monitoring data with analysis tools. The relevant performance data in a certain period is organized and regarded as a performance list with multiple attributes [11], and each attribute is described with a quantitative value, such as the grid service utilization and bandwidth utilization of the partial network. We extract relevant information from monitoring and accounting data in Table 1, and arrange the performance data set in time order. In the new data set, we mine certain association rules [12].

4.2. Algorithm of Fuzzy Association Rule Mining  The object data contains the categorical and quantitative attributes. For quantitative attribute mining, attribute values are usually first divided into a set of continuous regions, and then these regions are transformed to categorical values. The transformation process may bring the sharp-border problem. So, we apply the concept of fuzzy set to the process of quantitative value mining so as to avoid the deviation caused by the sharp-border problem [13].

Grid monitoring and accounting data have their characteristics: (1) Data set has multiple attributes, and these attribute values may have some potential relationships with each other. (2) The relationship exists not only at the same time but also after a certain period. For example, when utilization of a grid service is high, high bandwidth utilization might occur in the partial network connecting with the host running this grid service at that time or after a certain period.

Concerning the above characteristics, we propose a method of mining based on time-window. When mining the grid monitoring data, we take into account not only the operations on data set but also the influence of field-relevant knowledge on the mining process. For the aforementioned mining situation, the influence of time is obviously important. For the common situations, the interval between two events will decrease the relationship between them, which give much support to  Table 1.  Fraction of grid monitoring and accounting data Time Grid service utilization  Time Network utilization  Time Disk utilization  11:30:00 26%  11:20:00 5%  11:30:00 66% 11:32:00 20%  11:25:00 11%  11:30:30 64%  ? ?  11:30:00 15%  11:31:00 65% 11:46:00 44%  11:35:00 13%  ? ? 11:48:00 51%  ? ?  11:45:00 43% 11:50:00 59%  12:00:00 42%  11:45:30 59% 11:52:00 15%  12:05:00 69%  11:46:00 64%     the method based on time-window.

According to the predefined time-window w, the  total events space is divided into several sub-spaces that overlap each other. In fact, we divide a large data set into multiple parts by time-window. Each part contains all the attributes in a large data set, and the values of each attribute do not exceed the time-window range. In each time-window, we only consider the association between the first record and others. In addition, the time-windows are specially handled to guarantee that each record in the data set is considered.

Multiple records in a time-window are organized as a new record which forms an extension data set. Table 2 shows the performance data set and the time-window.

Next, we describe the mining process by handling data set in Table 2. Considering that the performance data is denoted from A to E. In the performance data set, each record contains all the items in a time-window and data item is composed of attribute name and quantitative value. For the transformation from quantitative value to fuzzy set, we provide the membership functions in Figure 1. Therefore, the attribute value is transformed to the corresponding membership value of the fuzzy set. We mine process through the operation of cardinality of fuzzy set.

1 Utilization  Membership Value   Low HighMiddle  0.1 0.50.2 0.90.8   Figure 1. The membership functions used in this example  Assume that minimum support threshold is s, minimum confidence threshold is c, and time-window is w, the detailed process is described as follows:  STEP 1: Transform the quantitative values of each data item into fuzzy sets. In this example, the utilization represent is divided into three fuzzy regions: Low (L), Middle (M) and High (H). Thus, three fuzzy membership values are produced for each data item according to the predefined membership functions.

Take the data item (A, 55%) as an example. The quantity 55% is converted into a fuzzy set (0.83/A.M+0.13/A.H) using the given membership functions. This step is repeated for the other item data, and the results are shown in Table 3. Each term such as A.M is then regarded as an item in the mining process.

STEP 2: Construct the extended data set according to the predefined time-window w. In this example, w=3. The extended data set constructed is shown in Table 4.

STEP 3: Calculate the scalar cardinality of each item in the records as the count value. The calculation is processed in terms of the data in Table 3. Take the region A.L as an example. Its scalar cardinality is 0.08+0.15+0.93=1.16. This step is repeated for the other regions, and the results that are considered as the candidate 1-itemsets C1 are shown in Table 5.

STEP 4: For each item, check whether its count is larger than or equal to the predefined min_sup s, and the attribute regions satisfy min_sup are taken as frequent 1-itemsets L1. Assume s is set as 3.0 in this example. Since the count values of A.M, A.H, B.H, C.H, D.H and E.H are larger than 3.0, these items are put in L1.

STEP 5: Generate the candidate set Cr+1 from Lr. C2 is generated from L1 as follows: (A.M, B.H), (A.M, C.H), (A.M, D.H), ? Note that the different regions from the same attribute cannot be joined to generate a large itemset such as A.M and A.H. In addition, the same itemsets with different sequences are regarded as different.

STEP 6: Determine the frequent itemset Lr+1 from Cr+1.

Table 2. The performance data set and time-window Record Performance data items  1 (A,55%),(B,87%),(C,91%),(D,90%),(E,31%) 2 (A,47%),(B,89%),(C,73%),(D,92%),(E,55%) w 3 (A,95%),(B,20%),(C,89%),(D,88%),(E,94%)   w 4 (A,51%),(B,46%),(C,17%),(D,38%),(E,13%) 5 (A,90%),(B,95%),(C,89%),(D,63%),(E,82%) 6 (A,44%),(B,19%),(C,53%),(D,50%),(E,24%) 7 (A,13%),(B,91%),(C,93%),(D,17%),(E,80%) 8 (A,59%),(B,96%),(C,23%),(D,39%),(E,57%) 9 (A,96%),(B,60%),(C,28%),(D,23%),(E,87%)  10 (A,58%),(B,89%),(C,15%),(D,97%),(E,35%)     Table 3. The fuzzy sets transformed from the data in Table 2 Record Fuzzy items of performance data  1 (0.83/A.M+0.13/A.H),(0.93/B.H),(1.00/C.H),(1.00/D.H),(0.48/E.L+0.37/E.M) 2 (0.08/A.L+0.90A.M),(0.98/B.H),(0.23/C.M+0.58/C.H),(1.00/D.H),(0.83/E.M+0.13/E.H) 3 (1.00/A.H),(0.75/B.L),(0.98/C.H),(0.95/D.H),(1.00/E.H) 4 (0.97/A.M+0.03/A.H),(0.10/B.L+0.87/B.M),(0.83/C.L),(0.30/D.L+0.60/D.M),(0.92/E.L) 5 (1.00/A.H),(1.00/B.H),(0.98/C.H),(0.57/D.M+0.33/D.H),(0.80/E.H) 6 (0.15/A.L+0.80/A.M),(0.78/B.L),(0.90/C.M+0.08/C.H),(1.00/D.M),(0.65/E.L+0.13/E.M) 7 (0.93/A.L),(1.00/B.H),(1.00/C.H),(0.83/D.L),(0.75/E.H) 8 (0.70/A.M+0.23/A.H),(1.00/B.H),(0.68/C.L+0.10/C.M),(0.28/D.L+0.63/D.M),(0.77/E.M+0.17/E.H) 9 (1.00/A.H),(0.67/B.M+0.25B.H),(0.55/C.L+0.27/C.M),(0.68/D.L+0.10/D.M),(0.92/E.H)  10 (0.73/A.M+0.20/A.H),(0.98/B.H),(0.88/C.L),(1.00/D.H),(0.38/E.L+0.50/E.M)  Table 4. The extended performance data set Record Fuzzy items of performance data  (0.83/A.M+0.13/A.H),(0.93/B.H),(1.00/C.H),(1.00/D.H),(0.48/E.L+0.37/E.M),  (0.08/A.L+0.90A.M),(0.98/B.H),(0.23/C.M+0.58/C.H),(1.00/D.H),(0.83/E.M+0.13/E.H), (1.00/A.H),(0.75/B.L),(0.98/C.H),(0.95/D.H),(1.00/E.H)  (0.08/A.L+0.90A.M),(0.98/B.H),(0.23/C.M+0.58/C.H),(1.00/D.H),(0.83/E.M+0.13/E.H),  (1.00/A.H),(0.75/B.L),(0.98/C.H),(0.95/D.H),(1.00/E.H), (0.97/A.M+0.03/A.H),(0.10/B.L+0.87/B.M),(0.83/C.L),(0.30/D.L+0.60/D.M),(0.92/E.L)  ? ?  (0.70/A.M+0.23/A.H),(1.00/B.H),(0.68/C.L+0.10/C.M),(0.28/D.L+0.63/D.M),(0.77/E.M+0.17/E.H),  (1.00/A.H),(0.67/B.M+0.25B.H),(0.55/C.L+0.27/C.M),(0.68/D.L+0.10/D.M),(0.92/E.H), (0.73/A.M+0.20/A.H),(0.98/B.H),(0.88/C.L),(1.00/D.H),(0.38/E.L+0.50/E.M)  9 (1.00/A.H),(0.67/B.M+0.25B.H),(0.55/C.L+0.27/C.M),(0.68/D.L+0.10/D.M),(0.92/E.H), (0.73/A.M+0.20/A.H),(0.98/B.H),(0.88/C.L),(1.00/D.H),(0.38/E.L+0.50/E.M) 10 (0.73/A.M+0.20/A.H),(0.98/B.H),(0.88/C.L),(1.00/D.H),(0.38/E.L+0.50/E.M)  Table 5. The support of each itemset Itemset Support Itemset Support  A.L 1.16 D.L 2.09 A.M 4.93 D.M 2.90 A.H 3.59 D.H 4.28 B.L 1.63 E.L 2.43 B.M 1.54 E.M 2.60 B.H 6.14 E.H 3.77 C.L 2.94 C.M 1.50 C.H 4.62  (a) Calculate the fuzzy membership value of the candidate itemset in each record. Hence, the minimum operation is used for the intersection.

Take (A.M, B.H) as an example. Its membership value for ID 1 is calculated as: min(0.83, 0.98)=0.83. Note that the membership value for (B.H, A.M) is 0.90. The results for the other records are shown in Table 6. The results for the other 2-itemsets can be derived in similar way.

(b) Calculate the count of each candidate 2-itemsets in the records. The results for this example are shown in Table 7.

(c) Check whether these counts are larger than or equal to the predefined min_sup 3.0. The itemsets satisfy min_sup are thus kept in L2.

Table 6. The membership values for (A.M, B.H) Record A.M B.H (A.M, B.H)  1 0.83 0.98 0.83 2 0.90 0.98 0.90 3 0 1 0 4 0.97 1 0.97 5 0 1 0 6 0.80 1 0.80 7 0 1 0 8 0.70 1 0.70 9 0 0.98 0  10 0.73 0.98 0.73  Table 7. The fuzzy counts of the itemsets in C2 Itemset Support Itemset Support  (A.M, B.H) 4.93 (A.H, E.H) 3.11 (A.M, C.H) 3.50 (B.H, A.M) 5.08 (A.M, D.H) 3.49 (B.H, A.H) 5.36 (A.M, E.H) 3.98 (B.H, C.H) 3.91 (A.H, B.H) 3.57 (B.H, D.H) 4.47 (A.H, C.H) 2.14 (B.H, E.H) 4.80 (A.H, D.H) 2.87 ? ?  Two above steps are like the process of generating the candidate itemsets and determining the frequent itemsets in algorithm Apriori [14].

STEP 7: If Lr+1 is null, then do the next step; otherwise, set r=r+1 and repeat STEPs 5 to 7. Since L2 is not null in this example, set r=2. STEPs 5 to 7 are then repeated to find L3. C3 is first generated from L2.

In similar way, C4 is generated from L3. Since no itemset is put in L4, it is an empty set. STEP 8 then begins after three iterations.

STEP 8: Construct the association rules for each large itemset.

(a) Form all possible association rules.

(b) Calculate the confidence factors for the above  association rules. Assume the given min_conf c is 0.80. Take the association rule A.H E.H as an example. The fuzzy count of (A.H, E.H) is calculated shown in Table 8.

Table 8. The membership values for (A.H, E.H) Record A.H E.H (A.H, E.H)  1 0.13 1 0.13 2 0 1 0 3 1 1 1 4 0.03 0.80 0.03 5 1 0.80 0.80 6 0 0.75 0 7 0 0.92 0 8 0.23 0.92 0.23 9 1 0.92 0.92  10 0.20 0 0 count 3.59 8.11 3.11  The confidence factor for the association rule A.H E.H is:  87.0 59.3 11.3  ).(  )..(  ).( ).,.(     1 == ?  = HA  HEHA  HAs HEHAs  (c) Check whether the confidence factors of the above association rules are larger than or equal to the predefined min_conf c. The rules that satisfy min_conf are output to user. These rules are thus considered as meta-knowledge concerning the given data set.

In the above process, the conventional algorithm Apriori for transactions database, gradually iterating from low level to high, may be used. But more feasible approach is mining the data set based on the predefined association rule patterns according to the field-relevant knowledge and requirements for the mining. In this example, we take the utilization of grid services as the premise of association patterns. Thus, the output results are shown in Table 9.

Considering the facts of grid monitoring and accounting data mining, we apply the time-window method since many association rules may not be included in a record. The main difference between the algorithm based on time-window and the conventional one is that each transaction is used as a unit of scan in the conventional algorithm, and time-window algorithm takes the multiple records divided by time- window, that is a new record in the extended data set,  as a scan scope. Due to introducing the concepts of time-window and fuzzy set, we can mine association rule for quantitative data based on the attribute and time dimensions in grid performance data set [15].

Table 9. The rules output to user ID Rule Support Confidence 1 A.M B.H 4.93 1.00 2 A.M E.H 3.98 0.81 3 A.H B.H 3.57 0.99 4 A.H E.H 3.11 0.87 5 B.H A.M 5.08 0.83 6 B.H A.H 5.36 0.87 7 (A.M, B.H) D.H 3.16 1.00  5. Application of Association Rule  Through the above association rule mining for the grid performance data, the important mete-knowledge behind of mass data can be acquired. To apply these results to optimize grid scheduling, we must further analyze and evaluate the association rules discovered, and select the rules that are conducive to optimization.

In the example above, the association rule (A.M, B.H) D.H describes the relationship between the utilization of grid service A and B and the bandwidth utilization of the partial network D. Since A.M denotes ?middle utilization of grid service A?, B.H ?high utilization of grid service B? and D.H ?high bandwidth utilization of the partial network D?, the corresponding optimization logic that describes the association rule above is defined as Figure 2.

// min_sup = s, min_conf = c, time-window = w struct {  object; status; time_window;  } conclusion; premise1 = utilization of A; premise2 = utilization of B; if (premise1 == MIDDLE) && (premise2 == HIGH) { conclusion.object = ?D?; conclusion.status = HIGH; conclusion.time_window = w; Predictioninfo(conclusion); //notify the prediction  information }  Figure 2. Optimization logic of association rule (A.M, B.H) D.H  When the optimization system detects the utilization of grid service A is middle and B is high, it notifies scheduler that the bandwidth utilization of the partial network D may be high within w time-window period.

Thus, scheduler should decrease further more resource     requests for the partial network D within the corresponding period to avoid the bottleneck of resource acquisition.

Whether the strong association rules discovered can be used to optimize grid scheduling still needs to be analyzed and discussed further. Whether a rule is interesting or not can be judged either subjectively or objectively. The user can judge if a given rule is interesting or not, and this judgment, being subjective, may differ from one user to another. However, objective interestingness measures, based on the statistical independence and correlation analysis, can be used as one step towards the goal of weeding out uninteresting rules from presentation to the user.

6. Conclusion  Grid scheduling technique is more complex than the conventional scheduling technique in high performance computing system, and grid scheduling is one of the major factors that affect the grid performance. The monitoring and accounting tools may record the activities and status of various grid objects in the form of large amount of historical performance data. In order to take good advantage of these data, this paper applies an association rule mining technique to the process of grid scheduling, constructs optimization logic according to the mining results, and finally optimizes the grid scheduling. In the process of data mining, a method of association rule mining is proposed based on time-window and fuzzy set, which can mine association rule for quantitative data based on the attribute and time dimensions in grid performance data set.

Although the proposed method works well in data mining for quantitative data, several limitations remain to be addressed. The above method assumes that the membership functions are known in advance. In the future, we will attempt to dynamically adjust the membership functions in the proposed mining algorithm so as to avoid the bottleneck of membership function acquisition.

In addition, for mining and analyzing the monitoring and accounting data that contain application activities with categorical attributes, we will also discuss and design specific data mining models for grid scheduling optimization.

