

Research of Commonly Used Association Rules Mining Algorithm in Data Mining    Ruowu Zhong  Huiping Wang Institute of Computer  Institute of Computer Shaoguan University Shaoguan University  Shaoguan, Guangdong Province, China Shaoguan, Guangdong Province, China   huipingwang2011@sina.cn    Abstract - This paper introduces the concept of data mining and its an important branch - association rules, describes the basic concept of association rules, the basic model of mining association rules; introduces the classical algorithm of association rules, and then classified discusses the association rules mining from several angles such as width, depth, partition, sampling and incremental updating. Finally, this paper prospects the association rules mining.

Index Terms - data mining; association rules; frequent itemsets; mining algorithm

I.  INTRODUCTION  Data mining is from a large number of incomplete, noisy, ambiguous and random data, extracting implicit in them, people do not know in advance but is potentially useful information and knowledge. Data mining is used to specify the model type that the data mining task to find. Data mining tasks can generally be divided into two categories: description and prediction. Descriptive mining tasks characterize the general characteristics of data in the database. Predictive data mining tasks predict on the base of the referring of current data.

Data mining system can mine models of a variety of types and various time granularities to meet the needs of different users or different applications. Its main function is as follows [1]: (1) conceptual description; (2) correlation analysis; (3) classification and prediction; (4) clustering; (5) outlier analysis; (6) evolution analysis.

Association rule mining is to help find the relationship between the itemsets in a large number of databases. By describing the potential rules between the items in databases, dependencies between multiple domains which meet the given support and the confidence threshold are found. The relationship is hidden and unknown in advance, is not got by the logic operation of a database or statistical methods. They are not the inherent properties of the data itself, but on the characteristics of data items simultaneously. The most typical example of association rules cases is: "80 percent of customers buy beers also buy diapers at the same time", its intuitive meaning is, how larger the tendency of customers buy certain products while they will buy other goods.

Discovering the rules like this is valuable for setting marketing strategy. Association rules can be applied to analysis of customer shopping, product shelf design, mailing  of commercial advertising, catalog design, additional sales, storage planning, network fault analysis, and classification of the users based on buying patterns.



II. BASIC CONCEPTS OF ASSOCIATION RULES  In order to more clearly describe the algorithm, the basic concepts of association rule mining are introduced first.

Definition 1.1 The data set of association rules is denoted by D, D is the transaction database, D = {t1, t2 ... tk ... tn}, tk = {il, i2, ... im ... ip}, tk (k = 1,2 , ... n) is called a transaction, im (m = l, 2, ... p) is called a item. In the transaction database, item is the name of goods or services, transaction also includes other information such as date, customer number and so on. Simply put, transaction is a collection of items.

Definition 1.2 Let I = {il, i2, ? , im} be a collection consisting of all items in D, any subset X of I is called Itemset in D, | X | = k item X is said to be k-Itemset. Let tk be transaction and let X be itemset, if X ? tk, we call transaction tk contains itemset X. Each transaction has its own unique identifier called TID.

Definition 1.3 The dataset D contains the number of item set X, we call this the support of the itemset X, denoted by ? x. The support of item X is denoted by Support (X), of which: Support (X) = (? x / | D |) * 100%, where | D | is the number of transaction in dataset D. If the support of X is not less than user-specified minimum support, then X is called frequent itemsets, referred to as frequent sets (or large itemsets); if the support of X is less than user-specified minimum support, then X is called non- frequent itemsets, referred to as non-frequent sets (or small itemsets).

Definition 1.4 Association Rules can be simply expressed as: R: X?Y, where: X? I, Y? I, and X? Y =? , it is said that if the itemset X occurs in a transaction, then Y will inevitably appears in the transaction. Therefore, X is called a prerequisite for the rule; Y is the result of the rule.

Definition 1.5 For the association rule R: X?Y, where: X? I, Y? I, and X? Y =? . Confidence of rule is defined  as: Confidence (R) = )X(Support  )YX(Support ? , it reflects the  conditional probability that a transaction having X also contains Y.

DOI 10.1109/ICICIS.2011.63     Definition 1.6 The support of association rules is the ratio of the number of transactions contain both X and Y and the number of all transactions, denoted Support (X?Y), that is, Support (X ? Y) = Support (X ? Y). Support reflects the probability that a transaction contains X? Y.

Support and confidence are two important concepts to describe association rules. If we do not consider support and confidence, there are a huge number of rules association rules in the transaction database. In fact, most people are usually interested in the association rules that satisfy certain support and confidence. In order to find valuable association rules, we need to specify two thresholds: minimum support and minimum confidence. The former is the minimum support that user-specified association rules must satisfy, which represents the minimum a set of items set in the statistical sense need to satisfy; the latter is the minimum confidence specified by the user association rules must satisfy, which reflects the minimum confidence of association rules.



III. THE BASIC MODEL OF ASSOCIATION RULES MINING  The task of association rules mining is to find out all the strong association rules in transaction database D with user- given minimum support and minimum confidence.

Corresponding itemsets of strong association rules A?B must be frequent itemsets, and the confidence of association rules A ?B derived from frequent itemsets A B is calculated by the frequent itemsets A and AUB's support. Therefore, the association rules mining can be decomposed into two steps:  The first step is to find out all the frequent itemsets in D quickly and efficiently, which is the central issue of association rules mining.

The second step is to produce a strong frequent itemsets.

We use frequent itemsets to generate the required association rules, based on the user-given minimum confidence to select.

Finally, strong association rules are generated.



IV. ALGORITHM FOR ASSOCIATION RULES MINING  A. The Classical Association Rule Mining Algorithm Among the algorithms, Apriori [2] algorithm is the most  classical association rule mining algorithms. The algorithm was first proposed by Agrawal et al in 1993. This algorithm is decomposed into two steps: discover frequent itemsets from candidate frequent itemsets, generate rules from frequent itemsets and pioneering the use of support-based pruning technology, and control the exponential growth of the candidate set.

Apriori algorithm strategy is to separate association rule mining tasks into two steps:  1) The generation of frequent itemsets: by iteration, finding all the itemsets that satisfy minimum support threshold, that is, find all frequent itemsets.

2) Generating association rules: extract high confidence rules from the frequent itemsets found by the former step, which are the strong association rules.

The first step for mining frequent itemsets, the algorithm will produce a large number of candidate items, and in order  to generate frequent itemsets, scanning databases need to loop through pattern matching to inspect candidates, the cost of computation time of this step will be much larger than the second step, first step is the core of the algorithm.

Here, the basic idea of the generating frequent itemsets is: The algorithm requires multi-step processing of data sets. The first step, statistics the frequency of the set with an element, and identify those itemsets that is not less than the minimum support, that is, the maximum one-dimensional itemsets. Then start the cycle processing from the second step until no more maximum itemsets generated. The cycle is: in the first step k, k-dimensional candidate is generated form (k-1) dimensional maximum itemsets, then scans the database to get the candidate itemset support, and compare with the minimum support, k-dimensional maximum set is found.

Apriori algorithm has two fatal performance bottlenecks: huge numbers of candidates are produced; with multiple scans of transaction database, tedious workload of support counting for candidates will be spent.

B. Commonly Used Association Rules Mining Algorithms  1) Data Set Partitioning Algorithm Data set partitioning algorithms include Partition  algorithm proposed by Savasere [3], DIC algorithm [4]proposed by Brin. Partition algorithm logically divides entire database into a few of separate memory blocks that can be stored in the data processing, saving time of accessing external memory I / O. It is considered separately for each logic block to generate the appropriate frequent sets, and then use "frequent itemset in at least one partition is frequently" the nature to unit the frequent sets in all the logic blocks to generate all possible global candidate sets. Finally, scans the database again to calculate global count of support of sets.

The whole process takes two scans of the database, but the number of candidate itemsets generated is relatively large.

DIC algorithm is also adopted by the thinking of dividing the database into several partitions, each partition is marked at the beginning in the process of scanning the database, candidate items can be added in each marked partition, the support can be calculated when the itemsets are calculated. In this algorithm, the times of scanning the database are less than the maximum number of frequent sets. If the data blocks are divided just the right time, the times of scanning the database is only two, and all the frequent itemsets can be found.

The main bottleneck of data set partitioning algorithm is the execution time is long and frequent itemsets generated is not very high accurate. But this type of algorithm has a high degree of parallelism, just scanned twice database, greatly reducing I / O operations to improve the efficiency of the algorithm.

2) Depth-First Algorithm Common depth-first algorithm includes FP-growth  algorithm algorithm [5], OIP algorithm [6], TreeProjection algorithm [7].

FP-growth algorithm is one of the latest and most efficient algorithms in depth-first algorithm. The algorithm does not subscribe to generate - and test paradigm of Apriori. Instead, it     encodes the data set using a compact data structure called and FP-tree and extracts frequent itemsets directly from this structure.

Compared with the Apriori algorithm, FP-growth algorithm has the following advantages: FP-growth algorithm only scans the database twice, to avoid multiple scans database; do not have a large number of candidates, in the mining process significantly reduces the search space, time and space efficiency are increased dramatically. But its difficulty lies in dealing with large and sparse database, in the mining processing and recursive computations require considerable space.

3) Breadth-First Algorithm Breadth-first algorithm, also known as hierarchical  algorithms, including Apriori algorithm proposed by Agrawal and others, AprioriTid algorithm[8] and AprioriHybrid [9] algorithm, DHP algorithm [10] proposed by Park et al and so on.

Apriori algorithm is breadth-first algorithm, ApriofiTid algorithm evolved based on the Apriori algorithm. This algorithm uses Apriori algorithm when scans the database for the first time, when scans for the second time, it no longer re- scans the entire database, but only scans the last generation of candidate items, and calculates the degree of support of frequent itemsets at the same time ,it reduces the time of scanning database and improves the algorithm efficiency.

AprioriHybrid algorithm is proposed based on AprioriTid algorithm and Apriori algorithm. This algorithm first uses Apriori algorithm when do the initial scan of the database.

When the size of candidate itemsets generated can be stored into memory for processing and then turn to AprioriTid algorithm, until we find all the frequent itemsets. DHP algorithm uses hash table technology to prune data sets and candidate sets to reduce the time and space. It uses a hash table to calculate a rough set of k- itemsets support when in the calculation of (k-1) ? itemsets. It eliminates meaningless k-item candidate set to reduce the number of k-itemsets, especially for controlling the number of Candidate 2 - itemsets. Overall, the breadth-first algorithm has the shortcoming of generating a large number of candidate items and need to repeatedly scan the database.

4) Incremental Update Algorithm Incremental updating algorithm is the use of association  rules have been mined to find new association rules on changed database or parameters, and delete obsolete data sets to maintain updated. Most of the current incremental updating algorithms are based on Apriori algorithm as the core for the improvement and evolution, including FUP algorithm and FUP2 algorithm [11] proposed by D. W. Cheng. et al, IUA and PIUA algorithm proposed by Feng Yucai et al [12], IUAR algorithm proposed by Gao Feng et al [13] and so on.

FUP algorithm is the improvement of the Apriori Algorithm, and it is a classical algorithm to solve the problem of incremental update. FUP algorithm is mainly targeted at the minimum support and minimum confidence unchanged, the database DB to be added, deleted or modified, how to  generate association rules of the updated database. It uses the frequent itemsets to avoid double-counting the number of frequent itemsets and improve the efficiency. FUP algorithm is inadequate: the algorithm spends a lot of time in dealing with huge sets of candidates; pattern matching on the candidate set requires costly repeatedly scan of the database.

FUP2 algorithm taking into accounts the increase in database and modifies or deletes the database, is suitable for the case of large increase and small deletion of database.

IUA, PIUA algorithms are mainly considered in the minimum support and minimum confidence changes and the database is constant, how to generate association rules in database.

IUAR algorithm is mainly considered in the minimum support and minimum confidence and the database the same time changes, how to generate the updated association rules.

5) Sampling Algorithm Sampling algorithms include adaptive mining algorithm  proposed by Park et al[14], Toivonen's Sampling [15] algorithm. Sampling algorithm randomly selects a subsets of the database D ' from the database D that can be transferred to the memory , and then finds all the rules in the subset of D' that may be set up in the database D, and then the rest (D- D ') to verify the accuracy of the results. It is suitable for mining in not too accurate but very high efficient environment.

Sampling algorithm has considerably reduced the time cost of scanning the database, but its biggest drawback is that the results may lead to data skew and inaccurate results. When a random sample selected can not represent the distribution of the entire database, it is possible to lose some global frequent itemsets and lead to inaccurate results. If the frequent itemsets contains all frequent itemsets in D, then we need to scan D only once. Otherwise, in order to reduce the impact of this problem, we can use a smaller support threshold to scan the database again on the random sample to generate frequent itemsets and find the missing frequent itemsets in the first scan. By scanning the database repeatedly, the frequent itemsets omission is reduced. For data skew, Lin and Dunham in the literature [16] discussed the Anti-skew algorithm to mine association rules, the number of scan data sets can be reduced to less than 2 times.

Association rule mining algorithms currently used include: Parallel mining algorithms, constraint-based mining algorithm, multi-level association rules mining algorithm, multi-dimensional association rule mining algorithm, weighted support algorithm and so on.



V. CONCLUSION  In this paper, association rules of data mining were discussed clearly and in details, and several commonly used mining algorithms were analyzed and compared and summarized. Multi-valued association rule mining, interestingness measures and assessment techniques, t the maintenance of enhanced association rules method, the interaction of enhanced association rule mining algorithm with the user, the application of association rule mining to the     privacy and information security issues are hot issue for further research.

However, the improvement of existing algorithms can not satisfy people's mining system for quick and timely response to demand, how to improve the efficiency of mining process, to interact with the user and generate visual results and so on are the focus of future research work.

