Identification of anomalies in processes of database alteration  Francesco Mercaldo

Abstract? Data, especially in large item sets, hide a wealth of information on the processes that have created and modified them. Often, a data-field or a set of data-fields are not modified only through well-defined processes, but also through latent processes; without the knowledge of the second type of processes, testing cannot be considered exhaustive. As a matter of fact, changes in the data deriving from unknown processes can cause anomalies not detectable by testing, which focuses on known data variation rules. History of data variations can yield information about the nature of the changes. In my work I focus on the elicitation of an evolution profile of data: the values data may assume, the change frequencies, the temporal variation of a piece of data in relation to other data, or other constraints that are directly connected to the reference domain. The profile of evolution is then used to detect anomalies in the database state evolution. Detecting anomalies in the database state evolution could strengthen the quality of a system, since a data anomaly could be the signal of a defect in the applications populating the database.

Index Terms?anomaly detection, outlier, data mining, intrusion detection, pattern recognition

I. INTRODUCTION The anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior [1]. This is a relevant problem as anomaly detection is applicable in a variety of domains, including intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, and detecting eco-system disturbances. My work focuses on the detection of anomalies affecting the variation of data fields by analyzing data-change logs. Data are altered by a series of well-defined processes for which it is possible to write rules to verify them. In addition, data undergo changes by processes not explicitly defined. Testing techniques can hardly identify anomalies in data arising from the latter type of processes. For this reason it is needed to complement testing, which verifies compliance to explicit processes, with means to validate the changes deriving from latent processes. Due to the intrinsic nature of latent process, which could be informal or unknown, it is required a methodology that is able to detect the process to verify it.

Consider a dataset with two well-defined processes with different tasks that modify a set of database fields, the first process changes 3 fields, the second 4. Of course, the knowledge of these processes would allow for defining test cases that check if data are changed accordingly to the intended behavior of processes. However, a latent rule, which is not explicitly included in any of the two models,  could exist, that entails the two processes are performed at least once within a period of one month. The thesis statement is to learn the latent rule by analyzing the running system (in particular the data change logs) and use it to verify the systems behavior at run-time. In other words in my work I focus on data changes over time, with the goal of learning latent processes, and the underlying rules, from the observed changes to capture and to make explicit latent processes. This kind of processes can have multiple causes, including defects in the system at different levels: incomplete or incorrect requirements, flawed design that does not include the appropriate controls on data and procedures, coding errors. These flaws can result into vulnerability breaches of the system or failures. In addition to complement testing, anomaly detection can be used in security: a model of anomalous traffic in a computer connected to the network could detect sending sensitive data to an unauthorized destination. Anomalies in transactions credit card could indicate identity theft or erratic readings from a sensor [1]. Furthermore anomaly detection can play a key role also in infrastructure management. In fact, some structural abnormalities in complex infrastructures, before being intercepted by the sensors or institutional testing activity, can be verifiable on other data that are not monitored by sensors. For example, it could happen that in a water distribution network, a series of small loss of intensity (no perceptible by sensors), causes a rise in a geographic area (abnormal) of the water consumption of all users.

So far research in this area focused on traditional static and dynamic outlier detection [1,2], widely applied in data mining problem to highlight the correlation between the variations. The problem of comparing the evolutionary history of the data has been published by Gupta et al. [3].

Lim [5] uses a combination of data mining and statistical analysis techniques on database logs for predicting failures and anomalies in the system.



II. THE APPROACH Over time, a variety of techniques for anomaly detection have been developed in various research communities.

Many of these techniques have been specifically developed for certain application domains, while others are more generic. In fact the problem of detecting anomalies in any system implies the existence of a concept of normality. The notion of "normal" is usually provided by a formal model that expresses the relationships  between the key variables involved in the dynamics of the system. Consequently, an event is classified as abnormal because the degree of   DOI 10.1109/ICST.2013.72     deviation from the profile of the characteristic behavior of the system, as specified by the model of normality, is quite high. The concept of normality is related to the concept of frequency: in fact, are considered part of the model learned the actions that are performed periodically in the same way.

The latent processes can be identified by variations on the data, it is critical distinguishing between sporadic variations, probable source of anomaly and latent processes, carried out at regular intervals. To find the patterns of variation of the fields that are repeated at regular intervals is the same as finding association rules in a dataset whose elements are the variations of the fields of databases.

The idea I?m exploring in my work is to mine association rules from database log to extract variation patterns. The association rules in this context represent the "normal" behavior of the dataset.



III. STATE OF DEVELOPMENT AND PRELIMINARY EVALUATION  An early proof of concept has been developed for a preliminary assessment study. The aim was to understand how robust are association rules in learning the variation patterns in the presence of noise, i.e. spurious changes in data. For the extraction has been used the algorithm Apriori[4]. Noise is modeled with one or more fields whose expected change has not taken place. It is important to understand the behavior of the leaning method when noise increases because it could alter the capability of extracting correct rules. Effectiveness is evaluated with 3 metrics: Max rule confidence, Precision and Recall.

The evaluation includes 11 treatments, where each treatment corresponds to a ?configuration?. Each configuration is determined by: the level of noise, i.e. number of no-changes and, an incremental number of temporal clusters without variations, i.e. in which changes are not observed. Noise configuration is represented in the matrix of variation as an equilateral triangle of 0. The ?maximum rule? is the rule that includes the greatest number of data fields; in terms of interpretation of the problem, it is also defined as the maximum rectangle variations occur within the matrix.

Remaining rules are a subset of maximum variation rule.

Figure 1 :   variation matrix An example of a variation matrix is reported in Figure 1, where the fields are indicated with F while temporal clusters  with C. Configuration noise is equal to 5 (an equilateral triangle along 5 non-variations).

For each treatment, the system was able to extract the maximum rule, with any configuration of noise.

The algorithm has been executed varying the confidence level from 0.1 to 1. Recall is always equal to 1 because the maximum rule is always extracted at each execution of the algorithm; precision, since each rule extracted is a subset of the maximum rule and there are no fields considered illegal in the test scenario is always equal to 1.

Figure 2 shows confidence that allowed the extraction of the maximum variation rule with noise variation. Note that trend line is increasing when the noise decreases. The maximum rule is extracted with a degree of confidence equal to 1 if all fields of the legal pattern undergo at least one change in each cluster temporal considered.

Figure 2 : max rule confidence

IV. FUTURE DEVELOPMENT In this paper a brief introduction of my work on anomaly detection in databases was presented.

The focus is detecting latent processes to ensure the correctness of the normal life of data. The proposed approach consists in the extraction of pattern fields that vary together, the problem has been traced to search for association rules. There are  several promising directions that could be taken in this area, first of all by identifying the temporal order in which the changes occurs. Another direction of research could be focused on the semantic meaning of change, in this context, we would be talking of data semantic anomaly detection. Another interesting development is represented by the research of the number of variations to which it is subjected every single field, so as to find rules also ordered by variations number.

REFERENCES [1] V.Chandola, A. Banerjee, V. Kumar,? Anomaly Detection: A  Survey", ACM Computing Surveys (CSUR), Vol. 41(3), 2009.

