

Abstract? Information visualization has progressed and taken big steps in previous decade, despite challenging complexities of presenting and transforming the data.

Visualization binds the perceptual capabilities of the human visual system. In the data, Human being looks for structure, pattern, features, anomalies, and relationship.

Visualization, support this by preparing the data in a way to drive particular sense that differentiate various interactions and understanding.

How human being receives and interacts with a visualization tools, can strongly influences his understanding of the data as well as the system?s usefulness. Therefore, understanding the tools, relationships, and how well be able to depict the blue print of the model in mind, is not an easy task. Too often, successful decision-making and analysis are more a matter of serendipity and user experience than of intentional design and specific support for such a task [2].

We need better metrics and benchmark repositories to compare tools, and we should also seek reports of successful adoption and demonstrated utility. Moreover, there is a large range of target audience with different background and therefore, examining the concept, data, and analytic methodologies for these class of audience also is a big step in the right way. Furthermore, we also should consider how tools -for transformation and presentation ? can improve mental activities of developer. This mental support has been defined as ?cognitive support? [3]. So, are we able to explicitly state and compare claims about how particular tool support cognition? Are there capable theories for backdrop, onto which suitable theories and claims can be painted?

Unfortunately, there are too many factors and relations which we should consider in order to be able to have a clear cut of measuring the relationships and their boundaries. In this paper, I?ll try to open the question and shed on some important and very difficult aspect of visualization evaluation.

1.1 INTRODUCTION  uman being visual system is a communication channel, taking input, process it, and generating output, with overlap being the amount of transmitted information.

Visualization which is the display of information, transforms data into images that effectively and accurately represent information. Therefore, visualization deals with the issues of transformation and representation.

Transformation is the process of converting data from its original form into graphic primitives and eventually into   M.K.Ghanbari is with School of Engineering & Technology, Department of Computer Science, and Alabama A&M University. Normal, AL 35762 USA. (e-mail: Muhammad.ghanbari@aamu.edu).

computer images [1]. Information visualization tools, tasks, and data vary significantly, and therefore, making a unified evaluating methodology is hard to create. Generally accepted and used data sets, tasks, measures, and methods are needed. Here we look at several comparisons methods:   1.2 Measurement versus detection:  Cleveland reported a study performed to rank graphical attributes in terms of human ability to perceive and classify changes [6]. The particular attributes tested were angle, area, color, hue, color saturation, density (amount of black), length (distance), and position along a common scale, position along identical, nonaligned scales, slope and volume.

In his experiment showed errors in perception ordered as follows (increasing order):  1. Position along a common scale 2. Position along identical, nonaligned scales 3. Length 4. Angle/slope(depends greatly on orientation and  type) 5. Area 6. Volume 7. Color hue, saturation, density  Another important factor in that context is:  2. Dimensionality of the graphical attributes  Human being always looks for structure, shape, feature, pattern, trends, anomalies, and relationships in data that could be from the following:  ? Numeric, symbolic, or combination ? Scalar, Vector, or Complex structure ? Various units ? Discrete or Continuous ? Spatial, quantity, categories, temporal, relational,  structural ? Accurate or approximate ? Dense or sparse ? Ordered or non-ordered ? Disjoint or overlapping ? Binary, enumerated, multilevel ? Independent or dependent ? Multidimensional ? Single or multiple sets ? May have similarity or distance metric ? May have intuitive graphical representation ? Has semantics which may be crucial in graphical  consideration  Scalability of visualization?s evaluation:  Muhammad Ghanbari, IEEE, Department of Computer Science, Alabama A&M University  H          Visualization supports this by presenting the data in various forms with different interactions.

Visualization can provide:  ? A quantitative overview of large and complex data sets.

? Summarize data ? Can assists in identifying region of interest ? Appropriate parameters for more focused  quantitative analysis.

?  3.  Empirical comparison of commercial information Visualization  Kobsa [4] in An empirical comparison of three commercial information visualization systems compares three commercially available systems 1-Eureka, 2-infozoom, and 3-Spotfire?s and came up with this conclusion that relatively simple and bound to the structure of the underlying data depends on many factors:  ? The properties of a visualization ? The operations that can be performed upon a  visualization ? The concrete implementation of a visualization  paradigm ? Visualization-independent usability problems,  studied by Kobsa found that users achieved only 68%-75% accuracy on simple questions involving some common commercial systems indicates that even these operations have room for improvement.

4. A knowledge Task-Based Framework for design and evaluation  Amar & Stasko, discuss support for bridging the analytic gap, and propose a framework for design and evaluation of information visualization systems, give three reasons for lack of certainty about current system to adequately support decision-making [2].

? limited affordances, ? predetermined representations ? decline of determinism in decision-making  They are arguing, most of current information visualization systems emphasis on use?s ability to ?unpack? the representations of data of interest and operate on them independently. In their paper they discuss analytic gaps, such as decision-making and learning. Furthermore, they classified limitation on current system as two folds:  1- Gap between perceiving a relationship and expressing confidence in correctness and utility of that relationship.

2- Worldview gap, representing the gap between what is shown to draw a representational conclusion for making a decision and as something go between. They propose three task forms that serve or diminish these gaps. In 1, they pen point  a. expose uncertainty  b. concretize relationships c. Formulate cause and effect  And in worldview, they suggest the following: a. Determination of Domain parameters b. Multivariate explanation c. Confirm hypothesis.

5. Task Analysis and Goal Information visualization is state of mind environment  which get insight into unstructured, multi-dimensional data, continuous or categorical. Insight is gained by understanding relations and patterns in the data. Information visualization deals with different data types, and specific user goals (exploration, analysis, or presentation). The visualization space ranges from 1D to 3D Cartesian coordinates and often multiple views are used. The limitation of the viewing space (i.e., screen space) is common, and the importance of interaction and interactive feedback is common consent. The goal is gaining insight about the underlying data through a visual metaphor relying on the human ability of visual perception.   Customer and task analyst looking into the requirement that a task should follow in order to fulfill the particular job that customer has in mind. Task analyst typically investigate, a wide range of factors, as described by Hacos and Radish [7]:  ? Personal, social, and cultural characteristics of users,  ? User preferences and values, ? Goals ( general & specific ) and how users achieve  them, ? User knowledge, experience and taught processes, ? Physical environment, ? Task to be performed with the system, ? Problems users would like to see the system to  solve, and ? Other tasks that must be performed while using the  system.

Task analyst might consider other considerations. For  example, user may not completely be aware of all constraints, environment, history, and data and so forth [8].

In a survey studies of information visualization systems by authors, found four thematic areas of evaluation:  1: Controlled experiments comparing design elements.

The studies in this category might compare specific widgets (e.g. comparing alphaslider designs [9]) or compare mappings of information to graphical display [10].

2: Usability evaluation of a tool. Those studies might provide feedback on the problems users encountered with a tool and show how designers went on to refine the design [11, 12].

3: Controlled experiments comparing two or more tools. This is a common type of study. For Example, They compared three tree visualization tools:  ? Space Tree ? Hyperbolic ? Window Explorer [9].

Those studies usually try to compare a novel technique with the state of the art.

4: Case studies of tools in realistic settings. This is the least common type of studies, [13]. The advantage of case studies is that they report on users in their natural environment doing real tasks, demonstrating feasibility and in-context usefulness. The disadvantage is that they are time consuming to conduct, and results may not be replicable and generalizable.

Geoffery Moore in his books, [14] and [15], explains the role of information visualization researchers to provide convincing evidence of utility, which is difficult for any new technology, but presents specific challenges for information visualization. Furthermore, he argue that  in matching tools with users  tasks and real problems, researchers should consider investing some resources to produce tailored implementations, and establish partnerships that allow them to publish in journals and magazines of the chosen application domain. In:   6. Improving user testing  Usability testing and controlled experiments remain the backbone of evaluation. The reports of such laboratory studies help potential adopters understand the potential and limitations of the tools. Furthermore, they also guide researchers trying to improve their tools.  Therefore, in our evaluation, we should look at all aspects of empirical evaluation. [16], confirms this fact by saying that experiments usually include locate and identify tasks, but that tasks requiring users to compare, associate, distinguish, rank, cluster, correlate or generalize are rarely covered.

Another point worth mentioning is comparative studies usually state overall performance for a combined set of tasks, while experience pin point toward case study is more preferable. This point has been studied and reported by [17].

The composition of a set of tasks always can favor one tool over another while measuring over all performance and therefore, introducing a bias.

In reviewing another aspect of visualization, some characteristic of information visualization is more important than others in some problem, for example, user will look at the same data in different perspectives and over a long time.

Thus, Looking at the same data from different perspectives, over a long time could be a challenge, too.

Most probably, studying and investigating the data from different point of view or prospective will give you more insight and vision about the job and therefore, help overall understanding and evaluation or Answering questions you didn?t know you had.

[17], reports that tasks used during laboratory user studies need to be simple enough to be accomplished in a short- or at least predictable ? amount of time and specific enough for performance to be measured.

Addressing universal usability:  Researchers work with high-end computers but they need  to address the range of devices and network speeds available in people?s home and businesses. Special algorithms are needed to guarantee rapid downloads and adequate interactions [18].

Cognitive Support in Information Visualization:  Artifact can do aid thinking. Cognitive support can be loosely defined as the assistance offered by an artifact for a user to think and solve problems [3].

As mentioned before, visualization system emphasis on user?s ability to unpack the representation of data of interest and operate on them. Therefore, cognitive support can be translated as a very important issue.

Cognitive support within tools can potentially be systematically designed, analyzed, compared, verified, and measured. Design ideas for implementing different classes of cognitive supports may some day be systematically collected into design cookbooks. In short, the cognitive support theories provide hope that we can begin to transform the product of helpful tools from a process dominated by intuition and guesswork into an engineering process guided by systematic analysis, theory application, validation, and measurement. Thus, what we need should not be collection of primitives and models, instead a collection that integrated and packaged in a way that exposes the way of supporting cognition by reengineering it.

7. Challenges with Controlled studies Controlled studies are one of the important aspects of the  visualization evaluation. They facilitate solid scientific basis to formulate theories of visualization characteristic and comparing alternative solutions. Nonetheless, for practical limitation reasons:  ? It is very hard to design a study whose result are general enough to be used in other context,  ? The statistical techniques needed to draw correct figures are often difficult to handle and  ? Prone to mistake.

Therefore, applicability is lacking full authority [19].

[20] ,reported that some of the benefits from information  visualization systems comes from deeper insight gained from much more involved exploratory tasks, but such insight tasks are extremely difficult to formulate and thus test. He explained that the three main kinds of testing methods are  ? Formative tests, where the users are asked to verbalize their thought while using interface.

? Summative tests, involve the collection of bottom-line measurement data, such as task completion time, or number of clicks. The classical kind of summative test is the formal experiment.

? Usage tests involve observation and/or recording of users over a longer period of time working with an interface, as discussed in a workshop at CHI 2005.

At the final, he suggests a few steps to move forward:          ? Looking for experienced analysts to act as test users, rather than computer science students.

? Providing extensive training in each of the infovis techniques under trail, to counter any biasing from prior experience.

? Formulating more involved tasks which more accurately reflect the kind of exploration and analysis tasks for which infovis systems excel.

? Automating the running of formal experiments as far as possible is a step toward making comparative evaluations more feasible.

[20], has defined an analytic task taxonomy and presented  with ten primitive analysis task types, representative of the kinds of specific questions that a person may ask when working with a data set. They grouped similar questions and iteratively refined the groups according to what they believed to be the essence of questions in each group. The ten tasks from the affinity diagramming analysis are:  ? Relative Value: Given a set of specific cases, find attributes of those cases  ? Filter: Given some concrete conditions on attribute values, find data cases satisfying those conditions.

? Compute Derived Value: Given a set of data cases, compute an aggregate numeric representation of those data cases.

? Find Extremism:  Find data cases possessing an extreme value of an attribute over its range within the data set.

? Sort: Given a set of data cases, rank them according to some ordinal metric.

? Determine Range: Given a set of data cases and an attribute of interest, find the span of values within the set.

? Characterize Distribution: Given a set of data cases and a quantitative attribute of interest, characterize the distribution of that attribute?s values over the set.

? Find Anomalies: Identify anomalies within a given set of data cases with respect to a given relationship or expectation, e.g. statistical outliers.

? Cluster: Given a set of data cases, find clusters of similar attribute values.

? Correlate: Given a set of data cases and two attributes, determine useful relationships between the values of those attributes.

[22], explains the most challenging, 10 problems in  information Visualization: ? Usability ? Understanding elementary perceptual-Cognitive  tasks ? Prior Knowledge  ? Education and training ? Intrinsic quality measures ? Scalability ? Aesthetics ? Paradigm shift from structures to dynamics ? Causality, visual information, and predictions ? Knowledge domain visualization   8. Conclusion:  In this review and study, I found out that there is strong need for cognitive support idea. Big factor of inability of evaluating and designs in Infovis, stems from inability to systematically explore,  cognitive support theories and exploiting perception which have not been considered, yet.

In evaluating tools, we should look at a level higher than details and features of evaluating tools. We have to simplify and collect available tools as well as comprehensive studies in the area.

Much of the current methodologies for designing visualization tools and interfaces are informal and seldom cognitive supported. Furthermore, in many visualization research programs rigorous evaluation is lacking, and so it would be hard to choose promising ideas to promote future advancement.

We still are quite far away from understanding how our research tools become products and thus collecting research success in the area should help teams for further progress.

9.  References [1] Editors Charles D. Hansen, Christopher R. Johnson (  2005) the visualization Handbook Elsevier Academic press.

[2] Robert, Amar, John Stasko, knowledge Task-Based Framework for Design and Evaluation of Information Visualization. IEEE Symposium on Information Visualization 2004  [3] Andrew Walenstein, dissertation, SIMON FRASER UNIVERSITY (2002).

[4] An empirical comparison of three commercial visualization systems, Proceedings of InfVis 2001, IEEE symposium on Information Visualization, San Deigo, CA 123-130.

[5]  Wehrend, S. and Lewis, C. (1990) A problem- oriented classification of visualization techniques.

In Proceedings of InfoVis 1990, IEEE, pp. 139-143.

[6]   W. Cleveland. The elements of graphing data.

Monterey,  Cal. Wadsworth, 1985 [7]    J.T. Hacos and J.C. Redish, User and Task Analysis for Interface Desin, pp 7-8. Tonanto: John Wiley & Sons, 1998.

[8]   [4] Komlodi, A., Sears, A., Stanziola, E., Information Visualization Evaluation Review, ISRC Tech. Report, Dept.

of Information Systems, UMBC. UMBC-ISRC-2004-1 http://www.research.umbc.edu/~komlodi/IV_eval (2004).

[9]    Ahlberg, C., Shneiderman, B... The alphaslider: a compact and rapid selector, Proc. of the SIGCHI conference on Human factors in computing systems (1994) 365 ? 371.

[10]   Irani, P. , Ware, C., Diagramming information structures using 3D perceptual primitives, ACM Transactions on Computer-Human Interaction, 10, 1 (2003), 1-19.

[11]  Sutcliffe, A. G., Ennis, M., Hu, J., Evaluating the effectiveness of visual user interfaces for information retrieval. International Journal of Human Computer Studies, 53, 5 (2000), 741-763.

[12]     Byrd, D. (1999). A scrollbar-based visualization for document navigation. Proc. of the fourth ACM Conference on Digital Libraries (1999) 122-129.

[13]    Trafton, J., Tsui, T., Miyamoto, R.; Ballas, J., Raymond, P.,Turning pictures into numbers: extracting and generating information from complex visualizations. International Journal of Human Computer Studies, 53, 5 (2000), 827-850.

[14]    Moore, G., Crossing the Chasm, Harpers Collins Publishers, New York, NY (1991).

[15]    Moore, G., Inside the Tornado, Harpers Collins Publishers, New York, NY (1995).

[16]   Komlodi, A., Sears, A., Stanziola, E., Information Visualization Evaluation Review, ISRC tech. Report dept. of information Systems,  UMBC. UMBC-ISRC-2004-1 htpp://www.research.umbc.edu/~komlodi/IV_eval (2004) [17]    plaisant, C., Grosjean, J., and Bederson, B. B. Space Tree:Supporting exploration in large node-link tree: Design evaluation and empirical evaluation  IEEE Symposium on Information Visualization  (2002),57-64.

[18]   Zhao, H., Shneiderman, B., Image-based highly interactive web mapping for geo-referenced data publishing, tech Report CS-TR-4431. (2003) www.cs.umd.edu/hcil.

[19]   Report on BELIV?06 workshop: Enrico Bertini, C., Plaisant, G., Santucci, Beyond time and errors: novel evaluation methods for information Visualization, May 23 2006.

[20]   Report on BELIV?06 workshop: Keith Andrews, Evaluation information Visualization, May 23 2006.

[21] Amar, R., Eagan, J., and Stasko J., Low-level Components of Analytic Activity in Information Visualization. In IEEE Symposium on Information Visualization (Minneapolis, MN, USA, 2005).

[22] Chen C., Top 10 unsolved Information Visualization problems. IEEE Computer Graphics and Applications.25(4).

