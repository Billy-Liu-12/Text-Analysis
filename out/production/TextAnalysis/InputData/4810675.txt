Discovering Informative Association Rules for  Associative Classification

Abstract?The application of association rule mining to classification has led to a new family of classifiers which are often referred to as Associative Classifiers (ACs). An advantage of ACs is that they are rule-based and thus lend themselves to an easier interpretation. However, it is common knowledge that association rule mining typically yields a sheer number of rules defeating the purpose of a human readable model. Hence, selecting and ranking a small subset of high-quality rules without jeopardizing the classification accuracy is paramount but very challenging. In this paper, Entropy-AC, a new associative classifier based on entropy, is proposed. Information gain and informative rules are defined at first. Then, the algorithm for constructing associative classifier based on informative rules is presented. Experimental results show the proposed associative classifier is effective.

Keywords-data mining; associative classifier; entropy; informative association rule

I. INTRODUCTION (HEADING 1) A new classification approach known as associative  classification integrates association mining and classification into a single system [1, 2]. Association mining, or pattern discovery, aims to discover descriptive knowledge from databases, while classification focuses on building a classification model for categorizing new data. Both association pattern discovery and classification rule mining are essential to practical data mining applications. If these two relevant jobs can be somehow integrated, great savings and conveniences to the user could result. Hence, considerable efforts have been made to integrate these two techniques into one system.

CBA is the first algorithms to use an association rule mining approach for classification [3]. CBA implements the famous Apriori algorithm [4] in order to discover frequent itemsets. Once the discovery of frequent itemsets is finished, CBA proceeds by converting any frequent itemsets that passes minimum confidence into a rule. In so doing, only one subset of the generated classification rules will be considered in the final classifier. Evaluating all the generated rules against the training data set does the selection of the subset. The ranking technique employed by the CBA considers principally confidence and support to order rules, and when two rules have identical support and confidence, the choice is based on that generated earlier. This means that the CBA selects rules with  lower antecedent cardinality first as it employs the Apriori step-wise algorithm.

Liu et al. improves CBA algorithm by using multiple supports and extracting long patterns [5]. These improvements resulted in a multiple local supports technique for the discovery of frequent itemsets for not only the dominant classes but also for the low frequency ones. In order to extract long patterns from data, a hybrid algorithm has also been presented. The basic idea of the hybrid algorithm is to segment the training data, and to apply the CBA algorithm on each segment. Finally, the classifier that achieves the highest accuracy on one of the segments is selected to classify new unseen data objects.

CMAR is an AC algorithm that selects and analyses the correlation between high confidence rules, instead of relying on a single rule [6]. It uses a set of related rules to make a prediction by evaluating the correlation among them. The correlation measures how effective the rules are based on their support values and class distribution. In addition, a new prefix tree data structure named a CR-tree is used to handle the set of rules generated and to speed up the rule retrieval process.

A new approach for building classification systems based on both positive and negative rules has been introduced in [7].

The interestingness of the rules for the proposed algorithm is based on the correlation coefficient that measures the strength of the linear relationship between a pair of variables. Besides confidence and support thresholds, correlation coefficient has been used for pruning the final classifier, giving a much reduced rules set if compared with support and confidence pruning methods. The algorithm in [7] finds the frequent itemsets using Apriori approach and ranks the rules using CBA rules ranking method.

The live-and-let-live (L3) [8] algorithm uses a lazy pruning approach to build the classifier. Once the classifier is constructed, its predictive power is then evaluated on test data objects to forecast their class labels.

Although with several advantages, such as high accuracy, easily understandable, the problems with associative classifiers are also remarkable. For example, association rule mining generates a sheer number of rules commonly outnumbering the observations in the training set. This defeats the purpose of readability of the classification model since no human would be willing to sift through hundreds of thousands of rules for     editing purposes. This leads to two other issues: How can we reduce the number of rules in the model and how can we effectively select rules to apply during classification?

In this paper, we address these issues by incorporating the concept of entropy [9] into the framework of associative classification. Specifically, information gain is defined for pruning and ranking association rules. The proposed parameter is calculated directly from the parameters of association rules (support and confidence), rather than totally from the original data. This problem is challenging because the goal is to prune and rank rules while preventing the accuracy of the classifier from dipping. Experimental results show the proposed associative classifier is effective.

The remaining of the paper is organized as follows. In Section 2, we introduce the general aspects of the associative classification. In Section 3, we define the information gain of informative rules, and propose the algorithm for discovering informative rules. Experimental results are presented in Section 4, and we conclude our works in Section 5.



II. ASSOCIATIVE CLASSIFICATION A typical associative classification system is constructed in  two stages: 1) discovering all the event association rules (in which the frequency of occurrences is significant according to some tests); 2) generating classification rules from the association patterns to build a classifier. In the first stage, the learning target is to discover the association rules inherent in a database. In the second stage, the task is to select a set of relevant association rules discovered to construct a classifier given the predicting attribute.

A. Association rule mining  An association rule is a kind of co-occurrence information on items [4]. Let I={i1, i2, ?, iM} be a finite set of items and D be a dataset containing N transactions, where each transaction  D is a list of distinct items. A set X?I is called an itemset.

The support of an itemset X, denoted as sup(X), is the percentage of transactions in D containing X. That is  ?  sup(X)=|X(t)|/|D| ?????????????????????????????????????  where X(t)={t?D| X?t}.

Let min_sup be the threshold minimum support value specified by user. If sup(X)?min_sup, itemset X is called a frequent itemset.

An association rule is an implication of the form X	Y, where X I, Y I and X?Y=?. Besides the above-mentioned support, the rule X	Y has confidence, denoted by conf(X	Y), is the percentage of transactions in D containing A which also contain B, i.e.

conf(X	Y)=sup(XY)/ sup(X)                     (2)  Rules that satisfy both a minimum support threshold (min_sup) and a minimum confidence threshold (min_conf) are called strong rules. The problem of association rule mining is to discover all strong rules.

B. Associative classifier  Consider the association rule in the view of a classification rule. Let A={A1, A2, ?, An} be a set of attribute domains, and a data object obj=(a1, a2, ?, an) be a sequence of attribute values, i.e. aj?Aj, 1?j?n. Given an itemset P=ai1ai2?aik where aij?Aij for 1?j?k and ij?ij? for j?j?, a data object obj is said to match P if and only if, for 1?j?k, obj has value aij in attribute Aij.

Definition 1 (Associative classifier). Let C={c1, c2, ?, cm} be a set of class labels. An associative classifier is the mapping R from the set of attribute values to a set of class labels  R: (A1, A2; . . . ; An)	C                         (3)  According to Eq. (3), given a test datum obj=(a1, a2, ?, an), the associative classifier returns a class label c?C.

Let an itemset variable be P and a class variable c. If we rewrite the rule in the form of  R: P	c and have a training set T={(Pi, ci)}, then the learning process is to induce the rule set  for which the element has the sup(P	c)?min_sup and conf(P	c)?min_conf. The procedure of associative classification rule mining is not much different from that of general association rule mining.

Now that we have a classification system, it requires a decision on which class to assign a new data object. First, we search for the rules in which the pattern matches the object.

Next, from these rules, we perform a prediction based on some predefined decision criterion.



III. ASSOCIATIVE CLASSIFICATION BASED ON INFORMATIVE RULES  There are two important issues related to AC methods:  On the one hand, as stated in [3, 5, 6], associative classifiers generate an overwhelming number of classification rules and it is very important to prune the rules to make the classifier effective and more efficient. We argue that pruning is also very important in order to allow domain experts to tune a classifier by editing rules if necessary.

On the other hand, rule sorting before building the classifier plays an important role in the classification process because the majority of AC algorithms utilize rule ranking procedures as the basis for selecting the classifier during pruning. In particular, the CBA [3] and CMAR [6] algorithms, for example, use database coverage pruning to build their classifiers, where this pruning tests rules according to the rule    ranking procedure. Therefore, the highest-order rules are generally evaluated first and are then inserted into the classifier and later used for predicting test data objects.

To deal with the above-mentioned two issues, we incorporate the entropy into the framework of AC. Information gain is defined to prune and rank association rules.

A. Informative association rules  Entropy is a measure commonly used in information theory [9]. It is incorporated into decision tree to characterize the impurity of an arbitrary collection of examples. Similar to information gain [10] used in decision tree, we can also define the following information gain for an association rule R: P	c:  1) Firstly, the information of database D is:  GD=  | | | | log  | | | |  m i  i  c c  D D? ?? i                            (4)  where |ci| (1?i?m)is the number of data objects whose class label is ci?C.

2) Then, we consider the data objects that match P, the precedent of R.

The number of objects that match P is:  N1= ( )  | | ( )  sup R D  conf R (5)  In these N1 objects, the number of data objects with class label c is:  N11=|D|*sup(R)                                 (6)  Meanwhile, in these N1 objects, the number of data objects with class label different from c is:  N12=N1?N11                                     (7)  Thus, the information of objects that match P is:  GP= 1 11 11 12 12  1 1 1 1  log log | |  N N N N N  D N N N N ? ? ? ??  ? ??  (8)  3) Finally, we consider the data objects that do not match P.

The number of objects that do not match P is:  N2=|D|?N1                                      (9)  Thus, the information of objects that do not match P is:   1 2 2  | | log  | |  | |m P  i  i ic cNG D N N?  ? ? ?? ? ?? ? ?                      (10)  Hence, the information gain of the R: P	c is:  Gain(R)=GD? GP? PG                        (11)  Definition 2. Rule R: P	c is called an informative rule if sup(R)?min_sup, conf(P	c)?min_conf, and Gain(R)>0.

Note that, the information gain is calculated directly from the parameters of association rules (support and confidence), rather than totally from the original data. That is, when all frequent itemsets are discovered, their supports can be reused for calculating confidence and information gain of informative rules. Thus, the computation of information gain is efficient.

B. AC based on informative association rules  In this subsection, we present the algorithm for constructing AC based on informative association rules.

Algorithm 1. Entropy-AC: Discover informative classification association rules  Input: dataset D, min_sup, min_conf  Output:  associative classifier AC  1) Discover all frequent itemsets that contain class attribute c;  2) for each frequent itemset fp do  3)     for each non-empty itemset sp fp and sp?c do  4)         if (conf(sp	c)?min_conf) and (Gain(sp	c)>0) then  5)                      AC?(sp	c);  6) Sort all rules in AC in information gain descending order.

In Algorithm 1, frequent itemsets whose supports are no lower than min_sup is discovered at first (Step 1). Since these itemsets are used for generating classification rules, those itemsets that do not contain class attribute will be ignored. By incorporating the constraint of containing class label, we use the recently proposed Index-BitTableFI algorithm [11] to discover frequent itemset. The informative association rules are discovered by the main loop (Steps 2-5). These rules should satisfy the following three conditions: 1) The class attribute c appear in the consequence part of the rules; 2) The confidence of the rules pass the minimum threshold min_conf ; 3) The information gain of the rule is positive. In Step 6, classification    rules are sorted in information gain descending order. That is, for two strong rules R1 and R2, if Gain(R1)>Gain(R2), then R1 has a higher rank than R2 in the associative classifier.



IV. EXPERIMENTAL RESULTS  We compare the performance of our algorithm Entropy- AC with CBA [3] and CMAR [6]. We used datasets from the UCI ML repository [12] and the performance of CBA and CMAR are from their respective authors? papers. We used a 10 fold cross-validation method for each dataset and what is reported are averages. Table 1 shows the comparative results of accuracy on five datasets namely Breast, Diabetes, Iris, Led7, and Pima.

TABLE I. COMPARISON OF CBA, CMAR, AND ENTROPY-AC  CBA CMAR Entropy- AC Breast 96.3 96.4 95.3 Diabetes 74.5 75.8 80.1 Iris 94.7 94.0 95.3 Led7 71.9 72.5 72.1 Pima 72.9 75.1 78.6 Average 82.06 82.76 84.28  Comparing the results in Table 1, we observe that Entropy- AC can achieve a better accuracy than CBA and CMAR on most experiments. This is because Entropy-AC exploits a small but informative rule set for classification. On the contrary, both CBA and CMAR consider all the strong rules.



V. CONCLUSIONS AND FUTURE WORK  Associative classification is becoming a common approach in classification since it extracts very competitive classifiers with regards to prediction accuracy if compared with rule induction, probabilistic and decision tree approaches.

However, challenges such as the exponential growth of rules, and rule ranking need more consideration. To solve these two problems, a new associative classifier Entropy-AC is proposed.

Similar to decision tree, the information gain for association rule is also defined. Then, the information gain is used for pruning and ranking informative rules. Experimental results show that the proposed method is effective.

Although informative rules can build a more accurate associative classifier, how to discover informative rules more  efficiently is still a challenging work. Thus, exploring more succinct data structure and more efficient pruning strategies are our future work.

