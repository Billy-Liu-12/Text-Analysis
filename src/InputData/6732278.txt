

Abstract?One of the main challenges of using cutting edge medical imaging applications in the clinical setting is the large amount of data processing required. Many of these applications are based on linear algebra computations operating on large data sizes and their execution may require days in a standard CPU. Distributed heterogeneous systems are capable of improving the performance of applications by using the right computation- to-hardware mapping. To achieve high performance, hardware platforms are chosen to satisfy the needs of each computation with corresponding architectural features such as clock speed, number of parallel computational units, and memory bandwidth.

In this paper we evaluate the performance benefits of using different hardware platforms to accelerate the execution of a transmural electrophysiological imaging algorithm, targeting a standard CPU with GPU and FPGA accelerators. Using this cutting edge medical imaging application as a case study, we demonstrate the importance of making intelligent computation assignments for improved performance. We show that, depending on the size of the data structures the application works with, the usage of an FPGA to run certain computations can make a big difference: a heterogeneous system with all three hardware platforms (CPU+GPU+FPGA) can cut the execution time by half, compared to the best result using one single accelerator (CPU+GPU). In addition, our experimental results show that combining CPU, GPU, and FPGA platforms in a single system achieves a speedup of up to 62x, 2x, and 1605x compared to systems with a single CPU, GPU, or FPGA platform respectively.



I. INTRODUCTION  In the past, invasive techniques for transmural electrophys- iological imaging required cardiac catheterization to measure electrical activity directly by touching a sensor to the interior walls of the heart. The large risk associated with this pro- cedure and the data inaccuracies caused by measuring elec- trical activity at different positions throughout many cardiac cycles drove researchers to find better techniques. Noninvasive transmural electrophysiological imaging (NTEPI) uses body surface potential electrical readings combined with anatomical data from MR imaging to model the electrical activity not only on heart surfaces but also deep into the 3D myocardium of the ventricles with better accuracy and less risk to the patient. Despite the initial success, the clinical translation of NTEPI is hindered by the tremendous computational cost of the algorithm.

This compute-intensive application, when implemented in a standard CPU, does not achieve a sufficient level of perfor- mance for its required purpose. Previous research by Camara  et al. [1] reported an average computational time of 250 hours on a regular desktop computer. This is not an acceptable execution time for a critical diagnostic tool used in the clinical environment. To make use of this new diagnostic technique in the clinical domain will require alternative hardware solutions such as GPU or FPGA accelerated implementations to oper- ate within an available time budget. Heterogeneous systems present a solution to this problem by combining different ar- chitectures such as CPU, GPU, and FPGA into a single system to provide best performance. Through proper computation-to- hardware assignments, the NTEPI application can achieve the performance required for the clinical environment. However, in order to take full advantage of heterogeneous systems some major challenges must be overcome. These include choosing the best computation-to-hardware assignments and choosing which hardware platforms to include in the system.

The core of the NTEPI application is composed of linear algebra computations such as dot product, matrix-vector mul- tiplication, matrix-matrix multiplication, matrix inverse, and matrix decomposition. In this paper we analyze three of these linear algebra computations on CPU, GPU, and FPGA archi- tectures with multiple implementations. These linear algebra computations are commonly used in many other compute- intensive applications in the medical field and other relevant fields, and our conclusions can be applied to those as well. Out of the literature [2][3][4][5], successful FPGA architectures for these computations are selected. Multiple implementations for the CPU [6][7] and GPU architectures [8][9][7] from commonly used scientific libraries are used for comparison.

These results can be used in systems where one or more of these architectures are available to assist in selecting the best implementation for any computation using a particular matrix size.

The main contributions of this work are:  ? Decomposition of the NTEPI application and identification of the bottleneck computations.

? Demonstration of the impact of computation-to- hardware assignments on performance of this medical imaging application.

? Evaluation of the performance benefits of these assignments, including data transfer costs, across several systems with various combinations of hardware platforms.

Fig. 1: The steps required for each NTEPI iteration.



II. RELATED WORK  Initial steps have been taken to parallelize whole heart EP simulation using complex, ionic models by Sato et al. [10] and Bartocci et al. [11]. However few efforts have reported on GPU acceleration of noninvasive EP imaging except for the work by Corraine et al. [12] that presented a 16x speedup compared to a high end CPU. This is the first work to analyze the performance of noninvasive EP imaging on a heterogeneous system containing CPUs, GPUs, and FPGAs.

Performance of various processing architectures have been evaluated for many computations. CPU, GPU, and FPGA implementations of a Low-Density Parity-Check decoder were compared by Falcao et al. [13] for data sizes 8000x4000 and 1024x512. They concluded that for the smaller data size the FPGA was faster and the GPU was faster at the larger data size. Sotiropoulos et al. designed an FPGA matrix-matrix multiplication architecture [3] and compared its performance to a standard CPU implementation. This comparison was only for specifically sized matrices and did not discuss their CPU im- plementation. The results showed that the FPGA outperforms the CPU with a speedup of up to 557x. A comparison of matrix decomposition by Yang et al. [14] evaluated the performance on CPUs, GPUs, and FPGAs. They analyzed four data sizes from 256 to 1024 and demonstrated that the FPGA was faster than GPU followed by the CPU for both single and double precision floating point. Compared to the previous works that contrasted the performance of each architecture against each other, in this work we combine their capabilities into a single system to achieve best performance.

Higher level functions such as 2D filtering by Llamocca et al. [15] and an implementation of Bayesian networks by Fletcher et al. [16] were evaluated using GPU and FPGA architectures. Grozea et al. evaluated a sorting algorithm on CPUs, GPUs, and FPGAs [17] to speed up the performance of network intrusion detection systems. Their results showed the highest performance architecture was CPU, followed by FPGA and then GPU. But when making the comparisons, the authors implemented the algorithms solely in one architecture and therefore, chose one particular processor over another. They did not discuss the best implementations for the computations in these higher level functions, only the best implementation for the whole algorithm.



III. NTEPI ALGORITHM  The NTEPI Algorithm employs a sequential maximum a posteriori (MAP) estimation of the transmural action potential (electrical propagation) distributions uk given the body-surface potential data (as measured from a standard ECG) from all samples up to the current sample k, denoted as ?1:k [18]. At each time step when a new sample is available, the computational steps shown in Figure 1 are executed. A typical patient analysis requires 2000-3000 iterations.

In each iteration, a Cholesky decomposition of the covari- ance matrix of uk?1 is performed. Then, a set of sample vectors U||?? are generated from the mean and covariance matrix of uk?1. Each sample vector in U||?? individually enters into simulation of the Alive-Panfilov models to predict a new set of sample vectors U?|| . From this, the mean and covariance matrix of u?k are predicted.

Since the sample ECG measurements ?k contain electrical noise from sources other than the heart such as the respiratory muscles that are located between the electrodes and the heart, a Kalman filter is used to reduce the impact of random noise from the data. The Kalman update process requires inverting an MxM matrix where M is the dimension of the body surface data ?k. Each iteration repeats the above prediction and update processes.

The computations required for the above calculations are common matrix operations such as addition, subtraction, element-wise and standard multiplication, scaling, inversion, and Cholesky decomposition. Previous work profiled this algo- rithm in detail [12] and found that the majority of the execution time is spent on the Alieve-Panfilov model. Moreover, we further investigated which specific computations are the bottle- neck and found that 98% of the time is spent on matrix-matrix multiplication, matrix inversion, and Cholesky decomposition.

As such, this work attempts to improve the overall NTEPI algorithm?s performance by focusing on these three types of computations. The number of each of these computations and the dependencies between them for each NTEPI iteration are shown in Figure 2.

Fig. 2: Dataflow Graph (DFG) of the bottleneck computations, matrix-matrix multiply (MM), Cholesky decomposition (Chol), and matrix inverse (Inv), in each iteration of the NTEPI algorithm.

Each iteration requires 12 computations broken down into 1 Cholesky decomposition, 1 matrix inversion, and 10 matrix- matrix multiplications. Between iterations there is no opportu- nity for overlap as every iteration is dependent on the previous update calculations. However, there is sufficient parallelism within each iteration to potentially keep all three CPU, GPU, and FPGA processors busy. In the next section we evaluate the performance of the three computation types and the data transfer costs to determine a schedule that improves the overall performance.



IV. RESULTS  To run the NTEPI algorithm in a heterogeneous system, each computation will need to be assigned to a hardware platform to be executed. As stated in [19], performance of a computation depends on the number of operations, the memory bandwidth requirements, control flow complexity, and data size. The NTEPI algorithm operates on matrix sizes that range from 500x500 to 8000x8000, depending on the size of the mesh used to represent the heart. The sample use case that we originally evaluated operated on a data size of 836x836. Larger data sizes allow for more precise electrical activity modeling but also dramatically increase the execution time. Additionally, double precision (DP) floating point is required in order to store the small action potentials and intermediary values used during calculation. However, we also investigate the potential performance improvement of moving to single precision (SP) floating point.

The performance of each computation and data size was evaluated in CPU, GPU, and FPGA hardware platforms with specifications as shown in Table I. For each computation in the FPGA, a well researched custom design was used without modification [2][3][4][5] in a Virtex 6 and Virtex 7 device, although we only show results for Virtex 7 as will be explained later. For the CPU [6][7] and GPU [8][9][7], high performance implementations that are commonly used in scientific comput- ing were used that stem from the original BLAS and LAPACK libraries. The AMD C Math Library (ACML) and MathWorks Matlab were used to implement computations in the CPU.

Parallel versions of these libraries are also available for GPUs in the form of Compute Unified BLAS (CUBLAS), Matrix Algebra on GPU and Multicore Architectures (MAGMA), and Matlab. Figure 3 shows the performance of the three computations for the data sizes evaluated in this work.

TABLE I: Hardware Platform Specifications  C P  U HW Platform Intel Core i7 2600 3.4GHz 16GB DDR3 @1333MHz  Implementations AMD C Math Library 5.1.0MathWorks Matlab 2012b 64b  G P  U HW Platform Nvidia Tesla K20 706MHz  5GB GDDR5 @5.2GHz  Implementations CUBLAS + MAGMA LibrariesMathWorks Matlab 2012b 64b  F P  G A HW Platforms  Xilinx Virtex 6 LX240T, ML605 512MB DDR3 @ 400MHz  Xilinx Virtex 7 VX485T, VC707 1GB DDR3 @ 800MHz  Implementations [2][3][4][5]  Fig. 3: Performance of matrix-matrix multiplication, Cholesky decom- position, and matrix inversion on CPU, GPU, and FPGA platforms for both single precision (SP) and double precision (DP) floating point.

(a) CPU+GPU system. (b) CPU+FPGA system. (c) GPU+FPGA system. (d) CPU+GPU+FPGA system.

Fig. 4: Schedules of a single iteration of the NTEPI algorithm on two and three-platform systems. The matrix inverse computation (Inv) can be placed in either CPU, GPU, or FPGA to achieve best performance depending on data size and precision. Whereas the Cholesky (Chol) and matrix-matrix multiply (MM) computations always perform best in FPGA and GPU platforms respectively. The CPU+GPU system?s schedule may be longer than the others since for some data sizes the Cholesky decomposition?s second best platform is the GPU.

Although multiple implementations were used for each hardware platform, only one showed the best results for each platform across our range of data sizes and precision. For the CPU, the Matlab implementation performed better than all other platforms and implementations for double precision ma- trix inversion at data size 500x500. The Matlab implementation in the GPU was also better than CUBLAS (for matrix-matrix multiplication) and MAGMA (for matrix inverse). However the FPGA?s ability to execute the complex control flow in Cholesky decomposition using custom parallel pipelines was better than all other platforms. Since the Cholesky decompo- sition computation implemented in the FPGA only required one operand per pipeline the design never utilized the entire memory bandwidth of the Virtex 6 device. Moreover, the same number of pipelines were implemented in both devices so the  Fig. 5: Design space for CPU, GPU, and FPGA hardware platforms for the three computations evaluated in this work across a range of data sizes using single precision (SP) and double precision (DP).

additional bandwidth of the Virtex 7 device was of no benefit, so both devices performed the same. Similarly, the matrix inversion design only required a single operand regardless of the size of the pipeline implemented and as a result both FPGA devices performed the same for this computation.

The design space chart in Figure 5 shows the best computation-to-hardware mappings for various data sizes. This chart was derived from the performances shown in the plots in Figure 3 and is based only on the execution times of each computation. Data transfer times also affect these mappings, next we define each platform?s communication interfaces and  Fig. 6: Distributed CPU, GPU, FPGA system showing each platform?s full duplex PCIe connections to both other platforms.

their connections within the system. Figure 6 shows the connections between each commercial-off-the-shelf (COTS) hardware platform as PCI Express interfaces with the specified number of lanes. The GPU?s PCIe bandwidth to the CPU using 16 lanes is assumed to be 8GBps, and the FPGA?s bandwith is assumed to be 2GBps using 4 lanes. We clearly define the CPU and its chipset as separate chips since the PCIe root complex within the chipset can act as a switch and route transactions between the various devices without interaction from the CPU.

Bittner et al. [20] presented a technique to enable direct GPU to FPGA communication. We assume that this functionality is enabled within this system and that the data transfer between the GPU and FPGA happens at the FPGA?s bandwidth and without any CPU interaction. These bandwidths along with the data size, determine the data transfer costs that are included in the scheduling decisions to choose which platform to assign a computation. The goal of assigning computations to different platforms is that the difference in execution time is larger than the associated data transfer time, leading to an overall decrease in execution time.

For our three computations, matrix-matrix multiplication will always be performed best in the GPU, and Cholesky decomposition in the FPGA. However, depending on the data size and precision for matrix inverse the CPU, GPU, or FPGA may be the best as shown in Figures 3e-f. Figure 4 shows the best schedules for the heterogeneous systems: CPU+GPU, CPU+FPGA, GPU+FPGA, and CPU+GPU+FPGA. Each schedule has multiple possible assignments for matrix inverse and depending on the data size and precision, different plat- forms will be chosen to achieve the best performance. In a system with only two of the hardware platforms where the best platform for a particular computation is not in the system, the second or third best platform must be used. For the CPU+GPU schedule in Figure 4a, the Cholesky decomposition cannot be scheduled in the best processor (FPGA) since it is not in the system. Figures 3c-d show that the performance of Cholesky decomposition is second best on both CPU and GPU platforms depending on data size and precision. Figure 4c shows the schedule for the CPU+FPGA system. In this system, a GPU is not available and Figures 3a-b show that the CPU platform always performs better than the FPGA for matrix- matrix multiplication and will be used in lieu of the GPU. In the GPU+FPGA system, there will be only one case where a second best processor is used. For the 500x500 double precision case, the matrix inverse will be assigned to the second best processor (GPU) instead of the CPU.

A. Medical Imaging Use Case  As stated above, we tested seven different system con- figurations: systems with a single hardware platform (CPU, GPU, and FPGA), two hardware platforms (CPU+GPU, CPU+FPGA, and GPU+FPGA), and three hardware platforms (CPU+GPU+FPGA). As expected, the best performing one is the three-platform CPU+GPU+FPGA system and we present our results by comparing this one to the rest. For the single GPU and FPGA systems and GPU+FPGA system, we assume they are also accompanied by a CPU for control purposes only, to initiate computations and data transfers, no computing happens on the CPU. Since there is no opportunity for overlap  (a) SP speedup over CPU, FPGA, and CPU+FPGA.

(b) SP speedup over GPU, CPU+GPU, and GPU+FPGA.

(c) DP speedup over CPU, FPGA, and CPU+FPGA.

(d) DP speedup over GPU, CPU+GPU, and GPU+FPGA.

Fig. 7: Speedup of the heterogeneous CPU+GPU+FPGA system for a single iteration of the NTEPI algorithm compared to single and two-platform systems across a range of data sizes using both single precision (SP) and double precision (DP) floating point.

between iterations, the execution time for a single iteration can be used to estimate the execution time for an application with any number of iterations.

Figure 7 shows the speedup of the three-platform system against the other six systems for single and double precision across a range of data sizes. For the sake of visibility, the results were split into two groups: those systems with a GPU (GPU, CPU+GPU, GPU+FPGA) in Figures 7b & 7d, and those systems without a GPU (CPU, FPGA, CPU+FPGA) in Figures 7a & 7c. The reason for this distribution leads to our first conclusion, that those systems including a GPU show performance closest to the best performance achievable with the CPU+GPU+FPGA system. This is due to the large number of computations that are best performed in the GPU (all matrix-matrix multiplications and some matrix inversions) and the large difference in execution time of these on the other platforms. The GPU achieves such high performance compared to the other platforms as a natural consequence of its inherent high parallelism. Matrix-matrix multiplication is especially appropriate for GPU in this range of matrix?s sizes. The results would be very different if we were working with a lower number of matrix-matrix multiplications or smaller matrix sizes. These two factors would reduce the level of parallelism of the application and make the GPU hardware resources overkill for the computational needs of the application.

We can see that the closest performance to the three- platform system is achieved by the GPU+FPGA system, with speedup of 1x (that is, equal performance) for almost all cases.

This shows that the contribution of the of the CPU is minimal, if not null, and that the full load of the execution lays on the GPU and FPGA platforms. Only for the double precision GPU+FPGA plot in Figure 7d, size 500x500, we can see a small speedup of 1.03x of the three-platform system over the GPU+FPGA. This slight improvement of 3%, achieved by the addition of the CPU to the system, is due to the smaller execution time of the matrix inverse in the CPU, which works well for that matrix size. Given that the CPU is required in such a system to control the data transfers and initiate computations in the GPU and FPGA platforms, it is natural to also use it to further improve performance.

Continuing with this analysis of the three-platform sys- tem versus the other two-platform systems (CPU+GPU and CPU+FPGA) we analyze the third component?s impact on the performance when added (FPGA and GPU respectively). For the CPU+FPGA system in Figures 7a and 7c, we can see that the three-platform system achieves performance almost equal to the system with no GPU for small matrix sizes, and the GPU gains relevance as the matrix size increases. As stated above, to really exploit the potential of the GPU?s hardware, large amounts of parallelism in the computations are necessary, which increases as we approach the 6000x6000 mark. Here, the addition of the GPU cuts the execution time by 12x for double and 6x for single precision compared to the CPU+FPGA system. On the other hand, looking at Figures 7b and 7d we can evaluate the impact of adding the FPGA by analyzing the performance of the CPU+GPU system. The performance of the three-platform and CPU+GPU systems are very similar, leading us to believe that adding the FPGA does not make a big difference. However, this depends again on the size of the data that we work with. Depending on the size of the matrix, the addition of an FPGA for certain computations can improve the performance considerably. This is the case for the 500x500 data size, where the performance is almost cut in half thanks  to the addition of the FPGA, which achieves best performance when executing the matrix inverse computation for that data size.

Current implementations of the NTEPI algorithm regularly execute 2000-3000 iterations. Future implementations will need to execute millions of iterations to improve accuracy and usability of the algorithm in the clinical setting. Moreover, every potential speedup is needed for this to become feasible.

We found that in comparing the system with three platforms, the potential speedup from moving to single precision ranged from 1.3x to 2x with an average of 1.7x.

In summary we can conclude that, out of the three hardware platforms in the full heterogeneous system, the one with the most relevant impact on performance is the GPU, while the one with the smallest impact on performance is the CPU.

For any application, the ratio of certain computations and the data sizes they operate on are the key factors to select the right hardware platforms for a heterogeneous system. For our specific application, given that the smallest matrix size that we could consider is 500x500 and the high ratio of matrix- matrix multiplications, we found that the single most important component of a heterogeneous solution is the GPU due to the high level of parallelism of both the application and the hardware. We found that the FPGA can be a key addition to the system for small matrix sizes, cutting the execution time by up to half for a critical medical diagnosis application.



V. CONCLUSIONS  In this work we have evaluated the performance of the NTEPI medical imaging algorithm on CPU, GPU, and FPGA hardware platforms. Although many previous works have com- pared and contrasted CPUs, GPUs, and FPGAs to determine which architecture is better, we show that a single system containing all three architectures results in higher performance.

Using design space charts, computations were mapped to hardware platforms to improve their individual performance based on data size and precision. The large differences shown between the fastest and second fastest implementations are key to enabling heterogeneous systems to spend time on data transfers and still achieve higher performance than single ar- chitecture systems. Then, schedules for a single iteration of the algorithm were constructed for two and three platform systems to improve the performance of the bottleneck computations in the application.

We compared the performance of single CPU, GPU, or FPGA systems and two-platform systems (CPU+GPU, CPU+FPGA, and GPU+FPGA) against the three-platform CPU+GPU+FPGA system. Our results showed that in the three-platform system, the GPU had the highest contribution to the overall performance of the application, and the CPU had the lowest. After the GPU, adding the FPGA can cut the performance of the system by up to half for small matrix sizes. As expected, the single platform systems were the worst performing, but out of these the GPU performed the best. The three-platform system performed up to 12x better than the two-platform systems, with the GPU+FPGA system achieving equal performance for all but one data size.

Future work will use scheduling algorithms to automat- ically determine the best or optimal schedule statically or dynamically at run time for the number and type of hardware platforms in the system. Following this, a simulation of the application to estimate the performance in a heterogeneous system will be used to validate the assignments or identify places for improvement. In the future, we will use the results from this paper to design an automatic framework to convert an application from its initial software implementation to a faster implementation in a heterogeneous system.

