2013 IEEE INTERNATIONAL WORKSHOP ON MACHINE LEARNING FOR SIGNAL PROCESSING, SEPT. 22?25, 2013, SOUTHAMPTON, UK

ABSTRACT  It is well known that the accuracy of classifiers strongly de- pends on the distribution of the data. Consequently, a versa- tile classifier with a broad range of design parameters is bet- ter able to cope with various scenarios encountered in real- world applications. Kung [1] [2] [3] presented such a classi- fier named Ridge-SVM which incorporates the advantages of both Kernel Ridge Regression and Support Vector Machines by combining their regularization mechanisms for enhancing robustness. In this paper this novel classifier was tested on four different datasets and an optimal combination of param- eters was identified. Furthermore, the influence of the param- eter choice on the training time was quantified and methods to efficiently tune the parameters are presented. This prior knowledge about how each parameter influences the train- ing is especially important for big data applications where the training time becomes the bottleneck as well as for applica- tions in which the algorithm is regularly trained on new data.

Index Terms? Ridge-SVM, unified model for super- vised learning, training time, parameter tuning, weight-error- curve (WEC)  1. INTRODUCTION  In the case of supervised learning, a set of training examples of the form [X ,Y] = {[x1, y1], [x2, y2], [xN , yN ]} is given.

N denotes the number of training examples and xi ? RM are the feature vectors, where M is its dimensionality. yi ? {?1, 1} is the teacher for the training vector xi. The aim is to find a hyperplane which separates the points with yi = ?1 and yi = 1 with a maximum margin.

Such a hyperplane can be written as xTw ? b = 0. In the case in which the data is linearly separable one can define two hyperplanes which choose the largest possible margin to separate the data, the so-called separation margin. Writing these two hyperplanes as xTw ? b = ?1 and xTw ? b = 1, the margin can then be derived as 2||w|| so that the aim is to minimize ||w||. Given the test vector x, the optimal linear  discriminant function is then  f(x) = xTw + b. (1)  1.1. Kernel induced vector spaces  By restricting the decision vector w to the form  w =  N? i  xiai = Xa, where a ? [a1 . . . aN ]T , (2)  it is ensured that the solution is unique and its vector norm is the smallest among the feasible solutions. In matrix notation this results in the empirical kernel space:  XTw + eb = XTXa+ eb = Ka+ eb = y. (3)  Unfortunately, in practice the data is often not linearly separable. In these cases a nonlinear decision boundary needs to be adopted by defining a new distance metric. In the kernel approach, a kernel function is used to calculate a nonlinear inner-product which results in the new distance metric. This metric is defined over a kernel-based vector space as  K(x,x?) = ~?(x)T ~?(x?). (4)  ~?(x) = [?(1)(x), ?(2)(x), ..., ?(J)(x)]T is the induced vector, where J is the number of basis functions which can be either finite or infinite. Note that min(J,N) is generally the rank of K. Thus K is nonsingular when J > N . Kernel based discriminant analysis can be found in the literature, e.g. in [3] [4] and [5].

For all the experiments conducted in this paper a Gaussian RBF Kernel defined as  K(x,x?) = exp  { ?||x? x  ?||2  2?2  } (5)  was used. It is one of the most popular kernel functions. This is partially due to the fact that it involves an infinite number of basis functions (J ??), and therefore the rank of K will be N which in turn implies that it will be non-singular.

1.2. Kernel Discriminant Analysis (KDA)  Eq. 3 can be extended to incorporate nonlinear kernel func- tions. Through a linear mapping this leads to the decision function similarly to Eq. 1 of the form  f(x) = uT ~?(x) + b (6)  =  N? i=1  ai~?(xi) T ~?(x) + b (7)  = aT~k(x) + b (8)  with ~k(x) = [ K(x,x1) K(x,x2) . . . K(x,xN )  ]T .

The decision rule can then be expressed as  sign[f(x)] = sign  [ N? i=1  K(x,xi)ai + b  ] . (9)  By definition the decision-hyperplane must be orthogonal to the decision vector w. If the data-hyperplane is represented by its normal vector p, and thus XTp = e, then this orthog- onality implies that wTp = 0, or aTe = 0 in the empirical kernel space. This is called the Orthogonal-Hyperplane Prop- erty (OHP).

Minimizing the margin and thus ||u|| in the empirical ker- nel space, the OHP property and the restriction w = Xa lead to the kernel-matrix-based optimization formulation  max a  L(a) = aTy ? 1 aTKa (10)  subject to aTe = 0. (11)  It can be shown that this optimization problem has the closed form solution a = K?1(y ? be) where b can be derived as b = y  TK?1e eTK?1e  which is referred to as the Kernel Discriminant Analysis (KDA).

1.3. Kernel Ridge Regression (KRR)  In order to reduce the sensitivity to random noise of the clas- sifier, the kernel matrix can be perturbed to K + ?I which is called Perturbational Discriminant Analysis (PDA) or Kernel Ridge Regression (KRR) [1]. Intuitively, by adding a con- stant term to the kernel matrix K, the algorithm becomes less dependent on the specific training data mitigating the over- fitting problem. KRR can be written as the solution of an optimization problem of the following form  max a  { aTy ? 1  aT [K+ ?I]a  } (12)  subject to aTe = 0 and thus similarly to KDA. The intro- duced penalty term ? controlled by the ridge factor ? ? avoids the oversubscription of vulnerable and weak components in the spectral space as shown in [3]. This in turn can prevent over-fitting.

1.4. Support Vector Machines (SVM)  The objective of Support Vector Machines (SVM), first pre- sented by Vapnik [6], is to find the optimal vector a for  max a  { aTy ? 1  aTKa  } (13)  subject to the OHP constraint aTe = 0 and 0 ? ?i ? C with ?i = aiyi. In this case the penalty factor C is the parameter allowing for a softer separation margin. More specifically, a small C value will increase the number of support vectors.

Because the final decision boundary depends on a weighted combination of support vectors a higher number of support vectors makes the classifier more stable ? though at a potential loss of accuracy.

2. RIDGE-SVM  Noting the similarity between the objective function of KRR (Eq. 12) and SVM (Eq. 13) Kung [1] [2] [3] presented a combined classifier Ridge-SVM (formerly PDA-SVM):  a = argmax a  ( aTy ? 1  aT (K+ ?I)a  ) (14)  subject to aTe = 0, Cmin ? ?i ? C with ?i = aiyi. The discriminant function is f(x) =  ?N i=1 aiK(xi,x) + b.

b can be derived according to the KKT conditions. This hy- brid classifier now combines the parameters from KRR and SVM so that Cmin, C and ? can be separately adjusted in or- der to improve the accuracy. Note that ? in KRR and C in SVM complement each other since increasing ? avoids over- fitting, whereas decreasing C leads to over-fitting. Further- more, the variance ? of the non-linear kernel needs to be ad- justed so that in total four parameters need to be tuned at the same time. But while this allows for a better fitting to the structure of the data, one has to search a four dimensional space for the optimal set of parameters.

3. WEIGHT-ERROR-CURVE DESIGN FOR RIDGE-SVM  The error margin associated with the i-th training vector is de- noted as ?i = yi?f(xi). Remembering that the classification reflects a voting on the i-th vector with weight |ai| according to Eq. 9, the weight-error-curve (WEC) then shows the rela- tionship between ?i and ai. It can be derived that in the case of KRR the WEC is a straight line with a negative slope con- trolled by ? as shown in Figure 1b. For the special case where ? = 0, this results in KDA which is shown in Figure 1a. The main advantage of KRR lies in the smooth transition in the center region, in comparison to SVM as shown in Figure 1c where there is an abrupt drop in the WEC. However, the dis- advantage of KRR lies in the two tail ends where so-called    anti-support vectors are assigned excessively large weights.

SVM in comparison has constant weights in both tails. This again supports the argument for a hybrid classifier where the advantages of both classifiers are combined: Figure 1d shows the WECs for Ridge-SVM which is a combination of all three WECs.

ai  (a) KDA  ai ??1  (b) KRR  ?i = ?1 ?i = 0  C  ai  (c) SVM  ?(C ?Cmin )  Cmin  C  ?i = 0  ?i = ?1  ai  (d) Ridge-SVM: Cmin ? 0  Fig. 1. Weight-error-curves (WECs) for positive training vec- tors for various classifiers. Ridge-SVM incorporates the ad- vantages of the WECs of both KRR and SVM.

3.1. Special cases of Ridge-SVM  Since Ridge-SVM is a reunification of KRR and SVM, these two can be viewed as special cases of Ridge-SVM as shown in Figure 2. More specifically, when Cmin = ?? and C = ? the optimization formulation reduces to KRR. Additionally setting ? = 0 further reduces KRR to KDA. In the same way, SVM can be seen as a special case of Ridge-SVM where ? = 0 and Cmin = 0. Note that these special cases can also be deduced visually from the WEC in Figure 1d by setting ?, C and Cmin accordingly.

(?,C,Cmin )  Cmin = 0 ? = 0  ? = 0  Cmin = ?? C =?  Ridge? SVM  KRR  KDA  SVM (C)(?)  Fig. 2. The existing classifiers KRR, KDA and SVM can be seen as special cases of Ridge-SVM.

4. EXPERIMENTAL RESULTS  In order to show that the additional parameters in Ridge- SVM lead to an improved accuracy experiments on four UCI datasets 1 were conducted to find the optimal set of parame- ters for ?,Cmin, C and ? through exhaustive search. While tuning these parameters we noticed a significant change in training time. Thus we further investigated how, given opti- mal parameters, changing one of the parameters affects the training time and the prediction accuracy.

4.1. Optimal parameters  Kung et al. [2] conducted experiments on 6 datasets from the UCI machine learning repository and on microarray cancer diagnosis using Ridge-SVM and compared its performance against SVM, LSS (linear least squares), KDA and KRR. In these experiments the parameter ? for the RBF kernel was optimized for the conventional SVM and then applied with the same value to all other classifiers. The upper bound on ?i was set to 10, so that C = 10. Regarding the prediction performance, a direct consequence of using such a relatively small C is that only the most selective subset of training vec- tors can be retained which will in turn adversely affect the prediction capability. Using a grid search for ? ? {0, 1, 2} and Cmin ? {?0.1,?0.5,?1} the best combination of pa- rameters was found.

This very limited grid search on two parameters is ex- tended in this work. Here all four parameters are varied extensively to find the optimal combination. Early experi- ments showed that for all the datasets increasing C beyond 10 yielded no additional improvement and lowering C gener- ally worsened the accuracy. Thus, for all experiments C was fixed to 10 as before.

The accuracy of the parameters was measured using a leave-one-out cross validation to ensure consistency of the achieved accuracy when repeating the experiments. For datasets with more than two classes we performed a one- versus-all scheme as described in [3]: given a k-class prob- lem, a one-versus-all classifier contains k binary classifiers, each trained to separate one of the k mutually exclusive classes from the rest. In other words, for the k-th binary classifier, patterns belonging to the k-th class are considered to be positive, while the rest is considered to be negative.

The class which has the highest total score is identified as the most likely class for the test example.

The features for the wine and liver datasets had to be normalized as their features differed by several orders of magnitude which is generally problematic for kernel meth- ods. In order to preserve the structure of the original data as much as possible, a log(X+1) normalization was performed on the two datasets. We compared the results from the exten- sive parameter tuning of the Ridge-SVM algorithm with those  1http://archive.ics.uci.edu/ml/    from the previous experiments conducted in [2]. As shown in Table 1, extensive parameter tuning makes even more appar- ent the advantage of Ridge-SVM over SVM, LSS, KDA and KRR: for three out of four datasets the accuracy increased by 8.41%, 0.87% and 0.67% respectively compared to the previously chosen parameters presented in [2]. Relatively to the accuracy of SVM, the next best algorithm, Ridge-SVM, is better for all four datasets by 1.33%, 0.57%, 1.16%, 9.34% respectively.

Dataset Iris Wine Liver Glass No. of samples (N) 150 178 345 214 No. of features (M) 4 13 6 9 No. of classes 3 3 2 6 Accuracy [%] SVM 96.00 98.31 73.04 64.02 LSS* 84.00 96.63 53.33 38.32 KDA* 84.00 92.70 53.33 38.32 KRR 95.33 91.57 72.17 56.07 Ridge-SVM Parameters from [2] 96.67 98.88 73.33 64.95 New parameters 97.33 98.88 74.20 73.36 Improvement over parameters from [2] +0.67 +0 +0.87 + 8.41 SVM +1.33 +0.57 +1.16 + 9.34 Improved parameters Cmin -0.5 -1 -1 -0.125 C 10 10 10 10 ? 1.5 0.5 0.75 0.0625 ? 1 0.5 1 0.6  Table 1. Ridge-SVM consistently performs better than any of the existing algorithms. * Note that repeated examples were removed from the Iris dataset because they cause poor perfor- mance for LSS and KDA when computing the Kernel inverse.

In order to keep the size of the dataset the same repeated ex- amples were allowed in the test set for the cross-validation.

4.2. Tradeoff: prediction performance vs. training time  While tuning the parameters for the Ridge-SVM classifier, we noticed that the choice of the parameters significantly affects the training time. Especially for big data applications where the training process is very time consuming, prior knowledge about how each parameters influences the training time can significantly reduce the time spent on finding optimal param- eters. Furthermore, in some applications when the algorithm has to be trained regularly on new incoming training data, a slightly worse prediction accuracy can be accepted in ex- change for a much faster training time.

In this paper we study the relationship between the choice of parameters and the training time by running experiments using Matlab. The quadratic optimization problem was solved  using the built in quadprog tool. All experiments were per- formed using the trust-region-reflective algorithm. Note that, in terms of the training time, the interior-point-convex and active-set algorithm yielded results in the same order.

Ridge-SVM can be tuned with three parameters Cmin, C and ?. Furthermore, ?, the variance of the kernel can be ad- justed.

Varying ?: all experiments showed, that even though the choice of ? significantly affects the accuracy, it does not im- pact the training time. This is due to the fact that, although it introduces a new distance metric to allow nonlinear deci- sion boundaries, it does not put tighter constraints on the op- timization problem. For all 4 datasets, values for ? in the range of [0.5 1] yielded the most accurate results. However, this strongly depends on the structure of the data and can vary greatly for other datasets.

0 1 5 10 2033.33  97.33  C  Ac cu  ra cy  [% ]      0 1 5 10 20  0.06  0.16  0.44  Tr ai  ni ng  ti m  e [s  ]  Accuracy Training time  Fig. 3. Iris (Cmin = ?0.5, ? = 1.5, ? = 1)  Varying C: somewhat surprisingly for the parameter C, reducing its value only increased the training time and de- creased the accuracy in all cases as shown in Figure 3 for the iris dataset and thus was just kept constant at C = 10 for all experiments. The tuning of Cmin and ? are far from being so straightforward. In the two main experiments, ? and Cmin were individually tuned while all other parameters were fixed to the optimal value as presented in the previous section. We then compared learning speed and prediction performance as shown in Figure 4 for Cmin and Figure 5 for ?.

Varying Cmin: in the case of Cmin, for three out of four datasets (i.e. except for the glass dataset), a lower value yielded a better prediction performance. For the iris and liver dataset Cmin = 0 which equals ?-adjusted SVM yielded the highest accuracy. For the glass dataset Cmin > 0 gave the best accuracy whereas for the wine dataset Cmin < 0 gave the best accuracy. Thus, Cmin can take both positive and negative values depending on the structure of the data. In the same way when Cmin is increased and approaches 0, training suddenly takes up to 10 times longer. Given the optimization formulation for Ridge-SVM, this is due to the following fact: for a fixed C, increasing Cmin tightens the constraints for ?i (given by Cmin ? ?i ? C) and thus limits the choices for the ?i?s which in turn makes the optimization problem more    ?0.5 ?0.25 0 0.25 0.5   97.33   Cmin  Ac cu  ra cy  [% ]  SVM      ?0.5 ?0.25 0 0.25 0.5  0.06  0.5  0.81   Tr ai  ni ng  ti m  e [s  ]  Accuracy Training time  (a) Iris (C = 10, ? = 1.5, ? = 1)  ?1 ?0.5 ?0.25 0 0.25 0.5  93.82  97.75  98.88   Cmin  Ac cu  ra cy  [% ]  SVM      ?1 ?0.5 ?0.25 0 0.25 0.5 0.09  0.5   1.2  1.5   Tr ai  ni ng  ti m  e [s  ]  Accuracy Training time  (b) Wine (C = 10, ? = 0.5, ? = 0.5)  ?1 ?0.5 0 0.125  73.91  74.21   Cmin  Ac cu  ra cy  [% ]  SVM      ?1 ?0.5 0 0.125  0.32 0.5   1.5   2.5  Tr ai  ni ng  ti m  e [s  ]  Accuracy Training time  (c) Liver (C = 10, ? = 0.75, ? = 1)  ?1 ?0.5 0  72.43  72.9  Cmin  Ac cu  ra cy  [% ]      ?1 ?0.5 0  0.12  0.5  0.62   Tr ai  ni ng  ti m  e [s  ]  Accuracy Training time  (d) Glass (C = 10, ? = 0.4, ? = 0.6)  Fig. 4. As C approaches 0, the training time significantly increases. Except for the glass dataset, a lower Cmin value yields a better accuracy. Thus, only in the case of the glass dataset, the user faces a tradeoff between accuracy and train- ing time.

0 0.25 0.5 1 2.575   95.33 97.33   ?  Ac cu  ra cy  [% ]      0 0.25 0.5 1 2.5  0.06  0.5  0.9   Tr ai  ni ng  ti m  e [s  ]  Accuracy Training time  (a) Iris (Cmin = ?0.5, C = 10, ? = 1)  0 0.25 0.4 0.5 0.75 1  98.31  98.88  ?  Ac cu  ra cy  [% ]      0 0.25 0.4 0.5 0.75 10  0.09  0.3  Tr ai  ni ng  ti m  e [s  ]  Accuracy Training time  (b) Wine (Cmin = ?1, C = 10, ? = 0.5)  0.125 0.5 0.75 1 2   73.33  74.21   ?  Ac cu  ra cy  [% ]      0.125 0.5 0.75 1 2  0.3    2.5  Tr ai  ni ng  ti m  e [s  ]  Accuracy Training time  (c) Liver (Cmin = ?1, C = 10, ? = 1)  0.0625 0.4 0.5 0.75 1  43.56  73.36  ?  Ac cu  ra cy  [% ]      0.0625 0.4 0.5 0.75 1  0.13  0.5   1.5  Tr  ai ni  ng ti  m e  [s ]  Accuracy Training time  (d) Glass (Cmin = ?0.125, C = 10, ? = 0.6)  Fig. 5. For each dataset a distinct peak value for ? yields the highest accuracy. The training time increases significantly when ? approaches 0. For the glass dataset there is a tradeoff between training time and accuracy.

difficult. In order to explain why the training time suddenly increases as Cmin increases above zero, we note that this dis- allows ?i from being negative. Recalling that ?i = aiyi, in order to fulfill the OHP constraint that aTe = 0, the signs of the ai values are fixed based on the training labels. Thus it restricts the choices for ?i much more than in the case where ?i can be positive or negative. Except for the glass dataset, there is no tradeoff between training time and accuracy and it is advised to choose a negative value for Cmin to keep the training time low. In order to be sure that there is no larger Cmin value for which the accuracy increases Cmin can be tuned by starting off with Cmin = ?? and then increasing the value. If at some point the accuracy drops Cmin should be kept at a low value. However, if the accuracy suddenly increases, as in the case of the glass dataset, one faces the tradeoff between accuracy and training time, especially when the training dataset is large or there is new training data arriv- ing frequently and thus the algorithm has to re-run often. In the case of the glass dataset (Figure 4d) increasingCmin from ?0.375 to?0.125 increases the accuracy by just 0.47% at the expense of a 25% longer training time. However, if Cmin is now further increased to 0, the training time is 4 times as long as for the case where Cmin = ?0.375 at no additional accu- racy. Thus one has to be careful when choosing Cmin so that the training time does not significantly increase and prefer- ably keep Cmin < 0.

Varying ?: in the case of ?, for all four dataset, there ex- ists a relatively small region of values for which Ridge-SVM yields the best prediction performance. If ? is chosen too large, the accuracy drops which is due to the fact that the up- dated kernel matrix K+ ?I is then dominated by ? which re- sults in under-fitting. Similarly, if ? is set to 0, this can lead to over-fitting so that the accuracy drops. The experimental re- sults suggest that the training time increases exponentially as ? approaches 0 which was consistent across all four datasets.

This is most likely due to the fact that as ? increases, K+ ?I is dominated by the identity matrix and thus the inverse can be calculated faster when solving the quadratic optimization problem. Thus, it is recommended to start with a relatively larger value of ? in the order of 10 ? 100 and decrease until the accuracy stops improving. If ? has to be decreased too much to reach the peak accuracy, it can lead to a tradeoff be- tween accuracy and training time: in the case for the glass dataset, ? = 0.0625 yielded the highest accuracy of 73.36% as opposed to 72.9% with ? = 0.4 at the expense of a twice as long training time.

5. CONCLUSION  It is well known that the accuracy of classifiers strongly de- pends on the distribution of the data. Thus it cannot be known in advance which learning algorithm and what parameters yield the highest prediction accuracy. In this paper Ridge- SVM, a unified model for kernel-based supervised classifica-  tion which allows extensive parameter tuning was applied to four UCI datasets. The additional parameters come at the ex- pense of having to spend more time on the tuning. However, as shown in this paper, it yields a higher accuracy than SVM and KRR by 1.33 %, 0.57%, 1.16 %, 9.34% respectively. The influence of the parameter choice on the training time was quantified and methods to efficiently tune the parameters are presented. It was shown that when both ? and Cmin approach 0 the training time significantly increases and often the accu- racy drops. However, in some cases this extra training time can result in a better accuracy of the classifier.

In future work, Ridge-SVM should be applied to a wider range of datasets in order to further demonstrate its flexibility through the range of parameters to yield a higher accuracy than SVM and KRR.

6. ACKNOWLEDGMENTS  This material is based on research by DARPA under agree- ment number FA8750-12-2-0126. The U.S. Government is authorized to reproduce and distribute reprints for Gov- ernmental purposes notwithstanding any copyright notation thereon.

The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily repre- senting the official policies or endorsements, either expressed or implied, of DARPA, the U.S. Government, or Princeton University.

7. REFERENCES  [1] S.Y. Kung, ?Kernel approaches to unsupervised and su- pervised machine learning,? in Advances in Multimedia Information Processing-PCM 2009, pp. 1?32. Springer, 2009.

[2] S.Y. Kung and Man-Wai Mak, ?Pda-svm hybrid: A unified model for kernel-based supervised classification,? Journal of Signal Processing Systems, vol. 65, no. 1, pp.

5?21, 2011.

[3] S.Y. Kung, Kernel Methods and Machine Learning, Cam- bridge University Press, 2014.

[4] Bernhard Scho?lkopf and Christopher JC Burges, Ad- vances in kernel methods: support vector learning, The MIT press, 1999.

[5] K-R Muller, Sebastian Mika, Gunnar Ratsch, Koji Tsuda, and Bernhard Scholkopf, ?An introduction to kernel- based learning algorithms,? Neural Networks, IEEE Transactions on, vol. 12, no. 2, pp. 181?201, 2001.

[6] Vladimir Vapnik, The nature of statistical learning the- ory, springer, 1999.

