MCAR: Multi-class Classification based on Association Rule

Abstract Constructing fast, accurate classifiers for large data  sets is an important task in data mining and knowledge discovery. In this research paper, a new classification method called multi-class classification based on association rules (MCAR) is presented. MCAR uses an efficient technique for discovering frequent items and employs a rule ranking method which ensures detailed rules with high confidence are part of the classifier.

After experimentation with fifteen different data sets, the results indicated that the proposed method is an accurate and efficient classification technique.

Furthermore, the classifiers produced are highly competitive with regards to error rate and efficiency, if compared with those generated by popular methods like decision trees, RIPPER and CBA.

Keywords: Data Mining, Classification, Association Rule, Prediction, Frequent Item.

1. Introduction  Classification is one of the most important tasks in data mining. Given a collection of records in a data set, each record consists of a group of attributes and one of the attributes is the class. Classification involves building a model from classified objects, in order to classify previously unseen objects as accurately as possible. There are many classification approaches for extracting knowledge from data such as divide-and? conquer [13] and separate-and-conquer [7]. The divide? and-conquer approach starts by selecting an attribute as a root node, and then it makes a branch for each possible level of that attribute. This will split the training instances into subsets, one for each possible value of the attribute. The same process will be repeated until all instances that fall in one branch have the same classification or the remaining instances cannot be split    any further. The separate-and-conquer approach starts by building up the rules in greedy fashion. After a rule is found, all instances covered by the rule will be discarded. The process is repeated until the best rule found has a large error rate.

Numerous algorithms have been derived from these approaches, such as decision trees [12] and RIPPER [4].

However, it is our view that traditional classification techniques often produce a small subset of rules, and therefore usually miss detailed rules that might play an important role in some cases. Furthermore, heuristic classification techniques use domain independent biases to derive a set of rules, and therefore, the rules generated by them are different in nature and more complex than those that users might expect or be able to interpret [11].

Since the presentation of association rules for market basket analysis in 1993 [1], it has received great attention and been studied extensively. At the present time, association rule mining is still one of the most important tasks in data mining for discovering rules that pass certain user thresholds in a data set. Association rules are considered a strong tool for market basket analysis that aims to find relationships between items in a sales transaction database [1, 2]. In discovering association rules, one tries to find groups of items that are frequently sold together in order to infer items from the presence of other items in the customer's shopping cart. Such information is useful in making strategic decisions like shelving, sales promotions, marketing, and planning.

In classification rules, there is only one and only one pr-specified target, i.e. the class. However, the target for association rule is not pre-specified. Association rule can predict any attribute in the data set. In the last few years, a new approach that integrates association rule mining with classification called associative classification has been proposed [8, 15]. A few accurate and effective classifiers based on associative classification have been presented recently, such as CPAR [16], CAEP [5], CMAR [8] CBA [9] and MMAC [15]. Many    experimental studies [8, 9] showed that assocIatIve classification is a promising approach, which builds more accurate classifiers than traditional classification techniques such as decision trees [12]. Moreover, many of the rules found by associative classification methods can not be discovered by traditional classification techniques [9].

In this paper, a new associative classification technique called MCAR is introduced, which extends the basic idea of association rule [I] and integrates it with classification to generate a subset of effective rules that fonn a multi-class classifier. Our classifier is of the fonn <rI, r2, . .. , rn, default class>, where ri is a rule and the default class is the majority class of the remaining unclassified instances in the training data set.

Basic concepts in the association rule and the associative classification approach are presented in Section 2. Our proposed algorithm is presented in Section 3. Experimental results are discussed in Section 4. Finally the conclusions are presented in Section 5.

2. Associative Classification  2.1 Basic Concepts of Association Rule  Let D be the training data set with n attributes (columns) A], A2, . ? .  ,An and IDI rows. Let C be a list of class labels. Specific values of attribute Ai and class C will be lower case a and c, respectively.

Definition 1: An item is defined as a set of attributes together with a specific values for each attribute in the  set, denoted < (Ail, ail), (Ai2' ai2), ... (Aim' aim?.

Definition 2: A rule r maps an item (or condition) to a specific class label, denoted:  ?Ail, ail), (Ai2' ai2), ... , (Aim' aim? tj> C.

Definition 3: The actual occurrence ActOcc(r) of a rule r in D is the number of rows of D that match r 's condition.

Definition 4: The support count SuppCount(r) of r is the number of rows of D that match r's condition, and belong to r's class.

Definition 5: The support of r is defined as the SuppCount(r )/IDI.

Definition 6: The minimum support which a rule in our rule base may have is denoted MinSupp.

Definition 7: The confidence of r is defined as SuppCount(r)/ A ctOcc(r).

Definition 8: The minimum confidence which a rule in our rule base may have is denoted MinConf.

Consider for instance the training data set shown in Table I and assume that MinSupp is 20% and MinConf  is 50%. The support of rule < (AI' Xl ) >---+ c1 is 3/1 0, which satisfies the MinSupp threshold. The confidence  of rule < (AI' Xl ) >---+ c1 is 3/5, and thus this rule also   satisfies the MinConf threshold and is a candidate rule in the classifier.

We will use a subset of standard set notation with the definition implied for item, so if 11 = < (An, all), (A22' a22), ... (AlmI' almI? and 12 = < (A2J, a2I), (A22' a22), ... (A 2m2, a2m2? are two items having disjoint attributes, then 11 Y 12 = < (An, all), (A 22, a22), ... (A ImI, almI), (A2J, a2J), (A 22, a22), ... (A 2m2, a2nd>, and we will say  l' <:;;; I, ' if there exists item I" for which l' u 1" = I, .

2.2 Associative Classification  Generally, in association rule mining, any item that passes MinSupp is known as a frequent item. If the frequent item consists of only a single attribute value, it is said to be a frequent single item. For example, with MinSupp = 20%, the frequent single items in Table 1 are  < (AI, Xl?' < (AI, X2?, < (A2' YI?, < (A2' Y2? and < (A2' Y3?. Current assocIatIve classification techniques generate frequent items by making multiple passes over the training data set. In the first pass, they find the support of each single item, and then in each subsequent pass, they start with items found to be frequent in the previous pass in order to produce new possible potential frequent items involving more attribute values, known as candidate items.

In other words, frequent single items are used for the discovery of potential frequent items that involve two attribute values, and frequent items that involve two attribute values are input for the discovery of candidate items involve three item values and so on. After frequent items have been discovered, assoCIatIVe classification methods derive a complete set of rules for those frequent items that pass MinConf  T bI 1 T a e ra ining RowId A l  A2  I xl  yl 2 xl  y2 3 xl  yl 4 xl  y2 5 x2 yl 6 x2 yl 7 x2 y3 8 xl  y3 9 x2 y4  10 x3 yl  d t 1 a a Class  c l c2 c2 c l c2 c l c2 c l c l c l  2.2.1 Related Work. One of the first algorithms to bring up the idea of using an association rule for classification was proposed in [9]. It has been named CBA. CBA implements the famous Apriori algorithm [2] that requires multiple passes over the training data set in order to discover frequent items. Once the discovery of frequent items finished, CBA proceeds by converting any frequent item that passes the MinConf into a rule in the classifier. In doing that, only one subset of the generated classification rules will be considered in the final classifier. Evaluating all the generated classification rules against the training data set does the selection of the subset. The frequent items discovery and rules    generation are implemented in two separate phases by CBA.

A method based on association rule for medical image classification has been presented in [3]. It consists of three major phases, phase one involves cleaning up the medical images and extracting target features. Phase two learns rules which are used to build the classifier in phase three.

Recently, a new technique, i.e. MMAC, which uses association rules mining in classification framework, has been developed in [15]. The proposed algorithm explores the problem of producing rules with multiple labels.

Moreover MMAC presents three measures for evaluating the accuracy of data mining classification approaches to a wide range of traditional and multi-label classification problems. Results for 28 different data sets show that the MMAC approach is an accurate and effective classification technique, highly competitive and scalable in comparison with other traditional and associative classification approaches.

Another associative classification algorithm that selects and analyses the correlation between high confidence rules, instead of relying on a single rule, has been developed in [8]. It uses a set of related rules to make a prediction decision by evaluating the correlation among them. The correlation measures how effective are the rules based on their support values and class distributions. In addition, a new prefix tree data structure named CR-tree to handle the set of rules generated and to speed up the retrieval process of a rule has been introduced. The CR-tree has proven to be effective in saving storage since many condition parts of the rules are shared in the tree.

3. MCAR Algorithm  The algorithm proposed in this paper consists of two phases: rules generation and a classifier builder. In the first phase, MCAR scans the training data set to discover frequent single items, and then recursively combines the items generated to produce items involving more attributes. MCAR then generates, ranks and stores the rules. In the second phase, the rules are used to generate a classifier by considering their effectiveness on the training data set. Figure 1 represents the MCAR algorithm, which we will explain in more detail below.

3.1 Data Types and Format  Data used by MCAR is in a test file format with a header which indicates file name, attribute names, and number of rows. Values for each instance are comma? separated, and the class attribute must be the last one in the header file. Currently, MCAR deal only with categorical data, where each attribute takes a value from   Input: Training data (D), MinSupp and MinCOIifthresholds  Output: A set of rules  Scan D for the set S of frequent single items Do  For each pair of disjoint items 11, 12 in S If <II u h> passes the MinSupp  threshold  S?Su<Iluh> Until no items which pass MinSupp are found  For each item I in S  Generate all rules I ? C which pass the MinConfthreshold  Rank all rules generated  Remove all rules l' ? c' from S where there is some rule I ? c of a higher rank and I ? 1'.

Fig.1 MCAR algorithm  a finite set of possible values. For continues-valued attributes a descretisation technique such as that discussed in [6] could be adapted for MCAR.

3.2 Building the Classifier  3.2.1 Frequent Items discovery and Rule Generation. In order to improve the efficiency of frequent items discovery and rules generation, MCAR employs a technique based on an intersection method that has been presented in [19]. Our method scans the training data set once to count the occurrences of single items, from which it determines those that pass the MinSupp threshold. It stores items along with their locations (rowIds) inside arrays. Then, by intersecting the rowIds of the frequent items discovered so far, we can easily obtain the remaining frequent items that involve more than one attribute. We also use rowIds for frequent single items to obtain support and confidence values for rules involving more than one item.

Consider items ?A], Xl? and < (Ab YI? in Table I, the following two sets represent the rowIds in which they occur, {l , 2, 3, 4, 8} and {I, 3, 5, 6, 10}. We can determine the support of a new item, such as < (A], Xl) , (Ab YI? by performing an intersection on the rowId sets for items ?A], Xl? and (A2' YI?. The resulting set {I, 3} will represent the tuples in which both items have occurred together in the training data. If the support of the new item < (A], Xl) , (A2' YI?, i.e. 2110, passes the    MinSupp threshold, then it is a candidate for the condition in a rule. Items that pass the MinSupp threshold are generated recursively from items having a smaller number of attributes, starting from the frequent single item derived in a single pass through the training data set.

Once an item has been identified as a frequent item, the MCAR algorithm finds all rules with that item as condition which pass the MinConf. It should be noted that every time a frequent item is found, only the rule with the largest confidence is considered by MCAR algorithm. In the case that an item has two rules with identical confidence, the choice of the rule will be random.

Since the training data set has been scanned only once to discover and generate the rules, this approach is highly effective in runtime and storage because it does not rely on the traditional approach [2], which requires multiple data scans. However in cases where there is large number of candidate items held in the main memory, the possible number of intersections required to generate frequent items may be tremendous. This is one drawback of the proposed algorithm, which may consume more resources like storage.

3.2.2 Ranking of the Rules. Rule ranking plays an important role in association mining. CBA and CMAR rank the rules mainly in terms of the confidence level of rules. When several rules have identical confidences or supports, CBA and CMAR randomly choose one of the rules, which may degrade accuracy. In order for addressing this issue, MCAR always looks for the best rules for the final classification system. The best rules are not only the ones with high confidence values as CBA and CMAR rules but also with large representation in the training data set. In addition, the high ranked rules of MCAR often general rules since MCAR prefer rules with less condition.

Definition 9: Given two rules, ra and rb, ra precedes rb if :

I. The confidence of ra is greater than that of rbo 2. The confidence values of ra and rb are the same,  but the support of ra is greater than that of rbo 3. The confidence and support values of r a and rb are  the same, but ActOcc(rahActOcC(rb).

4. Confidence, support and ActOcc values of ra and  rb are the same, but ra has fewer conditions in its left hand side than of rbo  5. All above criteria are identical for ra and rb, but ra was generated before rb.

3.2.3 Rules Evaluation and Classification. A rule is significant if and only if it covers at least one training instance. After the rules have been generated and ranked, an evaluation step tests each rule in tum against the training data set in order to remove rules which fail to   classify at least a single instance. At each step, all rows correctly classified by the current rule will be deleted from the training data set. Whenever a rule does not classify any rows of the data, it will be removed from the rules set because a higher ranked rule has correctly classified its instances. This process ensures that only high confidence rules remain in MCAR classifier.

In classification, let R be the set of generated rules and D the test data. The basic idea of the proposed method is to choose a set of high confidence, representative and general rules in R to cover D. In classifying a test object, the first rule in the set of ranked rules that matches its condition classifies it. This process ensures that only the highest ranked rules classify test objects.

4. Experimental Results  Experiments on 15 different data sets from the VCI data collection [10] were conducted using stratified ten? fold cross-validation. In cross- validation, the training data set is divided randomly into J 0 blocks, each block is held out once, and the classifier is trained on the remaining 9 blocks; then its error rate is evaluated on the holdout block. Thus, the learning procedure is executed ten times on slightly different training data sets [14].

Some of the selected data sets were reduced by ignoring their numerical attributes. Several experiments using ten-fold cross-validation have been performed to ensure that the removal of any numerical attributes does not significantly affect the classification accuracy. To do so, we only considered data sets where the error rate was not more than 6% worse than the error rate obtained on the same data set before the removal of any real/integer attributes.

Several studies have shown that the support threshold plays a major role in the overall prediction of classifiers derived by existing associative classification techniques [9, 15]. If the user sets the support threshold too high, many good quality rules will be ignored, on the other hand, if the support value is set too low, the problem of overfitting arises and many redundant rules will be generated which consequently consumes more processing time and storage. From our experiments, we observed that the classifiers derived when the support was set between 2%-5% achieved high accuracy, and most often better than that of decision trees (C4.5), RIPPER and CBA, and therefore, the MinSupp was set to 5%. The confidence threshold, on the other hand, has a smaller impact on the behaviour of any associative classification method, and it has been set in our experiments to 40%.

Three popular classification techniques decision trees (C4.5), RIPPER and CBA have been compared to MCAR in terms of classification accuracy, rule features and number of rules. The choice of such learning    T bl 2 A a e ccuracyan d ru es 0 f MCAR CBA C4 5 d RIPPER I ?th , , an algor! ms Classification Accuracy %  Data set MCAR CBA C4.5  Tic-Tac 99.76 98.85 83.61  Vote 90.10 87.91 88.27  CRX 83.05 85.31 80.72  Sick 94.65 93.90 93.87  Led7 72.32 71.10 73.34  Balioon 100.00 100.00 100.00 Contact-  lenses 74.66 66.67 83.33  Heart-s 80.26 79.25 78.55 Breast- cancer 72.61 69.66 72.52  Weather 70.88 85.00 50.00  Heart-c 81.40 79.87 78.21  Lymph 78.50 75.09 83.78  Mushroom 98.78 94.18 99.95 primary- tumour 40.50 25.47 42.47 Credit-  Card 71.90 70.40 70.5  methods is based on the different strategies they use to generate the rules. Table 2 gives the classification accuracy and the number of rules generated by MCAR, CBA, C4.5 and RIPPER algorithms. The experiments of C4.5 and RIPPER algorithms were conducted using the WEKA software system [18], which is an open java source code for the data mining community that includes implementations of different methods for several different data mining tasks such as classification, association rule and regression. CBA experiments were conducted using an implementation version provided by the authors of [17].

The results in Table 2 indicate that MCAR outperforms the CBA algorithm in terms of accuracy.

One of the principle reasons for this appears to be that MCAR often generate more rules than CBA. The increase in accuracy suggests that this is not simply overfitting and would likely justify the small increase in classification rate for MCAR over CBA in applications.

However, in some cases like "Led7" data set, the number of rules produced by MCAR is large, even though every rule covers at least a single instance in the training data.

Thus, a pruning method like Pessimistic error pruning [12] may be beneficial in such cases.

The won-loss-tied record of the proposed algorithm against CBA in term of accuracy is 12-2-1. The won? loss-tied record of the proposed method against C4.5 and RIPPER algorithms in term of accuracy are 9-5-1 and I 1-3-1, respectively.

Number of Rules  RIPPER MCAR CBA C4.5 RIPPER 97.59 26 25 95 14  87.53 84 40 4 4  84.92 97 43 36 3  93.84 17 10 1 2  69.43 192 50 37 19  100.00 3 3 3 2  75.00 9 6 4 3  78.23 31 22 2 2  70.97 71 45 4 3  64.28 6 6 5 3  79.53 72 44 12 5  77.70 48 38 12 6  99.90 48 45 33 14  36.28 28 1 23 5  71.70 162 116 103 3  We compared the runtime of the two aSSOCiatIve classification techniques (MCAR and CBA) on seven data sets in order to compare scalability and efficiency.

Table 3 represents the runtime in ms obtained in the experiments. It indicates that MCAR is faster than CBA in all cases. The intersection method that MCAR employed to find frequent items reduces gradually its runtime. Runtime experiments were conducted on Pentium III, 128 RAM. A larger sized memory should improve the runtime of both CBA and MCAR. In addition, our proposed algorithm was implemented using Java, whereas CBA has been implemented using C++. It is expected that runtime results of MCAR should significantly decrease in the case if C++ had been used to implement it with more code optimisation.

A deeper analysis of the rules produced by our method and CBA has been conducted compare effectiveness of  to the the  classifiers. Let us consider the classifiers derived by MCAR and CBA from the "contact lenses" data set shown in Figure 2a and 2b using a MinSupp of 5% and a MinConf of 40%. The "contact-lenses" data set  Table 3? Runtime in ms Data set MCAR CBA  Tic-Tac 657 1660  Led7 2000 4620  Heart-c 125 680  Heart-s 31 900  Lymph 282 470 pnmary- tumour 969 1380  CRX 547 1360    contains 24 training cases, and three classes (hard, soft, none). After analysing the rules generated, it was found that there is consistency in the rule features, in which four of the generated rules are identical. However, MCAR method derived three more rules than CBA, with all rules having confidence as large or greater for MCAR than CBA. The classifier derived by CBA algorithm consists of six rules and has an error rate of 33.33% on the "contact-lenses" data set. By comparison, the MCAR classifier has nine rules which all have 100% confidence, giving a much reduced 25.35 % error rate.

M CAR support & confidence values for rules derived fro m co ntact data set  0.8  0.6  0.4  0.2  o 2 3 4 5 6 7 8 9  --+-Supp Number of rules --+-Conf  Fig 2a. Support and confidence values of MCAR classifier  0.8  CSA support & confidence values for rules derived from contact data set  ? 0.6 +----------------{  0.4  0.2  2 3 4 N umber of rules  5 --+-Supp --+-Conf  Fig 2b. Support and confidence values of CBA classifier   5. Conclusions  Accuracy and speed are crucial factors in association rule and classification tasks in data mining. The main contributions of the MCAR approach are:  ? MCAR uses a technique for discovering frequent items that requires only one pass, consuming significantly less storage and runtime than multi? pass approaches.

? MCAR discovers and generates frequent items and rules in one phase. Other associative classification methods such as CPAR and CBA discover frequent items in one phase, and then determine which subset of them form the classifier in a separate phase.

? MCAR introduces a rule ranking technique that minimises the use of randomisation when a choice point must be made between two or more rules.

Performance studies on 15 data sets from VCI data collection indicated that MCAR is highly competitive when compared with the RIPPER, CBA and C4.5 algorithms in term of prediction accuracy and efficiency.

Our further work will investigate the extraction of multiple class labels using association rule discovery for a wide range of application problems. Moreover, we will enhance the proposed algorithm by treating continuous attributes using a descretisation method.

