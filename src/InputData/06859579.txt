High Productivity Data Processing   Analytics Methods with Applications

Abstract - The term ?big data analytics? emerged in order to engage in the ever increasing amount of scientific and engineering data with general analytics techniques that support the often more domain-specific data analysis process. It is recognized that the big data challenge can only be adequately addressed when knowledge of various different fields such as data mining, machine learning algorithms, parallel processing, and data management practices are effectively combined. This paper thus describes some of the ?smart data analytics methods? that enable a high productivity data processing of large quantities of scientific data in order to enhance the data analysis efficiency. The paper thus aims to provide new insights how various fields can be successfully combined.

Contributions of this paper include the concretization of the cross-industry standard process for data mining (CRISP- DM) process model in scientific environments using concrete machine learning algorithms (e.g. support vector machines that enable data classification) or data mining mechanisms (e.g. outlier detection in measurements). Serial and parallel approaches to specific data analysis challenges are discussed in the context of concrete earth science application data sets. Solutions also include various data visualizations that enable a better insight in the corresponding data analytics and analysis process.



I. INTRODUCTION ?Understanding climate change, finding alternative  energy sources, and preserving the health of an ageing population are all cross-disciplinary problems that require high-performance data storage, smart analytics, transmission and mining to solve? [1]. Scientific disciplines face huge challenges in data processing with the ever increasing amount of scientific data [1,2,3]. This raises the demand for ?smart data analytics methods?, which are required to provide a very concrete set of tools, techniques, frameworks, and infrastructures in order to perform ?big data analytics? with real data sets. This is one of the important elements of data-intensive science [3] among others such as preserving and managing large-scale data sets for their ?active re-use? (e.g. in-situ validations with real measurement data sets during a computational simulation on large-scale computing resources).

In order to effectively perform data-intensive science, it is important (i) to understand its relationship to other important and closely related concepts (e.g. machine learning algorithms, data mining practices, etc.), and (ii) to understand what are the fundamental techniques for  efficiently using available resources (e.g. high performance computing, high throughput computing, big data sharing and access, etc.). Hence, by considering (i) and (ii), we want to achieve a ?high productivity data processing? using a systematic approach and thus refine with this term the often generally used term ?big data analytics? with concrete methods. A wide variety of data analysis and analytics were arbitrary but we require a principled and systematic set of methods that guides the ?smart data analytics processes?. This enters the field of process models such as the Cross Industry Standard Process for Data Mining (CRISP-DM) [4].  We require ?innovative perspectives on data? as described by Provost et al. [5] who mentions that a ?data-science perspective provides practitioners with structure and principles, which give the data scientist a framework to systematically treat problems of extracting useful knowledge from data?. This is particularly useful when the general data analytics step (performed by data scientists) precedes the often scientific domain-specific data analysis.

In earlier work [6], we already gave insights into several concrete ?big data processing approaches? (e.g.

iterative map-reduce, scientific workflows, databases, massively-parallel processing algorithms, visualization and steering, etc.) that we leverage in the context of large- scale computing (e.g. PRACE [7]) and collaborative data infrastructures (e.g. EUDAT [8]). This paper goes a step further in disentangling the set of closely interrelated ?big data analytics concepts? described above in order to understand which techniques can be applied on which compute and storage resources to solve one specific data science problem. We provide methods in the context of two concrete data-intensive science case studies from the earth science domain we work with in a systematic fashion using CRISP-DM and selected algorithms and resources: (a) outlier detection [21] in measurement devices data, and (b) data classification [21] of multi- spectral images.

This paper is structured as follows. After the introduction to the problem domain, Section II provides a background to the concrete earth science data sets in question and briefly surveys systematic ways to perform ?data science? with selected required concrete techniques.

Section III offers the reader then methods in context of our two case studies well embedded in the overall systematic way of performing a high productivity processing of data.

The paper ends with some concluding remarks of several findings and outlines future work elements.

MIPRO 2014, 26-30 May 2014, Opatija, Croatia

II. BACKGROUND AND RELATED WORK A wide variety of process models exist that bear the  potential to support ?smart data analytics processes? with a concrete structure that often refer to ?phases? with certain ?steps?. CRISP-DM [4] (cf. Figure 1) is the most suitable one that we refined for our (parallel) smart data analytics since it clearly outlines ?six concrete phases? that are typical for a data mining project: (a) problem/business understanding; (b) data understanding; (c) data preparation; (d) modeling; (e) evaluation; (6) deployment.

Given the page restriction, we summarize them only briefly based on the CRISP-DM user guide [9] including refinements towards ?smart data analytics?, but would like to emphasize that we perform our work on the scientific data case studies using the CRISP-DM as a useful backbone process. To the best of our knowledge, a generally accepted ?big data analytics process model? for science and engineering problems is not existing except disconnected approaches from several business domains (e.g. predictive/customer analytics processes, etc.).

The ?(a) problem/business understanding phase? includes general steps (aka tasks) such as determining the objectives and goals (e.g. data analytics success criteria), performing a situation assessment (e.g. inventory of computing and storage resources). In phase ?(b) data understanding?, the tasks includes the creation of a couple of reports about the initial data (collection and data sets), including data (collection) description, exploration options, and data quality statements. The third phase ?(c) data preparation? refers to tasks that accurately described the data sets in question, their rational for inclusion/exclusion as well as required work on cleaning data or reformatting data if needed. The phase ?(d) modeling? includes the selection of modeling techniques, test designs and related parameter settings to build the model.  This phase depends highly on the scientific question that drives the ?smart data analytics? process as well as the next phase ?(e) evaluation?, which stands for several tasks related to assess the analytics results w.r.t.

the initial success criteria described in phase (a). The final phase ?(f) deployment? enables a systematic way of performing analytics on a regular basis including a monitoring and maintenance plan. In our given context of ?smart data analytics? we apply the CRISP-DM process not as an iterative process, but rather as a cyclic one refining information and models of phases as required.

According to Marb?n et al. [10], CRISP-DM is the ?de facto standard for developing data mining and knowledge discovery projects? that was one of the facts influencing our decision to use the CRISP-DM as a backbone process  for our ?smart data analytics? case studies. Another process model often mentioned in context and named after its phases is ?Sample, Explore, Modify, Model, and Assess (SEMMA)? [11] mostly driven by the SAS Institute. Although used in the business domain alongside a dedicated commercial tool, it rather concentrates on the modeling tasks in data mining compared to CRISP-DM that includes a more comprehensive ?perspective? on the overall data analytics process.

After the introduction of the major structure of performing ?smart data analytics? in a systematic way, we briefly introduce our two data-intensive case studies in order to have a focus on relevant and concrete techniques required. The field (and overlaps) of data mining, machine learning, knowledge discovery, and (parallel) data analysis & analytics is very large and thus we need to concentrate on two very specific problems given real world data sets. At the same time we face the challenge that many algorithms in the field of machine learning, artificial intelligence, or data mining have not a long history in taking advantage of parallel and distributed computing techniques in the field of ?high performance computing (HPC)? and ?high throughput computing (HTC)?. In this contribution, it is the rate of interconnection of computing nodes that differentiates HPC (i.e. fast interconnect within clusters, e.g. Infiniband) from HTC (i.e. moderate interconnect, e.g. usual Ethernet as found in Grids and Cloud infrastructures today).

The first case study raises the demand to tackle the problem of ?automatic outlier/anomaly detection? in measurement data sets that are part of the PANGAEA data collection [12]. With 7.9 billion data items (2013), better sensor equipment during the years, and its constant growing factor contribute to name this data source as ?big data?. Hodge et al. [13] present a good survey of principles in outlier detection referred to as different types: ?Type 1 ? Determine the outliers with no prior knowledge of the data (unsupervised clustering)?; ?Type 2 ? Model both normality and abnormality (supervised classification)?; ?Type 3 ? Model only normality or in a very few cases model abnormality (novelty detection and recognition) ?. We re-use these principles, but require a more fine-grained perspective when tackling a concrete data set with more elaborate elements for parallel approaches than mentioned in [13].

The second case study tackles the problem of ?classification of multi-spectral images? in data sets that are part of data collections acquired by the Quickbird satellite [14]. This is just one ?sensor? of many similar data sources that enable the study of changes in land usage, agricultural and forest climates. Experts refer to case studies like this as ?remote sensing science? that aims to interpret information using sensors that are not in direct physical contact with the object being observed.

Classification algorithms that work with these datasets have to treat both spectral and spatial information in order to obtain a satisfactory level of detection accuracy. A survey of related work in classification principles is nicely summarized by Moguerza et al. in [15] as follows: ?support vector machines (SVMs) have increasingly turned into a standard methodology in the computer science and engineering communities?.

Figure 1.  CRISP-DM process model (modified from [9])

III. METHODS FOR SMART DATA ANALYTICS Innovative methods for smart data analytics require a  solid foundational basis in order to enable a high productivity processing of ?big data? (i.e. towards petabytes) rather than being only usable with data sets of sizes that we often used in the past (i.e. gigabytes, terabytes). We therefore suggest that the methods are well embedded in a ?collaborative data infrastructure (CDI)? design with additional services that optimize data access and provide added value with services that enable data sharing, persistence, and safe replication. In earlier work, Riedel et al. [16] proposed a ?data infrastructure reference model? that follows a CDI approach and that consists of computing and storage resources relevant for our proposed concrete analytics methods. The design approach and some of its services (e.g. the B2SAFE safe data replication service) are currently implemented as part of the EUDAT data infrastructure.

In contrast to the rather elaborated reference architecture top-down work of the National Institute for Standard (NIST) big data working group [18], we aim to provide in this contribution a bottom-up approach based on concrete scientific case studies and their required reference architecture elements. In [6] we therefore suggest a derived concrete architecture design tailored for ?big data analytics? in a context of a CDI while this section aims to refine this architectural setup based on concrete techniques and tools that are used in the context of two specific case studies. Scientific results of them are expected to be published in domain-specific publications and thus this paper focusses on the pure technical findings of combining a wide variety of available tools and frameworks for ?smart data analytics?. Selected elements of the CRISP-DM process model tailored to our problem domain are given in context of the two case studies while full reports are already available or expected to become available as part of the EUDAT data infrastructure.

A. Automatic outlier detection in measurement datasets By following the tailored CRISP-DM model phase  ?(a) problem/business understanding? we define the goal  to automatically detect outliers in large quantities of measurement data, because manual ?quality control? is slow, error-prone, and becomes increasingly impossible as the amount of data set grow significantly in type and volume. The architectural setup in Figure 2 illustrates selected parts of the inventory of computing and storage resources such as the PANGAEA collection archive (i.e.

community storage data sets) and HPC resources (i.e.

computing nodes available at the Juelich Supercomputing Centre and UoIceland respectively).

Selected elements of phase ?(b) data understanding? include the task to collect initial data out of the large PANGAEA data collection that in our case refers to initial measurement datasets acquired by a cabled underwater observatory sensor in the open Swedish Fjord Koljoe Fjord (approximately 100 KM north of Gothenburg) installed as part of the HYPOX2 project [17]. The data sets are provided by PANGAEA data scientists that preserve and manage data sets in their own domain- specific data infrastructure.

The third phase ?(c) data preparation? includes tasks of describing the structure of the data. In our case study we work with datasets that consists of a time series of a month/file (i.e. tab separated data files) with underwater measurements. We furthermore had to remove some metadata elements in the plain data files that created problems when using it with the R Statistical Computing tool [20].

The phase ?(d) modeling? includes the selection of modeling techniques and in our case also the related appropriate outlier detection algorithms. There are currently two parallel approaches we explore as part of our ?smart data analytics? methods: (i) using the statistical computing tool R with the RMPI and outlier package (cf.

Figure 2); (ii) using a new implementation by G. Fox et al.

of the DOoR algorithm described by Bhaduri et al. in [22].

The automation of the approach is provided by using a HPC middleware (i.e. UNICORE [23]) that provided a seamless access to large-scale supercomputers. R scripts and the execution of outlier algorithms are performed with this system using workbench tools (cf. Figure 2).

Figure 2.  Architectural deployment for the automatic outlier detection in PANGAEA measurement datasets.

The (e) evaluation? phase, which stands for several tasks related to assess the analytics results w.r.t. the initial success criteria described in phase (a) is twofold: Firstly on a technical basis using data mining techniques; secondly, results of the automatic outlier detection are regularly shared with scientific experts from PANGAEA by using the ?B2SHARE EUDAT Data Service? (cf. Figure 2). They evaluate which ?smart data analytics? elements make sense and what approaches are not helpful. In addition, often used outlier data visualizations are available (e.g. normal probability plots, boxplots, histograms) as needed to further support this process (cf.

Figure 2 top right).

The final phase ?(f) deployment? enables a systematic way of performing automated outlier detection on PANGAEA datasets once the tests with initial datasets have been shown successful over time. This concrete ?smart data analytics method? is well embedded in a sustainable data infrastructure offering storage resources and services mostly independent of project funding, because of a network of trusted data centers in EUDAT and PRACE.

B. Classification of multi-spectral images We start again with our tailored CRISP-DM process  model, but perform a quicker walk-through as the ideas of each phase are the same, but the given data analytics problem is usually different. In phase ?(a) problem/business understanding? we define the goal of performing a data classification of buildings in street map with high accuracy using multi-spectral image datasets.

These data sets constantly grow in volume and thus require a more powerful classification approach and thus we define a ?speed-up of data classification? as a clear objective in this particular case study. The architectural setup in Figure 3 illustrates selected parts of the inventory of computing and storage resources such as parts of the Quickbird satellite data collection (i.e. community storage data sets) and HTC resources (i.e. computing nodes available at the UoIceland and as part of FutureGrid [24] respectively).

Phase ?(b) data understanding? include the task to collect initial multi-spectral image data sets out of the large commercial Quickbird data collection that in our case is done by the domain scientists that work with us.

In the phase ?(c) data preparation? we perform tasks to describe the structure of the data. In our case study we work with datasets that are in fact different types of images acquired by the QuickBird satellite: (i) a high- resolution panchromatic image or resolution 0.6m and a low-resolution (2.4m) multi-spectral image.

The phase ?(d) modeling? includes the selection of modeling techniques and in our case also the related appropriate parallel data classification algorithm. As described above, one of the most commonly applied principle technique in data classification are support vector machines (SVMs) [21]. There is currently one parallel approach we explore as part of our ?smart data analytics? method: using Twister [25,26] as shown in Figure 3. Twister improves the original map-reduce [27] approach (e.g. used in Apache Hadoop [28]) with iterative capabilities, which can be used to train an SVM classifier as described by Zhang and Fox in [29]. We need to mention here that the implementation in turn is based on the widely known libSVM library [30].

Also in this case study, the (e) evaluation? phase is twofold: Firstly on a technical basis using machine learning techniques separating the labelled data in three distinct sets: training data, test data and validation data; secondly, results of the data classification results are regularly shared with scientific experts by using the ?B2SHARE EUDAT Data Service? (cf. Figure 3). They evaluate which ?smart data analytics? elements make sense and what data classifiers have not enough accuracy.

In addition, visualizations are available (e.g. speed-up diagrams compared to serial approaches) as needed to further indicate that the training time of the SVM classifier was significantly faster. This is especially the case when considering the observed used manual approaches by using Matlab SVMs [31].

Figure 3.  Architectural deployment for the data classification using parallel support vector machines for Quickbird masurement datasets.

The final phase ?(f) deployment? enables a systematic way of performing data classification of multi-spectral image datasets once the tests with initial Quickbird image datasets have shown good accuracy of the trained parallel SVM classifiers. This concrete ?smart data analytics method? is well embedded in a sustainable data infrastructure offering computing and storage resources mostly independent of project funding, because of a network of trusted data centers in EUDAT with computational resources .



IV. CONCLUSIONS This section gives the reader some concluding remarks  although we are in the early phases of our ?smart data analytics process? in the described case studies ? but that is a process that might be never finished given the ever increasing amounts of big data sets. We are nevertheless able to derive some initial conclusions about structuring the process and thus performing ?big data analytics? in a systematic way in the concrete area of ?outlier detection? and ?image data classification? of scientific datasets.

Detailed findings that relate to the clever and innovative combinations of technology required for the ?smart data analytics? described in this paper are actively contributed and discussed as part of the Big Data Analytics (BDA) [19] interest group of the Research Data Alliance (RDA).

This enables the guidance of non-experts for ?big data analytics? and sharing of findings with the broader community (e.g. with members of the US NIST big data activities).

We firstly conclude that the general reference model design of a CDI offers ?added value services? (e.g. data sharing, replication, access optimization, data transfers) to the ?smart data analytics process?. It offers a solid foundation to perform ?higher level analytics? based on concrete algorithms and tools without the need to tackle simple problems such as deploying data transfer protocols.

We observe that services like B2SHARE represent an important element in a seamless cooperation between general data analytics experts and scientific domain- specific experts for rather temporary scientific data results whereby only a fraction qualifies for long-term data preservation (e.g. using the B2SAFE EUDAT data service).

Secondly we are able to conclude that the CRISP-DM is well suited as a backbone for a process we referred to as ?smart data analytics? although being originally designed for traditional data mining and knowledge discovery.

Tailoring this model by describing large-scale compute and storage resources, clarifying the concrete machine learning algorithms and data mining techniques, tools, and service deployments make the approach feasible to guide the ?high productivity data processing? of large quantities of scientific data. These findings are considered to be part of a report compiled by the RDA BDA group.

Thirdly, we can conclude that the architectural setup of the method for automatic outlier detection is plausible, however, we also have to acknowledge that the work on tailoring the algorithm to the specific problem at hand requires many (parameter) configurations (e.g. within the  HPC system UNICORE). Nevertheless, we identified all the required elements and implementations in order to use the method in practice today. Tailoring the CRISP-DM process model, when performing outlier detection, is very useful too, because of many reasons one of which is the clarity of describing all relevant aspects of the concrete proposed method (resources, data, algorithms, tools, etc.).

This clarity is an important fact during the communication of the general data analytics expert and the PANGAEA scientists. Using an easy and open approach for sharing the data for validation purposes by using the B2SHARE EUDAT data service was generally viewed as an important element of the method.

We can further conclude that the architectural setup of using parallel SVMs for classification of multi-spectral image data is possible to implement today. Nevertheless, the parameter tuning and creating the classifiers is not straightforward and needs time to achieve reasonable results. These findings are important, but further raise the demand for several software packages that need to be maintained and installed on relevant underlying hardware resources (i.e. Twister and libSVM). Hence, achieving a speed-up required for ?big data? is possible, but also requires a distributed system (e.g. with map-reduce).

Using CRISP-DM and the B2SHARE EUDAT data service was equally useful in this case study as described in the concluding remarks of the previous case study.

