An efficient algorithm for frequent itemsets in data mining

ABSTRACT  Mining frequent itemsets is one of the most investigated fields in data mining. It is a fundamental  and crucial task.

Apriori is among the most popular algorithms used for the problem but support count is very time-  consuming. In order  to improve the efficiency of Apriori, a novel algorithm, named BitApriori, for mining frequent  itemsets, is proposed.

Firstly, the data structure binary string is employed to describe the database. The support count  can be implemented by  performing the Bitwise And operation on the binary strings. Another technique for improving  efficiency in BitApriori  presented in this paper is a special equal-support pruning. Experimental results show the  effectiveness of the proposed  algorithm, especially when the minimum support is low.

Keywords: data mining; frequent itemsets; Apriori; binary string    1. INTRODUCTION    Data mining is a widely-used process for discovering  information from large databases, within which  association rules discovery is one of the most popular  technologies. It was first formulated in 1993 by Argawal  et al. [1]. Data mining based on association rules can be  divided into two parts: finding all frequent itemsets, and  generating reliable association rules from all frequent  itemsets. Frequent itemsets mining plays an essential  role in association rules mining. It is a very  time-consuming procedure. Frequent itemset and  association rules mining problems have attracted  increasing research interest. In order to solve these  mining problems more efficiently, hundreds of research  papers have presented new algorithms or improvements  on existing algorithms. Frequent itemsets mining  appears as a subproblem in many data mining problems,  besides association rules, such as correlations,    classification, clustering, Web mining, and so on. Since  association rules are useful and easy to understand, they  have been successfully applied to many business fields.

None of the prevalent methods can outperform other  methods for all datasets with every support threshold.

However, there are three algorithms that are playing a  central role. These algorithms are Apriori [2], Eclat [3]  and FP-growth [4]. Due to their efficiencies, many of  the developed algorithms are modifications or  combinations of these basic methods.

The first frequent itemsets mining algorithm, AIS, was  published by Agrawal et al. [1]. A year later, Apriori [2],  one of the most noticeable algorithms, was proposed by  the same authors. Over the next few years, studies on  improvements or extensions of Apriori have been  extensive. Many modifications to Apriori have been  proposed. DHP (direct hashing and pruning), for the  initial candidate set generation, proposed in Pork et al.

[5], is widely viewed as an effective algorithm. This  method efficiently controls the number of candidate  2-itemsets, pruning the size of database. In Brin et al.

[6], the dynamic itemset count (DIC) algorithm is  proposed. For finding large itemsets, it uses fewer  passes over the data than classic algorithms, and yet  uses fewer candidate itemsets than methods based on  sampling (Brin et al. [7]). A sampling algorithm  proposed in Toivonen [8] reduces the number of  database scans to a single scan, but still wastes  considerable time on candidate itemsets. Partitioning  technique is introduced by Savasere et al. [9].

Cluster-based association rule (CBAR) creates cluster  tables by scanning the database once in Tsay et al. [10].

The algorithms support count is performed on the  cluster table and it need not scan all transactions stored  in the cluster table.

Recently, a FP-growth algorithm is proposed by Han et  al. [4]. FP-growth is a depth-first search algorithm that  scans the database only two times. The data structure  FP-tree is used for storing frequency information of the  original database in a compressed form. No candidate    generation is required in the method. Dynamically  considering item order, intermediate result  representation, and construction strategy, as well as tree  traversal strategy, are introduced in Liu et at. [11]. An  array based implementation of prefix-tree-structure for  efficient pattern growth mining is proposed in Grahne et  al. [12].

In 1996, Zaki et al. [13] developed algorithm Elcat. In  Elcat, database is vertically represented. Later, dElcat  was introduced in Zaki et al. [14]. It employs a  technique, called diffset, for reducing memory  requirement.

Recently, BitTableFI with data structure BitTable has  been introduced by Dong et al. [15]. BitTable is used  horizontally and vertically to compress database for  quick candidate itemsets generation and support count,  respectively. Later, in Song et al. [16], Index-BitTableFI  employed index array to improve the algorithm.

978-1-4244-6487-6/10/$26.00 2010 IEEE  In this paper, the Trie, that has been shown to be a very  efficient data structure for mining frequent itemsets in  Ferenc [17], is employed to record frequent itemsets. In  Apriori, on one hand, support count is the most time  wasting process. On the other hand, candidate  generation dominates the run-time, which consumes  large quantities of processing memories, particularly in  dense, medium-size databases. In order to overcome  these issues, in this paper, a novel data structure, binary  string, is added into each leaf of the Trie, which  compresses the database for quick support counting and  avoiding candidate itemsets generation. An  equal-support pruning, based on two important  properties introduced in Ferenc [18], is employed in  BitApriori. It contributes to reducing not only the  execution time but also the required memory, for the  procedure. Experimental results show that equal-support  pruning is very efficient in BitApriori.

There are four major advantages in BitApriori. First, it    needs to scan the database only twice. Second, the  equal-support pruning contributes to reducing the height  of the Trie. Third, only frequent items in each  transaction are inserted as nodes into the Tree, and their  support count can be implemented by performing &  operation, on the corresponding binary strings. Finally,  all frequent itemsets are generated by checking the  leaves of Trie, without traversing the tree recursively,  which significantly reduces computing time.

The rest of this paper is organized as follows. Section 2  briefly revisits the problem definition, of frequent  itemset mining. Two important properties are listed in  Section 3. In Section 4, we propose the BitApriori  algorithm. Section 5 analyzes the performance of the  proposed algorithm. Section 6 concludes this study and  points out some directions for future research.

2. PROBLEM STATEMENT    Frequent itemsets mining can be formally stated as  follows. Let I be a finite set of symbols called items.

Any subset X I?  is called an itemset. An itemset  with k items is called a k-itemset. Let D be a set of  transactions, where each transaction T is a set of items,  such that T I? . For any itemset X I? , the set of  transactions that contains X is defined as the cover of X.

( ) { | }Dcover X t D X t= ? ?         (1)  The support of X is the number of elements in set  coverD(X), and the proportion of transactions in D, that  contain X, is denoted as sup(X).

| ( ) |  sup( )  | |  Dcover XX  D  =  (2)  An itemset X is called frequent if sup(X) is greater than  or equal to some given percentage of min_sup, where  min_sup is called the minimum sup. The set F is the set  of frequent itemsets.

{ | sup( ) }F X I X min_sup= ? ?      (3)  The task of frequent itemset mining (FIM) is to  compute the set F of all frequent itemsets for the given  database D and a support threshold min_sup.

3. TWO IMPORTANT PROPERTIES    In this section, two important properties are introduced,  which are used for equal-support pruning in BitApriori.

Property 3.1 Let , ,X Y Z I?  and X Y? . If  sup( ) sup( )X Y= , then sup( ) sup( )Y Z X Z? = ? .

Proof. For X Y? , so ( ) ( )D Dcover Y cover X? . If  sup( ) sup( )X Y= , according to definition of sup, then  | ( ) | | ( ) |D Dcover X cover Y= . So ( ) ( )D Dcover X cover Y= ,  ( ) ( ) ( ) ( )D D D Dcover X cover Z cover Y cover Z? = ?  can  be deduced. For  ( ) ( ) ( )D D Dcover X Z cover X cover Z? = ? ,  ( ) ( ) ( )D D Dcover Y Z cover Y cover Z? = ? , that means  ( ) ( )D Dcover X Z cover Y Z? = ? . So  sup( ) sup( )Y Z X Z? = ? .

If the support of a frequent itemset { }X i?  is equal to  the support of X, then support of every superset  { }X i Z? ?  is equal to the support of X Z? . So  there is no need to generate all such supersets anymore;  one only has to keep the information, such that every  superset of { }X i?  is represented by a corresponding  superset of X.

Property 3.2 Let Y be the prefix of itemset Y z? ,  where | | 1z = . If Y has a subset X such that  | | 1 | |X Y+ =  and sup( ) sup( )X z X? = , then  sup( ) sup( )Y z Y? = .

Proof. Derived from Property 3.1.

Based on property 3.2, a special equal-support pruning  can be designed. The equal-support pruning has two  main steps. First, before the support of a candidate  itemset Y z?  is counted, support of the itemset    Y z?  is equal to the support of the frequent itemset Y,  if one of its subset X has the same support with X z? .

The frequent itemset Y z?  is pruned, and put in the  equal-support set of its parent, frequent itemset Y.

Second, before a new frequent itemset is inserted into  the Trie, it is pruned and put in the equal-support set of  its parent, if its support is equal to its parent.

Lemma 3.1 When checking all subsets of a  (k+1)-itemset, no equal-support sets of nodes at depth d,  for all d<k+1, need to be considered, if the special  equal-support pruning based on Property 3.2 is  employed (see Ferenc [18]).

Proof. This statement can be proved by contradiction.

As we all know, if a frequent itemset is generated as a  node in the Trie, all its subsets have been generated as a  node already. Let the candidate denoted by P,  | | 1P k= + . One of its subsets is Q, Q={i1, i2, , ij, , ik}  i?I. Let Q={i1, i2, , ij-1} and E={i1, i2, , ij}. If ij is  in the equal-support set of 'Q , \E P Q?  has been  pruned at iteration 1j + . But \E P Q?  is a subset of  P, so P cannot be a candidate itemset.

In Apriori, the special equal-support pruning based on  Property 3.2 can be employed. Before the  (k+1)-candidate itemset is generated, the Apriori has to  check if all k-subsets of the candidate itemset are  frequent. An extra checking at the (k-1)-element  prefixes of k-subsets can be added, to apply the  equal-support pruning. If one of the k-subsets is in the  equal-support set, the candidate itemset can be pruned  immediately, and put in the equal-support set of its  parent. But to use the pruning based on Property 3.2, it  is required to check if the support of the newly  generated frequent itemset is equal to its parent. So in  Apriori, after support count, it must add an additional  tree traversal to check if the support of the new leaf is  equal to that of its parent. In BitApriori, the additional  tree traversal is not necessary.

4. THE PROPOSED BITAPRIORI ALGORITHM      The data structure and implementation related  techniques introduced in [19, 20] can also be employed  by BitApriori, which is an Apriori-like algorithm. The  main differences between Apriori and BitApriori lie in  candidate itemsets generation and support count  approach. These two steps consume the most time and  memory in the Apriori algorithm. The time required for  mining frequent k-itemset grows significantly when k  increases in Apriori. But BitApriori performs much  better because it has no candidate generation and needs  to traverse the Trie only once. The pseudocode for  BitApriori is shown in Table 1.

TABLE 1 THE PSEUDOCODE FOR BITAPRIORI  Input: dataset D, min_sup  Output: frequent itemsets  1: Scan database D once and delete infrequent items;  2: Sort frequent single items into nondecreasing order by supports;  3: Scan database D again and find all frequent 2-itemset, then establish  the Trie with the binary string in each leaf;  4: k ? 3;  5: while the height of the Trie is increased do  6:     generation(k);  7:     k ? k+1;  8: end while  9: write out the frequent itemsets from Trie;  As shown in Table 1, in Step 1, the database is scanned,  the support of each item is counted, and infrequent  items are deleted. In Step 2, frequent single items are  sorted into a nondecreasing order, according to their  respective supports. The database is scanned again to  find all frequent 2-itemsets, and record each of the  frequent 2-itemsets with a corresponding binary string.

After that the Trie with the binary string in each leaf is  established. Then the next layer of the Trie is generated,  until the height of the Trie is not increasing. In the last  step, the frequent itemsets are written out from the Trie.

TABLE 2 THE PSEUDOCODE FOR THE PROCEDURE GENERATION(K)  Procedure: generation(k)  1: for each node p in the (k-1)-layer of the Trie do  2:     for each node q?{brother nodes of p at right side} do  3:         check (k-1)-subsets;    4:         if equal-support do  5:             put the item in node q into the equal-support set of p;  6:         end if  7:         else if infrequent do  8:             continue;  9:             end if  10:        else  11:            string s ? (string in p) & (string in q);  12:            count ?The number of 1 in s;  13:            if count < min_sup*|D| do  14:                nothing;  15:            end if  16:            else if count is equal to the support in p do  17:                put the item of node q into the equal-support set of  p;  18:                end if  19:            else  20:                add a son node to p, recording count and the item in  q;  21:            end else  22:        end else  23:    end for  24:    delete the binary string in node p;  25:end for    The pseudocode of the procedure generation(k) is given  in Table 2. The k-layer of the Trie is generated. Each  node p, in the ( 1)k ? -layer of the Trie, may be  combined with one of its right side brother nodes q. All  ( 1)k ? -subsets are checked. If one of its ( 1)k ? -subsets  is in the equal-support set, the item in node q is put into  the equal-support set of p. If one of the ( 1)k ? -subsets  is infrequent, all of its supersets are infrequent. If none  of the ( 1)k ? -subsets is infrequent, or in the  equal-support set, the & operation between the string  of node p and q is performed. The support of the  candidate itemset is the number of 1 in the result  string. If the support is equal to p, the item in node q is  added into the equal-support set of p. If the support is  not equal to p, or is larger than the minimum support, a  new son node of p, recording the support and the item in  node q, is inserted into the Trie.

To demonstrate the process of BitApriori, an example is  given, as follows. As shown in Table 3, the example  database is in the second column. In the database, there  are 10 transactions.

TABLE 3 THE EXAMPLE DATABASE  TID Items Ordered frequent items  1 A B D E F L A E  2 A G O G A  3 C E I C E  4 A C D E G G C A E  5 A B C E G K G C A E  6 E H E  7 A B C E F J C A E  8 A C D C A  9 A C E G M G C A E  10 A C E G N G C A E  Suppose the support threshold min_sup is 40%. The  support of each item is counted, and infrequent items  are deleted, during the first scan of the database. The  support of each item is given as follows.

A:8, B:3, C:7, D:3, E:8, F:2, G:5, H:1, I:1, J:1, K:1, L:1,  M:1, N:1, O:1  Since the minimum support is 4, frequent items are  sorted into a nondecreasing list, according to their  respective supports. And if two items have the same  support, they will be sorted according to their  lexicographic order. The result is shown in Table 4.

TABLE 4 FREQUENT ITEMS  Item G C A E  support 5 7 8 8  TABLE 5 FREQUENT 2-ITEMSETS  TID Ordered  items  {G,  C}  {G,  A}  {G,  E}  {C,  A}    {C,  E}  {A,  E}  1 A E 0 0 0 0 0 1  2 G A 0 1 0 0 0 0  3 C E 0 0 0 0 1 0  4 G C A E 1 1 1 1 1 1  5 G C A E 1 1 1 1 1 1  6 E 0 0 0 0 0 0  7 C A E 0 0 0 1 1 1  8 C A 0 0 0 1 0 0  9 G C A E 1 1 1 1 1 1  10 G C A E 1 1 1 1 1 1  sup  4 5 4 6 6 6  In Step 2 of BitApriori, all frequent 2-itemsets are found,  as shown in Table 5. The Trie with the binary string  shown in each leaf is established, which is shown in Fig.

1.

5 8 8  G C A E A   C    E    A    E    E      Fig. 1.  Trie after generation(2)  The next step is to generate the third layer of the Trie.

First, itemset {G, C, E} is checked. All of its subsets are    frequent, and there is no equal-support pruning in its  subsets. So the Bitwise And Operation on binary  strings of 0001100011 and 0001100011 is employed. As  the result is 0001100011, the support of the itemset {G,  C, E} is 4, which is equal to {G, C}. E is put into the  equal-support set of {G, C}, and does not need to add a  new son node. When itemset {C, A, E} is checked, all  of its subsets are frequent, and none of them is in the  equal-support set. The corresponding two bit strings,  0001101111 and 0011101011, are operated by &. The  resultant string is 0001101011. The number of 1 in  string 0001101011 is 5. So the support of itemset {C, A,  E} is 5, which is not equal to the support of {C, A}. So  a new son node is added to {C, A}. The Trie after  generation(3) is shown is Fig. 2.

5 8 8  G C A E A   C   E   A   E   E    E   E    Fig. 2. Trie after generation(3)  In generation(4), there is only one node in layer 3 and,  therefore, the height of the Trie does not increase. All  frequent itemsets are in Fig. 2. In the last step, all  frequent itemsets are written out according to the Trie.

5. EXPERIMENTAL RESULTS      The proposed algorithm is tested on all the five datasets,  prepared by Roberto Bayardo, from UCI and PUMSB  datasets. The datasets are available at  http://fimi.cs.helsinki.fi. The characteristics of the  datasets are shown in Table 6. The first column contains  the names of the datasets. The second column shows the  number of items contained in each dataset. The third  column shows the average length of each transaction,  and the last column indicates the total number of  transactions in each dataset.

TABLE 6 DATABASE CHARACTERISTICS  Datasets Items Avg. length Records  mushroom 119 23 8,124  chess 75 37 3,196  pusmsb* 2,088 50.4 49,046  connect 129 43 65,557  pusmsb 2,113 74.0 49,046  In order to illustrate the performance of the proposed  algorithm, BitApriori is compared to the fast Apriori  implemented in Ferenc [20], and another similar,  recently published, algorithm Index-BitTableFI,  proposed by Song [16]. In order to show the efficiency  of the pruning technology employed in BitApriori,  another algorithm BitAprioriNE, which is the same as  BitApriori, except that it does not use the special  equal-support pruning, is designed. All of the above  four algorithms are implemented in C++ and compiled  with Microsoft Visual C++ 6.0. The experiments are  performed on a Windows XP PC equipped with a  Pentium 2.0 GHz CPU and 1.5 GB of RAM memory.

For each dataset, a mass of different support thresholds  are tested, and the five most important of them are  chosen for reporting in this paper. The experimental  results are shown in Fig. 3-7. In the figures, the  y-coordinate denotes the execution time (in seconds),  while x-coordinate denotes the support threshold.

0.05 0.06 0.07 0.08 0.09 0.1 0.11  Apriori  BitApriori  BitAprioriNE  Index-BitTableFI    Fig.3 Execution time comparison on mushroom          0.45 0.5 0.55 0.6 0.65 0.7 0.75  Apriori  BitApriori  BitAprioriNE  Index-BitTableFI    Fig.4 Execution time comparison on chess             0.25 0.3 0.35 0.4 0.45 0.5 0.55  Apriori  BitApriori  BitAprioriNE  Index-BitTableFI    Fig.5 Execution time comparison on pusmsb*           0.65 0.7 0.75 0.8 0.85 0.9 0.95  Apriori  BitApriori  BitAprioriNE  Index-BitTableFI    Fig.6 Execution time comparison on connect          0.675 0.725 0.775 0.825  Apriori  BitApriori  BitAprioriNE  Index-BitTableFI    Fig.7 Execution time comparison on pusmsb    As shown in Fig. 3, BitApriori outperforms all other  algorithms, and the dominance is apparent. In this  dataset, the special equal-support pruning works  efficiently. In Fig. 4, BitAprioriNE beats Apriori and  Index-BitTableFI. In Fig. 5, the effectiveness of the  proposed algorithm is verified, especially when the  minimum support is low. In Fig. 6, BitAprioriWE  exhausts the memory when the support threshold is  lower than 0.8, but BitApriori does not. That means the  special equal-support pruning contributes to save the  memory. In Fig. 7, when the threshold is larger than  0.725, Apriori beats the BitApriori. But BitApriori  outperforms the Apriori, when the threshold is lower  than 0.725.

Apriori does not use the binary string and the special  equal-support pruning. The BitTable is employed in  Index-BitTableFI, but there is no special equal-support  pruning, except for the frequent 2-itemsets.

BitAprioriNE outperforms Apriori in Fig. 3-5, but not in  Fig. 6-7, because of the limitation of memory.

BitAprioriNE does better than Index-BitTableFI in Fig.

4 and Fig. 5. In the mushroom, there is a vast number of  equal-support itemsets for frequent 2-itemset. So  Index-BitTableFI outperforms BitAprioriNE.

On one hand, the special equal-support pruning is a  useful technique for improving efficiency. The  performance is improved significantly, especially when  the databases contain many equal-support itemsets, such  as the mushroom. It also reduces memory requirement.

On the other hand, the technique of binary string in  BitApriori improves the efficiency of Apriori. These  two techniques combine perfectly in BitApriori. So  BitApriori has very good performance. It is the best for  all of the five datasets.

6. CONCLUSIONS    In this paper, two effective techniques are employed to  improve the performance of Apriori, by reducing the  cost of candidate generation, and by support counting.

The two effective techniques are integrated perfectly in  BitApriori, and improve the computational efficiency  significantly. Experimental results have shown that  BitApriori outperforms the fast Apriori and  Index-BitTableFI, especially when the minimum  support threshold is low.

When the database is large, the BitApriori may suffer  the problem of memory scarcity. So how to solve the  memory problem will be the question addressed in one  of our future works. And another work is to improve  Bitwise And Operation on the binary string, or replace it  by some more effective techniques.

