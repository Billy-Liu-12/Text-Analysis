An Improved Random Sampling Approach for Large Data Set Mining  Yuenan Zhang

Abstract?Though the association rule mining algorithms has simple ideas, their computations are large. It may cause that the existing frequent item set increases exponentially with the increasing of items number. Many sampling algorithms have low accuracy and they need to solve a series of NP hard problems.

Therefore, based on the research of existing sampling algorithms, a new hierarchical dichotomy sampling algorithm is proposed. It performs sampling on existing data before each mining. The frequent k-item set can be partitioned as averagely as possible during the average partition of data, to ensure the rules with high accuracy can be mined through fewer data. The experiments verify that the improved algorithm has superior performance on accuracy , compared to traditional algorithm, and it also shows advantage in the running time.

Keywords- large data set; frequent itemset; random sampling; hierarchical dichotomy; association rule

I. INTRODUCTION With rapid increase of data amount, the database is not a  previously small scaled database and it develops into a multi-dimensional massive one. Original data mining method can not obviously solve massive data problem so the sampling algorithm of large data sets appears at the same time. The sampling algorithm can effectively and rapidly mine frequent item set under condition that satisfies certain mining accuracy. Thus, data mining algorithm can analyze the sampling data which is far smaller than original big data set to largely improve the performance. The proposed sampling algorithms include simple random sampling, sequential sampling, two-stage sampling and hierarchical dichotomy sampling [1-3]. Simple random sampling refers to randomly select some data in original data set as the training sample which generates initial candidate set of association rules. Then, the rest of data is applied to verify and adjust the rules in candidate set. This method is simply realized. When data distribution is uneven, the sampling bias is large and the accuracy of mined association rule is not high. Sequential sampling is a gradually completed process of checking association rules. Initially, the algorithm does not fix the sample size and then reduces the range of the selected rule set through sequential extraction sample to further gradually eliminate candidate rule set [4].

Hierarchical dichotomy sampling is the new sample to be sampling the sample data under certain condition so the transaction number of new sample is 1/2 as much as original sample to iterate till the final sample is generated.

Hierarchical dichotomy reduces the samples quantity according to average division of one item set in sample. It adopts frequency 1-item set to compute the principle  coefficients which are taken as the judging condition for the end of algorithm. This cannot guarantee successive mining of algorithm, resulting in low accuracy.

Based on the analysis of current sampling algorithms and the improvement with integration between simple random sampling and hierarchical dichotomy, an improved hierarchical dichotomy algorithm is proposed in this paper.

While frequent n-itemsets is used to perform average division on sample to ensure average division of data.

Frequent n-itemsets can also be divided in average. Thus, the mining accuracy can be improved to a large extent and the maximal frequent item set can also be obtained when the algorithm comes to an end. Meanwhile, random sampling adopts the randomly selected exchanging method to ensure the sampling randomness more effectively.



II. IMPLEMENTATION OF ASSOCIATION RULE IN LARGE DATA SET MINING  With the increase of data amount, the traditional mining method can not meet people?s demand, so the sampling algorithm for large data set appears. The mining of sampling method which is applied in association rule is proposed by Hannu Toivonen [5] and he put forward a sampling-based association rule mining algorithm. The basic principle of algorithm is obtaining a random sample at first and perform association rule mining on samples. Then it utilizes the rest of data to check the obtained rules. However, the obtained rules may not be the total rules of original data so database needs to be scanned again to obtain the total rules. The sampling methods applied to association rule mining become more and more. The representative algorithms among them include two-stage sampling algorithm, sequential sampling of association rule, negative border- based sampling algorithm and the distributed sampling algorithm [6].

The principle method of random extraction comparison method is: during the first the extraction, it creates a random number r  in [1,n]. Extract ra  from  1 2 1, ,..., na a a ? and exchange ra  with the last element; during the second extraction it first crate a random number r  . Extract ra  from 1 2 1, ,..., n ia a a ? ?  and exchange ra with 1n ia ? ? ; during the thi  extraction, it creates a random number r  in [1,n-i+1]. Extract ra  from 1 2 1, ,..., n ia a a ? ? and exchange ra  with 1n ia ? ? . So it continues until m times of extraction complete. The m  elements located in the back of array The improved scheme for hierarchical dichotomy method is: under certain  condition we perform sampling on acquired samples database to get new samples.

Then the number of elements in new sample become half of the original sample. Based on the new sample we make   DOI 10.1109/ICSCSE.2016.156    DOI 10.1109/ICSCSE.2016.156     perform mining of frequent itemset, ensuring average division of frequent 1-itemset when data samples are divided averagely. The sampling  end is determined by criterion coefficient 1 11 1 1| | /  j j je L L L? ?? ?

III. IMPROVED SAMPLING ALGORITHM  A. Principle Idea In terms of big data association rules mining, massive  data is stored in the database. Random extraction comparison method will trigger large amount of read-write operation in database to affect mining efficiency [7].

Therefore we adopt the improved algorithm of random extraction exchange method. When the elements are selected each time, the changing relationship of element position is recorded to determine the generated random number which represents the position of the extracted elements in source array. However, this is not the genuine exchange between elements but a ?virtual exchange?. The improved algorithm is based on average division of frequent 1-itemset for sampling. Thus, after each sampling, the data quantity in new sample is 1/2 as much as original sample. After ith times, while ensuring the average division of sampling data, frequent i-itemsets can be averagely divided as much as possible, to ensure data mining in small size with high accuracy. The improved random sampling algorithm simplifies the mining procedures while largely improving the mining accuracy. After final sampling we can obtain the largest frequency itemset and test the largest frequency itemset. The follows provides an example of basic theory of improved sampling algorithm:  (1)  Supposing  there  are   n     elements  in  array a[1...n]    and  array   b[1...m]    stores the corresponding subscript of extracted elements;  (2)  Create random number in [1,n-i+1] and assign it to b[i] . It corresponds to  the  3rd   row in table 1;  (3)  According to virtual exchange table, perform scanning from the tail to head of  the  table,  to  determine the  subscript  of actually  extracted  elements corresponding to  each random number one by one;  (4)  The values in  b[i]   are needed subscripts of extracted elements.

Table1. Virtual exchange relational table of element location Times Range  Random number  Virtual exchange  1 [1,n] R1 R1 n 2 [1,n-1] R2 R2 n-1 3 [1,n-2] R3 R2 n-2  ? ? i [1,n-i+1] Ri Ri n-i+1  ? ? ? ? m [1,n-m+1] Rm Rm n-m+1  B. Algorithm Description Then , the detailed procedures of hierarchical dichotomy  sampling algorithm (HDS) is described as follows: Step 1: Scan original transaction database D  and mine  the frequent 1-itemset with  the  minimum  support  degree  minsup,  to acquire the set of items, that is, 1 1 2{ , ,..., }qL i i i? .

Step 2: Rearrange frequent 1-itemset in 01L by support degree descendingly and the result is still recorded as  1 1 2{ , ,..., }qL i i i? ;  Step 3: Perform the first sampling from D  to get sample 1D . So the data quantity of new samples becomes 1/2 of the  original samples, that is, 1| | | | /2D D?  . According to minsup, sample  1D is mined with 2-itemsets to get set  2L ;  Step 4: Compute the criterion coefficient e  . If  0e e? , perform the second sampling for 1D ; otherwise this sampling is invalid and the association rules mining begins with previous data set D ;  Step 5: If the sample data jD can be acquire after thj sampling, 1| | | | /2j jD D ?? , we will perform mining with frequent (j+1)-itemsets to get the set 1 1jL ?  Step 6: Repeat such procedures until the sample is completed, Then we can get the maximum frequent itemset and the last extracted sampling data 0D  . Meanwhile, 0D  is mined for the maximum frequent itemset with Apriori and the results can be tested.

In above procedures, step 3 is the first sampling process.

Step 5 and 6 are final sample data from constant sampling operation. Algorithm sampling constantly uses threshold value e to judge and this reflects in our processes step 4 and 6. Under condition of 0e e? , the constant sampling which adopts random sampling improvement reduces the scale of transaction database and finally obtains the smaller transaction database that fully represents original data.

Meanwhile, the mining method of frequency itemset can adopt any method to be implemented. The process of improved algorithm can quickly and accurately obtain frequency itemset in theory, since the frequent itemset is divided averagely during the process of data average division, in each part. Finally, the rest of data without containing frequent item set is also sampled. Actually, frequent item set and total data are semi-operated so data distribution is not changed to some extent.

C. Algorithm Realization Input :Original large data set D  to be mined, minsup  and 0e Output: The maximum frequent itemset and final  samples satisfying minsup and 0e Algorithm HDS { i=1;  1iD D? ?     Do { Miming (Di-1, minsup, i); // mine frequent i-itemsets  according to D and minsup  1 2{ , ,.... }i qL A A A? ; //mined frequent i-itemsets { }i iL sort L? ; // rearrange items by support degree  descendingly  1( , )i i iD getsample D L??  //  acquire Di by Di-1and Li Miming (Di, minsu, i); *  1 2{ , ,.... }i pL A A A? ; mined frequent i-itemsets *| | / | |i i ie L L L? ? ;  i++; } while ( 0e e? ) The realization of sampling algorithm getsample: Input: Original data set T to be extracted, frequent  itemset Li ={ 1 2, ,.... qA A A } sorted  by previous miming; Output: sample data T* after sampling;  Getsample { T*=null; // Initialize T* as null For (int i=1; i<=q; i++)     // perform sampling for each  item in set L { Ti =get Data(T,A i );   //find transactions contain Ai T=T- Ti ; T*= T*+Ti; } T*= T*+T } Method getdata is the kind of algorithm to find out  certain items from the data set.



IV. EXPERIMENT AND RESULTS ANALYSIS To analyze the operating efficiency of HDS algorithm,  We adopt JAVA language realize the algorithm. Operating platform includes Intel(R) Core(TM)2 CPU E7500 2.93GHz 2GB memory, Windows XP, database SQL Server 2005 and operating software JDK+eclipse. The experiment data is randomly generated through JAVA language and we compare the results in time cost and accuracy with various conditions among HDS algorithm, random sampling algorithm (RS) and Apriori algorithm. In order to compare the performance of operating time and accuracy between Apriori algorithm and sampling algorithm, it is supposed that the database size is constantly changing. We set the minimal support degree is 10 percent, the minimum threshold is 0.59 and the rows in database record are changed from 100 thousands to 500 thousands.

When the data set is too large to perform mining, the only method is sacrificing time for accuracy. The sampling algorithm applies smaller samples to replace original data sample to realize mining so it can obviously improve mining efficiency. From figure 1 we can see, the operating  time of sampling algorithm is obviously less than Apriori algorithm so sampling algorithm takes advantage of its time.

Compared to mining efficiency of these two sampling algorithms, the operating time of RS algorithm is a bit less than HDS algorithm. Since HAC algorithm is sampled according to frequent 1-itemset, it only needs to mine frequent 1-itemset to test the obtained samples after new samples. However, HDS algorithm takes sampling according to frequent n- itemset so it will use frequent n- itemset of sample for test after each sampling. It will be a bit slow in time with little difference.

10 15 20 25 30 35 40 45 50         The size of database(thousands) Ti  m e/  s    Apriori RS HDS  Figure 1. The comparison of time cost of HDS, RS and Apriori  According to figure 2, the mined frequent itemset of Apriori is 1. The difference degree of HDS algorithm is lower than RS algorithm, that is, the nearer between EHAC algorithm and Apriori algorithm, the higher the accuracy.

Therefore, the accuracy of HDS is obviously improved compared to RS, when adopting the sampling strategy based on frequent n-itemset sampling.

10 15 20 25 30 35 40 45 50  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9   The size of database(thousands)  D iff  er en  ce d  eg re  e     Apriori RS HDS  Figure 2. Difference degree comparison of HDS, RS and Apriori  The  mined  frequent  itemsets without sampling are the total frequent itemsets. Therefore, we set the difference degree as g  and Apriori as the base when comparing the difference degree. Then the difference degree of HDS and Apriori will be described as  ( ) / ( )g count E A count A? ? . ( )ount E A?     denotes the number of frequent 6-itemsets intersection of HDS and Apriori respectively;  ( )count A  denotes the number of frequent 6-itemsets of Apriori. Similarly, the difference of DS and Apriori can be described as  ( ) / ( )g count H A count A? ? .

The accuracy of HDS is superior to DS while its running  time is slower than the latter. We can not draw a conclusion when comparing the performance directly. So we set a object function to consider the two factors, to quantify the difference of two algorithms. Apriori algorithm is set as the base. We assume the object function  * * 1 2( , ) ( , ) ( , )f x y c t x y c e x y? ? ? ? . x and y correspond  to the algorithms in comparison. t?  and e?  represents the difference of time and accuracy, and c1 and c2 (c1+c2=1) represents the proportion of time and accuracy during the comparison of two algorithms. For Apriori, the algorithm with less accuracy difference and larger time difference is better, so the difference degree of accuracy should take negative value. The object function evolves into  1 2( , ) ( , ) ( , )f x y c t x y c e x y? ? ? ? . First we compute the time difference of HDS and Apriori as ( , )t E A? , which is also the average of five groups of difference. Next , we compute the accuracy difference of HDS, RS and Apriori, considering ( , ) ( , ) ( , )f E H f E A f H A? ? . If  ( , ) 0f E H ? , it shows HDS is better than RS. For an intuitive effect, we depict the line ( , )f E H in the coordinate axis, as shown in figure 3.

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 -0.05   0.05  0.1  0.15  0.2  Figure 3. Objection function of HDS algorithm  Figure  3  shows  the  performance  of  HDS  is  superior to  RS  algorithm  when 10 0.8c ; when  1 0.8c ? , the performance of HDS is lower than RS. However,  we  will not just consider the time factor when taking the performance of algorithm into  account.  If  we  sacrifice accuracy  to  exchange  for  time,  the  mining  results  may be  meaningless.  Generally  we  assume  the  running  time and  miming  time  are  equally  important , so the overall performance of HDS algorithm is better.



V. CONCLUSIONS Our study proposes an improved random sampling  algorithm based on frequent n-itemset. The algorithm has higher mining accuracy than the sampling algorithm which only depends on frequent 1-itemset. Furthermore, HDS algorithm can obtain the maximized frequent itemset and simplify the sampling procedures. Meanwhile, users can adjust mining accuracy and efficiency through changing principle coefficient. The analysis in theory and experiment show that the flexibility and high accuracy of HDS sampling algorithm are better, compared to Apriori algorithm and traditional random sampling algorithm. Meanwhile, the difficult NP of two-stage sampling and other sampling algorithms are also avoided. Since principle coefficient selection will directly affect mining efficiency and accuracy.

Thus, reasonable threshold value is determined and equilibrium point will be sought between efficiency and accuracy through lots of future experiments.

