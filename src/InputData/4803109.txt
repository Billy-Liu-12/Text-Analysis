Proceedings of International Workshop on Data Mining and Artificial Intelligence (DMAI' 08) 24 December, 2008, Khulna, Bangladesh

Abstract  Web mining is defined as applying data mining techniques to the content, structure, and usage of Web resources. The three areas of Web mining are commonly distinguished: content mining, structure mining, and usage mining. In all these areas, a wide range ofgeneral data mining techniques, in particular association rule discovery, clustering, classification, and sequence mining, are employed and developed further to reflect the specific structures of Web resources and the specific questions posed in Web mining. In this paper, we introduced a web document clustering approach that uses WordNet lexical categories and fuzzy c-means algorithm to improve the performance ofclustering problem for web document.

Experiments show that Fuzzy c-means algorithm achieves great performance optimization with comparison with the recent algorithms for document clustering.

1. Introduction  The Web is a huge collection of documents that comprises some features like (1) semi-structured (HTML, XML) information, (2) hyper-link information, (3) access and usage information and (4) dynamic page generation. The Web has made cheaper the accessibility of a wider audience to various sources of information. The advances in all kinds of digital communication have provided greater access to networks. It has also created free access to a large publishing medium. These factors have allowed people to use the Web and modem digital libraries as a highly interactive medium [3].

Web mining is the application of data mining techniques to the content, structure, and usage of Web resources. It is thus "the nontrivial process of identifying valid, previously unknown, and potentially   useful patterns" [8]. Web mining application works on huge amount of Web data to extract some patterns that can describe this data. Like other data mining applications, Web mining can profit from given structure on data (as in database tables), but it can also be applied to semi-structured or unstructured data like free form text. This means that Web mining is an invaluable help in the transformation from human- understandable content to machine-understandable semantics.

Web content mining can be used to detect co- occurrences of terms in texts. For example, co- occurrences of terms in newswire articles may show that "gold" is frequently mentioned together with "copper" when articles concern Canada, but together with "silver" when articles concern the US. Trends over time may also be discovered, indicating a surge or decline in interest in certain topics such as the programming languages "Java". Another application area is event detection: the identification of stories in continuous news streams that correspond to new or previously unidentified events [1].

Web content mining analyzes the content of Web resources. Today, it is mostly a form of text mining that can be defined as the process of extracting interesting information and knowledge from unstructured text. Text mining can be also defined as the application of algorithms and methods from the machine learning and statistics fields to texts with the goal of finding useful patterns [4]. In this paper, we deal with web content mining as an application of text mining process for input web documents.

One main reason for applying data mining methods to text document collections is to structure them. A structure can significantly simplify the access to a document collection for a user. Well known access structures are library catalogues or book indexes.

However, the problem of manual designed indexes is the time required to maintain them. Therefore, they are    Figure I: Fuzzy Web Document Clustering Approach  2.1. Feature Extraction  2. Web Document Clustering Approach  Documents' Vectors  ;:===------.

I WordNet I I Lexical I I II  .~ Clustered Documents  The first step in the proposed approach is feature extraction or documents preprocessing which aims to represent the input web documents collection (corpus) into vector space model. In this model a set of words (terms) is extracted from the corpus called "bag-of- words" and represent each document as a vector by the words (terms) it contains and their weights regardless of their order. Documents preprocessing step contains five sub-steps: Text Extraction, PoS Tagging, Stopword Removal, Stemming, and WordNet Lexical Category Mapping.

The second process is Document Clustering that applies fuzzy c-means algorithm on document vectors to obtain output clusters.

2.1.1. Text Extraction. The first step of the feature extraction process is to extract textual data from the  In this section we describe in details the components of the proposed fuzzy web document clustering approach. There are two main processes: first is Feature Extraction that generates output document vectors from input text documents using WordNet lexical information as illustrated in figure 1.

The rest of this paper is organized as following; section 2 show the proposed web document clustering approach. In section 3 a set of experiments is presented to compare the performance of the proposed approach with current document clustering algorithms. Related work is discussed and presented in section 4. Finally, conclusion is given in section 5.

r=-.:.:::.-----:...

I Web I : Documents : Q -------,."  very often not up-to-date and thus not usable for recent publications or frequently changing information sources like the World Wide Web. The existing methods for structuring collections either try to assign keywords to documents based on a given keyword set (classification or categorization methods) or automatically structure document to find groups of similar documents (clustering methods).

Text document clustering can be defined as the process of grouping of text documents into semantically related groups [2]. Clustering methods aims to form the input documents into a set of clusters.

Each cluster consists of number of documents that are similar to each other and dissimilar to documents of other clusters.

Many well-known methods of text mining have two problems: first, they don't consider semantically related words/terms (e.g., synonyms or hyper/hyponyms) in the document. For instance, they treat {Vehicle, Car, and Automobile} as different terms even though all these words have very similar meaning. This problem may lead to a very low relevance score for relevant documents because the documents do not always contain the same forms of words/terms.

Second, on vector representations of documents based on the bag-of-words model, text mining methods tend to use all the words/terms in the documents after removing the stop-words. This leads to thousands of dimensions in the vector representation of documents; this is called the "Curse of Dimensionality". However, it is well known that only a very small number of words/terms in documents have distinguishable power on clustering documents [9] and become the key elements of text summaries. Those words/terms are normally the concepts in the domain related to the documents.

Recent studies for fuzzy clustering algorithms [25, 26, 27, 28] have proposed a new approach for using fuzzy clustering algorithms in document clustering process. But these studies neglect the lexical information that can be extracted from text as we used WordNet lexical categories in our proposed approach.

In this paper, we introduced an efficient approach web document clustering approach that uses WordNet lexical categories and fuzzy c-means algorithm to improve the performance of clustering problem for web document. The proposed approach uses WordNet words lexical categories information [12] to reduce the size of vector space and present semantic relationships between words. The generated document vectors will be input for the clustering process. In the clustering process we use fuzzy c-means algorithm to increase the clustering accuracy as shown in the experiments.

web pages. Then convert each page into individual text document to apply text preprocessing techniques on it.

This step is applied on input Web documents dataset by scanning the web pages and categorizing the HTML tags in each page. Then exclude the tags that contain no textual information like formatting tags and imaging tags (i.e. <HTML>, <BODY>, <IMG>, etc.).

Also exclude all the scripts and codes that are found in the page like JavaScript and VBScript. Then extract the textual data from other tags (like paragraphs, hyperlinks, and metadata tags) and store it into individual text documents as input for next steps.

2.1.2. PoS Tagging. The second step is to PoS tag the corpus. The PoS tagger relies on the text structure and morphological differences to determine the appropriate part-of-speech. This requires the words to be in their original order. This process is to be done before any other modifications on the corpora. For this reason, PoS tagging is the first step to be carried out on the text documents as proposed in [2].

2.1.3. Stopwords Removal. Stopwords, i.e. words thought not to convey any meaning, are removed from the text. In this work, the proposed approach uses a static list of stopwords with PoS information about all tokens. This process removes all words that are not nouns, verbs or adjectives. For example, stopwords removal process will remove all the words like: he, all, his, from, is, an, of, and so on.

2.1.4. Stemming. The stem is the common root-form of the words with the same meaning appear in various morphological forms (e.g. player, played, plays from stem play). In the proposed approach, we use the morphology function provided with WordNet for stemming process. Stemming will find the stems of the output terms to enhance term frequency counting process because terms like "learners" and "learning" come down from the same stem "learn". This process will output all the stems of extracted terms.

The frequency of each stemmed word across the corpus can be counted and every word occurring less often than the pre-specified threshold (called Minimum Support) is pruned, i.e. removed from the extracted words list, to reduce the document vector dimension.

In our implementation we use minimum support value set to 10%, which means that the words found in less than 10% of the input documents is pruned.

2.1.5. WordNet Lexical Category Mapping. As proposed in [5], we use WordNet lexical categories to map all the stemmed words in all documents into their lexical categories. We use WordNet 2.1 that has 41   lexical categories for nouns and verbs. For example, the word "dog" and "cat" both belong to the same category "noun.animal". Some words also has multiple categories like word "Washington" has 3 categories (noun.location, noun.group, noun.person) because it can be the name of the American president, the city place, or a group in the concept of capital.

Some word disambiguation techniques are used to remove the resulting noise added by multiple categories mapping which are concept map and disambiguation by context as discussed with more details in [5].

2.2. Document Clustering  After generating the documents' vectors for all the input documents using feature extraction process, the proposed approach continues with the clustering process as shown in figure 1.

The problem of document clustering is defined as follows. Given a set of n documents called DS, DS is clustered into a user-defined number of k document clusters Dl, D2, ...Dk, (i.e. {Dl, D2, ...Dk} = DS) so that the documents in a document cluster are similar to one another while documents from different clusters are dissimilar.

There are two main approaches to document clustering, hierarchical clustering (agglomerative and divisive) and partitioning clustering algorithms [7]. In this process we apply three different clustering algorithms which are k-means (partitioning clustering), bisecting k-means (hierarchical clustering) and fuzzy c-means (fuzzy clustering).

2.2.1. K-means and Bisecting k-means. We have implemented the k-means and bisecting k-means algorithms as introduced in [7]. We will state some details on bisecting k-means algorithm that begins with all data as one cluster then perform the following steps:  1. Choose the largest cluster to split.

2. Use k-means to split this cluster into two sub-  clusters. (Bisecting step) 3. Repeat step 2 for some iterations (in our case 10  times) and choose the split with the highest clustering overall similarity.

4. Go to step 1 again until the desired k clusters are obtained.

2.2.2. Fuzzy c-means Algorithm. Fuzzy c-means is a data clustering technique wherein each data point belongs to a cluster to some degree that is specified by a membership grade while other classical clustering algorithms assign each data point to exactly one    cluster. This technique was originally introduced by Bezdek [16] as an improvement on earlier clustering methods. It provides a method that shows how to group data points that populate some multidimensional space into a specific number of different clusters. Most fuzzy clustering algorithms are objective function based: they determine an optimal (fuzzy) partition of a given data set c clusters by minimizing an objective function with some constraints [25].

In the proposed approach, we use the implementation of fuzzy c-means algorithm in MATLAB (fcm function). This function takes the generated document vectors as a matrix and the desired number of clusters then outputs the clusters centers and the optimal objective function values.

3. Experimental Results  Some experiments are performed on some real text documents to compare the performance of three text clustering algorithms which are k-means, bisecting k- means and fuzzy c-means. There are two main parameters to evaluate the performance of the proposed approach which are clustering quality and running time.

Fuzzy c-means algorithm is implemented in MATLAB and k-means, bisecting k-means algorithms are implemented in Java. Feature extraction process is also implemented in Java using Java NetBeans 5.5.1 and Java API for WordNet Searching (JAWS Library) to access WordNet 2.1.

3.1. Silhouette Coefficient (SC) for Clustering Quality Evaluation  For clustering, two measures of cluster "goodness" or quality are used. One type of measure allows us to compare different sets of clusters without reference to external knowledge and is called an internal quality measure. The other type of measures lets us evaluate how well the clustering is working by comparing the groups produced by the clustering techniques to known classes which called an external quality measure [7].

In our application of document clustering, we don't have the knowledge of document classes in order to use external quality measures. We will investigate one of the main internal quality measures which is silhouette coefficient (SC Measure).

To measure the similarity between two documents d1 and d2 we use the cosine of the angle between the two document vectors. This measure tries to approach the semantic closeness of documents through the size of the angle between vectors associated to them:   . d 1 ed 2 d,st (d 1,d2) = I I I I  d 1 ?d 2  Where (e) denotes vector dot product and (II) is the dimension of the vector. A cosine measure of 0 means the two documents are unrelated whereas value closed to I means that the documents are closely related [5].

Let OM ={O. ,??,Ok} describe a clustering result,  i.e. it is an exhaustive partitioning of the set of documents OS. The distance of a document d E DS to a cluster OJ E OM is given as:  L dist (d,p) dist (d ,D,) = peD, ID  i  I  Let further consider a(d ,D M  ) = dist (d ,D;) being  the distance of document d to its cluster 0i where  (d E Dj ) .b(d ,DM ) = miDdt!D, dist(d ,D;) 'tiD; E DM  is the distance of document d to the nearest neighbour cluster. The silhouette S (d, OM) of a document d is then defined as:  b(d ,D M  )-a(d ,D M  )  S(d ,D M  ) = max(b(d ,D  M ),a(d ,D  M ?  The silhouette coefficient as:  L S(P,D) SC(D)= peDS M  M IDS I  The silhouette coefficient is a measure for the clustering quality that is rather independent from the number of clusters. Experiences, such as documented in [11], show that values between 0.7 and 1.0 indicate clustering results with excellent separation between clusters, viz. data points are very close to the center of their cluster and remote from the next nearest cluster.

For the range from 0.5 to 0.7 one finds that data points are clearly assigned to cluster centers. Values from 0.25 to 0.5 indicate that cluster centers can be found, though there is considerable "noise". Below a value of 0.25 it becomes practically impossible to find significant cluster centers and to definitely assign the majority of data points.

3.2. Web Documents Datasets  We evaluate the proposed web document clustering approach on three different web document datasets: SCOTS, EMail 1200, and WebKB web documents corpuses. SCOTS corpus (Scottish Corpus Of Text and Speech) contains about 1,177 written and spoken texts, with about 4 million words of running text. 800/0    1.0...--------------------.

of this total is made up of written texts and 20% is made up of spoken texts and can be obtained from [15]. EMail1200 corpus contains about 1,245 test email documents for spam email detection and can be downloaded from [14]. WebKB dataset contains about 8,282 web pages collected from computer science departments of various universities in January 1997.

WebKB dataset is collected by the World Wide Knowledge Base (WebKb) project of the CMU text learning group and can be downloaded from [18].

0.8 Q) ::J (ij > Q) 0.6 ~ Q) ::J o ~ 0.4  0.2  -.- k-means ? Bisecting k-means .& Fuzzy c-means  3.3. Results  Figure 3: Silhouette values comparing all clustering algorithms using EMaill200 dataset     Clusters   Clusters  0.8  1.0 ...-----------.--"""":'k--m-e-a-ns--------.

? Bisecting k-means .& Fuzzy c-means  .&  0.2  Q) ::J  ~ Q) 0.6 ~ Q) ::J o ~ 0.4  First, we pass the three document datasets into the feature extraction process to generate the corresponding document vectors. The output vectors are used as an input for each clustering algorithm. We used the following list (2, 5, 10, 20, 50, 70, and 100) as the desired number of clusters for each algorithm.

Then output clusters for each dataset are measured using silhouette coefficient (SC Measure) and report the total running time of the whole process.

3.3.1. Clustering Quality. Figures 2, 3, and 4 show the silhouette values for the three datasets respectively.

In all experiments fuzzy c-means outperforms bisecting k-means and k-means algorithms in the overall clustering quality using silhouette measure.

This shows that the fuzzy clustering is more suitable for the unstructured nature of the text document clustering process itself.

1.0~---------------..

0.8  -.- k-means ? Bisecting k-means  .& .& Fuzzy c-means  Figure 4: Silhouette values comparing all clustering algorithms using SCOTS dataset  Figure 2: Silhouette values comparing all clustering algorithms using WebKB dataset  ? ?.--.--------------.~: .

---------------. .\.. .

------.

Q) ::J (ij > Q) 0.6 ~ Q) ::J ?: 0.4en  0.2  0.0  Clusters   3.3.2. Running Time. WebKB dataset, as mentioned in section 3.2, contains about 8.282 web documents.

This is considered a real challenge task that faces any clustering approach because of "Scalability". Some clustering techniques that are helpful for small data sets can be overwhelmed by large data sets to the point that they are no longer helpful.

For that reason we test the scalability of our proposed approach with the different algorithms using WebKB dataset. This experiment shows that the fuzzy c-means performs a great running time optimization with comparison to other two clustering algorithms.

Also, according to the huge size of WebKB dataset, the proposed approach shows good scalability against document size.

Figure 5 depicts the running time of the different clustering algorithms using WebKB dataset with respect to different values of desired clusters.

4. Related Work  Figure 5: Scalability of all clustering algorithms on WebKB dataset  cross-referencing is introduced in [6]. First, tenns with overlapping word senses co-occurring in a category are selected. A signature for a sense is a synset containing synonyms. Then, the list of noun synsets is checked for all senses for signatures similarity. The semantic context of a category is aggregated by overlapping synsets of different tenns senses. The original tenns from the category that belongs to the similar synsets will be finally added as features for category representation.

In [2] the authors explore the benefits of partial disambiguation of words by their PoS and the inclusion of WordNet concepts. They show how taking into account synonyms and hypernyms, disambiguated only by PoS tags, is not successful in improving clustering effectiveness because the noise produced by all the incorrect senses extracted from WordNet.

Adding all synonyms and all hypernyms into the document vectors seems to increase the noise.

Reforgiato [5] presented a new unsupervised method for document clustering by using WordNet lexical and conceptual relations. In this work, Reforgiato uses WordNet lexical categories and WordNet ontology in order to create a well structured document vector space whose low dimensionality allows common clustering algorithms to perfonn well.

For the clustering step he has chosen the bisecting k- means and the Multipole tree algorithms for their accuracy and speed.

Friedman et al. [26] introduced FDCM algorithm for clustering documents that are represented by vectors of variable size. The algorithm utilizes fuzzy logic to construct the cluster center and introduces a fuzzy based similarity measure which provided reasonably good results in the area of web document monitoring.

Rodrigues and Sacks [28] have modified the Fuzzy c-means algorithm for clustering text documents based on the cosine similarity coefficient rather than on the Euclidean distance. The modified algorithm works with nonnalized k-dimensional data vectors that lie in hyper-sphere of unit radius and hence has been named Hyper-spherical Fuzzy c-means (H-FCM). Also they proposed a hierarchical fuzzy clustering algorithm (H2_ FCM) to discover relationships between infonnation resources based on their textual content, as well as to represent knowledge through the association of topics covered by those resources [27].

The main problem about these studies is that they neglect any lexical information about the textual data in the documents. This infonnation helps in improving clustering quality as shown in section 3.3.

... ..... ......

...

... ...

10 100  Clusters  60~----.---:--'k--m-ea-n-s--------.

? Bisecting k-means ... Fuzzy c-means50  <i Q)  (f)  ~ 40  E i= 0) 30 c: 'c c: & 20  .-....

In the recent years, text document clustering has been introduced as an efficient method for navigating and browsing large document collections and organizing the results returned by search engines in response to user queries [13]. Many clustering techniques are proposed like k-secting k-means [17] and bisecting k-means [7], FTC and HFTC [19] and many others. From the perfonned experiments in [7] bisecting k-means overcomes all these algorithms in the perfonnance although FTC and HTFC allows to reduce the dimensionality if the data when working with large datasets.

WordNet is used by Green [20, 21] to construct lexical chains from the occurrences of tenns in a document: WordNet senses that are related receive high higher weights than senses that appear in isolation from others in the same document. The senses with the best weights are selected and the corresponding weighted tenn frequencies constitute a base vector representation of a document.

Other works [22, 23] have explored the possibility to use WordNet for retrieving documents by carefully choosing a search keyword. Dave [24] uses WordNet synsets as features for document representation and subsequent clustering. But the word sense disambiguation has not been perfonned showing that WordNet synsets decreases clustering performance in all the experiments. Hotho et al. [10] uses WordNet in an unsupervised scenario taking into account the WordNet ontology and lexicon and some strategy for word sense disambiguation achieving improvements of the clustering results.

Another technique for feature selection by using WordNet to discover synonymous terms based on     5. Conclusion  In this paper we proposed a fuzzy web document clustering approach based on the WordNet lexical categories and fuzzy c-means clustering algorithm. The proposed approach generates documents vectors using the lexical category mapping of WordNet after preprocessing the input web documents. We apply three different clustering algorithms, k-means, bisecting k-means and fuzzy c-means, to the generated documents vectors for clustering process. Output clusters of each algorithm are evaluated using silhouette coefficient as an internal cluster quality measure.

We have obtained some points that appeared in this work which are:  ? Using WordNet lexical categories in feature extraction process reduces the dimension of the generated documents vectors and achieves higher clustering quality.

? Fuzzy c-meanS clustering algorithm achieves higher clustering quality than classical clustering algorithms like k-means (partitioning clustering), and bisecting k-means (hierarchical clustering).

From the shown experimental results, we found that the proposed approach shows good scalability against the huge number of documents as in WebKB dataset along with different values of desired clusters.

6. References  [1] B. Berendt, A. Hotho, and G. Stumme, "Towards semantic web mining". In I. Horrocks and lA. Hendler, editors, International Semantic Web Conference, volume 2342 of Lecture Notes in Computer Science, Springer 2002, pages 264-278.

[2] 1. Sedding, and 'D. Kazakov, "WordNet-based Text Document Clustering", COLING 3rd Workshop on Robust Methods in Analysis ?f Natural Language Data, 2004, pp 104-113.

[3] M. Lan, C.L. Tan, H.B. Low, and S.Y. Sung, "A Comprehensive Comparative Study on Term Weighting Schemes", 14th International World Wide Web (WWW2005) Conference, Japan 2005.

[4] G. Stumme, A. Hotho, and B. Berendt. "Semantic Web Mining: State of the art and future directions", Web Semantics: Science, Services and Agents on the World Wide Web, Vol. 4, No.2, 2006, pp. 124-143.

[5] D. Reforgiato. "A new unsupervised method for document clustering by using WordNet lexical and conceptual relations." Journal of Information Retrieval, Vol (10), 2007, 563-579.

[6] S. Chua, and N. Kulathuramaiyer, "Semantic feature selection using wordnet". In Proceedings of the 2004 Intelligence, 2004, pp. 166-172.

[7] M. Steinbach, G. Karypis, and V. Kumar "A Comparison of Document Clustering Techniques", Department of Computer Science and Engineering, University of Minnesota, Technical Report #00-034,2000.

[8] U.M. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. "From data mining to knowledge discovery". Advances in Knowledge Discovery and Data Mining, AAAI / MIT Press, Cambridge, MA, 1996, pp 1-34.

[9] B.B. Wang, R.I. McKay, H.A. Abbass, and M. Barlow, "Learning text classifier using the domain concept hierarchy." Communications, Circuits and System~, China, 2002.

[10] A. Hotho, S. Staab, and G. Stu~e. "Wordnet improves text document clustering". ACM -:SIGIR Workshop on Semantic Web, 2003.

[11] L. Kaufman, and PJ. Rousseeuw, "Finding Groups in Data: an Introduction to Cluster Analysis", John Wiley & Sons, 1999.

[12] WordNet project available at http://wordnet.princeton.edu/  [13] O. Zamir, O. Etzioni, O. Madani, and R.M. Karp, "Fast and intuitive clustering of web documents". KDD 97, 1997, pp. 287-290.

[14] EMail 1200 dataset, available at: http://boole.cs.iastate.edu/bookJacad/bag/data/lingspam  [15] SCOTS Project, Scottish Corpus Of Text and Speech, http://www.scottishcorpus.ac.uk!

[16] J.C. Bezdec, "Pattern Recognition with Fuzzy Objective Function Algorithms", Plenum Press, ~ew York, 1981.

[17] B. Larsen, and C. Aone, "Fast arid effective text mining using linear-time document clustering". In Proc. of the 5th discovery and data mining, 1999, pp. 16-22.

[18] WebKB (The 4 Universities Dataset), available at: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/datal  [19] F. Beil, M. Ester, and X. Xu, "Frequent term-based text clustering". KDD 02, 2002, pp. 436-442.

[20] SJ. Green, "Building hypertext links in newspaper articles using semantic similarity". NLDB 97, 1997, pp. 178- 190.

[21] SJ. Green, "Building hypertext links by computing    semantic similarity". TKDE, 11(5), 1999, pp. 50-57.

[22] DJ. Moldovan, and R. Mihalcea, "Using wordnet and lexical operators to improve internet searches". IEEE Internet Computing, 4(1), 2000, pp.34-43.

[23] E.M. Voorhees, "Query expansion using lexical-semantic relations". In Proc. of ACM-SIGIR 1994, pp.61-69.

[24] K. Dave, "Mining the peanut gallery: Opinion extraction and semantic classification of product reviews". WWW 03 ACM, 2003, pp. 519-528.

[25] C. Borgelt, and A. Nurnberger, "Fast Fuzzy Clustering of Web Page Collections". PKDD Workshop on Statistical Approaches for Web Mining, 2004.

[26] M. Friedman, A. Kandel, M. Schneider, M. Last, B.

Shapka, Y. Elovici and O. Zaafrany, "A Fuzzy-Based Algorithm for Web Document Clustering". Fuzzy Information, Processing NAFIPS '04, IEEE, 2004.

[27] M. E. S. Mendes Rodrigues, L. Sacks, "A Scalable Hierarchical Fuzzy Clustering Algorithm for Text Mining". In Advances in Soft Computing, 2004.

[28] M. E. S. Mendes Rodrigues, L. Sacks, "Evaluating fuzzy clustering for relevance-based information access". In Fuzzy Systems, FUZZ-IEEE 2003.

