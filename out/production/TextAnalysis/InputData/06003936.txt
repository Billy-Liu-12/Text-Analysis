Using HMT and HASH_TREE to Optimize Apriori Algorithm

Abstract--On the basis of deep analysis to the Apriori  algorithm. In this paper, the HMT (HASH MAPPING  TABLE) and HASH_TREE methodologies are used to  optimize space complexity and time complexity. Using the  HMT compressed Itemsets, HASH_TREE can decentralize  support count process. The result of experimental show  that space complexity and time complexity of  Apriori  algorithm is Efficiency reduced by using HMT and  HASH_TREE.

Key words Data mining; association rules; Apriori algorithm;  HASH_TREE; mapping-table

I. TNTRODUCTION A large quantity of interesting relevance or related  association across the itemsets has been found by association rules mining. In the data mining field, association rules mining is an important topic and has been studied extensively by the industry. A typical example of the association rules mining is the market basket analysis. Association rules research helps to find the relationship among different products (items) in transaction databases and to find out the customer buyer behaviors, such as the purchase of a commodity impact on other goods. The results can be applied to goods shelf layout, storage arrangements, and classification of user according to buying patterns. Minimum support and minimum confidence are two important indicators of association rules [1].Association rules based on breadth-first algorithm which was designed by R. Agrawal and so in 1993, as the first association rules mining algorithm, uses pruning approach based on candidate itemsets. That is, according to a priori principle, reducing effectively the growth of a large number of candidate  itemsets generated from association rules. The algorithm uses a priori principle to generate candidate k-itemsets from frequent (k-1)-itemsets, and prunes candidate itemsets. Through the support counting, get candidate k-itemsets. Then the candidate k-itemsets generate frequent (k-1)-itemsets, so back and forth, until the frequent itemsets can not be generated. All the frequent itemsets will obtain association rules according to rules generation. For Apriori algorithm, there are two disadvantages. First, it must scan data sets repeatedly, which may lead to produce a large number of candidate itemsets. Second, the rare information is difficult to dig because the limitation of algorithm [3].The efficiency of Apriori algorithm has a greatly effect on performance and practicality of related data mining system. When there are many elements and minimum support threshold is low, the efficiency of Apriori algorithm is easy to become a bottleneck of performance. Subsequently, many researchers made a lot of research to the Apriori algorithm, and proposed optimization algorithms.

Optimization methods currently used include partition-based methods, hash-based methods [3], parallel methods and the use of sampling methods [4]. This paper proposes that the mapping-table converts the items in the data sets to achieve the purpose of compressing data set.

When counting the support, the use of HASH_TREE disperses the matching of candidate itemsets to reduce the time complexity of algorithm. At the same time, FP-GROWTH algorithm only scans transaction data sets twice [5]. It has been proven that FP-growth algorithm is better than Apriori algorithm in many aspects, but for   DOI 10.1109/BCGIn.2011.109     special data sets need much time to build FP-tree.

Therefore, improving Ariori algorithm is valuable.



II. ALGORITHM DESCRIPTION This article will use the following glossary: Table1 Glossary  Term Description  HMT The data sets of characters in the  file mapped to the integer, to  improve the matching efficiency,  and reduce memory footprint.

Detailed explanation behind  hash_ tree Use hash function to construct a  special data structure for candidated  itemsets.

hash_ node The node of hash tress it has  branch_ node and leaf_ node  branch_ node Be used for connect leaf node  leaf_ node Be used for connect bucket  bucket It has itemsets inside  A. the building process of HMT Definition 1: HMT is a One-dimensional mapping  table of the key-value pairs which bases on hash function, it is aimed at compress the data sets, to reduce memory footprint. When the HMT is established, it can query the mapped value according to the terms, or in turn.

Its key value is a string type of item, and its value is integer. Association rules mining for itemsets is all handling its key value?

The process of building: 1) Read the data set line by line, according to the list  separator to separate the items.

2) Query the HMT whether it has that item.

3) If the item already exists, ignores it; or else, adds it  into the HMT through the hash.

The above process can integrate with the compression  of data set, the integrated process is following: 1) Read the data set line by line, according to the list  separator to separate the items.

2) Query the HMT whether it has that item.

3) If the item already exists, ignores it; or else, adds it  into the HMT through the hash.

4) At the same time, insert the compressed data into the  memory map of the data set.

HMT processing and analysis:  In the Apriori algorithm, when use the HMT to compress the data sets, the processing of itemsets have become the treatment of integer data. That include the comparison of itemsets HASH calculate, the generation of subsets etc. When compress the itemsets, it will expend some handle time, but the time of support counting in Apriori is more than the compress time.

Example:  Supposed that there are data sets: A, C, D B, C, E A, B, C, E B, E First, read the first line of the data, A, C, D.

Because HMT is empty, there is no item A, C, D.

Then, after HASH respectively, insert A, C, D into  HMT, now the HMT is   Table2 Current status of HMT  Hash(A) A  Hash(C) C  Hash(D) D   Then, read the second line, B, C, D  Obviously, the B is not in the HMT, insert it into the HMT, C, D is already existed, so ignore it.

Table3 Current status of HMT  Hash(A) A  Hash(C) C  Hash(D) D  Hash(B) B   The same processing of the data set for every line, the  final HMT is Table4 Current status of HMT  Hash(A) A  Hash(C) C  Hash(D) D  Hash(B) B  Hash(E) E     B. the algorithm process 1) scan the data sets, get the candidate 1-itemsets,as all  non-repeated items in the data sets; a) Map all of the different items in the data sets  into the HMT; b) At the same time, embed the item mapping in  the data sets into the RAM; c) Read all mapped items in the HMT(the  candidate 1-itemset); 2) Scan the mapping data sets again, according to the  minimum support counting ,obtain the frequent 1-itemsets;  3) let k=2; 4) According to the frequent(k-1)-itemsets, obtain the  candidate k-itemsets(pruning) a) Connect the non-pruning candidate k-itemsets  with the frequent(k-1)-itemsets; b) Prune the candidate k-itemsets; c) Set up the(k-1)subsets for every items in the  candidate k-itemsets; d) Examine all the subsets whether belong to the  frequent (k-1)-itemsets, if not, subtract from the candidate k-itemsets.

5) According to the candidate k-itemsets, set up the hash_ tree; a) The selection of hash function: key=value mod  n b) The data structure of hash node c) Ergodic candidate k-itemsets d) Insert the level of branch node, every level  item of every itemsets of hash e) According to the above results by hash, Insert  this itemsets into the leaf node 1 corresponding with present branch node  f) Examine the itemsets number in the leaf node whether outstrip the threshold, if it outstrip, and the level of the corresponding branch node<K, transform the leaf node into branch node  g) Repeat above steps, until insert all itemsets.

6) Scan the data set, match the corresponding item hash  with the node of HASH_TREE, and execute the operation of support counting.

7) According to the minimum support counting, collect frequent k-itemsets from the HASH_TREE.

8) k+1, repeat the step4)--8), until the frequent k-itemsets is null.

9)  Query the HMT, output the results.



III. EXPERIMENTAL COMPARISION Experimental conditions, the hardware configuration,  CPU is P4 2.4G, Memory is 512M, Operating System is Linux.

Experimental data from the http://fimi.cs.helsinki.fi/.

Figure 1.  The experimental results of data set T10I4D100K.dat   Figure 2.  The experimental results of data set T40I10D100K.dat    Figure 3.  The experimental results of data set kosarak.dat      Figure 4.  The experimental results of data set mushroom.dat

IV. CONCLSION With the improvement of the Apriori by HMT and  HASH_TREE, the efficiency of Algorithm can be improved from Time complexity and Space complexity.

From the experimental results, we can see that the smaller the support, the more obvious effect. Thus, the improved algorithm can handle relatively large number of data. But, in the data set of mushroom, due to a large number of association rules, the processing time of this data set have more consumption than T10I4D100K larger data sets. In the course of the study find that the number of the branch of HASH_TREE and the threshold of the item sets under the bucket will affect the efficiency of the algorithm. The situation in this regard will be studied further in the future work.

[1] Jianxin Bi, Qishan Zhang. Engineering Science.

Association Rule Mining Algorithms. 2005, (7)4.

[2] Agrawal R, Imielinski T, Swami A. Mining  association rules between sets of items in large databases[C]//Proc 1993 ACM-SIGMOD Int Conf Management of Data, Washington, DC,May 1993:207-216.

[3] FU X, BUDZIK J, HAMMOND KJ. Mining navigation history for recommendation[A].

Intelligent User Interfaces[C]. New Orleans, ACM, 2000.

[4] Xiaohui Chen, Jia Wen, Haibo Xing, Lingfei Wang.

Candidate tree segmentation based on Apriori algorithm for distributed parallel. Computer Applicati ons. 2006.

[5] J.Han,J.Pei,and Y.Yin.Mining frequent patterns without candidate generation.In Proc.2000 ACM-SIGMOD Int.Conf.Management of Data(SIGMOD 00),Dalas,TX,May 2000.

