

Abstract ?Association rule mining is used to  uncover closely related item sets in transactions for deciding business policies.  Apriori algorithm is widely adopted is association rule mining for generating closely related item sets. Traditional apriori algorithm is space and time consuming since it requires repeated scanning of whole transaction database. In this paper we propose improved apriori algorithm based on compressed transaction database. Transaction database is compressed based on the consequence of interest.

Keywords - Apriori, Improved Apriori, Association rule  mining .



I.  INTRODUCTION  There is a huge amount of data around us which can be used to extract valuable information. Data mining is the process of discovering interesting and expensive information from large data. Association rule mining is one of the important techniques in data mining which is used to find association between various item sets from large transaction database. Apriori algorithm is one of the widely used algorithms for association rule mining. It was introduced by Agarwal in 1993 [1]; it is a strong algorithm which is used to find frequent item set. A basic property of apriori algorithm is ?every subset of a frequent item sets is still frequent item set, and every superset of a non-frequent  item set is not a frequent item set?[1].   Apriori algorithm is very costly in terms of space and time complexity since it requires repeated scan of database [1].  Many researchers have worked on improvement of apriori algorithm [2-8]. X. Luo and W.

Wang in [3] have converted an event database into matrix database, hence improving the computing time. This paper represents an improved apriori algorithm based on compressed database. R. Chang and Z. Liu in[4] have provided an optimized Apriori algorithm, on 2 items generation thus improving the time and space complexity.

Meaningless frequent itemsets exist in Apriori algorithm.

Y. Shaoqian [5] uses weighted support and confidence to optimize the Apriori algorithm. L. Lu and P. Liu in [6] have designed an algorithm which scans the database only once and without pruning. In [7], the authors have used ordered interested table to excavate frequent itemsets   quickly. T. Junfang in [8] discusses the advantage of a compressed database because of which time for searching associated items tremendously has decreased. In a recent research [10], few prominent attributes are identified to define class. Paper [11] introduces N-Dimensional inter- transaction association rule, support and confidence measures are defined.



II. ASSOCIATION RULE MINING   Let I = {i1, i2, i3,..., id} be the set of all items in a  market basket database and T = {t1,t2,t3,...,tn} be the set of all transactions. Each transaction ti contains a subset of items chosen from Item set I. A collection of one or more items is termed an item set. Support count is an important property of an item set. Support count refers to the number of transactions that contain a particular item set.

Mathematically, the support count, ?(i), for an item set i can be given as follows:  ?(i)= |{ti|X ? ti, ti ? T}| An association rule is an implication expression of the form A ? B , where A and B are disjoint item sets, i.e., A ? B = ? [12]. There are two important basic measures for association rules, minimum support and minimum confidence. Generally minimum support and minimum confidence are predefined by   user/analyst so that the rules which are not so interesting or not useful can be ignored. Support of A ? B is the total number of transactions where all items in A and B are together.

Confidence of A ? B, determines how frequently items in B appear in transactions that contain A. The formal definitions of these two metrics are given below,  Support (A ? B) = ? (A and B) Confidence (A ? B) = ? (A and B)/ ? (A)

III. APRIORI ALGORITHM   Apriori algorithm consists of follwoing  two steps a)Self Join  b)Pruining Apriori uses a level-wise search where K-itemsets are used to find (K+1)-itemset.

Improved Apriori algorithm based on Selection Criterion   Dr.V.Vaithiyanathan1,K.Rajeswari2, Prof. Rashmi Phalnikar3 , Mrs. Swati Tonge4 1Associate Dean Research, School of Computing, SASTRA University, Tanjore, India.

2Associate Professor, PCCOE, Pune and Ph.D Research Scholar, School of Computing, SASTRA University, Tanjore, India.

3 Department of Information Technology, MITCOE,Pune, India.

4 Department of Computer Engineering, PCCOE, Pune, India.

e-mail :  2raji.pccoe@gmail.com,  4swatitonge@yahoo.co.in         1) First, the set of frequent 1- itemsets is found which is denoted by C1.

2) The next step is support calculation which means the occurance of the item in the database. Here it is mandatory to scan the complete database.

3) Then the pruning step is carried out on C1 in which the items are compared with the minimum support parameter. The items which satisfies the minimum support criteria are only taken into consideration for the next process which are denoted by L1.

4) Then the candidate set generation step is carried out in which 2-itemset are generated, denoted by C2.

5) Again the database is scanned to calculate the support of the 2-itemset. Accoring to the minimum support the generated candidate sets are tested and only the itemset which satisfies the minimum support criteria are further used for 3-itemset candidate set generation.

6) This above step continues till there is no frequent or candidate set that can be generated.



IV. PROPOSED WORK   Performance of the apriori algorithm can be improved if the database size The key idea for compressing the database is the interested class label. This is applicable in many applications like feature selection, dimensionality reduction, feature extraction [12].  In feature selection, for any given dataset, the features can be analyzed, to find their association with the class label using Apriori algorithm. Rules are generated for item sets with expected minimum support and confidence. If the customer is interested on a specific class label, the tuples with this particular class label is taken for analysing the association level existing between attributes and desirable class label.

This method of selecting tuples based on desirable class label increases the efficiency of Apriori algorithm by reducing the number of iterations and time involved in finding association rules. As per example shown in Table I, suppose an analyst want to find all possible rules where I5 is the consequent. In this case the transaction T2, T3, T5, T6, T7 and T9 are simply ignored or deleted since these transactions does not have item set I5. The compressed database is shown in Table II.

Now the performance of apriori will be improved since unwanted transactions are removed.  Assuming minimum support is 2 and minimum confidence 50%, apriori requires 17 scan for the database given in Table I.

Frequent itemset are I,I2,I3 and I2,I3,I5. Rules are generated are given below.

I 1, I2 ? I5        Confidence=2/4=50%                          (1)  I 1, I5 ? I2        Confidence=2/2=100%            (2)  I 2, I5 ? I1        Confidence=2/2=100%                        (3)  I 1 ? I2, I5        Confidence=2/6=33%                          (4)  I2 ?I1, I5         Confidence=2/7=29%                           (5)  I5 ? I 1, I2        Confidence=2/2=100%                        (6)                  Rules 1, 2, 3 and 6 are selected since it satisfies minimum confidence.  Since analyst is interested in the rules where I5 is the consequent, rule 1 will be selected by the analyst. If compressed database given in Table II is used then Apriori algorithm will requires only 8 scans of compressed database.



V. ALGORITHM   Algorithm Compress_Database is used to compress the database based on the interested consequences. Once the database is compressed, Apriori algorithm can be applied to get frequent itemset . From Frequent item set rules are generated using apr-genrules algorithm based on class/consequence attribute.

Algorithm 1: Compress_Database D : Database N: Number of tuples in D C : Class Attribute/Interested Consequence  Method:  1. Set D?=D 2. Set N?=N 3. Set i=1; 4. Repeat  TABLE I TRANSACTIONAL DATA FOR AN ALLELECTRONICS BRANCH [1]    TID    List of Itemset  T1 I1,I2,I5  T2 I2,I4  T3 I2,I3  T4 I1,I2,I4  T5 I1,I3  T6 I2,I3  T7 I1,I3  T8 I1,I2,I3,I5  T9 I1,I2,I3     TABLE II COMPRESSED DATABASE  TID List of Itemset   T1 I1,I2,I5   T8     I1,I2,I3,I5         5. For each Ti ? D do 6.      Set   flag=false 7.      For each c ? C do 8.           flag=flag  v  Ti[c] 9.      If( flag = false) 10.           Set D?=D?- Ti 11.           Set N?=N?-1; 12. Until  i < > N   Algorithm 2: Apriori algorithm for frequent itemsets generation minsup: Minimum support threshold N :Number of tuples in original data set D   1. k=1 2. Fk={i|i?I? Support({i})?N?minsup} 3. Repeat 4. k=k+1 5. Ck=candidates generated from Fk-1 6. For each instance t ? T do 7. Ct=subset(Ck, t) 8. For each candidate itemset c ? Ct do 9. Support(c)= Support(c)+1 10.  End for 11.  End for 12.  Fk={c|c?Ck?Support(c)?N?minsup} 13.  Until Fk=Null 14. Result=?Fk                            Function  apr-genrules(fk, Hm,,C) 1.  k=|fk| 2. m=|Hm| 3. If k > m+1 then 4. Hm+1=m+1 5. For each hm+1 ? Hm+1 do 6.      Conf=Support(fk)/ Support(fk-hm+1) 7.      If Conf ? minconf  and  C ? hm+1   then 8.              Return  the rule (fk-hm+1)?hm+1 9.     Else 10.             delete hm+1 from Hm+1 11.     End if 12. End for 13. apr-genrules(fk, Hm+1) 14. End if

VI.  EXPERIMENTAL RESULTS   The proposed work is implemented on Core 2 Duo Machine with 2 GB RAM and 500 GB Hard Disk. The effectiveness of proposed algorithm is checked on real dataset from UCI repository. Fig. 1 shows the execution time measure on heart dataset with different support measures.

Fig. 1 Execution time of Apriori Algorithm with different minimum support threshold

VII. DISCUSSION The benchmark dataset is taken from University of California, Irvine [9] available online. The graph Fig. 1 shown above gives a comparison between the Apriori algorithm and Modified algorithm in terms of Run time [in seconds]. Apriori takes 0.76 seconds for 10% minimum support and Modified algorithm with compressed database has taken 0.06 seconds for execution.

For a minimum support, more data items will be chosen and hence time required will be large.



VIII. CONCLUSION   In this paper we have proposed an improved apriori algorithm based on compressed database. Database is compressed by deleting transactions which does not contain item set of interest as consequence. This compression is useful in feature selection and classification. Experimental results reveal the usefulness of the proposed technique.

