EVALUATING THE QUALITY OF TEST DATA UNDER THE INFLUENCE OF VIGILANCE PARAMETER IN FLEXFIS

Abstract? In this paper, we determine the influence of the vigilance parameter using a modified version of vector quantization used in Flexible Fuzzy Inference System (FLEXFIS) specifically for Takagi Sugeno fuzzy model. FLEXFIS adopts a single pass incremental learning approach for the antecedent parts of the rules? learning process. In order to achieve this learning process, an evolving version of vector quantization is used to either update or evolve new clusters or rules. It helps in the elimination of the outliers (samples with low dense region of the feature space). The use of vigilance parameter steers a tradeoff between plasticity and stability dilemma during the learning process. This is accomplished by selecting the best parameter grid search scenario in association with the cross validation procedure. This ensures some of the desired properties while training the systems during online operational mode such as computational complexity, robustness, preparametrizing of the number of clusters.It also overcomes the problem of cluster projection concept.

The adopted algorithm calculates the distance from a new data point to the surface instead of centers as in conventional vector quantization. An evaluation is done on the test data of weather forecasting. A comparative study of the performance analysis for both the conventional and incremental version of vector quantization is also presented in this paper.

1.INTRODUCTION  1.1 FLEXible Fuzzy Inference System (FLEXFIS)  The development of a successful fuzzy model for a chaotic system that exhibits a nonlinear dynamic behavior, with the identification of consequent parameters is an important yet difficult task which is traditionally tackled by trial and error process. It finds a reasonable connection between the adaptation of nonlinear premise parameter and linear consequent parameters. It uses the modified  version of vector quantization to eliminate the outliers that fall in the less dense region of the feature space.

1.2 Takagi-Sugeno Fuzzy model (TSK) The zero order Takagi-Sugeno fuzzy system model can approximate any real occurring nonlinear relationship to a certain degree of accuracy. The incremental learning for evolving Takagi-Sugeno fuzzy model is done by connecting premise and rule learning. This has been accomplished by updating the premise part and adapting the consequent parameter of the rule with respect to the winning cluster. It helps in constructing fuzzy inference system based on given input/ output data or knowledge of the human experts.

1.3 Vector Quantization  Vector quantization encodes large set of training data into a small set of representative points, thus achieving compression in representing the data. Such compression of data in weather forecasting results in over clustering of incoming data points. However, the modified version of vector quantization, known as the vector quantization incremental extended version eliminates the outliers while testing the quality of data in incremental learning in Takagi-Sugeno Fuzzy model. It has the ability to build clusters in incremental manner without preparameterizing the number of clusters. The vigilance parameter has been used for updates of cluster surface with each new incoming data point and generation of new clusters.

Already generated clusters are moved in local areas bounded by vigilance parameter. The adaptive vigilance parameter for the clustering prevents an incorrect cluster partition due to an inappropriate  2. LITERATURE SURVEY 2.1 Vector Quantization (VQ)  Fuzzy systems are widely used in areas such as pattern recognition, classification, fault detection, control [12], automation and identification of tasks.

Various approaches have been used to define linguistic interpretation models [11] and for rule base simplification. Fuzzy controller acts as a means of converting linguistic control strategy into automatic control strategy based on expert knowledge [13]. The vector quantization is a clustering technique which is used to partition the input space. The vector quantization is exploited in combination with the vigilance parameter.[10] The novel form of vector quantization is characterized by the ability to build up clusters in incremental manner without the need for preparameterizing the number of clusters, calculates the range of influence of clusters in each direction, finds the nearest winning cluster by calculating the distance from a new data point to the surface (instead of centers as in conventional VQ) of already obtained clusters. This prevents the generation of a new cluster very close to or even inside already existing ones. [1] A fuzzy modeling procedure comprises of two stages, viz., structure identification and parameter identification. Structure identification is related to the selection of the input variable and partition of the input space into fuzzy sub spaces. [2]  2.1.1 Drawback of Vector Quantization  Vector quantization cannot be reasonably applied for online processes, where incremental clustering is demanded, meaning clustering techniques which update their parameter for each newly loaded data block or even for each single data sample without taking into account prior data. This is because it iterates over the loaded data buffer several times. If this would be carried out for each incremental learning step i.e., for each actual loaded data block separately, the cluster centers would only represent a reliable partition of this data block and forget the older data completely Moreover, the number of clusters has to be known in advance, which can be a significant drawback, especially in case of high- dimensional data sets as the number of clusters can usually not be seen. Furthermore, in the case of online clustering the number of clusters are never known in advance as the data have to be processed through the algorithm as these are loaded or recorded.

It generates over clustering i.e., two new small clusters for data points lying near the range of influence of big cluster. Computing the distance of the new incoming point to the nearest cluster center  result in a wide data cloud. A new data point can lie near or even inside the range of influence, but still be far away from the cluster. In this fig 2.1, two clusters overlap inside the range of influence of another big cluster, thus resulting in over clustering. This  drawback can be overcome by an adaptive vigilance  nearest cluster. It eliminates over clustering that is formed in (i) thereby increasing the surface of the big cluster as shown in (ii). The surface of the big cluster adapts to some of the data points that appear in the over clustering.

(i) (ii) Fig 2.1 Drawback of VQ  2.2 Vigilance Parameter  Vigilance parameter has been used to update cluster with each new incoming data point and generation of new cluster. It steers the tradeoff between generating new clusters, rules and updating already existing ones. Already generated clusters are moved in local  plasticity and stability is ensured by adapting to new information without changing any already learned clusters. Therefore it overcomes plasticity-stability dilemma.

2.3 Curse of dimensionality  on the (p+1) dimensional space diagonal can be explained with the curse of dimensionality [1]. Accordingly, the higher the dimensions are the greater the distance between two adjacent data points would be. As the value of vigilance parameter is larger, the algorithm prevents the generation of too many clusters causing strong over fitting effects. The value of fac can be set as 0.2 to 0.4. However, reasonable performance cannot be ensured on an arbitrary data stream.

Appropriate cluster merging and splitting strategies after the learning process is adopted for compensating the inappropriate setting of fac.

However, this is a quite sophisticated issue as it depends on the position of the current sample relative to the cluster ellipsoid. Hence, the distance of the new data point to the surface of the multidimensional ellipsoid spanned by a cluster is calculated and the surface can be updated synchronously to the cluster center. As an approximation of the shortest distance    to the surface, the distance along the direction from the current point towards the cluster center is taken.

The local learning approach is necessary for a complete incremental learning of the system, as it triggers higher flexibility for adjoining rules when new operating conditions occur. The adaptation formulation of local learning results in weighted least squares, which calculates a new update for the linear parameter c1 each time a new data point comes in. in conditions should be incorporated into the system, which is shown in the fig 2.2.

Fig 2.2 Incorrect model with data cloud  The adaptation process viz., to extend the already generated models has four clusters represented with the identification of weather forecasting core parameters viz., minimum temperature, maximum temperature, rain, wind. In addition, we also find another collection of all these parameters formed as a separate cloud towards the rightmost upper corner of the diagram. This separate cloud is treated as a bad cloud that results in over clustering. Hence, such outliers forming part of the bad cloud have to be eliminated by way of using the modified version of extended incremental vector quantization. Such elimination doesn?t result in the entire elimination of the incoming data; instead those incoming data points have to be moved to their respective clusters by calculating the distance between such data points to their respective clusters. In order to overcome this deficiency a connection of recursive weighted least squares with an incremental premise parameter and rule evolution is used. The evolution of premise parameter and rule learning can be done with the usage of clustering the input/output space into local parts and projection of these clusters to form the premise parts of the rules. Hence Extended Incremental version of Vector Quantization in combination with the vigilance parameter is used for an update of cluster surface with new incoming data point, the generation of a new cluster whenever it is necessary because of the nature of new data. It is necessary to exclude outliers or other faulty points  from the update process, as this spoils the data space and the clustering process.

In section III, we discuss about our proposed system.

Section IV discusses with the performance analysis of the work by considering two factors viz., misclassification rate and vigilance parameter.

Section V deals with the conclusion and future work.

3. PROPOSED SYSTEM  To overcome the drawback of Vector Quantization, the modified version of vector quantization viz., extended incremental learning of Vector Quantization (VQ-INC-EXT) is used. This algorithm eliminates over clustering which was present in conventional Vector Quantization (VQ). The performance of  proposed system. The test data, when given as input to the Fuzzy Inference Engine, helps in deriving the humidity rate of that particular data. This humidity rate can be used to predict the climatic condition on a particular day. The data set with abnormal temperature values were given as input and the Fuzzy Inference system was trained in incremental manner.

The system was trained to adapt to abnormal weather conditions that were encountered. The adaptive vigilance parameter has increased the accuracy of cluster partitioning. The weather condition of the particular day is positioned based on the distance value. The identification of the incoming data to a particular cluster is done with the help of the already existing training set. The system is also trained for the new climatic conditions. The abnormal changes in the climatic conditions are also considered for evaluating the quality of the test data and assessing the performance of the vigilance parameter. The number of rules to be formed depends on the number of incoming data points. The approach followed to find the predicted values of the parameters during the learning process of the vigilance parameter is explained with the help of our algorithm viz., Evaluate_Vigilance that takes as input a set of data points and produces as output a set of predicted values in order to decide about the weather condition (normal and abnormal) on a particular day, is mentioned in what follows  Evaluate_vigilance Algorithm  Input : Data Set Output : Predicted Values  STEPS 1. Extract the distribution of patterns in terms  of various parameters in the feature space    2. Divide the input space into smaller local regions, identified as clusters.

3. Update the training set as and when new data points are derived.

4. Characterize the local properties by normalizing the membership functions in the fuzzy set.

5. Use sigmoidal threshold to characterize the global properties of the learning process.

6. Use adaptation of vigilance parameter for cluster updates.

7. Fine tune the vigilance parameter by fixing the threshold value  3.1 Extended Incremental Version of Vector Quantization (VQ-INC-EXT)  The VQ-INC-EXT algorithm finds the nearest winning cluster by calculating the distance from a new data point to the surface, instead of centers, of already obtained clusters. Clusters are built up in incremental manner. There is no need to preparameterize the number of clusters. The conventional vector quantization, with the conventional winning strategy generates two new small clusters for data points lying near the range of influence of the big cluster; but the VQ-INC-EXT extends the surface of this big cluster. The risk of approximation of the noise, identification of outliers, elimination of such outliers, significant over-fitting effect is low when VQ-INC-EXT was used. It gives better result when high noise is present in the data. It generates too few clusters for highly nonlinear approximation cases since it forms a big single cluster of data, considering the presence of similarities in the data.

The input to the experimental study is the data set derived from the weather database and the output is the set of predicted values stored in the inference engine. The pictorial representation of the modules specifications with respect to weather forecasting is given in figure 3.1.

A.2 Feature Extraction  3.2 Feature Extraction  3.2 Feature Extraction  Feature extraction is the environments where the distribution of patterns in feature space changes with respect to time. It is necessary to employ an adaptive learning algorithm which makes them suitable in well-known classical feature extraction and projection approaches. The weather details of a month or up to a maximum of three months are taken as the input data and they are distributed in the feature space. The subtractive clustering method is used for partitioning the data in the input space.

influence in the classification, the higher is the vigilance parameter, the more accurate is the classification. Table 3.1 shows the sample weather data set that are used as the input data for the feature extraction.

Table 3.1 Sample data set.

DATE MIN TEMP  MAX TEMP RAIN WIND  10/08/2012 26 29.4 16.4 23  11/08/2012 26.1 29.6 14.2 21  12/08/2012 25.6 31.3 0 18 13/08/2012 25.4 28.9 29.2 19  14/08/2012 25.8 28 62.8 19  15/08/2012 24.6 28.5 13.5 20  16/08/2012 27.6 32.4 34.5 28 19/08/2012 26.1 29.1 69 19  21/08/2012 26.7 32.6 0 18  26/08 /2012 18.7 24.9 17.8 36  28/08/2012 25.8 33.6 23.4 18  29/08/2012 24.5 36.7 0 13  30/08/2012 26.2 38.6 15.6 15  02/09/2012 24 33 56 19  3.3 Cluster Identification  Cluster identification is a process of identification of the winning cluster to which the incoming data point belongs. For the purpose of identification of the cluster multi-layer perceptron can be used because it exhibits the nonlinear behavior of various patterns.

The parameters for deriving the test data are minimum temperature, maximum temperature, minimum rain, maximum rain, minimum wind, maximum wind, average temperature, and average rain. These parameters are used for the cluster partitioning wherein the incoming data point viz., a new temperature, can be placed accordingly in a  Fig 3.1 Module description    particular cluster. Based on the partitions of the input space that is formed as clusters, the humidity for the particular day is calculated. This humidity is compared with the vigilance parameter which helps to predict the climatic condition of a particular day. It also helps to train the Fuzzy Inference System for the abnormal weather conditions that has been encountered. The test data are taken from the Weather database for a particular month. The adaptive vigilance parameter for the clustering prevents an incorrect cluster partition due to an  If the incoming weather condition is the first data point, it is considered as the data point and the cluster. If it lies inside the range of influence of any cluster, the distance of the selected data point to those cluster surfaces is calculated using Euclidean distance. The winning cluster is identified by taking the minimum of all the calculated distance. If the minimum distance is zero no new clusters are added.

Otherwise, if the current data point lies outside of all clusters? range of influence, the minimum distance of the new data point to the surface of all the clusters is calculated. The weather condition of the particular day is positioned based on the distance value. The incoming data may belong to any one of the two clusters already existing in fig 3.2 if not the system will be trained for the new climatic conditions.

3.4 Training set  Takagi-Sugeno fuzzy model recursively updates the structure of the model based on the potential of the input data an incremental version of subtractive clustering is used. A new rule is added when the distance from the new data point to the winning cluster is greater than the vigilance parameter. A rule is modified when the distance from the new data point to the winning cluster is lesser than the vigilance parameter. The condition for the generation of the new rule is show in fig 3.3  The training set which contains new weather conditions are used to train the weather forecasting system. These training set are shown in table 3.2. The clusters are incrementally updated and generated for creating a dynamically changing and evolving classifier.

Table 3.2 Training set  DATE  MIN  TEMP  MAX  TEMP RAIN WIND  08/12/2011 13 23 2 15  29/12/2011 14 24.5 52 70  03/01/2012 25.4 28.9 29.2 19  09/02/2012 28.9 39 0 15  12/02/2012 29.6 40 32.2 23  25/04/2012 32.6 41.2 0 13  16/05/2012 31.9 41.7 0 12  24/06/2012 29.6 41.2 0 13  24/07/2012 27.9 40.2 25.4 13  25/07/2012 31.3 41.2 28.5 24  A new rule will be generated whenever a cluster is being formed for the incoming data. These rules will be stored in Fuzzy Inference Engine hence the rules are in If-Then format. They are formed based on the temperature of the day, the amount of rain, wind etc.

The significance of the vigilance parameter determines the number of clusters, generation of the set of new rules. Thus the adaptation of vigilance parameter remedies the difficulties in choosing apriori values in data clustering. The rule set used for this experimental result is shown in table 3.3  Table 3.3 Rule set  RULE1:  IF (T < CT1MAX AND T > CT1MIN) AND (R < CR1MAX AND R > CR1MIN) AND (W < CW1MAX AND W > CW1MIN)  THEN C1 = T * TW1 + R * RW1+W * WW1.

RULE 2:  IF (T < CT2MAX AND T > CT2MIN) AND (R < CR2MAX AND R > CR2MIN) AND (W < CW2MAX ANDW > CW2MIN)  THEN C2 = T * TW2 + R * RW2+W * WW2.

RULE 3:  IF (T < CT3MAX AND T > CT3MIN) AND (R < CR3MAX AND R > CR3MIN) AND (W < CW3MAX AND W > CW3MIN) THEN C3 = T * TW3 + R * RW3+W * WW3.

Fig 3.2 Cluster identification  Fig. 3.3 Generation of new cluster (Rules)    3.5 Learning process  3.5.1 Local learning  Each rule is treated separately. The linear consequent parameters in the weighted least square that are used in the weighted least square approach are the normalized membership function values in a fuzzy set. Gaussian radial basis function is good at characterizing local properties. It is essential to trigger higher flexibility for adjoining rules when new operating conditions occur.

3.5.2 Global learning  Neural networks with sigmoidal function are good at characterizing global properties of the learning process. The sigmoidal threshold that is used for the weather forecasting system is the vigilance threshold.

3.6 Adaptation of vigilance parameter  The adaptive vigilance parameter is used for updating of the cluster surface with each new incoming data point or for the generation of new cluster. Already generated clusters are moved in local areas bounded by the vigilance parameter. A cluster is never initialized far away from the middle of a new upcoming data cloud.

3.6.1 Calculating the Surface of Clusters  If no new cluster needs to be set, the cluster center and the surface of the cluster are updated accordingly. In order to be able to project fuzzy sets from a cluster, the range of influence of the cluster should be available. A good estimation of this range is given by the variance of the data belonging to a clustering, i.e., for which a cluster was the nearest one. The calculation of this range in incremental mode is accomplished by the so-called recursive variance.

Fig 3.4 Calculating the Surface of Clusters  win,j is the distance of the old prototype cwin,j(old) to the new prototype cwin,j(new) of the cluster nearest to the current point x in the jth dimension, and kwin is the number of data points lying nearest to cluster cwin, which can therefore be simply updated through counting.

3.6.2 New Strategy for Selecting the Winning Cluster  The distance of the new incoming point is calculated incrementally to the surface of the multidimensional ellipsoid spanned by a cluster surface to the nearest  result in a precarious value in the case of wide data clouds: a new data point can lie near or even inside the range of influence, but be away from the center.

3.6.3 Incremental Update of Cluster Centers  The cluster center is updated using the formula in the fig 3.5 where cwin is the center in the (p+1) dimensional input/output space nearest to the current data point x and Rwin the learning gain. The choice of Rwin plays a central role, as it steers the degree of shifting the centers and is responsible for the convergence of the incremental learning in the least square sense so that the update becomes favourable.

Fig 3.5 Formula for incremental update of cluster center  3.7 Fine tuning process The optimal selection of vigilance parameter must be tuned in such a way that it must not under fit or over fit the model. The system may be over trained due to the inappropriate tuning of the vigilance parameter. This will make it memorize the individual input-output training pairs rather than settling in the mapping for all cases. A gain term can also be added such that the learning process in training the data set results in unsupervised learning of the network. While tuning the vigilance parameter it was noted that a value between 0.1 and 0.3 influenced the quality in a significant way. The value between 0.7 and 0.9 tends to over fit the system. A value varying from 0.4 and 0.6 influenced the quality of the test data very little hence the vigilance parameter was tuned to an optimal choice of 0.5.

In Fig 3.6, the x ? axis denotes vigilance parameter, y ? axis denotes performance.

K win 2winj (new) = ( K win 2winj (old) +  ki 2 winj + (cwinj(new)-x j )2  Cwin (new) = Cwin (old) + Rwin(x- Cwin(old))  Rwin = (init_gain/kwin)  Where kwin = no. of data points    4. PERFORMANCE ANALYSIS  This chapter deals with the performance analysis of this work. The performance analysis is done with respect to two factors viz., misclassification rate and vigilance parameter. The performance of the vigilance parameter in Vector Quantization (VQ) is compared with that of the performance of vigilance parameter in extended incremental version of Vector Quantization (VQ-INC-EXT). This results in the generation of performance analysis graph.

4.1 Misclassification Rate  The misclassification rate is evaluated using the K- Fold Cross-validation technique. The original sample is randomly partitioned into K subsamples. Of the K subsamples a single subsample is retained as the validation data for testing the system, the remaining K-1 subsamples are used as training data. The Cross- validation process is then repeated K-times with each of the K subsamples used exactly on the validation data.

The K results from the folds are then averaged to produce a single estimation. The value that is mostly given for the K is ten. Hence this K-Fold Cross- validation technique is otherwise known as 10-Fold Cross-validation technique. Weather conditions for 10 days are taken as the K subsamples. A single weather condition is taken as a validation data for testing the system while the remaining data are partitioned into clusters. The Euclidean distance from the validation data to the winning cluster is calculated. This process is repeated for the remaining samples and all the distances obtained are averaged to produce a single value.

4.2 Vigilance Parameter  Vigilance parameter has been used for update of cluster contain with each new incoming data point and generation of new cluster. The vigilance  classification, the higher is the vigilance parameters, the more accurate is the classification. It controls the number of clusters and size of clusters by the vigilance parameter. In the neural network, the vigilance parameter is related to a distance threshold or cluster diameter.

5. Comparison of VQ and VQ-INC-EXT  The performance of VQ-INC-EXT is higher when compared with the performance of conventional vector quantization. A graph for misclassification is plotted against the vigilance parameter. In conventional vector quantization the distance is calculated from the new data point to the center of the existing cluster, whereas in the VQ-INC-EXT the distance is calculated from the data point to the surface of the existing cluster. The vigilance parameter is made adaptive which prevents from the incorrect cluster partition because of prior setting of the vigilance parameter. This causes a reduction in the misclassification rate of the validation data. The performance analysis graph is generated for conventional Vector Quantization (VQ) and VQ- INC-EXT. In the fig 4.1 the performance analysis graph for Vector quantization is depicted. The performance of the Vector Quantization is inversely propotional to the rate of misclassification. Hence the performance of the Vector Quantization decreasess as the rate of misclassification increases thereby decreasing the quality of the system.

The fig 4.2 shows the performance analysis graph for Extended Incremental version of Vector Quantization (VQ-INC-EXT). The rate of misclassification shown in the performance anlysis graph for VQ-INC-EXT was comparitively lower than the rate of misclassification shown in conventional Vector Quantization. Hence the quality of the system has been increased in the Extended Incremental version of Vector Quantization where the weather forecasting system shows quite satisfactory accurate results in case of new climatic conditions. The goal has been accomplished by using VQ-INC-EXT wherein the system can automatically identify the cluster for a new weather conditions in online mode and keep high qualitative system up-to-date with newly recorded weather conditions  0.5  0.4  0.3  0.2  0.1  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9  Fig: 3.6 Fine Tuning Process    Figure 4.1 Performance analysis Graph of VQ  6. CONCLUSION AND FUTURE WORK  The adaptive vigilance parameter for the clustering part prevents an incorrect cluster partition due to an  automatically identify the cluster for a new weather condition in online mode and keep high qualitative system up-to-date with newly recorded weather conditions. The overall output is obtained via weighted average in Takagi-Sugeno model thus avoids the time consuming process of defuzzification required in Mamdani model. To overcome the limitation of the Fuzzy Inference?s reaching a saturation point, a forgetting term can be used in the learning process. Exponential decay of learning rate  stable state.. The selection of highly fit strings helps in obtaining a better, gradual increasing of improving solutions till a desired optimal/sub optimal solution is obtained. One way to overcome the overtraining of the network is to terminate training once a performance plateau has been reached.

We can use a recursive sample of data for the weather conditions that may help in evaluating  the calculation of the potential and rule evolution and replacement strategy based on the updated potentials.

This may result in lesser virtual memory to be used and without using more computational intensive re - training of the clusters with older samples.

