Research on Feature Extraction from Chinese Text for Opinion Mining  Shanzong Zhu, Yuanchao Liu, Ming Liu, Peiliang Tian

Abstract?more and more users and manufacturers concern about product reviews on the web, but it?s difficult to quickly find interesting content from massive information.

In order to mine sentiment polarity from review sentences, two approaches for product feature extraction and sentence opinion mining are proposed in this paper. Because of the characteristics of Chinese language, lexical analyzing tools are used to process review text, and association rule model is used to mine frequent items as candidate feature. In order to get better result, several filtering algorithms are proposed.

Experiment results demonstrate that relation between the precision and recall rate of feature extraction task with different minimum support thresholds in association rules mining, and the promising performance of our approach has also been shown.

Keywords- Opinion Mining; Feature Extraction;Sentiment detection;Association rule model

I.  INTRODUCTION Opinion mining has attracted much more attention  from researchers in recent years. Many methods have been proposed to solve the related problem [1-5] and lots of representative system has been developed [6-8]. Most of these methods could be divided into three kinds: document-level, sentence-level and feature-level. As unsupervised methods do not need manual work to get training corpus, so methods of this kind seems more suited for the problem, for most of electronic products have short lifecycles. This paper mainly focuses on opinion mining problem in feature level.

Yi et al. [7] use heuristics and selection algorithm to find bBNP (Beginning definite Base Noun Phrases) as candidate feature, and apply likelihood test method to select and sort features. This method has a high precision, but low recall rate. However, the way finding bBNP is not suited to Chinese language.

Popescu and Etzioni[9] use PMI(Point-wise Mutual Information) method to find product features. For example, use discriminators "of scanner", "scanner has" and "scanner comes with" to find relevant features (components and attributes) of scanner. Hu and Liu [10] use association rules to mine frequent features. This method has been applied to Google's Froogle system and Microsoft's online product search.

With the rapid expansion of web and development of e-commerce, more and more people buy products with network. Potential customers want to get some advice from other users before buying product they interested in.

Merchants and manufacturers can analyze the reviews online to obtain feedback from customers that buy their products. However, it?s difficult to quickly find interesting content from massive information, and it?s a waste of time and energy to read review to review. Different people care different aspects of product. Take automobile for instance: some people would like to know the fuel consumption  information, while other people may be concerned about the performance of safety. Nowadays mainstream search engines are based on keyword matching methods, which are not effective enough to find product reviews.



II. SURVEY OF OPINION MINING SYSTEM Opinion mining could be divided into two subtasks: (1)  extract product features from review database; (2) For each feature, mining sentiment from review sentence, including sentiment polarity and strength. A prototype system of opinion mining is showed in following Fig. 1.

Fig.1. the framework of opinion mining system   Feature extraction: firstly use lexical analysis tools to  do segmenting and POS (Part-Of-Speech) tagging for Chinese text, and then get generated transactions and apply association rule algorithm to mine frequent items as candidate features, and finally apply several filtering algorithm to filter redundancy features.

Mining opinion from review sentence: use product features and sentiment words as clues to find subjective sentence that contains sentiment, then use syntax analysis tools to parse review sentence to get polarity and strength of sentiment. This work needs a precise sentiment dictionary.



III. EXTRACT PRODUCT FEATURES FROM REVIEWS Product features can be divided into explicit features  and implicit features in reviews. For example, in the sentence ? n73 ???????? , the reviewer implicitly comments on the quality of product, however, the word ???  (quality) ? is not contained in the sentence. In this paper, product features mean explicit features that can be divided into five categories, showed in Table 1.

Product features appear as topics that people comment on in reviews, and they are mainly nouns or noun phrases in the context. So before using association rule model to mine frequent noun items as candidate feature, lexical analysis tools are used to process Chinese text.

978-0-7695-3904-1 2009 U.S. Government Work Not Protected by U.S. Copyright DOI 10.1109/IALP.2009.11    978-0-7695-3904-1 2009 U.S. Government Work Not Protected by U.S. Copyright DOI 10.1109/IALP.2009.11     Table 1. Explicit feature categories (cell phone) category sample sentence  Properties of product ?? ?? S9 ??????  Parts of product ?? ?? K750c??????  Features of parts ???? ?? SGH-D908 ????? ??  Related concepts of product  ?? ????? N93 ?????  Features Related concept  \???? ? ???? ??  Moto v3 ???????? ???????????? ????????    3.1 Lexical analysis Since there is no space between Chinese characters, we  need to segment a sequence of Chinese characters into words, and then tag the words with POS tag, so that nouns can be identified. To finish these works, we use a lexical analysis system called InsunElus[11], that?s developed by ITNLP Lab of Harbin Institute of Technology.

After lexical analysis, each line of reviews is used to generate transaction for association rule mining, and each line of text could generate one or zero transaction. Each transaction is generated from class-noun, which includes noun word and noun morpheme, in addition adnoun and gerund.

3.2 Mining frequent features based on association rule model  Following the original definition by Agrawal et al. [12] the problem of association rule mining is defined as: Let I ={i1, i2,?, im} be a set of items, and D be a set of transactions called the database. Each transaction in D has a unique transaction ID and contains a subset of the items in I. A rule is defined as an implication of the form X?Y where X, Y?I, and X?Y=?.  The support sup(X) of an item set X is defined as the proportion of transactions in the data set which contain the item set. If the support of item set surpasses the minimum threshold of support, then the item set is called frequent item set. Using association rule model to mine product features is to find those frequent items that satisfy a user-specified minimum support.

Finding all frequent item sets in a database is difficult since it involves searching all possible item sets. Apriori algorithm [13] is the best-known algorithm to mine association rules. It uses a breadth-first search strategy to counting the support of item sets and uses a candidate generation function which exploits the downward closure property of support. The algorithm works in two steps.

First, minimum support is applied to find all frequent item sets in a database. In a second step, these frequent item sets and the minimum confidence constraint are used to form rules. In this paper, we just need the first step to find all frequent item sets to get candidate features.

3.3 Feature filtering 3.3.1 Topic correlation filtering  Some frequent items found in association rule mining are high-frequency common nouns, most of which are not genuine product features. Product features are mainly nouns which are topic dependence, which means the occurrence of a word in texts of its own topic is higher than in texts of other topics. We can see that the occurrence of word ???(battery)? in reviews of cell phone topic is much higher than in common news articles.

Therefore, to filter high-frequency common words we need to evaluate the topic correlation of these words.

Utilizing the works of Yi et al. [7], which is based on the likelihood ratio[14] test by Dunning: For each candidate feature, compute the likelihood score -2log(L). If r2<r1, then -2log(L)=-2*lr, else -2log(L)=0, where the meanings of lr?r1?r2 and r are as follows:   11 21 12 22  11 1 12 1  21 2 22 2  ( ) log( ) ( )*log(1 ) log( ) log(1 ) log( ) log(1 )  lr C C r C C r C r C r C r C r  = + ? + + ? ? ? ? ? ? ?   (1)     11 12  Cr C C  = +                             (2)  21 22  Cr C C  = +                           (3)    11 21  11 12 21 22  C Cr C C C C  += + + +                    (4)   C11 and C12 are respectively the numbers of documents  containing a candidate feature in D+ and D-. D+ is a collection of documents focused on a topic, and D- is those not focus on the topic. C21 and C22 respectively represent the number of documents not containing candidate feature in D+ and D-. See  Table 2.

Table 2. Counting method for a candidate feature  D+ D-  occurrence C11 C12  non-occurrence C21 C22   3.3.2 Phrase filtering  In association mining, the algorithm does not consider the position of an item in a transaction (or a sentence).

However, in a natural language sentence, words that appear together and in a specific order are more likely to be meaningful phrases. Therefore, some of the feature phrases generated by association mining may not be genuine features. We evaluate the compactness and boundary of a phrase to determine it is product feature or not.

A. compactness filtering.

The idea of compactness filtering is to filter those  candidate features whose words do not appear together.

We use distances among the words in a candidate feature phrase to do the filtering. In this paper, if a feature phrase occurs in m sentences in the review database, and it is compact in at least 2 of the m sentences, then we call the phrase a compact feature phrase, which we?ll retain to be candidate feature.

For example, we have a candidate feature phrase ?? ??? ?, and can find these 3 sentences in review database:  (1)????????? N73 ???????? MP3,???????????  (2)??????????????? (3)?n73 ?????????? The phrase ?????? is compact in the first two  sentences but not compact in the last one. However, it is still a compact phrase as it appeared compactly two times.

B. Phrase boundary filtering based on information entropy.

In phrase boundary filtering, we first compute the uncertainty of occurrence between a phrase and its adjacent characters in review database, and then approximately decide whether the left side and right side of the phrase are the left-boundary and right- boundary, respectively. Here, so called the left (right) boundary of phrase S means that there is no character relatively fixed on the left (right) side of S. We apply information entropy (IE) to evaluate the uncertainty (flexibility) between a candidate feature phrase and its adjacent words on the two sides. The value of uncertainty between a phrase and its adjacent characters higher, more possible the phrase is a meaningful product feature. The definitions of left-side IE, right-side IE, and total IE of a phrase S are as these following formulas:  ( ) ( / ) log( ( / ))l la la a A  IE s P s s P s s ?  = ?? (5)  ( ) ( / ) log( ( / ))r rb rb b B  IE s P s s P s s ?  = ?? (6)  ( ) ( ) (1 ) ( )l rIE s IE s IE s? ?= + ?   (0 1)?< <        (7) In formula (5), if IEl(s)>iemin, then saying that S has a  left boundary, where iemin is a constant, representing a user-specified minimum IE threshold. IEl(S) represents the left IE of S, where A is a collection of Chinese characters that appear on the left side of S, and Sla represents a new string that is the combination of S and its left-side character a. P(Sla/s) represents the conditional probability that character a appears on the left side of S in the context, when the text contains S. Formula (6) is in a similar way.



IV. MINING OPINION FROM REVIEW SENTENCE In this paper, opinion mining task differs from the  traditional task of sentiment classification, and the mining elements of opinion include object (product feature), claim and sentiment (polarity and strength). For this reason, we  use product features and sentiment words as clues to find claim in reviews, then apply syntax analyzer to parse review sentence, and then compute the polarity and strength of sentiment in the sentence. We do not use the sentiment of polar words directly as the sentiment of a sentence, because a polar word in different contexts would cause different sentiments.

For example, in the sentence ??????????? ?????, ???? polar word with obvious positive polarity, however, the sentence has a different polarity of negative. That is because negative word ???? will cause the reverse of sentiment in sentence. For this considering, we apply syntax analyzer to distinguish similar problems.



V. EXPERIMENTAL RESULTS We first download review pages from product review  websites, and then some preprocessing works have been done, including cleaning HTML tags and redundancy filtering, to get review text for our experiment. The review texts for experiment are mainly from product review site it168 , including 500 reviews on cell phone. The resource using in the topic correlation filtering as texts that are not relevant to the topic are from People Daily published in January 1998, total 3079 news articles.

In order to evaluate the performance of our method, we select 328 cell phone features finally. The evaluating indicators include precision (P), recall rate (R) and F- measure. Formulas are as follows:  1 1( / ) 100%P C S= ?       (8) 1 2( / ) 100%R C S= ?          (9) (2 * ) / ( )*100%F measure P R P R? = +      (10)  Where S1 represents the number of features extracted by algorithm?S2 represents the number of features formed by manual picking?C1 represents the number of correct features in S2.

Association rule model is the baseline of our feature extraction method, so the minimum threshold of support in association rule will determine the quantity and quality of mined features. Fig. 2 shows the influence of different thresholds in association rule, where the minimum threshold of IE in phrase boundary filtering is 1.2, and the minimum threshold of topic correlation is 20 respectively by experiences.

Fig. 2 shows that the precision and recall rate of feature extraction task reaches a peak of 80% when threshold is 0.05% and 0.80% respectively, and the comprehensive performance will be achieved when threshold is between 0.1% and 0.2% in normal application.

The experiment result of each step in our algorithm is shown in Table 3, where the minimum support threshold in association rule is 0.1%?the minimum threshold of IE in phrase boundary filtering is 1.2?and the minimum threshold of topic correlation is 20.

www.pinglun.it168.com     In Table 3, we can see that the filtering methods are really effective, for the precision of our algorithm has risen up to 44.62(Topic filtering) from 19.67(Association rule), increasing about 127%, while the recall rate drops only 9.3%.

0%  10%  20%  30%  40%  50%  60%  70%  80%  90%  100%  0.05% 0.10% 0.20% 0.40% 0.60% 0.80%  Precision Recall rate F-measure   Fig. 2. Results by setting different minimum thresholds of  support   Table 3. Result in each step of feature extraction Association  rule Phrase  filtering Topic  filtering Precision (%) 19.67 33.48 44.62  Recall (%) 75.3 71.04 68.29 F-measure (%) 31.19 45.51 53.97

VI. CONCLUSION Mining sentiment from reviews on the web is  meaningful for both producer and consumer, and opinion mining domain has got more and more researchers? attention in recent years. In this paper we apply several NLP and data mining technologies for opinion mining in Chinese language, to mine sentiment from review sentence on products. We mainly focus on the task of extracting product feature from review database.

Experiment result shows that our algorithm for feature extraction is feasible and effective.

