Frequent Itemsets Mining Using Random Walks      for Record Insertion and Deletion

Abstract?In Association rules mining, the task of finding frequent itemsets in dynamic database is very important because the updates may not only invalidate some existing rules but also make other rules relevant. In this paper, we propose a new algorithm to maintain frequent itemsets of a dynamic database in the case of record insertion as well as deletion simultaneously.

Basically, the proposed algorithm maintains not only the support counts of frequent itemsets but also the support counts of prospective frequent itemsets, i.e., infrequent itemsets that promise to be frequent in the future, in an original database.

Prospective frequent itemsets, which are obtained by using the principle of Random Walks, can help to reduce a number of times to rescan the original database.

Keywords?frequent itemsets mining; incremental association rule mining; random walks; data mining; prospective frequent itemsets

I.  INTRODUCTION Association rules mining is a process of discovering  important correlations among items within transactions. The two major subtasks of association rule mining are frequent itemsets generation and strong association rules formation. In dynamic database, maintaining frequent itemsets is major challenge of association rule mining because a change of frequent itemsets may affect some previous association rules become invalid, or some new valid rules may be generated.

Reference [1] deals with this issue by reprocessing entire new databases without utilizing acquired information from previous mining. The computational requirements for frequent itemsets generation are truly costly. Therefore, several incremental updating algorithms have been proposed to deal with this important problem based on Apriori algorithms.

In this paper, we propose a new algorithm to maintain frequent itemsets of a dynamic database in the case of record insertion as well as deletion simultaneously. Our algorithm applies Random walks process and uses previously mined information to avoid reprocessing entire dynamic databases.

The algorithm can maintain frequent itemsets of a dynamic databases efficiently.

The rest of this paper is organized as follows: related work and fundamental concept is given in Section II. The proposed algorithm is described in Section III. Experiments are  conducted in Section IV. Conclusions are summarized in Section V.



II. PRELIMINARIES This section summarizes related work and briefly defines  the fundamental concept needed to facilitate the presentation of the proposed algorithm.

A. Related Work The first incremental updating technique to maintain  association rules when new transactions are inserted is Fast Update (FUP) algorithm proposed by [2]. The major idea of FUP is to store frequent itemsets with their support counts found from the prior association rule mining of an original database in an Apriori-like manner. For any candidate itemset from the newly inserted transactions which is also frequent in the original database, its new total support count can easily be updated. On the other hand, a candidate itemset from the newly inserted transactions will be scanned in the original database if it is frequent for the new transactions but not frequent in the original database. The extension algorithm of FUP is FUP2 [3] which can handle updating cases of data insertion as well as data deletion. However, FUP and FUP2 algorithm require rescanning an original database kth times when new frequent itemsets are found, where k is the length of maximal frequent itemsets.

To deal with multiple scan problem, the Efficient Dynamic Database Updating Algorithm (EDUA) was proposed in [4].

The algorithm firstly calculates set of candidate and frequent 2- itemsets by scanning an original database once in the prior mining. Frequent 2-itemsets are then used by performing join actions to form the set of candidate 3-itemsets. Based on the scan reduction technique [5] [6], candidate itemsets Ck for k>3 are generated from Ck-1.  All candidate 2-itemsets and frequent k-itemsets from an original database are kept in main memory for the subsequent incremental mining. When data is added to or deleted from the original database, EDUA algorithm scan inserted and deleted transactions twice to update support counts of all frequent itemsets and corresponding 2-itemsets in the original database. Finally, new candidate itemsets are checked against the updated database in order to discover the new frequent itemsets. The limitation of this algorithm is that it performs efficiently under the assumption that the number of      candidate k-itemsets is not much larger than frequent k- itemsets. In general, this assumption does not hold for a typical dataset.

Another incremental association rule mining algorithm, which using the concept of Pre-large itemsets, was proposed by [7] for data insertion and by [8] for data deletion. Basically, the algorithm has two support thresholds. Firstly, an upper support threshold is the same as a minimum support threshold of the conventional association rule mining algorithms. Secondly, a lower support threshold defines the lowest support factor for itemsets to be kept in memory. Itemsets that have their support factors between the upper support threshold and the lower support threshold are considered pre-large itemsets. By keeping pre-large itemsets in memory, the new incremental algorithm can avoid rescanning the original database until the accumulative amount of new transactions exceeds the safety bound at the expense of storage spaces.

On the other hand, [9] proposed keeping both frequent itemsets and promising frequent itemsets in memory to maintain association rules when new transactions are inserted into an original database. Promising frequent itemsets, infrequent itemsets that are expected to be frequent in the future, are estimated by using the principle of Bernoulli trials.

The extension work of this algorithm which can handle updating cases of data insertion as well as data deletion was proposed by [10]. The support counts of new candidate itemsets that are not previously frequent itemsets in an original database are approximated and pruned away if they are not potential frequent itemsets in an updated database. Then, an original database is rescanned only once to determine actual support counts of potential frequent itemsets.

In this paper, several experiments are conducted to compare the performance of the proposed algorithm with Apriori, FUP2, EDUA, Pre-large and Prob-based algorithm.

B. One-Dimentional Simple Random Walks In one dimension, the simple random walk may start from  the origin and move one step to the right (+1) or to the left (-1) at a time according to probabilities p and q, respectively, where 1p0 <<  (Fig. 1).

Fig. 1. One-dimentional simple random walks  Suppose that }n,...,3,2,1i:x{ i =  are sequence of identically and independently distributed random variables with p1q}1x{P,p}1x{P ii ?==?=== , where 1p0 << . Let  1n represents the number of steps taken to the right,  2n represents the number of steps taken to the left, N represent the number of total steps, and nS represents the stopping position after taking N steps. Thus, the ordinary random walk can be written as  n21n x...xxS +++=  (1)  Consider a walk consists of total N steps; the probability of walking 1n  right-steps is given by the Binomial distribution:  1111 nNn  nNn  1 qp)!nN(!n  !Nqp n N  )n(P ?? ?  ==  (2)  Usually we are interested in a net displacement of nS  steps to the right of the origin, which is determined by the total number of right-steps.  Since 21n nnS ?= and Nnn 21 =+ , so  )SN( 1n n1 +=  (3)  The probability of being a net displacement nS  steps to the right is provided by  )SN(2 1N)SN(   nn n  nn qp  ))!SN( 1N()!SN(   !N))SN( 1(P  +?+  +?+ =+  (4)  On the other hand, the random walks may start from the origin and move with steps +1, 0, -1 according to probabilities p, q and r, respectively where p, q, r ?  0 and 1rqp =++ . Let k represents the number of steps taken to the right, m represents the number of steps being still, and d represents the number of steps taken to the right. The probability of being stop at nS  after walking N steps can be considered as Trinomial distribution:  dmkdmkn rqp!d!m!k !Nrqp  d,m,k N  )Sx(P ===  (5)  C. Predicting Prospective Frequent Itemsets with Random Walks When transactions are inserted and deleted in an dynamic  database, an itemset.X may and may not appear in some transaction. Infrequent itemsets in an original database that possibly be frequent in an updated database can be predicted by calculating  )Sx(P n?  where nS  is minimum support count in updated database. The probability calculating of prospective frequent itemsets in this concept is similar to that of being stop at nS  after walking N steps in random walks.

The proposed algorithm needs to find not only frequent itemsets but also prospective frequent itemsets from an original database. Prior-updating procedure is assumed that the number of new transactions that be allowed to insert into and the number of deleted transactions from an original database are known in advance. Then, prospective frequent itemsets are predicted by applying random walks process.

Assume that an original database with |DB| transactions is going to be inserted by 1n  transactions and deleted by  p q  Sn -1             0           +1     2n transactions. Let k be the number of inserted transactions with itemset.X, m be the total number of inserted and deleted transactions without itemset.X, and d be the number of deleted transactions with itemset.X. Then  21 nnNdmk +==++  (6)  The updated support count of an itemset.X after N steps (total inserted and deleted transactions) can be represents as   =  += N  1i i0n xSS  (7)  Where 0S  is support count of an itemset.X in an original database and nS  is support count of an itemset.X in future updated database. The value of independent random variable  ix   can be -1, 0, or +1. Let p, q, and r be the probability that itemset.X will be inserted ( 1x i += ), not appear in inserted and deleted transaction ( 0x i = ), and be deleted ( 1x i ?= ) respectively. Assume that value of p, q, and r is accumulated from previous updating and stored in memory. The probability that support count of an itemset.X will increase to be nS  after insertion of 1n -transactions into and deletion of 2n - transactions from the original database of |DB| transactions can be considered as (5).

Even though the numbers of inserted transactions and deleted transactions are known in advance, the numbers of inserted and deleted transactions that an itemset.X appears are unknown. However, we know that support count of an itemset.X can be increased from 0S to be nS with exactly (k-d). Let a net displacement from the origin at step N be equal to R with the following constraints:   Rk2NdkNm  ,Rkd ,SSdkR 0n  +?=??= ?=  ?=?= (8)  Substituting the constraints of (8) in (5), we obtain  RkRk2Nkn r)rp1(p)!Rk()!Rk2N(!k !N)Sx(P ?+???  ?+? ==  (9)  Since the events x=Sn for each k are fundamentally unrelated, we can sum over all possible values of k in (9).

Therefore, the probability mass function (pmf) is  ???? ?+?  <? ==  ?+? +  +  = Rdk;r)rp1(p  )!Rk()!Rk2N(!k !N  Rdk;0 )Sx(P  RkRk2Nk |R|  1n  )R,0max(k  n    (10)  where +  1n1  represents the integer closest to but not  exceeding  1n1 +  and |R| represents non-negative value of R.

For any itemset with support count 0S  in an original database and Rdk <?   , there is no chance for that itemset to have support count at nS  in an updated database. But if  Rdk ?? , the value of k must be at least R in case 0SS 0n ?? or 21 nn ? . On the other hand, the value of k must be at least 0 in case 0SS 0n <?  or 21 nn < because k cannot be negative.

If Sn is a minimum support count of an updated database, the probability of an itemset.X to be a frequent itemset in the updated database can be obtained by   ?  = ?=<?=?  1S  0x nn  n )x(P1)Sx(P1)Sx(P  (11)  Here, infrequent itemsets that have the probability greater than Probpl; a threshold specified by user, are treated as prospective frequent itemsets. Probpl indicates the minimum probability level that an infrequent itemset in an original database will be a frequent itemset in an updated database.

Then, these prospective frequent itemsets are kept in order to reduce the number of times to rescan an original database. The higher Probpl is set, the lesser prospective frequent itemsets are kept and the more number of times needed to rescan the original database in order to discover new frequent itemsets.



III. UPDATING FREQUENT ITEMSETS FOR RECORD INSERTION AND DELETION  In a dynamic database, old transactions may be deleted from and new transactions may be added into an original database. Thus, an infrequent itemset in an original database may become frequent intemset in updated database if 1) it is frequent in inserted transactions and/or 2) it is infrequent in deleted transactions. Both cases are required to rescan the original database to obtain the support count of the itemset.

The proposed algorithm consists of two procedures. The first is a prior-updating procedure which discovers and stores frequent itemsets, prospective itemsets and their support count of an original database, i.e. DB. The second is an incremental updating procedure that updates all frequent itemsets, prospective frequent itemsets, and their support counts when either new transactions are added or old transactions are deleted from a dynamic database. A list of symbols used is given in Table I.

A. Pririor-Updating Procedure This procedure calculates all frequent itemsets and  prospective frequent itemsets of an original database. Prior- updating procedure is assumed that the number of new transactions that be allowed to insert into an original database and the number of deleted transactions are available. Parameter p  and r  are also known from prior update.

TABLE I.  MEANING OF SYMBOLS USED  DB Original database  DB? Updated database  x?  Support count of itemset X in DB  x??  Support count of itemset X in DB?  ?+ Set of added transactions  - Set of deleted transactions  +?x  Support count of itemset X in + ??x  Support count of itemset X in -  s Minimum support threshold  ? Minimum support count in DB  ?? Minimum support count in next update  +? kC  Candidate k-temsets in +  ?? kC  Candidate k-temsets in -  DB kF  Frequent k-itemsets in DB  kF?  Updated frequent k-itemsets in DB? DB kPF  Prospective frequent k-itemsets in DB  kFP ?  Updated prospective frequent k-itemsets in DB?   For example, an original database has 6 transactions as shown in Table II, p  and r of each itemset are shown in Table III, and a minimum support threshold is set to 40 percent. If one transaction is deleted and three transactions are inserted, then a minimum support count of an updated database is 4. The probability of an itemset to be a frequent itemset in the updated database )4x(P ?  is computed as shown in Table IV. If Probpl is set to 0.10, Item {B} is a prospective frequent 1-itemset because it is infrequent itemset in DB and its probability, which is equal to 0.4414, is greater than Probpl.

TABLE II.  AN EXAMPLE OF TRANSATION DATABASE  TID Items TID Items  100 ABC 400 AE  200 ADE 500 ABDE  300 DE 600 AE  TABLE III.  AN EXAMPLE OF P AND R FOR EACH ITEM  Itemset p r  A 0.625 0.250  B 0.375 0.125  C 0.125 0.125  D 0.375 0.125  E 0.625 0.125  TABLE IV.  INFREQUENT 1-ITEMSET WITH  )Sx(P n?  Item Count )4x(P ?  B 2 0.4414  C 1 0.0850    B. Incremental-Updating Procedure The main algorithm is shown in Fig 2. For k=1, 1-itemsets  obtained from prior-updating procedure are merged with 1- itemsets obtained from scanning ?+ and -. Then, the support count of the updated 1-itemsets is updated.  Only frequent 1- itemsets that exist in ?+ will be utilized to generate candidate 2- itemsets (C2).

Fig. 2. Incremantal updating procedure  For 2k ? , frequent k-itemsets and prospective k- frequent itemsets are updated repeatedly. Firstly, + is scaned to find  +? x . The candidate k-itemset which is not the member of k- itemsets obtained from prior-updating procedure is a new candidate k-itemset and is moved to newkC . Otherwise, the  Algorithm 1: Incremental mining Input: DB, ?+, - , DBkF ,  DB kPF , s  Output: 'kF , ' kPF  // k=1  1. scan ?+and - to find out +?x  and ??x  2. for each itemset X? DB1C +?  1C ??  1C  3.      ?+ ???+?=?? xxxx 4. end 5. '1F = {X? 1C | x??  |DB?| x s 6. '1PF = {X? 1C | 1FX ?? and plprob)x(P ????  // k 2 7. while ?????? ?? scanDB_TempFPF 1k1k  8.    +???? ??= 1k1k db  1k CFF  9.    +???? ??= 1k1k db  1k CFPPF 10.     if k=2 11.         db1  db  db 2 FFC ?=  12.     else 13.         ???= ?? )scanDB_TempPFF(C  db 1k  db 1k  db k  14.                    )scanDB_TempPFF( db  1k db  1k ?? ?? 15.     end 16.     update support count // call algorithm 2 17.     if |Temp_scanDB| |FPF| k  ' k ????  18.          Scanning an original database // call algorithm 3 19.     k++ 20. end 21. if Temp_scanDB ?? 22.     Scanning an original database // call algorithm 3     candidate k-itemset?s support count is updated and pruned from  kC  if support count is less than |DB?| x s.The support count of each new candidate k-itemset is estimated using the principle of maximum possible value. Since a new candidate 1-itemset is not in DBkF DBkPF , its support count in an original database is at best equal to ?-1 where ? is a minimum support count of the original database. The new candidate k-itemset, whose support count is less than |DB?| x s, are pruned from newkC  because such a candidate k-itemset has no chance to be a frequent itemset.

Fig. 3. Updating support count and frequent itemset procedure  Then, - is scaned to find ??x  and calculate x??  for each remaining candidate k-itemset, i.e. Ck newkC . If a remaining candidate k-itemset is the member of k-itemsets obtained from prior-updating procedure, its support count can easily be updated. The k-itemset whose support count satisfies the minimum support threshold is then a frequent itemset. For the k-itemset whose support count lower than the minimum support threshold, the probability of an itemset to be a frequent itemset in the updated database )x(P ???  will be calculated.

Infrequent itemsets that have )x(P ???  greater than Probpl, are treated as prospective frequent itemsets.

Contrarily, the remaining candidate k-itemset is a new candidate k-itemset. The support count of a new candidate k- itemset is estimated. The new candidate 1-itemset, whose support count is greater than or equal to |DB?| x s, are moved to Temp_scanDB. The original database is scanned only if the number of itemsets in Temp_scanDB is greater than  |FPF| k ' k ???  where ?  is a threshold specified by users.

Otherwise, the new candidate k-itemsets in Temp_scanDB will be kept. The procedure is repeated until no candidate k- itemsets can be generated. The proposed algorithm can thus find all frequent k-itemsets for the entire updated database.

Fig. 4. Scanning an original database procedure

IV. EXPERIMENT We used a synthetic dataset called T10I4D100K [11]. The  synthetic dataset comprises 100,000 transactions over 140 unique items. The performance of proposed algorithm is compared with Apriori and those algorithms which handle both case of record insertion and deletion, i.e. FUP2, pre-large and EDUA algorithm. However, since pre-large algorithm cannot deal with the case of insertion and deletion simultaneously, we first perform the case of deletion then follow by the case of insertion by using algorithm in [8] and [7], respectively. The accuracy of frequent itemsets obtained from these algorithms is checked with those obtained from Apriori algorithm.

For the first experiment, 20,000 transactions are inserted into and 10,000 transactions are deleted from an original database of 100,000 transactions. The support thresholds are varied between 0.01 and 0.05 with probpl=0.01. The execution time versus minimum support is shown in Fig. 5  For the second experiment, the impact of the incremental size is tested using several size |?+| and |?-|. As a preliminary experiment, the experiment is conducted only when the size of ?+ is twice as the size of ?- with a support threshold 0.05. The execution time of each algorithm is shown in Fig. 6.

According to Fig. 5 and 6, the execution time of the proposed algorithm is faster than that of Apriori, Pre-large and  Algorithm 2: Updating support count and frequent itemsets Input:  ?+, - , DBkF ,  DB kPF , s  Output: x?? , Temp_scanDB  1. scan ?+ to find out +?x  for each X in db kC  2. for each itemset dbkCX ?  3. if DBk DB k  db k PFFC ??  4.     move x to newkC  5. for each itemset DBk DB k PFFX ??  6. if dbkCx ?  7.    +?+?=?? xxx  8. else xx ?=?? 9.     if x??  < |DB?| x s 10.        remove X from Ck 11. for each itemset X? newkC  12. if +?x + (?-1) < |DB?| x s 13.     remove X from newkC  14. scan ?- to find out ??x  for each X in new k  db k CC ?  15. for each itemset X?Ck and X? DBkF DB kPF  16.     ?????=?? xxx 17. for each itemset X? newkC  18.      if ?+ ??? xx + (?-1)  |DB?| x s 19.            move X to Temp_scanDB 20. 'kF = {X?Ck | x??   |DB?| x s} 21. kkk FX|CX{FP ???=? and plprob)x(P ????  }  Algorithm 3: Scanning an original database  Input:   x??  , +?x ,  ??x , ' kF ,  ' kPF , Temp_scanDB, s  Output: 'kF , ' kPF  1. scan DB to find out x? for each X in Temp_scanDB 2. for each itemset X?Temp_scanDB  3.      ?+ ???+?=?? xxxx 4. newkF = {X?Temp_scanDB and x??   |DB?| x s}  5. newkk new k FX|CX{PF ??= and plprob)x(P ???? }  6. 'kF = ' kF  new kF  7. 'kPF = ' kPF  new kPF  8. clear Temp_scanDB     EDUA algorithms but slower than FUP2 and Prob-based algorithms. This is such a case because of calculating probability in the proposed algorithm becomes extremely laborious as n increases due to large value of n! Besides, the proposed algorithm calculates )x(P ???  for each and every infrequent itemset which has potential to be frequent itemset in updated database. Thus, approximation and infrequent itemsets pruning techniques are needed for further improvement.

In this paper, the performance of EDUA algorithm is relatively insufficient because the algorithm is based on the assumption that the number of candidate itemsets is not much larger than frequent itemsets. But in the experiments with a support threshold 0.05, the number of candidate itemsets is 14,023 while the number of frequent itemsets is only 199.

0.01 0.02 0.03 0.04 0.05  Apriori  FUP2  Pre-large  EDUA  Prob-based  Proposed   Fig. 5. The execution time (sec) for  |?+|=20K, |?-|=10K and |DB|=100K  transactions       10 20 30 40  Apriori  FUP2  Pre-large  EDUA  Prob-based  Proposed   Fig. 6. The execution time (sec) for  using several sizes |?+| - |?-|.

TABLE V.  THE NUMBER OF CANDIDATE ITEMSETS RESCANNED IN THE ORIGINAL DATABASE  k-itemset (k) Apriori FUP2  Pre- large EDUA  Prob- based  Proposed- algorithm  1 144 6 48 75 0 0  2 4278 24 4230 2172 5 15  3 979 1 1051 169 1 2  4 128 0 9 15 0 0  5 61 0 0 6 0 0  6 17 0 0 0 0 0  7 2 0 0 0 0 0  Total 5680 31 5338 2437 6 17    However, the proposed algorithm can prune away efficiently new candidate itemsets if they are not potential  frequent itemsets. Thus, the number of candidate itemsets need to be rescanned from the original database generated by the proposed algorithm is less than that generated by Ariori, FUP2, Pre-large and EDUA algorithm as shown in Table V.

In order to evaluate the prediction accuracy, the Precision (Pr) and Recall (Re) criteria are used. The result shows moderate accuracy of the proposed algorithm with precision=0.33, recall=1.00, and F1=0.50.



V. CONCLUSIONS AND FUTURE WORK In this paper, we have proposed an incremental  maintenance that can handle case of record insertion and deletion simultaneously. The proposed algorithm predicts prospective frequent itemsets by using random walk process and update frequent itemsets when transactions are added or deleted from database. The experimental results show that the execution time of the proposed algorithm is faster than that of Apriori algorithm but slower than FUP2 algorithm. It is quite obvious that calculating probability in the proposed algorithm becomes extremely laborious as n increases due to large value of n! Hence, work is going on for further improvement in our future work.

