A Mining Maximal Frequent Itemsets over the Entire History of Data Streams  Yinmin  Mao, Hong  Li, Lumin Yang, Zhigang Chen, Lixin Liu

Abstract: Mining maximal frequent itemsets has been widely concerned. However, mining data streams is more difficult than mining static databases because of the huge, high-speed and continuous characteristics of streaming data.

This paper presents an algorithm, called IDSM-MFI. The algorithm uses a synopsis data structure to store the items of transactions embedded data streams so far. It adopts a top- bottom and bottom-top method to mine the set of all maximal frequent itemsets in landmark windows over data stream, which can be output in real time based on users? specified thresholds. Theoretical analysis and experimental results show that our algorithm is efficient and scalable for mining the set of all maximal frequent itemsets over the entire history of data stream.

Keywords: data mining; data streams; maximal frequent itemsets  1. Introduction  Mining frequent itemsets is an essential step in many data mining problems, such as mining association rules, sequential patterns, closed patterns[1,2], maximal pattern[2, 3,4 ], and many other important data mining tasks. Recently, database and data mining communities have focused on a new data model, where data arrives in the form of continuous streams. It is often referred to data streams or streaming data. Many applications generate large amount of data streams in real time, such as sensor data generated from sensor networks, transaction flows in retail chains, Web record and click streams in Web applications, performance measurement in network monitoring and traffic management, call records in telecommunications, etc. Mining data streams is more difficult than mining static databases because of the huge, high-speed and continuous characteristics of streaming data. Consequently, previous multiple-pass data mining techniques studied for traditional datasets cannot be easily employed for the streaming data domain.

According to the data stream processing model [12], the research of mining frequent itemsets in data streams can be divided into three fields: landmark windows model [5], sliding windows model [7,10], and damped windows model [11, 8], as described briefly as follows. The first scholars to give much attention to mining all frequent itemsets over the entire history of the streaming data were Manku and Motwani [5]. The proposed algorithm Lossy Counting is a first single-pass algorithm based on a well-known Apriori  property. Lossy Counting uses a specific array- representation to represent the lexicographic ordering of the hash tree, which is the popular method for candidate counting. Teng et al. [7] proposed a regression-based algorithm, called FTP-DS, to find frequent itemsets in sliding windows. Moreover, Chang and Lee [10] also proposed a single-pass algorithm for mining recently frequent itemsets based on the estimation mechanism of the algorithm Lossy Counting. Chang and Lee [8] develop an algorithm estDec for mining frequent itemsets in streaming data in which each transaction has a weight and it decrease with age. In other words, older transactions contribute less toward itemsets frequencies. Reference [11] develops a FP- tree based algorithm [9], called FPstream, to mine frequent itemsets at multiple time granularities by a novel titled- time.

However, because of numerous redundant data and patterns in main memory for mining frequent itemsets, mining maximal frequent itemsets in data streams becomes a more important problem in recent years. There are few algorithms to solver this problem efficiently. In [12], Hua- Fu Li propose the DSM-MFI algorithm to mine maximal frequent itemsets over a data stream in landmark windows.

The algorithm DSM-MFI uses an efficient data structure named summary frequent itemset forest (abbreviated as SFI-forest) to store data stream, and uses top-down method to find the maximal frequent itemsets. The disadvantage of the DSM-MFI is that it stores a lot of projection of transaction, which consumes large numbers of memory usage and using top-down method to find the maximal frequent has to calculate support threshold of many itemsets, which is the most time-consuming in mining frequent itemsets. This paper proposes an improved algorithm called the improved algorithm of DSM-MFI (abbreviated as IDSM-MFI), which employs a synopsis data structure to be developed for incremental maintaining the essential information about maximal frequent itemsets embedded in the stream so far and combines bottom-up and top-down method to find maximal frequent itemsets. It can efficiently prune the superset of the short infrequent itemsets and the subset of the long maximal frequent itemsets.

The remainder of this paper is organized as follows.

Problem definition is given in Section 2. Algorithm IDSM- MFI to find the MFI is described in Section 3. The performance results are presented in Section 4. Section 5 concludes our study.

2009 First International Workshop on Database Technology and Applications  DOI 10.1109/DBTA.2009.125   2009 First International Workshop on Database Technology and Applications  DOI 10.1109/DBTA.2009.125     2. Problem definition  A data stream, DS = [W1, W2,??WN], is an infinite sequence of basic windows, where each basic window Wi, ?? i = 1, 2, ?, N  is associated with a window identifier i, and N is the window identifier of the ?latest? basic window BN. A basic window consists of a fixed sized number of transactions, where each transaction is composed of a set of items (named itemset). The size of a basic window W is denoted by |W|. The current length (abbreviated as CL) of data stream is |W1| + |W2| + ?+ |WN|.

Because it is unrealistic to store all the data into limited main memory or even in secondary storage, the single-pass algorithm for mining data streams has to sacrifice the correctness of their analytical results by allowing some frequency errors. Therefore, the true support of an itemset X is the number of transactions of the stream containing the itemset X as a subset, and denoted by X.tsup.

The estimated support of an itemset X is the estimated true support stored in the summary data structure, and denoted by X.esup. Note that 1 ? X.esup ? X.tsup. The current length of data stream with respect to an itemset X is |Wj|+|Wj+1|+?+|WN|, where basic window Wj is the first window containing X recorded in the current summary data structure, and is denoted by X.CL. In this paper, the itemsets embedded in the data streams can be divided into three types: frequent itemset, significant itemset, and infrequent itemset. An itemset X is called frequent if X.esup ? s?X.CL, where s is a user-defined minimum support threshold in the range of [0, 1]. An itemset X is called significant if s?X.CL > X.esup ? ??X.CL, where ? is a user- specified maximum support error threshold in the range of [0, s]. An itemset X is called infrequent if X.esup < ??X.CL.

An itemset is called maximal if it is not a subset of any other frequent itemsets.

3. The algorithm:IDSM-MFI  To efficiently find a set of maximal frequent itemsets in land window, we propose an algorithm IDSM-MFI: first, it reads a window of transactions from the buffer in main memory, and sorts the items of transactions in a synopsis data structure called summary extended frequent itemset tree (abbreviated SEFI-tree). Second, it prunes the infrequent information from the synopsis data structure.

Third, it searches the maximal frequent itemsets from the current synopsis data structure. Step 1 is performed in sequence for a new incoming basic window. Steps 2 and 3 are usually performed periodically or when it is needed.

3.1 The structure of SEFI-tree  Based on Apriori principle, DSM-MFI projects the transaction T into many sub-transactions. The detail [12] of  projection is defined as follows. A transaction T with m items, i.e., T = (x1, x2, ?, xm), is converted into m sub- transactions; that is, (x1, x2, ?, xm), (x2, x3, ?, xm), ?,and (xm), which are inserted into the current SFI-forest. Because SFI-forest stores sub-transactions of transaction, it repeatedly inserts large number of infrequent itemsets.

When DSM-MFI reads a transaction T from the current basic window and prune infrequent itemsets, it deletes each node of numerous infrequent itemsets via entire SFI-forest, which wastes a lot of storage space and consumes a great deal of execution time. Thus, we use the advantages of FP- tree and the characteristics of streaming data which requires that algorithm can be single streaming data scan for counting itemset?s frequent information. We design summary data structure called SEFI-tree. SEFI-tree stores each item of transaction in the current basic window. When all the transactions in the basic window are read, the infrequent itemsets are deleted from SEFI-tree so that SEFI- tree stores the nodes with x.esup???x.CL from begin window to the current window. Based on the ideas of DSM- MFI, we employ OFI-list to store sub-transactions of transaction. The characteristics of SEFI-tree are as follows.

Definition 1 (SEFI-tree) A summary extended frequent item tree is a tree structure defined below.

1. SEFI-tree consists of one EIS-tree (Extended Item Suffix Tree) and a FI-list (Frequent Item List).

2. Each node in the EIS-tree consists of five fields: itemname, node-count, window-id ,brother and link, where itemname registers which this node represents item, node- count registers the number of transactions represented by the portion of the path reaching this node, window-id registers the window identifier of the current basic window, brother links to the node in the EIS-tree carrying the same itemname, or null if there is none and link links to the next node in the EIS-tree carrying the same itemname, or null if there is none.

3. Each entry in the FI-list consists of four fields: itemname, node-count, window-id and node-link, which points to the first node in the EIS-tree carrying the itemname.

4. Each entry in the OFI-list consists of three fields: itemname, item-count, and window-id, which is expressed xi.OFI-list.

3.2 Constructing and Maintaining of SEFI-tree  When streaming data arrive in, each item is embedded in SEFI-tree and each sub-transaction of transaction is inserted into OFI-list. When all items in the current window are read, firstly, the all infrequent nodes which node-link of FI-list points to in the EIS-tree carrying the itemname are pruned, secondly, the infrequent node in the FI-list is pruned, and finally, the node?s OFI-list is deleted.

Algorithm 1 BuildMaintainSFI-tree: constructing and maintaining of SEFI-tree Input: DS=[w1,w2,?wn], a user-specified minimum support threshold s ? (0,1), and a user-defined maximum support error threshold?? (0,s); Output: A SEFI-tree generated so far;     FI-list={}; foreach basic window wj  do /* j=1,2,?,N */  foreach transaction T=(x1,x2,?xm) ? Wj(j=1,..N) do foreach item xi?T do If xi ? FI-list then  Create a new entry of form(xi,,1,j,node-link) into the FI-list; else  xi.esup= xi.esup+1; endif if EIS-tree has a child node with itemname such that  y.itemname=xi.itemname y.esup=y.esup+1;  else create a new entry of form (xi,1,j,brother,next) into the EIS-tree; endif  endofor call Transaction_Projection(T,j);  endfor call EIS-tree_pruning(EIS-tree,?,N); endfor Subroutine Transaction_Projection Input: An item-suffix transaction T=(x1,x2,?,xm), the current window-id j; Output: xi.OFI-list, ? i=1,2,?m; foreach item xi, ? i=1,2,?m  do OFI-list_maintenance([xi|X],xi.OFI-list,j) ; /* X=x1,x2,...xm [xi|X] is an item-suffix transaction with the item-suffix xi */ endfor Subroutine OFI-list_maintenance Input: An item-suffix transaction (xi,xi+1,?,xm), the current window-id j; Output: A modified xi.OFI-list ? i=1,2,?m; foreach iten xl do ; /* l=i+1,i=2,?m */ if xl ?  xi.OFI-list then create a entry (xl,1,j) into the xi.OFI-list; else  xl.esup= xl.esup+1; endif  endfor Subroutine EIS-tree_pruning Input: A EIS-tree, a maximum support errorthreshold ?, and the current window identifier N; Output: A EIS-tree which contains the set of all significant and frequent itemsets; foreach entry(i=1,2,..,d) ? FI-list,where d=|FI-list| do if xi.esup<??xj.CL then delete those nodes(itemname=xi) in EIS-tree via node-link structure;  delete the entry xi fron the FI-list; delete  xi.OFI-list;  delete xi  from other xj.OFI-list if it exists in xj.OFI-list (j=1,2,?.d;j ? i); endif  endfor  3.3 Finding maximal frequent itemsets from the current summary data structure  There are all significant itemsets and frequent itemsets in the FI-list. Based on Apriori principle, we design a tobo- botoMFI (top-bottom and bottom-top selection of maximal frequent) algorithm to find a set of maximal frequent itemsets from SEFI-tree.

Suppose there are k 1- itemsets such as e1,e2,?ek in the current FI-list and each item ei , ? (i=1,2,?k) , has a ei.OFI-list, whose     size is | ei.OFI-list|. Each item in ei.OFI-list denotes ei.o1,...,ei.oj, assume ji= |ei.OFI-list| in this paper. Based on Apriori principle, each item in the ei.OFI-list generates 1 2, ,... ii i i  j j j jc c c  itemsets, which combines ei  to a new itemsets respectively. We adopt top-bottom and bottom-top method and prune the supersets of infrequent and the subsets of long maximal frequent items to reduce the large number of item support threshold calculating.

Algorithm 2 tobo-botoMFI: Finding a set of maximal frequent itemsets Input: A current SEFI-tree, the current window identifier N, mining support threshold s, and a maximum support error threshold ?; Output: A set of all maximal frequent itemsets; Foreach ei in the FI-list do  Enumerate topjc itemsets and combine them with ei to a candidate maximal frequent itemsets E1 /*top=|ei.OFI-list|,? ,  |ei.OFI-list|/2 */ Enumerate bottomjc  itemset and combine them with ei to a candidate maximal frequent itemsets E2;    /*bottom=1, ?, |ei.OFI-list|/2 */  Do each itemsets E1m in the E1 and E2n in the E2 (m,n=1,2,?) Count E1m.esup by traversing the SEFI-tree; If E1m.esup>=s? E1m.CL  If E1m ? MFI and is not a subset of any other frequent itemsets in MFI then MFI=MFI?E1m; Prune the subsets of E1im from E2;  endif Count E2n.esup by traversing the SEFI-tree If E2n.esup>=s? E2n.CL  If E2n ? MFI and is not a subset of any other frequent itemsets in MFI then  MFI=MFI?E2n; else  Prune the subsets of E2n from E1; endif  Until E1 is {} and E2 is {}; endfor  3.3 Theoretical analysis of algorithm 3.3.1 Maximal estimated support error analysis Theorem 1 The error between true support and estimated support is below ?? x.CL.

Proof: Suppose x is a infrequent itemsets when it is in the window-i, and is a frequent itemsets when it is in window-p (p>i),  So . sup . sup . sup  p x t x e x ei  i = + ?  = i=1,2,?p  in IDSM-MFI, if x is a infrequent itemsets, we can know . sup .i ix e x CL? ??     1 1  . ..

i  p  i i  p  i  x esu p x C Lx C L = =  ? ? ? ? ?<? ?  . sup . sup . sup . sup .

p  i i  x t x e x e x e xCL =  = + < + ??? Hence, . sup . sup .x t x e xCL? < ?? .end 3.3.2 Comparison the complexity of time  Suppose N is the identifier of the current window, T is the number of the transactions of every basic window, I is the average number of the itemsets of every transaction, and k is the number of frequent itemset in the FI-list.

Calculating the complexity of time in DSM-MF includes two parts. First, while constructing and maintaining the SFI-forset, we need to insert 1 2( ... )II I IC C C T N+ + + ? ?  itemsets into SFI-forest. Then, in order to delete an infrequent  itemsets we need to traverse | / 2|  1 | / 2|  iT  i T i  C = + ?  [12] nodes; Second,  the next parts is discovering the maximal frequent itemset.

Each item ei, ? i=1,2,3?k, in the current FI-list has a ei.OFI-list. The size of ei.OFI-list is denoted by |ei.OFI-list|, which is expressed ji in this paper. Algorithm need  1 2   ( ... )i i i i  k j  j j j i  C C C =  + + +?  times support threshold calculating. So,  the complexity of time of DSM-MF is [ / 2]  2 1 2 [ / 2]  1 1  ( ) / 2 1 2 ( ( ... ))i i i i  T k ji  T i j j j i i  I I T N C C C C C C+ = =  ? ? ? + ? + ? + + +? ? (1), in  which C1 is the number of infrequent itemsets and C2 is the time of traversing one itemset. Calculating the complexity of time in IDSM-MF also includes two parts. First, we need to insert T N?  itemsets into the SEFI-tree. While in the part of deleting infrequent itemsets, the number of nodes needed to be deleted is equal to the number of the infrequent itemset in SEFI-tree, in which the number is denoted by C3.

Obviously, | / 2|  1 | / 2|  iT  i T i  C C = +  <? ; Second, the same to DSM-MF, It  need also  1 2  ( ... )i i i i  k j  j j j i  C C C =  + + +? times support threshold  calculating. But, IDSM-MFI needs 1 2  ( ... )i i i i  k j  j j j i  F F F =  + +? times  support threshold calculating by pruning operation, in which 1 2, ,..., i  i i i  j j j jF F F is respectively remnant itemsets of  1 2, ,..., i i i i  j j j jC C C  after pruning operation. Obviously,  1 2( ... )i i i i  j j j jF F F+ + ?  1 2( ... )i i i i  j j j jc c c+ + + . Hence, the complexity of  time of DSM-MFI is 1 2   1 3 2 ( ... )i i i i  k j  j j j i  T N C C C F F F =  ? + ? + ? + +? (2).Compare (1) and  (2), the result is (2) < (1).

3.3.3 Comparison the complexity of space  Suppose the number of 1-itemset is k. The space requirement of both DSM-MF and IDSM-MF consists of three parts. The difference is it is SFI-tree that is used to store frequent itemset in DSM-MFI, while it is EIS-tree that is used to store frequent itemset in IDSM-MFI. Hence, the complexity of space is O (2k) [12] in DSM-MF. And the  complexity of space is O (Ck) in IDSM-MF. Obviously, while k is huge, the result is 2k C K> ? .

4. Performance evaluation  Base on [12], all the experiments are performed on a 2.66GHz PertiumIII with 512MB, and the program is written in Microsoft Visual C++6.0. To evaluate the performance of DSM-MFI and IDSM-MFI algorithm, we conduct the empirical studies based on the synthetic datasets. The parameters of synthetic data generated by IBM synthetic data generator [6] are described as follows.

IBM Synthetic Dataset: T10.I5.D1M and T30.I20.D1M. The first synthetic dataset T10.I5 has average transaction size T with 10 items and the average size of frequent itemset I is 5-items. It is a sparse dataset. In the second dataset T30.I20, the average transaction size T and average frequent itemset size I are set to 30 and 20, respectively. It is a dense dataset. Both synthetic datasets have 1,000,000 transactions. In the experiments, the synthetic data stream is broken into blocks with size 50K (i.e., 50,000) for simulating the continuous characteristic of streaming data, where 1K denotes 1,000. Hence, there are total 20 blocks in these experiments. In this paper, ? is 0.01 4.1 Scalability Study of IDSM-MFI  In this experiment, we examine the two primary factors, execution time and memory usage. In Figure 1 (a), the execution time grows smoothly as the dataset size increases from 2,000K to 10,000K. The default value of minimum support threshold  is 0.1. The memory usage in Figure 1 (b) for both synthetic datasets is stable as time progresses, indicating the scalability and feasibility of algorithm IDSM-MFI.

(a) Execution Time         (b) Memory Usage Figure 1 Required resources of IDSM-MFI for IBM synthetic datasets  4.2 Comparison with Algorithm DSM-MFI In this experiment, we examine the execution time and  memory usage between DSM-MFI and IDSM-MFI by dataset T30.I20.D1M. In Figure 2 (a), we can see that the     execution time incurred by IDSM-MFI is quite steady and is shorter than that of DSM-MFI. The experiment shows that IDSM-MFI performs more efficiently DSM-MFI. In Figure 2 (b), the memory usage of IDSM-MFI is more stable and smaller than that of DSM-MFI. This is because IDSM-FI adopts a better search method to find sets of maximal frequent itemsets. Hence, it shows that IDSM-MFI is more suitable for mining long frequent itemsets in data streams.

(a) Execution Time         (b) Memory Usage Figure 2 Comparison with algorithm DSM-MFI  4.1 Comparison with Algorithm DSM-MFI on Different Support Threshold  In this experiment, we examine the execution time between DSM-MFI and IDSM-MFI by dataset T30.I20.D1000k and T10.I5.D1000K. In Figure 3, we can see that algorithm DSM-MFI is more suitable for large support threshold and sparse data sets. Algorithm IDSM- MFI is much better than algorithm DSM-MFI for the suitability of support threshold and data sets.

(a) T30.I20.D1000         (b)  T10.I5.D1000K Figure 3 Comparison with algorithm DSM-MFI on different support  threshold  5. Conclusion  In this paper we proposed a novel algorithm, IDSM- MFI, to discover and maintain maximal frequent itemsets in the current data stream landmark window. The algorithm uses a new SEFI-tree which is developed for incremental maintaining the essential information about maximal frequent itemsets embedded in the stream so far and a top- bottom and bottom-top method to mine the set of all maximal frequent itemsets in landmark windows over data stream. Theoretical analysis and experimental results show that our method can achieve better performance than a representation algorithm DSM-MFI in terms of both time and space overhead, especially when the minimum support is low, and the dataset is dense. In the future, we plan to extend our proposed algorithm to different data stream applications.

6. Reference  [1] M. Zaki and C. Hsiao. CHARM: An efficient algorithm for closed itemset mining. In SDM'02, April 2002.

[2] D. Burdick, M. Calimlim, and J. Gehrke. MAFIA: A maximal frequent itemset algorithm for transactional databases. In ICDE'01, April 2001.

[3] D. Burdick, M. Calimlim, and J. Gehrke. MAFIA: A maximal frequent itemset algorithm for transactional databases. In ICDE'01, April 2001.

[4] Ji Genlin,Yangmin.Fast Updating Maximum Frequent Itemsets. Chinese Journal of Computer, 2005,28(1):128-135  [5] G. S. Manku and R. Motwani. Approximate Frequency Counts Over Data Streams. In Proc. of the 28th VLDB conference, 2002.

[6] R. Agrawal and R. Srikant. Fast Algorithms for Mining Association Rules. In Conf. of the 20th VLDB conference, pages 487-499, 1994.

[7] W.G. Teng, M.-S. Chen, and P. S. Yu. A Regression-Based Temporal Pattern Mining Scheme for Data Streams. In Proc.

of the 29th VLDB Conference, 2003.

[8] J. Chang and W. Lee. Decaying Obsolete Information in Finding Recent Frequent Itemsets over Data Stream. IEICE Transaction on Information and Systems, Vol. E87-D,No. 6, June, 2004.

[9] J. Han, J. Pei, and Y. Yin. Mining Frequent Patterns without Candidate Generation. In Proc. of 2000 ACM SIGMOD, pages 1-12, 2000.

[10] J. Chang and W. Lee. A Sliding Window Method for Finding Recently Frequent Itemsets over Online Data Streams.

Journal of Information Science and Engineering, Vol. 20, No. 4, July, 2004.

