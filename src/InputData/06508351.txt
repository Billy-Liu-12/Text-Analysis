Relevant Association Rule Mining from   Medical Dataset Using New Irrelevant

Abstract?Association rule mining (ARM) is an emerging research in data mining. It extracts interesting association or correlation relationship in the large volume of transactions.

Apriori based algorithms have two steps.  First step is to find the frequent item set from the transactions. Second step is to construct the association rule. If ARM applied with medical dataset, it produces huge quantity of rules; most of these rules are irrelevant to the transaction. These irrelevant rules consume more memory space and misguide the decision making. Here irrelevant rule reduction is important. This paper proposes the n- cross validation technique to reduce association rules which are irrelevant to the transaction set. The proposed approach used partition based approaches are supported to association rule validation. The proposed algorithm called as PVARM (Partition based Validation for Association Rule Mining). The proposed PVARM algorithm is tested with T40I10D100K and heart disease prediction. The performance analysis attempted with Apriori, most frequent rule mining algorithm and non redundant rule mining algorithm to study the efficiency of proposed PVARM.

The proposed work reduces large number of irrelevant rules and produces new set of rules with high confidence. It is much use to mine medical relevant rule mining.

Index Terms?data mining; association rule mining; frequent itemset mining; rule elimination.



I.  INTRODUCTION Association rule mining aims to extract interesting  correlations, frequent patterns, associations or casual structures among sets of items in the transaction databases or other data repositories [1], [8]. Association rules are widely used in various areas such as telecommunication networks, market and risk management, inventory control etc. The problem is usually decomposed into two sub problems. One is to find those itemsets whose occurrences exceed a predefined threshold in the database; those itemsets are called frequent or large itemsets. The second problem is to generate association rules from those large itemsets with the constraints of minimal confidence. In many cases, the algorithms generate an  extremely large number of association rules, often in thousands or even millions [1]. It is nearly impossible for the end users to comprehend or validate such large number of complex association rules, thereby limiting the usefulness of the data mining results. Several strategies have been proposed to reduce the number of association rules, such as generating only ?interesting? rules [7], generating only ?non redundant? rules [8], or generating only those rules satisfying certain other criteria such as coverage, leverage, lift or strength. Zaki MJ proposed the closed itemset based rule mining algorithm. The frequent concept lattice is a very concise representation of all the frequent itemsets and the rules that can be generated from them. This kind process is required most cost effective then simple traditional approach.

The proposed algorithm are used the partition based technique to extract the frequent itemset. It is created the non overlapping and randomized three partitions. It is created the frequent itemsets and rules form the first partition. Another two partitions are used to validate the rule which is derived from first partition. Association rule mining problem extract the rules from complete transactions. It need to mine rule from other partitions. So this algorithm creates new set of partition to mine rules. It is repeated till user specified times for cross validation the association rules. This algorithm is also called as PVARM (Partition based Validation for Association Rule Mining). The PVARM mines the frequent itemsets using the apriori based candidate generation. The proposed algorithm is implemented with the synthetic datasets. It produced the good performance and it compared with Apriori, most interesting rule mining algorithm [7] and non redundant algorithm [8].

This paper proposes the PVARM algorithm for mining most interesting rules. The organization of the rest of the paper is as follows. Section II provides the definition of association rule mining. The existing algorithms detail in section III.

Section IV shows proposed PVARM. Section V discusses the experimental design of PVARM algorithm. Section VI    discusses the results. Finally, Section VII concludes the paper and discussed the future enhancement.



II.  ASSOCIATION RULE MINING This paper uses the standard definition of association rules  [1], [11], [12]. Let D be a set of n transactions such that D={T1, T2, T3,..,Tn}, Where Ti?I and I is a set of items, I = (i1, i2, i3, .. ,im}. A subset of I containing k items is called a k- itemset. Let X and Y be two itemsets such that X ? I, Y? I, and X ?Y= ?. An association rule is an implication denoted by X=>Y where X is called antecedent and Y is called the consequent.

This section proceeds to define association rule metrics.

Given an itemset X, support s(X) is defined as the fraction of transactions Ti ?D such that X?Ti. Consider P(X) the probability of appearance of X in D, and P(Y|X) the conditional probability of appearance of Y given X. P(X) can estimated as P(X)=s(X). The support of a rule X Y is defined as   s(X Y) = s(XUY). An association rule X Y has a measure of reliability called the confidence, defined as c(X Y) = s(X Y)/s(X). Confidence can be used to estimated P(Y|X): P(Y|X) = P(XUY)/P(X) = c(X Y). This paper uses third metric called lift, defined as l(X Y) = P(XUY)/(P(X) P(Y)) =c(X Y)/s(Y). Lift quantifies the relationship between X and Y. In general, a lift value greater than 1 provides strong evidence that X and Y depend on each other. A lift value below 1 state X depends on the absence of Y or vice versa. A lift value close to 1 indicates X and Y are independent.

The problem of mining association rules is defined as finding the set of all rules {X} {Y} such that s(X Y)>= ? , c(X Y)>= ? and l(X Y)>= ?  , given a support threshold , a confidence threshold and a lift threshold. An itemset X such that s(X)>= ? is called frequent. An association rule {X}  {Y} such that s(X Y)>= ? , c(X Y)>= ? and l(X Y)>= ?  is called valid association rule.



III.  EXISTING ALGORITHMS Roberto J. Bayardo Jr. and Rakesh Agrawal proposed the  most interesting rule mining algorithm. This paper shows that a single yet simple concept of rule goodness captures the best rules according to any of them [7]. Mohamed M.Zaki [8] presented a new framework for association rule mining based on the novel concept of closed frequent itemsets. The set of all closed frequent itemsets can be orders of magnitude smaller than the set of all frequent itemsets, especially for real datasets. These two algorithms are compared with proposed PVARM in section.



IV.  PVARM ALGORITHM Input : Dataset (D), Minimum support Threshold (?), Minimum confidence Threshold (?), Minimum lift Threshold (?), Number of times train/test/validate (n), Train sample fraction (?), Test sample fraction (?)   Output : R rules  Step 1: For I = 1 to n do Step 2: Create the partition Dtr ,  Dte and  Dva based on  ? and ? Step 3: Generate 1- itemset Search frequent k ? itemsets on Dtr for k ? {1?k} Compute train_support supp(X, Dtr) using (1) Step 4:  Generate rules from the generated frequent item sets.

For each rule x  y ? Dtr  Compute train_confiden conf(x  y) on Dtr using (2) Compute train_lift lift(x  y) on Dtr using (3)  Step 5: Let the rules set be Rtr.

Eliminate rules from Rtr Such that  supp(x  y)<? or conf( x  y)<? or lift( x  y)< ? Step 6: // validate the rules using test set // Validate rules Rtr on Dte.

set Rte = Rtr For each frequent itemset X  compute test_support supp(X, Dte) using (4) For each rule x  y ? Dte  compute test_confiden conf(x  y) on Dte using (5) compute test_lift lift(x  y) on Dte using (6) Eliminate rules from Rte  Such that supp(x  y)<? or conf( x  y)< ? or lift( x  y)< ?  Step 7: // validate the rules using validate set // Validate rules Rte on Dva  set Rva=Rte For each frequent itemset X  Compute validate_support supp(X, Dva) using (7) For each rule x  y ? Rva  Compute validate_conf conf(x y) on Dva using  (8) Compute validate_lift lift(x  y) on Dva using (9) Eliminate rules from Rva  Such that supp(x  y)<? or conf(x y)<? or lift(x y)< ?  Finally Let the rules set be RI   = Rva  Next I Step 8:  Get intersection of n rule sets and compute the average rule metrics with (10), (11) and  (12) R = R1 ? R2 ? R3 ? ?. ? Rn.



V.  PVARM ALGORITHM The proposed PVARM algorithm need the following  inputs: the dataset (D) which is contained the transaction sets.

Minimum support (?), Minimum confidence (?) and Minimum lift (?) are used to control frequent itemsets and association rules. Train sample fraction (?) and test sample fraction (?) are managed the size of the partitions. Finally this algorithm get the input n which is used make no of times cross validation.

The output of algorithm is produced the most strong and    interesting rules which are reliable to the transaction sets. In step 2, it has small procedure which is gone to make three transaction set partition. This procedure is used Fisher-Yates shuffle algorithm [9] to shuffle the transaction set. The Fisher? Yates shuffle is unbiased, so that every permutation is equally likely. The modern version of the algorithm is also rather efficient, requiring only time proportional to the number of items being shuffled and no additional storage space. The basic process of Fisher?Yates shuffling [9] is similar to randomly picking transaction id from transaction id set, one after another until there are no more left. What the specific algorithm provides is a way of doing this numerically in an efficient and rigorous manner that, properly done, guarantees an unbiased result. It has O(n) time complexity to shuffle the transaction set.  The shuffled transaction set is divided into three partitions in the manner of logical non overlapping randomized or non randomized partitions. These three partitions are called as Dtr, Dte and Dva. The Dtr are controlled by the train sample fraction ? percentage from the shuffled transaction set. The remaining shuffled transaction set divided into test sample fraction ? percentage. That partition is called Dte. The criteria of test sample fraction as follows: ? <= ?.

The size of Dtr and Dte are discussed in the section III. This algorithm is not considered the size of partition Dva. In step 3, the set of frequent 1-itemsets is found. This set is denoted L1.

L1 is used to find L2, the set of frequent 2-itemsets, which is used to find L3, and so on, until no more frequent k-itemsets can be found, and then algorithm ceases. In the cycle k, a set of candidate k-itemsets is generated at first. This set of candidates is denoted Ck. It computes the support for frequent itemset using (1). That is called train_support. If the train_support is not satisfied minimum support (?), that frequent itemset eliminate from list to further process. The remaining frequent itemset are used to construct further frequent itemset or association rule mining. Step 4 is to construct the association rule from the frequent itemsets which are found from the Dtr .

)1.......(....................

|| ||  )(sup tr tr xy  D D  yxp =?  After the association rules are mined, It calculates the train_confidence and train_lift using (2) and (3). Such that it eliminates the rules which are train_confidence( x ? y) <  ? or train_lift( x ? y) <  ? .In step 5, the remaining rules are set into the Rtr.

)2.........(....................

|| ||  )( tr x  tr xy  D D  yxconf =?  )3(....................

|||| ||||  )( tr y  tr x  tr xy  tr  DD DD  yxlift ? ?  =?  Step 6 and step 7 are going to validate the derived rules Rtr with Dte. Step 6, Let the Rtr set to be Rte. this step find the  frequent itemset form each rules and compute the support threshold for the frequent itemset on Dte using (4).

)4.......(....................

|| ||  )(sup te te xy  D D  yxp =?  Also it computes test_confidence and test_lift for the rules on Dte using (5) and (6). The new metrics are assigned to the rules. These rules are eliminated which are not satisfied the following values ?, ? and ?.

)5......(....................

|| ||  )( te x  te xy  D D  yxconf =?  )6.....(....................

|||| ||||  )( te y  te x  te xy  te  DD DD  yxlift ? ?  =?  Step 7, After the elimination of unsatisfied rules, those rules are stored in the Rva. It also same as step 6 but it used the Dva to compute the support, confidence and lift using (7), (8) and (9). The rules are filtered with modified measures which are not satisfied the ?, ? and ?.

)7...(..............................

|| ||  )(sup va va xy  D D  yxp =?  )8.....(..............................

|| ||  )( va x  va xy  D D  yxconf =?  )9........(....................

|||| ||||  )( va y  va x  va xy  va  DD DD  yxlift ? ?  =?  These rules are set in the RI. These steps 1-7 are repeated n times to achieve cross validation of rules. Finally the algorithm produced n set of rules. Repeated rules are eliminated from the list and it computes the average of metric for rules using (10), (11) and (12). These rules are called most interesting strong and valid to transaction sets.

? =  ?=? t  i iDyxp  t yxp  )10......(].........),[(sup1)(sup  ? =  ?=? t  i iDyxconf  t yxconf  )11(..........].........),[(1)(  ? =  ?=? t  i iDyxlift  t yxlift   )12...(..........].........),[(1)(

VI.  RESULT AND DISCUSSION All experiments described below were performed on a 1.6  GHz Intel Celeron Dual-core PC with 1GB of main memory    and 160GB of HDD, running Microsoft Windows 7. This paper used own implementation of most interesting rule mining algorithm and non redundant rule mining algorithm.

Also the proposed PVARM were coded in MATLAB.

The candidate generation and the support counting processes require an efficient data structure in which all candidate itemsets are stored since it is important to efficiently find the itemsets that are contained in a transaction or in another itemset. The proposed algorithm implemented with hash-tree data structure [4]. The time complexity and space complexity of hash tree is O(1). In order to efficiently find all k-subsets of a potential candidate itemset, all frequent itemsets in Lk are stored in a hash table.

The effect of filtering association rules are based Dte and Dva partitioned datasets. The association rules are tested for generality and validity by partitioning the input dataset into Dtr, Dte and Dva. Building the Dtr, Dte and Dva samples is repeated several times. This association rule algorithm produces different sets of rules with different samples, where each set of rules has different rules have slightly lower or higher metrics. The rules are valid on three samples. This paper extends the definition of association rules, given in Section II, to have three sets of metrics per rule based on Dtr, Dte and Dva. That is, each rule has nine metrics in total. In general, metrics on the training set are used only for search purposes, and metrics on the test set are used to validate rules and are taken as the actual rule metrics.This proposed algorithm computes three sets of rules, Rtr on Dtr ,Rte ? Rtr such that Rte also has metrics above ?, ?, ? on Dte and Rva ? Rte such that Rte also has metrics above ?, ?, ? on Dva. The computation of Rte is as follows. Each association has three sets of metrics, one for Dtr , one for Dte and one for Dva. This method search association rules based on Dtr to get Rtr based on thresholds ?, ?, ?. This approach sets Rte = Rtr. This process then compute support, confidence, and lift for each rule in Rte based on Dte. Rules whose test metrics on Dte are below ?, ?, ? are filtered out from Rte. This process is repeated a number of times (t) to achieve basic cross validation and to eliminate rules that cannot be generalized.

The association rules are tested for generality and validity by partitioning the input data set into a Dtr, Dte and Dva. A valid rule must have minimum metrics on Dte and Dva. The experiments show the importance of filtering rules on the test by varying k. The reduction is small in the numbers of association, with a reduction of about 10% - 15%. The reduction becomes much more important for the number of rules. For k =2, the impact is small in most cases, which indicates most rules can be generalized. For k =3, the reduction is more than 50%, providing evidence that many rules one particular to the Dtr. At k = 4, the number of rules in Dtr is about 30% of the total with a reduction of about 70%, providing evidence that most rules may be particular to Dtr.

The trend indicates there will a combination explosion of rules  that are valid only on Dtr. So that the reason this work can achieve memory and time efficiency.

The difference in the relative number of patterns for associations and rules can be explained by the fast that association and filtered Dtr, Dva and Dte based only one support, but rules require support, confidence and lift to be greater than or equal to the respective thresholds in Dtr and Dte. This kind of validation methodology made the database depended rules to compare with other methodology. The previous research works are attempted based on relationship or correlation analysis of transactions but the proposed work implies database dependent rules.

The proposed algorithm also tested with the T40I10D100K and heart disease prediction. The proposed PVARM are compared with traditional apriori, most interesting rule mining algorithm and non redundant algorithm. Table 1 shows the comparative study of no. of association rule mined from T40I10D100K and heart disease prediction dataset. The minimum support set as 50% and the minimum confidence set as 50%. The results are summarized in the Table 1. It reduced the 50% rules compared with non redundant algorithm, 42% rules are reduced from most interesting rule mining algorithm and 80% of rules reduced to compare with traditional aprori.

TABLE 1  NO. OF ASSOCIATION RULES IN APRIORI, MIR, NRRM AND PVARM WITH MINIMUM_SUPP=50%  Datasets Apriori MIR NRRM PVARM T40I10D100K 40321091 678542 22700 11209  Heart disease prediction 3210 2470 1056 710    The comparative study of proposed PVARM algorithm is shown in Figure 1 and 2. These figures are showed the comparative study of traditional apriori, most interesting rule mining algorithm, non redundant rule mining and proposed PVARM with T40I10D100K, mushroom, chess and heart disease prediction. These four datasets are having different transaction size, item size and other behaviors. So that the reason this selected to make the experimental studies.  The figure?s x axis has support level 10 to 100 percentages and y axis is carrying execution time in seconds. This experimental study considered the minimum support to count execution time. It does not consider the confidence and lift. But this work fix the minimum confidence=50% and lift=20% to count the execution time. From this experimental study, the proposed PVARM algorithm is performed well as compared to traditional Apriori, most interesting rule mining algorithm and non redundant rule mining algorithm. This work also concluded that the proposed PVARM is better algorithm to mine transaction relevant rules from synthetic and real datasets.

The following Figures 1 and 2 show  comparison of execution time  between Apriori, MIR, NRRM and proposed    PVARM using T40I10D100K  and Heart disease prediction dataset.  The proposed algorithm is performed better than Apriori, MIR, NRRM using T40I10D100K (larger dataset) and heart disease prediction (small dataset).

10 20 30 40 50 60 70 80 90 100        Support %  E xe  cu tio  n tim  e (in  s ec  )  T40I10D100K      Apriori  MIR NRRM  PVARM   Figure 1: Comparative study of execution time of  T40I10D100K dataset    10 20 30 40 50 60 70 80 90 100          Heart disease prediction  Support %  E xe  cu tio  n T  im e  (in s  ec )      Apriori  MIR NRRM  PVARM   Figure 2: Comparative study of execution time of Heart  disease prediction dataset

VII. CONCLUSIONS This paper focused on two main research issues. The first  issue is the large number of rules that are obtained by the standard association rule algorithm. The second issue is the validation rules on an independent set, which is required to eliminate unreliable rules, or rules that cannot be generalized.

In order to validate rules, this paper used the train-test-validate approach that uses three disjoint samples from a data set to search and validate rules. The proposed algorithm performs several train, test and validate cycle achieve the relevant rules.

Experiments on T40I10D100K and heart disease prediction,  these dataset are studied the impact of constraints and elimination of unreliable rules with validation on the test set.

The reduction in output size provided by validation is significant. The proposed method is mush useful to mine medical association rules. These methods may provide more efficiency compare to traditional Apriori, non redundant rule mining framework and most interesting rule mining and save the time for irrelevant rule findings from the dataset. In future this algorithm can apply parallel environment and distributed environment.

