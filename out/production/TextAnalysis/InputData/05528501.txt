Meticulous Classification Using Support Vector Machine for Brain Images

ABSTRACT    The objective of medical image retrieval system is to  provide a tool for radiologists to retrieve the images  similar to query image in content. Classification is an  important part in retrieval system. This paper proposed a  meticulous classification of MR-brain images using  support vector machine (SVM). We used both texture and  shape feature to express images, and then applied  statistical association rule miner (StARMiner) algorithm  to compute weight coefficient of each feature. A classifier  based on SVM was trained, the parameters of which were  optimized via many experiments. The result of glancing  classification could achieve 92.10%. Meticulous  classification can be applied in special body part retrieval  system for retrieving more accurate images and reducing  computational load.

1. INTRODUCTION  The development of digital medical imaging equipment  has changed the way of diagnosis and treatment over the  past decades. Nowadays, a great increasing number of  digital medical images are produced in hospitals and  medical research centers. Many hospitals purchase Picture  Archiving and Communication Systems (PACS) to  manage the huge medical images. Medical images not  only present the important anatomical information and the  function of body tissue, but also the characteristic of  health or illness in a visual way. They are of great value for  diagnosis, therapy, research, and education. In clinical  diagnosis process, it is beneficial and significant to find  the existing images with the same modality, the same  anatomical organization and the similar visual    characteristics of disease area as the query image. Clinical  decision support system and computer-aided diagnostics  are on the rise and create a need of powerful data  management and retrieval. Presently, PACS can only offer  a text-based retrieval. Therefore, Content-based medical  image retrieval (CBMIR) has been proposed from the  medical community for the inclusion into various  applications [1].

Categorization is an important step in large image  retrieval systems. It can reduce the computational load and  improve the retrieval accuracy. System of image retrieval  in medical applications (IRMA) [2] classifies finely  images according to anatomical areas, modalities, view  points and functional systems. The system of IRMA needs  to manually determine a four-axis code. Hayit Greenspan  in [3] present an automatic categorization method for  X-ray images, using the Gaussian mixture modeling  (GMM) and Kullback-Leibler (KL) measure, the aim of  which is to pre-label image categories according to body  parts and view points, such as hand, skull, skull side  view.

In this paper we tentatively put forward a meticulous  classification for axial brain images in typical MR-T1  scanning modalities. The brain images have been defined  into 14 categories according to the difference of  anatomical structure and content of images in a sequence  of axial brain images. Moreover, we have discussed how  to apply the meticulous classification into our brain  retrieval system so as to improve the retrieval accuracy  and reduce the computational load.

2. METHOD  The program of meticulous classification experiment is  mainly divided into three steps: feature extraction, feature  optimization and classifier design using SVM.

2.1. Feature Extraction  The first step is to extract the features of image data.

Before feature extraction, images are filtered by a  smoothing filter and the backgrounds of them are removed.

Then, firstly in the process of feature extraction, we use  3-scale SWT to decompose images into 12 sub-images.

The value of mean, variance, energy, consistency, contrast  and correlation of each sub-image are computed. Secondly    Fourier descriptor proposed by Zhang [4] is applied to get  the shape features: the centric distance of each edge pixel  is calculated; the gray value of edge pixel in the  preprocessed image serves as a weight coefficient of  centric distance of this edge pixel; then do discrete Fourier  transform towards the centric distance of edge pixel. We  selected the foremost eight Fourier coefficients as the  shape features.

978-1-4244-8012-8/10/$26.00 2010 IEEE 99  2.2.  Feature Optimization  Once the primitive features of images are extracted, the  next step is to optimize them. In this process, there are two  steps:  1) Gaussian normalization:  ' ( ) /(, ,f f m ki j i j j )?= ? ?          (1)  k is an integer, here, we chose the maximum k that  normalizes all features to [-0.5, 0.5].

2) Statistical association rule miner algorithm [5] is  applied to compute weight for each feature, which aims at  identifying the most relevant features and giving them  large weights from each image in the original literature.

We use this method to weigh features as follows:  T is the set of all training images. Tx is the set of images  from x-th class. f j is the j-th feature in the feature vector.

( )Tf xj?  and ( )Txf j? are respectively the mean and the  variance in the image set Tx. Three thresholds must be  given in this algorithm: (1) : the minimal difference  between the mean of f j in the set of x-th class and the mean  of f j in the set of non x-th class. (2) :  the maximal  variance of f j in the set of x-th class. (3) : the  minimal confidence level that refuse H0.

min??

max??

min?

If the x-th class and f j satisfy the following three  conditions, we say that there is an associate rule between  x-th class and f j, marked it as x f j? .

( ) ( ) minT T Tx xf fj j  ? ? ?? ? ? ?     (2)  ( ) maxTxf j  ? ?? ?                    (3)    : ( ) (0 )H T T Tx xf fj j  ? ?= ?           (4)  x f j? means that f j distinguish well the x-th class  from other classes. The feature that generates the most  associate rules for classes is the most discriminative. So  the weight of f j can be defined as:  10w rj j q= +                    (5)  rj is the number of associate rules for f j. When q=0, it  means removal of the feature that dont generate any rules;  when q=1, it means reserving all features.

2.3. SVM for Classification  SVM is a supervised learning method based on the  statistical learning theory and the Vapnik-Chervonenkis  dimension [6]. For a set of separable two-class objects, an  SVM finds the unique separating hyperplane which has  the maximum margin (denoted with 2 / w ) in the feature  space. The hyperplane H1 and hyperplane H2 respectively  define the border of positive class objects and the border  of negative class objects. Those objects that support the  hyperplane H1 and hyperplane H2 are called support  vectors. If there exists no hyperplane that can split the  positive class objects and negative class objects, a soft  margin will decide a hyperplane that splits the positive  class objects and the negative class objects as clearly as  possible, and while maximizing the distance to the nearest  clearly split objects. The soft margin method introduces a  slack variable i? , which measure the degree of  misclassification of the object xi.

( ) 1 ,iy w x bi i ?? + ? ? i n? ?  1 ,  { }1, 1yi? + ?    (6)  The optimization becomes a trade off between a large  margin and a small error penalty. The optimization  problem transforms to:  Min ( ) ( )1 w, w w   C ii  ? ? ?= ? + ?         (7)  s.t. ( )( ), 1y x w bi i i?? + ? ? ,    1 i n? ?     (8)  C is a parameter which trades classifier generality for  accuracy on the training set, and ( )x?  maps the original  input space into a high-dimensional feature space.

The dual of the SVM can be shown to be the following  optimization problem:    Max ( ) ( ) ( )11 , 12n nQ y yi i j i j ii i j? ? ??= ? ? ?? ?= = x xj  (9)  s.t. , 0 , 10   n  yi ii  ? =?= Ci?? ? i n? ?   (10)  This problem has a unique solution. Given ?? is the  solution of above problem, so  ( )* *   xi  n  w yi ii  ? ?= ?=         (11)  We can solute b* through any i?? . The final decision  function is:  ( , )   ( ) sgn  n  y K x x bi i i  i  f x ?? ?? ?? ?+?? ?? ?=? ?

=   (12)  K(x, xi) is a non-linear kernel function that replaces the  dot product ( ( ), ( ))x xi? ? .

Some common kernels include: Linear kernel,  Polynomial kernel, Radial Basis Function, Hyperbolic  tangent kernel.

Multi-class problem is usually done by reduced into  multiple binary class problems. In this paper,  one-against-one method applies to solve this multi-class  problem according to [7].

3. EXPERIMENTS & DISCUSSION  We collect 784 axial brain images in typical T1-MR  scanning modalities from 226 patients. These images take  Frankfort horizontal plane as reference plane, and are all  from medical imaging department of affiliated hospital of  tianjin medical university. Images are finely defined into  14 categories. The difference of vertical brain position of   two adjacent categories is about 1.0~2.0cm. Every    category images has different anatomical structure, tissue  shape and content from other category in a same sequence  of axial brain images. A representative image of each  category is shown in Fig.1. The dataset is divided into  training set and test set. Each class in training set contains  15 images, and the number of testing images of each class  is shown in Table 1. The images from every category are  all from different patients, so as to avoid poor training  caused by that the images from the same patient have a  certain degree similarity.

According to feature extraction method previously  mentioned, we selected the foremost eight Fourier  coefficients as the shape features. So each image is  expressed by a 80-dimensional feature vector. In the step  of Gaussian normalization, we set three different value of  k, 3 ,10 ,15? ?? ? ??  in order to verify the impact of  values range of features on the classifier when feature  dimension is higher. In StARMiner algorithm, three  thresholds must be given complying with the principle that  the most discriminative feature generates 14 associate  rules and the least generates zero associate rules. Through  a lot of experiments, we  set , and set q=1,  reserving all features.

0.01, 0.15, 0.95maxmin minu ? ?? = = = ?

We respectively take the polynomial, radial basis  function, hyperbolic tangent function as kernel function in  SVM classifier to get comparison results. What more, a  comparison between a classifier using K- nearest neighbor  (KNN) and a classifier using SVM shows in Table 1.

Table 2 shows the detailed classification result of the best  classifier.

Accurate classification rate is defined as the  percentage of images classified to i and rough  classification rate of i-th class as the percentage of images  classified to i-1, i or i +1 in testing i-th class. Specially, the  rough classification of 1-th class is defined as the  percentage of images classified to 1-th class or 2-th class;  the rough classification of 14-th class is defined as the  percentage of images classified to 13-th class or 14-th  class.

Experiment shows the range of new features after    Gaussian normalization by 3 ,10 ,15? ?? ? ??  are  respectively [-2.0196, 1.3144], [-0.4876, 0.4116],  [-0.3572, 0.2744], and the corresponding accurate  classification rates are respectively 0.17%, 62.05%,  59.46%. So, we let k is 10 in Gaussian normalization. The  comparison between different kernel function used in  SVM classifier shows that radial basis function is  observably better than others. When in radial basis  function, the classifier is optimal.

2 1? =  Because distinguish between images from two  adjacent classes is a little blurry and illness that appears in  one class images may also appears in its two adjacent class  images. Table 2 shows that images of each class in testing  set are almost classified to its true class or its four most  adjacent classes. The average of accurate classification  rate of the SVM classifier with radial basis function as  kernel is 62.05% and the average of rough classification  rate 92.10%.

We apply this classifier to automatically classify  images in our brain image database. In the process of  retrieval, the images in database, whose class label are in  the scope of [i-3,i+3], are selected to compare with the  query image which is supposed to have been classified  into i-th class. Applying this classifier in our system can  effectively prevent retrieving images which are not similar  to the query image on anatomical structure and content  from the database.

Fig.1. Representative images of each category    Table 1:  A Comparison of Classification Using KNN and SVM  12 =?

Table 2: Detailed Result of Classification Using SVM with RBF kernel ( ) 2 1? =  4. CONCLUSION  The main motivation of this work is to prevent retrieving  the images not similar to the query image on anatomical  structure and content in the database. Through a machine  learning method, we hope to achieve better precision in    retrieval and what is more, reduce the space of retrieval, so  as to improve efficiency. In this paper, we put forward a  meticulous classifier and plan to use it in our brain image  retrieval system to achieve our aforementioned intent.

Meticulous classification will be extended to other body  parts if it succeeds in brain image retrieval system. These  are our future work.

