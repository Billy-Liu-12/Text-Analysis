An Incremental Updating Algorithm forOnline Mining Association Rules

Abstract?Aimed at the limitation of the current FUP algorithm, which readily led to the low efficiency of frequent itemset of the updated database, a novel association rules mining algorithm named QAIS which is different from the classical dual phrase was proposed. On the basis of QAIS, an improved association rules mining algorithm named AIU was put forward further. AIU can efficiently maintain association rules when the transaction database was updated. At the same time, the minimum support and confidence were not changed. Experiments showed the proposed a1gorithm was appropriate to online association rules mining?  Keywords: data mining; association rules; incremental updating; online mining

I. INTRODUCTION  Two-phrase theory to find frequency items set is core idea of classical association rules mining algorithm [1][2].

Despite there are many improved works, those existing algorithms still have two questions that can't be overcome: Holding too much middle candidate frequent item sets and scanning database iteratively. To find satisfactory minimum frequent item sets that satisfy supports degree threshold consume a large amount of I/O time. Those algorithms cannot meet the needs of some application scenario. Traditional association rules mining algorithm generally regard database as static database present, In other words, association rule mined from the database only reflects the present status of the database. In order to make association rule mined out reliable in practice, when data in database are altered, association rules mining algorithm should produce a new rule or replace the some existing rules. So we must study incremental maintained association rule mining algorithms.

The key issue of data mining is to deal with a large amount of data, and to maintain rules efficiently.

Therefore, we should solve the following two problems emphatically: 1) Designing high effective algorithm, mining the association rules; 2) Designing valid algorithm to upgrade and maintain the association rules that were already mined. And the second question is as important as the first question. There is a large amount of research works about second question, but the mostalgorithms is to solve association updating problem in the promise of minimum support degree s, and minimum confidence degree c does not change.

Such as FUP [3] is a famous Incremental updating algorithm that incrementally maintains association rules  found already under the condition of two thresholds s and c does not change. Ref. [4] proposed an incremental algorithm that runs under the condition that transaction database DB does not change while two thresholds changes. Ref. [5] improved above-mentioned two algorithms.

When a new record is added to the database, supposed that two threshold values do not change, it can simplify updating question through finding new frequent item sets.

New association rules can be produced by new frequent item sets. A simple method to find new frequent item sets is to rerun association rules mining algorithms for whole database, But it is obvious unreasonable: 1) those old frequent item set that already were found will be unuseful; 2) It is expensive to scan database from the beginning.

FUP algorithm utilizes the frequent item sets that have already been produced. Finding new frequent item sets, candidate item sets would be pruned abundantly.

Experiments prove that the speed of FUP is 2 to 16 times as large as  Apriori or DHP, and FUP can reduce the size of candidate item sets notably. But FUP algorithm has some defects also. The foundation of FUP is similar to Apriori and DHP, which consists of much iteration.

Because FUP needs produce candidate item sets based on Apriori algorithm, though it execute pruning for original candidate item sets while scanning newly-increased database part db, its efficiency still very much low while producing frequent item sets after updating.

This paper proposes a fast association rules mining algorithm (Quick Aggregated Item Set, In brief QAIS) that need not produce candidate item sets first, and then present an new incremental algorithm AIU (Aggregated Increment Updating) based on QAIS algorithm. At the same time, optimization of proposed algorithm is discussed as well. Experiments results from synthesizing data sets prove validity and superiority of the new algorithms finally.



II. QUICK ASSOCIATION RULES MINING ALGORITHM QAIS  Suppose that SubItemSet is all subset collects of transaction Ti (Ti?DB) that does not contain empty sets, and SubItemSet is claimed as aggregation item sets, denoted AggItemSetDB briefly. We request all item subset of each transaction in transaction database, and store them into accumulation of item sets (majority in main memory and to be written to auxiliary memory on the amount of item sets reaching a threshold value). At   DOI 10.1109/WISM.2009.37     the same time, we write down arisen frequency of every item sets (support degree), and then scan overall aggregation item set to filter item sets that satisfy constraint of special minimum support degree to form the frequent item sets. Finally, we produce optimized association rules according to special confidence degree finally.

Symbol?s meaning is described as follows: DB: Transaction database; minsup: minimum support degrees; minconf: minimum confidence degree; max_T: the length of largest transaction. Suppose that all subsets of transaction that consist of i items is denoted as subitemseti, all subsets of aggregation item sets that consist of i items is denoted as aggitemseti.

First, we scan DB and fetch all subsets of every transaction into subitemset1, subitemset2 ? subitem- setmax_T , and match with every aggregation item sets aggitemset1, aggitemset2, ?,aggitemsetmax_T respect- tively , produce aggregation item sets that stored all item set of transaction AggItemSetDB. We find frequent item sets L that satisfies minimum support degree minsup in AggItemSetDB. Finally, we use eliminating redundant rule algorithm to preprocess frequent item sets, and then produce strong association rules.

Algorithm1 QAIS Input: DB, minsup, minconf, max_T.

Output: association rules that have get rid of  redundancies.

(1)L= ?; (2)AggItemSetDB=gen_AggregatedItemSet(DB); //generating Aggregated Item Sets (3)for each aggitemseti? AggItemSetDB begin (4)  Li={cj?aggitemseti |cj.count?minsup };  (5)  L= i T  i L max_  1=? ;// generating frequent item set L (6)end for (7)GenerateRules(L,minconf).

// generating association rules confidence In order to improve the running efficiency of  algorithm, the matching operation among all of not-empty subsets of transaction and produced aggregation item sets utilizes set operation such as intersecting, union and subtracting.

Algorithm2 gen_AggregatedItemSet(DB) (1)aggitemset ? ?; (2)for each transaction T ? DB begin (3)  subitemset=subset(T); (4)  for each subitemseti ?subitemset begin (5) New_QAIS=subitemseti ? aggitemseti; //Subtract i-  item Aggitemsets from transaction?s i-subitemset (6)  for each nj?New_QAIS  begin (7)add the item nj to aggitemseti; //Append itesmsets  nj  to i-item Aggitemsets?insert in term of sequnce? (8) nj.count =1 (9)  end for (10) Old_QAIS=aggitemseti? subitemseti; //Operate  intersection between i-item Aggitemsets and Transaction?s i-subitemset  (11)  for each oj? Old_QAIS begin (12)  oj.count++ (13)  end for (14)  end for (15) end for (16) return aggitemset; QAIS needs only one-pass scanning on database and  forms an aggregation item sets (AggregatedItemSet) that hold support degree counts. Aggregation item sets can replace the primitive database. It reduces the time to search the database repeatedly. Because merging and decomposing of frequent item sets can be finished in this aggregation item sets, it can reduce notably the time to find out all frequent item sets. QAIS algorithm can be used in mining association rules for initial database.



III.  INCREMENTAL UPDATING ALGORITHM AIU  A?  Symbol meaning and relevant concept Let L be the frequent item sets of database DB, L?  denotes frequent item sets of DB ? db, D, d, UD denote the number of transactions in DB, db, DB ? db respectively. Support degree counts X.supportD ?

X.supportd ? X.supportUD denote the number of transaction in DB, db, DB ? db that contain X respectively.

After some updating activities, a new incremental transactions db is added to the original database DB, and d denotes the number of transactions in db. For special minimum support s, an item set X is frequent in the updated database DB ? db if the support of X in DB ? db is no less than s, i.e., X.support ? s ? (D+d).

Thus the essence of the updating association rules is to find the frequent item set L' in DB ? db.

Lemma 1. An item set X in the original frequent item sets L is a loser for the updated database DB ? db (i.e., X will not appear in the new frequent item set L'), if and only if X.supportUD <s? (D+d).

Proof. Lemma 1 obviously comes into existence Based on the definitions of minimum support degree and frequent item sets. Note that a frequent item set in L may not be a frequent item sets in L', on the other hand, an item sets X not in L, may become a frequent item set in L'.

Lemma 2 An item set X not in the original frequent L can become a winner for the updated database DB ? db (i.e., X being included in the new frequent item sets L') if and only if X.supportd ? s?d.

Proof. Since X is not in the original frequent item sets L, X.supportD <s?D.

if X.supportd <s? d, then X.supportUD?X.supportD ?X.supportd < s ? (D+d). That is, X cannot become a frequent item sets in the updated database DB ? db. Thus we have the lemma.

B.   Basic idea of AIU algorithm     There is a simple method to incremental updating for association rules based on QAIS algorithm. That is, we scan db one-pass and use QAIS algorithm turn into aggregation item sets AggItemSetdb, compare with initial aggregation item set AggItemSetDB being produced before, if item sets in AggItemSetdb appear in AggItemSetDB, we calculate {I1, I2, ?, In }. supportD = {I1, I2, ?, In }. Otherwise add it to AggItemSetDB.

Finally we scan AggItemSetDB to filter the item set that is no less than s?(D+d) and output to new frequent item sets L?.

Because the scale of AggItemSetDB and AggItem- Setdb may be very large, merging two sets and producing new frequent item set in sequence running way is expensive. Now we consider that the improvement of the mentioned-above simple incremental updating algorithm.

We can directly produce the new frequent item sets while combining aggregation item sets.

The improvement of this algorithm is producing new frequent item sets L' while updating original aggregation item sets AggItemSetDB. It needn't wait to scan huge AggItemSetDB until finish to update aggregation item set AggItemSetDB, Operational efficiency of the algorithm is improved effectively.

Suppose that the initial frequent item set produced by QAIS algorithm is stored in aggregation item sets AggIt- emSetDB, we can construct basic steps of incremental updating algorithm that need not produce the candidate set according to above-mentioned lemmas:  1) Scan newly increased database db; produce new aggregation item sets AggItemSetdb. For all item sets X belong to primitive DB frequent item sets, update its support degree counts X. SupportUD. After finish scanning, for all X?L, we can find all losers in L through examining condition X.supportUD <s? (D+d) (according to the lemma 1). After moving these losers, we can determine item set that should be reserved in initial frequent item set of updated DB.

2) According to the lemma 2, if X?AggItemSetdb and

X.supportd < s ? d, then X definitely cannot appear in DB ? db. So all item sets in AggItemSetdb whose support degrees is less than s?d will be pruned. In this way, the time to produce new frequent item sets L' is reduced.

3) While scanning db, we update X.supportUD for item sets that belong to AggItemSetdb. We produce new frequent item sets of DB ? db by combining reserved initial frequent item sets from L and new frequent item sets db.

For example: support that DB =4, d =3, s =50%, T1, T2, T3, T4 is four transactions in initial database DB, T1', T2 ', T3 ' is three transactions of newly increased database db. We produce newly increased aggregation item set AggItemSetdb for db by QAIS algorithm, which keeps all item sets of db and corresponding support degree counts.

Where item set A, C, E, CE and AC belong to frequent item set L of DB. Suppose that A.supportd =1, C.supportd =2, E.supportd =3, CE. supportd =2, AC.supportd =1, and  their corresponding support degree counts in initial frequent item set is A.supportD =2, C.supportD =3, E.supportD =3, CE.supportD =2, AC.supportD =2 respectively. According to AggItemSetdb, We have: A.supportUD=3<7*50%, C.supportUD =5>7*50%, E.supportUD =6>7*50%, CE.supportUD =4>7*50%, AC.supportUD =33 <7*50%, then A, AC become the losers, only the winners C, E, CE will be reserved, and appear in L' finally.

D, DE, CD, and CDE is not contained by L, but still appear in AggItemSetdb, and D.supportd =3>3*50%, DE.supportd=3>3*50%, CD.supportd =2>3*50%, CDE.

supportd =2>3*50%, are all no less than local minimum support degree, so D, DE, CD and CDE is the potential candidate item sets. And remaining AD, AE, ?, ACDE are all less than local minimum support degree, that is, we needn't consider AD, AE, ?, ACDE while producing L'.

Only D, DE, CD, and CDE might appear in L'. Have D.Support that D.supportD=1, then D. supported =D.

supportD+D. supportd=1+3=4>7*50%, so D is sure to appear in L'. In this way, the new frequent item sets of updated database DB ? db consist of C, E, D, and CE.

While producing new frequent item sets L' above- mentioned, no matter the item sets of AggItemSetdb belong to initial frequent item sets L whether or not, which will all be collected into initial aggregation item set AggItemSetDB. (If there already exists the uniform accumulated support degree counts then overpass, If does not exist and add into), we should guarantee that it always keeps the newest aggregation item sets AggItemSetDB of DB ? db.

C. Formalized description of AIU algorithm  Algorithm3 AIU (DB, D, db, d, minsupport) Input: Initial database DB (the total amount of  transactions is expressed as D), initial frequent item sets L , newly increased database db (the total amount of transactions is expressed as d), minimum support spending s.

Output: New frequent item sets L' of updated database DB ? db.

(1) L?=? ;// L??new frequent item sets (2)AggItemSetdb=AggregatedItemSet(db)  //scan db to  generate new-increasing Aggregated Item Sets (3)for each aggitemset_dbi ? AggItemSetdb begin (4)  Li?=?; //i-item new frequent item sets (5)  for each X?aggitemset_dbi begin (6)    if X ?  Li then begin  //item sets in initial  frequent item sets L (7)      X.supportUD = X.supportDB +X.supportdb ; aggitemset_DBi.X.count = X.supportUD ; //update  corresponding support count in AggItemSetDB (9)      if X.supportUD ? s? (D+d) then Li?=Li?? {X};  //add winner to L1? (10)    end if (11)    else begin // not in initial frequent item sets L if X?aggitemset_DBi then     // there already exists this item sets in AggItemSetDB, update corresponding support count  (13)       X.supportUD = X.supportDB +X.supportdb; (14)        aggitemset_DBi.X.count = X.supportUD; (15)      end if (16)   else begin // If this item sets is not in  AggItemSetDB, add it into new AggItemSetDB and initial support count  (17)        add X into aggitemset_DBi; (18)        aggitemset_DBi.X.count = X.supportdb; (19)     end if (20) if (X.supportd ?  s ? d and

X.supportUD ? s? (D+d)) then (21)   Li?=Li?? {X}; (22)    end for //corresponding 11th sentence  (23)  L?= max_  T  i=? Li?; (24)end for  //corresponding 3 th  sentence (25)return L?.

D.   Issue on flexibility of QAIS/AIU Algorithm The two proposed algorithm is efficient for small data  sets that the length of transaction is relatively short. When these algorithms are used in very large database in the real world to mine association rules, Validity and flexibility become key problems. Many early tactics to produce association rules from transaction database DB still assume that frequent can be stored into main memory.

To raise QAIS/AIU algorithm?s flexibility, a alterable method is: First of all, we divide the transaction database of samples into a lot of subsets, assure that every subset can be stored in main memory; Then, we construct frequent item sets for every subset; Finally, the association rules will be derived form combination of all frequent item sets of subset. We can introduce novel data structure such as transaction table or index table to help for constructing association rules also.



IV. EXPERIMENT ANALYZING We adapt method proposed in Ref. [3] to generate  transaction database in our experiments, and generating data sets algorithm can be download in website [6].

The main parameters used to synthesize database is listed as follows:  D: The size of original database DB; d: the size of newly increased database db; |T|: the average length of the transactions; |I|: the average length of potential frequent item sets; |L|: the amount of potential frequent item sets; N: the total item sets counts.

Three secondary parameters are described as follows: Sq: the length of aggregation item set to produce  potential frequent item sets; Ps: the length of buffering pool to store the potential  frequent item sets; Mf: the multiplier factor with relation to the buffering  pool.

In order to compare D and d in the same database, we  generate a database whose size is D+d first. Then we regard the former D transactions as original database DB,  and regard the later d transactions as newly increased database db. So, all transactions are generated under the same statistical model, corresponds to reality status of incremental updating database.

In experiments we set up |L| =2000, N =1000, Sq =5, Ps =50, Mf =2000. We denote Tx. Iy. Dm.dn to express a database, it denotes D =mk, d =nk, |T| =x, |I| =y. For example, T10.I4.D100. D1 denotes that the average length of the transactions is 10, the average length of the potential frequent item set is 4, the size of original database DB is 100k, and the size of newly increased database db is 1k. We regard relative execution time of the algorithm as metric to measure performance in the experiment. We verify the comparison of performance between AIU and FUP from three different respects.

Figure 1. AIU vs. FUP as I changing  Experiment 1. T10.Ix.D100. d1 Suppose that T =10, D =100k, d =1k, minimum  support degree s=2.00%, figure 2 describes the comparison of two algorithm:  We can find out from figure 1, as the average length of the potential frequent item sets increasing, the performance ratio of AIU and FUP are corresponding to improve too. It prove that not only the performance of AIU algorithm is higher than FUP algorithm?s in short mining mode, but also this kind of performance is more and more remarkable when mining long mode. This kind of characteristic is correlated with characteristic of the algorithm: in the whole course AIU algorithm only needs to scan db twice, scan DB once, and have nothing to do with the largest length of frequent item sets; However FUP algorithm is based on Apriori algorithm thought, it needs to carry on k times scanning to databases, Namely when the largest length of frequent item sets is more large, it need more times to scan database corresponding, so FUP reduce the efficiency of algorithms remarkably.

So when mining long mode, AIU algorithm is valuable more.

Experiment 2. T10.I4.D100.dx Suppose that T =10, I=4, D=100k, minimum support  degree s=2.00%, we choose the different value of d, compare the performance of AIU and FUP. The experiment results are showed as figure 2.

Figure 2. AIU vs. FUP as d changing   Figure 3. AIU vs. FUP as s changing  We can discovery, as the newly appending database increasing, the performance ratio of AIU and FUP is reduced slowly too. But the performance of AIU is still higher than FUP. Because the counts used to scan database of FUP algorithm is fixed, namely the total counts to scan transactions is k(D + d), and one of AIU algorithm is (D+2d). When d is larger and larger, and occupy the main part of overall database, the Superiority of AIU algorithm to FUP algorithm will reduce. We can find out from the experimental result too, it is most status of AIU algorithm that when d < < D.

Experiment 3. T10.I4.D100.d1, s=x% Suppose that T=10, I=4, D=100k, d=1k, we choose the  different values of minimum support degree s?the results of performance comparison of AIU and FUP is described as figure 3.

We can find that as minimum support degree is increasing, the performance ratio of AIU and FUP is higher. Because when minimum support degree is large, the smaller the size of frequent item sets is, the lower the time to prune and refinement for frequent item set is.

Therefore the most part of running time of algorithm will be translated to scanning of the database. We know the running time of AIU used to scan database is less than FUP?s naturally, so AIU?s superiority in performance especially shows out at this moment.



V. CONCLUSIONS Because QAIS/AIU algorithm adopt mining mode in  one-pass scanning database way and need not to produce candidate item sets, it have higher performance superiority than FUP potentially in theory. Experiments results from synthesized data sets have verified the performances of QAIS/AIU algorithm are feasible and efficient.The performance of the conventional association rules mining algorithm is obviously improved. QAIS/AIU algorithm is suitable not only for mining association rules  from initial database one times and incremental updating, but also for long transaction and high support degree situations to mine large scale database.

