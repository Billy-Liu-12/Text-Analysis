Accelerated Search over Encrypted Cloud Data Fateh Boucenna??, Omar Nouali?, Adel Dabah??, Samir Kechid?

Abstract?Companies and other organizations such as hospitals seek more and more to enjoy the benefits of cloud computing in terms of storage space and computing power. However, out- sourced data must be encrypted in order to be protected against possible attacks. Therefore, traditional information retrieval sys- tems (IRS) are no longer effective and must be adapted in order to work over encrypted cloud data. In addition, in order to provide the ability to search over an encrypted index, we use the vector model to represent documents and queries which is the most used in the literature. During the search process, the query vector must be compared with each document vector which is a time consuming process since the data collection is generally huge.

Consequently, the search performance is degraded and the search process is too slow. To overcome this drawback, we propose the use of High Performance Computing (HPC) architectures to accelerate the search over encrypted cloud data. Indeed, we propose several techniques that take benefit from Graphics Processing Unit (GPU) and computer cluster architectures by distributing the work between different threads. In addition, in order to get the best performance, we design our solutions so that they can process several queries simultaneously. The experimental study using 400.000 documents demonstrates the efficiency of our proposals by reaching a speed-up around 46x.

Index Terms?Cloud computing, searchable encryption, data privacy, parallel computing.



I. INTRODUCTION  Companies and individuals need more and more storage space and computing power, that is why the cloud computing is increasingly used since its appearance. Moreover, cloud computing allows users to pay for what they use instead of buying their own servers which which involves significant economic savings.

However, the data are often sensitive and confidential (photos, emails, financial documents, etc.). Therefore, It is primordial to protect the outsourced data against possible external attacks and the cloud server itself. For this reason, the data must be encrypted before being outsourced.

Moreover, users tend to take advantage of the large storage space offered by the cloud to store a huge number of docu- ments. However, This can complicate the user?s task to retrieve a specific document. To overcome this problem, the use of an information retrieval system (IRS) becomes necessary into a cloud server.

Considering that the data hosted in the cloud server are encrypted. The issue is that a traditional IRS works only on clear data. Consequently, it is necessary to have an IRS able to search over encrypted data.

Many approaches have been proposed in the literature in order to perform search without any sensitive information  leakage [1], [2], [3], [4], [5]. The common point between them is the use of encrypted queries called trapdoors by the data users and a secure index to search over a collection of encrypted documents by the cloud server. After that, some works have tried to improve the quality of search by proposing semantic search approaches that exploit semantic graphs [6], [7] and ontologies [8] in order to improve the recall and the precision.

For privacy and feasibility concerns, we use the vector representation of documents and queries which is the most used in the literature, instead of an inverted index [4], [5], [8], [9], [10], [11]. Unfortunately, this kind of representation has a negative effect on the search performance in term of processing time, given that, during the search process, the query vector is compared with each document vector. For example, if there are one million documents, the similarity function which is time consuming operation will be called one million times.

Very few studies [2], [9] have focused on the search per- formance by compressing the index in order to accelerate the search process. However, this technique causes the degradation of the quality of results without significantly improving the performance.

To the best of our knowledge, all prior works assume that the server receives only one query at the same time. However, In practice, the server may receive several queries sent from different users simultaneously making the search performance issue more complicated, given that the search process is already slow while treating a single query. To overcome this challenge we propose several techniques that allow the server to treat multiple queries simultaneously and respond to each user within a reasonable time.

The goal of this work is to propose some techniques that allow to accelerate any searchable encryption scheme that uses a vector model to represent documents and queries without any degradation on the search quality in term of recall and precision. Our proposed parallelization techniques exploit several HPC architectures such as Multi-core CPU processor, Computer Cluster and Graphic Processing Unit (GPU) in order to accelerate the search process by distributing the work between several processes and treating lots of queries simultaneously. This solution allows to achieve a speed-up of 46x.

The first challenge of our work is to find the most appro- priate way to parallelize the search process and take benefit of each HPC architecture while respecting its programming model. Whereas, the second one consists of reducing the     response time of the server without degrading the search performance by treating several queries simultaneously.

The rest of the paper is organized as follows. First, we describe the background information of our work. After that, we give an overview of the problem that we have faced. Then, we present the techniques that we have proposed to accelerate the search over encrypted cloud data. Finally, we present the results of our experimental study.



II. BACKGROUND  In this section, we briefly describe the background infor- mation of our work, where a summary of the searchable encryption as well as high performance computing (HPC) architectures that we have used in this work are disclosed.

A. searchable encryption  A searchable encryption scheme is a system that allows a group of users to store, share and retrieve data, usually from a remote server [1]. For privacy concerns, the data must be encrypted before being outsourced. In addition, it is suggested that the data owner creates an index from the data collection in order to facilitate the search process. This index must also be encrypted before being outsourced with the data collection. Once the encrypted collection and the secure index are outsourced, the data owner shares the public keys with the users by using a secure communication protocol.

To perform a search, an authorized user formulates a query.

Then, he encrypts this query using the public key to construct a trapdoor. Finally, he sends the trapdoor to the remote server.

Upon the server receives the trapdoor, it utilizes the secure index to retrieve the most relevant documents before returning them to the user (Figure 1). Moreover, the search must be performed in complete safety and the server may learn nothing, neither about the users? need nor about the content of the returned documents.

We can classify the searchable encryption schemes that have been proposed in the literature into several categories based on different criteria such as the encryption method used and the structure of the index. According to the encryption method used (the encryption of the index), many approaches [11], [8], [5], [12] use knn method [13] because of its compatibility with the vector model. Other approaches [4] have tried to exploit the homomorphic encryption [14] when encrypting the index.

The remaining approaches exploit some techniques to create a secure index such as Johnson-Lindenstrauss transform [9], the bloom filter [2] and the hash functions [6]. According to the structure of the index used, there are very few approaches that utilize an inverted index [3]. However, for privacy and feasibility concerns, most approaches prefer to use the vector model while creating a secure index [4], [5], [8], [9], [10], [11].

The remaining approaches exploit tree or array structures in order to index the collection of documents [15].

In this paper, we will not discuss the encryption methods used in prior works. However, we will interest in the structures of the index that have been exploited. The vector model is the most used in the searchable encryption area for feasibility  Fig. 1. General architecture  and privacy concerns, given that it is very difficult to use an inverted index to perform a search over encrypted data. In the vector model, each document is represented by a vector of size n, where n is the number of terms in the collection. Each field of the vector represents an encrypted weight of a term in the document. During the search process, the server calculates the inner product between the query vector and each document vector (all vectors are encrypted). Then, it sorts the scores and returns the top k documents to the user. Unfortunately, the use of the vector model causes the slowdown in the search process since the query vector must be compared with each document vector. These two reasons (great use of the vector model and the slowdown in the search process) motivated us to propose a solution to accelerate the search process for each approach which is based on the vector model.

B. High performance computing (HPC) architectures  There are many kinds of HPC architectures, in this section we present two of them, namely, the Graphic Processing Unit (GPU) and the computer cluster.

1) Graphic processing unit (GPU): GPU is a graphic processor that has a highly parallel structure that makes it very efficient for graphic tasks such as video memory management, signal processing and image compression. Recently, other areas such as industry, research and science are interested in using the GPU in parallel with the CPU in order to accelerate calculations.

GPU architecture is based on SIMT paradigm (Single Instruction, Multiple Threads). Therefore, when a program (kernel) is launched on GPU, it will be executed in a parallel way by the different threads. GPUs are structured so that each set of threads is grouped in a block and each set of blocks is grouped in a grid. The number of threads and blocks is specified during the launch of the kernel. Threads of the same block can access to a cached memory called shared memory.

All threads have access to other slower and larger memories such as the global memory and the constant memory.

Unlike CPU that includes a limited number of cores op- timized for sequential processing, a GPU includes thousands of cores designed to deal effectively with a huge number of tasks simultaneously. Therefore, to take advantage of the use of GPU and get the best performance, it is recommended [16] to assign to the GPU the heaviest portions of source code that can be calculated in a parallel way such as large matrices.

The remainder of the source code that runs in a sequential way must be assigned to the CPU.

2) Computer cluster: A computer cluster is a grouping of a set of computers called nodes in a network that appears as a single computer much more powerful (processing power, storage space, system memory, etc.).

A computer cluster is constructed of a set of nodes. Each node can launch several processes simultaneously. Each node has a local memory that is shared between its different processes and is not accessible from other nodes. One process called Master is chosen to be responsible for the distribution of the processing between different nodes. A computer cluster architecture allows to perform calculations in a parallel way.



III. PROBLEM FORMULATION  Our work aims to improve the search performance. It is based on two main assumptions.

A. Assumption 1. Huge number of documents  In the information retrieval over encrypted data, it is very difficult to represent a collection of documents by an inverted index with respect to the security constraints and without causing any sensitive information leakage. It is mainly for this reason that the major approaches that have been proposed in the literature preferred to use a vector representation of documents to create a secure index. With this representation, each document and query is represented by a vector of size n, where n is the number of terms in the collection or the number of concepts in the ontology, depending on the approach. In addition, it is easier to encrypt vectors while keeping the ability to make calculations over them during the search process.

Cloud computing is often used when a user has a huge number of documents to be hosted or a large amount of data to be processed. During the search process, the cloud calculates the scalar product between each document vector and the query vector, sorts the result and returns the top k documents that have the highest scores to the user. Considering the huge number of documents on the one hand, and the non-use of an inverted index on the other hand, the search process may be slower than expected (in our experiments, it took about 32s to perform a search on a collection of 400,000 documents).

This drawback can cause users dissatisfaction or worse the abandonment of cloud computing.

Thus, our first contribution aims to improve the search performance and make the search process much faster. For this purpose, we have proposed four techniques that utilize some HPC architectures in order to achieve a parallel computing.

The idea is to exploit the parallel computing during the calculation of the similarity scores as well the sort of the  documents. In addition, these techniques are applicable in all approaches that use a vector representation of documents and queries [4], [5], [8], [9], [10], [11].

B. Assumption 2. Huge number of queries  Another problem that may be encountered in the infor- mation retrieval is when a server receives many queries simultaneously. This problem is even more pronounced in the searchable encryption given that no inverted index is used and hence the search is slower. This problem can be solved by providing enough servers in order to simultaneously process all queries that have been received at the same time by the cloud. Unfortunately, this solution is costly for the user who will pay for the use of these servers and it is risky for the provider who will have more difficulty to convince customers of the interest in cloud computing.

The goal of our second contribution is to manage several queries simultaneously on condition that these queries are destined to the same collection of documents. Thus, instead of processing sequentially the queries that have been received at the same time, the cloud server performs a search on the collection of documents taking into consideration all received queries and returns to each user a list of top k documents in response to his request. This new contribution allows the cloud to perform multiple searches simultaneously on the same collection of documents by using only one calculator and without causing any large deterioration in the search performance.



IV. ACCELERATED SEARCH OVER ENCRYPTED DATA  In this section, we present the techniques that we have proposed to accelerate the search process over encrypted cloud data. For that, we start with an overview of the proposed techniques. Then we present the different functions of the search process that we have accelerated using our proposed techniques. Finally, we explain the proposed solutions adapted for some HPC architectures that perform a parallel computing.

A. Overview of the proposed techniques  These new techniques aim to accelerate any searchable encryption approach based on the vector model. Indeed, these techniques allow to significantly reduce the search time with- out any degradation on the recall and precision. In other words, our solution has no effect on the search results, but the search process is much faster when using our techniques.

In fact, the acceleration is done at the cloud server where the search process is executed. As it is mentioned above, there are two assumptions that have been taken into consideration when designing our techniques. The first assumption is that there is a huge number of documents. The second one is that it is possible to receive lots of queries simultaneously. It is important to remember that each document is represented by a vector, and the same goes for the queries, and during the search process the query vector is compared with each document vector. Therefore, in order to accelerate the search process, we have exploited the parallel computing by distributing the     documents between different processes as well as treating multiple queries simultaneously.

B. Functions to accelerate  As was explained in the previous section, accelerating the search process is done at the cloud server. Therefore, we have proposed an accelerated version for each function supposed to be called by the cloud server. These accelerations are achieved by taking into account the two assumptions previ- ously presented (huge number of documents, lots of queries simultaneously received). We have adapted each function used by the cloud server to our assumptions. Indeed, we have designed these functions so that they can manage several queries simultaneously (contribution 2). The way these func- tions are exploited based on the architecture of the calculator (contribution 1) is explained in the next section.

With our techniques, the search process is composed of three main functions (Similarity, Sort, Merge). In the follow- ing, we detail them one after the other:  ? Similarity ( ? d i, Q) ? R. This function has two param-  eters, namely, a document vector ? d i and a set of query  vectors Q = {?q 1, ? q 2, ...,  ? q n}. The similarity function  computes the scalar product between the document vector ? d i and each query vector  ? q j , with j ? [1, n]. The result  is a list R of size n, where the jth field corresponds to the similarity score between the document di and the query qj .

? Sort (dij , D, k) ? r. This function returns the rank r of a document di regarding a set of documents D = {d1j , d2j , ..., dmj} with dij = (di, sij), where sij corresponds to the similarity score between the document di and a query qj . The rank r is calculated by counting the number of documents of the set D that have a higher score than the document di. During the computing process, if the rank r exceeds a given value k, the function will be broken and the value ?1 will be returned. This is justified by the fact that we are interested only by the top k documents that have the highest scores.

? Merge (D?, D??, k) ? D. This function takes as pa- rameters two sorted lists of documents D? and D?? as well as an integer k. Its aim is to merge the two lists D? and D?? into a single sorted list of top k documents (see algorithm 1). The merge function is often used in the case where two processors work simultaneously on different documents and each of them returns its own list of documents.

C. The proposed techniques  In this section, we present the different techniques that we have proposed in order to accelerate the search process. Each technique is an adaptation of the different functions of the search process (Similarity, Sort, Merge) for a high perfor- mance computing architecture (GPU, Multi-core Processor, Computer Cluster). Each technique operates in a parallel way where many processes work simultaneously. The difference  Algorithm 1 Merge Require: k; D? = {d?1j , d?2j , ..., d?kj}/d?ij = (d?i, s?ij);D?? =  {d??1j , d??2j , ..., d??kj}/d??ij = (d??i , s??ij) Ensure: D = {d1j , d2j , ..., dkj}/dij = (di, sij) x ? 1, y ? 1, z ? 1 while z ? k do  if s?xj > s??yj then dzj ? d?xj x ? x+ 1 z ? z + 1  else dzj ? d??yj y ? y + 1 z ? z + 1  end if end while  between these calculators is mainly at their architectures such as the number of threads that can be optimally executed simultaneously, the sharing memory between threads and the data transmission time.

1) Multi-core based solution (Multi-core processor): In this technique, several threads are executed simultaneously. The documents are distributed between them. In addition, each thread has access to the main memory. The search process in a multi-core processor is accomplished as follows:  1) First, the main process receives a set of queries destined for the same collection.

2) Upon it receives the queries, it launches a set of threads (the number of threads is greater than or equal to the number of cores).

3) After that, the main process sends to each thread the received queries as well as a subset of documents (the documents are equitably distributed between the different threads).

4) The threads work simultaneously. Each thread uses the Similarity function to calculate the similarity score between each of its documents and each query so that a document has several scores, each of them corresponds to a query.

5) Then, the main process synchronizes the threads. For that, it waits until all threads finish their work before starting the next step.

6) After that, for each query, each thread uses the Sort function to assign a rank to each of its documents compared with the whole documents of the collection, including those of other threads since the main memory is shared between all threads. Therefore, a document has several ranks where each rank corresponds to a query.

7) Finally, when all threads finish their works, the main process collects for each query, a list of top k documents.

2) GPU based solution: In this technique, we have adapted the search process for a GPU. For that, each GPU thread has to deal with only one document. It should be noted that the GPU     global memory can be accessed by any thread. The search process in a GPU can be performed as follows:  1) First, the CPU receives at the same time several queries destined for the same collection.

2) Upon it receives the queries, it transfers both the docu- ment vectors and the received queries to the GPU.

3) The GPU launches enough threads so that each thread deals with only one document.

4) After that, each thread uses the Similarity function to calculate the similarity score between a document and each received query so that each document will have a score for each query.

5) Each thread waits until all other threads finish their work before it proceeds to the next step.

6) Then, for each query, a thread calls the Sort function to assign a rank to its document compared with the documents of other threads given that the global memory is accessible by all threads. That way, each document will have a rank for each query.

7) Finally, the GPU transfers to the CPU a set of lists of top k documents where each list corresponds to the result of a query.

3) Hybrid solution (GPU-CPU collaboration): In this tech- nique, the idea is not to let the CPU pending while the GPU is working. For that, we thought of sharing the work between CPU and GPU. Each of them deals with a part of the collection by calling its own threads. Thus, this solution is a merger of the two previous solutions. In the following we give the details of this third solution:  1) First, the CPU receives a set of queries destined for the same collection of documents.

2) Upon it receives the queries, it launches a set of threads and sends to them the received queries as well as a part of the index (a subset of document vectors).

3) Then, it sends the other part of the index as well as the received queries to the GPU.

4) After that, Multi-core based solution (steps 4-7) is applied by the CPU to obtain for each query a list of top k documents.

5) Simultaneously, GPU based solution (steps 3-7) is ap- plied by the GPU to obtain for each query a list of top k documents.

6) Finally, for each query, the CPU calls the Merge function to merge the list of documents found by the GPU with the list of documents found by the CPU into a single list of top k documents.

4) Cluster based solution: In this technique, the search pro- cess is distributed between the different nodes of the computer cluster. It is important to know that the nodes do not share any memory between them. Thus, each node which is composed of a set of processes, works completely autonomously of other nodes. The search process in a computer cluster is performed as follows:  1) First, the master process receives several queries des- tined for the same documents collection.

2) Upon it receives the queries, the master process dis- tributed the treatment between different processes. For that, it sends to each process a subset of document vectors as well as the received queries.

3) Then, For each query, a process uses the Similarity function to assign a similarity score to each of its documents. At the end of this step, each document will have a score for each query.

4) The processes operate simultaneously and asyn- chronously. Therefore, when a process terminates the calculations of the similarity scores, it continues with calculating the ranks of documents. For that, for each query, each process calls the Sort function to assign a rank to each of its documents compared with the documents that it treats (rather than with all documents of the collection as is done in both Multi-core based solution and GPU based solution given that there is no shared memory between nodes in a computer cluster).

At the end of this step, each document will have a rank for each query.

5) After that, each process returns for each query a list of top k documents to the master process.

6) Whenever the master process receives the results, it calls the Merge function to merge the results of the same query until it obtains a final list of top k documents for each query.



V. RESULTS AND COMPARISON  We performed our experiments on Yahoo! Answers1 collec- tion that contains 142,627 queries and 962,232 documents.

We have applied our techniques on the approach entitled ?Semantic Searchable Encryption Scheme? (SSE-S) [8] that performs a semantic search through a secure index based on the vector model. We have performed a series of tests for each proposed technique. We have realized two kinds of experiments for each solution.

? The first experiment consists of calculating the time required for the search process in response to a query based on the number of documents. The number of documents that we have used is between 50,000 and 400,000.

? The second experiment consists of calculating the time needed to perform a search on a collection of 200,000 documents based on the number of queries that have been received simultaneously.

In the following, we analyze the results obtained in the four proposed techniques.

A. Sequential solution (Sequential search)  First, we tested the SSE-S approach that works in a se- quential way without any parallelism. The experiments were performed on a computer equipped with an ?Intel Xeon 2.13GHz? processor. Table I shows that the search process takes about 32s to treat 400,000 documents in response to a  1https://answers.yahoo.com/     Fig. 2. Time (ms) needed for the search process in response to a query based on the number of documents  single query. In addition, concerning the second experiment, table II shows that it takes longer than 206s to perform a sequential search on a collection of 200,000 documents in response to 20 queries that have been received at the same time.

B. Multi-core based solution (Solution one)  We have tested our first technique on a computer equipped with an ?Intel Xeon 2.13GHz (4 cores)? processor. For the first experiment where we calculate the time needed to perform a search based on the number of documents in response to a single query, we tested the first solution with different numbers of threads, in this case 4, 8 and 16 respectively.

Figure 2 shows that the Multi-core based solution acceler- ates the search process compared to the sequential search by reaching a speed-up around 4x. This acceleration is justified by the distribution of the work between threads that run simultaneously. In addition, Table I shows that increasing the number of threads may provide an improvement in the search performance. Therefore, it is preferable to use a larger number of threads than the number of cores.

For the second experiment, we tested this solution using 16 threads with the same processor. Remember that this experiment consists of calculating the time needed to perform a search on a collection of 200,000 documents based on the number of queries. Figure 3 shows that this solution brings an improvement in the search performance compared to the sequential search by achieving a speed-up around 4x.

C. GPU based solution (Solution two)  We tested this technique on ?NVIDIA Tesla C2075?2  graphic processor. Concerning the first experiment we have used a number of documents between 50,000 and 250,000 because of the limited memory space of this GPU. Figure 2 shows that there is an improvement compared to the sequential search due to an acceleration around 4.5x. However, there is not any significant improvement compared to the previous solution where we have used a multi-core processor. Indeed,  2Belonging to the CAPA (Parallel Computing and Applications) team of CERIST (Research Center on Scientific and Technical Information)  Fig. 3. Time (ms) needed to perform a search on a collection of 200,000 documents based on the number of queries  even if a GPU contains thousands of threads that operate simultaneously, the data transmission from the CPU memory to the GPU memory takes few additional seconds depending on the number of documents. Consequently, the time saved during the calculation process, is wasted during the data transmission. To overcome this problem, it is important to attribute as much as possible of work to the GPU such as treating several queries simultaneously instead of a single query.

Therefore, in the second experiment which consists in treating multiple queries simultaneously, the GPU is expected to have a better performance given that the threads will have more work. Figure 3 shows that our assumption is correct given that GPU based solution gives a better performance than both Sequential solution and Multi-core based solution.

Indeed, GPU based solution which consists in treating multiple queries simultaneously by a GPU achieves an acceleration around 8.5x.

D. Hybrid solution (Solution three)  We tested the third technique using a calculator equipped with an ?Intel Xeon 2.13GHz (4 cores)? processor and a ?NVIDIA Tesla C2075? graphic processor. In the first experi- ment, we tested different distributions of documents between GPU and CPU. The best performance was obtained by assign- ing 50% of documents to the GPU and 50% of documents to the CPU (Table I). However, this distribution is not always the most optimal, since, it depends on several criteria such as data transmission frequency, CPU frequency and GPU frequency.

Figure 2 shows that in the case of single query, Hybrid solution gives a better performance than both Multi-core based solution and GPU based solution by achieving an acceleration around 5.5x.

In the second experiment, we also tested several distribu- tions of documents between GPU and CPU. In this case, we chose to assign the greater part of documents to the GPU given that in the case of multiple queries, a GPU is much faster than a CPU. Figure 4 shows that the best performance was obtained by assigning 70% of documents to the GPU and 30% of documents to the CPU. Moreover, we notice that in     TABLE I TIME (MS) NEEDED FOR THE SEARCH PROCESS BASED ON THE NUMBER OF DOCUMENTS (FIRST EXPERIMENT)  Solution 100k doc 150k doc 200k doc 250k doc 300k doc 400k doc Sequential search 8,017.80 12,398.80 17,101.00 20,996.20 24,285.40 31,753.20  Multi-core (4 threads) 2,349.60 3,715.60 5,135.40 6,300.00 7,586.80 9,983.80 Multi-core (8 threads) 2,311.80 3,551.00 4,939.60 5,986.60 6,854.00 9,118.00 Multi-core (16 threads) 2,334.40 3,516.00 4,790.60 5,885.80 6,874.20 8,885.60  GPU 1,726.80 2,750.80 4,466.60 4,764.60 - - GPU-CPU (40/60) 1,814.60 2,571.80 3,523.00 4,375.80 5,135.20 6,561.20 GPU-CPU (50/50) 1,552.60 2,297.20 3,122.00 3,798.80 4,602.40 5,837.40 GPU-CPU (60/40) 1,363.80 2,310.40 3,124.00 3,811.00 4,593.60 6,383.80  Cluster (8 processes) 1,979.20 2,966.00 3,958.40 4,967.80 5,951.40 7,967.20 Cluster (16 processes) 1,777.60 2,660.20 3,545.20 4,438.60 5,337.80 7,104.40 Cluster (32 processes) 1,890.80 2,773.80 3,648.80 4,531.60 5,401.20 7,153.40 Cluster (64 processes) 2,210.00 3,088.80 3,836.40 4,871.40 5,887.20 7,463.00 Cluster (128 processes) 2,571.40 3,709.40 4,835.00 6,090.60 6,278.60 7,721.20  TABLE II TIME (MS) NEEDED TO PERFORM A SEARCH ON A COLLECTION OF 200,000 DOCUMENTS BASED ON THE NUMBER OF QUERIES (SECOND EXPERIMENT)  Solution 4 queries 8 queries 12 queries 16 queries 20 queries Sequential search 46,525.20 86,356.40 125,591.20 165,183.60 206,659.80  Multi-core (16 threads) 12,267.40 22,367.80 32,431.80 43,036.60 54,280.80 GPU 6,993.60 11,319.60 15,603.20 19,901.20 24,146.00  GPU-CPU (50/50) 6,949.80 12,129.60 17,217.00 22,287.00 27,802.80 GPU-CPU (60/40) 5,747.40 9,994.60 14,197.00 18,046.20 23,072.40 GPU-CPU (70/30) 5,477.60 8,543.20 11,558.40 14,515.60 17,702.40 GPU-CPU (80/20) 6,167.00 9,286.40 12,704.40 16,131.20 19,225.20  Cluster (8 processes) 4,796.60 6,016.00 7,283.20 8,501.80 9,666.40 Cluster (16 processes) 3,928.00 4,499.60 5,082.00 5,638.60 6,182.40 Cluster (32 processes) 3,835.00 4,108.00 4,388.20 4,658.80 4,922.40 Cluster (64 processes) 3,942.60 4,072.20 4,181.00 4,305.00 4,474.00  Cluster (128 processes) 4,850.20 4,946.60 5,006.60 5,083.80 5,131.60  Fig. 4. Different distributions of documents between GPU ad CPU (exp 2)  the case of multiple queries, Hybrid solution is better than the two previous solutions by reaching an acceleration around 12x (Figure 3).

E. Cluster based solution (Solution four)  We tested this technique on ?CERIST cluster IBNBADIS?3  which is composed of 32 compute nodes where each node is equipped with an ?Intel Xeon SandyBridge? bi-processor of 16 cores. Therefore, ?IBNBADIS? cluster has a total of 512 cores and a theoretical power of 8 TFLOPS.

3http://www.ibnbadis.cerist.dz/  In the first experiment, we have tested several numbers of processes, namely, 8, 16, 32, 64 and 128 processes that are executed simultaneously. Table I shows that the best performance is achieved when we have used 16 processes.

Indeed, given that, there is no shared memory between the nodes, launching a huge number of processes can degrade the search performance because of the large number of lists of results returned by each process that will be merged by the master process in a sequential way. Therefore, the number of processes must be chosen so that the search process will be distributed enough without bringing too much burden to the master process. Figure 2 shows that this solution gives a better performance than Multi-core based solution by reaching an acceleration around 4.5x.

Likewise, we have also tested several numbers of processes in the second experiment. Figure 5 shows that the best performance was achieved when we have used 64 processes.

In addition, Figure 3 shows that there is no a significant increase in the computing time even if the number of queries increases. Therefore, this solution allows to simultaneously process a huge number of queries without any degradation in the performance. Thus, Cluster based solution gives the best performance among all proposed solutions by achieving an acceleration around 46x. Moreover, the acceleration is enhanced as there are more documents or queries to be processed.

Fig. 5. Different number of processes used in Cluster based solution (experiment 2)  F. Synthesis  The aim of this experimental study is to show the interest of the different techniques that we have proposed. On the one hand, the first experiment shows the benefit of distributing the documents between multiple threads during the search process.

This experiment has validated the different solutions that we have proposed by achieving accelerations around a factor of 4, 4.5, 5.5 and 4.5 for Multi-core based solution, GPU based solution, Hybrid solution and Cluster based solution respec- tively. On the other hand, the second experiment has proved the advantage of processing multiple queries simultaneously.

This new method has improved the accelerations from 4.5x to 8.5x for GPU based solution, from 5.5x to 12x for Hybrid solution and from 4.5x to 46x for Cluster based solution.

Moreover, if we compare the four solutions that we have proposed, we notice that the Multi-core based solution is the cheapest but it allows to achieve an acceleration around 4x.

This solution is useful for an individual who outsources a small collection of documents. Whereas, GPU based solution requires a graphic processor which is not really expensive, in order to achieve an acceleration around 8.5x. Likewise, the Hybrid solution exploits both CPU and GPU as GPU based solution to achieve an acceleration around 12x, so Hybrid solution is definitely better than GPU based solution.

Moreover, the Hybrid solution is sufficient when the collection of documents is not too big and the number of users is small.

Thus, this solution is adequate for a small company. However, Cluster solution is the most expensive, but it achieves very good accelerations and can treat a large number of queries simultaneously over a large collection of documents without any degradation in the search performance. Therefore, it is adequate for a big company.



VI. CONCLUSION In this work, we dealt with the performance issue faced  in the search over encrypted data. Indeed, for security and feasibility purposes, most approaches that have been proposed in the literature opt for the choice of the vector model during the indexing process. However, this representation causes the degradation of the search performance since the query vector  must be compared with each document vector. To overcome this problem, we have proposed four techniques that use some high performance computing architectures in order to accelerate any approach based on the vector model. We have conducted an experimental study which confirmed that the distribution of documents between different processes as well as the processing of several queries simultaneously allow to drastically improve the search performance.

