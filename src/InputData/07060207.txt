

Abstract ? Determination of frequent sets from a large database is the key to Association Rule mining from the point view of efficiency of algorithms to scale up and discovering frequent sets which lead to useful association rules. Some of the existing methods have either very low or very high pruning, which is the cause of generation of larger or lesser number of frequent patterns. In this paper we have adopted a balanced approach for frequent pattern selection. Our proposed measure weighted support considers association and dissociation among items as well as the impact of null transactions on them for frequent set generation.

Impact of increasing itemset size on weighted support gives rise to variable threshold. The experimental results obtained after implementation of the proposed algorithm justify the approach.

Keywords ? Association rule mining; Dissociation; Weighted support; Null transaction impact factor; Jaccard similarity coefficient.



I. INTRODUCTION  Frequent pattern mining in a large database is one of the interesting research areas in data mining.

Frequent pattern generation basically depends on the associability of the items (attributes) in the database.

Apriori is the classical algorithm by Agarwal et al. [1, 2] and Han et al. [3] for frequent itemset generation.

Apriori employs an iterative approach known as a level-wise search, where associability of k-itemsets is used to determine k+1-itemsets. Association rule is traditionally measured on two parameters namely, support(s) and confidence(c). The mathematical formula of support and confidence are s A P A ,s AB  P AB  and c A B  P B/A P ABP A respectively, where A and B are any two itemsets and A B  and AB represents the itemset AUB and A B is an association rule which means if an itemset A occurs it is likely that B will also occur. An itemset is considered for mining rule if its support is greater than or equal to a threshold called min_sup and these itemsets are called frequent itemsets.

Association measured through support is a binary association where in each transaction the presence of all the items of an itemset is only considered. But it does not consider the cases where some items of an itemset are present and the other items are absent or all of them are absent. Frequent pattern generation using traditional support fails to capture this fact. We take a sample data to explain this further. Let T be a  database of 15 sample transactions on 4 items A, B, C and D as shown in Table I. The presence and absence of an item in the transaction is denoted by 1 and 0 respectively. We observe that s A  s B  0.6, s AB   0.4 and s BC   0.4. If min_sup = 0.3 both AB and BC are frequent being above min_sup and thus qualify for confidence measurement. Again, c A B   c B C  = 0.67, which shows that both A B and B C are valid association rules with equal confidence but we cannot conclude which one of these two expresses stronger association. Thus association alone fails to differentiate the one from the other. Let us now examine this further from the angel of dissociation.

TABLE I. Sample transactions of a database T  TID A B C D 1 1 1 0 0 2 0 1 1 0 3 0 0 0 1 4 1 1 1 0 5 0 1 0 0 6 1 0 0 0 7 0 1 1 1 8 1 1 1 1 9 0 0 0 1  10 1 1 0 0 11 1 0 0 0 12 0 0 0 1 13 1 1 1 0 14 1 0 1 0 15 1 1 1 1   Positive association of itemsets measures their joint occurrences. Negative association (dissociation [4]) on the other hand measures presence of one itemset when the other itemset is absent. High association should naturally imply low dissociation or vice versa and therefore dissociation has an impact in measuring association for confirmation of associativity. If we examine AB and BC in Table I we find, A and B are dissociated in 6 out of 15 transactions or dissociation(AB) = 0.4 whereas B and C are dissociated in 4 out of 15 transactions or dissociation(BC) = 0.27. We may conclude that BC is preferred to AB because of lesser dissociation other parameters remaining same. This result prompted us to look for a new measure of frequent set generation which is built on both association and dissociation.

A. Contribution  In this paper we have proposed a measure named Weighted Support (WS). We have considered two major types of associations. They are positive association (A?B) and negative association (A?~B or ~A?B). The transaction in which we have ~A?~B (null transaction) is not considered as positive or negative association. We have found out how positive and negative association rules are affected by this particular transaction and thus have focused on finding the impact of null transactions over positive association rules. As a matter of fact our proposed method is a composite measure under the consideration of positive association rules, negative association rules and null transaction impact factor.

B. Related Work Cohen et al. [5] considered similarity as a measure  in lieu of support for frequent set generation in case when confidence is high but support is low. However the authors did not consider the possibility of defining association through the minimization of dissociation.

Adaptive Apriori algorithm for frequent set generation in the work of Wang et al. [6] proposed variable MINSUP threshold.  In the classical Apriori algorithm min_sup is static for all itemsets irrespective of set size. But the number of frequent sets decreases as the size of itemset increases justifying variable MINSUP.

The work of Pal et al. [4] measures association against dissociation. The authors considered togetherness as the measure for frequent itemset generation in lieu of support. By definition, the value of togetherness is 1 for all 1-itemsets and thus all 1-itemsets are frequent in any database irrespective of the transactions and min_sup.  This implies all 2-itemsets are candidates in 2-items candidate generation and hence qualify for support calculation. Moreover, togetherness of any itemset is always greater than or equal to its support.

Thus it is very likely that more frequent sets will be generated in 2-itemsets and so on in higher order. In togetherness measure a larger number of higher order itemsets are likely to qualify as frequent itemsets compared to support measure. The work of Awadalla et al. [7] has suggested Enhanced Apriori algorithm based on mean, median and standard deviation. P. Tan et al. [8] in their work have designed an algorithm to select a small set of tables such that an expert can find the most appropriate measure by looking at just this small set of tables. X. Wu et al. [9] have designed constraints for reducing the search space and have used the increasing degree of the conditional probability relative to the prior probability to estimate the confidence of positive and negative rules. C.

Cornelis et al. [10] have suggested a new Apriori based algorithm, called PNAR that exploits the upward closure property of negative association rules.

B. Ramasubbareddy et al. [11, 12] and A. Samuel et al. [13] have considered ~ ~  as positive association rules, whereas most of the authors have considered it as negative rules. Many authors have counted frequent set for positive as well as negative rules generation. Some of the authors have noticed the above fact and tried to develop method which can reduce not only the complexity of calculation, but also filter the rules in a greater degree. The well known  weight based algorithms [14, 15, 16, 17, 18 ] support weight calculation of positive and negative rules.



II. ENUMERATION OF WEIGHTED SUPPORT  Enumeration of Weighted Support (WS) basically depends on three major factors. They are support of itemsets (s), Jaccard similarity coefficient (j) and Null transaction impact factor (?).

A. Support of itemsets  From the classical Apriori algorithm [1] proposed by Agarwal and Srikant we get the mathematical structure of support(s) as follows: Let I  A , A , ? , A  be the set of items in the database where each item A  is a Boolean attribute of the database table. Let D be the set of all transactions in the database, where each transaction TI , I , ? , I ; k m  is a set of items such that T  I.

Let C  represents the set of rows that have 1 in column A . According to classical Apriori support (s) of itemset A , A  = C C|D| .

Table II represents frequency of transactions of two itemsets A and B of a dataset in a 2 x 2 contingency table. The cells of the table corresponding to 4 possible combinations of the binary variables A and B contain their respective frequency.

Table II. 2 x 2 contingency table of A and B  B ~B A  ~A  From Table II we get, support s(AB)    B. Jaccard Similarity Coefficient One can say that less dissociation among itemsets  may be considered as strong association among those itemsets. So dissociation is an effective reverse process for justification of association. Scientist Jaccard has proposed a technique to find out the similarity among binary variables under the consideration of degree of dissociation along with association. Jaccard similarity coefficient [19] is defined as the quotient between the intersection and the union of the pair wise compared variables between two objects. Jaccard similarity coefficient [19, 20] of A  and A ,  j A , A   |C C |/ |C  U C | , where A and A  are two itemsets and C  and C  represent the set of rows that have 1 in columns A  and A  respectively.

On the basis of Table II data Jaccard similarity coefficient j A, B   . Incidentally, it is observed that Jaccard coefficient is same as togetherness measure suggested by Pal et al. [4].

C. Weighted Support While support tends to ignore the impact of dissociation on association though the effect of null transactions have an impact in the denominator, Jaccard coefficient tends to ignore null transactions in measuring association against dissociation. We introduce a measure Weighted Support (WS) to      include the effects of null transactions in the database on associativity among itemsets as well as dissociativity. WS is based on positive association among itemsets, negative association among itemsets and the effects of null transactions on association. The mathematical formula of WS is  WS  ?s  1 ? j     ?. (1)  where 0.5  ? 1.

? is the null transaction impact factor; s is the support according to classical Apriori algorithm and j is the Jaccard similarity coefficient. WS is a weighted linear combination of the two measures s and j. The use of ? and its range of values has been discussed in the next Section.

It can be easily observed from Eq. (1) that irrespective of the value of ?,  s WS j as j s. Thus the proposed measure will strike a balance between s and j.

D. Null Transaction Impact Factor Null transaction refers to those rows of the database where none of the item of the itemset occurs or simply the entry column(s) is/are 0 (zero).  Though the literature suggests a number of different measures for measuring associability among itemsets, there is no direct impact of null transactions on these measures. As a matter of fact, between association and dissociation which one is more influenced by the all-0 set is a debatable issue. One can argue that since 00 means absence of both items, it is difficult to predict if one of them was present in this transaction what would have happened to the other. In this paper we have basically harped on this point. Experimentally it is clear to us that the null transactions effects the frequent itemset generation deeply. In case of application of classical Apriori algorithm the denominator of support includes the null transaction set. So if its number is high support will be less and thus some practically frequent itemsets who have strong associativity according to Jaccard measure will be left out. On the contrary in the case of togetherness algorithm [4], the denominator excludes the null transaction set. So the value of denominator become comparatively less and togetherness become larger and thus it passes many itemsets as frequent sets which may include some impractical frequent itemsets having less associativity. So in both the cases effect of null transactions do not seem to have been given due consideration in pruning. To overcome this problem we have introduced an impact factor for null transactions.

Let D = total number of transactions in the database, C = set of rows that have 1 in column A where A  is a item or itemset, C  = complement of C .

So the support of null transaction for itemset A , A  = |C   C |/ | |.

On the basis of the above observation we make the following claim:  Lemma 1:  In any database D if the support of null transaction is negligible then its impact on frequent itemset generation is also negligible. If support of null transaction is < 0.5 then it should be upgraded to 0.5  for distribution of its impact on linear combination of support and Jaccard similarity.

Proof: Let s be the support according to Aprioi algorithm and J be Jaccard similarity coefficient. Now if we consider Table II, Eq. (1) can be expressed as WS  ?s  1 ? J  nN nN   1 nN nn n n N    N N N    where N ? n  N N  N 1 N   s 1  ? We consider two cases depending on the value of ?.

Case I: ?  0.5 (i.e. number of null transaction is greater or equal than 50%), WS  1.5s. A significant change in WS in comparison to support (s) is noticed and the value of ? is accepted in this case.

Case II: ?  0.5 (i.e. number of null transaction is less than 50%). WS < 1.5s There will not be any significant change of WS in comparison to support(s) and at the limiting condition it will be equal to s. This shows that ? has a negligible impact over itemsets when ? < 0.5. In such case the value of ? should be upgraded to 0.5. The equation for null transaction impact factor thus becomes  ?  max 0.5, |C   C |/ |D| .

E. Pruning through Minimum Weighted Support  (MIN_WS) To find out the frequent patterns of itemsets a threshold value needs to be set either by the user or the system. The system can even set a default to be overridden by user?s choice. The itemsets whose Weighted Support (WS) are less than the Minimum Weighted Support (MIN_WS) have to be pruned from the itemsets as non frequent itemsets. As a result the remaining itemsets will be considered as frequent itemsets.



III. PSEUDO CODE OF PROPOSED WEIGHTED SUPPORT ALGORITHM  Input: D (Database) and MIN_WS (Minimum weighted support threshold).

Output: L (Frequent pattern of data-set)   Procedure weighted support (D, MIN_WS) for (k=1; Lk-1 NULL; k++) { Ck = ws_gen (Lk-1); for each transaction t D ct (Ck, t), for each candidate c ct c.count++; } Lk= {c  Ck | c.count ? MIN_WS} return L= kLk .

Procedure ws_gen (Lk-1: frequent (k-1) itemset) For each itemset l1  Lk-1 For each itemset l2  Lk-1 if(l1[1]=l2[1]) (l1[2]=l2[2])  ????      (l1[k-2]=l2[k-2]) (l1[k-1]=l2[k-1]) then { c = l1  l2.

If c has_infrequent_subsets (c, Lk-1) then Prune c, Else add c to Ck.

} return Ck.

In the above algorithm Lk-1 is used to generate WS candidates Ck to find Lk for k 1. The ws_gen procedure generates the candidates and then eliminates those having a subset that is not frequent.

When all the candidates are generated the database (D) is scanned. For each transaction (t), a subset function is used to find all subsets of the transaction that are candidates and the count of each of these candidates is accumulated. The ws_candidates which satisfies the minimum weighted support are considered as frequent itemsets, L. The ws_gen procedure consists of two operations, namely, join and prune. Lk-1 is joined with Lk-1 to generate candidates. The prune component is used to remove candidates who do not posses minimum weighted support. The has_infrequent_subset procedure is related to pruning and is shown in prune procedure.

Procedure prune (C, D, MIN_WS) c  C do begin N1(c) = 0; N0(c) = 0; end c  C do begin t  D do begin if c has all 1?s then n1(c) = n1(c) + 1; else if c has all 0?s then n0(c) = n0(c) +1; end s(c) = n1(c) / |D|; J(c) = n1/ (|D| - n0(c)); ?(c) = max (0.5, n0(c) / |D|); WS(c) = ?(c) s(c) + (1-?(c)) J(c).

end L: =0; c  C do begin if WS(c) ? MIN_WS then L: = L U c; return L; end  In prune procedure n1 and n0 represents the set of all 1s and all 0s respectively. Thus n1 refers to direct association and n0 refers to null transaction set.

In stepwise method the algorithm computes support, Jaccard similarity coefficient and null transaction impact factor of candidates and finally accepts only those candidates whose WS value are higher or equal to min_ws.



IV. DYNAMIC WEIGHTED SUPPORT  Most of the well known algorithms follow the uniform minimum threshold value for the determination of frequent pattern of itemsets. But if an itemset becomes frequent with a specific threshold, its superset may or may not be the frequent  set w.r.t. the same threshold. So we may think of using more practical approach for minimum threshold. Basically minimum threshold possesses a close relationship with the size of the itemset.

Support of any itemset say AB will be greater than or equal to its superset, say ABC. So, support would tend to decrease as the size of the itemset increases.

From this we can assert that the user specified minimum threshold value should also decrease gradually with the increase of the itemset size. So in reality MIN_WS should have a dynamic nature and should be a decreasing function w.r.t the value of itemset size k. For example one can choose the following monotonically decreasing function for MIN_WS  2 1 where k = size of itemset and p = maximum threshold value set up by the user for minimum itemset size.

The limiting value of the above function is 0.5p. This dynamic nature of MIN_WS helps the developers to generate a moderate number of frequent patterns of itemsets.



V. PERFORMANCE ANALYSIS We have assessed the performance of Weighted Support (WS) algorithm against both of the classical Apriori algorithm [1] and Aprioridis algorithm [4].

We have made our experiment on EXTENDED BAKERY dataset1 of 50 items and have increased the number of transactions gradually from 1000 and then 5000 to 25000 in steps of 5000. To show the relative performance, we have chosen a threshold value 0.08 which refers to MIN_WS for WS algorithm, min_sup for Apriori algorithm [1] and min_togetherness for Aprioridis algorithm [4]. We have tested our algorithm for 3 different cases (a) 1-itemset (b) 2- itemset and (c) 3-itemset with the same threshold.

The experimental results are shown through the following bar charts. We have shown number of transactions along X- axis and number of frequent itemset along Y-axis.

Fig. 1. Performance of WS, Apriori and Aprioridis algorithms for 1-itemset with threshold = 0.08    1 https://wiki.csc.calpoly.edu/datasets/wiki/apriori      Fig. 2. Performance of WS, Apriori and Aprioridis algorithms for 2-itemset with threshold = 0.04   Fig. 3. Performance of WS, Apriori and Aprioridis algorithm for 3- itemset with threshold = 0.04  Now going back to Table I let us recheck our proposed measure. Both itemsets AB and BC have same support value 0.4 and with respect to min_sup = 0.3 both are frequent itemsets. If we set up the MIN_WS = 0.5 then only BC (WS= 0.5333) will be treated as frequent itemset but AB (WS=0.48) will be infrequent.



VI. CONCLUSION The classical support measure considers linear association among items ignoring the presence of dissociation among them. Again Jaccard coefficient (togetherness) considers both linear association and dissociation among items, but ignores the cases of the form ~A?~B. Many authors have categorized ~A?~B as a negative association and some authors suggested it as positive association. As a result an ambiguity occurs.  Our method named Weighted support does not consider null transaction (~A?~B) as either positive association or negative association, but takes into account its impact over positive association. Experimental results proved the effectiveness of the proposed weighted support algorithm for frequent set generation vis-?-vis traditional support and togetherness.  We have also discussed the necessity of dynamic minimum weighted support which expresses inverse proportional relationship with item size.

