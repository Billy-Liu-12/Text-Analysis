

Abstract-- Many Algorithms have been proposed to mine association rule that uses support and confidence as constraint.

We are proposing a method that can be combined with Apriori algorithm and reduces storage required to store candidate and the execution time by reducing CPU time. CPU time is saved by reducing candidate sets size and time required to calculate the support of each candidate. We are introducing the concept of checkpoint based on support value to reduce the execution time and overall storage space required to store candidate generated during scanning of dataset.

Keywords-- Association rule mining, checkpoint, CPU time

I. INTRODUCTION Association rule mining, one of the most important and  well researched techniques of data mining, was introduced by Agrawal et al. in 1993. It aims to extract interesting correlations, frequent patterns, associations or casual structures among sets of items in the transaction databases or other data repositories. Let I=I1, I2, ? , Im be a set of m distinct attributes, T be transaction that contains a set of items such that T?I, D be a database with different transaction records. An association rule is an implication in the form of X?Y, where X, Y?I are sets of items called itemsets, and X?Y =?. Here, X is called antecedent while Y is called consequent, the rule means X implies Y [1].

To select interesting rules from the set of all possible rules, constraints on various measures of significance and interest can be used. The best-known constraints are minimum thresholds on support and confidence.

The rule X?Y holds in the transaction set D with support [1] s, where s of an association rule (X?Y) is defined as the percentage/fraction of records that contain XUY to the total number of records in the database (n).

The support count of an itemset X, denoted by X.count, in a data set D is the number of transactions in D that contain X.

Then,  support ( X?Y )= .XUY count n    Suppose the support of an item is 0.1%, it means only 0.1 percent of the transaction contain purchasing of this item.

Confidence [1] of an association rule is defined as the percentage/fraction of the number of transactions that contain  XUY to the total number of records that contain X, where if the percentage exceeds the threshold of confidence an interesting association rule X?Y can be generated.

confidence(X?Y)= support ( X Y ) support( )X  ?   Confidence is a measure of strength of the association rules, suppose the confidence of the association rule X?Y is 80%, it means that 80% of the transactions that contain X also contain Y together, similarly to ensure the interestingness of the rules specified minimum confidence is also pre-defined by users.

This paper is organized as follows: related work is discussed in Section 2. Section 3 describes the proposed method. Section 4 presents the experimental results and analysis followed by conclusion.



II.  LITERATURE REVIEW Apriori, an influential algorithm, was first discussed by R.Agrawal and R.Srikant reported in [2]. Apriori employs an iterative approach known as a level-wise and breadth-first search, which k-itemsets are used to generate (k+l)-itemsets.

In terms of the feature of Apriori property, called anti- monotone, one can efficiently generate candidate itemsets by discarding unnecessary considered ones  However, the algorithms based on generated and tested candidate itemsets have two major drawbacks:  1. The database must be scanned multiple times to generate candidate sets. Multiple scans will increase the I/O load and is time-consuming.

2. The generation of huge candidate sets and calculation of their support will consume a lot of CPU time.

Direct Hashing and Pruning (DHP) [3] use a hash table for pre-computing the approximate support for the K+1itemsets, while the K-itemsets are processed. Despite its elegance, DHP still inherit the number of database scans from Apriori. Perfect Hashing and Pruning (PHP) [4] provide an optimized version of DHP.

Frequent itemsets can also be mined using vertical data format, which is the essence of the ECLAT (Equivalence CLASS Transformation) [5] algorithm developed by Zaki.

Mining can be performed on vertical data set by intersecting the TID sets of every pair of frequent single items. The      support count of an itemset is simply the length of the TID set of the itemset. The intersection of the TID sets of the frequent k-itemsets to compute the TID sets of the corresponding (k+1) itemsets. This process repeats, with k incremented by 1 each time, until no frequent itemsets or no candidate itemsets can be found.

FP-Growth was proposed by Han et al. in [6]. This algorithm creates a compact tree-structure, FP-Tree, representing frequent patterns, that alleviates the multi-scan problem and improves the candidate itemset generation. The algorithm requires only two full I/O scans of the dataset to build the prefix tree in main memory and then mines directly this structure. Mining the FP-tree structure is done recursively by building conditional trees that are of the same order of magnitude in number as the frequent patterns.

F. Wang et al. proposed An improved Apriori algorithm based on the matrix [7]. The matrix to count the support of the candidates of frequent itemsets, and do not need to scan the database again and uses the ?AND operation? to deal with the matrix to produce the largest frequent itemsets and others.

R. Chang et al. proposed APRIORI-IMPROVE [8] algorithm that generates L2 directly from one scan over the database without generating C1, L1 and C2 . And it replaces the hash tree by a hash table to reduce searching cost. It also uses an efficient horizontal data representation and optimized strategy of storage to save time and space.



III. PROPOSED METHOD  The goal of the proposed method is to reduce CPU time which is saved by reducing candidate set size. If candidate set size is less than time required to calculate the support of each candidate is less. We have proposed Method that reduces the number of candidate generated and time required to calculate the support of each candidate. In order to reduce CPU times, we have defined two type of checkpoint in dataset based on support value:  checkpoint1=Totaltransaction-support count+1; ? min_sup; checkpoint2=support count+1; if min_sup>50.

Support property: If support count is n than any item to be frequent it must be appearing in at least n transactions in the dataset.

All new candidates after checkpoint1 cannot be frequent based on support property. At checkpoint1 if min_sup<=50 or checkpoint2 if min_sup>50, scan the candidate set once and check the support value of all candidate. The estimated support value is used to remove infrequent itemsets at checkpoint.

Estimated Support value(Esupport)= support of candidate + Totaltransaction - checkpoint1; if min_sup<=50.

Estimated Support value(Esupport)= support of candidate + Totaltransaction - checkpoint2; if min_sup>50.

If Estimated Support value < support count then that candidate will be removed from the candidate set.

Consider a dataset as show in TABLE I. There are 10 transactions, namely |D|=10.

TABLE I SAMPLE DATASET   T i d I t e m s e t  1 1 , 2 , 4 , 5 2 1 , 3 , 5 , 1 3 1 , 2 , 4 , 8 4 1 , 3 , 6 , 8 5 1 , 2 , 3 , 5 6 2 , 2 , 1 5 , 6 7 8 , 1 , 2 , 6 8 1 , 3 , 1 0 , 6 9 1 1 , 4 , 5 , 6  1 0   1 , 1 3 , 2 , 5  Case 1: The minimum transaction support is 70% than support count=7,checkpoint1=4 and checkpoint2=8.

As shown in TABLE I, items 10,11,13 and 15 are new Candidate after Checkpoint1 and can't be Frequent. At Checkpoint2 Esupport of all candidate show in TABLE II. At checkpoint2 items 3,4,5,6 and 8 are removed because it can't be frequent and no need to calculate support for this item for further transaction.

TABLE II ESUPPORT FOR CANDIDATE SET AT CHECKPOINT2  Itemset Esupport  1 7+2>=7 2 5+2>=7 3 4+2<7 4 2+2<7 5 3+2<7 6 4+2<7 8 3+2<7   Case 2: The minimum transaction support is 30% than support count=3 and checkpoint1=8.

As shown in TABLE I, items 11 and 13 are new Candidate After Checkpoint1 and can't be Frequent. At Checkpoint1 Esupport of all candidate show in TABLE  III.

TABLE  III. ESUPPORT FOR CANDIDATE SET AT CHECKPOINT1  Itemset Esupport 1 7+2>=3 2 5+2>=3 3 4+2>=3 4 2+2>=3 5 3+2>=3 6 4+2>=3 8 3+2>=3  10 1+2>=3 15 1+2>=3      At checkpoint1 no candidate item are removed because it is possible that all candidate can be frequent.

Performance of the Proposed method is almost same as original algorithm:  1. If all candidate in dataset are before the checkpoint, than performance of proposed method is same as original algorithm.

2.  If minimum support is low.



IV. EXPERIMENTATION AND ANALYSIS  In order to evaluate the efficiency of the Proposed Method, several experiments were conducted and compare result with the well known Apriori Algorithm. The Apriori is implemented in java available in SPMF which is an open- source data mining platform [9].

All the experiments are performed in Eclipse SDK, Version : 4.2.1 on a i5 processor with 8 GB main memory running under windows 7 home premium operating system.

Let D denote the number of transactions, T the average transaction size, and N the number of items. The dataset parameters D, T, N for Retail market basket data set [10] and pumsb data set [10] are shown in TABLE IV.

In order to assess the performance of the proposed method, two different experiments are conducted. The first one is to examine the effect of the number of candidate in first level and the execution time to generate candidate set and second is the effect of number of candidate and time to generate candidate in each pass.

TABLE IV.  DATASET PARAMETER  Dataset Total Number Of Transaction Average  Transaction Size Number of Items  Retail market basket 88162 10.3 16470  Pumsb 49046 74 2113      Fig. 1 Comparison of Number of Candidate Generated in C1    Fig. 2 Comparison of Time required to Generated Candidate in C1  We have executed a proposed method on Retail market basket dataset, Fig. 1 shows comparison of number of candidate itemset(C1) generated in first pass and Fig. 2 shows comparison of time required to generate frequent itemset for different minimum support for Apriori algorithm and proposed method with Apriori algorithm. As shown in Fig. 1 and Fig. 2 time and number of candidate generated in proposed method is less compare to Apriori Algorithm in first pass as support increased.

TABLE V.  NO. OF CANDIDATE AND TIME REQUIRED TO GENERATE IN EACH LEVEL  No. of Frequent andidate  Apriori FApriori  No. of Candidate Time (ms) No. of Candidate Time (ms) Removed Candidate At Checkpoint2  Pass 1 20 2113 12387 1559 6604 1539 Pass 2 140 190 1114 190 1002 23 Pass 3 459 563 5037 563 4830 43 Pass 4 786 1014 11618 1014 11188 62  Pass 5 746 895 12043 895 11766 11 Pass 6 364 420 6213 420 6227 0 Pass 7 85 95 1531 95 1550 0 Pass 8 7 7 131 7 137 0 Pass 9 0 0 0 0 1 0  Total Time   50074  43105      We have executed a proposed method on pumsb dataset, TABLE V contains comparison of number of candidate and frequent itemset generated in each pass and time required to generate frequent itemset in each pass for min supp 90%. As shown in TABLE V if infrequent candidate is removed at checkpoint than the time required to generate frequent item is less in the proposed method as shown in pass 1 to pass 5. If infrequent candidate is not removed at checkpoint than extra time is added due to traversal of candidate set at checkpoint in proposed method as shown in pass 6 to pass 9.



V. CONCLUSION   We proposed a method based on support value that increase the performance of Apriori algorithm as describe above. This improvement is due to the fact that the proposed method minimizes the number of candidate generated and removed candidate at checkpoint which is infrequent which interns reduces storage and time required to calculate support of candidate. At present, there is further work in progress to extend the proposed algorithm in relation to reduced the I/O time.

