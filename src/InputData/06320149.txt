The Parallelization of Algorithm Based on Partition  Principle for Association Rules Discovery

Abstract- subsequently the expansion of the physical supports storage and the needs ceaseless to accumulate several data, the  sequential algorithms of associations' rules research proved to be  ineffective. Thus the introduction of the new parallel versions is  imperative. We propose in this paper, a parallel version of a  sequential algorithm "Partition". This last is fundamentally  different from the other sequential algorithms, because it scans  the data base only twice to generate the significant association  rules. By consequence, the parallel approach does not require  much communication between the sites. The proposed approach  was implemented for an experimental study. The obtained  results, shows a great reduction in execution time compared to  the sequential version and Count Distributed algorithm.

Keywords- Association rules; Distributed data mining; Partition; Parallel algorithms.



I. INTRODUCTION  For a few years, one has assisted a strong increase as well in the number as in volume of the information memorized by data bases scientific, economic, fmancial, administrative, medical, etc. These mega data bases are exploited little, whereas they hide decisive knowledge. To fill this need, a new discipline appeared called Data Mining.

The data mining tasks are divided between classification, estimate, prediction, grouping and segmentation. These tasks applies to the relational data bases, data warehouse is the reserve of data (oriented object, spatial data base, chronological data and temporal data, textual and multimedia base). The tasks of data mining are expensive treatments, and volumes of data available increase with the possibilities of storage, imposing the recourse to parallelism, from where the birth of distributed data mining.

The Distributed Data Mining (DDM) is the process of the traditional data mining which consists to extracting a new knowledge from partitioned data sources, distributed on different sites, each site applies an algorithm of data mining to the local data. The results are then combined with the minimum of interaction between the data sites. The majority of DDM algorithms are designated on the potential parallelism which they can apply to the provided distributed data.

Generally the same algorithm functions on each site of distributed data at the same time, producing a local model by site, later all the local models are regrouped to produce the  Hafida 8elbachir LSSD Laboratory, Department of Computer Sciences University of Sciences and Technologies USTO-MB  Oran, Algeria h belbach@yahoo.fr  final model. Primarily, the success of DDM algorithms resides in the minimal transfer of data. Among the algorithms of distributed data mining one quotes: distributed classification [1] [2] [3] [4] [5] [6] [7], the distributed clustering [8] [9] [10] [11] [12] and distributed association rules.

In this paper, one presents a new parallel algorithm which differs from the other algorithms, one proposing a parallelization of a sequential algorithm called Partition [13].

This last requires that two scans of data base, which will permit it possible to reduce the number of communications thus the phase of synchronization between the sites in the parallel version of this algorithm. We chose an architecture centralized to minimize the cost of communication, whose each site independently treats its data base during both scans and sends the results to the coordinator who will charge to cumulate the results.

The rest of paper is organized as follows; the next section is an introduction to associative rules extraction. Thereafter, we present the proposed parallel algorithm with its different phases in section 3. The experiments results of proposed parallel algorithm are exposed in section 4. We will conclude with future works in section 5.



II. THE EXTRACTION OF ASSOCIATION RULES  The technique of extraction association rules is a not supervised learning method; allow to discovering from a set of transactions, a set of rules which express a possibility of association between different attributes. Let us take the example of a supermarket where the articles bought by each consumer are recorded at the same time in a data base as a transaction. Starting from this example, one can find associative rules of the form "90% of the customers who buy chocolate and milk tend to buy cheese". Chocolate and milk constitute the antecedent of this rule, cheese is the consequence and 90% are confidence. The associative rules were used successfully in domains as the planning aid, the diagnosis aid in medical research, the improvement of telecommunication processes, the organization and access to Internet sites, the analysis of images and spatial data, statistical and geographical, etc.

A. The associative rules Either I = {II, 12 ... , I m}a set of m items and D a set of  transactions constituted by items (I I, [J , I  K of I).

The association rules is an implication of form X => Y where X and Yare included in I and X n Y = 0. X is called the condition or the antecedent and Y consequent. Two measures are defined for an associative rule: the support and confidence. The support it is the report of a transactions number for the which at the same time condition and the conclusion appear on the number total of transactions and confidence it is the report of a transactions number for the which at the same time condition and the conclusion appear on the transactions number for the which at least condition appears.

B. Research of association rules The search for association rules consists in finding the rules  whose support and confidence are superior respectively than the thresholds minimum of support MinSup and confidence MinConf fixed by the user. The discovery of association rules is done in two phases:  1) Research of the frequent itemsets: It consists in traversing iteratively the transactions set. During each iteration, a set of candidate itemsets is created. The supports of these item sets are calculated and the not frequent item sets are removed (itemset which have a support inferior to minsup).

This phase is expensive in terms of time run because the number of frequent itemsets depends exponentially on the number of items manipulated for N items, one has 2 N item sets potentially frequent.

2) Research from these itemsets the interesting rules: To use the set of frequent itemsets to deduce the searched rules.

Thus, if ABCD and AB are frequent itemsets, then one can generate rule AB=>CD. To know if this rule is to be retained or reject, one calculates the confidence of AB=>CD by the formula: conf = sup (ABCD)/sup (AB). If conf > minconf, then the rule is retained.

C. The sequential algorithms The algorithms of mining frequent itemsets proceed in an  iterative way. During K eme iteration, one seeks frequent K-itemsets, i.e. the item sets of length K Which are frequent, in the following iteration, it (K + I) erne, one will search the frequent (K + 1 )-itemsets and so on.

The first algorithms for extracting frequent itemsets, AIS [14], Apriori [15] and SETM [16] that were proposed in 1993. Other algorithms to reduce the time for extracting frequent itemsets  are then appeared, and several optimizations and data structures to improve the efficiency of the algorithm Apriori, include:  ? AprioriTid by Agrawal and Srikant in 1994. [15]  ? AprioriHybrid by Agrawal and Srikant in 1994. [15]  ? DHP (Direct Hashing and Pruning) by Park et AI in 1995 [17]  ? Partition by Savas ere and A I in 1995. [13]  ? Sampling by Toivonen in 1996. [18]  ? DIC (Dynamic Item sets Counting) by Bit and AI In 1997. [19]  To limit the number of candidates considered at each iteration, the algorithm Apriori (and his derivatives) is based on the following two properties:  Propertyl. All subsets of a frequent item sets are frequent.

This property allows a limited number of candidates generated during the Keme iteration by a conditional join of frequent item sets of size K-l, discovered previously.

Property2. All supersets of infrequent itemsets are Infrequent. This property allows the removal of a candidate of size K when at least one of its subsets of size K-I is not part of the previously discovered frequent itemsets.

D. The parallel algorithms Further the expansion of physical supports of storage and  the needs ceaseless to safeguard more and more data, the sequential algorithms of search for association rules have proven ineffective. Thus the introduction of the new parallel versions became imperative. The current parallel and distributed algorithms are based on sequential algorithm Apriori. Two paradigms of parallelism are used, the data parallelism and task parallelism. These parallelizations require a new property in addition to the two properties previously seen in the sequential extraction of association rules.

Property3. So that an item set is globally frequent it is necessary that it is locally frequent on at least a site.

Among the algorithms of the data Parallelism, one can quote, CD (Count Distribution) [20], FDM (Fast Distributed Mining) [22], DMA (Distributed Mining of Association Rules) [21] and ODAM (Optimized Distributed Mining Association) [23]. Also for the task parallelism, many algorithms were proposed, such as, DD (Distribution Dated) [20], IDD (Intelligent Distribution Dated) [26] and HPA (Hash Partioned Apriori) [24]. There is other algorithms which cannot be classified in the two families, like HD (Hybrid Distribution) [26 ], CAD (Condidate Distribution) and ECLAt[25].



III. THE PARTITION PARALLEL ALGORITHM  Many parallel and distributed algorithms were proposed for distributed mining of association rules. The objective of these is to reduce the cost of communication which constitutes an important factor to measure their performances. However, the majority of these algorithms habitually present a high number of data's scanning and multiple phases of synchronizations and communication which degrades their performances.

To resolve these disadvantages we chose the sequential algorithm of Partition [13], we are interested in parallelization of this algorithm. The latter require only two scans of the database. Which permit to reduce the number of communication in the parallel version of this algorithm.

A. Algorithm partition Proposed by A.Savasere, E.Omiecinski and S. Navath in  1995, the approach adapted by this algorithm is to divide the database into horizontal partitions of the same size, where the size of each partition depends on the memory size so that the entire partition can reside in memory with the intention that it is read only once in each phase, also depends on the average size of the transactions, the current threshold of minimal support and the number of items. It is executed on each subset of transactions (Partition) independently, producing in one 1er scanning a set of frequent local itemsets for each partition. The set of global candidates is formed as being the union of all sets of frequent local itemsets. To obtain the total support for these itemsets, one 2nd scanning of the database is necessary. The Figure. 1 presents the phases of Partition algorithm:  1 )  2)  3)  4)  5) 6)  7)  8)  9)  10) 11)  12)  13)  P= partition_database (D)  n= Number of partitions II Phase I for i =1 to n begin  read-in-partition (pi E P )  Li = gen-Iarge-itemsets(pi)  end II Merge Phase  for (i=2; L eft 0 ,j =1,2, . .  ,n; i++) do begin U .

C = .1=1,2, . .  ,n L end II Phase II for i=1 to n do begin  read-in-partition (pi E P )  For all candidates c E C gen-count ( c , pi) end  14) L G = ( c E I c.count 2: minSup )  15) Answer =L G  Figure 1. Algorithm partition  1) Phase 1 Takes N iterations, during the iteration I only the partition  pi is considered. The procedure gen _large _itemsets illustrated in Figure.2 takes the partition pi like entry and generates the frequent local item sets of all lengths (Lz i , L3i ? . ?  L1i).

Procedure gen-Iarge-itemsets (p : database partition) 1) = ( large l-itemsets along their tidlists )  2) for ( k=2 ; eft 0, ; k++ ) do begin  for all itemsets h E do begin for all itemsets I, E do begin  3) 4) 5) 6) 7) 8) 9)  if h[1] = 1, [1] 1\ h[2] = 1, [2] 1\ ... 1\ 11 [k- l] < I, [k-I] then c = h [I]. 11 [2] . . .  11 [k-I] . I, [k-I] if c cannot be pruned then  c.tidlist= h.tidlist n I,.tidlist if I c.tidlisti/lpi 2: minSup then  10) U (cl 11) end 12) end 13) end 14) return Uk  Figure 2. Procedure gen_Iarge_itemsets  The transactions are in the form: < TID , I j , I k ... I 11 > , the elements of transaction are supposed to be sorted in order lexicographical, The items in a itemset are also conserved sorted depending this order.

To determine at the same time the frequent itemsets for each partition, and to count the global supports during counting phase, Partition algorithm employment a new representation of data TID-list, TID-list associates to each item set X a list of all transactions' TID where this item set appears. In other words the database is transformed into pairs < X, TID-list >.

The TID-list are sorted so that their intersection can be calculated in a simple manner during joint's phase, which necessitates only one read of two lists. The TID-list change in each passage and can be permuted in the disk if there is not sufficient memory available to store them. As well, they permit the elimination of all useless data, because the TID-lists of not? frequent items can be removed easily. The example of Figure.3 illustrates the representation of an initial database and the intermediate results; the minimum support is equivalent to 3 occurrences.

1,3,4, !'!i 2,3,4 1.2,3,4 2,3,!'!i.6 1.2,3,6  {I) {2) {3) {')  ..m? -<Or  --tr.i) {1.3} .<>-:-<f  1.3.5 ? 2,3, 4, !'!i 1.2,3,4, !'!i 1,2,3

I.' '.5  TID.

3.5 1,3, !'!i 1.3  {2,3} 2,3,4, S {2-;?f 2,3 {3,4} 1,2,3  Figure 3. Representation of TIDs and TID-Lists  After the generation of the frequent local itemsets, the fusion phase joined the item sets who have the same length of all partitions n to generate the set of global candidate itemsets.

The set of global candidate item sets of length j is calculated as follows: ct = U i?l ?. . .? n Lji ?  2) Phase 2 The procedure gen final count install a meter for each  global candidate item set," count his support in the data base and generate the frequent global itemsets.

B. The parallel algorithm of Partition We describe in this section the parallel approach of the  algorithm Partition which can be effectively applied for the extraction of association rules in a distributed environment.

In our approach the adapted topology is viewed as a set of N sites (clients) managed by a site called coordinator (server).

The parallel algorithm as shown in FigureA execute in four phases:  Processor 1 ---------------  " , -. --. Phase!

Generate the frequent  Ioc,ditemsets  I -1IiI-' Phase3  Count local supports of candidates  ' , - --- -------- -- -  "  Coordinator ,--------------?  Phase 1 Generation the set  of global candidates  Phase 4 Combine the accounts  of supports  Processor N  , , , , -  Phase! +-1IiI Generate the frequent  Ioc,ditemsets  I - Phase3 +-.

Count local supports  of candidat"  --------------- ,, '  Figure 4. Schema of the parallel algorithm of Partition  Phase 1: each processor has a partition, it applies the procedure gen Jarge _item sets on local data in order to identify the set of frequent itemsets locally Li, then it sends the result to the coordinator. As the size of the local data in all nodes is approximately equal, this phase will take approximately equal time in all nodes realizing a balancing of charge. Moreover, only the data available locally are treated. Consequently, there is none cost of communication for synchronization or transfer of data between nodes.

Phase 2: After having received the result of phase I, the coordinator will undertake to identify the set of global candidates CG, making the union of all sets of itemsets frequent locally in each processors (treatement realized in merged phase of the algorithme Partition). The result is diffused to all processors. In the end of this phase, all nodes will have exactly the same set of candidate itemsets.

Phase 3: corresponds to phase II of the sequential algorithm, where each processor must determine the support for all itemsets in CG, then to send them to coordinator.

Phase 4: after having received the result of phase 3, the coordinator will undertake to identify the set of global frequent item sets making the summation of the supports for each item set.

1) The algorithm The database is partitioned on N sites (clients) in an  equitable way by dividing the number of transactions on the number of sites, i.e. each site will have a set of transaction  (partition), where each one is responsible to identify the set of frequent item sets in order to discover the interesting rules. The Figure.5 illustrates the role of the coordinator and different sites in each phase of the algorithm.

Entry: Database D, the number of sites P, the minimal support minsup  Exit: Set of frequent itemsets L G  Coordinator: partitions the data base horizontally to N partitions and assigns them  to the different sites.

Phase 1 :// in parallel each site generates the frequent local itemsets  For each site I? I ............ P do in parallel  L (il ? gen-Iarge-itemsets (Pi) II Find the set of frequent local  itemsets;  II Each site send L(il to coordinator to calculate the set of global  candidates ;  End of parallel  Phase 2 : II the coordinator calculate the set of global candidate itemsets  Coordinator: CG ? U",1...p L (I) ;  II Diffuse CU to all processors;  Phase 3 : II count the supports of global candidates  For each site I? I ............ P do in parallel  For each cE cV do  c.compter(O ? gen-count ( c , Pi) ;  I I Send the results to coordinator;  End  End of parallel  Phase 4 : lithe coordinator calculate the global supports of global candidates  Coordinator: For each cE CG do  c.compter ?Lf?l c. compter(i) licombine accounts; Return LG? cE C I L;?l c.compter(i) ? min _sup;  End  Figure 5. The parallel algorithm of Partition

IV. EXPERIMENTS  To study the performances of the parallel algorithm of partltlon, we realized several experiments varying the parameters of topology configuration (number of sites), the minimal threshold and the number of transactions. The database used contains 12 items and 50000 transactions.

A. Experiments of the sequential algorithm In the implementation of the sequential algorithm, we  realized several tests and this by moditying the number of partitions as well as the number of transactions and the value of minimal support.

From the results of the tests obtained, we can note that more the value of minimal support decreases, more the execution time increases, this is explained by the important number of candidates generated after each iteration. As well more the number of transaction increases, more the execution time increases. Also for the number of partitions, the execution time diminishes by increasing the number of partitions.

B. Experiments of the parallel algorithm The implementation of our parallel algorithm permitted to  obtain satisfactory results, because we could reduce the execution time comparing with the sequential version of the algorithm Partition. Moreover, we observe that more the number of sites (partitions) increases, more the execution time decreases. Noticing a great time saving especially for an important number of transactions, this implies a minimal cost of communication between the sites. This experimentation (see Figure.6 and Figure.7) shows also a moderate increase in the execution time by increasing the value of minimal support.

o  Figure 6.

"- "'  "' 5 Partitions .......... "' 8 Partitions ............ '""'" "'" 10 Partitions  .......... :---... , ?  support 30 40 50 60  The parallel algorithm of Partition on 50000 transactions  B I---,,----'?--------== 5 Partitions  "'  --8 Partitions 4 10 Partitions  o 30 40 so 60  Figure 7. The parallel algorithm of Partition on 25000 transactions  C. Comparative study To visualize the results obtained, one presents them in a  graphic way (see Figure.8, Figure.9, Figure. I 0, Figure.ll), where one notes a significant gain of the execution time, particularly for a high number of partitions, a large database and a minimal value of support.

Figure 8.

Figure 9.

40 50 60  -- Parti'tion sequential  -- Partition parallel  support  Sequential and parallel Partition on 5 partitions and 25000 transactions  '\..

"\. -- Partition sequential  "\. -- Partition "- parallel  support 40 50 60  Sequential and parallel Partition on 10 partitions and 25000 transactions       Figure 10.

'-  '" --Partition  '" sequential  --Partition  ? parallel  support 30 40 50 60  Sequential and parallel Partition on 5 partitions and 50000 transactions  100 Sr-----------      o  Figure 11.

-  30 40  --Partition sequential  -  --Partition parallel  support   Sequential and parallel Partition on 10 partitions and 50000 transactions  From the comparative results with Count Distribution algorithm, we observe in Figure.12 and Figure.l3, a satisfactory reduction of execution time. This is due to the exchanges of data (candidates) enter the nodes in CD algorithm during each passage, where the number of candidates is important for a value minimal of support and number important of transactions. Which is not the case for our parallel Partition algorithm? We notice also a remarkable gain of execution time increasing the nodes number on our algorithm.

10<5 SO  o  Figure 12.

20<5  Figure 13.

\.

\.

\.

\.

\.

\.

"- -  30 40  .......... "'-  --Partition Paralle:l  --CD  60 Support  CD and parallel Partition on 5 partitions and 25000 transactions  \.

\.

'\.

\.

'\.

'\.

..........

.......... -  --Partit.ion Parallel  Support  CD and parallel Partition on 10 partitions and 50000 transactions

V. CONCLUSION AND PERSPECTIVES  After a study of some distributed and parallel algorithms for extraction of association rules, we noticed that most of these algorithms usually require a high number of data scan and multi-phase of synchronizations and communication which degrade their performances. Our objective was to conceive a distributed algorithm for extracting frequent itemsets considering these criteria. After a detailed study of the algorithm Partition which requires only two scans of the database, we proposed our contribution where we are interested in reducing the execution time by parallelization of the algorithm Partition in a distributed and parallel environment.

Thus, we used an approach centralized to reduce the cost of communication.

The parallel Partition algorithm presented in this paper was implemented on different dimensions, the database's size, the support's value and the sites' number (partitions). The use of super-coordinator in communication permitted to reduce the number of messages exchanged between sites, because the sending of messages is done directly with the super? coordinator. In addition, our parallel algorithm has only two phases which imply the communication, as well as the size of the messages sent during the two phases is small, the reason is that this algorithm exchanges only the accounts for the locally frequent sets, whose the number represents one fraction of the candidates' number. These minimal costs of communication and scanning of data permitted to reduce the execution time and to amplify the performances of the algorithm.

Comparing the results with count distribution algorithm, we can note that the Parallel Partition algorithm represents a remarkable time-saving for the large databases, the number of sites is high and the support is smaller. As perspectives, this algorithm can be improved in two axes:  As perspectives, this algorithm can be improved in two axes: A single scan of the database. In other words, the  algorithm uses in phase II the calculations of supports solved in phase I without obligation of a second scan.

An intelligent partitioning of the database by applying a clustering on the entire database.

