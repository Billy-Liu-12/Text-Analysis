Association Analysis of Significant Frequent Colossal Itemsets Mined from High Dimensional

Abstract?Bioinformatics has contributed to a different form of datasets called as high dimensional datasets. The high dimen- sional datasets are characterized by a large number of features and a small number of samples. The traditional algorithms expend most of the running time in mining large number of small and mid-size items which does not enclose valuable and significant information. The recent research focused on mining large cardinality itemsets called as colossal itemsets which are significant to many applications, especially in the field of bioinfor- matics. The existing frequent colossal itemset mining algorithms are unsuccessful in discovering complete set of significant fre- quent colossal itemsets. The mined colossal itemsets from existing algorithms provide erroneous support information which affects association analysis. Mining significant frequent colossal itemsets with accurate support information helps in attaining a high-level accuracy of association analysis. The proposed work highlights a novel pre-processing technique and bottom-up row enumeration algorithm to mine significant frequent colossal itemsets with accurate support information. A novel pre-processing technique efficiently utilizes minimum support threshold and minimum cardinality threshold to prune irrelevant samples and features.

The experiment results demonstrate that the proposed algorithm has high accuracy over existing algorithms. Performance study indicates the efficiency of the pre-processing technique.



I. INTRODUCTION  Mining frequent itemsets have been the primary problem in Association Rule Mining (ARM). Frequent Itemset Mining (FIM) has a wide range of applications such as business data analysis, discovering interesting items from medical data, mining web traversal items, scientific data analysis, etc.

Extensive research work has been carried over the past decade in the field of FIM. The traditional FIM algorithms work on transactional datasets, which are characterized by a large number of rows (samples) and a small number of items (features) [1]?[5]. The FIM algorithms have shown to be inadequate when the average transactional length increases.

The growth of bioinformatics has contributed to a different form of datasets, which are characterized by a large number of rows (sample) and a small number of items (features).

The amount of information that can be revealed from high-dimensional datasets is potentially huge, but extracting  information and knowledge from these datasets is a non- trivial task. The traditional algorithms face an uphill task in mining frequent itemsets from high dimensional datasets.

Row enumeration based algorithms were proposed to mine frequent closed itemsets from high dimensional datasets [6]? [9]. The output result of existing row enumeration and column enumeration based algorithms includes small and mid-sized itemsets which often has no suitable information in many applications [10]?[13]. ARM gives greater importance to the larger itemsets called as colossal itemsets especially in the field of bioinformatics. Strong and long association rules can be generated from frequent colossal itemsets [12], [13].

The FIM algorithms encounter challenges in mining rela- tively large itemsets, as they spend an enormous amount of time in mining small and mid-sized frequent itemsets. The concept of colossal itemsets was first introduced by Zhu et al. in an algorithm based on pattern fusion [10]. The existing colossal itemset mining algorithms have been classified into two groups namely feature enumeration and row enumera- tion based algorithms. Pattern fusion algorithm is a feature enumeration based algorithm that attempts to mine the large cardinality itemsets by approximating the number of colossal itemsets generated rather than traversing each node of the tree. Approximating the number of colossal itemsets generated might lead to missing some of the significant frequent colossal itemsets. Pattern fusion algorithm will not be able to mine the complete set of frequent colossal itemsets leading to the generation of an incomplete set of association rules.

BVBUC [11] is a row enumerated based algorithm to mine frequent colossal itemsets. BVBUC algorithm mine frequent colossal itemsets till the minimum support threshold level and prune off the remaining search space. BVBUC algorithm will not be able to mine complete set of frequent colossal itemsets leading to the generation of an incomplete set of association rules. Many of the mined frequent colossal itemsets tend to provide wrong support information leading to the generation of an incorrect set of association rules.

To overcome the drawbacks a novel pre-processing tech- nique and a novel Frequent Colossal Itemset Mining (FCIM)  Indian Institute of Technology (Banaras Hindu University) Varanasi, India, Dec 9-11, 2016     algorithm is proposed. The proposed pre-processing technique will prune the irrelevant samples and features by efficient utilization of minimum support threshold and minimum car- dinality threshold. The proposed algorithm mine frequent colossal itemsets using row enumeration method and bitset approach. The proposed FCIM algorithm makes efficient uti- lization of minimum cardinality threshold to mine significant frequent colossal itemsets. Results shows that the proposed pre-processing technique is very effective in pruning irrelevant sample and features compared to existing techniques. Results show that the proposed FCIM algorithm outperforms the existing algorithms in terms of mining significant frequent colossal itemsets.



II. PRELIMINARIES  Let G = {G1, G2, G3,......, Gn} be a set of gene features (items) and S = {S1, S2, S3,......, Sm} be set of samples (rows).

The high dimensional bioinformatics dataset D consists of m number of samples and n number of gene features. Each Si has a unique sample identifier, sid and consists of set of gene features. An itemset X is defined as a non-empty subset of gene features, X ? G. An itemset consisting of l gene features is defined as l-itemset. Let S(Gj) signify the samples in which gene feature j of the dataset is present. A non-empty subset of sid's Y, ? S is defined as rowset. A rowset consisting of l sids is defined as l-rowset. Let F(Si) signify the gene features present in the ith sample of the dataset.

Example 1. Table I shows an example of high dimensional dataset D consisting of 6 rows, where each row is described with unique row identifier (sid), S= {1, 2, 3, 4, 5, 6} and 9 gene features, G= {G1, G2, G3, G4, G5, G6, G7, G8, G9 }.

TABLE I High Dimensional Dataset D  sid gene features  1 G1, G2, G3, G5 2 G1, G3, G6, G8 3 G1, G3, G5, G8 4 G2, G3, G6, G8 5 G4, G5, G6, G7 6 G4, G7, G9  Definition 1 (Support). The number of rows in which an itemset X occurs is called the support of an itemset, denoted by sup(X).

Example 2. In Table I, the support of an itemset X= {G3, G6, G8}, sup(X) is 2.

Definition 2 (Support Set). The rows in which an itemset X occurs is called support set of an itemset, denoted by supset(X).

Example 3. In Table I, the support set of an itemset X= {G3, G6, G8}, supset(X) is 24.

Definition 3 (Cardinality). The number of gene features in an itemset X is called as the cardinality of an itemset, denoted by card(X).

Example 4. In Table I, the cardinality of an itemset X= {G3, G6, G8}, card(X) is 3.

Definition 4 (Frequent Itemset). An itemset X is called frequent itemset if and only if sup(X) ? minsup, where minsup is user specified least support threshold.

Example 5. In Table I, an itemset X= {G1, G5} is frequent itemset with minimum support threshold set to 2, sup(X) ? 2.

Definition 5 (Frequent Colossal Itemset). An itemset X is called frequent colossal itemset if and only if it is frequent and card(X) ? mincard, where mincard is user specified least cardinality threshold.

Example 6. In Table I, an itemset X= {G3, G6, G8} is frequent colossal itemset with minimum support threshold set to 2 and minimum cardinality threshold set to 3, sup(X) ? 2 and card(X) ? 3

III. PROPOSED PRE-PROCESSING TECHNIQUE  The features not meeting the criteria of minsup are described as irrelevant features and the rows not meeting the criteria of mincard are described as irrelevant rows. The irrelevant features are pruned before proceeding with frequent itemset mining. In frequent colossal itemset mining features should meet the criteria of minsup and rows should meet the cri- teria of mincard. The irrelevant rows and features have to be pruned before proceeding with frequent colossal itemset mining. Pruning of irrelevant features helps in reducing the cardinality of rows and pruning of irrelevant rows helps in reducing the support of features. The existing pre-processing techniques fail to take advantage of the reduction in cardinality of rows as well as the reduction in support of features.

To overcome the drawback of existing pre-processing tech- niques, a novel pre-processing technique is proposed to prune irrelevant rows and irrelevant features with effective utiliza- tion of minsup and mincard. The proposed pre-processing technique utilizes bitset approach which is computationally fast and reduces the required memory space. The bitTable consisting of dataset characteristic is constructed. Table II shows the bitTable corresponding to high dimensional dataset D shown in Table I. If a row of high dimensional dataset D consists of feature j then the jth bit of the corresponding row in bitTable is set to 1, else set to 0. The rs indicates the number of features present in that particular row. For example number of features present in 5th row is 4. The cs indicates the support of the respective feature in the bitTable. For example, the support of feature G3 in bitTable is 4.

Algorithm 1 shows the proposed pre-processing technique.

The step 2 to step 6 in the proposed pre-processing technique prune the irrelevant features. The step 2 to step 6 scans through all the available features in the bitTable and checks the support of each feature with minsup. The irrelevant features will be     TABLE II bitTable corresponding to High Dimensional Dataset D  sid G1 G2 G3 G4 G5 G6 G7 G8 G9 rs  1 1 1 1 0 1 0 0 0 0 4  2 1 0 1 0 0 1 0 1 0 4  3 1 0 1 0 1 0 0 1 0 4  4 0 1 1 0 0 1 0 1 0 4  5 0 0 0 1 1 1 1 0 0 4  6 0 0 0 1 0 0 1 0 1 3  cs 3 2 4 2 3 3 2 3 1  Algorithm 1. Proposed Pre-Processing Technique Input: bitTable, minsup, mincard.

Output: pre-processed bitTable  Initialisation : flag = 1 1: while (flag==1) do 2: for (j=0;j<no of features;j++) do 3: if (cs<minsup) then 4: delete genefeature[j] 5: end if 6: end for 7: flag=0 8: for (i=0;i<no of samples;i++) do 9: if (rs<mincard) then  10: delete sample[i] 11: flag=1 12: end if 13: end for 14: end while  pruned in the step 4. The irrelevant features G9 from the bitTable shown Table II is pruned with minsup set to 2. Table III shows the bitTable after pruning the irrelevant features G9.

The pruning of irrelevant features G9 helps in reducing the cardinality of the 6th row. To take advantage of the reduction in cardinality of rows, the step 8 to step 13 in the proposed pre-processing technique prune the irrelevant rows. The step 8 to step 13 in the proposed pre-processing technique prunes irrelevant rows by scanning through all the available rows in the bitTable and checks the cardinality of each row with mincard. The irrelevant rows are pruned in the step 10.

The irrelevant row 6 from the bitTable shown in Table III is pruned with mincard set to 3. Table IV shows the bitTable  TABLE III bitTable after pruning irrelevant features G9  sid G1 G2 G3 G4 G5 G6 G7 G8 rs  1 1 1 1 0 1 0 0 0 4  2 1 0 1 0 0 1 0 1 4  3 1 0 1 0 1 0 0 1 4  4 0 1 1 0 0 1 0 1 4  5 0 0 0 1 1 1 1 0 4  6 0 0 0 1 0 0 1 0 2  cs 3 2 4 2 3 3 2 3  TABLE IV bitTable after pruning irrelevant row 6  sid G1 G2 G3 G4 G5 G6 G7 G8 rs  1 1 1 1 0 1 0 0 0 4  2 1 0 1 0 0 1 0 1 4  3 1 0 1 0 1 0 0 1 4  4 0 1 1 0 0 1 0 1 4  5 0 0 0 1 1 1 1 0 4  cs 3 2 4 1 3 3 1 3  TABLE V bitTable after pruning irrelevant features G4 and G7  sid G1 G2 G3 G5 G6 G8 rs  1 1 1 1 1 0 0 4  2 1 0 1 0 1 1 4  3 1 0 1 1 0 1 4  4 0 1 1 0 1 1 4  5 0 0 0 1 1 0 2  cs 3 2 4 3 3 3  TABLE VI bitTable after pruning irrelevant row 5  sid G1 G2 G3 G5 G6 G8 rs  1 1 1 1 1 0 0 4  2 1 0 1 0 1 1 4  3 1 0 1 1 0 1 4  4 0 1 1 0 1 1 4  cs 3 2 4 2 2 3  after pruning the irrelevant row 6. The pruning of irrelevant rows will affect the support of the features. For example, the pruning of irrelevant sample 6 affects the support of feature G4 and G7. The process of pruning irrelevant rows and irrelevant features continues until all the remaining features and rows meet the criteria of minsup and mincard respectively. Table V shows the bitTable after pruning the irrelevant feature G4 and G7 from the bitTable shown in Table IV. Table VI show the bitTable after pruning the irrelevant sample 5 from the bitTable shown in Table V. All the remaining feature and row in the bitTable shown Table VI meet the criteria of minsup and mincard respectively. The proposed pre-processing technique effectively utilizes minsup and mincard whereas, the existing pre-processing techniques fail to take advantage of the reduction in the cardinality of rows as well as the reduction in support of features.



IV. SEARCH STRATEGY  Frequent colossal itemsets are efficiently mined from high dimensional datasets by choosing an efficient and effective search strategy to traverse an enumerated tree. The row enu- meration tree can be traversed by either bottom-up or top-down search strategy. This section shows the reason for selection of bottom-up search strategy to traverse the row enumerated three. The bitset approach is utilized, which is computationally fast and reduces the required memory space. The top-down traversing of the row enumerated space implies that the dataset     Fig. 1. Top-Down Search Strategy  is searched starting from the largest rowset value and builds smaller rowset values during the process. Fig. 1 shows the top-down row enumeration tree with each node representing the rowset.

In top-down row enumerated tree the large cardinality itemsets are present at the final level of the tree. The top- down search strategy is inefficient in mining frequent colossal itemsets as the approach expend huge amount of time in traversing small and mid-sized itemsets at the initial levels of the tree. The top-down approach fails to take the advantage of anti-monotone property of minimum cardinality threshold.

The bottom-up traversing of the row enumeration space implies that the dataset is searched starting from the smaller rowset value and builds lager rowset values during the process.

Fig. 2 shows the bottom-up row enumeration tree with each node representing the rowset. The bottom-up search strategy is efficient in mining colossal itemsets as the large cardinality itemsets are present at the initial level of bottom-up row enumerated tree.

The anti-monotone property of minimum cardinality thresh- old is utilised by the bottom-up search strategy to cut down the search space. It means that if an itemset at a node represented by l-rowset is not colossal then an itemset at a child node presented by (l+1)-rowset is also not colossal. The top-down approach fail to utilize the pruning power of mincard. Both bottom-up and top-down search strategies have advantage and disadvantage. Depending on the advantages and disadvantages, the proposed algorithm traverse the row enumerated tree in a bottom-up strategy using bitset approach.



V. THE PROPOSED ALGORITHM  Bottom-up row enumeration approach is efficient for mining frequent colossal itemsets from high dimensional datasets.

Complete set of association rules cannot be generated from the  Fig. 2. Bottom-Up Search Strategy  Algorithm 2. FCIM Algorithm Input: pre-processed high dimensional dataset, minsup, min-  card.

Output: significant frequent colossal itemsets  Initialisation: SFCI= ? r = initial row in row enumeration All bit's in bit result initialized to 1  1: FCIM(r,bit result,SFCI)  Procedure 1. FCIM(rcomb,bit result,SFCI) 1: if node rcomb does not reach till minsup 2: return 3: calculate bit result at node rcomb 4: if (|bit result|< mincard) 5: return 6: if (|rcomb|? minsup) 7: Add bit result to SFCI 8: for each node rcomb in row enumeration 9: FCIM(rcomb,bit result,SFCI)  existing algorithms as they will not be able to mine complete set of frequent colossal itemsets. Some of the mined frequent colossal itemsets from the existing algorithm provide wrong support information leading to incorrect association rules. The algorithm 2 highlights the proposed FCIM algorithm. The proposed FCIM algorithm mine frequent colossal itemsets by performing the depth-first traversal of row enumeration space.

The pre-processed high dimensional dataset, minimum support threshold, and minimum cardinality threshold are provided as an input to the FCIM algorithm. The FCIM algorithm provides significant frequent colossal itemsets as an output.

The Significant Frequent Colossal Itemset (SFCI) is initial-     ized to null. The r is the initial row in row enumeration and all the bits in bit result is initialized to 1. The step 1 of procedure 1 indicates that, if the node rcomb does not reach till minsup then, that node and its subtree if exists is pruned to cut down the search space. For example, the node 3 and 4 along with its subtree in Fig. 2 is pruned when the minsup is set to 3. The bit result is calculated at node rcomb. The step 4 of procedure 1 indicates that, if the length of an itemset mined at node rcomb is less than mincard then, the subtree of node rcomb is pruned to cut down the search space. The mined itemset is added to SFCI if it is frequent and colossal. The algorithm continues with depth-first traversal of row enumeration space.



VI. RESULTS AND DISCUSSION  The section emphasizes on the effectiveness of pre- processing technique and accuracy of FCIM algorithm in mining frequent colossal itemsets. The experiments were conducted on a computer with a specification of 3.4GHz core i7-3770 CPU, 8GB RAM and 1TB hard disk. The proposed pre-processing technique has been compared to the pre-processing technique used in BVBUC, and the accuracy of FCIM algorithm has been compared with BVBUC algorithm.

The proposed pre-processing technique and FCIM algorithm have been implemented in C++. The BVBUC algorithm and its pre-processing technique have also been implemented in C++ to the best of our knowledge. The accuracy mentioned in the figures shows the number of frequent colossal itemsets mined. The experiments were conducted on two real high- dimensional datasets: ALL-AML leukemia and lung cancer [14]. ALL-AML leukemia consists of 38 samples (rows) and 7129 gene features, lung cancer consists of 32 samples and 12533 gene features. Table VII and Table VIII shows the comparison of proposed pre-processing technique (P) and the existing pre-processing technique (E) used in BVBUC for lung cancer and ALL-AML leukemia dataset respectively. The pre- processing experiment has been conducted for different minsup and mincard.

TABLE VII Comparison of proposed pre-processing technique and pre-processing  technique used in BVBUC for Lung cancer  ? minsup 10 15 20 ? mincard P E P E P E  Rows 32 32 32 32 32 32  Features 6825 6825 4873 4873 3555 3555  Rows 31 32 30 32 28 31  Features 6810 6825 4727 4873 3265 3555  Rows 30 31 26 28 0 20  Features 6603 6825 4180 4873 0 3555  Rows 0 26 0 10 0 0  Features 0 6825 0 4873 0 3555  The experiment results show that proposed pre-processing technique takes the advantage of the reduction in the cardi- nality of rows due to the pruning of irrelevant features and a reduction in support of features due to the pruning of irrelevant  TABLE VIII Comparison of proposed pre-processing technique and pre-processing  technique used in BVBUC for ALL-AML leukemia  ? minsup 10 15 20 ? mincard P E P E P E  Rows 38 38 36 36 36 36  Features 4179 4179 3265 3265 3233 3233  Rows 20 26 20 22 18 20  Features 3233 4179 3233 3265 3196 3233  Rows 20 22 19 20 16 20  Features 3233 4179 3212 3265 3125 3233  Rows 0 0 0 6 0 0  Features 0 4179 0 3265 0 3233  Fig. 3. Number of SFCI in % for ALL AML Leukemia Dataset with minsup set to 10  Fig. 4. Number of SFCI in % for Lung Cancer Dataset with minsup set to 10  rows. The experiment results show that for a given minsup and varying mincard the number of irrelevant features pruned by the existing pre-processing technique remains same, as it fails to take the advantage of the reduction in support of features due to the pruning of irrelevant rows. When the (minsup, mincard) reaches (10,4000), (20,4000), (30,3000) for the lung cancer dataset as shown in Table VII, the number of rows and features after pre-processing is zero, which indicates that     there are no frequent colossal itemsets. This also indicates that the proposed pre-processing technique yields the final results without the need for FCIM algorithm. However, even after existing pre-processing the existing algorithms has to go through an enormous number of row combinations to fetch the final results. Comparison of proposed pre-processing technique and existing pre-processing technique with the ALL-AML leukemia dataset as shown in Table VIII also reveal the similar results.

Fig. 3 shows the number of significant frequent colossal itemsets mined by the FCIM and BVBUC algorithm for ALL AML leukemia dataset when the minsup is set to 10. Fig.

4 shows the number of significant frequent colossal itemsets mined by the FCIM and BVBUC algorithm for lung cancer dataset when the minsup is set to 10. FCIM algorithm will be able to mine more than 65% of the significant frequent colossal itemsets. FCIM algorithm outperforms BVBUC algorithm in terms of mining significant frequent colossal itemsets.



VII. CONCLUSION AND FUTURE WORK The complete set of frequent colossal itemsets will not  be mined by the existing algorithms. The proposed pre- processing algorithm prunes the irrelevant gene features and samples by effective utilization of minsup threshold and min- card threshold. The proposed FCIM algorithm outperforms the existing frequent colossal itemset mining algorithms. The FCIM algorithm mines more than 65% of significant frequent colossal itemsets for different minsup threshold and mincard threshold. Future work includes the design of the new data structure of achieving 100% mining of significant frequent colossal itemsets.

