Progressive Modeling

Abstract  Presenrly, inducrive learning is srillperformed in LI frns- rraring barch process. The user has lirrle inreracrion wirh rhe sysrem and no control over rhefrnal accuracy and rrain- ing rime. lfrhe accuracy of rhe produced model is roo low, oil the compuring resources are misspent. In rhis paper, we propose a progressive modeling framework. In progressive modeling, rhe learning olgorithm esrimares online borh rhe accuracy of rhefinnl model and remaining training rime. lf the esrimred accurocy is far below expecrarion, the user can rerminare rraining prior ro complerion wirhour wosring furrher resources. If rhe user chooses ro complere rhe learn- ing process, progressive modeling will compure a model wirh expecred accuracy in expecred rime. We describe one implemenrarion of progressive modeling using ensemble of classifiers.

Keywords: estimation  1 Introduction  Classification is one of the most popular and widely used data mining methods to extract useful information from data- bases. ISO/IEC is proposing an international standard to be finalized in August 2002 to include four data mining types into database systems; these include association rules, clus- tering, regression, and classification. Presently, classifica- tion is performed in a ?capricious? batch mode even for many well-known commercial data mining software. An inductive learner is applied to the data; before the model is completely computed and tested, the accuracy of the final model is not known. Yet, for many inductive learning algo- rithms, the actual training time is not known prior to learn- ing either. It depends on not only the size of the data and the number of features, but also the combination of feature val- ues that utimately determines the complexity of the model.

During this possibly long waiting period, the only interac- tion between the user and program is to make sure that the program is still running and observe some status reports. If the final accuracy is tca low after some long training time,  Figure 1. An interactive scenario where both accuracy and remaining training time are es- timated  all the computing resources become futile. The users ei- ther have to repeat the same process using other parame- ters of the same algorithm, choose a different feature sub- set, select a complelely new algorithm or give up. There are many learners to choose from, a lot of parameters to select for each learner. countless ways to construct features, and exponential ways for feature selection. The unpredictable accuracy, long and hard-to-predict training time, and end- less ways to run an experiment make data mining frustrating even for experts.

1.1 Example of Progressive Modeling  In this paper, we propose a ?progressive modeling? con- cept to address the problems of batch mode learning. We illustrate the basic ideas through a cost-sensitive example even though the concept is applicable to both cost-sensitive and traditional accuracy-based problems.

We use a charity donation dataset (KDDCup 1998) that chooses a subset of population to send campaign letters.

The cost of a campaign letter is $0.68. It is only beneficial to send a letter if the solicited person will donate at least $0.68. As soon as learning starts, the framework begins to compute intermediate models, and report current accuracy as well as estimated final accuracy on a hold-out valida- tion set and estimated remaining training time. For cost- sensitive problem, accuracy is measured in benefits such as dollar amounts. We use the term accuracy to mean tra- ditional accuracy and benefits interchangeably where the meaning is clear from the context. Figure 1 shows a snap  0-7695-1754-4/02 $17.00 Q 2002 IEEE 163  http://SloQStatS.columbia.edu http://salocs.columbia.edu   shot of the new learning prucess. It displays that the ac- curacy on the hold out validation set (total donated charity minus the cost of mailing to both donors and non-donors) for the algorithm using the current intermediate model is $ I  2840.5. The accuracy of the complete model on the hold- out validation set when learning completes is estimated to he $14289.5f100.3 with at least 99.7% confidence. The additional training time to generate the complete model is estimatedto be 5.40f0.70 minutes with at least 99.7% con- fidence. This information continuously refreshes whenever a new intermediate model is produced, until the user explic- itly terminates the learning or the complete model is gener- ated.

The user may stop the learning process mainly due to the following reasons - i) intermediate model has enough accuracy, ii) its accuracy i s  not significantly different from that of the complete model, iii) the estimated accuracy of the complete model is too low. or iv) the training time is unexpectedly long. For the example shown in Figure 1, we would continue, since it is worthwhile to spend 6 more min- utes to receive at least $1400 more donation with at least 99.7% confidence. In this example. we illustrated progres- sive modeling applied to cost-sensitive learning. For cost- insensitive learning. the algorithm reports traditional accu- racy in place of dollar amounts.

Progressive modeling is significantly mbre useful than a batch mode learning process, especially for very large dataset. The user can easily experiment with different algo- rithms, parameters, and feature selections without waiting for a long time for a failure result.

2 Our Approach  W e  propose an implementation of progressive modeling based on ensembles of classifiers that can be applied to sev- eral inductive leaming algorithms. The basic idea is to gen- erate a small number of base classifiers to estimate the per- formance of the entire ensemble when all base classifiers are produced.

2.1 Main Algorithm  Assume that a training set S is partitioned into K disjoint subsets S j  with equal size. When the distribution of the dataset is uniform, each subset can be taken sequentially.

Otherwise, we can either completely ?shuffle? the dataset or  use random sampling without replacement to draw Sj.

A base level model Cj is trained from Si. Given an exam- ple z from a validation set S, (it can he a different dataset or the training set), Cj outputs probabilities for all possible class labels that z may be an instance of, i.e., pj(tilz) for class label e, .  Details on how to calculate pj(e,lz) can be found in [S].ln addition, we have a benefit matrix b[ei,Ej]  that records the benefit received by predicting an example of class e,  to he an instance of class e j .  For cost-insensitive (or accuracy-based) problems, V i ,  b[ti,l,] = 1 and Va # j ,  b[&, tj] = 0. Since traditional accuracy-based decision making is a special case of cost-sensitive problem. we only discuss the algorithm in the context of cost-sensitive deci- sion making. Using benefit matrix b[. . .I, each model Ci will generate an expected benefit or  risk ej(eilz) for every possible class t i .

ExpectedBenefit: ej(Eilz) = b[e i . , e i ]  . p j ( e i 8 1 z ) I ,  ( 1 ) Assume that we have trained k 5 K models {Cl ,  . . ,C,.).

Combining individual expected benefits. a e  have  E, e , V 8 I d k  Average Expected Benefit. Eh((,  I) = (2)  We then use optimal Jecision policy to choose the class la- bel with the maimal  expected benefit  Optimal Decision. L k ( z )  = argmax,, &( I ,  r )  (3)  Asruming lh3i f(r) is the true labcl of I. the accuracy of the ensemble uith k classifiers is  For accuracy-hasedproblems, A h  is usually normalized into percentage using the size of the validation set IS,I. Forcost- sensitive problems, it is customary to use some units to mea- sure benefits such as dollar amounts. Besides accuracy, we also have the total time to train CI to C k .

T k  = the total time to train{C,, . . . , C k }  ( 5 ) Next. based on the performance of k 5 K base classi-  fiers. we use statistical techniques to estimate both the ac- curacy and training time of the ensemble with K models.

We first summarize some notations. AK,  TK and M K are the true values to estimate. Respectively. they are the accuracy of the complete ensemble, the training time of the complete ensemble. and the remaining training time after k classifiers. Their estimates are denoted in lower case, i.e., a ~ ,  t K  and mK. An estimate is a range with a mean and standard deviation. The mean of a symbol is represented by a bar 0 and the standard deviation is represented by a sigma (a). Additionally, a d  is standard error or the standard deviation of a sample mean.

2.2 Estimating Accuracy  The accuracy estimate is based on the probability that ti is the predicted label by the ensemble of K classifiers for     example z.

P { L x ( z )  = t i )  the probability that ti is the prediction by the ensemble of size K (6)  Since each class label ti has a probability In be the predicted class, and predicting an instance of class !(x) as ti receives a benefit b[e(x), P i ] ,  the expected accuracy received for x by predicting with K base models is  a ( ~ )  = C b [ e ( x ) , e i i  .P{LK(x)  = e , )  (7) 1.

with standard deviation of o(a(x) ) .  To calculate the ex- pected accuracy on the validation set S,, we sum up the expected accuracy on each example x  aK = a ( x )  (8) ZES.

Since each example is independent, according to multino- mial form of central limit theorem (CLT). the total benefit of the complete model with K models is a normal distribu- tion with mean value of Eq[8] and standard deviation of  Using confidence intervals, the accuracy of the complete en- semble AK falls within the following range:  Withconfidencep,AK E  ti^ f t . o ( a ~ )  (IO)  When t = 3. the confidencep is approximately 99.7%.

Next we discuss how to derive P{LK(x) = t i } .  If  EK(t,lx) are known, there is only one label, L K ( z )  whose P{L,(x) = t i )  will he 1. and all other labels will have probability equal to 0. However, EK(P,Ix) are not known, wecannnly useitsestimateEt(P,lx) measuredfrom kclas- sifiers to derive P{LK(x) = t i ) .  From random sampling theory [21, Et(e,lz) is an unbiased estimate of EK(LiIx) with standard error of  According to central limit thereon, the true value E ~ ( t i 1 z ) falls within a normal distribution with mean value of p = Et(&lx)  and standard deviation of U = ud(Et(ti1x)). If Et(ei lx)  is high, it is more likely for Ex(Pilz) to he high, and consequently, for P{I+(x) = e , )  to be high. For the time being, we ignore correlation among different class la- bels, and compute naive probability P'{LK(z) = &), As- suming that rt is an approximate of maxt.

area in the rangeof [r,, +w) is the probability P '{LK(z)  = t i ) .

whereo = od(Et(&lx))  a n d p  = Et(eilx). When k 5 30.

to compensate the error in standard error estimation, we use Student-r distribution with df = k. We use the average of thetwolargestEt(tiIs)'stoapproximatemaxc, E K ( ~  lz)) The reason not to use the maximum itself is that if the asso- ciated label is not the predicted label of the complete model, the probability estimate for the true predicted label may he too low.

On the other hand, P { L k ( x )  = t i )  is inversely related  to the probabilities for other class labels to be the predicted label. When it is more likely for other class labels to be the predicted label, it will he less likely for& to be the predicted label. A common method to take correlation into account is to use normalization.

( i  Thus, we have derived P{Lx( z )  = ti) in order to esti- mate the accuracy in Eq[7].

Estimating Training Time Assuming that the training time for the sampled k models are TI to r t .  Their mean and stan- dard deviation are f and U(.). Then the total training time of K classifiers is estimated as  With confidencep. TK E t~ f t . U ( t K ) ,  where  CK = K .  .f and u ( t ~ )  = . (14)  To find out remaining training time M K ,  we simply deduct k . f f romEq[14] .

With confidencep, MK E l T l ~  f t .  u(mK),  where  t . K . U(T) A  r f l ~  = C K  - k . f  and u(mK) = o( tK)  (15)  2.3 Progressive Modeling  We request the first random sample from the database and train the first model. Then it requests the second ran- dom sample and train the second model. From this point on, the user will be updated with estimated accuracy, remaining training time and confidence levels. We have the accuracy of the current model (At) ,  and the estimated accuracy of the complete model (aK)  as well as estimated remaining train- ing time (mK). From these statistics. the user decides to continue or  terminate. The user normally terminates learn- ing if one of the following Sropping Criteria are met:     ~~ ~ ~~  Data : benefit matrix b[&'j,tj], training set S, vali-  Result : k 5 K classifiers dation set S. and K  begio partition S into K disjoint subsets of equal size {SI, .... S K h train CI from SI, and TI is the training time; k t 2; wbile k < K  do  train c k  from s k  and rk  is the training time; for z E S. do  calculate P{LK =&} (Eq[131); calculate d(z )  and its standard deviation M z ) )  (EqI71);  end estimate accuracy OK (EqlSI and Eq19l) and remaining training time rnx (F.qI151); if a K  and mK sarisfi sropping crireria then  end k t k + 1 ;  end return Cl,. . . , C,;  return c, , . . . , c*;  end  .Igorithm 1: Progressive Modeling Based on Averag- ing Ensemble  actual 7 f r a u d  490  rn The accuracy of the current model is sufficiently high.

Assume that SA is the target accuracy.

The accuracy of the current model is sufficiently close to that of the complete model. There won't be signif- icant improvement by training the model to the end.

Formally, t . u ( a ~ )  5 e.

The estimated accuracy of the final model is t m  low to be useful. Formally, if ( i i ~  + t . u ( a ~ ) )  << BA.

stop the learning process.

The estimated training time is too long, the user de- cides to abort. Formally, assume that OT is the target training time, if ( m ~  - t . u ( ~ K ) )  > OT, cancel the leaming.

As a summary of all the important steps of progressive modeling, the complete algorithm is outlined in Algorithm 1,  2.4 Efficiency  Computing K base models sequentially has complexity of K .  O ( f ( $ ) ) .  Both the average and standard deviation can be incrementally updated linearly in the number of ex- amples.

vious months as training data (406009 examples). Details about this dataset can be found in [9].

The third dataset is the adult dataset from UCI repository.

It is a widely used dataset to compare different algorithms on traditional accuracy. for cost-sensitive studies. we anifi- cially associate a benefit of $2 to class label F and a benefit of $1 to class label N. as summarized below:  actual N We use the natural split of training and test sets. so the results can be easily duplicated. The training set contains 32561 entries and the test set contains 16281 records.

3.2 Experimental Setup  We have selected three learning algorithms, decision tree learner C4.5, rule builder RIPPER, and naive Bayes learner.

We have chosen a wide range of partitions, K E {S, 16,32,64,128,256). The accuracy and estimated accuracy is the test dataset.

3.3 Accuracy  Since we study the capability of the new framework for both traditional accuracy-based problems as well as cost- sensitive problems, each dataset is treated both as a tradi- tional and cost-sensitive problem. The baseline traditional accuracy and total benefits of the batch mode single model are shown in the two columns under accuracy for traditional accuracy-based problem and benefits for cost-sensitive prob- lem respectively in Table I .  These results are the baseline that the multiple model should achieve.'  For the multiple model, we first discuss the results when the complete multiple model is fully constructed, then present the results of partial multiple model. Each result is the av- erage of different multiple models with K ranging from 2 to 256. In Table 2, the results are shown in two columns under accuracy and benefit. As we compare the respective results in Tables I and 2, the multiple model consistently and significantly beat the accuracy of the single model for all three datasets using all three different inductive learn- ers. The most significant increase in both accuracy and total benefits is for the credit card dataset. The total benefits have been increased by approximately $7.000 - $1 0,oOO. the ac- curacy has been increased by approximately I % - 3%. For the KDDCUP'YX donation dataset. the total benefit has been increased by $1400 for C4.5 and $250 for NB.

'Please nole hat we erpenmenled wilh different parameters for RIP- PER on he donation dataset. However, he most specific rule produced by RlPPER contains only one ru le ha@ covers 6 donors and one default rule hat always predict -donate. This succinct ru le will not find any donor and will not receive any donations. However, RIPPER performs reason- ably well far the credil card and adult darnels.

accuracy benefit Donation 9494% $13292.7 Credit Card 87.77% 1733980 Adult 84.38%  Credit Card 90.14% 1712541  .I. I..

Accuracy Based [[ Corl-sensitive accuracy 11 be"Cfi1  Donation II 94.94% II Credit Card 90.14% 1712541 Addl 84.84% $19725  Credit Card 85.46% $704285  Table 1. Baseline accuracy and total benefits  We next study the trends of accuracy when the number of partitions K increases. In Figure 2, we plot the accuracy and total benefits for the credit card datasets, and the total benefits for the donation dataset with increasing number of partitions K .  C4.5 was the base learner for this study. As we can see clearly that for the credit card dataset, the mul- tiple model consistently and significantly improve both the accuracy and total benefits over the single model by at least 1 % in accuracy and $4oooO in total benefits for all choices of K .  For the donation dataset, the multiple model boosts the total benefits by at least $1400. Nonetheless, when K increases, both the accuracy and total tendency show a slow decreasing trend. It would be expected that when K is ex- tremely large, the results will eventually fall below the base- line.

3.4 Accuracy Estimation  The current and estimated final accuracy are continu- ously updated and reported to the user. The user can termi- nate the leaming based on these statistics. As a summary, these include the accuracy of the current model Ar ,  the true accuracy of the complete model AK and the estimate of the true accuracy i i ~  with U ( Q K ) .  If the true value falls within the error range of the estimate with high confidence and the error range is small, the estimate is good. Formally, with confidencep, AK E i i ~  f t .  U ( Q K ) .  Quantitatively, we say an estimate is good if the error bound ( t  . U) is within 5% of the mean and the confidence is at least 99%. We chose k = 20%. K .  In Table 3, we show the average of estimated accuracy of multiple models with different number of part- tions K = {&. . . ,256).  The true value AK all fall within     Figure 2. Plots of accuracy and total benefits for credit card datasets, and plot of total benefits for donation dataset with respect to K  ' - D i f . T a B n * .  -, Olbl-Bh  c h m c . r d o b r ~ l ~ -  '- _33 I y m  uLIcMD.ym.w- ' cmC*am+ -'  ow cm-ma---  Credit  Adult  Donation  90.37% 90.08%+l.5% $804964 9799876f3212  1 85.6% 8 5 . 3 8 1 1 . 4 8  $16435 $16255f142 RIPPER Accuracy Based 11 Cost-sensitive  aCC"rnCY benefit Donation I1 94.9410% I1 $ O M Credit Card Adult  91 46f0 6% 5815612f34730 86 I f 0  4% $19875+390  Donation 94.94+0% 514282f530  Credit card  Adult  Table 2. Average accuracy and total bene- fits by complete multiple model with different number of partitions.

91.46% 91.24%+0.9% $815612 $82WIZ+3742  I 86.1% 85.9%+1.3% $19875 119668f258  the error range.

To see how quickly the error range converges with in-  creasing sample size, we draw the entire process to sample up to K = 256 for all three datasets as shown in Figure 3.

There are four curves in each plot. The one on the very top and the one on the very bottom are the upper and lower error bounds. Thecurrent benefits and estimated total benefits are within the higher and lower error hounds. Current benefits and estimated total benefits are very close especially when k becomes big. As shown clearly in all three plots, the error bound decreases exponentially. When k exceeds 50 (ap- proximately 20% of 256), the error range is already within 5% of the total benefits of the complete model. If we are satisfied with the accuracy of the current model, we can dis-  Credit card  Accuracy Based Cost-sensitive True Val I Estimate 11 True Val I Estimate I1  Donation I/ 94.94% I 94.94%fO% I/ $14702.9 I 514913f612  88.64% 89.0I%fI.Z% $798943 3797749f4523  RmPER Accuracy Based Con-sensitive  True Val I Estimate 11 True Val I Estimate n Donation 11 94.94% I 94.94%+0% $ 0 1  $of0  NB Accuracy Based Cost-sensitive  True Val I Estimate 11 True Val [ Estimate n dona ti^^ 11 94.94% I 94.94%+0% 11 $14282 I $1438~+120  Table 3. True accuracy and estimated accu- racy.

continue the learning process and return the current model.

For the three datasets under study and different number of panitions K, when k > 30%. K, the current model is usu.

ally within 5% error range of total benefits by the complete model. For traditional accuracy, the current model is usu- ally within 1 % error bound of the accuracy by the complete model (detailed results not shown).

Next, we discuss an experiment under extreme situations.

When K becomes too big, each dataset becomes trivial and will not he able to produce an effective model. If the esti-     Figure 3. Current beneflts and estimated final benefits when sampling size k Increases up to K = 256 for all three datesets. The error range Is 3 .  ~ ( ( I K )  for 99.7% contldence.

ILI Im (Y) na 1% I- Y) rm ,Y) na n o lunpwS*.INmmnCi.m*.)  IO im In na 2s -sb* W d d  C.ul*?l funm Sk.lNlunmolC.ul*.,  mation methods can effectively detect the inaccuracy of the complete model, the user can choose a smaller K. We par- titioned all three dataset into K = 1024 partitions. For the adult dataset, each partition contains only 32 examples but there are 15 attributes. The estimation results are shown in Figure 4. The first observation is that the total benefits for donation and adult are much lower than the baseline. This is obviously due to the trivial size of each data partition. The total benefits for the credit card dataset is $750,000. which is still higher than the baseline of $733980. The second ob- servation is that after the sampling size k exceeds around as small as 25 (out of K = 1024 or 0.5%). the error bound becomes small enough, implying that the total benefits by the complete model is very unlikely (99.7% confidence) to increase. At this point. the user should cancel the learn- ing for both donation and adult datasets. The reason for the ?bumps? in the adult dataset plot is that each dataset is too small and most decision trees will always predict N most of the time. At the beginning of the sampling, there is no variations or all the trees make the same predictions; when more trees are introduced, i t  starts to have some diversities.

However, the absolute value of the bumps are less than $50 as compared to $12435.

3.5 Training Efficiency  We recorded both the training time of the hatch mode single model plus the time to classify the test data, and the training time of the multiple model with k = 30%. K clas- sifiers plus the time to classify the test data k times. We then computed ratio of the recorded time of the single and mul- tiple models. called serial improvement. It is the number of limes that training the multiple model is faster than training the single model. In Figure 5, we plot the serial improve- ment for all three datasets using C4.5 as the base learner.

When K = 256, using the multiple model not only provide higher accuracy, but the training time is also 80 times faster for credit card. 25 times faster for both adult and donation.

4 Related Work  Online aggregation has been well studied in database community. It estimates the result of an aggregate query such as avg (AGE) during query processing. One of the most noteworthy work is due to [7], which provides an in- teractive and accurate method to estimate the result of ag- gregation. One of the earliest work to use data reduction techniques to scale up inductive learning is due to Chan [ I ] , in which he builds a tree of classifiers. In BOAT [6]. Gehrke et al build multiple bootstrapped trees in memory to ex- amine the splitting conditions of a coarse tree. There has been several advances in cost-sensitive learning [3]. Meta- Cost [4] takes advantage of purposeful mis-labels to max- imize total benefits. In 181, Provost and Fawcett study the problem on how to make optimal decision when cost is not known precisely.

5 Conclusion  In this paper, we have demonstrated the need for a pro- gressive and interactive approach of inductive learning where the users can have full control of the learning process. An important feature is the ability to estimate the accuracy of complete model and remaining training time. We have im- plemented a progressive modeling framework based on av- eraging ensembles and statistical techniques. One impor- tant result of this paper is the derivation of error bounds used in performance estimation. We empirically evaluated our approaches using several inductive learning algorithms.

First, we find that the accuracy and training time by the pro- gressive modeling framework maintain or greatly improve over batch mode learning. Second the precision of estima- tion is high. The error hound is within 5% of the true value when the model is approximately 25% - 30% complete.

Based on our studies, we conclude that progressive mod- eling based on ensemble of classifiers provide an effective     Figure 4. Current benefits and estimated final estimates when sampling size k increases up to K = 1024 for all three datasets. To enlarge the plots when k is small, we only plot up to k = 50. The error range is 3 .  u(uK)  for 99.7% confidence.

:::~ Figure 5. Serial improvement jir4 for all three datasets when early stopping Is used 3 10 I: 1 8 10  m  s I r n  1m m zra m ,rn 1% m zra m Irn Im m ma WC4P.n-  W d P . / C n .  W d P b  solution to the frustrating process of batch mode learning.

