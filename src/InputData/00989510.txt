Using Rule Sets to Maximize ROC Performance

Abstract  Rules are commonly used for classification because they are modular; intelligible and easy to learn. Existing work in classijication rule learning assumes the goal is to produce categorical classifications to maximize classijication accu- racy Recentiwork in machine learning has pointed out the limitations of classijication accuracy: when class distribu- tions are skewed, or error costs are unequal, an accuracy maximizing rule set can perform poorly. A more flexible use of a rule set is to produce instance scores indicating the like- lihood that an instance belongs to a given class. With such an abilit): we can apply rulesets effectively when distribu- tions are skewed or error costs are unequal. This paper empirically investigates different strategies for evaluating rule sets when the goal is to maximize the scoring (ROC) performance.

1. Introduction  Rules are commonly used in data mining because of sev- eral desirable properties: they are simple, intuitive, modu- lar, and straightforward to generate from data. Some work concentrates on association rule mining, in which individual rules are valued for the insight they can bring. Another area of work, termed classification rule learning, strives to gen- erate rules that collectively have good classification perfor- mance [6 ] .  It is the use of rules for classification with which this paper is concerned. Existing methods strive to opti- mize classification decisions, usually by maximizing accu- racy (or equivalently, minimizing error rate) on a training set. They usually try to construct small, compact rule sets while achieving high accuracy.

Recent work in machine learning and data mining has demonstrated problems with using accuracy as a metric [ 1 13. It can be irrelevant or misleading when classes are im- balanced or when misclassification costs are unequal. Ide- ally, costs should be taken into account and accuracy max-  imization should be replaced with cost minimization. If er- ror costs and class distributions are known exactly, a cost minimizing problem can sometimes be transformed into an accuracy maximizing one. However, in many cases neither costs nor class distributions are known exactly, and both can change over time and over contexts [9, 51. As conditions change, a high accuracy rule set may produce sub-optimal classifications. Data mining that uses rules for real world classification tasks will eventually face these problems.

One way to allow flexibility under uncertain conditions is to use probabilities. Instead of requiring that a classi- fier produce a hard (discrete) class decision for each in- stance, we can use the classification model to generate an estimated probability that an instance belongs to a specific class. Given such probabilities, simple decision theory can be used to generate thresholds under various assumptions of error costs and class distributions. This enables robust clas- sification systems that can operate in uncertain and chang- ing environments [ 101.

Probabilistic classification has been investigated with other model classes such as neural networks [I41 and en- sembles of decision trees [8, 211. These models tend to be much more complex than rule sets and may not have rules? appealing properties of modularity and intelligibility. An open question in data mining is how to use a set of rules to produce reliable instance probabilities. This approach raises issues of how to generate scores from rules, how to combine scores from different rules, and how to select rules for inclusion in a rule set.

This paper explores these issues empirically. We employ the area under an ROC curve (commonly referred to as the AUC) to evaluate and compare strategies [2]. The remain- der of this paper is structured as follows. We begin by dis- cussing ROC curves and the AUC as a tool for measuring instance scoring performance. We discuss rules and rule- sets, and how they can be used for classification. We then describe a number of experiments investigating the use of classification rules for scoring instances. We conclude by discussing future work and some related issues.

0-7695-1119-8/01 $17.00 0 2001 IEEE 131  mailto:acm.org   1 0  1 0  d 0 8  d O B a, a,  c r  a, a,  I-  !? !?

v) 0 6  a a 2 0 4  0 4  .- .- m 0 6  0 2  0 2  0 ?  0 I I I I   0 0 2  0 4  0 6  0 8  1 0  0 0 2  0 4  0 6  0 8  1 0  0 0 2  0 4  0 6  0 8  1 0  False Positive rate False Positive rate False Positive rate  Figure 1. ROC graphs and area under ROC curves.

2. ROC graphs and the AUC  To evaluate probabilistic classifiers we adopt Receiver Operating Characteristics (ROC) analysis. ROC graphs have long been used in signal detection theory to depict the tradeoff between hit rates and false alarm rates of classifiers [4, 171. ROC analysis has been extended for use in visualiz- ing and analyzing the behavior of diagnostic systems [ 161.

A discrete classifier applied to a test set generates two important statistics. The True Positive rate (also called hit rate and recall) of a classifier is:  positives correctly classified to ta l  positives  TP  rate KZ  The False Positive rate (also called false alarm rate) of the classifier is:  negatives incorrectly classified total negatives  FP rate E  On an ROC graph, TP  rate is plotted on the Y axis and FP rate is plotted on the X axis.

A discrete classifier-one that outputs only a class label-produces an (FP rate,TP rate) pair, so it corresponds to a single point in ROC space. Classifiers A and B in Fig- ure 1 a are discrete classifiers.

Several points in ROC space are useful to note. The lower left point (0,O) represents the strategy of never is- suing a positive classification; such a classifier commits no false positive errors but also gains no true positives. The opposite strategy, of unconditionally issuing positive classi- fications, is represented by the upper right point (1,l). Any  classifier that randomly guesses the class will produce per- formance on the diagonal line y = 2. The point (0 , l )  rep- resents perfect classification. Informally, one point in ROC space is better than another if it is to the northwest (TP rate is higher, F P  rate is lower, or both) of the first.

The diagonal line y = 2 represents the strategy of ran- domly guessing a class, and any classifier that appears in the lower right triangle performs worse than random guessing.

This triangle is therefore usually empty.

Classifiers can often be coerced into producing a proba- bility estimate or numerical rank for each instance. Such a ranking or scoring classifier can be used with a threshold to produce a binary classifier: if the classifier output is above the threshold, the classifier produces a Y, else a N. Each threshold value produces a different point in ROC space, so varying the threshold from --oi) to +CO produces a curve through ROC space. An ROC curve illustrates the error tradeoffs available with a given classifier. Figure la shows the curve of a probabilistic classifier, C, in ROC space. A more thorough discussion of ROC curves may be found in Provost and Fawcett?s article [ 101.

An important point about ROC graphs is that they mea- sure the ability of a classifier to produce good relative in- stance rankings. A classifier need not produce accurate, calibrated probability estimates; it need only produce rel- ative accurate scores that serve to discriminate positive and negative instances. Thus, although this paper refers to rule sets used as probabilistic classifiers, these classifiers only need to produce good relative scores.

2.1. Area under an ROC curve (AUC)  An ROC curve is a two-dimensional depiction of clas- sifier performance. To compare classifiers we often want to reduce ROC performance to a single number represent- ing average expected performance. A common method is to calculate the area under the ROC curve, abbreviated AUC [2, 71. Since the AUC is a portion of the area of the unit square, its value will always be between 0 and 1.0. How- ever, because random guessing produces the diagonal line between (0,O) and (1, l), which has an area of 0.5, no real- istic classifier should have an AUC less than 0.5.

The AUC has an appealing statistical property: the AUC of a classifier is equivalent to the probability that the clas- sifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance. This is equiva- lent to the Wilcoxon test of ranks [7]. It is possible for a high-AUC classifier to perform worse in a specific region of ROC space than a low-AUC classifier, but in practice the AUC performs very well and is often used when a general measure of predictiveness is desired.

Figure 1 b shows the areas under two ROC curves, A and B. B has greater area and therefore better average perfor- mance. Figure I C  shows the area under the curve of a bi- nary classifier A and a scoring classifier B. Classifier A rep- resents the performance of B when B is used with a single, fixed threshold. Though the performance of the two is equal at the fixed point (B?s threshold), B?s performance becomes inferior to A further from this point.

2.2. AUC with multiple classes  The two ROC axes represent tradeoffs between false pos- itives and true positives with two classes. ROC analysis has been extended to multiple classes [ 151, but the result in general is non-intuitive and computationally expensive. In practice, n classes are commonly handled by producing n different ROC graphs. Let C be the set of all classes. ROC graph i plots the classification performance using class ci as the positive class and all other classes cj#i E C as the negative class. Each such graph yields an AUC area.

For a single probabilistic classifier this produces n sep- arate curves with n different AUC values. The AUC val- ues can be combined into a single weighted sum where the weight of each class ci is proportional to the class?s preva- lence in the training set:  AUCtotai = AUC(ci) .p(ci) c,  EC  3. Rules and classification  A rule is a conjunction of conditions, the satisfaction of which implies membership in a class. For simplicity, con-  sider a two-class problem with classes p and n. An example of a simple rule and some of its performance statistics is:  X ~ A X ~ A X Q + P TP=15, P=IOO, TPrate=.15 FP=2, N=200, FPrate=.Ol  The second line specifies that within the dataset, 15 p ex- amples satisfy z1 A 22 A 23 (True Positives). There are 100 p examples altogether, yielding a true positive rate (TPrate) of .15. The rule matches two n examples (False Positives).

There are 200 n examples altogether, yielding a false posi- tive rate (FPrate) of .01.

3.1. Rule sets  For the purpose of classification, rules are usually aggre- gated into a rule set. When the set is ordered this is called a decision list [ 131. To classify an instance, each rule in the list is tried in sequence, and the first rule whose conditions are satisfied determines the hypothesized class. A decision list is an ?if-then-elseif- ...- else-? formulation of a boolean concept. General cases usually appear toward the end of the list with more specific cases (exceptions) placed at the front. If no rule matches, the final classification is the most prevalent class.

Given a decision list and an instance set, a 2 x 2 confu- sion matrix can be generated, representing the classification performance:  Hypothesized class P n  Actual p TP FN class n FP TN  This adds the TN (True Negatives) and FN (False Nega- tives) statistics. From this matrix we can estimate the error rate of the classifier:  Errorrate = F P  + F N T P +  T N  + F P  + F N  Accuracy = 1 - Error rate  Error rate weights FP and FN equally. If we have sep- arate error cost functions c ( F P )  and c ( F N ) ,  we instead want to measure expected cost:  Cost = F P  . c (FP)  + F N  . c ( F N ) Given such cost functions, simple decision analysis pro- vides a way to determine each instance?s optimal classifi- cation, if we can get an estimate of its class probability. For each instance I we should hypothesize the positive class p i f  [1 - P(PlI)l . c ( F P )  < P(Pl0  . c ( F N )     Note that the priorp(p) is incorporated into the posterior es- timate p(pl1). To minimize overall error cost, we thus need a way to estimate instance probabilities p ( p l 1 ) .  Given the statistics of a matching rule, we can generate a probability estimate simply as  T D  This measure is commonly called the conjidence of the rule.

This equation is often used in a Laplace corrected form:  Laplace correction smoothes probability estimates when the number of instances covered by a rule is small.

As mentioned above, we do not need accurate instance probabilities p ( p l 1 )  to make the classification decision. We only need good relative instance scores. Given a test set and knowledge of performance conditions, we can derive a suitable threshold [ 101.

3.2. Resolving rules  If rules were mutually exclusive, each instance would match at most one rule and a probability estimate could be taken directly from the rule?s confidence. However, rules can overlap and multiple rules may ?claim? an instance, re- sulting in potentially conflicting classifications and instance scores. Resolving these into a single class and score is called the resolution problem. Given the information avail- able from each rule, a number of strategies can be enumer- ated:  Random selection (RAND). A random matching rule is chosen, and its class and confidence are used as the winning class and score. RAND provides a baseline against which other strategies may be compared.

First matching rule (FIRST). Each rule in a list is tested in turn and the first matching rule wins. The rule?s class and confidence become the winning class and score. This strategy is appropriate when order is imposed, as in decision lists [13]. When rules are sorted by decreasing confidence, this method selects the highest confidence rule.

The rationale for weighted voting is that high con- fidence rules should have more influence than lower confidence ones.

5. Lowest false positive rate (LFPR). Among matching rules, the rule with the lowest false positive rate is se- lected. Its class and confidence are used as the class and score. The rationale for LFPR is to choose the matching rule that has the least chance of committing a false positive error.

With each method, if no rule fires on an instance, the majority class is used and the majority class prevalence is used as the instance score.

4. Experiments  Given the discussion above, we investigate the following questions:  1. Is instance scoring really necessary for good AUC per- formance?? Perhaps rule sets already produce good classification performance throughout ROC space.

Some prior work has found this not to be true with other model classes [ 1 I ] ,  but this hypothesis is worth testing this hypothesis with rule set classifiers.

2 .  What is the effect of various rule set resolution strate- gies on a rule set?s instance scores??

3. How well do rules perform relative to other methods for scoring instances??

4.1. Rule induction methods  Two rule induction methods were used in the experi- ments below. They are fairly different in operation and in results.

C4.5rules [I21 is a companion program to C4.5 which creates rule sets by post-processing decision trees.

C4.5rules begins by constructing a rule from each path to a leaf node, with each attribute test in the path becoming a conjunct in the rule. This results in potentially a large num- ber of rules, but the initial set of rules is mutually exclusive.

C4.5rules then examines the rules, testing each~conjunct to determine whether it is necessary; if rule accuracy is unaf- fected, the conjunct is deleted. After deleting conjuncts, the resulting rule set is no longer mutually exclusive and ex- haustive, so C4.5rules performs several final steps for im-  Equal voting (VOTE). Each rule is tested, and every matching rule contributes a single vote for its The majority class wins. The score assigned to the in- stance is the fraction of votes won by the majority.

Weighted voting (WVOTE). This is similar to VOTE, but each matching rule votes with a strength of its con- fidence. The class with the highest summed confi- dence wins, and the score is the average confidence.

proving the rule set. Finally it groups the rules by class, based on the number of false positive errors committed by each class subgroup. In the experiments reported here, both C4.5 and C4.5rules were used with their default settings.

Dataset Breast-wisc Car Cmc Covtype Crx German Glass Image Kr-vs-kp Mushroom Nursery Promoters Sonar Splice  Rt FIRST  50.1 f 0.5 61.7 f 7.0 63.7 f 4.1 64.4 f 3.8 75.0 f 4.3 67.1 f 4.9 70.1 f 9.1 86.0 f 4.1 52.9 f 3.9 53.4 f 2.5 93.7 f 0.4  47.1 f 20.6 57.0 f 11.0 45.9 f 2.5  RAND 69.9 f 6.8 74.0 f 2.7 63.2 f 3.8 69.0 f 1.9 70.0 f 7.5 58.2 f 4.7  68.4 f 10.1 88.8 f 2.1 66.2 f 3.0 90.1 f 1.8 90.2 f 0.6  51.8 f 12.6 48.3 f 11.2  57.8 f 2.3  h t i o n  strategies 11 Numberc WVOTE  94.7 f 3.4 94.3 f 1.4 63.9 f 4.0 73.3 f 1.5 90.2 f 4.2 71.9 f 4.9  71.7 f 10.5 92.3 f 1.6 88.8 f 2.4 99.9 f 0.1 96.0 f 0.3  83.5 f 16.2 65.8 f 12.8  87.3 * 1.6  Generated 306.5 107.6 196.6  1416.6 758.5 807.5 183.7 811.4  2328.3 2362.2 606.6  7432.2 10075.7 8406.8  LFPR 97.6 f 1.3 92.3 f 1.7 63.7 f 4.2 72.9 f 2.1 83.9 f 5.1 66.2 f 5.3  71.2 f 10.0 93.3 f 1.4 92.6 f 1.5  100.0 f 0.0 97.1 f 0.2  72.6 f 13.3 59.4 f 13.7 74.6 f 2.2  rules Fired 112.3  6.6 3.2  32.2 69.8 36.0 35.0 66.2  340.0 131.7 12.1  334.0 1869.2 214.9  VOTE 95.1 f 2.6 71.2 f 4.5 61.9 f 4.3 66.6 f 1.8 88.4 f 5.0 62.3 f 5.6  74.4 f 10.0 80.9 f 2.4 84.8 f 3.3 99.4 f 0.1 93.6 f 0.6  76.8 f 14.4 63.5 & 12.4  70.6 f 2.8  Table 1. Mean and standard deviation of AUCs using RL rules with various resolution strategies. The final two columns give the average number of rules generated and the average number of rules fired on each instance.

RL [3] is a MetaDENDRAL-style rule learner that per- forms a general-to-specific search of the space of conjunc- tive rules. This type of rule-space search is described in de- tail by Webb [ 181. RL uses a beam search for rules whose coverage and confidence are above user-defined thresholds.

In the experiments reported here, a beamsize of 100 was used along with rule constraints of confidence greater than 0.60, coverage greater than two instances, and no more than four conjuncts per rule. A Laplace corrected version of the confidence equation was used, to compensate for small sam- ple sizes.

With its default settings, RL only finds rules that cover examples not previously covered by other rules. In these experiments, RL was allowed to generate redundant rules in order to experiment with the effects of rule overlap. It is important to note that, unlike C4.5rules, RL does not try to produce a rule set that maximizes classification accuracy.

!

4.2. Datasets  Fourteen data sets were selected from the UCI Reposi- tory [ 11. In general, datasets were avoided that had extreme class skews since the purpose of this paper is not to exper- iment with learning under skewed distributions. Datasets were chosen that could produce reasonable performance with standard rule learners.

Each experiment reported below was performed using 10-fold cross-validation on the datasets. Means and stan- dard deviations of the experimental results are given. For readability, AUCs are reported as percentages of the total possible so they range from 0 to 100 instead of 0 to 1 .

4.3. The effect of resolution strategy  Tables 1 and 2 show the effect of different rule resolution strategies using rules from RL and C4.5rules, respectively.

Several observations can be made from these results. From the last two columns in each table we can see that RL gen- erated far more rules for each dataset than C4.Srules did, in some cases by one or two orders of magnitude. Also, the number of rules fired on average per instance is far greater for RL than for C4.5rules, indicating that rule contention is considerably higher for the rule sets created by RL. Nei- ther result is surprising. The RL parameters were chosen so that it would generate a large number of overlapping rules, resulting in high contention. On the other hand, C4.5rules begins with a set of mutually exclusive rules; although rules can overlap after conjunct deletion, they will not otherwise conflict.

The results of this contention are seen in the effect of the resolution strategy. There is little difference between reso- lution strategies with the C4.5rules results because there is little contention to resolve. The RL results in Table 1 show much greater variability.

The differences between resolution strategies are eval- uated more carefully in Table 3, in which they are com- pared in pairs using the mean AUC for each dataset shown in Table 1. Results are given for both the Sign Test (which ignores difference magnitudes) and the Wilcoxon Matched-Pair Signed-Ranks Test (which takes magnitude into account). Each test result is the probability that the first strategy is indistinguishable from the second in per- formance. RAND and FIRST both perform poorly, and are indeed virtually indistinguishable in performance. Uni-     Dataset Breast-wisc Car Cmc Covtype Crx German Glass Image Kr-vs-kp Mushroom Nursery Promoters Sonar Splice  WVOTE 97.4 f 3.6 98.3 f 0.7 66.5 f 4.9 82.2 f 1.4 90.1 f 3.4 67.9 f 9.6 75.7 f 5.9 99.0 f 0.5 99.7 f 0.2  100.0 f 0.0 99.8 f 0.1  88.4 f 12.8 77.8 f 13.7 97.2 f 0.7  RAND 96.6 f 3.2 97.6 f 0.7 66.1 f 5.4 81.5 f 1.2 89.1 f 3.7 68.2 f 9.0 76.9 f 5.0 99.0 f 0.5 99.7 f 0.2  100.0 f 0.0 99.7 f 0.1  89.2 f 10.0 77.7 f 11.6  97.1 f 0.7  Generated 8.2  78.6 39.1 63.5 12.9 23.4 12.2 28.6 26.3 11.5  336.8 8.0 9.1  76.2  Resolution strate  Pair  RAND vs FIRST  RAND vs VOTE RAND vs LFPR  RAND vs WVOTE FIRST vs LFPR FIRST vs VOTE  FIRST 97.6 f 3.0 98.4 f 0.7 66.8 f 5.0 83.1 f 1.5 90.9 f 2.6 68.1 f 9.7 77.1 f 5.2 99.1 f 0.5 99.9 f 0.1  100.0 f 0.0 99.8 f 0.1  89.4 f 10.6 78.2 f 12.3 97.5 f 0.6  Wins-Ties-Losses  8-0-6  4-0- I O 0-0- 14  0-0- 14 1-0-13 4-0- I O  LFPR 97.5 f 2.9 98.4 f 0.7 66.6 f 5.2 82.6 f 1.4 89.2 f 4.2  67.6 f 10.1 77.1 f 5.2 99.1 & 0.5 99.9 f 0.1  100.0 f 0.0 99.8 f 0.0  89.9 f 11.2 77.7 f 10.9  97.4 f 0.6  11 Number of rules es VOTE  95.8 f 2.9 96.3 f 0.8 67.0 f 2.6 80.3 f 1.2 84.7 f 3.3 67.7 f 8.4 75.8 f 8.7 98.0 f 0.5 99.5 f 0.4 99.8 f 0.0 99.6 f 0.1 88.9 f 9.9  76.4 f 11.2 97.2 f 0.8  Fired 2.2 1.3 1.2 1.6 1.9 1.1 1 .o 1.5 1.9 1.2 1.6 1.2 1.3 2.3  Table 2. Mean and standard deviation of AUCs using C4.5rules with various resolution strategies.

The final two columns give the average number of rules generated and the average number of rules fired on each instance.

form unweighted voting (VOTE) performs better. The two measures that take rule statistics into account, LFPR and WVOTE, perform best of all. WVOTE appears to have an advantage over LFPR in these domains but the difference is statistically inconclusive.

FIRST vs WVOTE ( 1  0-0- 14 LFPR vs VOTE 11 10-0-4 LFPR vs WVOTE 11 5-0-9 VOTEvsWVOTE 11 2-0- 12  Sign test 0.79 1 0.000 0.180 0.000 0.002 0.180 0.000 0.180 0.424 0.013  WMPSR test 0.194 0.000 0.01 1 0.000 0.000 0.013 0.000 0.194 0.153 0.003  Table 3. Pairwise comparisons of the results of resolution strategies from Table 1 *  4.4. Benefits of instance scoring  Another question is whether instance scoring is useful.

Perhaps the classification done by C4.5rules is already suf- ficient to provide good performance over the entire ROC space. To evaluate this hypothesis, the AUC performance of the C4.5rules was measured as if the rules were evalu- ated directly for classification; that is, as if they were inter- preted by the consultr program that comes with C4.5.

This evaluation produces a single ?accuracy point? in ROC space. This constitutes an ROC curve whose area can be measured. If probabilistic interpretation of rules has bene- fits for predictiveness, we might expect a situation as shown in Figure IC, where classifier B (probabilistic) has a larger area than classifier A (discrete).

Figure 2 shows ROC curves from a C4.5rules rule set on the Covtype domain. Discrete classification yields an FP  rate of .13 and a T P  rate of .70. Connecting this point to ( 0 , O )  and ( 1 , l )  yields an ROC ?curve? with flat sides.

If the same rules are used for instance scoring, a curve of greater area can be produced, as shown in the figure. At the accuracy point the two strategies are close in performance, but elsewhere in ROC space the instance scoring strategy exhibits a substantial predictive advantage. This can be seen in the ?bowing out? of the scoring curve, which has greater area than the discrete classification curve. This means that as conditions change away from the accuracy point, e.g., the class distribution becomes skewed or one type of error becomes more costly, instance scoring will have a definite advantage.

Table 4 compares these AUCs across the UCI domains.

?AUC using consultr? is the AUC from discrete classifi- cation and ?AUC using WVOTE? is the AUC using in- stance scoring with WVOTE. On nearly every dataset the WVOTE AUC exceeds the corresponding AUC that would result from direct classification. The results pass both the Sign and Wilcoxon tests at p < .05. This demonstrates that using rules to score instances results in a measurable predic- tive benefit over what would be realized from interpreting them as direct classification rules.

0.8  0 * $ 0.6 P   .- .- U m  n 2 0.4 b  0.2  Dataset Breast-wisc Car Cmc Covtype Crx German Glass Image Kr-vs-kp Mushroom Nursery Promoters Sonar Splice i Instance scoring (WVOTE) ------- Discrete classification +  AUC using consultr 95.6 f 3.4 95.3 f 2.2 65.1 f 2.7 76.0 f 2.7 85.6 f 4.8 64.4 f 3.4 74.4 f 7.5 97.7 f 0.7 99.6 f 0.4 100.0 f 0.0 98.8 f 0.3 90.1 f 11.0 73.9 f 6.8 94.8 f 1.3  I I I I 1 0 0.2 0.4 0.6 0.8 1  False positive rate  Figure 2. ROC curves of the Covtype domain  4.5. Comparison with Naive Bayes  Finally, how well do rules perform against other proba- bility estimation techniques? Rules have various desirable qualities such as modularity and intelligibility, but these qualities are shared by other model classes as well, such as Naive Bayes and linear threshold units. How do rules compare?

Table 5 shows ROC performance of Naive Bayes? and rules from C4.5rules using WVOTE resolution. In general, rules appear to perform better than simple Naive Bayes.

This observation passes a Sign test (p < 0.05), though it does not pass the Wilcoxon test at an acceptable level.

It is possible that more complex model classes such as decision tree ensembles [8, 211 or neural networks [14] would produce better probability estimates than rules do.

However, these model classes are more complex and ex- pensive, and lack some of the appealing characteristics of classification rules.

5. Discussion and Future Work  This paper has demonstrated that rules, commonly used for direct classification, can also be used effectively for probability estimation. In fact, when used this way their predictive performance increases. In general, they are com- petitive with a simple probability estimation method such as Naive Bayes. It is likely that further experimentation with the rule generation methods would result in better absolute performance from rules.

?Because Naive Bayes is representationally equivalent to a linear threshold unit, no separate test was done.

AUC using WVOTE 97.3 f 3.6 98.3 f 0.7 66.4 f 4.9 82.2 f 1.4 90.2 f 3.4 68.4 f 9.9 75.5 f 6.0 99.0 f 0.5 99.7 f 0.2 100.0 f 0.0 99.8 f .1 88.9 f 13 76.9 f 14 97.2 f 0.7  Table 4. Comparisons of AUCs under direct classification (?AUC using consultr?) and un- der instance scoring (?AUC using WVOTE?).

Several important issues have been left unaddressed in this paper.

Even when rule generation is efficient and effective, clas- sification performance can benefit from rule selection, and this is an area of ongoing work. Wilkins and Ma [ 191 proved that optimal rule selection is NP-hard because overlapping rules can have ?sociopathic? interactions; therefore, practi- cal rule selection techniques must be heuristic. The field of machine learning would benefit from a systematic study of heuristic rule selection methods.

The experiments in this paper employed two standard rule learning methods, C4.5rules and RL, but neither was designed to maximize AUC performance. An open ques- tion is how rule generation should be altered to produce rule with good AUC performance. Various ideas have been proposed, such as starting with a high confidence bias and dynamically adjusting it based on feedback from AUC per- formance. To our knowledge, no such research has been seriously pursued.

Finally, a related issue is how best to learn rules when one or more classes is rare. Induction under skewed distri- butions is an important open issue in machine learning that has received attention recently. The datasets used in this study were chosen to be fairly balanced in order to sidestep this issue so that off-the-shelf rule induction methods could be used. Research on other model classes with skewed data sets should provide valuable insights on rule induction as well.

Dataset Breast-wisc Car Cmc Covtype Crx German Glass Image Kr-vs-kp Mushroom Nursery Promoters Sonar Splice  AUC (Naive Bayes) 93.1 f 5.5 92.3 f 2.2 64.1 f 5.8 81.5 f 2.4 87.6 f 4.3 77.1 f 4.5 74.0 f 8.7 95.6 f 0.9 95.1 f 0.8 99.8 f 0.1 98.0 f 0.2 97.7 f 4.0  76.1 f 13.0 99.2 f 0.6  AUC (C4.5rules) 97.3 f 3.6 98.3 f 0.7 66.4 f 4.9 82.2 f 1.4 90.2 f 3.4 68.4 f 9.9 75.5 3z 6.0 99.0 f 0.5 99.7 f 0.2  100.0 f 0.0 99.8 f . 1 88.9 f 13  76.9 f 14.3 97.2 f .7  Table 5. Probability estimation: Naive Bayes versus rules from C4.5rules using WVOTE  6. Acknowledgments  Discussions with Foster Provost were helpful in clari- fying some rule learning issues. Two anonymous review- ers provided valuable comments. I thank Ross Quinlan for making C4.5 and C4.5rules available for non-commercial use; I thank Foster Provost for making the RL program available; and I thank the Weka project [20] for making their software available.

Much open source software was used in this work. I wish to thank the authors and maintainers of XEmacs, TEX, ET*, Per1 and its many user-contributed packages, and the Free Software Foundation's GNU Project. Scripts for the Sign and Wilcoxon tests were written by Rob van Son.

