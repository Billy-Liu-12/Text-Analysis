Mining Relevant Text from Unlabelled Documents

Abstract  Automatic classification of documents is an important area of research with many applications in the fields of doc- ument searching, forensics and others. Methods to perform classification of text rely on the existence of a sample of doc- uments whose class labels are known. However, in many situations, obtaining this sample may not be an easy (or even possible) task. In this paper we focus on the classifica- tion of unlabelled documents into two classes: relevant and irrelevant, given a topic of interest. By dividing the set of documents into buckets (for instance, answers returned by different search engines), and using association rule mining to find common sets of words among the buckets, we can ef- ficiently obtain a sample of documents that has a large per- centage of relevant ones. This sample can be used to train models to classify the entire set of documents. We prove, via experimentation, that our method is capable of filtering rel- evant documents even in adverse conditions where the per- centage of irrelevant documents in the buckets is relatively high.

1. Introduction  In information retrieval, such as content based image re- trieval or web page classification, we face an asymmetry between positive and negative examples [10, 2]. Suppose, for example, we submit a query to multiple search engines.

Each engine retrieves a collection of documents in response to our query. Such collections include, in general, both rel- evant and irrelevant documents. Suppose we want to dis- criminate the relevant documents from the irrelevant ones.

The set of all relevant documents in all retrieved collections represent a sample of the positive class, drawn from an un- derlying unknown distribution. On the other hand, the ir- relevant documents may come from an unknown number of different ?negative? classes. In general, we cannot approx- imate the distributions of the negative classes, as we may  have too few representatives for each of them. Hence, we are facing a problem with an unknown number of classes, with the user interested in only one of them.

Modelling the above scenario as a two-class problem, may impose misleading requirements, that can yield poor results. We are definitely better off focusing on the class of interest, as positive examples in this scenario have a more compact support, that reflects the correlations among their feature values.

Moreover, more often than not, the class labels of the data are unknown, either because the data is too large for an expert to label it, or because no such expert exists. In this work we eliminate the assumption of having even partially labelled data. We focus on document retrieval, and develop a technique to mining relevant text from unlabelled docu- ments. Specifically, our objective is to identify a sample of positive documents, representative of the underlying class distribution. The scenario of a query submitted to multiple search engines will serve as running example throughout the paper, although the technique can be applied to a variety of scenarios and data. Our approach reflects the asymmetry between positive and negative data, and does not make any particular and unnecessary assumption on the negative ex- amples.

2 Related Work  In [4] the authors discuss a hierarchical document clus- tering approach using frequent set of words. Their objective is to construct a hierarchy of documents for browsing at in- creasing levels of specificity of topics.

In [1] the authors consider the problem of enhancing the performance of a learning algorithm allowing a set of unla- belled data augment a small set of labelled examples. The driving application is the classification of Web pages. Al- though similar to our scenario, the technique depends on the existence of labelled data to begin with.

The authors in [6] exploit semantic similarity between terms and documents in an unsupervised fashion. Docu-     ments that share terms that are different, but semantically related, will be considered as unrelated when text docu- ments are represented as a bag of words. The purpose of the work in [6] is to overcome this limitation by learning a se- mantic proximity matrix from a given corpus of documents by taking into consideration high order correlations. Two methods (both yielding to the definition of a kernel func- tion) are discussed. In particular, in one model, documents with highly correlated words are considered as having sim- ilar content. Similarly, words contained in correlated docu- ments are viewed as semantically related.

3 The DocMine Algorithm  Given a document, it is possible to associate with it a bag of words [5, 3, 7]. Specifically, we represent a document as a binary vector ? ? ??, in which each entry records if a particular word stem occurs in the text. The dimensionality ? of d is determined by the number of different terms in the corpus of documents (size of the dictionary), and each entry is indexed by a specific term.

Going back to our example, suppose we submit a query to ? different search engines. We obtain ? collections, or buckets, of documents ?? ? ????? ? ? ? ?? ? ? ? ? ?.

While many documents retrieved by a specific search en- gine (a bad one) might be irrelevant, the relevant ones are expected to be more frequent in the majority of buckets. In addition, since we can assume that positive documents are drawn from a single underlying distribution, a compact sup- port unifies them across all buckets. On the other hand, the negatives manifest a large variation. We make use of these characteristics to develop a technique that discriminates rel- evant documents from the irrelevant ones. In details, we proceed as follows.

We mine each bucket ?? to find the frequent itemsets that satisfy a given support level. Each resulting itemset is a set of words. The result of this process is a collection of sets of itemsets, one set for each bucket: ?? ? ?????? is a frequent itemset in bucket ? ? for ? ? ?? ? ? ? ? ?, where it is possible that ?? ? ?, for some ?. Now we compute all itemsets that are frequent in ? buckets: 	? ? ?????? ? ??? ? ??? ? ? ? ? ? ????, for distinct ??? ? ? ? ? ??. In general ? ? ?? ?? ? ?. In our experiments we set ? ? ? since we consider a limited number of buckets (? ? ?), driven by the number of available documents per topic. We wish now to retrieve the documents that support the itemsets that are frequent in ? buckets. Then, for each ?? ? 	?, we select, in each of the ? buckets that contain ?? as frequent itemset, the documents that has ?? expressed within. The resulting collection of documents ? represent the presumed positive documents, relevant to our query.

The algorithm, which we call DocMine (Document Mining), is summarized in the following. The algorithm  takes as input the ? buckets of documents, and the minimum support (?????) for the computation of frequent itemsets.

1. Input: ? buckets of documents ?? ? ????? , ? ? ?? ? ? ? ? ?, ????????.

2. Compute frequent itemsets in each bucket ?? :  ? ? ? ?? ? ?? ? ??????? ??????? ?? ?????? ??,  ? ? ? ? ? ? ? ?  3. Compute all itemsets that are frequent in ? buckets:  ?? ? ? ?? ? ? 	?? ? 	?? ? ? ? ? ? 	???  4. Set ? ? ?.

5. for each ? ? ??  ? for each ? ? ?? ? ? ? ? ? such that ? ? ?????	??  ? for each ? ? ??? ? if d contains ?  ? ? ? ? 	 ???  6. Output: the set ? (presumed positive documents)  It is important to remark that the DocMine algorithm can be tuned to ignore itemsets of small size. Some words, in fact, may be common to documents of different topics (they would not discriminate). Our experience tells us that, for instance, combinations of two frequent words are not suffi- cient to discriminate among different topics.

4 Experimental Results  To test the feasibility of our approach we use the Reuters- 21578 text categorization collection [8], omitting empty documents and those without labels. Common and rare words are removed, and the vocabulary is stemmed with the Porter Stemmer [9]. After stemming, the vocabulary size is 12113.

In our experiments, we consider five buckets of docu- ments (? ? ?), and vary the percentage ? of relevant docu- ments (i.e., concerning the topic of interest) in each bucket from ??? to ???. As topics of interest, we select the top- ics with the largest number of documents available in the data set. Once we have identified a topic, the non relevant documents are randomly selected from the remaining top- ics. We observe that some documents in the Reuters data have multiple topics associated (e.g., grain and crops). In our experiments, a document is considered positive if it has the topic of interest among its associated topics. For each topic examined, we test three different values of the mini- mum support (???, ??, ??).

We have also investigated different threshold values (from 2 to 5) for the cardinality of the frequent itemsets (????). Only frequent itemsets of size above (or equal to)     the threshold are considered for the retrieval of relevant doc- uments. The rationale beyond this test is that if an item is too common across different documents, then it would have little discriminating power. The setting of a proper thresh- old for ???? allows to discard frequently used words (not removed during preprocessing) that are not discriminating.

Our experiments show that threshold values of 4 or 5 (de- pending on the value of the minimum support) give good results.

In the following tables we report, for each value of ?, the number of (retrieved) documents in ? (?? ?), the number of positive (relevant) documents in ? (????), the percent- age of positive documents in ? (?????) ?precision ?, and the percentage of positive documents retrieved by ? (?) ? recall?. Each caption has (in parenthesis) the total number of positive documents versus the total number of documents in the five buckets.

We have considered different topics in our experiments.

For lack of space, we report only the results for the topic earn (3776 documents). Similar results were obtained for the other topics. We distribute all the available positives among the buckets, and adjust the number of negatives ac- cordingly to the ? value considered.

Tables 1-4 show the results. Figures 1-3 plot the preci- sion values for the topic earn, for increasing threshold ? on the itemset size ????. Each line corresponds to a value of ? (percentage of positive documents in each bucket). The plots show that, in each case, the setting of ? ? ? allows the achievement of a precision value very close to 1. For larger support values (5% and 10%), ? ? ? suffices for the selection of an almost ?pure? sample of documents. Even in the adverse condition of 50% of irrelevent documents in the buckets, the DocMine algorithm is able to achieve a very high precision.

These results are very promising for the purpose of con- structing a classifier that uses the selected collection of doc- uments ? as a positive sample.

5 Conclusions  We have introduced a new algorithm, based on associa- tion rule mining, to select a representative sample of pos- itive examples from a given set of unlabelled documents.

Our experiments show that our method is capable of select- ing sets of documents with precision above 90% in most cases, when frequent itemsets of cardinality 4 or 5 are con- sidered. We emphasize that, in all cases, the precision tends to reach high levels, as the cardinality of the common item- sets grows, regardless of the value of the support, or the percentage of relevant documents in the original buckets.

Table 1. Topic earn: ? ? ??? (?????????).

?????? ???? ?? ? ?? ?? ????? r  ? ? 5323 2824 0.53 0.74 10% ? ? 2538 2204 0.87 0.58  ? ? 1848 1848 1.00 0.49 ? ? 1103 1103 1.00 0.29 ? ? 6441 3012 0.47 0.80  5% ? ? 4653 2597 0.56 0.69 ? ? 1972 1913 0.97 0.51 ? ? 1284 1284 1.00 0.34 ? ? 7246 3597 0.50 0.95  3% ? ? 5789 2943 0.51 0.78 ? ? 3671 2408 0.66 0.64 ? ? 1642 1628 0.99 0.43  Table 2. Topic earn: ? ? ??? (???????	?).

?????? ???? ?? ? ?? ?? ????? r  ? ? 4453 2932 0.66 0.78 10% ? ? 2725 2250 0.83 0.60  ? ? 1842 1841 0.99 0.49 ? ? 1403 1403 1.00 0.37 ? ? 5684 3507 0.62 0.93  5% ? ? 3985 2668 0.67 0.71 ? ? 2045 1999 0.98 0.53 ? ? 1381 1376 0.99 0.36 ? ? 5859 3561 0.61 0.94  3% ? ? 4636 2928 0.63 0.78 ? ? 3311 2490 0.75 0.66 ? ? 1879 1875 0.99 0.50  Table 3. Topic earn. ? ? ??? (???????	?).

