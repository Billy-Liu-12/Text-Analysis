IEEE-20180

Abstract-Modern digital databases are immersed with  massive collection of data. The proliferation, ubiquity and  increasing power of computer technology have increased  data collection and storage. As datasets have grown in size  and complexity, direct hands-on data analysis has  increasingly been augmented in-direct , automatic data  processing .There are so many existing algorithms to find  frequent itemsets in Association Rule Mining .In this  paper, we have modified FPtree algorithm as HCFPMine  frequency of tree (Horizontal Compact Frequent Pattern  Mining) combines all the maximum occurrence of  frequent itemsets before converting into the tree structure.

We have also used median value for finding same  frequency of items and inserted it as node in the tree  structure. We have explained it with an algorithm and  illustrated with examples and also depicted the runtime  and memory space for the construction of the tree  structure.



I. INTRODUCTION  An active research area in data mining is the efficient  discovery of frequent patterns from the large databases.

Current databases have a challenge to store large amount of  data, but there is a limitation for storing capacity. So there is a  need for implication of drilling unwanted data and to store the  necessary data. The essence of this process is the data mining.

Frequent pattern mining is a core research topic in data  mining research for many past years. Many research papers  deal with the efficient searching and storing the massive data  in a limited memory space. There has been a tremendous  growth in different algorithms which have focused on  efficient and scalable algorithms for frequent item set mining  in transaction databases. There are different platforms for  applications such as sequential pattern mining , structured  pattern mining,correlation mining , Association ,Classification  and clustering. There are different stages of Knowledge  Discovery in Databases such as i)Selection ii)Preprocessing  3)Transformation 4)Data Mining 5)Interpretation 6)Data  Visualization.

There are two data mining techniques such as a)Prediction  b )Description. Prediction makes use of the existing variables  in the database in order to predict unknown or future values  of interest. Description focuses on finding patterns describing  Dr.K.Alagarsamy, Associate Professor, Dept of MCA, Computer Center Madurai Kamaraj University, Madurai, India.

the data and the subsequent presentation for user  interpretation. Let I be a set of Items and D is a transaction  database where each transaction has a unique identifier(TID)  and contains a set of Items.The set of all tid's is denoted as T.

A set X CI is called an Itemset and a set Y C T is called a  tidset.An itemset with k Items is called a K-Itemset. We write  an itemset {A,C, W} as ACW and a tidset {2,4,5} as 245. The  support of an itemset X is denoted as supp(X=> Y) =  supp(XUY).support is the number of transactions that contain  the item set . P(X) = I / n I i (X C Di ) , 0< i < n where p is the Supp or Support and n is the number of transactions in  the database Di and confidence Conf(X=>Y) = Supp(XUY) / Supp(X). It is the ratio of the number of transactions that  include all the items. Association rule generation is usually  split up into two separate steps. They are as follows:  i)First minimum support is applied to all frequent itemset in a database.

ii)These frequent itemsets and constraint are used to form rules.

minimum confidence  The number of possible association rule mining (ARM) , given a number of products or items d, is too large , #rules  (d) = 3 d_2 d+l + 1. In an example such as  #Items # Rules  7 1930  25 847288609443  300 l.3691e+l43  From this, we can infer that it is necessary to filter these from  some methods; then only we can search the relevant  information from large databases.



II. Dataset Organization  Dataset organizations can be processed horizontally or  vertically. In a horizontal layout, the database is organized as a  set of rows, with each row representing a customer's transaction  in terms of the items that are purchased in the transaction  [5].There is an alternative approach to this data layout such as  vertical layout. It consists of each item associated with a  ICCCNT'12  26th _28th July 2012, Coimbatore, India    IEEE-20180  column of values representing the transaction in which it is  present. It has smaller effective database size, compact storage  of the database and better support of dynamic database.

A market-basket database is a two dimensional matrix where  the rows represent individual customer purchase transaction and  the columns represent the items on sale.

TABLE I  Transactions Database  TID LIST OF ITEMS  I (Milk , Sugar, Wheat)  2 (Milk, Icecream, Sugar, Flour)  3 (Milk, Sugar)  4 (Icecream, Milk, Wheat)  5 {Icecream, Wheat, Sugar}  TABLE :2  Horizontal Layout  TID LIST OF ITEMS  I M,S,W  2 M.I,S  3 M,S  4 I,M,W  5 I,W,S

III. Related Work  FIM algorithms could be broadly classified as candidate  generation algorithms or pattern growth algorithms, Within  these categories, further classification can be done based on  the traversal strategy and data structures used. Apart from  these several hybrid algorithms which combine desirable  features of different algorithms have been proposed. Apriori  Hybrid, VIPER ,Max Eclat, KDCI are some of them. Apriori  is a downward closure property among frequent k Itemsets  An k-itemset is frequent only if all of its sub-itemsets are frequent. Since the Apriori algorithm was proposed , there  have been extensive studies on the improvements or  extensions of Apriori such as hashing technique(Park et  al.1995) , partitioning technique (Savasere et al. 1995),  Sampling approach (Toivonen 1996) , dynamic itemset  counting (Brin et al. 1997), incremental mining (Cheung et al.

1996), parallel and distributed mining (Park et al 1995) ,  Pincer Search algorithm constructs the candidates in a bottom  up manner but searching was performed in a top down  approach at the same time. Max-Miner was based on  lookahead and superset frequency pruning.Depth project  finds long itemsets using a depth first search of a  lexicographic tree of itemsets ,FP-Growth algorithm works in  a divide and conquer approach and compressed representation  of all the transactions in the database, There are many  alternatives and extensions to the FP-Growth approach,  including depth first generation of frequent itemsets by  Aggarwal et. AI(2001), H-Mine by pei et. AI (2001) which  explores a hyper-structure mining of frequent patterns;  building alternative trees; exploring top-down and bottom-up  traversal of such trees in pattern-growth mining by Liu et  al(2002;2003) and array based implementation of prefix tree  structure for efficient pattern growth mining by Grahne and  Zhu(2003). Leung et al. proposed a UF-tree which is a variant  of the FP-tree. Each node in the UF-tree stores (i) an item,  (ii)its expected support, and (iii) the number of occurrence of  such expected support for such an item. SAM (Split an  dMerge) algorithm computes a conditional database,  processes this conditional database recursively and finally  eliminates the split item from the original (conditional)  database.



IV.Proposed Work - HCFPMINETREE  The existing algorithm of FPGrowth has two drawbacks.

First, mining of frequent itemsets from the FP-tree generates  huge conditional FP-tree and takes a lot of time and space.

Secondly, if we can change the minsupport count, this  algorithm may restart and scan database twice. We have  proposed a new approach such as HCFPMineTree to mine all  frequent itemsets; it improves the FP-tree algorithm. It  requires a very less memory space and it is quite easy to  traverse a tree structure. It employs more efficient searching,  compact memory structure than the FP tree.

4.1 Mining Of Frequent Itemsets By Using HCFP? Minetree  We have presented a new algorithm to mine frequent itemsets  without generation of candidate itemsets. The proposed  algorithm leads to efficient searching and compact frequent  pattern tree.



I. Scan the input transaction file for finding the frequency of each item.

ICCCNT'12  26th _28th July 2012, Coimbatore, India    IEEE-20180  2. Create a frequency list, in descending order of frequency,  for only the items to which frequency is higher than the  support count.

3. Create supported data list(dynamically combine items) in  which only the items for which frequency is higher than the  given support count.

4. For construction of tree structure ,we have to carry out the  following steps:  a. Create a root node as null node.

b. Find the frequency of each combination of items in  frequency list against the transactions in supporteddata  C. From this frequency list, choose the median(M) and  combine thefirst M items (combineditems) of items in  frequency list.

d. Select all the transactions in supporteddata (selectedtrans),  in which first M items are matching with combineditems.

e. Add the combineditems to the child of the current root  node.

f For the remaining items in selectedtrans, repeat the above procedure and add the node as child node of the current node  g. For the remaining transactions in supported data, repeat the  above steps b to f  We have worked with the following transaction database used  by Han[IO]  Table :3  Transactions Database  TID ITEMS BOUGHT  T1 f c a m p  T2 f c a b m  T3 f b  T4 c b P  T5 f c a mp  Arrange according to descending order and also check with  supportcount >= minsupport ; then we can enter the values  according to the maximum occurrence of transactions.

IC?  26th _28th July 201  Count the list of Transactions and it can be listed below.

Table :4  List of Transactions and its frequency  Tl  T2  T3  T4  T5  Fig.1 HCFPMINE Tree  4.2 HCFP(PREMine) Tree algorithm  Terms: InFreq - Infrequent (or) pre frequent itemsets FreqItem- frequent item sets       TD- Transaction Database and FreTD - Frequent Items Transaction Database, MOFI - Maximum Occurrence of Frequent Itemsets.

MOFIM - Median of MOFI  Steps (PREMINE Frequent Itemsets)  1. Present the transaction list in the horizontal transaction format.

2. Combine all the items and present in the HCFPMine.

3. Calculate the occurrence of each of all items until the end of the TD.

4. Check If Support Count (FreqItem) >= MinSupportCount.

Then store it to FreTD Else perform step 7.

5. Perform step3 until items (i) in the TD are equal to items (n).

6. Combine all FreqItems and store the output in FreTD.

IEEE-20180  7. Repeat step 4 for n times.

Algorithm HCFPMineTree (HCfpmine frequent ltemsets  with HIL)  Input: Horizontal Item list (HIL), Sort list as SL.

Output: The complete set of frequent itemset in compact tree format.

Method: Call PREMINE (LS, HIL, k) Begin For each FreqItem k:i in the FRETD Sort according to the maximum occurrence of the items as SL.

While the first item in SL <> last item Combines all the maximum occurrence of frequent items as MOFI in the HIL in the order of the items in the SL.

MOFI=FSI UFS2UFS3 Repeat the above steps until all the items are combined End //while End //for End//Begin Procedure HCFPMINETREE (PreMine) Begin Create root node as null For I to I to MOFI Find frequency of MOFI MOFIM = median (frequency ) Insert the MOFIM in the CFPMineTree Make a link with the root node Repeat for MOFI steps and insert it as next level.

Continue the above steps for the remaining maximum itemsets.

Maintain Itemset Association information as in Transaction database.

Continue until all the Items are inserted.

Store items to HCFPMINETREE.

End for End//  The following example is explained with the steps of the  HCFPMine tree .

Input dataset  21 54 27 38 11 15 70  21 54 27 38 15 70 80  21 54 27 3845 70 80  54 27 38 11 15 70 80  21 54 27 38 11 15 70  54 27 38 11 15 70 80  54 27 38 11 15 30 70  21 54 27 38 15 30 70  54 27 38 11 15 70 80  21 54 27 38 11 70 80  Min support count: 4  Freq of each item in input file after eliminating items  which are less than min support count  Table : 5  Items Support count  27 9  38 9  54 9  70 8  15 7  11 6  21 5  Step1: Find the frequency of combined items.

Freq Combined items  9 27  9 27 38  8 27 38 54  6 27 38 54 70  3 27 38 54 70 15  1 27 38 54 70 15 11  Step2: Find the median number of frequency items to  combine in a dynamically.

From the above table we need to find the optimal number  items to be combined. This is done by choosing the median of  the frequency and its corresponding item.

Median = ((n + I) %2) where n is the number of values in a set  of data.

In our case median freq is 3 and combined items are 27 38 54  Step3: Add the combined items as one of the child node of  the tree. Fig :2  ICCCNT'12  26th _28th July 2012, Coimbatore, India    IEEE-20180  Then remove those items from the transactions.

Step4: From those transactions which are combined in  the previous step, find the frequency of combined items.

27 38 54 70 15 I I  21 - Items in the order highest freq to lowest freq  Frequency Combined items  6 70  70 15  1 70 15 11  0 70 15 11 21  Fig : 3  Dynamically Combined Items in HCFPMine Tree  27 38 54  Same procedures will be followed for all the remained  items in the initially chosen list of transactions. After all  the items in those transactions are combined, Steps 1 to 4  will be followed for the remaining transactions.

v. EXPERIMENTAL RESULTS  VRLE is coded in C language and all experiments run on a pc of Intel ?Pentium ? Dual CPU T2410 @2.00 GHZ, with 2GB Main Memory, Windows Vista Operating system. We  have presented the following datasets which are publicly available at FIMI repository. [http://fimi.cs.helsinki.fi/data]; Workshop on Frequent Itemset Mining Implementations  DataSet No. Of Avg. #Transactions Size  Items Trans. (K.B.) Length  CHESS 76 37 3196 336  MUSHROOM 120 119 8124 581  CONNECT 130 43 67,557 9435  (FIMI'04).

Table : 6  Dataset Characteristics  For constructing the HCFPMine Tree , the following tables  shows the runtime for different dataset. Chess dataset is  derived from the steps of chess games. Mushroom dataset  describes the characteristics of various species of mushrooms.

Both are dense datasets. The following table depicts the  runtime and memory space for construction of HCFPMine  tree.

Table :6  Runtime for Constructing HCFPMine tree  Supp Chess Mushroom(S)  (%) (S)  1 4.794 34.34  2 3.652 32.75  5 1.89 10.832  Table :7  Memory Space for Constructing HCFPMine tree  Supp  (%) Chess  (KB) Mushroom  (KB)  Connect(S)  10.809  72.06  54.04  Connect  (KB)  ICCCNT'12  26th _28th July 2012, Coimbatore, India    IEEE-20180  [2] zfI? NGXIN CHEN, Data Mining and Uncertain I 1.536 2.188 0.780Reason ng Integrated Approach, Wiley-Interscience  Publica io nS, 2001.

2 1.464 2.924 1.004  [3]. B. Ki 5 1.644 7.12 1.012Prunin? i alpana , Dr. R. Nadarajan, Optimizing Search Space  n Frequent Itemset Mining with Hybrid Traversal  strategIes - A comparative performance on different data  Discussion  Dynamic Combination and Median : Before constructing  Hcfpmine tree , we have combined the items which have  same frequency through median value of frequent items .After  that we have inserted the combined items in the tree structure.

It saves lot of time and efficient way of traversal for frequent  pattern generation. Song[12] was already discussed that the  support of the itemset XUY is equal to the support of X, then  the support of the itemset XUYUZ is equal to the support of  the itemset XUZ. If the support of several itemsets are all  equal to the support of their common prefix itemset (subset)  that is frequent, then any combination of these itemsets will  be frequent. This will lead to an optimization for solution of  prefix path of FPGrowth algorithm. In our experiment we  have depicted the runtime performance and storage space for  different types of datasets.



VI. REFERENCES  [I] Jiawei Han et ai, Data Mining: concepts and Techniques, Morgan Kaugmann publishers, 2001.

organizations, lAENG, 2007.

[4]. Nittaya Kerdprasop and Kittisak kerdprasop, Mining  frequent patterns with functional programming and intI.

Journal of computer and Information science and  Engineering, 2007.

[5]. Pradeep Shenoy et ai, Turbo-Charging Vertical Mining Of large databases , IISC , Technical report , DSL, 2000.

[6]. Erwin, A. Gopalan, R.P., and Achuthan, N.R.,"CTU? Mine: An Efficient High Utility Itemset Mining Algorithm Using the Pattern Growth Approach",IEEE 7th International conferences on computer and Information Technology, pp 71-76, 2007.

[7]. R.Agrawal, T. Imielinski and A. Swami ,"Mining  association rules between sets of Items in large databases", in   on Management of data, pp. 207-216 , 1993.

[8]. Jiawei Han, Jian pei, Yiwen Yin, Runying Mao, "Mining  frequent patterns without candidate Generation: A frequent?  pattern tree Approach" Data Mining and Knowledge  Discovery, volume 8, Issue I, pp 53-57 , January 2004 ..

[9]. Mohammed J. Zaki and Karam Gouda, "Fast Vertical  Mining using Diffsets", In proceedings of the ninth ACM   and Data Mining, Washington, D.C, 326-335, 2003.

[10]. J. Han, J. Pei, Y. Yin and R. Mao, " Mining Frequent  Pattern Without Candidate Generation: A Frequent Pattern  Tree", Springer, volume 8, 53-87,2004.

[11]. M.J Zaki, C.l Hsiao, "CHARM. An Efficient Algorithm For Closed Association Rule Mining," Technical  report 99-10, computer science department Resselaer  polytechnic Institute, October 1993.

[12]. M. Song and Rajasekaran, "A Transaction Mapping  Algorithm for Frequent Itemsets Mining", IEEE Trans. on  knowledge and Data Engineering, vol 18, No. 4, 2006.

[13]. R. Agrawal, T. Imielinski and A.N. Swami,. "Mining  Association Rules between Sets of Items in Large Databases,"  ICCCNT'12  26th _28th July 2012, Coimbatore, India    IEEE-20180  proceedings ACM SIGMOD IntI. Con? Management of  Data, pp 207-216 May 1993.

[14]. R. Agrawal and R. Srikant, "Fast Algorithms for Mining  Association Rules", IntI. Conf. on very large Databases, pp  487-499,1994.

[15]. B. Goethals, "Survey on Frequent Pattern Mining"  manuscript, 2003[16] S. Sarawagi, S. Thomas and R.

Agrawal, "Integrating Association Rule Mining with  Databases: Alternatives and Implications", ACM SIGMOD  IntI. Con? Management of Data, June 1998.

