

<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">25-29 July, 2004 - Budapest, Hungary MembershipMap: A Data Transformation Approach for Knowledge Discovery in Databases Hichem Frigui Department of Electrical and Computer Engineering University of Memphis Memphis, TN 38152 E-mail: hfrigui@memphis.edu Abstrad- We propose a new data transformation approach that facilitates many data mining, interpretation, and analysis tasks. Our approach, called MembershipMap, strives to extract the underlying structure or subsoncepts of each raw attribute automatically, and uses the orthogonal union of these sub- concepts to define a new, semantically richer, space. The sub- concept labels of each point in the original space determine the position of that point in the transformed space. Since sub-concept labels are prone to uncertainty inherent in the original data and in the initial extraction process, a combination of labeling schemes that are based on different measures of uncertainty will he presented. In particular, we introduce the CrispMap, the Fuz;yMap, and the PossibilisficMap. We outline the advantages and disadvantages of each mapping scheme, and we show that the three transformed spaces are complementary The proposed transformation is illustrated with several data sets, and we show that it can be used as a flexible pre-processing tool to support such tasks as: sampling, data cleaning, and outlier detection.



I. INTRODUCTION Knowledge Discovery in Databases (KDD) aims at dis- covering interesting and useful knowledge from large data repositories, such as patterns, trends, and associations. The KDD process consists of the following main phases[l]: . Data preparation and preprocessing: This phase gen- erally consists of cleaning of imperfect or noisy data, integration of data from multiple sources, selection of relevant data, and transformation of data.

Data Mining: This phase consists of the application of intelligent methods to extract knowledge from data.

Data Mining relies on several essential tools, such as clustering, classification, and association rules. . Pattern evaluation and presentation: This phase in- volves the identification and evaluation of interesting and useful knowledge from the mined information and its presentation to the user in a suitable format.

The preprocessing phase, is an essential but compared to the other phases, largely ignored subject. Traditionally, it has taken a backseat to the data mining algorithms. However, without adequate preparation of the data, the outcome of any data mining algorithm can he disappointing, hence the saying ?garbage in garbage out?.

In this paper, we propose a new data transformation ap- proach that facilitates many data mining, interpretation, and analysis tasks. Our approach, called MembershipMap, takes into account the intricate nature of each attribute?s distribution independently of the others and uses mixed cluster labels to create a new space. This separate treatment of the attributes allows the complete freedom to adapt different ways to com- pute the concept membership functions for different attributes depending on prior knowledge and attribute type such as categorical versus numerical. The MembershipMap has several advantages including: 1) The mapping can be exploited in any of the three complementary spaces: crisp, fuzzy, or possibilistic.

2) According to their location in the transformed space, data can be easily labeled as noise, boundary, or seeds.

3) The MembershipMap is suitable for data sets with features that vary widely in scale and type.

4) The MembershipMap can be updated incrementally as new records and attributes are added or removed.

11. RELATED WORK A. Data Preprocessing Data preprocessing can take many forms including normal-    Data preprocessing can take many forms including normal- izatiodscaling, transformatiadprojection, cleaning, and data reduction.

I )  Data Normalization: Involves scaling the attribute val- ues to make them lie numerically in the same intervaliscale, and thus have the same importance. This step is particu- larly useful for distance-based methods such as clustering and nearest neighbor classification. Some data normalization techniques can be found in [2]. In general, a single linear transformation of the data to a space where all dimensions are normalized is reliable only when the data has a uni-modal distribution. If this is not the case, then simple transformations such as z-score or min-max scaling can destroy the structure (shape, size, density) of the clusters, and can even create ?ghost? clusters (when scattered data is normalized within a small interval). Other conditions that can make normalization even more difficult and less satisfactory are the presence of noise, and the presence of categorical amibutes.

2) Data Transformation: Maps data to an equal or lower dimensional space. In general, the goals are to facilitate visualization, select a set of relevant amibutes, or map object data to an Euclidean space. Energy preserving techniques such as the Karhunen-Loeve (KL) transform map the n-dimensional 1147 0-7803-8353-2/04/$20.00 0 2004 IEEE FUZZ-IEEE 2004 belongingness and can handle uncertain cases. Fuzzy labels, (uf), satisfy the constraints: 0 5 (uf);j 5 1 and points to a lower k-dimensional space using linear projections via Principal Component Analysis (PCA). These techniques large databases with too many attributes.

are not applicable to categorical data and may be too slow for Another class of transformations attemots to oreserve the C x ( u f ) ; j  = 1; j = 1.. . N .  (2) 1-1 1-1 topology of the data by maintaining the pair-wise dissimi- larities (or their order) between objects. Multi-Dimensional Scaling (MDS)[3], Self-organizing maps [4], and Sammon?s mapping [5] fall into this category. The sharpest criticism Many fuzzy partitional clustering algorithms assign this type of labels in each iteration. For instance, the Fuzzy C-Means (FCM) [I31 assigns labels using: against MDS consists of their high computational and stor- age requirements ( O ( N ? ) ) .  Jagadish[6] suggested relying on domain experts to derive k feature extraction functions to be used to map each object into a point in k-dimensional space.

Faloutsos and Lin [7] generalized this approach by requiring experts to define only the objects? dissimilarity values.

3) Datu Cleuning: Includes smoothing out noise, identifi- cation of outliers, irrelevant features, and identification and mending of missing data. Outlier detection is important both as a pre-requisite to later discovery tasks that cannot handle outliers gracefully, or even on its own, if outliers represent the knowledge nuggets to be discovered.

4) Dufa Reduction: Includes reducing the number of at- tributes (feature reduction), the number of records (data sam- pling), or the number of possible values taken by an attribute (numerosity reduction). Feature Reduction has been addressed through feature selection techniques [XI, [9].

Data Sampling may he necessary to reduce the number of items to be mined when the data size is very large. Sampling techniques are discussed in the context of KDD in [lo]. A typical side effect of sampling when the data is distributed into clusters, is losing small clusters.

Attribute Numerosity Reduction is a technique that reduces the number of values taken by an attribute. It can be achieved by several techniques such as numerical attribute discretization by histogram binning.

B. Dura Partitioning und Labeling Clustering aims at classifying a set of N unlabeled data    Clustering aims at classifying a set of N unlabeled data points X = { x j l j  = 1,. .. , N} into C clusters GI,. . .,Gc, and assigning a label to each point to represent information about its belongingness. In general, there are three types of cluster labels depending on the uncertainty framework: crisp. fizzy, and possibilisfic. Each labeling paradigm uses a different uncertainty model, and thus, have a different semantic interpretation.

In crisp labeling, each data point xj is assigned a binary membership value,(u,)ij, in each cluster Gi according to: 1 if xj E Gi 0 otherwise The K-Means [ 111, K-Medoids [ 121, and other crisp partitional clustering algorithms assign this type of labels to each data sample in each iteration of the algorithm.

Fuzzy labeling allows for partial or gradual membership values. This type of labeling offers a richer representation of (3) where dij is the distance between cluster i and point x;, and mc(1,m) is a constant that controls the degree of fuzziness.

Possibilistic labeling relaxes the constraint that the mem- berships must sum to one. It assigns ?ppicality? values, (ap),  that do not consider the relative position of the point to all clusters. As a result, if xj is a noise point, then Cgl(up);; &lt;&lt; 1, and if xj is typical of more than one cluster, we can have CE1(up);j &gt; 1. Many robust partitional clustering algorithms [14], [15] use this type of labeling in each iteration. For instance, the Possibilistic C-Means [I51 assigns labels using (4) where vi is a cluster-dependent resolutiodscale parameter [ 151. Robust statistical estimators, such M-estimators and W- estimators [16] use this type of memberships to reduce the effect of noise and outliers.

111. MEMBERSHIPMAPS GENERATION In this paper, we propose a data transformation approach that facilitates a variety of data interpretation, analysis, and mining tasks. Our approach maps the original feature space into a membership unit hypercube with richer information content. The proposed mapping starts by clustering each attribute independently. Each cluster, thus obtained, can be considered to form a ?subspace? that reflects a ?more specific? concept along that dimension. The orthogonal union of all the subspaces obtained for each attribute compose the trans- formed space. The class labels (crisp, fuzzy, or possibilistic) determine the position of data points in the new space. The proposed preprocessing can be considered as a special type of ?semantic mapping? that maps the data from the ?.raw? feature space onto a new space characterized by a semantically richer dichotomization of the original features. Our approach is outlined in Fig.1.

The proposed transformation is different from other tech- niques such as z-score normalization, and min-max scaling.

This is because it takes into account the intricate nature of each attribute?s distribution independently of the others and aggregates the mixed cluster labels to create a new space. Thus, it is suitable for data sets with features that vary widely in scale and type. Moreover, our approach differs from techniques that impose a grid or a hyperbox structure on the data space. This is 25-29 July, 2004 * Budapest, Hungary Fig. I .  Illusmion of the MembenhipMap because the conceptual units in the membership space are not constrained to take any specific shape, size, or number. This information is automatically derived from the data distribution.

Note that the membership functions can be computed using many techniques. This task is generally trivial when performed on a univariate projection of the data (compared to the entire data set in the original space). Also. these memberships can be estimated automatically based on statistical distributions or histogram density. They can even be user-defined depending on the conceptual meaning attached to the particular attribute.

on the conceptual meaning attached to the particular attribute.

Most importantly, separate treatment of the attributes allows the complete freedom to adapt different ways to compute the membership functions for different attributes depending on prior knowledge and attribute type.

1v. PROPERTIES OF THE MEMBERSHIP MAPS A .  n e  Crisp Map Crisp labeling assigns membership values in {O,l}. Thus, the transformed vectors are binary-attribute vectors. The af- vantages of this simple representation include: 1) Comparison of two data samples: The similarity behveen two data samples can be easily assessed using their mapped vectors. This is because the number of common bits between their two corresponding binary vectors represents the number of shared concepts.

2 )  Querying the CrispMap: The transformed data can be queried easily to retrieve samples that satisfy celtain criteria. For instance, ?data samples that are similar from the point of view of Attribute kl and dissimilar from the point of view of Amibute k2? or ?data samples that are similar in at least Nk attributes?. These queries can be implemented efficiently using binary masking.

The limitations of the crisp Map are a direct consequence of their failure to model areas of overlap and uncertainty, as well as their inability to model outliers or noise.

1) Points on areas of overlap between clusters: These points will be mapped to the same image in the new space as points at the very core of either one of the overlapping clusters. Hence there is no way to distin- guish between these points and those at the core.

2) Outliers and border points: No matter how distant an original data sample is from its cluster, it will be mapped to the same point as any point at the core of the cluster.

This means that points on the outskirts of a cluster are indistinguishable from points at its center.

B. The Fuqv and Possibilistic Maps Fuzzy and possibilistic memberships are confined to ,O, 1,.

Hence, the transformed vectors are continuous-valued, and may embrace more space in the MembershipMap. For the case of FuzzyMap, the constraint that the memberships along each dimension must sum to one will constrain the placement of the transformed data. The Fuzzy and Possibilistic maps offer the following advantages: I )  Identifying interesting regions via simple queries: Without further processing, it is possible to use the transformed space to query and retrieve data that satisfy certain properties. For example, as we will show in the next section, it is possible to identify data samples that correspond to seeds, noise, and boundary points: 2) Mining the Transformed Space: Mining is relatively easy in this new space because all the feature vectors  are normalized and each feature value has a special meaning: strength of matching between the data and one of the I-D clustersiconcepts. Moreover, fuzzy set union, intersection, and other aggregation operators [ I  71 can be used to develop semantic similarity measures. This can be very helpful in case the original data has too many attributes, making the Euclidean distance unreliable.

v. EXPLORING THE MEMBERSHIPMAPS The MembershipMap can be mined just like the original data space, for clusters, classification models, association rules, etc. This task is expected to be easier in the membership space because all features are confined to the interval [O,l], FUZZ-IEEE 2004 and have special meaning within this interval. Moreover, since different areas of the transformed spaces correspond to differ- ent concepts, the MembershipMap can he explored in a pre- data mining phase to uncover useful underlying information.

In this paper, we focus our attention in exploring the fuzzy and possibilistic Membership spaces, which we will refer to as FuzzyMap and PossibilisticMap respectively. We outline how these maps could he used to uncover seed points; noise    how these maps could he used to uncover seed points; noise and outliers; and boundary points.

A. Identijfving Seed Points In applications involving huge amounts of data,"Seed points" identification may be needed to reduce the complexity of data-driven algorithms. This process can be extremely difficult, if not impossible, for data with uneven, noisy, hetero- geneous distributions. In addition to sampling, seed points can offer excellent initialization for techniques that are sensitive to the initial parameters such as clustering.

Using the PossibilisticMap, seed points can be identified as points with high typicality, where typicality is defined as BB w I*, m Dlln i B. Identijfving Noise Points and Outliers Noise and outlier detection is a challenging problem. In fact, when data has a large, unknown proportion of outliers, most learning algorithms break down and cannot yield any useful solution. Using the PossibilisticMap, Noise and outliers can be identified as those oints located of xj in the PossibilisticMap, then xj is a noise point if llxjpll is small.

C. Identihing Boundary Points The process of boundary points identification is paramount in many classification techniques. It can be used to reduce the training data to (or emphasize) a set of boundary points to achieve higher focus in learning class boundaries. Using the FuzzyMap, boundary points can be identified as points that do not have a strong commitment to any cluster of a given attribute. In other words, they are points that belong to multiple units and have low punty values, where the degree of purity of xj, Pxj, is defined as words, if xp'[{u~;)p11=1,...,CX; Pxj = min { m a  {uji)) }. (6) k=l,,.. ,n k1, ... .CI. - M N", ,st4 1" "km i D. Illustrative example Fig.2 displays a simple data set with 4 clusters. First, the projection of the data along each dimension is partitioned into 2 clusters using the FCM [13]. Next, possibilistic la- bels were assigned to each xj in all 4 clusters using (4), In Fig. 2(h), we have quantized Lxj into 4 levels: 1 and each point xj is mapped to x,'= uIj (1) ru2j (1) ,ulj  (2) ,uZj (2) , (a) (b) (4 Fig. 2. Exploring the MembershipMaps. (a) Data set. Points with (b) different typicality, (c) low punty.

Txj 20.95 (black squares); 0.80&lt;Txj &lt;0.95; 0.50&lt;Txj &lt;0.80; and 0.25&lt;Txj&lt;0.50 (squares with shade that gets lighter).

As can be seen, points located at the core of each cluster have the highest degree of typicality and could be treated as seeds. The degree of typicality decreases as we move away from the clusters. Points near the origin, i.e., IlxrII is small (&lt;0.25), are identified and displayed using the "+" symbol in Fig.Z(b). As can be seen, these are generally noise points. To identify boundary points, fuzzy labels were assigned to each xj in all 4 clusters using eq. (3). and each xj is mapped to xf= pit), U$),  U:?, us) , Fig 2(c) displays the set of points {xi I Pxj &lt; 0.75) as small squares. As can he seen, these are mainly the cluster boundaries.

E. Discussion 1 ' .

The richness of the discovered information comes at the cost of added dimensionality. In fact, the MembershipMap can significantly increase the dimensionality of the feature space, especially if the data has a large number of clusters.

However, the cost of increasing the dimensionality can be offset by the significant benefits that can be reaped by putting this knowledge to use to facilitate Data Mining tasks. For example, we have shown how Fuzzy and Possibilistic Maps can easily identify seed, border, and noise points. If taken into account, these samples can be used to obtain a significantly reduced data set by sampling, noise cleaning, and removing border points to improve classicluster separation. This cleaning    border points to improve classicluster separation. This cleaning can facilitate most subsequent supervised and unsupervised learning tasks. It is also possible, after the cleaning, to go back to the original feature space. Thus, benefiting both from lower dimensionality and lower data cardinality.

The cost of the high dimensional MembershipMap can also be offset by the following additional benefits: ( i )  All attribute values are mapped to the interval [O,l]; (ii) Categorical at- tributes can be mapped to numerical labels, thus, data with mixed attribute types can be mapped to numerical labels within [O,l]. As a results, a larger set of DM algorithms can be investigated and (iii) Distanceisimilarity values can be easily interpreted and thresholded in the new space.

Even though the attributes are treated independently, they do get combined at a later stage. Hence, information such as correlations is not lost in the membenhip space. In fact, treat- ing the attributes independently in the initial (1-D clustering) is analogous to traditional feature scaling methods such as 25-29 Ju/y, 2004 * Budapest, Hungary min-max or normalization, except that the MembershipMap approach does not destroy intricate properties of an attribute's distribution using a single global transformation. In a sense, our approach applies a specialized local transformation to each from the PossibilisticMap without my knowledge about the true class labels, is higher for those samples located near the m e  class centroid.

subcluster along an attribute, and is thus non-linear. :~ 1.0 . .. . .  ..-- - ._ 5 .. _. . .

.*Oe ,= 0.1

VI. EXPERIMENTAL RESULTS - -L - _ -  _.

. . . -.&gt;* . .

We illustrate the performance of the proposed transforma- tion using several benchmark data sets'. These are the Iris, Wisconsin Breast Cancer, Heart Disease, and the satellite data the 3 classes. The Wisconsin Breast Cancer data has 9 features O P  . . .  . sets. The Iris data has 4 features and 50 samples in each of 0 0  0 5  1 .o 1.5 29 D/slan- fO f r Y B  desa --an ~ ~~~ ~ ~ ~~~ (a) Typicality vs. distance to true class mean ?--- ~ --:- and 2 classes, namely, benign and malignant. These 2 classes have 444 and 239 samples respectively. The Heart Disease data has 13 features and 2 classes. The first class, absence of heart disease, has 150 samples. and the second class, presence of heart disease, has 120 samples. The satellite data has 35 partitioned into a training set (4,435) and a testing set (2,000).

information extracted from the Membership maps.

The first step in the MembershipMap transformation con- sists of a quick and rough segmentation of each feature. This is a relatively easy task that can rely on histogram thresholding *, ,, .- ~ _ -  _ -  ? g o a  -_ - features, 6 classes, and a total of 6,435 samples. This data is L = _  I_ _-~.. - -- , * The ground truth of these sets will he used to validate the 6.4 O B  0 8 - .

Memberohip I" erfYsl51800 (b) Purity vs. membership to true class Fig. 3. Validating identified regions of interest or clustering I-D data. The results reported in this paper were obtained using the Self-Organizing Oscillators Network (SOON) clustering algorithm [IS]. SOON has the advantage of finding the optimal number of clusters in an unsupervised way. We have also hied other simple clustering algorithms such as the K-Means, where the number of clusters i s  specified after inspecting the I-D projected data. The results were comparable. Next, fuzzy and possibilistic labels were assigned    comparable. Next, fuzzy and possibilistic labels were assigned to each sample using equations (3) and (4) with m=1.5, and q=1. Finally, the fuzzy (possibilistic) labels were aggregated to create the FuzzyMap (PossibilisticMap). The Iris maps have 15 dimensions, the Wisconsin Breast Cancer maps have 50 dimensions, the Heart Disease maps have 39 dimensions, and the Satellite maps have 107 dimensions.

To validate the degree of purity, we use the ground truth and compute the membership of each sample using eq.(3), wherc the distances are computed with respect to the classes' centroids. Fig. 3(b) displays a scatter plot of the sample purity versus their membership in the true class. The distribution shows a positive correlation indicating that the purity com- puted using the FuzzyMap without any knowledge about the true class labels can estimate the degree of sharing among the multiple classes, i.e, boundary points. Moreover, we notice that all samples from class I have high purity, and the few samples with low purity belong to either class 2 or 3. This information is consistent with the known distribution of the Iris data where class 1 is well-separated while classes 2 and 3 overlap.

A. Identifiiing Regions of Interest B. Clustering the Membership Maps To identify seed points, noise and outliers for the Iris data, This experiment illustrates the advantage of using the Mem- we compute the typicality of each sample. Points with high bership Maps to improve clustering. We apply the Fuzzy C- typicality would correspond to seed points, while points with Means to cluster the different data sets in the original space, low typicality would correspond to noise points. Since the FuzzyMap, and PossibilisticMap. For all cases, we fix the Iris data has 4 dimensions, the identified points could not be visualized as in Fig. 2. Instead, we rely on the ground truth to Of clustcrj to the actual number Of classes, use the same random points as initial centers. validate the results. We use the class labels and compute the  centroid of each Then, we compute the distance from To assess the validity of each partition we use the true class each point to the centroid of its class, Theoretically, points laheis to the purity Of the The are with small distances would correspond to seeds, and points shown in Table I .  For each confusion matrix, the column refers Fig, 3(a) displays to the cluster index while the row refers to the true class index. large distances to a plot of the sample typicality their distance As can be seen, both maps improve the clustering results for to the true class centroid. The distribution clearly suggests data we note here that a partition a negative Typicality, which was extracted only of the Iris data could be obtained using other clustering algorithms and/or mnltinle clusters to renresent each class. - 'Available at "hnp: / /~~ . ics .uc i .edui  mlearn/MLRepositoryhhnP However, our goal here is to illustrate that the Membership TABLE I CLUSTERINO RESULTS FUZZ-IEEE 2004 that is uncovered, and can even be avoided by going hack to the original feature space after data reduction and cleaning.

Data Set /I Originalspace 111 FutzyMap 1 1  PossMap Thus, benefiting both from lower dimensionality and lower ]I 50 0 0 11) 50 0 0 II 50 0 0 data cardinalitv.

The projection and clustering steps in the MembershipMap are not restricted to the original attributes. In fact, our approach can be combined with other feature reduction techniques.

For instance, Principal Component Analysis can he used to Breast 422 22 Cancer 19 220 3 236 3 236 117 33 ~ ~~ ~~~ ~~~ I Disease 11 23 97 111 21 99 )I 21 99 first project data on a subset of Eigenvectors, and then the membershir, transformation can he auulied on the Droiected TABLE II CLASSIFICATION RESULTS .. . _    .. . _ data in the same way. In fact, one can even project the data on selected 2-D or 3-D subsets of the attributes, and then apply membership transformations. I Original Space [ FuzyMap I PossMap Training I 89.29% 1 88.86% I 93.57% Testing I 85.50% I 86.30% I 87.30% ACKNOWLEDGMENTS This material?is based upon work supported by the National Science Foundation under Grant No. IIS-0133415 Maps can improve the distribution of the data so that simple clustering algorithms are reliable. REFERENCES C. Classification using the Membership Maps This experiment uses the 36-D satellite data to illustrate the use of the Membership Maps to improve data classification.

Only the training data is used to learn the mapping. The test data is mapped using the same cluster parameters obtained for the training data. We train a Back Propagation Neural Networks with 36 input units, 20 hidden units, and 6 output units on the original data and two other similar nets with 107 input units, 20 hidden units, and 6 output units on the Fuzzy and possibilistic maps. Table I1 compares the classification results in the 3 spaces. As can he seen, the results on the testing data are slightly better in both transformed spaces. The above results could be improved by increasing the number of hidden nodes to compensate for the increased dimensionality in the mapped spaces. Moreover, the classifications rates could be improved if one exploits additional information that could be easily extracted from the possibilistic and fuzzy maps.

For instance, by using boundary points for bootstrapping and filtering noise points.



VII. CONCLUSIONS  We have presented a new mapping that facilitates many data mining tasks. Our approach strives to extract the underlying sub-concepts of each attribute, and uses their orthogonal union to define a new space. The sub-concepts of each attribute can be easily identified using simple I-D clustering or histogram thresholding. Moreover, since fuzzy and possibilistic labeling can tolerate uncertainties and vagueness, there is no need for accurate sub-concept extraction. In addition to improving the performance of clustering and classification algorithms by taking advantage of the richer information content, the MembershipMaps could he used to formulate simple queries to extract special regions of interest, such as noise, outliers, boundary, and seed points.

There is a natural trade-off between dimensionality and information gain. Increased dimensionality of the Member- shipMap can be offset by the quality of hidden knowledge [ I ]  U. Fayyad, F. PiatetskqGhapiro, P. Smyth, and R. Uthurusamy. Ad- wnces in Knowledge Discowr,v and Data Mining, MIT Press. 1996.

[Z] D. Pyle. Doto Preporrnion for Data Mining, Morgan Kaufmann 1999.

131 R. N. Shepard. ?The analysis of proximilies: multidimensional scaling with an unknown distance function I and 11,? PsychomePiko. vol. 27.

pp. 125-139 219-246. 1962.

[4] T. Kohonen. Sel/lOqpnizotion and Associative Memoy, Springer Verlag, 1989.

[5] J. W. Sammon. ?A nonlinear mapping for data analysis:? IEEE Transactions on Computms. vol. 18. pp. 401-409. 1969.

[6] H. V. Jagadish. ?A retrieval technique for similar shape,? in ACM SICMOD. 1991, pp. 208-217.

(71 Chnstos Faloutsos and King-Ip Lin ?FastMap: A fast algorithm for indexing, dala-mining and visualization of traditional and multimedia datasets.? in SICMOD. 1995. 00. 163-174. , ..

[SI  H. Almkllim and T. G. Dienerich. ?Learning with many irrelevant feamres,? in Ninth National Conf AI, 1991, pp. 547-552.

[91 K. Kim and L. A.  Rendell. ?The feahrre selection problem: Traditional methds and a new algonthm.? in Tenth Notional Con/ AI, 1992. pp.

129-1 34.

[ I O ]  Jyrki Kivinen and Heikki Mannilq ?The power of sampling in knowledge discovety,? 1994. pp. 77-85.

[I11 R.O.Duda and P. E. Ha&amp; Patfen, Closslficnfion and Scene Analysis, John Wiley and Sons. 1973.

[I21 L. Kaufman and P. R O U S S ~ ~ U W ,  Finding Groups in Data: An lnmducrion    [I21 L. Kaufman and P. R O U S S ~ ~ U W ,  Finding Groups in Data: An lnmducrion to Cluster Ano!vsis, John Wiley and Sons. 1990.

[I31 J. C. Bezdek, Pattem Recognition with Fury Objective Function Algorithms. Plenum Press, New York, 1981.

1141 H.  Frigui and R. Krishapuram. ?A robust competitive clustering algorithm with applications in computer vision,? IEEE Pons. Pan Annlwis Mach. Intell., vol. 21, no. 5. pp. 45M65, 1999.

[I51 R. Knshnapuram and J. Keller, ?A possibilistic approach lo clustering,? IEEE Trans. Fuzzy Sysfems, vol. 1,  no. 2. pp. 98-110, May 1993.

[I61 F. R. Hampel, E. M. Roncheni, P. J. Rousseeuw. and W. A. Stahel, Robust Stnti.vtics the Appmoch Based on Influence Funoions, John Wiley &amp; Sans, New York 1986.

1171 2. Wang and G.  Klir. F u q  measure theo?, Plenum Press. New York 1992.

[I81 M.B. Rhouma and H. Frigui, ?Self-organization of a population of coupled oscillaton with application to clustering:? IEEE Trans. Pntl.

