Cost and time aware ant colony algorithm for data replica in Alpha Magnetic Spectrometer experiment

Abstract?Huge collections of data have been created in recent years. Cloud computing provides a way to enable massive amounts of data to work together as data-intensive services.

Considering Big Data and the cloud together, which is a practical and economical way to deal with Big Data, will accelerate the availability and acceptability of analysis of the data. Providing an efficient mechanism for optimized data-intensive services will be- come critical to meet the expected growth in demand. Because the competition is an extremely important factor in the marketplace, the cost model for data-intensive service provision is the key to provide a sustainable service market. As data play the dominant role in execution of data-intensive service composition, the cost and access response time of data sets influence the quality of the service that requires the data sets. In this paper, a data replica selection optimization algorithm based on an ant colony system is proposed. The performance of the data replica selection algorithm is evaluated by simulations. The background application of the work is the Alpha Magnetic Spectrometer experiment, which involves large amounts of data being transferred, organized and stored. It is critical and challenging to be cost and time aware to manage the data and services in this intensive research environment.

Keywords?service provision, data-intensive, ant colony opti- mization, cloud computing, Big Data, QoS

I. INTRODUCTION  In recent years, huge collections of data have been cre- ated by the advances in technology areas such as digital sensors, communications, computation, and storage. Scientists and computer engineers have coined a new term for the phenomenon: ?Big Data?. Big Data allows us to discover much more about the world, not only using structured internal data but also using unstructured data from external sources.

Cloud computing has become a viable, mainstream solution for data processing, storage, and distribution. It provides unlimited resources on demand. Considering Big Data and the cloud together, which is a practical and economical way to deal with Big Data, will accelerate the availability and acceptability of analysis of the data. To put Big Data to work, increasing numbers of companies are starting to use the cloud to publish Big Data as a data service. Many data services in the area of Big Data analytics have now become available.

Cloud infrastructure and platforms will play an important role in accessing, processing, and analyzing massive amounts of data, all of which are concerned with huge cost and energy consumption to maintain the data centers.

Like business sectors, where Big Data offerings are cross- organizational, Big Data projects are normally multidisci- plinary in the scientific area, and on a multinational scale. The background of this work is the Alpha Magnetic Spectrometer (AMS) experiment which uses cloud computing to process huge amounts of data. The AMS experiment is a large-scale international collaborative project, and the only large physics experiment on the International Space Station. The purpose of the AMS experiment is to study the universe and its origin by searching for antimatter and dark matter while performing precision measurements of cosmic rays composition and flux.

The AMS lab is based at CERN in Switzerland. The key technology for accessing the data remotely collected from AMS relies on data services based on the cloud computing.

This is actually supported by IBM Cloud Computing Center located at Southeast University in China. Typically, at our center, we receive 200G bytes data from AMS and generate 700G bytes data after processing them, on each single day.

To retrieve and access the data, we use dedicated 40Gbps InfiniBand network connectors between CERN and Southeast University, and our Cloud Computing Center has IBM servers with 3500 core and 500TB storage, which is shown in Fig. 1.

To solve a complex scientific problem such as in the AMS experiment, scientists need to combine data from various sources. It is necessary to design a workflow of various data- intensive services and get a composite data-intensive service when using Web service technologies. The cost and response time of each service in the data-intensive service composi- tion are critical for the composite service. The data-intensive service composition has the following challenges. First, large number of data sets and increasing functionally equivalent services make the composition complex. Second, the size and  Fig. 1: IBM Cloud Computing Center in Southeast University  2013 IEEE International Congress on Big Data  DOI 10.1109/BigData.Congress.2013.41     the number of distributed data sets make the communication and storage costs increase, which effect the performance of the whole application process. Third, the cost of transferring data to and from service endpoints increases as the number of data sets increase. Fourth, the dynamic nature of cloud computing and data replication need a dynamic and adaptive mechanism to regulate the interaction between users and providers.

In [19], it was pointed out that a service composer would need a cost-control mechanism to limit spending on the constituent services, that is to say, minimizing the cost of service provision. Similarly, service providers will also need a cost-control mechanism to limit spending on data sets, if they are to maintain a competitive position and win contracts with the service composer. Each service requests data sets from the storage resources (or data servers). Each of these data sets may be replicated at several locations (typically three) that are connected to each other and to the service endpoints through networks of varying capability [17], [27], [34]. Users can access data remotely from a data server or store a copy of data locally for future use. As data play the dominant role in execution of data-intensive service composition, the movement of mass data influences the performance of the whole process.

Especially, the access cost of each data replica on one data server is different from that on other data servers [30], and the cost of service relates to the amount of data transferred. Thus, it is necessary to present a data replica selection mechanism for service providers to lower the costs of services and improve the qualities of services.

In this paper, we address the challenges listed above for data-intensive service composition by making the following two contributions. First, we give a cost and time aware model for data replica. The model has presented the access response time and cost of each data replica. Second, we model the data replica selection problem as a multi-objective optimization problem using an ant colony system algorithm. The ants select data sets from multiple data replicas based on the cost and response time of transferring data from data servers to service endpoints, so that the total cost and response time of a service is minimized.

The remainder of this paper is organized as follows. The next Section reviews related work. Section III introduces the foundation problem and data cost model. Section IV details the data replica selection. Section V investigates how an ant colony system algorithm could be used to solve data replica selection problem. Section VI shows the experiments and analysis. Finally, Section VII concludes this paper.



II. RELATED WORK  Data replication is considered to be an important technique used in cloud computing to speed up data access, reduce bandwidth consumption, execution cost and users waiting time, and increase data availability [14]. Data is replicated across large geographic distances in the cloud [1]. Given the dynamic distribution of data and data requests from users, the ?best? replica site for users to access data is determined based on some criteria by the replica selection process. The user response time is the most relevant metric in the existing works on replica selection. As this metric cannot be computed in advance, some other factors such as network and server performance are adopted to estimate the response time.

The literature has provided two types of approaches for replica selection. The static replica selection approaches select the nearest replica to the user according to some static metric such as the geographical distance in miles, topological distance in number of hops, and HTTP request latency [9], [11], [12], [25]. The experimental results in [24], [36] showed the static metrics ignored the network dynamic conditions so they were not sufficient predictors for the expected response time for user requests. The dynamic replica selection approaches have emerged to improve the estimation of the expected user response time based on network factors such as round-trip time, network bandwidth, and server request latency [6], [10], [13], [20], [21], [37].

Quite a few replica selection strategies are proposed for data Grid. In [7], we proposed an effective data aggrega- tion based adaptive long term resource load point prediction mechanism, where a data aggregation concept is introduced to reduce the number of prediction step. The authors of [21] used a neural network algorithm to predict the transfer time for different sites that hold replicas and then proposed a k- nearest neighbor rule to select the best replica. Their goal is to minimize the access latency. Since the status of data replica changes dynamically, the neural network algorithm does not always give the right decision. Also, the misclassification in k-nearest neighbor rule will increase when large files are transferred and the rule needs to save all previous requests which needs time to search [3]. The paper [4] used a reverse Vickrey auction to select the cheapest replica of a file. The cost of a replica is proportional to the combination of access time and waiting time in the queue. The economic model using auction protocol to select the cheapest data replica performs long term optimization, however, it is not efficient for single data request.

A few studies propose replica selection strategies based on bio-inspired algorithms [16], [18], [28]. Bio-inspired algo- rithms offer many advantages for dealing with data-intensive service provision problems [26], [32]. Bio-inspired optimiza- tion algorithms have been proposed to solve the service pro- vision problem, because of the simplicity of the algorithms and the rapid convergence to optimal or near-optimal solu- tions [15], [31], [33]. Biological entities can learn from their environment. They can sense the surrounding conditions and adaptively invoke behaviors suited to the conditions. Biological inspired systems are typically made of a population of simple agents, which try to build the feasible solution to apply the stochastic decision policy repeatedly. They are decentralized and self-organized systems. In order to deal with the dynamic changes of data replicas and network conditions in cloud computing, as well as the constraints of different users and the flexibility of the selection criteria, the ant colony optimization algorithm will be used to solve data replica selection problem in this paper. To the best of our knowledge, this is the first work that shows how to use ant colony optimization algorithm to solve data replica selection problem.



III. BACKGROUND  A. Foundation problem  In general, data-intensive service composition will be sup- ported cooperatively by service composers, service providers,     AS1  AS2  ASn  abstract services  csn,1  csn,2  csn,m-1  csn,m  concrete services  data set 1  data set 2  data set k  data set k-1  data sets  replica 1  replica 2  replica l  replica l-1  data replicas  Application  Fig. 2: Service selection and data replica selection graph  and data providers. The service composer seeks optimal strate- gies to select elementary services provided by multiple service providers. From the service composer?s point of view, it is important to be able to assess the value of the needed services and how much it wants to pay for them to satisfy its users? requirements as well as to minimize its cost. From the service provider?s perspective, it is important to be able to analyze its competitive position and improve its offers if it is to win contracts with the service composer. The service provider requests the data from the data provider. The costs of the data affect the total cost of services. Therefore the costs of service and data have a crucial impact on the cost of service composition.

A data-intensive service composition environment can be considered to consist of a set of z data servers, D = {d1, d2, . . . , dz}. Suppose that a composite service CS is composed of a set of n abstract services, which is denoted by AS = {AS1, AS2, . . . , ASn} (Note: Since a composite service is similar to a workflow [5], tasks and abstract services are used interchangeably). It is assumed that there are m concrete services, csi = {csi,1, csi,2, . . . , csi,m}, mapping to each abstract service ASi. Each concrete service csi,j is associated with a QoS vector qij = [q1ij , q  ij , . . . , q  r ij ] with r  QoS parameters. Each abstract service, ASi ? AS, requires a set of k data sets, denoted by DT i, that are distributed on a subset of D. Each data set dt ? DT i has l data replicas.

Specifically, for a data set dt ? DT i, Ddt ? D is the set of data servers  ( each denoted by ddt  ) on which dt is  replicated and from which it is available. Also, a data server can hold multiple data sets at a time. Fig. 2 describes the service selection and data replica selection.

Consider that a data-intensive service csi,j has been chosen to replace ASi, the service endpoint is connected by links of different bandwidths with all the data servers. The cost for the task ASi includes three parts: the data access cost, the data transfer cost, and the service related cost. The data access cost is the sum of the price of all data sets. The price of a data set is the fee that a data user has to pay to the data provider for the data usage. The data transfer cost are proportional to the transfer time, which depends on the available network bandwidth between data server and service endpoint. The service related cost which mainly includes the cost to provision the service and the cost to process data sets.

Thus, selecting the reasonable data replicas can improve the response time of the service and also decrease the cost of the service.

B. Data cost model  The data providers use different pricing models to supply data sets, such as the usage-based pricing model, the package- based pricing model, the flat fee subscription-based pricing model, and the combination-based pricing model.

In the usage-based pricing model (UB), users need to pay data sets for respective usage. This is also called the short-term option. The base price for an access of data set dt is denoted by pdt, the total cost of udt usages of dt is given by (1).

costUB(dt, udt) = pdt ? udt (1)  In the package-based pricing model (PB), the data provider offers users a certain amount of data sets for a fixed fee. For example, if users use two data sets frequently and the price of the package of the two data sets is lower than the sum of individual data set, then the user can purchase the package.

Assume a user needs a set of pk data sets, the base price for an access of each data set dtv is denoted by pdtv (v ? {1, 2, . . . , pk}). The data provider offers a package of pk data sets for a price of ppk, and  pk? v=1  pdtv < ppk. Thus, the total cost  of udt usages of pk data sets is given by (2).

costPB(dt1,2,...,pk, udt) = ppk ? udt (2)  In the flat fee subscription-based pricing model (SB), users need to pay once for a data set and afterwards they can access the data set for a period of time. The cost of access the data set in this period is independent from the number of usages.

This is also called the long-term option. In this model, service providers can move the data set closer to service endpoint that requires it. The total cost of data set dt during the period of st is given by (3).

costSB(dt, st) = fstdt (3)  where fstdt is the fee of data set dt during the period of st.

In the combination-based pricing model (CB), the service provider pays a flat fee and surcharge depending on the usage of the data set. In this model, the total cost of udt usages of dt is given by (4).

costCB(dt, udt) =  { fdt, udt ? Udt fdt + ?dt ? (udt ? Udt), udt > Udt  (4)  where fdt is the flat fee of dt, Udt represents the maximal amount of usages of dt, ?dt denotes the surcharge of each access when the usage of dt extends the threshold Udt.

The service providers (data users) can switch from one pricing model to another pricing model back and forth, accord- ing to their demands. If they choose the flat fee subscription- based pricing model or the combination-based pricing model, they can store a copy of the data sets locally. This can reduce the response time and enhance the usage of bandwidth and other QoS attributes.



IV. DATA REPLICA SELECTION  A. Agent based data replica  The selection of data replicas is performed by a set of data replica (DR) agents. DR agents are located on each service endpoint and use an optimization approach for selecting the optimal replica of a data set and a caching function to make informed decisions about local data caching. As the authors in [22] distinguished caching and replication, replication is assumed to be a server side phenomenon that the server decides when and where to create a replica of its data, where caching is defined as a user side phenomenon that the user selects the best replica and caches a copy of the replica at the local machine.

A DR agent is invoked by a service that needs to access a data set. If the service endpoint does not hold the data set, the DR agent consults replica location service [23] for replica locations. Then DR agent uses the optimization approach to select the optimal replica of that data set, and it might invoke the caching function. The purpose of the caching function is to create, on the corresponding service endpoint, a copy of the requested data set. A caching function is only invoked if the service broker decides that having a local copy of the data set is economically beneficial, in the situation that the service endpoint has space to store the data set.

B. The data access response time  In order to lower the cost of data-intensive service com- position solution, this paper regards the response time and the cost of a data replica as the criteria for the data replica selection process. The data access response time is defined as the time that elapses from when a service requests for a data set until it receives the complete data set. Since the data-intensive service composition problem in this paper is concerned with the time to complete a task, it also takes into account the storage request queue (data server load) and the storage media speed (I/O data transfer rate), which are mentioned in [2], [4], [29].

Thus, the access response time of data set dt, Trt(dt), includes the data transfer time Tt(dt, ddt, y), the storage access latency Tsal(ddt), and the request waiting time Twt(ddt), which can be calculated by (5).

Trt(dt) = Tt(dt, ddt, y) + Tsal(ddt) + Twt(ddt)  Tt(dt, ddt, y) = size(dt)/bw(ddt, y)  Tsal(ddt) = size(dt)/sp(ddt)  Twt(ddt) = nr? i=1  ( size(dti)/sp(ddt)  ) (5)  where size(dt) is the size of data set dt, bw(ddt, y) is the network bandwidth between data server ddt and service endpoint y, sp(ddt) is the storage media speed, nr is the number of data requests waiting in the queue prior to the underlying request for dt. The data transfer time Tt(dt, ddt, y) is the time to transfer the data set from the remote site that houses the data replica to the local site which has the service that requested the replica. It depends on the network bandwidth and the size of the data replica. The storage access latency is the delayed time for the storage media to serve the requests and it depends on the size of the data and storage type [2].

Each storage media has many requests at the same time and it serves only one request at a time. The current request needs to wait until all requests prior it in the queue finish.

C. Cost and time aware model for data replica  For task ASi requires a set of k data sets, denoted by DT i = {dt1, dt2, . . . , dtk}, that are distributed on a subset of D. It is assumed that there are l data servers on which each data set dtv ? DT i (v ? {1, 2, . . . , k}) is replicated and from which it is available. For data set dtv , the price of its replica is denoted by pv = {pv1, pv2, . . . , pvl }, the network bandwidth between its replica site and service endpoint is denoted by bv = {bv1, bv2, . . . , bvl }, and the data transfer rate of its replica site is denoted by spv = {spv1, spv2, . . . , spvl }. A binary decision variable xvq (q ? {1, 2, . . . , l}) is used to represent only one data server is selected for each data set, where xvq is set to 1 if data server dvq is selected to access data set dt  v and 0 otherwise. The cost of data sets for ASi, Cost(DT i), can be described by (6).

Cost(DT i) = k?  v=1  Cost(dtvq)  Cost(dtvq) = l?  q=1  xvq  ( pvq +  ( size(dtv)/bvq  ) ? tcost) (6)  The data response time, Trt(DT i), can be described by (7).

Trt(DT i) =  k? v=1  Trt(dt v q)  Trt(dt v q) =  l? q=1  xvq  ( size(dtv)/bvq  + size(dtv)/spvq + nr? r=1  ( size(dtr)/spvq  )) (7)  Subject to: l?  q=1  xvq = 1, x v q ? {0, 1}, v ? {1, 2, . . . , k}. (8)  For each data replica dtvq , the goal of data replica selection process is to minimize its cost and response time. Hence, in this paper the data replica selection is a multi-objective optimization problem. In order to evaluate the quality of a data replica, a utility function is used and then the problem is transformed into a single objective optimization problem. This paper adopts the simple additive weighting (SAW) approach in the multiple criteria decision making (MCDM) [35] technique for the utility function. The utility computation includes two phases: the scaling phase and the weighting phase. The scaling phase is used to normalize all data replica selection criteria to the same scale, independent of their units and ranges. The weighting phase is used to compute the overall utility for each data replica by using weights depending on users? priorities and preferences.

The utility of a data replica dtvq is computed according to (9).

U(dtvq) = Cmax ? Cost(dtvq) Cmax ? Cmin ? ?1  + Tmax ? Trt(dtvq) Tmax ? Tmin ? ?2  (9)     where ?1, ?2 ? [0, 1], and ?1 + ?2 = 1. ?1 and ?2 represent weights of cost and response time of data replica with values normally provided by the users based on their own preferences.

Cmax and Cmin are the maximum and minimum value of the cost of all replicas of data set dtv , and Tmax and Tmin are the maximum and minimum value of the access response time of all replicas of data set dtv .

The utility of a set of k data sets, DT i, is given by (10).

U(DT i) = CMAX ? Cost(DT i) CMAX ? CMIN ? ?1  + TMAX ? Trt(DT i) TMAX ? TMIN ? ?2  (10)  where CMAX = k?  v=1 Cmax, CMIN =  k? v=1  Cmin, TMAX =  k? v=1  Tmax, and TMIN = k?  v=1 Tmin. Thus, for each task ASi,  the optimization problem is to find a set of data servers to access each data set such that U(DT i) is maximized.



V. DYNAMIC DATA REPLICA SELECTION BASED ON AN ANT COLONY SYSTEM ALGORITHM  The field of ?ant colony algorithms? studies models derived from the behavior of real ants and it is widely used for combinatorial optimization problems. Ant colony algorithms are developed as heuristic methods to identify efficient se- lections and have been applied to identify optimal solutions for service composition problems. The features of ant colony algorithms include positive feedback and local heuristics. An ant colony optimization (ACO) algorithm iteratively performs a loop containing two basic procedures. The first is how the ants construct solutions to the problem, and the second is how to update the pheromone trails. Using ACO algorithms to solve combinatorial optimization problems, requires a representation of the problem and the definition of the meaning of pheromone trails, as well as the heuristic information.

The ant colony system (ACS) is an algorithm inspired by the ant system (AS) but it differs from AS in three main aspects [8]. First, the state transition rule provides a direct way to balance between exploration of new replicas and exploitation of a priori and accumulated knowledge about the problem. Second, the global updating rule is applied only to replicas which belong to the best ant selection. Third, a local pheromone updating rule is applied while ants construct a solution.

A. State transition rule  When a DR agent is invoked by a service, all ants are initially positioned on the service endpoint. After the DR agent gets all the replica location of each data set, the ant moves from the service endpoint and visits each data replica and then return to the service endpoint. When ant k arrives at replica i, it will choose j to move to by applying the rule given by (11).

j =  { argmaxl?Dki {[?l][?l]?}, if q ? q0; J, otherwise.

(11)  where q is a random variable uniformly distributed in [0, 1], q0(0 ? q0 ? 1) is a parameter, and J is a random variable  selected according to the probability distribution given by (12) (with ? = 1).

pkij =  ??? ??  [?j ] ?[?j ]  ?  ?  l?Dk i  [?l]?[?l]? , if j ? Dki ;  0, otherwise.

(12)  where pkij represents the probability with which ant k, currently at replica i, chooses to go to replica j. Dki is the set of replicas that ant k has not accessed yet. ? is a parameter to control the influence of ?j , ? is a parameter to control the influence of ?j .

?j is the pheromone density of replica j. Here, ?j = U(dtj) which is computed according to (9) as heuristic information.

B. Global update rule  After all ants arrive at the service endpoint again, a global pheromone updating rule is performed. The pheromone level is updated by applying the global updating rule (13).

?j = (1? ?)?j + ???j (13) where ?(0 < ? < 1) is the pheromone evaporation rate, and  ??j =  { U, if ?j ? Dbs ; 0, otherwise .

where U is the utility of Dbs which is computed according to (10). Dbs is the best set of data servers found since the start of the algorithm. This formula indicates the pheromone trail update, both the evaporation and the new pheromone deposit, only apply to the replicas of Dbs. There are two types of global updating rule. One is ?iteration-best?, the best selection in the current iteration of the trial, and the other is ?global-best?, the best selection from the beginning of the trial. Experiments have shown that the global-best is slightly better, it is therefore used in our experiment.

C. Local update rule  When finding a set of data replicas, the ants use a local pheromone updating rule, which they apply immediately after having accessed replica j, as shown in (14).

?j = (1? ?)?j + ??0, ?j ? DT i. (14) where ?(0 < ? < 1) and ?0 are two parameters. At the begin- ning of the selection process, a constant amount of pheromone is assigned to all the replicas, namely, ?j = ?0 = CP (CP is a constant, ?j ? DT i). The local updating rule will reduce the pheromone trail of replica j after an ant visits it. In other words, it allows an increase in the exploration of replicas that have not been visited yet and, in practice, has the effect that the algorithm would not show stagnation behavior [8].

The data replica selection algorithm based on ACS is given in Algorithm 1 on the next page. The time complexity of the algorithm is O(noa ? k ? l), where noa, k, and l denote the number of ants, the number of data sets, and the number of data replicas for each data sets, respectively.

Algorithm 1 Data replica selection based on ant colony system algorithm Input: MaxIt: the maximum number of iterations; noa: the number of artificial ants; DT i: the required data sets; Output: D: a set of data servers to access each data set;  1: D = ?;step = 0;?0 = CP ; 2: while step < MaxIt do 3: step = step+ 1; 4: set all ants at service endpoint; 5: Dant = ?; 6: for each ant k do 7: as = ?; // data server list for each ant 8: while ant k does not come back do 9: ant k chooses a replica according to the state  transition rule (11); 10: update data server list as; 11: apply the local updating rule (14); 12: end while 13: if U(as) > U(Dant) then 14: Dant = as; 15: end if 16: end for 17: when all ants return to service endpoint again, apply  global updating rule (13) to Dant; 18: if U(Dant) > U(D) then 19: D = Dant; // keep the global-best set of data servers  to D; 20: end if 21: end while 22: return D.



VI. EXPERIMENTS AND ANALYSIS  The values of parameters considered in this paper are: ? = 1, ? = 2, q0 = 0.9, ?0 = 0.1, CP = 0.1, ? = 0.1, ? = 0.1, ?1 = 0.5, ?2 = 0.5, noa = 10, MaxIt = 1000.

The performance of the proposed algorithm is affiliated to the size of the data replica selection problem. The size of the problem depends on the number of data sets required by each service and the number of data replicas for each data set.

Thus, we generate two test groups. The first test group includes 4 test scenarios with different numbers of data replicas. The number of data replicas for each data set ranges from 3 to 6, in increments of 1. The number of required data sets is fixed at 10. The second test group includes 4 test scenarios with different numbers of required data sets. The number of data sets is 15, 20, 25 and 30. The number of data replicas for each data set is fixed at 6. This two test groups are designed to test how the running time of the proposed algorithm will change as the number of data sets and the number of data replicas change.

All test scenarios are run twenty times and the average values are reported. The price of a data replica, the net- work bandwidth (Mbps) between each data server and service endpoint, and the storage media speed (Mbps) are randomly drawn from a uniform distribution of the interval [1,100]. The  size (MB) of a data set is randomly drawn from a uniform distribution of the interval [1000,10000]. The number of data request in the waiting queue is randomly drawn from a uniform distribution of the interval [1,10]. These values will not affect the running time of the proposed algorithm, so their values can be in any intervals.

The simulation results are shown in Fig. 3 on the next page. Fig. 3(a) to Fig. 3(d) show the results of the first test group. Fig. 3(e) to Fig. 3(h) show the results of the second test group. In Fig. 3(a) to Fig. 3(h), the blue line denotes the utility of the best selection from the beginning of the trial, and the red point denotes the utility of the best selection of each iteration. The value of ?GUtility? is the utility of the best selection from the beginning of the trial and it depends on the cost and response time of data sets. That is to say, the change of the value of ?GUtility? has no significance for the simulation results. The value of ?FRIT? is the number of iterations when the best utility appeared and from this iteration the value of the best utility will not change.

According to the value of ?FRIT? in Fig. 3(a) to Fig.

3(d), as the number of data replicas increases from 3 to 6 in increments of 1, the ants need more iteration times to find the best selection. Fig. 3(i) shows the effect of the number of data replicas on the running time for finding the best selection.

According to the value of ?FRIT? in Fig. 3(e) to Fig. 3(h), as the number of required data sets increases, the number of iterations also increases. When the number of required data sets is 15, 20, 25, and 30, the number of iterations to find the best selection is 27, 30, 40 and 56. All the figures in Fig. 3 show the proposed algorithm can solve the data replica selection problem efficiently.



VII. CONCLUSIONS  In our AMS data processing center, service composition involves large amounts of data being transferred, organized and stored, which makes the economics of the whole composition more complex. It is critical and challenging to be cost and time aware to manage the data and services in this intensive research environment. As data play the dominant role in execution of data-intensive service composition, the cost and response time of a data set influence the quality of the service that requires the data set. In the process of data-intensive service composition, service providers need to access a variety of data sets and pay for what they use. Service providers should make decisions about how to pay for the data sets and where to access them based on the number of service requests. They will always try to trade-off costs and gains when providing services. Thus, service providers need a data replica selection strategy, which can select the best replicas in order to reduce the response time and cost of their services. A novel data replica selection algorithm based on ant colony optimization is proposed, and the performance of the algorithm has been studied by simulations. The experimental results show that the proposed algorithm can solve data replica selection problem efficiently. Meanwhile, if the service endpoint has space to store a local copy of the data set and this is economically beneficial, a caching function will be invoked. To design a caching function for each data replica agent is also a part of our future work.

100 200 300 400 500 600 700 800 900 1000 0.94  0.95  0.96  0.97  0.98  0.99   Iterative Time  G lo  ba l B  es t U  til ity  GUtility=0.9489, FRIT=11  100 200 300 400 500 600 700 800 900 1000 0.94  0.95  0.96  0.97  0.98  0.99   Iterative Time  Ite ra  tiv e  B es  t U til  ity  (a) Each data set has 3 data replicas  0 100 200 300 400 500 600 700 800 900 1000 0.94  0.95  0.96  0.97  0.98  0.99   Iterative Time  G lo  ba l B  es t U  til ity  GUtility=0.99564, FRIT=20  0 100 200 300 400 500 600 700 800 900 1000 0.94  0.95  0.96  0.97  0.98  0.99   Iterative Time  Ite ra  tiv e  B es  t U til  ity  (b) Each data set has 4 data replicas  100 200 300 400 500 600 700 800 900 1000 0.94  0.95  0.96  0.97  0.98  0.99   Iterative Time  G lo  ba l B  es t U  til ity  GUtility=0.98601, FRIT=24  100 200 300 400 500 600 700 800 900 1000 0.94  0.95  0.96  0.97  0.98  0.99   Iterative Time  Ite ra  tiv e  B es  t U til  ity  (c) Each data set has 5 data replicas  100 200 300 400 500 600 700 800 900 1000 0.94  0.95  0.96  0.97  0.98  0.99   Iterative Time  G lo  ba l B  es t U  til ity  GUtility=0.97521, FRIT=26  100 200 300 400 500 600 700 800 900 1000 0.94  0.95  0.96  0.97  0.98  0.99   Iterative Time  Ite ra  tiv e  B es  t U til  ity  (d) Each data set has 6 data replicas  100 200 300 400 500 600 700 800 900 1000 0.94  0.95  0.96  0.97  0.98  0.99   Iterative Time  G lo  ba l B  es t U  til ity  GUtility=0.99003, FRIT=27  100 200 300 400 500 600 700 800 900 1000 0.94  0.95  0.96  0.97  0.98  0.99   Iterative Time  Ite ra  tiv e  B es  t U til  ity  (e) Each service needs 15 data sets  100 200 300 400 500 600 700 800 900 1000 0.94  0.95  0.96  0.97  0.98  0.99   Iterative Time  G lo  ba l B  es t U  til ity  GUtility=0.98621, FRIT=30  100 200 300 400 500 600 700 800 900 1000 0.94  0.95  0.96  0.97  0.98  0.99   Iterative Time Ite  ra tiv  e B  es t U  til ity  (f) Each service needs 20 data sets  100 200 300 400 500 600 700 800 900 1000 0.94  0.95  0.96  0.97  0.98  0.99   Iterative Time  G lo  ba l B  es t U  til ity  GUtility=0.98639, FRIT=40  100 200 300 400 500 600 700 800 900 1000 0.94  0.95  0.96  0.97  0.98  0.99   Iterative Time  Ite ra  tiv e  B es  t U til  ity  (g) Each service needs 25 data sets  0 100 200 300 400 500 600 700 800 900 1000 0.94  0.95  0.96  0.97  0.98  0.99   Iterative Time  G lo  ba l B  es t U  til ity  GUtility=0.98115, FRIT=56  0 100 200 300 400 500 600 700 800 900 1000 0.94  0.95  0.96  0.97  0.98  0.99   Iterative Time  Ite ra  tiv e  B es  t U til  ity  (h) Each service needs 30 data sets  3 3.5 4 4.5 5 5.5 6 0.1  0.15  0.2  0.25  0.3  0.35  Number of data replicas  R un  ni ng  ti m  e to  fi nd  th e  be st  s et  o f d  at a  re pl  ic as  ( s)  Required data sets is fixed to 10  (i) The effect of the number of data replicas on the running time for finding the best set of data replicas.

