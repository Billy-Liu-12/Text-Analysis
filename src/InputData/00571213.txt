DATA MINING IN A LARGE DATABASE ENVI

ABSTRACT  Data mining, the process of discovering hidden and potentially useful information from very large databases, has been recognized as one of the most promising research topics in the 1990s. The essential problem faced in the mining of association rules is the generation of large items, which are items that are present in at least s% (minimal support) of the total database tuples. As the large items and their counts information usually require much storage space, the minimal cover concept is introduced to achieve reductions in the storage size.

Percentage contour, an extension of minimal cover, is further introduced to aid in the handling of large databases.

1 INTRODUCTION  database. Given a database of sales transactions, association rules can be described as follows :  Data mining can be defiied as the process of discovering hidden and potentially useful information from very large databases. It has been recognized as one of the most promising research topics in the 1990s by both database and machine learning researchers [2-lo]. The purpose of database mining is to transform information, expressed in terms of stored data, into knowledge, expressed in terms of generalized statements, or rules about some characteristics, or relationships occurring in data.

Many important types of rules can be derived from databases. They include rules like classification, sequential patterns and similar sequences [5] ,  etc.

In [SI, Agrawal et. a1 introduced a new class of rules called association rules, and further provide the algorithm for finding such rules from a  0-7803-3280-6/96/%5.00 @1996 IEEE - 988 -  The essential problem of mining association rules is to find all items, that are present in at least s% of the total database tuples. The value s% is known as the minimal support. It is important not to confuse support with confidence. While confidence is a measure of a rule?s strength, support corresponds to statistical significance [SI. We refer to all items that have fractional transaction support above the minimal support as large items and all others as small items or non-large items.

Discover all association rules such that in at least c% of transactions, the presence of an item (or a set of attributes) implies the occurrence of another item. The value c% is the confidence factor or confidence threshold of the rule. One such example is ?If a customer buys bread and butter, then the customer also buys milk in at least 90% of the cases?. The antecedent of this rule consists of bread and butter, while the consequent consists of milk alone.

Consider a supermarket with a ?large collection of items. Common business decisions that the management of the supermarket has to make include what items to put on sales, how to arrange the items on the shelves in order to maximize the profit, etc. [SI. Thus, association rules are considered important rules in the real world and have been widely applied to various databases.

Applications on databases include customer behaviour analysis on sales data obtained from a large retailing company [SI, a course enrollment database in a university [4] and a telephone company fault management database [4].

When association rules are frequently probed by users, it is expensive to search the database to discover the rules every time. Alternatively, we can choose to store all the association rules. However, the number of association rules is usually much larger than the number of large items (as shown in [4]). Thus, it is still preferable to store the large items and their counts, rather than the association rules.

If there are only a few large items, storage of the large items and their counts do not cause much problem. However, once the number of large items becomes huge, the storage is no longer feasible. To solve the above problem, we introduce the concept of minimal covers, which are able to reduce the amount of information stored. We further extend the concept of minimal covers and present the concept of percentage contours, which can aid in the handling of large databases. Applications of both the minimal covers and percentage contours will be presented as well.

The rest of this paper is organized as follows. In Section 2, we will be giving a formal description of the underlying model. In Section 3, we present to readers the definition of minimal cover and its storage structure. By extending the concept of minimal covers, we further present the concept of percentage contour. In Section 4, we conduct four operations based on minimal covers and percentage contours. Concluding remarks will be given in Section 5.

2 DESCRIPTION OF MODEL  Let T be a database of transactions or tuples where each tuple t is represented as a binary vector. We would then have Attr(T), which is the set of all attributes in T. An item I = {I,, Iz, ..., I,,,}, where each element Ii, for 1 I i I m, is an attribute, is a nonempty subset of Attr(T). A tuple t is known to satisfy an item I if for all attributes Ii in I, t[Ii] = 1.

For a given item I, count(1) or the count of item I, is the total number of tuples in the database that satisfy I. If there exists two items, X and Y, such that X c Y, then X is known as the parent or ancestor of Y. On the other hand, Y becomes the  child or descendant of X. Also, if the number of different attributes between X and Y is m, then Y is known as the m-extension of X. Since X, being the parent of Y, consists of less attributes than Y, therefore count(X) 2 count(Y) will always hold.

By an association rule, we mean an implication of the form X j Y. X and Y are both items which can consist of either a single attribute or a set of attributes. One point to note is that Y does not contain any common attributes as X (that is : X n Y = 0). The rule X Y is satisfied in the set of tuples T with the confidence threshold 0 5 c 5 1 iff at least c% of tuples in T that satisfy X also satisfy Y. We use the notation X * Y I c to specify that the rule X Y has a confidence threshold of c, or simply X a Y, if no confusion arises. The rule X  count (XY) Y I c is also equivalent to 2 c. The  count (x) support for a rule X a Y is defined to be the fraction of tuples in T that satisfy t[Ii] = 1 for all Ii in XY.

Support of an item I is defined to be the fraction of tuples in the database that satisfy I. If we are interested only in those items that have their support exceeding a certain threshold, this support becomes the minimal support. Therefore I is considered to be large if its support is greater than or equal to the minimal support.

As shown in [8], the problem of mining association rules can be decomposed into two subproblems :  Generate all large items X, such that count(X) 2 s * Card(T); where s is the minimal support and Card(T) is the total number of tuples in the database T.

0 For each large item X = IIIz ... Ik, k 2 2, generate all possible association rules Y (X - Y) ,  such that Y c X and 1 I length(Y) I (k-1).

(Note : length(Y) refers to the length of Y).

The length of an item is equivalent to the number of attributes it consists of. In general, an n-length item, or an item in level-n, will consist of n attributes.

Once the large items have been determined, the solution to the second subproblem becomes quite straightforward. In this paper however, we shall not  - 989 -    have any discussions on the determination of large items since we are placing our efforts on the introduction of minimal covers and its applications.

Before we embark on the discussion of minimal covers, we would just like to mention that the large items generated from the database are stored according to their lengths.

3 CONCEPT OF MINIMAL COVERS  In this section, we shall introduce to readers the concept of minimal covers, which can be used to reduce the amount of information or data stored.

Such a reduction is particularly important when the databases concerned are distributed. The reason is : For distributed databases, we cannot avoid the possibility of transmitting data from one site to another. Since transmission of huge data takes time and causes much traffic over the network, it will be beneficial if we can transmit minimum possible data and yet not miss out on any important information.

As a result, the concept of minimal covers is proposed.

3.1 FORZMAL MODEL OF MINIMAL COVERS  An itemset is a set consisting of items. Given an itemset S, we define the generating set of S, g(S), as a collection of items in S and all their ancestors.

g(S) is also called the closure of S. In turn, S is called the generator of g(S).

Definition 3.1. An itemset X is a cover of itemset S iff X is a subset of S and g(X)=g(S).

Definition 3.2. For itemset X to be a minimal cover of itemset S ,  X must be a cover of S and X E Y for every cover Y of S.

Example. Given S = { 11, 12, 13, 4, 1112, 1113, Illcl,  Let X = { I lb  1113,114, 1213 1, Y = { 13, 4, I IL  111213 } and Z = { 114, 111213 }.

Itemset X is not a cover since item 111213 cannot be generated from X. Both itemsets Y and Z are covers, since all items present in S can be generated from them. However Z c Y, therefore itemset Y is not a minimal cover of S.

1213, 111213 1.

The minimal cover of itemset S, denoted as MC(S), is actually the set of all maximal items of S. We call items that are not included in any other items of S as maximal items. In another word, maximal items can never be the parents of any items in S.

We present the following theorem to illustrate the content of a minimal cover.

Theorem 3.1. The minimal cover of itemset S is exactly the set of all maximal items of S.

Proof. Let M = set of all maximal items of S. For any arbitrary item x in S, x is either in M or is an ancestor of some item y in S. In the latter case, y is either in M or y is also an ancestor of some item z in S .  The list goes on and eventually, the expansion will end at an item in M. This shows that x is either in M or is the ancestor of some item in M. Thus M is indeed a cover and therefore contains the minimal cover.

On the other hand, a cover of S must always contain M; otherwise it cannot be a cover. Ci  For any given generating set g(S), its corresponding minimal cover contains sufficient and necessary information to allow reconstruction of g(S). Thus, minimal covers are considered as the most compact subsets with respect to generating sets. To store a minimal cover MC, it is divided into subsets according to the lengths of its items. Figure 3-1 shows the storage structure.

m:numberofsJbsets  I,:hphofitemsinenchsJbset  n, : n u m k  of items m each SubM  Figure 3-1 : Storage Format of a Minimal Cover  3.2 PERCENTAGE CONTOUR  We shall now embark on the discussion of the extension of minimal covers, the percentage contour. A contour is defined as a collection of itemsets SI&, ... ,S,, in which any two arbitrary itemsets Si and S,, 1 I i I n, 1 I j I n, is either Si c Sj or Sj Si.

- 990 -    Suppose we have a database T, with Card(T) referring to the number of tuples in T. For 0 5 c I 1, where c is any arbitrary support value, we define a percentage set as GE(c) = {all items in T which have counts 2 c * Card(T)}. Usually we have GE(0) = {all items in T} and GE( 1 )  = 0. When c1 > c2, GE(c2) - GE(cl) = ( X I X is an item in T and c1* Card(T) > count(X) 2 c2 * Card(T)}. For 1 > CI> c2> ... > c,> 0, the set PC = ( GE(ci) 1 i = 1 ,  2, . . -, n } forms the percentage contour of database T, where GE(cl) c GE(c2) . . . c GE(c,).

Let a percentage contour PC = { GE(cl), . . . ,GE(c,), where CI> c2 > ... > Cn }. To keep the storage space of PC small, we store only the minimal cover MC, for each i, instead of the whole set of GE(c,). To further save on storage space, we store in PC the set Ac, = GE(c,) - GE(C,.~), instead of GE(c,) itself.

Now that we store only Ac, = GE(c,) - GE(c,.,), we need to reconstruct the set GE(c,) using GE(c,.J U Ac,.

The storage of the percentage contour is slightly modified from that of a minimal cover, The differences are : (i) there is an additional field pi, indicating the lower percentage bound of a particular percentage set; (ii) the maximal items belonging to all the percentage sets are stored separately from the percentage set information. The modified storage format is shown in Figure 3-2.

t  p, : lowcr hound of ik pcrccntrgc rangc  m : numher of suhtee  Ir :length of itcms in tach suhsct  nr : number of itcms in each suhsct  Minimal cover, being the most compact way in representing a complete set, can be used to facilitate large distributed database mining in various ways.

Four important and useful types of operations that can be applied to the minimal covers are discussed below. For discussion, assume that we have a database T that is distributed over n sites, with each site i having a fragment Ti. Let the nth site be the coordinator while the other sites are the participants.

4.1 UNION OPERATION  We are interested in finding for each site, all the large items that have a local support of at least c.

After which we would like to compile the items from each site into a single result set.

First of all, all the participants have to transmit S,(c) = {all items in TI that have count 2 c * Card(T,) ) . After receiving S,(c) from all participants, the coordinator will perform Su = u:=,S,(c), which includes its own set, S,(c).

(Note : The transmission of all SI sets, discussed in this section and later sections, are done using minimal covers, so that we can reduce the amount of data to be transmitted.)  The purpose of the union operation is not solely to combine all items collected from each site into a single set. It needs to identify items that may be redundant due to the presence of their descendants in the result set. Therefore, all items, except those that belong to the highest level, will be examined.

Any items found to be covered by any higher level items will be excluded from the final result set. The end result is a union result set that is kept to the minimum possible size.

In [ I ] ,  a new algorithm for determining the large items has been proposed. This algorithm brings into light another area whereby the union operation proves to be useful even in a centralized database.

Figure 3-2 : Storage Format of a Percentage Contour  Information  Basically the algorithm executes in two phases. In the first phase, the database is horizontally divided into a number of non-overlapping partitions. The  * oPERAT1oNS OF AND PERCENTAGE CONTOURS  partitions are considered one at a time and all the large items for that partition are generated. At the  - 991 -    end of this phase, all the local large items for each partition are merged to generate a set of potential global large items. In the second phase, the actual support for these items are generated and the large items are identified.

After a brief look at the algorithm, we notice that the union operation is applicable at the end of the first phase, whereby the local large items from each partition are merged into a single set. This merging process is equivalent to the union operation, whereby all items from different sets are combined into a single union result set.

4.2 INTERSECTION 0 PERATIO N  Now we wish to find all large items that have global minimal support of at least c. Let us start off the discussion with n = 2. We have Sl(cl) = {all items in TI that have count 2 c1 * Card(T1)} and Sz(c2) = {all items in T2 that have count 2 c2 * Card(T2)j.

For any item X in [Sl(cl) n S2(c2)], it is true that count(X) 2 [cl * Card(T1) + c2 * Card(T2)]. If [c1 * Card(T1) + c2 * Card(T2)I 2 c * Card(T), then all items in [Sl(cl) n S2(c2)] are large items with respect to the support factor c.

In general, the coordinator will first collect S,(c,) from participant i, 1 5 i 5 n, where 0 5 c, 5 1 ,  and c:=, c,* Card(Ti) = c * Card(T). ( we select c, = c for all i = 1, 2, ..., n if we do not have any prior knowledge). The coordinator will generate two sets,  sI = n;=, s,(~,) and sC = n:=, S, (cl) and transmit them to the participants. (Note : The complement operation will be discussed in Section 51.3.) All items in SI becomes part of the result set since they are already known to be large. However all items in S c  are ignored since they are already known to be small. Only the remaining items, that is those items that are neither in SI nor in Sc ,  will be considered for further examination.

4.3 COMPLEMENT OPERATION  For this operation, given an itemset S,, we are interested in finding all items that are not present in S,. To simplify our discussion, assume we have a  centralized database T with tuples that have four attributes, { 11, 12, 13, 14 }. With four attributes, we  can form a total of (:).(;)+(;]+(:)= 4 + 6 + 4 + 1 = 15 different combinations of attributes.

For example, we have items 11, 12, 13 and I4 in level L1 and a single item 111213b in level L4.

Suppose SI now contains the following items : { 11, 12, 13, 1112, 1113, I213 }. The complement set Sc is supposed to contain all possible combinations that are not present in S,. In this case,  11121314 1.

S C  = { 14, 1114, I2L7 1314, 1112139 1112149 111314, I213I4,  To keep the complement result set small, we only need to store items that are not present in SI and have length less than or equal to the maximum possible length in SI. All other items with greater length are trivially known, to be part of the complement set. Similar to the union operation, our complement result set is dept to the minimum possible size. Therefore, when the need arises for the above SC to be transmitted to another site, only { 14, IIL, 1214, 1314 } need 'to be transmitted. All others { 111213, 111214, 1113L, 121314, 11121314 } can be constructed by the receiver itself, provided it knows the maximum possible number of attributes (which is four in this case).

I  4.4 DIFFERENCE OPERATION  Given two sets SI and S2, the difference result set S D  = S2 - SI. Basically, the difference operation is useful when given two itemsets, we are interested in collecting all items that are present in one set but not the other.

One application of the difference operation is : Refer to the example given in Section 5.1.3, we mentioned that the complement set SC contains all items that are not present in a given set SI. Assume we are to find the complement set for SI ,  which contains { 11, 12, 13, 1112, 1113, 1213 } .  Let S2 contain all items with length less than or equal to maxlength, { 11, ..., 1112, ..., 1314 }. By performing SD = S2 - S I ,  we are able to obtain the complement set of SI ,  that has been kept to the minimum possible size.

- 992 -    5 CONCLUSION  The main advantage of the minimal cover lies in the savings of storage space. In fact, savings on the storage space increases, as the minimal support value decreases. As we know that transmission of data from one site to another creates much traffic over the network, the decrease in data stored is especially beneficial when there is a need for transmission. We deduce that not only can minimal cover be applied to distributed database systems, which occur often in real-world applications, it is also attractive to a cliendserver environment.

By extending the concept of minimal covers, we introduce the percentage contour, which is defined as a collection of percentage sets. Each set is just a collection of items that have counts greater than or equal to a certain percentage of the total database tuples. The percentage contour is economically stored as a set of minimal covers, in order to save storage space. The number of percentage sets in a percentage contour is only limited by the storage space available. As percentage contour can be used to facilitate large database mining, the more percentage sets it contains, the more accurate will be the results.

We further explore the various areas under which the minimal covers and the percentage contours proved to be useful. Four operations, namely union, intersection, complement and difference operations, which when applied on the percentage contours, helped us to achieve different purposes. For example, the intersection and complement operations together helped us to determine global large items for a distributed database. Another area of application is the compact storage of counts information, which facilitates the answering of the fallowing 2 queries :  Given an item X, estimates its count, Given a value 0 < c < 1, search for all items X that have count(X) 2 c * Card(T), where Card(T) = total number of tuples in database T.

