Rare Itemset Mining Mehdi Adda1, Lei Wu2, Yi Feng3

Abstract? A pattern is a collection of events/features that occur together in a transaction database. Previous studies in the field are often dedicated to the problem of frequent pattern mining where only patterns that appear frequently in the input data are mined. As a result, patterns involving events/features that appear in few data sets are not captured. In some domains, such as the detection of computer attacks, fraudulent transactions in financial institutions, those patterns, also known as rare patterns, are more interesting than frequent patterns. We propose a framework to represent different categories of interesting patterns and then instantiate it to the specific case of rare patterns. Later on, we present a generic framework to mine patterns based on the Apriori approach. In this paper we are interested by the patterns composed of a set of items, also called itemsets. Thus, we instantiate the generalized Apriori framework to mine rare itemsets. The resulting approach is Apriori-like and the mine idea behind it is that if the itemset lattice representing the itemset space in classical Apriori approaches is traversed on a bottom-up manner, equivalent properties to the Apriori exploration of frequent itemsets are provided to mine rare itemsets. This include an anti-monotone property and a level- wise exploration of the itemset space. As demonstrated by our experiments, our approach is effective in identifying all rare itemsets and is more efficient than the existing approach.



I. INTRODUCTION  Nowadays, data mining techniques are widely used to process huge amount of information in order to extract more valuable data (a.k.a. knowledge) to be directly interpreted or exploited to feed other processes. In many cases, data mining techniques are used to discover patterns that can be of interest to a specific domain of application. As a matter of fact, different categories of patterns exist, such as sequences, itemsets, association rules and graph patterns. The choice of a technique depends not only on the nature of input data but also depends on what we want to obtain and what view or part of the data we want to be described or represented in a more intelligible and concise way. To filter out the patterns, some criteria are used. The most known of them are the support and the confidence. While the support represents the number of times a pattern occurs in the initial database (a.k.a. frequency), the confidence represents a percentage value that shows how frequently a part of the pattern, called premise, occurs among all the records containing the whole rule body.

Herein, we classify pattern categories according to the spe- cific use of the support threshold. In fact, by setting ranges for the support, one can obtain different categories of patterns. For example, if we set a minimum support threshold that a pattern have to satisfy to be considered as an interesting pattern,  we obtain what is so called frequent patterns. By setting a maximum support threshold we obtain another category of patterns called rare patterns. Whereas frequent patterns focus on mining patterns that appear more frequently in the database, rare patterns aim to discover patterns that are less frequent.

Rare patterns can be used in different domains such as biology and medicine [7], [9], security [5], etc. For example, in medicine by analyzing clinical databases one can discover rare patterns behind them that will help to make decisions about the clinical care. In the security field, normal behavior is very frequent, whereas abnormal or suspicious behavior is less frequent. Considering a database where the behavior of people in sensitive places such as airports are recorded, if we model those behaviors, it is likely to find that normal behaviors can be represented by frequent patterns and suspicious behaviors by rare patterns.

As one can notice, each category of patterns explores the data seeking for a specific kind of knowledge. Other categories of patterns than frequent and rare patterns can be mined.

For example, one can also be interested by the patterns with a frequency smaller that an upper limit and greater than a lower limit. However, the existing frameworks generally deal only with a specific category of patterns. In this paper we start by proposing a generalized framework to model the different categories of patterns based on the frequency constraints and that take into account the possibility of having data set and the pattern spaces at different levels of abstraction.

The proposed framework is later on used to instantiate the two families of patterns that are of interest to us: frequent patterns and rare patterns. In one side, several techniques and algorithms have been developed in order to efficiently mine either frequent patterns or a subcategory of frequent patterns such as closed [8], and maximal patterns [6]. However, until now few algorithms and techniques are developed to mine rare patterns. Algorithms usually used to mine frequent patterns are not suitable for mining rare patterns. In fact, in order to discover rare patterns the frequent pattern mining algorithms have to set the support to a low value which will result on a combinatorial explosion and will produce a huge number of patterns with only part of them are rare. Thus, techniques adapted to rare patterns have to be developed. As for any new field, there is two possibilities:(i) develop new algorithms to tackle the new challenges encountered or (ii) adapt existing algorithms and techniques to fit the new problem. The first path may need more energy and may take more time whereas the second alternative could be of help in cases where the   DOI 10.1109/ICMLA.2007.106    DOI 10.1109/ICMLA.2007.106    DOI 10.1109/ICMLA.2007.106    DOI 10.1109/ICMLA.2007.106    DOI 10.1109/ICMLA.2007.106    DOI 10.1109/ICMLA.2007.106     new problem can be directly linked and represented using all the background and knowledge gathered when solving old problems. As a matter of fact, there are two possibilities, either develop dependent applications and techniques for rare pattern mining or exploit existing frameworks and adapt them to our problematic. In this paper we investigate the second way to tackle the problem of rare pattern mining. Indeed, we have investigated the adaptation of one of the well known approaches to mine frequent itemsets: Apriori to the problem of mining the rare itemset mining. That is, our goal is to absorb the spirit of the Apriori approach used for frequent itemset mining and apply it to rare itemset mining. We start by factorizing the key elements of Apriori approaches such as the traversal of the itemset space, the pruning principle, the combination of itemsets at a level to generate new itemset in the next level, and then we propose an Apriori generalized framework that abstracts those elements. With the Apriori generalized framework on hands, the instantiation of a specific framework to tackle the Apriori based mining approach for rare itemsets became straightforward.

The remainder of this paper is organized as follows. The problem of rare itemset mining and related works are presented in Sections II and III. Then, a first framework to represent different categories of interesting patterns is presented in Section IV. In Section V we begin by presenting the main idea behind our approach and show how the approach is effective using an example. Then, we present the generalized Apriori framework which by means of instantiation lead to an Apriori- based rare itemset mining approach and an algorithm called AfRIM (Apriori for Rare Itemset Mining). The experimental results of mining rare itemsets using the AfRIM algorithm are described in Section VI. Concluding remarks and perspectives are presented in Section VII.



II. THE PROBLEM OF RARE ITEMSET MINING  In this section we present an example of rare itemset mining.

The input data is composed of a database of transactions, and each transaction is identified by an id and composed of a set of items. In the real world, a transaction can be seen as the basket bought by a customer during a determined period of time (day, week, month, etc.). Each basket is composed of a set of items that are purchased together. In Table I we represent an abstract database, denoted by D, where the alphabet letters are considered as items. Given the database of transactions such as presented in Table I, our goal is to find the set of items, also called itemsets, that are present in at most two transactions.

The number of times an itemset occurs in the database is called the itemset support. In our case the maximum support is equal to 3.

The set of all itemsets that can be generated from the transaction database are presented in Figure 1 by a diagram of the subset lattice for five items with the associated frequencies in the database. In the lattice each level is composed of itemsets having the same length. The top element in the lattice is the empty set and each lower level k contains all of the itemsets of length k, also denoted k-itemsets and the last level contains an itemset composed of all items (i.e.

id Transaction t1 {a, b, c, d} t2 {b, d} t3 {a, b, c, e} t4 {c, d, e} t5 {a, b, c}  TABLE I TRANSACTION DATABASE  {}  b d 4 4 4 4 2  : Rare pattern  : Frequent pattern  a b c d e  ab ac ad ae bc bd be cd ce de 3 3 1 1 3 2 1 2 2 1  ab ac ad ae bc bd be cd ce de  abc abd abe acd ace ade bcd bce bde cde 3 1 1 1 1 1 1 1 0 1  abcd abde abce acde bcde 1 0 0 1 0  abcde  Fig. 1. Lattice representing an hierarchically ordered space of itemsets and their frequencies. Frequent itemsets are represented by squares and rare itemsets by ovals.

a, b, c, d, e). Lines between nodes show a subset relationship between itemsets. For each itemset we computed its support.

For example, the itemset composed of the items b and d denoted by ?bd? have 2 as support, and we denote it by ?bd, 2?. In fact, the itemset bd is present in the transactions t1, t2. The set of rare itemsets we are looking for are those in the lattice with support less than 3. The rest of itemsets are frequent (having support greater or equal to 3). In Figure 1 rare itemsets are drawn with ovals where frequent itemsets are drawn with rectangles. Thus, after counting the frequencies of each itemset, we obtain the following set of rare itemsets ?e, 2?, ?ad, 1?, ?ae, 1?, ?bd, 2?, ?be, 1?, ?cd, 2?, ?ce, 2?, ?de, 1?, ?abd, 1?, ?abe, 1?, ?acd, 1?, ?ace, 1?, ?ade, 1?, ?bcd, 1?, ?bce, 1?, ?bde, 0?, ?cde, 1?, ?abcd, 1?, ?abde, 0?, ?abce , 0?, ?acde, 1?, ?bcde, 0?, ?abcde, 0?.



III. RELATED WORK  The previous methods to mine itemsets can be divided into two categories, namely frequent itemset mining and rare itemset mining techniques. Whereas the problem of frequent itemset mining have been widely studied and many algorithms have been proposed, the problem of rare itemset mining just begin to spark the researchers interest and today only one algorithm is proposed to mine all rare itemsets. Frequent and rare itemset approaches are briefly reviewed in this section.

A. Frequent Itemset Mining  Itemset space has two properties, a monotone property and an anti-monotone property. They are as follows: every non- empty subset of a frequent itemset is a frequent itemset, and every superset of a non-frequent itemset is non-frequent. Based     on those property, the algorithm Apriori [1] was developed to efficiently mine frequent itemsets. Later on, new algorithms have been proposed to mine frequent itemsets such as Eclat [12], FP-Growth [4] and TM [10]. A recent survey on frequent itemsets mining techniques is presented in [3].

The Apriori approach was widely and successfully used to generate all frequent itemsets contained in a transaction database. Apriori exploits the monotonicity property of the support of itemsets. Apriori-based algorithms perform top- down breadth-first search through the space of all itemsets. In the first pass, the support of each individual item is counted, and the frequent ones are inserted to the frequent itemset set of level 1. In each subsequent pass, the frequent itemsets determined in the previous pass is used to generate new item- sets called candidate itemsets. The support of each candidate itemset is counted, and the frequent ones are determined. This process continues until no new frequent itemsets are found.

B. Rare Itemsets  Recently, a work presented in [11] proposes an approach mine rare itemsets that is based on the Apriori algorithm used to mine frequent itemsets. The main idea consists at traversing the itemset space by the Apriori algorithm used to mine frequent itemsets and collect at each level the itemsets that are usually pruned out in the original algorithm and are used as seed for a second algorithm in order to mine the remaining rare itemsets. In this perspective, the approach mines rare itemsets using two algorithms : Apriori-rare and Arima. Apriori-rare is a modification of the Apriori algorithm used to mine frequent itemsets. Apriori-rare generates a set of all minimal rare generators, also called MRM, that correspond to the itemsets usually pruned by the Apriori algorithm when seeking for frequent itemsets. The authors defined an MRM itemset as a rare itemset that do not have any subset which is rare. Arima takes as input the set of rare MRMs and generates the set of all rare itemsets split into two sets: the set of rare itemsets having a zero support and the set of rare itemsets with non-zero support. Rare itemsets are generated by growing each itemset in the MRM set. In fact, if an itemset is rare then any extension of that itemset will result a rare itemset. To make the distinction between zero and non-zero support itemsets, the support of each extended itemset is calculated. For the sake of concision, we latter on refer to this approach as Arima.



IV. GENERALIZED FRAMEWORK FOR PATTERN MINING  A. Problem definition  The general problem of pattern mining can be summarized as follows. Consider a universe of objects, or items, I, a universe ? composed of sets of items from I, also called itemsets, a database D made of records combining items from I ( D ? ? ), and a minimum frequency, or support, threshold ?1 (a.k.a minsupp), and a maximum frequency threshold ?2 (a.k.a maxsupp) (See Definition 4.1), such that 0 ? ?1 < ?2 ? |D|+1 where |D| represents the number of records contained in D also known as cardinality.

Definition 4.1: Let D be a subset of the data set ?, ? the pattern space, and f in ?, the frequency, or support, of f , w.r.t.

D, denoted by suppD(f), corresponds to the number of data sets in D that are instances of f . More formally, we have the following: suppD : ? ? N such that : ?f ? ? : suppD(f) =?  d??(d?f).

Two languages, the pattern language ? composed of patterns  and the data one ? composed of itemset transactions, and two binary relations underly the problem: the generality between patterns, denoted by 	, and instantiation between a data set and a pattern, denoted by ?. When a transaction, say d, is instance of a pattern, f , d?f returns the value 1, otherwise its value is 0. Generality follows instantiation as given a pattern f ? ? and a super-pattern thereof f ? (f 	 f ?), each record d ? ? instantiating f (d?f ) instantiates f? as well.

We define the domain of interpretation of a pattern f with respect to the data set space ?, denoted by [f ]?, as the set of the pattern instances. When considering only a subset of the data set space, say D, each record in D is instance of at least one pattern from ?. The set of all those patterns, constitute what we call the coverage set. A formal definition is given below (see Definition 4.2).

Definition 4.2: Given a couple (?,?) of pattern and data spaces, and the data set D such that D ? ?. The coverage of the data set D w.r.t. the pattern space ?, denoted by C??(D), is a set of patterns such that each pattern have at least one instance in D. More formally, the mapping C?? : 2? ? 2? is defined by C??(D) = {f |f ? ? and ?d ? D such that d?f}.

Remark 4.1: The coverage set of the data set space is the pattern space. In other words, C??(?) = ?.

Pattern mining amounts to extracting the family FD?1,?2 of itemset collections, or patterns that are present in D at least in ?1 records and at most in (?2 ? 1) records (see Definition 4.3). Proposition 4.1 states that the coverage set is equivalent to the largest family of patterns.

Definition 4.3: Given D from ?, and ?1, ?2 such that 0 ? ?1 < ?2 ? |D|+1, the pattern family FD?1,?2 is a set of patterns such that FD?1,?2 = {f |f ? C??(D)?(?1 ? suppD(f) < ?2)}.

The patterns in FD?1,?2 are also called interesting patterns.

Proposition 4.1: Let ?,? be the pattern and data spaces, respectively, D be a subset of ? and the family of patterns FD1,|D|+1 ? ?, then C??(D) = FD1,|D|+1.

In the remainder of this paper, a pattern mining problem will be represented by two entities. The first component is the underling framework composed of the pattern space, the data space, the generalization and instantiation relationships.

Thus, the generalized framework stood for the quadruplet ??,?,	,??. The second component consists of the pattern family FD?1,?2 that represents a category of patterns contained in the data set D, subset of the data space, and that have support within the interval [?1, ?2[. This family of patterns is also represented by the triplet ?D, ?1, ?2?.

B. Frequent pattern mining and Rare pattern mining instances of the generalized framework  Frequent pattern mining and rare pattern mining are a special case of pattern mining and can be modeled using the framework defined above. In one hand, the problem of frequent pattern mining consists at looking only for patterns with     support at least equal to a fixed threshold. More formally, it is related to the extraction of the family of patterns FD?1,?2=|D|+1.

In other hand, in rare pattern mining we are exclusively looking for non common patterns: patterns with a frequency smaller than a fixed threshold. The problem is equivalent to extracting the family FD?1=0,?2 of patterns. In both cases (frequent pattern mining and rare pattern mining), both data set and pattern spaces are reduced to sets of itemsets, i.e., ? = ? = 2I , while 	 and ? boil down to set-theoretic inclusion. Hence the mining goal amounts to finding all the subsets of a family of sets contained in D ? ?. In other words, in case where the data and pattern spaces are composed of items without generalization among them, the frequent and rare pattern mining problems will be instantiated from the generalized framework by the quadruplet ?2I , 2I ,?,?? and by the pattern families FD?1,?2=|D|+1 and FD?1=0,?2 .



V. RARE ITEMSET MINING  A. Apriori for Rare Itemset Mining : The main idea  Our claim is that all non-frequent itemsets, also called rare itemsets, contained in a transaction database can be efficiently mined using an Apriori-like approach by traversing the itemset lattice in a reverse way compared to the traversal performed in the classical Apriori approach. The mining approach benefits of the same advantages as Apriori does. In other words, the backward traversal method is endowed with a property that leads to prune out potentially non-rare itemsets in the mining process. Starting from this assumption, we show how the approach is effective and practical for finding all rare itemsets using an example and then we present the theoretical foundations of the approach.

Hereafter, we give the intuition behind our approach to mine rare itemsets using an Apriori-like approach. Let D be the transaction database presented in Table I, and I the set of items contained in a transaction of the database D and |I| the cardinality of I. Our goal is to find the set of items, also called itemsets or itemsets, that are present in at most two transactions. The number of times an itemset occurs in the database is called the itemset support. In our case the maximum support is set to 3.

To find rare itemsets of size k ? 1, also denoted (k ? 1)- rare itemsets, with support under 3 we start by looking for rare itemsets with size from |I| to k. In our example, to find the 1-rare itemsets we have to find the 5-rare itemsets, 4-rare itemsets, 3-rare itemsets and 2-rare itemsets. The initialization goes through a two stage process. It begins by generating ?abcde?, the largest itemset, composed of all items in I, this itemset is also called a candidate of size 5 or 5-candidate. As we can notice this itemset is not present in any transaction which means its support is null (suppD(?abcde?) = 0) and by then is considered as a rare itemset. This itemset forms the seed for the next step. After that, we generate the 4-candidates from the rare itemset ?abcde? obtained in the previous stage by removing one item at once from it. That is, when we remove the item a we obtain the candidate ?bcde?, and by removing items b, c and d we obtain the candidates ?acde?, ?abde?, ?abce? and ?abcd?, respectively. The support of each  of those 4-candidates is either 0 or 1 which is less than the fixed maximum support. Consequently, all the 4-candidates are 4-rare itemsets. The initialization process ends after generating the 4-rare itemsets, then starts a recursive process to generate the remaining itemsets. The 3-candidates are generated by intersecting rare itemsets of size 4 that have in common 3 items. For example, ?cde? is generated by intersecting ?acde? and ?bcde?, and ?abc? is the result of the intersection of the rare itemsets ?abcd? and ?abce?. By further combining the 4-rare itemsets we obtain the remaining 3-candidates: ?bde?, ?bce?, ?bcd?, ?ade?, ?ace?, ?acd?, ?abe?, ?abd?. Once again, we check the support of each 3-candidate and keep only rare ones. The only candidate that is discarded from the set of 3- rare itemsets is ?abc? because its support (3) is greater than the fixed maximum threshold. Similarly to the previous step, the 3-rare itemsets are combined in such a way to generate the set of 2-candidates by intersecting itemsets having 2 items in common. For example, the intersection of the 3-rare itemsets ?cde? and ?bde? is the 2-candidate ?de?. However, the itemsets ?bc?, ?ac?, ?ab? are not considered at all due to the fact that they are subitemsets of the non-rare itemset ?abc?. Indeed, both ?bc?, ?ac? and ?ab? have a support equal to 3. This principle is also called candidate pruning where the itemsets that are subitemsets of a frequent itemset are not considered in any combination operation (see Section V-B.2). The 2-rare itemsets generated at this stage are ?de?, ?ce?, ?cd?, ?be?, ?bd?, ?ae?, ?ad?. In the last step, the 1-candidates are generated by intersecting the 2-rare itemsets obtained in the previous step.

Due to the pruning step, the itemsets ?b?, ?c? and ?d? are not considered and we can easily verify that the support of both ?b?, ?c? and ?d? is 4. The only 1-rare itemset is ?e?. The process can stop for two reasons : (1) either there is no more candidates can be generated or we reach a level where all rare itemsets are composed of only one item. In our case, the process stops because no more candidate is generated from the 1-rare itemset ?e?.

This example confirms our initial assumptions and encour- age us to explore further the approach and build a novel method for rare itemset mining on a top of a generalized theoretical framework for itemset mining we propose.

B. Apriori for Rare itemset Mining : the details  1) Overview: What makes Apriori interesting is the ability to avoid the exploration of the whole itemset space. In fact, a great amount of non-frequent itemsets may be discarded in the mining process and only itemsets that are likely to be frequent are explored. The key idea is that if an itemset is non-frequent so are all its supersets. Such property is called the anti-monotone property.

To prune non-frequent itemsets, Apriori exploits the anti- monotone property on a top-down traversal of the itemset space. Hence, the patterns that contain a non-frequent itemset are pruned out from the mining space and only potential candidates are further considered for support test.

In the case of rare itemsets a top-down traversal of the item- set space can not benefit of such property to prune out non-rare itemsets. In fact, the top-down traversal of the itemset space     is not endowed with an equivalent anti-monotone property to prune non-rare itemsets. That is, apriori no conclusion can be made about the frequency of an itemset just by knowing that its subsets are non-rare itemsets. However, an interesting property is that all supersets of a non-rare itemset are not rare.

Our idea is to exploits the later property to prune non- rare itemsets on a bottom-up traversal approach. Indeed, when exploring the itemset space from the bottom to the top, if an itemset is not rare than all its supersets are not rare and by then are pruned out in the mining process. In this respect, instead of growing itemsets to obtain longer ones such as done in the classical Apriori approach, we do the inverse : reducing itemset sizes at each step starting from the itemset that contains all the items considered in the data set.

In the next section we factorize the principles behind the classical Apriori framework and propose a generalized representation which by means of a specific instantiation lead to formalize the Apriori rare itemset mining approach based on the principle enounced below.

2) Apriori framework: Level-Wise traversal of the pattern space: An Apriori-  based approach to mine interesting patterns have to traverse the pattern space and iteratively generate patterns at a level based on the interesting patterns of the precedent level. To determine a level, a range measure need to be defined. Usually, such measure is closely related to the generality relationship among patterns and defined as a function from ? to N. In other words, the pattern space is layered and then traversed by moving from a level to another. That is, a level is composed of a set of patterns having the same rank value. When traversing the sliced pattern space according to a rank function, all interesting patterns will be reached (see Proposition 5.1). The interesting patterns obtained in each pass are used to generate candidate patterns for the next pass. The support of each candidate is then counted, and the interesting ones are kept. This process continues until no new interesting patterns are found or no new candidates are generated.

Proposition 5.1: Given a pattern framework ??,?,	,??, the pattern family FD?1?2 such that the pattern space ? is endowed with a rank function ?, and a partial order that preserves the following monotone property: a pattern at level y is interesting if all the associated patterns at level x w.r.t.

the partial order are interesting, then all patterns in FD?1?2 can be discovered by traversing the levels defined by ?.

Pattern composition: Now, to generate a pattern at a level, a pattern operation that merges two or more interesting patterns of a precedent level is performed (see Definition 5.1).

Definition 5.1: A pattern operation w.r.t. a pattern fam- ily is defined as an n-array relation that associates for a given pattern and a list of arguments another pattern in the same pattern family space. More formally, given the pattern framework ??,?,	,??, the family of patterns FD?1,?2 , and a pattern f in FD?1?2 . A pattern operation on f and the list of arguments arg1, arg2, ..., argn w.r.t. to FD?1,?2 , denoted by P(f, arg1, arg2, ..., argn) is a pattern that belongs to FD?1,?2 .

The set of pattern operations will be represented by Op.

In Definition 5.1 a pattern operation involves a list of arguments whose semantics are not presented for the sake  of generality. Thus, specific arguments with precise semantics have to be set for each pattern operation. For instance, the arguments can be, but not exhaustively, data related to a pattern or to a pattern component. Based on Definition 5.1, we present two kind of more specific operations : (i) generalization opera- tions (see Definition 5.2) and (ii) specialization operations (see Definition 5.3). Whereas generalization operations lead to a more general pattern than the input patterns, the specialization operations lead to a pattern that is more specific than the combined ones.

Definition 5.2: A pattern specialization operation w.r.t.

FD?1?2 is an element of Op that associates to a given pattern a more specific one. In other words, if we denote this operation by Ps then we have Ps(f) 	 f . The set of specialization operations is denoted by the symbol Sp.

Definition 5.3: A pattern generalization operation w.r.t.

FD?1?2 is a relation from Op that associates to a pattern a more general one. That is, considering the pattern f ? FD?1?2 and Gop denotes the generalization operation, then f 	 Sop(f).

Generalization operations are represented by the set Gp.

Finally, based on the definitions of pattern specialization and pattern generalization operations, we define two more operations that combine two or more patterns and results a patterns which is wether more general or more specific than the combined ones (see Definition 5.4 and Definition 5.5).

Definition 5.4: A combining specialization operation, de- noted by CSP , is a specialization operation that transforms two or more patterns and the result is a pattern which is more specific than the involved patterns. That is to say, CSP is in Sp and if the patterns f1, f2, ..., fp can be combined w.r.t. CSP than CSP (f1, f2, ..., fp, argp+1, argp+2, ..., argn) 	 f ? such that f ? ? {f1, f2, ..., fp}.

Definition 5.5: A combining generalization operation, de- noted by CGP , is a generalization operation that transforms two or more patterns and the result is a pattern which is more general than the involved patterns. For instance, CGP is in Gp and if the patterns f1, f2, ..., fp can be com- bined w.r.t. CGP than ?f ? {f1, f2, ..., fp} we have f CGP (f1, f2, ..., fp, argp+1, argp+2, ..., argn).

For example, as shown in Section III the classical Apriori approaches generate a pattern at a level k by merging two patterns of level k-1 that share common k-2 items. The merge operation is reduced to the set union which is a special case of CGP operations.

Pruning principle: It is noteworthy that the move from a level to another can be optimized. For example, some of those optimizations could be to refine the pattern operations by defining canonical operations and by then reduce the redundancy when generating the patterns or by reducing the amount of the candidates that are generated at each pass of the mining process by pruning out candidates that are likely to be not interesting.

The pruning principle is generally based on an anti- monotone property defined on the top of the pattern space en- dowed with a partial order (see Definition 5.6). This property may state that some patterns will certainly not be interesting if we do already know that some patterns related to them are not interesting. Doing so, the set of candidates may be     considerably reduced. In fact, instead of taking into account all possible combinations of the interesting patterns at a level by means of the combination operations, only subset of it will be considered and the remaining subset will be the patterns that are related to the non-interesting patterns of the precedent level.

Definition 5.6: (Anti-monotone and monotone properties).

A property p defined on top of the pattern family FD?1?2 is monotone if and only if whenever a pattern f ? FD?1?2 satisfies p, so does any pattern that is a successor of f w.r.t. the partial order defined on top of ?.The property p is anti-monotone iff whenever a pattern does not satisfies p, so does any super- pattern of f in FD?1?2 .

The pruning principle enounced in Proposition 5.2 leads at the reduction of the patterns space. Indeed, in the traversing process only interesting patterns discovered at a level that yield potentially interesting patterns at the next level are considered.

Proposition 5.2: Given a pattern framework ??,?,	,??, the pattern family FD?1?2 and the binary relation R a par- tial order on ? elements (R is reflexive, antisymmetric and transitive), then it is possible to prune out part of the pattern space composed of non-interesting patterns during the mining process if it exists an anti-monotone property on ? w.r.t. R.

To sum up, the generalized Apriori framework is defined by the following elements:  ? Pattern framework: ??,?,	,?? that define the pattern space, the data set space, the generality and instantiation relationships,  ? Pattern family: ?D, ?1, ?2? or FD?1?2 representing the category of patterns to be mined,  ? Rank measure: a measure ? to divide the pattern space into levels,  ? Monotone property on the interestingness of patterns: to ensure the completeness of the process of interesting pattern discovery.

? Anti-monotone property: necessary to prune out poten- tially non interesting patterns,  ? Pattern operation: to move from a level to another by combining interesting patterns.

3) Apriori Rare instantiates the generic Apriori framework: From the generalized Apriori framework, we instantiate an Apriori framework for rare itemset mining as follows. The pattern framework we consider is constituted of the quadruplet ?2I , 2I ,?,?? where I is a set of items present in the data transactions, both the itemset and data languages are limited to itemsets, and the generalization relation among itemsets is reduced to the set inclusion relationship, so is the instantiation relation among itemsets and data sets. The second component is the itemset family FD0? where D represents the transaction database and ? the maximum support threshold. The rank of an itemset is its cardinality (number of items of the itemset) and the itemset space is explored from the higher-rank level to lower-rank level according to the monotone property: all supersets of a rare itemset are rare, in let us say that if an itemset of level k is rare, so does all its supersets at level k + 1 which guaranties to reach all rare itemsets in the traversal process. Furthermore, an itemset of level k is  obtained by merging two rare itemsets of level k + 1 and the pruning principle given hereafter is used to check that the remaining subsets are also rare. The operator used to combine the itemsets consists of the set intersection operator (see Definition 5.7). For the pruning principle, we use the following anti-monotone property:all subsets of a non-rare itemset are non-rare.

Definition 5.7: Let D be the data set, ? an integer, and FD0,? the itemset family. An intersection-based generalization operation, denoted by CiGP , is a combining generalization operation that takes two itemsets having k + 1 items as parameters and generate a itemset that is the intersection of those itemsets if they share k items. For instance, CiGP is a CGP and if f1, f2 two itemsets from FD0,? then CiGP (f1, f2) = f1 ? f2 only if |f1| = |f2| and f1, f2 share |f1| ? 1 items. As a result CiGP (f1, f2) ? f1 and CiGP (f1, f2) ? f2.

Once the Apriori-based framework is instantiated, we build an Apriori algorithm, called AfRIM for Apriori Rare item- set Mining (see Algorithm 1) to mine rare itemsets. More specifically, the algorithm performs a level-wise descent in ???,	???. The starting point is the computation of the unique N -rare candidate and the (N ? 1)-rare itemsets whereby candidates comprise all itemsets having N ? 1 items (line 9 and 11). Then, the CANDIDATETEST() procedure computes the support of each candidate and returns only itemsets having support less than the fixed maximum support (maxsupp = ?) (line 10 and 12).

For each of the subsequent levels k, the candidates are generated by combining rare (k + 1)-rare itemsets (line 16).

In the prune step, a candidate f ? f ? is inserted only if all its (k+1)-supersets (supersets of cardinality k+1) occur in Fk+10,? (line 17). Then, the support of the Fkc candidates is counted (line 19). All candidates that turn out to be rare are inserted to FD0,? .

The process stops whether the set of generated candidates is empty (Fkc =) or rare itemset is generated at the level k. The example presented in Section V-A follows the AfRIM process when looking for rare itemsets on the database of transactions presented in Table I.

4) Implementation: The Apriori rare algorithm, AfRIM, has been implemented in java jdk.1.6.0. In order to efficiently count the support of a candidate, we used a hash-tree [2] that stores the candidate itemsets. A node of the hash-tree either contains a list of itemsets (a leaf node) or a hash table (an interior node). In an interior node, each bucket of the hash table points to another node. The root of the hash-tree is defined to be at depth 1. An interior node at depth k points to nodes at depth k + 1, and itemsets are stored in leaves.

As the implementation of Arima is not available, we have developed a java implementation based on the pseudo code provided in [11]. As for our algorithm, we have also used a hash-tree to count the candidate support.



VI. EXPERIMENTATIONS AND PERFORMANCE STUDY  The experiments were run on an Intel Pentium4 2.66Ghz processor with 1000MB main memory running Windows XP professional. The synthetic data set are generated using a     Algorithm 1 AfRIM 1: Input: 2: D, ?; ? Set of transactions, support threshold 3: CD0,?; ? itemsets contained in D 4: Output: 5: FD0,?; ? Set of frequent itemsets  6: Initialization: 7: I ? set of items contained in the transactions of D; 8: N ? |I|; 9: FNc ? I; ? N -candidate  10: FN0,? ? CANDTEST(FNc ,D); ? rare N -rare 11: FN?1c ? {f ? {i} : f ? FN0,? ? i ? I}; ?  (N ? 1)-candidates 12: FN?10,? ? CANDTEST(FN?1c ,D); ? rare (N ? 1)-rare  itemsets  13: Method: 14: for (k = N ? 2;Fk+10,? ?= ?; k ??) do 15: Fkc ? ?; 16: Fkc ? {f ? f ? : f, f ? ? Fk+10,? ? |f ? f ?| = k}; ?  (k)-candidates 17: Fkc ? Fkc ? {f : f ? Fkc ? ?f ? ? (CD0,? ? Fk+10,? ) ?  |f ?| = k + 1 ? f ? f ?}; 18: ? prune out the non-rare (k)-candidates 19: Fk0,? ? CANDTEST(Fkc ,D); ? rare k-rare itemsets 20: FD0,? ? FD0,? ? Fk0,?; 21: end for 22: return FD0,?;  method provided by KDD Research Group in IBM Almaden Research Center 1. The data set files has 100.000 transactions and 56 items with an average of 8 items per transaction.

In this paper, we report three sets of experiments in which the runtime of AfRIM algorithm is compared with the Arima algorithm based on the data set size (see Figure 2), the maximum support (see Figure 3) and on the number of items considered (see Figure 4) . When measuring runtime with respect to the data set size we kept the number of items set to 8 and the maximum support threshold set to 10% and varies the size of the input data set file from 1000 to 100.000. Similarly, to measure the runtime w.r.t. the maximum support, we kept the number of transactions set to 10.000 and the number of items set to 8 while the support is decreased from 40% to 1%.

In the third set of experiments we set the maximum support threshold to 20% and the number of transactions set to 10.000 while the number of items varied from 2 to 56. For the sake of presentation, in this paper we only report the results where the number of items is varied from 2 to 10. That is to say, as the number of items grows and oversteps 10, the runtime of Arima drastically increase and AfRIM is several dozens of magnitude faster that AfRIM. For example, when the number of items is 12, Arima takes about 1193000 milliseconds whereas Apriori- rare AfRIM spends only 25079 milliseconds making it more than 47 times faster than Arima.

1http://www.almaden.ibm.com/cs/quest/syndata#AssocSynData    ARMP  Arima  m ill i.s )     ru nt im  e( m  0 20000 40000 60000 80000 100000 dataset size  Fig. 2. Time variation with number of transaction in the data set           ARMP  Arima  maxsupp(%)  ru nt im  e( m illi .s)  Fig. 3. Time variation with maximum support threshold  The three figures show that the proposed algorithm is faster and performs better than the existing algorithm some times by several degrees of magnitude than the existing one. We believe that the main reason for this big difference in performance is related to the fact that Arima have not only to run the classical Apriori algorithm to find the minimum rare itemsets but also run the Arima algorithm to find the rest of rare itemsets by analyzing the MRP generated by the modified Apriori.



VII. CONCLUSION AND FUTURE WORK  In this paper we propose two frameworks. A structural framework to represent the different categories of patterns based on the frequency constraint which by means of an instantiation process leads to the representation of frequent and rare pattern mining problems. A mining framework that consists on an abstract model that factorizes the classical Apriori approach. Then, an Apriori method is instantiated to tackle the specific problem of rare itemset mining. The      ARMP  Arima  m ill i.s )        ru nt im  e( m  4 5 6 7 8 9 10 11 number of items  Fig. 4. Time variation with number of items     experimental results given in this paper show that Apriori- rare is several degrees of magnitude faster than the existing algorithm.

For frequent pattern mining several algorithms such as Eclat, FP-Growth [4], TM [10] have been proposed to improve the performances of the mining process and deal with some bottlenecks of the Apriori algorithm such as the candidate gen- eration and the support counting. However, those techniques are designed to mine frequent itemsets and do not seek for rare itemsets. We are currently investigating the adaptation of those techniques to the problem of rare itemset mining.

