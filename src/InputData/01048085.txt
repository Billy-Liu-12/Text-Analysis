Rule Generation With the Pattern Repository

Abstract   Efficient algorithms for mining frequent patterns are  crucial to many tasks in data mining. Since the Apriori algorithm was proposed in 1994, there have been several methods developed to improve its performance. However, most still adopt its candidate set generation-and-test approach. In addition, many methods do not generate all frequent patterns, making them inadequate to derive all association rules.

The calculation of association rules from raw itemsets using Apriori is an intractable problem.  By using a new structure called a Pattern Repository, the same rules can be derived in linear-time proportional to the number of unique items found.  If the selected rules are all we need, the calculation can give results in real- time.  In addition, the calculation can easily be divided into subsets for distributed processing and large datasets can be stored on disk that adds to the I/O overhead, but still offers a linear time calculation of rules.

1. Introduction   Data mining refers to the extraction of previously unknown and potentially useful information from large amounts of data.  Three fundamental areas of data mining are association analysis, clustering and classification. A fundamental part of the data-mining task is finding frequent patterns in a given dataset. Frequent patterns are ones that occur at least a user specified number of times (the minimum support) in the dataset. This technique allows us to perform essential tasks such as discovering association relationships among items, verify correlations, and perform sequential pattern mining.

The Apriori algorithm finds frequent patterns by employing a bottom-up search [1]. It generates candidate sets starting at size 2 up to the maximum frequent set size.

At each pass, it determines which candidates are frequent by counting their occurrences that exceed the minimum support threshold.  It then checks how often these candidates occur together to measure the support.  Due to  combinatory explosion, this leads to poor performance when frequent pattern sizes are large.

To avoid this problem, some algorithms output only ?maximal frequent? sets [2,3,4].  Max-Miner uses a bottom-up search and a heuristic to try to identify frequent sets as early as possible [2].  Pincer-Search uses a bottom-up search in conjunction with top-down pruning [4].  Even though performance improvements may be substantial, the ?maximal frequent? sets have important limitations. A complete set of valid rules cannot be extracted without complete support information of the frequent subsets.  Almost all previous algorithms use the candidate set generate-and-test approach.

Frequent Pattern (FP) Tree based mining is an exception [5].  It has performance improvements over Apriori since it uses a compressed data representation (nodes and a tree structure) and does not need to generate candidate sets.  However, FP-tree-based mining uses a complex data structure and performance gains are very sensitive to the support threshold setting.  In addition, large databases may exceed available memory space even when the data is compressed.

In a previous paper, the concept of a Pattern Repository was described [6].  The function of the repository is to read and store patterns from the raw itemsets in a compact form.  Later, the information about association rules could be derived from the Pattern Repository and calculated for display.  The Pattern Repository information allows pattern data to be extracted in an organized way, such that calculation time is spent only on possible rules and candidate generation in Apriori is avoided.  The resulting rule generation time is linearly proportional to the number of rules found and is not effected by the size of the database.

2. Related Work  2.1 Apriori   A good example of association rule mining is market basket analysis.  A transaction consists of a collection of       items purchased at a grocery store or a supermarket.

These are simply the items that went into the basket.  By finding association rules among the transactions, the retailers can derive some useful information about the customers buying habits.  This information could be used in turn to better market and advertise what the store sells.

We form association rules by using the Apriori algorithm as described below.

An association rule is an implication of the form X Y, where X and Y are each a set of items which occur together in a significant number of baskets.  The support S is the percent of the total transactions in which X and Y occur together.  The confidence C is the number of transactions where X and Y occur divided by the number of transactions where X occurs expressed as a percent.

An interesting rule is one where S and C meet some minimum threshold requirements referred to as the minimum support and minimum confidence respectively.

An example is as follows.  Assume the minimum support is 5% and minimum confidence is 50%. Bread and peanut butter are bought together in 5% of the transactions (S).

Whenever bread appears in a transaction, peanut butter also appears about 50% of the time (C).  Since these figures for S and C meet or exceed the minimums, then there is a valid rule of the form Bread  ??????????	? ???	 	????? Note that when peanut butter appears in a transaction and bread is bought only 40% of the time, the rule fails because C < 50%.

Most of the previous work in association rule development uses some form of Apriori.  The steps are as follows:  a. Find all frequent items. Scan the database to find the frequency of occurrence of each item and decide which ones are above the threshold (S).

b. Generate itemsets by checking all of the possible combinations of the frequent items to see if they meet the minimum support requirement. For example, if A, B and C are frequent items, then check if {A, B}, {B, C}, and {A, C} meet the minimum support and are thus frequent itemsets.

c. Generate longer itemsets by combining valid items from step b.  If the itemsets {A, B} and {B, C} are valid, then they can be combined into {A, B, C}.  Note that each item can appear only once and that it would not be useful to try patterns other than those found in step b.  If {A, B} or {B, C} were not a frequent itemset then {A, B, C} could not be one since {A, B} and {B, C} are part of it.

d. If longer combinations of rules are possible, then continue until no longer combinations can be generated.

e. Finally, check the confidence level (C) for each of the possible subsets of the itemsets to find the valid rules.

If {A, B, C} is valid then we must check A ??????????? {B, C} ????? ?????? ?!??"?????? ?! ???? ????? B}, {A, B} #%$ &(' )*),+ -?$ ./)*0213)*)*$?$ ./),13+ 4/+ 135/1 confidence level.

The above algorithm finds all of the length 1 frequent itemsets and then proceeds to find the length 2, 3, etc.

frequent itemsets.  Note that at each level, the frequency count of each frequent itemset must be retained so that the confidence level can be computed.  For instance, as shown in step 5 above, to compute confidence for rule A 687:9<;?=?>@??ACBEDGFIH?JEKL>NM O???PLBE??JEQ?RSK<MUTWV?JEXC687:9<;?=UY   If the database is large, then there are many frequent  items and many possible combinations.  This results in an exponential growth in the number of combinations to consider when generating candidates.  The retention of previous rules and search of new rules can become very expensive.  This is especially true if there are many long rules, which pass the support test.  Length 10 rules require 10 passes through the database and a search for every possible rule combination.  Some obvious ways to make this process cheaper include not scanning the entire database on each pass and eliminating the candidate generation.

2.2  The Frequent Pattern Tree   The FP-Tree [5] algorithm scans the database twice.

The first time is to determine the frequent items that will be used to create the FP-tree and sort items in frequency order. The second scan actually creates the tree as a graph.  The top node of the graph is the root.  The first node, underneath the root, is the most frequent item for each record scanned along with a count.  Many records, when sorted by most frequent items first, will contain the same most frequent item. The basic process involves laying out each record in frequence order and creating a node for each item under the root.  As more items are added, there will be common prefixes.  For instance, one record {A,B,C} has a common prefix with {A,B,D}, namely {A,B}.  Nodes are not repeated, but the counts for A and B nodes are incremented. When the C node is reached, a new node at the same level for C is created with the value D (assuming it does not already exist).

Note that non-frequent items are ignored in the FP-tree construction.  In addition, a linked list of frequent items is also maintained, thus every occurrence of A is linked to every other node with A in the tree.

The inherent advantages of this structure are the relatively compact representation of the database and the exclusion of non-frequent items.  This makes it easy to fit the FP-tree into memory and thus easy to scan for rule development.  After completion of construction, the tree is mined for frequent patterns as follows:   a. Derive a set of conditional paths.  These are suffix  patterns from the FP-tree.

b. Construct a conditional FP-tree for the conditional  paths.

c. Explore the conditional tree recursively to find the frequent patterns and determine the support level for each pattern found.

Note that the tree contains only frequent items. No  time is wasted with non-frequent items.  In addition, since the most frequent items are near the top or root of the tree, the mining algorithm will find most of the valid frequent patterns early in the search.

The FP-tree mining algorithm works well, but there are some limitations.

a. The database must be scanned twice.

b. Update of the database requires a complete repetition  of the scan process and construction of a new tree, because the frequent items may change with database update.

c. Lowering the minimum support level requires a complete rescan and construction of a new tree.

d. The mining algorithm is designed to work in-memory and performs poorly if a lot of memory paging is required.

To achieve the fastest rule development, it would be desirable to avoid scanning the database twice and eliminating the rescan requirement when an update occurs.

3. Rule Mining with the Pattern Repository   The use of the Pattern Repository [6] avoids extra database scans and the update problems mentioned in Section 2.2.  The Pattern Repository takes the itemset data in one or more batches and stores the itemset information in a compact form.  The Repository is always up to date in that any and all information inserted is immediately ready for use.  Each unique symbol in the itemset is replaced with a token, which is just a unique number.

Each itemset is replaced with a number representing its pattern in the database.  The patterns contain sufficient information to reproduce the items in each itemset and statistics about each item.

The primary purpose of the Pattern Repository is to deliver selected patterns and statistical information to calculate the association rules attributed to them.  The selected patterns refer to the ability of the Pattern Repository to deliver only those patterns, that contain one or more selected tokens.  As we shall see later, this greatly simplifies the association rule derivation.  We will select and count only those items, that contribute to the rules instead of generating and counting candidates.

3.1 Refinement of Pattern Repository Output   The basic output of the Pattern Repository is the patterns in token form.  Note that the patterns can be filtered by one or more tokens that might be present in  each pattern.  Thus we can specify the filter as all of the patterns that contain {12} or {2, 5, 12}.

This is a good starting point, but some improvements are necessary to prevent redundant processing of unnecessary items. The necessary refinements are as follows:  a. Removal of infrequent items based on the current support setting.  The infrequent items do not contribute to the association rules.

b. Removal of those items that are part of the filter.

Their presence is already assumed and the count is simply the number of records returned.

c. Sorting the tokens found in ascending order.  This allows elimination of duplicates when processing.

Otherwise the duplicates would be counted twice.

d. Counting of the frequent tokens found.

e. Allow specification of an extra filter parameter.

Removal of all tokens below a certain value, which will be called the minimum value token.

The above refinements facilitate counting just the tokens that are needed to derive the valid itemsets.

Otherwise there would extra overhead in counting tokens which do not contribute to the final rules or which have already been counted.

3.2 Association Rule Search With the Pattern Repository   The rule development with the Pattern Repository can be described as follows.  The search starts by iterating through each frequent item in the itemset using the token number in ascending order.  That is token ?1? is checked to see if it qualifies as a frequent item and then token ?2? and so on.  As each token is determined to be frequent, it is processed as follows:   a. The Pattern Repository extracts records that contain  the token and the counts of the other items in the transactions.

b. The other items that meet the minimum support level are put in numerical order and paired with the original token in a list. This list is permanently added to the rule list along with the counts.  This will be needed later to test which rules meet the minimum confidence threshold and thus are valid.

c. The list pairs are now processed one at a time in numerical order.  Once again, the Pattern Repository extracts those records that contain the token pairs and counts the tokens that meet the minimum support level.

d. Step (c) continues in a depth-first manner until none of the tokens found meet the minimum support threshold.

Assuming that the minimum support allowed it, the first rule explored would be {1,2,3,4,5,6, etc.}  No repeats are allowed.  To prevent exploration of repeats, tokens found that are less than the highest end token are also ignored.  The assumption is that numerically smaller combinations have already been       explored and thus a wasted repeat calculation.  For example, if {1,2,5,6} is explored then {1,2,3,4} has already been tried and failed.

e. When no tokens are found, the depth-first search returns to the previous level and processes the next itemset on the list.

f. This continues until all of the combinations found at all levels are tested. The process stops when all of the pairs found in step (b) are processed.

g. The process repeats for the next single token in the list starting with step (a).

The basic process is simply an exhaustive depth-first  search of all possible combinations.  Note that rules, which cannot exist, are not examined.  For instance if {1,2} never occurs, it will never be counted or placed on a search list.  In addition, repeat rules ({1,2} is the same as {2,1}) are not done again because of the numerical precedence.  Note that {1} and {2} counts are retained so we can decide if 1 ZW[L\@Z ]C^`_?_?aba cE_@^`d eEd ^Cf?^ confidence threshold.

3.3 Time Complexity of the Pattern Repository Search   The time complexity can be numerically expressed by looking at the steps involved in the search and the operations of the Pattern Repository.

a. If the items are maintained in a Hash table then the lookup of patterns in the Pattern Repository is O(1) or O(1 + g  depending on loading. If the lookup is done on a modern database the search would require log2(m) similar to a binary tree search where m is the number of unique items.  However in the case of a database, disk I/O would mask this.  Effectively, the overhead can be expressed simply as a constant and can be ignored.

b. The search iterates through each frequent item. So the starting point is n, which is the number of frequent items.

c. At each level in the depth-first search, we must assign some estimate of the percent of ?p? combinations that will meet the minimum support criterion and be used for the next level.  No true estimate can actually be done since the number is dependent on the actual data.

d. The search continues to some maximum depth ?q?, which again must be estimated.

The first level is n frequent items.  At the second level for each frequent item from level one we get some fraction of n items that pass the minimum support level and so on.  The resulting equation is n * (n * p2)(n * p3) ? (n * pq). The variable ?q? will be less than the number of frequent items.

If the value for ?p? is based on probability alone the following holds.  Assuming 10 items are taken 2 at a time, random probability would give 1/10 * 1/10 = 1/100.  100 items taken 3 at a time would be 1/100 * 1/100 * 1/100 =  1/1,000,000 etc.  On the average this is probably true.

Most of the tests for the minimum support level will fail because the appearance of items together in a transaction is in fact a result of probability.  The association rules that survive the test are in fact exceptions that exceed random probability.

An important point is that the (n * p2)(n * p3) ? (n * pq) part of the equation above will reduce to a constant.

For instance, if p is 1/10 and n is 1000 then (n * p2) = 10.

That is there are 10 rules generated for each item in the first level on the average.  However, if we continue to (n * pq) at some point this part will drop below 1.  Assume that this happens at (q ? 3), the equation becomes n * (n * p2)(n * p3) ? (n * pq - 4).  The parts at (q - 3), (q ? 2), etc are not tested because the list is empty.  The portion (n * p2)(n * p3) ? (n * pq - 4) is replaced with a constant C.

This implies that the number of rules tested is C * n.

How large is the constant? It is related to how much association there is in the database. For instance, if no rules pass the minimum support threshold, then C = 1.

We search for pairings at the second level, find none and stop.  In short the actual run-time is still very sensitive to the minimum support setting.  A lower setting would make the C larger and the search take longer.

3.4 An Example of Search With Pattern Repository   Taking a look at the example database in Figure 1, the first step of rule generation with the Pattern Repository skips the frequent item determination.  This information was stored when the Pattern Repository was created.  All that is required is iterating through each of the frequent items in their token order.  Thus if A, B, C, D, E, F, G are frequent items, they are replaced by their tokens 1,2,3,4,5,6,7. The pattern repository also reports 250 records in the system and these items are frequent based on a minimum support of 24% or 60.

Figure 1. A Sample Database Frequency List h i j?k l?m/nj?o pLq jsr?tsj?osuwvx y y*z?z{ | }?~ ? y*z ?? ? ?/~? ~ ?/?p ? y??z? ? y*?/~   The iteration proceeds as follows.

The starting tokens are processed in token order.  The  patterns which contain the ?1? token are retrieved from the pattern repository.  Since token ?1? occurs 100 times the equivalent of 100 patterns are retrieved.  The Pattern Repository will remove the infrequent items in the pattern       so they will be ignored.  Also the target token ?1? is removed since its presence is assumed in each pattern.  If an item is present in the patterns retrieved, the implicit assumption is that they are frequent and are not part of the target.  The minimum value token is set to 0. This means that any tokens are allowed which are not part of the filter and are frequent.

A count is made of all of the other frequent items that are contained in these patterns.  Note that the occurrence, the number of times that token ?3? occurs is counted.  If 75 occurrences of  ?3? are counted, the support of this itemset is 75/250 = 0.3 or 30%.  After all of the items are counted, those meeting the minimum support are stored along with the count. 30% meet the minimum support and itemset {1,3} is stored with a count of 75.  This is called a frequent itemset.

If the sets {1,3}, {1,5} and {1,7} are the only itemsets that satisfy the support requirement, then each is recursively explored depth-first.  Working in token order, all of the patterns of any length containing {1,3} are checked first.  Each successful frequent itemset and its count are recorded in a hash table.  These successful itemsets will be checked as described in Section 2.1 (Apriori step e.) to determine which of them meet the minimum confidence level and thus are valid rules.

In the recursive processing, we first look at all records that contain {1,3} and set the minimum token value to 4.

We already know that ?2? failed the support test and ?3? is already in the pattern.  Remember that the processing is being done in token order, thus the last item in the itemset specifies the minimum token value.

The assumption is that lower value tokens have already been tried in order and failed.  In the perfect case, the itemset would build as {1,2,3,4,5,6,7} assuming that all of the intermediate itemsets met the minimum support requirement while performing the depth-first search. In this case after {1,2} is checked recursively, then {1,3} is checked.  Then  {1,3,2} would come up since {1,2,3} is already valid.  We do not want to check this again. This is avoided by setting the minimum token value to ?4?.

Returning to our example, the tokens for the {1,3} filter are counted.  The only ones left are ?4?, ?5?, ?6? and ?7?, since the minimum token value is 4.  Tokens ?4? and ?6? are not counted because they are not present in the patterns.  Tokens ?5? and ?7? do meet support requirements. The frequent itemsets {1,3,5} and {1,3,7} are recorded.

Now filter {1,3,5} is checked and the only frequent itemset found is {1,3,5,7}.  This is recorded.  Then filter {1,3,5,7} is processed with no results and then filter {1,3,7} is processed with no results either.  They do not have any results because the minimum token value is ?8? (there are no tokens at or above 8).

The search falls back to filter {1,5}.  The only possibilities at this point are ?6? and ?7?. This process  will find {1,5,7} a frequent itemset and {1,5,6} will fail.

In fact we already know it will fail since {1,6} failed.  We could have kept a list of failed tokens, but the overhead in searching would be as bad as extra counting. Filter {1,5,7} will not find anything since there are no tokens above ?7?. The search falls back to {1,7}, which again finds nothing.

The next iteration starts with filter {2}, then {3} through {7}.  Note that the same rules apply as shown above.  We would not consider {2,1} since {1,2} has already been considered.

The final step would analyze the frequent itemsets for valid rules based on the minimum confidence level.  This can be done since each frequent itemset has a count.

The above process finds all of the association rules that the Apriori process would have found.  However, many of the expensive parts of the search in Apriori have been eliminated.  The only rules, which are present, are counted, and there is no candidate generation.   The database records are scanned only once.  The Pattern Repository handles database updating automatically.  The time complexity of this process is linear and proportional to the number of frequent items.

3.5 Scaling of the Algorithm   Note that the search routine described in Section 3.2 can be divided into discrete independent steps.  For example, the search can be divided among 4 processors.

One processor could look for all rules containing A.  The second processor could look for all rules containing B etc.

The load of each search could be split among the available processors.

In addition, the structures for the Pattern Repository can be translated to a disk based storage scheme, which adds overhead but does not substantially change how it operates.  A disk-based implementation of this algorithm has been tested and can handle several million itemsets with some increase in time overhead, but little impact in the linear time complexity of the algorithm.  A modern database or proprietary scheme can store the pattern repository information on disk and retrieve the same information using the best available indexing methods.

With this method, the size of the database is limited only by disk storage and indexing implementation.

4. Comparison of the Pattern Repository and the FP Tree   The Pattern Repository by design handles scaling and updating.  It can be divided among several processors and can be moved to disk storage for large databases.  In- memory operations are not a requirement.  The updating process simply adds new data to the repository.

The Frequent Pattern Tree design requires an in- memory operation and is not updateable. If support levels or new data are present, the entire database must be read twice again.  However, when all of the rules are processed with one database that fits in memory and with one minimum support value only, the FP-Tree consistently works faster than the Pattern Repository based rule development.

The real purpose of the Pattern Repository is for (1) targeted rules of the form ?all rules that include A, B or C? and (2) targeted rules in which the minimum support value will change.  Testing, discussed in the next section, will show that for commonly available Pentium desktops all of the rules that contain ?A? in a 100,000 record set can be solved on the average in about 1 second.  In addition, if we analyze for 3 different support levels or 3 different sets of targeted rules, it will take about 3 seconds.

The FP-Tree approach must rescan the entire database and process all of the rules to get the same results.  When the support level decreases, the whole process will have to be repeated. Since each process under test conditions takes about 20 seconds for 100,000 records, it will be much slower for these types of problems.

5. Testing   The testing platform was a Pentium 4 (1.5 gigahertz) with 500 megabytes of memory.  The operation system was Windows 2000 professional. Java JDK 1.3 was used to implement in-memory versions of the Pattern Repository and the Frequent Pattern Tree.  The FP-Tree Java implementation was based on a technical report [7] of the algorithm and was provided by Hao Huang at the Colorado School of Mines.

The dataset was generated using the algorithm described in [7].  It has 100,000 records and an average transaction size of 25.  The maximal potentially frequent item size on the average was 20.  The number of transaction read was varied from 10,000 to 100,000 for the test (see Figure 2).

Figure 2. Processing Time in Seconds ??? ???s?w?s?w??? ?/?s? ?s?s? ??? ?s? ?L?L? ? ???G???????s???8? ? ?/? ? ?*??? ????? ? ? ????? ????? ? ? ????? ????? ? ? ? ??? ????? ??? ? ? ??? ????? ?L? ? ?*????? ????? ??? ?    The testing involved targeted rules of the form of all rules containing ?A?, ?B? or ?C?.  The runs were repeated using a random combination of 3 frequent items and averaged.  The support level was set at 2.5% of the number of transactions.   There is no practical way to subdivide the problem with the FP-Tree so just the processing time for all rules is reported.

The advantage of the Pattern Repository is evident for targeted rules.  Note that this same advantage would apply if the support level were changed to 3 different values instead of searching for 3 different targeted rules.  This would require 3 runs of the FP-tree algorithm or about 60 seconds for 100,000 records.  The rules derived at 3 different support levels with the Pattern Repository would still take about 3 seconds.

6. Summary   The proposed method of calculating association rules has several advantages.   The speed of the calculation for targeted rules is fast enough to respond to the user in real time.  An even faster response can be obtained if the depth of the search is limited.  Adding new records does not create any update problems, which are common in existing implementations.  The system is scaleable using multiple processors or disk storage.  These scaling routines exact few penalties in terms of time complexity.

