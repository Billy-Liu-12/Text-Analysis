An Improved Apriori Algorithm Based on Pruning  Optimization and Transaction Reduction

Abstract?The paper analyzes the basic ideas and the shortcomin- gs of Apriori algorithm, studies the current major improvement strategies of it. In order to solve the low performance and efficie- ncy of the algorithm caused by its generating lots of candidate sets and scanning the transaction database repeatedly, it studies the pruning optimization and transaction reduction strategies, and on this basis, the improved Apriori algorithm based on pruning optimization and transaction reduction is put forward.

According to the performance comparison in the simulation experiment, by using the improved algorithm, the number of frequent item sets is much less and the running time is significan- tly shortened as well as the performance is enhanced then finally the algorithm is improved.

Keywords- Apriori;   frequent itemsets;   pruning optimization; transaction reduction

I.  INTRODUCTION Association rules mining is an important research content  in the field of data mining. It helps to find the association relationship among the large number of database items and its most typical application is to find the new useful rules in the sales transaction database, which reflects the customer purchasing behavior patterns, such as the impact on the other goods after buying a certain kind of goods. These rules can be used in many fields, such as customer shopping analysis, additional sales, goods shelves design, storage planning and classifying the users according to the buying patterns, etc.

Apriori algorithm is the most classical basic one of the algorithms in realizing the association rules mining. The algorithm has its unique advantages in looking for frequent itemsets, however, it is with the low performance and efficiency caused by generating lots of candidate sets and scanning the transaction database repeatedly in the execution process, which is the main disadvantage of Apriori algorithm.

The continuous research and improvement on the Aprior algorithm will promote the algorithm of association rules mining more mature and more efficient, push forward the association rules mining?s successful application in commercial fields and its broader prospect.



II. BASIC CONTENT AND ANALYSIS OF APRIORI  A. Basic Content of Apriori Apriori is a classic algorithm for learning association rules  in data mining, proposed by R.Agrawal and R.Srikant in 1994[1].there are two properties: ?all nonempty subset of a frequent itemset must also be frequent; all superset of non- frequent itemset must also be non-frequent? ,the properties[2] is used in Apriori algorithm to scanning the database , resulting in Boolean association rules frequent itemsets.

Specifically ,Apriori uses an iterative search method layer by layer, where k-dimensional itemsets are used to explore (k+1)-dimensional itemsets. First ,the set of frequent 1- dimensional itemsets is found and denoted L1, Next,L1 is used to find L2,the set of L2 frequent 2-itemsets ,which is used to find L3,and so on ,until no more frequent k-dimensional itemsets can be found .Finally, getting the rules from large set of data items. how Li-1 is used to find  Li is consisting of two- step process ,join and prune actions as followed:  1) The join step: Join Lk-1 with itself ,than combine the same extension item appeared to generate a possible candidate k-dimensional itemsets ,this set of candidates is denoted Ck ,Ck ? Lk .

2) The prune step: Scan the database to determine the count of each candidate in Ck. When the count is less than the minimum support count, it should be delete from the candidate itemsets. meanwhile ,if any (k-1)-dimensional subset of a candidate k-dimensional itemsets is not in Lk-1,the the candidate cannot be frequent either ,after this we can get the k- dimensional itemsets ,which is denoted Lk.

B. Algorithm  Analysis Apriori algorithm is a well-known association rules  algorithm and it has its unique advantages in mining frequent itemsets, has been used for most commercial products ,but at the same time, as the classical association rule extraction algorithm ,Apriori also has its shortcomings:  1)Algorithm will spend a large overhead to deal with large candidate set ,the generation from Lk-1 to Ck is exponential growth , so if judge the frequent itemsets before the generation         of the candidate frequent itemsets Ck, we can avoid creating an invalid candidate itemsets, reducing the scan database time .

2)Many repeat comparison of itemset in join step ,and the join may result a large set of candidates, if it can avoid repeating the comparison ,excluding the possibility of invalid candidate, the efficiency of the algorithm can be improved.

3)Repeatedly scanning the transaction database requires a lot of I/O load. After generating candidate frequent items it also go back and scan the database to determine whether these candidates are frequent itemsets. in the process of scanning ,the length of some items is small can not support the generation of frequent itemsets, but still repeat scan , so there is room for improvement.



III. THE MAINLY IMPROVED STRATEGIES OF APRIORI Since the Apriori algorithm was proposed , many  researchers make a in-depth research and  present some effective improvement methods.

A. Data Partitioning Method In order to reduce the number of database scans , Savasere  proposed the division method based on partition[3]. the database id divided into many segments, the core idea is :the frequent itemsets on the database at least one segment in the database is frequent ,then the union set of fenquent itemsets of each segment is the set of potential frequent itemsets.

the algorithm consists of two steps: 1)The algorithm subdivides the transactions into many  nonoverlapping partitions, small partitions can be completely loaded into memory, so you can call any sort of frequent itemsets mining algorithm to exploit the frequent itemsets in each partition. this requires scanning a trip to the database.

2)Together the frequent item set of each segment ,the result is generate a giant candidate sets on the entire database .this requires re-scan the database to verify whether each candidate is a real set of frequent itemsets.

therefore, using the data partitioning method ,the whole process only scan the database twice.

B.  Hash-based Technique The DHP algorithm on behalf of  hash-based optimization  method proposed by Park[4] mainly, can be used to reduce the size of the candidate k-dimensional itemsets, Ck, for k>1.

The basic idea is :when scanning each transaction in the database to generate the frequent k-itemsets from the candidate k-dimensional itemsets ,at the same time ,produce (k+1)- itemsets for each transaction ,then hush them to the different hash buckets of a hash table structure, and increase the corresponding bucket counts ,so that when produce the candidate (k+1)-dimensional itemsets can exclude some meaningless candidate set according to the minimum support threshold.

C. Sampling Sampling-based approach was first proposed by Mannila et-  al, to further improve by Toivonen et al[5].

The basic idea of the sampling method is to pick a random sample S of the given data D, and then search for frequent itemsets in S instead of D. Because we are searching for frequent itemsets in S rather than in D, it is possible that we will miss some of the global frequent itemsets. To lessen this possibility, we use a lower support threshold than minimum support to find the frequent itemsets local to S.

This method suitable for requirements for higher efficiency of mining [6]but not too high accuracy of mining environments.



IV. AN IMPROVED ALGORITHM BASED ON PRUNING OPTIMIZATION AND TRANSACTION REDUCTION  A. Pruned Optimization Strategy Property 1: In K-dimension frequent itemset X = {i1,  i2, ??? ,ik} ,  if there is a item named ij which can meet the Count (ij) <k-1, then X is not a frequent itemset.Among them, j < k, and Count (ij) means the number of itemsets containing the item ij in the collection of k-1-dimension frequent itemsets named Lk-1.

Proof: Suppose X is a K-dimensional frequent  itemset, then all the K K-1-dimensional subsets of X are in  Lk-1, and every item of X named ij will exit in k-1 subsets of X.It means that the number of itemsets containing item ij is k-1,so every item named ij of X could make the value of Count ij to be no less than k-1 because the collection named Lk-1 includes all subsets of X, but This contradicts with the condition, so the assumption does not hold, so X is not a frequent itemset.

Pruned optimization strategy: In generating K- dimensional candidate itemsets, on the basis of the existing (K- 1)-dimensional frequent itemsets, we can use a temporary table to count the frenquency of the items all in the frequent itemsets,if the frenquency of an item is less than K-1, this item can not be used to generate the k-dimensional itemsets, so we can delete all the frequent itemsets containing the item in Lk-1, this can effectively reduce the scale of Lk-1 and generate Ck more efficient.

For example, make association rules mining in a simple transaction database, as below:    Figure 1.  Execution process map  after adding a temporary table.

In the above example, if we use  Apriori algorithm directly, it means the lack of the temporary table T1 in  TID T1 T2 T3 T4  Items 1,3,4 2,3,5  1,2,3,5 2,5  Scanning D IS  SUP  5 3  D:SUPPORT=2      IS ---- ItemSets  Compare with support  IS  SUP  C1 L1  Generate C2  IS 1,2 1,3 1,5 2,3  SUP  2,5 3,5   Compare with support  IS 1,3 2,3 2,5 3,5  SUP  Count  Item  Frequency  C2 L2' C1 IS 2,3 2,5 3,5  SUP  L2  IS 2,3,5  SUP  Delete the itemsets containing item 1  Compare with support IS  2,3,5 SUP   C3 L3            step (2) ,so the set of 2-dimensional frequent itemsets is {{1,3},{2,3},{2,5},-{3,5}}, therefore, all the possible  3- dimensional  candidate set is {{1,2,3}, {1,2,5}, {1,3,5}, {2,3 ,5} },Then  scan  the  transaction database  D  and  compare the frenquency of candidate itemsets with the mini?support, finally, obtain the conclusion that  3- dimensional frequent itemsets is {2,3,5};In another way, if we use the Pruned optimization strategy for Apriori , the process will like this :we have added a temporary table T1 to count the frenquency of the items all in the 2-dimensional  frequent itemsets,then we can see the frenquency of the item named 1 is less than two, so we can delete all the itemsets containing the item named 1,delete itemset {1,3}in fact, there are {2,3},{2,5} and {3,5} remained,then generate the only  3-dimensional candidate itemset {2,3,5} ,through the scanning databases and comparative support, get the final 3-dimensional frequent itemsets  {2,3,5}. Therefore, it can be found that using pruned optimization strategy can filter out  non-frequent candidate itemsets {1,2,3}, {1,2,5} and {1,3,5}, Thus avoid a lot of  scanning the database and compar- ison to determine and   eliminate   non-frequent candidate itemsets, reduce unnecessary time cost[7], This effect will be more clearly reflected in a large transaction database.

B. Transaction Reduction Strategy Property 2: For generating K-dimensional  frequent item-  sets, if the length of a transaction is less than k, thetransaction is not necessary to scan.

Proof: generating  K-dimensional frequent itemsets  must scan the transaction database to calculate the frequency of fre- quent itemsets,if the length of a transaction is less then K, it means the number of items it contains is no more than K, it indicates that the transaction can not contains the K- dimensional frequent  itemsets[9], so there is no need to scan this transaction.

Property 3:Recorded Sk is the set of all items which  K- dimensional frequent itemsets contains,when scan the database for generating  K+1-dimensional frequent itemsets , any item named a including in (Sk-1 - Sk) is not necessary to scan in all transactions.

Proof: For any item named a including in (Sk-1 ? Sk), a is not exist in  K-dimensional frequent itemsets, furthermore,a is not exist in  K+1-dimensional frequent itemsets certainly,so a can not support the generation of K+1-dimensional frequent itemsets, there is no need to scan the item a.

So, according to property 2 and property 3 ,we can design the following optimization strategy:  Transaction Reduction strategy:When scan the database for generating  K+1-dimensional frequent itemsets, calculate the length of all transactions, delete the transaction whose length is less than K+1 directly, for the remaining transactions, delete the items including in (Sk-1?Sk),and re-calculate the length of every transaction, if a transaction?s length is less than K+1, delete the transaction in order to compress the transaction and reduce the scale of the database.

Therefore, when generate  the new K-dimensional frequent itemsets, take the Transaction Reduction strategy firstly.It will  compress the size of the transaction and reduce the scale of the database in the next database scanning. Especially, when the  value of K  is large, taking the transaction reduction strategy before scanning database will reduce the scale of transaction database constantly,and avoid a lot of repetitions scanning of short transactions, greatly reduce the time spent on the scanning of database,and finally improve the efficiency of the Apriori algorithm significantly.

C. The  Improved  Algorithm The improved algorithm is based on the classic Apriori  algorithm,and take Pruned optimization strategy to optimize the process of pruning in order to reduce the  generation of frequent itemsets, and meanwhile, take Transaction Reduction strategy to compress the transaction of database in order to reduce the scale of the transaction database tobe scanned.All of these will effectively decrease the system overhead and reduce the running time, improve the efficiency of Apriori algorithm significantly.

Here is the improved  algorithm named BE - Apriori.

BE - Apriori Algorithm: Input: transactiondatabase D, minimum support  min_sup Output: all the frequent itemsets denoted L of D The algorithm:  1 L1={ 1-dimensional frequent itemsets,} 2 for(k=2;Lk-1!= ? ;k++){ 3   if(k!=2){ 4      Count the frequency named |Lk-1(a)| of every item  named a Exists in Lk-1.

5       If (|Lk-1(a)| <k-1) delete all the itemsets  contained a in Lk-1 and update Lk-1.

6      } 7   for each l1?  Lk-1, for each l2?  Lk-1 8 { 9     if((l1[1]=l2[1] ? ??? ? (l1[k-2]=l2[k-2]) ?  (l1[k-  1]<l2[k-1])) 10     { c=l1 ? l2; 11       for each k-1 subset s of c{ 12         If( s ?  Lk-1) 13         { delete c ;}     Break ; 14       } 15       Ck=Ck ? {C} ; 16     for each t?D{ 17      If Count[t]<k delete t ; 18      Else{ delete the items of t  including in (Sk-1 ?  Sk)      and recalculate the length of t ; 19       If Count[t]<k delete t ; 20      Ct=subset (Ct, t) ; 21      for each c?Ct   c.count ++ ;} 22      } 23     Lk={c?Ck |c.count > min_sup} ; 24     return L=L1 ? L2 ? ??? ? Lk ;  }

V. PERFORMANCE COMPARISON In order to confirm the optimization for Apriori ,we have  taken some comparison tests between Apriori and the improved         algorithm named BE - Apriori. Experimental  environment: CPU ,Pentium (R) Dual-CoreT4300, RAM 2.0G. Operating sy- stem ,Windows ;  programming language, C; development plat- form , VC6.0.Choose the data set named retail as the Experimental test data which come from the frequent itemsets mining library of FIMI (Frequent Itemset Mining Implementations), provided by a Belgian supermarket anonym- ously ,and contains 88163 consumption records of 5133 customers.We use the first 3000 records  for the experiment.

We have done several tests about the retail data set used the classical  Apriori and  the improved BE-Apriori separately , The results obtained are as follows:  The numbers of frequent itemsets generated under different support are shown as below:   Figure 2.  The comparison of the number of frequent itemsets generated  under different support.

According to the above results,it can be seen that the number of frequent itemsets generated by  the improved   BE- Apriori is less than the number generated by the pure Apriori algorithm. Furthermore, with the  value of the support selected is increasing, the effect of resulting reduction in the number of frequent itemsets  generated has become obvious increasingly.

The comparison of the running time of two algorithms  is shown as below:   Figure 3.  The comparison of the running time of two algorithms  under different support  According to the above results,it can be seen that the running time of  the improved   BE-Apriori is less than the the running time of  the pure Apriori under different support .

When the support increases from 10 to 12,15,20, the running time of  the improved BE-Apriori is reduce rapidly relative to the Apriori algorithm.When the  support is 20, the running time  of the improved BE-Apriori is less than half of the pure Apriori algorithm.

Therefore, we find that, compared with the pure Apriori al- gorithm, the improved BE-Apriori algorithm an significantly reduce the number of frequent itemsets generated, the running time was obviously shortened,the efficiency of the improved B- E-Apriori is higher than Apriori.



VI.  CONCLUSION This paper briefly reviews the topic  of association rules  mining, analyzes the ideas and the shortcomings of Apriori algorithm, and compare several different styles of the major improvement strategies.Then studies the pruning optimization and transaction reduction strategy, finally put forward the improved Apriori algorithm based on pruning optimization and transaction reduction.The experiments about data set retail show that ,compared with the pure Apriori algorithm, the improved  algorithm named BE-Apriori has decreased the number of frequent itemsets generated, reduced the running time obviously. The improved  algorithm have a good advantage of low system overhead and good operating performance,  its efficiency is significantly higher than the Apriori algorithm.With the expansion of the scale of data, this advantage will become obvious increasingly.

