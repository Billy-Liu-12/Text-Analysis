<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">Proceedings

Abstract: Traditional technology of classifier fusion can not make full use of the characteristics of heterogeneous classifiers to deal with various problems. This work suggests a new technology of information fusion using multiple agents, each of which uses a quite different classification algorithm such as decision tree algorithm, simple Na?ve Bayes algorithm and the newly emerging classification algorithm based on atomic association rules. Information fusion of these heterogeneous multi-classifiers is based on the classifier behavior, properties of training dataset and the instance to be classified. The proposed technology has following advantages: (1) high classification accuracy; (2) no need of fusion training, and (3) fast learning and prediction. The experimental results on 10 UCI standard datasets show that accuracy of the proposed fusion technology is noticeably higher than that of traditional voting method.

Keywords: Fusion of heterogeneous classifiers; Agent; associative classification; decision tree; Na?ve Bayes 1. Introduction Information fusion technology of multi-classifiers can surpass the capability limit of a single classifier. It can overcome the shortcomings of single classifier and realize the knowledge integration among multiple classifiers to achieve better performance. Especially, the information fusion of heterogeneous multi-classifiers can display their advantages of complementary among different classifier models and can make the classifying prediction more accurate and robust.

In this paper, we adopt multi-agents based technology to realize the knowledge fusion of heterogeneous classifiers.

Agents can imitate human beings? group behaviors to solve problems. According to this, every classifier is designed to be an agent with striking characteristics. Each agent has both strength and weakness. We use two types of agents: model-responsible agent (for short, Model Agent) and the combining classifier agent (for short, Combining Agent).

Model Agents learn a classification model using a specified algorithm on a dataset and predict an unlabelled instance using the learned model. Combining Agent realizes the information fusion of multiple Model Agents? prediction results and gives the final class label of the unlabelled instance.

When agents collectively do a classifying job, first of all, every Model Agent must complete its learning on its own and produce an independent classifying model. When a classifying prediction task is performed, the Combining Agent sends every Model Agent the instance to be forecast.

Model Agents use classifying model to forecast the label of the instance, meanwhile, it evaluates the instance?s properties and sends the overall evaluation results with the class label to the Combining Agent.

The combining Agent gives the final class label to the unlabelled instance based on the results of all Model Agent in terms of control logic. Thus, Combining Agent probably surpasses the capability limits of single classifiers. Hence, the agent based system of heterogeneous multi-classifiers can achieve a high classifying accuracy without additional fusion training of classifier group.

2. Agent based system of heterogeneous multiple classifiers    classifiers 2.1. Choosing classifiers of different properties When choosing heterogeneous classifiers, two parameters must be taken into consideration: a high prediction accuracy and short time for model building. In order to structure the Model Agents, this work utilizes three different types of classifying algorithms, which meet the requirements mentioned above. Two of them, decision tree classification and simple Na?ve Bayes algorithm are quite popular in machine learning community. The third one is a new algorithm (classification based on atomic associative rules, i.e., CAAR [2]) which first appeared at the ICMLC of these three algorithms are quite different. Decision Tree is based on entropy, simple Na?ve Bayes is based on 0-7803-9091-1/05/$20.00 ?2005 IEEE  Guangzhou, 18-21 August 2005 post-pr classifi Am human outstan ?easy-f CAAR of this (1 apply t confide (0.98*m (2 order b rules o rule. A which c (3 classify in the classify sequen ith to gh In of ial ng an y, ur nt ers ne va es to obability, and CAAR is based on associative cation using confidence and support.

ong these, the newly emerging CAAR imitates  behaviors in classifying things and makes use of ding features of the instances in dataset and the irst? strategy to classify. The details related to can be found in literature [2]. The three main steps algorithm?s principle are described below: ) Discover all the atomic class associative rules, he associative rules of strong rules with the highest nce (maxConf) and near-highest confidence axConf) to partial classification.

) Sort the strong rules according to the ascending ased on the rule?s confidence and support, test the    ased on the rule?s confidence and support, test the n the dataset and the delete the records covered by a fter passing through the dataset, the redundant rules atch no instances in test are deleted.

) For the remaining instances, use the above partial ing process again until the number of valid records dataset becomes zero. Finally, combine the ing rules generated at each partial classification in CAAR algorithm only uses strong atomic rules w the highest confidence and near-highest confidence classify. Thus, it has the following characteristics: a hi classifying accuracy, top speed and robust performance.

literature [2], there is a detailed description and analysis CAAR algorithm. Though CAAR uses several part classifications (on average, with 7 passes), its runni speed, close to decision tree classification, is far faster th that of CBA algorithm [4].

2.2. Multi-classifier system integrating JADE and Weka Agents possess many characteristics such as autonom intelligence, sociability and self-adaptation, etc. In o study, agents are constructed in Java Agent developme environment, that is, JADE platform [10]. The classifi are designed on the platform of Weka Java Machi Learning [9]. Decision tree algorithm J48 (a ja implementation of C4.5 [1]) and Simple Na?ve Bay (hereafter referred as Bayes) are realized. Our work is ce to form a classification model.

realize CAAR algorithm and combining classifier under  Figure 1. User interface of the heterogeneous multi-classifier system integrating Weka and JADE.) Guangzhou, 18-21 August 2005 Weka pla is to mo JADE p jade.core Weka h commun heteroge network file Ge "weka\gu be regist the GUI 3. Info clas 3.1. In The of the cla classifyin condition between ratio of  distributi A C of each different advantag of classif dynamic For two cat propertie classifica (as show the ome may tform. An easy way to integrate JADE and Weka dify the abstract class "Classifiers" by importing    dify the abstract class "Classifiers" by importing ackage and make class "Classifiers" inherit .Agent. In this way, all classifying algorithms in ave the basic functions of agent, such as 3.1.1. Influence of datasets The properties of the datasets directly influence capability of Model Agent [8]. For a given dataset, s algorithms may achieve high accuracy while others ication. So the agent based system of neous multi-classifiers can be used for distributed environment. After the construction of agents, the nericObjectEditor.props under the directory i" must be revised [6]. So the agent classifier can ered and integrated with Weka GUI. Figure 1 gives of our experiment dealing with dataset wine.

rmation fusion of heterogeneous multiple sifiers fluencing factors re are many factors that determine the performance ssification. For a given classification algorithm, its g capability will change according to different s such as properties of datasets, correlations the features or attributes and the class labels, the attributes missing values and uniformity of class on, etc.

ombining Agent needs to know the characteristics Model Agent and the complementary effects of classifying algorithms, and make full use of the es of all Model Agents. So it can break the limits ying capabilities of single Model Agent by using a fusion strategy [7].

information fusion of multi-classifiers, there are egories of important influencing factors: the s of dataset, which have a great effect on the tion model, and that of the instance to be classified n in figure 2).

perform unsatisfactorily. That is the algorithm?s adaptation to the specified problems. Up to now, there is no classification algorithm that can produce ideal model suitable for all problems, so some hybrid algorithms attract the interest of researchers [3][5].

Therefore, if heterogeneous Model Agents can be used to deal with the specified problem complementarily, the chance for success will be much bigger. The key problem is how to measure the influencing factors related to dataset.

As for decision tree algorithm, two main factors that affect the classifying accuracy of decision tree is Ratio of the number of Missing Values (RMV) to the total number of values of the training dataset, and the uniformity of the class distribution (UCD) of the datasets. The definition of UCD is given as follows:  max |C| 1i imax )d/)dd((|C| 11UCD ?

= ??=    (1) Where C is a set of class labels and di is the number of instances that belonging to ith class, and dmax=max{di}, | . C|i1 ??

For CAAR algorithm, the internal associative relationship between dataset?s attributes and the class labels is the main factor influencing the classifiers? adaptation. As described in [2], in the first partial classification of CAAR, based on mean confidence (MC) and mean support (MS) of ten strongest atomic association rules, the CAAR divides datasets into two categories: type-P datasets that satisfy the constraining conditions of MC&gt;0.8 and MS&gt;0.02, and type-N datasets that do not satisfy the constraining conditions. The test on 26 datasets of UCI shows that 20    conditions. The test on 26 datasets of UCI shows that 20 datasets belong to type-P and 6 ones belong to type-N. It is obvious that the classifying capability of CAAR on type-P datasets is much greater than on type-N.

z Parameters of CAAR model learned on a dataset (MC and MS). Influence of the dataset { z Uniformity of Class distribution and ratio of missing values in dataset (UCD and RMV)  Confidence of a CAAR rule r classifying an instance (r.Conf).

An instance?s ratio of missing values (IRMV) Influencing factors  {  Influence of the instance to be predicted { z The max probability of an instance being a class (maxP) Figure 2. Factors that influence information fusion of heterogeneous multi-classifiers z z Guangzhou, 18-21 August 2005 As for Bayes algorithm, it depends on the assumption that attributes are independent. This is the main factor that affects the adapting capability of Bayes classifiers. To quantatively evaluate the correctness of this hypothesis is very difficult. Thus, in terms of dataset-related factors, this paper only evaluates the influences that internal association features of datasets bring to CAAR and influences that the uniformity of class distribution of datasets and the ratio of missing values have on J48.

3.1.2. Influence of the instance to be classified If classifying models are used to predict an unlabelled instance, the results of three Model Agents are directly relevant to the properties of the instances to be classified.

Sc (for CAAR)  &amp; Sj (for J48)   &amp; RMV&lt;0.05 IRMV&lt;0.10 UCD&gt;0.60  maxP&gt;0.90 Sb (for Bayes) X }S,S,S{X jbc?

X MC&gt;0.80 r.Conf&gt;0.70 MS&gt;0.02  Figure 3. Logic illustration of classifiers which satisfy the constrain conditions As to decision tree, the prediction accuracy is heavily affected by the instance?s ratio of missing values (IRMV).

The more missing values, the worst case the prediction is.

As to CAAR, the classifying rule?s confidence is considered an important factor that influences the classifying accuracy.

For Bayes algorithm, the maximum value of probability distribution of the class labels for the instances, to be classified, determines which class the instance belongs to. If the maximum probability is only 51%, the other 49% probability indicates the instance?s label is not    other 49% probability indicates the instance?s label is not the chosen one. Under this condition, Bayes algorithm must produce inferior classification.

cS bS jS 3False Vote Pc=Pj= Pb Pc= Pj Pc Pc=Pb Pj=Pb bjc  PPP ??

Pb true false 3True Sc Sb Pj maxP &gt;r.Conf 2True Sc Sb2True2True Sc Sj Pc Sj Sb Pb 1True 1True 1True Sc Sb Pj Sj bS cS jS bS S cS Sj cS bS j jS   Figure 4. Control logic of information fusion of three classifiers Guangzhou, 18-21 August 2005 It is thus evid heterogeneous Model 3.2. Control flow o classifiers In order to contr classifiers, a logical Agent to see whether If the Model Agent ca datasets and of unlabe pass the constrain test true; otherwise, it?s conditions which are CAAR, obviously if it must be type-P (i.

r.Conf&gt;0.7. If these co of Sc is false. As to uniformity of class di and the dataset?s ratio and the instance?s rati is true. As to Bayes c of classifying the insta true.

With regard to classifying prediction Model Agent CAAR respectively. The C    information fusion o control logic (shown method not only takes classifiers but also consideration. Those symbol is true have logic variables are s e e a t l f f Dataset | Anneal 3 Auto 2 Breast 1 Diabetes Hepatic 1 Iono 3 Labor 1 Tic-tac Wine 1 Zoo 1 Ment that the information fusion of Agents is very important.

f information fusion of multi- ol the information fusion of multiple symbol is designed for every Model the appointed constrains are satisfied.

n simultaneously satisfy constrains of lled instances, then the Model Agent , and the value of its logical symbol is  false. Figure 3 shows the constrain "3True") or simultaneously false (i.e., "3False"), thre Model Agents have the same priority in determining th class label of the instance, so information fusion comes to voting state. If the dispute among three Model Agents can? be settled by voting (for example, ), the fina class label is determined by directly comparing maxP o Bayes with r.Conf of CAAR as shown in figure 4.

bjc  PPP ??

4. Experimental results and discussion 4.1. Experimental environment The agent-based system for information fusion oneeded by three Model Agents. As to s logical symbol Sc is true, the dataset e., MC&gt;0.80 and MS&gt;0.02) and nditions are not met, the logical value decision tree J48, if the dataset?s stribution (UCD) is bigger than 0.60 of missing values is less than 0.05, o of missing values less than 0.10, Sj lassifiers, if the maximum probability nce is bigger than 0.90, then the Sb is the given unlabelled instance for , the three class labels predicted by , J48 and Bayes are Pc, Pb and Pj, ombining Agent carries out the f multi-classifiers according to the in figure 4). It is clear that our fusion advantage of the characteristics of all takes the classified instance into Model Agents whose constrain test the final say. When three classifiers? imultaneously true (i.e., a state of heterogeneous multi-classifiers is realized on the platform of JADE and Weka. Different from literature [2], CAAR is integrated into the environment of Weka in this experiment.

Since Weka uses 10-fold stratified cross-validation. The datasets at first are sorted according to class labels before    datasets at first are sorted according to class labels before the 10-fold stratified cross validation. Thus, there are some differences in classification results of CAAR algorithm. Ten datasets from UCI machine learning repository (http://www.ics.uci.edu/~mlearn/) were used in the experiment (indicated in Table 1). A standard entropy-based method is used to discrete all the continuous attributes in the datasets. The computer used in experiment is a PC with 2.0GB P4 and 256MB memory. The OS is Windows XP.

4.2. Experimental results and discussion Table 1 provides properties of ten experimental datasets, and the prediction accuracy of 10- fold stratified cross validation of different classifying algorithms. In table 1, |A| and |C| respectively refer to the number of attributes and the number of class labels in datasets. The column Table 1. The properties of datasets and experimental results of classification Properties of datasets Accuracy of classifier (%) A| |C| Type UCD RMV MMP CAAR J48 Bayes VF HCF 8 6 P 0.32 0.63 0.955 96.33 91.65 95.99 95.77 97.10 5 7 N 0.66 0.01 0.906 71.22 80.98 67.32 79.02 78.54 0 2 P 0.89 0.00 0.997 97.14 95.57 97.28 97.28 96.85 8 2 P 0.89 0.00 0.828 75.78 77.08 77.99 77.86 77.34 9 2 P 0.73 0.05 0.948 84.52 83.23 84.52 84.52 86.45 4 2 P 0.90 0.00 0.992 93.73 90.60 91.17 94.59 94.87 6 2 P 0.89 0.34 0.945 94.74 84.21 91.23 91.23 94.74 9 2 N 0.89 0.00 0.727 70.88 86.01 70.35 75.57 86.01 3 3 P 0.96 0.00 0.995 97.75 93.82 98.87 98.31 99.44 6 7 P 0.50 0.00 0.951 96.04 92.08 93.07 95.05 96.04 ean 0.76 0.10 0.924 87.81 87.52 86.78 88.92 90.74  Guangzhou, 18-21 August 2005 "Type" stands for the dataset type which is corresponding to the classifying capability of CAAR on the specified dataset.

UCD stands for the uniformity of class distribution in datasets. RMV stands for the ratio of the number of missing values to the total number of attribute values in a dataset.

MMP is the average value of all the maximum probabilities (maxP) classifying all the instances using Bayes algorithm.

CAAR, J48, Bayes, VF and PF are corresponding to the prediction accuracy in 10-fold stratified cross-validation using CAAR algorithm, J48 algorithm, Bayes algorithm, voting fusion algorithm and our proposed algorithm of heterogeneous classifier fusion, respectively.

After analyzing the properties of datasets, we can see that for the datasets of Anneal and Labor, their RMV values is bigger than others, and the prediction accuracy of J48 is clearly lower than that of CAAR and Bayes. On the dataset Tic-tac (type-N), the classifying capability of CAAR is comparatively weaker. At the same time, the MMP value of Bayes is only 0.727. Consequently, on this dataset, these two classifying algorithms are not as good as J48. On dataset Auto (type-N), the classifying capability of CAAR is weaker, but that of J48 is better. But as to Bayes, though the parameter of MMP is 0.906, its classifying effects are comparatively worse. This illustrates the fact that it is unrealistic to assume that attributes of the dataset are independent one another.

Table 1 clearly shows that in the 10 datasets, as to single classifiers, the prediction accuracy of CAAR is the highest (87.81%). The accuracy of combining classifiers is higher than that of any single classifier. Among these, the accuracy of our proposed fusion algorithm is 90.74%, which is noticeably higher than 88.92% of traditional voting fusion method.

5. Conclusions This work proposes a new information fusion technology of multi-classifiers. Using heterogeneous multi-classifiers can make full use of the characteristics of quite different classification algorithm. Our technology takes many factors into consideration, including the    takes many factors into consideration, including the properties of classifiers, datasets and the classifying instance etc. The technology can effectively integrate the information from heterogeneous multi-classifiers. It has the following characteristics: a high accuracy for prediction, robust performance, short time to build a classification model, and no need for fusion training.

Acknowledgements This paper was supported by the Natural Science Foundation of Guangdong Province of China (grant No.

31340) and the Science and Technology Plan Project of Guangzhou City in Guangdong Province of China (grant No. 2004Z3-E0091). Weka was provided by the University of Waikato in New Zealand. JADE was provided by Telecom Italia Lab, the R&amp;D branch of the Telecom Italia Group.

