The Research of K-means Clustering Algorithm Based on Association Rules

AbstractWith the continued expansion of network  resources and the rapid change of old and new information,  the traditional information retrieval is difficult to adapt to  the need of management of mass electronic data. It is a very  important aspect of how to locate the information you want  conveniently and accurately and improve the efficiency of  search engines.

Keywords-Clustering;K-means;Data Mining;Apriori

I. INTRODUCTION  With the rapid development of Internet, society has  entered the Internet age. According to incomplete  statistics, the number of Internet users in China in 2008  reached 200 million people, 89% of those Internet users  mainly use the Internet to obtain information, of which  88.8% will make use of Internet search engines search  information, second only to send and receive E-Mail. Web  search engines have become an important part of the  building.

With the rapid growth of network information  resources, the result obtained through the search engine is  very large. Users have to filter the results list one by one to  get the results they want. According to survey, users will  generally turn back to read no more than five pages of the  results. How to quickly and efficiently extract valuable  information from the massive network information,how to  organize the display form of the query results and to make  it effective in the management and decision-making role is  becoming the object that computer science community and    the information industry compete to research and develop.

Clustering algorithm is one of the key technologies.



II. THE ORIGINAL K-MEANS ALGORITHM  From a practical point of view, clustering analysis is  one of the main tasks of data mining.It is now used in  many areas, such as data mining and knowledge discovery,  pattern recognition and so on. There are many clustering  analysis algorithm, of which the most well-known is the  K-means algorithm.It is a clustering algorithm based on  division. Its idea is that when a class is confirmed, the  geometric mean of data points in the class will be taken as  the center of the class. K-Means algorithm takes k as a  parameter and makes the n objects into k-clusters. Divided  according to the average of a cluster of objects.Algorithm  first randomly select k objects, suppose the initial object  represents a cluster mean or center. Division of the  remaining objects is according to their distance from the  center. And then re-calculate the average of the new  division of the cluster. Repeat the above process until  criterion function converges. Clustering-based data set to  be X = (X1, X2,..., Xi), dividing into k clusters, with the  following definition.

Definition 1: K-means criterion function  ??

= ?

?=  k  i cx  i  i  mxE      E is the sum of squared error for all objects.Among  these x  is the point in space; mi is the average of cluster  Ci. This criterion can guarantee that the results of cluster  generated compact and independent as possible.

K-means algorithm needs to give k in advance, and  also sensitive to the initial value. The picking of initial  cluster centers has a great impact on the clustering results.

Therefore, a reasonable choice of initial cluster centers is    crucial. Different initial values will lead to different  results; moreover, it is sensitive to the "noise" and the  isolated point data. A small amount of such data can have  a huge impact on the average. So how to eliminate "noise"  and the isolated point data and to find a suitable initial  focal point to gain a better clustering result and to improve  the accuracy of clustering results of the k-means algorithm  is of great significance.



III. APRIORI-BASED K-MEANS CLUSTERING ALGORITHM  The object here to deal with is not the complete Web  document, but results options on the search engine query  results. In the search results, many items of characteristics  of information are accompanied. For example: in the  search "network attack", generally content or summary of  hacker ?  loopholes and other characteristics of  information appears. If the relevant characteristics of the  information appear more frequently, the results which are  checked out are closer to the demand. Combinations of  association rules can eliminate a lot of "noise" and isolated  point data in the search results which can reduce the  computation.

A. Association analysis  Association analysis is an important way to discover  knowledge in data mining, if two or more data items are  repeated between the values and the probability of  repetition between the very high, there is a link. In the  establishment of the Knowledge Base,the association  analysis using APRIORI algorithm to excavate data.

Apriori algorithm is used to found rules like90% of  customers will buy when you buy goods A product B.

Here are a few basic rules associated with the concept.

Set I = (i1, i2,..., in) is a collection of items, set D as a  collection of services, transaction T is a collection of items   Engineering  978-0-7695-3972-0/10 $26.00  2010 IEEE  DOI 10.1109/CESCE.2010.26   and T? I.Set A as an itemset of I, if A ? T, then the  transaction T contains A.

Definition 1: Let A, B are two sets, and A ? B = ?.

The support degree for the rules is S, the confidence    degrees is C. Support for S refers to the probability in D,  consist of A ? B (also includes A, B), that is, P (A ? B).

Confidence level C is the probability in D, with A the  same time, the transaction also includes B, that is P (B | A).

Such association rules can be expressed as the implied  form as following: A ? B [S, C].

Definition 2: a collection of items is item-set. For the  item-set that contains k-item itemsets called the k-itemsets.

If you meet the minimum support itemsets, then it is called  frequent itemsets.

The purpose of association rules is to find frequent  itemsets. To carry out pruning and to exclude the low  degree support itemsetsis is an important part of Apriori  algorithm. Association algorithm first excavates frequent  sets that have a certain degree of support. If there is a  certain similarity between the affairs correspond to these  frequent sets, then it is called initial clustering.

The resulting data generally requires a discard, discrete  data pre-processing and format conversion. These  discretization of continuous variables in order to facilitate  the discovery of association rules. Here take five attributes  for data mining(time?title?content?summary?Url),the  recording property mainly include time, title, content,  summary, URL. Association mining of Apriori algorithm  is of the form A ? B [S, C].

B. Improved K-means algorithm  Through the analysis above, improved algorithm first  deal with the results of the search by Apriori algorithm to  identify frequent sets.The second is the generation of  association rules.Select the item set that focus on to meet  the minimum support and minimum confidence from the  frequent sets. After iterate the algorithm,the (k-1)- frequent  sets becomes the k- frequent sets?The third step is to scan  the item of minimum support and minimum  confidence.And find out the object closest to one degree of  support, form the initial cluster center. Finally, classify  sample points and  adjust cluster centers according to K-  means criterion function;Iterative alternate the two  processes,then becomes the criterion function converges.

Step 1 Discretization of the search results  Step 2 First, get the frequent sets with the minimum  support degree of s, length of l    Suppose transaction set T = (t1, t2... tn), the frequent  itemsets generate by Apprior algorithm Ck = Apriori (T, s,  l)  Step 3 Put the generated frequent itemsets as the initial  clustering of data. Suppose there are database that contains  k-cluster and n objects.

Step 4 Scan each cluster in the minimum support and  minimum confidence of an object which is close to 1  mostly?forming of the initial cluster center.

Step 5 Iterative algorithms  REPEAT  For i=1 to n  According to average of cluster objects, assigned  each object to the most similar cluster.

For j=1 to k  ?

?

=  icx  ii Cxm /    Computer E?

Until E  No significant change.

C. Experimental Analysis  After dealing with the new clustering algorithm,  clustering can provide more accurate information  basically.And the accessibility is stronger, especially for  the cluster of large amount of document data, its more  efficient. Make use of Google as initial search engine of  the initial results list ,experimenting 60 different  queries,minimum support s = 50%.We can see that the  new algorithm improves the accuracy of clustering and  reduce the number of iterations.

Table 1 Experimental Results  Search  Document  Number  Cluster number Algorithm iterations  original  clusters  number  present    clusters  number  Original  Iterations  present  Iterations  SQL  Injection 220 80 50 20 10  Web  Attack 300 90 70 28 13  TV 120 20 10 5 3  Mobile 130 12 6 3 3

IV. CONCLUSIONS  K-means clustering algorithm is a kind of classical  clustering algorithms to solve the problem,which is simple  and rapid.At the same time, It has good scalability and  high efficiency when deal with large data sets.But the K-  means algorithm requires the user to give the generated  clusters data in advance.It is also sensitive to the initial  value and not suitable for the cluster of different  sizes.Moreover,it is so sensitive to "noise" and the isolated  points of data that a small amount of such data can bring a  tremendous impact on the average.In order to solve these  problems, this paper proposes an improved  K-means  algorithm which is based on association rules.It can greatly  improve search accuracy by generating frequent itemsets  with Apriori algorithm and using the method of selecting  the initial cluster center. In addition, the improved K-  means algorithm can also save computation time by  pruning and removing the unwanted data.

[1]K.Ali,S.Manganaris,and R.Srikant.Partial Classification using  Association Rules.In Proc.of the 3rd Intl. Conf.on Knowledge Discovery  and Data Mining,pages 115-118,NewPort Beach,CA,August 1997.

[2]C.C Aggarwal and P.S. Yu. Mining Large Itemsets for Association  Rules.Data Engineering Bulletin,21(1):23-31,March 1998.

[3]Han J,et a1.Data Mining?Concepts and Techniques[M]. Beijing?

Machinery Industry Press,2001?149-222.

[4]Margaret,H-Dunham.Data Mining ? Introductory and Ad ?

vancedTopics[M]. Beijing?Tsinghua University Press,2003?164-192    ?

