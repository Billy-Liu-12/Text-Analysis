Mining Association Rules Using Fast Algorithm

Abstract The most time consuming operation in Priori-like  algorithms for association rule mining is the computation of the  frequency of the occurrences of itemsets (called candidates) in the  database. In this paper, a fast algorithm has been proposed for  generating frequent itemsets without generating candidate  itemsets and association rules with multiple consequents. The  proposed algorithm uses Boolean vector with relational AND  operation to discover frequent itemsets. Experimental results  shows that combining Boolean Vector and relational AND  operation results in quickly discovering of frequent itemsets and  association rules as compared to general Apriori algorithm .

Keywords- Association Rule Mining (ARM); Frequent  itemsets; Boolean vector; relational AND operation

I.  INTRODUCTION  Association rule mining has been investigated by many  researchers and practitioners for many years [1, 2, 3, 4, 5].

Association rules are used to identify relationships among a set  of items in database. These relationships are not based on  inherent properties of the data themselves (as with functional  dependencies), but rather based on co-occurrence of the data  items. In this setting, attributes which represents items are  assumed to have only two attributes and thus referred as  Boolean attributes. If an item is contained in a transaction, the  corresponding attribute value will be 1; otherwise the value  will be 0. ARM is undirected or unsupervised data mining over  variable-length data and it produces clear and understandable  results. It has an elegantly simple problem statement: to find  the set of all subsets of items or attributes that frequently occur  in many database records or transactions, and additionally, to  extract rules on how a subset of items influence the presence of  another subset. A huge calculation and a complicated  transaction process are required during the two procedures.

Therefore mining the Apriori-like algorithms [6] is very  unsatisfactory when transaction database is very large.

In this paper, an algorithm has been proposed to generate    frequent itemsets without candidate itemsets generation and  also generation of association rules with multiple consequents  using Boolean vector and AND operation methods.



II. MATERIALS & METHODS  A. Association Rule Mining  The Apriori-like algorithms adopt an iterative method to  discover frequent itemsets. The process of discovering frequent  itemsets need multiple passes over the data. The algorithm  starts from frequent 1-itemsets until all maximum frequent  itemsets are discovered. The Apriori-like algorithms consist of  two major procedures: the join procedure and the prune  procedure. The join procedure combines two frequent k-  itemsets, which have the same (k-1) prefix, to generate a (k+1)  itemset as a new preliminary candidate. Following the join  procedure, the prune procedure is used to remove from the  preliminary candidate set all itemsets whose k-subset is not a  frequent itemsets [6]. From every frequent itemset of k>=2,  two subsets A and C, are constructed in such a way that one  subset C, contains exactly one item in it and remaining k-1  items will go to the other subset A. By the downward closure  properties of the frequent itemsets these two subsets are also  frequent and their support is already calculated. Now these two  subsets may generate a rule A ?C, if the confidence of the rule  is greater than or equal to the specified minimum confidence.

B. Data Preprocessing  Preprocessing is often required before applying any data  mining algorithms to improve performance of the results. The  preprocessing procedures are used to scale the data value  either 0 or 1. We use the following method for transforming  the database into Boolean values is given in Fig.1.

Figure 1. Data preprocessing method      The raw database rdb is a  m x n matrix where m is the  number of transactions and  n is the number of attributes. bdb  is the resultant Boolean matrix. rdb[i,j] denotes item at the i  th    row and j  th  column of the initial matrix. bdb[i,j] denotes item at  the i  th  row and j  th  column of the resultant matrix.

C.  Algorithm For Frequent Itemset Generation  1) Definitions  Let I={i1, i2, , in} be a set of items, where each item  ij corresponds to a value of an attribute and is a  member of some attribute domain Dh={d1, d2, , ds},  i.e. ij ? Dh. If I is a binary attribute, then the  1. For all i<=m  do  2. for all j<=n do  3.    if j  th  item is present in i  th  row  4.           Set rdb[i,j] =1  5.       else  6.            set rdb[i,j] =0  7.  End do  8. End do  400978-1-4244-4791-6/10/$25.00 c2010 IEEE  Dom(I)={0,1}. A transaction database is a database  containing transactions in the form of (d, E), where d ??  Dom(D) and E ? I.

Let D be a transaction database, n be the number of  transactions in D, and minsup be the minimum support  of D. The new_support is defined as new_support =  minsup  n.

Proposition 1: By Boolean vector with AND operation,  if the sum of 1 in a row vector Bi is smaller than k, it  is not necessary for Bi to involve in the calculation of    the k- supports.

Proposition 2: According to [7], Suppose Itemsets X is  a k-itemsets; |FK-1(j)| presents the number of items j  in the frequent set FK-1. There is an item j in X. If  |FK-1(j)| is smaller than k-1, itemset X is not a frequent  itemsets.

Proposition 3: |FK| presents the number of k-itemsets in  the frequent set FK. If |FK| is smaller than k+1, the  maximum length frequent itemsets is k.

The proposed algorithm for finding the frequent itemsets  without candidate generation consists of following four steps:  1. Transforming the similarity matrix into the Boolean  matrix.

2. Generating the set of frequent 1-itemsets F1  3. Pruning the Boolean matrix  4. Generating the set of frequent k-itemsets FK(k>1)  A detailed description of the proposed algorithm for finding  frequent itemset is described in Fig. 2.

Figure 2. Algorithm for frequent itemset generation  D. Algorithm for Generation of Association Rules  This algorithm is used to generate association rules from  the already generated frequent itemsets (by algorithm 2). It  takes minimum confidence from the user and discovers all  rules with a fixed antecedent and with different consequent.

This algorithm uses the fact that:  If there exists two rules  A?C and A?{C? X} where X ? A ? C then the confidence  of the second cannot be larger than the first one.

This algorithm checks if a given set is a subset of another  set or not. To perform this operation each item in an itemset is  represented as an integer where a bit corresponding to as item  is set to 1. For example suppose a database with 8 attributes,  itemset {1,2, 5}  is represented as 38 as follows. 0 0 1 0 0 1 1 0  To check if set {2,5} is a subset of {1,2,5} we represent  {2,5} like above and is evaluated to 36. Now we perform AND  operation 38 & 36. The result is checked for equality with the  first itemset ({2, 5}). If they are equal then it is a subset  otherwise it is not. In this case the result is obvious. Similarly  difference of two sets is done during production of the rules.

This algorithm is capable of finding all association rules  with a fixed antecedent and with different consequents from  the frequent itemsets subject to a user specified minimum  confidence very quickly. The proposed algorithm is avoiding  the unnecessary checking for the rules based on the above  lemma 1.The algorithm generate the rules with a fixed  antecedent part. When all the rules with that antecedent are  generated it will go to the next antecedent. For a given  Input: The transaction database bdb,    and new_support  Output: The set of frequent itemsets F  //  Frequent  1-itemset generation  1. For each column Ci of pdb  2.     If sum(Ci) >= new_support  3.        F1 = Ii  4.     Else delete Ci from pdb  // By Proposition 1  5. For each row Rj of pdb  6.    If sum(Rj) < 2  7.         Delete Rj from pdb  // By Proposition 2 and 3  8. For (k=2;| Fk-1|>k-1;k++)  9. {  // Join procedure  10.    Produce k-vectors combination for all  columns of bdb;  11.   For each k-vectors combination  {Ci1,Ci2, Ci3  ,Cik }  12.      {  13.           B=  Ci1  Ci2  .Cik  14.           If sum(B)>= new_support  15.           Fk={ Ii1, Ii2,,Iik };      }  // Prune procedure  16.      For each item Ii in Fk  17.            If | Fk(Ii)| < k  18.      Delete the column Ci according to Item  Ii from bdb;  19.       For each row Rj from bdb  20.              If sum(Rj) < k+1  21.                Delete Rj from bdb;  22.        k=k+1  23. }  24.

Return F= F   ?F   .?F  k  2010 IEEE 2nd International Advance Computing Conference 401  antecedent if all rules in the level, where k is the number of    items in the consequent, have confidence less than the  threshold, i.e.  no rules are generated, and then the confidence  of any rule in k+1 level also cannot be more than threshold. So  checking for rules from this level onward can be avoided  without missing any rules. Now the maximum possible  confidence of the rule in the k+1 level will be minimum  confidence of the two itemsets from which this is constructed.

Since the confidence of only one of them is larger than the  threshold, others must be less than the threshold. So the  confidence of the rule in k+1 will be less than threshold. So, it  is not necessary to check for the rules in the next level without  missing any valid rule. So it can be concluded that the  proposed algorithm is complete.

Figure 3. Algorithm for association rule generation

III. RESULTS  The proposed algorithm has been implemented in Java and  tested on Linux platform. Comprehensive experiments on  market basket test database T40I10D100K from IBM Quest    project [8] has been conducted to study the impact of  normalization and to compare the effect of proposed algorithm  with Apriori algorithm. Figure 4 gives the experimental results  for execution time (generating frequent itemsets and finding  rules) vs. user specified minimum supports and shows that  response time of the proposed algorithm is much better than  that of the Apriori algorithm. In this case, confidence value is  set 90% for the rule generation, which means that all the rules  generated are true in 90% of the cases.

Figure 4.  Performance of Apriori and Proposed (Minsup vs.

Execution time)      Figure 5.  Performance of Apriori and Proposed (Minsup vs.

Memory usage)  Figure 5 gives the experimental results for memory usage  vs. user specified minimum supports and results show that  proposed algorithm uses less memory than that of Apriori  algorithm because of the Boolean and relational AND bit  operations.

Figure 6.  Association rules and Minsup in Apriori algorithm  Input: F={fk| fk is set of frequent k-itemsets  with sorted  on support count in descending order, l<=k<=maxsize};  conf  the minimum confidence.

Output: the strong association rules discovered with  their confidence.

1. for all fk, fk ? F, 1<=k<=maxsize-1 do begin  2. rsup=support(fk)*miconf  3. found=0  4. for all fm, fm ? Fk+1<= m <=maxsize do begin  5. if (support(fm)>=rsup) then begin  6. if(fk ? fm) then begin  7. found=found+1  8. conf=support(fm)/ support(fk)  9. generate the rule fk = ( fm- fk) &= conf and  support=support(fm)  10. end if  11. else  12.  if (found<2)    13. continue step1 with next k  14. else  found=0  15. endif  16. endif  17. end do  18. end do  402 2010 IEEE 2nd International Advance Computing Conference      Figure 7.  Association rules and Minsup in Proposed algorithm  The number of association rules decreases along with an  increase in minimum support under a given specific minimum  confidence, which shows an appropriate Minsupport (or  Minconfidence) can constraint the number of association rules  and avoid the occurrence of some association rules so that it  cannot yield a decision. These results have shown in Figures 6  and 7. The results are as expected and quite consistent with our  intuition.



IV. CONCLUSION  In this paper, a novel method of mining frequent itemsets  and faster generation of association rules has been proposed to  generate frequently occur itemsets very quickly. The proposed  algorithm does not produce candidate itemsets, it spends less  time for calculating k-supports of the itemsets with the Boolean  matrix pruned, and it scans the database only once and needs  less memory space when compared with Apriori algorithm.

The proposed algorithm is good enough for generating  association rules from spatial gene expression data very fast  and memory efficient. Finally, the large and rapidly increasing  compendium of data demands data mining approaches,  particularly association rule mining ensures that genomic data  mining will continue to be a necessary and highly productive  field for the foreseeable future.



V. REFERENCES  [1] Aggarawal, C. and P. Yu. A new framework for itemset generation. In  Proceedings of the PODS Conference, Seattle, WA, USA, June 1998,  pages 1824, 1998..

[2] Agrawal, R., T. Imielinski, and A. Swami. 1993. Mining association  rules between sets of items in large databases. In Proceedings of the   pages 207216, Washington, D.C., May 1993.

[3] Silverstein, C., S. Brin, and R. Motwani. Beyond market baskets:    Generalizing association rules to dependence rules. Data Mining and  Knowledge Discovery, 3968, 1998..

[4] Kotsek, P. & Zendulka J. Comparison of Three Mining Algorithms for  Association Rules. Proc. of 34th Spring Int. Conf. on Modeling and  Simulation of Systems (MOSIS'2000), Workshop Proceedings  Information Systems Modeling (ISM'2000), pp. 85-90, 2000.

[5] Han, J., Pei, J., & Yin, Y, Mining frequent patterns Candidate  generation. In Proc. 2000 ACM-SIGMOD Int. Management of Data  (SIGMOD'00), Dallas, TX, 2000.

[6] Agrawal, R. & Srikant, R. Fast Algorithms for Mining Association  Rules in large databases. In Proceedings of the 20th International  Conference on Very Large Databases pp. 487-499. Santiago, Chile,  1994.

[7] Xu, Z. & Zhang, S. An Optimization Algorithm Base on Apriori for  Association Rules. Computer Engineering 29(19), 83-84, 2003.

