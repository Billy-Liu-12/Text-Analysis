Diagnosing development software  release to predict field failures

Abstract  With the advancement of analytical engines for big data, the healthcare industry has taken a big leap to minimize  escalations on healthcare expenditure, while providing a reliably working solution for the customers based on the  slice and dice of the collected information. The research and development (R & D) departments of the healthcare  players are providing more focus on the stability and the usage of the system in the field. The field studies have  created a reliability based feedback loop that has helped R & D provide hotfixes and service packs in shrinking time  lines to better answer the customized needs of the user. Given the variety of possible optimizations in the actual  usage, the software-hardware product combine such as the Philips Magnetic Resonance (MR) modality has to ensure  that the business critical workflows are ever stable. In a nutshell, fault prediction becomes an important aspect for the  R & D department because it helps address the situation in an effective and timely fashion, for both the end-user and  the manufacturer to alleviate process hiccups and delays in addressing the fault. Reliability growth plot using the  Weibull probability plots helps to predict failures that guide reliability centric maintenance strategies [1]  ; however, this  will be a passive application of prediction for the new software yet to be released for market. This paper tries to  address the case where a fault/failure at the customer-end can be better predicted for software-under-development  with the help of analysis of field data. The terms failures and faults are interchangeably used in the paper to represent  error events that can occur at an installed base.

Background  The rationale behind fault prediction is to provide a  prognosis for future faults based on the current data  records from the field. In case of Philips MR, we use  machine logs to describe and record every failure that  has occurred in the field. One of the models that help  us in fault prognosis is rough set theory [2]  which is  currently used to mathematically model big data in  our organization and the system logs are a part of this  model. The theory helps us create decision tables and  rules that help discern the failures.

Method  For a MR modality, every fault is unique. Some failures  require Physics models for prediction like a magnet  quench. Some faults are based on the natural wear  and tear and mechanics of the system like error in  table movement and some are based on the software  workflows like a process crash. A combination of  reliability models applied for the Philips MR sub-  systems eventually come together at the system level  for an enhanced fault prediction. For predictive fault  modeling, we concentrated on failures like slowness  of the system over time, scan aborts that can occur on  a Philips MR, process crashes that can occur with  customer usage over time, etc. Typically, these are  errors which contribute to a considerable number of     the major faults from the field error database, given  the fact that the software provides the visual interface  to the Philips MR modality where the user would  spend much of his time. We also intend to introduce  algorithms that help us predict hardware faults, in the  near future; however, this paper limits itself to a  describe predictability of the software errors in a MR  system.

While leveraging big data information gathering  system for Philips MR, we have collected tremendous  information on failures occurring at installed bases.

These are mapped against the existing software flows  in the system. In this way, the changes made to the  developmental releases of the software can be super-  imposed to the existing field failures. The possibility of  occurrences of these failures will then become  predictable for all deployments and can also be  predicted per installed base.

Any developmental change can typically add, modify  or delete software from the releases. For the ease of  prediction, a big-bang modification in the software  can be considered to be an addition of new feature  set into the system. Thus, we prioritized to model only  on two parameters, namely, new software and  modified software. It was easier to model the  modified software to fit error database because the  mapping is a direct result of the software flow. The  new software definition was a different challenge  because it can?t be mapped directly based on field  relevance and hence we had to also consider and  combine the existing quality practices in an R & D  organization to leverage on statistics based on the  past data to predict defects (and thus field errors) that  can arise out of the new software [3] [4]  .

Results  Leveraging on many test runs, both manual and  automated for a current software-under-  development, a set of critical errors were monitored  and the actual results were fed back into the  predictive model along with the data from the field. In  at least twelve weeks of continuous execution, we  were able to successfully predict a majority of the  monitored errors. At the time of writing this paper, the  success of prediction stood at 6.5 times out of 10,  which we intend to improve by at least another 3 units  to reach an industry standard prediction. We notice  that larger the monitored error collection better will  be the prediction hits because the model will walk  across different flows to give a correct prediction. We  also see that the number of defects found from this  method also had a 70% jump in detecting possible  issues when compared to those found through the  currently reliability test suites for the product.

Conclusion  The learning from the installed bases were applied  using a fault predictive model that is currently being  tested within the purview of the R & D organization of  Philips MR business for a new product initiative under  a validation phase of the software development  process. Before the release can be made for a limited  set of installed bases and consequently monitored, we  hope to validate that the fault prediction  methodology and improve predictability by using field  data to bring near-to-accurate results to the  customer.

