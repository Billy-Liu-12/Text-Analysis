Knowledge Discovery from Databases on the Semantic Web

Abstract  We provide a flexible method for Knowledge Discovery  from semantically heterogeneous data, based on the  specification of ontology mappings from the local data  sources to pre-existing (superior) ontologies in an ontology  server.  We also provide an innovative method for the  construction of a Dynamic Shared Ontology; data integration is then carried out by minimisation of the  Kullback-Leibler information divergence using the EM  algorithm. The new knowledge learned by this process is  potentially richer than any of the contributing data sources.

We also show how the approach may be extended to Knowledge Discovery from a number of database  attributes; association rules or Bayesian Belief Networks  may then be induced. An architecture for a KDD system in  such an environment is described; this is an extension of a  previous architecture for distributed data processing that  we have already implemented.

1. Background  We are concerned with the integration of data that have  heterogeneous classification schemes where local  ontologies, in the form of such classification schemes, may  be mapped onto each other; this is frequently achieved via  a superior ontology such as an internationally recognised  classification. Thus the user may specify an ontology that is  familiar/interesting to him, the system may utilise a  superior ontology, which the user maps his ontology to,  and the data provider may utilise a third ontology which is  also mapped to the superior ontology. In such situations  metadata, describing the various ontologies in the system,  and mappings between these ontologies are used to  describe the native data in the heterogeneous databases.

The mappings, which are generally provided ab initio by  the databases owners, are described by correspondence  matrices.

When the system is searching for new rules, we first  construct a dynamic shared ontology (DSO) by analysing  the correspondence matrices to determine the derivable  granularity after integration. This phase, which involves  constructing the DSO, can be thought of as Metadata  Integration, where the metadata take the form of  ontologies. The resolution of such ontological conflicts  remains an important problem for database integration and  for Knowledge Discovery in Databases.

Once the metadata have been integrated, the aggregate  data, in the form of summaries, are then integrated by  minimisation of the Kullback-Leibler information  divergence using the EM (Expectation-Maximisation)  algorithm. The EM algorithm provides an intuitive  approach to the integration of aggregates. A significant  contribution of this paper in extending our previous work  [6, 9, 10] is that we no longer need to specify ontology  mappings to a global ontology; instead, dynamic shared  ontologies are computed on-the-fly. This approach  provides a way of mapping ontologies that is appropriate to  the Semantic Web, and which is much more flexible than  previous centralised approaches. In previous work [4] we  have used the EM algorithm for the related problem of  aggregation of imprecise and uncertain information for  knowledge discovery in databases.

Finally we show how the approach may be extended to  Knowledge Discovery using the Cartesian product of a  number of attributes; association rules or Bayesian Belief  Networks may then be induced. An architecture for a KDD  system in such an environment is also described; this is an  extension of a previous architecture for distributed data  processing that has been implemented as part of the EU-  funded MISSION project [8].

Our problem has a unique maximum likelihood solution  in that the Dynamic Shared Ontology is defined as the  finest level of granularity that is derivable from the  contributing ontologies. Our problem differs from that in  [3], where a common ontology is derived that can then be  used for querying. Such a common ontology is not  dynamic.

2. Definitions  Definition 2.1: An ontology is defined as the Cartesian- product of a number of attributes, along with their  corresponding classifications.

Definition 2.2: Two ontologies are defined to be  semantically equivalent if there is a mapping between their  respective classifications (domain values).

Definition 2.3: An ontology, together with a set of counts  for each element in the Cartesian-product of the  classifications in the ontologies, comprises a multi-  dimensional contingency table.

The product (or join) of two classifications P and Q is  the coarsest partition that is finer than both P and Q. The  sum (or meet) of two classifications P and Q is the finest  partition that is coarser than both P and Q. The relationship  between two classifications may be described by a  correspondence matrix [2, 3], in which a 1 represents a  mapping between the two categories and a zero represents  the absence of such a mapping. We use the term micro  class for an element of a product classification and the term  macro class for an element of a sum classification.

Definition 2.4: A combination of two classifications C1 and C2 is a classification whose classes are subsets of the  macro classes in the sum of C1 and C2 and supersets of the  micro classes in the product of C1 and C2.

In what follows, we develop an approach that seeks to  integrate aggregate views that are heterogeneous with  respect to classifications by computing the richest ontology  that can be derived from the contributing ontologies. This  we call the Dynamic Shared Ontology (DSO), which  provides terminology for the query response. Mappings  from the local data source ontologies to the DSO allow  semantic integration to be carried out. The DSO may be at  the granularity of the product of the classifications or at a  granularity that is intermediate between that of the product  and that of the contributing ontologies. Our aim here is to  answer the query at the finest (richest) granularity possible  using the data sources available. In many cases this will be  finer than any of the contributing ontologies, thus  providing a superstructure for the discovery of new  knowledge.

Definition 2.5: An ontology O1 is said to be refined by  another ontology O2 if O1 and O2 can be combined to  provide a derivable classification finer than O1. The refined  ontology is denoted by  O = O1 ? O2.

The correspondence matrices identify how the local  ontologies, via their classifications, map onto other  ontologies; these mappings can be stored in an ontology  server. Typically local ontologies map onto one of a  number of standard (or superior) ontologies, which then  map onto each other. We can then chain these mappings [5]  to provide mappings from local ontologies to the Dynamic  Shared Ontology. A query involving multiple datasets may  thus be answered using the Dynamic Shared Ontology that  is created by the algorithm described in Section 3. The  creation of the DSO involves analysis of the  correspondence matrices to determine the derivable  granularity after integration.

3. Metadata Integration - DSO  A query is issued using an ontology OQ = CQ1 x?x CQm,  where CQ1, ?, CQm are the classifications relating to  attributes (concepts) A1, ?, Am that are of interest to the  user. OQ is the user's chosen language of discourse. The  data used to answer this query are then from those datasets  in the system that adhere to ontologies that are semantically  equivalent to OQ. Thus we answer the query using datasets  that have classifications that we can map onto the relevant  classifications in OQ. Depending on the nature of these  mappings, we may be able to refine the classification in OQ using the classifications in Ci (for one or more of the  available datasets DSi , i = 1, ..., m), thus providing new  knowledge in the form of richer (finer grained) knowledge  than would be obtainable from any of the contributing  datasets individually. Typically, the user might create an  ontology OQ by mapping to the (superior) Ontology Server  ontology OS and the Data Providers D1 and D2 may have  mapped their data (with ontologies O1 and O2, respectively)  to OS. The system needs to construct an ontology of  discourse ? the DSO - and carry out the mappings  accordingly.

The algorithm Ontint that refines the contributing  ontologies to the DSO proceeds by iterating through the  classifications of each ontology to identify if a  classification can be used to refine OQ (the query ontology)  to form a richer DSO; this is called classification  refinement. If any classification of the current ontology can  refine a classification of the DSO (initially this is OQ) then  ontology refinement can be achieved; in this case we  update the classifications and correspondence matrices  accordingly. When we refine classification C1 by C2, for  each macro category of the sum of C1 and C2 we first test if  refinement is possible for that macro category; such  refinement is possible if the number of non-zero elements  in that macro category of the correspondence matrix is less  than macrosum, where macrosum is the sum of the number  of values (from C1 and C2) in the corresponding macro  category. For each macro category, the labels from C1 are  added to the refined classification C, if they cannot be  refined further. If a new element can be added to C,  corresponding to a label from C2, then this is done next.

Finally, the new value labels that are intersections of a  value from C1 and a value from C2 are added to the refined  ontology.

Once we have computed the DSO and correspondence  matrices, metadata integration is complete; the next stage,  which we discuss in Section 4, is to integrate the aggregate  data at a level of detail prescribed by the DSO.

4. Data Integration  We now develop an algorithm that allows us to integrate  different aggregates, or aggregate views, where the data  exhibit semantic heterogeneity. The approach, which     integrates the aggregates by minimisation of the Kullback-  Leibler information divergence using the EM algorithm, is  scalable to any number of databases. The EM (Expectation-  Maximisation) algorithm is a widely used general class of  iterative procedures used for learning in the presence of  missing information [1]. In our case, the problem may be  shown to belong to a general class termed Linear Inverse  Problems (LININPOS) by Vardi and Lee [11]. Such  problems may not in general have a unique solution;  however, we will show that, in our case, the process of  reduction to the Dynamic Shared Ontology is guaranteed to  lead to uniqueness. We integrate the aggregates to give  probabilities of each class in the Dynamic Shared  Ontology; such probabilities may then be further processed  for knowledge discovery, e.g., via association rules or  Bayesian Belief Networks. However, if required, we can  easily use the same framework to integrate cardinalities,  instead of probabilities, by scaling accordingly.

Notation 4.1: We consider a DSO with domain  D = {v1, ..., vk}.

The elements of D are the base values for attribute A,  which are grouped to give classes of the classification  schemes. Then for aggregate data views  Vr, r = 1, ?, m,  the domain D is partitioned into sets, or classes,  ( r1S , ?, rrgS ),  where nrs is the cardinality of set Srs; each partition  represents a classification. Here gr is the number of classes  in the classification for attribute A in aggregate data view  Vr. We further define:  ? ? ? ?  = otherwise0  S vif1 rsi irsq  and fr s = nr s/N (the proportion of values within the entire data collection), where  N= ?? = =  m  1r  g  1j  rj  r  n (the total cardinality).

The matrix Qr = {qirs} is the correspondence matrix that  relates the classification of Vr to the DSO.

Definition 4.1: The Kullback-Leibler information  divergence between two distributions  p = (p1, ?, pn)  and q = (q1, ?, qn)  is defined as  .)/log()||( j  jj?= qppqpD j  Definition 4.2: The aggregate integration operator from a  number of aggregate data views V1, ?, Vm derives  probabilities ?1, ?, ?k for values of attribute A, and is denoted aggint. It is defined as a vector-valued function:  aggint (V1, ?, Vm) = (?1, ?, ?k) where the ?I?s are computed from the iterative scheme:  k.1,...,=ifor))/((* m  1r  k  1u  urs  )1(  uirs  g  1=s  r  )1(  i  )(  i  r  ? ?? = =  ??= qqf ns nn ???  (1)  Here ?i is the probability of value vi for i = 1, ?, k for the integrated aggregate view in the Dynamic Shared  Ontology. Where the data are crisp, i.e., if we are trying to  determine vi from a singleton set { }iv=rsS  of aggregate Vr, then the contribution to ?i is simply frs. Otherwise, if { }iv  is a proper subset of rsS , we apportion the proportion frs to ?i where vi? rsS in the proportion  ?i /( ? ? rsj Sv  j? ).

The iteration equations (1) that define the aggint  operation minimise the Kullback-Leibler information  divergence between the aggregated probabilities {?i} and the data { }rsf . This is equivalent to maximising the likelihood of the model given the data. When the  distributed aggregates are summarised under the DSO, the  iterative scheme defined by equations (1) has a unique  solution. The scheme always converges to a set of feasible  probabilities (i.e., between 0 and 1 and summing to 1).

5. Knowledge Discovery and KDD  Architecture  In the previous sections we have described a methodology  for integrating metadata and data from semantically  heterogeneous databases based on a single attribute.

However, for Knowledge Discovery we are usually  interested in considering a number of attributes  simultaneously: our current approach readily extends to a  Cartesian product of attributes using the concept of a  datacube, as follows  Definition 5.1: An additive summary function ? is a set function the maps a set onto the integers or real numbers,  such that for two disjoint sets S1 and S2:  ?(S1 ? S2) = ?(S1) + ?(S2).

Definition 5.2: A datacube D consists of the Cartesian  product of attributes A1,?, An with corresponding  classifications C1, ?, Cn, along with additive summary  functions ?1,?, ?k. We restrict the definition to a single additive summary function ?, which aggregates by counting the number of values in a subset of the Cartesian  product.

Since the aggregation operator aggint allows us to  compute joint probabilities for any combination of  attributes, we may therefore use it to assess the validity of  any rules of this type. Thus, if we wish to examine the rule:  A1, ..., Ar ? B1, ..., Bs where A1, ..., Ar, B1, ..., Bs are all attributes of a relation R,  then we compute the joint probabilities:     Prob(A1, ?, B1, ..., Bs)  and  Prob(A1, ..., Ar)  from which we may assess the validity of the rule by  calculating the conditional probability:  Prob(B1,...,Bs | A1,...,Ar) = Prob(A1,...,Ar, B1,...,Bs) /  Prob(A1,...,Ar).

By using such an approach we may determine the strength  and support for various rules under consideration.

Thresholding then takes place to decide if the candidate  rules are sufficiently likely to merit informing the user.

We have previously developed the MISSION system  [7, 8], which provides a means of querying semantically  heterogeneous databases distributed over the Internet. The  result of a query to the MISSION system is a datacube (or  table) of the type commonly occurring in Data Warehouses  and Statistical Databases. The user interfaces via a query  interface that specifies the context of a query; the query  may be composed with the assistance of a browser, which  utilises agents to liaise with the Ontology Server.

Within the Agent Platform reside query agents for pre-  integration of semantically heterogeneous distributed data,  in particular matching and negotiation agents [5]. Other  query agents then construct an operator stack to transform  the data to match the query, or sub-queries. The modified  system, incorporating a Knowledge Discovery Engine, is  illustrated in Figure 1.

User  interface  Browser  Query interface  Client  Knowledge  Discovery Engine  Knowledge Base  Rule Extractor  Agent Platform  Matching agents  Negotiation Agents  Other Query Agents  Library Server  Ontology Server Library  Data Servers  Native  database  Native  database  Brokering Agent  Information Agent  Retriever  Figure 1. The KDD Architecture  6. Acknowledgements  This work was partially funded by MISSION and  COSMOS [8] within EUROSTAT?s FRAMEWORK V  EPROS initiative.

7. References  [1] Dempster, A.P., Laird, N.M., and Rubin, D.B.

Maximum Likelihood from Incomplete Data via the  EM Algorithm. Journal of the Royal Statistical  Society, Series B, 39, 1977, 1-38.

[2] Malvestuto, F.M. The Derivation Problem for  Summary Data. Proc. ACM-SIGMOD Conference on  Management of Data, New York, ACM, 1988, 87-96.

[3] Malvestuto, F.M. A Universal-Scheme Approach to  Statistical Databases containing Homogeneous  Summary Tables. ACM Transactions on Database  Systems, 18, 1993, 678-708.

[4] McClean, S.I., Scotney, B.W., and Shapcott, C.M.

Aggregation of Imprecise and Uncertain Information  Data Engineering (TKDE), 13(6), 2001, 902- 912.

[5] McClean, S.I., P?irc?ir, R., Scotney, B.W., and Greer,  K.R.C. A Negotiation Agent for Distributed  Heterogeneous Statistical Databases. Proc. 14th IEEE  Database Management (SSDBM), 2002, 207-216.

[6] McClean, S.I., Scotney, B.W., and Greer, K.R.C. A  Scalable Approach to Integrating Heterogeneous  Aggregate Views of Distributed Databases. IEEE  Transactions on Knowledge and Data Engineering  (TKDE), 2003, 15(1), 232-235.

[7] McClean, S.I., Karali, I., Scotney, B.W., Greer,  K.R.C., Kapos, G.-D., Pairceir, R., Hong, J., Bell,  D.A., and Hatzopolous, M. Agents for Querying  Distributed Statistical Databases over the Internet.

International Journal on Artificial Intelligence Tools  (IJAIT), 11(1), 2002, 63-94.

[8] MISSION (Multi-agent Integration of Shared  Statistical Information over the (inter)Net), IST project  No. 1999-10655; COSMOS (Cluster Of Systems of  Metadata for Official Statistics), IST project No. IST-  2000-26050.

[9] Scotney, B.W., and McClean, S.I. Efficient  Knowledge Discovery through the Integration of  Heterogeneous Data. Information and Software  Technology, 41, 1999, 569-578.

[10] Scotney, B.W., McClean, S.I., and Rodgers, M.C.

Optimal and Efficient Integration of Heterogeneous  Summary Tables in a Distributed Database. Data and  Knowledge Engineering, 29, 1999, 337-350.

[11] Vardi, Y., and Lee, D. From Image Deblurring to  Optimal Investments: Maximum Likelihood Solutions  for Positive Linear Inverse Problems (with discussion).

Journal of the Royal Statistical Society Series B, 1993, 569-612.

