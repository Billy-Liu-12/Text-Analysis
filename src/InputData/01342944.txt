

Abstract  This study proposes novel multiagent learning approach based on utilizing fuzzy mining for modular cooperative learning systems. It incorporates fuzziness and online analytical processing (OLAP) based mining to effectively process the information reported by agents. First, we describe a fuzzy data cube OLAP architecture to facilitate effective storage and processing of the state information reported by agents. This way, the action of the other agent, even not in the visual environment? of the agent under consideration, can simply be predicted by extracting online association rules from the constructed data cube. Second, we present a new action selection model, also based on association rules mining. Finally, we generalize not sufficiently experienced states, by mining multi-level association rules from the proposed fuzzy data cube. Experimental results obtained on a well- known pursuit domain show the robustness and effectiveness of the proposed approach.

Index Terms: data cube, data mining, fuzziness, multiagent systems, OLAP, reinforcement learning.

1. Introduction  One approach to model multiagent learning is to augment the state of each agent with the information about other existing agents [14, 17, 18]. However, as the number of agents in a multiagent environment increases, the state space of each agent grows exponentially. This way, even simple multiagent learning problems become computationally intractable by standard reinforcement learning approaches. In order to remedy the problem of combinatorial explosion in multiagent reinforcement learning, some methods have been proposed including modular architecture and generalization of states. Most of the proposals are based on Q-learning, which is the most widely used method in multiagent learning. However, it has some drawbacks, including modeling other learning agents and experiencing some states less than others  ? Visual depth is assumed 6 in this paper, unless otherwise specified.

during the learning phase. Having some states not experienced sufficiently does not prevent expecting an agent to select an appropriate action in each state. On the other hand, an agent cannot exhibit a certain behavior in some states that may be experienced sufficiently before completing the learning process.

In order to handle these problems, in this paper we propose a novel fuzzy modular architecture and learning approach for multiagent systems. The proposed approach integrates fuzzy online analytical processing (OLAP) [5] based association rules mining [2, 7] and modularity into the learning process. First, we describe a fuzzy data cube OLAP architecture which facilitates effective storage and processing of the state information reported by agents.

This way, the action of the other agent, even not in the visual environment of the agent under consideration, can simply be predicted by extracting online association rules from the constructed data cube. Second, we present a new action selection model, which is also based on association rules mining. Finally, we generalize not sufficiently experienced states, by mining multi-level association rules from the proposed fuzzy data cube. Experimental results obtained on a well-known pursuit domain show the robustness and effectiveness of the proposed fuzzy OLAP mining based modular learning approach. Finally, we tested the scalability of the approach presented in this paper and compared it with our previous work on modular-fuzzy Q-learning [11, 12].

The main contributions of our work described in this paper can be summarized as follows: 1) integrating the modular approach with the concepts of fuzziness and association rules mining; 2) defining a fuzzy data cube for both the internal model database and the database holding the Q-values; 3) predicting the actions of the other hunter agents even when they are not seen in the visual environment, by using internal model association rules mined from the fuzzy data cube; 4) selecting the appropriate action of the agent under consideration by using association rules mined from the constructed fuzzy data cube; and 5) generalizing states by mining multi- level association rules from the constructed fuzzy data cube in order to effectively accomplish the capturing task.

The rest of the paper is organized as follows. Section 2 provides the necessary background and related work.

0-7695-2101-0/04 $ 20.00 IEEE     Section 3 describes a variant of the pursuit problem, to be used as the platform for the experiments throughout this study. Our fuzzy OLAP mining based learning approach is presented in Section 4. In Section 5, we discuss the results of the experiments conducted for the considered environment. Section 6 is summary and conclusions.

2. Background and Related Work  2.1 Q-Learning Algorithm  According to the Q-learning process, the agent selects an action based on an action-value function, called Q- function, which is updated using the agent?s experience.

Definition 1 (Q-function): Given action a in state s and reward r, the Q-function of a in s, denoted Q(s, a) is formally defined as: )),(max(),()1(),( ??  Aa asQrasQasQ  ?? ++?= ??? ,  where A is the set of all possible actions, ? (0??<1) and ? (0???1) denote learning rate and discount parameter, respectively; and ),( ll asQ  is value of action al in state sl.

2.2 Fuzzy Association Rules  Given a set of items I, an association rule is a correlation of the form X?Y, where IYX ?)( U  and  ?=)( YXI . The intended meaning of X?Y is that when a given transaction ti? I contains the items in X, then it is most likely to contain the items in Y as well. A rule is generally rated according to several criteria, none of which should fall below a certain threshold.

Support defines the absolute number or the proportion of transactions present in D and contain YX U , i.e.,  ||)sup( YXDYX U=?  or ||  || )sup(  D  D YX YXU=? . Confidence is  the proportion of correct applications of the rule, i.e.,  ||  || )(  X  YX  D  D YXconf U=? , where DX={T?D | X?T} denotes  transactions present in database D and contain items in X, and |DX| is its cardinality.

Fuzzy association rules are also easily understandable to humans because of the linguistic terms associated with fuzziness [4, 6, 13]. To define fuzzy association rules, given a database of transactions T= {t1, t2, ?, tn}, its corresponding set of attributes I, and the fuzzy sets associated with quantitative attributes in I. Each transaction ti contains values of some attributes from I and each attribute in I has at least two corresponding fuzzy sets. We use the following form for fuzzy association rules [13]:  If   X={x1, x2, ?, xp} is A={f1, f2, ?, fp} then Y={y1, y2, ?, yq} is B={g1, g2, ?, gq},  where A and B contain the fuzzy sets associated with corresponding attributes in X and Y, respectively, i.e., fi is fuzzy set related to attribute xi and gj is the fuzzy set  related to attribute yj.  Finally, for a rule to be interesting, it should have enough support and high confidence.

2.3 Overview of Existing Approaches  There are several studies described in the literature where the internal model of each other learning agent is explicitly considered. For instance, Littmann [14] introduced 2-player zero-sum stochastic games for multiagent reinforcement learning. Hu and Wellman [9] introduced a different multiagent reinforcement learning method for 2-player general-sum games. However, according to both methods, while estimating the other agent?s Q-function, the agent under consideration should observe the other agent?s actions and the actual rewards received from the environment. Also, the former agent must know the parameters used in Q-learning of the latter agent. Then, Nagayuki et al [15] proposed another approach to handle this problem; their learning method is also based on Q-learning with one agent estimates the other agent?s policy instead of Q-function. Thus, there is no need to observe the other agent?s actual rewards received from the environment, and to know the parameters that the other agent uses for Q-learning.

Finally, Ishiwaka et al [10] presented a method for two kinds of prediction needed for each hunter agent acting in pursuit domain. One of these predictions is the location of the other hunter and prey agents, and the other is the movement direction of the prey at the next time step. For this reason, they performed some experiments on a continuous action state space and showed the effectiveness of their approach.

In a previous work [11, 12], we proposed a new and robust multiagent architecture by successfully combining advantages of the modular approach, fuzzy logic and the internal model.

(a) (b)  Fig. 1. Sample (a) initial position; (b) goal state  3. Problem Domain  Samples of the multiagent environment considered in this paper are shown in Figure 1. It is a variant of the well-known pursuit domain. It has the following characteristics. First, it is fully dynamic, partially  0-7695-2101-0/04 $ 20.00 IEEE     observable and non-deterministic. Second, five agents: four hunters and a prey exist in an n?n grid world as shown in Figure 1; the initial position of each agent is determined randomly. Third, at each time step, agents synchronously execute one out of five actions: staying at the current position or moving from the current position north, south, west, or east. More than one hunter agent can share the same cell. However, a hunter cannot share a cell with the prey. Also, an agent is not allowed to move off the environment. The latter two moves are considered illegal and any agent that tries an illegal move is not allowed to make the move and must stay in its current position. Finally, hunters are learning agents and the prey either escapes randomly when no hunter is located or moves in a way to maximize the total sum of Manhattan distance to the located hunters. Fourth, every agent can see objects at a certain distance. The distance and the cells it covers are, respectively, called the visual depth and the visual environment of the agent. A hunter can locate the relative position and recognize the type of the other agents in its visual environment. Finally, the prey is captured, either when the hunter agents occupy its four neighbor positions as shown in Figure 1-b, or when it is surrounded by hunter agents and the border of the environment (at one of the four corners). Then, the prey and all the four hunters are relocated at new random positions in the grid world and the next trial starts.

4. Fuzzy Association Rules Based Modular Reinforcement Learning  4.1 Fuzzy Data Cube Construction  Consider a quantitative attribute, say x, it is possible to define at least two corresponding fuzzy sets with a membership function per fuzzy set such that each value of attribute x qualifies to be in one or more of the fuzzy sets specified for attribute x.

Definition 2 (Degree of Membership): Consider an attribute x and let },...,,{ 21 lxxxx fffF =  be a set of l fuzzy sets  associated with x; jxf?  denotes the membership function of the  j-th fuzzy set jxf  in xF ; it  is a mapping from the  domain of x into the interval [0,1]. Formally, ]1,0[?= xf Domainjx?  For every value v of x, if  1)( =vj xf  ?  then v totally and  certainly belongs to fuzzy set jxf . On the other hand, 0)( =vj  xf ?  means that v is not a member of the fuzzy set  j xf . All the other values between 0 and 1, exclusive,  specify a partial membership degree.

The concept described in Definition 2 is used in  building a fuzzy data cube as outlined next.

Definition 3 (Fuzzy Data Cube): Consider a data cube with n dimensions, and given an association rules mining  task involved with dimensions d1, d2, ?, dn of the data  cube. Each dimension of the cube contains 1  +? =  k  i il  slots,  where k is the number of attributes in dimension X; il  is  the number of membership functions (fuzzy sets) for attribute xi in dimension X; and ?+1? in the formula represents a special slot named ?Total?, which stores the aggregation values of the previous slots. These aggregation values show one of the essential features of the fuzzy data cube structure.

D  left, down  left, middle  left, up  right, up  N ot  A vailable  left, down  left, middle  left, up  right, up  Not Available  othera  othera  othera  othera  othera  ...

...

Hunter  Prey  Action  Both Not Available  C ount 2  Count 1  Count 3  Total Count  ??????????? ??????????? ????  C  ???? ???????? ???????? ????  ??????? ???? ???? ????  Count 3  A B  Fig. 2. The proposed fuzzy data cube to hold internal model related knowledge  In a previous contribution, we grouped the state space of the hunter agents in a pursuit domain into different fuzzy labels; then we integrated the state space of the hunters with fuzzy internal model approach [11, 12]. In the study described in this paper, we facilitate viewing the relationship between the actions and the state space of agents from different perspectives by extracting interesting rules from the fuzzy data cube.

Shown in Figure 2 is a fuzzy data cube with three dimensions, representing the internal model of a hunter agent. Two dimensions of this cube deal with the states of the hunter and the prey in the hunter?s visual environment, and the third dimension represents the action space of the other hunter. In the fuzzy data cube shown in Figure 2, the dimensions concerning the state space are values of the two coordinates x and y, such as [left, down] and [left, middle], where left shows the x- coordinate, whereas down and middle represent the y- coordinate. Finally, each cell of the cube shown in Figure 2 holds the sharing rate computed with respect to the observed action of the other agent and states.

Proposition 1 (Sharing Rate Computation): Consider a fuzzy data cube with 3 dimensions, if the location of the other hunter has membership degrees )( hunterotherx?  and  )( hunterothery?  along the x and y axes, respectively; the location of the prey has corresponding membership degrees )( preyx?  and )( preyy? , respectively; and the estimated action of the other agent has membership degree ),( hunterotheraction?  then the sharing rate of all the  0-7695-2101-0/04 $ 20.00 IEEE     fuzzy sets to the corresponding cell is computed as: )().().( hunterotherpreyhunterother actionstatestate ??? , where  )().()( hunterotherhunterotherhunterother yxstate ??? =  and )().()( preypreyprey yxstate ??? = .

4.2 Mining from the Internal Model Data Cube  Most of the work already done on multiagent learning assumes a stationary environment, i.e., the behavior of the other agent is not considered in the environment. Whereas it is more natural to consider a dynamic environment in the sense that an agent always learns and each other agent may change its behavior with time too. In such a case, the standard Q-learning approach is not appropriate.

In the work described in this paper, as a given agent executes an action, we also consider the other agent?s action. For this purpose, we must have an internal model database to hold the actions of the other hunter agent.

Such database is constructed by employing a fuzzy logic approach and then transforming it into a fuzzy internal model data cube, as presented in the previous section.

This leads to the data cube shown in Figure 2. In this cube, as long as a hunter agent observes new states during the learning process, the observed action of the other agent in the corresponding state(s) is updated. So, as mentioned earlier, each cell contains the sharing rate calculated with respect to the given state and the observed action. Finally, in order to explicitly express the dependency of the other agent?s action, the hunter?s Q- function is adjusted according to the formalism given next in Definition 4.

Definition 4 (Hunter?s Q-Function):  Consider a hunter h1, which tries to estimate the action of an agent h2. The corresponding Q-function is represented as ),,,( otherself aaSQ  where S denotes a state that hunter h1 can observe; )( selfself Aa ? and )( otherother Aa ?  are actions of h1 and h2,  respectively; here, selfA and selfA  represent sets of all  possible actions for h1 and h2, respectively.  Let ),(maxarg ///  / otherAa other asFa  otherother ? = , )().()( preypreyprey yxstate ??? =  and )().()( hunterotherhunterotherhunterother yxstate ??? = , then  )],,(max.)[().(.

),,()].().(.1[),,( ////  **  / otherselfkAa  otherselfkotherselfk  aasQrhunterotherprey  aasQhunterotherpreyaasQ  selfself ? +  +??  ????  ???  According to Definition 4, deciding on whether the action of a given agent is good or not depends on the action of the other agent. In other words, othera  is a hidden  and major factor in selecting the action selfa . In this study,  the action othera  of the other agent is estimated based on the association rules extracted from the constructed data cube. If one hunter observes the other hunter in its visual environment, then the association rule otheraS ?  can be  easily mined from observations of the other hunter?s past actions. On the other hand, if one hunter could not perceive the other hunter, a prediction is done based on the following proposition in order to estimate the direction of the action of the unseen other agent.

Proposition 2 (predicting the action of the unseen hunter): Consider two hunter agents h1 and h2, and a prey agent P, if h1 cannot visualize h2, while P is at a certain location in the visual environment of h1 , then the action of h2 is predicted with respect to the general trend of the actions taken by h2 in the visual environment of h1 when P was at the same location.

For instance, assume that the prey is at location [6, 6] and the other hunter agent is out of the visual environment. In this case, a cell of row C of the data cube shown in Figure 2 is updated with respect to row D, which shows the number of times the prey visited location [6, 6], regardless of the position of the other hunter.

To satisfy data mining requirements as described in Section 2.3, the user specifies at the beginning of the learning process a minimum support value for the action count, indicated as count3 in Figure 2. If the count value of a state reaches this minimum support value, then it is assumed that the state has been experienced sufficiently.

In such a case, the hunter agent under consideration estimates the action of the other agent with respect to the highest confidence value. Otherwise, if a state is not experienced sufficiently, then the agent under consideration estimates the action of the other agent with respect to the user specified confidence value. If the number of occurrences of a state-action pair is less than the user specified minimum confidence value, then such action is not selected in the corresponding state. Also, if there are more actions exceeding the minimum confidence value in a state, then the possibility of selecting an action ai is computed as:  ? ? ? ?  = )(  )(  )( )|(  MinConfAa j  i i  j  aSconf  aSconf Sap  where )( iaSconf ?  is the confidence value of the rule  iaS ? , and )(MinConfA  is the possible set of actions that exceed the minimum confidence value of the corresponding agent. At the beginning of the learning stage, the minimum confidence value is set to 0%. Then, this value is increased gradually to a bigger value to let the agents learn the special properties of different situations; the exploration in the environment is encouraged at the early stages of the learning process.

4.3 Mining Based Multiagent Modular Learning  In this section, we present our approach of utilizing the constructed fuzzy data cube in mining OLAP association rules that show the state-action relationship, and hence guide agents in making their decisions on the next move.

The dimensions of this virtual data cube contain the state  0-7695-2101-0/04 $ 20.00 IEEE     information and the actions of both hunters. The developed modular architecture includes fuzzy internal state data cube to hold past actions of the other agents, and fuzzy learning data cube to contain the Q-values of the learning process. The mining process employed for this purpose is described in Algorithm 1.

Table 1. A part of a snapshot of the fuzzy data cube at a particular moment of the learning process  Action State  selfa  selfa  selfa  selfa  selfa  count  (s0, 5othera ) 52.125 94.657 24.696 83.232 34.763 128.10  (s1, 5 othera )  74.632 19.632 13.698 92.820 69.834 82.20  (s2, 5 othera )  91.852 64.633 25.367 41.140 80.978 462.10  (s3, 5 othera )  69.854 78.012 96.325 53.901 12.658 45.13  S1  Total(S1) 288.463 256.934 160.086 271.093 198.233 717.53 .

.

.

(s0, 2othera ) 23.478 87.412 74.987 54.410 92.103 514.0  (s1, 2othera ) 8.954 91.789 73.587 45.031 76.657 97.8  (s2, 2othera ) 72.683 54.312 37.189 87.205 10.002 214.0  (s3, 2othera ) 94.127 51.978 7.014 46.879 34.478 310.6  Sn  Total(Sn) 199.242 285.491 192.777 233.525 231.240 1136.4  Algorithm 1: (Mining Based Multiagent Learning) The proposed mining based multiagent learning process involves the following steps: 1. The hunter under consideration observes the current  state S and estimates the other agent?s action othera  based on the association rules extracted from the fuzzy data cube. If the occurrence number of S  is greater than the specified minimum support value, then action othera , having the highest confidence value, is selected. If the occurrence number of S  is less than the minimum support value, then the action othera  is selected, based on the value of )|( Sap i , from the actions exceeding the  minimum confidence value in state S  at that moment.

2. The action selfa  is selected according to the estimated  value othera . For instance, Table 1 shows a part of a snapshot of the data cube at a certain moment of the learning process. While each cell gives the Q-value of the corresponding state-action pair, the count variable indicates the number of occurrences of the corresponding state in case of chosen othera  (assume that  othera  is chosen). In a way similar to the previous  association rules mining process, if the total count value of a state- 5  othera  pair is greater than or equal to the  minimum support value determined before, then it is assumed that the relevant state and 5othera  were experienced sufficiently. In this case, the hunter agent under consideration selects the action with the highest confidence value.

3.  Each cell in the row Total(S1) in Table 1 shows the sum of Q-values of the corresponding state- selfa  pair in  case 5othera  is chosen. If the state- 5othera  pair is not experienced sufficiently, the agent selects its action with respect to )),(|( ? selfself aSQap 1 ;  4.  After the state-action rules have been found for all the available modules, the hunter under consideration executes the action  selfa  selected by the mediator rule  miner. The action is chosen based on the criteria:  ? =  =?    l  i otherselfi  Aa aaSQ  selfself  ),,(maxarg  5.  Simultaneously, the other hunter executes action * othera .

6.  The environment changes to a new state /S 7.  The hunter under consideration receives a reward r  from the environment and updates the fuzzy data cube as follows: 7.1 All the cells )),(( *otherk aSsF ?  are updated in the  fuzzy internal model data cube.

7.2 All the cells ),),(( *otherselfk aaSsQ ?  are updated  according to Definition 4.

8. If the new state /S  satisfies a terminal condition, then  terminate the current trial. Otherwise, let SS ?/  and go back to step 1.

Up Not Available  left  right  Not Available  othera  othera  othera  othera  othera  Hunter  Prey  Action  Both Not Available  Total  Total  Total  Total Sum Total  ??? ???  ????? ????? ???  ??? ??? ???middle ??? ??? ???  ?????? ??????  ?????? ??????  Middle Down   Fig. 3. The cube generated from rolling up  4.4. State Generalization by Mining Multiple- Level Fuzzy Association Rules  Experiments showed that when the learning process is over, some state-action pairs are experienced sufficiently, while others are never visited according to the escaping policy of the prey. The agent selects its own action based on the information obtained from the environment. In some states, this information is not sufficient to mine association rules. In such a case, the information or the data is generalized from low levels to higher levels.

The new data cube shown in Figure 3 is obtained by rolling up the fuzzy data cube shown in Figure 2 along the two dimensions: Prey and Hunter. Different strategies of setting minimum support threshold for different levels of abstraction can be used, depending on whether a threshold is to be changed at different levels [8]. We decided to use reduced minimum support at lower levels, where lower  0-7695-2101-0/04 $ 20.00 IEEE     levels of abstraction use smaller minimum support values; this is the common trend for most work on OLAP mining.

5. Experimental Results  We conducted some experiments to test the effectiveness of learning by extracting association rules that correlate states and actions. We concentrated on testing changes in the main factors that affect the proposed learning process: minimum support, minimum confidence and visual depth. All the experiments have been conducted on a Pentium III 1.4GHz CPU with 512 MB of memory and running Windows 2000. Further, the learning process in the experiments consists of a series of trials and each reported result is the average value over 10 distinct runs. Each trial begins with a single prey and four hunters placed at random positions inside the domain and ends when either the prey is captured or at 2000 time steps. Individual hunters immediately receive a reward=100 upon capturing the prey.

To show the effectiveness and applicability of the approach presented in this paper, we compare it with our previous work on modular-fuzzy Q-learning [11, 12], FQL in the rest of this section. Finally, we used the following parameters in the Q-learning process: learning rate ?=0.8, discount factor ?=0.9, the initial value of the Q-function is 0.1, and the visual depth of the agents is set to 6, unless specified otherwise. The domain size was chosen as 30?30; and in all the experiments, the x-axis represents the number of trials and the y-axis gives the average number of steps required to capture the prey.

1 000  1 250  1 500  1 750  2 000  0 500 0 1 00 00 1 500 0 2 000 0 2 50 00 300 00 350 00 4 00 00 450 00 500 00  Trials  A ve  ra ge  T im  e St  ep s  MinSup6K(Conf 0%->20%) MinSup6K(Conf 10%->20%)  MinSup8K(Conf 0%->20%) FQL  Fig. 4. Learning curves of hunters for different values of minimum support and confidence  The first experiment is dedicated to investigate the learning curves for different minimum support and confidence values; the results are shown in Figure 4.

Three curves represent results for the approach presented in this paper and one curve corresponds to FQL. It can be easily seen from Figure 4 that learning curve labeled MinSup6K(Conf 0%?20%)), minimum support value set to 6K, converges to the near optimal solution faster than that labeled MinSup8K(Conf 0%?20%), although the latter requires less number of steps. On the other hand,  when the minimum support value is fixed at 6K and the minimum confidence value is changed from 10% to 20%, the agent converges slower and requires more steps to capture the prey because the agent is not given the opportunity to discover its environment enough as in the case when the minimum confidence starts at 0%. The curves plotted in Figure 4 also demonstrate advantage of the proposed approach over FQL in terms of average steps and number of trials.

0 10000 20000 30000 40000 50000  Trials  A v  er ag  e Ti  m e  St ep  s  FS3VD6  FS3VD8  FS4VD6  FS4VD8  Fig. 5. Effect of the change in visual depth and number of fuzzy set  The second experiment investigates the effect of the number of fuzzy sets and the value of visual depth. Here, we compared four different cases namely, FS3VD6, FS3VD8, FS4VD6 and FS4VD8, where FS and VD denote, respectively, number of fuzzy sets and visual depth. The results are plotted in Figure 5. When the visual depth and the number of fuzzy sets are set at 4 and 8, respectively, we realized that the decision space resolution of the hunter decreases. However, the hunter captures the prey faster because it visualizes a larger area.

On the other hand, by selecting the number of fuzzy sets as 4 and setting the visual depth to 6, we observed a slower convergence but less convergence steps.

The next two experiments are dedicated to investigate the multiple levels case, and the main goal is to generalize a state that has not been experienced enough with the other states. In such cases, the agent observes the environment from a higher level and decides on the best action. In both experiments, the number of fuzzy sets is set to 4 and the visual depth of the agents is set to 6. The results are reported in Figure 6 and Figure 7.

In Figure 6, we concentrated on two different total count values (200K and 250K) and two different minimum support values (6K and 8K). When the total count values reach the pre-determined threshold value and the minimum support value of the current state is below the threshold, the agent goes up to a high level in order to get more general information from the environment.

While Figure 6 gives the results for the case when only states of the hunter agents are generalized, Figure 7 shows the results in case states of both the prey and the hunter agents are generalized. It can be easily seen from Figure 7 that it is not a good approach to generalize states of the prey and the hunter agents together.

0-7695-2101-0/04 $ 20.00 IEEE              0 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000  Trials  A ve  ra ge  T im  e S  te ps  TotalCount200K(6K) TotalCount200K(8K) TotalCount250K(8K) FQL  Fig. 6. Learning curves when only states of hunter agents are generalized           0 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000  Trials  A ve  ra ge  T im  e S  te ps  TotalCount200K(6K)  TotalCount250K(8K)  Fig. 7. Learning curves when states of both prey and hunters are generalized  6. Summary and Conclusions  In this paper, we proposed a novel multiagent reinforcement learning approach based on fuzzy OLAP association rules mining. For this purpose, we started by embedding the fuzzy set concept into the state space in order to decrease the number of states that an agent could encounter. Then, we defined a fuzzy data cube for holding all environment related information obtained by agents.

By using this cube effectively, we extracted fuzzy association rules from the previous actions of agents.

Based on these rules, we handled two important problems that are frequently faced in multiagent learning. First, we estimated the action of the other hunter agent, even when it is not in the visual environment of the agent under consideration. Second, we presented a new action selection method in order for the agents to take the most appropriate action. For this purpose, we generalized the states that were not experienced sufficiently. In fact, multiagent learning is a very difficult problem in general, and the results obtained may depend on specific attributes of the problem. However, experimental results obtained on a well-known pursuit domain showed that the proposed fuzzy OLAP mining based learning approach is promising for emerging adaptive behavior of multiagent systems. Currently, we are investigating the possibility of applying our method to more complex problems that require continuous state space and to develop and improve different corresponding algorithms.

