<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">A  Dichotomous Algorithm for Association Rule Mining

Abstract The analysis of large amounts of data requires important computing resources that may not be available, even in current environment, and there are traditionally two main ways for solving this problem. The first is to use multi-processor machines, and the second is to use computer clusters. The main drawbacks of these solutions are the expensive cost of machines and their specific utilization. In order to avoid these drawbacks, distributed algorithms for mining association rules have been proposed. However, these algorithms either run high-synchronous methods or lack flexibility to be adapted to machines with limited available resources. In this paper, a distributed dichotomous algorithm (DDA) is proposed for association rule mining. The main features of DDA are that this algorithm does not require a high level of synchronization and that it does not process data replication and redundant calculations. In addition, DDA can partition recursively the tasks and the data set so as to be processed by machines with limited available resources.

1. Introduction Since the amount of collected data to be mined is often very large in data mining procedures, parallel mining algorithms are required. The design of parallel mining algorithms in the environment of multi-processors machines or in the environment of computer clusters has motivated several research works (for example [2, 3, 4, 7, 8, 10]). However, the main drawbacks of these algorithms are the expensive cost of machines and their specific utilization for data mining tasks. Hence, it is interesting to implement a parallel algorithm in a non-specific environment, in which computers are connected by a network and used for everyday work. In such an environment, the implemented algorithm works at idle time of these computers, for example during the night or during the weekend.

The main problem for implementing such an algorithm is that the resources are diverse and limited. Existing distributed algorithms (for example [2, 10]) do not adapt well to this kind of environment because of the following reasons: the required resources are generally important, the level of synchronization is high, and data replication as well as redundant calculations are necessary.

In this paper, we propose a new algorithm, called distributed dichotomous algorithm (DDA) to cope with the problems just mentioned. DDA works without data replication and redundant calculations, and moreover, the required degree of synchronization is low. Additionally, the flexibility of DDA allows to partition recursively the tasks and the data set until they fit the limited resources of computers.

The organization of the rest of this paper is the following. Section 2 gives a brief review of association rule mining and the Apriori algorithm [1], which is the  core of most algorithms for association rule mining. The introduction of several existing distributed algorithms for association rule mining is also included. The algorithm DDA is presented in Section 3, and Section 4 concludes with a presentation of our ongoing works.

2. Related work    2. Related work 2.1 Problem statement We recall from [1] the basic concepts and terminology of association rule mining. Let I = {i1, ?, il} be a set of Proceedings of the 15th International Workshop on Database and Expert Systems Applications (DEXA?04) 1529-4188/04 $ 20.00 IEEE N TXY items. Let D be a database consisting of a set of transactions, where each transaction T consists of a unique identifier (tid) associated with a set of items T_items such that T_items ? I. Given an itemset X ? I, a transaction T contains X if X ? T_items.

An association rule is an implication of the form X ? Y, where X ? I, Y ? I and X ? Y = ?. The association rule X ? Y holds in D with confidence c if the probability that a transaction in D which contains X also contains Y is c. The association rule X ? Y has support s in D if the probability that a transaction in D contains both X and Y is s. More formally, the support and the confidence of a given association rule X ? Y, denoted by sup(X ? Y) and conf (X ? Y) respectively, are defined as follows: sup(X ? Y) = and conf (X ? Y) = where N is the total number of transactions and where TXY and TX denote respectively the number of transactions containing X ? Y and the number of transactions containing X. The ratios (TXY /N) and (TX /N) are also called the support of X ? Y and the support of X, respectively.

The task of mining association rules is to find all the association rules whose support is greater than a user- specified minimum support threshold and whose confidence is greater than a user-specified minimum confidence threshold. An itemset X is large (or frequent) if its support is greater than the minimum support threshold. An itemset of size k is called a k-itemset, and we recall that all subsets of a large itemset are large.

The main task in association rule mining is to find all large itemsets, since all association rules whose support and confidence are greater than the corresponding thresholds can be generated from the large itemsets without having to access the data set.

2.2 Apriori algorithm The Apriori algorithm [1] is now recognized as a standard that is used as the basis in many other algorithms that aim at improving the performance of the computation.

Since our algorithm DDA is based on the Apriori algorithm, we briefly recall it below.

Apriori Algorithm Input: a dataset D; Output: All large n?itemsets; Begin Compute the set L1 of all large 1?itemsets; i = 2; While Li?1 ? ? do Generate Ci from Li?1; // Ci = new candidate itemset of size i; Prune Ci from Li?1;  For all transaction T ? D do Increment the count of all candidates in Ci that are contained in T; Li = { c ? Ci | c is large}; i = i+1; End while; Return ?i Li; End We also recall that there are two principal data layouts for association rule mining task: the horizontal data layout and the vertical data layout. The data set of    layout and the vertical data layout. The data set of horizontal data layout consists of a set of transaction identifiers (tid). Each tid is associated with the list of items contained in this transaction. On the other hand, in vertical data layout, a data set consists of a set of items associated with a tid-list. The tid-list for each item is the list of transaction ids that contain this item.

2.3 Distributed data mining algorithms Several distributed algorithms have been proposed for association rule mining (for example [2, 3, 4, 10]). In [6, 9], surveys of these algorithms are presented. These algorithms can be generally classified in three types: count distribution algorithm, data distribution algorithm, and itemset clustering algorithm. The basic concepts of these algorithms are the following.

Count distribution algorithm The data set is partitioned horizontally and distributed to each processor. All processors generate the complete set of k?itemset candidates Ck from the set of large (n?1)?itemsets Lk?1. Each processor thus counts independently Ck with its local data set of the partition.

Next, the processors exchange their local counts concerning Ck to get the global counts.

We note that all processors must synchronize to get the global counts. Since only the local counts of candidates are exchanged among the processors, the communication in the network is largely reduced.

Nevertheless, the algorithm does not use the memory of each processor efficiently, since each processor is used for counting the same candidates at the same time.

Data distribution algorithm Instead of counting the same candidates, each processor in the data distribution algorithm counts mutually exclusive candidates. In order to obtain the X XY T T Proceedings of the 15th International Workshop on Database and Expert Systems Applications (DEXA?04) 1529-4188/04 $ 20.00 IEEE global counts of the candidate n?itemsets, each processor has to broadcast its local data to all other processors.

As a consequence, in spite of its efficient use of memory, the data distribution algorithm requires a high degree of synchronization and processes to data exchanges between the machines.

Itemset clustering algorithm In this kind of algorithms, large itemsets are computed separately by different clusters. Clusters are formed form different criteria, such as prefixes in [2] or potential maximal large itemsets in [10]. The information related to each cluster is gathered together in order to facilitate the computing.

The main advantage of the itemset clustering  algorithm lies in the asynchronized computing of large itemsets among the processors, but at the cost of data replication and redundant computation among the processors.

3. General Dichotomous Distributed Algorithm 3.1 Partitions of itemsets The essential idea of our algorithm is to partition the sets of candidate itemsets. The set L1 = {i1, ?, in}, of all large 1?itemset is first partitioned into two or three subsets (according to the parity of the cardinality of L1).

Those subsets are then used to partition the set of all k?itemsets (for a given k &gt; 1) accordingly.

Assuming that we have two machines M1 and M2 for computing the large itemsets, it is then possible to distribute the information related to the sets of candidates    distribute the information related to the sets of candidates at any level n so that no redundant computation occurs.

We illustrate our method in the following example.

Example 1 Let us consider the following set of large 1?itemsets L1 = {a, b, c, d, e} In this case, we partition L1 into three subsets A, M, and B such that A = {a, b}, M = {c} and B = {d, e}. The tid-lists associated to a, b and c (c, d and e, respectively) are assigned to M1 (M2, respectively). Then, the candidate 2?itemsets are partitioned so that ab, ac, bc, ad and ae are assigned to M1 and de, cd, ce, bd, and be are assigned to M2. The way this partitioning is processed is explained later on in this section.

Note that M1 can process independently all large itemsets built up form items a, b or c and that M2 can process independently all large itemsets built up form items c, d or e. Moreover, the information concerning d and e is transferred from M2 to M1 for generating ad and ae. Similarly, the information concerning b is transferred from M1 to M2 for generating bd and be.

We emphasize that the introduction of the set M is necessary for balancing the tasks of each machine. The detail of such request can be found in [5].

As in the Apriori algorithm, the candidate itemsets are considered according to a fixed order in L1 denoted by &lt;i.

Then, L1 is partitioned into three sets A, B, and M as follows: - if n = 2*m is even, let A = {i1, ?, im}, B = {im+1, ?, i2*m} and M = ?; - otherwise, assuming n = 2*m+1, let A = {i1, ?, im}, M = {im+1}, and B = {im+2, ?, i2*m+1}.

For the sake of simplification, our presentation will concentrate on the case where n = 2*m+1. Note that the presentation is valid for the case of n = 2*m by dropping the statements concerning M.

Let k be an integer such that 1 ? k ? n, and let Ik be the set of all k?itemsets that can be obtained from the items in L1. Then the following sets form a partition of Ik: A1?Ak = {?1??k | ?j, 1 ? j ? k, ?j?A }; A1?Ak?1M = {?1??k | ?j, 1 ? j ? k?1, ?j?A, ?k?M}; A1?ApB1?Bk?p = {?1??k | ?j, 1 ? j ? p, ?j?A, ?j, p+1 ? j ? k, ?j?B} for 1 ? p ? k?1; A1?ApMB1?Bk?1?p = {?1??k | ?j, 1 ? j ? p, ?j?A, ?p+1?M, ?j, p+2 ? j ? k, ?j?B} for 1 ? p ? k?2; MB1?Bk?1 = {?1??k | ?1?M, ?j, 2 ? j ? k?1, ?j?B}; B1?Bk = {?1??k | ?j, 1 ? j ? k, ?j?B }.

Hence, if M is non-empty, there are 2*k+1 sets in the partition of k?itemsets, and in the case where M is empty, there are k+1 sets in the partition of k?itemsets. We order  these sets as follows: A1?Ak &lt;P A1?Ak?1M &lt;P A1?Ak?1B1 &lt;P A1?Ak?2MB1 &lt;P A1?Ak?2B1B2 &lt;P ? &lt;P A1B1?Bk?1 &lt;P M B1?Bk?1 &lt;P B1?Bk.

Example 1 (continued) Recalling that we previously considered the following set of large 1?itemsets L1 = {a, b, c, d, e}, with A = {a, b}, M = {c}, B = {d, e}, and for k = 2, we have: A1A2 = {ab}, A1M = {ac, bc}, A1B1 = {ad, ae, bd, be}, MB1 = {cd, ce}, B1B2 = {de}.

Moreover, in this case, we have: A1A2 &lt;P A1M &lt;P A1B1 &lt;P MB1&lt;P B1B2 3.2 Dichotomous Distributed Algorithm One of the main features of our algorithm is to balance the workload of the machines involved in the computation. To this end, the sets of k?itemsets defined Proceedings of the 15th International Workshop on Database and Expert Systems Applications (DEXA?04) 1529-4188/04 $ 20.00 IEEE    1529-4188/04 $ 20.00 IEEE earlier are assigned to each machine M1 and M2 so as they both have the same number of candidates to process.

Considering the case where M is not empty, we recall that we have 2*k + 1 such sets. Therefore, we assign the first k sets in the partition to M1, and the last k sets to M2. The (k+1)th set is split into two subsets of the same cardinality, each of them being assigned to one machine.

It can be seen that the (k+1)th set in the partition is of the form A1X1?Xk?1 where X is either A or M or B, according to the value of k. The two subsets of A1X1?Xk?1 are denoted (A1X1?Xk?1)1 and (A1X1?Xk?1)2, meaning that (A1X1?Xk?1)1 is assigned to M1 and (A1X1?Xk?1)2 to M2.

We now give an abstract version of our algorithm called Distributed Dichotomous Algorithm (DDA) for only one machine, and in the case where we have an odd number of large 1-itemsets. The full version of the algorithm can be found in [5], and we recall that the case of an even cardinality of large 1?itemsets can be obtained by deleting the parts containing the set M (since M = ? in this case). Furthermore, DDA is based on the vertical data layout.

Distributed Dichotomous Algorithm (DDA) Input: M1 (Machine 1): set of large 1?itemsets A = {a1, ?, am} and a set M = {m}; M2 (Machine 2): set of large 1?itemsets B = {b1, ?, bm} and a set M = {m}; where m is a large 1?itemset and a1 &lt;i?&lt;i am &lt;i m &lt;i b1 &lt;i ?&lt;i bm for an order &lt;i.

Output: All large n?itemsets related to A ? M ? B // The computations by M1 are not presented here, see [5] // for a complete version of the algorithm For M2 Begin i = 1; // i is the length of itemset M2Li = M ? B; // Large i?itemsets in M2 While M2Li ? ? do Case of i = 1 Transfer {a4k+2, a4k+3 | ?k, 0 ? k &lt; m/4} from M1; generateCandidates(M2Li+1); pruneVerifyCandidates(M2Li+1); i = 2 Transfer A1M and (A1B1)1 from M1; generateCandidates(M2Li+1); pruneVerifyCandidates(M2Li+1); i = 2*k+1 (k ? 1) generateCandidates(M2Li+1); Ask (A1?AkMB1?Bk)1 and A1?Ak+1B1?Bk from M1; PruneVerifyCandidates(M2Li+1); i = 2*k (k &gt; 1) generateCandidates(M2Li+1); Ask (A1?AkB1?Bk)1 and A1?AkMB1?Bk?1 from M1; PruneVerifyCandidates(M2Li+1); End case i = i+1; End while Return ?i M2Li; End The procedure generateCandidates generates the candidate itemsets as in Apriori (an (n+1)?itemset candidate ?1??n+1 is generated from two large n?itemsets ?1??n?1?n and ?1??n?1?n+1 that differ only on their last item), except in the following cases for M2: when n = 2*k with k &gt; 1, for the sets (A1?AkB1?Bk)2 and    with k &gt; 1, for the sets (A1?AkB1?Bk)2 and A1?Ak?1MB1?Bk, and when n = 2*k+1 with k &gt; 1, for the sets (A1?AkMB1?Bk)2 and A1?AkB1?Bk+1.

For example, according to the method in Apriori, the generation of the (2*k+1)-itemset candidates in the set (A1?AkMB1?Bk)2 in M2 needs the large (2*k)-itemsets in the set A1?AkMB1?Bk-1, that is stored in M1. In addition, the (2*k+1)-itemset candidates in the set A1?AkB1?Bk+1 in M2 are generated form the large (2*k)- itemsets in the set A1?AkB1?Bk, and half of this set, (A1?AkB1?Bk)1, is stored in M1. Hence, in order to reduce the dependency and data communication between machines, the candidates in the sets (A1?AkMB1?Bk)2 and A1?AkB1?Bk+1 are generated from the data in M2 only as follows: an (n+1)?itemset candidate ?1??n+1 is generated from two large n?itemsets ?1?3??n+1 and ?2?3??n+1 that differ only on their first item. With this modification, the candidate itemsets in (A1?AkMB1?Bk)2 can be computed from A1?Ak?1MB1?Bk in M2, and the candidates in A1?AkB1?Bk+1 can be obtained from A1? Ak?1B1?Bk+1 in M2. The same principle can also be applied to the generation of candidate itemsets in (A1?AkB1?Bk)2 and in A1?Ak?1MB1?Bk with k &gt; 1.

The aim of the procedure Ask is to obtain from the other machine the information concerning certain large itemsets in order to prune the itemset candidates. For example, the call by M2 ?Ask (A1?AkB1?Bk)1 and A1?AkMB1?Bk-1 from M1? allows M2 to know from M1 all large itemsets from the sets (A1?AkB1?Bk)1 and A1?AkMB1?Bk-1 for pruning the candidates in (A1?AkMB1?Bk)2 and in A1?AkB1?Bk+1. It is important to note that a call to this procedure does not result in the transfer of tid-lists. Only the identifiers of the large itemsets are transferred. On the other hand, such a transfer Proceedings of the 15th International Workshop on Database and Expert Systems Applications (DEXA?04) 1529-4188/04 $ 20.00 IEEE occurs in the calls of the procedure Transfer, but in this case, the tid-lists associated to the corresponding itemsets are also transferred. Note however that such calls are only necessary for getting large 2? and 3?itemsets.

Since every itemset is associated with its tid-list, the verification of a (n+1)?itemset is obtained from the intersection of the tid-lists associated with the two large n?itemsets that generate it. We also recall that DDA does not transfer any tid-list between machines for finding large n?itemsets for n ? 4, and that only some of these sets need additional information for pruning. Moreover, if this additional information from the other machine, obtained by the procedure Ask, cannot arrive in time, DDA calculates directly the large itemsets with the concerned tid-lists. In this case, however, it may happen that some pruning is not performed. As a consequence, we claim that DDA does not require a high level of synchronization and does not process to important data transfer. Furthermore, it is worth noting that, since we partition the itemsets at each level, DDA does not proceed to redundant computations, i.e. all itemsets are processed by one machine, only.

The following proposition states that our algorithm DDA is correct and complete, in the sense that all large itemsets are computed. The details of the proof can be found in [5].

Proposition - DDA is correct and complete.

It is important to note that our method also allows considering more than two machines. Indeed, the different sets of itemsets to be processed can be partitioned recursively in order to adapt to machines whose available resources are limited. We recall in this respect that there is no such flexibility in the itemset clustering algorithm.

Moreover, in these algorithms, the reduction of    Moreover, in these algorithms, the reduction of synchronization level is possible, but at the cost of an increase of data replication and of redundant calculations, which is not the case in our approach.

Concerning the count distribute algorithm and data distribute algorithm, they can adapt to diverse machine resources. However, contrary to our approach, their level of synchronization is high, which causes a significant reduction of their performance in highly distributed environments.

4. Conclusion In this paper, the distributed dichotomous algorithm (DDA) based on the partition of itemsets is proposed for association rule mining. The main features of our algorithm are that it does not require a high level of synchronization and that it works without data replication and redundant calculations. In addition, thanks to its flexibility, it can be applied to environments in which available computing resources are limited.

We are currently working on the implementation of the algorithm in different environments. Moreover, we shall compare the efficiency of DDA with other existing algorithms mentioned in this paper. We also pay attention to possible optimizations of the algorithm under different network environments. For example it is well known that the policy of data transfer depends on the network technology: in the presence of hubs, the information is transferred to all machines, whereas in the presence of switches, the information is transferred to the receivers only. It is clear that these different environments may have an impact on the way information transfer can be optimized in our algorithm.

