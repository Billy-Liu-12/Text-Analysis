

?This research was supported by the DFG (project 524/88) and by the Heinz-Nixdorf-Foundation  anger baby blue light ocean working mad 353 boy 162 sky 175 dark 647 water 314 hard 132 fear 83 child 142 red 160 lamp 78 sea 233 loafing 99 hate 61 cry 113 green 125 bright 30 blue 111 sleeping 79 rage 49 mother 71 color 66 sun 25 deep 48 playing 65 temper 35 girl 51 yellow 56 bulb 23 waves 48 man 48    For the calculation of the connective weights, a window is shifted from left to right over the text.

After each shift the connections between all pairs of words occuring in the window are updated.

Two different types of windows have been compared:  1. sentence windows which include all words in a sentence. During the process of learning they  2. continuous windowk which contain a constant number of words and are shifted over the text  Seven different types of connections have been computed. The corresponding formulas are derived from different assumptions about autoassociative learning.

In formula 1 the weight between two words i and j is identical to the number of windows or sentences in which both words occur together. It is based on the assumption that the associative strength between two words increases by a constant amount whenever they appear in close neigh- borhood, independent of their overall frequencies. This formula gives symmetric weights with a range between zero and the number of windows or sentences.

are shifted sentence by sentence;  word by word.

wij = f . P ( i  & j ) where wi, is the associative weight between word i and word j, f is the total number of windows, and P ( i  & j) is the probability of words i and j both occuring in a window.

In formula 2 the association from word i to word j is identical to the conditional probability of word j given i. Here the sum of all weights which emanate from a node tends to be the same for all t4e nodes of the network. This property is in agreement with the so-called fan effect, i. e.

the observation of Anderson [l] and others that the amount of activation which spreads out from a node i to  a node j is inversely related to the sum of all associations leading from i.

where P ( i )  is the probability of word i occuring in a window.

Formula 3 gives the conditional probability of word i given j. Its value decreases whenever  j is observed without i .  This effect would be in accordance with the observation of Barnes & Underwood [2] and others that associative learning is impeded by retroactive inhibition.

P(i  & j )  W ?  - - lJ - P ( j )  (3)  Formula 4 is equal to the probability that i and j CO-occur, divided by the probability of their joint co-occurrence in case of independence.

P ( i  & j )  20 . .  - - a? - P ( i ) P ( j )  (4)  Taking the logarithm of equation 4 leads us to formula 5 defining ?mutual information?, which is used by Church & Hanks [3] in a similar context.

Formulas 1 to  4 yield positive weights. Formulas 1, 4 and 5 give symmetric weights, and formulas 2 and 3 asymmetric weights with a maximum value of 1.

Formula 6 is based on the assumption that the connective weight between two words is equal to the difference between an excitatory and an inhibitory relation, the latter being reinforced whenever one word appears without the other. The derivation for this formula is given in [8].

I  Formula 7 was suggested in [7] for the construction of a constraint satisfaction network that relates typical features to different types of objects.

P(1i & j ) P ( i  & +) P(i  & j )P(l i  & -+)  w;j = -In (7)  where P(i & ~ j )  is the probability that word i but not j occurs in a window.

In formulas 1 to 7 the connective weights between two words are independent of the order of  succession of these words in the text. Many studies of human memory have shown, however, that after the successive presentation of two words the association of the former to the latter word, the SO- called forward association, will be stronger than its reciprocal, the so-called backward association.

In order to take account of this observation we constructed, for each of the formulas 1 to 7, two variants. In the first variant, the term P(i & j) has been replaced by P(i --+ j ) ,  the probability that j appears in a window after i, and in the second variant P(i & j) has been replaced by P(i + j ) , where P(i & j )  = P(i  -+ j )  + P(i  6 j ) .

3. Prediction of the Associative Responses For the prediction of the associative responses of the Russel & Jenkins study, we have compared two assumptions. According to the first assumption (later on referred to as weight model), the associative response should be the word with the highest connective weight to the stimulus. In this case the probability that a subject responds with word j to a given stimulus i should be proportional to the connective weight wij.

According to the second assumption (the propagation model), the associative stimulus induces a process of propagation where an activation emanates from the stimulus word and spreads out over the lexical network in several steps. Hereby the word which receives the highest activity during propagation is considered as the associative response of the system. To test this assumption we activate, for each example separately, the node corresponding to the stimulus word and propagate this activity according to the following equation:  Ui(b) = input;(t) + C U j ( t  - 1)Wij (8) j  where a;(t) is the activity of word i after step t. inputi(t)is the external activation of the stimulus word and w;j is the associative weight between word i and word j. wii be 0.

After each step of propagation the words of the net are ranked according to their activities.

In [6] it is shown, that by the process of propagation not only first order relationships between words (direct connections) are taken into account, but also higher order relationships.

4. Results In order to determine the validity of the different predictions we calculated, for each example separately, the ranks of the words which were given as associative responses. In the case of the weight model, the lowest rank has been given to the word with the highest connective weight to the stimulus, in the case of the propagation model t o  the word with the highest activity after propagation. The measure of the overall quality of a model has been defined as:  where index 1 denotes the 100 association examples, and k the five most frequent responses to stimulus word 1 in the Russel & Jenkins association experiment. Nk is the number of persons who responded with word k, Rk the rank of word k in our simulation.

M is the mean rank of the 5 most frequent human responses when looked up in the simulation word list, but weighted by the frequency of the responses and averaged over all 100 association examples. The range of M is between 1 and 371. If no correlation between predictions and observations existed, M would be 185.5, half the number of words in the network.

I Formula I l l 1 2 1 3 1 4 1 5 1 6 1 7 1  stimulus c* response  stimulus + response  stimulus t response  I M fDroDaeation model) 11 49.9 I 4!

Formula 1 2 3 4 5 6 7 M (weight model) 47.6 47.6 42.8 42.8 42.8 46.8 42.8 M (propagation model) 47.6 47.6 35.7 36.8 37.2 37.6 36.9 M (weight model) 57.9 57.9 54.2 54.2 54.2 59.8 54.5 M (propagation model) 57.9 56.5 43.3 43.8 49.9 45.6 49.7  70.4 70.4 68.1 68.1 68.1 72.9 68.2 M (propagationmodel) 70.4 67.8 53.9 53.2 62.3 56.2 62.2  . M (weight  Table 2: Values for M for different formulas with and without propagation. The co-occurences were determined on the basis of sentence-wise evaluation, with word order not being considered  Table 2 gives the mean ranks M when the weights were computed with different formulas on the basis of sentence-wise co-occurences in Grolier?s Electronic Encyclopedia (without regard to word order). All formulas give reasonably good predictions of the associative answers, and with five of the formulas the predictions improve after propagation. It is surprising that the different methods for computing the connective weights produce similar results. The same observation has been made in earlier studies, where lexical nets have been applied to predict which query-words are used by data-bank searchers in on-line searches [8].

With an M of 35.6, formula 3 gives the best prediction of the associative responses. This is confirmed when the co-occurrences of word-pairs are determined using windows of constant width and when we take word order into account (see table 3). This result supports the assumption that the overall frequency of the response word, which is a measure of the amount of retroactive inhibition, has effect on the responses in the free association task.

Table 3: Values for M for different formulas with and without propagation, with order information taken into account. The co-occurences were determined using windows of width 18  When the connective weights are learned on the basis of scientific abstracts, the predictions become worse. When using sentence wise co-occurences and formula 1 we obtain a mean rank M of 69.1, which compares to an M of 49.9 when using the Grolier text corpus. This difference might be attributed to  the fact that the language used in the abstracts is very specific and highly repetitive.

I I I I I I I I I I 10 20 30 40 50 60 70 80 WINDOW-SIZE  Fig. 1: M for different window-sizes  Figure 1 shows the values for when the connective weights are computed with windows of different sizes us- ing formula 1. The same procedure has also been applied for some of the other formulas. The optimal width of the window was always found to be around 18 words. For compari- son, the average sentence length in Grolier?s Electronic Encyclopedia is 22.8 words. (Non alpha-characters are counted as words.) Learning by continuously shifting windows and learning sentence by sentence give similar predictions.

Table 3 gives the values for M when the weights are learned with windows of width 18, with and without regard to word order inside the windows. The predictions are better when the direction of the connections corresponds to  word order rather than vice versa. However, the best results are found when word order within the windows is not considered at all. This is probably due to the higher number of available observations when word order is neglected.

4. Conclusions The evaluation of different sets of assumptions concerning the prediction of word associations from texts leads to the following results:  1. Type and length of the text corpus are of importance. The larger the text, i.e. the higher the absolute frequencies of the stimulus or response words, the better the predictions are. Ten million words should be considered a minimum. When we reduced our original vocabulary of 371 words to those 355 words which in Grolier?s Electronic Encyclopedia have an absolute frequency of 10 or more, and removed the 16 association examples where at least one of these words occured, the mean rank M improved from 35.6 to 27.5.

2. The window size should be around 18 words, but the window type is not critical. Word order needs not be considered.

3. Different formulas for computation of the weights yield similar predictions, as long as they take the retroactive inhibition of the response word into account.

4. The predictions are improved, when second- and third-order dependencies between words are considered. For this a large vocabulary is advantageous.

When following these rules, our simulation program gives unrivalled predictions of the words people come up with in the free association task.

