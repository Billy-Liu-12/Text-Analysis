Improved CBA Classification Algorithm Based on Rough Set

Abstract   CBA is a classification algorithm integrating  association rule mining and classification. CBA has  been widely used in data mining areas because it has  higher accuracy than C4.5. When the samples  become more and more large and characteristic  attributes become more and more numerous, CBA  algorithm becomes much lower. In this paper, an  improved CBA algorithm based on rough set is  proposed. The improved CBA algorithm applies  rough set to induce attributes, and prune candidate  rules with PEP method. Experimental results  illustrate that the improved CBA algorithm is  efficient and it has higher accuracy than CBA and  C4.5.

Keyword Data mining, CBA classification,  Rough set, Attributes induction, PEP    1. Introduction   Classification is one of the most widely used  techniques in data mining. Given a set of cases with  class labels as a training set, classification is to build  a classifier to predict future data objects for which  the class label is unknown. At present, classification  is widely applied in medical diagnosis, financial  cheat analysis and text classification. Previous  studies have developed many techniques for building  classifiers, such as decision trees, ANN, CBA [1] and  Bayes. CBA is widely used in many fields because of  its simplicity and high classification accuracy.

In recent years, extensive research has been  carried to improve the CBA, e.g., ADT, CMAR [2]  and CPAR. These algorithms improved the  classification accuracy and have better classification  efficiency than CBA and C4.5 [3]. However, these  algorithms neglect the dependencies between the  characteristic attributes. When the number of  samples is huge and the characteristic attributes is  numerous, these algorithm will generate a huge of  redundant rules which affect the classification  efficiency.

Rough sets[4] was originally proposed by Pawlak  as a mathematical approach to handle imprecision,  vagueness and uncertainty in data analysis. Now,  Rough sets has been widely applied in data mining  [5,6,7]. An important application of rough sets theory  is attribute reduction. Based on Rough sets theory,  the redundant attributes can be successfully  eliminated on condition that the reduced decision  table has the equal classification ability with the  original decision table. This is useful for us to make  correct and compact decisions.

In this paper, we proposed an improved CBA  algorithm for accurate and efficient classification and  make the following contributions: First, we apply  rough sets to reduce characteristic attributes in  decision table, and then the redundant attributes will  be deleted. It can avoid the generation of a large  number of redundant rules. So we can generate  correct and compact rules. Second, In the procedure  of rule generation, we apply PEP(pessimistic errors  pruning) to prune biased rules which affect the  accuracy of classification.

2. Attribute reduction based on rough set  theory 2.1 Basic concepts of rough set theory   In rough set theory, an information system S is  denoted by S=<U,A,V,f>,where U={x1,x2,?,xn} is a  finite set of objects, called universe of discourse; A is  a nonempty set of attributes; V=  a A?  Va, Va represents  the domain of a; and f: U?A? V, called an information function, assigns an attribute value to  each x in U, i.e., f(x,a) Va for all x U, a A.

Definition 1 Let S=<U, A> be an information  system, for each subset B ? A, definite a discernibility relation IND(B):  IND(B)={(x,y) U?U ? a B,f(x,a)=f(y,a)} for each object x U, the equivalence class of x is  defined as  [x]B={y ? y U,(x,y) IND(B)}.

Definition 2 In an information system,  S=<U,A>, for each subset X U and an attribute set  R A, the lower and upper approximations of X are  respectively defined as follows:  R X={ Y U / R   Y ?  X  };  R X={ Y U / R Y ?  X ? ? }; Definition 3 Let S=<U,C D> is a decision  table, C is the condition attribute set ,D is the  decision attribute set ,for an attribute set B ? C, the positive region of D relative to B is defined as :  POSB(D)={ B X X U/IND(D)}.

POSB(D) actually is the object set which can be  accurately divided into equivalence class of D  according to the information of U/B.

If B ? C, and POSB(D)= POSC(D), then B is called the relative reduction of C.

Definition 4 Let S=<U,C D> is an decision  table with U={x1,x2,?,xn}. The discernibility matrix  of S is an n?n matrix, denoted by M(s) and defined  as  Mij ={a C f(xi,a) ? f(xj,a)and w(xi,xj)} } Given two objects xi, xj, Mij is the attribute set  which can discern xi and xj. It is obvious that M(S) is  a symmetrical matrix. Therefore, M(S) can be  simplified to its upper triangular form. If there exist  an element concluding only one attribute in m(S),  then the element is the unique attribute which can  discern the two relative objects. We call it core  attribute; our main work is to reduce the none-core  attributes.

Definition 5 Let S=<U,C D> is a decision  table, the discernibility function of S is defined as:  =  ( , )i j  ij  x x U U  M ? ?  ?  It can be known that the disernibility function  has following character: After  is converted to  minimum disjunctive normal form, all the  conjunctive form of  are all the reductions of  original decision table [ 8].

2.2 The attribute reduction algorithm based  on rough set   The main idea of the algorithm is: Firstly,  compute the discernibility matrix of decision table S  according to the samples; Secondly, obtain all core  attributes from M(S), and set the element to ? which conclude core attribute; For the other elements  in M(S), construct the relative Boolean form Pij;  Thirdly, absorption and distribution laws are  employed to convert the Boolean expression from  conjunctive form to disjunctive form. At last, the  discernibility function  are gained and converted to  conjunctive normal form, of which all the  conjunctive forms are all the reductions of the  decision table S. The attribute reduction algorithm  (called algorithm 1) is illustrated in Figure 1.

Input: the decision table S=<U,C D>  Output: reduced attribute set C ? (1)// Compute the discernibility matrix M(S),  extract core attributes CORE.

For i=2 to n do  For j=1 to i-1 do  Mij ={ a C f ( xi , a ) ?  f (xj , a ) and w ( xi , xj ) } ;  If Mij =1  Then CORE=CORE  Mij ;  End for  End for  (2)// Set the elements concluding core attributes  to       , construct relative conjunctive form  of the other elements.

For i=2 to n do  For j=1 to i-1 do  If Mij ?  CORE?        Then  Mij=      ; Else Pij=         ak; P=P Pij ;  End for  End for  (3)// Convert P to disjunctive form P ? .

P ? =CNF(P);  (4)// Compute the discernibility function ,  and convert  to disjunctive normal form.

=P? CORE; Q=CNF( );  ??  ?  k ija M? ?   Figure 1.The attribute reduction algorithm    3. Rule pruning   The number of rules generated by CBA_RG  algorithm can be huge to make the classification  effective and efficient; we need to prune rules to  delete redundant and noisy information. There are  various rule pruning methods designed to reduce the  classifier?s size and to increase the accuracy of  classification. We adapt PEP method to prune rules  which affect the accuracy of classification. PEP  method was proposed by Quinlan that aims to avoid  the necessity of a separate test data set. It is based on  the number of errors and the size of the training  sample.

A candidate rule generated by CBA_RG is of the  form: r=<condset,y>, where  condset={(C1,a1),(C2,a2),?(Cn,an)} is a finite set of  items, called condition set, y Vd, Vd represents the  domain of decision attribute d. If there exist a rule r  =<condset,y>, which satisfies r.condset- r .condset  =1, then r  is called the father rule of r.

The errors number of the rule r is  e(r)=r.condsupCount-r.rulesupCount, an estimate of  the miss-classification number is e' = C  (e(r)+1/2), where C=r.condset - r .condset, the  standard error is calculated in this way:  SE( e' (r))=[(condsupCount- e' (r))/condsupCount] 1/2    Accordingly, for the rule r , the estimate of the  miss-classification number is e' ( r )=e( r )+1/2.

PEP suggests pruning the rule r if its correct  number of miss-classification is greater than that for  r , e.g. e' ( r )< e' (r)+SE( e' (r)). We adapt PEP  method because it has following advantages: the  same training set is used for both generating and  pruning the CAR (Classification Association Rule),  and it is much quick because it only has to make one  pass and looks at each rule only once. Our  experimental results show that this pruning method is  effective and efficient.

4. The improved CBA algorithm 4.1. CBA algorithm    The main idea of CBA algorithm is: find all  association rules whose right-hand-side is  classification class attribute. This kind of rule is  called the class association rule (CAR). Through  sorting and testing, take the most effective CARs to  cover the training set. It consist of two parts, a rule  generator (called CBA_RG), which is based on  algorithm Apriori for finding association rules, and a  classifier builder (called CBA_CB).

4.2. The improved CBA algorithm   Input: the decision table S=<U,C D>  Output: a Classifier  (1)// Reduce attributes with algorithm 1.

C ? =Reduction(S); Del(S);  (2)// Generate CARs.

F1={large 1_ruleitem};  CAR1=genRule(F1);  For(k=2;Fk-1 ?  ;k++) do  Ck=candidateGen(Fk-1);  Count_rulesup(Ck);  Fk={c Ck c.rulesupCount? minsup}; prCARk=PEP(CARk);  End for  prCARs=           prCARi;  (3)// Build the classifier referring to CBA_CB .

Classifier=CBA_CB(prCARs);  1i to k U  =  ?   Figure 2. The improved CBA algorithm  Our proposed algorithm build the classifier as  follows: Firstly, the redundant attributes of the  training set are reduced by algorithm 1, this is useful  to make correct and compact decisions. Secondly,  generate CARs with Apriori algorithm, and the  CARs that affect the efficient of classification will be  pruned with PEP method. Finally, the classifier will  be built referring to CBA_CB. The improved CBA  algorithm (called algorithm 2) is illustrated in Figure  2.

5. Experimental results   For performance testing several health related data  sets from UCI ML repository were used. All  experiments are performed on a 2.8 Hz Pentium-4  PC with 1GB main memory, running Microsoft  Windows/NT.

The improved CBA algorithm was tested against  CBA and C4.5. CBA and C4.5 were implemented by  their authors respectively. The improved CBA  algorithm was implemented with Java. For CBA and  the improved CBA, data set were cleaned and  discretized before using. For C4.5, which is capable  of using continuous values, both discretized and non-  discretized data sets were used and the best results  were presented. The experimental results are shown  in table 1.

Table 1. Accuracy results for each algorithm Data set  No. of samples  No. of attributes  C4.5 CBA Improved  CBA  Breast 699 10 94.4 96.6 97.7  Heart 270 13 81.2 82.7 84.3  Horse 368 22 83 84.4 85.2  Liver 345 7 74.8 76.2 77.3  Lung 32 56 81.4 83.5 84.8  As can be seen from the Table 1, the improved  CBA outperforms both C4.5 and CBA on accuracy.

Furthermore, when the number of attributes and  samples of the training set are huge, the advantage of  the improved CBA is more obvious, e.g. Breast and  Lung, the improved CBA wins C4.5 over 3% in  accuracy.

Figure 3. The comparison of CBA and  improved CBA on runtime      To test the scalability of the improved CBA, we  compare the runtime of CBA and the improved CBA  on five data sets. The results are shown in Figure 3.

As can be seen from the table, the run time of the  improved CBA is about half of the CBA?s. It  indicates that the improved CBA is much more  efficient than CBA.

6. Acknowledgment   This research is supported by Industry Plans  Projects of Guizhou Province in China and  Informatization Special Fund Project of Guizhou  Province in China.

7. Conclusions   In order to improve the efficiency of CBA, a new  CBA algorithm based on rough set theory is  proposed in the paper. This method applies rough set  to reduce redundant attributes in database, and adapt  PEP method to prune rules which affect the accuracy  of classification. Experimental results proved that the  improved CBA is more efficient than CBA and it has  higher accuracy compared with CBA and C4.5.

Therefore, the classifier build by this method is  effective and efficient.

Through analysis of the experiment result, we  found that the improved algorithm performs well  when handling with data set which has little  continual attributes, but when data set conclude many  continual attributes, the algorithm?s performce is not  ideal. In our future work, we will focus on how to  discretizate the continual attribute effectual  according respective data set.

8. References  [1] B. Liu, W. Hsu, and Y. Ma, ?Integrating classification  and association rule mining?, In KDD?98, New York,  1998, pp.80-86  [2] W. Li, J. Han, J. Pei, ?CMAR: Accurate and efficient  classification based on multiple class-association  rules?, In Proc. of the ICDM, 2001,pp.369-376  [3] J. R. Quinlan, C4.5: Programs for Machine Learning,  Morgan Kaufmann, 1993  [4] Z. Pawlak, Rough Sets-Theoretical Aspects of Rea-  soning about Data, Kluwer Academic Publishers,  Boston, 1991.

[5] Z. Pawlak, ?Rough sets and intelligent data analysis?,  Information Sciences, vol. 147. 2002, pp. 1-12  [6] SHI Huawang, ?The Risk Early-warning of Hiden  Danger in Coal Mine Based on Rough Set-neural  network?, Proceeding of the 2nd International  Conference on Risk Management and Engineering  Management, 2008, pp. 314-317  [7] Huawang Shi, Wanqing Li, Wenqing Meng, ?A New  Approach to Construction Project Risk Assessment  Based on Rough Set and Information Entropy?, 2008   Innovation Management and Industrial Engineering.

