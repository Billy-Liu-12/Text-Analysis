Automatic Content-Based Recommendation in e-Commerce

Abstract  The amount of information in e-Commerce is increasing far more quickly than our ability to process. Recommender systems apply knowledge discovery techniques to help peo- ple find what they really want. However, all of the previ- ous approaches have an important drawback: items added newly cannot be found. In this paper, a general framework is proposed for supporting automatic recommendation of the new item to the potential users based on the concept of influent sets. We propose a simple efficient indexing struc- ture and a heuristic information retrieval technique algo- rithm for searching reverse k nearest neighbour in high- dimensional dataset. And experimental evaluation reveals that our approach outperforms the previous algorithm and enhances the performance efficiently. 1  1. Introduction  With the rapid development of e-Commerce and its in- creased popularity ease-use tools, the world is becoming more and more a global marketplace. But the amazing num- ber of news, advertisements and other information of prod- ucts in e-Commerce sites makes us feel it is necessary to find some new technologies that can dramatically reduce the useless information and help us sift through all the available information to find which is most valuable to us. Recom- mender system is one of these technologies by identifying particular items that are likely to match each user?s tastes or preferences.

Some of most widely used technologies for building rec- ommender systems include collaborative filtering (CF) [1],  1This work is supported by the National Natural Science Founda- tion of China (60205007), Natural Science Foundation of Guangdong Province (031558, 04300462), Research Foundation of National Sci- ence and Technology Plan Project (2004BA721A02), Research Founda- tion of Science and Technology Plan Project in Guangdong Province (2003C50118) and Research Foundation of Science and Technology Plan Project in Guangzhou City (2002Z3-E0017).

association rules discovery [2], Bayesian networks [3] and Horting [4], etc. Unfortunately, all of these existing ap- proaches for recommender system must rely on the ac- cessed records of items or the usage history users and just focus on the current demands of user, so it?s an inevitable thing that they have an important drawback: items or pages which is added to a site recently cannot be found. Because for these new items or pages, there are no accessed records, no ratings from users and still no relationship are found by data mining algorithms, all of which make people hardly discover them. This is generally referred to as the ?new item problem?. How can potential users find these new items as soon as possible? How a new CD album be recommended in good time to the real customers who seem to have spe- cial favor on it. In practice, many new items are added into the e-Commerce sites every minute, hour or day. In this pa- per, we propose a general framework aiming at solving this problem. A new query mechanism is explored to automati- cally recommend the new items to those users that seem to have the similar tastes or interests matching with the content features of this item.

The rest of the paper is organized as follows: We first describe the general framework of our automatic recom- mender system and its key components in section 2. Section 3 gives the theoretical foundation of reverse k nearest neigh- bour and the basic structure of our indexing tree RkNN-Tree.

In section 4, we report the experiment results for applying our new algorithm to different high-dimensional datasets.

We give the conclusion in section 5.

2. A Framework of Automatic Recommender System for New Item  Recently, more and more attentions have been paid on users? taste and preference to improve the retrieval process in order to generate more meaningful and suitable retrieval results for users. And in real world, users may want to re- quire the newest relevant information as soon as it appears.

Compared to those passive information acquirement models    such as search agents, our active information acquirement model can dramatically save much time for people who are always busy in this information exploded age. In our auto- matic recommender system, we divide the users into groups who appear to have similar preferences according they ac- cessed similar content in Internet. Semantic content fea- tures of a new item will be extracted to match the format of feature vector database. After adding it to database, we provide an efficient way to calculate and retrieval the sim- ilarities/distances between the feature vectors of new item and users clusters. Actually, it is a process to find ?influent sets? of new item, which can be accomplished by a reverse k nearest neighbour query. Finally, this item will be rec- ommended to these groups of users that seem to have the similar tastes or interests matching with the semantic con- tent feature of this item.

Site Database  Content Features Extraction  Features Vectors  Database  Users Database  Users Interest  Clustering Users Cluster  Interest Features  Descriptions  Content Features  Extraction  Reverse k Nearest  Neighbor SearchAutomatically  Recommend  New Item  Figure 1. Architecture of recommender sys- tem on content-based retrieval  Pages, texts, scripts, and images, etc. are stored in a large database. Through content features extraction, those infor- mation are formalized by a series of feature vectors, which map individual instances to points in a high dimensional.

2.1. Extracting Semantic Features from Pageviews  In Web applications, Pageviews are semantically mean- ingful entities such as items. Through content features extraction, each pageview p can be represented as a k- dimensional feature vector, where k is the total num- ber of extracted features from the site in a global dic- tionary. Each dimension in a feature vector represents the corresponding feature weight within the pageview.

Thus, the feature vector for a pageview p is given by:  p = ?fw(p, f1), fw(p, f2), ..., fw(p, fk)? where is fw(p, fj) the weight of the jth feature in pageview p, for some j ?{1,. . . ,k}.

2.2. Extracting Semantic Features from User Trans- actions  We take feature extraction method from user transac- tions database described by Dai and Bamshad Mobasher in [5]. User transactions are relevant subsets of pageview in each user session. Data preparation results in a set of n pageviews,P = {p1, p2, ..., pn} and a set of m user transactions, T = {t1, t2, ..., tm} where each ti ? T (with a unique identifier TID) is a sub- set of P . Each transaction twas viewed as an l- dimensional vector over the space of pageviews reference:  t = ?w(p1, t), w(p2, t), ..., w(pl, t)? where each p ? P for some i ? {1, . . . , n} and w(pi, t) is the weight associated with pageview p in the transaction t representing its significance. The weights can be deter- mined in a number of ways meeting the demand. Content mining may use a function of the duration of the associated pageview in order to capture the user?s interest in a content page. Thus, the set of all user transaction can be viewed as an m? n transaction-pageview matrix, denoted by TP .

For the whole collection of pageviews in the site, we have the n ? k pageview-feature matrix PF = {p1, p2, . . . , pn}. Then, if we map each pageview of PF in a transaction tito one or more content features, we can get a new matrix TF = {t?1, t?2, . . . , t?m},where  t?i = ?fw(t?i, f1), fw(t?i, f2), ..., fw(t?i, fk)? each t?i is a k-dimensional vector over the feature space and fw(t?i, fj)is the weight of the jth feature in transaction t  ? i,  for some j ? {1, . . ., k}. Thus, a user transaction can be represented as a content feature vector, reflecting that user?s interests in particular concepts or topics.

2.3. User Clustering based on Similar Interest  The use of clustering in information retrieval is base on the Clustering Hypothesis ?closely associated things tend to be relevant to the same request?. According to the hypoth- esis, if we do good job at clustering similar or non-similar users, we would have enhance the chance of improving the quality of recommendation. Based on the above matrix TF , we define a measure for the similarity/distance between users. Suppose TFA = {ta1, ta2, . . . , tak} and TFB = {tb1, tb2, . . . , tbk} are transaction- feature matrix of user A and user B respectively.

Dist(A, B) = sim(A, B) = ?  k  i=1 (sup(fai)+sup(fbi))??k  i=1 sup2(fai)+sup2(fbi)  where sup(fai) is support of feature sup(fai) in the matrix TFA, that is, the occurrence frequency of feature sup(fai) in the matrix TFA and sup(fai) has the same meaning of sup(fai). We accomplished the users clustering in the process of building our indexing structure RkNN-Tree which will be introduced in detailed in next section.

New Item  Figure 2. Users clustering based on their transactions similarities  2.4. Recommending New Item to Users Clusters by the Concept of Influent Sets  K Nearest Neighbor (kNN) problem is an important topic in real work applications and research domains such as in information retrieval, multimedia systems, spatial data- bases, and data mining etc. Recently, more and more atten- tions have been paid on its reverse version, which is known as ?Influence Sets? problem. The Reverse k Nearest Neigh- bor problem is to find all points in a data set that take a given query point as one of their k Nearest Neighbor [6]. This no- tion arises in examples such as finding the set of customers affected by the opening of a new store outlet location, or deciding the service scope managed by a certain call center, etc. In our case, recommender system will notify the subset of Web users who will find a newly added item most rel- evant. Based on the inherent information of different user clusters, we can finish the recommendation of new item to the right user clusters just by performing a simple reverse k nearest neighbor query which take the feature vector of new item as a query vector. The results returned are a set of users who have the similar tastes or interests matching with the content features of this item. In what follows, we will describe this in detailed.

3. Reverse k Nearest Neighbor Query  Firstly, we give the formal definitions of these two prob- lems. Let S denote a set of n feature vectors v ? ? and let Dist :? ?d ? ?d ? ?+0 is denoted as the simi- larity/distance measure function which given in subsection 2.3. For a query feature vector q ? ? and a query parameter k ? 1.

Definition 1 The k Nearest Neighbor of q is defined as: kNN(q) = {S?|S? ? S ? ?v1 ? S? ? ?v2 ? S :  Dist(q, v1) ? Dist(q, v2)} with |S?| = k ? n.

Definition 2 The Reverse k Nearest Neighbor of q is de- fined as:  RkNN(q) = {v ? S|q ? kNN(v)} What is worthy of notice is RkNN(q) may be empty, or  have one or more elements.

Lemma 3 v ? RkNN(q)? q ? kNN(v)  Proof: This is obvious regarding the definitions of kNN and RkNN.

Corollary 4 Suppose kNNv is the distance between v and its k Nearest Neighbor,  Dist(v, q) ? kNNv ? v ? RkNN(q) Dist(v, q) > kNNv ? v /? RkNN(q)  Proof: If Dist(v, q) ? kNNv, then q is a kNN or is a tie of kNN of v. According Theorem 1, v is RkNN of q.

Otherwise, v is NOT RkNN of q and vice versa.

Notice k nearest neighbor and reverse k nearest neighbor are not symmetric. That is, q ? kNN(v) = q ? RkNN(v) and vice versa.

q  s  t r u  Figure 3. k nearest neighbor and reverse k nearest neighbor are not symmetric  According to the definition 1 and 2, 2NN(q) = {r, t}, R2NN(q) = {r, s, t, u}. So RkNN(q) will return empty, or have k or more than k elements.

3.1. Challenges on Reverse k Nearest Neighbor Query in High-Dimension  Traditional clustering techniques generally have bad ac- curacy and low performance for very large-scale problems.

Because the number of features in a transaction vector usu- ally is in tens to a few hundreds in typical applications.

Some techniques like SVD [10] have been widely used to increase density and reduce the dimensionality of high- dimensional feature space. However in many cases, the re- sulting from reducing the dimensionality will still have a quite large dimensionality. And the remaining dimensions are all relatively important which means that any efficient indexing method must guarantee a good selectivity on all those dimensions.

In high-dimensional spaces, the case of RNN would be much more complicated with dimension growing. The ex- tended property in [8] becomes ?for any query point q, RNN(q) will return at most 6 data points in a 2-dimensional data space and 12 in 3-dimensions. In addition, this number    will increase exponentially with increase in dimensional- ity. In the L? case, the cardinality of RNN(q) is at most 3d-1 in d-dimensions?. The exponential increase in the number of RNN with the increase in dimensions makes all current algorithms for searching RNN practically impossi- ble and infeasible for higher dimensions because number of searching space would be very high. Furthermore, the exit- ing approaches for kNN queries in high-dimension such as X-Tree, VA-File and IQ-Tree can not be adapted to RkNN queries directly because of knearest neighbor and reverse knearest neighbor are not symmetric directly.

3.2. Structure of RkNN-Tree  Based on the above studies, we e developed the RkNN- Tree, which is a new heuristic indexing techniques based on a simple, yet efficient algorithm to search both kNN and RkNN in high-dimensional datasets. By using the RkNN- Tree to reorganize the dataset S and utilizing its heuristic information in directory node to implement indexing query, we can achieve a good experimental performance in high- dimensional datasets.

Figure 4. The structure of RkNN-Tree  In our RkNN-Tree, a leaf node is a 2-element vector (v ptr, k dist), where v ptr refers to a feature vector v in the d?dimensional dataset and k dist is the distance from v to its k nearest neighbor in the dataset. A directory node con- tains an n+2 array of branch of the form (Child ptr*, MBS, max k dist). Child ptr* is an array of n points referring to its child nodes in the tree. There are two cases: ? If Child ptr points to a leaf node, then MBS is the min-  imum sphere region containing all of data node in this leaf.

And max k dist = max(k dist ? all Leaf in this direc- tory node).

? If Child ptr points to a directory node, then MBS =  max(MBS ? child directory node). And max k dist = max(max k dist ? child directory node).

In our approach, the indexing structure using minimum bounding spheres (MBSs) instead of minimum bounding  rectangles (MBRs) to represent the shape of regions has two reasons. First, the diameters of region will affect the per- formance of query sensibly. Diameters of MBRs (diago- nals) are longer than that of MBSs. To distribute data points into smaller region can help reduce the overlapping of re- gions and improve the performance of query. Second, the storage of a sphere just has a center (1 DATA type) and a radius (1 float type) while a rectangle needs two diagonal points (2 DATA type) to represent it. In high-dimensional data spaces, the former make itself outstanding by saving about half storage space in contrast of the latter.

3.3. Storage and Search Strategies  For disk-resident databases, the number of blocks which must be accessed is taken as a measure of the amount of I/O and hence a cost of queries. Therefore current studies of disk-resident indexing techniques mainly focus on reducing the cost of page access, it has been notice that minimiza- tion of page access doesn?t lead to the optimization of CPU response time, because the time for a disk access includes the data transfer time as well as the disk seek time which turns out to be more expensive than the previous one. Under the observations, we take the strategy of storing the page in the same level of tree continuously, which is different from the previous methods. That means, all pages including the nodes in the same level of tree will be saved in a continu- ous memory (physical) space. And to take the advantage of the property of continuous storage strategy, we change the traditional Depth-First Traversal mode to Breadth-First Traversal mode. The BFT mode traverses the tree level by level and takes more advantage of the disk buffer and gains higher disk transfer rate. The successive pages can be prefetched into the disk buffer to reduce the number of physical accesses to disk when the bus is busy.

We insert all users into the RkNN-Tree one by one ac- cording their representing feature vectors. In this process, we keep the heuristic information for each node such as MBS, etc. Once the requirement of searching reverse k nearest neighbor for some query vector q, from the root of RkNN-Tree, the algorithm must determine which branches should be travelled or pruned under the current directory node. The aim of a good search strategy is to reduce the number of node that must be travelled to the least. The best travel order is not only dependent on the distance from q to each MBS from the root to the leaf, but also determined by the size of MBS and the layout of each sub-MBS. So before starting the search, we calculate MINDIST(q, n) be- tween q and the MBS of current node n and generate an ABL (Active Branches List) whose points refer to an or- dered branches list according these MINDISTs. Then the algorithm judge whether MINDIST(q, n) greater than the field max k dist, if yes, that means none of vectors under    this directory node would take q as its k nearest neighbor.

So this branch can be pruned and all nodes under it would not be traveled. Experiment evaluation shows these strate- gies can improve search performance dramatically.

3.4. Reverse k Nearest Neighbor Query Algorithm  Algorithm 1 Reverse k Nearest Neighbor Query Algorithm 1: RkNN ? ?; 2: n? |S|; 3: rknn num? 0; 4: if current node p is a leaf node then 5: temp k dist = Dist(q, p); 6: if temp k dist ? p.k dist then 7: RkNN [rknn num] = p; 8: rknn num = rknn num + 1; 9: end if  10: end if 11: if current node d is a directory node then 12: genBranchList(q, branchList) //ABL 13: sortBranchList(branchList) 14: for i? 1, d.num do 15: temp k dist = MINDIST (q, d.bi.MBS) 16: if temp k dist ? d.bi.max k dist then 17: RkNNQuery(d.bi.child ptr, q) 18: end if 19: end for 20: end if  Notice that k nearest neighbor queries can still be accom- plished efficiently the same as that in traditional R*-Tree[7].

3.5. The Maintenance of the RkNN-Tree  The RkNN-Tree is dynamic and can be updated incre- mentally. The processes of insertion, deletion and update can be completed easily by simply combining several kNN and RkNN query operations. We first take a look at in- sertion. When a point v?is to be inserted into a generated RkNN-Tree. We perform a kNN search operation to find its kNNv? , and create the form (v?, k distv? ) which is required in leaf node. Next, an RkNN query can give us the infor- mation of those points that are affected, i.e. RkNN(v?). For each vi ? RkNN(v?) we recomputed its k distvi by per- form kNN(vi)and the field MBS and max k dist of its as- cendant nodes will also need to be adjusted up to the root of the tree. This can be done in a way very similar to the RkNN query algorithm. At last, we insert v? the same as in a normal R*-tree. Now, we turn our attention to deletion.

Firstly we find the target point v?? to delete and remove it from the appropriate leaf L. Just like insertion operation, those affected points in RkNN(v?) must be found and re- computed their kNN and the relevant directory nodes must  be adjusted. The update operation is composed of two steps: after a deletion operation, insertion supervenes.

4. Experimental Evaluation  Since there is no other reverse k nearest neighbor algo- rithms were given in the published literature, we choose the straightforward algorithm introduced in [6] which finds reverse k nearest neighbor by a sequentially scan through all the vector database and determine those vectors v that make q falling into its precomputed Circle(v, k distv), that means, have q as their k nearest neighbor. We conduct a set of experiments to analyze response time and page ac- cesses of RkNN queries, under different dimension and size datasets. All experiments were perform on a C Pentium 4 @ CPU 1.7G running Linux Ret Hat 9, with 528MB of main memory and 15GB of disk. All programs have been implemented in standard C and use double float data type.

The block size used for our experiment is 4KByte and all indexing structures were allowed to use the same amount of cache. We measured both total CPU response time as well as page accesses.

4.1. Effect of the Size of Datasets  Let us first compare how the RkNNQuery perform on various size datasets, which include 5 uniform distributed datasets of fix dimension (8) and various dataset sizes (50K, 75K, 100K, 125K and 150K). We also randomly generated a set of 500 queries for evaluating the searching perfor- mance of RkNNQuery and RkNNScan. We observed that the former is obviously faster than the latter when datasets is small. As datasets size increases, RkNNScan have to com- puter more distances between query point and data point, while special tree structure and heuristic search strategies of RkNNTree give more wise advices to its query algorithm, costing less response time and page access.

0.5   1.5   2.5   3.5   25 50 75 100 125 150  Datasets Size(K)  R es  po ns  e T  im e(  s)  RkNNQuery  RkNNScan  Figure 5. Comparisons on Response Time on Different Datasets Size    Table 1. Comparisons on Page Access on Dif- ferent Datasets Size  Page Access 25k 50k 75k 100k 125k 150k  RkNNScan 25k 50k 75k 100k 125k 150k  RkNNQuery 10.7 101.7 591.8 674.18 718.62 778.54  4.2 Effect of Dimension  In the second experiment, we generated 5 uniform dis- tributed datasets of fix datasets size (100K) and varied di- mension (4, 6, 8, 10 and 12) to demonstrate the impact of dimension. Additionally, we also randomly create a set of 500 queries for evaluating the searching performance of RkNNQuery and RkNNScan. we find the former continues to outperform the latter.

0.5   1.5   2.5   3.5  2 4 6 8 10 12  Dimension  R es  po ns  e T  im e(  s)  RkNNQuery  RkNNScan  Figure 6. Comparisons on Response Time on Different Dimension Datasets  Table 2. Comparisons on Page Access on Dif- ferent Dimension Datasets  Page Access 2 4 6 8 10 12  RkNNScan 100k 100k 100k 100k 100k 100k  RkNNQuery 4.5 19.9 49.5 674.2 1486.6 2165.7  5 Conclusions  This paper present a different view of integrating seman- tic knowledge into the recommendation process based on information retrieval techniques in high dimensions. We also give the necessary theoretical notions of a new frame- work and relevant efficient structure, which focus on captur- ing the underlying common properties and relations among the users and items (pages). Instead of being found pas- sively, items can discovery their potential Web users auto-  matically and be recommended to these users actively in this general approach.

