Fuzzy Methodology for Enhancement of Context Semantic Understanding

Abstract? One of the many issues that confront traditional statistical approaches of natural language understanding (NLU) is on how to overcome the insufficient co-occurrence informa- tion caused by the limited boundary of statistical approaches.

Researchers have long used the imparting of human knowledge into statistical approaches, including definition of rules and collections of hierarchy of concepts. However, these are difficult to define even for a domain expert. They are also very much people and domain dependent. This study proposes a fuzzy approach to tackle these issues in a way as to provide a methodology for logical reorganizing context in order to tackle the issue of boundary limitation, to create the more reasonable and understandable word association which will be referenced as membership degree in latter stage, and to make the processes of imparting of human knowledge easier and less domain dependent. The accomplishment of these tasks could be achieved through the concept of Precisiated Natural Language (PNL).



I. INTRODUCTION  Due to issues such as word synonymy and polysemy, un- derstanding of natural language (NLU) is semantically related and very much dependent on the sentence organization. As such, use of World Knowledge (or human knowledge) is a common assistant to automatic NLU. However, the apporach of how to effectively apply World Knowledge on NLU is still a challenging problem. One of the major difficulties is how to impart machine understanding with the ?unbounded? nature of human knowledge. Human cognition is built upon a procedure of granulation. When a particular event happens, the event triggers all related memory and the brain recon- structs the knowledge pertaining to the event?s environment.

Unfortunately, there doesn?t exist yet a powerful algorithm to mimic exactly the human way of reconstructing knowledge from previous experience. The current popular approach to tackle this issue is to use frequency based measures to predict the semantic understanding given the assumption that the historical words relations remains always true in the unpredicted context. However the traditional statistical approaches cannot deal completely with this issue because any statistical approach (for instance, n-gram approaches) to natural language is sub-sufficient. This is due to the fact that the estimated parameters are based on insufficient information (for instance, only the near neighbor?s occurrence is consid- ered during the word frequency analysis) and therefore might lead to imprecise outcome. As such, we propose to apply the  fuzzy logic-based World Knowledge first to reorganize the planning of the original context before using the frequency based measure to compute the semantic coefficients between words. The reason behind the use of fuzzy techniques instead of human-supplied lexicon and rules is intuitive. In fact, even a domain expert cannot provide a comprehensive lexicon and domain rules. Fuzzy World Knowledge functions as a compensation to insufficiency of human-supplied lexicon so that the missing data can be compensated through the fuzzy- supplied knowledge. Through the tuning of human-supplied lexicon by fuzzy Word Knowledge, we try to compensate for the gaps of statistical approach when applied to NLU.

Throughout this paper, the fuzzy based World Knowledge is synonymous of Precisiated Natural Language (PNL)[1], [2]. In particular, we propose three kinds of sets of word knowledge to tackle the issue of insufficient information discussed above.

The remainder of the paper is organized as follows. In Section 2, the classical statistical and fuzzy approaches are reviewed. In Section 3, we describe a novel formalism of human knowledge through three sets of knowledge, which forms the basis of the proposed methodology. In Section 4, we illustrate with an example to explain how to apply the proposed methodology to enhance a statistical measure to accurately compute the semantic coefficients of words. In Sec- tion 5 experimental results illustrate the performance of the proposed approach, followed by comments and discussions.

Finally, conclusions are given in Section 6.



II. OVERVIEW AND BACKGROUND  Statistical approaches are widely used today by NLU researchers. For instance, the corpus approach [3], [4] has been the earliest language model based on a pure statistical model. The assumption that underlies the theory is that a document might be represented by a hyperspace vector consisting of significant words selected by a certain metric.

The major issue of the corpus approach pertains the lack of capability in tackling the difficulties caused by synonymy and polysemy within the English language domain as argued in [5]. Meanwhile, the issue of sparse vector representation [6] must also be considered. In order to better understand the semantic meaning of words, researchers have used the     machine readable semantic lexicons: WordNet ([7], [8]), Ox- ford Advanced Learner ([9], [10]), Longman Dictionary of Contemporary English ([11]), and Funk and Wagnalls (F&W) Dictionary ([12]). Other semantic approaches, [13], [14], took advantage of knowledge from the predefined syntactic patterns and semantic slots (called ?Heuristic?). However, the issue of how to optimize the right semantic level arises, as well the issue of boundary limitation has always existed.

Besides, the semantic approaches are inevitably geared toward the representation of static knowledge. They are more suitable for defining, for instance, the semantic ontology with static human-defined semantic hierarchy. However, actions (in other words, events organized by logical relations) are difficult to depict by static semantic definitions. In essence, actions can only be dynamically described and accordingly, represented by continuous states. Fuzzy logic represents a good frame- work supporting continuous transition of states.

In earlier research work on fuzzy logic as applied to natural language processing, researchers, for instance Zadeh in [15], focused on modelling adverb modifiers in natural language through fuzzy sets. However, this approach has its limitation, as it depicts modifiers only. In order to characterize other types of words? meanings by fuzzy sets, Zadeh introduced hierarchies of fuzzy concepts in [16]. Besides, considering the relation between possibility and fuzzy sets, Zadeh in [17], [18] took advantage of possibility of a proposition to define its corresponding fuzzy set. However, the previous research work was mainly limited to simple proposition transitions and focused on characterization of modifiers and the words with obvious distinguishing degree, such as ?hot?, ?red?, and ?tall? . This by itself cannot cope with natural sentences. Other researchers, for instance in [19], [20], have used a fuzzy relation system to depict semantic association through con- struction of a fuzzy thesaurus for a certain domain. However, the main issue is that the fuzzy thesaurus are in essence static so that a lexine in such static lexicons cannot dynamically adjust its original conceptual definition and relations when a new situation arises. More recently, Zadeh [1] introduced the theory of fuzzy information granulation (TFIG), where Zadeh suggested a novel formalism for natural language processing, which he called: Precisiated Natural Language (PNL).



III. PROPOSED APPROACH  The modelling of natural language (i.e. free context) repre- sents the main topic of this paper. An appropriate model must satisfy two basic requirements: (1) enable human knowledge to translate into statistical approaches; (2)enable rules to simplify manual annotation of the training corpus. We base our modelling approach through integration of three kinds of sets of knowledge extracted from words: Zadeh?s recent findings on Epistemic Lexicon (EL) in [2], Semantic pattern, and Predicate dictionary, as well generic rules.

A. Predicate Dictionary  In our research, we believe verbs (?predicate?) are more important than nouns. As such, we define a dictionary particularly for the interested predicates. The dictionary is differentiated from others in the sense of ?direction?, which will be used in the latter stage for construction of membership functions. Some examples are listed in Table I.

B. Epistemic Lexicon (EL)  The words association is described by Epistemic Lexicon (EL). In [2], an EL defines a granulation lexicon in a format described as ?EL:= [Word: + [attribute=[Word, fuzzy measurement]]]?. The parameters are words in the language, relations (R) between words, and the attributes of words. An example of granulated attribute relations in the form of EL is shown in Fig. 1, where Ai is the ith attribute of lexine, representing a super concept; and Gi, is a group of granulated concepts with a fuzzy measurement.

In our research, we adopt this definition and further update  Predicate Class Assigned Words  Agree agree, affiliate, ally, coincide  Against refuse, damage, cessation, argue, challenge  Action-up boost, increase, bounce, develop  Action-down decrease, reduce, discourage, dump, fall  Action-in buy, import, offer,proposal, order  Action-out disclose, exclude, explore, export, pay, release  TABLE I: Examples for Predicate Dictionary  1 n  G G  (Attribute)  (Granular members)1 n  Profitability TechnologyCapital Human Resource  Cash / Mid  Stock/ High Sales / High  Manager / Mid  Bio-tech / Hight  e-commerce/ Mid  Example:  Format:  ...

Lexine A A...

Fig. 1: An Example of Granulated Attribute Relations in the Form of Epistemic Lexicon     it as a normal form as in (1):  EL ={L1, L2, . . . , Ln} (1a) Li ={Lj |Final }@ a Fuzzy Measurement, (1b)  where Lj /? {L1, . . . , Ln} Final ={Predicate class|Concept} (1c)  The lexine Li depicted by EL is usually difficult to be assigned to a certain concept as described in WordNet. As compared with the definition of lexine in EL, we manually define common-sensed concepts. And in order to tackle the issue of hierarchical definitions of concepts like in WordNet, in this paper, we adopt the flat definition. Some examples are depicted in Table II.

C. Semantic Pattern  In order to enable the system to capture semantic mean- ings we are interested, we adopt pairs of semantic pat- terns introduced in [21]. For instance, to depict the key word ?appointed?, we have a pair of semantic patterns: (1) <subject> passive-verb, for instance ?<Gareth L. Reed> has been appointed? indicates <Gareth L. Reed> is the subject and belongs to the concept of ?PEOPLE?; (2) passive-verb <drt: direct objective>, for instance ?has been appointed <president>? indicates <president> is the drt and belongs to the concept of ?MANAGEMENT-STAFF?. As such, this pair of semantic patterns are triggered if the system matches the two semantic patterns with word ?appointed?.

D. Rules  Besides the definitions of the knowledge discussed above, we design rules to enhance the static EL. The rules defined for the selected domain are summarized as follows:  1) One lexine might belong to multiple concepts (classes); 2) Any lexine in predicate dictionary is bi-sense-directed  and takes only one of two directions: positive or neg- ative depending on its context; positive direction (+) indicates the tendency of up or in and negative direction (-) indicates the tendency of down or out respectively;  3) In a given sentence, suppose the number of directions Di is n (in practice, we restrict n ? 2. As such,  Concept Assigned Words  Equity share, stock, debt, stake, option  Organization company, bank, Corp., firm, subsidiary, spinoff, venture  Trading-place stock exchange  Money Market bond, treasury, security, warrant  Legal court, regulation  Business Type service, manufacture, PC (Mac, computer), gas, oil,  TABLE II: Examples for Concept Dictionary  a complicated sentence is usually divided into sub- sentences). Then the overall direction of the given sentence is decided by  ?n i=1 Di, where{  Di > 0, if Di is a positive direction Di < 0, if Di is a negative direction  4) In the process of reorganizing the context, the subject in the same cluster must remain consistent, so does the direction;  5) Lexine?s relationship can be inherited by its attributes.

For instance, lexine Institute has the following at- tributes: management-staff, performance, legal, scan- dal (which are defined by the original EL). If action Against(Institute), then Against(Institute.management- people, Institute.performance, Institute.legal, Insti- tute.scandal); And vice versa.



IV. METHODOLOGY BASED ON FORMALIZED KNOWLEDGE  We describe a methodology based on the formalized knowl- edge discussed above and illustrate an implementation in a selected domain.

1) Selection of document For simplicity, the implementation is based on unique- topic documents selected from WSJ. Ten documents are randomly selected from WSJ with the unique topic on ?Company-acquisition?.

2) Manual definition of three sets of knowledge Given the selected documents, we define predicate dic- tionary and semantic patterns. In this paper, we define 6 predicate classes and manually assign appropriate words for each class according to the semantic association degree to the class.

Then, we manually define EL (such as depicted in Table III). In essence, EL is a static word association. We will see later that the original definition of EL has been adjusted by applying the rules described earlier.

Throughout this paper, the lexis we are interested in include Buyer, Seller, Management-change, Company- operation, and Profitability.

Finally, in order to let the system automatically capture the interested contents, we manually define semantic patterns discussed above.

3) Reorganizing of the original documents First, we use of the shelf semantic parser to parse the  Lexine Attribute Value (in Fuzzy Measurement)  Buyer Organization very possible  Management-staff possible  Seller Organization very possible  Management-staff possible  TABLE III: Example of Components of Epistemic Lexicon     sentences in the original documents, with the assistance of the predicate dictionary and semantic patterns. Next, we apply the defined generic rules to adjust the parsed output (depicted as ?reorganize the context?). Finally, we apply a statistical tool TRUST in [22] to compute the statistical corpus matrix.

Here we illustrate with an example on how this method- ology enhances the system to correctly identify context.

Consider the sentence, ?Beech-Nut Corp. damaged its image over the sale of apple juice that turned out to be water?, in which the word sale is the targeted word.

Here, we show step by step how the system correctly understands the sale drops down.

Procedure of Deduction Given by original definitions of the predicate dic- tionary and EL: Against(damage) and Company- operation(image, sale, . . . ) Start: Against(damage, image) ?? Against(damage, Company-operation.image) (By definition of EL) ?? Against(damage, Company-operation) (By 5th rule) ?? Against(damage, Company-operation.sale) (By 5th rule ) ?? Against(sale) ?? Performance-down(sale)

V. RESULTS AND DISCUSSIONS  A. Results  In this section, we apply the statistical tool TRUST on the parsed outputs. As an example, we only discuss the spacial distribution of ?company? which belongs to one of two possible concepts: ?Buyer? or ?Seller (companies are sold)?.

? Fig. 2(a) shows two different distributions of ?company?, given ?company? belongs to ?Buyer? or ?Seller? before tuning (tuning means applying the proposed method- ology). In the figure, the origin denotes the central word ?company?; X-axis denotes all other words in the selected corpus (due to the limits of the page, only 8 words with their concepts are listed); Y-axis denotes the distance between the central word ?company? and other words on X-axis. The larger the distance between ?com- pany? and another word, the less correlated these two words; ?circles? denote the distribution of ?company? belonging to concept ?Buyer? and ?stars? denote the distribution of ?company? belonging to concept ?Seller?.

For instance, the figure shows, before tuning, ?company? belonging to concept ?Buyer? is more related to word ?board? than ?company? belonging to ?Seller? before tuning.

? Fig. 2(b) shows two different distributions of ?company?, given ?company? belongs to ?Buyer? or ?Seller? after tuning. In the figure, the origin, the X-axis, and the Y- axis are similar to what has been defined in Fig. 2(a).

Comparing Fig. 2(b) and Fig. 2(a), we discover that some distributions have changed. For instance, Fig. 2(b) shows, after tuning, ?company?, belonging to concept ?Seller?, is more related to word ?board? than ?com- pany? belonging to ?buyer?, which is contrary to the results shown in Fig. 2(a). After re-checking the original documents, we find that this seemingly contradictory result is in accordance with human intuition: when a company is being sold, the decision must be passed by the director board of the sold company and accordingly, the word ?board? instead of the sold company usually appears within the context. However, when a company plans to takeover another company, the word ?board? seldom appears. In order to more clearly understand the tuning effect on the distributions, we change the views and outline the results in Fig. 3.

The statistical outputs by applying of the proposed method- ology might be viewed as the semantic coefficients of words.

These semantic coefficients can be directly applied on con- struction of fuzzy membership function in the latter stage, which used to be achieved by domain experts? arbitrary experience.

The proposed approach can be also used to identify the se- mantic meaning of a given word (word sense disambiguation).

Consider the sentence, ?The juice scandal forced Beech-Nut to pay a 2.2 million fine and 7.5 million to settle a lawsuit? in the domain of ?company-acquisition?, the system can accurately tell that Beech-Nut is a sold company because the tuned EL shows ?scandal?, ?pay fine?, and ?lawsuit? are more related to a ?seller? than a ?buyer?. Other possible applications of the proposed approach include: (1) operation as an assistant to a semantic parser (e.g. anaphor resolution) and, (2) operation as an assistant to a syntactic parser (e.g. pp attachment).

B. Comments and discussions  The main features of the proposed methodology that makes it an efficient tool for NLU can be summarized as follows: (1) simplify manual annotation. Human?s annotation is much more accurate than any intelligent machine. However, manual annotation is tedious and time consuming. Even for a domain expert, it is difficult to outline a collection of concepts which is good enough to cover most of the corpus. Besides, even with the available set of concepts, it is difficult to assign a word to its appropriate concept(s) because the belongingness of the word is multi-valued and dynamic. To tackle this issue, the methodology is proposed in a way where  ? the issue of assigning words? belongingness might be tackled through the three sets of knowledge;  ? by using the three sets of knowledge, the system dy- namically differentiates words? directions which will be applied on construction of membership functions;  (2) imparts human knowledge into statistical universe. Con- sidering the cons of statistical tools, researchers have tried to impart human knowledge into statistical universe to enhance     0 1 2 3 4 5 6 7 8  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  D  is ta  nc e  ? ?  ? ?  ? >  la rg  er Distribution of Concepts Referring Buyer and Seller before Tuning  company in Buyer company in Seller  action?up: expand  action?out: sell  action?in: purchase  agree: approve  perf?up: sale  action?in: buy  manage: board  legal: court  company  wrong  (a) Distribution of Concepts before Tuning  0 1 2 3 4 5 6 7 8  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9   D is  ta nc  e ?  ? ?  ? ?  > la  rg er  Distribution of Concepts Referencing Buyer and Seller after Tuning  company in Buyer company in Seller  company action?up: expand  action?out: sell  action?in purchase  agree: approve  perf?up: sale  action?in: buy  manage: board  legal: court  correct  (b) Distribution of Concepts after Tuning  Fig. 2: Distribution of Concepts Referring Buyer:company and Seller:company Before v.s. After Tuning  0 1 2 3 4 5 6 7 8  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9   D is  ta nc  e ?  ? ?  ? ?  > la  rg er  Distribution of Concepts Referencing Buyer Before and After Tuning  company in Buyer Before Tuning company in Buyer After Tuning  company action?up: expand  action?out: sell  action?in: purchase  agree: approve  perf?up: sale  action?in: buy  manage: board  legal: court  (a) Distribution of Concepts Referring Buyer:company  0 1 2 3 4 5 6 7 8  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  D  is ta  nc e  ? ?  ? ?  ? >  la rg  er  Distribution of Concepts Referencing Seller Before and After Tuning  company in Seller before Tuning company in Seller after Tuning  company action?up: expand  action?out: sell  action?in: purchase  agree: approve  perf?up: sale  action?in: buy  manage: board  legal: court  (b) Distribution of Concepts Referring Seller:company  Fig. 3: Distribution of Concepts Referring Buyer:company and Seller:company Before and After Tuning  NLU through the fuzzy technique. In previous research work, however, the major bottleneck was on the difficulties encoun- tered in generating the membership functions (MF) of natural words. In this work, we provide a solution to this through the proposed methodology into a statistical tool. We use the local knowledge (i.e. definition of EL and Predicate Dictionary) to tune the global corpus matrix (i.e. words distance distribution and concept distance distribution). As such, the original EL (human-defined, incomprehensive and inaccurate) is tuned and enlarged so that EL becomes closer to human common  sense and in accordance with its objective distribution (refer to Table IV). After achieving the tuned EL, the system can easily generate the MF for interested words in the selected corpus;  (3) free from the boundary limitation. For statistical ap- proaches, only the near neighbors? occurrences can be taken into account. For instance, 3 ? gram only calculates the co-occurrences of 3 words on the central word?s left and right respectively. If the window size increases up to 4, the complexity of computation will increase several orders     Lexine Attribute Value (in Fuzzy Measurement)  Buyer action-in very possible  agree very possible management-staff not very possible  legal not very possible Seller  action-out very possible legal possible  management-staff possible sale hard to tell  TABLE IV: Example of Components of Epistemic Lexicon after Tuning  of magnitude depending on the corpus size. However, the proposed approach tackles this issue by logically reorganizing related sentences together no matter how far away these sentences are from each other. For instance, according to the word?s direction defined by predicate dictionary and lexine?s attributes defined by EL, the generic rules reasonably connects sentences which have the same subject and direction.

(4) free from decision rules. The decision rules are the pivot component for both statistical and fuzzy approaches. However the decision rules are difficult to discover (for statistical approaches) and require a great deal of domain knowledge (for fuzzy approaches). In this paper, we solve such issue by simply defining 5 generic rules and the experiment shows they are satisfactory. What is more important, is that they are less domain dependent.

On the other hand, the proposed approach is still domain dependent in some sense. Indeed, for each domain, we have to re-define EL, concepts, and semantic patterns. In practice, it is preferable to constrain the technique to a narrowly defined domain and make use of a large text corpus.



VI. CONCLUSION  In this paper, a fundamental research on how to formal- ize human knowledge through three sets of knowledge is thoroughly discussed. The proposed work can either tackle issues challenging the traditional statistical approaches such as how to overcome the insufficient co-occurrence information caused by the limited window size, or strengthen machines the capability to impart human knowledge into statistical approaches.

Based on this research, the experiment is also presented to demonstrate the potential capabilities in tackling issues existing in traditional statistical measures and classic fuzzy approaches. The results presented indicate that the use of proposed approach enhances the obtained words semantic coefficients.

Although the proposed approach decreases the domain experts? impact on the final results, it is still necessary to apply domain knowledge to help the approach more accurately depict the semantic relations between concepts and words.

In order to make the approach compatible to multi-domain,  in future research work, we will focus on expanding the unique topic domain to more complex ones. Besides, we will explore how to construct membership function based on words semantic coefficients through the proposed approach in our future research work.

