

Privacy Preserving Data Classification with Rotation Perturbation  Keke Chen Ling Liu College of Computing, Georgia Institute of Technology  {kekechen, lingliu}@cc.gatech.edu  1 Introduction  Data perturbation techniques are one of the most popu- lar models for privacy preserving data mining [3, 1]. It is especially convenient for applications where the data own- ers need to export/publish the privacy-sensitive data. A data perturbation procedure can be simply described as follows.

Before the data owner publishes the data, they randomly change the data in certain way to disguise the sensitive in- formation while preserving the particular data property that is critical for building the data models. Several perturba- tion techniques have been proposed recently, among which the most typical ones are randomization approach [3] and condensation approach [1].

Loss of Privacy vs. Loss of Information.

Perturbation techniques are often evaluated with two basic metrics, loss of privacy and loss of information (resulting in loss of accuracy for data classification). An ideal data perturbation algorithm should aim at minimizing both pri- vacy loss and information loss. However, the two metrics are not well-balanced in many existing perturbation tech- niques [3, 2, 5, 1].

Loss of privacy can be intuitively described as the dif- ficulty level in estimating the original value from the per- turbed data. In [3], the variance of the added random noise is used as the level of difficulty for estimating the original values. However, later research [5, 2] reveals that variance is not an effective indicator for randomization approach since the original data distribution has to be known. In addi- tion, paper [8] shows that the loss of privacy is also subject to the special attacks that can reconstruct the original data from the perturbed data.

The loss of information typically refers to the amount of critical information preserved about the datasets after the perturbation. Different data mining tasks, such as classi- fication and association mining, typically utilize different set of properties of a dataset. Thus, the information that is considered critical to data classification may differ from those critical to association rule mining. We argue that the exact information that need to be preserved after pertur- bation should be ?task-specific?. Since most classification models typically concern multi-dimensional properties, per-  turbation techniques for data classification should perturb multiple columns together. To our knowledge, very few perturbation-based privacy protection proposals so far have considered multi-dimensional perturbation techniques.

Contribution and Scope of the paper.

Bearing these issues in mind, we have developed a random rotation perturbation approach to privacy preserving data classification. In contrast to other existing privacy preserv- ing classification methods [1, 3, 9], our approach exploits the task-specific information about the datasets to be clas- sified, aiming at producing a robust data perturbation that exhibits a better balance between loss of privacy and loss of information, without performance penalty.

Concretely, we observe that the multi-dimensional geo- metric properties of datasets are the critical ?task-specific information? for many classification algorithms. One in- tuitive way to preserve the multi-dimensional geometric properties is to perturb the original dataset through rota- tion transformation. We have identified and proved that kernel methods, SVM classifiers with the three popular ker- nels, and the hyperplane-based classifiers, are the three cat- egories of classifiers that are rotation-invariant.

Another important challenge for the rotation perturba- tion approach is the privacy loss measurement (the level of uncertainty) and privacy assurance (the resilience of the rotation transformation against unauthorized disclosure).

Given that a random rotation based perturbation is a multi- dimensional perturbation, the privacy guarantee of the mul- tiple dimensions (attributes) should be evaluated collec- tively to ensure the privacy of all columns involved and the privacy of the multi-column correlations. We design a uni- fied privacy model to tackle the problem of privacy evalu- ation for multi-dimensional perturbation, which addresses three types of possible attacks: direct estimation, approx- imate reconstruction, and distribution-based inference at- tacks. With the unified privacy metric, we present the pri- vacy assurance of the random rotation perturbation as an op- timization problem: given that all rotation transformations result in zero-loss of accuracy for the discussed classifiers, we want to pick one rotation matrix that provides higher privacy guarantee and stronger resilience against the three types of inference attacks.

2 Rotation and Classifiers  In this section, we first describe rotation transformation and the set of geometric properties of the datasets significant to most classifiers, and then we define rotation-invariant classifiers.

Notations for Datasets Training dataset is the part of data that has to be exported/published in privacy-preserving data classification. A classifier learns the classification model from the training data and then is applied to classify the unclassified data. Suppose that X is a training dataset con- sisting of N data rows (records) and d columns (attributes).

For the convenience of mathematical manipulation, we use Xd?N to notate the dataset, i.e., X = [x1 . . .xN ], where xi is a data tuple, representing a vector in the real space Rd.

Each data tuple belongs to a predefined class, which is de- termined by its class label attribute yi. The class labels can be nominal (or continuous for regression). The class label attribute of the data tuple is public, i.e., privacy-insensitive.

All other attributes containing private information needs to be protected.

Properties of Geometric Rotation Let Rd?d represent the rotation matrix. Geometric rotation of the data X is gener- ally notated as a function g(X), g(X) = RX . Note that the transformation will not change the class label of data tuples, i.e., Rxi still has the label yi.

A rotation matrix Rd?d is defined as a matrix having the follows properties. Let RT represent the transpose of the matrix R, rij represent the (i, j) element of R, and I be the identity matrix. Both the rows and the columns of R are orthonormal, i.e., for any column j,  ?d i=1 r  ij = 1,  and for any two columns j and k, ?d  i=1 rijrik = 0. The similar property is held for rows. The definition infers that RT R = RRT = I . It also implies that by changing the order of the rows or columns of rotation matrix, the result- ing matrix is still a rotation matrix. A key feature of rota- tion transformation is preserving length. It follows that ro- tation also preserves inner product and Euclidean distance between any pair of points. In general, rotation preserves the geometric shapes such as hyperplane and hyper curved surface in the multidimensional space.

Rotation-invariant Classifiers We can treat the classi- fication problem as function approximation problem ? the classifiers are the functions learned from the training data.

Therefore, we can use functions to represent the classi- fiers. Let f?X represent a classifier f? trained with dataset X and f?X(Y ) be the classification result on dataset Y . Let T (X) be any transformation function, which transforms the dataset X to another dataset X ?. We use Err(f?X(Y )) to notate the error rate of classifier f?X on testing data Y and let ? be some small real number, |?| < 1.

Definition 1. A classifier f? is invariant to some transforma- tion T if and only if Err(f?X(Y )) = Err(f?T (X)(T (Y )))+  ? for any training dataset X and testing dataset Y .

It follows that the strict condition f?X(Y ) ? f?T (X)(T (Y )) trivially guarantees the invariance property.

If a classifier f? is invariant to rotation transformation, we specifically name it as a rotation-invariant classifier.

The initial result shows several popular classifiers deal- ing with numerical data are rotation-invariant. Due to the space limitation, we will ignore the concrete proofs [4], and summarize that KNN , general Kernel methods, SVM clas- sifiers using polynomial, radial basis, and neural network kernels, and Hyperplane-based classifiers are invariant to rotation.

3 Evaluating Privacy Quality for Random Rotation Perturbation  The goals of rotation based data perturbation are twofold: preserving the accuracy of classifiers, and preserv- ing the privacy of data. The discussion about the rotation- invariant classifiers has proven that the rotation transforma- tion theoretically guarantees zero-loss of accuracy for three popular types of classifiers. We dedicate this section to dis- cuss how good the rotation perturbation approach is in terms of preserving privacy. The critical step to identify the good rotation perturbation is to define a multi-column privacy measure for evaluating the privacy quality of any rotation perturbation to a given dataset. With this privacy measure, we can employ some optimization methods to find good ro- tation perturbations for a given dataset.

For data perturbation approach, the quality of preserved privacy can be understood as the difficulty level of estimat- ing the original data from the perturbed data. Basically, the attacks to the data perturbation techniques can be sum- marized in three categories: (1)estimating the original data directly from the perturbed data [3, 2], without any other knowledge about the data (naive inference); (2) approxi- mately reconstructing the data from the perturbed data and then estimating the original data from the reconstructed data [8, 6] (approximation-based inference); and (3) if the distri- butions of the original columns are known, the values or the properties of the values in the particular part of the distribu- tion can be estimated [2, 5] (distribution-based inference).

A multi-colum metric should be applicable to all three types of inference attacks to determine the robustness of the per- turbation technique. We will focus on the first two attacks in this paper.

3.1 Privacy Model for Multi-column Perturbation  Unlike the existing value randomization methods, where multiple columns are perturbed separately, the random ro- tation perturbation needs to perturb all columns together,       where the privacy quality of all columns is correlated under one single transformation.

Since in practice different columns(attributes) may have different privacy concern, we consider that a general- purpose privacy metric ? for entire dataset is based on col- umn privacy metric. An abstract privacy model is de- fined as follows. Let p be the column privacy metric vector p = (p1, p2, . . . , pd) for d columns, and there are privacy weights associated to the columns, respectively, notated as w = (w1, w2, . . . , wd). ? = ?(p,w) defines the overall privacy guarantee. Basically, the design of privacy model should consider determining the three factors p, w, and function ?. We summarize our design of privacy metric as follows.

Unified Column Privacy Metrics Below we extend the variance-based privacy metric [3] to the multi-column uni- fied metric. Let Y be a random variable, representing a column of the dataset, Y? be the perturbed/reconstructed result of Y, and D be the difference between Y and Y?.

Let E[D] and V ar(D) denote the mean and the variance of difference (VoD) respectively. E[D] is not effective in protecting privacy, thus VoD becomes the primary measure in terms of the first level of inferences. Unfortunately, this single-column privacy metric does not work across differ- ent columns since it ignores the effect of value range and the mean of the original data column. It is easy to under- stand that the same amount of VoD is not equally effective for different value ranges. One effective way to unify the different value ranges is via normalization.

Let si = 1/(max(Yi) ? min(Yi)), ti = min(Yi)/(max(Yi)?min(Yi)) denote the constants de- termined by the value range of the column Yi. The column Yi is scaled to range [0, 1], generating Ysi, with the trans- formation Ysi = si(Yi? ti). This allows all columns to be evaluated on the same base, eliminating the effect of diverse value ranges. The normalized data Ysi is then perturbed to Y?si. Let D  ? i =Y  ? si ? Ysi. We use V ar(D?i), instead of  V ar(Di), as the unified column privacy metric.

Composing the Column Metrics Having the unified  column metrics p, we can compose the multiple metrics into one metric for optimization. Let w denote the impor- tance of columns in terms of preserving privacy. Intuitively, the more important the column is, the higher level of privacy guarantee will be required for the perturbed data column.

Therefore, we let  ?d i=1 wi = 1 and use pi/wi to represent  the weighted column privacy.

The first composition function is the minimum privacy guarantee among all columns. Concretely, when we mea- sure the privacy quality of a multi-column perturbation, we need to pay special attention to the column having the low- est weighted column privacy, because such columns could become the breaking point of privacy. Hence, we design the minimum privacy guarantee ?1 = mindi=1{pi/wi}. Sim-  ilarly, the average privacy guarantee of the multi-column perturbation, ?2 = 1d  ?d i=1 pi/wi, is another interesting  measure. With the definition of privacy guarantee, we can evaluate and optimize the privacy quality of a give pertur- bation.

Multi-column Privacy Analysis for Random Rotation Perturbation With the variance metric over the normalized data, we can formally analyze the privacy quality of random rotation perturbation. Let X be the normalized dataset, X ?  be the rotation of X , and Id be the d-dimensional identity matrix. Thus, VoD can be evaluated based on the difference matrix X ??X , and the VoD for column i is the element (i,i) in the covariance matrix of X ??X , which is represented as  Cov(X ? ? X)(i,i) = Cov(RX ? X)(i,i) = ((R ? Id)Cov(X)(R ? Id)T )(i,i) (1)  Let rij represent the element (i, j) in the matrix R, and cij be the element (i, j) in the covariance matrix of X . The VoD for ith column is computed as follows.

Cov(X ? ? X)(i,i) = d?  j=1  d?  k=1  rijrikckj ? 2 d?  j=1  rijcij + cii  (2) We develop a simple method to implement a fast local  optimization. As shown in Equation 2, the privacy metric of column i is only related to the row vectors of rotation.

Therefore, swapping the rows of rotation matrix could pro- vide a better rotation that provides higher privacy guaran- tee. This method can significantly reduce the search space and thus provides better efficiency as we observed in exper- iments.

ICA-based Attack to Rotation Perturbation Indepen- dent Component Analysis (ICA) [7] is the most potential method to breaching the privacy protected by rotation per- turbation. However, we argue that ICA is in general not effective in breaking the rotation perturbation, in practice.

ICA can be briefly describes as follows. Let matrix X com- posed by the source signals, where each row vector is a sig- nal, and the observed mixed signals X ? be X ? = AX . ICA model can be applied to estimate the independent compo- nents (the row vectors) of the original signals X , from the mixed signals X ?, if the four conditions are satisfied [7].

Three factors make the ICA attacks are often quite in- effective for rotation perturbation. First, two of the four conditions, although reasonable for signal processing, are not common for data classification: 1) The source signals are independent and 2) All the source signals must be non- Gaussian with possible exception of one signal. In addition, the ordering of the reconstructed signals can not be deter- mined.

In practice, we can evaluate the effectiveness of ICA at- tacks with difference between the constructed data and the original data. Since the ordering of the reconstructed row       Privacy Quality for Breast-w Data  0.2  0.3  0.4  0.5  0.6  0.7  0.8  1 11 21 31 41 # of iterations  P ri  va cy  Q u  al it  y (S  ta n  d ar  d D  ev ia  ti o  n )  LocalOpt Non-Opt ICA reconstruction  Figure 1. ICA reconstruction has no effect on privacy guar- antee.

Privacy Quality for Votes Data  0.4  0.45  0.5  0.55  0.6  0.65  0.7  1 11 21 31 41 # of iterations  P ri  va cy  Q u  al it  y (S  ta n  d ar  d D  ev ia  ti o  n )  LocalOpt Non-Opt ICA Reconstruction Combined  Figure 2. Example that ICA undermines the privacy guar- antee.

Minimum Privacy Guarantee   0.1  0.2  0.3  0.4  0.5  0.6  0.7  Br ea  st- w  Cr ed  it-a  Cr ed  it-g  Di ab  ete s  E.

Co  li  He ar  t  He pa  titi s  Ion os  hp er  e Iris  Tic -ta  c-t oe  Vo tes  W ine  P ri  va cy  Q ua  lit y  (s td  ev )  Condensaton-Min Rotation-Min  Figure 3. Comparison on min- imum privacy level with con- densation approach.

vectors is not certain, we estimate the VoDs with the best effort of the d! possible row orderings. Let X?k be the ICA reconstructed data with one of the orderings, and P kica be the minimum privacy guarantee for X?k, k = 1 . . . d!. The ordering that gives lowest minimum privacy quality is se- lected as the most likely ordering and the corresponding privacy quality is the undermined privacy quality.

Algorithm. Combining the local optimization and the test for ICA attacks, we develop a random iterative algo- rithm to find a better rotation in terms of privacy qual- ity. The algorithm runs in a given number of iterations.

In each iteration, it randomly generates a rotation matrix.

Local optimization through row-swapping rows is applied to find a better rotation matrix, which is then tested by the ICA reconstruction. We take the combination P = min{Pica, Popt} as the final privacy guarantee. The rota- tion matrix is accepted as the best perturbation yet if it pro- vides highest P among the previous perturbations.

4 Experimental Result  We design three sets of experiments. The first set is used to show that the discussed classifiers are invariant to rota- tions. The second set shows privacy quality of the good rotation perturbation. The third one compares the privacy quality between the condensation approach and the ran- dom rotation approach. Due to the space limitation, we re- port some results of the later two sets of experiments. The datasets are all from UCI machine learning database. Three results are selected to show the effectiveness of the rotation perturbation approach.

Figure 1 represents a typical scenario that ICA attacks are totally ineffective, while Figure 2 shows, when ICA at- tacks are substantial, the algorithm can also find a rotation that has the highest combined privacy guarantee in the ran- dom rotation matrices. Figure 3 demostrates the rotation approach can provide much higher privacy quality than the condensation approach [1].

5 Conclusion  Loss of privacy and loss of information/accuracy are treated as two conflict factors in privacy preserving data classification. In this paper, we propose a rotation based perturbation technique that guarantees zero loss of accu- racy for many classifiers. Meanwhile, we can adjust the rotation to find a locally optimal rotation in terms of ba- sic privacy guarantees and possible attacks, where the opti- mality is measured by a new multi-column privacy metric.

Experiments show that the rotation perturbation can greatly improve the privacy quality without sacrificing accuracy.

