An Effective Algorithm for Mining Positive and  Negative Association Rules

Abstract?Recently, mining negative association rules has received some attention and been proved to be useful in real world. This paper presents an efficient algorithm (PNAR) for mining both positive and negative association rules in databases.

The algorithm extends traditional association rules to include negative association rules. When mining negative association rules, we adopt another minimum support threshold to mine frequent negative itemsets. With a correlation coefficient measure and pruning strategies, the algorithm can find all valid association rules quickly and overcome some limitations of the previous mining methods. The experimental results demonstrate its effectiveness and efficiency.

Keywords-data mining; association rule; frequent itemset; correlation coefficient; pruning strategy

I.  INTRODUCTION The research and application of data mining technology are  a hot spot in database and artificial intelligence at recent years.

Association Rules Mining introduced by R. Agrawal [1] is an important research topic among the various data mining problems. Association rules have been extensively studied in the literature for their usefulness in many application domains such as market basket analysis, recommender systems, diagnosis decisions support, telecommunication, intrusion detection, and etc..

All the traditional association rule mining algorithms were developed to find positive associations between itemsets.

Several algorithms has been developed to cope with the popular and computationally expensive task of association rule mining, such as Apriori [1], AIS [2], DHP [3], Partition [4], and etc..

With the increasing use and development of data mining techniques and tools, much work has recently focused on finding negative patterns, which can provide valuable information. However, mining negative association rules is a difficult task, due to the fact that there are essential differences between positive and negative association rule mining. We will attack two key problems in negative association rule mining:  (1) How to effectively search for negative frequent itemsets.

(2) How to effectively identify negative association rules.

Although some researchers pointed out the importance of negative associations, only some groups of researchers ([5], [6], [7] and etc.) proposed an algorithm to mine these types of associations. This not only illustrates the novelty of negative association rules, but also the challenge in discovering them.



II. BASIC KNOWLEDGE  A. Concepts and Definitions Let I={i1,i2,...,in} be a set of n distinct literals called items.

Let DB be a set of transactions, where each transaction T is a set of items, and each transaction is associated with a unique identifier called TID. Let A, called an itemset, be a set of items in I. The number of items in an itemset is the length (or the size) of an itemset. Itemsets of length k are referred to as k- itemsets. A transaction T is said to contain A if A ? T. An association rule is an implication of the form A? B, where A ? I, B ? I, and A?B=? . We call A the antecedent of the rule, and B the consequent of the rule.

The rule A?B has a support (denoted as supp) s in DB if s% of the transactions in DB contains A?B. In other words, the support of the rule is the probability that A and B hold together among all the possible presented cases. i.e.

supp(A?B)=supp(A?B)=P(A?B).                               (1) The rule A ? B has a measure of its strength called  confidence (denoted as conf) c if c% of transactions in DB that contain A also contain B. In other words, the confidence of the rule is the conditional probability that the consequent B is true under the condition of the antecedent A. i.e.

conf(A?B)= P(B|A)=supp(A?B)/supp(A).                    (2)  B. Classical Method The classical method is well known as the support-  confidence framework for association rule mining [1]. It can be decomposed into the following two issues:  (1) Generate all frequent itemsets: All itemsets that have a support greater than or equal to user-specified minimum support (ms) are generated.

DOI 10.1109/CSSE.2008.1199     (2) Generate all the rules that have a user-specified minimum confidence (mc) in the following naive way: For every frequent itemset X and any B ? X, let A=X-B. If the rule A?B has the mc, then it is a valid rule.

The generative rules are called interesting positive rules.

A frequent itemset (denoted as PL) [8] is an itemset that meets the user-specified ms. Accordingly we define an infrequent itemset (denoted as NL) as an itemset that does not meet the user-specified ms.

The second subproblem is straight forward and can be done efficiently in a reasonable time. However, the first subproblem is very tedious and computationally expensive for very large database and this is the case for many real life applications.

In order to generate the frequent itemsets, an iterative approach is used to first generate the set of frequent 1-itemsets L1, then the set of frequent itemsets L2, and so on until for some value of r the set Lr is empty. At this stage the algorithm can be terminated. During the k-th iteration of this procedure a set of candidates Ck is generated by performing a (k-2)-join on the frequent itemsets Lk-1. The itemsets in this set Ck are candidates for frequent itemsets, and the final set of frequent itemsets Lk must be a subset of Ck. Each element of Ck needs to be validated against the transaction database to see if it indeed belongs to Lk.

The validation of the candidate itemset Ck against the transaction database seems to be bottleneck operation for the algorithm. In order to improve the algorithm efficiency, the apriori property is introduced that all subsets of a frequent itemset A in DB are also frequent in DB, and all supersets of an infrequent itemset A in DB are also infrequent in DB.

C. Negative Association Rules The negation of an itemset A is indicated by ?A, which  means the absence of the itemset A. We call a rule of the form A?B a positive association rule, and rules of the other forms (A? ?B, ?A?B and ?A? ?B) negative association rules.

The support and confidence of the negative association rules can make use of those of the positive association rules [9].

The support is given by the following formulas:  supp(?A)=1-supp(A).                                                         (3)  supp(A??B)=supp(A)-supp(A?B).                                 (4)  supp(?A?B)=supp(B)- supp(A?B).                                (5)  supp(?A??B)=1-supp(A)-supp(B)+supp(A?B).             (6)  The confidence is given by the following formulas:  )(sup )(sup)(sup)(  Ap BApApBAconf ??=?? .                         (7)  )(sup1 )(sup)(sup)(  Ap BApApBAconf  ? ??=?? .                         (8)  )(sup1 )(sup)(sup)(sup1)(  Ap BApBpApBAconf  ? ?+??  =??? .   (9)  The negative association rules discovery seeks rules of the three forms with their support and confidence greater than, or equal to, user-specified ms and mc thresholds respectively.

These rules are referred to as an interesting negative association rule.



III. PNAR ALGORITHM  A. Correlation Coefficient When mining positive and negative association rule at the  same time, we will find that the mining rules are contradictory frequently. For example, the rules of the forms A? ?B and ?A ? B may be mined together, but the two rules are contradictory. In order to resolve these contradictions, we can judge the types of mining association rules by the correlation coefficient [10]. Let A and B for the two itemsets. The correlation coefficient (denoted as corrA,B) can show the relevance of the two itemsets.  As follows:  )(sup)(sup )(sup  , BpAp BApcorr BA  ?=                                   (10)  The value of correlation coefficient exist the following three situations:  (1) If corrA,B>1, then A and B are positive correlation. The more A occurs in a transaction the more B will likely also occur in the same transaction and vice versa.

(2) If corrA,B?1, then A and B are independent. The B occurs in a transaction, and it has no concern with whether the A occurs in the same transaction or not.

(3) If corrA,B<1, then A and B are negative correlation. The more A occurs in a transaction the less B will likely also occur in the same transaction and vice versa.

By the definition of correlation coefficient, we can conclude the below lemmas:  Lemma 1: If the itemset A and B are positive correlation, then the forms of A?B or ?A? ?B will be mined.

Lemma 2: If the itemset A and B are negative correlation, then the forms of A? ?B or ?A?B will be mined.

B. Pruning Strategies As we have seen, there can be an exponential number of  infrequent itemsets in a database, and only some of them are useful for mining interesting association rules. Therefore, pruning strategy is critical to efficient search for interesting frequent negative itemsets. When mining negative association rules, we can adopt different minimum support (dms) and minimum confidence (dmc) threshold to improve the usability of the rules.

Through the experimental analysis, we found that the association rules of the forms A ? B and ?A ? ?B have considerable proportion when mining both positive and negative association rules. In particular, the number of the form ?A? ?B is very large, and these rules including pure negative itemsets are usually of little use in real application. For     example, we assume that the database DB in a supermarket contain n transactions. Now we concern the sale of tea (t) and coffee (c).  Suppose we mine the rule of the form ?t? ?c, which means customers not to buy tea and coffee in a transaction, the result is not useful to our market basket analysis. So we adopt a pruning strategy that we will not to consider the part negative association rules of the form ?A? ?B to improve mining efficiency. The search space can be significantly reduced by the pruning strategy.

In addition, we are only interested in those absence itemsets whose positive counterparts are frequent for market basket analysis when mining negative association rules. For example, the absence itemset ?A is not show if the itemset A is not frequent. The pruning strategy is more benefit to generate frequent 1-itemset. Because of reducing the number of frequent 1-itemset, the number of frequent and infrequent k-itemset is reduced accordingly.

C. PNAR Algorithms As mentioned before, the process of mining both positive  and negative association rules can be decomposed into the following three subproblems, in a similar way to mining positive rules only.

(1) Generate the set PL of frequent itemsets and the set NL of infrequent itemsets.

(2) Extract positive rules of the form A?  B in PL.

(3) Extract negative rules of the forms A ? ?B and  ?A?B in NL.

Let DB be a database, and ms, mc, dms and dmc given by  the user. Our algorithm for extracting both positive and negative association rules with a correlation coefficient measure and pruning strategies is designed as follows:  Algorithm 1: Positive and Negative Association Rules (PNAR)  //Input: DB, ms, mc, dms, dmc, respectively a set of transactions, minimum support, minimum confidence, different minimum support, different minimum confidence.

//Output: AR: Positive and Negative Association Rules.

// Generate the itemsets PL and NL  (0)  positiveAR? ? ; negativeAR? ? ; /*positive and negative AR itemsets*/  (1) PL?? ; NL?? ; (2) scan the database and find the set of frequent 1-itemset  (L1)  (3) PL?PL?L1  (4) for (k = 2; Lk-1?? ; k + +){ (5)    Ck= Lk-1 ? L k-1  (6)    for each i?Ck {  (7)       s=supp(i)  (8)       if s?ms then{  (9)          Lk?Lk?{i}  (10)        PL?PL?Lk (11)      }  (12)      else {  (13)        NLk?NLk?{i}  (14)       NL?NL?NLk (15)      }  //Generate positive association rules in PL  (16) for each frequent itemset i in PL  (17)    for each expression  A?B= i and  A?B=? { (18)       corrA,B=supp(A?B)/( supp(A) * supp(B))  (19)       if corrA,B >1 then  (20)          if conf(A?B)?mc then (21)             positiveAR ? positiveAR?{ A?B } (22)          }  //Generate negative association rules in NL  (23) for each infrequent itemset i in NL  (24)    for each expression  A?B= i and  A?B=? { (25)       corrA,B=supp(A?B)/( supp(A) * supp(B))  (26)       if corrA,B <1 then  (27)       if supp(A??B)?dms and conf(A? ?B)?dmc then (28)             negativeAR ? negativeAR?{ A? ?B }  (29)       if supp(?A?B)?dms and conf(?A?B)?dmc then  (30)             negativeAR ? negativeAR?{ ?A?B } (31)     }  (32)   }  (33) }  (34) AR?positiveAR? negativeAR  (35) ruturn AR  PNAR generates not only all positive association rules in PL, but also negative association rules in NL. When mining negative association rules, we adopt different threshold to improve the usability of the frequent negative itemsets. With a correlation coefficient measure and pruning strategies, the algorithm can find all valid association rules quickly. An example of mining positive and negative itemsets is given below for illustrative purposes.

D. Experimental Results For the convenience of comparison, we conducted our  experiments on the synthetic dataset to study the behaviors of the algorithm.

Example. Let us consider a small transactional table with 10 transactions and 6 items. In Table 1 a small transactional database is given.

TABLE I.  A TRANSAACTION DATABASE TD  TID Items 1 A,C,D 2 B,C 3 C 4 A,B,F 5 A,C,D  TID Items 6 E 7 B,F 8 B,C,F 9 A,B,E  10 A,D In the following tables, Lk is denoted as all frequent k-  itemset. Given that ms=0.3, all the positive and negative frequent itemsets can then be discovered in Table 2 by PNAR algorithm if we adopt the same ms. Given that dms=0.4, all the positive and negative frequent itemsets can then be discovered in Table 3 if we adopt the different ms. And in Table 4, we compare three algorithms in the same TD.

TABLE II.  FREQUENT ITEMSETS GENERATED FROM THE DATABASE WITH THE SAME MS IN TABLE 1  L1 A B C D F  Supp 0.5 0.5 0.5 0.3 0.3  L2 AD BF  A?B A?C A?F  Supp 0.3 0.3 0.3 0.3 0.4  B?C B?D C?D  0.3 0.5 0.3  C?F D?F ?AB ?AC  0.4 0.3 0.3 0.3  ?BC ?BD ?DF  0.3 0.3 0.3  L3 AD?F A?BD B?DF  Supp 0.3 0.3 0.3  ?AB?D ?BD?F  0.3 0.3    TABLE III.  FREQUENT ITEMSETS GENERATED FROM THE DATABASE WITH THE DIFFERENT MS IN TABLE 1  L1 A B C D F  Supp 0.5 0.5 0.5 0.3 0.3  L2 AD BF  A?F B?D C?F  Supp 0.3 0.3 0.4 0.5 0.4  From Table 4 we discover that the PNAR algorithm can reduce the number of the positive and negative frequent itemsets efficiently. Especially the number of frequent 2- itemset is less than before in a certain extent. From Table 4 we  can detect the number of frequent 2-itemset is less six itemsets than SRM algorithm [6], and the number of frequent 2-itemset with the different ms is less ten itemsets than those with the same ms. So the PNAR algorithm can reduce the search space efficiently and improve the efficiency, and can overcome some limitations of the previous mining methods.

TABLE IV.  COMPARISON OF THE THREE ALGORITHMS  L1 Apriori 5  SRM 10 PNAR 5  L2   L3   L4   PNAR 5 5 0 0  ms 0.3 0.3 0.3 0.3  dms  0.4

IV. CONCLUSION In this paper, we have designed a new algorithm for  efficiently mining positive and negative association rules in databases. Our approach is novel and different from existing research. We have designed pruning strategies for reducing the search space and improving the usability of mining rules, and have used the correlation coefficient to judge which form association rule should be mined. It is shown by empirical studies that the proposed approach is effective, efficient and promising.

