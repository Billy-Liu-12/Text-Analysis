Learning Associations of Conjuncted Fuzzy Sets for Data Prediction

Abstract? Fuzzy Associative Conjuncted Maps (FASCOM) is a fuzzy neural network that represents information by con- juncting fuzzy sets and associates them through a combination of unsupervised and supervised learning. The network first quantizes input and output feature maps using fuzzy sets.

They are subsequently conjuncted to form antecedents and consequences, and associated to form fuzzy if-then rules. These associations are learnt through a learning process consisting of three consecutive phases. First, an unsupervised phase initializes based on information density the fuzzy membership functions that partition each feature map. Next, a supervised Hebbian learning phase encodes synaptic weights of the input-output as- sociations. Finally, a supervised error reduction phase fine-tunes the fine-tunes the network and discovers the varying influence of an input dimension across output feature space. FASCOM was benchmarked against other prominent architectures using data taken from three nonlinear data estimation tasks and a real- world road traffic density prediction problem. The promising results compiled show significant improvements over the state- of-the-art for all four data prediction tasks.



I. INTRODUCTION  Neural networks and fuzzy systems represent key method-  ologies in soft computing [1]. Neural Networks take in-  spiration from brain functions and connectionist structures  to be highly capable in learning and adaptation, while  fuzzy systems mimic human-like knowledge representation  and reasoning. Each methodology possess its own share  of drawbacks. On one hand, neural networks are typically  black boxes and have a problem of being opaque. On the  other hand, fuzzy systems face design problems such as  determining membership functions, as well as identifying and  inferring fuzzy rules.

By integrating neural networks with fuzzy logic, the archi-  tecture?s transparency can significantly improve. Conversely,  design problems faced by fuzzy systems may be alleviated  through brain-inspired techniques, such as self organization.

Thus, the hybridization of neural networks with fuzzy sys-  tems results in powerful tools for machine learning and logic  inference because their strengths are complimentary and are  hence able to maximize the desirable properties of both  methodologies. Such hybrid intelligent systems are widely  known as neuro-fuzzy systems or fuzzy neural networks (the  name used henceforth).

Fuzzy neural networks are generally grouped into two  distinctively different paradigms based on how they represent  fuzzy if-then rules. The first paradigm is called the linguistic  H. Goh and J.-H. Lim are with the Computer Vision and Image Under- standing Department, Institute for Infocomm Research, A*STAR (Agency for Science, Technology and Research), 21 Heng Mui Keng Terrace, Singapore 119613 (email: {hlgoh,joohwee}@i2r.a-star.edu.sg).

C. Quek is with the Centre for Computational Intelligence, School of Computer Engineering, Nanyang Technological University, Block N4, Nanyang Avenue, Singapore 639798 (email: ashcquek@ntu.edu.sg).

fuzzy model (e.g. Mamdani model [2]), whereby both the  antecedent and consequence are represented using fuzzy  sets. The second is known as the precise fuzzy model (e.g.

TSK model [3]?[5]) in which the antecedent is a fuzzy set  while the consequence is expressed as linear equations [6],  [7]. Linguistic fuzzy models excel in interpretability while  lacking in accuracy, while the inverse is true for precise fuzzy  models (see [7], [8] for discussion).

This paper proposes the Fuzzy Associative Conjuncted  Maps (FASCOM) architecture, which is a variant of a  fuzzy linguistic model, whereby a fuzzy if-then rule Rk is  represented in the form of  Rk : if x1 is Ak,1 and . . . and xI is Ak,I  then y1 is Ck,1 and . . . and yS is Ck,S (1)  where X = [x1, . . . , xI ] T and Y = [y1, . . . , yS ]  T are the  input and output vectors respectively, Ak,i, i ? {1 . . . I} and Ck,s, s ? {1 . . . S} represent linguistic labels of input and output linguistic variables, while I and S are the number  of antecedents and consequences respectively. Each rule is  weighted by vk denoting the strength of the Rk.

The current research direction in fuzzy neural networks  is to learn, modify and infer fuzzy rules based on past  experience. This is achieved using either unsupervised and/or  supervised learning techniques to identify, learn and adjust  fuzzy if-then rules.

In unsupervised learning approaches [9]?[12], algorithms  are used to identify fuzzy rules before applying neural  network techniques to adjust rules without the need for a  priori outputs. However, due to the heavy reliance on the  training data, non-representative data may lead to ill defined  rules and hence producing an inaccurate models.

For supervised learning approaches [13], [14], rules are  identified by mapping input-output data pairs. However, a  drawback is that the semantics of the fuzzy neural network  remains opaque, contradicting the original conceived objec-  tive of combining fuzzy logic with neural networks.

FASCOM is encoded through a learning process consisting  of one unsupervised learning phase followed by two consecu-  tive supervised learning phases. The initial phase involves the  unsupervised initialization of membership functions of fuzzy  sets of data dimensions. This is followed by a supervised  Hebbian learning phase to determine the synaptic weights be-  tween associated nodes in the network. The final supervised  phase fine-tunes the network by reducing the error produced  by the system. The entire learning process consisting of the  three learning phases will be described in detail in Section III  and the contributions of the individual phases to the accuracy  of data prediction is discussed in V-A.4.

The following contributions of the proposed architecture  are described in this paper:  1) Improving the precision of output prediction by using  uniform information density topology over one with a  proportional distribution of fuzzy sets in feature space.

2) Using a novel supervised error reduction algorithm  to fine-tune the network and discover the varying  influence of an input dimension across output space.

3) Improvements in accuracy over the state-of-the-art for  three Nakanishi?s nonlinear estimation tasks, as well as  a a real-world road traffic density prediction problem.



II. THE FASCOM ARCHITECTURE  The FASCOM architecture (Fig. 1) has six layers, with  each layer performing a specific fuzzy operation. The inputs  and outputs are vectors X= [x1, . . . , xi, . . . , xI ] T and Y =  [y1, . . . , ys, . . . , yS ] T , where I and S denote the number of  input and output linguistic variables respectively. In layer 1,  input linguistic node IFi, represents the ith input linguistic  variable of input xi. In layer 2, input label node ILi,j ,  represents the jth linguistic label of the ith input linguistic  variable. ILi,j nodes corresponding to the same input xi are  grouped in input map IMi. Layer 3 consists of antecedent  nodes Ak,l grouped into input conjuncted maps ICMk. Ak,l represents either a single antecedent that is mapped from an  IM, or multiple antecedents which are a conjunction of two  or more ILi,j nodes from different IM. In layer 4, conse-  quence node Cq,p represents either a single consequence or  multiple consequences, and nodes are organized into output  conjuncted maps. Layer 5 consists of output maps OMs,  each containing output label nodes OLs,r, with each node  symbolizing the rth linguistic label of the sth output variable.

In layer 6, output linguistic nodes ODs denotes the sth  output linguistic variable of output ys.

Fig. 1. Six-layered structure of the Fuzzy Associative Conjuncted Maps (FASCOM) architecture.

1516 2008 International Joint Conference on Neural Networks (IJCNN 2008)    In the hybrid associative memory, every ICMk is con-  nected to every OCMq via a (ICMk,OCMq) association, each representing a fully connected two-layered heteroasso-  ciative network. In a (ICMk,OCMq) network, when Ak,l is linked with Cq,p, a fuzzy if-then rule is formed between  them (i.e. if Ak,l then Cq,p). Memory between two nodes  Ak,l and Cq,p is stored as synaptic weight wk,l?q,p, while  modulatory weight mk?q,p signifies the connection strength  between ICMk and Cq,p. As a result, a rule is weighted by  resultant weight vk,l?q,p, which is computed as  vk,l?q,p = mk?q,p ? wk,l?q,p. (2) As a convention, the output nodes and maps are denoted  using Z with the subscripts specifying its origin. For exam-  ple, ZIFi represents the output of node IFi and ZIMi denotes  the output vector of nodes in IMi. All outputs from a layer  are propagated to the corresponding inputs at the next layer  with unity link-weight ? = 1.0 where a connection exists, except for the connections between layers 3 and 4 where they  are weighted by vk,l?q,p.

A. Layer 1: Input Fuzzifier  A IFi node in layer 1 fuzzifies the input into a fuzzy  membership function is given by  ZIFi = ?i(xi) (3)  where ?i(?) ? [0, 1] is the membership function of IFi. If a fuzzy input is presented to a node in this layer, the node  simply redirects it as the output of the node (i.e. ZIFi = xi).

Two fuzzifying functions were identified: Gaussian G(?), and Laplacian of Gaussian LoG(?). The LoG(?) function produces similar effects as lateral inhibition in the biolog-  ical neural circuitry [15], which is used to increase signal  discrimination by improving feature contrast.

B. Layer 2: Single Antecedent  In layer 2, a ILi,j node computes the fuzzy subsethood  measure between ?i,j(?) and ZIFi . Adopting the minimum T-norm operator for the intersection operation, ZILi,j can be  approximated as (4), where Z+ IFi  ? [0, 1] and Z? IFi  ? [?1, 0] are positive and negative components of ZIFi respectively,  and ?i,j(?) is the membership function of input label ILi,j .

C. Layer 3: Multiple Antecedent  In layer 3, output ZAk,l ? [?1, 1] is known as the firing strength, resulting from the conjunction of ILi,j nodes. Using  the algebraic product T-norm operator, ZAk,l is given by  ZAk,l = I?  i=1  Ji? j=1  ?ILi,j?Ak,l=1  ZILi,j (5)  where ?ILi,j?Ak,l is a link weight of 1 for a connection  between ILi,j and Ak,l, and is otherwise 0.

D. Layer 4: Multiple Consequence  For layer 4, output ZCq,p ? [?1, 1] of Cq,p is its activation level ?Cq,p , which is obtained through the recalling process  to be explained in Section IV, and given by  ZCq,p = ?Cq,p . (6)  E. Layer 5: Single Consequence  In layer 5, ZOLs,r ? [?1, 1] is the maximum activation level of connected Cq,p nodes, defined as  ZOLs,r = max x?q,y?p  (?Cx,y?OLs,r ? ZCx,y ) (7)  where ?Cq,p?OLs,r is a link weight of 1 if Cq,p is connected  to OLs,r, and is 0 otherwise.

F. Layer 6: Output Defuzzifier  Finally, layer 6 defuzzifies the fuzzy outputs from layer 5  to produce crisp outputs. The center of area defuzzification  scheme sCOA is applied for every ODs as follows  ZODs = sCOA  ( Rs? r=1  ZOLs,r  ) (8)  where ?Rs  r=1 ZOLs,r is the aggregated output membership  function.



III. PHASES IN THE LEARNING PROCESS  The learning process (Fig. 2) consists of three consecutive  phases: 1) unsupervised membership function initialization,  2) supervised Hebbian learning [16], and 3) supervised error  reduction. During learning, the output layers 4 to 6 are  functionally similar to input layers 1 to 3, which are fuzzifi-  cation for layer 6 (Section II-A), subsethood calculation for  layer 5 (Section II-B), and conjunction formation for layer  4 (Section II-C). For clarity, an alternate symbol ?? Z is used  to represent an output obtained via this reverse propagation  process.

Fig. 2. Phases in the learning process (highlighted in gray).

ZILi,j = max x?i  (min(Z+ IFi  (x), ?i,j(x))) ? max x?i  (min(|Z? IFi  (x)|, ?i,j(x)) max x?i  (?i,j(x)) (4)  2008 International Joint Conference on Neural Networks (IJCNN 2008) 1517    A. Unsupervised Membership Function Initialization  A linguistic label for a particular linguistic variable, is  represented by a neuron in a feature map. In FASCOM,  feature maps may be scalar or cyclic [17] while member-  ship functions of neurons may be discrete, rectangular or  Gaussian. The first two result in crisp or classical sets while  the third results in fuzzy sets. The number of neurons in a  feature map is fixed according to the number of labels of a  linguistic variable.

The initialization of membership functions involves a  three-stages. First, the centroids of membership functions of  these neurons are then placed evenly across the feature space  represented by the feature map. Next, a membership function  (i.e. discrete, rectangular, or Gaussian) is applied for each  neuron. Finally, uniform information density equalization is  performed.

The third stage of equalization based on information  density results in a disproportion in neuron allocation. This is  inspired by biological sensory maps, whereby more neurons  are allocated to areas with higher usage or sensitivity require-  ments [18]?[22]. In this case, FASCOM allocates neurons to  optimize information density [23] of the training data across  feature space (Fig. 3d). This can be achieved by first forming  a histogram of data points and smoothing it using a Gaussian  smoothing function (Fig. 3a). This is followed by equalizing  the histogram (Fig. 3b) and mapping the equalized scale to  the feature map (Fig. 3c). With more neurons allocated to  the areas with higher information concentration, the output  will be more responsive and precise.

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1      i  N  Data histogram Smoothed histogram  (a) Step 1 & 2: Insert and smooth data  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1       i  N  Equalized histogram  (b) Step 3: Histogram equalization  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  0.2  0.4  0.6  0.8   i  ?(i)  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  0.2  0.4  0.6  0.8   i  ?(i)  (c) Step 4: Mapping of equalized scale to feature map  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  0.2  0.4  0.6  0.8   i  ?(i)  (d) Final neuron allocation  Fig. 3. Uniform information density allocation of neurons.

B. Supervised Hebbian Learning  The second stage of the learning process is a hybrid of  Hebbian learning [16], whereby the simultaneous stimulation  of two connected neurons results in an increase of synaptic  efficacy between them. Assuming neurons i and j are re-  spective Ak,l and Cq,p nodes in ICMk and OCMq, then  the change in synaptic weight ?wi?j between i and j can  be computed based on their firing strengths Zi and ?? Z j as  follows:  ?wi?j =  { 0 if (Zi ? 0 and ??Z j ? 0), |?wi?j |max ? Zi??Z j otherwise.

(9)  where |?wi?j |max is the maximum possible absolute change in synaptic weight. ?wi?j is aggregated with the current wi?j to form the new synaptic weight, as in  wi?j = wi?j + ?wi?j . (10)  |?wi?j |max is determined by the mechanisms of synaptic plasticity, forgetting and long-term facilitation [15] result in  the following concepts:  1) The effects of both synaptic plasticity and forgetting  are approximated as constants through time. Through  mutual cancellation, the approximated combined effect  is a DC component.

2) The effect of long-term facilitation is initially high, but  decays as the memory strengthened. When the memory  is strong, the effect of facilitation is insignificant.

3) The initial effect of long-term facilitation is very much  greater than that of the resultant effect of synaptic  plasticity and forgetting.

A decaying sigmoid function is used to describe the effects  of these three concepts, defined by  |?wi?j |max = ?kLTF 1 + exp(?a[|wi?j | ? c]) + kLTF + kDC  (11)  where kLTF is the initial effect of long-term facilitation, kDC is the resultant DC component from synaptic plasticity and  forgetting, while a and c define the slope of the function.

The process of modifying synaptic weights is performed  for all training samples, and may result in memories being  stored to be strong like long-term memory or weak like short-  term memory.

C. Supervised Error Reduction  After Hebbian learning, the supervised error reduction  phase is performed to fine-tune the network. The varying in-  fluences of an input dimension across output feature space is  discovered stored as modulatory weights mk?q,p ? ?nonneg .

Table I lists the symbols used to explain the algorithm  shown in Algorithm 1. Modulatory weights are first ini-  tialized to 1.0 (line 1). Temporary modulatory weights are then assigned for every training input-output pair n (line 4).

Next, memory is recalled using the process described in  Section IV (line 5) and an initial error is obtained for each  output node (line 6). Next, using only one input conjuncted  map ICMk in each iteration of k, memory is again recalled  (line 8) and a new error for each output node is computed  (line 9). The difference between the new and initial errors  is computed (line 10) and based on this, the temporary  modulatory weights for the training instance n are modified  (line 11). After processing all ICMs for all training samples,  the modulatory weights are updated (line 14) and the entire  1518 2008 International Joint Conference on Neural Networks (IJCNN 2008)    process is repeated until a terminating condition is met.

The algorithm terminates when: 1) the maximum number of  epoch is reached, or 2) the total change in error falls below  a low threshold (i.e.

?Q  q=1  ?Pq p=1 ?ECq,p ? 0).

TABLE I  LIST OF SYMBOLS USED IN SUPERVISED ERROR REDUCTION PHASE  Symbol Description  mk?q,p : Modulatory weight between ICMk and Cq,p  u (n) k?q,p  : Temporary value of mk?q,p for the nth training input-output pair  Umod : Factor that updates temporary  modulatory weights u (n) k?q,p  N : No. of training input-output pairs  ZL3 = [Z (1) L3 , . . . , Z  (N) L3 ] : Training input vector  ?? Z L4 = [  ?? Z  (1) L4 , . . . ,  ?? Z  (N) L4 ] : Training output vector  Z (n) L3 = [Z  (n) ICM1  , . . . , Z (n) ICMK  ] : nth training input vector ?? Z  (n) L4 = [  ?? Z  (n) OCM1  , . . . , ?? Z  (n) OCMQ  ] : nth training output vector  ?L4 = [?OCM1 , . . . , ?OCMQ ] : Propagated output of layer 4  ?OCMq = [?Cq,1 , . . . , ?Cq,Pq ] : Propagated output of OCMq  ?Cq,p : Activation level of Cq,p  E initCq,p : Initial squared-error for Cq,p  EnewCq,p : New squared-error for Cq,p  ?ECq,p : Squared-error change for Cq,p  begin SUPERVISED ERROR REDUCTION ALGORITHM  1: initialize mk?q,p = 1 ?k ? K, ?q ? Q,?p ? Pq 2: while not stable do  3: for all training vectors n ? {1...N} do 4: u  (n) k?q,p = mk?q,p ?k ? K, ?q ? Q,?p ? Pq  5: with all ICM, compute ?  6: E initCq,p = (?Cq,p ? ?? Z  (n) Cq,p  )2 ?q ? Q,?p ? Pq 7: for all ICMk ?k ? K do 8: with only ICMk, compute ?  9: EnewCq,p = (?Cq,p ? ?? Z  (n) Cq,p  )2 ?q ? Q,?p ? Pq 10: ?ECq,p = EnewCq,p ? E initCq,p ?q ? Q,?p ? Pq 11: u  (n) k?q,p = Umod ? u(n)k?q,p ?q ? Q, ?p ? Pq  12: end for k ? {1...K} 13: end for n ? {1...N} 14: mk?q,p =  N  N? n=1  (u (n) k?q,p)?k ? K, ?q ? Q, ?p ? Pq  15: end while  end SUPERVISED ERROR REDUCTION ALGORITHM  Algorithm 1: Supervised error reduction algorithm.

Umod as defined in (12), determines the magnitude of  change in u (n) k?q,p. If the error decreases when using only  ICMk, it means that ICMk is important for output predic-  tion and u (n) k?q,p is strengthened. When the opposite occurs,  ICMk is insignificant and u (n) k?q,p should be reduced. The  magnitude of change in u (n) k?q,p also depends on the number  of IM preceding ICMk, and the current epoch.

Umod =  1 + exp(?d?ep ? ?ECq,p) (12)  where ?(? > 1) is a parameter for the initial learning rate,  d =  { 1 + ? if ? ? 0,  1+|?| otherwise  , ? = I+12 ? I?  i=1  (?IMi?ICMk)  and ep = (1 ? ? ?max  )2, where ? and ?max are current and maximum epochs.



IV. RECALLING PROCESS  In the recalling process (Fig. 4), based on the input  presented to layer 1 of the encoded network, input functions  of layers 1 to 3 are performed (Sections II-A, II-B and II-  C). The memory recall process then occurs in the hybrid  associative memory. Finally, the output functions of layers  4 to 6 are performed (Sections II-D, II-E and II-F) and the  estimated output is recalled. Here, we explain the signaling  processes of neurons within the hybrid associative memory.

Fig. 4. The recalling process (highlighted in gray).

In the hybrid associative memory, i is a signal transmitting  Ak,l node from layer 3, and j is a Cq,p node from layer 4  that receives inputs from N neurons in layer 3. Based on  firing strength Zi, i transmits a signal defined by  ai =  { 0 for d?min ? Zi ? d+min, kprop ? Zi otherwise.

(13)  where ai is the output of i, kprop ? [0, 1] is a propagation factor to improve network stability, and d+min ? [0, 1] and d?min ? [?1, 0] are predefined positive and negative thresh- olds. Nodes j in layer 4 then receive these output signals  and update their activation levels ?j , as in  ?j =  N? i=1  vi?j ? ai (14)  where vi?j is the resultant weight between i and j.



V. DATA PREDICTION EXPERIMENTS  Four data prediction tasks were used to benchmark our  architecture against other existing architectures. They include  three separate sets of data from Nakanishi?s nonlinear esti-  mation tasks [24] and real-world dataset for predicting the  density of highway traffic [25].

2008 International Joint Conference on Neural Networks (IJCNN 2008) 1519    TABLE II  BENCHMARKING RESULTS ON NONLINEAR ESTIMATION FOR THE NAKANISHI ESTIMATION TASKS  Architecture Example A Example B Example C  MSE R MSE R MSE R  FASCOM 0.181 0.929 8.170 ? 103 0.999 13.930 0.953 Hebb-RR 0.185 0.911 2.423 ? 104 0.998 15.138 0.947 SVM 0.258 0.876 2.423 ? 105 0.993 29.510 0.925 RSPOP 0.383 0.856 2.124 ? 105 0.983 24.859 0.922 DENFIS 0.411 0.805 5.240 ? 104 0.995 69.824 0.810 POP-CRI 0.270 0.877 5.630 ? 105 0.946 76.221 0.733 ANFIS 0.286 0.853 2.969 ? 106 0.780 38.062 0.875 EFuNN 0.566 0.720 7.247 ? 105 0.946 72.541 0.756  A. Nakanishi?s Nonlinear Estimation Tasks  The Nakanishi?s dataset [24] consists of three examples  of real-world nonlinear estimation tasks. The tasks for data  prediction are namely: 1) a nonlinear system, 2) the human  operation of a chemical plant, and 3) the daily stock price  of a stock in a stock market.

Based on these three experiments, FASCOM?s perfor-  mance based on data prediction accuracy was benchmarked  against Hebb-RR [27], SVM [26], RSPOP [8], DENFIS [28],  POP-CRI [29], ANFIS [30], [31] and EFuNN [32]. Two  performance measures are used for this evaluation, namely  the mean squared error (MSE) and the Pearson product-  moment correlation coefficient (R).

1) Example A: Nonlinear System: FASCOM was used to  model a nonlinear system given by  y = (1 + x?21 + x 1.5 2 )  2 : (1 ? x1, x2 ? 5) (15) The dataset consists of four input variables (x1,x2,x3,x4)  and one output variable (y), whereby only x1, x2 are useful  and x3, x4 are irrelevant.

The mean of modulatory weights discovered was com-  puted across the output feature space. It was found that  the input conjuncted maps ICMs representing x3, x4 and  x3?x4 exerted no influence on the outcome of y and could be discarded. This is similar to the results obtained by [8], [24],  [27]. For this example task, FASCOM was most accurate  when compared to other benchmarked architectures.

2) Example B: Chemical Plant Operation: This example  involves the human operation of a chemical plant, whereby  five inputs representing monomer concentration (x1), charge  of monomer concentration (x2), monomer flow rate (x3),  and local temperatures inside the plant (x4, x5), are used  to estimate the set point for monomer flow rate (y).

Similar to [24], FASCOM discarded x2, x4 and x5. As  a comparison, [8] discarded x1, x2 and x5, while [27]  discarded all but x3. Also, we deduce that the set point for  monomer flow rate (y) depends mainly on the combination  of monomer flow rate (x3) and monomer concentration (x1)  (i.e. x1 ? x3). For this example, with a MSE of 8.170? 103 and near perfect R value, FASCOM?s performance was  significantly better than the state-of-the-art.

3) Example C: Stock Price Forecasting: The prediction of  the price of a stock y for this example is performed with the  ten inputs, which are the past and present moving averages  over a middle period (x1,x2), past and present separation  ratios with respect to moving average over both short and  middle periods (x3,x4,x8,x10), present change of moving  average over both short and long periods (x5,x9) and past  and present price changes (x6,x7).

The supervised error reduction algorithm significantly re-  duced inputs corresponding to x1, x2, x3, x5, x6, x8 and  x9. [24] discarded inputs x1, x2, x3, x6, x7, x9 and x10, [8]  discarded x1, x2, x3, x6 and x10, and [27] discarded only x2 and x5. Interestingly, all methods did not discard x4, which  means the present separation ratio with respect to moving  average over a middle period (x4) is a critical component to  predict stock prices. Similar to the previous two examples,  FASCOM outperformed all other benchmarked architectures.

4) Discussion: The three tasks were also used to analyze  the effects of three different experimental initializations of  the learning process:  1) using all three phases in the learning process  2) omitting membership function initialization (phase 1)  3) omitting error reduction (phase 3)  For each task, each initialization?s MSE was computed  and normalized with respect to the highest MSE amongst  the three initializations. For all three tasks, the comparison  between the three initializations indicates that the inclusion  of both phases 1 and 3 produced a MSE that is significantly  lower than when either one was omitted (Fig. 5). This shows  that both phases 1 and 3 are crucial in improving the accuracy  of data prediction and their existence within the learning  process is justified.

Task A Task B Task C  0.2  0.4  0.6  0.8   N or  m al  iz ed  M S  E  With phase 1, 2 & 3 Omit phase 1 Omit phase 3  Fig. 5. Comparison between the three different experimental initializations.

1520 2008 International Joint Conference on Neural Networks (IJCNN 2008)    B. Highway Traffic Density Prediction  The raw traffic data [25], [33] was collected for three  straight lanes and two exit lanes, at site 29 located at exit 5  along the east bound direction of the Pan Island Expressway  (PIE) in Singapore, using loop detectors embedded beneath  the road surface (Fig. 6). Data spanning a period of six days  from September 5 to 10, 1996, for the three straight lanes  (i.e. lanes 1 to 3) were considered for this experiment. The  dataset has four input attributes representing the normalized  time and the traffic density of each of the three lanes.

Fig. 6. Photograph of site 29 where the traffic data was collected.

The purpose of this experiment is to model the traffic  flow trend and subsequently produce predictions of the traffic  density of each lane at time t+? , where ? = 5, 15, 30, 45, 60 min is the prediction time interval. Three cross-validation  groups (CV1, CV2 and CV3) of training and test sets  were used for evaluation purposes [34]. The mean squared  error (MSE) and the Pearson product-moment correlation  coefficient (R) were computed for each predictions run. From  the example in Fig. 7, we observe that the prediction accuracy  decreases as the time interval ? increases.

To evaluate the accuracy of prediction, the ?Avg MSE?  indicator was computed by taking the average MSE across  all 45 prediction runs (3 lanes, 5 time intervals and 3 cross-  validation groups). Also, the ?Var? indicator reflecting the  consistency of predictions over different time intervals across  the three lanes was computed by taking the change in the  mean of R from ? = 5min to ? = 60min expressed as a percentage of the former. This was then averaged across all  three lanes to produce ?Avg Var?.

The results of traffic density prediction was compared  to Hebb-RR [27], SVM [26], RSPOP [8], POP-CRI [29],  DENFIS [28], GeSoFNN [34] and EFuNN [32]. FASCOM  significantly outperformed all other architectures based on  the results as shown in Fig. 8, with the best combination of  ?Avg MSE? (0.098) and ?Avg Var? (19.0%) as compared to other architectures. The results indicates that the output pre-  diction by FASCOM are both highly accurate and consistent  over different time intervals. This is desirable.

0 100 200 300 400 500 600 700 800       Time instance  N or  m al  iz ed  d en  si ty Actual  Predicted R=0.904   MSE=0.082  (a) Prediction at ? = 5 min  0 100 200 300 400 500 600 700 800       Time instance  N or  m al  iz ed  d en  si ty Actual  Predicted R=0.886   MSE=0.099  (b) Prediction at ? = 15 min  0 100 200 300 400 500 600 700 800       Time instance  N or  m al  iz ed  d en  si ty Actual  Predicted R=0.873   MSE=0.108  (c) Prediction at ? = 30 min  0 100 200 300 400 500 600 700 800       Time instance  N or  m al  iz ed  d en  si ty Actual  Predicted R=0.839   MSE=0.135  (d) Prediction at ? = 45 min  0 100 200 300 400 500 600 700 800       Time instance  N or  m al  iz ed  d en  si ty Actual  Predicted R=0.816   MSE=0.152  (e) Prediction at ? = 60 min  Fig. 7. Traffic density prediction of lane 1 using CV1.

0.08 0.1 0.12 0.14 0.16 0.18 0.2 0.22       Avg MSE  A v g  V a r  (% )  (0.143, 19.1) RSPOP  POP-CRI  (0 .189, 33.2) EFuNN  FASCOM (0 .098, 19.0)  Hebb-RR (0.114, 29.6)  (0 .150, 33.2) DENFIS  SVM (0.120, 39.6)  GenSoFNN (0.164, 17.4)  (0.167, 21.9)  Fig. 8. FASCOM outperforms other benchmarked architectures for highway traffic density prediction.

2008 International Joint Conference on Neural Networks (IJCNN 2008) 1521

VI. CONCLUSIONS  This paper proposed the Fuzzy Associative Conjuncted  Maps (FASCOM) fuzzy neural network, which represents  information using conjuncted fuzzy sets and learns associa-  tions between them through three consecutive unsupervised  and supervised phases in the learning process. During the first  phase of unsupervised membership function initialization,  neurons are allocated to optimize information density. For the  supervised learning phase, the single pass Hebbian learning  performed is based on the memory mechanisms of synaptic  plasticity, forgetting and long-term facilitation. Finally, the  final supervised phase involves fine-tuning the network using  the supervised error reduction algorithm, which uses an error  comparison to determine the influence of an input dimension  across output space.

In the series of experiments performed, we showed that  each of the phases contributed to the overall performance  of the architecture. We were also able to demonstrate FAS-  COM?s effectiveness in performing nonlinear data prediction  on a variety of real-world problems of different natures and  consisting of noisy data, such as nonlinear system modeling,  chemical plant operation, stock price forecasting and traffic  density prediction. For every experiment, FASCOM pro-  duced the best data prediction accuracy when benchmarked  against existing architectures.

