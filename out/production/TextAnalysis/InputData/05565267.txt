

AbstractThis paper presents a new associative classification  algorithm based on BitTable, i.e., associative classification  algorithm based on BitTable (BitTableAC). BitTableAC  employs BitTable to mine association rules efficiently, and fuzzy  c-means (FCM) to partition quantitative attributes. It also  adopts a new jointing and pruning technique to generate useful  candidate itemsets directly. The experiments on datasets from  UCI Machine Learning Repository demonstrate that the  proposed algorithm performs well in comparison with other  classification algorithms.



I. INTRODUCTION  N recent years, the mining of association rules has  attracted tremendous amount of attention. Since  association rules can discover useful rules in data sets and  provide a complete picture of the domain. We can assign a  more accurate class probability estimation to each new data  case by making use of association rules to build a classifier.

Building a classification with association rules is called  associative classification (AC). A typical AC system is  constructed in two steps: 1) discovering all association rules  according to some tests and 2) generating classification rules  from the association rules and selecting a small set of rules to  construct a classifier.

In step 1, finding frequent itemsets from large databases,  by using support confidence based framework is not a trivial  task [1]. The search space is exponential in the number of  database attributes and with millions of database records the  problem of I/O minimizations become paramount [2]. Due to  its important and difficult, finding frequent itemsets attracts  huge attention in numerous research communities, a lot of  algorithms have been proposed in the different mining  literature algorithm [3, 4]. Agrawal [5] proposed Apriori  algorithm which traverses the search space in a pure  breadth-first manner and finds support information by  explicitly generating and counting each node. FP-growth  algorithm [6] uses prefix-tree structure to mine frequent  itemsets without generating candidates and scans the database  only twice. Many of them are based on Apriori algorithm.

Jong Soo Park [7] proposes DHP algorithm using direct  hashing and pruning. John D. Holt  [8] proposes IHP  algorithm using inverted hashing and pruning. And Yingjiu  Li [9] extends the Apriori algorithm with effective horizontal  pruning technique. These algorithms mainly focus on pruning    Jie Dong is with Faculty of Electronic Information and Electrical  Engineering, Dalian University of Technology, Dalian, 116024, P. R. China  (corresponding author e-mail: dongjie@dlut.edu.cn).

Jie Lian is with Faculty of Electronic Information and Electrical  Engineering, Dalian University of Technology, Dalian, 116024, P. R. China  (e-mail: jielian@dlut.edu.cn).

the candidate itemsets to reduce the candidate itemsets  amount.

Pruning technique has greatly improved the performance  of finding frequent itemsets, but the candidate itemsets  generation and support count process of these Apriori-like  algorithms is still the most time-wasted process. Yuh-Jiuan  Tsay [10] proposes CBAR algorithm that uses cluster table to  load the databases into main memory. The algorithms  support count is performed on the cluster table and dont need  to scan all the transactions stored in the cluster table, so the  time of support count is saved.

In step 2, Associative classification is firstly proposed in  CBA [11]. CBA operates in three main steps: First, it  discretizes quantitative attributes, and then it adopts Apriori  algorithm to discovery all association rules by making  multiple passes over the training data set. Finally, it sorts the  rules by descending confidence and prunes them to get a  minimal number of rules that are necessary to cover training  data and achieve satisfying accuracy. For discovering  classification rules more efficiently, CMAR uses FP-growth  approach to find frequent itemsets [12]. It stores and retrieves  mined association rules in a prefix tree data structure, known  as a CR-tree and prunes rules based on confidence,  correlation and database coverage. Different from other  associative methods, CMAR determines the class label using  a set of rules to avoid classification bias. The measurement of  the quality of association rules in associative classification  has been studied by Janssens et. al [13]. The authors couple  CBA with intensity of implication, a measure to calculate the    distance to random choices of small, even non statistically  significant subsets. By using the new measure, the adapted  CBA algorithm can generate much better results than the  other classifiers like C4.5, original CBA and CART at the 5%  level of significance.

The rest of paper is organized as follows. In Section 2, the  algorithm is described mainly in two parts: mining  association rules based on BitTable structure, classify dataset  with association rules. In Section 3, experiment results on  both synthetic and real databases are given, and Section 4 is  the conclusions.



II. ALGORITHM DESCRIPTIONS  BitTable is a set of integer whose every bit represents an  item [14, 15]. In BitTableFI, BitTable is used to compress the  candidate itemsets and the database. For candidate itemsets  compressing, if candidate itemset C contains item i, bit i of  the BitTables element is marked as one; otherwise, marked  as zero. For database compressing, the BitTable is used  vertically. If item i appears in transaction T, bit i of BitTables  BitTableAC: Associative Classification Algorithm Based on BitTable  Jie Dong, Jie Lian  I   August 13-15, 2010 - Dalian, China  978-1-4244-7050-1/10/$26.00 c2010 IEEE 529        T element is marked as one. If the size S of items or database  transactions is greater than CPUs word size W, an array  whose size is S/W+1 is used to store the compressed data.

Example 1. Consider D of Table. 1 and a candidate itemset  c= {2 3 5}, c can be compressed to 13 whose binary code is  01101. And database D of Table. 1 can be compressed to {10  7 14 8 7} for representation for {{1 0 1 0}, {0 1 1 1} {1 1 1 0}  {1 0 0 0} {0 1 1 1}} as shown in Table 2.

Table 1  DatabaseExample  Database D  TID Item  1 1, 3, 4,    2 2, 3, 5  3 1, 2, 3, 5  4 2, 5    Table 2  Example of Compressing Database Into BitTable  TID Item 1 Item 2 Item 3 Item 4 Item 5                          BitTable 10 7 14 8 7    Before quick candidate itemsets generation can be  performed, frequent 1-itemsets should be found first. All  non-frequent 1-itemsets is neglected to reduce the size of  BitTable and improve the performance of the generation  process, because all non-frequent 1-itemsets is unuseful for  further generation. Then, frequent 1-itemsets are rearranged  according to their lexicographic order and each item is  marked with the sequence number as item ID. After frequent  2-itemsets was generated, frequent itemsets BitTable is    constructed. Each element in BitTable represents a frequent  itemset as described in 3.1. According to Apriori property,  candidate itemsets Ck+1 should be constructed by join two  frequent itemsets of Lk that have common k-1 items. For each  element E1 in frequent itemsets BitTable BLk, a middle  variable MID is generated by replacing the last bit 1 of E1  with 0. Each element E2 after E1 performs a Bitwise And  operation denoted & with MID. If the result equals to MID,  the two elements E1 and E2 in Lk have the common k-1 items.

Then, a candidate itemset is generated by performing Bitwise  Or operation denoted | on E1 and E2 and added to candidate  itemsets BitTable BCk+1. Compressing frequent itemsets into  BitTable saves a lot of memory, and Bitwise And/Or  operation is greatly faster than comparing each item in two  frequent itemsets in Apriori.

Example 2. Consider D of Table. 1 and suppose MinSup=2.

After the first scan of the whole database, L1 = {1 2 3 5} is  generated. Item 4 is not frequent 1-itemset, so frequent  1-itemsets are rearranged and itemset {1 2 3 4} is used in the  mining process instead of L1 = {1 2 3 5} to reduce the size of  BitTable. When BitTableFI finished, frequent itemsets are  reverted. From L1 and C1, L2 = {{1 3} {2 3} {2 4} {3 4}} can  be generated. There are totally 4 items in frequent 1-itemsets,  so the binary code length of frequent itemsets BitTables  element is 4. Frequent itemsets BitTables binary codes are  {{1 0 1 0}{0 1 1 0}{0 1 0 1}{0 0 1 1}}, so its corresponding  BitTable BL2 is {10 6 5 3}. Itemset {1 3}MID is 8 whose  binary code is {1 0 0 0} by replacing the last 1 of {1 0 1 0}  with 0. Performing Bitwise And operation on {6 5 3} which  represents {2 3} {2 4} {3 4} respectively, only 0 is generated,  so {1 3} {2 3}, {1 3} {2 4}, {1 3} {3 4} can not be jointed to  generate candidate itemsets. Itemset {2 3}s MID is 4. The  result of performing Bitwise And operation on 5 which  represents {2 4} equals the MID, so {2 3} {2 4} can be jointed.

After performing Bitwise Or operation, 7 which represents {2  3 4} is generated and added to the candidate itemsets. The  progress is shown in Fig. 1.

L2  Itemset Support  {1 3} 2  {2 3} 2  {2 5} 3    {3 5} 2      L2[i],L2[j] BitTable BL2  {1 3}  {2 3} 10, 6  {1 3}  {2 4} 10, 5  {1 3}  {3 4} 10, 3  {2 3}  {2 4} 6, 5  {2 3}  {3 4} 6, 3  {2 4}  {3 4} 5, 3      gen_ht(L2[i]) MID & L2[j] BitTable BC3  8 0  8 0  8 0  4 4 7  {2,3,4}  4 0  4 0    Fig. 1 Example of quick candidate itemsets generation    An initialization of database BitTable should be  accomplished first before quick candidate itemsets support  count algorithm can be performed. The whole database is  compressed into a database BitTable so that it can even be  stored in main memory. The database BitTables  compressing proportion is about (F1 Items/(CPU word size*  All Items)). For example, if the database contains 1000 items,  32k transactions and 200 frequent 1-itemsets, for 32bit CPU,  the compressed database BitTables size is 1/160 of origin  database. The initialization progress not only compresses the  database, but also is the foundation of the quick candidate  itemsets support count. The initialization progress of database  BitTable acts as follows: For each item in frequent 1-itemsets,  the binary values of the item in database transactions are  combined to an integer which is added to the database  BitTable later. If the size of the database transactions is larger  than the CPU word size, an integer array is used.

The quick candidate itemsets support count algorithm counts  the support of the candidate itemsets directly on the database  BitTable. For every item in each candidate itemset, the    element of the database BitTable corresponding to the item  can be directly found out, because they have the same order as  the frequent 1-itemsets. Performing Bitwise And operation on         the corresponding elements of the items in each candidate  itemset, the count of the bit 1 of the result is the support of the  candidate itemset. If the support is greater than the MinSup,  the candidate itemset becomes a frequent itemset.

Example 3. In database D, L1 = {1 2 3 5}. For item 1, its  corresponding database value is {1 0 1 0}, so its database  BitTable value is 10 whose binary value is 1010. After the  initialization of the database BitTable, the database BitTable  is compressed as {10 7 14 7}. When counting support for C3 =  {7} representing {2 3 4}, 7 & 14 & 7 equals 6 representing {0  1 1 0}, so the support of candidate itemsets {2 3 4} is 2, which  means {2 3 5} is a frequent itemset, as shown in Fig. 2.

item 2         &  item 3         &  item 5           support         Fig. 2 Example of candidate itemsets support count  Finally, the pseudo-code of BitTableFI is shown below.

First, BitTableFI generates frequent 1-itemsets and initializes  the database BitTable, and then uses quick candidate itemsets  generation and quick candidate itemsets support count  described above to generate all frequent itemsets until there is  no itemset in Lk.

BitTableFI(Database D,int MinSup)  {  L1 = { frequent 1-itemsets};  BitTable[] db = database_bittable_init(D,L1);  L2=getl2froml1(L1);  int k=2;  while (Lk.NotEmpty())  {  k++;  Ck = quick_candidateitemsets_generation(Lk-1);  Lk = quick_candidateitemsets_support_count(Ck, db, MinSup);  }  }    In BitTableAC, we use all the fuzzy classification rules  together to classify the test data since the decision made by  multiple rules is likely to be more reliable. Different rules  have different consequences, many of which are conflict. The  problem is how to decide which class label should be  assigned to the test data. We can simply label the test data  case using the highest confidence rule that has the highest  firing strength, as confidence can be viewed as a probability  estimate indicating the likelihood that the test data belong to  rule and the firing strength is the degree of the data matching  to the rule. Given a test data I, consider the two rules in Table  1.

Rule C and D have the highest firing strength and Rule D  has higher confidence than Rule C, so we classify I to y4 with  rule D. However, though rule A and B have lower firing  strength, their confidence and support are much higher than  those of rule D. It seems more reasonable to classify I to y1 or  y2 with rule A or B. To score each fuzzy classification rules  more reasonably, we introduce a new parameter to score the  weight of a fuzzy classification rules R to the test data I:   , , ( )I R I R R RF S C?< > < >=  According to the new parameter, Rule B has the highest  score, ?<I, B>=0.3136. So the test data I is labeled with y2.

We classify the test data with all the fuzzy classification  rules in BitTableAC. So after scoring each fuzzy  classification rules with parameter ?, we calculate the weight  of each class to the test data:  ,  , where  and ( )  i  I r  r  y ir R cons r yN  ?

?

< >  = ? =  ?

(1)  where R is the set of all fuzzy classification rules, cons(r) is  the consequence of the rule r, and N is the number of r in R  which satisfies cons(r) = yi. We classify the data to the class  with the max value of  iy  ? .

Finally, the pseudo-code of BitTableAC is shown as  follows:  Input: training data set Tr  test data set Te  support threshold MinSup  confidence threshold MinConf  the num of fuzzy partitions (NFP )  Output: class label set of test data C    Algorithm:  1 generate NFP fuzzy partitions of each quantitative  attributes with FCM  2   mine fuzzy classification rules R satisfying threshold  MinSup and MinConf from Tr  3 for each data d in Te  4 for each rule r in R  5 compute ? of d to r  6 end for  7 count the weight ? of each class to d  8 classify the test data to the class with the max ?

9 store the classification result in C  10 end for

III. EXPERIMENT RESULT  We have implemented BitTableAC in Matlab 7.1. The  experiments are performed on a 2.4GHz Pentium4 PC with  768MB memory, running Windows 2000. Datasets used in  experiments come from UCI Machine Learning Repository, a  commonly used benchmarking database. Their basic  information is listed in Table 3. We compare BitTableAC  with some other classification algorithms on classification  accuracy. The second part focuses on discussing the effect of  the varying parameters on the classification accuracy of  BitTableAC. All the experiments are repeated ten times, and  the results are the average. We mainly study the classification  accuracy and do not take the classification efficiency into  consider, so we just modify the Apriori algorithm to mine  association rules for its simplicity. In other words, the  efficiency of the BitTableAC can easily be improved by  adapting other efficient mining algorithms.

Table 3  Datasets used in experiments  Dataset Size Attributes Classes  1 Glass 214 9 7  2 Iris 150 4 3  3 Breast 699 10 2    4 Heart 270 13 2  5 Diabetes 768 8 2  6 Pima 768 8 2    For a classifier, classification accuracy is a basic  performance measurement, which is the ratio of the number  of cases truly predicted by the classifier over the total number  of cases in the whole test dataset, e.g.,  Number of cases truly predictedClassification accuracy= 100%  Total number of cases  (2).

We compared BitTableAC with some associative  classification algorithms on accuracy, such as CBA and  CMAR. We also include the C4.5 and LIBSVM results on the  same datasets as a reference. C4.5 is a well-known traditional  classifier based on decision tree induction technique, and  LIBSVM is an accurate support vector machine classifier.

For the fair of the comparison, we do not implement these  algorithms, but their classification accuracies are obtained  from their research literatures. For BitTableAC, the MinSup,  MinConf and NFP (num of fuzzy partitions) are set to 5%, 2%  and 3, respectively.

The comparison results are shown in Table 4. As shown in  this table, BitTableAC has the satisfactory classification  accuracy. It achieves the highest accuracy in four of six  datasets used in experiments and also outperforms other  algorithms on average.

Table 4  Experiment Results  Dataset C4.5 LIBSVM CBA CMAR BitTableAC  1 Glass 68.70 77.57 72.60 70.10 75.23  2 Iris 93.60 94.00 92.90 94.00 96.25  3 Breast 95.70 96.14 95.80 96.40 98.53  4 Heart 82.50 88.45 81.50 82.20 86.67  5 Diabetes 72.10 73.83 75.30 75.80 80.00  6 Pima 75.50 79.69 73.10 75.10 81.82  Average 81.35 84.95 81.87 82.27 86.42

IV. CONCLUSION  In this paper, an accurate associative classifier BitTableAC  is proposed. It employs BitTable to mine association rules    efficiently, and fuzzy c-means (FCM) to partition quantitative  attributes. To evaluate the performance of the proposed  algorithm, we compare BitTableAC with other well-known  classifiers on accuracy including previous associative  classifiers, C4.5 and LIBSVM on 6 test datasets from UCI  Machine Learning Repository. The results show that, in terms  of accuracy, BitTableAC outperforms others.

