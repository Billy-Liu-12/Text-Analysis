Research and Application of Improved Apriori Algorithm to Electronic  Commerce

Abstract?In order to analyze the shopping habits of consumers and more accurately mine the characteristics of consumption, we hereby have proposed and proved Theorem 1-3 to improve the classical Apriori algorithm, resulting in the reduction of database access. We improved the efficiency in the frequent-itemsets-based establishment of strong association rule. With this new design we fulfilled the timely recommendation of related products to customers, reflecting the principle of personalized service in e-shopping.

Keywords-electronic commerce; association rule; apriori algorithm;  frequent itemset

I.  INTRODUCTION As a new application of the Internet, e-commerce has  entered all areas of traditional business activities, influencing and changing various aspects of social life both directly and indirectly. There is no doubt that the emergence of electronic commerce promotes productive force, and at the same time brings a revolution in business, management and the ways people live[1]. Fig. 1 shows the basic model of e-commerce.

Figure 1.   E-commerce Model  Generally speaking, seasonal alteration would influence sales. Selling goods in selling seasons would be scarce, therefore sellers and producers need to figure out the real-time information about the purchasing of goods to ensure the delivery rate of the hot commodities. When the off-season arrives, the sales of certain commodities will decline, thus sellers and producers would be well advised to grasp consumers' interest by mining strong association rules in order to optimize the promotion of products and increase sales. In the system of modern market economy, with the collection and storage of large amounts of data, businessmen are increasingly interested in mining association rules from their databases. From a large number of records of daily transaction, people would find interesting relationships that can help to optimize and allocate resources rationally and assist decision-making of  business, such as cheap analysis, cross-shopping[2] and classified design.

The analysis of shopping basket is a typical example of association rule mining[3]. The general process is to analyze customers? purchasing habits via digging into the associations between different goods in customers? shopping baskets. By understanding consumption preference, retailers could develop efficient marketing strategies. For example, in a single experience shopping in a large supermarket, if we calculated the possibility of customers buying computers and office software (or what types of office software), the supermarket agents could optimize the layout of shelves and guide consumption on the basis of the parameter. If the parameter is greater than the minimum value fixed by dealers in advance, then computers and office software should be laid closely to each other, which may stimulate customers to purchase the two kinds of commodities at the same time.

Through the above analysis, we can see that the anticipation of purchase activities has practical application.

In the research, we obtained association rules through data mining, valuable information and storage. Then, according to the preference, we can speculate their future purchase activities[4]. However, traditional Apriori algorithm of association rule requires a full scan in databases to find each collection of frequent itemset ?Lk?. When utilizing frequent itemsets to generate strong association rules, the algorithm needs to judge each frequent itemsets, which increases the complexity and reduces efficiency. The proposed algorithm in the research is based on the traditional Apriori algorithm but fulfills reduction in the frequency of traversal in the databases and optimization in the methodology of Lk generation, and in strong association rule construction the new algorithm uses relative theorems to shrink the number of frequency itemsets. This requires analysis from a large number of historical data of sales to identify the degree of associations among a number of commodities, extracting association rules that are applied to recommendation systems[5] in visual form to meet customers? personalized demands[6].



II. ASSOCIATION RULE  A. Data Mining Simply speaking, data mining extracts useful and  potential rules hidden in a massive, incomplete, noisy, fuzzy and random database[7]. In general situations, data mining is regarded as a fundamental step in the process of discovery of database knowledge, interacting with users or  2012 11th International Symposium on Distributed Computing and Applications to Business, Engineering & Science  DOI 10.1109/DCABES.2012.51     knowledge bases by transmiting interesting models to the users or storing new knowledge in knowledge bases[3].

Based on the perspective, a typical data mining system has the following main components, shown in Fig. 2.

Figure 2.  Data Mining System  B. The Definition of Association Rule and the Processes of Mining Essentially, Association rule mining seeks valuable  associations between different attributes in data sets. It is an important branch of data mining. Let I={i1, i2, ?, im} be a collection of items, D be a collection of transactions in a database. Each transaction T is a collection of different items, therefore T ? I. Presumed A to be a set of items, a transaction T would contain A if and only if A ? T.

Association rule is an implication like the form ?A?B? where A ? I, B ? I and A?B=? . Established rules like ?A?B? have parameters like ?support degree? and ?confidence level?. Specifically, support(A?B)=P(A B); confidence(A?B)=P(B|A). Besieds, association rules meeting the minimum support degree(min_support) and the minimum confidence level (min_confidence)[8] are the strong association rules which will be used as knowledge to output.

The process of association rule mining is divided into two steps:  ? Generate all frequent itemsets: the frequency of itemsets counted from each transaction in databases should not be less than min_ support.

? Produce strong association rules by the frequent itemsets obtained: the rules must meet the min_support and the min_confidence.

For convenience, we introduce the following concepts: ?Itemset? represents the collection of items. ?K-  Itemset? signifies a collection that contains K items. For example, {Computer, Office_Sofeware} is a 2-Itemset.

?Support count? means the frequency of an itemset. It is equal to the number of transactions that contain the itemset. ?Frequent Itemset? shows the itemset that meets the minimum support degree, usually denoted by lk.

C. Improved Apriori Algorithm 1) Apriori Algorithm  The classical algorithm of association rule was firstly proposed by Agrawal et al., called Apriori. It is an iterative method of layer by layer search, applying K-Itemset to search for (K+1)-Itemset. Firstly, the algorithm identifies  the collection of Frequent 1-Itemset, denoted by L1.

Collection of Frequent 2-Itemset ?L2? is computed from L1. We use the same method to acquire Lk (K 2) until it cannot find Lk-1. Actually, the process contains two steps- connection and pruning. In order to improve the efficiency of obtaining frequent itemsets in each loop, Apriori algorithm introduces ?Apriori characteristic? which means that non-empty subset of a collection of frequent itemsets must be frequent. Using the Apriori characteristic, non- frequent itemsets in the set of candidate itemsets ?Ck? will be pruned. Fig. 3 shows the main steps of the Apriori algorithm.

Figure 3.  Main Steps of the Apriori Algorithm  2) Improvement of Apriori Algorithm Through the analysis of the traditional Apriori  algorithm, we know that it needs a full scan in databases to find each Lk (K 1), thus reducing its efficiency. In order to reduce the frequency of traversal of the databases, the research proposes an improved algorithm of Lk generation, which is realized via JSP dynamic web technology[10].

Based on the above viewpoints, the key point of optimization of Lk creation is to reduce the frequency of database access; therefore we put forward the following improvements.

In order to describe the remaining content suitably, we further introduce the following symbols: C[k] is a set of candidate itemsets in which each itemset?s length is k; L[k] is a collection of frequent itemsets in which each itemset?s length is k as well; T[k] is a subset of databases and it corresponds to C[k], also called reference database. To calculate each itemset?s support degree in C[k], T[k] replaces DB as a search domain; Function ?count(t)? represents the number of elements in a transaction in DB; Function Apriori_Gen(L[k]) is used to produce C[k+1] by L[k]; Function Subset(C[k+1], t) is used to get a subet based on transaction ?t?, and each itemset in the subet contains (k+1) items.

The pseudo code of the improved Apriori algorithm is given as follows:  (1) T[1] = D; (2) L[1] = {Find_Frequent_1_Itemsets(T[1])};     (3) for(k =1; L[k]! =?; k++) (4) begin (5) C[k+1] =Apriori_Gen(L[k]); (6) Foreach t T[k] (7)    begin (8) sum =count(t); (9) if(sum < k+1) T[k+1] = T[k] t; (10) end (11) Foreach t T[k+1] (12)    begin (13) Ct=Subset(C[k+1], t) (14) Foreach candidate c C[k+1] (15)        if(c Ct) then (16) c. support ++; (17) end (18) L[k+1] = {c C[k+1] | c.support >= min_  support} (19) end (20) L = kLk; (21) Return Generate_Rules(L); Multiple stages are involved in the improved  algorithm. The first stage is to scan the database and to copy it to T[1], and then construct the collection of Frequent 1-Itemset ?L1?, shown in step (1) and (2); the second stage is to generate C[k+1] by L[k], as in the step (5); thirdly, prune T[k] for originating T[k+1], illustrated in the step (6) - (10); forthly, using T[k+1] insteads the database to calculate the support degree of each candidate itemset in C[k+1], illuminated in step (11) - (18); fifthly, if L[k+1] is an empty set, exit the loop and combine all Lk by L = kLk, otherwise rewind to the step (3); the loop continues; sixthly, according to the L acquired in the fifth stage, we use function ?Generate_Rules(L)? to optimize the process of getting strong association rules, shown in step (21) (described in section 2.3.2.3 in detail).

The basic principle of the first improved part in the new algorithm is: If the number of elements contained in a transaction ?t? in DB is less than k+1, then t is invalid when counting support degree of each itemset in C[k+1].

The improved Apriori algorithm introduces a variation ?T[k]?, which makes the calculation do not need to re-visit the DB. When k=1, T[1]=D; When k 2, the method to produce T[k] is shown in the step (5)-(10) in the above algorithm. The following two theorems is to prove that T[k] is reasonable to instead DB.

Theorem 1: The subset of the collection of frequent itemsets ?L[k]? is frequent, including k itemsets of L[k-1] at least.

Proof: The function ?Apriori_Gen(L[k-1])? generating C[k] is defined as follows:  (1) For any two itemsets ?a, b? in L[k 1], the algorithm performs the computation of a ? b. The specific procedure is: a={i1, i2, ?, ik-2, ik-1}, b={j1, j2,?, jk-2, jk-1}, if i1=j1, i2=j2,?, ik-2 = jk-2, ik-1<jk-1, then C[k]=a ? b={i1, i2,?, ik-2, ik-1, jk-1}.

(2) Foreach c C[k] If  Subset(C[k 1], c)?L[k 1]  then  C[k] = C[k] c Due to the above definition and the fact that L[k] is  produced by C[k], the (k 1)-dimensional subset of C[k] is  involved in L[k 1] and therefore frequent. The subset has at least k( 1kkC  ? =k ) itemsets.

Theorem 2: Let t represent any transaction in a  database. Function count(t) signifies the number of elements in t, then t T[k], if and only if count(t)?k.

Proof: Firstly, from Theorem 1, it is known that k items constitute an itemset of L[k], so if count(t)?k, then it can be used to count the support degree of candidate itemsets, therefore t T[k]. Furthermore, we use reductio ad absurdum to prove if t T[k], then count(t)?k. Assume that ?count(t)<k? which means the number of elements in t is less than k, thus t does not contain any itemset in C[k].

When computing the ?support count? of c(c C[k]), t is invalid. In other words, computing the ?support count? do not need to scan t, therefore t?T[k] which contradicts the proposition, thus if t T[k], then count(t)?k. QED. (quod erat demonstrandum)  From theorem 1 and 2, we can use T[k] to count the ?support count? of c(c C[k]) and the the number of itemsets in T[k] is apparently less than that of in DB; therefore utilizing T[k] to substitute DB would reduce the frequency of DB access.

a) Calculation of Support Degree One of important factors to judge interest degree of a  rule is to measure its potential usefulness. we usually use ?support degree? to evaluate it. Support degree of an association rule is a percentage describing the qualified tuples? shares in all tuples. The expression using predicate logic is as follows: Let D be the object domain, A ? D, B ? D and A ? B=?. Unary predicate Support(X) signifies the support degree of X. Num(Y) denotes the number of tuples that contain Y. Then  Support(A?B) = Num(A ? B) ? Num(D)    (1) ?Num(A ? B)? is called ?support count? which usually  substitutes support degree to measure the usefulness of a rule.

b) Calculation of Confidence Level Each association rule like ?A?B? should have  parameters to evaluate its effectiveness, one of which is the degree of confidence. The expression using predicate logic is as follows: Let D be the object domain, A ? D, B ? D, and A ? B=?. Predicate Confidence(X) indicates the confidence degree of X. Num(Y) denotes the number of tuples that contain Y. Then  Confidence(A?B) = Num(A ? B) ? Num(A)          (2) c) Construction of Strong Association Rule  After preparing all frequent itemsets, they could generate strong association rules. The following equation is another way to calculate confidence degree:  Confidence(A?B) = P(B|A) = Support_Count(A ? B) ? Support_Count(A)                                                  (3) In (3), Support_Count(A ? B) is the number of  transactions that contain itemset ?A ? B?; Support_Count(A) is the number of transactions that contain itemset ?A?. According to (3), strong association rules are created as follows:  ? For each frequent itemset ?a?, the first step is to produce all non-empty subset of it.

? For any non-empty subset ?b? of ?a?, if suport_count(a)?support_count(b)?min_confidenc e, then the algorithm outputs a rule ?b?(a b)?     where min_confidence represents minimum confidence level.

In fact, the function ?Generate_Rules(L)? further optimize the process according to Theorem 3.

Theorem 3: Let L be the collection of frequent itemsets, l be an itemset of L. For any subset a, b l, if support_count(a) < support_count(b) and a l-a, then b l-b.

Proof: confidence(a l a) = supprot_count(l) ? supprot_count(a) confidence(b l b) = supprot_count(l) ? supprot_count(b)  supprot_count(a) < supprot_count(b) and a l-a confidence(b l-b) < confidence(a l-a)  and confidence(a l-a) < min_ confidence b l-b  Q.E.D  Theorem 3 shows that the second advantage of the improved Apriori algorithm is that we could further implement the operation of pruning to reduce the frequency of traversal of the frequent itemsets in L. In fact, we can pre-sort frequent itemsets in L according to their support degrees. If the confidence level of a frequent itemset is lower than the minimum degree of confidence, then other frequent itemsets whose support degree is higher than that of it will not produce strong association rules.



III. A CASE REPORT In order to apply the improved Apriori algorithm to  electronic commerce, we develop a shopping site that achieves shopping online recommendation system. The front module of the website is used for commodity display, sales and recommendation, while the back module of it is designed for management of commodities and mining association rules. When customers enter the shopping site, the system will predict their future purchasing activities according to the goods already in their shopping baskets and recommend related products in order to stimulate consumption and gain more profit.

The development tool of the website is Macromedia Dreamweaver 8; the database management tool is Microsoft SQL Server 2000 and the WEB Server is Tomcat 6.0.

The mechanism of goods recommendation is the core part of the website. Its logical process is described as follows:  ? In the admin interface, users input a minimum support count and a minimum confidence level according to the sales of all kinds of goods, shown in Fig. 4.

? Based on the minimum support count, the improved Apriori algorithm results in a collection of frequent itemsets ?L?.

? On the basis of minimum confidence level, we obtain a set of strong association rules which are stored in the database table ?tb_Association? shown in Table 1.

? According to the goods already purchased by customers, the system queries the table ?tb_Association? and recommends other relative goods to the customers, shown in Fig. 5.

Figure 4.  Sales List  The form of recommendation is shown in Fig. 5:   Figure 5.  Recommendation of Commodity  From Fig. 6, it could be learned that frequent itemsets that cannot generate strong association rules are pruned according to Theorem 3, which improves the efficiency of the algorithm.

Figure 6.  Comparision of Efficiency

IV. CONCLUSION In the research, data mining is applied to e-commerce  shopping areas, with an improved Apriori algorithm proposed. We establish the mechanism of recommendation of commodities and raise the method of calculating the support count and confidence level. From the application of the case, users could quickly build association rules by the improved Apriori algorithm and recommend appropriate products to customers in a timely approach.

The rules digged out are stored separately in a data table ?tb_Association? shown in Table 1. Each time when customers settle accounts, system will scan what the customers has already purchased and query this table to recommend other products to them.

