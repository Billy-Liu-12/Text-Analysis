Novel Vertical Mining On Diffsets Structure

Abstract  Mining frequent patterns on the vertical data  structures usually shows improvements of performance over the classical horizontal structure. This is because the  vertical data structure supports fast frequency counting  via intersection operations on transaction identifiers (tids). Recently, Diffsets [2], a vertical data  representation, has been introduced for the sake of the  size of memory required to store intermediate tids in the mining process. In this paper, we will present a new  vertical mining algorithm on the Diffset structure called  Fast Diffsets Vertical Mining (FDVM). Primarily, FDVM uses the concept of pattern growth on the Diffset  structure, and we will show that FDVM outperforms  previous methods in mining the complete set of frequent patterns.  Our experimental results indicate that  significant performance improvement can be gained, especially for large databases, over previously proposed  vertical and horizontal mining algorithms.

1. Introduction  Mining frequent itemsets (or frequent patterns) has  been studied extensively in the research area of data  mining. Particularly, it has played an important role in  mining association rules [3,12], correlations [6], causality  [19], sequential patterns [4], episodes [14],  multidimensional patterns [11,13], max-patterns [10],  partial periodicity [9], emerging patterns [7], and many  other important data mining tasks.

Most of the previous works on mining associations are  based on the traditional horizontal transactional database  format [3,8,12,13,15,16,18], a variant of Apriori-like  approach [1]. However, a number of vertical mining  algorithms have been proposed recently for mining  associations [5,17,20,21]. In the vertical format, each item  is associated with its corresponding tidset, the set of all  transactions (or tids) which contains that item. Mining  algorithms on the vertical format have shown to be very  effective and usually outperform horizontal approaches.

This advantage stems from the fact that frequent patterns  can be counted via tidset intersections, instead of using  complex internal data structures (Candidate generation and  counting occur in a single step). Vertical mining  approaches take advantages on no distinction of candidate  generation and support counting phases.

Diffset [2] is a vertical data representation that only  keeps track of the difference in the tids of a candidate  pattern from its generating frequent patterns. It drastically  cut down the size of memory required to store  intermediate results. The initial database stored in the  diffset format, instead of the tidset, can also reduce the  total database size.

In this paper, we present a novel vertical mining  algorithm called FDVM, which can mine a complete set of  frequent patterns on Diffset structure. FDVM utilizes a  pattern growth strategy for efficiently enumerating all  complete sets of frequent patterns. This algorithm  recursively generates all subsets from 1-itemset condition  patterns without computing the support values of all their  subsets and, also, there are no candidate generation  procedure and no complex data structures such as hash  trees. It can be instantiated using either the traditional  vertical tidset or the diffset structure. We will show that  our new algorithm combined with diffset can outperform  both dEclat [2] and Viper [17].

The remaining of the paper is organized as follows.

Section 2 introduces the concept of diffset structure and its  construction method. Section 3 proposes a new frequent  pattern-mining algorithm, called Fast Diffset Vertical  Mining (or FDVM, for short), on the Diffset structure.

Section 4 presents the result of our experimentation  comparing the performance of our approach with others?.

Section 5 summarizes our work and discusses future  research issues.

2. Diffset Structure: Design and Construction  Let I = {a1, a2, ?, am} be a set of items, and DB = {T1,  T2, ?, Tn} be a transaction database, where Ti (i  {1, ?, n}) is a transaction which contains a set of items in I. In  addition, an index i is a unique identifier (called tid) of a transaction Ti. The support (or occurrence frequency) of     an itemset (or a pattern) X, denoted by (X), is the number of transactions in DB that contain X. An itemset X is called  a frequent itemset (or a frequent pattern) if its support is no less than a predefined minimum support threshold,  (%) .

To illustrate the concept of Diffset structure, it is  assumed that DB = {1, 2, 3, 4, 5}, as shown in Table 1, be an example of a transaction database, and I = {A, B, C, D,  E} be a set of five different items in the database. Figure 1  depicts a common data format that has been used often in  mining associations. In this traditional horizontal  approach, each transaction has a tid along with the itemset  comprising the transaction. In contrast, the vertical format  maintains for each item its tidset, the set of all tids where  the item occurs.

TID Item Bought  1 A,T,W  2 C,D,W  3 A,C,T,W  4 A,C,D,W  5 A,C,D,T,W  6 C,D,T  Table 1: A transaction database  The Diffset structure is based on the concept of vertical  database format. It avoids storing the entire tidset of each  item by keeping track of only the differences in the tids  between the tidset of each item and the whole tidset (the  set of all tids). These differences in tids are stored in what  has been called the diffset. When we set a minimum support to the Diffset structure, an item ai whose  corresponding tidset has the cardinality more than  100/*DBDB , must be cut off. Figure 2(a) shows  Diffset structure with  = 50%. In Figure 2(b), Diffset  structure is sorted by the support in an ascending order  and, hence, it has a better chance that more postfix can be  shared.

Horizontal Structure    1 A T W  2 C D W  3 A C T W  4 A C D W  5 A C D T W  6 C D T  Vertical Structure  A C D T W  1 2 2 1 1  3 3 4 3 2  4 4 5 5 3  5 5 6 6 4  6   5    Figure 1: Common Data Formats  DiffSet Structure  A C D T W  2 1 1 2 6  6  3 4  (a)  A D T C W  2 1 2 1 6  6 3 4  (b)  Figure 2.  Diffset Data Format  Figure 3: Compute Support on Diffset  Figure 3 shows the different regions for the tidset and  diffsets of item a and b. Let t(a) denote the tidset of an  element a. Also, the diffset of an element a is denoted by  d(a) and defined by d(a) = t(a?) = U t(a), and the diffset  of an itemset ab  defined by d(ab) = t(a) t(b) = t(b?)  t(a?). We can compute the support of an itemset ab on  diffset structure by evaluating the cardinality of )(Area  shown in Figure 3. Thus, the support of itemset ab is  (ab)  =  t(a)  t(b)  = (a) d(ab)  , where  (a)  =  t(a)  = U d(a)  The symbol of set, e.g., {a, b}, has been ignored where  there is no confusion could arise.

a b     Example 1. Based on the example database in Table 1  and its diffset that arises in Figure 2, we will illustrate how  to evaluate the support of itemsets and, then, determine  whether the itemsets are frequent patterns. It is assumed  that the support threshold is specified as 50% which equals  to 3. Now, let?s examine a 2-itemset AD first. We found  that d(AD) = t(D?) t(A?) = {1, 3}, and the support of  item A is (A) = 4. Consequently, the support of AD is  (AD) = (A)  |d(AD)| = 4  2 = 2. According to the  definition, itemset AD is not a frequent pattern.  If we  evaluate a 3-itemset ATW, we will obtain that |d(ATW)| =  | | = 0, and (AT) = 3. Hence, (ATW) = 3  0 = 3 which  leads to a conclusion that ATW is a frequent pattern.

3. FDVM (Fast Diffset Vertical Mining)  In this section, we will introduce how to carry out  mining of a complete set of frequent patterns on Diffset  structure. In addition, we will show some important  properties of Diffset structure that can facilitate the  frequent pattern mining. Basically, the search space for  enumeration of the complete set of frequent patterns is  given by the power set I  which is exponential m2  in  Im , the number of items.

Primarily, our FDVM algorithm can be separated into  two main parts. The first one is to find the long patterns,  and the other is to generate all patterns from the long  patterns. In the first part, we will evaluate the supports of  a?s conditional patterns  to find the long pattern (if  ) and the next long pattern. After that, hidden  patterns can be generated by combining the elements of  the long patterns with the same a?s condition and, then,  testing those patterns for the condition of hidden patterns.

In the second part of the algorithm, we will generate all  subitemset from the long patterns and the next long  patterns of each condition without computing the supports  by following Algorithm 2 ( ) which is based on the  concept of Lemma 3.1. Then, we can combine the hidden  patterns and all subitemset together to get the complete set  of frequent patterns  Algorithm 1. <FDVM>  Step 1:  Find frequent items using the vertical data  format  Step 2: Convert data into the vertical diffset format  and, then, sort the items by their supports in an ascending  order  Step 3: Find the long pattern and the low support of  each X?s condition by using Procedure Condition ( )  a?s conditional patterns means patterns which have a as  the prefix.

Step 4: Find the long pattern based on the next long  pattern of each condition by using Procedure Condition ( )  Step 5: Generate hidden itemsets from all combinations  of the elements in all long patterns with the same a?s  condition and, then, ignore some patterns if they satisfy  either of the 2 criteria  5.1 if the patterns has already tested in Step 3 and  Step 4  5.2 if the patterns consists of items in their low  supports  Step 6: Test the remaining hidden itemsets whether  they are frequent patterns or not  Step 7: Generate all subitemset from each long pattern  by Procedure Extract ()  Step 8: Combine the results from Step 6 and Step 7 to  receive the complete set of frequent patterns  Procedure Condition( itemsX , )  {  RXTemp ,  // Computer support forward in X?s Condition  for j = position(X) in items to |items|  {   jXTempTemp  // Test long pattern with jX  if supmin_)(TempSupport then  TempR  else  RTemp  // Test next long pattern  if supmin_)( jXXSupport then  }{ jXNextNext  else }{ jXLowLow  }  }{RLongLong  }  Lemma 3.1 (Fragment growth) [8]. Let X be a frequent  itemset in DB, B be X?s conditional pattern base, and a be  an itemset in B. Then, the support of X a in DB is  equivalent to the support of a in B.

Corollary 3.1 (Pattern Growth) [8]. Let X be a frequent  itemset in DB, B be X?s conditional pattern base, and a be  an itemset in B. Then, X a is frequent in DB if and only  a is frequent in B.

Algorithm 2.

Input :   T (long pattern with X?s condition)  Output: CS (The set of frequent patterns)  L    = length of long pattern with X?s condition     SIi  = Subitem of long pattern at i position  Yij  =  i-itemset number j in Complete set of frequent  Patterns  Procedure )(TExtract  {  1SITemp ; //set X?condition to 1-itemset  for i = 1 to TL //expand 1 to L itemset  {  //expand (i-1)-itemset to i-itemset  for all TempY ji )1(  with itemseti )1(  {  //find complete i-itemset  for  k=position (i-1)of jiY )1(   to L  }{ )1( kji SIYTempTemp  }  }  }{TempCSCS  }  Example 2. Let us examine the mining process based  on the Diffset structure in Figure 2 b. We collect all long  patterns and next long patterns with ai condition.

Let?s start with A?s conditional pattern base. We try to  extend itemset A by adding item D to it and obtain (AD)  = 2 which is less than . Thus, we keep item D in  Low_Sup. Next, we try to extend itemset A by adding item  T and have (AT) = 3 which is equal to . Therefore,  item T can be added to itemset A. Next, we try to extend  itemset AT by adding item C and have (ATC) = 2 which  is less than . Thus, item C can?t be added to itemset AT.

To find a next long pattern, we compute (AC) = 3  support between item A and C not less than . Thus, save  C itemset in to the next long  pattern itemset . Last we try  to extends itemset AT by adding item W and compute  (ATW) = 3, that equals . Thus, itemsets ATW and C  are the long pattern and the next long pattern in A?s  conditional pattern base, respectively.

Now, let?s consider D?s conditional pattern base. We  try to extend itemset D by adding item T to it and compute  (DT) = 2 which is less than . Thus, we save item T to  the Low_Sup. Next, itemset D is extended by adding item  C and compute (DC) = 4 which is more than . That  means we can add item C to D. Then, item W is added to  itemset DC and compute (DCW) = 3 which equals .

Thus, we can add item W to DC. Therefore, itemset DCW  is the long pattern in D?s conditional pattern base.

Then, let?s examine T?s conditional pattern base. We  try to extends itemset T by adding item C to it and  compute (TC) = 3 which is greater than . Thus, we can  add item C to itemset T. Next, item W is added to itemset  TC and compute (TCW) = 2 which is less than . To  find a next long pattern, we compute (TW) = 3 which is  more than . Thus, we save item W into the next long  pattern itemset. We obtain that itemsets TC and W are the  long pattern and the next long pattern in T?s condition  pattern base, respectively.

Now, let?s move to C?s conditional pattern base. We  can find both long pattern and next long pattern in a  similar manner to the above. As a result, CW is the long  pattern in C?s conditional pattern base. Similarly, W is the  long pattern in the conditional pattern base of W.

Next, we have to find long pattern from the next long  patterns which are {C} and {W}. The result is that the  respective long patterns with A?s and T?s conditions are  AC and TW.

Next, we find hidden patterns. Previously, we receive  ATW and AC as A?s conditional long patterns. Thus, we  can combine their elements and have itemsets {ATC},  {AWC} and {ATWC}. Then, based on Criteria 5.1 in  Algorithm 1, we can ignore {ATC} and {ATWC}. In the  same manner, for T?s conditional long patterns, we can  generate hidden pattern {TCW} and ignore it by same  criteria. Then, only itemset {AWC} remain to be testified  for the condition of frequent pattern. According to its  support, (AWC) = 3, AWC is a frequent pattern. Next,  we have to extract subpattern from the long patterns (see  Example 3)  Example 3. We will extract all subitemsets from each  a?s conditional long patterns found in Example 2 by using  Algorithm 2. With A?s conditional long pattern {ATW},  we can extract all the patterns {A, AT, AW, ATW}. With  D?s conditional long pattern {DCW}, we have {D, DC,  DW, DCW}. After extracting all subitemsets from all long  patterns and combining them together, we will obtain the  frequent patterns {A, AT, AW, ATW, D, DC, DW, DCW,  T, TC, C, CW, W, AC, TW}. Then, we can combine those  frequent patterns with the hidden pattern {ACW} in order  to receive the complete set of frequent pattern.

4. Experimental Results  In this section, we will present performance  comparisons of three algorithms - FDVM, dEclat [2] and  Viper [17]. All experiments were performed on a 933  MHz Pentium PC with 256 MB of Memory, running on  Windows2000 Server. All the programs were written in  C#. The dataset, so-called PUMSB dataset, chosen for  testing the performances of the algorithms is publicly  available from the website  http://ftp2.census.gov/census_1990/.

We will report the experimental results on two  datasets. The first one, denoted as 1D , is the PUMSB  dataset containing census data with 7,117 items. In this  dataset, the average  size of maximal potential frequent  itemsets is 74, while the number of transactions in the  dataset is 49,046. The second data set, denoted as 2D , is  the PUMSB* dataset which is drived from PUMSB by  removing items with the support of 80% or more from the  7,117 items. The average size of maximal potential  frequent itemsets is reduced to 50, while the number of  transactions in the dataset is still 49,046. There are  numerous frequent itemsets in both data sets, as the  support threshold goes down. There are pretty long  frequent itemset as well as a large number of short  frequent itemsets in them. They contain abundant mixtures  of short and long frequent itemsets.

4.1 Comparison of FDVM and Viper  The scalability of FDVM and Viper [17] as the support  threshold decreases from 100 to 60% on pumsb dataset  and decreases from 50 to 5% on pumsb* dataset is shown  in Figure 4. It can be seen that FDVM?s scalability is  much better than Viper. This is because as the support  threshold goes down, the number as well as the length of  frequent itemsets increases dramatically. The candidate  sets that Viper must handle become extremely large.

25 30 35 40 43 45 50 60 70 80 90 95 Support threshold(%)  R un  tim e(  se c.

)  D2 FDVM  D2 Viper  D1 FDVM  D1 Viper  Figure 4. Scalability with support thresholds.

Figure 5 depicts the run time per itemset of FDVM. It  shows that FDVM has good scalability with the reduction  of the minimum support threshold. Although the number  of frequent itemsets grows exponentially, the run time of  FDVM increases in a much more conservative way. Also,  Figure 5 indicates that as the support threshold goes down,  the run time per itemset decreases dramatically. This is  why the FDVM can achieve good scalability with the  support threshold.

0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4  25 30 35 40 43 45 50 60 70 80 90 95  Support threshold (%)  R un  tim e /it  em se  t(s ec  .)  D2 runtime /itemset  D1 runtime /itemset  Figure 5. Run time of FDVM per itemset versus  support threshold  The data set 2D has been used to test the scalability  with the number of transactions. The support threshold is  set to 25%, and the results are presented in Figure 6.

1000 2000 3000 4000 5000 6000 7117  Number of transactions  R un  tim e (s  ec .)  FDVM  Viper  Figure 6. Scalability with numbers of transactions  In Figure 6, it is obvious that both algorithms show  linear scalability with the number of transactions from  1,000 to 7,117. However, FDVM is much more scalable  than Viper. As the number of transactions grows up, the  difference between the two methods becomes larger and  larger. Overall, FDVM is about an order of magnitude  faster than Viper in large databases, and this gap grows  wider when the minimum support threshold reduces.

4.2 Comparison of FDVM and dEclat  dEclat is an efficient algorithm proposed by Zaki and  Gouda [2]. The basic idea of dEclat is composed of  computing diffsets for all distinct pairs of itemsets and  checking the support of the itemsets.

According to our experimental results, both algorithms  are efficient in mining frequent patterns. However, a closer  study shows that FDVM is better than dEclat when  support threshold is low and the database is quite large.

25 30 35 40 43 45 50 60 70 80 90 95  Support threshold(%)  R un  tim e(  se c.

)  D2 FDVM  D2 dEclat  D1 FDVM  D1 dElat  Figure 7: Scalability with thresholds.

As shown in Figure 7, both FDVM and dEclat have  good performance when the support threshold is simply  low, but FDVM is still better. As shown in Figure 8, in  which the support threshold is set to 25% on 2D , both  FDVM and dEclat have linear scalability with the number  of transactions, but FDVM is more scalable.

1000 2000 3000 4000 5000 6000 7117 Number of transactions  R un  tim e(  se c.

)  FDVM  dEclat  Figure 8: Scalability with numbers of transactions  Major costs occurring in dEclat algorithm are  computation of diffsets for all distinct pairs of itemsets and  checking of the supports of the itemsets. Particularly, in a  database with a large number of frequent items, the  computation cost becomes quite large. In contrast, FDVM  uses only the notion of the subitemset generation. This is a  main reason why FDVM has distinct advantages when the  support threshold is low and when the number of  transactions is large.

5. Conclusions  We have proposed a novel vertical data-mining  algorithm, called FDVM, that can mine a complete set of  frequent patterns on a diffset structure. Our approach has  been shown to be an efficient mining algorithm for large  databases.

In fact, FDVM has more advantages than other  methods in several ways. Firstly, in contrast to dEclat  algorithm [2], FDVM generates a subitemset with X ?s condition pattern without computing and checking the  support of the subitemset. Secondly, the size of memory  required in mining will be drastically cut down from the  diffsets structure.

In this work, we have implemented the FDVM  algorithm and investigated its performance in comparison  with several frequent pattern-mining algorithms in large  databases. Our performance study has shown that the  algorithm mines both short and long patterns in large  databases more efficiently than previous methods did.

