Acquiring Compressor Design Case Knowledge

Abstract  Product design case databases contain a lot  potential knowledge, which can tell us the relations among parameters and some interesting experience patterns. While designing product, it is supposed to support the designers to make decisions better. Therefore many researchers are trying to find an effective approach to discover the unknown knowledge. In this paper, we presented several algorithms which combining rough set theory and information entropy for knowledge discovery. With these algorithms, the potential knowledge was mined out from the original product design databases. It was generated in the form of several association rules. Also a case study was presented to demonstrate the process of these algorithms. And the efficiency was proved at last. It turns out that the process of these algorithms could acquire knowledge from design databases effectively.

Keywords: Rough set, design knowledge, KDD, association rules    1. Introduction  The product design is kind of a process of  knowledge innovation on the basis of original information resource. So if we can discover the previously unknown and useful knowledge from  the original resource, it will be helpful for especially the new designers. It could not only shorten the design cycle, cut the cost, but also insure the quality of product design process.

As we know, the designers usually spend a mass  time on reading references or asking others for help during the process of product design. Sometimes, they just make decisions depending on their own experience.

Unfortunately, current researches mostly focus on how to deliver and use knowledge that we already know, while less was on the potential knowledge. Therefore the potential knowledge is not used effectively. And it is a very common problem in industrial product design of Chinese manufacturing.

In this paper, a database of compressor design is studied, which records each compressor type?s design parameter. While several algorithms based on rough set theory and information entropy are integrated and used to discover the association rules and potential experience knowledge from the compressor design case database.

2. Rough Set and Information Entropy-based Algorithms   Rough Set (RS) theory is a valid mathematical  theory to deal with imprecise, uncertain, and vague information [1]. With many practical and   DOI 10.1109/NPC.2008.79    DOI 10.1109/NPC.2008.79     interesting applications, rough set approach seems to be of fundamental importance to AI and cognitive sciences, especially in the areas of machine learning, knowledge acquisition, decision analysis, knowledge discovery from databases, expert systems, inductive reasoning and pattern recognition [2]. Mostly for RS could induct the attribute and value effectively. The empirical results obtained show that they are very powerful and that some important knowledge has been extracted from datasets.

In the light of information view, the information entropy in RS theory describes a decision information system?s uncertainty objectively. Thus information entropy is frequently used to measure the significance of the attribute as the heuristic information is designed.

Furthermore some literatures have proved that information entropy-based reduction algorithm has better applicability, comparing with algebra entropy-based. Many literatures have the description of the information view based on rough set theory [1][3][4].

This section begins with some concepts of rough set and information entropy that would be used in this paper. Then the algorithms are presented in detail, which would be used to acquire association rules in compressor design database in next section.

2.1.Basic Concepts   For the convenience of illustration, some basic  notions are introduced at first.

Definition1. A decision table  A decision table is defined as , , ,U C DV f< ? >,  where U is a finite set of objects. C is its condition attribute set and D is its decision attribute set. Set of attributes? values V is associated, with the attribute determining function f. The formulas below are based on this decision table.

Definition2. A decision table is defined as  , , ,U C D V f< ? > . P and Q are equivalence  clusters defined on U. The P positive region of Q is defined as  ( ) ( ) /P X U Q  POS Q P X??= ?   (1)  Definition3. A decision table is defined as  , , ,U C D V f< ? > . Let { }1 2 m/ , , ,U D Y Y Y= ? .

Then the ability of C to classify the decision table is defined as  1 1  _ | ( ) | ( ) =  m m  i C i i i  C  C Y pos Y K D  U U = == ? ?  (2)  Definition4. A decision table is defined as  , , ,U C D V f< ? > . Let { }1 2, , , nP X X X= ?  and { }1 2 m, , ,D d d d= ? , where ( )P P C? is a subset of condition attributes. Then the conditional information entropy of D with reference to P is defined as  1 1  | | | | ( | )  | | | |  cm n j i j i  j i  d X d X H D P  U U= =  ? ? =?? (3)  Where Cjd is the complement of jd ,  1, 2, ,i n= ? , 1, 2, ,j m= ? .

Definition 5. The set of all indispensable attributes in C with respect to D is called the core of C and is denoted by Core(C, D).

( ) { } ( ){ }( , ) : C C aCore C D a C POS D POS D?= ? ? (4)  The concept of the core can be used as the starting point for computation of reduction. Some methods for computing the core of the information system are provided in [8][9]. More specific descriptions about rough set are available in [6].

2.2. Information Entropy-Based Discretization Algorithm   Discretization is very important in rough-set-based knowledge acquisition [3]. The basic idea of this algorithm is that the significance degree is taken as heuristic information to select cut points until the classification ability of the new decision table equaled the primitive one. It measures the cut point?s significance degree. The point that has the maximum significance degree is chosen and added to the final set of cut points. Meanwhile check whether the classification ability of the new decision table equals to the primitive value.

If ?yes?, the algorithm is over. While if the answer is ?no?, continue to add new cut point.

Thus, the consistence of classification is ensured.

Algorithm Description:  Input: an information system , , ,U C DV f< ? >  Output: a discretized decision table Method: Step1? Initialize the cut points of each attribute in decision table. First, make each set of attribute  values ia C?  ascending ordered:  1 2 ...i i ii a ii a a a  a m al v v v r= < < < = Thus the initial cut point set is  1( )  (i=1,2,...,m 1)  i i  i  a a j ji  j a  v v o +  + = ?    Therefore each attribute ia  has a set of  initial cut points i pO ? , and a set of candidate  cut points pO  which contains all the initial cut  points of the decision table.

Step2: Discretize the decision table by the initial cut points?and use Formula (1) to calculate the  initial classification ability ( )cK D .

Step3 ? Create the set of final cut points:  { }1 2= , , , nO O O O? , iO =?, 1, 2, ,i n= ? .

Step4?For each cut point ijo  in the candidate  cut point set pO , use Formula (5) to calculate the  value of entropy { }( )| ijH D O o?  when we add this new cut point to iO .

( ) 1 1  | | | | ( | { }) 5  | | | |  cm k j i j i  j j i  d Y d Y H D O o  U U= =  ? ? ? =??  where 1 2{ , ,..., }mD d d d= , 1 2{ , ,..., }kY Y Y Y=  is the equivalence clusters after adding ijo .

Step5 ? Select the cut point which has the  minimum value of { }( )| ijH D O o? . If more than one point have the minimum value, we  choose the one ikc  whose number of cut points  is less than others. Then let { }=i i ikO O o? ?  { }=i p i p ikO O o? ? ? ; Step6: Discretize decision table by O , and  calculate the classification ability ( )'cK D . If  ( )'cK D  equals ( )cK D ? then the end, or turn to step4.

Suppose the average number of condition attributes is m, while the total number is k?and the amount of objects is n. The algorithm reduces the time complexity effectively. Its time  complexity is ( )2 2 2O n m k , which is much better than Greedy Algorithm.

2.3. Information Entropy-based Attribute Reduction Algorithm   Attribute reduction algorithm based on rough  set play an important role in KDD. It can discern all objects discernible by the original information system. Here we use an information entropy-based attribute reduction algorithm EIEAAR (Extended Information Entropy Algorithm for Attribute Reduction) [5].

Algorithm Description:  Input: an information system , , ,U C DV f< ? >  Output: the minimal attribute subset R Method: Step1: Calculate the core attributes Core(C) of decision table, initialize R=Core(C).

Firstly, calculate ( | )H D C  Then, for each attribute ia C? , calculate  ( | { })iH D C a? . If  ( | { })iH D C a? > ( | )H D C , add ia  to R.

Step2: If ( | )H D C = ( | )H D R ? then the  end, otherwise turn to step3.

Step3?for each attribute ia C R? ? , calculate  the entropy { }( )| iH D R a? . Choose the one  ia  which has the minimum value of  { }( )| iH D R a? , and add it to R. If more than one point get the minimum value, just choose one of them.

Step4 ? If { }( )| iH D R a?  equals  ( | )H D C , then go to the end, or turn back to  step3.

In EIEAAR, calculate core and non-core attributes is the basis operations. Suppose |U|=n, |C|=m, the complexity degree of calculating core  attributes is ( )2O mn , and the complexity degree of calculating non-core attributes is  ( )3O n , therefore the total complexity degree is  ( ) ( )2 3T O mn O n= + . Usually the amount of m is less than n in real engineering problems, so this algorithm is much effective while dealing with real problems.

2.4. Rule Generation Algorithm   The process of rule generation is also a value  reduction process. The aim of the algorithm is to delete the redundant data and obtain knowledge rules from information table.

Algorithm Description: Input: an information system S Output: rule matrix S' Method: Step1?Scan the condition attributes column by column. Suppose we hide one column at first. In the rest table, if we can find conflict objects, keep their value in this column. If there are the same objects, mark their value by a ?*?. And for the rest value, just mark them by a ???.

Step 2: Delete the same records. Then scan the objects that contain ???. If we can make a decision just by the unmarked attribute values, we should change ??? into ?*?. If we cannot, change ??? back to its previous value. And if we find a record whose attribute value all marked, we also change ??? into its previous value.

Step3: delete the same records and the one whose condition attribute value is all marked by ?*?.

Step4: check the table to see if there is a couple of records that are the same expert one attribute, and for this different attribute, one of records is marked by a ?*?. Like step2, if we can make a     decision just by one of these two records, delete the other one. Otherwise delete this one.

Through the method above, we would obtain a new decision table. In this table, each record can be regarded as a rule.

3. Case Study   The current approach to knowledge discovery is illustrated on the following example. As  figure1 shows, there are seven steps while acquiring compressor design case knowledge using rough set theory.

In this section, we use 70% objects of the compressor design database as a training set. And the rest would be used to test the rules generated later. As we have confirmed both condition attribute set and decision attribute set, next process is the data pretreatment. First, scan the whole database, eliminate several objects which  have some attribute value missing. Then change the non-numerical value into numerical.

Meanwhile, select all the continuous attribute value for discretization. As we have two decision attributes in this example, and each one is  calculated separately, we just give the details of the whole process of D1. Table 1 is the first 15 objects from compressor design database, in which the discrete attributes value has been transformed into numerical.

Table 1. An example of mining knowledge from compressor design database  No condition attributes  decision attributes  S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 S11 S12 S13 S14 S15 S16 S17 S18 S19 D1 D2 1 3.5 29.5 118 120 180 138 1 1 2 1 0 0 2 1 2 1 1 1 3 5 28.6 2 3.6 36 124 123 180 161 2 2 1 5 1 1 2 4 2 1 2 1 3 7 28.69 3 3.62 31.1 120 123 150 161 4 1 2 4 1 1 2 4 1 2 1 1 3 7 34.2 4 3.6 36 124 120 250 161 2 2 1 5 1 1 2 4 1 2 2 1 3 7 28.69 5 3.6 36 124 120 120 161 2 2 1 5 1 1 2 6 1 2 2 1 3 7 28.69 6 3.6 36 133 110 180 156 3 1 1 2 1 1 1 6 1 2 1 1 3 5 32.8 7 3.6 36 124 116 180 161 2 2 1 5 1 1 2 5 1 2 2 1 3 7 28.69 8 3.6 36 133 119 220 151 3 1 1 2 1 1 1 6 2 2 1 1 3 5 32.8 9 3.6 36 133 119 220 130 3 1 1 2 1 1 1 6 2 2 1 1 3 5 32.8 10 3.6 36 133 119 160 151 3 1 1 2 1 1 1 6 1 2 1 1 3 5 32.8 11 3.6 36 133 119 160 151 3 1 1 2 1 1 1 6 1 2 1 1 3 5 32.8 12 3.6 36 133 119 220 151 3 1 1 2 1 1 1 6 2 2 1 1 3 5 32.8 13 3.6 36 124 111 180 161 2 2 1 5 1 1 2 6 1 2 2 1 3 7 28.69 14 3.6 36 133 110 180 156 3 1 1 2 1 1 1 6 1 2 1 1 3 5 32.8 15 3.6 36 124 111 180 161 2 2 1 5 1 1 2 6 1 2 2 1 3 7 28.69     Table 2. Description of attributes attribute description attribute description  S1 coil resistance S11 displacement variable S2 static friction torque S12 displacement control form S3 compressor diameter S4 pulley diameter S13 freeze oil?s kind S5 gas capacity S14 Pulley form S6 displacement S15 Angle form S7 compressor name S16 Interface form S8 structure S17 sucker S9 Min. rotation rate S18 coil voltage S10 Max. rotation rate S19 coil current D1 cylinder number D2 Max. trip   Table 3. Candidate cut points set  attribute candidate cut points S1 3.55  3.61  8.91 S2 29.45 29.75 30.55 32.65 34.25  35.15  39.50 S3 119.0  122.0  128.5 S4 110.5 113.5 117.5 119.5 121.5  124.5 128.0 131.0 133.0 136.5 S5 127.5 142.5 155.0 170.0 190.0  210.0 235.0 257.5 282.5 S6 134.0  144.5  153.5  158.5  Then, discretization was used to the attributes S1~S6. After step1 of discretization, the candidate  cut points set pO was calculated, just as Table 3  shows.

Then continue step2 ? 5 of discretization  algorithm for the candidate cut points, until we gained final cut points. Table 4 lists some  important data in the process of choosing cut points. From this table, it is obviously to know that there is only one final cut point.

It is also very obvious to see that through this discretization algorithm, not only the number of cut points is reduced, but the number of condition attribute is also reduced. This is a very cheerful result. In addition, this makes the complexity of the rest algorithm decreased a large range.

So far, the values of the dicision table have been transformed into discrete ones. So we now go to attribute reduction. The algorithm is just as the session 2.2 presented. After step1 and step2 of attribute reduction algorithm, the core attribute set is obtained, which only contains attribute S6.

Calculate ( | )H D R , it?s easy to find out  ( | )H D R = ( | )H D C =0, so the core attribute  is only S6.

Table 4. Important data in the process of choosing cut points  No.

cut point  corresponding information entropy  condition attribute  the classification ability after adding the point  1 158.5 0 S6 1   Table 5. The results of attribute reduction decision table  consistent /inconsistent  object amount  attribute amount before reduction  core attribute amount  attribute amount after reduction  (C, D1) Consistent 56 19 1 1 Attribute reduction makes the condition  attributes reduced to only one attribute S6. Then we need to generate the rules, and the result can be seen in Table 6.

Table 6. Results of D1 value reduction No. S6 D1 1 0 5 2 1 7  Repeat above process, the reduction results of D2 (Max. trip) are generated as Table 7 shows.

Table 7. Results of D2 value reduction  No. S3 D2 1 0 28.6 2 1 34.2 3 2 28.69 4 3 32.8  From the Table 6 and Table 7, each list has a corresponding rule. They are as follows.

Rule1: IF displacement<=158.5, THEN numCylinder =5 Rule2: IF displacement >158.5 ? THEN numCylinder =7 Rule3: IF diaCP <=119?THEN maxTrip =28.6 Rule4: IF 119< diaCP <=122?THEN maxTrip =34.2 Rule5: IF 122< diaCP <=128.5?THEN maxTrip =28.69 Rule6: IF diaCP >128.5?THEN diaCock =32.8  (numCylinder means the number of cylinders, diaCP means compressor diameter, maxTrip means the maximum trip of compressor.)  As these rules are generated by our method, next we need to know whether the rules have sense.

4. Rule Evaluation   In order to evaluate the association rules we generated from compressor design database in the preceding section, we have to introduce several concepts related.

Definition6. Consistent Degree  The consistent degree of a rule is defined as  ( ) ( ) ( )( ) ( )( )/i i i icer r card C r D r card C r= ? (6)  where ( )( ) 0icard C r ? , ( ) ( )( )i icard C r D r? denotes the number of objects that fit the Rule  ir , ( )( )icard C r  denotes the number of  objects that fit the condition ( )iC r  of Rule ir .

So the consistent degree tells us the reliability of  Rule ir  in compressor decision table.

Definition7. Coverage Degree For the consistent degree can only evaluate  the accuracy of each rule, it can not describe how many objects a rule can cover. So the concept of coverage degree is presented here.

It is defined as  ( ) ( ) ( )( ) ( )( )cov /i i i ir card C r D r card D r= ? (7)  where ( )( ) 0icard D r ? , ( ) ( )( )i icard C r D r? ? 0, ( )( )icard D r  denotes the number of objects that fit decision attributes ( )iD r  of the rule ir . Therefore, the coverage degree denotes the coverage degree of Rule ir  through the whole compressor decision table.

Definition8. Supportive Degree is defined as  ( ) ( ) ( )( ) ( )sup /i i ir card C r D r card U= ? (8)  where U denotes a finite set of objects. ( )sup ir  denotes the intensity of Rule ir .

Table 8 shows the evaluation result of the six rules. In this table, it can be seen that all rules generated have high consistent and coverage degree. Thus, it can be observed that all rules represent the original compressor design database very well. Therefore, we can affirm that this method do acquire potential knowledge successfully. In addition, the results are highly accepted by the designers.

Table 8. The result of evaluation rules consistent coverage supportive  1 100% 100% 72.2% 2 100% 100% 27.8% 3 100% 100% 58.2% 4 100% 100% 11.4% 5 100% 100% 13.9% 6 100% 100% 16.5%    5. Conclusions  This paper has presented how to acquire  knowledge in compressor design case database using rough set theory. Several algorithms combining rough set theory and information entropy are proposed as a system method that can be used in knowledge discovery in databases (KDD), which were fiistly used to the domain of compressor design.

And the case study, it was proved to be effective and practical. For we have discussed the results with the company designers. They are very surprised to know that the data they are using all the time has the rules that they never found or even thought of before. They said,? this work is very guidable for new designers, even experienced designers do not aware of these relations.? And after putting the rules in practice for a period, it is widely admitted by all the designers.

In the future work, we will focus on finding a better process model to acquire the potential association rules in database by analyzing the characteristics of compressor design database and improving the efficiency of algorithms.

Acknowledgements   This work is supported by the Major State Basic Research Development Program of China (973 Program) under Grant No.2003CB317005, and Shuguang Program of Shanghai Educational Committee under Grant No.05SG15.

