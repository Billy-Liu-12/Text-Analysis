World Congress on Internet Security (WorldCIS-2013)

Abstract-Given the complexity and velocity of the interactions  among vastly heterogeneous elements on the Internet; the  colossal amounts of information generated and exchanged,  coupled with the increasingly evasive nature of new forms of  electronic crimes, as well as the relative immaturity of current  Digital Forensics tools, Law Enforcement Agencies are easily  outpaced and overwhelmed with the types of electronic crimes  experienced today. In this paper, we describe the architecture of  a comprehensive automated Digital Investigation platform  termed as the Live Evidence Information Aggregator (LElA). It  makes use of the strong points of hypervisor technologies, large  scale distributed file systems, the resource description framework  (RDF), peer-to-peer networks, and innovative collaborative  mechanisms in order to introduce a level of speed, accuracy and  efficiency to match up with the imminent age of massively  distributed cybercrime in the context of Internet of Things.

Keywords-Digital Forensics, Cybercrime, Digital Evidence, Big  Data, Hadoop, Hypervisors, P2P, Collaborative Live Investigation

I. INTRODUCTION  The Internet today, in its most basic fonn, is a massively distributed, complex information sharing organism. Billions of interconnected elements of varying capabilities, attributes and functions work together through this information sharing, providing society with an endless array of beneficial everyday services. Such services span various activities that are part of the social, economic and the political structures of society.

Business, leisure, education, transportation, healthcare, and various government services are some of these activities that have adopted the Internet, in some way or another, in order to improve the ways in which these services have traditionally been delivered. One only needs to think of recent innovations such as mobile banking and payment systems, online shopping, e-Voting, e-Health services, transport information systems, online social networks, or crowd sourcing crisis information in order to see how critical these services, and their underlying technological infrastructures, actually are in maintaining a modicum of order in an individual's life today. From this, it is not difficult to realize that we greatly rely on these Critical Internet Infrastructures in order to support and facilitate our normal day to day activities.

In the same way that everyday activities are quickly being adopted into the "Digital Realm" through the use of technology and the Internet, so are malevolent activities. Evildoers are hard at work also creating novel and innovative ways to perpetrate   their malicious activities on the Internet. The crime scene is quickly moving from a physical reality to a more virtual one.

The forms of evidence found in these "Digital Crime Scenes" have also moved from the traditional fingerprints, footprints, hair samples, blood samples or other DNA related evidence, into more digital artifacts.. Such digital forms of evidence commonly include hard-disk drives, live (RAM) memory, network traffic captures, mobile devices, RAID sets [1], and virtually any other form of technology that records past events of its actions; that can be captured and can be analyzed during or after the criminal event and whose integrity can be verified.

This opens the floor to almost any fonn of computer system or device (physical or virtual) that can be thought of, and thus brings in the concept of heterogeneity among devices.

Different devices may have different operating systems, software applications, storage formats, encoding fonnats and communication protocols [2]. This heterogeneity makes the job of a Digital Investigator a lot more difficult because of the wide variety in which evidence could manifest itself in.

Additionally, Electronic Crime cases today commonly involve more than just a one device. Several physical devices including laptops, mobile phones, GPS devices and even embedded devices such as onboard vehicle computer systems may be seized at a go and brought in for analysis. The vast realm of the Internet also comes into play including web application accounts, online email accounts, cloud storage facilities, network traffic captures and logs [3]. All these evidence forms could easily be part of a single case in today's world and even more so in the imminent realm of the Internet of Things. The sheer volume of data that one would have to sift through in order to investigate a single case could be in the order of Terabytes and can be a more than daunting task to perform. [4]  The final problem that investigators grapple with is the relative immaturity of present forensic tools. Current industry standard forensic tools such as EnCase, FTK, XRY, Volatility and Wireshark 1, so far, do not cater for the highly divergent nature of digital evidence sources. Most, if not all tools, focus on a single niche area such as Filesystem Data, Live Memory, Network Traffic, Mobile Devices or Log data. There are no tools that provide a comprehensive method to interface with all  1 EnCase? Forensic and FTK? (Forensic ToolKit) are traditional file system forensics tools. MicroSystemation XRY is for mobile forensics, Volatility for live memory forensics and Wireshark for network forensics.

World Congress on Internet Security (WorldCIS-2013)  the variety present to provide a uniform investigation platform.

In addition to this, current tools have rather limited capabilities for dealing with extremely large datasets, and quickly become problematic when presented with such.

In this paper, we present the multi-tiered architecture of a comprehensive, scalable, distributed incident response and digital investigation platform, that is, LELA-the Live Evidence Information Aggregator. LElA aims at combating cybercrime through innovatively solving the aforementioned problems of the inherent complexity of the "Internet-of-Things scale" of impending cases, as well as the insurmountably large amounts of heterogeneous data against the current deficiency in physical resources, as well as skill, within Law Enforcement Agencies.

The rest of this paper is organized as follows: In Section II, we review related work outlining shortcomings of previous similar solutions. Section III describes the requirements for a comprehensive digital investigation platform. The functionality of the LELA system is described in Section IV. The proof of concept implementation and results are outlined in Section V.

In Section VI, we discuss the shortcomings and future work of this study. Finally, we conclude this paper with a summary in Section VII.

IT. BACKGROUND AND RELATED WORK  There have been several efforts directed towards making the Digital Investigation process more efficient. Most have been motivated by the requirements of legal systems, evolution in the digital crime scene, the sheer workload that burdens the analysts and technological revolutions. Some of these efforts include: Delegation and Collaboration among teams; Reduction of evidence sizes through filtering out known files; and simple automation of important but mundane, repetitive tasks (such as indexing data for subsequent searches, file carving, parsing running process and TCP flows). Most of these capabilities have been implemented in current industry standard forensic tools, however, investigators and analysts still continue to be overburdened. This is because of the massive amounts of heterogeneous and disjointed datasets that they are tasked to make sense of and present unambiguous conclusions.

Several unidirectional solutions have been proposed in a bid to solve this multi-faceted problem, however, none has been unequivocally successful. In recent times there have been initiatives to centralize processing and data collection onto powerful centralized mainframes and data storage centres, respectively. There has also been a push towards having the different parties, involved in solving a case to work together, even from geographically separate locations [5], particularly among technical staff in niche areas (filesystem forensics, network forensics, live memory forensics or mobile forensics) and the legal experts. Collaboration has been the mainstay of the attempt to get cases solved faster.

Reducing the amount of data that is needed to be collected is also a means of reducing the amount of time needed to analyze the data. This has previously been done through "Known File Filtering" as well as through heuristic analysis of the entire captured data. Network Security Monitoring has also been an avenue for gathering data through the assistance of Intrusion Detection Systems (IDS's) assisted through Artificial   Intelligence. However, this has been the specific mandate of the IDS, centralized or distributed, as the case may be, with terminating (end) devices or intermediary devices generally playing very minor roles in this task.

As far as is known to the author, there has not been much done, through any single initiative, in terms of expanding the scope of data captured to be the mandate of all possible devices of reasonable capability. Enabling individual devices to natively act as part of the Incidence Response System, towards the aim of collecting potential evidentiary data, has not been widely studied. Additionally, collaboration on the human processing level has been emphasized, but it has not been introduced among unrelated networked devices. These devices could possibly be harnessed to work together towards aiding in intelligent real-time capturing, filtering and processing in order to attain and retain that which could be considered as possible evidentiary data, antecedent to the event of a crime being detected. It is for these reasons that we delve into this area to explore it further.

Notable related studies include [6], that describes a live network forensics system that provisions varying Intrusion Detection Systems on host machines based on their respective resource costs. It works in a virtualized environment where snapshots are taken periodically and used to revert the system back to the point before an attack began. Each system rollback results in varying IDS's being deployed to collect new and possibly better information. This presupposes that the attacker re-enacts their malicious behavior repetitively each time their efforts are thwarted by the system. Storage of the potential evidentiary information in a forensically sound manner is not particularly dealt with in this study, however, the generated attack graphs may be useful in understanding the actions that were undertaken by the attacker.

In [7]-[9] distributed system architectures for proactive collection and summarization of evidence, with centralized data storage and processing, are described. They are, however, particularly directed at closed domain enterprise systems, where there is some form of control and order instigated by system administrators. The system being proposed in this study is aimed at being universal - applying to the entire Internet.

The work done by Redding in [10] is the most closely related study done in the area of pro-active and collaborative computer forensic analysis among heterogeneous systems.

Redding proposes a peer-to-peer framework for network monitoring and forensics through which network security events can be collected and shared among the peers. "Analysis, forensic preservation and reporting of related information can be performed using spare CPU cycles," [10] together with other spare, under-utilized, or unused resources. The system described, however, is again designed for an "administratively closed environment." This means that all the devices that are within the domain of this system are centrally controlled. An administratively open system is what is not dealt with in Redding's work, and thus it is what is sought after in the current study as will be described later in this paper.

There have been several efforts in terms of standardization of evidence formats (in the form of pre-processing) in order to facilitate uniform, seamless data exchange and communication     World Congress on Internet Security (WorldCIS-2013)  of forensic artifacts between heterogeneous entities. Members of the Common Digital Evidence Storage Format Working Group have generally been at the forefront of this [2]. Other notable efforts include [11] which makes use of the Resource Description Framework (RDF) from Semantic Web technologies as a common data representation layer for digital evidence related metadata, using ontologies for describing the vocabulary related to this data, and [12] where a detailed ontology of Windows Registry artifacts of interests is introduced. The Open Forensic Integration Architecture (FIA) in [3] and FACE [4] describe methods for the integration of digital evidence from multiple evidence sources in a bid to facilitate more efficient analysis. The Advanced Forensic Format [13], AFF4 [1] and XlRAF [14] describe annotated evidence storage formats that allow for addition of arbitrary metadata as well as interoperability among different tools.

All these evidence integration ideas assume that acquisition of the relevant data from the respective sources has already been performed. This is a stumbling block in the process of making digital investigations more efficient. This is because these methods have not been integrated into the evidence capture process. They are seen as a subsequent step rather than part of the process. As part of this study we integrate these hitherto unlinked processes.

The proposed idea that this study covers is composed of several areas of specialization, namely: The Internet of Things (loT), Intrusion Detection Systems, Peer to Peer Networks, Virtualization infrastructures, Large Scale Cloud storage and Semantic Web technologies. Most of these technologies have been previously harnessed in different capacities, singularly or in small clusters, towards the benefit of digital forensics for today's complex internetworked and intertwined cyber realm.

However, to the author's knowledge, there has so far not been any work done that aims to merge all these technologies together in order to provide a singular scalable solution that solves the recurring problems of large amounts of data, several sources of data, heterogeneity among systems, insufficient processing power, security and privacy - that are constantly troubling digital forensic analysts and law enforcement agencies worldwide.

Ill. CHARACTERISTICS OF THE DESIRED SOLUTION  In light of the current state of electronic crime, the present state of forensic tools, and from experience, we describe below a wish-list of characteristics that one would like to have in a Cyber-Law Enforcement solution:  ? Distribution: The ability to deal with massive amounts of distribution in terms of participants, data storage, processing and dissemination. The system needs to be able to handle the heterogeneity that may come with distributed systems as well.

? Scalability: Large scale interconnectivity, as well as the possibility of new entities joining, as well as others leaving the system dynamically and gracefully without drastic negative effects on the system. The ability to easily improve or extend the capabilities of the system through new modules is also desired.

? Availability: Providing suitable levels of functionality as and when required.

? Universality: Among the heterogeneity and lack of standardization among vendors of different systems, there needs to be some standardization and common understanding between the systems on the level of communication and storage of potential evidentiary information.

? Responsiveness: The system should be able to aptly detect when a security policy has been irrecoverably violated, thus collecting information in order to pursue the perpetrators of the criminal actions. This also improves on efficiency and privacy in that the system does not have to perpetually be collecting all possible information from all possible systems.

? Resource Sharing: Today, large complex problems that are being solved through collaboration and sharing of resources as seen in Crowd sourcing, P2P networks, and cloud infrastructures. They provide on demand rapid availability of large amounts of resources from collective resource pools providing speed, efficiency and the benefits from "the wisdom of the crowd".

? Integrity (Trust, Reliability & Accuracy): As a system facilitating law enforcement in digital crimes, the levels of trust, reliability, accuracy and integrity of the information needs to be high enough to be accepted as a veritable source of evidentiary information for a court of law. The Daubert standards and the chain of custody need to be adhered to.

? Privacy & Confidentiality: Personally identifiable and secret information must be maintained as anonymous and confidential as is reasonably acceptable, unless incriminated. Unauthorized access to such information is not to be allowed.

? Security: In addition to ensuring the security of the potential evidentiary information that it aims to collect and process, it must also take its own security into consideration - especially in terms of authentication, authorization, accountability and non-repudiation of activities undertaken.



IV. THE LIVE EVIDENCE INFORMATION AGGREGATOR  The hypervisor-based, peer-to-peer distributed system that is meant to be the LElA is a 4-tiered architecture with the following main components that work together in varying capacities:  a) The Host-based Hypervisor (HbH)  b) The Peer-to-Peer Distribution Architecture (P2P-da)  c) The Cloud-based Backend (CBB)  d) The Law Enforcement Controller (LEe)  The diagrams that follow portray the architecture of the LElA.

Figure 1 shows 3 layers of the architecture in a figurative manner, while Figure 2 displays the layered nature of the entire architecture.

World Congress on Internet Security (WorldCIS-2013)  HBH  llost 8Ji!oed IfyJK'fVlSOf (libH) ?m1 UYft  ?:ttf-::::::-?_ Pt't!f.lo-Peer Oistribuhon AtdWtec:hn{P1P4I} ... ,..

CIIOud?Nsed 8.id:end (UB) S\j)S.y5tC'lI\ I.liytf  Figure 1. High level view of the LElA architecture  HBH HBH HBH } Host?based  HVpervlso' (HBHI  Peer4to--Peer } DistributIon Architecture  (PIP.dol  } CkJud?8ased llackend (CBBI  i--- ---- --- - --:e? ---- --- --- --- I } ?;o,c.m.nt 1 ___________________________ ? Controller (LEq  Figure 2. Layered/Tiered representation of the LElA architecture  The functionality of each of the layers of the LElA system is briefly described in the following sections.

A. The Host-based Hypervisor (HBH) System  The Host-based Hypervisor (HbH) system is composed of a virtualization layer managed by a hypervisor - a privileged secure platform managing the guest operating system (OS).

The hypervisor contains an inbuilt host-based intrusion detection system. Security utilities within the guest OS such as anti-malware tools and intrusion detection systems maintain their own data and logs that are accessible to the HbH ..

Individual HbH systems communicate with each other through the Peer-to-Peer Distribution Architecture (P2P-da) in sharing information about malicious activity that they have discovered. This collaboration of HbH systems helps improve accuracy of TDSs and eventually data collection.

The HbH maintains a hash list of the local files on its guest operating system (Local - Known Data Hash-List, L-KDHL) which is periodically cross-checked and updated against a Master - Known Data Hash-List (M-KDHL) stored at the CBB. This is managed by the Cloud-based Backend Differencing Engine (CBB-DE) component of the CBB. User data and its corresponding hashes are also subjected to a similar procedure.

Having privileged access, the HbH is able to directly interact with the file system, network interfaces, memory   caches and other low-level resources, which are all primary sources of prime evidentiary data in digital investigations. The embedded IDS's (em-IDS) also collect information mostly in the form of logs which are parsed to result in alerts. When evidentiary data from the local HbH, or neighbouring HbH is in transit towards the CBB, it is always held in temporary storage and in an encrypted form.

B. The Peer-to-Peer Distribution Architecture (P2P-da)  The P2P-da is a convergence of P2P protocols that provides a reliable, scalable overlay network among heterogeneous devices. The particular P2P protocols that contribute to the formation of the overall P2P-da functionality are: Epidemic (Gossiping) protocols [15], Gradient (Hierarchical) overlay protocols [16] and the Bit-torrent protocol[17].

1) Maintenance of the P2P Overlay Here a gradient overlay network based on a utility metric of  the relative performance capability of a HbH system is built and maintained through the "random neighbor" information sharing paradigm of gossiping protocols. A hierarchy of systems is thus formed with the less endowed elements on the outer edges and the more capable elements closer towards the centre of the LElA system (that is, the CBB).

2) Dissemination and Aggregation of Malicious Behaviour  Tnformation & Alerts Collaboration across the entire LElA system is facilitated  through message passing over the P2P-da. This enables more informed detection and evidence collection mechanisms. There are 2 main message types: Management messages, and security incident control messages. The former enables dissemination of information to keep detection mechanisms up-to-date, while the later facilitates reaction to the detection of a malicious event.

3) Incident response data collection This is triggered by the detection of malicious events via  the collective knowledge gained through collaborating HbH systems, the em-IDS and guest OS security mechanisms.

Correspondence with the Cloud-Based Backend-Differencing Engine (CBB-DE) filters out known system files through facilitating the hash comparisons. Primary analysis for IP addresses and hostnames on the data collected may result in triggering of other HbH systems to capture data.

The actual data collection procedure involves 3 stages:  a) Data Partitioning  Different data formats (memory dumps, logs, files, packet captures, disk images) are compressed and stored temporarily on the HbH system in a modified AFF4 data structure that contains simple RDF metadata describing the evidence. This data structure is termed as the Incident Data Archive (IDA). Each IDA data structure is partitioned in equal size pieces that will be referred to as shards. The shard is a signed and encrypted partition of the IDA analogous to the idea of a "piece" in the BitTorrent Protocol. A metadata file termed as the "reflection" (which corresponds to the BitTorrent Metadata file) is also created and sent directly to the     World Congress on Internet Security (WorldCIS-2013)  CBB. In this way the CBB acts as the "tracker" and "leaches" TDAs from participating HbH systems in the P2P-da, thus benefiting from the high throughput of the BitTorrent protocol  b) Shard Distribution  Shards are distributed to more capable neighbours (supporters), facilitated by the gradient overlay. Each time a shard is passed on it increases its "heat level". After a certain "heat" threshold (the "melting point") a HbH system is obliged to directly upload to the CBB, else an election procedure is initiated to determine which previously supporting HbH should be delegated the uploading task. For reliability reasons, HbH systems are only allowed to partake in uploading a certain number of IDA shards governed by the "dependency value".

c) Rapid fragment reconstruction  For a particular shard, downloads are initiated from all their respective supporter locations. This is done for redundancy purposes. Similar to the BitTorrent Protocol download, priority is given to the shards that are the least commonly available, that is, those that have the fewest recorded supporters.

In order to reconstitute the IDA, individual hashes of shards are verified as they are received, against that in the reflection. Several supporters upload at the same time, thus if a shard is in error, that from another supporter is taken. Once successfully transferred, shards are deleted from supporting HbH systems.

C. The Cloud-based Backend (CBB)System  The CBB system is a highly available, scalable, responsive, centralized back end storage service capable of storing large amounts of data in a homogeneous form. It is subdivided into 3 major components: The Storage System (SS), the Differencing Engine (DE) and the HbH Master Peers.

The Storage System (SS) is built upon the Hadoop HDFS architecture [18] that provides not only the raw storage capabilities but also scalability, availability, reliability and responsiveness. The Differencing Engine (DE) filters out known files before having them stored on the CBB. This is provisioned through the Mapreduce [19] capabilities supported by Hadoop. The DE also provides a query-response mechanism to the HBH systems with information on known benign data as part of the Master Known Data Hash-List (M-KDHL). The M? KDHL contains data about known files, memory processes, protocol flows, and log entries and thus enables their removal from IDAs being prepared. This reduces the size of IDAs before being stored on the Storage System (SS) of the CBB.

The HbH Master Peers are a particular set of well-endowed peers that are directly connected to the core CBB system (that is, the SS and DE) providing an interface to the rest of the LElA system through the P2P-da. They do not have other core functionalities unrelated to their LELA responsibilities and are essentially the backbone of the P2P-da and ultimately the provider of connectivity of the LELA system outwards to the other HBH systems. The HBH Master Peers also serve as the   central point through which system software updates and malicious event detection heuristics are originated from and disseminated outwards to the HBH systems in the wild.

D. The Law Enforcement Controller System  The Law Enforcement Controller is the main interface that law enforcement personnel interact with in order to perform their directed analysis for a particular digital investigation case.

Through it, a Law Enforcement Agent can initiate specific queries to the data sets stored on the CBB, thus retrieving detailed, structured information as well as new knowledge inferred through correlation of data originating from different sources that may help in solving a case. The aim of this is to automate otherwise manual tasks of correlating data from different heterogeneous sources in order to pose valid assertions based on the data that could assist a forensic analyst in performing their duties of making sense of digital artifacts.

This functionality is described in more detail by Dosis in [20].

Additionally, from the new found knowledge, patterns of malicious activities are to be learnt and stored. These Malicious Activity Patterns are to be used as feedback to the HbH systems in order to improve the detection capabilities of the inbuilt IDS's and thereby also improve the accuracy of collection of data of potential forensic evidentiary use.



V. PROOF OF CONCEPT EVALUATION AND RESULTS  A partial proof of concept of the LELA system was created.

It focused on the remote extraction and compression of disk data evidence from small scale devices over the Internet and the subsequent storage on a Hadoop cluster. Due to time constraints, the P2P-da and the HbH were not actually developed, however the networking capability between devices and the cloud storage, as well as the concept of privileged access to the device being captured, were maintained.

Four different small scale devices were used in testing and measuring the performance of the application. The table below outlines the specifications of the devices being captured.

TABLE!. SMALL SCALE DEVICE SPECIFICATIONS  Device Platform RAM Disk  Chumby Classic Busybox v 1.6.l 64MB 64MB  HTC Incredible S Android OS v2.3.3  768MB 1. 1GB (Gingerbread)  HTC MyTouch CyanogenMod 10.2 Alpha 768MB 4GB  4G Slide  Samsung Galaxy Android OS, v4.0.3  Tab 2 1GB 8GB (WiFi Only)  (Ice Cream Sandwich)  In evaluating the performance of the proof of concept LELA system, the time taken to perform the evidence capture on certain fixed of partition sizes was measured. The resulting data was recorded and is portrayed in the graph below.

World Congress on Internet Security (WorldCIS-2013)  o 500 1000 1500 7000 1500 '000 3500 4000 4500 Partition Size Used 1MB)  Figure 3. Perfonnance of the LElA Proof of Concept  The Chum by device was captured, but not graphed as its partitions were too small (32 MB each) to provide useful data.

From the graphs one can see a step-like curve in all of the cases. All curves also seem to begin with a more linear shape before a more exponential relationship takes over. This is particularly clear with the "HTC MyTouch 4G Slide". Tn general, it seems like there is more of an exponential relationship between the Partition Size and the File Transfer Time. One could posit that as the partition sizes increase, even to sizes substantially larger than those in the graph, the relationship will become ever more exponential. This is a particularly clear motivation for a more efficient data transfer mechanism such as a high-throughput P2P network between the device being captured and the eventual evidence storage location.



VI. CONCLUSION  Tn this study we outlined the plethora of problems that plague digital investigations making them cumbersome slow processes. The architecture of a comprehensive, more efficient cyber-Iaw enforcement platform was proposed. Finally, a small proof of concept application was developed and its performance analyzed.



VII. FUTURE WORK  Though this architecture is promising, several parameters within the communication protocols need further optimization.

A PKI infrastructure can be infused in the system in order to improve the security of the communication and storage facilities. The concept of privacy needs to be addressed within the scope of this solution. Finally, an experiment with a wider scope would be greatly desired in order to better drive this architecture towards becoming a reality.

