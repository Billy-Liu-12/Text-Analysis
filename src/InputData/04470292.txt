Connections between mining frequent itemsets and learning

Abstract  Frequent itemsets mining is a popular framework for pattern discovery. In this framework, given a database of customer transactions, the task is to unearth all pat- terns in the form of sets of items appearing in a sizable number of transactions. We present a class of models called Itemset Generating Models (or IGMs) that can be used to formally connect the process of frequent item- sets discovery with the learning of generative models.

IGMs are specified using simple probability mass func- tions (over the space of transactions), peaked at spe- cific sets of items and uniform everywhere else. Under such a connection, it is possible to rigorously associate higher frequency patterns with generative models that have greater data likelihoods. This enables a generative model-learning interpretation of frequent itemsets min- ing. More importantly, it facilitates a statistical sig- nificance test which prescribes the minimum frequency needed for a pattern to be considered interesting. We illustrate the effectiveness of our analysis through ex- periments on standard benchmark data sets.

1 Introduction  Frequent itemsets mining is a popular framework for pattern discovery. Originally proposed for market bas- ket analysis [1], the idea of discovering frequent item- sets (and deducing association rules) has since gained wide applicability [3] in many application domains.

This paper presents a formal connection between fre- quent itemsets mining and learning of generative mod- els for the data in the form of simple probability mass functions over the space of transactions. We define a class of models called Itemset Generating Models (or IGMs). Each IGM generates transactions by embed-  ding a specific pattern in a transaction according to a probability distribution that is peaked at the pattern in question, and is uniform everywhere else. In this man- ner, each itemset is associated with a specific IGM.

We prove that given any two itemsets, the IGM asso- ciated with the more frequent itemset is the one more likely to generate the database of transactions. Such a connection allows for a generative model-learning in- terpretation of the frequent itemsets mining process.

More importantly, it facilitates a formal statistical sig- nificance test, which prescribes the minimum frequency needed to regard an itemset as significant. This is use- ful because frequency threshold is a user-defined para- meter that can be very hard to fix in typical applica- tions. Simulation experiments show that our theoret- ically motivated thresholds are effective in discovering patterns embedded in the data.

In general, connecting pattern discovery frameworks with generative model-learning is a useful idea. There is a growing need for unified frameworks in data min- ing [8] that can simultaneously exploit efficient count- ing procedures in pattern discovery with the rigorous statistical basis of generative model-learning. Moti- vated by such considerations, in [6], a formal connec- tion was established between pattern discovery and model-learning in the context of temporal data min- ing. The ideas presented in this paper have a similar flavor, applied to the framework of frequent itemsets discovery in transaction databases.

The rest of the paper is organized as follows: Sec. 2 defines Itemsets Generating Models and presents the main results of the paper. The generative model- learning interpretation is discussed in Sec. 3. Sec. 4 describes the statistical significance test for itemsets.

Simulation experiments are described in Sec. 5 and Sec. 6 presents conclusions.

DOI 10.1109/ICDM.2007.83    DOI 10.1109/ICDM.2007.83    DOI 10.1109/ICDM.2007.83    DOI 10.1109/ICDM.2007.83    DOI 10.1109/ICDM.2007.83     2 Itemset generating models  We begin with some background. Let D = {T1, . . . , TK} be a database of (unordered) customer transactions at a store. Each transaction, say Ti, is a collection of items purchased by a customer in one visit to the store. A non-empty set of items is called an itemset. An itemset is denoted by a tuple, say ? = (A1, A2, . . . , AN ), where each Aj is an item (or symbol) from some finite alphabet, say A. Since ? has N items, it is referred to as an N -itemset. Trivially, each transaction in the database is an itemset. How- ever, an arbitrary itemset may or may not be contained in a given transaction, Ti. The frequency of an itemset is the number of transactions in D that contain it. An itemset whose frequency exceeds a user-defined thresh- old is referred to as a frequent itemset. The Apriori algorithm [1] exploits the so-called anti-monotonicity property of the itemset frequency to carry out an effi- cient level-wise search for all frequent itemsets. Over the years, many other algorithms have been proposed for frequent itemsets mining (e.g., see [4] for a compar- ison of several approaches). The analysis we present in this paper is independent of the algorithm used for frequent itemsets mining.

2.1 Data likelihood under IGMs  Let A denote an alphabet (universal set of items) of size M , ? = (A1, . . . , AN ) be an N -itemset, and the tuple T = (t1, . . . , t|T |) denote a transaction over A.

An Itemset Generating Model (or IGM) is specified by a pair, ? = (?, ?), where ? ? A is an N -itemset, referred to as the ?pattern? of ?, and ? ? [0, 1] is re- ferred to as the ?pattern probability? of ?. The class of all IGMs, obtained by considering all possible N - itemsets over A, and by considering all possible pattern probability values ? ? [0, 1], is denoted by I.

The probability model according to which an IGM ? = (?, ?) generates a transaction T , is as follows: We refer to (the power set) 2?, as the pattern space, and to 2?? as the noise space (?? denotes the complement of ?, i.e., A \ ?). The space of all possible transactions is 2A (= 2? ? 2??). The IGM generates two itemsets, denoted by ??p ? 2  ? and ??n ? 2 ??, independent of each  other, and according to the following probability mass functions:  ??p =  {  ? wp ?  ?? ( ? wp (  1?? 2N?1  )  (1)  ??n = ? ?? ? ?? wp  (  2M?N  )  (2)  The full transaction generated is given by T = (??p , ? ? n ).

Eq. (1) is a probability mass function over 2? that is  peaked at ??p = ? (so long as ? > ( 2 )  N ), and is uniform everywhere else, and Eq. (2) is a uniform probability distribution over 2??. Since ??p and ?  ? n are independent  of each other, for ? ? (0, 1), the probability of T = (??p , ?  ? n ) under ?, can be written as:  P [T | ?] =  (  1 ? ?  2N ? 1  )1?z?(T ) ( 1  2M?N  )  (3)  where, z?(?) indicates set containment: z?(T ) = 1, if ? ? T , and z?(T ) = 0, otherwise. For the case, ? = 0, we have  P [T | ?] =  { (  (2N?1)2M?N  )  if z?(T ) = 0  0 if z?(T ) = 1 (4)  Similarly, if ? = 1, we have  P [T | ?] =  {  0 if z?(T ) = 0 (  2M?N  )  if z?(T ) = 1 (5)  These extreme cases correspond to two distinct (triv- ial) situations: ??p = ? in all transactions that the  model generates, and ??p 6= ? in all transactions that the model generates.

An IGM can generate a database of transactions by drawing iid samples according to Eq. (3). Intuitively, if ? is large, the IGM would generate data with ? ap- pearing in many transactions. This makes IGMs a rea- sonable class of models to use for connections with fre- quent itemset mining. We formalize these ideas in the subsections to follow.

Consider D = {T1, . . . , TK}, a database of K trans- actions. Each Ti is a collection of items over A. Us- ing Eq. (3), the expression for likelihood of D, under ? = (?, ?), ? ? (0, 1), is as follows:  P [D | ?] = ?f? (  1 ? ?  2N ? 1  )K?f? (1   )K(M?N)  (6)  where, f? = ?K  i=1 z?(Ti) is the frequency of ? in D.

2.2 Class of IGMs with a fixed ?  Initially, we restrict our attention to IGMs with a fixed pattern probability ? ? (0, 1). Later, in Sec. 2.3, we consider the full class of IGMs with all possible values for the pattern probability parameter.

Definition 1. The subclass I?, is defined as the collec- tion of IGMs (out of I) with a fixed pattern probability parameter ? ? (0, 1).

Definition 2. Consider an N -itemset, ? = (A1, . . . , AN ) (? ? 2  A). The IGM associated with itemset ? is given by ?? = (?, ?) ? I?.

Theorem 1. Let D be a database of transactions over alphabet A. Let ? and ? be two N -itemsets that occur in D, with frequencies f? and f? respectively. Consider the class, I?, of IGMs with ? > (  2N  ), and let ?? and ?? be the corresponding IGMs (from I?) that are as- sociated with ? and ? according to Definition 2. Then we have, P [D | ??] > P [D | ?? ] if and only if f? > f?.

It follows from Theorem 1 that, if ? is the most frequent N -itemset in D, then for any other N -itemset, ?, the data likelihoods under ?? and ?? must obey: P [D |??] ? P [D |?? ]. This gives us the next theorem.

Theorem 2. Given a database D of transactions over an alphabet A, the maximum likelihood estimate over the class of IGMs, I?, with ? > (  2N  ), is the IGM cor- responding to the most frequent N -itemset in D (with correspondence as prescribed by Definition 2).

In other words, when ? > ( 1 2N  ), frequencies of N - itemsets in D are ?sufficient statistics? for maximum likelihood estimation over IGMs in I?. For I? with ? < ( 1 2N  ), Theorems 1 & 2 do not hold. When ? < ( 1 2N  ), data likelihood decreases with increasing frequency of the corresponding N -itemset.

Remark 1. Given a database, D, of K transactions over alphabet A (of size M) and an IGM ? = (?, ?), with ? of size N and ? ? ( 1  2N ), we have  P [D | ?] ?  (   2N ? 1  )K (   )K(M?N)  . (7)  Further, for large values of N , this upper bound ap- proaches ( 1  2M )K , which is simply the probability of D  when the transactions are drawn iid according to a uni- form distribution over the whole of 2A.

For full proofs of these and other theorems that fol- low, please refer to [5].

2.3 The final itemsets-IGM association  Remark 1 shows that IGMs with ? ? ( 12 ) N cannot  be meaningfully associated with frequent itemsets. For the itemset-IGM connection to be useful, we need a way to estimate ? from the data and associate each itemset with an IGM from the full class, I, of IGMs over A.

This section describes how this is done.

Definition 3. Given an N -itemset, ? = (A1, . . . , AN ), the subclass I(?) is defined as the collection of all IGMs out of I of the form ? = (?, ?), with ? ? [0, 1].

It can be shown that the IGM specified by the tu- ple (?, f?/K), has the highest likelihood for D, over  the class, I(?), of IGMs defined using the N -itemset ?. However, (f?/K), may or may not be greater than (1/2N ). Based on this fact, and bearing in mind Re- mark 1, we prescribe the final itemset-IGM association according to the following definition.

Definition 4. Consider an N -itemset, ? = (A1, . . . , AN ) (? ? 2  A). Let f? denote the frequency of ? in the given database, D, of K transactions. The itemset ? is associated with the IGM, ? = (?, ??), with ?? = (  f? K  ), if ( f? K  ) > ( 1 2N  ), and with ?? = 0 otherwise.

Theorem 3. Let D be a database of K transactions over the alphabet A. Let ? and ? be two N -itemsets that occur in D, with frequencies f? and f? respectively.

Let ?? and ?? be the corresponding IGMs (from I) that are associated with ? and ? according to Defini- tion 4. If ?? and ?? are both greater than (  2N  ), then we have, P [D | ??] > P [D | ?? ] if and only if f? > f?.

Finally, under the final itemset-IGM association of Definition 4, Theorem 3 gives rise to Theorem 4 about maximum likelihood estimation for D over I, the class of all IGMs (defined using patterns of size N).

Theorem 4. Let D be a database of K transactions over alphabet A (of size M). Let ? be the most fre- quent N -itemset in D and let ?? = (?, ??) be the IGM corresponding to ? (as prescribed by Definition 4). If  P [D |??] > (  2N?1  )K (   )K(M?N) , then ?? is a maxi-  mum likelihood estimate for D, over the class, I, of all IGMs.

To summarize, the final association between item- sets and IGMs is prescribed by Definition 4, which fixes the pattern probability parameter of the IGM associated with an itemset, based on its frequency in the data. This is the association that we use in all our analysis. The main result connecting itemsets and IGMs is given by Theorem 3. This connection is tight, in the sense that under the conditions of Theorem 3, the itemset-IGM association of Definition 4 ensures that ordering with respect to frequencies among N -itemsets over A is preserved as ordering with respect data like- lihoods among IGMs in I.

In the following sections we discuss some interesting consequences of the itemsets-EGH connetion. First, in Sec. 3, we now have a generative model-learning in- terpretation of frequent itemsets discovery. Then, in Sec. 4, we prescribe a statistical significance test for itemsets discovered in the data.

3 Generative model-learning interpre-  tation  Each IGM generates transactions by embedding its corresponding itemset with a certain probability. The- orem 4 states that (under reasonable conditions), if ?1 is the most frequent N -itemset, then ??1 is an MLE for the data (over I). A typical database would contain several transactions, where different transactions may be generated by different IGMs. ??1 , however, regards all items other than those in ?1 as noise. So we re- move ?1 from all transactions in the data, and obtain the next most frequent itemset, say ?2, in the data.

??2 will now be an MLE for this reduced part of the data. Once again, ??2 regards all items outside of ?2 as noise. So, we next remove ?2 from the data, obtain the next most frequent itemset, ?3, and so on. In this manner, we can interpret frequent itemsets mining as a generative model-learning exercise.

There is another interesting aspect to describing the data using such generative models. Based on the itemsets-IGM connections, it is possible to learn a mix- ture of IGMs for the data. More work is needed to resolve the details of these mixture models. However, one can see that in principle our theoretical connections can facilitate such modeling.

4 Significance test for itemsets  There are many ways to define and measure signif- icance of itemsets and/or association rules. For exam- ple, chi-squared tests have been used to fix minimum support thresholds [10], or to prune/summarize associ- ations [7], etc. Such tests typically rely on the normal approximation to the binomial, and in some situations, e.g. when the transactions data is sparse, this approx- imation breaks down and the tests become ineffective.

In [2], a baseline frequency is defined for each itemset and the ratio of actual-to-baseline frequencies is pro- posed as a measure of significance. In [11], union-type bounds are used to estimate probability of an itemset under a random Bernoulli model. These bounds, how- ever, require both alphabet-size and itemset-size to be sufficiently large, and this somewhat limits applicabil- ity of the test. We adopt a different approach to assess- ing significance of itemsets based on generative models for transactions in the form of IGMs.

To assess statistical significance of an itemset, we directly use the itemset-IGM connections and set-up a hypothesis test to compare data likelihood of the as- sociated IGM against likelihood under a random uni- formly distributed model. Given a user-defined level  of the test (i.e. for a given probability of wrongly as- sessing an itemset as significant), the test yields a fre- quency threshold for frequent itemset discovery. The tight connections between itemsets and IGMs, based on the theorems of Sec. 2 show that IGMs are a good class of models for embedding patterns in transactions.

This makes our significance test reasonable, since the test tries to reject the hypothesis of a random iid model on the evidence of at least one model better than ran- dom chance. Due to space constraints, we omit details of how the test is derived and present only the final testing procedure here.

Given an N -itemset, ?, we first fix the allowed level of the test at, say, ?. Since K and N are known, we compute ? using,  ? = K  2N +  ?  (  K  2N  )(  1 ?  2N  )  ??1(1 ? ?) (8)  where, ??1(?) denotes inverse of the cumulative distri- bution function (cdf) of the standard normal random variable. Our significance test now declares ? as sig- nificant, if f? > ?.

Note that we now have a size-dependent frequency threshold for itemsets (with smaller itemsets requiring larger frequencies to be regarded as significant). Also note that, for typical values of N and K (i.e. with K ? N), ? changes very little over 0 < ? ? 0.5.

This is because, ( K2N ) dominates the expression for ? in Eq. (8). This allows for automatically fixing the threshold for N -itemsets at ( K  2N ). Sometimes, we may  want to use higher thresholds (corresponding to very small ? values), but in the absence of any information about the data, ( K  2N ) is a good initial threshold to try.

5 Experimental results  This section presents results obtained on some benchmark synthetic data from IBM Almaden Quest group?s data generator1 as well as on proprietary data obtained from attack graphs produced by the Netra [9] configuration analysis tool. Our experiments seek to illustrate the utility of our theoretical connections. To this end, synthetic data is particularly useful, since we can vary data generation parameters and assess per- formance by checking if the patterns embedded by the data generator are indeed discovered during the mining process.

The first experiment illustrates effectiveness of our theoretically motivated frequency thresholds. We run  1The IBM synthetic data generator was obtained from http://www.almaden.ibm.com/cs/projects/iis.

10 20 30 40 50 60 70 80 90 100           %  o f  e m  b e  d d  e d  p a  tt e  rn s f  o u  n d  s ig  n if ic  a n  t  Number of embedded patterns  Figure 1. Percentage of embedded patterns  found significant in synthetic data with vary-  ing numbers of embedded patterns. (100K  transactions, avg. transaction length of 20,  1K items and avg. size of embedded patterns  of 7. No. of embedded patterns is varied from  10 to 100.

the frequent itemsets mining algorithm on several syn- thetic data sets (generated using the IBM Quest data generator for different parameter values). Each data set is associated with a collection of patterns that the generator used as potential correlations to embed during the data generation process (Here, we refer to these correlations as embedded patterns). We now ask whether these patterns come up as significant patterns according to our significance test of Sec. 4. We con- sider an N -itemset as significant (in a data set of K transactions) if its frequency exceeds ( K2N ).

Fig. 1 plots the percentage of embedded patterns found significant as a function of the number of embed- ded patterns in different synthetic data sets. To study the effect of number of embedded patterns, we gener- ate several synthetic data sets by varying the number of embedded patterns between 10 and 100. The plot shows that a very high percentage of embedded pat- terns is always found significant by our analysis. Simi- lar results were obtained when we studied the effect of varying average size of embedded patterns, number of transactions, average length of transactions, and num- ber of items as well. We omit these results here due to space limitations. Note that the IBM synthetic data generation model [1] is quite different from our IGMs.

Despite this, our significance test based on IGMs is able to reliably detect the patterns embedded in the synthetic data.

In a second experiment, we varied frequency thresh-  0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.01            N u  m b  e r  o f  fr e  q u  e n  t it e  m s e  ts  Frequency threshold      2?itemsets  3?itemsets  Figure 2. No. of frequent 2 & 3-itemsets  v/s frequency threshold in IBM Quest syn- thetic data (100K transactions, avg. trans-  action length of 15, 1K items, avg. size of  embedded patterns of 10 and 10 embedded  patterns). The vertical lines indicate theoret-  ical frequency thresholds, namely, ( 122 ) for 2- itemsets & ( 123 ) for 3-itemsets. The graph for 3-itemsets meets Y-axis at 3197.

old over a range, and, for each case, recorded the num- ber of frequent N -itemsets discovered, for different N .

Imagine a data set in which no specific patterns are embedded (or equivalently, a data set in which all pat- terns are equally likely to appear in the transactions).

In such a case, all 2-size (or 3-size) patterns would have very low (and also very similar) frequencies. Thus, very low frequency thresholds would lead to a combinatorial explosion of frequent itemsets. This would continue to be the case even as the threshold is increased until a critical point, beyond which, no pattern would be fre- quent (since the data is inherently random). Even for data sets in which specific patterns are embedded, the behavior would be very similar for low thresholds, but as the threshold is increased, only the specific patterns embedded in the data would remain frequent. So, a graph of the number of frequent itemsets versus the fre- quency threshold would ?level-off? for larger frequency thresholds.

Figs. 2-3 plot the number of frequent 2 & 3-itemsets discovered under various frequency thresholds for one synthetic dataset using the IBM Quest data generator, and for one proprietary data set from Netra. Theo- retical thresholds are indicated by vertical lines. The graphs show that for very low threshold values, almost all possible frequent 2 and 3-itemsets become frequent.

As the threshold increases, the number of frequent 2- itemsets and 3-itemsets decrease, and the plots tend     0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.01        N u  m b  e r  o f  fr e  q u  e n  t it e  m s e  ts  Frequency threshold      2?itemsets  3?itemsets  Figure 3. No. of frequent 2 & 3-itemsets ver-  sus frequency threshold in Netra data. The vertical lines indicate theoretical frequency  thresholds, namely, ( 122 ) for 2-itemsets & ( 23 )  for 3-itemsets.

to level-off beyond the theoretical thresholds. Similar plots were obtained when the parameters of synthetic data generation were varied as earlier.

In general, setting the frequency threshold for fre- quent itemsets mining can be a tricky exercise. The plots in Figs. 2-3 show that while a low threshold might lead to a combinatorial explosion of the number of pat- terns, a higher threshold might miss significant pat- terns present in the data. Our significance test based on the itemsets-IGM connections help alleviate this problem. The key feature of our significance test is the size-dependent frequency thresholds for itemsets.

If we want to discover frequent itemsets up to size N (in data with K transactions) we use ( K2N ) as the threshold for the mining process. However, an l-itemset (with l < N) discovered as frequent according to the ( K 2N  ) threshold is not considered interesting unless its  frequency exceeds (K 2l  ). Our experiments show that, using these theoretically motivated thresholds, we can expect to avoid selection of most of the noisy patterns and identify only those correlations that are stronger than random chance.

6 Conclusions  In this paper, we have presented a theoretical con- nection between the process of frequent itemset dis- covery and learning of generative models. Based on our connection we derived a significance test for item- sets that uses just the frequencies of itemsets as test statistics. The test leads to size-dependent frequency thresholds for itemsets, with smaller itemsets requiring  greater frequencies to be considered significant. We ex- perimentally validated our analysis both on benchmark as well as proprietary data sets.

There are many other aspects of the itemsets-IGM connection that would be worthwhile investigating fur- ther. One is the learning of a mixture model for the data in the form of a convex combination of IGMs.

Another interesting possibility is to modify the signifi- cance test to incorporate the simultaneous assessment of significance of (not one but) a set of itemsets. This can lead to more reliable detection of significant pat- terns at considerably lower frequencies. We will ad- dress some of these aspects in our future work.

