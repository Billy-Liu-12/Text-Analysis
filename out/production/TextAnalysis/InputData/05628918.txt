Contextual Approach to Data Discretization

Abstract This paper presents a new discretization  algorithm that takes into account the behavior of asso-  ciated variables. Indeed, in the context of association  rules extraction, for example, the goal is to find inter-  connected data. Thus, instead of computing numeric  variables independently we choose to compute them  in their context, i.e. in association with the rest of the  variables to consider. The proposed approach is based  on the joint use of statistical constraints (objective  measures) that are in charge of determining the real  significance of the relationships between variables  and human constraints (subjective measures) defined  by the domain expert and concerning thresholds  determination.



I. INTRODUCTION  The goal of each discretization algorithm is to  find a small number of homogeneous and repre-  sentative classes of data. The obtained classes con-  stitute the starting point for knowledge extraction  algorithms.

In this paper, we distinguish two types of dis-  cretization: out-of-context discretization a.k.a. tra-  ditional and contextual discretization.

Traditional discretization deals with variables in-  dependently from the rest of the variables set,  i.e. without considering the existence of potential  interactions between them.

Choosing a discretization method is not a trivial  task. As noted by [12], the number of classes  depends on several points: properties of the dis-  tribution, visual constraints and also on the aimed  goals. [12] give an overview of different types  of discretization algorithms - for instance, some  are guided by the user, where continuous ranges  are divided into sub-ranges by the user-specified  parameter or clustering algorithms such as k-means  (specifying number of clusters), this is called super-  vised discretization [5]. Of course if no class infor-    mation is available, unsupervised discretization is  the sole choice. In this case, results are correlated  with the degree uniformity of the continuous values  distribution.

The approach proposed by [8] is one of the  approaches that contributed in this direction. In  the context of mining association rules, they based  the partitioning on the distribution of values of  numerical variables only.

Indeed, the idea of their approach is to achieve a  balanced search over and under the average of each  examined numeric attribute to verify the validity  compared to the average of the adjacent regions,  pair by pair, and merge them in the affirmative case.

In order to merge these regions, [8] relied on a  comparison measure calculated using the average of  all or part of the numeric attributes of the database  from which is subtracted a step representing a  minimum difference. This adjusted average repre-  sents the minimum threshold over which inequality  between populations is significant.

Contribution In this sitting, we present a new  method for partitioning numeric variables that takes  into account the existing interactions between vari-  ables values. The proposed discretization allows  to form distinct classes following the associated  variables.

The construction of these classes is based on a set  of predefined parameters. This discretization uses  both objective and subjective measures. Objective  measures are statistical measures that quantify the  degree of significance and relevance of links be-  tween associated variables. Human considerations  are bounded by subjective knowledge of the field.

Moreover, this method uses tables to synthetically  summarize the way variables interact by highlight-  ing interesting zones. Therefore, it reduces the  number of records retaining only valid ones relative  to the predefined constraints set.

2010 Fifth International Multi-conference on Computing in the Global Information Technology  978-0-7695-4181-5/10 $26.00  2010 IEEE  DOI 10.1109/ICCGI.2010.32     Paper organization We first explain traditional  discretization in Section 2. Section 3 introduces  the proposed approach. Section 4 defines the con-  textual discretization method. Section 5 describes  the consolidation process and presents a complete  example with the obtained results on UCI database.

Conclusions are presented in Section 6.



II. TRADITIONAL DISCRETIZATION  A discretization is said to be traditional when it  is done in advance and remains unchanged whatever  may be the variables to which it is associated.

Let Y , Y ? and Z be numeric variables, and let  f be the density function of variable Z. Numeric  variables Y , Y ? and Z take their respective val-  ues in the intervals [ymin; ymax], [y?min; y?max] and  [zmin; zmax]. We denote by zi, zj and zk values of  Z, by yi yj and yk values of Y and by y?i, y?j and  y?k values of Y ?. The discretization chosen by the  expert for Z is represented by two intervals I1 =  [zmin; zi] and I2 =]zi; zmax]. This discretization is  not regular, i.e. the intervals are neither of equal  width nor of equal frequency.

Figure 1 shows that the obtained distribution is  uniform, except in the interval [zj ; zk] where there  is a high concentration of individuals with Y taking  its values in the interval [yj ; yk].

Similarly, figure 2 combines variable Z to  variable Y ?. In this case, individuals are uniformly  distributed according to the values of Z, except  in the interval [zmin; zi] where there is a high  concentration of individuals with Y ? taking its  values in the interval [y?j ; y?k].

Z  Y  I1= [z min;z j]  I2 = ]z j;z k]  I3= ]z k;z max]  zmin z j zmaxzk  f  ymin  ymax    yk  yi  Fig. 1. Distribution of individuals along Y and Z.

The traditional discretization can be processed  using a large number of methods. The clustering  Z  Y  I 1 = [zmin ;z i]  I 2 = ]z i;zmax ]  f  ymin  ymax  y  k  yi  zmin zmaxz i  Fig. 2. Distribution of individuals along Y and Z.

method seems to be one of the most used. This  technique enables the forming of classes called  clusters, where the goal is to maximize intra-class  similarities and to minimize inter-classes ones. As  shown in the illustration of [10] given by figure 3,  where X is a numerical variable and f its density  function and where clusters are numbered from C1  to C4, each cluster Ci consists of a centroid noted gi  around which the closest elements in terms of intra-  class distance gravitate. The constituted clusters  revolve around a gravity centre labeled g in terms  of intra-class distance.

C1  C4  C3  C2  -classe  inter-classe  g1  g2  g3  g4  g  intra  distance  s distance    f  X  Fig. 3. Clustering example schema.

The result of clustering depends largely on both  chosen algorithm and fixed distance. One can distin-  guish two families: merging algorithms and splitting  ones. The merging method considers each value as a  cluster and then these clusters are merged to form  the largest possible clusters. The splitting method  starts from a unique cluster constituted of all values  of the considered variable and then iteratively splits  it progressively to form small clusters.

The common point between the various tradi-  tional methods of discretization is that the variables  are computed one at a time whatever their effective  is regardless of the applicative context. That means   that potential interactions caused by the presence of  other variables are ignored. This kind of discretiza-  tion does not restore adequate intervals, faithful to  the existing links in the database.

In the following section we present our approach  which allows the application context to be taken  into account.



III. PROPOSED APPROACH  This section presents our discretization approach  that allows for saving, at various degrees, the exist-  ing relationships between databases variables.

Our approach is based on a set of experts  considerations and statistics. The algorithm allows  the domain expert to achieve a preliminary dis-  cretization (rough classification) in order to reduce  the numeric variables modalities by specifying a  maximum threshold noted modMax.

If the number of states is below the predefined  maximum threshold, then the numeric variable is  discretized directly by following the contextual  discretization process. Otherwise, a traditional dis-  cretization is applied. Two cases are considered:  1) The number of distinct states of the numeric  variable is lower then mod_Max: each value  is to be considered as a new variable;    2) The number of distinct states of the numeric  variable is at least equaled to mod_Max:  a preliminary (traditional) discretization is  performed using a clustering method based  on the nearest neighbor. Relative to this clus-  tering, the neighborhood notion is attached  to a maximum distance noted ecart_Max  predefined by the user.

The following example is derived from an UCI  databases called Wages [6]. This database contains  534 records and 11 variables where 5 of them are  numeric.

Lets consider the users predefined parameters,  maximum states fixed to mod_Max = 10 and  maximum distance fixed to ecart_Max = 10%.

The numeric variable wage has 238 distinct  values, therefore it is preliminarily discretized by  performing a clustering algorithm.

Table I lists the characteristics of built clusters for  the numeric variable wage using the predefined pa-  rameters and consolidated with a strong constraint  that fixes the minimum effective in a cluster to five  (5).

clusters effective min max  C1 94 2.85 4.70  C2 145 4.75 7.14  C3 197 7.30 12.67  C4 94 13.00 26.29  TABLE I  CLUSTERING OF THE NUMERIC VARIABLE "WAGE".

This table restitutes the four clusters built for the  variable "wage" : "wage = [2.85; 4.7]", "wage =  [4.75; 7.14]", "wage = [7.3; 12.67]" and "wage =  [13.00; 26.29]".

Notice that the total effective represented by  the clusters is 530. The missing records are not  significant enough to be kept (three records where  the wage is less then 2.84$/h and a sole record  where the wage =44.50$/h).

These results are not definitive. In this paper  we consider the variables interactions therefore,  when all the numeric variables are discretized, we    compare the sets of the considered records in order  to keep only common records.

The aim of our approach is to obtain a homo-  geneous partitioning based on variables behavior  when they are gathered. We present the contextual  discretization method in the next section.



IV. CONTEXTUAL DISCRETIZATION  When the discretization is carried out regarding  the associated variables behavior, this is called  contextual discretization, which is based on the  number of distinct values for a numeric variable.

Let us consider the association formed by item-  sets X and Y of the database ?. One can distinguish  three behaviors based on their dependency states:  1) incompatibility: ((X)ei?? ? (Y )ei?? = ?);  2) independence: (Pr(X/Y ) = Pr(Y )) i.e. the  previous appearance of X does not change  the probability of appearance of Y ;  3) logical implication: ((X)ei?? ? (Y )ei??).

Where (X)ei?? and (Y )ei?? are the ith transac-  tion containing respectively X and Y . These three   ? ?

(Y)  ?

(X)  Independence  between Y and X  Logical implication  Pr(Y) 10  Zone  rpulsive  entre M et Z  Zone  attractive  entre M et Z  Incompatibility  Pr(Y /X)  Repulsive zon  betwe n X and Y  Attractive zone  b tween X and Y  1 2 3  e    e  (Y)e  (Y)e(X)e (X)e  Fig. 4. Different states of dependency between X and Y.

states delineate two interesting zones: repulsion and  attraction. In order to highlight these zones, we use  a tabular pruning and regrouping strategy based on  contingency tables and coordinated with statistical  measures.

This independence can be quantified using the  contingency table of the variations [13], [11], which  is computed from the contingency tables of the-  oretical and observed elements. The contingency  tables allow us to determine whether the variables  are independent or not. This is done by computing  the difference between observed and theoretical  effectives.

The contingency table of variations table II quan-  tifies the independence between two characters X  and Y , where X has t distinct values and Y has r  distinct values.

X = x1    X = xj    X = xt  Y = y1n11 ?

n1n1  N  n1j ?

n1nj  N  n1t ?

n1nt  N     Y = yi ni1 ?

nin1  N  nij ?

ninj  N  nit ?

nint  N       Y = yrnr1 ?

nrn1  N  nrj ?

nrnj  N  nrt ?

nrnt  N   0    0    0 0  TABLE II  VARIATIONS CONTINGENCY TABLE OF THE ASSOCIATION  (X-Y).

In this table, nij is the number of individuals  satisfying the ith value of character X and the jth  value of character Y . Moreover,  ni. =  r?

j=1  nij , n.j =  t?

i=1  nij , n =  t?

i=1  ni. =  r?

j=1  nj..

The theoretical frequency (under independence  assumption) for a sample of size N is ni.n.j  N  .

If (nrj = nrnjN ) then X and Y are independent,  otherwise the independence hypothesis is rejected.

If (nrj ? nrnjN > 0) then the variables are in  attraction otherwise, they are in repulsion as shown  in figure 4.

Table III gives rounded variations computed from    the association (education?wage) where null cells  are ignored.

education  wage [6;7] [8;9] [10;13] [14;18]  [2.85; 4.70] 3 4 15 -22  [4.75; 7.14] 1 3 8 -12  [7.30; 12.67] -2 -3 1 5  [13.00; 26.29] -1 -4 -25 30  TABLE III  VARIATIONS CONTINGENCY TABLE OF THE ASSOCIATION  (EDUCATION-WAGE)  The question now is to know whether the vari-  ations are pure coincidence or whether they are  expressing a significant dependency between the  two variables. So in order to enable the submitted  results of the contingency variations table, we  perform further tests designed to rate the variations  degree of significance. The purpose of these tests is  to prune those cells with low degree of significance  from the initial contingency variations table.

To measure the significance of the relationship  between the two itemsets, we carry out one of  the most common tests: the ?2 attributed to (Karl  Pearson, English mathematician, 1857 to 1936).

This measure is widely used in data analysis and  helps to determine whether the two variables X  and Y are significantly correlated depending on the  observed variations of the contingency table.

Table IV summarizes the computed ?2 of the  association (education?wage). Values in the cells  represent the local ?2. Their sum gives the total  quantity of ?2.

The most significant deviation equals 26.47059  (302/3). This over-representation is the ratio be-  tween the square of the difference found in the  variations contingency table and the theoretical  contingency table of the association (education  = [14;18]- wage = [13.00;26.29]). The value of  global ?2 equals 81.026.

education  wage [6;7] [8;9] [10;13] [14;18] Total    [2.85; 4.70] 9.00 3.20 4.24 14.23 -  [4.75; 7.14] 0.50 1.28 0.79 2.72 -  [7.30; 12.67] 1.33 0.90 0.01 0.35 -  [13.00; 26.29] 1.00 3.20 11.79 26.47 -  Total - - - - 81.026  TABLE IV  ?2 OF THE ASSOCIATION (EDUCATION-WAGE).

Compared with the ?2 table associated to conven-  tional risk threshold noted ?, whwich is commonly  fixed to 5%, the critical value equals 16.92. This  means that the dependency between education and  wage is proven.

One problem remains: the given result is global,  and we are interested in a local diagnostic to  indicate the cells that contribute the most in the  ?2. To deal with this problem, we use the adjusted  standardized residual [4] noted RSA.

RSA (Equation 3) corresponds to the ratio be-  tween the adjusted residual of Haberman [2] (Equa-  tion 1) and the variance noted var (equation 2).

Equation 1 restitutes the adjusted residual of  Haberman which equals the standardized residual  of Pearson divided by standard error.

RA(xi, yj) =  nij ?

ninj  N?

ninj  N  (ni/N)(1? nj/N)  (1)  V ar(xi, yj) = (1?

ni  N  )(1?

nj  N  ) (2)  RSA(xi, yj) =  RA(xi, yj)  V ar(xi, yj)  (3)    Table V gives the set of intervals in which the  variations are not significant. These intervals are  computed based on the risk ?.

RSA ?

[?1.65;+1.65] 10%  [?1.96;+1.96] 5%  [?2.58;+2.58] 1%  [?3.29;+3.29] 0.1%  TABLE V  INTERVALS IN WHICH NON-SIGNIFICANCE IS DETECTED.

When the RSA is applied to the association  (education ? wage) with ? = 5%, we obtain the  results listed in table VI.

education  wage [6;7] [8;9] [10;13] [14;18]  [2.85; 4.70] 1.83 1.69 3.88 -4.94  [4.75; 7.14] 0.41 0.76 1.37 -1.88  [7.30; 12.67] -0.72 -0.62 0.17 0.59  [13.00; 26.29] -0.99 -1.49 -6.35 6.56  TABLE VI  ADJUSTED STANDARDIZED RESIDUAL OF THE ASSOCIATION  (EDUCATION-WAGE).

The negative values denote under-  representativeness, and the positive ones over-  representativeness.

We can now transpose these results to initial  variations contingency, table III.

Table VII represents the enhanced variations  contingency. In order to notice the differences, all  grayed cells are pruned thanks to the RSA.

education  wage [6;7] [8;9] [10;13] [14;18]  [2.85; 4.70] 3 4 15 -22  [4.75; 7.14] 1 3 8 -12  [7.30; 12.67] -2 -3 1 5  [13.00; 26.29] -1 -4 -25 30  TABLE VII  ENHANCED CONTINGENCY TABLE OF VARIATIONS FOR THE  ASSOCIATION (EDUCATION-WAGE).

This table is the starting point of the consolida-  tion process described below.



V. CONSOLIDATION PROCESS  The first step to build the largest zones is to  scan the enhanced contingency table of variations  looking for the cell with the highest absolute value.

This cell constitutes the first rectangle noted R1  with the coordinates of upper left point pl and lower  right point pr .

The second scan compares all the values in the  immediate neighborhood of R1. The expansion is  performed in descending fashion, i.e. starting from  the largest value with similar behavior of the con-  stituted rectangle (+,-). The rectangle R1 is then  expanded to the new coordinates.

The expansion of the formed rectangle continues  by performing the summation of its vicinity cells  values. We start by including cells whose values  sum is the largest. The expansion stops once meet-  ing null cells and the consolidation process ends  when all maximal rectangles are formed. These  results allow restricting the number of records to  be processed by only keeping those retained after  the consolidation phase.

When applying this process to the enhanced  contingency table of variations resulting from the  (education-wage) taken from Wages database,  only would be considered the records included  in the clusters "education=[10;13]" or "educa-  tion=[14;18]" associated to "wage=[2.85;4.70]" or  "wage=[13;26.29]".

Notice, in the case of the association  (education ? wage), that the consolidation  process enables to focus on 172 records instead of  526 (32%).



VI. CONCLUSIONS  In this paper, we describe our discretization ap-  proach focusing on the potential interactions be-  tween a databases variables. This approach makes  it possible to carry out a contextual discretization  maximizing the informativeness and highlighting  the variables dependencies.

The use of a tabular representation is particu-  larly interesting in the case of numeric variables    where knowledge is synthetically summarized. The  formed attractive and repulsive zones, based on  human considerations and statistics, guarantee that  potentially interesting knowledge will be discovered  with decreasing by the way the search space.

