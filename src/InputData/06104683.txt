A New Classification Algorithm Based on Certainty Architecture

Abstract?This paper proposed a kind of  classification algorithm which brought in certainty  factors of expert system as certainty theory proof  based on certainty and support architecture aimed  at the problem that classical association rules  algorithm based on confidence and support  architecture couldn?t generates association rules  accurately facing some questions on mathematical  statistics. With experiment verified, this  classification algorithm based on certainty and  support architecture has a better performance  improved the accuracy and perceptiveness of  association rules effectively.

Keywords- association rules; certainty; confidence

I. INTRODUCTION  Association rules describes certain relationships of the set of attribute items in the database, which is not instructive and simple[1].

Since the classic Apriori algorithm was proposed, the domestic and foreign scholars have focused on the efficiency of the association rules. For instance, Mannila, Toivonen and Verkamo introduced the technology of decision tree pruning to the classic association rules theory[2].

Park, Chen and Yu proposed the association rules algorithm based on hash [3]. Savasere, Omiecinski and Navathe also proposed the association rules algorithm based on partition [4].

Although the previous studies improved the mining efficiency of association rules to some  extent,  and reduced the space and time complexity greatly of the association rules[5].

However, it has not done much to improve the framework system of the association rules algorithm based on the support and confidence[6]. this paper proposes a new association rules algorithm with framework system based on certainty and support architecture. This algorithm introduces a certainty factor in expert system to measure the certainty of association rules. It accurately describes the relationship between the conditional probability and the prior probability.

The algorithm not only resolves the problem of missing association rules fundamentally, but also it improves the accuracy of association rules mined.



II.THE PROBLEMS BASED ON CONFIDENCE ARCHITECTURE  Definition1 If the item sets UX ? , the total number of transaction of non-empty transaction database is N, the support degree of X is S/N, we denote it as sup(X), that is P(X). If sup(X)?min_sup which min_sup is the given minimum support threshold, we call X frequent item sets[7].

Definition2 Supposed that YX ?  is a association rule, which meet UX ? , UY ?  ?=?YX ,the conditional probability of Y is P(Y/X), we call it the confidence degree of  YX ? , we denote it as )( YXconf ? [8].

DOI 10.1109/ICINIS.2011.4     Definition3 As for the association rules ?? )sup( YX min_sup, ?? )( YXconf  min_conf, we call YX ? as strong association rule [9].

The confidence YX ? is called as conditional probability. Although the confidence degree interpreted by conditional probability reflects the relevance of between X and Y, it separates the relevance of the conditional probability P(Y/X) and the prior probability P(Y), which may lead to three problems are as following: ? When ?? )sup( YX min_sup and min_conf ? P(Y|X) ? P(Y), the association rule generated is lack of credibility. Due to min_conf ? P(Y|X) output YX ? will be strong association rule. But P(Y|X) ? P(Y) shows that the probability of Y reduced under the premise of X. Instance X kept Instance Y from advancing. At this time, it is easy to return the strong association rules YX ?  which is normal association rule in fact.

? When ?? )sup( YX min_sup and min_conf ? P(Y|X) ? P(Y), we may miss some important association rules. Due to P(Y|X) ? min_conf, output YX ? would not be strong association rule. But P(Y|X) ? P(Y) shows that the probability of Y increased under the premise of X.  Instance X deduced Instance Y.

As a result we ought to get the association rule YX ?  as a useful association rule.

? When )( YXconf ? = P(Y|X)=P(Y), we can easily get P(YX) = P(X) P(Y). Instance Y and X are independent. Therefore, the association rules generated was not accurate.



III. THE THEORY OF CERTAINTY FACTOR  The theory of certainty factor is proposed by Shortliffe in 1975, which was a reasoning model.

The specific formulas are as follows:  ( ) ( ) [ ]  ( ) ( )[ ] ( ) [ ] ( )?  ?  ?  ? ?  ?  ?  ? ?  =  HPMAX HPHPEHPMAX  MAXHP EHMB  0,1 ,  1,01 ,    ( ) ( ) [ ]  ( ) ( )[ ] ( ) [ ] ( )?  ?  ?  ? ?  ?  ?  ? ?  = =  HPMIN HPHPEHPMIN  MINHP EHMD  0,1 ,  1,01 ,    ( ) ( ) ( )EHMDEHMBEHCF ,,, ?=  CF is the certainty factor based on some assumption H under the situation that the posterior evidence exists. MB(H,E) is the increased confidence measure caused by the existence of E. MD(H,E) is the increased no-confidence measure caused by the existence of E. The relationship between MB(H,E) and MD(H,E) can be expressed by table 1.

TABLE I. THE RELATIONSHIP OF VALUE  conclusion MB,MD,CF  If ( )EHP =1 is ture MB=1,MD=0,CF=1  If ( ) 1=? EHP  is false MB=0,MD=1,CF=-1  Lack of  evidence ( ) ( )HPEHP =  MB=0,MD=0,CF=0  ? If CF(Y, X)>0, it shows the appearance of X increases the probability of Y. The larger the value of CF(Y, X) is, the greater the increase probability Y is.

? If CF(Y, X)<0, it shows the appearance of X decreases the probability of Y. The smaller the value of CF(Y, X) is, the greater the reduction probability Y is.



IV. ALGORITHM DESCRIPTION  In general, we should set certain threshold to fix on the conditions association rules met in advance, the threshold ]1,0(?? , The range of ?  based mainly on the association strength that the classification system required. When 1=? , the evidence of posterior support the hypothesis proposed fully. This paper take 0>? .

? Scan all k-item sets in the database, find all k-item sets that its support is greater than the minimum support by connecting operation,     deleing operation, etc. Then generate k-frequent sets.

? Calculate the certain degree of k-frequent sets, find all k-frequent sets that meet the certain condition, derive the certain association rules.

Classification-SC algorithm description: Input: data item sets k, minimum support  threshold min_sup, minimum confidence threshold min_conf.

Output: all the item sets that meet the requirements  Step 1 Initialize the database and scan all the item sets that the length of them is k according to the initial conditions, and the initial value of k is 1.

Step 2 Calculate the support of each k-item set, remove the k-item set that?s support is less than min_sup. If the k-item set is empty, we finish the algorithm.

Step3 Connect the k-item set in accordance with Apriori. If the k+1-item set are empty, we finish the algorithm.

Step4 Calculate the support of each k+1-item set, remove the k-item set that?s support is less than min_sup. If the k-item set is empty, we finish the algorithm.

Step5 Calculate the certain of each k+1-item set, remove all k+1-item set that?s certain is less than threshold ? . If k+1-item set are empty, we finish the algorithm.

Step6 Get the certain association rules according to the result of k+1-item certain frequent set, then k++.

We use pseudo-language to describe the classification algorithm based on certain-support framework.

Input: DataSet minsup Output: AruleSet // the association rules set of  certain classification-SC(AruleSet,DataSet,minsup,ce  r_threshold) Step1 FOR i=1 TO Length(DataSet) do Step2  FrequentSet[i] ? GenFrequentSet(i,DataSet)  Step3 IF FrequentSet[i] = THEN RETURN //if i-item data set is empty, finish the algorithm  Step4 FOR j=1 TO Length(FrequentSet[i]) do Step5 IF FrequentSet[i][j] < minsup THEN  Remove(FrequentSet[i][j]) Step6AruleSet[i] ? GenAruleSet(FrequentSe  t[i], cer_threshold) //generate i-item certain association rules  Step7 DataSet ?  (FrequentSet[i]) // the i+1-item frequent set is got by i-item frequent set.



V. SIMULATED EXPERIMENT  Example 1: The value of min_sup is 30% and the value of min_conf is 60%.

TABLE II. TRADING STATISTICS  pens bought pens not bought total The pencils  bought 4200 2800 7000  The pencils  not bought 1800 1200 3000  Total 6000 4000 10000  The analysis of Apriori algorithm is as follows:  supmin_%42 4200sup >==?  confconf min_%70 4200 >==  ( ) ( )pencilbuypenbuy ??  is a strong  association rules.

The analysis of classification-SC algorithm is as follows:  supmin_%42 4200sup >==    According to the certain formula:  ( ) ( ) ( )EHMDEHMBEHCF ,,, ?=  ( ) 10 << BP?  ( ) ( ) ( ) 0,,, =?=? ABMDABMBABCF ( ) ?<? ABCF ,  ( ) ( )pencilbuypenbuy ??  isn?t a strong association rules.

The percentage of customers who has bought pencil is 70% in all the customers who has bought pen. That is the average probability of customers bought pencil in all customers is the same with the conditional probability of customers has first bought pen and then has bought pencil. Therefore, we can see from the above analysis that the behavior of buying pen did not promote the behavior of buying pencil.

Example 2 The value of min_sup is 20% and the value of min_conf is 20%.

TABLE III. THE RELATIONSHIP AMONG THE PRODUCT  PARTS  Defective products  Qualified  products  total  equipment from B 52 636 688  equipment from  others  11 1800 1811  Total 63 2436 2499  The analysis of Apriori algorithm is as follows:  confconf min_%56.7  supmin_%08.2 52sup  <==  >==?  )( )(  productsdefectiveresult Bfromequipmentproduce  ?? ???  isn?t a strong association rule.

The analysis of classification-SC algorithm is as follows:  supmin_%08.2 52sup >==  ( ) 10 << BP?  ( ) ( ) ( )( ) ?>=?=  ? ?=?  %2.5 9748.0  02521.007558.0  , BP  BPABPABMB   ( ) 0, =? ABMD

VI. CONCLUSION  The strong association rules got from the classic association rules algorithm based on support-confident framework might go against the objective laws of reality. This is because the association rules based on confident-framework can not describe the relationship between the evidences and the assumptions accurately. In addition, we can not measure the influence which prior event had on the back event. The paper introduced certainty factors to verify the confidence degree, accordingly describing the relationships between the conditional probability and prior probability. What?s more, the algorithm resolved the problem that important association rules often were missed.

Reference  [1] Zhun Zhou, Bingru Yang, Yunfeng Zhao, Wei Hou, ?Research on algorithms for association rules mining based on FP-tree?, Systems and Control in Aerospace and Astronautics, ISSCAA 2008, 2nd International Symposium on, pp. 1-5.

[2]. R.Agrawal, T.Imielinski and A.Swami, ?Mining association rules between sets of items in large databases?, Proceedings of the 1993 ACM SIGMOD ACM Press, 2003, pp.207-216.

[3] Heikki Mannila, Hannu Toivonen and A. Inkeri Verkamo. ?Efficient Algorithms for Discovering Association Rules?, Proceedings of AAAI?94 Workshop on Knowledge Discovery in Databases, Seattle, Washington. AAAI Press , 1994, pp.181-192.

[4] PMehmet Kaya, PReda Alhajj, ?Utilizing Genetic Algorithms to Optimize Membership Functions for Fuzzy Weighted Association Rules Mining?, Applied Intelligence, vol. 24, Feb. 2006, pp.7-15.

[5] Amihood Amir, Yonatan Aumann, Ronen Fildman, Moshe Fresko, ?Maximal Association Rules: A Tool for Mining Associations in Text?, Journal of Intelligent Information System, vol.25, Nov. 2005, pp.333-345.

[6] Davy Janssens, Geert Wets, Tom Brijs, Koen Vanhoof, ?Adapting the CBA algorithm by means of intensity of implication?, Information Sciences, vol. 173, June 2005, pp. 305-318.

[7] Muhammad Fauzi Mohd. Zain, Md. Nazrul Islam, Ir.

Hassan Basri, ?An expert system for mix design of high performance concrete?, Advances in Engineering Software, vol.36, May.2005, pp.325-337.

[8] Dou Shen, Jian-Tao Sun, Qiang Yang, Zheng Chen, ?A comparison of implicit and explicit links for web page classification?, Proceedings of the 15th international conference on World Wide Web, May.2006, pp.643-650.

