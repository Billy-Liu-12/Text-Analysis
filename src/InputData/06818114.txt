

Abstract?Mining frequent patterns with multiple minimum supports is an important generalization of the association-rule- mining problem, which was proposed by Liu et al. Instead of setting a single minimum support threshold for all items in basic Apriori, they allow users to specify different minimum supports to different items, and an Apriori-based algorithm, named MSapriori, is developed to mine all frequent patterns with these multiple minimum supports. MSapriori is different in several aspects with the basic Apriori and is not easier to understand than the basic Apriori. So in this paper, we propose an algorithm, named MSB_apriori, which uses basic Apriori to solve this problem. Then we compare MSB_apriori and MSapriori in run- time and space in details and find an optimized approach to MSB_apriori. Accordingly, an optimized MSB_apriori, named MSB_apriori+, is proposed. Experimental results on real-life datasets show that the MSB_apriori+ is much more efficient than MSB_apriori and close to MSapriori. The advantages of MSB_apriori+ lie in that 1) it may be more suitable than MSapriori in some real applications; and 2) it is easier to understand and can be used as a substitute.

Keywords-Frequent Pattern; Multiple Minimum Supports; Apriori;

I.  INTRODUCTION Frequent pattern mining plays an important role in mining  association rules. Most of the frequent pattern mining algorithms (e.g., Apriori [1]) use ?single minimum support (minsup) framework? to discover complete set of frequent patterns. Minsup is used to prune the search space and to limit the number of frequent patterns generated. However, using only a single minsup implicitly assumes that all items in the database are of the same nature or of similar frequencies in the database. This is often not the case in real-life applications [2].

In the retailing business, customers buy some items very frequently but other items very rarely. Usually, the necessities, consumables and low-price products are bought frequently, while the luxury goods, electric appliance and high-price products infrequently. In such a situation, if we set minsup too high, all the discovered patterns are concerned with those low- price products, which only contribute a small portion of the profit to the business. On the other hand, if we set minsup too low, we will generate too many meaningless frequent patterns and they will overload the decision makers, who may find it  difficult to understand the patterns generated by data mining algorithms.

To solve this rare item problem, Liu et al. proposed MSApriori algorithm to find frequent patterns with ?multiple minsups framework? and it can reflect different natures and frequencies of items [2]. In this framework, each item in the database can have its own minimum item support (MIS) specified by the user and each pattern can satisfy a different minsup depending upon the items within it. Also, Hu et al.

proposed an FP-growth-like algorithm known as Conditional Frequent Pattern-growth (CFP-growth) to mine frequent patterns [3]. Since downward closure property no longer holds in ?multiple minsups framework,? the CFP-growth algorithm has to carry out exhaustive search in the constructed Tree structure. Kiran et al. proposed an improved CFP-growth algorithm, called CFP-growth++, by introducing four pruning techniques to reduce the search space [4].

Most of the above algorithms are based on modifying the classical algorithms Apriori and FP-growth. They are not easier to understand than the basic Apriori. For example, MSapriori is different in several aspects with the basic Apriori, such as in the steps of generating L1, generating candidate C2 and pruning steps. Indeed, we can use basic Apriori to mine frequent patterns with multiple minsups although some other classical algorithms, such as FP-growth, are more efficient than Apriori and can be used as substitutes, we still use Apriori because it is the first proposed algorithm, and it is widely used, studied, and is easily accepted. So in this paper, we propose an algorithm, named MSB_apriori, which uses basic Apriori to mine frequent patterns with multiple minimum supports. The main idea of MSB_apriori is as follows: mine frequent patterns L by basic Apriori and one minimum support first, then choose those itemsets that satisfy the definition of frequent patterns with multiple minimum supports from L. MSB_apriori is not only easy to understand, but it may be more suitable than MSapriori in some real applications. For example, the user may specify a threshold in the beginning, adjust the threshold after evaluating the discovered result, or change the threshold after a period of time after receiving volumes of transactions. The minimum support threshold therefore should be variable to suit the need of the user. MSB_apriori can be used here only if the minimum MIS of items is no less than minsup. Otherwise, users need to     re-run the original program, which may cost expensively in run-time and space.

The contributions of this paper are as follows:  1) We propose an algorithm, named MSB_apriori, to mine frequent patterns with multiple minimum supports using basic Apriori.

2) We compare MSB_apriori and MSapriori in run-time and space in details and find an optimized approach to MSB_apriori.

3) We propose an optimized MSB_apriori algorithm, MSB_apriori+, by reducing the scanning times of database to improve the performance of MSB_apriori.

The remaining of the paper is organized as follows. In Section 2, we introduce the related work. Section 3 introduces the method based on basic Apriori algorithm of finding frequent patterns, and through experiment to compare the performance of this and MSapriori algorithm. In Section 4, we optimize the basic Apriori algorithm by reducing the scanning times to improve its run-time. We evaluate the performance of the three algorithms through experiment results. Finally, the conclusion is drawn in Section 5.



II. RELATED WORK The occurrence of rare item problem with the usage of  traditional data mining techniques to discover knowledge involving rare items was introduced in [5]. In [2], ?multiple minsups framework? has been introduced to address rare item problem, and Liu et al. proposed MSapriori algorithm for extracting frequent patterns. In this framework, all items in the dataset are sorted in ascending order according to their MIS values. Keeping the ordering in subsequent operations and any frequent itemsets and letting k-patterns be generated from (k- 1)-pattern that share the same prefix, the complete set of frequent patterns with MMSs can be easily discovered.

The concept of MMSs has been widely applied to ARM, including generalized association rules with MMSs [6] [7], ARM with MMSs under the maximum constraint [8], fuzzy ARM with MMSs [9] [10], and partial periodic pattern mining with MMSs [11]. To improve the performance of ARM with MMSs, [3] extended the FP-growth algorithm to discover frequent patterns with MMSs. A data structure, named the MIS-tree, which extends the FP-tree structure [12], as well as the mining algorithm, CFP-growth, is proposed by Han et al. to improve the mining process with only one scan of the database.

The results showed the CFP-growth significantly outperforms the MSapriori. For the efficient mining of sequential patterns with MMSs, Hu et al. [13] proposed a compact data structure, called a Preorder Linked Multiple Supports tree (PLMS-tree), to store and compress the entire sequence database. Based on a PLMS-tree, an efficient algorithm, Multiple Supports ? Conditional Pattern growth (MSCP-growth) was developed to discover the complete set of patterns.

Kiran et al. [14] proposed a preliminary algorithm to improve the performance of CFP-growth by suggesting two pruning techniques for reducing the size of constructed tree structure. As the frequent patterns mined with ?multiple  minsups framework? do not satisfy downward closure property, the CFP-growth algorithm has to carry out exhaustive search in the constructed Tree structure. CFP-growth++, an improved CFP-growth algorithm was proposed in [4], by introducing four pruning techniques to reduce the search space.

Note that the above algorithms generally use a fixed support threshold to frequent itemsets. However, the threshold will be changed to cope with the needs of the users and the characteristics of the incoming data in reality. The mining with respect to this changeable support is referred to as variable support mining. The VSMDS (Variable Support Mining of Data Streams) algorithm was proposed by Lin et al. for efficient variable mining of frequent itemsets in a stream of transactions [15]. This algorithm uses a compact structure (called PFI-tree) to maintain the set of potential frequent itemsets and update their support counts. A synopsis vector is designed to approximate past transactions with a flexible distance threshold.



III. MSB_APRIORI ALGORITHM  A.  preliminaries Let I ={i1, i2,..., im} be set of all items, and a transaction  database DB = <T1,T2,...,Tn>, where Ti (i ? [1..n]) is a transaction which contains a set of items in I. Each transaction is associated with an identifier, called TID. The support of a pattern (or an itemset) X, denoted as s(X), is the number of transactions containing X in DB. The pattern X is frequent if its support is no less than a user defined minimum support (minsup) threshold value, i.e., s(X) ?minsup. A pattern containing k number of items is a k-pattern. The support of a pattern can also be represented in percentage of |DB|. In this paper, we use the terms ?itemset? and ?pattern? interchangeably.

Let MIS(i) denote the MIS value of item i. The minimum support of an itemset A= {a1, a2... ak}, denoted as mins(A), is the minimum MIS value among the items in A, i.e. ,  mins(A)=min [MIS(a1), MIS(a2), ... , MIS(ak)].

A is a frequent itemset if it?s actual support is no less than  mins(A), i.e., s(A) ? mins(A).

B. MSB_apriori algorithm The main idea of MSB_apriori is that 1) we use basic  Apriori algorithm and the minimum MIS value of all items in I as support threshold to obtain itemsets L first, and 2) we choose all itemsets l (l?L) that satisfies s(l) ? mins (l).

The MSB_apriori algorithm is given below:  Algorithm: MSB_apriori  Input: D: Transaction Database with lexicographic order; MIS (i): MIS Value of each item  Output: msL: Frequent Patterns with multiple minimum supports  1) Use Apriori algorithm and the minimum MIS value of all items to get Frequent Patterns L;  2) msL=? ;     3) msL1 = {<l> | l ? L1, s(l) ? MIS(l)}; 4) for (k = 2; Lk ? ?; k++) do begin 5)     for each itemset l in Lk do begin 6)             mins(l) = min(MIS(l[1]),?, MIS(l[k]));  //mins(l)  is the minimum value of MIS(l [i]) 7)            if(s(l) ?  mins(l)) then 8)                  msLk = msLk?l; 9)     end 10) end 11) msL =?k msLk? In this algorithm, we use Apriori algorithm and the  minimum MIS value of all items to get Frequent Patterns L (line 1).Each itemset l in L1 has only one item, line 3 puts the L1 that meets s(l) ? ? MIS(l) into msL1. From line4 to line 8, we mine the frequent itemsets msLk from the remaining Lk, it has two steps: First, we find mins(l) that is the minimum MIS value of all items in each itemset l (line 6). Second, for each itemset l that meets s(l) ? mins(l) can be added to msLk (line 7, 8).

Obviously, MSB_apriori algorithm can obtain the same results as MSapriori.

C. performance analysis We evaluate the performance of MSB_apriori and compare  with MSapriori algorithm by testing run-time and space on real datasets. The run-time of MSB_apriori here is including the run-time of basic Apriori. The space is the maximum value of the itemsets? number in Ck. For example, all candidate itemsets include C1 and C2. C1 has 4 itemsets, C2 has 2 itemsets, so the space is 4. All experiments are performed on a Pentium 4 Celeron 2.1G PC with 2G main memory, running on Microsoft Windows XP. All the programs are written in MyEclipse 8.5.

Real datasets used in run-time and space experiments are Mushroom, Chess, Connect, and Nursery, where they can be obtained at http://archive.ics.uci.edu/ml/datasets.html. TABLE I presents details of the datasets. Mushroom dataset with a dense nature contains hypothetical sample data corresponding to gilled mushrooms. Chess dataset consists of chess positions described only by the coordinates of the pieces on the board.

Connect database contains all legal 8-ply positions in the game of connect-4 in which neither player has won yet, and in which the next move is not forced. Nursery Database was derived from a hierarchical decision model originally developed to rank applications for nursery schools.

TABLE I.  DATASETS FOR RUNTIME AND SPACE TESTS  Data Set # of Trans # of Items  Mushroom 8124 23  Chess 3196 9  Connect 67557 6  Nursery 12960 28   In our experiments, we use the method proposed in [2] to  assign MIS values to items. We use the actual frequencies of  the items in the DB as the basis for MIS assignments. The formula can be stated as follows:   ? ? ? >  = OtherwiseLS  LSiMiM iMIS  )()( )(                    (1)  M(i) = ?f(i)                                             (2)  M(i) is the actual frequency of item i in the DB. f(i) is the actual frequency (or the support expressed in percentage of the data set size) of item i in the data. LS denotes the minimum MIS value of all items. ? (0???1) is a parameter that controls how the MIS value for items should be related to their frequencies. If ? = 0, we have only one MS, LS, which is the same as the traditional association rule mining.

In the experiments, we set ? = 0.6 and use different LS values to reflect the differences in the four date sets. The results of the experiment are as follows:  Mushroom DataSet         0.25 0.3 0.35 0.5 0.6 0.7  Minimum Support  R u n  T i m e ( s )  MSB_apriori  MSapriori    Chess DataSet   0.5   1.5   2.5  0.001 0.035 0.05 0.1 0.4 0.6 Minimum Support  Ru n  Ti me  (s )  MSB_apriori  MSapriori   Connect DataSet        0.001 0.02 0.2 0.4 0.6 0.7  Minimum Support  R un   T i m e ( s)  MSB_apriori  MSapriori    Nursery DataSet        0.02 0.03 0.06 0.08 0.1 0.2 Minimum Support  Ru n  Ti me  (s )  MSB_apriori  MSapriori   Figure 1. Run-time(s)  Mushroom DataSet         0.25 0.3 0.35 0.5 0.6 0.7 Minimum Support  Sp ac  e  MSB_apriori  MSapriori    Chess DataSet       0.001 0.035 0.05 0.1 0.4 0.6  Minimum Support  Sp ac  e  MSB_apriori  MSapriori   Connect DataSet    0.001 0.02 0.2 0.4 0.6 0.7 Minimum Support  Sp a ce  MSB_apriori  MSapriori    Nursery DataSet          0.02 0.03 0.06 0.08 0.1 0.2  Minimum Support  S p a c e  MSB_apriori  MSapriori   Figure 2. Occupy Space  From Figure 1 and Figure 2, we can see that as the minimum support increases, these algorithms are getting     closer and closer whether in run-time or in space in these datasets because of decreases in the total number of candidate and large itemsets. The MSB_apriori algorithm is not as efficient as MSapriori in run-time and space, this comes from two reasons. One is that the number of MSB_apriori algorithm?s candidate itemsets is larger than MSapriori algorithm?s. The other is that in the MSB_apriori algorithm, it also takes more time in obtaining the frequent itemsets L. For these points, we can optimize the algorithm properly to reduce the run-time and space.



IV. OPTIMIZED MSB_APRIORI ALGORITHM: MSB_APRIORI+  A. MSB_apriori+ algorithm Based on the above analysis, for obtaining the frequent  itemsets L by Apriori, we optimize the MSB_apriori algorithm. We call the optimized MSB_apriori algorithm, MSB_apriori+. We sort the items in I in ascending order according to their MIS values and then all itemsets follow this order. It can reduce the number of candidate itemsets, because different order of items will generate different number of candidate itemsets. The less number of candidate itemsets has, the less scanning times of database will be took. In obtaining the supports of candidate itemsets, we sort the candidate itemsets in the lexicographic order, not MIS values, because the database is in the lexicographic order. It can scan the database only one time to get the supports so as to save much time.

MSB_apriori+ algorithm is given below:  Algorithm: MSB_apriori+  Input: D: Transaction Database with lexicographic order; MIS(i): MIS Value of each item  Output: msL: Frequent Patterns with multiple minimum supports  1) msL=? 2) minsup = min(MIS(i1),?, MIS(im)); // minsup is  the minimum value of MIS(ik) 3) M = sort(I, MS); /* according to MIS(i)?s stored in  MS */ 4) C1=init-pass(M , D); /* make the first pass over D */ 5) L1={c? C1|c.count?minsup}; //all itemsets follow  the sorted order M 6) for (k=2? Lk-1??? k++) do begin 7)      Ck=apriori_gen(Lk-1)?// New candidates 8)      Ck?= Sort all candidates c?Ck in lexicographic  order 9)     for all transactions t?D do begin 10)        Ct=subset(Ck? ,t)?//Candidates contained in t 11)         for all candidates c of Ck? ? Ct  do 12)               c.count ++? 13)     end? 14)     Lk={c? Ck |c.count?minsup}? 15) end? 16) msL1 = {<l> | l ? L1, s(l) ? MIS(l)}; 17) for (k = 2; Lk ? ?; k++) do begin  18)     for each itemset l in Lk do begin 19)             mins(l) = min(MIS(l[1]),?, MIS(l[k]));  //mins(l)  is the minimum value of MIS(l [i]) 20)            if(s(l) ?  mins(l)) then 21)                  msLk = msLk?l; 22)     end 23) end 24) msL =?k msLk?  B. performance analysis In this section, we compare the performance of three  algorithms through extensive experiments. The algorithms include MSB_apriori algorithm, MSapriori algorithm and optimized MSB_apriori algorithm. This Experiment also uses the same datasets and method as section ?. The results of the experiment are as follows:  Mushroom DataSet         0.25 0.3 0.35 0.5 0.6 0.7  Minimum Support  R u n  T i m e ( s )  MSB_apriori  MSapriori MSB_apriori+    Chess DataSet   0.5   1.5   2.5  0.001 0.035 0.05 0.1 0.4 0.6 Minimum Support  R u n  T i m e ( s )  MSB_apriori MSapriori MSB_apriori+    Connect DataSet        0.001 0.02 0.2 0.4 0.6 0.7  Minimum Support  R un  T i me  (s )  MSB_apriori MSapriori MSB apriori+    Nursery DataSet        0.02 0.03 0.06 0.08 0.1 0.2 Minimum Support  R u n  T i m e ( s )  MSB_apriori  MSapriori  MSB_apriori+   Figure 3. Run-time(s)  Mushroom DataSet         0.25 0.3 0.35 0.5 0.6 0.7  Minimum Support  Sp ac  e  MSB_apriori  MSapriori  MSB_apriori+    Chess DataSet       0.001 0.035 0.05 0.1 0.4 0.6  Minimum Support  Sp ac  e  MSB_apriori  MSapriori  MSB_apriori+   Connect DataSet       0.001 0.02 0.2 0.4 0.6 0.7 Minimum Support  S p a c e  MSB_apriori  MSapriori  MSB_apriori+    Nursery DataSet         0.02 0.03 0.06 0.08 0.1 0.2  Minimum Support  S p a c e  MSB_apriori MSapriori MSB_apriori+   Figure 4. Occupy Space  From Figure 3 and Figure 4, although different itemsets have different number of items, we can see that with the increment of minimum support, these three algorithms are getting closer and closer whether in run-time or in space. The MSB_apriori+ algorithm is much more efficient than     MSB_apriori whether in run-time or space and it is gradually close to MSapriori algorithm. This is because 1) we sort the item i in I in ascending order according to MIS(i) and then all itemsets follow this order; and 2) we sort the candidate itemsets in the lexicographic order, not MIS values in obtaining the supports of candidate itemsets. So the run-time and space can be saved.



V. CONCLUSIONS AND FUTURE WORK Mining frequent patterns with multiple minimum supports  is an important problem because the items in database are often not of the same nature. To solve this problem, most of the existed algorithms, MSapriori and CFP-growth, for example, are proposed by modifying the classical algorithms. These algorithms are not easier to understand than the classical algorithms in some degree. In order to use an easy way to solve the this problem, this paper uses the basic Apriori algorithm because Apriori was the first proposed algorithm to mine frequent patterns and has been widely used and studied. This paper has proposed the MSB_apriori algorithm and its optimized one, MSB_apriori+. We have compared them with MSapriori in run-time and space in details. Experimental results on real-life datasets show that the MSB_apriori+ is much more efficient than MSB_apriori. Although MSB_apriori+ is not as efficient as MSapriori in run-time and space, it may be more suitable than MSapriori in some real applications. Moreover, it is easier to understand and can be used as a substitute when users do not care more about run- time and space.

As a part of future work, we are planning to conduct extensive experiments by considering different types of datasets. And we are also planning to use basic FP_growth algorithm to mine frequent patterns with multiple minimum supports and to compare it with MSB_apriori, CFP_growth, and CFP_growth++.

