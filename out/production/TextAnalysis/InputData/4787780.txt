MFCC and ARM Algorithms for Text Categorization

ABSTRACT  Text categorization is continuing to be one of the most researched NLP problems due to the ever-increasing amounts of electronic documents and digital libraries. In this paper, we present a novel text categorization method that combines the Multitype Features Coselection for Clustering and Association rule mining, for constructing text classifiers. The high dimensionality of text in a document has not been fruitful for the task of categorization, for which reason, feature clustering has been proven to be an ideal alternative to feature selection for reducing the dimensionality. We, therefore, use Multitype Features Coselection for Clustering (MFCC) to generate an efficient representation of documents and apply Association rule mining for training text classifiers. The method was extensively tested and evaluated. The proposed method achieves higher or comparable classification accuracy and Fl results compared with Decision tree. MFCC improves clustering performance.

1. INTRODUCTION Text Categorization is the task of assigning a given  text document to one or more predefined categories. This problem has received a special and increased attention from researchers in the past few decades due to many reasons like the gigantic amount of digital and online documents that are easily accessible and the increased demand to organize and retrieve these documents efficiently. A large number of machine learning, knowledge engineering, and probabilistic- based methods have been proposed for TC [1], [2], [3], [5], [6], [7], and [8]. The most popular methods include Bayesian probabilistic methods, regression models, example-based classification, decision trees, decision rules, Rocchio method, neural networks, and association rules mining. In this paper, we explore the application of a learning technique, called association rule mining, with the Multitype Features Coselection for Clustering [4] into the text categorization problem. Association rule mining in the learning phase and makes the following contributions: First, anew technique for text categorization that makes no assumption of term independence is proposed. This method proves to perform as well as other methods in the literature. Second, it is fast during both training and categorization phases. Third, the classifier that is built using our approach can be read, understood and modified by humans. The resulting classifier is able to perform both single..class classification, by which each document is assigned a unique class label, and multiple class  classification, by which a document could be classified in many classes simultaneously.

The main contributions of this paper are as follows: 1) the paper proposes a new TC technique based on an existing state-of..the-art feature clustering technique and an association rule mining. 2) The devised TC technique outperforms decision tree across all three benchmark data sets on small training sizes where the training set contains very few examples. This is suitable for applications where labeled training data is very limited, like in classifying personal e-mail messages or memos. 3) The devised TC technique accepts only binary features (based on learning..logic), which makes it a perfect pick for applications in which features are essentially binary. Also, this aspect gives it an advantage in computational time complexity. The rest of this paper is organized as follows: Next, we describe prior related work.

Section 3 describes the learning in association rule mining and Multitype features Co selection. Section 4 discusses text categorization by association rule mining. Finally, Section 5 discusses the conclusions and future work.

2. RELATED WORK In this section, we briefly discuss the related research in  text categorization and, for more details, we will refer to a certain publication, e.g., [6]. Support Vector Machines (SVM) have been prominently used for text categorization (Joachims [14] and Dumais et al. [2]) using the bag..of-words model.

Dumais et al. [2] showed that using Mutual Information (MI) for feature selection combined with SVM outperforms other learners such as Rocchio, decision trees, Naive Bayes, and Bayesian Nets. Their results show a 92.0 percent break-even point on the 10 largest categories of the Reuters-21578 data set [2]. Moreover, Yang and Liu [17] and Joachims [5] confirmed the suitability and performance of SVM for text categorization using bag-of-words. Decision tree learners attempt to select from training data some informative words using an information gain criterion, then predict the category of a document based on the occurrence of word combinations.

Among the most popular decision tree-based methods are ID-3 and C4.5 [14], [15]. Decision rule methods generate classifiers by inductive rule learning. Examples of decision rule methods include Charade, DL-ESC, and RIPPER [6].

While traditional machine learners employ attribute value representations, the use of logic programming     representations led to the establishment of Inductive Logic Programming (ILP) [12]. The effectiveness of ILP methods for TC lies in formulating classifiers based on word order.

Several techniques have been proposed for feature selection and dimensionality reduction; see, for example, [8], and [6].

Both Baker and McCallum [11] and Slonim and Tishby [16] used agglomerative clustering algorithms and using Naive Bayes showed that the feature size is greatly reduced by using distributional clustering without significant loss of categorization accuracy. Other methods such as Latent Semantic indexing (e.g., [13]) have been applied and have been shown to be inferior to feature clustering [11]. Dhillon et al. [1] proposed an information-theoretic framework that captures the optimality of word clusters in terms of generalized Jensen- Shannon divergence between multiple probability distributions.

3. THE PROPOSED METHOD We propose a new TC method based on a successful  feature clustering technique and an association rule mining.

Our approach depends on representing the text document as a projection on clusters formed from the input data set, then applying the Association rule mining to build text classifiers.

Next, we present the Multitype Features Coselection for Clustering approach.

3.1 MULTITYPE FEATURES COSELECTION FOR CLUSTERING a) Feature Coseleetion  First, it should be made clear that the selection of each type feature is limited in each type of feature, and the clustering is an iterative one. After one iteration of clustering, each data object will be assigned to a cluster. In [9], Liu et al.

assumed each cluster corresponded to a real class. Using such information, they did supervised feature selection, such as  Information Gain (IG) and X2 statistic (CHI) [10] during k- means clustering. MFCC tries to fully exploit heterogeneous features of a Web page like title, URL, anchor text, hyperlink, etc., and to find more discriminative features for unsupervised learning. We first use different types of features to do clustering independently. Then, we get different sets of pseudoclass, which are all used to conduct iterative feature selection (IF) for each feature space. Then, we get different sets of pseudoclass, which are all used to conduct iterative feature selection (IF) for each feature space. After normal selection, some data fusion methods are used to combine the selection results in each space, i.e., feature coselection. In each iteration of clustering, the coselections in several spaces are conducted one by one after clustering in all feature spaces.

That is to say, all intermediate clustering results in different feature spaces have been achieved before any coselection.Thus, the sequence of coselection will not affect the final performance. The general idea of coselection for k- means clustering is described in Fig. 1.

Suppose that we categorize data objects with M heterogeneous features into L clusters. Let fvn be one  dimension of the feature vector, icri be the intermediate clustering results in the ith feature space, and SF be a fusion function. The pseudo algorithm is listed as follows:  ,-- IYR'_1_fJ.'t~!J.Jpjl~_ -- ------- ------ -- --- -: TY~-n~-f~C!!~!!~prce  : : JU'/\: Intermediate A~ition~1 \~. 1  Result InfqrmatiQn () :....1'"7 '-.../ I ~)~ / ...;------ ---_. __.

"'------"'& l~~"~rqrce ??t Additio~al  : \ Ih'Ne; I InformatIon : It.8Nti,o: "-../  : Feature Selectionscore~: r-------- -------, : I f  :-?-?-----?--?-----------?-?-------?--?-Additk;~~1 !OrO i Information l \ ) 1  I '-..--" I I I  Type-3-FeatureSp'ace Fig. 1. The basic idea ofmultitype feature coselection.

Suppose that we categorize data objects with M heterogeneous features into L clusters. Let fvn be one dimension of the feature vector, icri be the intermediate clustering results in the ith feature space, and SF be a fusion function. The pseudo algorithm is listed as follows: Loop for N iterations ofk-means clustering { Loop for M feature spaces  { Do clustering in feature space m  } Loop for M feature spaces  { For feature space m, do feature selection using results in all feature spaces. For fvn, one dimension of the feature vector in space m, a feature selection score fss (fvn, icri) is obtained by using intermediate clustering results icri in feature space i.

Then a combined score fss (fvn) is achieved by fusing the scores based on different result sets.

M  fss(fvn) = SF(fss(fvn,ieri)) (1) i=1  } }  In (1), fss (fvn, icri) can be the value calculated by the selection function or rank among all features. The feature selection criteria we used are introduced in Section c. For SF, voting, average, and max are three available choices.

Depending on the choices of fss and SF, we obtain five fusion models including voting, average value, max value, average rank, and max rank. The equations are listed as follows:  M  Voting (Val (fvn)) = L vote(fvn,ieri) i=1  (fv . ) {o - val(fvn, ieri) < st }  vote n, leri = I-val(fvn,ieri) >= st    Average (Val (fvn) = (~va/(.fvn, icn)) / M M  Max (Val (fvn)) = argmax(val(.fvn,ieri? I  Average Rank (Rank (fvn)) = (tRank{.fvn, icn)) / M  M  Max Rank (Rank (fvn))) = arg max(Rank(.fvn, ieri? (2) I  In the above equation, val(.fvn, ieri) is the value calculated by the selection function, Rank(.fvn, ieri) is the  rank of fvn in the whole feature list ordered by val(.fvn, ieri) , and st is the threshold of feature selection. After feature coselection, objects will be reassigned, features will be reselected, and the pseudoclass-based selection score will be recombined in the next iteration. Finally, the iterative clustering and feature coselection are well integrated. In each of the iterations, the whole feature space should be reconsidered. The reason is that our method can help find more effective features through a mutual reinforcement process. Properly selected features will help clustering and vice versa. That is to say, some discriminative features will not be found until late in the clustering phase. This can be verified by some empirical results.

3.2 ASSOCIATIONRULE MINING 3.2.1 Association Rule Generation  Association Rule Mining is a data mInIng task that discovers relationships among items in a transactional database. However, most popular algorithms designed for the discovery of all the types of association rules, are apriori- based.

Formally association rules are defined as follows: Let I = {i1, i2 ... im} be a set of items. Let D be a set of  transactions, where each transaction T is a set of items such that T ~ I. Each transaction is associated with a unique identifier TID. A transaction T is said to contain X a set of items in I, if X ~ T .An association rule is an implication of the form "X => Y", where X ~ I , Y ~ I, and X nY=<j). The rule X => Y has a support's in the transaction set D if s% of the transaction in D contain X U Y. In other words, the support of the rule is the probability that X and Y holds together among all the possible presented cases. It is said that the rule X => Y holds in the transaction set D with confidence c if c % of transaction in D that contains X also contain Y. In other words, the confidence of the rule is conditional probability that the sequent Y is true under the condition of the antecedent X. The problem of discovering all association rules from a set of transactions consists of  generating the rules that have a support and confidence greater than given thresholds. These rules are called strong rules.

The main idea behind apriori algorithm is to scan the transactional database searching for k-item sets (k-items be belonging to the set of items I). As the name of the algorithm suggests, it uses prior knowledge for discovering frequent item sets in the database. The algorithm employs an iterative search and uses k -item sets discovered to find (k+1) item sets.

The frequent item sets are those that have the support higher than a minimum threshold.

4. BUILDING AN ASSOCIATIVE TEXT CLASSIFIER  In this paper we present a method to build a categorization system that merges association rule mining task with the classification problem. This model is graphically presented in Fig 2.

Data preprocessing represents the first step. At this stage cleaning techniques can be applied such as stop words removal, streaming or term pruning according to the TF/IDF values (term frequency/inverse document frequency). The next step in building the associative classifier is the generation of association rules using an apriori - based algorithm. Once the entire set of rules has been generated an important step is to apply some pruning techniques for reducing the set of association rules found in the text corpora. The last stage in this process is represented by the use of association rules set in the prediction of classes for new documents. The first three steps belong to the training process while the last one represents the testing phase. If a document Di is assigned to a set of categories C={ Ct,C2, cm } and afterwards pruning the set of term T = { t},t2, tn } is retained , the following transaction is used to model the document Di :{CI,C2 Cm,t},t2, tn } and the association rules are discovered from such transactions representing all documents in the collection.

Testing Set  Model validation  Figure 2. Construction phases for an association rule based text categorizer    Fi+-{ ceCilc.support>cr} } Sets+-ui{ ceFili>1} R=<j> for each itemset I in Sets do {  R+-R+{I~Cat}  4.3 Pruning the Set ofAssociation Rules There are two issues that must be addressed in this  case. One of them is that such a huge amount of rules could contain noisy information which would mislead the classification process. Another is that a huge set of rules would make the classification time longer. The pruning methods are the following: eliminate the specific rules and keep only those that are more general and with high confidence, and prune unnecessary rules by database coverage.

Definition 1: Being given two rules T 1 => C and T2 => C we say that the first rule is more general ifT1~ T2. The first step of this process is to order the set of rules. This is done accordingly to the following ordering definition.

Definition 2: Being given two rules Rl and R2 .Rl is higher ranked than R2 if:  (1) Rl has higher confidence than R2 (2) If the confidences are equal, supp (Rl) must exceed  supp (R2)  (3) Both confidences and support are equal, but Rl has less attributes in left hand side than R2 With the sit of association rules sorted, the goal is to select a subset that will build an efficient and effective classifier. In our approach we attempt to select a high quality subset of rules by selecting those rules that are general and have high confidence.

Algorithm Pruning the set of association rules Input: The set of association rules that were found in the  association rule mining phase(s) and the training text collection (D).

Output: A set of rules used in the classification process  Method: (1) sort the rules according to Definition 1 (2) for each rule in the set S  12.

13.

14.

15.

16.

17.

18. }  In ARC-BC algorithm step (2) generates the frequent 1- itemset. In steps (3-13) all the k-frequent item sets are generated and merged with the category in Ci. Steps (16-18) generate the association rules. The document space is reduced in each iteration by eliminating the transactions that do not contain any of the frequent item sets. This step is done by FilterTable (Di-I, Fi-1) function. This problem leads us to the next subsection where pruning methods are presented.

Although the rules are similar to those produced using a rule- based induced system, the approach is different. In addition, the number of words belonging to the antecedent could be large (in our experiments up to 10 words), while in some studies with rule-based induced systems, the rules generated have only one or a pair of words as antecedent.

4.2 Associanon Rule Generation The association rules discovered in this stage of the  process is further processed to build the associative classifier.

Using the apriori algorithm on our transactions representing the documents would generate a very large number of association rules, most of them irrelevant for classification.

There are two approaches that we have considered in building an associative text classifier. The first one ARCAC (Association Rule-based Classifier with All Categorized) is to extract association rules from the entire training set following the constraints discussed above. As a result we propose a second solution ARC-BC (Associative Rule-based Classifier by Category) that solves such problems. In this approach we consider each set of documents belonging to one category as a separate text collection to generate association rules from. If a document belongs to more than one category this document will be present in each set associated with the categories that the document falls into.

Algorithm ARC-BC Find association rules on the training set of the text collection when the text corpora are divided in subsets by category.

Input: A set of documents (D) of the form Di : {Ci, t I, t2, tn} where ci is the category attached to the document and tj are the selected terms for the document; A minimum support threshold; A minimum confidence threshold; Output: A set of association rules of the form t1/\ t2 /\ /\ tn ~ Ci, where Ci is the category and tj is a term;  Method: 1. C1+- { Candidate 1 term-sets and their support } 2. F1+-{Frequent 1 term-sets and their support} 3. for(i+-F i_1*<j>;i+-i+1) do{  4. Ci+-(Fi-1 ~ Fi-1) 5. Ci+-Ci-{ c\(i-l) item-set of c~ Fi-1} 6. Di+-FilterTable (Di-1,Fi-1) 7. for each document d in Di do { 8. for each c in Ci do {  9. c. support +- c. support+Count(c,d) 10. } 11. }  ~lDaMcoUecnonP~prou~mg  In our approach, we model text documents as transactions where items are words or phrases from the document as well as categories to which the document belongs, as described above. A data cleaning phase is required to weed out those words that are of no interest in building the associative classifier. We consider stop wording and term pruning as well as transformation of documents into transactions as a preprocessing phase. Stop word removal and term pruning is done according to the TF /IDF values and a given list of stop words. We have opted to selectively turn on and off stop wording depending upon the data set to categorize. It is only after the terms are selected from the cleansed documents that the transactions are formed. The subsequent phase consists of discovering association rules from the set of cleansed transactions.

(3) find all those rules that are more specific according to (Definition 2)  (4) prune those that have lower confidence (5) a new set of rules S is generated (6) for each rule R in the set S (7) go over D and find those transactions that are covered  by the rule R (8) if R classifies correctly at least one transaction (9) select R  (10) Remove those cases that are covered by R  4.4 Prediction ole/asses Associated with New Documents The set of rules that were selected after the pruning phase  represent the actual classifier. Given a new document, the classification process searches in this set of rules for finding those classes that are the closest to be attached with the document presented for categorization. However, in the text categorization domain, multi-class categorization is an important and challenging problem that needs to be solved. In our approach we give a solution to this problem by introducing the dominance factor. By employing this variable we allow our system to assign more than one category. The dominance factor 8 is the proportion of rules of the most dominant category in the applicable rules for a document to classify. Given a document to classify, the terms in the document would yield a list of applicable rules. If the applicable rules are grouped by category in their consequent part and the groups are ordered by the sum of rules' confidences, the ordered groups would indicate the most significant categories that should be attached to the document to be classified. We call this order category dominance, hence the dominance factor8. The dominance factor allows us to select among the candidate categories only the most significant. When 8 is set to a certain percentage a threshold is computed as the sum of rules' confidences for the most dominant category times the value of the dominance factor.

Then, only those categories that exceed this threshold are selected. Take K Classes (S,8) function selects the most k significant classes in the classification algorithm.

The next algorithm describes the classification of a new  document.

Algorithm: Classification of a new object Input: A new object to be classified 0; the associative  classifier (ARC); the dominance Factor8; The confidence thresholdt;  Output Categories attached to the new object Method:  (1) S+--~ /* set of rules that match 0*/ (2) For each rule r in ARC (3) If(rco) {count++} (4) if (count = 1) (5) fr.conf +-- r.conf (6) S+--Sur (7) Else if (r.conf>fr.conf-t) (8) S+--Sur (9) Else exit  (10) divide S in subsets by category: SI,S2 ...Sn (11) For each subset St,S2 Sn (12) Sum the confidences of rules and divide by the number of rules in Sk (13) If it is single class classification (14) Put the new document in the class that has the highest confidence sum (15) Else /*multi-class classification*/ (16) Take K Classes(S,8) (17) Assign these k classes to the new document  5. CONCLUSION AND FUTURE WORK Our study provides evidence that association rule  mining can be used for the construction of fast and effective classification for automatic text categorization. We have presented an association rule-based algorithm for building the classifier: ARC-BC that considers categories one at a time. As most of the recent text categorization research focuses on addressing specific issues in TC (e.g., feature selection, clustering, and dimensionality reduction), very few new approaches are being devised. This paper proposes a new TC approach benefiting from the recent advances in feature clustering and dimensionality reduction coupled with a fairly effective logic-based learning technique. The method was extensively tested with numerous experiments using well- known benchmark data sets and compared with exact experimental settings against Decision tree. The proposed method outperformed the decision tree method on all training testing settings using WebKB data set and on most experiments conducted with the 20NG data set. The associations rule based classifier performs well and its effectiveness is comparable to most well known text classifiers. One major advantage of the association rule based classifier is its relatively fast training time. Moreover, the rules generated are understandable and can easily be manually updates or adjusted if necessary. In the case of ARC-BC, when new documents are adjusted and the rules could be incrementally updated.

The introduction of the dominance factor 0 allowed multi- class categorization. We are working on reducing the number of features, thus better discrimination among classes is expected.

