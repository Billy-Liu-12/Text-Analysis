Rule Mining using Many-sorted Logic

Abstract?Inductive Logic Programming (ILP) is used in relational data mining to discover rules in first order logic, given data in multiple relations. This form of data mining has to be distinguished from market basket analysis where the data comes from a single relational table. Although ILP addresses the problem of dealing with data from multiple relational tables, the fact remains that the efficiency of inferring rules in first order logic is significantly less than that of many-sorted logic. Further, many sorted logic is a closer reflection of the real world of objects that belong to sorts, in the presence of a sort hierarchy. We propose a new approach to ILP using many-sorted logic that is more computationally efficient than the approach based on unsorted first order logic.

Keywords-Data mining; business intelligence; machine learning; Big Data; Inductive Logic Programming; many-sorted logic; sort hierarchy

I.  INTRODUCTION Many data mining methods assume that the data is from a  single relational table and consists of tuples of items. This approach is used, for example, in market basket analysis.

Relatonal data mining, however, assumes that data resides in multiple relational tables with many connections among them.

Inductive Logic Programming (ILP) has been widely used as a method for learning relational patterns in data [1]. ILP is a branch of machine learning based on logic programming. ILP makes use of the Horn clause representation of first order logic formulas. Specifically, ILP aims at deriving hypotheses from a set of positive and negative examples and background data.

Data in relational tables can be easily translated into first order logic and this representation can be used by ILP algorithms.  Consider for example, the rules expressed in typical logic programming terminology.

Uncle(X,Y) :- Brother(X,Z), Parent(Z,Y).

Uncle(X,Y) :- Husband(X,W), Sister(W,Z), Parent(Z,Y).

ILP aims at inferring such rules from a set of facts, consisting of positive and negative examples, and background knowledge. To infer the rules above, for example, the following information may be available.

Uncle(t,f), Uncle(b,j), -- positive examples  Not Uncle(t,c), Not Uncle(b,t) ? negative examples  Parent(b,f), Parent(c,f), Parent(a,j), Parent(t,j), Brother(t,c),  Sister(c,t), Husband(t,a), Husband(b,c).

The Parent, Brother, Sister, and Husband predicates are background information in this example. The problem that ILP aims to solve is the following. Given background knowledge, B, a set of positive examples, P, and a set of negative examples N, find a hypothesis  such that:    The first condition is that of completeness while the second condition is that of consistency. Here, we have assumed that all the predicates and rules are expressed as Horn clauses. One approach to ILP is Plotkin?s relative least general generalization [2]. This approach consists of relativizing each literal with the complete background knowledge, conversion to clause normal form, inverse-unification of compatible literals, deletion of all negative literals not containing a variable occurring in a positive literal, and conversion back to Horn clause form. This corresponds to the unified framework of inverse resolution and relative least general generalization.

To illustrate with an example, consider the following background knowledge:      and the positive examples,    We start with   and relativize it with the background knowledge to arrive at:        We do the same for the other positive example:          Next, we convert the two Horn clauses to clause normal form:         And      The anti-unification [3] of compatible literals is done next:  from the two daughter literals,  ) from the first two parent literals  from the two female literals.

) from the second and third parent literals  and other negative literals.

The next step is to delete all negative literals that contain variables not occurring in a positive literal, and writing the remainder in clause form.

)  Converting this back to Horn clause form, we obtain the final rule (hypothesis), as the definition of daughter:  )  Or,    This is the hypothesis that gives a definition of the predicate .  We will next explore how the efficiency of deriving the hypothesis can be improved.

In Section II, we describe many-sorted logic briefly and in Section III, we describe the rule mining algorithm using many- sorted logic. Section IV contains results and conclusions.



II. MANY-SORTED LOGIC The basic idea of many-sorted logic is that the universe of  discourse is divided into subsets, and variables are restricted to range over these subsets. Each subset is called a sort. Many sorted logics find applications in artificial intelligence. See for example [4]. One of the main reasons for preferring many- sorted logic to the usual (unsorted) first order logic is the computational efficiency of deductions. Bloch and Frisch [5] showed that many-sorted logic can be more efficient than unsorted logic because of the way in which the search space is traversed and irrelevant branches are pruned.

Many results have been reported to show that many-sorted logic is more efficient than unsorted logic [6,7]. For example, the number of clauses used in the problem representation was 49, 27, and 42 for three different problems. However, for the same problems, the number of clauses was 1, 3, and 30 respectively for the many-sorted logic approach. The number of derived clauses in the proof for the three problems was 102,  33, and 130 for the unsorted case, while the number was 0, 5, and 55 respectively for the many-sorted logic.

It is easy to see that proofs in many-sorted logics are much shorter than in unsorted logic. This leads us to infer that, for rule mining with the ILP approach, many-sorted logic will reduce the number of steps required to derive the hypothesis. In the remainder of the paper, we show how this can be achieved.

As stated above, in many-sorted logic, the universe of discourse is divided into non-empty subsets called sorts. These sorts could be pairwise disjoint or there could be a taxonomic hierarchy among the sorts. For the purposes of this paper, we assume the existence of a sort hierarchy. If  and  are sorts, we write  to indicate the fact that is a subset of . The universe of discourse is itself written  as ? (read top) to indicate that every sort is subset of the universe. A special sort, called bottom, denoted by N (null set) lies at the bottom of the sort hierarchy and is a subset of every sort. A term of type N is said to be ill-sorted.

Unlike the unsorted case, a variable x must belong to a sort or class S to make sense. We write x : S to denote the fact that the variable of type S. Functions and predicates are also defined to make sense only when the sorts of their arguments are specified. A function f(x1:S1,  x2:S2, ?, xn:Sn) = y:Sm indicates that the function f takes arguments x1 of type S1, x2 of type S2, ?,xn of type Sn, and computes a result y of type Sm. A predicate P(x1:S1,x2:S2,?,xn:Sn) takes arguments x1 of type S1, x2 of type S2, ?,xn of type Sn, and yields a value True or False.

As an example, we take the unsorted relation discussed earlier and convert it to the many-sorted case. Note here that the sorts would be Male, Female, and Human, where Human is the parent class of the Male and Female classes. The background knowledge can be represented as        The positive examples would be    and   The first step is to relativize each positive example with the background knowledge.

And      464 2014 IEEE International Advance Computing Conference (IACC)      Converting to clause normal form, we obtain:        And        Anti-unification of compatible literals yields:    ,    And others. We drop negative literals that contain variables not occurring in positive literals.

Writing in clause normal form,    Converting the clause to Horn form, we obtain the rule:    In order to understand the anti-unification step, it is necessary to consider the sort hierarchy. In this example, the class Human is the parent class of Male and Female. We assume that the class Human is the union of Male and Female classes for the purpose of this example. In the anti-unification step,  and  combine to form a variable which is the union of the two classes, Male and Female. It should be noted that there is a significant elimination of literals that define a type of an individual, for example, female(ann).

By such elimination of sort literals, we can achieve more efficiency than in the unsorted case.



III. THE ALGORITHM We now describe the many-sorted inductive logic  algorithm. The algorithm assumes that there are no function symbols in the predicates, and can be easily extended to handle function symbols.

Step 1: Relativization; Relativize each positive example with the background knowledge. Here, each positive example is a predicate of the form ,?, , where the c?s are constants (instances) of classes denoted by the S?s.

Background knowledge, consisting of predicates of the form  ,?,  is converted into a conjunction of literals and made the ?if? part of the Horn clause, while each  positive literal is the ?then? (head) of the Horn clause. The result of this step will be Horn clauses of the form:  ,?, ,?, ,?, ?    Step 2: Convert each Horn clause formed in Step 1 to clause normal form. The result of this step will be clauses of the form:  ,?,   ,?, ,?, ?    Step 3: Anti-unify compatible pairs of literals. This is a crucial step in many-sorted logic. Two literals are compatible if they are of the same sign, have the same predicate symbol and their corresponding arguments are compatible. Two arguments  are compatible if  is a sub-sort (subclass) of or vice versa, or the union of  and  is defined in the sort  hierarchy. In this case, we replace , by a variable   where is one of  or , or their union.

Step 4: Delete all negative literals that contain variables not occurring in any positive literal. Write the remaining literals in clause form and convert them back to Horn clause form.

Theorem: The algorithm above achieves a reduction of literals in each step equal to the number of unsorted predicates of the form  that are included in the background knowledge.

Proof: Suppose there are n literals of the form , where is a constant and  indicates that    belongs to the type .

It is clear from the way the many-sorted predicates are constructed that such unsorted predicates are ?absorbed? in the sorted predicates, and thereby eliminated from further processing. If there are N such unsorted predicates, each step will have N less literals to consider.

Therefore, the algorithm leads to a more efficient implementation of inductive logic programming.



IV. RESULTS AND CONCLUSION The algorithm described in the previous section has been  tested and has shown an improvement in efficiency of inferring rules from multi-relational data. For example, we found a problem that required 135 steps with unsorted logic required 45 steps using many-sorted logic. Another problem requiring 38 steps in the unsorted case took 12 steps in the many-sorted case. Yet another problem that takes 100 steps in the unsorted case took 9 steps in the many-sorted case. We propose to use many-sorted logic for inductive logic programming and machine learning of rules from relational data.

Most real world problems have a many-sorted structure, with different classes, instances of each class, and relations among such instances. It is natural and straightforward to model the real world data in many-sorted logic. This approach has the potential of drastically improving the efficiency of data mining software. For example, it could be useful in time- critical applications where a large amount of data has to be examined in a short time.

