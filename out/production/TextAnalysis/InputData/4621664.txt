Information Driven Association Rule Hiding Algorithms

Abstract  Privacy is one of the most important properties an infor- mation system must satisfy. A relatively new trend shows that classical access control techniques are not sufficient to guarantee privacy when datamining techniques are used.

Privacy Preserving Data Mining (PPDM) algorithms have been recently introduced with the aim of sanitizing the data- base in such a way to prevent the discovery of sensible in- formation (e.g. association rules). A drawback of such algorithms is that the introduced sanitization may disrupt the quality of data itself. In this paper we introduce a new methodology and algorithms for performing useful PPDM operations, while preserving the data quality of the under- lying database.

1 Introduction  Intense work in the area of data mining technology and in its applications to several domains has resulted in the devel- opment of a large variety of techniques and tools able to au- tomatically extract knowledge from large amounts of data.

At the present moment, these tools are the only effective way to extract useful knowledge from the increasing num- ber of very large databases which cannot be manually ana- lyzed, due to their huge sizes. However, as with other kinds of useful technologies, knowledge discovery processes can be misused. For example, malicious subjects may recon- struct sensitive information for which they do not have ac- cess authorization. Usual data mining techniques can be deployed over non-sensitive data in order to infer private  information. For private information we mean any infor- mation usually about the owner of the corresponding data, that such owner is not willing to release. Current security techniques are not suitable to prevent inferences of private information carried out through the use of such data mining techniques.

For this reason, many research efforts have been devoted in order to seek privacy-preserving data mining (PPDM from now on) techniques, i.e. data mining techniques which yield useful information without violating the privacy of data owners. As a result, various database sanitization tech- niques have been proposed for hiding privacy-related items or patterns by removing some of the publicly available data or adding noise to them. Some tests whose results are found in [4, 7] show that actually some problems occur when ap- plying PPDM algorithms in contexts in which the mean- ing of sanitized data has a critical relevance. More specif- ically, these tests show that sanitization often introduces non-correct information. Such phenomenon is due to the fact that current PPDM techniques do not take into account two relevant issues:  ? Relevance of data: not all the information stored in the database has the same relevance and not all the in- formation can be dealt in the same way.

? Structure of the database: information stored in a database is strongly influenced by the relationships be- tween different data items (e.g. integrity constraints).

These relationships are not always explicit.

We argue that the general problem of current PPDM al- gorithms is related to data quality, intended as the consis- tency between real-world data and their representations in  on Information Technology, IT 2008 19-21 May 2008, Gdansk, Poland     an informations systems (such as a database). See Section 3 for a detailed discussion on the connection between privacy and data quality.

Of course, in order to perform a data quality-aware sani- tization, any PPDM technique needs some additional infor- mation about the database to be sanitized. In this paper, we propose a suitable framework for expressing such additional information and define a corresponding data quality-aware sanitization technique.

2 Related works  Traditionally, the association rule mining task consists in identifying a set of strong association rules. Srikant and Agrawal in [12], explore and expand the idea of interest- ing rule. Further, Agrawal et al.[1] and Park et al.[10] decompose the challenge of association mining into two main steps called large itemsets identification and associa- tion rules identification. Atallah et al. [3], propose heuristic techniques for the modification of data based on perturba- tions, in order to avoid an unauthorized use of association rule mining. Oliveira and Zaiane [8] propose a heuristic- based framework for preserving privacy in mining frequent itemsets. They focus on hiding a set of frequent patterns, containing highly sensitive knowledge. They propose a set of sanitization algorithms, that only remove information from a transaction database, also known in the statistical disclosure control area as non-perturbative algorithms, un- like those algorithms, that modify the existing information by inserting noise into the data, referred to as perturbative algorithms. The algorithms proposed by Oliveira and Za- iane rely on a item-restriction approach, in order to avoid the addition of noise to the data and limit the removal of real data. Evfimievski et al. [6] propose a framework for mining association rules from transactions consisting of cat- egorical items, where the data has been randomized to pre- serve privacy of individual transactions, while ensuring at the same time that only true associations are mined. An- other reconstruction-based technique is proposed by Rivzi and Haritsa [11]. They propose a distortion method to pre- process the data before executing the mining process. Their goal is to ensure privacy at the level of individual entries in each customer tuple.

3 The Data Quality Approach  Any PPDM sanitization operating on a target database D has an effect on the data quality contained in it. In order to preserve the quality of the database, we propose to take into account the relevance and the intended use of the data contained in the DB, represented as a function ?(DB) of the database D. Now, by taking another little step in the  discussion, it is evident that the function ?(DB) has a different output for different databases. This implies that a PPDM algorithm trying to preserve the data quality needs to know specific information about the particular database to be protected before executing the sanitization.

3.1 Data Quality Concepts  Traditionally, data quality (DQ) is a measure of the con- sistency between data as stored in an information system and the same data as in the real world [9]  In the context of PPDM, DQ has a similar meaning, with however an important difference. The difference is that in the case of PPDM the real world is the original database, and we want to measure how close the sanitized database is to the original one with respect to some relevant properties.

More in detail, we consider relevant the parameters al- lowing to capture a possible variation of meaning or that are indirectly related with the meaning of the information stored in a target database. Therefore, we identify as most appropriate DQ parameters for PPDM Algorithms the fol- lowing dimensions:  ? Accuracy: it measures the proximity of a sanitized value a? to the original value a. In an informal way, we say that a tuple t is accurate if the sanitization has not modified the attributes contained in the tuple t  ? Completeness: it evaluates the percentage of data from the original database that are missing from the sani- tized database. Therefore a tuple t is complete if after the sanitization every attribute is not empty.

? Consistency: it is related to the integrity constraints holding on the data and it measures how many of these constraints are still satisfied after the sanitization.

For a formal definitions of those parameters see [5].

3.2 The Information Quality Model  In the previous section, we have presented a way to mea- sure DQ in the sanitized database. These parameters are, however, not sufficient for the construction of a new PPDM algorithm aimed at DQ preservation.

It is necessary to provide a formal description of the in- formation that the PDDM algorithm is intended to extract from the sanitized DB. From now on, we refer to such in- formation as aggregate information and the relevance of DQ properties for each aggregate information (for some aggre- gate information not all the DQ properties have the same relevance). Moreover, for each aggregate information, it is necessary to provide a description of the relevance (in term of consistency, completeness and accuracy level required)    at the attribute level and the constraints the attributes must satisfy. In order to achieve such goals we have adopted the Information Quality Model (IQM) presented in [5]  The Information Quality Model is composed in a bottom-up fashion by a number of components whose de- scriptions are detailed below. Intuitively, the task of these components is to provide useful information in order to find out whether a given database can be sanitized (e.g. for hid- ing given association rules) without incurring in a data qual- ity loss. The basic bricks of the IQM is the Data Model Graph (DMG) , whose task is to represent the attributes in- volved in an aggregate information and their constraints) and Aggregation Information Schema (AIS), which mea- sures the relevance of different aggregations. Such infor- mation is grouped into what we call information Schema Set (ASSET).

3.3 The DQDB Algorithm (distortion based algorithm)  In the Distortion Based Algorithms, the database saniti- zation (i.e. the operation by which a sensitive rule is hid- den), it is obtained by the distortion of some values con- tained in the transactions supporting the rule (partially or fully, depending from the strategy adopted) to be hidden.

In what follows we assume to work with categorical data- bases. The sanitization strategy we adopt is similar to the one presented in the Codmine algorithms [7].

The first operation required is the identification of the rule to hide. In order to do this, we use the well known APRIORI algorithm [2].

The output of this phase is a set of rules. From this set, a sensitive rule will be identified. The next phase implies the determination of Zero Impact Items (if they exist) asso- ciated with the target rule (Figure 1).

INPUT: the IQM Schema A associated to the target database, the rule Rh to be hidden OUTPUT: the set (possibly empty) Zitem of zero impact items discovered  1.Begin 2. zitem=?; 3. Rsup=Rh; 4. Isup=list of the item contained in Rsup; 5. While (Isup ?= ?) 6. { 7. res=0; 8. Select item from Isup; 9. res=Search(Asset, Item); 10. if (res==0)then zitem=zitem+item; 11. Isup=Isup-item; 12.} 13.return(zitem); 14.End  Figure 1. Zero Impact Algorithm  If the Zero Impact itemset is empty, this means no item exists contained in the rule that can be assumed to be non  relevant for the DQ of the database. In this case it is nec- essary to simulate what happens to the DQ of the database, when a non zero impact item is modified. The strategy we adopt in this case is the following: (a) We compute how many steps are needed in order to downgrade the confidence of the rule under the minimum threshold (as it is possible to see from the algorithm code, there is little difference be- tween the case in which the item selected is part of the antecedent or part of the consequent of the rule). (b) By inspecting the IQM we calculate the impact on the DQ in terms of accuracy and consistency for each items. The Item Impact Rank Algorithm is reported in Figure 2.

INPUT: the IQM schema, the rule Rh to be hidden, the sup(Rh), the sup(ant(Rh)), the Threshold T h OUTPUT: the set Qitem ordered by Quality Impact  1. Begin 2. Qitem=?; 3. Confa=  Sup(Rh) Sup(ant(Rh))  4. Confp=Confa; 5. Nstep if ant=0; 6. Nstep if post=0; 7. While (Confa > T h) do 8. { 9. Nstep if ant++;  10. Confa= Sup(Rh)?Nstep if ant  Sup(ant(Rh))?Nstep if ant ; 11.} 12.While (Confb > T h) do 13.{ 14. Nstep if post++;  15. Confa= Sup(Rh)?Nstep if post  Sup(ant(Rh)) ;  16.} 17.For each item in Rh do 18.{ 19. if (item ? ant(Rh)) then N=Nstep if ant; 20. else N=Nstep if post; 21. For each AIS ? Asset do 22. { 23. node=recover item(AIS,item); 24. Accur Cost=node.accuracy W eight ? N ; 25. Constr Cost=Constr surf(N,node); 26. item.impact=item.impact+  (Accur Cost ? AIS.Accur weight)+ +(Constr Cost ? AIS.COnstr weight);  27. } 28.} 29.sort by impact(Items); 30.Return(Items); End  Figure 2. Item Impact Rank Algorithm for the Distortion Based Algorithm (ant(Rh) indi- cates the antecedent component of a rule)  Finally the Sanitization algorithm works as follows:  1. It selects all the transactions fully supporting the rule to be hidden.

2. It computes the Zero Impact set and if there exist such type of items, it randomly selects one of these items. If this set is empty, it calculates the ordered set of Impact Items and it selects the one with the lower DQ Effect.

3. Using the selected item, it modifies the selected trans- actions until the rule is hidden.

The final algorithm is reported in Figure 3. Given the al- gorithm, for sake of completeness we try to give an idea about its complexity. The search of all zero impact items, assuming the items to be ordered according to a lexico- graphic order, can be executed in N ? O(lg(M)) where N is the size of the rule and M is the number of dif- ferent items contained in the Asset. The cost of the Im- pact Item Set computation is less immediate to formulate.

However, the computation of the sanitization step is linear (O(?Sup(Rh)?)). Moreover the constraint exploration in the worst case has a complexity in O(NC) where NC is the total number of constraints contained in the Asset. This operation is repeated for each item in the rule Rh and then it takes |Items|O(NC). Finally in order to sort the Item- set, we can use the MergeSort algorithm, and then the com- plexity is |itemset|O(lg(|itemset|)). The sanitization has a complexity in O(N) where N is the number of transaction modified in order to hide the rule.

INPUT: the Asset Schema A associated to the target database, the rule Rh to be hidden, the target database D OUTPUT: the Sanitized Database  1.Begin 2. Zitem=DQDB Zero Impact(Asset,Rh); 3. if (Zitem ?= ?) then item=random sel(Zitem); 4. else 5. { 6. Impact set=Items Impact rank(Asset,Rh,Sup(Rh),Sup(ant(Rh)), Threshold); 7. item=best(Impact set); 8. } 9. Tr={t ? D|tfully support Rh} 10.While (Rh.Conf >Threshold) do 11.sanitize(Tr;item); 12.End  Figure 3. The Distortion Based Sanitization Algorithm  4 Conclusions  The problem of the sanitization impact on the quality of the database has been, to the best of our knowledge, never addressed by previous approaches to the PPDM. We believe that the core of this problem has to be linked with the lack of information given to the PPDM algorithms about the mean- ing and the relevance of the information stored into the data- base. On the basis of this consideration and starting from a previously done work of Bertino and Nai, we have pro- posed and developed a suite of algorithms for association rule PPDM aimed by the scope of mitigating the DQ im- pact of the sanitization over a target database. Preliminary test results, show that such an approach is effective against the information corruption due to the PPDM sanitization.

The study is however far to be considered finished, in fact we plan to extend our algorithms in the context of associ- ation rule PPDM blocking family. Moreover, we plan to  apply the same approach in the context of clustering PPDM algorithms.

