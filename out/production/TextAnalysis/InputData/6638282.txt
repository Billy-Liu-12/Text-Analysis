ONLINE LOGISTIC REGRESSION ON MANIFOLDS

ABSTRACT  This paper describes a new method for online logistic regression when the feature vectors lie close to a low-dimensional mani- fold and when observations of the feature vectors may be noisy or have missing elements. The new method exploits the low- dimensional structure of the feature vector, finds a multi-scale union of linear subsets that approximates the manifold, and per- forms online logistic regression separately on each subset. The union of subsets enables better performance in the face of noisy and missing data, and offsets challenges associated with the curse of dimensionality. The effectiveness of the proposed method in predicting correct labels of the data and in adapting to slowly time-varying manifolds are demonstrated using numerical exam- ples and real data.

Index Terms? Online learning, manifold learning, subspace tracking, logistic regression, big data  1. INTRODUCTION  Logistic regression [1] is a highly effective technique for super- vised learning of classifiers. It is widely used in data mining, can- cer classification using microarray data [2], computational adver- tising, phishing email scams, and social network analysis [3]. In recent years, online algorithms have become a popular approach to logistic regression for large-scale problems [4, 5], due to its simple implementation and effectiveness in processing massive quantities of data.

In many scenarios, we have to process streaming data (xt, yt), where xt ? RD is a ?feature vector? with large D and yt ? {0,1} is a label. The goal is to learn a classifier via updates based on training pairs, so that when only xt is available we may predict the label y?t. When the feature dimension D is large, this task can be particularly challenging due to the well-known curse of dimensionality. We can side-step this problem by exploiting low- dimensional structure underlying xt. In many settings, however, this low-dimensional structure must be updated from streaming data and may be slowly changing over time. We must update the model to track the dynamic structure. Moreover, missing data is frequently encountered in high-dimensional data analy- sis: it is often impossible or too costly to obtain complete data, as in internet tomography [6]. The primary challenges are to find this (possibly dynamic) low-dimensional structure within a high-dimensional feature space and exploit this in the logistic regression framework using streaming data.

In this paper, we develop a new method for multiscale online logistic regression for data on dynamic manifolds, which we call  This work was supported by DARPA award # FA8650-11-1-7150, # HR0011- 10-C-0073, AFOSR award # FA9550-10-1-0390, AFOSR award # FA9550-11-1- 0028, NGA # HM01773-1-0006.

MOUSSE-Logit. It regresses on low-dimensional linear subsets that are approximations of the manifold; moreover, it is able to handle noisy and missing elements in the feature vectors. Our recent work on Multiscale Online Union-of-Subsets Estimation (MOUSSE) algorithm enables online estimation and tracking of a set of linear approximations to the manifold. Based on this algo- rithm, we project data onto the local linear approximations, and perform logistic regression in the lower dimensional projected space. We show that when the feature vector has an intrin- sic manifold structure, MOUSSE-Logit can: (1) regress on the low-dimensional space with much lower computational cost; (2) achieves better performance working in a low-dimensional space than logistic regression in the ambient space; and (3) achieves better performance than logistic regression based on projecting feature vectors on a single subspace. Numerical examples and real-data experiments demonstrate the strong performance of MOUSSE-Logit.

Related work includes sparse logistic regression [7], which imposes an `1 penalty on the logistic parameter in regression, so that the number of features (entries in feature vector) used in regression is small. In contrast, MOUSSE regresses on learned projections of the feature vectors. In this sense, MOUSSE-Logit is closer to logistic regression based on the batch (not online) approach of sufficient dimension reduction [8].

2. FORMULATION  2.1. Model  Suppose we are given training data {(x?t , yt,?t)}, t = 1,2,?.

The observed feature vector x?t is defined as follows. Assume the ?ideal? or true feature vector vt lies on submanifold Mt ? RD, and let xt be a noisy realization of vt:  xt = vt +wt, (1)  where wt is a Gaussian noise with zero mean and variance ?2.

Here D denotes the ambient dimension. The intrinsic dimension of the submanifoldMt is d; we assume d ? D. The underlying submanifoldMt may vary slowly with time. At each time t we only observe x?t on a subset of indices ?t. Let P?t represent the ??t??D matrix that selects the axes of RD indexed by ?t; we observe  x?t ? P?txt. (2)  The label yt ? {0,1} may not be observed for test data.

Assume the labels {yt} are generated according to a Logistic  model described as follows [9]. Define the logistic function as  h(w;p, b) = 1 1 + e?p?w?b , (3)     where w ? Rm is a vector of features, p ? Rm denotes the weight vector, b ? R denotes an intercept or offset, and the notation ? denotes transpose of a matrix or vector. We assume the labels {yt} are Bernoulli with probability p of being 1 that depends on vt:  P(yt = 1) = h(vt; ?, b), (4)  where ? ? RD is the weight vector on the feature vector, and b ? R is the intercept.

Our goal is to use online logistic regression to design a clas- sifier based on streaming training data {(x?t , yt,?t)} which ex- ploits the low-dimensional submanifold structure of vt.

2.2. Attribute and relational data  Our formulation is motivated by the following two types of prob- lems. In the first scenario, we may want to infer the label of data according to its attribute. For example, in a recommendation sys- tem, xt is the feature vector of a customer, yt indicates whether a customer likes a given product. In a reputation system (say credit scoring system), xt are the features of a user, and yt is a label indicating whether the user is reliable or not.

In the second scenario, we may have relational data for a pair of users (nt,mt), which consist of their feature vectors znt ? X in space X , zmt ? X and a label yt that takes value 1 if user nt and user mt are related (e.g., they are friends) at time t. Then we would like to infer whether two people for whom we have not observed relationship indicators are likely to become friends.

This scenario can be mapped to the first scenario. We can define a metric that compares two feature vectors (for example, take entry- wise difference): dist(znt , zmt) ? X ?X ? RD. Then we may use xt = dist(znt , zmt) as features in logistic regression.

3. LOGISTIC REGRESSION  In the following, we will present two approaches for logistic re- gression: regression in the original RD feature space, and regres- sion in the Rd projected feature domain.

3.1. Logistic regression in RD feature space  With training data (x?t , yt,?t)Nt=1, we can find parameters (?, b) to the logistic regression model (4) by maximizing the log- likelihood function with observed data entries, which is given by  `(?, b;{x?t , yt,?t}) ? N  ? t=1  {yt logh(x?t ; ??t , b)  +(1 ? yt) log[1 ? h(x?t ; ??t , b)]} , (5)  where ??t = P?t?.

In the online setting, we discuss how to update estimates  for parameters (?, b). Let the estimated parameters at time t be (?t, bt). At time t+ 1, when there is new data with feature vector xt+1 and unknown label, we can predict its label using  y?t+1 = { 1, h(x?t+1 ; ??t+1 , bt) ? 1/2; 0, otherwise.

(6)  When both the feature vector xt+1 and the label yt+1 are avail- able, we can update (?t, bt) by stochastic gradient descent. Con- sider the following one-sample log-likelihood function with data (x, y):  `(?, b;x, y) ? y logh(x; ?, b) + (1 ? y) log(1 ? h(x; ?, b)). (7)  Let [w]m denote the m-th element of vector w. The gradient of (7) is given by  ?`(?, b;x, y) ?[?]m  = ?(h(x; ?, b) ? y)[?]m, m = 1, . . . ,D, (8)  ?`(?, b;x, y) ?b  = ?(h(x; ?, b) ? y) (9)  With missing data, we can update the parameter ?t on the dimen- sions where the feature vector are observed:  ?t+1 = ?t + ?(h(x?t+1 ; ??t+1 , bt) ? yt+1)[xt+1]m, m ? ?t+1, bt+1 = bt + ?(h(x?t+1 ; ??t+1 , bt) ? yt+1), (10)  where ? > 0 is the step-size.

3.2. Logistic regression in Rd projection space  When feature vectors lie in a low-dimensional space, we can exploit this structure to reduce computational complexity and achieve better predication performance. Note that label yt de- pends on xt through the logistic function with argument ??xt, so in the following we focus on this inner product ??xt.

First we consider a special case, and demonstrate that when there is no noise in the observed feature vector x?t = vt (i.e., ?t = {1, . . . ,D} so there is no missing data), and the feature vector vt lies in a d-dimensional subspace embedded in RD, then logistic regression in the ambient space and the projected space are identical. In particular, assume vt = U?, for some orthogonal basis U ? RD?d, where ? = U?vt ? Rd is the coefficient of vt.

Let a = U?? ? Rd be the projection of ? on the linear subspace.

In the following, we will show  ??vt = a??. (11)  To see this, we decompose ? into a component that lies in U , ??, and a component that is orthogonal to U , ??, with ? = ?? + ??.

Note that ?? = Ua, ?? = (I ? UU?)?. By this decomposition, ??vt = ???vt + ???vt, with  ???vt = a ?U?U? = a??, (12)  ???vt = ??(I ?UU?)U? = ??(U ?UU?U)? = 0, (13)  since U?U = I . Hence, we have (11).

Since yt depends on vt through the logistic function with ar-  gument ??vt, the above argument demonstrates that with knowl- edge of the subspace U , we can first project vt onto the subspace, and then perform logistic regression in the projection space.

The benefit of performing logistic regression in the projec- tion space is two-fold: first, when the observed feature vector is noisy (x?t = P?t(vt+wt)), knowledge ofU helps with denoising and imputing missing elements; second, projection significantly lowers computational complexity: instead of estimatingD+1 pa- rameters (?, b) in ambient space, in the projection space we only need to estimate d + 1 parameters (a, b).

When vt lies on a low-dimensional manifold rather than a lin- ear subspace, we can use a union of linear subsets to approximate the manifold, and project feature vectors onto the union of sub- sets. The above arguments motivate our new algorithm described in Section 4.

The main computational cost of MOUSSE-Logit comes from computing the subspace tracking, which is on the order ofO(dD) [10], higher than computing standard online logistic regression (complexity O(D)). The memory requirement for MOUSSE- Logit, dominated by storing the subspace, is on the order of O(Dd), higher than online logistic regression (where the mem- ory requirement dominated by storing the weight vector O(D)).

However, the benefits of MOUSSE-Logit include imputing the missing data using subspace projection and achieving better performance by denoising, as demonstrate in Section 5.

4. MOUSSE-LOGIT ALGORITHM  In this section, we present the new algorithm, MOUSSE-Logit, which approximates the manifold in the feature vector space by union of linear subsets that are organized in a tree structure, projects the feature vectors onto the linear subsets, and then fits a logistic model using projected feature vectors. To obtain the approximation to the manifold, and update the estimate using streaming data, we use our recently developed Multiscale Online Union of Subspace Estimation (MOUSSE) algorithm [10]. To update logistic regression parameters, we develop a stochastic gradient descent method based on projected feature vectors. The MOUSSE-Logit algorithm is summarized in Algorithm 1.

4.1. Union-of-subset approximation  MOUSSE uses a union of low-dimensional subsets M?t to ap- proximate Mt, and organizes these subsets using a tree struc- ture. The idea for a multiscale tree structure is drawn from the multiscale harmonic analysis literature [11]. The leaves of the tree are subsets that are used for the submanifold approximation.

Each node in the tree represents a local approximation to the sub- manifold at one scale. The parent nodes are subsets that contain coarser approximations to the submanifold than their children.

More specifically, the approximation at each time t consists of a union of subspaces Sj,k,t that is organized using a tree structure.

Here j ? {1, . . . , Jt} denotes the scale or level of the subset in the tree, where Jt is the tree depth at time t, and k ? {1, . . . ,2j} denotes the index of the subset for that level. The approximation M?t at time t is given by:  M?t = ? (j,k)?At  Sj,k,t, (14)  where At contains the indices of all leaf nodes used for approx- imation at time t. Also define Tt to be the set of indices of all nodes in the tree at time t, with At ? Tt. Each of these subsets lies on a low-dimensional hyperplane with dimension d and is parameterized as  Sj,k,t = {v ? RD ? v = Uj,k,tz + cj,k,t, z???1j,k,tz ? 1, z ? Rd}.

(15)  The matrix Uj,k,t ? RD?d is the subspace basis, and cj,k,t ? RD is the offset of the subset from the origin. The diagonal matrix  ?j,k,t ? diag{?(1)j,k,t, . . . , ? (d) j,k,t} ? R  d?d,  with ?(1)j,k,t ? . . . ? ? (d) j,k,t ? 0, contains eigenvalues of the covari-  ance matrix of the projected data onto each subset. This parame- ter specifies the shape of the ellipsoid by capturing the spread of the data within the subset. Each node also has a parameter ?j,k,t measuring the approximation error associated with each subset and is used in evaluating the distance of a sample to the subset.

MOUSSE estimates this tree of subsets by balancing the ap- proximation errors and the number of subsets used for approxi- mation to prevent data-overfitting (more details in [10]).

4.2. Online update of parameters and tree structure  MOUSSE-Logit uses the same tree structure as MOUSSE, and it also includes two more parameters for each node: aj,k,t and bj,k,t, which are the slope and intercept of the logistic model fit- ted using data associated and projected onto the subset Sj,k,t. In summary, the parameters used in MOUSSE-Logit are  {Uj,k,t, cj,k,t,?j,k,t, ?j,k,t, aj,k,t, bj,k,t}(j,k)?Tt . (16)  For initialization of these parameters {Uj,k,t, cj,k,t,?j,k,t, ?j,k,t} in (16) and the tree structure, we apply MOUSSE initialization on the training feature vectors {(xt,?t)}. To initialize the lo- gistic regression parameters (aj,k, bj,k), we use projected data {(?t, yt)} to fit a logistic model.

When a new sample (xt+1, yt+1,?t+1) becomes available, MOUSSE-Logit updates the parameters, and the tree structure if necessary. There are three main steps: (a) find the subset in the M?t which is closest to xt+1; (b) use MOUSSE to update M?t by updating parameters and the tree structure; (c) using stochastic gradient descent to update logistic parameter (aj,k,t, bj,k,t) of the closest subset and all its ancestor.

More details about the first two steps can be found in [10].

In the following we focus on updating of the logistic regression parameters, which can be achieved by performing stochastic gra- dient descent using the log-likelihood function (19) as the cost function. Given the union of subsets approximation estimated by MOUSSE, to update logistic parameters or predict label, we first project the feature vector with missing entries on to a subset with minimum distance. The distance metric used is the approximate Mahalanobis distance [10], since it captures the low-dimensional structure of the data. Denote the pseudoinverse operator that computes the coefficients of a vector in the subspace spanned by V as V # ? (V ?V )?1V ?. When U is an orthogonal matrix, we have U# ? U?. Given a subset with parameters {U, c,?, ?}, we compute the projection coefficient of xt with support ?t onto the subset as  ?t = U#?t(x?t ? c?t). (17) This projection essentially imputes the missing entries using the subspace rather than filling in zeros. Then the orthogonal com- ponent is given by ??t = (I ? U?tU  # ?t  )(x?t ? c?t). Using the projections, the approximate Mahalanobis distance of xt to the subset is defined as  ??(xt,S) ? ??t ??1?t + ??1???t ?2, (18)  which combines Mahalanobis distance (commonly used in clas- sification) and the Euclidean distance.

In the following, let ?t denote the projection coefficients of x?t projected onto the corresponding minimum distance subset.

Using the projected training feature vectors, we can estimate the     logistic parameters by maximizing the following log-likelihood function  `(a, b;{?t, yt}) ? N  ? t=1 yt logh(?t;a, b)  + (1 ? yt) log[1 ? h(?t;a, b)].

(19)  In the online setting, we discuss how to update estimates for parameters (a, b). Let the estimated parameters at time t be (at, bt). At time t+ 1, when there is new data with feature vector xt+1 and unknown label, we can predict its label using  y?t+1 = { 1, h(?t+1;at, bt) ? 1/2; 0, otherwise.

(20)  When both the feature vector x?t+1 and the label yt+1 are avail- able, we can update (at, bt) by stochastic gradient descent, by finding the minimum distance subset, computing the projec- tion ?t+1, and updating the logistic regression parameters using stochastic gradient descent to maximize the log likelihood func- tion. Let (at, bt) denote the parameter at time t, we can update these parameters by  [at+1]m = [at]m ? ?(h(?t+1;at, bt) ? yt)[?t+1]m,m = 1, . . . , d bt+1 = bt ? ?(h(?t+1;at, bt) ? yt), (21)  where ? > 0 is the step-size.

Algorithm 1 MOUSSE-Logit 1: Initialize parameters (16) and tree structure 2: for t = 1,2, . . . do 3: Given data x?t , its support ?t (and for training, label yt) 4: Find the set with minimum approximate Mahalanobis dis-  tance distance (18) to x?t 5: Compute projection ?t (17) using parameters of minimum  distance set (U?, c?,??, ??) 6: If label is not available, predict label via (20) using  (a?, b?) of the minimum distance set 7: Update parameters (U?, c?,??, ??) using MOUSSE 8: If label is available, update logistic regression parameters  of the minimum distance set and all its ancestors using (21).

9: end for  5. NUMERICAL EXAMPLES  We compare the average error in predicting labels using three methods: (1) Online logistic regression in RD (described in Sec- tion 3.1); (2) Online logistic regression in Rd by projecting fea- ture vectors on an estimated single subspace, which corresponds to restricting K = 1 in MOUSSE; (3) MOUSSE-Logit in Rd.

The manifold we generate is a chirp-signal, with ambient dimension D = 100 and intrinsic dimension d = 2. The two- dimensional parameters [f0,?] are uniformly random generated: frequency f0 ? [1,100], and phase ? ? [0,1]. Define vt ? RD with its n-th element being  [vt]n = sin [2?(f0zn + k2t z2n +?)] , (22)  where the indices, zn = 10?4n,n = 1,2, . . . ,100, corresponds to regularly spaced samples between 0 and 0.01. The parameter kt controls how fast the submanifold changes:  kt = ?????????  0.1t t = 1,2, . . . ,1000, 0.1(t ? 1000) t = 1001, . . . ,2000, 200 ? 0.1(t ? 1000) t = 2001, . . . ,3000.

For MOUSSE, we use PETRELS-FO for tracking [10], and pa- rameter values are ? = 0.3, ? = 0.02, ? = 0.9.

We use the first 1000 samples generated by (22) for training, and the remaining 2000 samples for test. We perform training with a batch method using a variant of k-means, as detailed in [10], followed by batch logistic regression on the learned sub- manifold approximation. Then for the test procedure, we first use each of the three algorithms to predict label y?t, then reveal the true label and use (x?t , yt,?t) to update the classifier.

The performance metric is given by average prediction error on the test samples: Pe = T ?1?T+1000t=1001 I{y?t ? yt}, T = 2000.

In the following, we average Pe over 50 independent trials; each trial is a sequence generated by (22).

We also compare these methods on real data for credit card fraud detection: the user E-commerce transaction history data set in 2008 UCSD data mining competition1. We choose an arbitrary sequence from the data set, which corresponds to the spending pattern of one user over time. The series has 144 samples, and each sample is a transaction record that consists of a D = 41 dimensional feature vector and a label that indicating if the trans- action is normal (yt = 0) or a fraud (yt = 1). We randomly per- mute the order of these samples for each of our 50 trials, and use 72 samples for training and the remaining 72 samples for test.

The parameters for MOUSSE-Logit are d = 3, ? = 15, ? = 0.05, ? = 0.9. We obtain Pe for each of 50 random permutations, and then average Pe across these trials. The comparison results are shown in Table 1, where in the first column ? is the noise vari- ance and the second number is the percentage of missing entries in the feature vector. The credit card data does not have missing entries or known noise statistics. In all cases, MOUSSE-Logit outperforms the other two methods, especially in the real data experiment.

Table 1: Comparison of average Pe? std Dataset Online Single Subspace MOUSSE-  Logistic Online Logistic Logit ? = 0, 60% 0.0559 ?0.0323 0.2053 ?0.0730 0.0410 ?0.0182 ? = 0.1, 40% 0.0455 ?0.0275 0.1716?0.0616 0.0343 ?0.0179  credit data 0.3883 ?0.1706 0.1389 ?0.0995 0.1107?0.0615  6. CONCLUSIONS  We have presented a new method, MOUSSE-Logit, for online lo- gistic regression when the feature vectors lie close to a manifold with intrinsic dimension dmuch smaller than the ambient dimen- sion D. MOUSSE-Logit has low complexity (proportional to d) and good performance, as demonstrated using numerical exam- ples and a real-data experiment.

Acknowledgement  We thank Jiaji Huang for his help on part of numerical examples.

1 Data available at http://www.cs.purdue.edu/commugrate/  data_access/all_data_sets_more.php?search_fd0=20 More detail about the data can be found in [10].

7. REFERENCES  [1] D. W. Hosmer and S. Lemeshow, Applied Logistic Regres- sion, Wiley, 2000.

[2] L. Shen and E. C. Tan, ?Dimension redunction-based penal- ized logistic regression for cancer classification using mi- croarray data?, IEEE Trans. Computational Biology and Bioimformatics, vol. 2, no. 2, pp. 166 ? 175, 2005.

[3] R. Xi, N. Lin, and Y. Chen, ?Compression and aggregation for logistic regression analysis in data cubes?, IEEE Trans.

on Knowledge and data engineering, vol. 1, no. 1, pp. 1 ? 14, Aug. 2008.

[4] W. D. Penny and S. J. Roberts, ?Dynamic logistic regres- sion?, in Neural Networks, 1999. IJCNN ?99. International Joint Conference on, 1999, vol. 3, pp. 1562? 1567.

[5] H. B. McMahan and M. Streeter, ?Open problem: Better bounds for online logistic regression?, in COLT/ICML Joint Open Problem Session, JMLR: Workshop and Conference Proceedings, 2012.

[6] L. Balzano, B. Eriksson, and R. Nowak, ?High rank ma- trix completion and subspace clustering with missing data?, in Proc. of Conf. Artificial Intelligence and Stats (AIStats), 2012.

[7] K. Koh, S.-J. Kim, and S. Boyd, ?An interior-point method for large scale `1-regularized logistic regression?, J. of Ma- chine Learing Res., vol. 8, pp. 1519 ? 1555, 2007.

[8] J. Nilsson, F. Sha, and M. Jordan, ?Regression on manifolds using kernel dimension reduction?, in Proc. Inter. Conf.

Machine Learning (ICML), 2007, pp. 697 ? 704.

[9] A. Agresti, Categorical data analysis, Wiley-Interscience, 2002.

[10] Y. Xie, J. Huang, and R. Willett, ?Changepoint detection for high-dimensional time series with miss- ing data?, IEEE Journal of Selected Topics in Sig- nal Processing, vol. 7, no. 1, pp. 12?27, Feb. 2013, http://people.ee.duke.edu/?yx44/MOUSSE.pdf.

[11] D. Donoho, ?Cart and best-ortho-basis selection: A con- nection?, Annals of Stat., vol. 25, pp. 1870?1911, 1997.

