The Optimal Decision Rules Discovery  Huaicheng Kou and Yunjie Wu

Abstract - Aiming at the data set expressed by decision-list, proposed an algorithm of optimum decision rules discovery based on granule computing. It increased the efficiency of decision rules mining by two ways, improving speed of getting frequent item sets with granule computing and deleting invalid candidate sets. Finally the result of calculating test showed the algorithm is validity.

Index Terms -Granule Computing, Optimal decision rule, decision table, association rule

I. INTRODUCTION  Today, the man highly concerned the improvement of algorithm efficiency in research field of the association rules, such as C4.5[1], CN2[2] and RIPPER[3] etc that would reduce time consuming, increase classification precision and return the smaller rule set. The paper also considered that way from two different methods.

Firstly introduced concept of the granule, and improved the data mining speed with granule computing. But varied  with references [9, 10] that defined granule with binary  digit string directly, the paper built the relation of granule, knowledge, transaction set and binary computing fundamentally from the concept of granule. That included defining the relationship between granule and knowledge, replacing the operation of granule with that of transaction set via mapping, and expressing transaction set with binary digit string.

Secondly it improved the mining efficiency with simplifYing rule set. In those fields the non-redundant association rules mining was a well known concept that has been introduced by many books, and documents [5-8] also researched the problem of simplifYing rule set. Based on literatures [4, 5] the paper further studied rule optimization expressed by decision table.



II. THE ASSOCAITON RULE MINING BASED ON GRANULE COMPUTING  The data set we researched is expressed by decision table, so the rules we got are decision rules with determined antecedent and consequent. Let follow data set: information  system S=(U,A(CUD),V ,f) , having n transactions,  P and X are item set in the condition attributes set. Z is decision item set, that having one and only one attribute, and the decision item Z E Z. Because any data set with more decision attributes all can be separated into many single    LijunZhao  department of project planning and management Beijing Hol/ysys Co.,LTD  Beijing, 100096, China  zhaolijun@hollysys.com  attribute data set for mining rules, the single data set defined above is practical and valid.

A. The Granule and Granule computing Definition I: information granule refer to the information  blocks attached by those own characters and properties that are separated from lots of information, when the man disposal problem with large mount of complicated information. On association rules discovery, every frequent candidate set are all combined with different item sets that appear feature information is classified by item sets, so every frequent candidate can be treated as a information granule.

In the information system S , for transaction x any one  attribute a has f(a, x) = v ,  in that v E Va <;;;; V . If expressed  the mapping with form of ordered pair (a, v), which is called  basic granule. Since the mapping f , granule also can be  represented by the pair of attribute and mapping (a,f(a)). If  changes the attribute a of basic granule into attribute set Al ,  (AI,f(AI)) is called the granule of the information system.

Definition 2: gs(Gr) is elements set of granule Gr that  denotes the mapping function from granule to transaction set.

If Al is an attributes set on S , gs(Gr) = rl(AI) for  granule Gr = (AI, f(AI)).

Definition 3: the standard characterizing how much knowledge a granule contained calls granule size.

Currently many ways can calculate the granule size. To realize algorithm conveniently, here adopt the definition of support on association rules discovery.

Definition 4: the granule size of Gr = (AI,f(AI)) equals  to the element number in its set (support), which is  I gs(Gr) 1=1 rl(AI) I? B. The association rule mining based on granule computing  A granule represents a type of knowledge, and the knowledge is reflected by transaction set. So conversely, the relation among transaction sets can also mirror that among granules. That is, for the granule and its corresponding  . ' f l gs(Grl /\ Gr2) I .

h transactIOn set 1 ? mmsup , t en Grl ? Gr2 .

I gs(Grl) I  Digital operation is more convenient than set operation of transaction set in information system, so establish  FITME2010    {l'X E gs(Gr) mapping rp:U-Hp(U,Gr)=x? ,XEU , on  O,X ? gs(Gr) universeU . And we record its range as setgsd(Gr), which is  a digital string with length n .so, in information system,  granule Gr , set of granule element gs(Gr) and digital  string gsd(Gr) have related to support and confidence of  association rules. And because systematically define concept and operation of association rules mining via applying granule to that, we can discover association among granules by correlation operation.



III. THE OPTIMAL DICISION ASSOCIATION REULE DICOVERY  A. Relevant definition The subset combined by all transactions containing item  set P , is called cover of P on universe U , and recorded as cov(P). cov(P ? z) denotes the transaction including not  only P but also z , so obviously, cov(P ? z) ? cov(P) ,  cov(PX) <;;; cov(P).

Definition 5[5]. Giving two rules P ? z and PX ? z ,  we say that the latter is more specific than the former and the former is more general than the latter.

If a decision rules set exist some condition as follow, which can be deleted.

1) Redundant rule: Let P ? z and X ? z , if PBX , then two rules is redundant. So we should delete any one.

2) Non-stronger rule: Let P ? z and PX ? z ,  if corif(P ? z) ? corif(PX ? z) , obviously X does not  increase the probability z appears. So that rule can be deleted.

B. The relationship of item sets among rules Now consider the possible relation between condition item  sets and decision item in rules, and analyze those features to reduce invalid rules. On U transaction can be divided into two part cov(z) and cov(-,z) . We express the relation of  cov(z) and cov(-,z) that can delete rule with figure 1A-C,  and get theorem and corollary through analyzing that one by one.

--,z z -.z z --,Z z  p p x  x  Fig. I A B C  Theorem 1. If supp(P -,z) = supp(PX -,z), PX ? z and  any rules being more specific than it can be deleted. The proving process refers literature [5]. Such as figure lA, it equal to X contains P on -,z area, so can delete second type rules.

Corollary 1. If supp(P) = supp(PX) , PX ? z and any  rules being more specific than that can be deleted, and also  corif(P ? z) = corif(PQX ? z) . Such as figure 1B  X contains P not only on the region -,z but also z ,  so in whole data set there is cov(X);;;2 cov(P) , and can get  Corollary through theorem 1.

Theorem 2. If supp(P) = supp(X) = supp(PX), item set  P and X are equivalent. So delete any one kind of rule PQ ? Z or XQ ? Z . Such as figure 1 C only need to  consider any one for condition item sets, since the cover of P  and X is completely same in whole data set and P ? X .

After discussing relation between two item sets, we consider special case of single item set. Now take item set P as an example to analyze figure 2A-C.

--,z z --,z z  I I I  --,z z  : P ===: I  I I I  ? ___ I Fig. 2 A B C  Item set P belongs to region -,z completely such as figure 2A, so support(P,z) = 0 and any rules like  PQ ? z would not exist. In association rules mining if it  happened, rules will be deleted because support is less that minimum support. So it dose not need theorem or corollary.

Corollary 2. If support(P, z) = support(P) , P ? z and  any rules being more specific that it will not appear. Such as  figure 2B, since z contains item set P , support(P, -,z) = 0 or  support(P, z) = support(P). And then corif(P ? z) = 100% .

The confidence of rule is less than 100%, so no rule being more specific is valid. That can be deduced by theorem 1 too.

Corollary 3. If support(P,-,z)=support(-,z) , and  P ? z any rules being more specific will not appear. Such as  2C, support(X, -.z) = support(PX, -.z) that is valid for any  item set X, since support(P,-.z) = support(-,z). And since  X is arbitrary, P ? z any rules being more specific will not appear through theorem 1.

In present section we obtain two theorems and three corollaries, in which theorem 1 and corollary 1, 2, 3 consider  itself and more specific rule than it, and can remove more specific rule with smaller confidence, theorem 2 is involved in the treatment of redundant rules caused by equivalent item sets. The equivalent item sets do not need to be taken into account alone, which is particular feature of decision rules mining.

C. The optimal decision rule set Now we try to induct the former two theorems and three  corollaries to a unified definition.

Definition 5. There are condition items set P and Q on  universe U . If cov(Q) = cov(P) , we call P and Q are  equivalent. The rules redundant items set correspond such as P ? z and Q ? Z , are called redundant rules. And the  decision rules set not including redundant is called non? redundant decision rules set.

If integrating the concept optimal rules set[5] further, we can get the concept optimal decision rules set.

Definition 6. On universe U a non-redundant decision  rules set not including the rules which confidence are less than or equal to that of more general rules than it, is the optimal decision rules set.

The optimal decision rules can determine if there are redundant attributes by jUdging redundant items set and remove invalid attributes by judging relation of all condition items sets and decision item, so it can deletes and reduces attributes in the process of data mining and rough set is not necessary too as a pre-treated way[II]. Moreover it can distinguish whole attributes is not redundant but part of that is redundant, or attribute is valid but part of that is invalid, because the optimal decision rules discovery starts from items set.



IV. THE OPTIMAL DECISION RULES DISCOVERY BASED ON  GRANULE COMPUTING  To combine algorithm of the .optimal decision discovery granule calculation adopts forward pruning more fitly, rather than mixing running that find all frequent items sets started with some item until search all items.

The theorem and corollary of the optimal decision rules discovery mainly involve redundant rules and specific rules weakening. Because the corollaries are stricter and should remove more candidates than theorem, their applying order would affect rationality and efficiency of pruning program.

Because of limit pages, do not write the whole algorithm program and only introduce implementation steps as following.

Stepl Let the optimal rules set R = 0 , and equivalent items set E = 0  Step2 generate basic granules and their corresponding set Step3 decompose basic granules, generate granule groups  of single condition attribute and add granule groups into frequent candidate set. k=O  Step4 k++. If k-frequent candidate set is an empty set, jump to step II  Step5 simplifY k-frequent candidate set by support Setp6 simplifY k-frequent candidate set by the optimal  decision rules Setp7 simplifY k-frequent candidate set by item set contain  multiple equivalent items Step8 add equivalent item sets to E , and add k-frequent  item set toR Step9 generate new candidate set (K+I) from k-frequent  item set   SteplO calculate the support of new candidate set, and jump to step4  Step I I delete unconfident rules by confidence in set R Stepl2 return the optimal decision rule set R

V. EXPERIMENTAL RESULTS  The program is realized by VC, and performed on a Lenovo computer, which CPU is Core2 Duo 2GHz, memory is I G and the operation system is Windows XP Professional SP2. The data for test adopt Anneal and Mushroom data sets of UCI, such as table I.

TABLE I DESCRIPTION OF UCI DATA SETS  Data set Coooition attribute Decision attribute Anneal 38  Mushroom 22  We prune two data sets with Aprior algorithm, association rules discovery based on granule computing and the optimal decision rule discovery based on granule computing. The aim doing that is to distinguish the contribution degree of each one method to increase the mining efficiency. Figure 3 and 4 show comparison result, in which abscissa is support (%) and ordinate is time consuming of algorithm (s) .

.-??---------------------------.

-+-Aprior  12 -.- Granule computing ? ? -,-New algorithm  ? -.............. ?  It--- ? ? ----- ? ?     0.1 0.2 0.3  Fig. 3 Comparion result of algorithm time consumping with Anneal data set   ? -+-Aprior -'-Granule computing  '" -,-New algorithm  ? "'-.

?  ------ ? ----  ---,    ----  ----  " o  O. 1 0.2 0.3  Fig.4 Comparison result of algorithm time consuming with Mushroom data set  With different minimum support, we can see, the algorithm efficiency of the optimal decision rule discovery based on granule computing is higher, and it can save half of time than Aprior algorithm at least. The effect of Granule computing to increase efficiency is smaller than the optimal decision rule, so its way to improve efficiency through    deleting candidate set is very valid. As the minimum supports reducing the time consuming of three algorithms are close gradually. It is because the candidate sets reduce gradually as filtering effect of support is stronger, which equal to operable object decrease to improve efficiency, so the time to save become finite.



VI. CONCLUSION  The paper proposed a optimal decision rule algorithm based on granule computing that is fit for association rule mining expressed by decision table. The algorithm improves pruning efficiency from two ways, increase the speed of generating frequent item sets by granule and delete invalid candidate sets by the optimal decision rule. The experiment proves the algorithm is valid.

