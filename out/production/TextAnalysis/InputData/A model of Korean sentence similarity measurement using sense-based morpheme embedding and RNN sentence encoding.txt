A Model of Korean Sentence Similairty Measurement  Using Sense-based Morpheme Embedding and RNN

Abstract?Sentence similarity measurement is an important technology that can be apply to various applications in the natural language processing. Recently, an encoder-decoder model using recurrent neural network (RNN) has achieved remarkable results. This paper proposes a model for measuring Korean sentence similarity based on sense-based morpheme embedding and gated recurrent units (GRU) encoder. We evaluate the measurement model consist of experimentally optimized morpheme embedding and sentence encoding models. In the measurement of sentence similarity, the proposed model encoded using the pre-trained morpheme embedding improves the performance compared with the character-embedding model. In addition, it can be used effectively in the question and answering (Q&A) system.

Keywords?vector representation; word embedding; RNN encoder-decoder; sentence similairty

I.  INTRODUCTION Recently, many studies utilize vector representation  actively in various research and applications in natural language processing (NLP). The vector representation is a model of word or phrase vectors expressed by a continuous real number. By expressing the implicit information such as language model or word sense as the multidimensional vector, it is possible to reduce the cost of analysis and complex feature engineering. The vector representation become more important for various applications, such as question and answering (Q&A) [1], document summarization [2], and paraphrasing [3].

The deep learning model implements the latest model of vector representation. Word2Vec [4] using shallow neural network and Phrase2Vec [5] extending it to the phrase level are representative. In addition, recurrent neural network (RNN) models apply to sentence encoding [6] for learning sequential data, e.g. time series and sensory data.

When constructing the model using the deep learning methods, many parameters can be modified in the vector representation model, instead of requiring complex feature extraction. However, there has not been much research yet how better models can be trained in Korean corpora.

In this paper, we provide two vector representation models with several Korean corpora: sense-based morpheme embedding improved from word2vec [4], and sentence encoding based on gated recurrent unit (GRU) [6]. In order to construct a model for measuring sentence similarity, we try to find out how to set and learn the model based on the above both models.

The contributions of our work are summarized as follows:  ? We adjust the hyper-parameters and find an optimal model that extracts the best performance at each stage, and use the model for the sentence similarity analysis.

? We make an optimal model of measuring sentence similarity through various experiments. In particular, we apply the model to the Q&A cases to measure the performance and see if it is useful.

The rest of this paper is as follows: Section II introduce a morpheme-embedding model for pre-training and a sentence encoding models based on GRU. In addition, we propose a model for measuring sentence similarity in Section III. We make the pre-trained morpheme-embedding model based on word sense and morpheme features using Word2Vec. The sense-based morpheme embedding model is used as input to GRU model for the sentence encoding. In Section IV, we evaluate quality of the morpheme embedding models depending on the dimensionality of the morpheme vectors. In addition, we evaluate performance of the model for measuring sentence similarity. In section V, we conclude with some ongoing and future works.



II. SENSE-BASED EMBEDDING AND SENTENCE ENCODING In this section, we describe the own implemented models  into two parts: The previously described part is a sense-based morpheme-embedding model for pre-training used in sentence embedding. The rest of this section provides the sentence- encoding model based on gated recurrent units (GRU). The model converts a sentence into a vector as the outcome, which is from the recurrent neural network (RNN) state.

A. Sense-based Morpheme Embedding  Fig. 1. A model for sense-based morpheme embedding.

The sense-based morpheme embedding model is shown in the Fig. 1. The model trains word by morpheme level to be suitable for language model analysis of Korean. The Korean is a kind of the agglutinative language, which is composed of a combination of a stem morpheme and an ending morpheme.

The model used in this paper focuses on stem morphemes with meaning. The ending morpheme can learn the probability of combining with stem morpheme in a separate language model.

At first, we select the Korean corpus to learn the model. We use corpus of Korean Wikipedia (https://dumps. wikimedia.org /kowiki/) which is a Korean web encyclopedia. In Korean Wikipedia corpus (as of June 2016), the corpus was formed by parsing only title and text. Total 2,619,773 English and Korean sentences and 77,172,818 morphemes were analyzed by ETRI morpheme analyzer include numbers and symbols.

Fig. 2. A model for GRU-based sentence encoding.

B. Sentence Encoding Using RNN (GRU) The morpheme vectors of each morpheme are loaded from  the result of the pre-trained morpheme embedding using the Word2Vec [4] model. The loaded morpheme vector is input and output to the RNN model. Where the input layer of the RNN model is the morpheme-embedding vector of the current morpheme and the output layer is the morpheme-embedding vector of the next morpheme.

The model for GRU-based sentence encoding is shown in Fig. 2. when learning the first morpheme, the morpheme- embedding vector corresponding to "James99NNG" as an input into the RNN (GRU) model learned by the next morpheme "Earl02NNG" as an output. In this paper, we use a 300 dimensional morpheme-embedding vector. Finally, the parameters of the GRU in/out/forget gate are stored in the learning model.

This sentence vector encodes using a 300?300 dimension of softmax layer of a single layer. Finally, we have learned to minimize the L2 error for the decoded result and the displaced output.



III. SENTENCE SIMILARITY MEASUREMENT MODEL  Fig. 3. A method of sentence similarity measurement.

The learned model extract the sentence vector for the evaluation sentence through the Korean encoder-decoder based on language model. A GRU-based encoder that expresses a sentence as a vector can output state information every repetition of the sentence length. In this paper, a fixed 300- dimensional vector is output from the GRU as shown in Fig. 3.

We calculate cosine similarity as the similarity value between two sentence vectors: source and target sentences. The formula of the cosine similarity is as follows Eq. (1). The similarity value has a range from -1 to 1, and the closer to 1, it means that the both sentences are similar sense.

We have implemented the GRU model with character-level and morpheme-level as follows to compare performance.

(1)         A. Character-based Sentence Encoding In this model, the input sentence decodes into phonemic  units, and the GRU model learns the character language model to sentence vector. It is an early implementation model and devised as a way to learn the language models without analyzer for morphemes and word sense disambiguation (WSD).

Fig. 4. A model for character-based sentence encoding.

The trained language model analyze a sentence to initial, medial, and final consonant level. The results of the final RNN states defines as a sentence vector. The model is a bi- directional GRU model as shown in Fig. 4.

B. Morpheme-based Sentence Encoding  Fig. 5. A model for morpheme-based sentence encoding.

To overcome problems of the model for the character-based sentence encoding, we design a new encoder as shown in Fig.

5. The phonemes do not have a meaningful information, since the phonemes are too small to distinguish the meanings of a word. Therefore, the model shows only similar patterns of phonemes, not reflected semantic similarity. The new encoder trains the morpheme language model derived by the pre-trained model of the sense-based morpheme embedding.



IV. EXPERIMENTS AND EVALUATION  A. Evaluation of Morpheme Embedding In this experiment, we found out the optimal vector size for  the best quality of the sense-based morpheme-embedding model. We evaluated the quality using Precision @ K method.

We extracted 10 most similar words by corpus, exclusive of the words which had no synonyms, among top 100 frequently appeared in Korean Wikipedia corpus and Namuwiki corpus.

Three annotators evaluated the most similar word list of 84 words as accept or reject. The Fleiss's Kappa value of the annotators? agreement was 0.289 (fair agreement). The embedding models set min-cut to 10 and iteration to 10.

Fig. 6. Ratios of accepted synonyms in the most similar words.

As shown in Fig. 6, when the dimension size was 300, the model extract the most number of synonyms from the list of 10 most similar words. We could confirm that the embedding models were good quality, when the vector size was from 100 to 350 from the both corpus. When applying the word embedding method to applications, it was recommended to use an embedding models made of a 100 or more dimensional vector size for Korean corpus.

Fig. 7. A PCA analysis result of sentence encoding.

B. Evaluation of Sentence Encoding Fig. 7 shows the PCA analysis of sentences in the  evaluation set. We extended this to [0.5, 0.51] and compared the TF-IDF of the whole region with respect to the morpheme except the upper 10 symbols of the partial region. When extracting 255 sentences distributed in a specific area, we could qualitatively confirm that extracted many sentences related to chemistry and science.

C. Evaluation of Sentence Similarity Measurment We constructed the evaluation set to measure sentence  similarity using morpheme embedding and GRU encoder in the Q&A system. We used a quiz corpus that consist of pairs of question and answer. The questions taken from the Korea TV show ?JangHak quiz? and the evidence was a sentence that could reason answers for the questions. Experimental data used only pairs of correct question-answer pairs in total 19 million evaluation sets. After eliminating redundancy, there were 8,486 valid pairs and 128,793 invalid pairs for 2,882 questions. After sorting according to the similarity using the proposed measurement model, we counted the number of pairs with correct pairs in Top 1 and Top 5 as shown in Table I.

TABLE I.  RATIOS OF CORRECT QUESTION-ANSWER PAIRS    As a result, the morpheme-based RNN model proposed in this paper was better than character-based RNN in both cases.

The table shows that the proposed model can apply to the analysis of sentence similarity for the query response system.

In addition, we also evaluated 200 number of question- answer pairs manually extracted from the evaluation sets by two annotators. The 5-Likert scale criteria for manual evaluation were as follows:  ? 1 point: not relevant ? 2 points: A portion of the answer is related to the  question, but the pairs do not describe the same topic ? 3 points: A portion of the answer is related to the  question, and the pairs described the same topic ? 4 points: A lot of question and answer pairs are  relevant, and a description of the same topic ? 5 points: The pair of the question and answer are  almost the same  Table 2 shows a result of manual evaluation for the sentence similarity measurement. 2 points were not similar, and over 3 points were analyzed with similar sentences. As a result, the Kappa coefficient was 0.489, which indicates that the manual evaluation results are good agreement from each other.

TABLE II.  MANUAL EVALUATION OF SENTENCE ENCODING MODEL  a. Cohen?s kappa value between two annotators:0.489

V. CONCLUSION We proposed a model for measuring sentence similarity of  Korean. We evaluated the model based on using the optimized sense-based morpheme embedding and gated recurrent units (GRU) encoder. We found out the best dimensionality for the sense-based morpheme embedding models experimentally in Korean corpora. We evaluated whether it could utilize to find valid answer for a given question in a question and answer (Q&A) system. The proposed sentence encoder using the pre- trained morpheme embedding model improved the results of evidences searching for questions compared with the character- embedding model. In addition, we could also find errors in the evaluation set.

In future works, we will improve the measurement model to grasp the negative meaning using input feeding and attention algorithms. In addition, we will refine the evaluation sets for conducting various experiments using more objective evaluation metrics. The measuring model of sentence similarity is currently limited to the question and answer (Q&A) system.

We expect that the proposed model can adapt to various natural language processing applications such as sentence generation, similarity analysis, and document summarization.

