

Section title Editors: Konrad Hinsen, hinsen@cnrs-orleans.fr | Konstantin L?ufer, laufer@cs.luc.edu   Leadership Computing Editors: James J. Hack, jhack@ornl.gov | Michael E. Papka, papka@anl.gov  Next-Generation Machines for Big Science  James J. Hack | Oak Ridge National Laboratory Michael E. Papka | Argonne National Laboratory and Northern Illinois University  A ddressing the scientific grand challenges identi- fied by the US Department of Energy (DOE) Office of Science programs alone demands a to- tal leadership-class computing capability of 150  to 400 Pflops by the end of this decade. The successors to DOE?s three most powerful leadership-class machines are set to arrive in 2017 and 2018?the products of the Collaboration Oak Ridge Argonne Livermore (CORAL) initiative, a national laboratory?industry design/build ap- proach to engineering next-generation petascale computers for grand challenge science. These mission-critical ma- chines will enable discoveries in key scientific fields such as energy, biotechnology, nanotechnology, materials sci- ence, and high-performance computing and will serve as a milestone on the path to deploying exascale computing capabilities.

Meeting a Need Since 2012, three US DOE supercomputers?two that sup- port open scientific research and one focused primarily on nuclear security?have been among the top five fastest ma- chines in the world. Each has followed a familiar life cycle: a debut on the international  supercomputing scene followed by years of robust use before eventually meeting their limits in the face of increasing demand.

Such is the fate of machines whose capabilities represent the ?highest domestic priority? with respect to DOE mis- sion needs.

The US faces serious economic, environmental, and na- tional security challenges based on its dependence on fossil fuels and the need to be energy independent. These machines, hosted in the DOE Office of Science?s Leadership Comput- ing Facility centers at Argonne National Laboratory and Oak    Leadership Computing  64  July/August 2015  Ridge National Laboratory and at the DOE?s Na- tional Nuclear Security Administration?s laboratory, Lawrence Livermore National Laboratory, are the computational engines that are helping research- ers across a broad range of disciplines look for new alternative energy sources and develop new energy technologies.

Teams of technicians, performance engineers, domain scientists, and computational scientists are needed to prepare these behemoths to effectively and efficiently execute the massive computations driving high-end simulation science and modeling.

From day one, the petascale capabilities of these machines are maximized: jobs run round the clock throughout the year.

Research teams are given time (typically tens of millions of hours, or core-hours) to investigate many of the fundamental questions in science to- day?to capture new opportunities in combustion science, battery technology, materials science, and fusion energy. What these teams can look for and learn is limited only by the machine?s physical ca- pabilities, power constraints, and contention for a shared resource.

Maintaining US leadership in computational sci- ence and engineering requires the best  resources? including a succession of computers with world-class speed and capability. All the major priorities highlighted in the DOE?s 2014 strategic plan?preparing for climate change, securing our leadership in clean energy technologies, maintain- ing science and engineering as a cornerstone of our economic prosperity, and enhancing our nuclear security?depend critically on advanced comput- ing and have sharply focused the mission need for leadership-computing facilities.

Increasing the supercomputing capability to hundreds of petaflops, a process now under- way, is vital to the US achieving its goals?and to achieving exascale computing down the road.

These pre-exascale systems will be many times more powerful than their predecessors, they?ll be diverse (providing two distinct solutions), and they?ll be deployed and operational within a 2017?2018 timeframe.

These new supercomputers are the outcome of the CORAL partnership, formed in early 2014 to simplify and streamline the processes to procure for each lab a radically more powerful and archi- tecturally diverse pre-exascale system, with the long term goal of delivering exascale systems that will be 20 to 40 times faster than today?s leading supercomputers.

CORAL aggregated the expertise of three na- tional labs to define the ideal system requirements for their users?the scientific community?and then provided multiple large-machine awards as an incentive to industry to align their business plans to design and develop systems to meet these needs.

CORAL issued a single request for proposals for multiple laboratory acquisitions where the win- ning bids would both deliver and support the new architecture. In this way, vendors would have a large stake in the success of these pre-exascale ma- chines, and the CORAL partners could encourage and fund applied research to find a specific solution to meet a specific need that could be hardened as a deliverable.

The CORAL partnership selected two mul- tivendor bids: IBM/Nvidia/Mellanox and In- tel/Cray. Oak Ridge and Lawrence Livermore announced their private-sector partners last fall.

Computer manufacturer IBM, along with Open- POWER Foundation partners Nvidia and Mel- lanox, would build two separate machines using a heterogeneous architecture, or one that couples central processing units (CPUs) with general-pur- pose graphics processing units (GPUs).

The Oak Ridge Leadership Computing Facil- ity?s (OLCF?s) Summit, the IBM machine sched- uled to arrive in 2017, will deliver more than five times the computational performance of Titan?s 18,688 nodes, while using roughly 3,400 nodes.

Summit?s architecture will consist of nodes with multiple IBM POWER9 CPUs and Nvidia Volta GPU accelerators, using a coherent memory space that includes high-bandwidth memory on the GPUs and a high-speed NVLink interconnect be- tween the POWER9 CPU and Volta GPUs. Sum- mit?s peak computational capability is expected to meet or exceed 150 Pflops.

This past spring, the Argonne Leadership Computing Facility (ALCF) announced that tech- nology manufacturer Intel and computer manu- facturing company Cray would deliver a massively parallel manycore system 18 times more powerful than Argonne?s current system, Mira. It will be based on the third-generation Xeon Phi family of chips from Intel.

The new system, Aurora, is scheduled to ar- rive in 2018 and will be built using these advanced processors, second-generation Intel Omni-Path Ar- chitecture interconnect technology, a new memory architecture, and a Lustre-based parallel file sys- tem?all integrated by Cray?s high-performance    www.computer.org/cise     65  computing software stack. Intel will coordinate the overall system and silicon design and integration, while Cray will provide software stack expertise and its large system manufacturing capabilities. Aurora?s peak computational capability is expected to be at least 180 Pflops.

Ramping Up To ensure that, starting from day one, the capa- bility of a leadership machine is sufficient to meet the scientific challenges that it will eventually support, the Leadership Computing Facility cen- ters run separate programs that provide training and code support to a small number of project teams and then turn them loose on the machines.

These Early Science Program projects (science application codes) represent a large portion of the machines? current and projected computational workloads.

OLCF?s Center for Accelerated Applica- tion Readiness (CAAR) is currently working with 13 partnership teams to refactor and port their codes for Summit. Teams consist of core applica- tion developers and OLCF staff and receive sup- port from the IBM/Nvidia Center of Excellence at Oak Ridge. The CAAR application readiness phase is scheduled to continue through spring 2017 and involve intermediate versions of the Summit architecture while the full-scale system is being constructed.

The ALCF?s Early Science Program will also award grants of preproduction time to project teams that will consist of application developers and facility staff, along with domain science ex- perts. The Early Science Program for ALCF-3 will have two phases and two separate calls for partici- pation. The first, Theta, targets an early production system based on Intel?s second-generation Xeon Phi processor. The second, Aurora, targets the fully integrated system. (The call for participation for Aurora is expected in mid-2016.) The Early Science teams for both systems will receive technical sup- port from the ALCF.

As Office of Science?designated scientific user facilities, OLCF and ALCF support ambitious sci- entific initiatives for a wide range of investigations through DOE?s highly competitive user programs, INCITE (which covers a broad range of research campaigns) and ALCC (which targets DOE mis- sion-specific investigations). These centers provide their user communities with unique opportuni- ties and resources to explore their science at scales otherwise unavailable, thus allowing more rapid  progress and yielding new insights that come from higher-fidelity investigations and simulations that incorporate more complete treatments of the phe- nomena being modeled.

Because many federal agencies rely on the Leadership Computing Facility to conduct impor- tant parts of their research, DOE will gather and take into account the high-performance comput- ing needs of those agencies in this next phase of petascale computing. Even as these next-generation machines are being rolled out, DOE has started planning the evolution of its leadership-computing capabilities for the next decade.

