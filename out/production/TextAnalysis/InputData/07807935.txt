

Abstract? Pattern discovery is the important part of knowledge discovery in Database , comes under Data mining .To  discover useful patterns ,association rule mining is one of the  most popularized and revealing technique in data  mining.Association rule mining plays a key role in decision  making by discovering useful relations between attributes in the  database.For this, first Frequent itemsets need to calculate  followed by Candidate itemset.While generating frequent  itemsets,frequent-1 itemset can be generated easily. But frequent  2-itemsets suffered from both time and space complexity.More  overhead and space complexity  occurred in a generation of  frequent 2-itemset is the issue of this paper.For more I/O  throughput it is essential to generate frequent itemsets as fast as  possible and space efficient.To possible this,intermediate data  generated by pairing each item with another item in itemset  needs access of random read/write. To access random data for  low latency, Apache HBase is the solution.Based on performed  results,it is shown that if dataset stored through HBase on  HDFS,space and time complexity can be achieved better with  Apriori MapReduce algorithm for finding association rules .

Keywords?Pattern discovery, Data mining, Hadoop,  Distributed system,  HDFS, MapReduce, HBase

I.  INTRODUCTION Data mining also called as Knowledge Discovery in  Database(KDD),is always the focused issue in Information  mining research.Its objective is extracting hidden patterns and  rules from a large dataset which are unknown to human life.

This can be possible using Apriori algorithm implemented on MapReduce framework.After generation of association  rules,we can work effectively in fraud detection, Market  basket analysis, Medical diagnosis, Protein Sequences,Census  data.For association rules, two basic and important steps are the generation of frequent itemsets and creation of candidate  itemsets based on user defined minimum support count.This  can be done using algorithm called as Apriori MapReduce  algorithm(AMA).This algorithm provides scalability as  execute in a parallel manner but suffered from somewhat time and space complexity in a generation of frequent 2-  itemset.With the help of AMA,HBase can provide the good  solution for space complexity and helps to minimize time  constraints.It nearly minimizes half of the time for accessing the records through HBase as compared to AMA with HDFS.



II. LITERATURE REVIEW  The sequential association rule mining algorithm i.e.

Apriori algorithm[1] scans the entire transaction database at a time in each pass and it incurs high I/O cost and communication overhead to the disk.

Ashwini A. Pandagale, Department of Computer Science and Engineering,  Walchand College of Engineering, Sangli, Maharashtra, India  (ashupandagale@gmail.com)  Anil R . Surve, Department of Computer Science and Engineering, Walchand  College of Engineering, Sangli, Maharashtra, India  (anil.surve@walchandsangli.ac.in)  When facing a large volume of the database, can be in petabytes,zettabytes will be impractical storage and processing on one node and process single threaded under big data situation with varying data features.

Big data has brought challenges to data mining such as  finding customer behavior helpful for business analytics,fraud  detection, Market basket analysis[9] [11], Medical diagnosis,,  Protein Sequences,Census data.With massive database it is  essential to get results in less time.Apriori algorithm could not  fulfill the time constrained requirements.To solve the above -  mentioned problems much of the work on Apriori algorithm is  done. R.Agrawal and Shrikant,founder of Apriori algorithm  presented parallel apriori algorithms like Count  Distribution,Data Distribution,Candidate Distribution[6].But  these parallel algorithms faced a problem of synchronization  and communication delay.MapReduce is itself an algorithm ,a  programming model for processing ultra large dataset on a  cluster of nodes which makes easier for implementation of  parallel mining algorithms introduced by Google[7].Mehul  Vora presented hybrid approach where HDFS contains data  and address of stored data maintained by HBase.This hybrid  architecture[4] makes easy and faster data search with  accuracy in a retrieval of the data which is a rigorous need for  low latency while accessing items for finding size 2-frequent  itemset. There are number of parallel algorithms proposed. S.

Singh analyzes the performance of the Apriori algorithm with  different data structures[8] using Hadoop cluster.More about  the implementation of HBase for quick and random access  data is explained further.

Remaining paper is organized as follows:Section III gives  a brief idea of the Apache Hadoop Framework components  ,including HDFS and MapReduce.It also contains features of  HBase.Section IV explained the methodology in detail with  the experimental setup.With this, performance analysis study  is explained for the results concluded in Section V.



III. HADOOP  Hadoop, an open source framework which is overseen by Apache Software Foundation,so called as Apache Hadoop [3].

As companies invest, much prefers open source software.Hadoop provides storage and processing stored data using map reduce.In 2004, Doug Cutting creates the Hadoop framework and reached at a top-level project in January 2008 by Apache Software Foundation.With Doug, many other contributors make Hadoop attention from the rapidly growing user community.

Hadoop project  is developed in Java includes mainly:  Hadoop Common: The collection of normal utilities that support other Hadoop modules.

Hadoop Distributed File System (HDFS): A distributed file system that provides high storage capacity and access to required application data.

Hadoop YARN: A structure as a central platform to deliver consistent operations, security, and data governance tools        across Hadoop clusters.

Hadoop MapReduce: A YARN-based system for parallel processing of large data sets.

A. Hadoop Distributed File System (HDFS)  Hadoop File System is well known distributed storage used by  Hadoop applications. It uses commodity hardware for  processing the data. Not at all like other appropriated  frameworks, HDFS is profoundly fault tolerant, scalable and  uses low-price hardware. To store tremendous data, the  documents are put away over various machines called as  Datanodes. These files are put away in a repetitive manner to  support replication in future data losses in case of failure.

Features of HDFS: ? Distributed parallel storage and processing. ?  ? ? Hadoop provides a command interface to collaborate  with HDFS. ? ?  ? The Namenode i.e. Masternode maintains the metadata and datanode keeps blocks of data at appropriate node which help the users to easily check the status of the cluster. ?  ? ? Streaming access (write once, read many times) to file  system data. ? ? ? Provides file authentication and validation. ?    B. MapReduce  MapReduce [7] is a handling strategy and a  programming model for appropriate processing based on  Java. Map and Reduce are the two important tasks of the  MapReduce algorithm. Map takes a dataset and converts it into intermediate data in the form of key/value pairs.

Secondly, reduce task, used for aggregation takes the output  from a map as an input and combines those <key, value>  pairs to give a count of each word. The reduce task can not  start until map job gets completed.The main characteristic of MapReduce over traditional systems is that it is scalable for  data processing. Under the MapReduce component, mappers  and reducers are primary data processing models.

MapReduce follows Write once Read many strategies.That means once an application is written in the MapReduce  form, scaling the application to run over hundreds, thousands  or even tens of thousands of machines in a cluster need only  configuration change. Scalability is the attraction of many  programmers because it doesn?t need to change the program code. And everybody wants to save from the hectic job  which is taken care by scalability.

C. HBase  HBase is a distributed, no SQL database [2] built on top of the Hadoop file system. Its main characteristic is column-oriented storage. It is a part of the Hadoop ecosystem  that provides random access of data to read/write in real time manner. We can store the data in HDFS either directly or through HBase. It is used whenever there is a need to write heavy applications. Whenever we need to provide fast random access to available data, HBase is the perfect solution.

? Table is a collection of rows. ?  ? Row is a collection of column families. ?  ? Column family is a collection of columns. ?  ? The column is a collection of <key, value>  pairs. ? HBase [5], internally, has special catalog tables named -  ROOT- and .META. These special catalog tables maintain the current list, state, and location of all regions buoyed up on the cluster. The -ROOT- table carries the list of .META.

table regions. While .META. table carries the list of all user- space regions.

Likewise HDFS and Map/Reduce, HBase also follow master/slave architecture. The responsibility of HMaster (master) for assigning regions to HRegionServers (slaves)  and for recovering from HRegionServer failures.

HRegionServer is responsible for managing client requests for reading and writing. HBase uses Zookeeper [12], another Hadoop subproject, plays an important role in the  management of HBase cluster. There is no support for the SQL query language in HBase,so called as noSQL. There is HBql project [10] which adds a dialect of SQL and JDBC  bindings for HBase.

Fig. 1. HBase  The following image shows column families in a column-oriented database:           Fig. 2. Example of data storage in HBase

IV. PERFORMANCE EVALUATION  A. Methodology  R.Agrawal  proposed  Apriori  algorithm. In  this  paper Apriori algorithm implemented with MapReduce for finding association rules is used.

Algorithm 1: Apriori MapReduce algorithm  Map transaction t in dataset to all Map nodes;  1. Scan the dataset to get the support S of each item.

2. min_sup = num / total  no. of items; 3. If support S is greater than threshold value  i.e.min_sup then add an item to frequent 1-itemset.

4. Compute frequent item set for each map node using  min_sup and collect all together in reduce phase.

5. Remove items that do not meet the min_sup.

6. Again to find frequent k-itemsets, calculate frequent item  set with an  additional item by joining in each map node.

7. Collect the frequent item set at the reduce node and count item frequencies compared with min_sup.

8. Remove the items that do not meet the min_sup in Reduce Node using prune( ).

This paper represents a systematic evaluation of a noSQL database HBase and shows the comparison of  performance by storing metadata (file address stored in HDFS) in HDFS using HBase.The actual dataset used for evaluation is  stored in HDFS.This approach for data arrangement achieves  time complexity and space complexity.Generating frequent-2 itemsets creates n*n items while pairing each item with  another items. It requires lots of space and time to find candidate frequent itemsets because of more read and write .

As a result, it suffered from location overhead. HBase can effectively useful in random access to read and write. If we  store metadata directly in HDFS it will take time to perform read and write operation.As actually metadata stored in local  file system of the operating system.And when it needs to fetch the block(a chunk of data),location of block is fetched  maintained by Namenode. It?s time consuming process.

B. Experimental Setup  Experiments were performed on Hadoop-HBase cluster of one master and four slaves is used to test the performance.Hadoop 2.7.1 version is used for installation.The hardware required for the master was a processor of Intel- Itanium having 8-core CPU and 64 GBs of RAM.All slaves with dual core machines and 4 GBs of RAM having 64-bit architecture each. Client was running on master node with Ubuntu 14.10.

C. Results  As mentioned earlier, the issue of this paper is to reduce the time and space complexity occurred during generation of frequent 2-itemset. For this, tests were conducted on Hadoop- HBase cluster with 1.37GB dataset having 5267657 items, 1692082 transactions. For different min_sup values, the time required to run the dataset is calculated as shown in Fig.3.

Minimum support is used to define how often a rule is applicable to a given dataset. The rules derived from min_sup are strong rules used in decision making.

Fig. 3. Time required according to different min-sup   Fig.4 shows a comparison of Hadoop cluster and Hadoop-HBase cluster for different number of nodes.While storing Data in HDFS,Namenode does frequent open and close to access the data when needed. Also it does not provide random access to Read/Write data.In contrast,Master Server in HBase opened for time until the process gets completed.With  this advent, it provides random Read/Write access.  This is mainly  required in generating and processing frequent 2- itemset. For results,we consider min_sup=0.33.

Fig. 4. Comparison of Hadoop And Hadoop-HBase for different number of nodes  Also the issue of space complexity is solved using HBase.As it provides column-oriented  database mainly used for large tables.



V. CONCLUSION  In this era,digital data is generated in amount having 3 V?s properties i.e. Volume,Velocity and Variety. This data is in the form of big data. For business intelligence,data should  be analyzed weekly or monthly to increase the sell of products.

To get the customer behavior  useful patterns are very helpful  ,can be achieved by finding association rules.We can achieve this by the implementation of Apriori algorithm on Hadoop  MapReduce framework.But still frequent 2-itemset suffered the problem of time complexity and space required to store  temporary data.It needs continuous access which can be  random read or write.HBase is the better solution to store metadata instead of directly HDFS.Also due to scalability, a  number of nodes can be added and all metadata will be maintained by HBase database.All the experimental results  shows the time required to access the items for pairing with each other and to prune the unnecessary itemsets using defined  min_sup get minimizes through HBase with a different number of nodes. From this,we reached the conclusion that  HBase is very effective to random read/write access. As a result,the problem in finding size 2-frequent itemsets gets  accelerated and provides space while pairing each element with other elements.This can be very effective for business  intelligence to find customer behavior and in decision making.

