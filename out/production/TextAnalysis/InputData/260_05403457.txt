An Algorithm for Mining Association Rules Based on  Sets Operation

Abstract?The discovery of association rules is an important data-mining task for which many algorithms have been proposed. However, the efficiency of these algorithms needs to be improved to handle real-world large datasets. In this paper, we present an efficient algorithm which is based on sets operation and compares it with traditional algorithms. The improved algorithm only needs to scan the database once to reduce computation time. Experiment results indicate that the new algorithm has good efficiency.

Keywords- data mining, association rules, frequent item sets, set theory

I.  INTRODUCTION Data mining has recently attracted considerable attention  from database practitioners and researchers because of its applicability in many areas such as decision support, market strategy and financial forecasts. Combining techniques from the fields of machine learning, statistics and databases, data mining enables us to find out useful and invaluable information from huge databases. Most large organizations regularly practice data mining techniques. One of the most popular techniques is association rule mining which is the automatic discovery of pairs of element sets that tend to appear together in a common context. An example would be to discover that the purchase of certain items in a supermarket transaction usually implies that another set of items is also bought in that same transaction.

The problem of association rule mining introduced by Agrawal et al. aims at finding frequent item sets according to a user specified minimum support and the association rules according to a user specified minimum confidence[1]. Finding frequent item sets is computationally more expensive than finding association rules. An efficient association rule mining algorithm is highly desired for finding frequent item sets.

Apriori, Apriori TID and Apriori Hybrid algorithms for association rule mining were developed by Agrawal et al.[2].

All these algorithms find frequent sets in a bottom-up fashion.

A combinatorial explosion of item sets occurs when the minimum support is set low amounting to a high execution time. Pincer search algorithm developed by Lin et al. is a 2- way algorithm that conducts a search in both bottom-up and top-down manner[3]. An additional overhead of maintaining  the maximal frequent candidate set and maximal frequent set is involved.

FP-Tree growth algorithm developed by Chen et al.

compresses the database into a conditional pattern tree and mines frequent item sets separately[4]. This algorithm incurs an additional cost by processing items in each transaction in the order of increasing support count and heavily uses memory when the dataset is large. Zaki et al. proposed an approach for finding frequent item sets using equivalence classes and hyper graph clique clustering[5]. Hyper graph clique clustering produces more refined candidate sets as compared to equivalence class approach but identifying cliques in a graph is an NP-Complete problem. A. Schuster et al. present a new distributed association rule mining (D-ARM) algorithm that demonstrates super linear speed-up with the number of computing nodes[6]. Tsay et al. present an efficient algorithm named cluster-based association rule (CBAR). The CBAR method is to create cluster tables by scanning the database once, and then clustering the transaction records to the k-th cluster table, where the length of a record is k. Moreover, the large itemsets are generated by contrasts with the partial cluster tables[7].

The work presented in this paper is a new set based approach for finding frequent item sets. The best feature of one scan algorithm is that it requires only one scan of the database.

The one scan algorithm aims at reducing the I/O drastically and the illustrative example is shown that the proposed algorithm is efficient and feasible.



II. PROBLEM STATEMENT Ming association rule is one of main contents of data  mining research at present, and emphasizes particularly is finding the relation of different items in the database. And the basic concept is described in Table 1.

Table 1. Basic concept of association rules Concept Explain Formula  confidence Probability of set Y appear only if X appear P(Y|X)  Support Probability of set Y and X appear simultaneity P(Y?X)  Expected confidence Probability of set Y appear P(Y)  2009 Second International Workshop on Computer Science and Engineering  DOI 10.1109/WCSE.2009.39   2009 Second International Workshop on Computer Science and Engineering  DOI 10.1109/WCSE.2009.39   2009 Second International Workshop on Computer Science and Engineering  DOI 10.1109/WCSE.2009.640     Lift  Ratio of confidence to expected confidence P(Y|X)/P(Y)  Given a transaction database DB, I={i1,i2,?,in} is a set of items with n different itemsets in DB, each transaction T in DB is a set of item (i.e. itemsets), so T?I.

Definition 1: Let I={i1,i2,?,in} be a set of items, then D={<Tid,T>| T?I } is a transaction database, where Tid is an identifier which be associated with each transaction.

Definition 2: Let X?I,Y?I, and X?Y=?, we called X?Y as association rule.

Definition 3: Suppose c is the percentage of transaction in D containing A that also contain B, then the rule X?Y has confidence c in D. If s is the percentage of transactions in D that contain A? B, then the rule has support s in D.

Definition 4: Hypothesis X?I, minsup is the smallest support. If the frequency of X is greater than or equal to minsup, then X is called as frequent itemset, otherwise X is called non-frequent itemset.

The Apriori algorithm was originally presented by Agrawal and Srikan. It finds frequent itemsets according to a user-defined minimum support. In the first pass of the algorithm, it constructs the candidate 1-itemsets. The algorithm then generates the frequent 1-itemsets by pruning some candidate 1-itemsets if their support values are lower than the minimum support. After the algorithm finds all the frequent 1-itemsets, it joins the frequent 1-itemsets with each other to construct the candidate 2-itemsets and prune some infrequent itemsets from the candidate 2-itemsets to create the frequent 2-itemsets. This process is repeated until no more candidate itemsets can be created.

Classical Apriori algorithm is shown as follows.

(1) C1= {candidate 1-itemsets}; (2) L1={c?C1|c.count?minsup}; (3)  for (k=2; Lk-1??; k++) do begin (4)    Ck=apriori-gen(Lk-1); (5)    for all transactions t?D do begin (6)        Ct=subset(Ck,t); (7)        for all candidates c?Ct do (8)           c.count++; (9)    end (10)   Lk={ c?Ck|c.count?minsup} (11) end (12) Answer = ?Lk; The association rule mining is a two-step process as  follows.

Step 1: Find all the frequent itemsets in the transaction  database. If support of itemset X, support(X)?minsup, then X is a frequent itemset. Otherwise, X is not a frequent itemset.

Step 2: Generate strong association rules from frequent itemsets. For every frequent itemset A, if B ?A, B??, and support(A)/support(B) ? minconf, then we have association rule B?(A-B).

The second step is relatively easy, and its algorithm generation can be found in the reference. The present focuses in research are to find highly efficient algorithms in the first  step. In this paper, we discuss how to mine association rules quickly based on sets operations.



III. SETS THEORY As an mathematic concept, set can not be defined  imprecisely. Generally, set consists of certain and distinguishable transactions. For a given item and set, we can judge that this item is belong to the set or not. If successful, this item is called its element. Of course, some sets may belong to other sets. The basic operations in set theory are including combination (?), intersection(?), supplement and minus. Let A and B are these given sets, and E is aggregated sets. These operations are defined as follows[8].

Combination A?B?{x | x?A or x?B}.

Intersection A?B={x?x?A and x?B}.

Supplement A?{x | x?E and x?B}.

Minus  A-B?{x | x?A or x?B}.

Theorem 1: Suppose X?T, Y?T, and X?Y. If Y is a  frequent itemset, then X must be a frequent itemset. That is (? X)(? Y)(Y?T and X?Y and Y.count ?mincount)?(X.count ?mincount).

Where X.count and Y.count stands for X?s count and Y?s count in the transaction, and mincount is the minimum support degree count.

Proof: Let T={t1,t2,?,tn} be records of D. According to the definition of the minimum support degree, we get the formulation mincount=minsup* |D|. Since Y is a frequent itemset, there are at least mincount records included Y. Since X?Y ,there are at least mincount records included X from the transitivity of the inclusive relationship. So, X.count ?mincount. That is, X must be a frequent itemset.

When we mines 2-itemset or k-itemset(k>2), all frequent itemsets? non-emptied subsets must be frequent ones. Thus, in Apriori algorithm, an association rule is applied to create candidate itemsets as follows[9].

Lk*Lk={p?q|p?Lk,q?Lk,p.item1=q.item1, ? , p.itemk < q.itemk}  (1) When k=1, the operation is single association.

Theorem 2: Suppose X and Y are itemsets, if X?Y, then support(X)?support(Y). That is, X is a subset of Y, the number of transaction in D containing X is not less than that of Y.



IV. THE IMPROVED ALGORITHM Finding frequent itemsets in transaction databases has  been demonstrated to be useful in several business applications. And the problem could be decomposed into two problems:  (1) Find out all frequent itemsets and their support counts. A frequent itemset is a set of items which are contained in a sufficiently large number of transactions, with respect to a support threshold minimum support.

(2) From the set of frequent itemsets found, find out all the association rules that have a confidence value exceeding a confidence threshold minimum confidence.

Since the solution to the second problem is straightforward, major research efforts have been spent on the first problem. The improved algorithm is also focus on the second problem and it is based on set theory. It is required to generate candidate itemsets, compute the support, and prune the candidate itemsets to the frequent itemsets in each iteration. The following description is its formalization process.

Improved_algorithm (T, minsup) Inputs: D: Transaction Database minsup: Minimum Support Outputs: Li: Frequent Itemset //support(Itemset) > = minsup Temporary variables: Ck: Candidate Itemset Steps: Get L1 for 1-itemsets; // L1 consists of Itemsets, support  set and support.

For  ( k = 2; Lk-1 ? ?; k++) do begin  Ck = gen ( Lk-1);// create frequent k-itemsets end Answer = ?Lk;  And the function gen(Lk-1) is given as follows.

gen(Lk-1) Inputs: Lk-1:frequent itemset,//supp(Itemset)>=minsup //and size of every itemset = k-1 Outputs: Ck: candidate Itemsets with Size k Steps: Ck={Lk-1.Itemset*Lk-1.Itemset} Tk={Lk-1.Tset?Lk-1.Tset}// Tset is the common support  set.

For all candidates c?Ck do  c.support=Count(c.Tk);//the support number If c.support < minsup then  delete c; end  end

V. EXPERIMENT AND ANALYSIS Let D = {T1 , T2 , T3 , ?, Tn} be a database of  transactions. Each transaction Ti={i1,i2,?,im} contains a set of items from I = {i1,i2,?,ip}, where p? m such that Ti? I. An itemset X with k items from I is called a k-itemset. A transaction Ti contains an itemset X if and only if X?Ti. The support of an itemset X in T denoted as support(X) indicates the number of transaction in D containing X. An itemset is frequent if its support, support(X) is greater than a support threshold called minimum support, i.e. minsup. For example, suppose we have a D ={T1,T2,?,T6} and I={A,B,C,D,E} where T1={B, C, D, E}, T2={B, C, D}, T3={A, B, D}, T4={A,  B, C,D,E}, T5={A,B,C} and T6={B,E}. The transaction database D is shown in the following Table 2.

Table 2 A simple transaction database Transaction ID Itemset T1 {B,C,D,E} T2 {B,C,D} T3 {A,B,D} T4 {A,B,C,D,E} T5 {A,B,C} T6 {B,E}  When scan the transaction database once, we may establish Table 3, which lists five 1-itemsets contained by the transactions. Thus, it is convenient to figure out the support of all candidate sets, for instance, support({A})=3, can be directly achieved from Table 3.

Table 3 The frequent 1-itemsets and their support 1-itemset transactions support A T3,T4,T5 3 B T1,T2,T3,T4,T5,T6 6 C T1,T2,T4,T5 4 D T1,T2,T3,T4 4 E T1,T4,T6 3  Suppose the minsup is 3. After obtaining the frequent 1-itemsets, we can combine them each other to get the intersection and calculate their support. If their support is less than minsup, we may cancel them. Thus, we can establish the frequent 2- itemsets, shown in Table 4.

Table 4 The frequent 2-itemsets and their support 2-itemset transactions support A,B T3,T4,T5 3 B,C T1,T2,T4,T5 4 B,D T1,T2,T3,T4 4 B,E T1,T4,T6 3 C,D T1,T2,T4 3  The frequent 3-itemsets are obtained with the formula (1). In fact, they are figured out by the intersection and combination of all the frequent 2-itemsets, i.e. L2*L2. In this example, only {B,C}, {B,D} and {C,D} may be combined each other to establish the candidate 3-itemset {B,C,D}, and their support sets are {T1,T2,T4}, given in Table 5.

Table 5 The frequent 3-itemsets 3-itemset transactions support B,C,D T1,T2,T4 3 Table 5 presents one frequent 3-itemsets. However, there are no 4-itemsets because no candidate 4-itemsets can be created.

Thus, the process stops.



VI. CONCLUSIONS Frequent itemsets mining is one of the most important  areas of data mining. Existing implementations of the Apriori based algorithms focus on the way candidate itemsets generated, the optimization of data structures for storing itemsets. In this paper, we presents an improved algorithm which the set theory is introduced. Compared with the     traditional Apriori algorithm, it scans the transaction database only once and calculate the support without access the transaction database to reduce computation time and its performance is enhanced. When the frequent (k-1)-itemsets are created, because of the decreasing data, we may not make the coupling among the data tables when the candidate k- itemsets are created, but the data structure for storing itemsets and Tsets, i.e. the support sets can be defined to simulate the coupling process. Thus, the velocity of mining will be improved.

