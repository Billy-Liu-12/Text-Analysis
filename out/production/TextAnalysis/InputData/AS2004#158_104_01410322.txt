Mining Ratio Rules Via Principal Sparse Non-Negative Matrix  Factorization

Abstract Association rules are traditionally designed to capture statistical relationship among itemsets in a given database. To additionally capture the quantitative association knowledge, F.Korn et al recently proposed a    paradigm named Ratio Rules [4] for quantifiable data mining. However, their approach is mainly based on Principle Component Analysis (PCA) and as a result, it cannot guarantee that the ratio coefficient is non-negative.

This may lead to serious problems in the rules? application. In this paper, we propose a new method, called Principal Sparse Non-Negative Matrix Factoriza- tion (PSNMF), for learning the associations between itemsets in the form of Ratio Rules. In addition, we provide a support measurement to weigh the importance of each rule for the entire dataset.

1. Introduction Association rules are one of the major representations in representing the knowledge discovered from large databases. The problem of association rule mining (ARM) in large transactional databases was introduced in [1, 3], Its basic idea is to discover important and interesting associations among the data items. The form of such association is as following: { , } (80%)bread milk butter=> To find association rules, most prevalent approaches assume the transactions only carry Boolean information and ignore the valuable knowledge inherent in the quantities of the items. In fact, considering that the quantities of the items normally contain valuable information for us, it is necessary to provide a definition of quantitative association rules when the datasets contain quantitative attributes. Several efficient algorithms for mining quantitative association rules have been proposed in the past [2, 7]. A notable algorithm is the work [4], where they provided a stronger set of rules as Ratio Rules.

A rule under this framework is expressed in the following form: : : : : ( , , ) bread milk butter a b c a b c is arbitrary numerical values = This rule states that for each a amount spent on bread, a customer normally spends b amount on milk and c amount on butter.

Principal Component Analysis (PCA) is often used to discover the eigen-vectors of a dataset. Ratio Rules [4] can represent the quantitative associations between items as the principal eigen-vectors, where the values a, b and c in the example above correspond to the projections of the eigenvector. Because the element of eigen-vector can be either positive or negative, sometime the ratio coefficient of Ratio Rules may contain negative value, such as : : 1: -2 : -5Shoe Coat Hat =    Obviously, such rule loses the intuitive appeal of associations between items, because a customer?s spending should always be positive.

Our method amounts to a novel application of non- negative matrix factorization (NMF) [5]. However, we cannot directly apply NMF for our purpose, because it is still difficult to explain that these latent components represent the latent association between items in a quantifiable dataset. We need to provide a bridge to bring NMF closer to association rules.

In this work, we propose a novel method called Principal Sparse Non-Negative Matrix Factorization (PSNMF), which adds the sparsity constraint as well as the non-negativity constraint in the standard NMF[5].

The rest of the paper is organized as follows: Section 2 describes the problem and the intuition behind the Ratio Rules. Section 3 introduces our new algorithm (PSNMF).

Section 4 presents the experimental results. Section 5 concludes the paper. The convergence of PSNMF learning procedure is provided in Appendix.

2. Problem Definition The problem that we tackle is as follows. Given a N M? matrixV (e.g., market basket databases), the entity ijv gives the amount spent by customers on the product.

The goal is to find all Ratio Rules of the form: (ICDM?04) 0-7695-2142-8/04 $ 20.00 IEEE (a) A data matrix with 2-dimension (b) Ratio Rules identified by PCA c)Ratio Rules identified by PSNMF Fig 1. A data matrix and latent associations discovered by PCA and PSNMF ( )1 2 3: : : ... : 0M iv v v v v ?

The above form means that customers who buys the items will spend 1v , 2v ? respectively on each itemset.

Fig1.(a) lists distribution of the matrix V which is organized with N customers and M ( 2)M = products Here we assume that the dataset is consisted with two clusters. Our goal is to capture the associations between items. We list two Ratio Rules discovered by PCA[4] in Fig.1 (b), where one contains negative values: : -0.77 : 0.64bread butter = Obviously, the negative association between items (?bread? and ?butter?) does not make sense. Furthermore, it is obvious that the Ratio Rules deviate with the latent associations behind the distribution of these points.

In fact, from Fig 1.(a), we find that the latent associations are not mutually orthogonal, while the method by PCA imposes the orthogonality constraint on these ratio rules. Therefore, Ratio Rules based on PCA    cannot truly reflect the latent associations among the items correctly. Compared to Fig.1 (b), Fig 1(c) illustrates the Ratio Rules captured by our proposed PSNMF.

Surprisingly, each rule could be treated as an association in the two clusters respectively.

3. Principal Sparse Non-Negative Matrix Factorization (PSNMF) Given a M N? non-negative matrixV , denote a set of P M basis components by a M P? matrixW , where each transaction (column vector) can be represented as a linear combination of the basis components using the approximate factorization: (1)V WH?

where H is a P N? coefficients matrix.

3.1 Non-negative Matrix Factorization(NMF) Because the entries of W and H calculated by PCA may contain negative values, NMF [5] is proposed as a procedure for matrix factorization which imposes non- negative instead of orthogonal constraint, and NMF uses the I-divergence of V fromY , which is defined as , ( || ) ( log ) (2)ijij ij ij i j ij v D V Y v v y y = ? + As the measurement of fitness for factorizing V into [ ]ijWH Y Y= , a NMF factorization is defined as , min ( ) . 0, 1 (3)ijW H i D V WH s t W H w j? = ??

The above optimization can be done by using multipli- cative update rules [5].

3.2 Sparse Non-negative Matrix Factorization (SNMF) Although NMF is successful in Matrix Factorization, the NMF model does not impose the sparse constraints.

Therefore, it can hardly yield a factorization, which reveals local sparse features in the dataV . Related sparse coding is proposed in the work of [6] for matrix factorization.

Inspired by the original NMF and sparse coding, the aim of our work is to propose Sparse Non-negative Matrix Factorization (SNMF), which imposes the sparse and non- negative constraint. Therefore, we put forward the following constrained divergence as objective function: ( )    , 1 2 3 ( || ) ( log ) + (4) , , ,..., .

ij ij ij ij j i j jij T j j j j pj v D V Y v v y l y l h h h h denotes the column of H ?= ? + =  where [ ]ijWH Y Y= , and ? obtained by experience was assumed a positive constant. As the measurement of fitness for factorizing V into [ ]ijWH Y Y= , a SNMF factorization is defined as: , min ( ) (5) . , : 0 0, 1 W H ij ij i D V WH s t i j W H and i w? ? ? ? =?

0 20 40 60 80 100 $spend on bread $s pe nd on bu tte r -20 0 20 40 60 80 100 -20    (0.64290.76595) $spend on bread $s pe nd on bu tte r (-0.765950.6429) 0 20 40 60 80 100 (0.63907 0.36093) (0.20718 0.79282) $spend on bread $s pe nd on bu tte r (ICDM?04) 0-7695-2142-8/04 $ 20.00 IEEE Notice that we have chosen to measure sparseness by a linear activation penalty (i.e. minimum the 1-norm of the column of H ). A Sparse solution to the above constrained minimization can be found by the following update rules: ( ) ( ) (6) ik kl kl il iki i ik klk w h h v w w h ?= +  (7)ljkl kl kj ljj j kl ljl h w w v h w h =    To make the solution unique, we further require that the 1- normal of the column vector in matrix W is one. In addition, matrix H needs to be adjusted accordingly.

(8)kl kl klkw w w= (9)kl kl klkh h w= It is proved that the objective function is non-increasing under the above iterative updating rules, and the convergence of the iteration is guaranteed (in Appendix).

3.3 Principal SNMF When the dataset V is decomposed with W and H , each column value of H represents the corresponding projection on the basis space W . As a whole, the sum of every row vector of H represents the importance of corresponding base. Therefore, we define a support measurement after normalizing every column of H : (10)kl kl klkh h h= Definition. For every rule (column vector) ofW , we define a support measurement: ( ) (11)i ij ij j ij support w h h= Consequently, we can measure the importance of each rule for the entire dataset by their support values. The more value of support implies the more importance of such rule for the whole dataset.

In order to select the principal k rules as Ratio Rules, firstly, we rank the whole rules in descending by the support value. And then, retain the first k principal rules as Ratio Rules because they are more important than others. About the selection of k value, a simple method is taken such as: ( ) min (12) ( ) k ii Mk ii support w threshold support w = =  >      From above (12), Ratio Rules are obtained effectively according that the sum of k support values of rules cover threshold (i.e.90%) of the grand total support values.

4. Experiments Synthetic dataset: We have applied both the PSNMF and the PCA to a dataset that consists of two clusters, which contains 25 Gaussian distribution points on x-y plain (generated with mu=[3;5], sigma=[1,1.2;1.2,2]) and 50 points on y-z plain.

(Fig2.)(Generated with mu=[3;5], sigma=[2,1.6;1.6,2]).

0 10 20 30 0 Y X Z Fig 2.Dataset with two clusters Table 1. Ratio Rules based on PSNMF and PCA PSNMF 1RR 2RR 3RR (X) 0.000 0.696 0.020 (Y) 0.493 0.304 0.980 (Z) 0.507 0.000 0.000 ( )iSum w 49.88 21.64 3.488 ( )iSupport w 0.665 0.289 0.046 (a)Based on PSNMF (b) Based on PCA Table 1.(a) lists all the rules and corresponding support values. After ranking such rules, Ratio Rules are obtained since 1 2( ) ( ) = 0.9535>90%support w support w+ .

:: : : 0 : 0.493 : 0.507 (0.6650) :: : : 0.696 : 0.304 : 0 (0.2885) rule X Y Z rule X Y Z => => For example, 2/3 transactions (the cluster with distribution on y-z plain) are mostly depended on 1rule and others on 2rule . Therefore, the corresponding support value (0.665) of 1rule does not contradict with intuition. Otherwise, Table 1.(b) lists the Ratio Rules by PCA which is difficult to explain the negative association obviously.

Real Dataset: NBA ( 459 11? ) This dataset comes from basketball statistics obtained from the 97-98 season, including minutes played, Point per Game, Assist per Game, etc. The reason why we select this dataset is that it can give a intuitive meaning of such latent associations. Table 2 presents the first three Ratio Rules ( 1RR , 2RR , 3RR ) by the PSNMF. Based on a general knowledge of basketball, we conjecture the 1RR represent the agility of a player, which gives the ratio of Assists per Game and Steals, is 0.206:0.220 1:1? . It means that the average player who possess one time of assist per game will be also steal the ball one time, and so does 2 (0.117 : 0.263 1: 2.25)RR ? . In this case, traditional method cannot give such information behind the dataset.

PCA 1RR 2RR (X) -0.52 0.72 (Y) -0.77 -0.15 (Z) -0.38 -0.68 (ICDM?04) 0-7695-2142-8/04 $ 20.00 IEEE Table 2. Ratio Rules by PSNMF from NBA field 1RR 2RR 3RR Games 0.450 Minute 0.013 Points Per Game 0.010 Rebound Per Game 0.117 Assists per Game 0.206 Steals 0.220 Fouls 0.263 3Points 5. Conclusion In this work, we proposed Principal Sparse Non- Negative Matrix Factorization (PSNMF) for learning sparse non-negative components in matrix factorization. It aims to learn latent components which are called Ratio Rules. Experimental results illustrate that our Ratio Rules are more suited for representing associations between items than that by PCA.

