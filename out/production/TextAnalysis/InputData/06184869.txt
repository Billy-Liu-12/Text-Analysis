Auto determining parameters in class-association mining

Abstract?This work proposes an approach to determine automatically parameters in classification rule mining based on association rules. Such parameters are the thresholds of support and confidence, and the maximal size of rules. The approach is based on statistical data get on the dataset during the mining process. In particular, the thresholds of support and confidence are not fixed, but varied dependently on each other and on the size of rules.

Keywords-Data mining; key itemset; association rule; classi- fication.



I. INTRODUCTION  The association rule based classification has received extensive attention in data mining and machine learning research [12], [13], [14], [19], [20], [21], [22]. The approach consists of two main steps: (i) Extraction of a set of high quality class-association rules, called a classifier, from the learning data, and (ii) Selection of significant rules from this set for classifying unseen data. In general, the high quality rules are defined on the notions of support and confidence. The support of a rule shows how frequent the rule is applied to the learning data, and the confidence represents the precision of the rule. For extracting high quality rules, the user needs to define the thresholds of support and of confidence. This is not a simple problem, and usually the user needs to carry out many experiments on the same dataset for adjusting those thresholds that naturally depend on the characteristics of the dataset.

This work proposes an approach to determine automati- cally the thresholds of support and confidence, and also other important parameters (e.g. the threshold of the maximal support, the maximal size of rules) that can affect the quality of the resulting classifier. The proposed approach is empir- ical and based on statistical data get on the dataset during the mining process. In particular, the defined thresholds of support and confidence are not fixed, but varied dependently on each other and on the size of rules.

A. Background  Association rule mining [1], [2] has important applications in marketing/sales-products, finance-stock market, medicine, geographic information, scientific discovery, etc. For exam- ple, the market basket data analysis [1] discovered: 80% of customers who bought cereals and sugar also bought milk.

Association rules (ARs) are rules of the form ? ? ? , where ? and ? are called itemsets: sets of items (valued attributes). When ? is just an item that represents a class label, ? ? ? is called a class-association rule (CAR). The size of an itemset ? is number of items in ? , and the size of an AR ? ? ? is number of items in ? ? ? . To define the interestingness of ARs, many measures are defined: support, confidence, laplace [6], gain [10], lift (interest) [3], conviction [5]. Among them, the most popular measures are support and confidence.

? Let ? be an itemset. The support of ? , denoted by ????(?), is the number of objects in the dataset that have all the items of ? . The relative support of ? is the ratio between ????(?) and the number of all the objects in the dataset. An itemset is called frequent if its support is above some threshold defined by the user.

? The support of ? ? ? is ????(? ? ? ).

? The confidence of ? ? ? , denoted by ????(? ? ? ),  is ????(??? )????(?) .

An AR is considered interesting if its support and con-  fidence are above some thresholds, usually denoted by ??????? (for support) and ??????? (for confidence). The thresholds ??????? and ??????? are usually defined by the user, after carrying out many experiments on the dataset.

A large threshold ??????? allows to limit the number of frequent itemsets. However, even with a large support con- straint, for a dense dataset the number of frequent itemsets can be still tremendous. The concepts of closed and key itemsets [15], [24] are interesting solutions to representative ARs. An itemset ? is called a key (generator) if there is no itemset ? ? such that ? ? ? ? and ????(?) = ????(? ?).

An itemset ? is called closed, if there is no itemset ? ? such that ? ? ? ? and ????(?) = ????(? ?). For an itemset ? , the closure of ? , denoted by ?(?), is a closed itemset such that ????(?) = ????(?(?)). For any itemset ? such that ? ? ? ? ?(?), we have ????(?) = ????(?) = ????(?(?)). Based on this property, an AR built on key and/or closed itemsets represents an equivalence class of ARs with the same support and the same confidence.

Apriori [2] and its variants are the algorithms for mining ARs that compute frequent itemsets by level-wise: candidate itemsets of size ? > 1 are generated on frequent itemsets of size ? ? 1. FP-growth [11] uses the prefix tree structure   DOI 10.1109/AINA.2012.18     for compactly stocking datasets in the main memory and recursively mines frequent itemsets in the depth-first manner.

Class-association mining is an approach to build classi- fiers based on association rule mining: CBA [14], CMAR [13], HARMONY [21]. Instead of computing one rule at a time, using heuristics based on statistical analysis (e.g., information gain) as in the classical approach [8], [17], the class-association mining approach computes a set of class- association rules, based on the methods for mining frequent itemsets.

In classification, CARs based on small size itemsets are efficient in time computing and classification accuracy [14], [13], [16]. The size of rules is an important parameters in class-association rule mining: a too small (or too large) size rule can be too general (or respectively too specific) for use in practice.

B. Related work  Using the Apriori or FP-growth like algorithms for mining ARs or CARs, without specific knowledge, users can have difficulties in setting ???????. A large ??????? can ex- clude many useful ARs or CARs, and a small ??????? can include many useless rules. For mining ARs, an approach to efficiently improve ??????? is to repeat the mining process and to vary the frequency constraint, using techniques for tightening or relaxing the frequency constraint (the increase or the decrease of ???????), based on the knowledge from the previous mining results that are summarized in a tree structure called the tree boundary [9].

HARMONY [21] is an efficient algorithm for building classifiers based on AR mining. An important difference between HARMONY and both CBA and CMAR is that during the CAR mining process, HARMONY does not need to specify the threshold ??????? . Instead, it maintains for each training object a top-? list of the highest confidence rules mined so far (? ? 1) that correctly classify the object. However, HARMONY needs to specify the threshold ???????. At the end of the process, the maintained rules associated with the training objects are regrouped into lists, by their class labels, to form the classifier. In those lists, rules are sorted in descending order of confidence, and the rules with the same confidence, are sorted in descending order of support.

ART [4] is a method for building decision tree that employs an association rule mining algorithm to efficiently build partial classification models. When a decision is taken, the objects in the input dataset that are not covered by the selected association rules are grouped together in the ?else? branches for further processing. In ART, the minimum support threshold is a percentage of the size of the current dataset (e.g., the remaining objects that are grouped in the ?else? branches). For automatically determining the threshold ??????? , ART uses the following approach: once the most accurate association rule has been found, only similarly  accurate rules are used to build the decision list, with some ?tolerance? interval.

The present approach to determine the thresholds ??????? and ??????? is different from ART, as we do not base on decision tree and we do not need to specify a percentage for determining ???????. In our approach, ??????? is varied with the current size of CARs. In particular, ??????? and ??????? are dependent on each other and on the size of CARs.

C. Contribution Outlines  This work proposes an approach to determine au- tomatically parameters in mining CARs, particularly, ???????,??????? , and the size of rules. The approach does not need to repeat the mining process, but can be ap- plied directly to the (unique) mining process. The approach is experimented on an algorithm for mining CARs based on small frequent key itemsets [16], using a set-enumeration technique [18]. The approach can also be applied by level- wise as the Apriori-like algorithms. Experiments on large and small UCI categorical datasets [7] show that the ap- proach is efficient.

In what follows, Section 2 summarizes the algorithm for mining CARs, using the set enumeration technique. Section 3 presents the approach to determine automatically parame- ters in mining classification rules based on frequent itemsets.

Section 4 shows the comparison results of the experiments of the methods. Finally, discussions and conclusions are in section 5.



II. MINING CLASSIFICATION RULES BY SET ENUMERATION  Figure 1 represents successive steps of the subset enu- meration of the itemset ???. The last prefix tree in this figure represents the enumeration?s result, i.e. the sub- itemsets ?, ??, ??, ???, ?, ??, ?. In the last tree, each node is associated with an item value, the support of the itemset that the node represents, and a list of couples. For instance, the first node is associated with itemset ?, support 1, and the list (? : 1). In general, in place of (? : 1) is a list (?1 : #?1, ..., ?? : #??), where ??, 1 ? ? ? ?, is a class label and #?? the number of ???s occurrences at the node.

For building the prefix tree ? of the itemsets extracted from a dataset ?, the basic algorithm, called ?????, works as follows. Beginning with ? initially empty, ????? succes- sively reads the objects of ?, and for each object (? : ?), where ? is a list of items and ? is a class label, ????? enumerates all sub-itemsets of ? and updates the prefix tree with these sub-itemsets and the class label ?. After reading all objects of ?, the prefix tree represents all itemsets extracted from ?, each one is associated with a support and a list of couples of class labels and occurrence numbers.

Though the prefix tree structure offers a compact represen- tation, the main memory can be overloaded, as the number     a a  c  d  a  c  d  c  d  a  c  a  c  d  c  dd  a  c  d  d  a  c  d  c  dd  d  1, (C:1)  1, (C:1)  1, (C:1) 1, (C:1)  1, (C:1) 1, (C:1)1, (C:1)  Figure 1. Building the prefix tree with the object (??? : ?).

of itemsets can be exponential with respect to the number of items. One solution is to set a maximal size for the extracted sub-itemsets and to adapt the level-wise computing for the enumeration technique. In fact, we can begin by reading ?, enumerating all the sub-itemsets of size 2 into the prefix tree ?, and pruning infrequent or non-key itemsets. Then re- reading ?, enumerating all the sub-itemsets of size 3 into the prefix tree ?, pruning infrequent or non-key itemsets, and so on until the itemsets of the maximal size are enumerated.

The maximal size of itemsets that can be extracted is an important parameter (subsection III-B).

At the end of the extracting process, using the resulting prefix tree, the method finds, for each training object, a list of the highest confidence and support rules that classify correctly the object. Those rules of all the training objects are grouped by class label to form the classifier. Detailed information of the method is in [16].

The method for classifying a new object is similar to the default method of HARMONY: for each class label ?, we search the classifier for all rules that cover the object and compute the sum of confidences of these rules. The object is predicted with the class label of the maximal sum.



III. AUTOMATICALLY DETERMINING MINING PARAMETERS  We do not tend to show formally the relations between the mining parameters and the characteristics of the datasets, but we observe these relations and through experiments on some datasets, we try to establish them as better as possible. Once the formulas of mining parameters are defined, we applied them in a unique mining process of each dataset that we show in section IV.

A. Confidence and rules? size  When building classification rules by exploring the prefix tree of itemsets, if the current node has only one class label, then we consider it as a leaf node and build a CAR.

Otherwise, we can ask if the path to the node is sufficiently developed, i.e. adding more items to the path does not result in better quality rules. In fact, each path from the root to a node corresponds to the left-hand side of a CAR. If the left- hand side is too short (or too long), then the corresponding rule can be too general (respectively, too specific) so that it is not useful for the classifier, because, perhaps, of low confidence (respectively, not applicable in practice, even with the high confidence). The good rule?s size can be adjusted based on the confidence of the rule. However, a rule with very high confidence can be very specific and then may not be applicable in practice. Therefore, we do not adjust the rule?s size on the highest confidence, but propose the following approach, based on the normal distribution.

Formally, let  ? ? be the set of class labels that appear in the dataset and ?? be the set of class labels that appear at the current node ?; ?? ? ?.

? ??? be the average of the expected supports of the class labels in ??, regarding the relative support of those classes with respect to the dataset. That is ??? =?  ????? ?????(??)??? ??  , where ?????(??) is the relative sup- port of ??, ?? is the number of the objects that have all the items on the path to ?, and ?? is the number of the class labels associated to ?.

? ??? be the standard deviation of the local supports of the class labels in ?? with respect to ???. That is, ??? =?  ?(???????)2 ??  , where ?? ? ?? and ??? is the local support of ?? (the number of occurrences of ?? at the node ?).

? ?? be the maximal local support with respect to the node ? and ??? be a class label in ?? with support ??.

To estimate the signification of a class label with respect to the path, we use the normal distribution. We know that a normal variable has 95% of chances to be in the interval of the average +/- 1.96 stddev (the standard deviation). Outside this interval are the specific values that can be considered to be significant. However, for the nodes that have a small number of class labels (e.g. 2), this interval is reduced to the average +/- 1.47 stddev.

Precisely, if the difference between ?? and ??? is significant, that is ?????? ? 1.96???? (or ?????? ? 1.47? ??? for 2 class labels), then we consider ? as a leaf, stop the development for the branch at p, and use the path to ? to build a significant CAR with the class label ???.

B. Maximal rules? size  Existing work in classification shows that small classifica- tion rules are sufficient to build a good classifier. The above approach to adjust the size of CARs can be reinforced by     setting a limit on the size (the number of items on the left- hand size of a CAR). In effect, we observe that the limit is proportional to the number of the class labels (?????) and the number of the objects (?????) in the dataset. When ????? is large, then naturally we need more rules in the classifier than when ????? is small. Moreover, when the dataset is large, it is normally built on a large set of items, and/or each object is naturally represented by a combination of many items, and therefore, the classifier could have more rules than when the dataset is small. A classifier with more rules has normally the rule with large left-hand side. However, such proportion is rather moderate, and is computed using the logarithm of ????? and the magnitude of ?????. Now, let ??????? be the maximal size of rules during the rule extraction.

??????? = ???{3, ????(2? ???(?????))} where ????(?) is the natural number ? such that ??1 < ? ? ?, and ??? denotes the natural (Neperian) logarithm. In case ??????? < 5 and ????? ? 10000, ??????? is adjusted using ?????:  ??????? = ??????? + ?????(???10 ?????  )  where ?????(?) is the natural number ? such that ? ? ? < ? + 1, and ???10 denotes the base 10 logarithm.

C. Dataset density  The density of a dataset represents the occurrences of the items in the dataset. This can be measured by the ratio of the number of the objects in the dataset to the number of all objects that can be created using all items of the dataset.

This ratio is of course in the interval [0, 1]. The more this number is near to 1, the more the dataset is dense. However, it is difficult to estimate the density using this measure.

Instead, we define another measure of density that is easy to compute and to use for estimating the density. In fact, a dataset is dense if we can find an item in almost every object. As a consequence, when we consider two datasets that have the same number of objects, for a dense dataset, the absolute support of an item is, in general, larger than the support of the same item of a non dense dataset. That is the density can be estimated by the ratio of the total of the supports of all items of the dataset to the number of the objects of the dataset. Moreover, we observe that the density is moderately in inverse proportion to the number of items and to the number of class labels of the dataset. Precisely, we define  ??????? = ???????  ????? ? ???(?????? ??????) where ???????, ?????, ?????, and ?????? represent re-  spectively the total of the supports of all items, the number of the objects, the number of class labels, and the number of all items of the dataset.

D. Maximal support  When a dataset is very dense, the items that appear almost in any object are not relevant for classification and should be removed from the dataset. Therefore, we define an upper threshold for the support, called ???????. This parameter is naturally in proportion to the number of the objects in the dataset. However, the more the dataset is dense, the larger the number of useless items is. That is the number of useless items is somehow proportional to the density of the dataset.

Therefore, the threshold ??????? is somehow in inverse proportion to the density. The experiments show that this inverse proportion is not direct, and weaker than the inverse proportion to the logarithm of density. In fact, we found that the following formula is appropriate for ???????.

??????? = ?????  ?  ???????  E. Minimal supports  ??????? is an important parameter that is fixed by most approaches to mining ARs. Setting this parameter is important because a large ??????? allows to expect the large application of a selected rule, but can exclude many other useful rules, and a small ??????? allows to include many rules, but these rules can be rarely applied in real ap- plications. The efficiency of the resulting classifier depends importantly on this parameter. Usually the value of ??????? is defined after carrying out many tests for each dataset.

In the present approach, we do not need to carrying out such many tests. The parameter ??????? is automatically determined during the unique mining process. Moreover, the value of ??????? is not fixed, but is defined in function of the current size of the CARs under consideration, during the CAR mining process.

For this, we compute the average ? and the standard deviation (??????) ? of the supports of itemsets generated in the current step. The ??????? is determined by the following formula:  ??????? = ???(2, ?  ???(?)? ???(1 + ?????) )  In fact, we consider that ??????? is proportional to the average of the supports of the itemsets of the current step and inversely proportional to the logarithms of ?????? and the number of class labels. Large ?????? means the support of the itemsets of the current step varies in a large interval, so we need to consider the small ??????? to cover enough itemsets.



IV. EXPERIMENTAL RESULTS  The present approach is called ??????? (Auto- determining parameters for a SIMple method for building classifiers). ??????? is implemented on ??? (a SIMple method for building classifiers) [16]. ??? and ???????     are implemented in ? and experimented on a laptop with a Intel Core Duo, P9500 2.53 GHz and 3.5 GB memory, running Ubuntu 8.10. As we can see ??? has several common and different points with HARMONY, and as HARMONY is competitive with many important approaches [21], this report uses ??????? as a reference. We shall use this laptop for running ??????? , ??? , and ??????? on the same 23 UCI categorical datasets (10 large datasets and 13 small datasets) obtained from the author of [7], using the 10-fold cross validation. Table I represents the characteristics of these 23 UCI datasets.

Table I THE 23 UCI DATASETS.

Dataset #obj #item #class 13 small datasets  anneal 798 106 6 auto 205 142 7  breast 699 48 2 glass 214 52 7 heart 303 53 5  hepatitis 155 58 2 horseColic 368 94 2 ionosphere 351 104 2  iris 150 23 3 pimaIndians 768 42 2  ticTacToe 958 29 2 wine 178 68 3 zoo 101 43 7  10 large datasets adult 48482 131 2 chess 28056 66 18  connect 67557 66 3 led7 3200 24 10  letRecog 20000 106 26 mushroom 8124 127 2  nursery 12960 32 5 pageBlocks 5473 55 5 penDigits 10992 90 10 waveform 5000 100 3  Table II recalls the report [21] on the accuracy comparison of HARMONY with FOIL [17], CPAR [23], and SVM (SVM?????????? version 1.01, http://www.cs.cornell.edu/ People/tj/svm light/svm multiclass.html), using the 10-fold cross validation. Based on this report, we carry out the statistical test of significant difference between the average accuracy of HARMONY and of FOIL, CPAR, and SVM.

The results of the test are shown in Table III . In Tables II and III, HAR, FOI, CPA denote respectively HARMONY, FOIL, and CPAR. Moreover, in Table III, column H/F and the next column Rk show respectively the difference between the average accuracy of HARMONY and of FOIL and the corresponding signed-rank used in the Wilcoxon signed- rank test on HARMONY and FOIL. Similarly, columns H/C (or H/S) and their corresponding column Rk show respectively the difference between the average accuracy of HARMONY and of CPAR (respectively, of HARMONY and of SVM??????????) and the corresponding Wilcoxon signed- rank test.

For the last three lines of Table III, the first line represents the average accuracy and the pairs of the signed-rank sums (in form of fractions). The second line (labelled Paired ? <) represents the ?-values of the Student paired tests. And the last line (labelled Wilco. ? <) represents the ?-values of the Wilcoxon signed-rank tests. These test results show that HARMONY is effectively competitive with FOIL, CPAR, and slightly competitive with SVM??????????.

Table II ACCURACY COMPARISONS OF HARMONY REPORTED IN [21].

Dataset FOI CPA SVM HAR 13 small datasets  anneal 96.9 90.2 83.8 91.5 auto 46.1 48.0 55.5 61  breast 94.4 94.8 96.8 92.4 glass 49.3 48.0 46.0 49.8 heart 57.4 51.1 60.4 56.5  hepatitis 77.5 76.5 81.83 83.2 horseColic 83.5 82.3 83.3 82.5 ionosphere 89.5 92.9 89.4 92.0  iris 94.0 94.7 94.7 93.3 PimaIndians 73.8 75.6 74.2 72.3  ticTacToe 96.0 72.2 70.8 92.3 wine 86.4 92.5 94.9 91.9 zoo 96.0 96.0 86.0 93.0  Average 80.1 78.1 78.3 80.8 10 large datasets  adult 82.5 76.7 84.2 81.9 chess 42.6 32.8 29.8 44.9  connect 65.7 54.3 72.5 68.0 led7 62.3 71.2 73.8 74.6  letRecog 57.5 59.9 67.8 76.8 mushroom 99.5 98.8 99.7 99.9  nursery 91.3 78.5 91.3 92.8 pageBlocks 91.6 76.2 91.2 91.6 penDigits 88.0 83.0 93.2 96.2 waveform 75.6 75.4 83.2 80.5 Average 75.7 70.7 78.7 80.7  The runnable program of HARMONY is kindly provided by the authors of [21]. The parameter setting for HAR- MONY is done as described in [21]: ??????? is set to 50 and 10 respectively for large and small datasets, and items are sorted in the correlation coefficient ascending order (the order with which HARMONY gets the best results in general). In particular, because connect and ionosphere are too dense datasets, only the items with supports no greater than 20, 000 and 190 respectively are considered to generate classification rules. This consideration is also applied for ??? , but not for ??????? , where all parameters are automatically determined during the mining process.

The common point of ??? and ??????? is: the itemset mining starts with itemsets of size 2.

The particular parameters of ??? are: (i) the maximal size of itemsets is set manually and, (ii) as HARMONY, ??????? is set to 50 and 10 respec-  tively for large and small datasets, but the infrequent itemset pruning is performed only for the first step, not for further     Table III STATISTICAL TESTS OF THE SIGNIFICANT DIFFERENCE BETWEEN THE AVERAGE ACCURACY OF HARMONY AND FOIL, CPAR, AND SVM,  BASED ON REPORT [21]  Datasets H/F Rk H/C Rk H/S Rk adult -0.6 4 5.2 13 -2.3 -9 chess 2.3 10.5 12.1 16 15.1 21.5  connect 2.3 10.5 13.7 19 -4.5 -16 led7 12.3 21 3.4 11 0.8 3.5  letRecog 19.3 23 16.9 22 9 20 mushroom 0.4 2 1.1 4 0.2 1  nursery 1.5 8.5 14.3 20 1.5 7 pageBlocks 0 1 15.4 21 0.4 2 penDigits 8.2 20 13.2 18 3 12 waveform 4.9 16 5.1 12 -2.7 -11  anneal -5.4 -17 1.3 5 7.7 19 auto 14.9 22 13 17 5.5 17  breast -2 -10 -2.4 -8 -4.4 -15 glass 0.5 3 1.8 7 3.8 13 heart -0.9 -6 5.4 14 -3.9 -14  hepatitis 5.7 19 6.7 15 1.37 5 horseColic -1 -7 0.2 1 -0.8 -3.5 ionosphere 2.5 13 -0.9 -3 2.6 10  iris -0.7 -5 -1.4 -6 -1.4 -6 pimaIndians -1.5 -8.5 -3.3 -10 -1.9 -8  ticTacToe -3.7 -15 20.1 23 21.5 23 wine 5.5 19 -0.6 -2 -3 -12 zoo -3 -14 -3 -9 7 18  Sums 192.5?68.5 ?38  ?94.5  Paired ? < 0.05 0.0007 0.0887 Wilco. ? < 0.05 0.01  steps, excepting for itemsets of support 1.

The following notations are used in Tables IV, V and VI.

? L-sec: the total runtime for building classifiers, from the  beginning of the execution of a program until the classifier is completely built.

? C-sec: the total runtime for classifying test datasets.

Note: ?the total runtime? means the total runtime in sec-  onds of the ten executions of each 10-fold cross validation; it is not the average runtime.

? Acc.: the average accuracy of prediction (in each 10- fold cross validation).

? Ms: the maximal size of itemsets to be extracted. For HARMONY, Ms is the result of the support equivalent item elimination or the unpromising conditional database elimi- nation [21]. For SIM, Ms is set manually. For AutoSIM, Ms is automatically determined by expressions in subsections III-A and III-B.

? Dens and Msup denote respectively the density of the dataset and the maximal support (???????) (subsections III-C and III-D).

? mSup: the interval of ??????? in descending order.

Recall that for AutoSIM, the threshold ??????? varies and is defined in subsection III-E  The statistical tests of the significant difference between the average accuracy of HARMONY and AutoSIM are shown in Table VII. The last three lines of this table have the same meaning as explained for Table III. On the results  Table IV EXPERIMENTS ON THE 13 SMALL DATASETS  HARMONY Dataset Ms L-sec C-sec Acc.

anneal 5 1.46 0 93.93 auto 6 25.1 0 72.00  breast 6 0.15 0.01 91.18 glass 5 0.48 0.01 68.58 heart 8 3.03 0.01 56.33  hepatitis 6 2.39 0 82.01 horseColic 5 5.95 0.01 81.68 ionosphere 6 392 0.02 87.71  iris 3 0.04 0.01 93.98 pimaIndians 7 0.48 0.02 69.22  ticTacToe 5 0.66 0.07 96.42 wine 4 0.44 0.00 90.57 zoo 5 0.19 0.00 89.00  Total/Avg. 432.27 0.27 82.51  SIM Dataset Ms L-sec C-sec Acc.

anneal 4 1.45 0.01 93.26 auto 3 0.89 0.02 73.00  breast 3 0.10 0.00 89.86 glass 5 0.28 0.00 70.00 heart 5 1.60 0.02 60.33  hepatitis 3 0.43 0.01 85.33 horseColic 4 3.63 0.03 83.89 ionosphere 4 8.77 0.05 90.57  iris 3 0.00 0.00 94.00 pimaIndians 3 0.39 0.00 68.42  ticTacToe 4 0.83 0.85 97.89 wine 4 0.55 0.00 94.12 zoo 3 0.14 0.00 91.00  Total/Avg. 19.06 0.16 83.97  AutoSIM Dataset Ms L-sec C-sec Acc.

anneal 4 0.87 0.02 94.04 auto 4 3.25 0.01 75.50  breast 3 0.11 0.00 91.59 glass 4 0.29 0.00 71.90 heart 4 0.50 0.02 59.33  hepatitis 3 0.18 0.00 80.00 horseColic 3 1.07 0.01 83.61 ionosphere 3 3.45 0.06 90.29  iris 3 0.00 0.00 93.33 pimaIndians 2 0.16 0.04 69.22  ticTacToe 3 0.52 0.01 93.05 wine 3 0.38 0.02 91.18 zoo 4 0.16 0.00 96.00  Total/Avg. 10.94 0.19 84.02  of the statistical tests shown in Table VII, we can conclude that although the parameters are automatically determined, AutoSIM is competitive with HARMONY, and on the results shown in Table III, we can say that AutoSIM is also competitive with FOIL, CPAR, and slightly competitive with SVM (SVM?????????? version 1.01).



V. CONCLUSIONS AND DISCUSSIONS  Through the experimental results we can conclude that the proposed approach to auto-determine parameters in CAR     Table V EXPERIMENTS ON THE 10 LARGE DATASETS  HARMONY Dataset Ms L-sec C-sec Acc.

adult 10 353.8 5.42 83.40 chess 4 4.98 2.75 44.93  connect 7 74.13 23.35 77.30 led7 7 0.82 0.7 74.35  letRecog 13 1106 4.36 70.82 mushroom 5 4.18 0.13 100  nursery 5 3.98 0.24 92.94 pageBlocks 6 3.62 0.07 91.17 penDigits 8 128.6 0.88 96.03 waveform 12 3840 0.78 77.92 Total/Avg. 5521 38.02 80.89  SIM Dataset Ms L-sec C-sec Acc.

adult 3 80.48 33.84 84.39 chess 5 21.35 14.22 60.31  connect 4 245.36 222.23 77.68 led7 5 1.41 0.07 74.34  letRecog 4 347.84 15.23 71.20 mushroom 3 21.28 0.45 100  nursery 5 14.28 1.60 98.34 pageBlocks 4 4.08 0.04 91.21 penDigits 4 141.45 4.62 96.99 waveform 3 25.16 1.14 79.84 Total/Avg. 901.7 268.8 83.43  AutoSIM Dataset Ms L-sec C-sec Acc.

adult 4 57.09 10.23 83.66 chess 6 26.27 14.33 60.12  connect 4 488.9 110.6 79.94 led7 5 1.56 0.05 74.44  letRecog 7 914.5 27.54 75.57 mushroom 3 21.71 0.44 99.64  nursery 5 14.63 1.61 98.34 pageBlocks 3 0.63 0.09 92.16 penDigits 5 427.7 7.67 97.42 waveform 3 27.95 0.94 80.06 Total/Avg. 1981 173.5 84.13  mining is efficient on both the runtime for building classifiers and the accuracy of the built classifiers. These results also confirm that classification rules of small size are sufficient for accurate classifiers. Indeed, almost all the maximal sizes determined by AutoSIM are strictly less than the maximal sizes determined by HARMONY, excepting the one for dataset chess. In this exception case, the larger size con- tributes to the accuracy of the classifier built by AutoSIM.

Moreover, we can see that beside the varied ??????? that efficiently contributes to the accuracy of classifiers, the proposed measure of density and the parameter ??????? are also interesting. In effect, the proposed measure of density corresponds to the density appreciation on connect and ionosphere. Using this measure, we can see that other datasets are also dense. For the dataset connect, the value of ?????? determined by AutoSIM is very different from the one given by HARMONY. We think that this difference  Table VI PARAMETERS COMPUTED BY AUTOSIM  13 small datasets Dataset Dens MSup mSup anneal 2.30 657 4:2 auto 3.72 133 2:2  breast 2.69 491 32:14 glass 1.64 170 2:2 heart 2.40 219 3:2  hepatitis 3.89 99 3:3 horseColic 3.30 246 4:2 ionosphere 5.93 202 3:2  iris 1.03 133 3:2 PimaIndians 1.87 591 2:2  ticTacToe 2.26 704 23:10 wine 2.47 128 2:2 zoo 2.91 69 2:2  10 large datasets Dataset Dens MSup mSup  adult 2.64 34472 64:10 chess 0.91 11400 37:2  connect 7.08 37278 138:17 led7 1.42 2639 54:9  letRecog 2.11 14937 37:2 mushroom 4.20 5108 69:42  nursery 1.63 10321 102:6 pageBlocks 1.90 4196 2:2 penDigits 2.41 7938 31:2 waveform 3.69 3245 27:12  Table VII STATISTICAL TESTS OF THE SIGNIFICANT DIFFERENCE BETWEEN THE  AVERAGE ACCURACY OF HARMONY AND AUTOSIM  Dataset HARMONY AutoSIM Difference Rank adult 83.40 83.66 0.26 4 chess 44.93 60.12 15.189 23  connect 77.30 79.94 2.640 15 led7 74.35 74.44 0.09 2  letRecog 70.82 75.57 4.75 20 mushroom 100 99.64 -0.36 -5  nursery 92.94 98.34 5.4 21 pageBlocks 91.17 92.16 0.989 9 penDigits 96.03 97.42 1.39 10 waveform 77.92 80.06 2.14 13  anneal 93.93 94.04 0.109 3 auto 72.00 75.50 3.5 19  breast 91.18 91.59 0.409 6 glass 68.58 71.90 3.320 17 heart 56.33 59.33 3.0 16  hepatitis 82.01 80.00 -2.01 -12 horseColic 81.68 83.61 1.929 11 ionosphere 87.71 90.29 2.580 14  iris 93.98 93.33 -0.650 -8 pimaIndians 69.22 69.22 0.0 1  ticTacToe 96.42 93.05 -3.370 -18 wine 90.57 91.18 0.610 7 zoo 89.00 96.00 7.0 22  Avg/Sums 80.89 84.13 233?43 Paired ? < p < 0.0115 Wilco. ? < p < 0.01     is an element, among the others, that contributes to improve the quality of the classifiers on connect, pageBlocks, auto, and zoo.

AutoSIM is a method for building classifiers based on CARs, where all parameters are automatically determined during the unique mining process. The approach combines the exploration of the data characteristics (to determine the mining parameters) and the extraction of CARs, in each step of a level wise subset enumeration. When the dataset is updated, to update the classifier, the CARs mining should be redone, and all parameters are automatically redetermined in function of the updated dataset. The method frees the user of the difficult task of determining those parameters. It is well appropriate for learning in the context of evolving data, such as the web context. However, the formulas of the parameters are built based on empirical observations.

For further work, we project to consolidate the method with more formal elements.

